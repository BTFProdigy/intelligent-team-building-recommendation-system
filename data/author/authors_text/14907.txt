Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 933?939,
Prague, June 2007. c?2007 Association for Computational Linguistics
Single Malt or Blended? A Study in Multilingual Parser Optimization
Johan Hall? Jens Nilsson? Joakim Nivre??
Gu?ls?en Eryig?it? Bea?ta Megyesi? Mattias Nilsson? Markus Saers?
?Va?xjo? University, School of Mathematics and Systems Engineering
E-mail: firstname.lastname@vxu.se
?Uppsala University, Dept. of Linguistics and Philology
E-mail: firstname.lastname@lingfil.uu.se
?Istanbul Technical University, Computer Engineering Dept.
E-mail: gulsen.cebiroglu@itu.edu.tr
Abstract
We describe a two-stage optimization of the
MaltParser system for the ten languages in
the multilingual track of the CoNLL 2007
shared task on dependency parsing. The
first stage consists in tuning a single-parser
system for each language by optimizing pa-
rameters of the parsing algorithm, the fea-
ture model, and the learning algorithm. The
second stage consists in building an ensem-
ble system that combines six different pars-
ing strategies, extrapolating from the opti-
mal parameters settings for each language.
When evaluated on the official test sets, the
ensemble system significantly outperforms
the single-parser system and achieves the
highest average labeled attachment score.
1 Introduction
In the multilingual track of the CoNLL 2007 shared
task on dependency parsing, a single parser must be
trained to handle data from ten different languages:
Arabic (Hajic? et al, 2004), Basque (Aduriz et al,
2003), Catalan, (Mart?? et al, 2007), Chinese (Chen
et al, 2003), Czech (Bo?hmova? et al, 2003), English
(Marcus et al, 1993; Johansson and Nugues, 2007),
Greek (Prokopidis et al, 2005), Hungarian (Csendes
et al, 2005), Italian (Montemagni et al, 2003), and
Turkish (Oflazer et al, 2003).1 Our contribution is
a study in multilingual parser optimization using the
freely available MaltParser system, which performs
1For more information about the task and the data sets, see
Nivre et al (2007).
deterministic, classifier-based parsing with history-
based feature models and discriminative learning,
and which was one of the top performing systems
in the CoNLL 2006 shared task (Nivre et al, 2006).
In order to maximize parsing accuracy, optimiza-
tion has been carried out in two stages, leading to
two different, but related parsers. The first of these is
a single-parser system, similar to the one described
in Nivre et al (2006), which parses a sentence deter-
ministically in a single left-to-right pass, with post-
processing to recover non-projective dependencies,
and where the parameters of the MaltParser system
have been tuned for each language separately. We
call this system Single Malt, to emphasize the fact
that it consists of a single instance of MaltParser.
The second parser is an ensemble system, which
combines the output of six deterministic parsers,
each of which is a variation of the Single Malt parser
with parameter settings extrapolated from the first
stage of optimization. It seems very natural to call
this system Blended.
Section 2 summarizes the work done to optimize
the Single Malt parser, while section 3 explains how
the Blended parser was constructed from the Single
Malt parser. Section 4 gives a brief analysis of the
experimental results, and section 5 concludes.
2 The Single Malt Parser
The parameters available in the MaltParser system
can be divided into three groups: parsing algorithm
parameters, feature model parameters, and learn-
ing algorithm parameters.2 Our overall optimization
2For a complete documentation of these parameters, see
http://w3.msi.vxu.se/users/nivre/research/MaltParser.html.
933
strategy for the Single Malt parser was as follows:
1. Define a good baseline system with the same
parameter settings for all languages.
2. Tune parsing algorithm parameters once and
for all for each language (with baseline settings
for feature model and learning algorithm pa-
rameters).
3. Optimize feature model and learning algorithm
parameters in an interleaved fashion for each
language.
We used nine-fold cross-validation on 90% of the
training data for all languages with a training set size
smaller than 300,000 tokens and an 80%?10% train-
devtest split for the remaining languages (Catalan,
Chinese, Czech, English). The remaining 10% of
the data was in both cases saved for a final dry run,
where the parser was trained on 90% of the data for
each language and tested on the remaining (fresh)
10%. We consistently used the labeled attachment
score (LAS) as the single optimization criterion.
Below we describe the most important parameters
in each group, define baseline settings, and report
notable improvements for different languages during
development. The improvements for each language
from step 1 (baseline) to step 2 (parsing algorithm)
and step 3 (feature model and learning algorithm)
can be tracked in table 1.3
2.1 Parsing Algorithm
MaltParser implements several parsing algorithms,
but for the Single Malt system we stick to the one
used by Nivre et al (2006), which performs labeled
projective dependency parsing in linear time, using a
stack to store partially processed tokens and an input
queue of remaining tokens. There are three basic
parameters that can be varied for this algorithm:
1. Arc order: The baseline algorithm is arc-
eager, in the sense that right dependents are
attached to their head as soon as possible, but
there is also an arc-standard version, where the
attachment of right dependents has to be post-
poned until they have found all their own de-
pendents. The arc-standard order was found
3Complete specifications of all parameter settings for all
languages, for both Single Malt and Blended, are available at
http://w3.msi.vxu.se/users/jha/conll07/.
to improve parsing accuracy for Chinese, while
the arc-eager order was maintained for all other
languages.
2. Stack initialization: In the baseline version
the parser is initialized with an artificial root
node (with token id 0) on the stack, so that arcs
originating from the root can be added explic-
itly during parsing. But it is also possible to ini-
tialize the parser with an empty stack, in which
case arcs from the root are only added implic-
itly (to any token that remains a root after pars-
ing is completed). Empty stack initialization
(which reduces the amount of nondeterminism
in parsing) led to improved accuracy for Cata-
lan, Chinese, Hungarian, Italian and Turkish.4
3. Post-processing: The baseline parser performs
a single left-to-right pass over the input, but it
is possible to allow a second pass where only
unattached tokens are processed.5 Such post-
processing was found to improve results for
Basque, Catalan, Czech, Greek and Hungarian.
Since the parsing algorithm only produces projective
dependency graphs, we may use pseudo-projective
parsing to recover non-projective dependencies, i.e.,
projectivize training data and encode information
about these transformations in extended arc labels
to support deprojectivization of the parser output
(Nivre and Nilsson, 2005). Pseudo-projective pars-
ing was found to have a positive effect on over-
all parsing accuracy only for Basque, Czech, Greek
and Turkish. This result can probably be explained
in terms of the frequency of non-projective depen-
dencies in the different languages. For Basque,
Czech, Greek and Turkish, more than 20% of the
sentences have non-projective dependency graphs;
for all the remaining languages the corresponding
4For Arabic, Basque, Czech, and Greek, the lack of im-
provement can be explained by the fact that these data sets allow
more than one label for dependencies from the artificial root.
With empty stack initialization all such dependencies are as-
signed a default label, which leads to a drop in labeled attach-
ment score. For English, however, empty stack initialization did
not improve accuracy despite the fact that dependencies from
the artificial root have a unique label.
5This technique is similar to the one used by Yamada and
Matsumoto (2003), but with only a single post-processing pass
parsing complexity remains linear in string length.
934
Attributes
Tokens FORM LEMMA CPOSTAG POSTAG FEATS DEPREL
S: Top + + + + + +
S: Top?1 +
I: Next + + + + +
I: Next+1 + +
I: Next+2 +
I: Next+3 +
G: Head of Top +
G: Leftmost dependent of Top +
G: Rightmost dependent of Top +
G: Leftmost dependent of Next +
Figure 1: Baseline feature model (S = Stack, I = Input, G = Graph).
figure is 10% or less.6
The cumulative improvement after optimization
of parsing algorithm parameters was a modest 0.32
percentage points on average over all ten languages,
with a minimum of 0.00 (Arabic, English) and a
maximum of 0.83 (Czech) (cf. table 1).
2.2 Feature Model
MaltParser uses a history-based feature model for
predicting the next parsing action. Each feature of
this model is an attribute of a token defined relative
to the current stack S, input queue I, or partially built
dependency graph G, where the attribute can be any
of the symbolic input attributes in the CoNLL for-
mat: FORM, LEMMA, CPOSTAG, POSTAG and
FEATS (split into atomic attributes), as well as the
DEPREL attribute of tokens in the graph G. The
baseline feature model is depicted in figure 1, where
rows denote tokens, columns denote attributes, and
each cell containing a plus sign represents a model
feature.7 This model is an extrapolation from many
previous experiments on different languages and
usually represents a good starting point for further
optimization.
The baseline model was tuned for each of the ten
languages using both forward and backward feature
6In fact, for Arabic, which has about 10% sentences with
non-projective dependencies, it was later found that, with an
optimized feature model, it is beneficial to projectivize the train-
ing data without trying to recover non-projective dependencies
in the parser output. This was also the setting that was used for
Arabic in the dry run and final test.
7The names Top and Next refer to the token on top of the
stack S and the first token in the remaining input I, respectively.
selection. The total number of features in the tuned
models varies from 18 (Turkish) to 56 (Hungarian)
but is typically between 20 and 30. This feature se-
lection process constituted the major development
effort for the Single Malt parser and also gave the
greatest improvements in parsing accuracy, but since
feature selection was to some extent interleaved with
learning algorithm optimization, we only report the
cumulative effect of both together in table 1.
2.3 Learning Algorithm
MaltParser supports several learning algorithms but
the best results have so far been obtained with sup-
port vector machines, using the LIBSVM package
(Chang and Lin, 2001). We use a quadratic kernel
K(xi, xj) = (?xTi xj + r)
2 and LIBSVM?s built-
in one-versus-one strategy for multi-class classifica-
tion, converting symbolic features to numerical ones
using the standard technique of binarization. As our
baseline settings, we used ? = 0.2 and r = 0 for
the kernel parameters, C = 0.5 for the penalty para-
meter, and ? = 1.0 for the termination criterion. In
order to reduce training times during development,
we also split the training data for each language into
smaller sets and trained separate multi-class classi-
fiers for each set, using the POSTAG of Next as the
defining feature for the split.
The time spent on optimizing learning algorithm
parameters varies between languages, mainly due
to lack of time. For Arabic, Basque, and Catalan,
the baseline settings were used also in the dry run
and final test. For Chinese, Greek and Hungarian,
935
Development Dry Run Test Test: UAS
Language Base PA F+L SM B SM B SM B
Arabic 70.31 70.31 71.67 70.93 73.09 74.75 76.52 84.21 85.81
Basque 73.86 74.44 76.99 77.18 80.12 74.97 76.92 80.61 82.84
Catalan 85.43 85.51 86.88 86.65 88.00 87.74 88.70 92.20 93.12
Chinese 83.85 84.39 87.64 87.61 88.61 83.51 84.67 87.60 88.70
Czech 75.00 75.83 77.74 77.91 82.17 77.22 77.98 82.35 83.59
English 85.44 85.44 86.35 86.35 88.74 85.81 88.11 86.77 88.93
Greek 72.67 73.04 74.42 74.89 78.17 74.21 74.65 80.66 81.22
Hungarian 74.62 74.64 77.40 77.81 80.04 78.09 80.27 81.71 83.55
Italian 81.42 81.64 82.50 83.37 85.16 82.48 84.40 86.26 87.77
Turkish 75.12 75.80 76.49 75.87 77.09 79.24 79.79 85.04 85.77
Average 77.78 78.10 79.81 79.86 82.12 79.80 81.20 84.74 86.13
Table 1: Development results for Single Malt (Base = baseline, PA = parsing algorithm, F+L = feature model
and learning algorithm); dry run and test results for Single Malt (SM) and Blended (B) (with corrected test
scores for Blended on Chinese). All scores are labeled attachment scores (LAS) except the last two columns,
which report unlabeled attachment scores (UAS) on the test sets.
slightly better results were obtained by not splitting
the training data into smaller sets; for the remain-
ing languages, accuracy was improved by using the
CPOSTAG of Next as the defining feature for the
split (instead of POSTAG). With respect to the SVM
parameters (?, r, C, and ?), Arabic, Basque, Cata-
lan, Greek and Hungarian retain the baseline set-
tings, while the other languages have slightly dif-
ferent values for some parameters.
The cumulative improvement after optimization
of feature model and learning algorithm parameters
was 1.71 percentage points on average over all ten
languages, with a minimum of 0.69 (Turkish) and a
maximum of 3.25 (Chinese) (cf. table 1).
3 The Blended Parser
The Blended parser is an ensemble system based
on the methodology proposed by Sagae and Lavie
(2006). Given the output dependency graphs Gi
(1 ? i ? m) of m different parsers for an input sen-
tence x, we construct a new graph containing all the
labeled dependency arcs proposed by some parser
and weight each arc a by a score s(a) reflecting its
popularity among the m parsers. The output of the
ensemble system for x is the maximum spanning
tree of this graph (rooted at the node 0), which can
be extracted using the Chu-Liu-Edmonds algorithm,
as shown by McDonald et al (2005). Following
Sagae and Lavie (2006), we let s(a) =
?m
i=1 w
c
iai,
where wci is the average labeled attachment score of
parser i for the word class c8 of the dependent of a,
and ai is 1 if a ? Gi and 0 otherwise.
The Blended parser uses six component parsers,
with three different parsing algorithms, each of
which is used to construct one left-to-right parser
and one right-to-left parser. The parsing algorithms
used are the arc-eager baseline algorithm, the arc-
standard variant of the baseline algorithm, and the
incremental, non-projective parsing algorithm first
described by Covington (2001) and recently used
for deterministic classifier-based parsing by Nivre
(2007), all of which are available in MaltParser.
Thus, the six component parsers for each language
were instances of the following:
1. Arc-eager projective left-to-right
2. Arc-eager projective right-to-left
3. Arc-standard projective left-to-right
4. Arc-standard projective right-to-left
5. Covington non-projective left-to-right
6. Covington non-projective right-to-left
8We use CPOSTAG to determine the part of speech.
936
root 1 2 3?6 7+
Parser R P R P R P R P R P
Single Malt 87.01 80.36 95.08 94.87 86.28 86.67 77.97 80.23 68.98 71.06
Blended 92.09 74.20 95.71 94.92 87.55 88.12 78.66 83.02 65.29 78.14
Table 2: Recall (R) and precision (P) of Single Malt and Blended for dependencies of different length,
averaged over all languages (root = dependents of root node, regardless of length).
The final Blended parser was constructed by reusing
the tuned Single Malt parser for each language (arc-
standard left-to-right for Chinese, arc-eager left-to-
right for the remaining languages) and training five
additional parsers with the same parameter settings
except for the following mechanical adjustments:
1. Pseudo-projective parsing was not used for the
two non-projective parsers.
2. Feature models were adjusted with respect to
the most obvious differences in parsing strategy
(e.g., by deleting features that could never be
informative for a given parser).
3. Learning algorithm parameters were adjusted
to speed up training (e.g., by always splitting
the training data into smaller sets).
Having trained all parsers on 90% of the training
data for each language, the weights wci for each
parser i and coarse part of speech c was determined
by the labeled attachment score on the remaining
10% of the data. This means that the results obtained
in the dry run were bound to be overly optimistic for
the Blended parser, since it was then evaluated on
the same data set that was used to tune the weights.
Finally, we want to emphasize that the time for
developing the Blended parser was severely limited,
which means that several shortcuts had to be taken,
such as optimizing learning algorithm parameters
for speed rather than accuracy and using extrapo-
lation, rather than proper tuning, for other impor-
tant parameters. This probably means that the per-
formance of the Blended system can be improved
considerably by optimizing parameters for all six
parsers separately.
4 Results and Discussion
Table 1 shows the labeled attachment score results
from our internal dry run (training on 90% of the
training data, testing on the remaining 10%) and the
official test runs for both of our systems. It should
be pointed out that the test score for the Blended
parser on Chinese is different from the official one
(75.82), which was much lower than expected due
to a corrupted specification file required by Malt-
Parser. Restoring this file and rerunning the parser
on the Chinese test set, without retraining the parser
or changing any parameter settings, resulted in the
score reported here. This also improved the aver-
age score from 80.32 to 81.20, the former being the
highest reported official score.
For the Single Malt parser, the test results are on
average very close to the dry run results, indicating
that models have not been overfitted (although there
is considerably variation between languages). For
the Blended parser, there is a drop of almost one
percentage point, which can be explained by the fact
that weights could not be tuned on held-out data for
the dry run (as explained in section 3).
Comparing the results for different languages, we
see a tendency that languages with rich morphology,
usually accompanied by flexible word order, get
lower scores. Thus, the labeled attachment score is
below 80% for Arabic, Basque, Czech, Greek, Hun-
garian, and Turkish. By comparison, the more con-
figurational languages (Catalan, Chinese, English,
and Italian) all have scores above 80%. Linguis-
tic properties thus seem to be more important than,
for example, training set size, which can be seen by
comparing the results for Italian, with one of the
smallest training sets, and Czech, with one of the
largest. The development of parsing methods that
are better suited for morphologically rich languages
with flexible word order appears as one of the most
important goals for future research in this area.
Comparing the results of our two systems, we
see that the Blended parser outperforms the Single
Malt parser for all languages, with an average im-
937
provement of 1.40 percentage points, a minimum of
0.44 (Greek) and a maximum of 2.40 (English). As
shown by McDonald and Nivre (2007), the Single
Malt parser tends to suffer from two problems: error
propagation due to the deterministic parsing strat-
egy, typically affecting long dependencies more than
short ones, and low precision on dependencies orig-
inating in the artificial root node due to fragmented
parses.9 The question is which of these problems is
alleviated by the multiple views given by the compo-
nent parsers in the Blended system. Table 2 throws
some light on this by giving the precision and re-
call for dependencies of different length, treating de-
pendents of the artificial root node as a special case.
As expected, the Single Malt parser has lower preci-
sion than recall for root dependents, but the Blended
parser has even lower precision (and somewhat bet-
ter recall), indicating that the fragmentation is even
more severe in this case.10 By contrast, we see that
precision and recall for other dependencies improve
across the board, especially for longer dependencies,
which probably means that the effect of error propa-
gation is mitigated by the use of an ensemble system,
even if each of the component parsers is determinis-
tic in itself.
5 Conclusion
We have shown that deterministic, classifier-based
dependency parsing, with careful optimization, can
give highly accurate dependency parsing for a wide
range of languages, as illustrated by the performance
of the Single Malt parser. We have also demon-
strated that an ensemble of deterministic, classifier-
based dependency parsers, built on top of a tuned
single-parser system, can give even higher accuracy,
as shown by the results of the Blended parser, which
has the highest labeled attachment score for five lan-
guages (Arabic, Basque, Catalan, Hungarian, and
9A fragmented parse is a dependency forest, rather than a
tree, and is automatically converted to a tree by attaching all
(other) roots to the artificial root node. Hence, children of the
root node in the final output may not have been predicted as
such by the treebank-induced classifier.
10This conclusion is further supported by the observation
that the single most frequent ?frame confusion? of the Blended
parser, over all languages, is to attach two dependents with the
label ROOT to the root node, instead of only one. The frequency
of this error is more than twice as high for the Blended parser
(180) as for the Single Malt parser (83).
Italian), as well as the highest multilingual average
score.
Acknowledgements
We want to thank all treebank providers for making
the data available for the shared task and the (other)
organizers for their efforts in organizing it. Special
thanks to Ryan McDonald, for fruitful discussions
and assistance with the error analysis, and to Kenji
Sagae, for showing us how to produce a good blend.
Thanks also to two reviewers for useful comments.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 201?204.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(2003), chapter 7, pages 103?127.
C.-C. Chang and C.-J. Lin, 2001. LIBSVM: A Library
for Support Vector Machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille?
(2003), chapter 13, pages 231?248.
M. A. Covington. 2001. A fundamental algorithm for
dependency parsing. In Proc. of the 39th Annual ACM
Southeast Conf., pages 95?102.
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110?117.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
938
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proc. of the Joint Conf. on Empirical Methods in Nat-
ural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of the Human Language
Technology Conf. and the Conf. on Empirical Meth-
ods in Natural Language Processing (HLT/EMNLP),
pages 523?530.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
R. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeille? (2003), chapter 11,
pages 189?210.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency. In Proc. of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
99?106.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Marinov.
2006. Labeled pseudo-projective dependency parsing
with support vector machines. In Proc. of the Tenth
Conf. on Computational Natural Language Learning
(CoNLL), pages 221?225.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proc. of the
Joint Conf. on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL).
J. Nivre. 2007. Incremental non-projective dependency
parsing. In Human Language Technologies: The An-
nual Conf. of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL-HLT),
pages 396?403.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003. Building a Turkish treebank. In Abeille? (2003),
chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
practical issues in the construction of a Greek depen-
dency treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT), pages 149?160.
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In Proc. of the Human Language Technol-
ogy Conference of the NAACL, Companion Volume:
Short Papers, pages 129?132.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
8th International Workshop on Parsing Technologies
(IWPT), pages 195?206.
939
Statistical Dependency Parsing of Turkish
Gu?ls?en Eryig?it
Department of Computer Engineering
Istanbul Technical University
Istanbul, 34469, Turkey
gulsen@cs.itu.edu.tr
Kemal Oflazer
Faculty of Engineering and Natural Sciences
Sabanci University
Istanbul, 34956, Turkey
oflazer@sabanciuniv.edu
Abstract
This paper presents results from the first
statistical dependency parser for Turkish.
Turkish is a free-constituent order lan-
guage with complex agglutinative inflec-
tional and derivational morphology and
presents interesting challenges for statisti-
cal parsing, as in general, dependency re-
lations are between ?portions? of words
? called inflectional groups. We have
explored statistical models that use dif-
ferent representational units for parsing.
We have used the Turkish Dependency
Treebank to train and test our parser
but have limited this initial exploration
to that subset of the treebank sentences
with only left-to-right non-crossing depen-
dency links. Our results indicate that the
best accuracy in terms of the dependency
relations between inflectional groups is
obtained when we use inflectional groups
as units in parsing, and when contexts
around the dependent are employed.
1 Introduction
The availability of treebanks of various sorts have
fostered the development of statistical parsers
trained with the structural data in these tree-
banks. With the emergence of the important role
of word-to-word relations in parsing (Charniak,
2000; Collins, 1996), dependency grammars have
gained a certain popularity; e.g., Yamada and Mat-
sumoto (2003) for English, Kudo and Matsumoto
(2000; 2002), Sekine et al (2000) for Japanese,
Chung and Rim (2004) for Korean, Nivre et al
(2004) for Swedish, Nivre and Nilsson (2005) for
Czech, among others.
Dependency grammars represent the structure
of the sentences by positing binary dependency
relations between words. For instance, Figure 1
Figure 1: Dependency Relations for a Turkish and
an English sentence
shows the dependency graph of a Turkish and
an English sentence where dependency labels are
shown annotating the arcs which extend from de-
pendents to heads.
Parsers employing CFG-backbones have been
found to be less effective for free-constituent-
order languages where constituents can easily
change their position in the sentence without
modifying the general meaning of the sentence.
Collins et al (1999) applied the parser of Collins
(1997) developed for English, to Czech, and found
that the performance was substantially lower when
compared to the results for English.
2 Turkish
Turkish is an agglutinative language where a se-
quence of inflectional and derivational morphemes
get affixed to a root (Oflazer, 1994). At the syntax
level, the unmarked constituent order is SOV, but
constituent order may vary freely as demanded by
the discourse context. Essentially all constituent
orders are possible, especially at the main sen-
tence level, with very minimal formal constraints.
In written text however, the unmarked order is
dominant at both the main sentence and embedded
clause level.
Turkish morphotactics is quite complicated: a
given word form may involve multiple derivations
and the number of word forms one can generate
from a nominal or verbal root is theoretically in-
finite. Derivations in Turkish are very produc-
tive, and the syntactic relations that a word is in-
89
volved in as a dependent or head element, are de-
termined by the inflectional properties of the one
or more (possibly intermediate) derived forms. In
this work, we assume that a Turkish word is rep-
resented as a sequence of inflectional groups (IGs
hereafter), separated by ?DBs, denoting derivation
boundaries, in the following general form:
root+IG1 + ?DB+IG2 + ?DB+? ? ? + ?DB+IGn.
Here each IGi denotes relevant inflectional fea-
tures including the part-of-speech for the root and
for any of the derived forms. For instance, the de-
rived modifier sag?lamlas?t?rd?g??m?zdaki1
would be represented as:2
sag?lam(strong)+Adj
+?DB+Verb+Become
+?DB+Verb+Caus+Pos
+?DB+Noun+PastPart+A3sg+P3sg+Loc
+?DB+Adj+Rel
The five IGs in this are the feature sequences sep-
arated by the ?DB marker. The first IG shows the
part-of-speech for the root which is its only inflec-
tional feature. The second IG indicates a deriva-
tion into a verb whose semantics is ?to become?
the preceding adjective. The third IG indicates
that a causative verb with positive polarity is de-
rived from the previous verb. The fourth IG in-
dicates the derivation of a nominal form, a past
participle, with +Noun as the part-of-speech and
+PastPart, as the minor part-of-speech, with
some additional inflectional features. Finally, the
fifth IG indicates a derivation into a relativizer ad-
jective.
A sentence would then be represented as a se-
quence of the IGs making up the words. When a
word is considered as a sequence of IGs, linguis-
tically, the last IG of a word determines its role
as a dependent, so, syntactic relation links only
emanate from the last IG of a (dependent) word,
and land on one of the IGs of a (head) word on
the right (with minor exceptions), as exemplified
in Figure 2. And again with minor exceptions, the
dependency links between the IGs, when drawn
above the IG sequence, do not cross.3 Figure 3
from Oflazer (2003) shows a dependency tree for
a Turkish sentence laid on top of the words seg-
mented along IG boundaries.
With this view in mind, the dependency rela-
tions that are to be extracted by a parser should be
relations between certain inflectional groups and
1Literally, ?(the thing existing) at the time we caused
(something) to become strong?.
2The morphological features other than the obvious part-
of-speech features are: +Become: become verb, +Caus:
causative verb, +PastPart: Derived past participle,
+P3sg: 3sg possessive agreement, +A3sg: 3sg number-
person agreement, +Loc: Locative case, +Pos: Positive Po-
larity, +Rel: Relativizing Modifier.
3Only 2.5% of the dependencies in the Turkish treebank
(Oflazer et al, 2003) actually cross another dependency link.
Figure 2: Dependency Links and IGs
not orthographic words. Since only the word-
final inflectional groups have out-going depen-
dency links to a head, there will be IGs which do
not have any outgoing links (e.g., the first IG of the
word bu?yu?mesi in Figure 3). We assume that such
IGs are implicitly linked to the next IG, but nei-
ther represent nor extract such relationships with
the parser, as it is the task of the morphological
analyzer to extract those. Thus the parsing mod-
els that we will present in subsequent sections all
aim to extract these surface relations between the
relevant IGs, and in line with this, we will employ
performance measures based on IGs and their re-
lationships, and not on orthographic words.
We use a model of sentence structure as de-
picted in Figure 4. In this figure, the top part repre-
sents the words in a sentence. After morphological
analysis and morphological disambiguation, each
word is represented with (the sequence of) its in-
flectional groups, shown in the middle of the fig-
ure. The inflectional groups are then reindexed
so that they are the ?units? for the purposes of
parsing. The inflectional groups marked with ?
are those from which a dependency link will em-
anate from, to a head-word to the right. Please
note that the number of such marked inflectional
groups is the same as the number of words in the
sentence, and all of such IGs, (except one corre-
sponding to the distinguished head of the sentence
which will not have any links), will have outgoing
dependency links.
In the rest of this paper, we first give a very brief
overview a general model of statistical depen-
dency parsing and then introduce three models for
dependency parsing of Turkish. We then present
our results for these models and for some addi-
tional experiments for the best performing model.
We then close with a discussion on the results,
analysis of the errors the parser makes, and con-
clusions.
3 Parser
Statistical dependency parsers first compute the
probabilities of the unit-to-unit dependencies, and
then find the most probable dependency tree T ?
among the set of possible dependency trees. This
90
Bu eski ev+de +ki g?l+?n b?yle b?y? +me+si herkes+i ?ok etkile+di
Mod
Det
Mod
Subj
Mod
Subj
Obj
Mod
bu
+Det
eski
+Adj
ev
+Noun
+A3sg
+Pnon
+Loc
+Adj g?l
+Noun
+A3sg
+Pnon
+Gen
b?yle
+Adv
b?y?
+Verb
+Noun
+Inf
+A3sg
+P3sg
+Nom
herkes
+Pron
+A3pl
+Pnon
+Acc
?ok
+Adv
etkile
+Verb
+Past
+A3sg
This               old             house-at+that-is         rose's            such                     grow +ing              everyone        very      impressed
Such growing of the rose in this old house impressed everyone very much.
+?s indicate morpheme boundaries. The rounded rectangles show the words while the inflectional groups within
the words that have more than 1 IG are emphasized with the dashed rounded rectangles. The inflectional features
of each inflectional group as produced by the morphological analyzer are listed below.
Figure 3: Dependency links in an example Turkish sentence.
w1
  ##
IG1

IG2

? ? ? IG?g1

IG1 IG2 ? ? ? IG?g1
w2
  $$
IG1

IG2 ? ? ? IG?g2

IGg1+1 ? ? ? IG?g1+g2
. . .
. . .
wn
  ##
IG1 IG2 ? ? ? IG?gn

? ? ? IG??n
?i =
Pi
k=1 gk
Figure 4: Sentence Structure
can be formulated as
T ? = argmax
T
P (T, S)
= argmax
T
n?1
?
i=1
P (dep (wi, wH(i)) |S)(1)
where in our case S is a sequence of units (words,
IGs) and T , ranges over possible dependency
trees consisting of left-to-right dependency links
dep (wi, wH(i)) with wH(i) denoting the head unit
to which the dependent unit, wi, is linked to.
The distance between the dependent units plays
an important role in the computation of the depen-
dency probabilities. Collins (1996) employs this
distance ?i,H(i) in the computation of word-to-
word dependency probabilities
P (dep (wi, wH(i)) |S) ? (2)
P (link(wi, wH(i)) |?i,H(i))
suggesting that distance is a crucial variable when
deciding whether two words are related, along
with other features such as intervening punctua-
tion. Chung and Rim (2004) propose a different
method and introduce a new probability factor that
takes into account the distance between the depen-
dent and the head. The model in equation 3 takes
into account the contexts that the dependent and
head reside in and the distance between the head
and the dependent.
P (dep (wi, wH(i)) |S) ? (3)
P (link(wi, wH(i))) |?i ?H(i)) ?
P (wi links to some head
H(i) ? i away|?i)
Here ?i represents the context around the depen-
dent wi and ?H(i), represents the context around
the head word. P (dep (wi, wH(i)) |S) is the prob-
ability of the directed dependency relation be-
tween wi and wH(i) in the current sentence, while
P (link(wi, wH(i)) |?i ?H(i)) is the probability of
seeing a similar dependency (with wi as the depen-
dent, wH(i) as the head in a similar context) in the
training treebank.
For the parsing models that will be described
below, the relevant statistical parameters needed
have been estimated from the Turkish treebank
(Oflazer et al, 2003). Since this treebank is rel-
atively smaller than the available treebanks for
other languages (e.g., Penn Treebank), we have
91
opted to model the bigram linkage probabilities
in an unlexicalized manner (that is, by just taking
certain morphosyntactic properties into account),
to avoid, to the extent possible, the data sparseness
problem which is especially acute for Turkish. We
have also been encouraged by the success of the
unlexicalized parsers reported recently (Klein and
Manning, 2003; Chung and Rim, 2004).
For parsing, we use a version of the Backward
Beam Search Algorithm (Sekine et al, 2000) de-
veloped for Japanese dependency analysis adapted
to our representations of the morphological struc-
ture of the words. This algorithm parses a sentence
by starting from the end and analyzing it towards
the beginning. Bymaking the projectivity assump-
tion that the relations do not cross, this algorithm
considerably facilitates the analysis.
4 Details of the Parsing Models
In this section we detail three models that we have
experimented with for Turkish. All three models
are unlexicalized and differ either in the units used
for parsing or in the way contexts modeled. In
all three models, we use the probability model in
Equation 3.
4.1 Simplifying IG Tags
Our morphological analyzer produces a rather rich
representation with a multitude of morphosyntac-
tic and morphosemantic features encoded in the
words. However, not all of these features are nec-
essarily relevant in all the tasks that these analyses
can be used in. Further, different subsets of these
features may be relevant depending on the func-
tion of a word. In the models discussed below, we
use a reduced representation of the IGs to ?unlex-
icalize? the words:
1. For nominal IGs,4 we use two different tags
depending on whether the IG is used as a de-
pendent or as a head during (different stages
of ) parsing:
? If the IG is used as a dependent, (and,
only word-final IGs can be dependents),
we represent that IG by a reduced tag
consisting of only the case marker, as
that essentially determines the syntactic
function of that IG as a dependent, and
only nominals have cases.
? If the IG is used as a head, then we use
only part-of-speech and the possessive
agreement marker in the reduced tag.
4These are nouns, pronouns, and other derived forms that
inflect with the same paradigm as nouns, including infinitives,
past and future participles.
2. For adjective IGs with present/past/future
participles minor part-of-speech, we use the
part-of-speech when they are used as depen-
dents and the part-of-speech plus the the pos-
sessive agreement marker when used as a
head.
3. For other IGs, we reduce the IG to just the
part-of-speech.
Such a reduced representation also helps alleviate
the sparse data problem as statistics from many
word forms with only the relevant features are
conflated.
We modeled the second probability term on the
right-hand side of Equation 3 (involving the dis-
tance between the dependent and the head unit) in
the following manner. First, we collected statis-
tics over the treebank sentences, and noted that,
if we count words as units, then 90% of depen-
dency links link to a word that is less than 3 words
away. Similarly, if we count distance in terms of
IGs, then 90% of dependency links link to an IG
that is less than 4 IGs away to the right. Thus we
selected a parameter k = 4 for Models 1 and 3 be-
low, where distance is measured in terms of words,
and k = 5 for Model 2 where distance is measured
in terms of IGs, as a threshold value at and beyond
which a dependency is considered ?distant?. Dur-
ing actual runs,
P (wi links to some head H(i) ? i away|?i)
was computed by interpolating
P1(wi links to some head H(i) ? i away|?i)
estimated from the training corpus, and
P2(wi links to some head H(i) ? i away)
the estimated probability for a length of a link
when no contexts are considered, again estimated
from the training corpus. When probabilities are
estimated from the training set, all distances larger
than k are assigned the same probability. If even
after interpolation, the probability is 0, then a very
small value is used. This is a modified version of
the backed-off smoothing used by Collins (1996)
to alleviate sparse data problems. A similar inter-
polation is used for the first component on the right
hand side of Equation 3 by removing the head and
the dependent contextual information all at once.
4.2 Model 1 ? ?Unlexicalized? Word-based
Model
In this model, we represent each word by a re-
duced representation of its last IG when used as a
dependent,5 and by concatenation of the reduced
5Remember that other IGs in a word, if any, do not have
any bearing on how this word links to its head word.
92
representation of its IGs when used as a head.
Since a word can be both a dependent and a head
word, the reduced representation to be used is dy-
namically determined during parsing.
Parsing then proceeds with words as units rep-
resented in this manner. Once the parser links
these units, we remap these links back to IGs to
recover the actual IG-to-IG dependencies. We al-
ready know that any outgoing link from a depen-
dent will emanate from the last IG of that word.
For the head word, we assume that the link lands
on the first IG of that word.6
For the contexts, we use the following scheme.
A contextual element on the left is treated as a de-
pendent and is modeled with its last IG, while a
contextual element on the right is represented as
if it were a head using all its IGs. We ignore any
overlaps between contexts in this and the subse-
quent models.
In Figure 5 we show in a table the sample sen-
tence in Figure 3, the morphological analysis for
each word and the reduced tags for representing
the units for the three models. For each model, we
list the tags when the unit is used as a head and
when it is used as a dependent. For model 1, we
use the tags in rows 3 and 4.
4.3 Model 2 - IG-based Model
In this model, we represent each IG with re-
duced representations in the manner above, but
do not concatenate them into a representation for
the word. So our ?units? for parsing are IGs.
The parser directly establishes IG-to-IG links from
word-final IGs to some IG to the right. The con-
texts that are used in this model are the IGs to
the left (starting with the last IG of the preceding
word) and the right of the dependent and the head
IG.
The units and the tags we use in this model are
in rows 5 and 6 in the table in Figure 5. Note
that the empty cells in row 4 corresponds to IGs
which can not be syntactic dependents as they are
not word-final.
4.4 Model 3 ? IG-based Model with
Word-final IG Contexts
This model is almost exactly like Model 2 above.
The two differences are that (i) for contexts we
only use just the word-final IGs to the left and the
right ignoring any non-word-final IGs in between
(except for the case that the context and the head
overlap, where we use the tag of the head IG in-
6This choice is based on the observation that in the tree-
bank, 85.6% of the dependency links land on the first (and
possibly the only) IG of the head word, while 14.4% of the
dependency links land on an IG other than the first one.
stead of the final IG); and (ii) the distance function
is computed in terms of words. The reason this
model is used is that it is the word final IGs that
determine the syntactic roles of the dependents.
5 Results
Since in this study we are limited to parsing sen-
tences with only left-to-right dependency links7
which do not cross each other, we eliminated the
sentences having such dependencies (even if they
contain a single one) and used a subset of 3398
such sentences in the Turkish Treebank. The gold
standard part-of-speech tags are used in the exper-
iments. The sentences in the corpus ranged be-
tween 2 words to 40 words with an average of
about 8 words;8 90% of the sentences had less
than or equal to 15 words. In terms of IGs, the
sentences comprised 2 to 55 IGs with an average
of 10 IGs per sentence; 90% of the sentences had
less than or equal to 15 IGs. We partitioned this
set into training and test sets in 10 different ways
to obtain results with 10-fold cross-validation.
We implemented three baseline parsers:
1. The first baseline parser links a word-final IG
to the first IG of the next word on the right.
2. The second baseline parser links a word-final
IG to the last IG of the next word on the
right.9
3. The third baseline parser is a deterministic
rule-based parser that links each word-final
IG to an IG on the right based on the approach
of Nivre (2003). The parser uses 23 unlexi-
calized linking rules and a heuristic that links
any non-punctuation word not linked by the
parser to the last IG of the last word as a de-
pendent.
Table 1 shows the results from our experiments
with these baseline parsers and parsers that are
based on the three models above. The three mod-
els have been experimented with different contexts
around both the dependent unit and the head. In
each row, columns 3 and 4 show the percentage of
IG?IG dependency relations correctly recovered
for all tokens, and just words excluding punctu-
ation from the statistics, while columns 5 and 6
show the percentage of test sentences for which
all dependency relations extracted agree with the
7In 95% of the treebank dependencies, the head is the
right of the dependent.
8This is quite normal; the equivalents of function words
in English are embedded as morphemes (not IGs) into these
words.
9Note that for head words with a single IG, the first two
baselines behave the same.
93
Figure 5: Tags used in the parsing models
relations in the treebank. Each entry presents the
average and the standard error of the results on the
test set, over the 10 iterations of the 10-fold cross-
validation. Our main goal is to improve the per-
centage of correctly determined IG-to-IG depen-
dency relations, shown in the fourth column of the
table. The best results in these experiments are ob-
tained with Model 3 using 1 unit on both sides of
the dependent. Although it is slightly better than
Model 2 with the same context size, the difference
between the means (0.4?0.2) for each 10 iterations
is statistically significant.
Since we have been using unlexicalized models,
we wanted to test out whether a smaller training
corpus would have a major impact for our current
models. Table 2 shows results for Model 3 with no
context and 1 unit on each side of the dependent,
obtained by using only a 1500 sentence subset of
the original treebank, again using 10-fold cross
validation. Remarkably the reduction in training
set size has a very small impact on the results.
Although all along, we have suggested that de-
termining word-to-word dependency relationships
is not the right approach for evaluating parser per-
formance for Turkish, we have nevertheless per-
formed word-to-word correctness evaluation so
that comparison with other word based approaches
can be made. In this evaluation, we assume that a
dependency link is correct if we correctly deter-
mine the head word (but not necessarily the cor-
rect IG). Table 3 shows the word based results for
the best cases of the models in Table 1.
We have also tested our parser with a pure word
model where both the dependent and the head are
represented by the concatenation of their IGs, that
is, by their full morphological analysis except the
root. The result for this case is given in the last row
of Table 3. This result is even lower than the rule-
based baseline.10 For this model, if we connect the
10Also lower than Model 1 with no context (79.1?1.1)
dependent to the first IG of the head as we did in
Model 1, the IG-IG accuracy excluding punctua-
tions becomes 69.9?3.1, which is also lower than
baseline 3 (70.5%).
6 Discussions
Our results indicate that all of our models perform
better than the 3 baseline parsers, even when no
contexts around the dependent and head units are
used. We get our best results with Model 3, where
IGs are used as units for parsing and contexts are
comprised of word final IGs. The highest accuracy
in terms of percent of correctly extracted IG-to-IG
relations excluding punctuations (73.5%) was ob-
tained when one word is used as context on both
sides of the the dependent.11 We also noted that
using a smaller treebank to train our models did
not result in a significant reduction in our accu-
racy indicating that the unlexicalized models are
quite effective, but this also may hint that a larger
treebank with unlexicalized modeling may not be
useful for improving link accuracy.
A detailed look at the results from the best per-
forming model shown in in Table 4,12 indicates
that, accuracy decrases with the increasing sen-
tence length. For longer sentences, we should em-
ploy more sophisticated models possibly including
lexicalization.
A further analysis of the actual errors made by
the best performing model indicates almost 40%
of the errors are ?attachment? problems: the de-
pendent IGs, especially verbal adjuncts and argu-
ments, link to the wrong IG but otherwise with the
same morphological features as the correct one ex-
cept for the root word. This indicates we may have
to model distance in a more sophisticated way and
11We should also note that early experiments using differ-
ent sets of morphological features that we intuitively thought
should be useful, gave rather low accuracy results.
12These results are significantly higher than the best base-
line (rule based) for all the sentence length categories.
94
Percentage of IG-IG Percentage of Sentences
Relations Correct With ALL Relations Correct
Parsing Model Context Words+Punc Words only Words+Punc Words only
Baseline 1 NA 59.9 ?0.3 63.9 ?0.7 21.4 ?0.6 24.0 ?0.7
Baseline 2 NA 58.3 ?0.2 62.2 ?0.8 20.1 ?0.0 22.6 ?0.6
Baseline 3 NA 69.6 ?0.2 70.5 ?0.8 31.7 ?0.7 36.6 ?0.8
Model 1 None 69.8 ?0.4 71.0 ?1.3 32.7 ?0.6 36.2 ?0.7
(k=4) Dl=1 69.9 ?0.4 71.1 ?1.2 32.9 ?0.5 36.4 ?0.6
Dl=1 Dr=1 71.3 ?0.4 72.5 ?1.2 33.4 ?0.8 36.7 ?0.8
Hl=1 Hr=1 64.7 ?0.4 65.5 ?1.3 25.4 ?0.6 28.7 ?0.8
Both 71.4 ?0.4 72.6 ?1.1 34.2 ?0.7 37.2 ?0.6
Model 2 None 70.5 ?0.3 71.9 ?1.0 32.1 ?0.9 36.3 ?0.9
(k=5) Dl=1 71.3 ?0.3 72.7 ?0.9 33.8 ?0.8 37.4 ?0.7
Dl=1 Dr=1 71.9 ?0.3 73.1 ?0.9 34.8 ?0.7 38.0 ?0.7
Hl=1 Hr=1 57.4 ?0.3 57.6 ?0.7 23.5 ?0.6 25.8 ?0.6
Both 70.9 ?0.3 72.2 ?0.9 34.2 ?0.8 37.2 ?0.9
Model 3 None 71.2 ?0.3 72.6 ?0.9 34.4 ?0.7 38.1 ?0.7
(k=4) Dl=1 71.2 ?0.4 72.6 ?1.1 34.5 ?0.7 38.3 ?0.6
Dl=1 Dr=1 72.3 ?0.3 73.5 ?1.0 35.5 ?0.9 38.7 ?0.9
Hl=1 Hr=1 55.2 ?0.3 55.1 ?0.7 22.0 ?0.6 24.1 ?0.6
Both 71.1 ?0.3 72.4 ?0.9 35.5 ?0.8 38.4 ?0.9
The Context column entries show the context around the dependent and the head unit. Dl=1 and Dr=1 indicate
the use of 1 unit left and the right of the dependent respectively. Hl=1 and Hr=1 indicate the use of 1 unit left and
the right of the head respectively. Both indicates both head and the dependent have 1 unit of context on both sides.
Table 1: Results from parsing with the baseline parsers and statistical parsers based on Models 1-3.
Percentage of IG-IG Percentage of Sentences
Relations Correct With ALL Relations Correct
Parsing Model Context Words+Punc Words only Words+Punc Words only
Model 3 None 71.0 ?0.6 72.2 ?1.5 34.4 ?1.0 38.1 ?1.1
(k=4, 1500 Sentences) Dl=1 Dr=1 71.6 ?0.4 72.6 ?1.1 35.1 ?1.3 38.4 ?1.5
Table 2: Results from using a smaller training corpus.
Percentage of Word-Word
Relations Correct
Parsing Model Context Words only
Baseline 1 NA 72.1 ?0.5
Baseline 2 NA 72.1 ?0.5
Baseline 3 NA 80.3 ?0.7
Model 1 (k=4) Both 80.8 ?0.9
Model 2 (k=5) Dl=1 Dr=1 81.0 ?0.7
Model 3 (k=4) Dl=1 Dr=1 81.2 ?1.0
Pure Word Model None 77.7 ?3.5
Table 3: Results from word-to-word correctness evaluation.
Sentence Length l (IGs) % Accuracy
1 < l ? 10 80.2 ?0.5
10 < l ? 20 70.1 ?0.4
20 < l ? 30 64.6 ?1.0
30 < l 62.7 ?1.3
Table 4: Accuracy over different length sentences.
95
perhaps use a limited lexicalization such as includ-
ing limited non-morphological information (e.g.,
verb valency) into the tags.
7 Conclusions
We have presented our results from statistical de-
pendency parsing of Turkish with statistical mod-
els trained from the sentences in the Turkish tree-
bank. The dependency relations are between
sub-lexical units that we call inflectional groups
(IGs) and the parser recovers dependency rela-
tions between these IGs. Due to the modest size
of the treebank available to us, we have used
unlexicalized statistical models, representing IGs
by reduced representations of their morphological
properties. For the purposes of this work we have
limited ourselves to sentences with all left-to-right
dependency links that do not cross each other.
We get our best results (73.5% IG-to-IG link ac-
curacy) using a model where IGs are used as units
for parsing and we use as contexts, word final IGs
of the words before and after the dependent.
Future work involves a more detailed under-
standing of the nature of the errors and see how
limited lexicalization can help, as well as investi-
gation of more sophisticated models such as SVM
or memory-based techniques for correctly identi-
fying dependencies.
8 Acknowledgement
This research was supported in part by a research
grant from TUBITAK (The Scientific and Techni-
cal Research Council of Turkey) and from Istanbul
Technical University.
References
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In 1st Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Seattle, Washington.
Hoojung Chung and Hae-Chang Rim. 2004. Un-
lexicalized dependency parser for variable word or-
der languages based on local contextual pattern.
In Computational Linguistics and Intelligent Text
Processing (CICLing-2004), Seoul, Korea. Lecture
Notes in Computer Science 2945.
Michael Collins, Jan Hajic, Lance Ramshaw, and
Christoph Tillmann. 1999. A statistical parser for
Czech. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 505?518, University of Maryland.
Michael Collins. 1996. A new statistical parser based
on bigram lexical dependencies. In Proceedings of
the 34th AnnualMeeting of the Association for Com-
putational Linguistics, Santa Cruz, CA.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Compu-
tational Linguistics and 8th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 16?23, Madrid, Spain.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 423?430, Sapporo,
Japan.
Taku Kudo and Yuji Matsumoto. 2000. Japanese
dependency analysis based on support vector ma-
chines. In Joint Sigdat Conference On Empirical
Methods In Natural Language Processing and Very
Large Corpora, Hong Kong.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Sixth Conference on Natural Language Learning,
Taipei, Taiwan.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings of
the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL?05), pages 99?106,
Ann Arbor, Michigan, June.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In 8th Confer-
ence on Computational Natural Language Learning,
Boston, Massachusetts.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of 8th
International Workshop on Parsing Technologies,
pages 23?25, Nancy, France, April.
Kemal Oflazer, Bilge Say, Dilek Zeynep Hakkani-Tu?r,
and Go?khan Tu?r. 2003. Building a Turkish tree-
bank. In Anne Abeille, editor, Building and Exploit-
ing Syntactically-annotatedCorpora. Kluwer Acad-
emic Publishers.
Kemal Oflazer. 1994. Two-level description of Turk-
ish morphology. Literary and Linguistic Comput-
ing, 9(2).
Kemal Oflazer. 2003. Dependency parsing with an
extended finite-state approach. Computational Lin-
guistics, 29(4).
Satoshi Sekine, Kiyotaka Uchimoto, and Hitoshi Isa-
hara. 2000. Backward beam search algorithm for
dependency analysis of Japanese. In 17th Inter-
national Conference on Computational Linguistics,
pages 754 ? 760, Saarbru?cken, Germany.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In 8th International Workshop of Parsing
Technologies, Nancy, France.
96
Dependency Parsing of Turkish
Gu?ls?en Eryig?it?
Istanbul Technical University
Joakim Nivre?? ?
Va?xjo? University, Uppsala University
Kemal Oflazer?
Sabanc? University
The suitability of different parsing methods for different languages is an important topic in
syntactic parsing. Especially lesser-studied languages, typologically different from the languages
for which methods have originally been developed, pose interesting challenges in this respect. This
article presents an investigation of data-driven dependency parsing of Turkish, an agglutinative,
free constituent order language that can be seen as the representative of a wider class of languages
of similar type. Our investigations show that morphological structure plays an essential role in
finding syntactic relations in such a language. In particular, we show that employing sublexical
units called inflectional groups, rather than word forms, as the basic parsing units improves
parsing accuracy. We test our claim on two different parsing methods, one based on a probabilis-
tic model with beam search and the other based on discriminative classifiers and a deterministic
parsing strategy, and show that the usefulness of sublexical units holds regardless of the parsing
method. We examine the impact of morphological and lexical information in detail and show that,
properly used, this kind of information can improve parsing accuracy substantially. Applying
the techniques presented in this article, we achieve the highest reported accuracy for parsing the
Turkish Treebank.
1. Introduction
Robust syntactic parsing of natural language is an area in which we have seen tremen-
dous development during the last 10 to 15 years, mainly on the basis of data-driven
methods but sometimes in combination with grammar-based approaches. Despite this,
most of the approaches in this field have only been tested on a relatively small set
of languages, mostly English but to some extent also languages like Chinese, Czech,
Japanese, and German.
? Department of Computer Engineering, Istanbul Technical University, 34469 Istanbul, Turkey.
E-mail: gulsen.cebiroglu@itu.edu.tr.
?? School of Mathematics and Systems Engineering, Va?xjo? University, 35260 Va?xjo?, Sweden.
E-mail: joakim.nivre@msi.vxu.se.
? Department of Linguistics and Philology, Uppsala University, Box 635, 75126 Uppsala, Sweden.
? Faculty of Engineering and Natural Sciences, Sabanc? University, 34956 Istanbul, Turkey.
E-mail: oflazer@sabanciuniv.edu.
Submission received: 5 October 2006; revised submission received: 3 April 2007; accepted for publication:
16 May 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 3
An important issue in this context is to what extent our models and algorithms
are tailored to properties of specific languages or language groups. This issue is es-
pecially pertinent for data-driven approaches, where one of the claimed advantages
is portability to new languages. The results so far mainly come from studies where a
parser originally developed for English, such as the Collins parser (Collins 1997, 1999),
is applied to a new language, which often leads to a significant decrease in the measured
accuracy (Collins et al 1999; Bikel and Chiang 2000; Dubey and Keller 2003; Levy and
Manning 2003; Corazza et al 2004). However, it is often quite difficult to tease apart the
influence of different features of the parsing methodology in the observed degradation
of performance.
A related issue concerns the suitability of different kinds of syntactic representation
for different types of languages. Whereas most of the work on English has been based
on constituency-based representations, partly influenced by the availability of data
resources such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), it
has been argued that free constituent order languages can be analyzed more adequately
using dependency-based representations, which is also the kind of annotation found,
for example, in the Prague Dependency Treebank of Czech (Hajic? et al 2001). Recently,
dependency-based parsing has been applied to 13 different languages in the shared
task of the 2006 Conference on Computational Natural Language Learning (CoNLL)
(Buchholz and Marsi 2006).
In this article, we focus on dependency-based parsing of Turkish, a language that
is characterized by rich agglutinative morphology, free constituent order, and predom-
inantly head-final syntactic constructions. Thus, Turkish can be viewed as the repre-
sentative of a class of languages that are very different from English and most other
languages that have been studied in the parsing literature. Using data from the recently
released Turkish Treebank (Oflazer et al 2003), we investigate the impact of different
design choices in developing data-driven parsers. There are essentially three sets of
issues that are addressed in these experiments.
 The first set includes issues relating to the treatment of morphology in
syntactic parsing, which becomes crucial when dealing with languages
where the most important clues to syntactic functions are often found in
the morphology rather than in word order patterns. Thus, for Turkish, it
has previously been shown that parsing accuracy can be improved by
taking morphologically defined units rather than word forms as the basic
units of syntactic structure (Eryig?it and Oflazer 2006). In this article, we
corroborate this claim showing that it holds in both approaches we
explore. We also study the impact of different morphological feature
representations on parsing accuracy.
 The second set of issues concerns lexicalization, a topic that has been very
prominent in the parsing literature lately. Whereas the best performing
parsers for English all make use of lexical information, the real benefits of
lexicalization for English as well as other languages remains controversial
(Dubey and Keller, 2003; Klein and Manning 2003; Arun and Keller 2005).
 The third set concerns the basic parsing methodology, including both
parsing algorithms and learning algorithms. We first introduce a statistical
parser using a conditional probabilistic model which is very sensitive to
the selected representational features and thus clearly exposes the ones
358
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
with crucial importance for parsing Turkish. We then implement our
models on a deterministic classifier-based parser using discriminative
learning, which is one of the best performing dependency parsers
evaluated on a wide range of different languages.
Additionally we address the following issues:
 We investigate learning curves and provide an error analysis for the best
performing parser.
 For most of our experiments we use as input the gold-standard tags from
the treebank. However, in our last experiments we evaluate the impact of
automatic statistical morphological disambiguation on the performance of
our best performing parser.
The rest of the article is structured as follows. Section 2 gives a very brief introduc-
tion to Turkish morphology and syntax and discusses the representation of morpholog-
ical information and syntactic dependency relations in the Turkish Treebank. Section 3
is devoted to methodological issues, in particular the data sets and evaluation metrics
used in experiments. The following two sections present two different dependency
parsers trained and evaluated on the Turkish Treebank: a probabilistic parser (Section 4)
and a classifier-based parser (Section 5). Section 6 investigates the impact of lexicaliza-
tion and morphological information on the two parsers, and Section 7 examines their
learning curves. Section 8 presents an error analysis for the best performing parser,
and Section 9 analyzes the degradation in parsing performance when using automatic
morphological disambiguation. Section 10 discusses related work, and Section 11 sum-
marizes the main conclusions from our study.
2. Turkish: Morphology and Dependency Relations
Turkish displays rather different characteristics compared to the more well-studied
languages in the parsing literature. Most of these characteristics are also found in
many agglutinative languages such as Basque, Estonian, Finnish, Hungarian, Japanese,
and Korean.1 Turkish is a flexible constituent order language. Even though in written
texts the constituent order predominantly conforms to the SOV order, constituents may
freely change their position depending on the requirements of the discourse context
(Erguvanl? 1979; Hoffman 1994). However, from a dependency structure point of view,
Turkish is predominantly (but not exclusively) head final.
Turkish has a very rich agglutinative morphological structure. Nouns can give rise
to about 100 inflected forms and verbs to many more. Furthermore, Turkish words may
be formed through very productive derivations, increasing substantially the number of
possible word forms that can be generated from a root word. It is not uncommon to find
up to four or five derivations in a single word. Previous work on Turkish (Hakkani-
Tu?r, Oflazer, and Tu?r 2002; Oflazer 2003; Oflazer et al 2003; Eryig?it and Oflazer 2006)
has represented the morphological structure of Turkish words by splitting them into
inflectional groups (IGs). The root and derivational elements of a word are represented
1 We, however, do not necessarily suggest that the morphological sublexical representation that we use for
Turkish later in this article is applicable to these languages.
359
Computational Linguistics Volume 34, Number 3
by different IGs, separated from each other by derivational boundaries (DB). Each IG is
then annotated with its own part of speech and any inflectional features as illustrated
in the following example:2
araban?zdayd?
(?it was in your car?)
araban?zda DB yd?
araba+Noun+A3sg+P2pl+Loc
? ?? ?
IG1
DB +Verb+Zero+Past+A3sg
? ?? ?
IG2
?in your car? ?it was?
In this example, the root of the word araban?zdayd? is araba (?car?) and its part of speech is
noun. From this, a verb is derived in a separate IG. So, the word is composed of two IGs
where the first one, araban?zda (?in your car?), is a noun in locative case and in second
plural possessive form, and the second one is a verbal derivation from this noun in past
tense and third person singular form.
2.1 Dependency Relations in Turkish
Because most syntactic information is mediated by morphology, it is not sufficient
for the parser to only find dependency relations between orthographic words;3 the
correct IGs involved in the relations should also be identified. We can motivate this
with the following very simple example: In the phrase spor araban?zdayd? (?it was in
your sports car?), the adjective spor (?sports?) should be connected to the first IG of
the second word. It is the word araba (?car?) which is modified by the adjective, not
the derived verb form araban?zdayd? (?it was in your car?). So a parser should not just
say that the first word is a dependent of the second but also state that the syntactic
relation is between the last IG of the first word and the first IG of the second word, as
shown:
spor
Mod

araban?zda DB yd?
In Figure 1 we see a complete dependency tree for a Turkish sentence laid on top of the
words segmented along IG boundaries. The rounded rectangles show the words and
IGs within words are marked with dashed rounded rectangles. The first thing to note
in this figure is that the dependency links always emanate from the last IG of a word,
because that IG determines the role of that word as a dependent. The dependency links
land on one of the IGs of a (head) word (almost always to the right). The non-final IGs
(e.g., the first IG of the word okuldaki in Figure 1) may only have incoming dependency
2 +A3sg = 3sg number agreement, +P2pl = 2pl possessive agreement, +Loc = Locative Case.
3 For the same reason, Bozsahin (2002) uses morphemes as sublexical constituents in a CCG framework.
Because the lexicon was organized in terms of morphemes each with its own CCG functor, the grammar
had to account for both the morphotactics and the syntax at the same time.
360
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
Figure 1
Dependency links in an example Turkish sentence.
?+?s indicate morpheme boundaries. The rounded rectangles show words, and IGs within words
that have more than one IG are indicated by the dashed rounded rectangles. The inflectional
features of each IG as produced by the morphological analyzer are listed below the IG.
links and are assumed to be morphologically linked to the next IG to the right (but we
do not explicitly show these links).4
The noun phrase formed by the three words o?g?rencilerin en ak?ll?s? in this example
highlights the importance of the IG-based representation of syntactic relations. Here
in the word ak?ll?s?, we have three IGs: The first contains the singular noun ak?l (?intelli-
gence?), the second IG indicates the derivation into an adjective ak?ll? (?intelligence-with?
? ?intelligent?). The preceding word en (?most?), an intensifier adverb, is linked to this IG
as a modifier (thus forming ?most intelligent?). The third IG indicates another derivation
into a noun (?a singular entity that is most intelligent?). This last IG is the head of a
dependency link emanating from the word o?g?rencilerin with genitive case-marking (?of
the students? or ?students? ?) which acts as the possessor of the last noun IG of the third
word ak?ll?s?. Finally, this word is the subject of the verb IG of the last word, through its
last IG.
2.2 The Turkish Treebank
We have used the Turkish Treebank (Oflazer et al 2003), created by the Middle East
Technical University and Sabanc? University, in the experiments we report in this ar-
ticle. The Turkish Treebank is based on a small subset of the Metu Turkish Corpus
(www.ii.metu.edu.tr/?corpus/corpus.html), a balanced collection of post-1990 Turk-
ish text from 10 genres. The version that has been used in this article is the version used
in the CoNLL-X shared task publicly available from www.ii.metu.edu.tr/?corpus/
treebank.html.
This treebank comprises 5,635 sentences in which words are represented with IG-
based gold-standard morphological representation and dependency links between IGs.
4 It is worth pointing out that arrows in this representation point from dependents to heads, because
representations with arrows in the opposite direction also exist in the literature.
361
Computational Linguistics Volume 34, Number 3
The average number of IGs per word is 1.26 in running text, but the figure is higher for
open class words and 1 for high frequency function words which do not inflect. Of all
the dependencies in the treebank, 95% are head-final5 and 97.5% are projective.6
Even though the number of sentences in the Turkish Treebank is in the same range
as for many other available treebanks for languages such as Danish (Kromann 2003),
Swedish (Nivre, Nilsson, and Hall 2006), and Bulgarian (Simov, Popova, and Osenova
2002), the number of words is considerably smaller (54K as opposed to 70?100K for the
other treebanks). This corresponds to a relatively short average sentence length in the
treebank of about 8.6 words, which is mainly due to the richness of the morphological
structure, because often one word in Turkish may correspond to a whole sentence in
another language.
3. Dependency Parsing of Turkish
In the following sections, we investigate different approaches to dependency parsing
of Turkish and show that using parsing units smaller than words improves the parsing
accuracy. We start by describing our evaluation metrics and the data sets used, and
continue by presenting our baseline parsers: two naive parsers, which link a dependent
to an IG in the next word, and one rule-based parser. We then present our data-driven
parsers in the subsequent sections: a statistical parser using a conditional probabilistic
model (from now on referred to as the probabilistic parser) in Section 4 and a deter-
ministic classifier-based parser using discriminative learning (from now on referred to
as the classifier-based parser) in Section 5.
3.1 Data Sets and Evaluation Metrics
Our experiments are carried out on the entire treebank and all our results are reported
for this data set. We use ten-fold cross-validation for the evaluation of the parsers, except
for the baseline parsers which do not need to be trained. We divide the treebank data
into ten equal parts and in each iteration use nine parts as training data and test the
parser on the remaining part.
We report the results as mean scores of the ten-fold cross-validation, with standard
error. The main evaluation metrics that we use are the unlabeled attachment score
(ASU) and labeled attachment score (ASL), namely, the proportion of IGs that are
attached to the correct head (with the correct label for ASL). A correct attachment is
one in which the dependent IG (the last IG in the dependent word) is not only attached
to the correct head word but also to the correct IG within the head word. Where relevant, we
also report the (unlabeled) word-to-word score (WWU), which only measures whether
a dependent word is connected to (some IG in) the correct head word. It should be
clear from the discussion in Section 2.1 and from Figure 1 that the IG-to-IG evaluation
is the right one to use for Turkish even though it is more stringent than word-to-
word evaluation. Dependency links emanating from punctuation are excluded in all
5 Half of the head-initial dependencies are actually not real head-initial structures; these are caused by
some enclitics (addressed in detail in the following sections) which can easily be recovered with some
predefined rules.
6 A dependency between a dependent i and a head j is projective if and only if all the words or IGs that
occur between i and j in the linear order of the sentence are dominated by j. A dependency analysis with
only projective dependencies corresponds to a constituent analysis with only continuous constituents.
362
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
evaluation scores. Non-final IGs of a word are assumed to link to the next IG within the
word, but these links, referred to as InnerWord links, are not considered as dependency
relations and are excluded in evaluation scoring.
3.2 Baseline Parsers
We implemented three baseline parsers to assess the performance of our probabilistic
and classifier-based parsers. The first baseline parser attaches each word (from the last
IG) to the first IG of the next word while the second parser attaches each word to the
final IG of the next word. Obviously these two baseline parsers behave the same when
the head word has only one IG. The final punctuation of each sentence is assumed to
be the root of the sentence and it is not connected to any head. The first two lines of Ta-
ble 1 give the unlabeled attachment scores of these parsers. We observe that attaching
the link to the first IG instead of the last one gives better results.
The third baseline parser is a rule-based parser that uses a modified version of the
deterministic parsing algorithm by Nivre (2006). This parsing algorithm, which will be
explained in detail in Section 5, is a linear-time algorithm that derives a dependency
graph in one left-to-right pass over the input, using a stack to store partially processed
tokens and a list to store remaining input tokens in a way similar to a shift-reduce
parser. In the rule-based baseline parser, the next parsing action is determined according
to 31 predefined hand-written rules (Eryig?it 2006; Eryig?it, Adal?, and Oflazer 2006).
The rules determine whether or not to connect the units (words or IGs) on top of the
stack and at the head of the input list (regardless of dependency labels). It can be
seen that the rule-based parser provides an improvement of about 15 percentage points
compared to the relatively naive simpler baseline parsers which cannot recover head-
initial dependencies.
4. Probabilistic Dependency Parser
A well-studied approach to dependency parsing is a statistical approach where the
parser takes a morphologically tagged and disambiguated sentence as input, and
outputs the most probable dependency tree by using probabilities induced from the
training data. Such an approach comprises three components:
1. A parsing algorithm for building the dependency analyses (Eisner 1996;
Sekine, Uchimoto, and Isahara 2000)
2. A conditional probability model to score the analyses (Collins 1996)
Table 1
Unlabeled attachment scores and unlabeled word-to-word scores for the baseline parsers.
Parsing Model ASU WWU
Attach-to-next (first IG) 56.0 63.3
Attach-to-next (last IG) 54.1 63.3
Rule-based 70.5 79.3
363
Computational Linguistics Volume 34, Number 3
3. Maximum likelihood estimation to make inferences about the underlying
probability models (Collins 1996; Chung and Rim 2004)
4.1 Methodology
The aim of our probabilistic model is to assign a probability to each candidate depen-
dency link by using the frequencies of similar dependencies computed from a training
set. The aim of the parsing algorithm is then to explore the search space in order to find
the most probable dependency tree. This can be formulated with Equation (1) where S
is a sequence of n units (words or IGs) and T ranges over possible dependency trees
consisting of dependency links dep(ui,uH(i) ), with uH(i) denoting the head unit to which
the dependent unit ui is linked and the probability of a given tree is the product of the
dependency links that it comprises.
T? = argmax
T
P(T|S) = argmax
T
n?1
?
i=1
P(dep (ui,uH(i) ) |S) (1)
The observation that 95% of the dependencies in the Turkish treebank are head-
final dependencies motivated us to employ the backward beam search dependency
parsing algorithm by Sekine, Uchimoto, and Isahara (2000) (originally designed for
Japanese, another head-final language), adapted to our morphological representation
with IGs. This algorithm parses a sentence starting from the end moving towards the
beginning, trying at each step to link the dependents to a unit to the right. It uses a
beam which keeps track of the most probable dependency structures for the partially
processed sentence. However, in order to handle head-initial dependencies, it employs
three predefined lexicalized rules7 (also used in our rule-based baseline parser). For
every new word, the parser starts by checking if any of the rules apply. If so, it constructs
a right-to-left link whereH(i) < i and directly assigns 1.0 as the dependency probability
(P(dep (ui,uH(i) ) |S) = 1.0). If none of the rules apply, it instead uses probabilities for
head-final dependencies.
For the probability model, we adopt the approach by Chung and Rim (2004), which
itself is a modified version of the statistical model used in Collins (1996).8 In this model
in Equation (2), the probability of a dependency link P(dep (ui,uH(i) ) |S) linking ui to a
head uH(i) is approximated with the product of two probabilities:
P(dep (ui,uH(i) ) |S) ? P(link(ui,uH(i) ) |?i ?H(i) ) (2)
P(ui links to some head dist(i,H(i)) away |?i)
7 The rules check for enclitics such as de, ki, mi, written on the right side of and separately from the word
they attach to, for the verb deg?il, which gives a negative meaning to the word coming before it and for
nominals which do not have any verbs on their right side.
8 The statistical model in Collins (1996) is actually used in a phrase-structure-based parsing approach, but
it uses the same idea of computing probabilities between dependents and head units. We also tried to
employ the statistical model of Collins, where the distance measure ?i,H(i) is included in the link
probability formula (P(dep (ui,uH(i) ) |S) ? P(link(ui,uH(i) ) |?i,H(i) )) , but we obtained worse results
with this.
364
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
In this equation,
 P(link(ui,uH(i) ) |?i ?H(i) ) is the probability of seeing the same dependency
within a similar context where ?i represents the context around the
dependent ui and ?H(i) represents the context around the head uH(i), and
 P(ui links to some head dist(i,H(i)) away |?i) is the probability of seeing
the dependent linking to some head a distance dist(i,H(i)) away, in the
context ?i.
In all of the following models, dist(i,H(i)) is taken as the number of actual word
boundaries between the dependent and the head unit regardless of whether full words
or IGs were used as units of parsing.9
To alleviate the data sparseness, we use the interpolation of other estimates while
calculating the probabilities in Equation (2).10 We use a strategy similar to Collins (1996)
and we interpolate with estimates based on less context:
P(x|y) ? ? ? P1(x|y) + (1 ? ?) ? P2(x) (3)
where ? = ?/(?+ 1) and ? is the count of the x occurrences
During the actual runs, the smoothed probability P(link(ui,uH(i) ) |?i ?H(i) ) is estimated
by interpolating two unsmoothed empirical estimates extracted from the treebank:
P1(link(ui,uH(i) ) |?i ?H(i) ) and P2(link(ui,uH(i) )). A similar approach was employed for
P(ui links to some head dist(i,H(i)) away |?i) and it is estimated by interpolating
P1(ui links to some head dist(i,H(i)) away |?i) and P2(ui links to some head dist(i,H(i))
away). If even after interpolation, the probability is 0, then a very small value is
used. Further, distances larger than a certain threshold value were assigned the same
probability, as explained later.
4.2 The Choice of Parsing Units
In the probabilistic dependency parsing experiments, we experimented with three dif-
ferent ways of choosing and representing the units for parsing:11
1. Word-based model #1: In this model, the units of parsing are the actual words
and each word is represented by a combination of the representations of all the IGs
that make it up. Note that although all IGs are used in representing a word, not all the
information provided by an IG has to be used, as we will see shortly. This representation,
however, raises the following question: If we use the words as the parsing units and
9 We also tried other distance functions, for example, the number of IGs between dependent and head
units, but this choice fared better than the alternatives.
10 We tried many other different interpolation and backoff models where we tried to remove the neighbors
one by one or the inflectional features. But we obtained the best results with a two-level interpolation by
removing the contextual information all at once.
11 Clarifying examples of these representations will be provided in the immediately following section.
365
Computational Linguistics Volume 34, Number 3
find the dependencies between these, how can we translate these to the dependencies
between the IGs, since our goal is to find dependencies between IGs? The selection
of the IG of the dependent word is an easy decision, as it is the last IG in the word.
The selection of the head IG is obviously more difficult. Because such a word-based
model will not provide much information about the underlying IGs structure, we will
have to make some assumptions about the head IG. The observation that 85.6% of the
dependency links in the treebank land on the first (and possibly the only) IG of the
head word and the fact that our first baseline model (attaching to the first IG) gives
better performance than our second baseline model (attaching to the last IG), suggest
that after identifying the correct word, choosing the first IG as the head IG may be a
reasonable heuristic. Another approach to determining the correct IG in the head word
could be to develop a post-processor which selects this IG using additional rules. Such
a post-processor could be worth developing if the WWU accuracy obtained with this
model proves to be higher than all of the other models, that is, if this is the best way
of finding the correct dependencies between words without considering which IGs are
connected. However, as we will see in Section 4.4, this model does not give the best
WWU.
2. Word-based model #2: This model is just like the previous model but we rep-
resent a word using its final IGs rather than the concatenation of all their IGs when it
is used as a dependent. The representation is the same as in Word-based model #1 when
the word is a head. This results in a dynamic selection of the representation during
parsing as the representation of a word will be determined according to its role at that
moment. The representation of the neighboring units in context will again be selected
with respect to the word in question: any context unit on the left will be represented
with its dependent representation (just the last IG) and any neighbor on the right will
be represented with its representation as a head. The selection of the IG in the head
word is the same as in the first model.
3. IG-based model: In this model, we use IGs as units in parsing. We split the IG-
based representation of each word and reindex these IGs in order to use them as single
units in parsing. Figure 2 shows this transfer to the IG-based model. We still, however,
need to know which IGs are word-final as they will be the dependent IGs (shown in
the figure by asterisks). The contextual elements that are used in this model are the
IGs to the left (starting with the last IG of the preceding word) and to the right of the
dependent and the head IG.
Figure 2
Mapping from word-based to IG-based representation of a sentence.
366
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
4.3 Reduced Dynamic Representations for IGs
In all three models, it is certainly possible to use all the information supplied by the
full morphological analysis in representing the IGs.12 This includes the root words
themselves, major and minor parts of speech,13 number and person agreement markers,
possessive agreement markers, case markers, tense, aspect, mood markers, and other
miscellaneous inflectional and semantic markers especially for derivations. Not all of
these features may be relevant to the parsing task, and further, different features may
be relevant depending on whether the IG is being used as a dependent or a head. Also,
in order to alleviate the data sparseness problem that may result from the relatively
modest size of the treebank, an ?unlexicalized? representation that does not contain the
root word needs to be considered so that statistics from IGs that are otherwise the same
except for the root word (if any) can be conflated.14 After some preliminary experimen-
tation, we decided that a reduced representation for IGs that is dynamically selected
depending on head or dependent status would give us the best performance. We explain
the representation of the IGs and the parameters that we used in the three models.
 When used as a dependent (or part of a dependent word in models 1
and 2) during parsing;
? Nominal IGs (nouns, pronouns, and other derived forms that
inflect with the same paradigm as nouns, including infinitives, past
and future participles) are represented only with the case marker,
because that essentially determines the syntactic function of that IG
as a dependent, and only nominals have cases.
? Any other IG is just represented with its minor part of speech.
 When used as a head (or part of a head word in models 1 and 2);
? Nominal IGs and adjective IGs with participle minor part of
speech15 are represented with the minor part of speech and the
possessive agreement marker.
? Any other IG is just represented with its minor part of speech.
Figures 3?5 show, for the first three words in Figure 1, the unlexicalized reduced
representations that are used in the three models when units are used as dependents
and heads during parsing.
12 See Figure 1 for a sample of such information.
13 A minor part-of-speech category is available for some major part-of-speech categories: pronouns are
further divided into personal pronouns, demonstrative pronouns, interrogative pronouns, and so on. The
minor part-of-speech category always implies the major part of speech. For derived IGs, the minor part of
speech mostly indicates a finer syntactic or semantic characterization of the derived word. When no
minor part of speech is available the major part of speech is used.
14 Remember that only the first IG in a word has the root word.
15 These are modifiers derived from verbs. They have adjective as their major part of speech and
past/future participle as their minor part of speech. They are the only types of IGs that have possessive
agreement markers other than nominals.
367
Computational Linguistics Volume 34, Number 3
Figure 3
Reduced IG representation for Word-based model #1.
Figure 4
Reduced IG representation for Word-based model #2.
4.4 Experimental Results
In this section, we first evaluate the performance of the models described in Section 4.2.
We then investigate the impact of different choices of morphological features on the best
performing IG-based model. In addition to the parsing model, the parser is given the
following parameters:
 the number of left and right neighbors of the dependent (Dl, Dr) to define
the dependent context ?i,
16
 the number of left and right neighbors of the head (Hl, Hr) to define the
head context ?H(i),
 the size of the beam (beamsize), and
16 In terms of parsing units, the number of words for word-based models and the number of IGs for
IG-based models.
368
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
Figure 5
Reduced IG representation for IG-based model.
 the distance threshold value beyond which P(ui links to some head
dist(i,H(i)) away |?i) is assigned the same probability.
Table 2 gives the ASU scores for the word-based and IG-based models for the
best combinations of contexts used for each case. We also provide WWU scores for
comparison, but again stress that the main evaluation criterion is the ASU score. For
all three models, the beamsize value is selected as 3 and distance threshold is selected
as 6.17 It can be seen that the performance of the word-based models is lower than
our rule-based baseline parser (Table 1) with ASU = 70.5, even though it is better than
the first two rather naive baselines. On the other hand, the IG-based model outper-
forms all of the baseline parsers and word-based models. It should also be noted that
the IG-based model improves not only the ASU accuracy but also the word-to-word
accuracy compared, to the word-based models. Thus, the IG-based model not only
helps to recover the relations between correct IGs but also to find the correct head
word.
In Table 3, we also present results from experiments employing different represen-
tations for the IGs. A more detailed investigation about the use of limited lexicalization
and inflectional features will be presented later in Section 6. Here, we will see what
would have happened if we had used alternative reduced IG representations compared
to the representation described earlier, which is used in the best performing IG-based
model.
Table 3 gives the results for each change to the representational model. One can
see that none of these representational changes improves the performance of the best
performing model. Only employing major part-of-speech tags (#1) actually comes close,
and the difference is not statistically significant. Lexicalization of the model results in
a drastic decrease in performance: Using the surface form (#6) gives somewhat better
17 As stated earlier in Section 4.1, our distance function is calculated according to the word boundaries
between the dependent and the head units. In the treebank, 95% of the dependency links link to a word
that is less than six words away. Thus all the distances larger than or equal to six are conflated into the
same small probability.
369
Computational Linguistics Volume 34, Number 3
Table 2
Unlabeled attachment scores and unlabeled word-to-word scores for the probabilistic parser.
Parsing Model (parameters) ASU WWU
Word-based model #1 (Dl=1, Dr=1, Hl=1, Hr=1) 68.1?0.4 77.1?0.7
Word-based model #2 (Dl=1, Dr=1, Hl=1, Hr=1) 68.3?0.3 77.6?0.5
IG-based model (Dl=1, Dr=1, Hl=0, Hr=1) 72.1?0.3 79.0?0.7
results than using root information (#5). Also, dynamic selection of tags seems to help
performance (#3) but using all available inflectional information performs significantly
worse possibly due to data sparseness.
5. Classifier-Based Dependency Parser
Our second data-driven parser is based on a parsing strategy that has achieved a high
parsing accuracy across a variety of different languages (Nivre et al 2006, 2007). This
strategy consists of the combination of the following three techniques:
1. Deterministic parsing algorithms for building dependency graphs (Kudo
and Matsumoto 2002; Nivre 2003; Yamada and Matsumoto 2003)
Table 3
Unlabeled attachment scores for different choices for morphological features.
Model ASU
IG-based model
# (Dl=1, Dr=1, Hl=0, Hr=1) 72.1?0.3
1 Using major part of speech 71.2?0.2
instead of minor part of speech
2 Using only minor part of speech and 68.3?0.2
no other inflectional features
3 Using minor part of speech for all 71.0?0.3
types of IGs together with case and
possessive markers for nominals
and possessive marker for adjectives
(but no dynamic selection)
4 Using all inflectional features in 46.5?0.4
addition to minor part of speech
5 Adding root information to the best 53.7?0.2
performing IG-based model
6 Adding surface form information to the best 54.4?0.2
performing IG-based model
370
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
2. History-based models for predicting the next parser action (Black et al
1992; Magerman 1995; Ratnaparkhi 1997; Collins 1999)
3. Discriminative classifiers to map histories to parser actions (Kudo and
Matsumoto 2002; Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson
2004)
A system of this kind employs no grammar but relies completely on inductive learning
from treebank data for the analysis of new sentences, and on deterministic parsing
for disambiguation. This combination of methods guarantees that the parser is robust,
never failing to produce an analysis for an input sentence, and efficient, typically
deriving this analysis in time that is linear in the length of the sentence.
In the following sections, we will first present the parsing methodology and then
results that show that the IG-based model again outperforms the word-based model. We
will then explore how we can further improve the accuracy by exploiting the advantages
of this parser. All experiments are performed using the freely available implementation
MaltParser.18
5.1 Methodology
For the experiments in this article, we use a variant of the parsing algorithm proposed
by Nivre (2003, 2006), a linear-time algorithm that derives a labeled dependency graph
in one left-to-right pass over the input, using a stack to store partially processed tokens
and a list to store remaining input tokens. However, in contrast to the original arc-eager
parsing strategy, we use an arc-standard bottom-up algorithm, as described in Nivre
(2004). Like many algorithms used for dependency parsing, this algorithm is restricted
to projective dependency graphs.
The parser uses two elementary data structures, a stack ? of partially analyzed
tokens and an input list ? of remaining input tokens. The parser is initialized with an
empty stack and with all the tokens of a sentence in the input list; it terminates as soon
as the input list is empty. In the following, we use subscripted indices, starting from 0,
to refer to particular tokens in ? and ?. Thus, ?0 is the token on top of the stack ? (the
top token) and ?0 is the first token in the input list ? (the next token); ?0 and ?0 are
collectively referred to as the target tokens, because they are the tokens considered as
candidates for a dependency relation by the parsing algorithm.
There are three different parsing actions, or transitions, that can be performed in
any non-terminal configuration of the parser:
 Shift: Push the next token onto the stack.
 Left-Arcr: Add a dependency arc from the next token to the top token,
labeled r, then pop the stack.
 Right-Arcr: Add a dependency arc from the top token to the next token,
labeled r, then replace the next token by the top token at the head of the
input list.
18 http://w3.msi.vxu.se/users/nivre/research/MaltParser.html.
371
Computational Linguistics Volume 34, Number 3
In order to perform deterministic parsing in linear time, we need to be able to predict
the correct parsing action (including the choice of a dependency type r for Left-Arcr
and Right-Arcr) at any point during the parsing of a sentence. This is what we use a
history-based classifier for.
The features of the history-based model can be defined in terms of different linguis-
tic features of tokens, in particular the target tokens. In addition to the target tokens,
features can be based on neighboring tokens, both on the stack and in the remaining
input, as well as dependents or heads of these tokens in the partially built dependency
graph. The linguistic attributes available for a given token are the following:
 Lexical form (root) (LEX)
 Part-of-speech category (POS)
 Inflectional features (INF)
 Dependency type to the head if available (DEP)
To predict parser actions from histories, represented as feature vectors, we use sup-
port vector machines (SVMs), which combine the maximum margin strategy introduced
by Vapnik (1995) with the use of kernel functions to map the original feature space
to a higher-dimensional space. This type of classifier has been used successfully in
deterministic parsing by Kudo and Matsumoto (2002), Yamada and Matsumoto (2003),
and Sagae and Lavie (2005), among others. To be more specific, we use the LIBSVM
library for SVM learning (Chang and Lin 2001), with a polynomial kernel of degree 2,
with binarization of symbolic features, and with the one-versus-one strategy for multi-
class classification.19
This approach has some advantages over the probabilistic parser, in that
 it can process both left-to-right and right-to-left dependencies due to its
parsing algorithm,
 it assigns dependency labels simultaneously with dependencies and can
use these as features in the history-based model, and
 it does not necessarily require expert knowledge about the choice of
linguistically relevant features to use in the representations because SVM
training involves implicit feature selection.
However, we still exclude sentences with non-projective dependencies during train-
ing.20 Because the classifier-based parser not only builds dependency structures but also
assigns dependency labels, we give ASL scores as well as ASU scores.
19 Experiments have also been performed using memory-based learning (Daelemans and Bosch 2005). They
were found to give lower parsing accuracy.
20 Because the frequency of non-projective dependencies in the Turkish Treebank is not high enough to
learn such dependencies and mostly due to the unconnected punctuations with which we are dealing by
adding an extra dependency label, we did not observe any improvement when applying the
pseudo-projective processing of Nivre and Nilsson (2005), which is reported to improve accuracy for
other languages.
372
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
5.2 Experimental Results
In this section, our first aim is to confirm the claim that using IGs as the units in parsing
improves performance. For this purpose, we start by using models similar to those
described in the previous section. We use an unlexicalized feature model where the
parser uses only the minor POS and the DEP of tokens and compare the results with the
probabilistic parser. We then show in the second part how we can improve accuracy by
exploiting the morphological structure of Turkish and taking advantage of the special
features of this parser.
5.2.1 Comparison with the Probabilistic Parser. In order to compare with the results of the
previous section, we adopt the same strategy that we used earlier in order to present
inflectional groups. We employ two representation models:
 Word-based model, where each word is represented by the concatenation
of its IGs,
 IG-based model, where the units are inflectional groups.
We take the minor POS category plus the case and possessive agreement markers for
nominals and participle adjectives to make up the POS feature of each IG.21 However,
we do not employ dynamic selection of these features and just use the same strategy
for both dependents and the heads. The reason is that, in this parser, we do not make
the assumption that the head is always on the right side of the dependent, but also
try to find head-initial dependencies, and the parser does not know at a given stage
if a unit is a candidate head or dependent. In the IG-based model, InnerWord relations
(Figure 5), which are actually determined by the morphological analyzer, are processed
deterministically without consulting the SVM classifiers.22
The feature model (Feature Model #1) to be used in these experiments is shown
in Figure 6. This feature model uses five POS features, defined by the POS of the two
topmost stack tokens (?0, ?1), the first two tokens of the remaining input (?0, ?1), and
the token which comes just after the topmost stack token in the actual sentence (?0 + 1).
The dependency type features involve the top token on the stack (?0), its leftmost and
rightmost dependent (l(?0), r(?0)), and the leftmost dependent of the next input token
(l(?0)).
The results for this feature model and the two representation models can be seen
in Table 4. We again see that the IG-based model outperforms the word-based model.
When we compare the unlabeled (ASU) scores with the results of the probabilistic parser
(from Table 2), we see that we do not obtain any improvements neither for the IG-based
model nor for the word-based model. This is probably the combined effect of not using
21 Thus, we are actually combining some inflectional features with the part-of-speech category and use
them together in the POS feature.
22 Because only the first IG of a word carries the stem information (and the remaining IGs has null ? ?
values for this field), a lexicalized model can easily determine the InnerWord links without need for a
deterministic model. For the unlexicalized models, it is necessary to process InnerWord relations
deterministically in order to get the full benefit of IG-based parsing, because the classifiers cannot
correctly predict these relations without lexical information (Eryig?it, Nivre, and Oflazer 2006). However,
for the lexicalized models, adding deterministic InnerWord processing has no impact at all on parsing
accuracy, but it reduces training and parsing time by reducing the number of training instances for the
SVM classifiers.
373
Computational Linguistics Volume 34, Number 3
Figure 6
Feature models for the classifier-based parser.
Table 4
Unlabeled and labeled attachment scores for the unlexicalized classifier-based parser.
Parsing Model ASU ASL
Word-based model 67.1?0.3 57.8?0.3
IG-based model 70.6?0.2 60.9?0.3
the lexical information for head-initial dependencies that we use in our rules in the
probabilistic parser, and of not using dynamic selection.23
5.2.2 Exploiting the Advantages of the Classifier-Based Parser. To exploit the advantages
of the classifier-based parser, we now describe a setting which does not rely on any
linguistic knowledge on the selection of inflectional features and lets the classifier of the
parser select the useful combinations of the features. As SVMs can perform such tasks
successfully, we now explore different representations of the morphological data in the
IG-based model to see if the performance can be improved.
As shown in earlier examples, the inflectional information available for a given
token normally consists of a complex combination of atomic features such as +A3sg,
+Pnon, and +Loc. Eryig?it, Nivre, and Oflazer (2006) showed that adding inflectional
features as atomic values to the feature models was better than taking certain subsets
with linguistic intuition and trying to improve on them. Thus we now present results
with the feature model where the POS component only comprises the minor part of
speech and the INF comprises all the other inflectional features provided by the tree-
bank without any reduction. We investigate the impact of this approach first with an
unlexicalized model (Feature Model #2 in Figure 6) and then with a lexicalized model
(Feature Model #3 in Figure 6) where we investigate two different kinds of lexicalization:
one using just the root information and one using the complete surface form as lexical
features.
Table 5 gives the results for both unlexicalized and lexicalized models with INF
features included in the feature model. We can see the benefit of using inflectional
features separately and split into atomic components, by comparing the first line of
the table with the best results for the IG-based model in Table 4. We can also note
23 Actually, the equivalent of this IG-based model is the probabilistic model #3 in Table 3 (with no dynamic
selection), which does not do significantly better than this classifier-based model.
374
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
Table 5
Unlabeled and labeled attachment scores for enhancements of the IG-based model.
Feature Model ASU ASL
Feature Model #2 (no lexicalization) 72.4?0.2 63.1?0.3
Feature Model #3 (lex. with surface forms) 75.7?0.2 66.6?0.3
Feature Model #3 (lex. with roots) 76.0?0.2 67.0?0.3
the improvement that lexicalized models bring:24 In contrast to the probabilistic parser,
lexicalization using root information rather than surface form gives better performance,
even though the difference is not statistically significant. The improvement in ASU
score is 3.9 percentage points for the lexicalized model (with root) over the IG-based
model of the probabilistic parser with ASU=72.1?0.3. A similar case can be observed for
WWU accuracies: Including INF and lexicalization with roots gives WWU=82.7?0.5 on
the entire treebank, which provides an improvement of 3.3 percentage points over the
IG-based model of the probabilistic parser (withWWU=79.0?0.7).
6. The Impact of Inflectional Features and Lexicalization
In the previous sections, we presented our parsers using optimized parameters and
feature representations. We have observed that using complete inflectional features and
lexicalized models improves the accuracy of the classifier-based parser significantly,
whereas for the probabilistic parser adding these features has a negative impact on
accuracy. In this section, we investigate the influence of different inflectional features
and lexical information on both parsers using the best performing IG-based models,
in order to get a more fine-grained picture. The results of the experiments with the
classifier-based parser are not strictly comparable to those of other experiments, because
the training data have here been divided into smaller sets (based on the major part of
speech category of the next token) as a way of reducing SVM training times without a
significant decrease in accuracy. For the probabilistic parser, we have not used dynamic
selection while investigating the impact of inflectional features.
6.1 Inflectional Features
In order to see the influence of inflectional features, we tested six different sets, where
each set includes the previous one and adds some more inflectional features. The
following list describes each set in relation to the previous one:
Set 1 No inflectional features except for minor part of speech
Set 2 Set 1 + case and possessive markers for nominals, possessive markers for partici-
ple adjectives
Set 3 Set 2 + person/number agreement features for nominals and verbs
Set 4 Set 3 + all inflectional features for nominals
24 The unlabeled exact match score (that is, the percentage of sentences for which all dependencies are
correctly determined) for this best performing model is 37.5% upon IG-based evaluation and 46.5% upon
word-based evaluation.
375
Computational Linguistics Volume 34, Number 3
Set 5 Set 4 + all inflectional features for verbs
Set 6 Set 5 + all inflectional features
Figure 7 shows the results for both the probabilistic and the classifier-based parser.
The results shown in Figures 7b confirm the importance of case and possessive features,
which was presupposed in the manual selection of features in Section 4. Besides these,
the number/person agreement features available for nominals and verbs are also impor-
tant inflectional features even though they do not provide any statistically significant
increase in accuracy (except for ASU in Figure 7b [Set 3]). Another point that merits
attention is the fact that the labeled accuracy is affected more by the usage of inflectional
features compared to unlabeled accuracy. The difference between Set 1 and Set 2 (in
Figure 7b) is nearly 4 percentage points for ASU and 10 percentage points for ASL. It
thus appears that inflectional features are especially important in order to determine the
type of the relationship between the dependent and head units. This is logical because
in Turkish it is usually not the word order that determines the roles of the constituents
in a sentence, but the inflectional features (especially the case markers). We again see
from these figures that the classifier-based parser does not suffer from sparse data even
if we use the full set of inflectional features (Set 6) provided by the treebank, whereas the
probabilistic parser starts having this problem even with Set 3 (Figure 7a). The problem
gets worse when we add the complete set of inflectional features.
6.2 Lexicalization
In order to get a more fine-grained view of the role of lexicalization, we have investi-
gated the effect of lexicalizing IGs from different major part-of-speech categories. We
expand this analysis into POS categories where relevant. The results are shown in Ta-
ble 6, where the first column gives the part-of-speech tag of the lexicalized units, and
the second and third columns give the total frequency and the frequency of distinct roots
for that part-of-speech tag. We again see that the probabilistic parser suffers from sparse
data especially for part-of-speech tags that appear with a high number of distinct roots.
We cannot observe any increase with the lexicalization of any category. The situation is
different for the classifier-based parser. None of the individual lexicalizations causes a
decrease. We see that the lexicalization of nouns causes a significant increase in accuracy.
Figure 7
Accuracy for feature sets 1?6:
a) Unlabeled accuracy for probabilistic parser
b) Unlabeled and labeled accuracy for classifier-based parser
376
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
Table 6
Unlabeled and labeled attachment scores for limited lexicalization (n = count, d = number of
distinct roots).
Probabilistic Classifier-based
n d ASU ASU ASL
None - - 72.1?0.3 72.8?0.2 63.2?0.3
Adjectives 6446 735 68.7?0.2 72.9?0.2 63.2?0.3
Adverbs 3033 221 69.8?0.3 73.1?0.2 63.4?0.3
Conjunctions 2200 44 67.8?0.4 74.1?0.2 64.2?0.3
Determiners 1998 13 71.8?0.3 72.8?0.2 63.3?0.3
Duplications 11 9 72.0?0.3 72.8?0.2 63.2?0.3
Interjections 100 34 72.0?0.3 72.8?0.2 63.2?0.3
Nouns 21860 3935 53.7?0.3 73.9?0.2 64.6?0.3
Numbers 850 226 71.4?0.3 72.9?0.2 63.3?0.3
Post-positions 1250 46 70.9?0.3 72.9?0.2 63.2?0.3
Pronouns 2145 28 72.0?0.2 72.8?0.2 63.2?0.3
Punctuations 10420 16 72.1?0.3 73.4?0.2 63.7?0.3
Questions 228 6 71.9?0.2 72.8?0.2 63.2?0.3
Verbs 14641 1256 59.9?0.4 72.9?0.2 63.8?0.3
Lexicalization of verbs also gives a noticeable increase in the labeled accuracy even
though this is not statistically significant. A further investigation on the minor parts of
speech of nouns25 shows that only common nouns have this positive effect, whereas the
lexicalization of proper nouns does not improve accuracy. We see that the lexicalization
of conjunctions also improves the accuracy significantly. This improvement can be at-
tributed to the enclitics (such as de, ki,mi, written on the right side of and separately from
the word they attach to), which give rise to head-initial dependencies. These enclitics,
which are annotated as conjunctions in the treebank, can be differentiated from other
conjunctions by lexicalization which makes it very easy to connect them to their head
on the left.
Because we did not observe any improvement in the probabilistic parser, we con-
tinued further experimentation only with the classifier-based parser. We tried partially
lexicalized models by lexicalizing various combinations of certain POS categories (see
Figure 8). The results show that, whereas lexicalization certainly improves parsing
accuracy for Turkish, only the lexicalization of conjunctions and nouns together has
an impact on accuracy. Similarly to the experiments on inflectional features, we again
see that the classifier-based parser has no sparse data problem even if we use a totally
lexicalized model.
Although the effect of lexicalization has been discussed in several studies recently
(Dubey and Keller 2003; Klein and Manning 2003; Arun and Keller 2005), it is often
investigated as an all-or-nothing affair, except for a few studies that analyze the distri-
butions of lexical items, for example, Bikel (2004) and Gildea (2001). The results for
25 IGs with a noun part-of-speech tag other than common nouns are marked with an additional minor part
of speech that indicates whether the nominal is a proper noun or a derived form?one of future
participle, past participle, infinitive, or a form involving a zero-morpheme derivation. These latter four
do not contain any root information.
377
Computational Linguistics Volume 34, Number 3
Figure 8
Unlabeled and labeled attachment scores for incrementally extended lexicalization for the
classifier-based parser.
Turkish clearly show that the effect of lexicalization is not uniform across syntactic
categories, and that a more fine-grained analysis is necessary to determine in what
respects lexicalization may have a positive or negative influence. For some models
(especially those suffering from sparse data), it may even be a better choice to use some
kind of limited lexicalization instead of full lexicalization, although the experiments
in this article do not show any example of that. The results from the previous section
suggests that the same is true for morphological information, but this time showing that
limited addition of inflectional features (instead of using them fully) helps to improve
the accuracy of the probabilistic parser.
7. The Impact of Training Set Size
In order to see the influence of the training set size on the performance of our parsers,
we designed the experiments shown in Figure 9, where the x-axis shows the number
of cross validation subsets that we used for training in each step. Figure 9 gives the
ASU scores for the probabilistic parser (unlexicalized except for head-initial rules) and
the classifier-based parser (unlexicalized and lexicalized). We observe that the relative
improvement with growing training set size is largest for the classifier-based lexicalized
model with a relative difference of 5.2?0.2 between using nine training subsets and one
training subset, whereas this number is 4.6?0.3 for the unlexicalized classifier-based
model and 2.5?0.2 for the unlexicalized probabilistic model. We can state that despite its
lower accuracy, the probabilistic model is less affected by the size of the training data.
We can see from this chart that the relative ranking of the models remain the same,
except for sizes 1?3, where the probabilistic parser does better (or no worse than) the
unlexicalized classifier-based models. Another conclusion may be that classifier-based
models are better at extracting information with the increasing size of the data in hand,
whereas the probabilistic model cannot be improved very much with the increasing size
of the data. We can observe this situation especially in the lexicalized model which is
improved significantly between size = 6 subsets and size = 9 subsets, whereas there is
no significant improvement on the unlexicalized models within this interval.
378
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
68
69
70
71
72
73
74
75
76
77
1 2 3 4 5 6 7 8 9
# cross validation sets used in training
probabilistic unlex.
classifier-based unlex.
classifier-based lex.
Figure 9
Unlabeled attachment score for different training set sizes.
8. Error Analysis
In this section, we present a detailed error analysis on the results of our best per-
forming parser. We first evaluate our results on different dependency types. We then
investigate the error distribution in terms of distance between the head assigned by
the parser and the actual head. Finally, we look at the error distribution in relation
to sentence length. In the analysis, the results are aggregated over all ten folds of the
cross-validation.
8.1 Accuracy per Dependency Type
Table 7 gives theASU, labeled precision, labeled recall and labeled F-score for individual
dependency types. The table is sorted according to the ASU results, and the average
distance between head and dependent is given for each type.
We see that the parser cannot find labeled dependencies for the types that have
fewer than 100 occurrences in the treebank, with the single exception of RELATIVIZER,
the enclitic ki (conjunction), written separately from the word it attaches to. Because this
dependency type always occurs with the same particle, there is no sparse data problem.
If we exclude the low-frequency types, we can divide the results into three main
groups. The first group consists of determiners, particles, and nominals that have an
ASU score over 79% and link to nearby heads. The second group mainly contains
subjects, objects, and different kinds of adjuncts, with a score in the range 55?79% and
a distance of 1.8?4.6 IGs to their head. This is the group where inflectional features are
most important for finding the correct dependency. The third group contains distant
dependencies with a much lower accuracy. These are generally relations like sentence
modifier, vocative, and apposition, which are hard to find for the parser because they
cannot be differentiated from other nominals used as subjects, objects, or normal mod-
ifiers. Another construction that is hard to parse correctly is coordination, which may
require a special treatment.
379
Computational Linguistics Volume 34, Number 3
Table 7
Attachment score (ASU), labeled precision (P), labeled recall (R) and labeled F-score for each
dependency type in the treebank (n = count, dist = dependency length).
Label n dist ASU P R F
SENTENCE 7,252 1.5 90.5 87.4 89.2 88.3
DETERMINER 1,952 1.3 90.0 84.6 85.3 85.0
QUESTION.PARTICLE 288 1.3 86.1 80.0 76.4 78.2
INTENSIFIER 903 1.2 85.9 80.7 80.3 80.5
RELATIVIZER 85 1.2 84.7 56.6 50.6 53.4
CLASSIFIER 2,048 1.2 83.7 74.6 71.7 73.1
POSSESSOR 1,516 1.9 79.4 81.6 73.6 77.4
NEGATIVE.PARTICLE 160 1.4 79.4 76.4 68.8 72.4
OBJECT 7,956 1.8 75.9 63.3 62.5 62.9
MODIFIER 11,685 2.6 71.9 66.5 64.8 65.7
DATIVE.ADJUNCT 1,360 2.4 70.8 46.4 50.2 48.2
FOCUS.PARTICLE 23 1.1 69.6 0.0 0.0 0.0
SUBJECT 4,479 4.6 68.6 50.9 56.2 53.4
ABLATIVE.ADJUNCT 523 2.5 68.1 44.0 54.5 48.7
INSTRUMENTAL.ADJUNCT 271 3.0 62.7 29.8 21.8 25.2
ETOL 10 4.2 60.0 0.0 0.0 0.0
LOCATIVE.ADJUNCT 1,142 4.2 56.9 43.3 48.4 45.7
COORDINATION 814 3.4 54.1 53.1 49.8 51.4
S.MODIFIER 594 9.6 50.8 42.2 45.8 43.9
EQU.ADJUNCT 16 3.7 50.0 0.0 0.0 0.0
APPOSITION 187 6.4 49.2 49.2 16.6 24.8
VOCATIVE 241 3.4 42.3 27.2 18.3 21.8
COLLOCATION 51 3.3 41.2 0.0 0.0 0.0
ROOT 16 - 0.0 0.0 0.0 0.0
Total 43,572 2.5 76.0 67.0 67.0 67.0
8.2 Error Distance
When we evaluate our parser based on the dependency direction, we obtain an ASU
of 72.2 for head-initial dependencies and 76.2 for head-final ones. Figure 10a and
Figure 10b give the error distance distributions for head-initial and head-final depen-
dencies based on the unlabeled performance of the parser. The x-axis in the figures gives
the difference between indexes of the assigned head IG and the real head IG.
As stated previously, the head-initial dependencies constitute 5% of the entire de-
pendencies in the treebank. Figure 10a shows that for head-initial dependencies the
parser has a tendency to connect the dependents to a head closer than the real head
or in the wrong direction. When we investigate these dependencies, we see that 70%
of them are connected to a head adjacent to the dependent and the parser finds 90% of
these dependencies correctly. Thus, we can say that the parser has no problem in finding
adjacent head-initial dependencies. Moreover, 87% of the errors where the error distance
is equal to 1 (Figure 10a)26 are due to the dependents being connected to the wrong IG
26 Meaning that the actual head and assigned head are adjacent.
380
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
Figure 10
Error distance distributions a) for head-initial dependencies b) for head-final dependencies.
of the correct head word. When we investigate the ability of the parser in finding the
dependency direction, we see that our parser has a high precision value (91%) and a
relatively lower recall value (80%).
The parser is 100% successful in finding the direction of head-final dependencies.
Furthermore, the errors that it makes while determining the correct head have a roughly
normal distance distribution, as can be seen from Figure 10b.27 We can see from the same
figure that 57% of the errors fall within the interval of ?2 IGs away from the actual
head.
8.3 Sentence Length
Figure 11 shows the distribution of errors over sentences of different lengths. The
x-axis shows the sentence length (measured in number of dependencies), the y-axis
shows the error count, and the z-axis shows the number of sentences. As expected,
the distribution is dominated by short sentences with few errors (especially sentences
of up to seven dependencies with one error). The mean number of errors appears to
be a linear function of sentence length, which would imply that the error probability
27 Error distances with less than 40 occurrences are excluded from the figure.
381
Computational Linguistics Volume 34, Number 3
Figure 11
Error distribution versus sentence length.
per word does not increase with sentence length. This is interesting in that it seems to
indicate that the classifier-based parser does not suffer from error propagation despite
its greedy, deterministic parsing strategy.
9. The Impact of Morphological Disambiguation
In all of the experiments reported herein, we have used the gold-standard tags provided
by the treebank. Another point that deserves investigation is therefore the impact of
using tags automatically assigned by a morphological disambiguator, in other words
the accuracy of the parser on raw text. The role of morphological disambiguators for
highly inflectional languages is far more complex than assigning a single main POS
category (e.g., Noun, Verb, Adj) to a word, and also involves assigning the correct mor-
phological information which is crucial for higher level applications. The complexity of
morphological disambiguation in an agglutinative language like Turkish is due to the
large number of morphological feature tag combinations that can be assigned to words.
The number of potential morphological tag combinations in Turkish for all practical
purposes is very large due to productively derived forms.28
The two subsequent examples, for the words kalemi and asmadan, expose the two
phenomena that a Turkish morphological disambiguator should deal with. The outputs
of the morphological analyzer are listed below the words. The first example shows that
all three possible analyses of the word kalemi have ?Noun? as the POS category but they
differ in that they have different stems and inflectional features. In the second example
28 For the treebank data, the number of distinct combinations of morphological features is 718 for the
word-based model of the classifier-based parser and 108 for the IG-based model.
382
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
we see that the possible analyses also have different IG segmentations; the first two
analyses of the word asmadan consists of two IGs whereas the last one has one IG.
kalemi
kale +Noun+A3sg+P1sg+Acc (?my castle? in accusative form)
kalem +Noun+A3sg+P3sg+Nom (?his pencil?)
kalem +Noun+A3sg+Pnon+Acc (?the pencil? in accusative form)
asmadan
as +Verb+Pos DB +Adverb+WithoutHavingDoneSo (?without having hanged (it)?)
as +Verb+Pos DB +Noun+Inf2+A3sg+Pnon+Abl (?from hanging (it)?)
asma +Noun+A3sg+Pnon+Abl (?from the vine?)
The task of the morphological disambiguator is to choose one of the possible mor-
phological analyses and thus to find the correct inflectional features including parts
of speech, and the IG structure. We first used the two-level morphological analyzer of
Oflazer (1994) to analyze all the words in the treebank.29 This morphological analyzer
simultaneously produces the IG segmentation and the relevant features encoded in all
analyses of a word form. We then used the morphological disambiguator of Yu?ret and
Tu?re (2006), which has a reported accuracy of 96% for Turkish.
When tested on our treebank data, the accuracy of the morphological disambiguator
is 88.4%, including punctuation (which is unambiguous) and using a lookup table for
the words that are not recognized by the morphological analyzer.30 The lower accuracy
of the morphological disambiguator on the treebank can be due to different selections
in the annotation process of the morphological disambiguator training data (Yu?ret and
Tu?re 2006), which is totally different from the treebank data.
In order to investigate the impact of morphological disambiguation errors, we used
our best IG-based model and a lexicalized word-based model with our classifier-based
parser.31 We again evaluated our parsing models with ASU, ASL, and WWU scores.
There is no problem when evaluating withWWU scores because this metric only takes
into account whether the head word assigned to a dependent is correct or not, which
means that any errors of the morphological disambiguator can be ignored. Similarly, in
calculating ASU and ASL scores for the word-based model, dependencies are assumed
to be connected to the first IG of the head word without taking into consideration any
errors in tags caused by the morphological disambiguator. But when evaluating with
the ASU and ASL scores for the IG-based model, one problem that may appear is that
the disambiguator may have assigned a totally different IG structure to the head word,
compared to the gold standard (cf. the three analyses of the word asmadan). In this case,
we accept a dependency link to be correct if the dependent is connected to the correct
head word and the head IG has the same POS category as the gold-standard. This is
reasonable because we know that some of the errors in inflectional features do not affect
the type of dependency very much. For example, if we put the adjective ku?c?u?k (?small?)
29 We noted that 39% of the words were ambiguous and 17% had more than two distinct morphological
analyses.
30 The words not recognized by the morphological analyzer are generally proper nouns, numbers, and
some combined words that are created in the development stage of the treebank and constitute 6.2% of
the whole treebank. If these words are excluded, the accuracy of the tagger is 84.6%.
31 For this model, we added LEX features for ?0, ?0, ?1 to the feature model of our word-based model in
Table 4.
383
Computational Linguistics Volume 34, Number 3
Table 8
Impact of morphological disambiguation on unlabeled and labeled attachment scores and
word-to-word scores.
ASU ASL WWU
Word-based Gold standard 71.2?0.3 62.3?0.3 82.1?0.9
Tagged 69.5?0.3 59.3?0.3 80.2?0.9
IG-based Gold standard 76.0?0.2 67.0?0.3 82.7?0.5
Tagged 73.3?0.3 63.2?0.3 80.6?0.7
in front of the example given previously (ku?c?u?k kalemi), then the choice of morphological
analysis of the noun has no impact on the fact that the adjective should be connected
to the noun with dependency type ?MODIFIER?. Moreover, most of the errors in POS
categories will actually prevent the parser from finding the correct head word, which
can be observed from the drop inWWU accuracy.
Table 8 shows that the IG-based model and the word-based model are equally
affected by the tagging errors and have a drop in accuracy within similar ranges. (It
can also be seen that, even with automatically tagged data, the IG-based model gives
better accuracy than the word-based model.) We can say that the use of an automatic
morphological analyzer and disambiguator causes a drop in the range of 3 percentage
points for unlabeled accuracy and 4 percentage points for labeled accuracy (for both
word-based and IG-based models).
10. Related Work
The first results on the Turkish Treebank come from Eryig?it and Oflazer (2006) where the
authors used only a subset of the treebank sentences containing exclusively head-final
and projective dependencies. The parser used in that paper is a preliminary version of
the probabilistic parser used in this article. The first results on the entire treebank appear
in Nivre et al (2007), where the authors use memory-based learning to predict parser
actions, and in Eryig?it, Adal?, and Oflazer (2006), which introduces the rule-based parser
used in this article.
The Turkish Treebank has recently been parsed by 17 research groups in the CoNLL-
X shared task on multilingual dependency parsing (Buchholz and Marsi 2006), where it
was seen as the most difficult language by the organizers and most of the groups.32 The
following quote is taken from Buchholz and Marsi (page 161): ?The most difficult data
set is clearly the Turkish one. It is rather small, and in contrast to Arabic and Slovene,
which are equally small or smaller, it covers 8 genres, which results in a high percentage
of new FORM and LEMMA values in the test set.?
The results for Turkish are given in Table 9. Our classifier-based parser obtained
the best results for Turkish (with ASU=75.8 and ASL=65.7) and also for Japanese, which
is the only agglutinative and head-final language in the shared task other than Turkish
(Nivre et al 2006). The groups were asked to find the correct IG-to-IG dependency links.
When we look at the results, we observe that most of the best performing parsers use
32 The Turkish data used in the shared task is actually a modified version of the treebank used in this article;
some conversions are made on punctuation structures in order to keep consistency between all languages.
384
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
Table 9
CoNLL-X shared task results on Turkish (taken from Table 5 in Buchholz and Marsi [2006]).
Teams ASU ASL
Nivre et al (2006) 75.8 65.7
Johansson and Nugues (2006) 73.6 63.4
McDonald, Lerman, and Pereira (2006) 74.7 63.2
Corston-Oliver and Aue (2006) 73.1 61.7
Cheng, Asahara, and Matsumoto (2006) 74.5 61.2
Chang, Do, and Roth (2006) 73.2 60.5
Yu?ret (2006) 71.5 60.3
Riedel, C?ak?c?, and Meza-Ruiz (2006) 74.1 58.6
Carreras, Surdeanu, and Marquez (2006) 70.1 58.1
Wu, Lee, and Yang (2006) 69.3 55.1
Shimizu (2006) 68.8 54.2
Bick (2006) 65.5 53.9
Canisius et al (2006) 64.2 51.1
Schiehlen and Spranger (2006) 61.6 49.8
Dreyer, Smith, and Smith (2006) 60.5 46.1
Liu et al (2006) 56.9 41.7
Attardi (2006) 65.3 37.8
one of the parsing algorithms of Eisner (1996), Nivre (2003), or Yamada and Matsumoto
(2003) together with a learning method based on the maximum margin strategy. We
can also see that a common property of the parsers which fall below the average
(ASL=55.4) is that they do not make use of inflectional features, which is crucial for
Turkish.33
Another recent study that has promising results is C?ak?c? and Baldridge (2006),
where the authors use the MSTParser (McDonald, Lerman, and Pereira 2006), also used
in the CoNLL-X shared task (line 3 in Table 9). Following the work of Eryig?it and Oflazer
(2006) and Nivre et al (2006), they use the stem information and the case information
for nominals and they also report an increase in performance by using these features.
Similar to one of the models (?INF as a single feature?) in Eryig?it, Nivre, and Oflazer
(2006), where the feature names of the suffixes provided by the morphological analyzer
are concatenated and used as a feature to the classifier, they use the surface forms of
the suffixes as a whole. We can say that the models in this article cover this approach in
that each suffix is used as a single feature name (which is shown to perform better than
using them concatenated to each other in Eryig?it, Nivre, and Oflazer [2006]). Because in
Turkish, the same suffixes take different forms under vowel harmony34 and the surface
forms of some different suffixes are structurally ambiguous,35 using them with their
feature names is actually more meaningful. C?ak?c? and Baldridge (2006) report a word-
to-word accuracy of 84.9%, which seems competitive, but unfortunately from this we
33 Actually, there are two parsers (Bick 2006 and Attardi 2006 in Table 9) in this group which try to use parts
of the inflectional features under special circumstances.
34 For example, in the words ev+de (?at home?) and okul+da (?at school?), the suffixes -de and -da are the same
locative case suffixes (+Loc) but they take different forms due to vowel harmony.
35 For example, in the word ev+in, the surface morpheme -inmay indicate both a second singular possessive
suffix (+P2sg) which will give the word the meaning of ?your house? and a genitive case (+Gen) which
will give the word the meaning of ?of the house?, as the underlying lexicalmorphemes are different.
385
Computational Linguistics Volume 34, Number 3
are not able to gauge the IG-to-IG accuracy which we have argued is the right metric
to use for Turkish, and their results are not comparable to any of the results in the
literature, because they have not based their experiments on any of the official releases
of the treebank. In addition, they use an evaluation metric different from the ones in
the literature in that they only excluded some of the punctuations from the evaluation
score.
11. Conclusions
In this article, we have investigated a number of issues in data-driven dependency pars-
ing of Turkish. One of the main results is that IG-based models consistently outperform
word-based models. This result holds regardless of whether we evaluate accuracy on
the word level or on the IG level; it holds regardless of whether we use the probabilistic
parser or the classifier-based parser; and it holds even if we take into account the
problem caused by errors in automatic morphological analysis and disambiguation.
Another important conclusion is that the use of morphological information can
increase parsing accuracy substantially. Again, this result has been obtained both for the
probabilistic and the classifier-based parser, although the probabilistic parser requires
careful manual selection of relevant features to counter the effect of data sparseness.
A similar result has been obtained with respect to lexicalization, although in this case
an improvement has only been demonstrated for the classifier-based parser, which is
probably due to its greater resilience to data sparseness.
By combining the deterministic classifier-based parsing approach with an adequate
use of IG-based representations, morphological information, and lexicalization, we have
been able to achieve the highest reported accuracy for parsing the Turkish Treebank.
Acknowledgments
We are grateful for the financial support
from TUBITAK (The Scientific and Technical
Research Council of Turkey) and Istanbul
Technical University. We want to thank Johan
Hall and Jens Nilsson in the language
technology group at Va?xjo? University for
their contributions to the classifier-based
parser framework (MaltParser) within which
we developed the classifier-based parser for
Turkish. We also want to thank Deniz Yu?ret
for providing us with his morphological
disambiguator, and Es?ref Adal? for his
valuable comments. Finally, we want to
thank our three anonymous reviewers for
insightful comments and suggestions
that helped us improve the final version of
the article.
References
Arun, Abhishek and Frank Keller. 2005.
Lexicalization in crosslinguistic
probabilistic parsing: The case of French.
In Proceedings of ACL?05, pages 302?313,
Ann Arbor, MI.
Attardi, Giuseppe. 2006. Experiments with a
multilanguage non-projective dependency
parser. In Proceedings of CONLL-X,
pages 166?170, New York, NY.
Bick, Eckhard. 2006. Lingpars, a linguistically
inspired, language-independent machine
learner for dependency treebanks. In
Proceedings of CONLL-X, pages 171?175,
New York, NY.
Bikel, Daniel M. 2004. A distributional
analysis of a lexicalized statistical parsing
model. In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing, pages 182?189, Barcelona.
Bikel, Daniel M. and David Chiang. 2000.
Two statistical parsing models applied to
the Chinese treebank. In Proceedings of the
2nd Chinese Language Processing Workshop,
pages 1?6, Hong Kong.
Black, Ezra, Frederick Jelinek, John D.
Lafferty, David M. Magerman, Robert L.
Mercer, and Salim Roukos. 1992. Towards
history-based grammars: Using richer
models for probabilistic parsing. In
Proceedings of the 5th DARPA Speech and
Natural Language Workshop, pages 31?37,
New York, NY.
386
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
Bozs?ahin, Cem. 2002. The combinatory
morphemic lexicon. Computational
Linguistics, 28(2):145?186.
Buchholz, Sabine and Erwin Marsi.
2006. CONLL-X shared task on
multilingual dependency parsing. In
Proceedings of CONLL-X, pages 149?164,
New York, NY.
C?ak?c?, Ruket and Jason Baldridge. 2006.
Projective and non-projective Turkish
parsing. In Proceedings of the 5th
International Treebanks and Linguistic
Theories Conference, pages 43?54, Prague.
Canisius, Sander, Toine Bogers, Antal
van den Bosch, Jeroen Geertzen, and Erik
Tjong Kim Sang. 2006. Dependency
parsing by inference over high-recall
dependency predictions. In Proceedings of
CONLL-X, pages 176?180, New York, NY.
Carreras, Xavier, Mihai Surdeanu, and Lluis
Marquez. 2006. Projective dependency
parsing with perceptron. In Proceedings of
CONLL-X, pages 181?185, New York, NY.
Chang, Chih-Chung and Chih-Jen Lin, 2001.
LIBSVM: A Library for Support Vector
Machines. Software available at
www.csie.ntu.edu.tw/?cjlin/libsvm.
Chang, Ming-Wei, Quang Do, and Dan Roth.
2006. A pipeline model for bottom-up
dependency parsing. In Proceedings of
CONLL-X, pages 186?190, New York, NY.
Cheng, Yuchang, Masayuki Asahara, and
Yuji Matsumoto. 2006. Multi-lingual
dependency parsing at NAIST. In
Proceedings of CONLL-X, pages 191?195,
New York, NY.
Chung, Hoojung and Hae-Chang Rim. 2004.
Unlexicalized dependency parser for
variable word order languages based on
local contextual pattern. In Proceedings of
the 5th International Conference on Intelligent
Text Processing and Computational
Linguistics, pages 109?120, Seoul.
Collins, Michael. 1996. A new statistical
parser based on bigram lexical
dependencies. In Proceedings of ACL?96,
pages 184?191, Santa Cruz, CA.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of ACL?97, pages 16?23,
Madrid.
Collins, Michael. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania,
Philadelphia.
Collins, Michael, Jan Hajic, Lance Ramshaw,
and Christoph Tillmann. 1999. A statistical
parser for Czech. In Proceedings of ACL?99,
pages 505?518, College Park, MD.
Corazza, Anna, Alberto Lavelli, Giorgio
Satta, and Roberto Zanoli. 2004. Analyzing
an Italian treebank with state-of-the-art
statistical parsers. In Proceedings of the 3rd
Workshop on Treebanks and Linguistic
Theories, pages 39?50, Tu?bingen.
Corston-Oliver, Simon and Anthony Aue.
2006. Dependency parsing with reference
to Slovene, Spanish and Swedish. In
Proceedings of CONLL-X, pages 196?200,
New York, NY.
Daelemans, Walter and Antal Vanden
Bosch. 2005.Memory-Based Language
Processing. Cambridge University Press,
Cambridge.
Dreyer, Markus, David A. Smith, and
Noah A. Smith. 2006. Vine parsing and
minimum risk reranking for speed and
precision. In Proceedings of CONLL-X,
pages 201?205, New York, NY.
Dubey, Amit and Frank Keller. 2003.
Probabilistic parsing for German using
sister-head dependencies. In Proceedings
of ACL?03, pages 96?103, Sapporo.
Eisner, Jason. 1996. Three new probabilistic
models for dependency parsing: An
exploration. In Proceedings of the 16th
International Conference on Computational
Linguistics, pages 340?345, Copenhagen.
Erguvanl?, Eser Emine. 1979. The Function
of Word Order in Turkish Grammar.
Ph.D. thesis, UCLA.
Eryig?it, Gu?ls?en. 2006. Tu?rkc?enin Bag?l?l?k
Ayr?s?t?rmas? (Dependency Parsing of Turkish).
Ph.D. thesis, Istanbul Technical University.
Eryig?it, Gu?ls?en, Es?ref Adal?, and Kemal
Oflazer. 2006. Tu?rkc?e cu?mlelerin kural
tabanl? bag?l?l?k analizi [Rule-based
dependency parsing of Turkish sentences].
In Proceedings of the 15th Turkish Symposium
on Artificial Intelligence and Neural
Networks, pages 17?24, Mug?la.
Eryig?it, Gu?ls?en, Joakim Nivre, and Kemal
Oflazer. 2006. The incremental use of
morphological information and
lexicalization in data-driven dependency
parsing. In Computer Processing of Oriental
Languages, Beyond the Orient: The Research
Challenges Ahead, pages 498?507,
Singapore.
Eryig?it, Gu?ls?en and Kemal Oflazer. 2006.
Statistical dependency parsing of Turkish.
In Proceedings of EACL?06, pages 89?96,
Trento.
Gildea, Daniel. 2001. Corpus variation and
parser performance. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing, pages 167?202,
Pittsburgh, PA.
387
Computational Linguistics Volume 34, Number 3
Hajic?, Jan, Eva Hajic?ova?, Petr Pajas, Jarmila
Panevova?, Petr Sgall, and Barbora Hladka?.
2001. Prague dependency treebank 1.0
(final production label). CDROM CAT:
LDC2001T10., ISBN 1-58563-212-0.
Hakkani-Tu?r, Dilek, Kemal Oflazer, and
Go?khan Tu?r. 2002. Statistical
morphological disambiguation for
agglutinative languages. Journal of
Computers and Humanities, 36(4):381?410.
Hoffman, Beryl. 1994. Generating context
appropriate word orders in Turkish. In
Proceedings of the Seventh International
Workshop on Natural Language Generation,
pages 117?126, Kennebunkport, ME.
Johansson, Richard and Pierre Nugues. 2006.
Investigating multilingual dependency
parsing. In Proceedings of CONLL-X,
pages 206?210, New York, NY.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing. In
Proceedings of ACL?03, pages 423?430,
Sapporo.
Kromann, Matthias T. 2003. The Danish
dependency treebank and the underlying
linguistic theory. In Proceedings of the 2nd
Workshop on Treebanks and Linguistic
Theories, pages 217?220, Va?xjo?.
Kudo, Taku and Yuji Matsumoto. 2002.
Japanese dependency analysis using
cascaded chunking. In Proceedings of the
Conference on Computational Natural
Language Learning, pages 63?69, Taipei.
Levy, Roger and Christopher Manning. 2003.
Is it harder to parse Chinese, or the
Chinese treebank? In Proceedings of ACL?03,
pages 439?446, Sapporo.
Liu, Ting, Jinshan Ma, Huijia Zhu, and
Sheng Li. 2006. Dependency parsing based
on dynamic local optimization. In
Proceedings of CONLL-X, pages 211?215,
New York, NY.
Magerman, David M. 1995. Statistical
decision-tree models for parsing. In
Proceedings of ACL?95, pages 276?283,
Cambridge, MA.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The
Penn treebank. Computational Linguistics,
19(2):313?330.
McDonald, Ryan, Kevin Lerman, and
Fernando Pereira. 2006. Multilingual
dependency analysis with a two-stage
discriminative parser. In Proceedings of
CONLL-X, pages 216?220, New York, NY.
Nivre, Joakim. 2003. An efficient algorithm
for projective dependency parsing. In
Proceedings of the 8th International Workshop
on Parsing Technologies, pages 149?160,
Nancy.
Nivre, Joakim. 2004. Incrementality in
deterministic dependency parsing. In
Proceedings of the Workshop on Incremental
Parsing: Bringing Engineering and Cognition
Together, pages 50?57, Barcelona.
Nivre, Joakim. 2006. Inductive Dependency
Parsing. Springer, Dordrecht.
Nivre, Joakim, Johan Hall, and Jens Nilsson.
2004. Memory-based dependency parsing.
In Proceedings of the Conference on
Computational Natural Language Learning,
pages 49?56, Boston, MA.
Nivre, Joakim, Johan Hall, Jens Nilsson,
Atanas Chanev, Gu?ls?en Eryig?it, Sandra
Ku?bler, Stetoslav Marinov, and Erwin
Marsi. 2007. Maltparser: A
language-independent system for
data-driven dependency parsing. Natural
Language Engineering Journal, 13(2):95?135.
Nivre, Joakim, Johan Hall, Jens Nilsson,
Gu?ls?en Eryig?it, and Stetoslav Marinov.
2006. Labeled pseudo-projective
dependency parsing with support vector
machines. In Proceedings of CONLL-X,
pages 221?225, New York, NY.
Nivre, Joakim and Jens Nilsson. 2005.
Pseudo-projective dependency parsing. In
Proceedings of ACL?05, pages 99?106, Ann
Arbor, MI.
Nivre, Joakim, Jens Nilsson, and Johan Hall.
2006. Talbanken05: A Swedish treebank
with phrase structure and dependency
annotation. In Proceedings of LREC,
pages 1392?1395, Genoa.
Oflazer, Kemal. 1994. Two-level description
of Turkish morphology. Literary and
Linguistic Computing, 9(2):137?148.
Oflazer, Kemal. 2003. Dependency parsing
with an extended finite-state approach.
Computational Linguistics, 29(4):515?544.
Oflazer, Kemal, Bilge Say, Dilek Z.
Hakkani-Tu?r, and Go?khan Tu?r. 2003.
Building a Turkish treebank. In A. Abeille?,
editor, Treebanks: Building and Using Parsed
Corpora. Kluwer, London, pages 261?277.
Ratnaparkhi, Adwait. 1997. A linear
observed time statistical parser based on
maximum entropy models. In Proceedings
of the Conference on Empirical Methods in
Natural Language Processing, pages 1?10,
Providence, RI.
Riedel, Sebastian, Ruket C?ak?c?, and
Ivan Meza-Ruiz. 2006. Multi-lingual
dependency parsing with incremental
integer linear programming. In
Proceedings of CONLL-X, pages 226?230,
New York, NY.
388
Eryig?it, Nivre, and Oflazer Dependency Parsing of Turkish
Sagae, Kenji and Alon Lavie. 2005. A
classifier-based parser with linear run-time
complexity. In Proceedings of the 9th
International Workshop on Parsing
Technologies, pages 125?132, Vancouver.
Schiehlen, Michael and Kristina Spranger.
2006. Language independent probabilistic
context-free parsing bolstered by machine
learning. In Proceedings of CONLL-X,
pages 231?235, New York, NY.
Sekine, Satoshi, Kiyotaka Uchimoto, and
Hitoshi Isahara. 2000. Backward beam
search algorithm for dependency analysis
of Japanese. In Proceedings of the 17th
International Conference on Computational
Linguistics, pages 754?760, Saarbru?cken.
Shimizu, Nobuyuki. 2006. Maximum
spanning tree algorithm for non-projective
labeled dependency parsing. In
Proceedings of CONLL-X, pages 236?240,
New York, NY.
Simov, Kiril, Gergana Popova, and Petya
Osenova. 2002. HPSG-based syntactic
treebank of Bulgarian (BulTreeBank). In
Andrew Wilson, Paul Rayson, and Tony
McEnery, editors, A Rainbow of Corpora:
Corpus Linguistics and the Languages of
the World. Lincom-Europa, Munich,
pages 135?142.
Vapnik, Vladimir N. 1995. The Nature of
Statistical Learning Theory. Springer, New
York, NY.
Wu, Yu-Chieh, Yue-Shi Lee, and Jie-Chi
Yang. 2006. The exploration of
deterministic and efficient dependency
parsing. In Proceedings of CONLL-X,
pages 241?245, New York, NY.
Yamada, Hiroyasu and Yuji Matsumoto.
2003. Statistical dependency analysis with
support vector machines. In Proceedings of
the 8th International Workshop on Parsing
Technologies, pages 195?206, Nancy.
Yu?ret, Deniz. 2006. Dependency parsing as a
classification problem. In Proceedings of
CONLL-X, pages 246?250, New York, NY.
Yu?ret, Deniz and Ferhan Tu?re. 2006.
Learning morphological disambiguation
rules for Turkish. In Proceedings of
HLT/NAACL?06, pages 328?334,
New York, NY.
389

This article has been cited by:
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 221?225, New York City, June 2006. c?2006 Association for Computational Linguistics
Labeled Pseudo-Projective Dependency Parsing
with Support Vector Machines
Joakim Nivre
Johan Hall
Jens Nilsson
School of Mathematics
and Systems Engineering
Va?xjo? University
35195 Va?xjo?, Sweden
{nivre,jha,jni}@msi.vxu.se
Gu?ls?en Eryig?it
Department of
Computer Engineering
Istanbul Technical University
34469 Istanbul, Turkey
gulsen@cs.itu.edu.tr
Svetoslav Marinov
School of Humanities
and Informatics
University of Sko?vde
Box 408
54128 Sko?vde, Sweden
svetoslav.marinov@his.se
Abstract
We use SVM classifiers to predict the next
action of a deterministic parser that builds
labeled projective dependency graphs in
an incremental fashion. Non-projective
dependencies are captured indirectly by
projectivizing the training data for the
classifiers and applying an inverse trans-
formation to the output of the parser. We
present evaluation results and an error
analysis focusing on Swedish and Turkish.
1 Introduction
The CoNLL-X shared task consists in parsing texts
in multiple languages using a single dependency
parser that has the capacity to learn from treebank
data. Our methodology for performing this task is
based on four essential components:
? A deterministic algorithm for building labeled
projective dependency graphs (Nivre, 2006).
? History-based feature models for predicting the
next parser action (Black et al, 1992).
? Support vector machines for mapping histories
to parser actions (Kudo and Matsumoto, 2002).
? Graph transformations for recovering non-
projective structures (Nivre and Nilsson, 2005).
All experiments have been performed using Malt-
Parser (Nivre et al, 2006), version 0.4, which is
made available together with the suite of programs
used for pre- and post-processing.1
1www.msi.vxu.se/users/nivre/research/MaltParser.html
2 Parsing Methodology
2.1 Parsing Algorithm
The parsing algorithm used for all languages is the
deterministic algorithm first proposed for unlabeled
dependency parsing by Nivre (2003) and extended
to labeled dependency parsing by Nivre et al (2004).
The algorithm builds a labeled dependency graph in
one left-to-right pass over the input, using a stack
to store partially processed tokens and adding arcs
using four elementary actions (where top is the token
on top of the stack and next is the next token):
? SHIFT: Push next onto the stack.
? REDUCE: Pop the stack.
? RIGHT-ARC(r): Add an arc labeled r from top
to next; push next onto the stack.
? LEFT-ARC(r): Add an arc labeled r from next
to top; pop the stack.
Although the parser only derives projective graphs,
the fact that graphs are labeled allows non-projective
dependencies to be captured using the pseudo-
projective approach of Nivre and Nilsson (2005) .
Another limitation of the parsing algorithm is that
it does not assign dependency labels to roots, i.e., to
tokens having HEAD=0. To overcome this problem,
we have implemented a variant of the algorithm that
starts by pushing an artificial root token with ID=0
onto the stack. Tokens having HEAD=0 can now
be attached to the artificial root in a RIGHT-ARC(r)
action, which means that they can be assigned any
label. Since this variant of the algorithm increases
the overall nondeterminism, it has only been used
for the data sets that include informative root labels
(Arabic, Czech, Portuguese, Slovene).
221
FO L C P FE D
S: top + + + + + +
S: top?1 +
I: next + + + + +
I: next+1 + +
I: next+2 +
I: next+3 +
G: head of top +
G: leftmost dep of top +
G: rightmost dep of top +
G: leftmost dep of next +
Table 1: Base model; S: stack, I: input, G: graph;
FO: FORM, L: LEMMA , C: CPOS, P: POS,
FE: FEATS, D: DEPREL
2.2 History-Based Feature Models
History-based parsing models rely on features of the
derivation history to predict the next parser action.
The features used in our system are all symbolic
and extracted from the following fields of the data
representation: FORM, LEMMA, CPOSTAG, POSTAG,
FEATS, and DEPREL. Features of the type DEPREL
have a special status in that they are extracted during
parsing from the partially built dependency graph
and may therefore contain errors, whereas all the
other features have gold standard values during both
training and parsing.2
Based on previous research, we defined a base
model to be used as a starting point for language-
specific feature selection. The features of this model
are shown in Table 1, where rows denote tokens in
a parser configuration (defined relative to the stack,
the remaining input, and the partially built depen-
dency graph), and where columns correspond to data
fields. The base model contains twenty features, but
note that the fields LEMMA, CPOS and FEATS are not
available for all languages.
2.3 Support Vector Machines
We use support vector machines3 to predict the next
parser action from a feature vector representing the
history. More specifically, we use LIBSVM (Chang
and Lin, 2001) with a quadratic kernel K(xi, xj) =
(?xTi xj +r)2 and the built-in one-versus-all strategy
for multi-class classification. Symbolic features are
2The fields PHEAD and PDEPREL have not been used at all,
since we rely on pseudo-projective parsing for the treatment of
non-projective structures.
3We also ran preliminary experiments with memory-based
learning but found that this gave consistently lower accuracy.
converted to numerical features using the standard
technique of binarization, and we split values of the
FEATS field into its atomic components.4
For some languages, we divide the training data
into smaller sets, based on some feature s (normally
the CPOS or POS of the next input token), which may
reduce training times without a significant loss in
accuracy (Yamada and Matsumoto, 2003). To avoid
too small training sets, we pool together categories
that have a frequency below a certain threshold t.
2.4 Pseudo-Projective Parsing
Pseudo-projective parsing was proposed by Nivre
and Nilsson (2005) as a way of dealing with
non-projective structures in a projective data-driven
parser. We projectivize training data by a minimal
transformation, lifting non-projective arcs one step
at a time, and extending the arc label of lifted arcs
using the encoding scheme called HEAD by Nivre
and Nilsson (2005), which means that a lifted arc is
assigned the label r?h, where r is the original label
and h is the label of the original head in the non-
projective dependency graph.
Non-projective dependencies can be recovered by
applying an inverse transformation to the output of
the parser, using a left-to-right, top-down, breadth-
first search, guided by the extended arc labels r?h
assigned by the parser. This technique has been used
without exception for all languages.
3 Experiments
Since the projective parsing algorithm and graph
transformation techniques are the same for all data
sets, our optimization efforts have been focused on
feature selection, using a combination of backward
and forward selection starting from the base model
described in section 2.2, and parameter optimization
for the SVM learner, using grid search for an optimal
combination of the kernel parameters ? and r, the
penalty parameter C and the termination criterion ?,
as well as the splitting feature s and the frequency
threshold t. Feature selection and parameter opti-
mization have to some extent been interleaved, but
the amount of work done varies between languages.
4Preliminary experiments showed a slight improvement for
most languages when splitting the FEATS values, as opposed to
taking every combination of atomic values as a distinct value.
222
Ara Bul Chi Cze Dan Dut Ger Jap Por Slo Spa Swe Tur Total
LAS 66.71 87.41 86.92 78.42 84.77 78.59 85.82 91.65 87.60 70.30 81.29 84.58 65.68 80.19
UAS 77.52 91.72 90.54 84.80 89.80 81.35 88.76 93.10 91.22 78.72 84.67 89.50 75.82 85.48
LAcc 80.34 90.44 89.01 85.40 89.16 83.69 91.03 94.34 91.54 80.54 90.06 87.39 78.49 86.75
Table 2: Evaluation on final test set; LAS = labeled attachment score, UAS = unlabeled attachment score,
LAcc = label accuracy score; total score excluding Bulgarian
The main optimization criterion has been labeled
attachment score on held-out data, using ten-fold
cross-validation for all data sets with 100k tokens
or less, and an 80-20 split into training and devtest
sets for larger datasets. The number of features in
the optimized models varies from 16 (Turkish) to 30
(Spanish), but the models use all fields available for
a given language, except that FORM is not used for
Turkish (only LEMMA). The SVM parameters fall
into the following ranges: ?: 0.12?0.20; r: 0.0?0.6;
C: 0.1?0.7; ?: 0.01?1.0. Data has been split on the
POS of the next input token for Czech (t = 200),
German (t = 1000), and Spanish (t = 1000), and
on the CPOS of the next input token for Bulgarian
(t = 1000), Slovene (t = 600), and Turkish (t = 100).
(For the remaining languages, the training data has
not been split at all.)5 A dry run at the end of the
development phase gave a labeled attachment score
of 80.46 over the twelve required languages.
Table 2 shows final test results for each language
and for the twelve required languages together. The
total score is only 0.27 percentage points below the
score from the dry run, which seems to indicate that
models have not been overfitted to the training data.
The labeled attachment score varies from 91.65 to
65.68 but is above average for all languages. We
have the best reported score for Japanese, Swedish
and Turkish, and the score for Arabic, Danish,
Dutch, Portuguese, Spanish, and overall does not
differ significantly from the best one. The unlabeled
score is less competitive, with only Turkish having
the highest reported score, which indirectly indicates
that the integration of labels into the parsing process
primarily benefits labeled accuracy.
4 Error Analysis
An overall error analysis is beyond the scope of this
paper, but we will offer a few general observations
5Detailed specifications of the feature models and learning
algorithm parameters can be found on the MaltParser web page.
before we turn to Swedish and Turkish, focusing on
recall and precision of root nodes, as a reflection of
global syntactic structure, and on attachment score
as a function of arc length. If we start by considering
languages with a labeled attachment score of 85% or
higher, they are characterized by high precision and
recall for root nodes, typically 95/90, and by a grace-
ful degradation of attachment score as arcs grow
longer, typically 95?90?85, for arcs of length 1, 2
and 3?6. Typical examples are Bulgarian (Simov
et al, 2005; Simov and Osenova, 2003), Chinese
(Chen et al, 2003), Danish (Kromann, 2003), and
Swedish (Nilsson et al, 2005). Japanese (Kawata
and Bartels, 2000), despite a very high accuracy, is
different in that attachment score drops from 98%
to 85%, as we go from length 1 to 2, which may
have something to do with the data consisting of
transcribed speech with very short utterances.
A second observation is that a high proportion of
non-projective structures leads to fragmentation in
the parser output, reflected in lower precision for
roots. This is noticeable for German (Brants et al,
2002) and Portuguese (Afonso et al, 2002), which
still have high overall accuracy thanks to very high
attachment scores, but much more conspicuous for
Czech (Bo?hmova? et al, 2003), Dutch (van der Beek
et al, 2002) and Slovene (Dz?eroski et al, 2006),
where root precision drops more drastically to about
69%, 71% and 41%, respectively, and root recall is
also affected negatively. On the other hand, all three
languages behave like high-accuracy languages with
respect to attachment score. A very similar pattern
is found for Spanish (Civit Torruella and Mart?? An-
ton??n, 2002), although this cannot be explained by
a high proportion of non-projective structures. One
possible explanation in this case may be the fact that
dependency graphs in the Spanish data are sparsely
labeled, which may cause problem for a parser that
relies on dependency labels as features.
The results for Arabic (Hajic? et al, 2004; Smrz?
et al, 2002) are characterized by low root accuracy
223
as well as a rapid degradation of attachment score
with arc length (from about 93% for length 1 to 67%
for length 2). By contrast, Turkish (Oflazer et al,
2003; Atalay et al, 2003) exhibits high root accu-
racy but consistently low attachment scores (about
88% for length 1 and 68% for length 2). It is note-
worthy that Arabic and Turkish, being ?typological
outliers?, show patterns that are different both from
each other and from most of the other languages.
4.1 Swedish
A more fine-grained analysis of the Swedish results
reveals a high accuracy for function words, which
is compatible with previous studies (Nivre, 2006).
Thus, the labeled F-score is 100% for infinitive
markers (IM) and subordinating conjunctions (UK),
and above 95% for determiners (DT). In addition,
subjects (SS) have a score above 90%. In all these
cases, the dependent has a configurationally defined
(but not fixed) position with respect to its head.
Arguments of the verb, such as objects (DO, IO)
and predicative complements (SP), have a slightly
lower accuracy (about 85% labeled F-score), which
is due to the fact that they ?compete? in the same
structural positions, whereas adverbials (labels that
end in A) have even lower scores (often below 70%).
The latter result must be related both to the relatively
fine-grained inventory of dependency labels for ad-
verbials and to attachment ambiguities that involve
prepositional phrases. The importance of this kind
of ambiguity is reflected also in the drastic differ-
ence in accuracy between noun pre-modifiers (AT)
(F > 97%) and noun post-modifiers (ET) (F ? 75%).
Finally, it is worth noting that coordination, which
is often problematic in parsing, has high accuracy.
The Swedish treebank annotation treats the second
conjunct as a dependent of the first conjunct and as
the head of the coordinator, which seems to facil-
itate parsing.6 The attachment of the second con-
junct to the first (CC) has a labeled F-score above
80%, while the attachment of the coordinator to the
second conjunct (++) has a score well above 90%.
4.2 Turkish
In Turkish, very essential syntactic information is
contained in the rich morphological structure, where
6The analysis is reminiscent of the treatment of coordination
in the Collins parser (Collins, 1999).
concatenated suffixes carry information that in other
languages may be expressed by separate words. The
Turkish treebank therefore divides word forms into
smaller units, called inflectional groups (IGs), and
the task of the parser is to construct dependencies
between IGs, not (primarily) between word forms
(Eryig?it and Oflazer, 2006). It is then important
to remember that an unlabeled attachment score
of 75.8% corresponds to a word-to-word score of
82.7%, which puts Turkish on a par with languages
like Czech, Dutch and Spanish. Moreover, when
we break down the results according to whether the
head of a dependency is part of a multiple-IG word
or a complete (single-IG) word, we observe a highly
significant difference in accuracy, with only 53.2%
unlabeled attachment score for multiple-IG heads
versus 83.7% for single-IG heads. It is hard to say
at this stage whether this means that our methods
are ill-suited for IG-based parsing, or whether it is
mainly a case of sparse data for multiple-IG words.
When we break down the results by dependency
type, we can distinguish three main groups. The first
consists of determiners and particles, which have
an unlabeled attachment score over 80% and which
are found within a distance of 1?1.4 IGs from their
head.7 The second group mainly contains subjects,
objects and different kinds of adjuncts, with a score
in the range 60?80% and a distance of 1.8?5.2 IGs to
their head. In this group, information about case and
possessive features of nominals is important, which
is found in the FEATS field in the data representation.
We believe that one important explanation for our
relatively good results for Turkish is that we break
down the FEATS information into its atomic com-
ponents, independently of POS and CPOS tags, and
let the classifier decide which one to use in a given
situation. The third group contains distant depen-
dencies, such as sentence modifiers, vocatives and
appositions, which have a much lower accuracy.
5 Conclusion
The evaluation shows that labeled pseudo-projective
dependency parsing, using a deterministic parsing
algorithm and SVM classifiers, gives competitive
parsing accuracy for all languages involved in the
7Given that the average IG count of a word is 1.26 in the
treebank, this means that they are normally adjacent to the head
word.
224
shared task, although the level of accuracy varies
considerably between languages. To analyze in
depth the factors determining this variation, and to
improve our parsing methods accordingly to meet
the challenges posed by the linguistic diversity, will
be an important research goal for years to come.
Acknowledgments
We are grateful for the support from T ?UB?ITAK
(The Scientific and Technical Research Council of
Turkey) and the Swedish Research Council. We also
want to thank Atanas Chanev for assistance with
Slovene, the organizers of the shared task for all
their hard work, and the creators of the treebanks
for making the data available.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora, volume 20 of Text, Speech and Language
Technology. Kluwer Academic Publishers, Dordrecht.
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. ?Floresta
sinta?(c)tica?: a treebank for Portuguese. In Proc. of LREC-
2002, pages 1698?1703.
N. B. Atalay, K. Oflazer, and B. Say. 2003. The annotation
process in the Turkish treebank. In Proc. of LINC-2003.
E. Black, F. Jelinek, J. D. Lafferty, D. M. Magerman, R. L. Mer-
cer, and S. Roukos. 1992. Towards history-based grammars:
Using richer models for probabilistic parsing. In Proc. of the
5th DARPA Speech and Natural Language Workshop, pages
31?37.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003. The
PDT: a 3-level annotation scenario. In Abeille? (Abeille?,
2003), chapter 7.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith. 2002.
The TIGER treebank. In Proc. of TLT-2002.
C.-C. Chang and C.-J. Lin, 2001. LIBSVM: A Library
for Support Vector Machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang, and
Z. Gao. 2003. Sinica treebank: Design criteria, representa-
tional issues and implementation. In Abeille? (Abeille?, 2003),
chapter 13, pages 231?248.
M. Civit Torruella and Ma A. Mart?? Anton??n. 2002. Design
principles for a Spanish treebank. In Proc. of TLT-2002.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
S. Dz?eroski, T. Erjavec, N. Ledinek, P. Pajas, Z. ?Zabokrtsky, and
A. ?Zele. 2006. Towards a Slovene dependency treebank. In
Proc. of LREC-2006.
G. Eryig?it and K. Oflazer. 2006. Statistical dependency parsing
of Turkish. In Proc. of EACL-2006.
J. Hajic?, O. Smrz?, P. Zema?nek, J. ?Snaidauf, and E. Bes?ka. 2004.
Prague Arabic dependency treebank: Development in data
and tools. In Proc. of NEMLAR-2004, pages 110?117.
Y. Kawata and J. Bartels. 2000. Stylebook for the Japanese
treebank in VERBMOBIL. Verbmobil-Report 240, Seminar
fu?r Sprachwissenschaft, Universita?t Tu?bingen.
M. T. Kromann. 2003. The Danish dependency treebank and
the underlying linguistic theory. In Proc. of TLT-2003.
T. Kudo and Y. Matsumoto. 2002. Japanese dependency anal-
ysis using cascaded chunking. In Proc. of CoNLL-2002,
pages 63?69.
J. Nilsson, J. Hall, and J. Nivre. 2005. MAMBA meets TIGER:
Reconstructing a Swedish treebank from antiquity. In Proc.
of the NODALIDA Special Session on Treebanks.
J. Nivre and J. Nilsson. 2005. Pseudo-projective dependency
parsing. In Proc. of ACL-2005, pages 99?106.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based depen-
dency parsing. In Proc. CoNLL-2004, pages 49?56.
J. Nivre, J. Hall, and J. Nilsson. 2006. MaltParser: A data-
driven parser-generator for dependency parsing. In Proc. of
LREC-2006.
J. Nivre. 2003. An efficient algorithm for projective depen-
dency parsing. In Proc. of IWPT-2003, pages 149?160.
J. Nivre. 2006. Inductive Dependency Parsing. Springer.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r. 2003.
Building a Turkish treebank. In Abeille? (Abeille?, 2003),
chapter 15.
K. Simov and P. Osenova. 2003. Practical annotation scheme
for an HPSG treebank of Bulgarian. In Proc. of LINC-2003,
pages 17?24.
K. Simov, P. Osenova, A. Simov, and M. Kouylekov. 2005.
Design and implementation of the Bulgarian HPSG-based
treebank. In Journal of Research on Language and Com-
putation ? Special Issue, pages 495?522. Kluwer Academic
Publishers.
O. Smrz?, J. ?Snaidauf, and P. Zema?nek. 2002. Prague depen-
dency treebank for Arabic: Multi-level annotation of Arabic
corpus. In Proc. of the Intern. Symposium on Processing of
Arabic, pages 147?155.
L. van der Beek, G. Bouma, R. Malouf, and G. van Noord.
2002. The Alpino dependency treebank. In Computational
Linguistics in the Netherlands (CLIN).
H. Yamada and Y. Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In Proc. of IWPT-
2003, pages 195?206.
225
Proceedings of the Linguistic Annotation Workshop, pages 117?120,
Prague, June 2007. c?2007 Association for Computational Linguistics
ITU Treebank Annotation Tool
Gu?ls?en Eryig?it
Department of Computer Engineering
Istanbul Technical University
Istanbul, 34469, Turkey
gulsen.cebiroglu@itu.edu.tr
Abstract
In this paper, we present a treebank anno-
tation tool developed for processing Turk-
ish sentences. The tool consists of three
different annotation stages; morphological
analysis, morphological disambiguation and
syntax analysis. Each of these stages are
integrated with existing analyzers in order
to guide human annotators. Our semi-
automatic treebank annotation tool is cur-
rently used both for creating new data sets
and correcting the existing Turkish treebank.
1 Introduction
Annotated corpora is essential for most of the nat-
ural language processing tasks. Developing new
annotated corpora becomes crucial especially for
lesser studied languages where we encounter many
difficulties for finding such data. Turkish is one
of the languages which still suffer from scarcity
of annotated resources. The most reliable data set
for Turkish is the Metu-Sabanc? Turkish Treebank
(Oflazer et al, 2003) consisting of 5635 sentences
annotated with dependency structures. Unfortu-
nately, the data size of this treebank remained un-
changed during recent years. There exist also some
other small data sets manually pos-tagged by differ-
ent research groups.
In this study, we introduce our treebank annota-
tion tool developed in order to improve the size of
the existing data sets for Turkish (particularly the
treebank). Our main motivation for developing a
new tool is the inability of the existing tools (e.g.
Atalay et al (2003) and DepAnn (Kakkonen, 2006)
which seems to be the most suitable tools for our
task) in either reflecting the peculiar morphologi-
cal and dependency structure of Turkish or provid-
ing suitable automatic analyses for guidance. We
also aim to speed up the annotation process by using
graphical user-friendly interfaces and transforming
the annotation process from a manual (starting from
scratch) procedure into a controlling and correcting
procedure. In the rest of this paper, we first intro-
duce the framework of the tool and then the details
of its different annotation stages. We then close with
conclusions and future work.
2 Framework
<automatic>
Morphological
Analysis
<manual>
Morphological
Disambiguation
<manual>
Dependency
Analysis
Plugin 1:
Morphological
Analyzer
Plugin 2:
POS tagger
Plugin 3:
Parser
input
output
Stage 1
Stage 2
Stage 3
Figure 1: Data Flow
ITU treebank annotation tool takes raw sentences
as input and produces results in both the Turk-
ish treebank original XML format (Atalay et al,
2003) and Conll treebank data format (Buchholz and
117
Figure 2: Morphological Analysis and Disambiguation Screen
Marsi, 2006) which is now recognized by many of
the state of the art dependency parsers.
The tool consists of three levels of annotation
and can be used to produce results for each of
them; these are morphological analysis, morpho-
logical disambiguation and syntax analysis stages.
Each of these stages uses plugins in order to guide
the human annotators (referred as annotators in the
remaining part). Figure 1 gives the data flow be-
tween the annotation stages and the plugins which
will be explained in detail in the following sections.
3 Morphological Analysis
The most important characteristic of Turkish which
distinguishes it from most of the well-studied lan-
guages is its very rich morphological structure.
Turkish which is an agglutinative language has a
very productive derivational and inflectional mor-
phology. This rich structure of the language has
been represented in the literature (Oflazer et al,
2003; Hakkani-Tu?r et al, 2002; Eryig?it and Oflazer,
2006) by splitting the words into inflectional groups
(IGs) which are separated from each other by deriva-
tional boundaries. Each IG is then annotated with its
own part-of-speech and inflectional features.
We are using the morphological analyzer of
Oflazer (1994) which provides all the possible mor-
phological analyses together with the IG structure.
The output provided by the morphological analyzer
for each word in the example sentence ?S?imdi eski
odanday?m.? (I?m now in your old room.) can be
seen from Figure 2 (the listed items under each
word with radio buttons in front). We can see from
the figure that the derived word ?odanday?m? (I?m
in your room) is composed of two IGs:
(1,?oda+Noun+A3sg+P2sg+Loc?)
? ?? ?
IG1
(2,?Verb+Zero+Pres+A1sg?)
? ?? ?
IG2
The first IG is the noun ?oda? (room) which
takes the meaning of ?in your room? after taking
the 3rd singular number-person agreement (+A3sg) ,
2nd person possessive agreement (+P2sg) and loca-
tive case (+Loc) inflectional features. The sec-
ond IG is the derived verb ?being in your room?
in present tense (+Pres), with 1st singular number-
person agreement (+A1sg) inflectional features1.
The morphological analysis stage is totally auto-
matic except that the user can enter other analyses
to the text boxes under each word if the correct one
is not within the above listed items or the analyzer
couldn?t suggest any analysis. This latter case gen-
erally occurs for numerical values (e.g., numbers,
1+Zero means no additional suffix is used for the derivation.
118
dates) and unknown words. For numerical values,
we use a preprocessor to produce the analysis, but
for unknown words, the annotators are asked to en-
ter the appropriate analysis.
4 Morphological Disambiguation
The second stage is the morphological disambigua-
tion where the annotator is asked to choose one of
the possible analyses for each word. The annota-
tor may consult to an automatic analyzer by clicking
the checkbox at the top of the screen in Figure 2.
In this case we activate the part-of-speech tagger of
Yu?ret and Tu?re (2006) which uses some rules auto-
matically derived from a training corpus. The results
of this tagger is reflected to the screen by selecting
automatically the appropriate radio button for each
word. After finishing the disambiguation, the anno-
tator saves the results in XML format (shown at the
bottom panel of Figure 2) and proceeds trough the
syntax analysis.
5 Syntax Analysis
The syntactic annotation scheme used in the Turk-
ish treebank is the dependency grammar represen-
tation. The aim of the dependency analysis is to
find the binary relationships between dependent and
head units. The dependency structure of Turkish
has been mentioned in many studies (Oflazer et al,
2003; Oflazer, 2003; Eryig?it et al, 2006) and it is
argued that for Turkish, it is not just enough to de-
termine the relationships between words and one
should also determine the relationships between in-
flectional groups. Figure 3 gives an example of this
structure2. In this screen, the annotator first selects
a dependent unit by selecting the check box under it
and then a head unit and the appropriate dependency
relation from the combo box appearing under the
constructed dependency. In this figure, we see that
the adjective ?eski? (old) is connected to the first IG
of the word ?odanday?m? since it is the word ?oda?
(room) which is modified by the adjective, not the
derived verb form ?odanday?m? (I?m in your room).
On the other hand, the adverb ?s?imdi? (now) is con-
nected to the second IG of this word and modifies the
verb ?being in the room?. The graphical interface is
designed so that the annotator can easily determine
the correct head word and its correct IG.
2The arrows in the figure indicates the dependencies ema-
nating from the dependent unit towards the head unit.
In each step of the syntactic annotation, the par-
tially built dependency tree is shown to the anno-
tators in order to reduce the number of mistakes
caused by the inattentiveness of the annotators (such
as the errors encountered in the original Turkish
treebank; cycled dependencies, erroneous crossing
dependencies, unconnected items, dependencies to
nonexistent items). Extra cautions are taken with
similar reasons in order to force the annotators to
only make valid annotations:
? Only the check boxes under final IGs of the
words become active when the annotator is
about to select a dependent since the dependen-
cies can only emanate from the last IGs of the
dependents.
? The dependents may only be connected to the
IGs of other words, thus the check boxes of the
IGs within the dependent word become passive
when selecting a head unit.
Similar to the morphological disambiguation
stage, the annotator may want to consult to an auto-
matic analyzer. We use the data-driven dependency
parser of Nivre et al (2006) as an external parsing
guide which is shown to give the highest accuracy
for Turkish and for many other languages. The out-
put of the parser (pre-trained on the Turkish tree-
bank) is reflected to the screen by automatically con-
structing the dependency tree. The annotator may
then change the dependencies which he/she finds in-
correct.
6 Conclusions and Future Work
ITU treebank annotation tool is a semi-automatic
annotation tool tailored for the particular morpho-
logical structure of Turkish where we need to an-
notate units smaller than words. It has three an-
notation levels and uses pluggable analyzers in or-
der to automate these levels. These are a rule-based
morphological analyzer, and machine learning based
part-of-speech tagger and dependency parser. The
tool which aims to provide a user-friendly platform
for the human annotators, also tries to minimize the
number of errors due to the complexity of the anno-
tation process of Turkish. The tool is designed and
used only for Turkish in its current state, however
it can be used for other languages with similar mor-
phological structure (particularly other Turkic lan-
119
Figure 3: Dependency Analysis Screen
guages) by replacing the external analyzers. By us-
ing this tool, we observed significant acceleration
both in correcting the existing treebank and devel-
oping new data sets. However education of new hu-
man annotators still remains as a difficult point and
requires a lot of time. Hence in the future, we aim to
develop online education tools which teach the an-
notators and tests their performance. We also aim to
carry the platform to the web and supply an environ-
ment which can be reached from different places by
volunteer researchers and collect the data in a single
place.
Acknowledgment
The author wants to thank Kemal Oflazer for his
valuable comments, Engin Tas?k?n and Cihat Eryig?it
for their help during the development of the tool.
References
Nart B. Atalay, Kemal Oflazer, and Bilge Say. 2003. The
annotation process in the Turkish treebank. In Proc. of
the 4th International Workshop on Linguistically In-
terpreteted Corpora, Budapest.
Sabine Buchholz and Erwin Marsi. 2006. Conll-X
shared task on multilingual dependency parsing. In
Proc. of the 10th CoNLL, pages 149?164, New York,
NY.
Gu?ls?en Eryig?it and Kemal Oflazer. 2006. Statistical de-
pendency parsing of Turkish. In Proc. of the EACL,
pages 89?96, Trento.
Gu?ls?en Eryig?it, Joakim Nivre, and Kemal Oflazer. 2006.
The incremental use of morphological information and
lexicalization in data-driven dependency parsing. In
Proc. of the ICCPOL, pages 498?507, Singapore.
Dilek Hakkani-Tu?r, Kemal Oflazer, and Go?khan Tu?r.
2002. Statistical morphological disambiguation for
agglutinative languages. Journal of Computers and
Humanities, 36(4):381?410.
Tuomo Kakkonen. 2006. Depann - an annotation tool
for dependency treebanks. In Proc. of the 11th ESSLLI
Student Session, pages 214?225, Malaga.
Joakim Nivre, Johan Hall, Jens Nilsson, Gu?ls?en Eryig?it,
and Stetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector ma-
chines. In Proc. of the CoNLL-X, pages 221?225, New
York, NY.
Kemal Oflazer, Bilge Say, Dilek Z. Hakkani-Tu?r, and
Go?khan Tu?r. 2003. Building a Turkish treebank.
In A. Abeille?, editor, Treebanks: Building and Us-
ing Parsed Corpora, pages 261?277. Kluwer, Dor-
drecht/Boston/London.
Kemal Oflazer. 1994. Two-level description of Turk-
ish morphology. Literary and Linguistic Computing,
9(2):137?148.
Kemal Oflazer. 2003. Dependency parsing with an ex-
tended finite-state approach. Computational Linguis-
tics, 29(4):515?544.
Deniz Yu?ret and Ferhan Tu?re. 2006. Learning morpho-
logical disambiguation rules for Turkish. In Proc. of
the HLT-NAACL, pages 328?334, New York, NY.
120
Erratum
In Section 5 of the article ?Dependency Parsing of Turkish? by Gu?ls?en Eryig?it, Joakim
Nivre, and Kemal Oflazer (September 2008, Vol. 34, No. 3: 357?389), some abbreviations
were misinterpreted during the copyediting process.
The third sentence of Section 5.2 should be as follows: ?We use an unlexicalized
feature model where the parser uses only the minor part-of-speech category (as POS)
and dependency type of tokens (as DEP) and compare the results with the probabilistic
parser.?
The first sentence of the second paragraph of Section 5.2.1 should start as follows:
?We take the minor part-of-speech category. . . .?
The ?POS? abbreviation used on page 20 should be read as ?minor part-of-
speech,? and the ?POS? abbreviations on pages 21, 26, 27, and 28 should be read as
?part-of-speech.?
? 2008 Association for Computational Linguistics
This article has been cited by:
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 131?134,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
TURKSENT: A Sentiment Annotation Tool for Social Media
Gu?ls?en Eryig?it
Dep. of Computer Eng.
Istanbul Technical University
gulsen.cebiroglu@itu.edu.tr
Fatih Samet C?etin and Meltem Yan?k
Dep. of Information Technology
Turkcell Global Bilgi
fatih.cetin@global-bilgi.com.tr
meltem.yanik@global-bilgi.com.tr
Tanel Temel
Dep. of Information Technology
Turkcell Global Bilgi
tanel.temel@global-bilgi.com.tr
I?lyas C?ic?ekli
Dep. of Computer Eng.
Hacettepe University
ilyas@cs.hacettepe.edu.tr
Abstract
In this paper, we present an annotation tool
developed specifically for manual senti-
ment analysis of social media posts. The
tool provides facilities for general and tar-
get based opinion marking on different
type of posts (i.e. comparative, ironic,
conditional) with a web based UI which
supports synchronous annotation. It is
also designed as a SaaS (Software as a
Service). The tool?s outstanding features
are easy and fast annotation interface, de-
tailed sentiment levels, multi-client sup-
port, easy to manage administrative mod-
ules and linguistic annotation capabilities.
1 Introduction
Today, monitoring social media is a vital need for
companies and it has a high commercial value. So
almost all companies have social media accounts
and departments for following the social media
about their business sectors. In recent decade,
the studies on sentiment analysis has gained high
popularity and several academic (Pang and Lee,
2007; Liu, 2012) and commercial (Radian6, 2013;
Lithium, 2013) projects emerged in this field. Al-
though there are many works (Bosco et al, 2013;
Wiebe et al, 2005) on creating sentiment corpora,
up to our knowledge there are no publicly avail-
able and professional sentiment annotation tools.
A huge contact center communicates with the
customers for different trade marks on behalf
of them and provides detailed CRM1, impact
1CRM: Customer Relationship Management
and competitor analysis reports. With this pur-
pose, they employ thousands of customer rep-
resentatives among which an increasing percent-
age should deal with social media monitoring, the
new channel of communication. In such an envi-
ronment, the monitoring should be done via pro-
fessional and synchronous UIs (user interfaces)
where the performance of each human agent has
high importance. Most of the current commercial
monitoring tools leaks the following features:
- a detailed sentiment analysis interface for
feature based and comparative opinion dec-
larations,
- an effective and synchronous annotation in-
terface,
- on-demand data loading,
- linguistic annotation modules,
- detailed data analyses for corpus creation (to
be used in supervised machine learning).
The aim of our work is to fulfill all of the above
listed requirements and provide a platform for ef-
fective annotation of social media data. The tool
has the following sentiment and linguistic annota-
tion layers:
- general and target based sentiment
- text normalization
- named entity
- morphology
- syntax
The sentiment annotation module of TURK-
SENT may operate multilingually whereas the lin-
guistic annotation module is initially configured
131
specific to Turkish following the work in ITU
Treebank Annotation Tool (Eryig?it, 2007). It is
also possible to adapt this part to other languages
by plugging relevant linguistic adapters (for semi-
automatic annotation).
TURKSENT will be freely available for aca-
demic projects as a SaaS.
Figure 1: Application Flow
2 Architecture
Figure 1 gives an idea about the flow of our appli-
cation. In our system, the web data is monitored
continuously. It is first of all filtered according
to the target sector by the ?sector filter? and it is
then stored in the relevant database domains. In
our system, each domain represents a workspace
which consists of the related sector data (collected
via web or uploaded manually to the system), an
administrator and a bunch of human annotators.
2.1 Sentiment Annotation
Our choice of SaaS design has the following goals:
- Platform independence (No special machine
or no special operating system)
- Accessibility (Accessible from anywhere
anytime by multiple users)
- No installation effort (Browser based appli-
cation)
- No need to deploy updates to clients
Figure 2 gives a sample sentiment annota-
tion screen-shot on an example Tweet (?Samsung
Galaxy S4?s hardware features are amazing but
software is not stable as Iphone?). The upper
half of the screen (up to the table) show the gen-
eral sentiment part which is tagged as both2 (the
ambivalent smiley). General sentiment tagging
means identifying the sentimental class regardless
of a target. In other words, extracting dominant
sentimental class of an instance. In this stage the
annotator is also expected to select an appropriate
comment category and sentence type.
The lower half is for target based sentiment an-
notation. These deep sentiments are represented
as tuples consisting of the brand, product/service,
feature and sentiment tags. For example, the first
tuple in the sample Tweet will be composed as
the following: <Samsung, Galaxy S4, hardware,
positive>which means the hardware feature of the
Samsung Brand?s product Galaxy S4 had a posi-
tive impact on the Tweet?s author.
2.2 Linguistic Annotation
Recent researches on sentiment analysis show that
it is not possible to really understand the sentiment
of a sentence without any natural language pro-
cessing (NLP). And the addition of NLP features
to these systems increases the success ratios of
the automatic analyzers dramatically. In order to
be able to prepare a sentiment corpus, being able
to annotate the focus data within the same plat-
form is an important issue. Furthermore, the web
data has severe differences when compared to for-
mal natural language text and it needs additional
preprocessing before linguistic phases. With this
need, we added a linguistic annotation interface to
our application which is basically a reimplementa-
tion and adaptation of a previous academic study
(Eryig?it, 2007) according to our needs.
In this layer, the linguistic expert annotator is
asked to first normalize the instances (i.e. mis-
spellings, exaggerations, web jargon), and then de-
termine the entities (ex: ?Galaxy S4?), select the
appropriate postag categories for words and anno-
tate the syntactic parts of a post. It is also possible
to operate this layer semi-automatically by using
the pretrained linguistic tools and outputting their
2Other options are: positive, negative and neutral(no sen-
timental expression at all).
132
Figure 2: Sentiment annotation
results to the human experts and taking their cor-
rections. This speed-up procedure is only avail-
able for Turkish now, but the tool is developed as
a pluggable architecture to support further studies
on other languages. Figure 3 shows some sample
screenshots for the linguistic layer.
2.3 Administrative Operations
TURKSENT has a simple and easy-to-use admin
interface. A user who has administration rights has
the ability to perform the actions listed below:
- Creating a workspace (with a focus data and
annotator group)
- Determining the data subsets for linguistic
annotation
- Controlling/Changing the ongoing annota-
tions
- Defining configurable items (sentence types,
comment categories, product/service list, fea-
ture list, brand list)
- Defining linguistic tags (pos tags, named en-
tity types, dependency types)
3 Usability
The usability is seriously taken into account dur-
ing the design and development of our application.
The spent time per post is a high concern within
big operations. End-user software tests are accom-
plished and observed for each step. On the final
UI design, every action can be done via keyboard
without the need of mouse usage. Almost every
text areas has strong auto-completion feature in it-
self. While an annotator is working on an issue,
it is possible to deliver any idea-suggestion to the
administrator within seconds. And if an annotator
need to browse his/her previous annotations, can
easily search and find within them.
4 Conclusion
In this work, we presented a professional sen-
timent annotation tool TURKSENT which sup-
ports synchronous annotations on a web-based
platform. The study is a part of an automatic sen-
timent analysis research project. That is why, it
both aims to manually annotate the sentiments of
web posts and to create a sentiment corpus also
annotated linguistically (to be used in automatic
133
Figure 3: Linguistic Annotations
sentiment analysis). With this purpose it consists
different layers of annotation specific to web data.
It serves as a SaaS and designed as dynamic as
possible for future use on different sectors and lan-
guages.
Acknowledgment
This work is accomplished as a part of a
TUBITAK-TEYDEB (The Scientific and Tech-
nological Research Council of Turkey - Tech-
nology and Innovation Funding Programs Direc-
torate) project (grant number: 3120605) in ?Turk-
cell Global Bilgi? Information Technology De-
partment. The authors want to thank Derya
Do?nmez and Mehmet Osmanog?lu for design and
implementation.
References
Cristina Bosco, Viviana Patti, and Andrea Bolioli.
2013. Developing corpora for sentiment analysis
and opinion mining: the case of irony and senti-tut.
Intelligent Systems.
Gu?ls?en Eryig?it. 2007. ITU Treebank Annotation Tool.
In Proceedings of the ACL workshop on Linguistic
Annotation (LAW 2007), Prague, 24-30 June.
Lithium. 2013. Lithium. http://www.lithium.
com/.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies. Morgan and Claypool Publishers.
Bo Pang and Lillian Lee. 2007. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1?135.
Radian6. 2013. Radian 6. http://www.
salesforcemarketingcloud.com/
products/social-media-listening/.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2-3):165?210.
134
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 129?134,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Representation of Morphosyntactic Units and Coordination Structures
in the Turkish Dependency Treebank
Umut Sulubacak Gu?ls?en Eryig?it
Department of Computer Engineering
Istanbul Technical University
Istanbul, 34469, Turkey
{sulubacak, gulsen.cebiroglu}@itu.edu.tr
Abstract
This paper presents our preliminary conclu-
sions as part of an ongoing effort to construct a
new dependency representation framework for
Turkish. We aim for this new framework to ac-
commodate the highly agglutinative morphol-
ogy of Turkish as well as to allow the annota-
tion of unedited web data, and shape our deci-
sions around these considerations. In this pa-
per, we firstly describe a novel syntactic repre-
sentation for morphosyntactic sub-word units
(namely inflectional groups (IGs) in Turkish)
which allows inter-IG relations to be discerned
with perfect accuracy without having to hide
lexical information. Secondly, we investigate
alternative annotation schemes for coordina-
tion structures and present a better scheme
(nearly 11% increase in recall scores) than the
one in Turkish Treebank (Oflazer et al, 2003)
for both parsing accuracies and compatibility
for colloquial language.
1 Introduction
In recent years, dependency parsing has globally
seen great deal of attention, and has constituted
the underlying framework for the syntactic pars-
ing of many multilingual studies. Even though
constituency parsing and grammars are still the
preferred formalism for some well-researched lan-
guages, others may have certain traits that put con-
stituency parsing in an unfavorable position against
dependency parsing, such as flexible constituent or-
dering, which is typical of several prominent lan-
guages including Turkish. Although Turkish is de-
cidedly more workable over the dependency for-
malism, it has invariably fallen short of usual pars-
ing accuracies compared to other languages, as seen
clearly in some recent works such as (McDonald and
Nivre, 2011).
There are more parameters to parsing than the for-
malism alone, among which the correctness of the
corpora used in learning procedures and the annota-
tion schemes of syntactic relations are held in con-
sideration as part of this work. Between the two,
the emphasis is on the annotation scheme, which
is proven to significantly affect the parsing per-
formance (Bosco et al, 2010; Boyd and Meurers,
2008). Our motivation for this research is that these
factors must also contribute to some extent to the
performance deficiency in parsing Turkish, besides
the inherent difficulty of parsing the language. Our
aim is to investigate these points and suggest im-
provements where applicable.
2 Parsing Framework and Data Set
As our parsing framework, we use MaltParser (Nivre
et al, 2007) which is a data-driven dependency
parser with an underlying SVM learner based on
LIBSVM (Chang and Lin, 2001). MaltParser is
widely used and has shown high performances
across various languages (Nivre et al, 2006a). We
run MaltParser with Nivre?s Arc-Standard parsing
algorithm (Nivre, 2003) and use the same optimized
parameters as in (Eryig?it et al, 2008). We also use
the learning features from the last cited work as our
baseline feature set and an updated version from
(Eryig?it et al, 2011) of the same data set (Oflazer,
2003). The only difference from the configuration
of (Eryig?it et al, 2011) is that our baseline parser
does not exclude non-projective sentences from the
corpus for training, which explains the baseline ac-
129
Figure 1: The original and the novel IG representations for the word sag?lamlas?t?r?lmas?n?n, which respectively comes
to mean strong, to become strong, to strengthen, to be strengthened and of the strengthening of after each derivation.
The new morphological tags introduced after each derivation pertain to the relevant IG, and common morphological
features for the IGs of a single word such as the agreement are given under the final IG. Model I is the original
representation, while Model II is the new representation we propose.
curacy differences (e.g. 67.4% against our 65.0% in
labelled attachment score).
3 Proposed Annotation Schemes
3.1 IGs
Within the context of data-driven parsing, the most
apparent problem of languages with productive
derivational morphology is that words can poten-
tially yield a very large morphological tag set, which
causes severe sparsity in the morphological features
of words. To alleviate this problem, words are
split into morphosyntactic parts called inflectional
groups (IGs), taking intermediate derivational af-
fixes as boundaries. It is a known fact that analyz-
ing sentences as being composed of IGs rather than
surface word forms yields better results in major
NLP problems such as morphological disambigua-
tion (Hakkani-Tu?r et al, 2002) and syntactic parsing
(Eryig?it et al, 2008).
Within the domain of dependency parsing, IGs
as syntactic tokens are not as free as independent
words, since the IGs of each word must be con-
nected to each other with an exclusive dependency
relation named DERIV. However, other tokens are
free to be connected to an arbitrary IG of a word,
with the added benefit of more compact morpholog-
ical feature sets to help make the distinction.
Other languages with productive derivation, such
as Uralic or Ugric languages, or those orthographi-
cally differing from the well-studied European lan-
guages, such as Semitic languages, can also benefit
from using non-word-based morphosyntactic pars-
ing tokens, as evidenced for instance by the recent
considerations of splitting up tokens based on mor-
phemes for Hebrew (Tsarfaty and Goldberg, 2008).
3.1.1 Current IG Representation
Since MaltParser accepts input in the standard
data format of the CoNLL-X Shared Task (Buch-
holz and Marsi, 2006), the ways in which IGs can
be represented for the parser are limited. The stan-
dard method for annotating IGs using the CoNLL-X
data fields, as described in (Eryig?it et al, 2008), in-
volves marking up the FORM and LEMMA fields with
underscores rather than with lexical data as shown in
Figure 1. At first, this method is convenient, as cur-
rent feature vectors readily take lexical information
into account, and as such, a linear transition-based
parser would easily learn to connect adjacent words
as IGs of the same word as long as the head word
has an underscore for a stem. However, an obvious
drawback is that the actual lexical information gets
lost in favor of marking IGs, preventing the potential
usage of that information in deciding on inter-word
dependencies.
3.1.2 Proposed IG Representation
As an improvement over the original IG repre-
sentation described in Section 3.1.1, we propose a
slightly different annotation scheme which does not
lock out the lexical data columns, by making use of
130
a new column named IG. This new column takes a
boolean value that is true for non-final IGs of multi-
IG words much like the original FORM column, ef-
fectively marking the dependents that must be con-
nected to the next token in line with the dependency
relation DERIV. Once this representation gets inte-
grated, lexical information may be assigned to the
FORM and LEMMA columns, of which the former
gets surface lexical forms of the current stage of
derivation, and the latter gets the FORM data of the
previous IG.
3.2 Coordination Structures
Among the most controversial annotation schemes
are those of coordination structures (CS), which are
groups of two or more tokens that are in coordina-
tion with each other, usually joined with conjunc-
tions or punctuation, such as an ?and? relation. The
elements in coordination are the conjuncts of the CS,
all of which are semantically linked to a single ex-
ternal head. A large variety of annotation methods
are employed by different corpora, as thoroughly
explained in (Popel et al, 2013). We chose three
schemes to compare for our parser, which are il-
lustrated in Figure 2. There does not seem to be a
standard annotation rising as the best scheme, which
is convenient because different schemes would have
advantages and disadvantages against different for-
malisms and algorithms.
Figure 2: I) The original annotation scheme in the Turk-
ish Treebank. II) Swedish Style, an alternative scheme in
the manner of Talbanken (Nivre et al, 2006b). III) Stan-
ford Style, another alternative scheme in the manner of
the Stanford dependencies (De Marneffe and Manning,
2008), all with a head-right configuration as per (Popel et
al., 2013), as would be appropriate for the predominantly
head-final Turkish.
3.2.1 Current Coordination Representation
In the original Turkish Treebank, CSs are anno-
tated as shown in scheme I in Figure 2, which ap-
pears to be problematic in several ways. This struc-
ture requires a prior conjunct to be connected to an
intermediate conjunction, which in turn would be
connected to a posterior conjunct, completing the
coordination. The CS is then represented by the
posterior conjunct, and the dependency relation be-
tween the prior conjunct and the conjunction must
be identical to the dependency relation between the
posterior conjunct and the external head, even if it
would not semantically make sense.
Considering the tokens are processed incremen-
tally from left to right during parsing, one difficulty
with this method lies in correctly guessing the de-
pendency relation between the prior argument and
the conjunction before the posterior argument and
the external head are even encountered, and unsur-
prisingly, directional parsers fail at this task more
often than usual, resulting in added recall error for
many dependency relations not necessarily related to
coordinations. Another problem is that the scheme
requires an intermediate conjunction or punctuation
to work, which cannot be relied on even for edited
texts, and would fare much worse if applied on web
data. One final drawback of this method is that it is
arguably more confusing for human annotators com-
pared to a straightforward method in which the argu-
ments in coordination are directly connected.
3.3 Proposed Coordination Representation
The drawbacks we have identified in the original CS
annotation scheme encourage us to explore alterna-
tive approaches to coordinations. After investigating
many annotation methods, we expect that the repre-
sentation shown as the Swedish Style in Figure 2 will
have the best performance in alleviating the issues
described in Section 3.2.1.
Evaluating the Swedish Style representation, we
observe that the CS does not depend on correctly
placed conjunctions between the arguments, which
increases compatibility in the absence of well-
formatted sentences. Additionally, the dependency
relation between the CS and the external head is
not duplicated with this method, which should con-
tribute to the reduction of recall error for many de-
pendency types. Finally, we believe this scheme is
easier for human annotators to understand and apply,
131
and decreases the risk of annotation errors, which
are very common in the Turkish Treebank.
4 Experiments
In order to practically evaluate our proposed IG and
coordination representations, we first took our ini-
tial data set as our baseline, and then applied certain
manual and automatic transformations to the data in
order to create the experimental data sets. Since all
of our data were based on a training corpus without
an exclusive validation set, we decided to apply 10-
fold cross-validation on all of our models to better
evaluate the results.
For our tests on IG representations, we attempted
to automatically transform our baseline corpus by
populating the new IG column with boolean data
derived from the IG relations in the gold-standard,
and then automatically fill out the null lexical fields
by an automatic morphological synthesis procedure
using our morphological tool (Oflazer, 1994). The
synthesis procedure, albeit a non-trivial implemen-
tation, successfully covered the majority (over 95%)
of the lexical data, and we were able to manually an-
notate the remaining unrecognized tokens. To allow
MaltParser to recognize the new fields, the CoNLL-
X sentence format has been slightly adjusted and
submitted as a custom input data format, and the
baseline feature vector has been augmented with two
extra features for the IG column information from
the tokens on top of the Stack and Input pipes. The
final model is named the LexedIG model.
On the other hand, we needed to perform a com-
plete selective manual review of the corpus and cor-
rect numerous annotation errors in CSs before a
healthy conversion could be made. Afterwards, we
ran automatic conversion routines to map all CSs
to the aforementioned Swedish Style and the com-
monly used Stanford Style in order to compare their
specific performances. Since a sizeable amount of
manual corrections were made before the conver-
sions, we took the manually reviewed version as an
intermediate model in order to distinguish the contri-
bution of the automatic conversions from the manual
review.
4.1 Metrics
For every model we evaluated via cross-validation,
we made specific accuracy analyses and report the
precision (P ), recall (R) and F scores per depen-
Baseline LexedIG
P R F P R F
ABLAT 61, 46% 77, 44% 0, 69 61, 50% 76, 67% 0, 68
APPOS 66, 67% 12, 87% 0, 22 58, 97% 11, 39% 0, 19
CLASS 72, 98% 71, 80% 0, 72 72, 57% 71, 61% 0, 72
COORD 83, 95% 53, 70% 0, 66 83, 57% 54, 87% 0, 66
DATIV 60, 69% 71, 57% 0, 66 61, 08% 70, 68% 0, 66
DERIV 100,00% 100,00% 1,00 100,00% 100,00% 1,00
DETER 91, 18% 93, 70% 0, 92 91, 23% 93, 80% 0, 92
INSTR 44, 64% 38, 38% 0, 41 45, 87% 40, 96% 0, 43
INTEN 87, 99% 81, 95% 0, 85 87, 35% 81, 84% 0, 85
LOCAT 73, 40% 79, 25% 0, 76 73, 90% 79, 60% 0, 77
MODIF 86, 04% 81, 58% 0, 84 86, 33% 81, 74% 0, 84
MWE 71, 72% 58, 72% 0, 65 71, 50% 59, 42% 0, 65
NEGAT 92, 56% 70, 00% 0, 80 92, 86% 73, 13% 0, 82
OBJEC 77, 90% 71, 36% 0, 74 78, 32% 71, 92% 0, 75
POSSE 87, 44% 80, 80% 0, 84 86, 58% 81, 27% 0, 84
QUEST 86, 10% 77, 16% 0, 81 85, 77% 77, 16% 0, 81
RELAT 70, 00% 49, 41% 0, 58 70, 49% 50, 59% 0, 59
ROOT 68, 83% 99, 77% 0, 81 69, 63% 99, 77% 0, 82
S.MOD 54, 25% 50, 25% 0, 52 54, 29% 50, 92% 0, 53
SENTE 93, 25% 89, 63% 0, 91 93, 20% 89, 68% 0, 91
SUBJE 69, 54% 68, 94% 0, 69 69, 87% 69, 65% 0, 70
VOCAT 69, 61% 29, 46% 0, 41 69, 23% 29, 88% 0, 42
Table 1: Specific accuracies per dependency relation for
the IG-related models.
dency relation. Furthermore, we also calculated gen-
eral accuracies as micro-averages from the cross-
validation sets, for which we used two metrics,
namely the labelled attachment score ASL and the
unlabelled attachment score ASU , which are both
accuracy metrics that compute the percentage of cor-
rectly parsed dependencies over all tokens, where
the unlabelled metric only requires a match with the
correct head, and the labelled metric additionally re-
quires the correct dependency relation to be chosen.
4.2 Results and Discussion
Our test results with the LexedIG model suggest
that our proposed IG representation works perfectly
well, as the perfect precision and recall scores of the
original model for DERIV relations are preserved
in the new model. Besides this, the reconstructed
lexical information that we had populated the new
model with caused only slight changes in overall ac-
curacy that are not statistically significant, which is
likely due to the sparsity of lexical data. Regardless,
a model with lexical information for all tokens is es-
sentially superior to a similarly performing model
without such information. We foresee that being
able to see lexical forms in the data would increase
both the speed and the accuracy of human annota-
tion. Additionally, as these experiments were done
in preparation for the parsing of web data, we be-
lieve that in the near future, with the ability to un-
supervisedly parse large amounts of data found on
132
Baseline Corrected Swedish Style Stanford Style
P R F P R F P R F P R F
ABLAT 61, 46% 77, 44% 0, 69 61, 70% 79, 46% 0, 69 61, 25% 79, 19% 0, 69 61, 84% 80, 20% 0, 70
APPOS 66, 67% 12, 87% 0, 22 62, 86% 9, 78% 0, 17 65, 79% 12, 82% 0, 21 67, 57% 12, 82% 0, 22
CLASS 72, 98% 71, 80% 0, 72 72, 54% 71, 76% 0, 72 72, 33% 74, 52% 0, 73 72, 78% 74, 22% 0, 73
CONJU N/A N/A N/A N/A N/A N/A 79, 78% 72, 38% 0, 76 76, 99% 60, 85% 0, 68
COORD 83, 95% 53,70% 0, 66 83, 88% 54,23% 0, 66 79, 15% 64,64% 0, 71 73, 82% 58,68% 0, 65
DATIV 60, 69% 71, 57% 0, 66 61, 57% 71, 77% 0, 66 60, 62% 72, 83% 0, 66 61, 36% 73, 81% 0, 67
DERIV 100, 00% 100, 00% 1, 00 100, 00% 100, 00% 1, 00 100, 00% 100, 00% 1, 00 100, 00% 100, 00% 1, 00
DETER 91, 18% 93, 70% 0, 92 91, 08% 93, 74% 0, 92 91, 14% 94, 28% 0, 93 91, 15% 93, 97% 0, 93
INSTR 44, 64% 38, 38% 0, 41 46, 72% 39, 48% 0, 43 46, 05% 41, 08% 0, 43 45, 25% 41, 49% 0, 43
INTEN 87, 99% 81, 95% 0, 85 87, 30% 82, 71% 0, 85 87, 46% 82, 47% 0, 85 87, 83% 82, 26% 0, 85
LOCAT 73, 40% 79, 25% 0, 76 73, 92% 79, 35% 0, 77 72, 33% 78, 91% 0, 75 72, 42% 79, 73% 0, 76
MODIF 86, 04% 81, 58% 0, 84 85, 80% 81, 47% 0, 84 85, 80% 81, 84% 0, 84 85, 83% 81, 06% 0, 83
MWE 71, 72% 58, 72% 0, 65 72, 46% 59, 09% 0, 65 74, 11% 58, 87% 0, 66 72, 55% 60, 18% 0, 66
NEGAT 92, 56% 70, 00% 0, 80 92, 68% 66, 28% 0, 77 92, 91% 73, 29% 0, 82 92, 00% 71, 43% 0, 80
OBJEC 77, 90% 71, 36% 0, 74 77, 61% 71, 54% 0, 74 78, 42% 72, 12% 0, 75 78, 67% 72, 08% 0, 75
POSSE 87, 44% 80, 80% 0, 84 87, 03% 80, 68% 0, 84 87, 69% 83, 37% 0, 85 87, 25% 82, 89% 0, 85
QUEST 86, 10% 77, 16% 0, 81 86, 15% 77, 78% 0, 82 86, 15% 78, 05% 0, 82 86, 15% 78, 05% 0, 82
RELAT 70, 00% 49, 41% 0, 58 71, 67% 49, 43% 0, 59 72, 13% 50, 57% 0, 59 69, 35% 49, 43% 0, 58
ROOT 68, 83% 99, 77% 0, 81 68, 84% 99, 49% 0, 81 70, 41% 99, 79% 0, 83 66, 28% 99, 81% 0, 80
S.MOD 54, 25% 50, 25% 0, 52 51, 31% 49, 28% 0, 50 53, 55% 50, 09% 0, 52 53, 88% 49, 91% 0, 52
SENTE 93, 25% 89, 63% 0, 91 92, 74% 89, 02% 0, 91 93, 50% 88, 80% 0, 91 93, 36% 88, 90% 0, 91
SUBJE 69, 54% 68, 94% 0, 69 69, 61% 68, 14% 0, 69 69, 89% 69, 70% 0, 70 69, 75% 69, 60% 0, 70
VOCAT 69, 61% 29, 46% 0, 41 67, 86% 24, 78% 0, 36 61, 05% 25, 66% 0, 36 69, 62% 24, 34% 0, 36
Table 2: Specific accuracies per dependency relation for the coordination-related models.
the web, sparse data will no longer be a significant
problem, and lexical data will gain further value.
A comparison of the alternative CS models with
the baseline suggests that, while the manual cor-
rection itself did not cause a noticeable change,
the automatic conversion procedures that it made
possible resulted in significant improvements. The
Swedish Style and Stanford Style models fared
slightly better in the accuracy of some dependency
types commonly joined in CSs such as SUBJECT,
OBJECT, and DATIVE, INSTRUMENTAL and
ABLATIVE.ADJUNCTs, but not always enough to
warrant statistical significance. Apart from those,
the largest improvement is in the COORDINATION
relation itself, which had a slight drop in precision
for both final models (likely due to the increased av-
erage dependency distances) but at the great benefit
of the recall increasing from 53.70% to 58.68% for
the Stanford Style and 64.64% for the Swedish Style.
5 Conclusion
In this paper, we proposed novel annotation schemes
for Turkish morphosyntactic sub-word units and co-
ordination structures that are superior to the Turk-
ish Treebank representations in terms of ease of use,
parsing performance and/or compatibility with sen-
ASU ASL
Baseline 74.5%? 0.2 65.0%? 0.2
LexedIG 74.6%? 0.1 65.1%? 0.2
Baseline 74.5%? 0.2 65.0%? 0.2
Corrected 74.5%? 0.1 65.0%? 0.2
Swedish Style 74.5%? 0.2 65.6%? 0.2
Stanford Style 73.2%? 0.2 64.1%? 0.2
Table 3: General parsing accuracies for all models, in-
cluding standard error.
tences that are not well-formed. Our findings sub-
stantiate our thesis that annotation schemes have
both room for improvement and a high impact po-
tential on parsing performance. In the light of our
results, we intend to sustain our research and draw
better annotation schemes for other syntactic struc-
tures such as copulae and modifier sub-types to serve
not only Turkish, but also other languages with rich
morphology.
Acknowledgments
The authors would like to acknowledge that this
work is part of a research project supported by ICT
COST Action IC1207 and TU?BI?TAK 1001 (Grant
Number 112E276).
133
References
Cristina Bosco, Simonetta Montemagni, Alessandro
Mazzei, Vincenzo Lombardo, Felice dell?Orletta,
Alessandro Lenci, Leonardo Lesmo, Giuseppe Attardi,
Maria Simi, Alberto Lavelli, et al 2010. Comparing
the influence of different treebank annotations on de-
pendency parsing. In LREC.
Adriane Boyd and Detmar Meurers. 2008. Revisiting the
impact of different annotation schemes on pcfg pars-
ing: A grammatical dependency evaluation. In Pro-
ceedings of the Workshop on Parsing German, pages
24?32. Association for Computational Linguistics.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning, pages 149?164. Association
for Computational Linguistics.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: A
Library for Support Vector Machines. Software avail-
able at http://www.csie.ntu.edu.tw/?cjlin/libsvm.
Marie-Catherine De Marneffe and Christopher D Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Coling 2008: Proceedings of the work-
shop on Cross-Framework and Cross-Domain Parser
Evaluation, pages 1?8. Association for Computational
Linguistics.
Gu?ls?en Eryig?it, Tugay Ilbay, and Ozan Arkan Can. 2011.
Multiword expressions in statistical dependency pars-
ing. In Proceedings of the Second Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(IWPT), pages 45?55, Dublin, Ireland, October. As-
sociation for Computational Linguistics.
Gu?ls?en Eryig?it, Joakim Nivre, and Kemal Oflazer. 2008.
Dependency parsing of Turkish. Computational Lin-
guistics, 34(3):357?389.
Dilek Hakkani-Tu?r, Kemal Oflazer, and Go?khan Tu?r.
2002. Statistical morphological disambiguation for
agglutinative languages. Journal of Computers and
Humanities, 36(4):381?410.
Ryan McDonald and Joakim Nivre. 2011. Analyzing
and integrating dependency parsers. Computational
Linguistics, 37(1):197?230.
Joakim Nivre, Johan Hall, Jens Nilsson, Gu?ls?en Eryig?it,
and Stetoslav Marinov. 2006a. Labeled pseudo-
projective dependency parsing with support vector
machines. In Proceedings of the 10th Conference
on Computational Natural Language Learning, pages
221?225, New York, NY.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006b. Tal-
banken05: A swedish treebank with phrase structure
and dependency annotation. In Proceedings of the fifth
International Conference on Language Resources and
Evaluation (LREC), pages 1392?1395.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?ls?en Eryig?it, Sandra Ku?bler, Stetoslav Mari-
nov, and Erwin Marsi. 2007. Maltparser: A
language-independent system for data-driven depen-
dency parsing. Natural Language Engineering Jour-
nal, 13(2):99?135.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies, pages
149?160, Nancy.
Kemal Oflazer, Bilge Say, Dilek Z. Hakkani-Tu?r, and
Go?khan Tu?r. 2003. Building a Turkish treebank. In A.
Abeille?, editor, Treebanks: Building and Using Parsed
Corpora, pages 261?277. Kluwer, London.
Kemal Oflazer. 1994. Two-level description of Turk-
ish morphology. Literary and Linguistic Computing,
9(2):137?148.
Kemal Oflazer. 2003. Dependency parsing with an ex-
tended finite-state approach. Computational Linguis-
tics, 29(4):515?544.
Martin Popel, David Marec?ek, Jan S?te?pa?nek, Daniel Ze-
man, and Zde?ne?k Z?abokrtsky?. 2013. Coordination
structures in dependency treebanks. In Proceedings
of the 51st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 517?527, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Reut Tsarfaty and Yoav Goldberg. 2008. Word-based
or morpheme-based? annotation strategies for mod-
ern hebrew clitics. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Bente Maegaard,
Joseph Mariani, Jan Odijk, Stelios Piperidis, and
Daniel Tapias, editors, Proceedings of the Sixth In-
ternational Conference on Language Resources and
Evaluation (LREC?08), Marrakech, Morocco, may.
European Language Resources Association (ELRA).
http://www.lrec-conf.org/proceedings/lrec2008/.
134
