A Multi-Path Architecture for Machine Translation of  
English Text into American Sign Language Animation 
 
 
Matt Huenerfauth 
Computer and Information Science 
University of Pennsylvania 
Philadelphia, PA 19104 
matthewh@seas.upenn.edu 
 
 
Abstract 
The translation of English text into American 
Sign Language (ASL) animation tests the 
limits of traditional MT architectural designs.  
A new semantic representation is proposed 
that uses virtual reality 3D scene modeling 
software to produce spatially complex ASL 
phenomena called ?classifier predicates.?  The 
model acts as an interlingua within a new 
multi-pathway MT architecture design that 
also incorporates transfer and direct 
approaches into a single system. 
1 Introduction and Motivation 
American Sign Language (ASL) is a visual/spatial 
natural language used primarily by the half million Deaf 
individuals in the U.S. and Canada.  ASL has a distinct 
grammar, vocabulary, and structure from English, and 
its visual modality allows it to use linguistic phenomena 
not seen in spoken languages (Liddell, 2003; Neidle et 
al., 2000).  English-to-ASL translation is as complex as 
translation between pairs of written languages, and in 
fact, the difference in modality (from a written/spoken 
to a visual/spatial manually performed system) adds 
new complexities to the traditional MT problem. 
Building an English-to-ASL MT system is important 
because although Deaf students in the U.S. and Canada 
are taught written English, the difficulties in acquiring a 
spoken language for students with hearing impairments 
prevents most Deaf U.S. high school graduates from 
reading above a fourth-grade level (students age 18 and 
older reading text at a typical 10-year-old level) (Holt, 
1991).  Unfortunately, many Deaf accessibility aids 
(e.g. television closed captioning or teletype telephone 
services) assume that the viewer has strong English 
literacy skills.  Since many of these individuals are 
fluent in ASL despite their difficulty reading English, an 
ASL MT system could make more information and 
services accessible in situations where English 
captioning text is above the reading level of the viewer 
or a live English-to-ASL interpreter is unavailable. 
Researchers in graphics and human figure modeling 
have built animated models of the human body that are 
articulate enough to perform ASL that native signers 
can understand (Wideman and Sims 1998).  Most 
animation systems use a basic instruction set to control 
the character?s movements; so, an MT system would 
need to analyze an English text input and produce a 
?script? in this instruction set specifying how the 
character should perform the ASL translation output.  
The MT task is conceived of as translation from English 
text into this script because ASL has no written form. 
While linguists use various ASL glosses, all were 
designed to facilitate linguistic study, not to serve as a 
natural writing system, and so they omit certain details. 
Since there is no ASL orthography used by the Deaf 
community, there are no natural sources of ASL 
corpora.  To collect a corpus for statistical MT research, 
a movement annotation standard must be developed, 
ASL performances videotaped, and finally the videos 
manually transcribed ? a slow and expensive process 
(Niedle, 2000).  Motion-capture glove technology may 
seem like a solution to this problem, but this type of 
data cannot easily be synthesized into novel and fluent 
ASL animations.  The difficulty in obtaining large 
corpora of ASL is why statistical approaches to the 
English-to-ASL MT problem are not currently practical. 
2 ASL Linguistic Issues 
As opposed to spoken/written languages, ASL relies on 
the multiple simultaneous channels of handshape, hand 
location, palm orientation, hand/arm movement, facial 
expressions, and other non-manual signals to convey 
meaning.  To express additional meaning, ASL may 
modify aspects of the manual performance of a sign 
(handshape, timing, motion path, repetition, etc.), 
perform an additional grammatical facial expression, or 
systematically use the areas of space around the signer. 
ASL signers use the space around them for several 
grammatical, discourse, and descriptive purposes.  
During a conversation, an entity under discussion 
(whether concrete or abstract) can be ?positioned? at a 
point in the signing space.  Subsequent pronominal 
reference to this entity can be made by pointing to this 
location, and some verb signs will move toward or away 
from these points to indicate their arguments.  
Generally, the locations chosen for this pronominal use 
of the signing space are not topologically meaningful; 
that is, one imaginary entity being positioned to the left 
of another in the signing space doesn?t necessarily 
indicate the entity is left of the other in the real world. 
Other ASL expressions are more complex in their 
use of space and position invisible objects around the 
signer to topologically indicate the arrangement of 
entities in a 3D scene being discussed.  Special ASL 
constructions called ?classifier predicates? allow signers 
to use their hands to represent an entity in the space in 
front of them and to position, move, trace, or re-orient 
this imaginary object in order to indicate the location, 
movement, shape, or other properties of some 
corresponding real world entity under discussion.  A 
classifier predicate generally consists of the hand in one 
of a closed set of semantically meaningful shapes as it 
moves in a 3D path through space in front of the signer. 
For example, the sentence ?the car drove down the 
bumpy road past the cat? could be expressed in ASL 
using two classifier predicates.  First, a signer would 
move a hand in a ?bent V? handshape (index and middle 
fingers extended and bent) forward and downward to a 
point in space in front of his or her torso where an 
imaginary miniature cat could be envisioned.  Next, a 
hand in a ?3? handshape (thumb, index, middle fingers 
extended) could trace a path in space past the ?cat? in an 
up-and-down fashion as if it were a car bouncing along 
a bumpy road.  Generally, ?bent V? handshapes are 
used for animals, and ?3? handshapes, for vehicles. 
The ability of classifier predicates to topologically 
represent a three-dimensional scene make them 
particularly difficult to generate using traditional 
computational linguistic methods and models.  To 
produce this pair of classifier predicates, there must be a 
spatial model of how the scene is arranged including the 
locations of the cat, the road, and the car.  A path for the 
car must be chosen with beginning/ending positions, 
and the hand must be articulated to indicate the contour 
of the path (e.g. bumpy, hilly, twisty).  The proximity of 
the road to the cat, the plane of the ground, and the 
curve of the road must be selected.  Other properties of 
the objects must be known: (1) cats generally sit on the 
ground and (2) cars usually travel along the ground on 
roads.  The successful translation of the English text 
into these classifier predicates used a great deal of 
semantic analysis, spatial knowledge, and reasoning. 
3 ASL MT Architectural Designs 
There is an architectural spectrum along which most 
MT systems can be classified; loosely they are grouped 
into three basic designs: direct, transfer, or interlingua 
(Dorr et al, 1998).  Direct systems process individual 
words of the source language text; translation is 
achieved without performing any syntactic analysis.  
Transfer systems do analyze the input text to some 
syntactic or semantic level, and then a set of ?transfer? 
rules produce a corresponding syntactic or semantic 
structure in the target language. Finally, a generation 
component converts this structure into a target-language 
text.  Interlingual systems take this analysis of the input 
text one step further: the source is analyzed and 
semantically processed to produce a typically language-
independent semantic representation called an 
?interlingua,? and then a generation component 
produces the target-language surface form from there.  
These design choices are often pictured as a pyramid, as 
in Figure 1, adapted from a figure in (Dorr et al, 1998). 
Generally, in the absence of statistical or case-based 
information, the higher up the pyramid that the source 
text is analyzed, the more complex and subtle are the 
divergences the system can handle.  In particular, at the 
interlingual level, a knowledge base can supplement the 
linguistic information, producing translations that use 
world knowledge and that may convey more 
information than was present in the source text (devoid 
of context).  However, any of the approaches can 
produce a correct translation for certain inputs since not 
all sentences require such sophisticated analysis to be 
translated ? some exhibit little translation divergence.  
Another trend as one goes up the MT pyramid is that the 
Figure 1: Pyramid of MT Architecture Designs. 
amount of domain specific development work that must 
be performed increases dramatically.  While direct 
systems may only require a bilingual lexicon, transfer 
systems also require analysis and transfer rules. 
Interlingual systems require interlingual representations 
and sometimes domain specific knowledge bases. 
Non-statistical direct approaches to English-to-ASL 
MT generally produce simple translations that are often 
little more than word-to-sign dictionary look-ups.  With 
the addition of some basic sentence reordering 
heuristics, such systems can occasionally produce 
acceptable output on simple English inputs or on those 
English-ASL sentence pairs that have similar word 
order.1  Since no syntactic analysis is performed, there is 
no chance that an input sentence will be outside the 
linguistic coverage of the system; so, the translation 
process will always produce some output.  Even if an 
English word is not in the translation lexicon, manual 
fingerspelling can be used to express the word. 
Transfer MT designs address most of the linguistic 
shortcomings of direct systems but do require additional 
linguistic resources to be developed.  There have been a 
few transfer-based English-to-ASL systems built 
(Huenerfauth, 2003), and several have had success in 
particular aspects of the MT task, like expressing 
adverbials (Zhao et al, 2000) or representing ASL 
phonological information (Speers, 2001; S?f?r and 
Marshall, 2001).  These systems show promise that a 
transfer approach could someday handle most ASL 
sentences that do not require complex or topological use 
of the signing space.  As the ?bumpy road? example 
illustrates, generating classifier predicates would require 
more than a simple syntactic or semantic analysis ? 
spatial analogy, scene visualization, and/or some degree 
of iconicity seem to be involved.2  
For this reason, ASL transfer systems merely omit 
classifier predicates from their coverage; however, 
many English concepts lack a fluent ASL translation 
without them.  Further, these predicates are common in 
ASL; signers generally produce a classifier predicate at 
least once per minute (once per 100 signs) (Morford and 
MacFarlane, 2003).  So, systems that cannot produce 
classifier predicates are not a viable long-term solution 
to the English-to-ASL MT problem.  To supply the 
semantic understanding, spatial reasoning, and world 
knowledge that classifier predicate generation demands, 
an interlingual approach (one with deeper semantic 
analysis and 3D spatial representations) is required. 
                                                          
1
 Direct systems more readily convert English text into a 
signing system like Signed Exact English, a manually coded 
form of English, not a distinct natural language, like ASL. 
2
 Linguists debate whether classifier predicates are 
paralinguistic iconic gestures, non-spatial polymorphemic 
constructions, or compositional yet spatially-aware 
expressions (Liddell, 2003), but transfer approaches to MT 
seem ill-suited to producing classifier predicates in any case. 
4 A Multi-Path MT Architecture 
While an interlingual approach to the classifier 
predicate translation task sounds useful, there is a 
problem. It?s hard to built a true interlingual system for 
anything but a carefully limited domain; building the 
linguistic and knowledge resources needed for 
interlingual translation on less restricted texts can entail 
too much overhead to be practical.  What is special 
about the MT problem for ASL ? and the reason why 
interlingual translation may be possible ? is that we can 
characterize and identify the ?hard? input sentences, the 
ones that require classifier predicates for translation.  
These are spatially descriptive English input texts, those 
generally containing: spatial verbs describing locations, 
orientations, or movements; spatial prepositions or 
adverbials with concrete or animate entities; or lexical 
items related to other common topics or genres in which 
classifier predicates are typically used.  Such genres 
(e.g. vehicle motion or furniture arrangement in a room) 
could be detected using the features mentioned above. 
While an interlingual approach is needed to translate 
into classifier predicates, there are a vast number of 
English input sentences for which such deep analysis 
and reasoning would not be necessary.  As we've seen 
from the direct and transfer discussion above, these 
resource-lighter approaches can often produce a correct 
translation from lexical or syntactic information alone.   
This analysis suggests a new multi-path architecture 
for an MT system ? one that includes a direct, a transfer, 
and an interlingual pathway.  English input sentences 
within the implemented interlingua?s limited domain 
could follow that processing pathway, those sentences 
outside of the interlingual domain but whose syntactic 
features fall within the linguistic coverage of the 
analysis and transfer rules could use the transfer 
pathway, and all other sentences could use the direct 
pathway with its bilingual dictionary look-up. 
Limiting the domain that the transfer and interlingua 
components must handle makes the development of 
these components more manageable.  The transfer 
pathway?s analysis grammar and transfer rules would 
not have to cover every possible English sentence that it 
encounters: some sentences would simply use the direct 
translation pathway.  Limiting domains has an even 
more dramatic benefit for the interlingual pathway. 
Instead of building interlingual analysis, representation, 
and generation resources for every possible domain, the 
interlingual development can focus on the specific 
domains in which classifier predicates are used: walking 
upright figures, moving vehicles, furniture or objects 
arranged in a room, giving directions, etc.  In this way, 
the ?depth? of divergence-handling power of some 
translation approaches and the ?breadth? of coverage of 
others can both be part of this multi-path architecture.   
This design does more than just restrict the domains 
for which the interlingua must be implemented; it also 
reduces the ontological complexity that the entire 
interlingua must support.  The domains listed above 
share a common feature: they all discuss the movement, 
location, orientation, and physical description of entities 
in three-dimensional scenes.  Some complex 
phenomena whose handling often makes designing an 
interlingual representation quite difficult ? abstract 
concepts, beliefs, intentions, quantification, etc. ? do not 
need to be represented.  In a sense, this multi-path 
architecture doesn?t just limit the things that must be 
represented, but the ?type? of these things as well. 
Having multiple processing pathways does not mean 
that there is necessarily a new problem of choosing 
which to use.  The system could be implemented as a 
?fall back? architecture in which the system could 
attempt the most complex approach (interlingual) and 
drop back to each of the simpler approaches whenever it 
lacks the proper lexical, syntactic, semantic, or 
knowledge resources to succeed for the current 
pathway.  In this way, the linguistic coverage of each of 
the levels of representation would define exactly how 
input sentences would be routed through the system.   
If the system were to use a more complex pathway 
than was necessary during translation, then, if properly 
implemented, output would be produced that could have 
been created using a simpler pathway. This is an 
acceptable, if less efficient, result.  If the system lacked 
the linguistic resources to translate a sentence using the 
sophisticated level of processing it required, then the 
output would be more English-like in structure than it 
should.  Because most Deaf users of the system would 
have had experience interacting with hearing people 
who used non-fluent English-like signing or manually 
signed forms of English, like Signed Exact English or 
Sign Supported English, then they may still find this 
overly English-like translation useful. 
5 A Spatial Interlingua for ASL MT 
When ASL signers describe a spatially complex 3D 
scene using classifier predicates, they visualize the 
elements of the scene as occupying an area of space that 
is generally within arm?s reach in front of their torso.  
So, signers have a spatial model of the scene under 
discussion that they can consider when selecting and 
generating classifier predicates to convey information.  
An automated system for creating classifier predicates 
may be able to use an analogous representation.   
One way to produce this model is to incorporate 
virtual reality 3D scene representation software into the 
MT system?s interlingual pathway.  After analyzing the 
English text, the movements of entities under discussion 
could be identified, and a 3D virtual reality model of the 
scene could be constructed and/or modified to reflect 
the information in the English text.  This spatial model 
could serve as the basis for generating the 3D and 
spatially analogous (topological) motions of the signing 
character?s hands while performing classifier predicates.   
Fortunately, a system for producing a changing 3D 
model of a scene from an English text has been built: 
the Natural Language Instructions for Dynamically 
Altering Agent Behaviors system (Bindiganavale et al, 
2000; Badler et al, 2000) (herein, ?NLI?).  The system 
displays a 3D virtual reality scene and accepts English 
input text containing instructions for the characters and 
objects in the scene to follow. It updates the animation 
so that objects obey the English commands.  NLI has 
been used in military training and equipment repair 
domains and can be extended by augmenting its library 
of Parameterized Action Representations (PARs), to 
cover additional domains of English input texts. 
PARs are feature/value structures stored as a library 
of templates with slots specifying: the agent moving, the 
path/manner or translational/rotational nature of the 
motion, terminating conditions, speed/timing, and other 
motion information.  English lexicalized syntactic 
structures are associated with PARs so that the analysis 
of a text can be used to select a PAR template and fill its 
slots.  PARs serve as 3D motion primitives and are used 
as hierarchical planning operators to produce a detailed 
animation specification; so, they contain fields like 
preconditions and sub-actions used in NLI?s animation 
planning process (Badler et al, 2000).  A PAR generally 
corresponds to an English motion verb (or a set of 
related verbs); so, to extend NLI for use in an ASL 
context, additional PARs will be developed for English 
motion verbs that often produce classifier predicates. 
The MT system?s interlingual pathway will use the 
NLI software to analyze the English source text as if it 
were commands for the entities mentioned in the text.  
The NLI can create and maintain a 3D model of the 
location and motion of these entities.  The MT system, 
unlike other applications of the NLI software, does not 
care about the exact shape or appearance of the objects 
being modeled (generic box-like shapes could be used 
for each).  Instead, the location and motion paths of 
these objects in a generic 3D space are important, since 
these are used to build classifier predicates.    
The MT system would use the spatial model to 
instantiate a transparent miniature animation of these 
objects; this animation would be overlaid on an area of 
the virtual reality space in front of the torso of the 
character performing the ASL animation output.  In the 
?bumpy road? example, a small invisible object would 
be positioned in space in front of the chest of the 
signing character to represent the cat.  Next, a 3D 
animation path and location for the car (relative to the 
cat) would be chosen in front of the character?s chest. 
When objects in this ?invisible world? are moved or 
reoriented to reflect information in the English text, the 
animated ASL-signing character can position its hand 
inside of the transparent (possibly moving) object to 
indicate its new location, orientation, or movement path. 
By choosing an appropriate handshape for the character, 
a classifier predicate is thus produced that conveys the 
spatial information from the English text.  Extensions of 
this design for more complex classifier predicate 
constructions are discussed in (Huenerfauth, 2004). 
This interlingual pathway design would pass along 
most of the spatial modeling and reasoning burdens to 
the NLI software, which was designed for this task.  It 
can select relative locations and motion paths for objects 
in the 3D scene based on prepositions and adverbials in 
the English input text.  It uses collision avoidance, 
physical constraints, generic and specialized motion 
primitives, and hierarchical motion planning operators 
to produce the necessary detail for a 3D animation from 
the limited information in a corresponding English text. 
The full architectural diagram is shown in Figure 2.  
This design visually resembles the pyramid in Figure 1: 
direct pathway at the bottom, transfer across the middle, 
and interlingual pathway over the top of the pyramid.  
The three paths no longer represent the design choices 
possible for different systems; they are now processing 
pathways within a single ?pyramidal? architecture. 
6 Virtual Reality as Interlingua 
The 3D model produced by the NLI software serves as 
an intermediary between the English text analysis and 
the classifier predicate generation in this architecture, 
but that does not necessarily make it an interlingua.  In 
fact, the design differs from interlingual representations 
elsewhere in the MT literature significantly.  To explore 
this issue, consider a general definition of an interlingua 
as: a typically language-neutral semantic representation 
useful for MT that may incorporate knowledge sources 
beyond the basic semantics of the input text. 
First, the model represents those aspects of the input 
text?s meaning significant for translation to classifier 
predicates; thus it serves as a semantic representation 
within the 3D motion domain ? albeit a non-traditional 
one due to the ontological simplicity of this domain.  
Second, this proposed architectural design has 
illustrated how this 3D scene representation is useful for 
MT.  Third, the NLI software?s ability to incorporate 
physical constraints, collision detection, and spatial 
reasoning shows how the 3D model can use knowledge 
sources beyond the original text during translation. 
So, the final determinant of this model?s interlingual 
status is its language-neutrality.  The 3D coordinates of 
objects in a virtual reality model are certainly language-
neutral. However, ASL linguists have identified 
discourse and other factors beyond the 3D scene model 
that can affect how classifier predicates are generated 
(Liddell, 2003).  If the classifier predicate generator 
needs these features, then the degree to which they are 
modeled in a language-neutral manner will affect 
whether the pathway is truly interlingual.  Until the final 
implementation of the generator is decided, it is an open 
issue as to whether this pathway is an interlingua or 
simply a spatially rich semantic transfer design. 3 
7 Discussion and Future Work 
While English-to-ASL MT motivated the multi-path 
pyramidal architecture, the design is also useful for 
other language pairs.  Merging multiple MT approaches 
in one system alleviates the traditional trade-off 
between divergence-handling power and domain 
specificity, thus making resource-intensive approaches 
(e.g. interlingual) practical for applications that require 
broad linguistic coverage.  This architecture is useful 
when a system must translate a variety of texts but 
perform deeper processing on texts within particular 
important or complex domains.  It is also useful when 
the input is usually (but not always) inside a particular 
sublanguage.  Transfer or interlingual resources can be 
developed for the domains of interest, and resource-
lighter (broader coverage) pathways can handle the rest. 
While the English-to-ASL system had no statistical 
pathways, nothing prevents their use in a multi-path 
pyramidal architecture.  Statistical approaches could be 
used to develop a direct pathway, and hand-built 
analysis and transfer rules for a subset of the source 
language could create a transfer pathway.  A developer 
could thus use a stochastic approach for most inputs but 
manually override the MT process for certain texts (that 
                                                          
3
 Kipper and Palmer (2000) examined PARs as an 
interlingua for translation of motion verbs between verb-
frame and satellite-frame languages.  Unlike this system, 
they did not use PARs within a 3D scene animation; the 
PAR itself was their interlingua, not the 3D scene. 
Figure 2: Multi-Path ?Pyramidal? MT Architecture. 
are important or whose translation is well understood).  
Likewise, a transfer pathway may use statistically 
induced transfer rules and parsers, and an interlingual 
pathway may be manually built for specific domains. 
While the pyramidal architecture has applications 
across many languages, the 3D scene modeling software 
has benefits specific to ASL processing.  Beyond its use 
in classifier predicate generation, the 3D model allows 
this system to address ASL phenomena that most MT 
architectures cannot.  The non-topological use of the 
signing space to store positioned objects or ?tokens? 
(Liddell, 2003) for pronominal reference to entities in 
the discourse can easily be implemented in this system 
by taking advantage of the invisible overlaid 3D scene.  
The layout, management, and manipulation of these 
?tokens? is a non-trivial problem, and the richness of the 
virtual reality spatial model can facilitate their handling. 
The NLI software makes use of sophisticated human 
characters that can be part of the scenes being controlled 
by the English text.  These virtual humans possess skills 
that would make them excellent ASL signers for this 
project: they can gaze in specific directions, make facial 
expressions useful for ASL output, and point at objects 
or move their hand to locations in 3D space in a fluid 
and anatomically natural manner (Badler et al, 2000).  
If one of these virtual humans served as the signing 
character, as one did for (Zhao et al, 2000), then the 
same graphics software would control both the invisible 
world model and the ASL-signing character, thus 
simplifying the implementation of the MT system. 
Currently, this project is finishing the specification 
of the multi-path design and investigating the following 
issues: deep generation techniques for creating multiple 
interrelated classifier predicates, surface generation of 
individual classifier predicates from compositional rules 
or parameterized templates, and ASL morphological 
and syntactic representations for the transfer pathway.  
Another important issue being examined is how to 
evaluate the ASL animation output of an MT system ? 
in particular one that produces classifier predicates. 
Acknowledgements 
I would like to thank my advisors Mitch Marcus and 
Martha Palmer for their guidance, discussion, and 
revisions during the preparation of this work. 
References 
R. Bindiganavale, W. Schuler, J. Allbeck, N. Badler, A. 
Joshi, and M. Palmer. 2000. ?Dynamically Altering 
Agent Behaviors Using Natural Language 
Instructions.? 4th International Conference on 
Autonomous Agents.  
N. Badler, R. Bindiganavale, J. Allbeck, W. Schuler, L. 
Zhao, S. Lee, H. Shin, and M. Palmer.  2000.  
?Parameterized Action Representation and Natural 
Language Instructions for Dynamic Behavior 
Modification of Embodied Agents.? AAAI Spring 
Symposium. 
B. Dorr, P. Jordan, and J. Benoit.  1998.  ?A Survey of 
Current Paradigms in Machine Translation.? 
Technical Report LAMP-TR-027, Language and 
Media Processing Lab, University of Maryland. 
J. Holt. 1991. Demographic, Stanford Achievement Test 
- 8th Edition for Deaf and Hard of Hearing Students: 
Reading Comprehension Subgroup Results. 
M. Huenerfauth. 2003. ?Survey and Critique of 
American Sign Language Natural Language 
Generation and Machine Translation Systems.?  
Technical Report MS-CIS-03-32, Computer and 
Information Science, University of Pennsylvania 
M. Huenerfauth. 2004. ?Spatial Representation of 
Classifier Predicates for Machine Translation into 
American Sign Language.? In Proceedings of the 
Workshop on the Representation and Processing of 
Signed Languages, 4th International Conference on 
Language Resources and Evaluation (LREC 2004). 
K. Kipper and M. Palmer. 2000. ?Representation of 
Actions as an Interlingua.? In Proceedings of the 3rd 
Workshop on Applied Interlinguas, ANLP-NAACL.  
S. Liddell. 2003. Grammar, Gesture, and Meaning in 
American Sign Language.  UK: Cambridge U. Press.   
J. Morford and J. MacFarlane. ?Frequency 
Characteristics of ASL.? Sign Language Studies, 3:2. 
C. Neidle. 2000. ?SignStream?: A Database Tool for 
Research on Visual-Gestural Language.? American 
Sign Language Linguistic Research Project, Report 
Number 10, Boston University, Boston, MA, 2000. 
C. Neidle, J. Kegl, D. MacLaughlin, B. Bahan, and R. 
G. Lee.  2000.  The Syntax of American Sign 
Language: Functional Categories and Hierarchical 
Structure. Cambridge, MA: The MIT Press. 
?. S?f?r and I. Marshall.  2001.  ?The architecture of an 
English-text-to-Sign-Languages Translation System.?  
In G. Angelova, ed., Recent Advances in Natural 
Language Processing. Tzigov Chark, Bulgaria. 
d'A. Speers.  2001.  Representation of ASL for Machine 
Translation. Ph.D. Diss., Linguistics, Georgetown U.  
C. Wideman & M. Sims. 1998. ?Signing Avatars.? 
Technology & Persons with Disabilities Conference. 
L. Zhao, K. Kipper, W. Schuler, C. Vogler, N. Badler, 
and M. Palmer.  2000.  ?A Machine Translation 
System from English to American Sign Language.? 
Association for Machine Translation in the Americas. 
Proceedings of the ACL Student Research Workshop, pages 37?42,
Ann Arbor, Michigan, June 2005. c?2005 Association for Computational Linguistics
American Sign Language Generation:  
Multimodal NLG with Multiple Linguistic Channels 
 
Matt Huenerfauth  
Computer and Information Science  
University of Pennsylvania 
Philadelphia, PA 19104 USA 
matt@huenerfauth.com 
 
 
 
Abstract 
Software to translate English text into 
American Sign Language (ASL) animation 
can improve information accessibility for 
the majority of deaf adults with limited 
English literacy.  ASL natural language 
generation (NLG) is a special form of mul-
timodal NLG that uses multiple linguistic 
output channels.  ASL NLG technology has 
applications for the generation of gesture 
animation and other communication signals 
that are not easily encoded as text strings.  
1 Introduction and Motivations 
American Sign Language (ASL) is a full natural 
language ? with a linguistic structure distinct from 
English ? used as the primary means of communi-
cation for approximately one half million deaf 
people in the United States (Neidle et al, 2000, 
Liddell, 2003; Mitchell, 2004).  Without aural ex-
posure to English during childhood, a majority of 
deaf U.S. high school graduates (age 18) have only 
a fourth-grade (age 10) English reading level (Holt, 
1991).  Technology for the deaf rarely addresses 
this literacy issue; so, many deaf people find it dif-
ficult to read text on electronic devices.  Software 
for translating English text into animations of a 
computer-generated character performing ASL can 
make a variety of English text sources accessible to 
the deaf, including: TV closed captioning, teletype 
telephones, and computer user-interfaces  (Huener-
fauth, 2005).  Machine translation (MT) can also 
be used in educational software for deaf children to 
help them improve their English literacy skills.   
This paper describes the design of our English-
to-ASL MT system (Huenerfauth, 2004a, 2004b, 
2005), focusing on ASL generation. This overview 
illustrates important correspondences between the 
problem of ASL natural language generation 
(NLG) and related research in Multimodal NLG.   
1.1 ASL Linguistic Issues 
In ASL, several parts of the body convey meaning 
in parallel: hands (location, orientation, shape), eye 
gaze, mouth shape, facial expression, head-tilt, and 
shoulder-tilt.  Signers may also interleave lexical 
signing (LS) with classifier predicates (CP) during 
a performance.  During LS, a signer builds ASL 
sentences by syntactically combining ASL lexical 
items (arranging individual signs into sentences).  
The signer may also associate entities under dis-
cussion with locations in space around their body; 
these locations are used in pronominal reference 
(pointing to a location) or verb agreement (aiming 
the motion path of a verb sign to/from a location).   
During CPs, signers? hands draw a 3D scene in 
the space in front of their torso.  One could imag-
ine invisible placeholders floating in front of a 
signer representing real-world objects in a scene.  
To represent each object, the signer places his/her 
hand in a special handshape (used specifically for 
objects of that semantic type: moving vehicles, 
seated animals, upright humans, etc.).  The hand is 
moved to show a 3D location, movement path, or 
surface contour of the object being described. For 
example, to convey the English sentence ?the car 
parked next to the house,? signers would indicate a 
location in space to represent the house using a 
special handshape for ?bulky objects.?  Next, they 
would use a ?moving vehicle? handshape to trace a 
3D path for the car which stops next to the house. 
37
1.2 Previous ASL MT Systems 
There have been some previous English-to-ASL 
MT projects ? see survey in (Huenerfauth, 2003).  
Amid other limitations, none of these systems ad-
dress how to produce the 3D locations and motion 
paths needed for CPs.  A fluent, useful English-to-
ASL MT system cannot ignore CPs.  ASL sign-
frequency studies show that signers produce a CP 
from 1 to 17 times per minute, depending on genre 
(Morford and MacFarlane, 2003).  Further, it is 
those English sentences whose ASL translation 
uses a CP that a deaf user with low English literacy 
would need an MT system to translate.  These Eng-
lish sentences look structurally different than their 
ASL CP counterpart ? often making the English 
sentence difficult to read for a deaf user. 
2 ASL NLG: A Form of Multimodal NLG 
NLG researchers think of communication signals 
in a variety of ways: some as a written text, other 
as speech audio (with prosody, timing, volume, 
and intonation), and those working in Multimodal 
NLG as text/speech with coordinated graphics 
(maps, charts, diagrams, etc).  Some Multimodal 
NLG focuses on ?embodied conversational agents? 
(ECAs), computer-generated animated characters 
that communicate with users using speech, eye 
gaze, facial expression, body posture, and gestures 
(Cassell et al, 2000; Kopp et al, 2004).   
The output of any NLG system could be repre-
sented as a stream of values (or features) that 
change over time during a communication signal; 
some NLG systems specify more values than oth-
ers.  Because the English writing system does not 
record a speaker?s prosody, facial expression or 
gesture1, a text-based NLG system specifies fewer 
communication stream values in its output than 
does a speech-based or ECA system.  A text-based 
NLG system requires literate users, to whom it can 
transfer some of the processing burden; they must 
mentally reconstruct more of the language per-
formance than do users of speech or ECA systems. 
Since most writing systems are based on strings, 
text-based NLG systems can easily encode their 
output as a single stream, namely a sequence of 
                                                        
1
 Some punctuation marks loosely correspond to intonation or 
pauses, but most prosodic information is lost.  Facial expres-
sion and gesture is generally not conveyed in writing, except 
perhaps for the occasional use of ?emoticons.?  ;-) 
words/characters.  To generate more complex sig-
nals, multimodal systems decompose their output 
into several sub-streams ? we?ll refer to these as 
?channels.?  Dividing a communication signal into 
channels can make it easier to represent the various 
choices the generator must make; generally, a dif-
ferent processing component of the system will 
govern the output of each channel.  The trade-off is 
that these channels must be coordinated over time.   
Instead of thinking of channels as dividing a 
communication signal, we can think of them as 
groupings of individual values in the data stream 
that are related in some way.  The channels of a 
multimodal NLG system generally correspond to 
natural perceptual/conceptual groupings called 
?modalities.?  Coarsely, audio and visual parts of 
the output are thought of as separate modalities.  
When parts of the output appear on different por-
tions of the display, then they are also generally 
considered separate modalities.  For instance, a 
multimodal NLG system for automobile driving 
directions may have separate processing channels 
for text, maps, other graphics, and sound effects.  
An ECA system may have separate channels for 
eye gaze, facial expression, manual gestures, and 
speech audio of the animated character.   
When a language has no commonly-known writ-
ing system ? as is the case for ASL ? then it?s not 
possible to build a text-based NLG system.  We 
must produce an animation of a character (like an 
ECA) performing ASL; so, we must specify how 
the hands, eye gaze, mouth shape, facial expres-
sion, head-tilt, and shoulder-tilt are coordinated 
over time.  With no conventional string-encoding 
of ASL (that would compress the signal into a sin-
gle stream), an ASL signal is spread over multiple 
channels of the output ? a departure from most 
Multimodal NLG systems, which have a single 
linguistic channel/modality that is coordinated with 
other non-linguistic resources (Figure 1). 
Figure 1: Linguistic Channels in Multimodal Systems 
English Text 
Driving Maps 
Other Graphics 
Prototypical Driving- 
Direction System 
Sound Effects 
Left Hand  
Head-Tilt 
Eye-Gaze 
Facial Expression 
Right Hand  
Prototypical ASL System 
Linguistic
 Ch
an
n
els
 
38
Of course, we could invent a string-based nota-
tion for ASL so that we could use traditional text-
based NLG technology.  (Since ASL has no writ-
ing system, we would have to invent an artificial 
notation.)  Unfortunately, since the users of the 
system wouldn?t be trained in this new writing sys-
tem, it could not be used as output; we would still 
need to generate a multimodal animation output. 
An artificial writing system could only be used for 
internal representation and processing, However, 
flattening a naturally multichannel signal into a 
single-channel string (prior to generating a mul-
tichannel output) can introduce its own complica-
tions to the ASL system?s design.  For this reason, 
this project has been exploring ways to represent 
the hierarchical linguistic structure of information 
on multiple channels of ASL performance (and 
how these structures are coordinated or uncoordi-
nated across channels over time). 
Some multimodal systems have explored using 
linguistic structures to control (to some degree) the 
output of multiple channels.  Research on generat-
ing animations of a speaking ECA character that 
performs meaningful gestures (Kopp et al, 2004) 
has similarities to this ASL project.  First of all, the 
channels in the signal are basically the same; an 
animated human-like character is shown onscreen 
with information about eye, face, and arm move-
ments being generated.  However, an ASL system 
has no audio speech channel but potentially more 
fine-grained channels of detailed body movement. 
The less superficial similarity is that (Kopp et. 
al, 2004) have attempted to represent the semantic 
meaning of some of the character?s gestures and to 
synchronize them with the speech output.  This 
means that, like an ASL NLG system, several 
channels of the signal are being governed by the 
linguistic mechanisms of a natural language.  
Unlike ASL, the gesture system uses the speech 
audio channel to convey nearly all of the meaning 
to the user; the other channels are generally used to 
convey additional/redundant information.  Further, 
the internal structure of the gestures is not gener-
ally encoded in the system; they are typically 
atomic/lexical gesture events which are synchro-
nized to co-occur with portions of speech output.  
A final difference is that gestures which co-occur 
with English speech (although meaningful) can be 
somewhat vague and are certainly less systematic 
and conventional than ASL body movements.  So, 
while both systems may have multiple linguistic 
channels, the gesture system still has one primary 
linguistic channel (audio speech) and a few chan-
nels controlled in only a partially linguistic way. 
3 This English-to-ASL MT Design 
The linguistic and multimodal issues discussed 
above have had important consequences on the 
design of our English-to-ASL MT system.  There 
are several unique features of this system caused 
by: (1) ASL having multiple linguistic channels 
that must be coordinated during generation, (2) 
ASL having both an LS and a CP form of signing, 
(3) CP signing visually conveying 3D spatial rela-
tionships in front of the signer?s torso, and (4) ASL 
lacking a conventional written form.  While ASL-
particular factors influenced this design, section 5 
will discuss how this design has implications for 
NLG of traditional written/spoken languages. 
3.1 Coordinating Linguistic Channels 
Section 2 mentioned that this project is developing 
multichannel (non-string) encodings of ASL ani-
mation; these encodings must coordinate multiple 
channels of the signal as they are generated by the 
linguistic structures and rules of ASL.  Kopp et al 
(2004) have explored how to coordinate meaning-
ful gestures with speech signal during generation; 
however, their domain is somewhat simpler.  Their 
gestures are atomic events without internal hierar-
chical structure.  Our project is currently develop-
ing grammar-like coordination formalisms that 
allow complex linguistic signals on multiple chan-
nels to be conveniently represented.2 
3.2 ASL Computational Linguistic Models 
This project uses representations of discourse, se-
mantics, syntax, and (sign) phonology tailored to 
ASL generation (Huenerfauth, 2004b).  In particu-
lar, since this MT system will generate animations 
of classifier predicates (CPs), the system consults a 
3D model of real-world scenes under discussion.  
Further, since multimodal NLG requires a form of 
scheduling (events on multiple channels are coor-
dinated over a performance timeline), all of the 
linguistic models consulted and modified during 
ASL generation are time-indexed according to a 
timeline of the ASL performance being produced. 
                                                        
2
 Details of this work will be described in future publication. 
39
Previous ASL phonological models were de-
signed to represent non-CP ASL, but CPs use a 
reduced set of handshapes, standard eye-gaze and 
head-tilt patterns, and more complex orientations 
and motion paths.  The phonological model devel-
oped for this system makes it easier to specify CPs. 
Because ASL signers can use the space in front 
of their body to visually convey information, it is 
possible during CPs to show the exact 3D layout of 
objects being discussed.  (The use of channels rep-
resenting the hands means that we can now indi-
cate 3D visual information ? not possible with 
speech or text.)  To represent this 3D detailed form 
of meaning, this system has an unusual semantic 
model for generating CPs.  We populate the vol-
ume of space around the signer?s torso with invisi-
ble 3D objects representing entities discussed by 
CPs being generated (Huenerfauth, 2004b).  The 
semantic model is the set of placeholders around 
the signer (augmented with the CP handshape used 
for each).   Thus, the semantics of the ?car parked 
next to the house? example (section 1.1) is that a 
?bulky? object occupies a particular 3D location 
and a ?vehicle? object moves toward it and stops. 
Of course, the system will also need more tradi-
tional semantic representations of the information 
to be conveyed during generation, but this 3D 
model helps the system select the proper 3D mo-
tion paths for the signers? hands when ?drawing? 
the 3D scenes during CPs.  The work of (Kopp et 
al., 2004) studies gestures to convey spatial infor-
mation during an English speech performance, but 
unlike this system, they use a logical-predicate-
based semantics to represent information about 
objects referred to by gesture.  Because ASL CPs 
indicate 3D layout in a linguistically conventional 
and detailed way, we use an actual 3D model of 
the objects being discussed.  Such a 3D model may 
also be useful for ECA systems that wish to gener-
ate more detailed 3D spatial gesture animations.   
The discourse model in this ASL system records 
features not found in other NLG systems.  It tracks 
whether a 3D location has been assigned to each 
discourse entity, where that location is around the 
signer, and whether the latest location of the entity 
has been indicated by a CP.  The discourse model 
is not only relevant during CP performance; since 
ASL LS performance also assigns 3D locations to 
objects under discussion (for pronouns and verbal 
agreement), this model is also used for LS. 
3.3 Generating 3D Classifier Predicates 
An essential step in producing an animation of an 
ASL CP is the selection of 3D motion paths for the 
computer-generated signer?s hands, eye gaze, and 
head tilt.  The motion paths of objects in the 3D 
model described above are used to select corre-
sponding motion paths for these parts of the 
signer?s body during CPs.  To build the 3D place-
holder model, this system uses preexisting scene-
visualization software to analyze an English text 
describing the motion of real-world objects and 
build a 3D model of how the objects mentioned in 
text are arranged and move (Huenerfauth, 2004b).  
This model is ?overlaid? onto the volume in front 
of the ASL signer (Figure 2).  For each object in 
the scene, a corresponding invisible placeholder is 
positioned in front of the signer; the layout of 
placeholders mimics the layout of objects in the 3D 
scene.  In the ?car parked next to the house? exam-
ple, a miniature invisible object representing a 
?house? is positioned in front of the signer?s torso, 
and another object (with a motion path terminating 
next to the ?house?) is added to represent the ?car.?   
The locations and orientations of the placehold-
ers are later used by the system to select the loca-
tions and orientations for the signer?s hands while 
performing CPs about them.  So, the motion path 
calculated for the car will be the basis for the 3D 
motion path of the signer?s hand during the classi-
fier predicate describing the car?s motion. Given 
the information in the discourse/semantic models, 
the system generates the hand motions, head-tilt, 
and eye-gaze for a CP.  It stores a library contain-
ing templates representing a prototypical form of 
each CP the system can produce.  The templates 
TEXT: 
THE CAR 
PARKED NEXT 
TO THE HOUSE.
Visualization
Software
3D MODEL:
Overlay in
front of ASL
signer
Convert to 3D
placeholder
locations/paths
Figure 2: Converting English Text to 3D Placeholder
40
are planning operators (with logical pre-conditions, 
monitored termination conditions, and effects), 
allowing the system to ?trigger? other elements of 
ASL signing performance that may be required 
during a CP.  A planning-based NLG approach, 
described in (Huenerfauth, 2004b), is used to select 
a template, fill in its missing parameters, and build 
a schedule of the animation events on multiple 
channels needed to produce a sequence of CPs.   
3.4 A Multi-Path Architecture 
A multimodal NLG system may have several pres-
entation styles it could use to convey information 
to its user; these styles may take advantage of the 
various output channels to different degrees.  In 
ASL, there are multiple channels in the linguistic 
portion of the signal, and not surprisingly, the lan-
guage has multiple sub-systems of signing that 
take advantage of the visual modality in different 
ways.  ASL signers can select whether to convey 
information using lexical signing (LS) or classifier 
predicates (CPs) during an ASL performance (sec-
tion 1.1).  These two sub-systems use the space 
around the signer differently; during CPs, locations 
in space associated with objects under discussion 
must be laid out in a 3D manner corresponding to 
the topological layout of the real-world scene un-
der discussion.  Locations associated with objects 
during LS (used for pronouns and verb agreement) 
have no topological requirement.  The layout of the 
3D locations during LS may be arbitrary. 
The CP generation approach in section 3.3 is 
computationally expensive; so, we would only like 
to use this processing pathway when necessary.  
English input sentences not producing classifier 
predicates would not need to be processed by the 
visualization software; in fact, most of these sen-
tences could be handled using the more traditional 
MT technologies of previous systems.  For this 
reason, our English-to-ASL MT system has multi-
ple processing pathways (Huenerfauth, 2004a).  
The pathway for handling English input sentences 
that produce CPs includes the scene visualization 
software, while other input sentences undergo less 
sophisticated processing using a traditional MT 
approach (that is easier to implement).  In this way, 
our CP generation component can actually be lay-
ered on top of a pre-existing English-to-ASL MT 
system to give it the ability to produce CPs. This 
multi-path design is equally applicable to the archi-
tecture of written-language MT systems.  The de-
sign allows an MT system to combine a resource-
intensive deep-processing MT method for difficult 
(or important) inputs and a resource-light broad-
coverage MT method for other inputs.   
3.5 Evaluation of Multichannel NLG 
The lack of an ASL writing system and the mul-
tichannel nature of ASL can make NLG or MT 
systems which produce ASL animation output dif-
ficult to evaluate using traditional automatic tech-
niques.  Many such approaches compare a string 
produced by a system to some human-produced 
?gold-standard? string.  While we could invent an 
artificial ASL writing system for the system to 
produce as output, it?s not clear that human ASL 
signers could accurately or consistently produce 
written forms of ASL sentences to serve as ?gold 
standards? for such an evaluation.  And of course, 
real users of the system would never be shown arti-
ficial ?written ASL?; they would see full anima-
tions instead.  User-based studies (where ASL 
signers evaluate animation output directly) may be 
a more meaningful measure of an ASL system. 
We are planning such an evaluation of a proto-
type CP-generation module of the system during 
the summer/fall of 2005.  Members of the deaf 
community who are native ASL signers will view 
animations of classifier predicates produced by the 
system.  As a control, they will also be shown an-
imations of CPs produced using 3D motion capture 
technology to digitally record the performance of 
CPs by other native ASL signers.  Their evaluation 
of animations from both sources will be compared 
to measure the system?s performance.  The mul-
tichannel nature of the signal also makes other in-
teresting experiments possible.  To study the 
system?s ability to animate the signer?s hands only, 
motion-captured ASL could be used to animate the 
head/body of the animated character, and the NLG 
system can be used to control only the hands of the 
character.  Thus, channels of the NLG system can 
be isolated for evaluation ? an experimental design 
only available to a multichannel NLG system. 
4 Unique Design Features for ASL NLG 
The design portion of this English-to-ASL project 
is nearly complete, and the implementation of the 
system is ongoing.  Evaluations of the system will 
41
be available after the user-based study discussed 
above; however, the design itself has highlighted 
interesting issues about the requirements of NLG 
software for sign languages like ASL.   
The multichannel nature of ASL has led this 
project to study mechanisms for coordinating the 
values of the linguistic models used during genera-
tion (including the output animation specification 
itself).  The need to handle both the LS and CP 
subsystems of the language has motivated: a multi-
path MT architecture, a discourse model that stores 
data relevant to both subsystems, a model of the 
space around the signer capable of storing both LS 
and CP placeholders, and a phonological model 
whose values can be specified by either subsystem.   
Since this English-to-ASL MT system is the first 
to address ASL classifier predicates, designing an 
NLG process capable of producing the 3D loca-
tions and paths in a CP animation has been a major 
design focus for this project.  These issues have 
been addressed by the system?s use of a 3D model 
of placeholders produced by scene-visualization 
software and a planning-based NLG process oper-
ating on templates of prototypical CP performance. 
5 Applications Beyond Sign Language 
Sign language NLG requires 3D spatial representa-
tions and multichannel coordinated output, but it?s 
not unique in this requirement.  In fact, generation 
of a communication signal for any language may 
require these capabilities (even for spoken lan-
guages like English).  We have mentioned 
throughout this paper how gesture/speech ECA 
researchers may be interested in NLG technologies 
for ASL ? especially if they wish to produce ges-
tures that are more linguistically conventional, in-
ternally complex, or 3D-topologically precise.   
Many other computational linguistic applica-
tions could benefit from an NLG design with mul-
tiple linguistic channels (and indirectly benefit 
from ASL NLG technology).  For instance, NLG 
systems producing speech output could encode 
prosody, timing, volume, intonation, or other vocal 
data as multiple linguistically-determined channels 
of the output (in addition to a channel for the string 
of words being generated).  And so, ASL NLG 
research not only has exciting accessibility benefits 
for deaf users, but it also serves as a research vehi-
cle for NLG technology to produce a variety of 
richer-than-text linguistic communication signals. 
Acknowledgments  
I would like to thank my advisors Mitch Marcus 
and Martha Palmer for their guidance, discussion, 
and revisions during the preparation of this work. 
References 
Cassell, J., Sullivan, J., Prevost, S., and Churchill, E. 
(Eds.). 2000. Embodied Conversational Agents. 
Cambridge, MA: MIT Press. 
Holt, J. 1991. Demographic, Stanford Achievement Test 
- 8th Edition for Deaf and Hard of Hearing Students: 
Reading Comprehension Subgroup Results. 
Huenerfauth, M. 2003. Survey and Critique of ASL 
Natural Language Generation and Machine Transla-
tion Systems. Technical Report MS-CIS-03-32, 
Computer and Information Science, University of 
Pennsylvania. 
Huenerfauth, M. 2004a. A Multi-Path Architecture for 
Machine Translation of English Text into American 
Sign Language Animation. In Proceedings of the 
Student Workshop of the Human Language Tech-
nologies conference / North American chapter of the 
Association for Computational Linguistics annual 
meeting: HLT/NAACL 2004, Boston, MA, USA. 
Huenerfauth, M. 2004b. Spatial and Planning Models of 
ASL Classifier Predicates for Machine Translation. 
10th International Conference on Theoretical and 
Methodological Issues in Machine Translation: TMI 
2004, Baltimore, MD. 
Huenerfauth, M. 2005. American Sign Language Spatial 
Representations for an Accessible User-Interface.  In 
3rd International Conference on Universal Access in 
Human-Computer Interaction.  Las Vegas, NV, USA. 
Kopp, S., Tepper, P., and Cassell, J. 2004. Towards 
Integrated Microplanning of Language and Iconic 
Gesture for Multimodal Output. Int?l Conference on 
Multimodal Interfaces, State College, PA, USA. 
Liddell, S. 2003. Grammar, Gesture, and Meaning in 
American Sign Language. UK: Cambridge U. Press. 
Mitchell, R. 2004. How many deaf people are there in 
the United States. Gallaudet Research Institute, Grad 
School & Prof. Progs. Gallaudet U.  June 28, 2004. 
http://gri.gallaudet.edu/Demographics/deaf-US.php 
Morford, J., and MacFarlane, J. 2003. Frequency Char-
acteristics of ASL.  Sign Language Studies, 3:2. 
Neidle, C., Kegl, J., MacLaughlin, D., Bahan, B., and 
Lee R.G. 2000. The Syntax of American Sign Lan-
guage: Functional Categories and Hierarchical Struc-
ture. Cambridge, MA: The MIT Press. 
42
Proceedings of the Workshop on Embodied Language Processing, pages 51?58,
Prague, Czech Republic, June 28, 2007. c?2007 Association for Computational Linguistics
Design and Evaluation of an American Sign Language Generator 
Matt Huenerfauth 
Computer Science Department 
CUNY Queens College 
The City University of New York 
65-30 Kissena Boulevard 
Flushing, NY 11375 USA 
matt@cs.qc.cuny.edu 
Liming Zhao, Erdan Gu, Jan Allbeck 
Center for Human Modeling & Simulation 
University of Pennsylvania 
3401 Walnut Street 
Philadelphia, PA 19104 USA 
{liming,erdan,allbeck} 
@seas.upenn.edu 
 
 
Abstract 
We describe the implementation and 
evaluation of a prototype American Sign 
Language (ASL) generation component 
that produces animations of ASL classifier 
predicates, some frequent and complex 
spatial phenomena in ASL that no previous 
generation system has produced.  We dis-
cuss some challenges in evaluating ASL 
systems and present the results of a user-
based evaluation study of our system. 
1 Background and Motivations 
American Sign Language (ASL) is a natural lan-
guage with a linguistic structure distinct from Eng-
lish used as a primary means of communication for 
approximately one half million people in the U.S. 
(Mitchell et al, 2006).  A majority of deaf 18-year-
olds in the U.S. have an English reading level be-
low that of an average 10-year-old hearing student 
(Holt, 1991), and so software to translate English 
text into ASL animations can improve many peo-
ple?s access to information, communication, and 
services.  Previous English-to-ASL machine trans-
lation projects (S?f?r & Marshall, 2001; Zhou et 
al., 2000) could not generate classifier predicates 
(CPs), phenomena in which signers use special 
hand movements to indicate the location and 
movement of invisible objects in space around 
them (representing entities under discussion). Be-
cause CPs are frequent in ASL and necessary for 
conveying many concepts, we have developed a 
CP generator that can be incorporated into a full 
English-to-ASL machine translation system. 
During a CP, signers use their hands to position, 
move, trace, or re-orient imaginary objects in the 
space in front of them to indicate the location, 
movement, shape, contour, physical dimension, or 
some other property of corresponding real world 
entities under discussion.  CPs consist of a seman-
tically meaningful handshape and a 3D hand 
movement path. A handshape is chosen from a 
closed set based on characteristics of the entity de-
scribed (whether it be a vehicle, human, animal, 
etc.) and what aspect of the entity the signer is de-
scribing (surface, position, motion, etc).   For ex-
ample, the sentence ?the car parked between the 
cat and the house? could be expressed in ASL us-
ing 3 CPs.  First, a signer performs the ASL sign 
HOUSE while raising her eyebrows (to introduce a 
new entity as a topic).  Then, she moves her hand 
in a ?Spread C? handshape (Figure 1) forward to a 
point in space where a miniature house could be 
envisioned.  Next, the signer performs the sign 
CAT with eyebrows raised and makes a similar 
motion with a ?Hooked V? handshape to a location 
where a cat could be imagined.  Finally, she per-
forms the sign CAR (with eyebrows raised) and 
uses a ?Number 3? handshape to trace a path that 
stops at between the ?house? and the ?cat.?  Her 
other hand makes a flat surface for the ?car? to park 
on.  (Figure 3 will show our system?s animation.) 
    
Figure 1: ASL handshapes: Spread C (bulky object), 
Number 3 (vehicle), Hooked V (animal), Flat (surface). 
51
2 System Design and Implementation  
We have built a prototype ASL generation module 
that could be incorporated into an English-to-ASL 
machine translation system.  When given a 3D 
model of the arrangement of a set of objects whose 
location and movement should be described in 
ASL, our system produces an animation of ASL 
sentences containing classifier predicates to de-
scribe the scene.  Classifier predicates are the way 
such spatial information is typically conveyed in 
ASL.  Since this is the first ASL generation system 
to produce classifier predicate sentences (Huener-
fauth, 2006b), we have also conducted an evalua-
tion study in which native ASL signers compared 
our system's animations to the current state of the 
art: Signed English animations (described later). 
2.1 Modeling the Use of Space 
To produce classifier predicates and other ASL 
expressions that associate locations in space 
around a signer with entities under discussion, an 
English-to-ASL system must model what objects 
are being discussed in an English text, and it must 
map placeholders for these objects to locations in 
space around the signer?s body.  The input to our 
ASL classifier predicate generator is an explicit 3D 
model of how a set of placeholders representing 
discourse entities are positioned in the space 
around the signing character?s body (Huenerfauth, 
2006b).  This 3D model is ?mapped? onto a vol-
ume of space in front of the signer?s torso, and this 
model is used to guide the motion of the ASL 
signer?s hands during the performance of classifier 
predicates describing the motion of these objects 
The model encodes the 3D location (center-of-
mass) and orientation values of the set of objects 
that we want to our system describe using ASL 
animation.  For instance, to generate the ?car park-
ing between the cat and the house? example, we 
would pass our system a model with three sets of 
location (x, y, z coordinates) and orientation (x, y, 
z, rotation angles) values: for the cat, the car, and 
the house.  Each 3D placeholder also includes a set 
of bits that represent the set of possible ASL classi-
fier handshapes that can be used to describe it. 
While this 3D model is given as input to our 
prototype classifier predicate generator, when part 
of a full generation system, virtual reality ?scene 
visualization? software can be used to produce a 
3D model of the arrangement and movement of 
objects discussed in an English input text (Badler 
et al, 2000; Coyne and Sproat, 2001).   
2.2 Template-Based Planning Generation 
Given the 3D model above, the system uses a 
planning-based approach to determine how to 
move the signer?s hands, head-tilt, and eye-gaze to 
produce an animation of a classifier predicate.  The 
system stores a library of templates representing 
the various kinds of classifier predicates it may 
produce.  These templates are planning operators 
(they have logical pre-conditions, monitored ter-
mination conditions, and effects), allowing the sys-
tem to trigger other elements of ASL signing per-
formance that may be required during a grammati-
cally correct classifier predicate (Huenerfauth, 
2006b).  Each planning operator is parameterized 
on an object in the 3D model (and its 3D coordi-
nates); for instance, there is a templated planning 
operator for generating an ASL classifier predicate 
to show a ?parking? event.  The specific loca-
tion/orientation of the vehicle that is parking would 
be the parameter passed to the planning operator.   
There is debate in the ASL linguistics commu-
nity about the underlying structure of classifier 
predicates and the generation process by which 
signers produce them.  Our parameterized template 
approach mirrors one recent linguistic model (Lid-
dell, 2003), and the implementation and evaluation 
of our prototype generator will help determine 
whether this was a good choice for our system. 
2.3 Multi-Channel Syntax Representation 
While strings and syntax trees are used to represent 
written languages inside of NLP software, these 
encodings are difficult to adapt to a sign language.  
ASL lacks a standard writing system, and the mul-
tichannel nature of an ASL performance makes it 
difficult to encode in a linear single-channel string.  
This project developed a new formalism for repre-
senting a linguistic signal in a multi-channel man-
ner and for encoding temporal coordination and 
non-coordination relationships between portions of 
the signal (Huenerfauth, 2006a).  The output of our 
planner is a tree-like structure that represents the 
animation to be synthesized.  The tree has two 
kinds of non-terminal nodes: some indicate that 
their children should be performed in sequence 
(like a traditional linguistic syntax tree), and some 
non-terminals indicate that their children should be 
performed in parallel (e.g. one child subtree may 
52
specify the movement of the arms, and another, the 
facial expression).  In this way, the structure can 
encode how multiple parts of the sign language 
performance should be coordinated over time 
while still leaving flexibility to the exact timing of 
events ? see Figure 2.  In earlier work, we have 
argued that this representation is sufficient for en-
coding ASL animation (Huenerfauth, 2006a), and 
the implementation and evaluation of our system 
(using this formalism) will help test this claim. 
 
Figure 2: A multichannel representation for the sentence 
?The cat is next to the house.?  This example shows 
handshape, hand location, and eye gaze direction ? 
some details omitted from the example: hand orienta-
tion, head tilt, and brow-raising. Changes in timing of 
individual animation events causes the structure to 
stretch in the time dimension (like an HTML table). 
2.4 Creating Virtual Human Animation 
After planning, the system has a tree-structure that 
specifies activities for parts of the signer?s body.  
Non-terminal nodes indicate whether their children 
are performed in sequence or in parallel, and the 
terminal nodes (the inner rectangles in Figure 2) 
specify animation events for a part of the signer?s 
body.  Nodes? time durations are not yet specified 
(since the human animation component would 
know the time that movements require, not the lin-
guistic planner).  So, the generator queries the hu-
man animation system to calculate an estimated 
time duration for each body animation event (each 
terminal node), and the structure is then ?balanced? 
so that if several events are meant to occur in par-
allel, then the shorter events are ?stretched out.?  
(The linguistic system can set max/min times for 
some events prior to the animation processing.) 
2.5 Eye-Gaze and Brow Control 
The facial model is implemented using the Greta 
facial animation engine (Pasquariello and Pela-
chaud, 2001).  Our model controls the motion of 
the signer?s eye-brows, which can be placed in a 
?raised? or ?flat? position. The eye motor control 
repertoire contains three behaviors: fixation on a 
3D location in space around the signer?s body, 
smooth pursuit of a moving 3D location, and eye-
blinking.  Gaze direction is computed from the lo-
cation values specified inside the 3D model, and 
the velocity and time duration of the movement are 
determined by the timing values inside the tree-
structure output from the planner.  The signer?s 
head tilt changes to accommodate horizontal or 
vertical gaze shifts greater than a set threshold.  
When performing a ?fixation? or ?smooth pursuit? 
with the eye-gaze, the rate of eye blinking is de-
creased.  Whenever the signer?s eye-gaze is not 
otherwise specified for the animation performance, 
the default behavior is to look at the audience. 
2.6 Planning Arm Movement 
Given the tree-structure with animation events, the 
output of arm-planning should be a list of anima-
tion frames that completely specify the rotation 
angles of the joints of the signer?s hands and arms.  
The hand is specified using 20 rotation angles for 
the finger joints, and the arm is specified using 9 
rotation angles: 2 for the clavicle joint, 3 for the 
shoulder joint, 1 for the elbow joint, and 3 for the 
wrist.  The linguistic planner specifies the hand-
shape that should be used for specific classifier 
predicates; however, the tree-structure specifies the 
arm movements by giving a target location for the 
center of the signer?s palm and a target orientation 
value for the palm.  The system must find a set of 
clavicle, shoulder, elbow, and wrist angles that get 
the hand to this desired location and palm orienta-
tion.  In addition to reaching this target, the arm 
pose for each animation frame must be as natural 
as possible, and the animation between frames 
must be smooth.  The system uses an inverse 
kinematics (IK) which automatically favors natural 
arm poses.  Using the wrist as the end-effector, an 
elbow angle is selected based on the distance from 
shoulder to the target, and this elbow angle is 
fixed.  We next compute a set of possible shoulder 
and wrist rotation angles in order to align the 
signer?s hand with the target palm orientation.  
Disregarding elbow angles that force impossible 
wrist joint angles, we select the arm pose that is 
collision free and is the most natural, according to 
a shoulder strength model (Zhao et al, 2005). 
dominant 
hand shape Hook V
dominant 
hand location 
to cat 
location
eye gaze audience to house location audience 
to cat 
location
non-dominant 
hand location
to house 
location 
non-dominant 
hand shape Spread C 
?
ASL  
Noun  
Sign:  
HOUSE 
?
?
ASL  
Noun  
Sign:  
HOUSE 
ASL  
Noun  
Sign:  
CAT 
time 
53
2.7 Synthesizing Virtual Human Animation 
This animation specification is performed by an 
animated human character in the Virtual Human 
Testbed (Badler et al, 2005).  Because the Greta 
system used a female head with light skin tone, a 
female human body was chosen with matching 
skin.  The character was dressed in a blue shirt and 
pants that contrasted with its skin tone.  To make 
the character appear to be a conversational partner, 
the ?camera? inside the virtual environment was 
set at eye-level with the character and at an appro-
priate distance for ASL conversation. 
2.8 Coverage of the Prototype System 
Our prototype system can be used to translate a 
limited range of English sentences (discussing the 
locations and movements of a small set of people 
or objects) into animations of an onscreen human-
like character performing ASL classifier predicates 
to convey the locations and movements of the enti-
ties in the English text.  Table 1 includes shorthand 
transcripts of some ASL sentence animations pro-
duced by the system; the first sentence corresponds 
to the classifier predicate animation in Figure 3. 
3 Issues in Evaluating ASL Generation 
There has been little work on developing evalua-
tion methodologies for sign language generation or 
MT systems. Some have shown how automatic 
string-based evaluation metrics fail to identify cor-
rect sign language translations (Morrisey and Way, 
2006), and they propose building large parallel 
written/sign corpora containing more syntactic and 
semantic information (to enable more sophisticated 
metrics to be created).  Aside from the expense of 
creating such corpora, we feel that there are several 
factors that motivate user-based evaluation studies 
for sign language generation systems ? especially 
for those systems that produce classifier predicates. 
These factors include some unique linguistic prop-
erties of sign languages and the lack of standard 
writing systems for most sign languages, like ASL. 
Figure 3: Images from our system?s animation of a classifier predicate for ?the car parked between the 
house and the cat.?  (a) ASL sign HOUSE, eyes at audience, brows raised; (b) Spread C handshape and 
eye gaze to house location; (c) ASL sign CAT, eyes at audience, brows raised; (d) Hooked V handshape 
and eye gaze to cat location; (e) ASL sign CAR, eyes at audience, brows raised; (f) Number 3 handshape 
(for the car) parks atop Flat handshape while the eye gaze tracks the movement path of the car. 
(a) (b) (c) 
(d) (e) (f) 
54
Most automatic evaluation approaches for gen-
eration or MT systems compare a string produced 
by a system to a human-produced ?gold-standard? 
string.  Sign languages usually lack written forms 
that are commonly used or known among signers.  
While we could invent an artificial ASL writing 
system for the generator to produce as output (for 
evaluation purposes only), it?s not clear that human 
ASL signers could accurately or consistently pro-
duce written forms of ASL sentences to serve as 
?gold standards? for such an evaluation.  Further, 
real users of the system would never be shown arti-
ficial written ASL; they would see animation out-
put.  Thus, evaluations based on strings would not 
test the full process ? including the synthesis of the 
?string? into an animation ? when errors may arise. 
Another reason why string-based evaluation 
metrics are not well-suited to ASL is that sign lan-
guages have linguistic properties that can confound 
string-edit-distance-based metrics.  ASL consists 
of the coordinated movement of several parts of 
the body in parallel (i.e. face, eyes, head, hands), 
and so a string listing the set of signs performed is 
a lossy representation of the original performance 
(Huenerfauth, 2006a).  The string may not encode 
the non-manual parts of the sentence, and so 
string-based metrics would fail to consider those 
important aspects.  Discourse factors (e.g. topicali-
zation) can also result in movement phenomena in 
ASL that may scramble the sequence of signs in 
the sentence without substantially changing its se-
mantics; such movement would affect string-based 
metrics significantly though the sentence meaning 
may change little.  The use of head-tilt and eye-
gaze during the performance of ASL verb signs 
may also license the dropping of entire sentence 
constituents (Neidle et. al, 2000).  The entities dis-
cussed are associated with locations in space 
around the signer at which head-tilt or eye-gaze is 
aimed, and thus the constituent is actually still ex-
pressed although no manual signs are performed 
for it.  Thus, an automatic metric may penalize 
such a sentence (for missing a constituent) while 
the information is still there.  Finally, ASL classi-
fier predicates convey a lot of information in a sin-
gle complex ?sign? (handshape indicates semantic 
category, movement shows 3D path/rotation), and 
it is unclear how we could ?write? the 3D data of a 
classifier predicate in a string-based encoding or 
how to calculate an edit-distance between a ?gold 
standard? classifier predicate and a generated one. 
4 Evaluation of the System 
We used a user-based evaluation methodology in 
which human native ASL signers are shown the 
output of our generator and asked to rate each ani-
mation on ten-point scales for understandability, 
naturalness of movement, and ASL grammatical 
correctness.  To evaluate whether the animation 
conveyed the proper semantics, signers were also 
asked to complete a matching task.  After viewing 
a classifier predicate animation produced by the 
system, signers were shown three short animations 
showing the movement or location of the set of 
objects that were described by the classifier predi-
cate.  The movement of the objects in each anima-
tion was slightly different, and signers were asked 
to select which of the three animations depicted the 
scene that was described by the classifier predicate. 
Since this prototype is the first generator to pro-
duce animations of ASL classifier predicates, there 
are no other systems to compare it to in our study.  
To create a lower baseline for comparison, we 
wanted a set of animations that reflect the current 
state of the art in broad-coverage English-to-sign 
English Gloss ASL Sentence with Classifier Predicates (CPs) Signed English Sentence 
The car parks between the house and 
the cat. 
ASL sign HOUSE; CP: house location; sign CAT; CP: cat location; sign 
CAR; CP: car path. 
THE CAR PARK BETWEEN THE HOUSE 
AND THE CAT 
The man walks next to the woman. ASL sign WOMAN; CP: woman location; sign MAN; CP: man path. THE MAN WALK NEXT-TO THE 
WOMAN 
The car turns left. ASL sign CAR; CP: car path. THE CAR TURN LEFT 
The lamp is on the table. ASL sign TABLE; CP: table location; sign LIGHT; CP: lamp location. THE LIGHT IS ON THE TABLE 
The tree is near the tent. ASL sign TENT; CP: tent location; sign TREE; CP: tree location. THE TREE IS NEAR THE TENT 
The man walks between the tent and 
the frog. 
ASL sign TENT; CP: tent location; sign FROG; CP: frog location; sign 
MAN; CP: man path. 
THE MAN WALK BETWEEN THE TENT 
AND THE FROG 
The man walks away from the 
woman. 
ASL sign WOMAN: CP: woman location; sign MAN; CP: man path. THE MAN WALK FROM THE WOMAN 
The car drives up to the house. ASL sign HOUSE; CP: house location; sign CAR; CP: car path. THE CAR DRIVE TO THE HOUSE 
The man walks up to the woman. ASL sign WOMAN; CP: woman location; sign MAN; CP: man path. THE MAN WALK TO THE WOMAN 
The woman stands next to the table. ASL sign TABLE; CP: table location; sign WOMAN; CP: woman 
location. 
THE WOMAN STAND NEXT-TO THE 
TABLE 
 Table 1: ASL and Signed English sentences included in the evaluation study (with English glosses). 
55
translation.  Since there are no broad-coverage 
English-to-ASL MT systems, we used Signed Eng-
lish transliterations as our lower baseline.  Signed 
English is a form of communication in which each 
word of an English sentence is replaced with a cor-
responding sign, and the sentence is presented in 
original English word order without any accompa-
nying ASL linguistic features such as meaningful 
facial expressions or eye-gaze. 
Ten ASL animations (generated by our system) 
were selected for inclusion in this study based on 
some desired criteria.  The ASL animations consist 
of classifier predicates of movement and location ? 
the focus of our research.  The categories of people 
and objects discussed in the sentences require a 
variety of ASL handshapes to be used. Some sen-
tences describe the location of objects, and others 
describe movement.  The sentences describe from 
one to three objects in a scene, and some pairs of 
sentences actually discuss the same set of objects, 
but moving in different ways.  Since the creation of 
a referring expression generator was not a focus of 
our prototype, all referring expressions in the an-
imations are simply an ASL noun phrase consist-
ing of a single sign ? some one-handed and some 
two-handed.  Table 1 lists the ten classifier predi-
cate animations we selected (with English glosses). 
For the ?matching task? portion of the study, 
three animated visualizations were created for each 
sentence showing how the objects mentioned in the 
sentence move in 3D.  One animation was an accu-
rate visualization of the location/movement of the 
objects, and the other two animations were ?con-
fusables? ? showing orientations/movements for 
the objects that did not match the classifier predi-
cate animations.  Because we wanted to evaluate 
the classifier predicates (and not the referring ex-
pressions), the set of objects that appeared in all 
three visualizations for a sentence was the same.  
Thus, it was the movement and orientation infor-
mation conveyed by the classifier predicate (and 
not the object identity conveyed by the referring 
expression) that would distinguish the correct visu-
alization from the confusables.  For example, the 
following three visualizations were created for the 
sentence ?the car parks between the cat and the 
house? (the cat and house remain in the same loca-
tion in each): (1) a car drives on a curved path and 
parks at a location between a house and a cat, (2) a 
car drives between a house and a cat but continues 
driving past them off camera, and (3) a car starts at 
a location between a house and a cat and drives to 
a location that is not between them anymore. 
To create the Signed English animations for 
each sentence, some additional signs were added to 
the generator?s library of signs.  (ASL does not 
traditionally use signs such as ?THE? that are used 
in Signed English.)  A sequence of signs for each 
Signed English transliteration was concatenated, 
and the synthesis sub-component of our system 
was used to calculate smooth transitional move-
ments for the arms and hands between each sign in 
the sentence.  The glosses for the ten Signed Eng-
lish transliterations are also listed in Table 1. 
4.1 User-Interface for Evaluation Study 
An interactive slideshow was created with one 
slide for each of the 20 animations (10 from our 
ASL system, 10 Signed English).  On each slide, 
the signing animation was shown on the left of the 
screen, and the three possible visualizations of that 
sentence were shown to the right (see Figure 4).  
The slides were placed in a random order for each 
of the participants in the study.  A user could re-
play the animations as many times as desired be-
fore going to the next signing animation.  Subjects 
were asked to rate each of these animations on a 1-
to-10-point scale for ASL grammatical correctness, 
understandability, and naturalness of movement.  
Subjects were also asked to select which of the 
three animated visualizations (choice ?A,? ?B,? or 
?C?) matched the scene as described in the sen-
tence performed by the virtual character. 
After these slides, 3 more slides appeared con-
taining animations from our generator.  (These 
were repeats of 3 animations used in the main part 
of the study.)  These three slides only showed the 
Video # 1 
Next 1 
CLICK TO START MOVIE 
A
B
C
CLICK TO START MOVIE
CLICK TO START MOVIE
CLICK TO START MOVIE
  Figure 4: Screenshot from evaluation program. 
56
?correct? animated visualization for that sentence.  
For these last three slides, subjects were instead 
asked to comment on the animation?s speed, col-
ors/lighting, hand visibility, correctness of hand 
movement, facial expression, and eye-gaze.  Sign-
ers were also asked to write any comments they 
had about how the animation should be improved. 
4.2 Recruitment and Screening of Subjects 
Subjects were recruited through personal contacts 
in the deaf community who helped identify friends, 
family, and associates who met the screening crite-
ria.  Participants had to be native ASL signers ? 
many deaf individuals are non-native signers who 
learned ASL later in life (and may accept English-
like signing as being grammatical ASL).  Subjects 
were preferred who had learned ASL since birth, 
had deaf parents that used ASL at home, and/or 
attending a residential school for the deaf as a child 
(where they were immersed in an ASL-signing 
community).  Of our 15 subjects, 8 met al three 
criteria, 2 met two criteria, and 5 met one (1 grew 
up with ASL-signing deaf parents and 4 attended a 
residential school for the deaf from an early age).   
During the study, instructions were given to par-
ticipants in ASL, and a native signer was present 
during 13 of the 15 sessions to answer questions or 
to explain experimental procedures.  This signer 
engaged the participants in conversation in ASL 
before the session to produce an ASL-immersive 
environment.  Participants were given instructions 
in ASL about how to score each category.  For 
grammaticality, they were told that ?perfect ASL 
grammar? would be a 10, but ?mixed-up? or ?Eng-
lish-like? grammar should be a 1.  For understand-
ability, ?easy to understand? sentences should be a 
10, but ?confusing? sentences should be a 1.  For 
naturalness, animations in which the signer moved 
?smoothly, like a real person? should be a 10, but 
animations in which the signer moved in a 
?choppy? manner ?like a robot? should be a 1. 
4.3 Results of the Evaluation  
Figure 5 shows average scores for grammaticality, 
understandability, naturalness, and matching-task-
success percentage for the animations from our 
system compared to the Signed English anima-
tions.  Our system?s higher scores in all categories 
is significant (? = 0.05, pairwise Mann-Whitney U 
tests with Bonferonni-corrected p-values). 
Subjects were asked to comment on the anima-
tion speed, color, lighting, visibility of the hands, 
correctness of hand movement, correctness of fa-
cial expressions, correctness of eye-gaze, and other 
ways of improving the animations.  Of the 15 sub-
jects, eight said that some animations were a little 
slow, and one felt some were very slow.  Eight 
subjects wanted the animations to have more facial 
expressions, and 4 of these specifically mentioned 
nose and mouth movements.  Four subjects said 
the signer?s body should seem more loose/relaxed 
or that it should move more.  Two subjects wanted 
the signer to show more emotion.  Two subjects 
felt that eye-brows should go higher when raised, 
and three felt there should be more eye-gaze 
movements.  Two subjects felt the blue color of the 
signer?s shirt was a little too bright, and one dis-
liked the black background.  Some subjects com-
mented on particular ASL signs that they felt were 
performed incorrectly.  For example, three dis-
cussed the sign ?FROG?: one felt it should be per-
formed a little more to the right of its current loca-
tion, and another felt that the hand should be ori-
ented with the fingers aimed more to the front.  
Some participants commented on the classifier 
predicate portions of the performance.  For exam-
ple, in the sentence ?the car parked between the cat 
and the house,? one subject felt it would be better 
to use the non-dominant hand to hold the location 
of the house during the car?s movement instead of 
using the non-dominant hand to create a platform 
for the dominant hand (the car) to park upon. 
5 Conclusions and Future Work 
Unlike an evaluation of a broad-coverage NLP sys-
tem, during which we obtain performance statistics 
Average Scores for Survey Questions
& Matching-Task-Success Percentage
0
10 
20 
30 
40 
50 
60 
70 
80 
90 
100
Grammatical Understandable Natural Matching Task
Our System 
Signed English
Figure 5: Grammaticality, understandability, natu-
ralness, and matching-task-success scores. 
57
for the system as it carries out a linguistic task on a 
large corpus or ?test set,? this paper has described 
an evaluation of a prototype system.  We were not 
measuring the linguistic coverage of the system but 
rather its functionality. Did signers agree that the 
animation output: (1) is actually a grammatically-
correct and understandable classifier predicate and 
(2) conveys the information about the movement 
of objects in the 3D scene being described?  We 
expected to find animation details that could be 
improved in future work; however, since there are 
currently no other systems capable of generating 
ASL classifier predicate animations, any system 
receiving an answer of ?yes? to questions (1) and 
(2) above is an improvement to the state of the art. 
Another contribution of this initial evaluation is 
that it serves as a pilot study to help us determine 
how to better evaluate sign language generation 
systems in the future.  We found that subjects were 
comfortable critiquing ASL animations, and most 
suggested specific (and often subtle) elements of 
the animation to be improved. Their feedback sug-
gested new modifications we can make to the sys-
tem (and then evaluate again in future studies).  
Because subjects gave such high quality feedback, 
future studies will also elicit such comments. 
During the study, we also experimented with re-
cording a native ASL signer (using a motion-
capture suit and datagloves) performing classifier 
predicates. We tried to use this motion-capture data 
to animate a virtual human character superficially 
identical to the one used by our system.  We hoped 
that this character controlled by human movements 
could serve as an upper-baseline in the evaluation 
study.  Unfortunately, the motion-capture data we 
collected contained minor errors that required post-
processing clean-up, and the resulting animations 
contained enough movement inaccuracies that na-
tive ASL signers who viewed them felt they were 
actually less understandable than our system's an-
imations.  In future work, we intend to explore al-
ternative upper-baselines to compare our system?s 
animations to: animation from alternative motion-
capture techniques, hand-coded animations based 
on a human?s performance, or simply a video of a 
human signer performing ASL sentences. 
Acknowledgements 
National Science Foundation Award #0520798 
?SGER: Generating Animations of American Sign 
Language Classifier Predicates? (Universal Access, 
2005) supported this work.  Software was donated 
by UGS Tecnomatix and Autodesk.  Thank you to 
Mitch Marcus, Martha Palmer, and Norman Badler.  
References 
N.I. Badler, J. Allbeck, S.J. Lee, R.J. Rabbitz, T.T. Broderick, 
and K.M. Mulkern. 2005. New behavioral paradigms for 
virtual human models. SAE Digital Human Modeling. 
N. Badler, R. Bindiganavale, J. Allbeck, W. Schuler, L. Zhao, 
S. Lee, H. Shin, & M. Palmer. 2000. Parameterized action 
representation & natural language instructions for dynamic 
behavior modification of embodied agents. AAAI Spring. 
R. Coyne and R. Sproat. 2001. WordsEye: an automatic text-
to-scene conversion system. ACM SIGGRAPH. 
J.A. Holt. 1991. Demographic, Stanford Achievement Test - 
8th Edition for Deaf and Hard of Hearing Students: Read-
ing Comprehension Subgroup Results.  
? . S?f?r & I. Marshall. 2001. The architecture of an English-
text-to-Sign-Languages translation system.  Recent Ad-
vances in Natural Language Processing. 
Matt Huenerfauth. In Press. Representing American Sign Lan-
guage classifier predicates using spatially parameterized 
planning templates. In M. Banich and D. Caccamise (Eds.), 
Generalization. Mahwah: LEA. 
Matt Huenerfauth. 2006a. Representing Coordination and 
Non-Coordination in American Sign Language Anima-
tions. Behaviour & Info. Technology, 25:4. 
Matt Huenerfauth. 2006b. Generating American Sign Lan-
guage Classifier Predicates for English-to-ASL Machine 
Translation.  Dissertation, U. Pennsylvania. 
Liddell, S. 2003. Grammar, Gesture, and Meaning in Ameri-
can Sign Language.  UK: Cambridge U. Press. 
R.E. Mitchell, T.A. Young, B. Bachleda, & M.A. Karchmer. 
2006. How Many People Use ASL in the United States? 
Why estimates need updating. Sign Language Studies, 6:3. 
S. Morrissey & A. Way. 2006. Lost in Translation: The prob-
lems of using mainstream MT evaluation metrics for sign 
language translation. 5th SALTMIL Workshop on Minority 
Languages, LREC-200 
C. Neidle, J. Kegl, D. MacLaughlin, B. Bahan, & R.G. Lee. 
2000. The Syntax of American Sign Language: Functional 
Categories and Hierarchical Structure. Cambridge: MIT. 
S. Pasquariello & C. Pelachaud. 2001. Greta: A simple facial 
animation engine. In 6th Online World Conference on Soft 
Computing in Industrial Applications.   
L. Zhao, K. Kipper, W. Schuler, C. Vogler, N.I. Badler, & M. 
Palmer. 2000. Machine Translation System from English to 
American Sign Language.  Assoc. for MT in the Americas. 
L. Zhao, Y. Liu, N.I. Badler. 2005. Applying empirical data 
on upper torso movement to real-time collision-free reach 
tasks. SAE Digital Human Modeling. 
58
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 229?237,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Cognitively Motivated Features for Readability Assessment 
 
 
 Lijun Feng  No?mie Elhadad  Matt Huenerfauth 
 The City University of New York,  Columbia University  The City University of New York, 
 Graduate Center  New York, NY, USA  Queens College & Graduate Center 
 New York, NY, USA  noemie@dbmi.columbia.edu New York, NY, USA 
 lijun7.feng@gmail.com  matt@cs.qc.cuny.edu 
 
  
 
Abstract 
We investigate linguistic features that correlate 
with the readability of texts for adults with in-
tellectual disabilities (ID).  Based on a corpus 
of texts (including some experimentally meas-
ured for comprehension by adults with ID), we 
analyze the significance of novel discourse-
level features related to the cognitive factors 
underlying our users? literacy challenges.  We 
develop and evaluate a tool for automatically 
rating the readability of texts for these users.  
Our experiments show that our discourse-
level, cognitively-motivated features improve 
automatic readability assessment. 
1 Introduction 
Assessing the degree of readability of a text has 
been a field of research as early as the 1920's. 
Dale and Chall define readability as ?the sum 
total (including all the interactions) of all those 
elements within a given piece of printed material 
that affect the success a group of readers have 
with it. The success is the extent to which they 
understand it, read it at optimal speed, and find it 
interesting? (Dale and Chall, 1949). It has long 
been acknowledged that readability is a function 
of text characteristics, but also of the readers 
themselves.  The literacy skills of the readers, 
their motivations, background knowledge, and 
other internal characteristics play an important 
role in determining whether a text is readable for 
a particular group of people. In our work, we 
investigate how to assess the readability of a text 
for people with intellectual disabilities (ID). 
Previous work in automatic readability as-
sessment has focused on generic features of a 
text at the lexical and syntactic level.  While such 
features are essential, we argue that audience-
specific features that model the cognitive charac-
teristics of a user group can improve the accura-
cy of a readability assessment tool.  The contri-
butions of this paper are: (1) we present a corpus 
of texts with readability judgments from adults 
with ID; (2) we propose a set of cognitively-
motivated features which operate at the discourse 
level; (3) we evaluate the utility of these features 
in predicting readability for adults with ID. 
Our framework is to create tools that benefit 
people with intellectual disabilities (ID), specifi-
cally those classified in the ?mild level? of men-
tal retardation, IQ scores 55-70.  About 3% of 
the U.S. population has intelligence test scores of 
70 or lower (U.S. Census Bureau, 2000).  People 
with ID face challenges in reading literacy.  They 
are better at decoding words (sounding them out) 
than at comprehending their meaning (Drew & 
Hardman, 2004), and most read below their men-
tal age-level (Katims, 2000).  Our research ad-
dresses two literacy impairments that distinguish 
people with ID from other low-literacy adults: 
limitations in (1) working memory and (2) dis-
course representation.  People with ID have 
problems remembering and inferring information 
from text (Fowler, 1998).  They have a slower 
speed of semantic encoding and thus units are 
lost from the working memory before they are 
processed (Perfetti & Lesgold, 1977; Hickson-
Bilsky, 1985).  People with ID also have trouble 
building cohesive representations of discourse 
(Hickson-Bilsky, 1985).  As less information is 
integrated into the mental representation of the 
current discourse, less is comprehended.   
Adults with ID are limited in their choice of 
reading material.  Most texts that they can readi-
ly understand are targeted at the level of reada-
bility of children.  However, the topics of these 
texts often fail to match their interests since they 
are meant for younger readers.  Because of the 
mismatch between their literacy and their inter-
ests, users may not read for pleasure and there-
fore miss valuable reading-skills practice time.  
In a feasibility study we conducted with adults 
229
with ID, we asked participants what they enjoyed 
learning or reading about.  The majority of our 
subjects mentioned enjoying watching the news, 
in particular local news.  Many mentioned they 
were interested in information that would be re-
levant to their daily lives.  While for some ge-
nres, human editors can prepare texts for these 
users, this is not practical for news sources that 
are frequently updated and specific to a limited 
geographic area (like local news). Our goal is to 
create an automatic metric to predict the reada-
bility of local news articles for adults with ID.  
Because of the low levels of written literacy 
among our target users, we intend to focus on 
comprehension of texts displayed on a computer 
screen and read aloud by text-to-speech software; 
although some users may depend on the text-to-
speech software, we use the term readability. 
This paper is organized as follows.  Section 2 
presents related work on readability assessment. 
Section 3 states our research hypotheses and de-
scribes our methodology.  Section 4 focuses on 
the data sets used in our experiments, while sec-
tion 5 describes the feature set we used for rea-
dability assessment along with a corpus-based 
analysis of each feature.  Section 6 describes a 
readability assessment tool and reports on evalu-
ation.  Section 7 discusses the implications of the 
work and proposes direction for future work. 
2 Related Work on Readability Metrics 
Many readability metrics have been established 
as a function of shallow features of texts, such as 
the number of syllables per word and number of 
words per sentence (Flesch, 1948; McLaughlin, 
1969; Kincaid et al, 1975). These so-called tra-
ditional readability metrics are still used today in 
many settings and domains, in part because they 
are very easy to compute. Their results, however, 
are not always representative of the complexity 
of a text (Davison and Kantor, 1982). They can 
easily misrepresent the complexity of technical 
texts, or reveal themselves un-adapted to a set of 
readers with particular reading difficulties. Other 
formulas rely on lexical information; e.g., the 
New Dale-Chall readability formula consults a 
static, manually-built list of ?easy? words to de-
termine whether a text contains unfamiliar words 
(Chall and Dale, 1995).  
Researchers in computational linguistics have 
investigated the use of statistical language mod-
els (unigram in particular) to capture the range of 
vocabulary from one grade level to another (Si 
and Callan, 2001; Collins-Thompson and Callan, 
2004). These metrics predicted readability better 
than traditional formulas when tested against a 
corpus of web pages. The use of syntactic fea-
tures was also investigated (Schwarm and Osten-
dorf, 2005; Heilman et al, 2007; Petersen and 
Ostendorf, 2009) in the assessment of text reada-
bility for English as a Second Language readers. 
While lexical features alone outperform syntactic 
features in classifying texts according to their 
reading levels, combining the lexical and syntac-
tic features yields the best results. 
Several elegant metrics that focus solely on 
the syntax of a text have also been developed.  
The Yngve (1960) measure, for instance, focuses 
on the depth of embedding of nodes in the parse 
tree; others use the ratio of terminal to non-
terminal nodes in the parse tree of a sentence 
(Miller and Chomsky, 1963; Frazier, 1985).  
These metrics have been used to analyze the 
writing of potential Alzheimer's patients to detect 
mild cognitive impairments (Roark, Mitchell, 
and Hollingshead, 2007), thereby indicating that 
cognitively motivated features of text are valua-
ble when creating tools for specific populations. 
Barzilay and Lapata (2008) presented early 
work in investigating the use of discourse to dis-
tinguish abridged from original encyclopedia 
articles.  Their focus, however, is on style detec-
tion rather than readability assessment per se.  
Coh-Metrix is a tool for automatically calculat-
ing text coherence based on features such as re-
petition of lexical items across sentences and 
latent semantic analysis (McNamara et al, 
2006).  The tool is based on comprehension data 
collected from children and college students. 
Our research differs from related work in that 
we seek to produce an automatic readability me-
tric that is tailored to the literacy skills of adults 
with ID.  Because of the specific cognitive cha-
racteristics of these users, it is an open question 
whether existing readability metrics and features 
are useful for assessing readability for adults 
with ID.  Many of these earlier metrics have fo-
cused on the task of assigning texts to particular 
elementary school grade levels.  Traditional 
grade levels may not be the ideal way to score 
texts to indicate how readable they are for adults 
with ID.  Other related work has used models of 
vocabulary (Collins-Thompson and Callan, 
2004).  Since we would like to use our tool to 
give adults with ID access to local news stories, 
we choose to keep our metric topic-independent. 
Another difference between our approach and 
previous approaches is that we have designed the 
features used by our readability metric based on 
230
the cognitive aspects of our target users.  For ex-
ample, these users are better at decoding words 
than at comprehending text meaning (Drew & 
Hardman, 2004); so, shallow features like ?sylla-
ble count per word? or unigram models of word 
frequency (based on texts designed for children) 
may be less important indicators of reading diffi-
culty.  A critical challenge for our users is to 
create a cohesive representation of discourse.  
Due to their impairments in semantic encoding 
speed, our users may have particular difficulty 
with texts that place a significant burden on 
working memory (items fall out of memory be-
fore they can be semantically encoded).   
While we focus on readability of texts, other 
projects have automatically generated texts for 
people with aphasia (Carroll et al, 1999) or low 
reading skills (Williams and Reiter, 2005). 
3 Research Hypothesis and Methods 
We hypothesize that the complexity of a text for 
adults with ID is related to the number of entities 
referred to in the text overall.  If a paragraph or a 
text refers to too many entities at once, the reader 
has to work harder at mapping each entity to a 
semantic representation and deciding how each 
entity is related to others.  On the other hand, 
when a text refers to few entities, less work is 
required both for semantic encoding and for in-
tegrating the entities into a cohesive mental re-
presentation.  Section 5.2 discusses some novel 
discourse-level features (based on the ?entity 
density? of a text) that we believe will correlate 
to comprehension by adults with ID.   
To test our hypothesis, we used the following 
methodology.  We collected four corpora (as de-
scribed in Section 4).  Three of them (Britannica, 
LiteracyNet and WeeklyReader) have been ex-
amined in previous work on readability.  The 
fourth (LocalNews) is novel and results from a 
user study we conducted with adults with ID.  
We then analyzed how significant each feature is 
on our Britannica and LiteracyNet corpora.  Fi-
nally, we combined the significant features into a 
linear regression model and experimented with 
several feature combinations. We evaluated our 
model on the WeeklyReader and LocalNews 
corpora. 
4 Corpora and Readability Judgments  
To study how certain linguistic features indicate 
the readability of a text, we collected a corpus of 
English text at different levels of readability.  An 
ideal corpus for our research would contain texts 
that have been written specifically for our au-
dience of adults with intellectual disabilities ? in 
particular if such texts were paired with alternate 
versions of each text written for a general au-
dience.  We are not aware of such texts available 
electronically, and so we have instead mostly 
collected texts written for an audience of child-
ren.  The texts come from online and commercial 
sources, and some have been analyzed previous-
ly by text simplification researchers (Petersen 
and Ostendorf, 2009).  Our corpus also contains 
some novel texts produced as part of an experi-
mental study involving adults with ID. 
4.1 Paired and Graded Generic Corpora: 
Britannica, LiteracyNet, and Weekly 
Reader 
The first section of our corpus (which we refer to 
as Britannica) has 228 articles from the Encyclo-
pedia Britannica, originally collected by (Barzi-
lay and Elhadad, 2003).  This consists of 114 
articles in two forms: original articles written for 
adults and corresponding articles rewritten for an 
audience of children.  While the texts are paired, 
the content of the texts is not identical: some de-
tails are omitted from the child version, and addi-
tional background is sometimes inserted.  The 
resulting corpus is comparable in content. 
Because we are particularly interested in mak-
ing local news articles accessible to adults with 
ID, we collected a second paired corpus, which 
we refer to as LiteracyNet, consisting of 115 
news articles made available through (West-
ern/Pacific Literacy Network / LiteracyNet, 
2008).  The collection of local CNN stories is 
available in an original and simplified/abridged 
form (230 total news articles) designed for use in 
literacy education. 
The third corpus we collected (Weekly Reader) 
was obtained from the Weekly Reader corpora-
tion (Weekly Reader, 2008).  It contains articles 
for students in elementary school.  Each text is 
labeled with its target grade level (grade 2: 174 
articles, grade 3: 289 articles, grade 4: 428 ar-
ticles, grade 5: 542 articles).  Overall, the corpus 
has 1433 articles. (U.S. elementary school grades 
2 to 5 generally are for children ages 7 to 10.) 
The corpora discussed above are similar to 
those used by Petersen and Ostendorf (2009).  
While the focus of our research is adults with ID, 
most of the texts discussed in this section have 
been simplified or written by human authors to 
be readable for children.  Despite the texts being 
intended for a different audience than the focus 
of our research, we still believe these texts to be 
231
of value.  It is rare to encounter electronically 
available corpora in which an original and a sim-
plified version of a text is paired (as in the Bri-
tannica and LiteracyNet corpora) or texts labeled 
as being at specific levels of readability (as in the 
Weekly Reader corpus). 
4.2 Readability-Specific Corpus: LocalNews 
The final section of our corpus contains local 
news articles that are labeled with comprehen-
sion scores.  These texts were produced for a fea-
sibility study involving adults with ID.  Each text 
was read by adults with ID, who then answered 
comprehension questions to measure their under-
standing of the texts.  Unlike the previous corpo-
ra, LocalNews is novel and was not investigated 
by previous research in readability. 
After obtaining university approval for our ex-
perimental protocol and informed consent 
process, we conducted a study with 14 adults 
with mild intellectual disabilities who participate 
in daytime educational programs in the New 
York area.  Participants were presented with ten 
articles collected from various local New York 
based news websites.  Some subjects saw the 
original form of an article and others saw a sim-
plified form (edited by a human author); no sub-
ject saw both versions.  The texts were presented 
in random order using software that displayed 
the text on the screen, read it aloud using text-to-
speech software, and highlighted each word as it 
was read.  Afterward, subjects were asked aloud 
multiple-choice comprehension questions. We 
defined the readability score of a story as the 
percentage of correct answers averaged across 
the subjects who read that particular story. 
A human editor performed the text simplifica-
tion with the goal of making the text more reada-
ble for adults with mild ID.  The editor made the 
following types of changes to the original news 
stories: breaking apart complex sentences, un-
embedding information in complex prepositional 
phrases and reintegrating it as separate sentences, 
replacing infrequent vocabulary items with more 
common/colloquial equivalents, omitting sen-
tences and phrases from the story that mention 
entities and phrases extraneous to the main 
theme of the article.  For instance, the original 
sentence ?They?re installing an induction loop 
system in cabs that would allow passengers with 
hearing aids to tune in specifically to the driver?s 
voice.? was transformed into ?They?re installing 
a system in cabs. It would allow passengers with 
hearing aids to listen to the driver?s voice.? 
This corpus of local news articles that have 
been human edited and scored for comprehen-
sion by adults with ID is small in size (20 news 
articles), but we consider it a valuable resource.  
Unlike the texts that have been simplified for 
children (the rest of our corpus), these texts have 
been rated for readability by actual adults with 
ID.  Furthermore, comprehension scores are de-
rived from actual reader comprehension tests, 
rather than self-perceived comprehension.  Be-
cause of the small size of this part of our corpus, 
however, we primarily use it for evaluation pur-
poses (not for training the readability models). 
5 Linguistic Features and Readability  
We now describe the set of features we investi-
gated for assessing readability automatically.  
Table 1 contains a list of the features ? including 
a short code name for each feature which may be 
used throughout this paper.  We have begun by 
implementing the simple features used by the 
Flesh-Kincaid and FOG metrics: average number 
of words per sentence, average number of syl-
lables per word, and percentage of words in the 
document with 3+ syllables. 
5.1 Basic Features Used in Earlier Work 
We have also implemented features inspired by 
earlier research on readability.  Petersen and Os-
tendorf (2009) included features calculated from 
parsing the sentences in their corpus using the 
Charniak parser (Charniak, 2000): average parse 
tree height, average number of noun phrases per 
sentence, average number of verb phrases per 
sentence, and average number of SBARs per sen-
tence. We have implemented versions of most of 
these parse-tree-related features for our project.  
We also parse the sentences in our corpus using 
Charniak?s parser and calculate the following 
features listed in Table 1: aNP, aN, aVP, aAdj, 
aSBr, aPP, nNP, nN, nVP, nAdj, nSBr, and nPP.   
5.2 Novel Cognitively-Motivated Features  
Because of the special reading characteristics of 
our target users, we have designed a set of cogni-
tively motivated features to predict readability of 
texts for adults with ID.  We have discussed how 
working memory limits the semantic encoding of 
new information by these users; so, our features 
indicate the number of entities in a text that the 
reader must keep in mind while reading each 
sentence and throughout the entire document.  It 
is our hypothesis that this ?entity density? of a 
232
text plays an important role in the difficulty of 
that text for readers with intellectual disabilities. 
The first set of features incorporates the Ling-
Pipe named entity detection software (Alias-i, 
2008), which detects three types of entities: per-
son, location, and organization.  We also use the 
part-of-speech tagger in LingPipe to identify the 
common nouns in the document, and we find the 
union of the common nouns and the named entity 
noun phrases in the text.  The union of these two 
sets is our definition of ?entity? for this set of 
features.  We count both the total number of 
?entity mentions? in a text (each token appear-
ance of an entity) and the total number of unique 
entities (exact-string-match duplicates only 
counted once).  Table 1 lists these features: nEM, 
nUE, aEM, and aUE.  We count the totals per 
document to capture how many entities the read-
er must keep track of while reading the docu-
ment.  We also expect sentences with more enti-
ties to be more difficult for our users to semanti-
cally encode due to working memory limitations; 
so, we also count the averages per sentence to 
capture how many entities the reader must keep 
in mind to understand each sentence.   
To measure the working memory burden of a 
text, we?d like to capture the number of dis-
course entities that a reader must keep in mind.  
However, the ?unique entities? identified by the 
named entity recognition tool may not be a per-
fect representation of this ? several unique enti-
ties may actually refer to the same real-world 
entity under discussion.  To better model how 
multiple noun phrases in a text refer to the same 
entity or concept, we have also built features us-
ing lexical chains (Galley and McKeown, 2003).  
Lexical chains link nouns in a document con-
nected by relations like synonymy or hyponomy; 
chains can indicate concepts that recur through-
out a text.  A lexical chain has both a length 
(number of noun phrases it includes) and a span 
(number of words in the document between the 
first noun phrase at the beginning of the chain 
and the last noun phrase that is part of the chain).  
We calculate the number of lexical chains in the 
document (nLC) and those with a span greater 
than half the document length (nLC2).  We be-
lieve these features may indicate the number of 
entities/concepts that a reader must keep in mind 
during a document and the subset of very impor-
tant entities/concepts that are the main topic of 
the document.  The average length and average 
span of the lexical chains in a document (aLCL 
and aLCS) may also indicate how many of the 
chains in the document are short-lived, which 
may mean that they are ancillary enti-
ties/concepts, not the main topics. 
The final two features in Table 1 (aLCw and 
aLCe) use the concept of an ?active? chain.  At a 
particular location in a text, we define a lexical 
chain to be ?active? if the span (between the first 
and last noun in the lexical chain) includes the 
current location.  We expect these features may 
indicate the total number of concepts that the 
reader needs to keep in mind during a specific 
moment in time when reading a text.  Measuring 
the average number of concepts that the reader of 
a text must keep in mind may suggest the work-
ing memory burden of the text over time.  We 
were unsure if individual words or individual 
noun-phrases in the document should be used as 
the basic unit of ?time? for the purpose of aver-
aging the number of active lexical chains; so, we 
included both features. 
5.3 Testing the Significance of Features 
To select which features to include in our auto-
matic readability assessment tool (in Section 6), 
Code Feature
aWPS average number of words per sentence
aSPW average number of syllables per word
%3+S % of words in document with 3+ syllables
aNP avg. num. NPs per sentence
aN avg. num. common+proper nouns per sentence
aVP avg. num. VPs per sentence
aAdj avg. num. Adjectives per sentence
aSBr avg. num. SBARs per sentence
aPP avg. num. prepositional phrases per sentence
nNP total number of NPs per sentence
nN total num. of common+proper nouns in document
nVP total number of VPs in the document
nAdj total number of Adjectives in the document
nSBr total number of SBARs in the document
nPP total num. of prepositional phrases in document
nEM number of entity mentions in document
nUE number of unique entit ies in document
aEM avg. num. entity mentions per sentence
aUE avg. num. unique entit ies per sentence
nLC number of lexical chains in document
nLC2 num. lex. chains, span > half document length
aLCL average lexical chain length
aLCS average lexical chain span
aLCw avg. num. lexical chains active at  each word
aLCn avg. num. lexical chains active at  each NP
Table 1: Implemented Features
233
we analyzed the documents in our paired corpora 
(Britannica and LiteracyNet).  Because they con-
tain a complex and a simplified version of each 
article, we can examine differences in readability 
while holding the topic and genre constant.  We 
calculated the value of each feature for each doc-
ument, and we used a paired t-test to determine if 
the difference between the complex and simple 
documents was significant for that corpus. 
Table 2 contains the results of this feature se-
lection process; the columns in the table indicate 
the values for the following corpora: Britannica 
complex, Britannica simple, LiteracyNet com-
plex, and LiteracyNet simple.  An asterisk ap-
pears in the ?Sig? column if the difference be-
tween the feature values for the complex vs. 
simple documents is statistically significant for 
that corpus (significance level: p<0.00001).   
The only two features which did not show a 
significant difference (p>0.01) between the com-
plex and simple versions of the articles were: 
average lexical chain length (aLCL) and number 
of lexical chains with span greater than half the 
document length (nLC2).  The lack of signific-
ance for aLCL may be explained by the vast ma-
jority of lexical chains containing few members; 
complex articles contained more of these chains 
? but their chains did not contain more members.  
In the case of nLC2, over 80% of the articles in 
each category contained no lexical chains whose 
span was greater than half the document length.  
The rarity of a lexical chain spanning the majori-
ty of a document may have led to there being no 
significant difference between complex/simple. 
6 A Readability Assessment Tool 
After testing the significance of features using 
paired corpora, we used linear regression and our 
graded corpus (Weekly Reader) to build a reada-
bility assessment tool.  To evaluate the tool?s 
usefulness for adults with ID, we test the correla-
tion of its scores with the LocalNews corpus. 
6.1 Versions of Our Model 
We began our evaluation by implementing three 
versions of our automatic readability assessment 
tool.  The first version uses only those features 
studied by previous researchers (aWPS, aSPW, 
%3+S, aNP, aN, aVP, aAdj, aSBr, aPP, nNP, nN, 
nVP, nAdj, nSBr, nPP).  The second version uses 
only our novel cognitively motivated features 
(section 5.2).  The third version uses the union of 
both sets of features.  By building three versions 
of the tool, we can compare the relative impact 
of our novel cognitively-motivated features.  For 
all versions, we have only included those fea-
tures that showed a significant difference be-
tween the complex and simple articles in our 
paired corpora (as discussed in section 5.3). 
6.2 Learning Technique and Training Data 
Early work on automatic readability analysis 
framed the problem as a classification task: 
creating multiple classifiers for labeling a text as 
being one of several elementary school grade 
levels (Collins-Thompson and Callan, 2004).  
Because we are focusing on a unique user group 
with special reading challenges, we do not know 
a priori what level of text difficulty is ideal for 
our users.  We would not know where to draw 
category boundaries for classification.  We also 
prefer that our assessment tool assign numerical 
difficulty scores to texts.  Thus, after creating 
this tool, we can conduct further reading com-
prehension experiments with adults with ID to 
determine what threshold (for readability scores 
assigned by our tool) is appropriate for our users. 
Feature
Brit. 
Com.
Brit. 
Simp. Sig
LitN. 
Com.
LitN. 
Simp. Sig
aWPS 20.13 14.37 * 17.97 12.95 *
aSPW 1.708 1.655 * 1.501 1.455 *
%3+S 0.196 0.177 * 0.12 0.101 *
aNP 8.363 6.018 * 6.519 4.691 *
aN 7.024 5.215 * 5.319 3.929 *
aVP 2.334 1.868 * 3.806 2.964 *
aAdj 1.95 1.281 * 1.214 0.876 *
aSBr 0.266 0.205 * 0.793 0.523 *
aPP 2.858 1.936 * 1.791 1.22 *
nNP 798 219.2 * 150.2 102.9 *
nN 668.4 190.4 * 121.4 85.75 *
nVP 242.8 69.19 * 88.2 65.52 *
nAdj 205 47.32 * 28.11 19.04 *
nSBr 31.33 7.623 * 18.16 11.43 *
nPP 284.7 70.75 * 41.06 26.79 *
nEM 624.2 172.7 * 115.2 82.83 *
nUE 355 117 * 81.56 54.94 *
aEM 6.441 4.745 * 5.035 3.789 *
aUE 4.579 3.305 * 3.581 2.55 *
nLC 59.21 17.57 * 12.43 8.617 *
nLC2 0.175 0.211 0.191 0.226
aLCL 3.009 3.022 2.817 2.847
aLCS 357 246.1 * 271.9 202.9 *
aLCw 1.803 1.358 * 1.407 1.091 *
aLCn 1.852 1.42 * 1.53 1.201 *
Table 2: Feature Values of Paired Corpora
234
To select features for our model, we used our 
paired corpora (Britannica and LiteracyNet) to 
measure the significance of each feature.  Now 
that we are training a model, we make use of our 
graded corpus (articles from Weekly Reader).  
This corpus contains articles that have each been 
labeled with an elementary school grade level for 
which it was written.  We divide this corpus ? 
using 80% of articles as training data and 20% as 
testing data.  We model the grade level of the 
articles using linear regression; our model is im-
plemented using R (R Development Core Team, 
2008).  
6.3 Evaluation of Our Readability Tool 
We conducted two rounds of training and evalua-
tion of our three regression models.  We also 
compare our models to a baseline readability as-
sessment tool: the popular Flesh-Kincaid Grade 
Level index (Kincaid et al, 1975).  
In the first round of evaluation, we trained and 
tested our regression models on the Weekly 
Reader corpus.  This round of evaluation helped 
to determine whether our feature-set and regres-
sion technique were successfully modeling those 
aspects of the texts that were relevant to their 
grade level.  Our results from this round of eval-
uation are presented in the form of average error 
scores.  (For each article in the Weekly Reader 
testing data, we calculate the difference between 
the output score of the model and the correct 
grade-level for that article.)  Table 3 presents the 
average error results for the baseline system and 
our three regression models.  We can see that the 
model trained on the shallow and parse-related 
features out-performs the model trained only on 
our novel features; however, the best model 
overall is the one is trained on all of the features.  
This model predicts the grade level of Weekly 
Reader articles to within roughly 0.565 grade 
levels on average.   
 
Readability Model (or baseline) Average Error 
Baseline: Flesh-Kincaid Index 2.569 
Basic Features Only 0.6032 
Cognitively Motivated Features Only 0.6110 
Basic + Cognitively-Motiv. Features 0.5650 
Table 3: Predicting Grade Level of Weekly Reader 
 
In our second round of evaluation, we trained 
the regression model on the Weekly Reader cor-
pus, but we tested it against the LocalNews cor-
pus.  We measured the correlation between our 
regression models? output and the comprehen-
sion scores of adults with ID on each text.  For 
this reason, we do not calculate the ?average er-
ror?; instead, we simply measure the correlation 
between the models? output and the comprehen-
sion scores. (We expect negative correlations 
because comprehension scores should increase as 
the predicted grade level of the text goes down.)  
Table 4 presents the correlations for our three 
models and the baseline system in the form of 
Pearson?s R-values.  We see a surprising result: 
the model trained only on the cognitively-
motivated features is more tightly correlated with 
the comprehension scores of the adults with ID.  
While the model trained on all features was bet-
ter at assigning grade levels to Weekly Reader 
articles, when we tested it on the local news ar-
ticles from our user-study, it was not the top-
performing model.  This result suggests that the 
shallow and parse-related features of texts de-
signed for children (the Weekly Reader articles, 
our training data) are not the best predictors of 
text readability for adults with ID.   
 
Readability Model (or baseline) Pearson?s R 
Baseline: Flesh-Kincaid Index -0.270 
Basic Features Only -0.283 
Cognitively Motivated Features Only -0.352 
Basic + Cognitively-Motiv. Features -0.342 
Table 4: Correlation to User-Study Comprehension 
7 Discussion 
Based on the cognitive and literacy skills of 
adults with ID, we designed novel features that 
were useful in assessing the readability of texts 
for these users.  The results of our study have 
supported our hypothesis that the complexity of a 
text for adults with ID is related to the number of 
entities referred to in the text.  These ?entity den-
sity? features enabled us to build models that 
were better at predicting text readability for 
adults with intellectual disabilities.  
This study has also demonstrated the value of 
collecting readability judgments from target us-
ers when designing a readability assessment tool.  
The results in Table 4 suggest that models 
trained on corpora containing texts designed for 
children may not always lead to accurate models 
of the readability of texts for other groups of 
low-literacy users.  Using features targeting spe-
cific aspects of literacy impairment have allowed 
us to make better use of children?s texts when 
designing a model for adults with ID. 
7.1 Future Work 
In order to study more features and models of 
readability, we will require more testing data for 
tracking progress of our readability regression 
235
models.  Our current study has illustrated the 
usefulness of texts that have been evaluated by 
adults with ID, and we therefore plan to increase 
the size of this corpus in future work.   In addi-
tion to using this corpus for evaluation, we may 
want to use it to train our regression models.  For 
this study, we trained on Weekly Reader text 
labeled with elementary school grade levels, but 
this is not ideal.  Texts designed for children may 
differ from those that are best for adults with ID, 
and ?grade levels? may not be the best way to 
rank/rate text readability for these users.  While 
our user-study comprehension-test corpus is cur-
rently too small for training, we intend to grow 
the size of this corpus in future work.   
We also plan on refining our cognitively moti-
vated features for measuring the difficulty of a 
text for our users.  Currently, we use lexical 
chain software to link noun phrases in a docu-
ment that may refer to similar entities/concepts.  
In future work, we plan to use co-reference reso-
lution software to model how multiple ?entity 
mentions? may refer to a single discourse entity.  
For comparison purposes, we plan to imple-
ment other features that have been used in earlier 
readability assessment systems.  For example, 
Petersen and Ostendorf (2009) created lists of the 
most common words from the Weekly Reader 
articles, and they used the percentage of words in 
a document not on this list as a feature.   
The overall goal of our research is to develop 
a software system that can automatically simplify 
the reading level of local news articles and 
present them in an accessible way to adults with 
ID.  Our automatic readability assessment tool 
will be a component in this future text simplifica-
tion system.  We have therefore preferred to in-
clude features in our tool that focus on aspects of 
the text that can be modified during a simplifica-
tion process.  In future work, we will study how 
to use our readability assessment tool to guide 
how a text revision system decides to modify a 
text to increase its readability for these users. 
7.2 Summary of Contributions 
We have contributed to research on automatic 
readability assessment by designing a new me-
thod for assessing the complexity of a text at the 
level of discourse.  Our novel ?entity density? 
features are based on named entity and lexical 
chain software, and they are inspired by the cog-
nitive underpinnings of the literacy challenges of 
adults with ID ? specifically, the role of slow 
semantic encoding and working memory limita-
tions.  We have demonstrated the usefulness of 
these novel features in modeling the grade level 
of elementary school texts and in correlating to 
readability judgments from adults with ID.   
Another contribution of our work is the collec-
tion of an initial corpus of texts of local news 
stories that have been manually simplified by a 
human editor.  Both the original and the simpli-
fied versions of these stories have been evaluated 
by adults with intellectual disabilities.  We have 
used these comprehension scores in the evalua-
tion phase of this study, and we have suggested 
how constructing a larger corpus of such articles 
could be useful for training readability tools. 
More broadly, this project has demonstrated 
how focusing on a specific user population, ana-
lyzing their cognitive skills, and involving them 
in a user-study has led to new insights in model-
ing text readability.  As Dale and Chall?s defini-
tion (1949) originally argued, characteristics of 
the reader are central to the issue of readability.  
We believe our user-focused research paradigm 
may be used to drive further advances in reada-
bility assessment for other groups of users. 
Acknowledgements 
We thank the Weekly Reader Corporation for 
making its corpus available for our research.  We 
are grateful to Martin Jansche for his assistance 
with the statistical data analysis and regression. 
References  
Alias-i. 2008. LingPipe 3.6.0. http://alias-
i.com/lingpipe (accessed October 1, 2008) 
Barzilay, R., Elhadad, N., 2003. Sentence alignment 
for monolingual comparable corpora. In Proc 
EMNLP, pp. 25-32. 
Barzilay R., Lapata, M., 2008. Modeling Local Cohe-
rence: An Entity-based Approach. Computational 
Linguistics. 34(1):1-34. 
Carroll, J., Minnen, G., Pearce, D., Canning, Y., Dev-
lin, S., Tait, J. 1999. Simplifying text for language-
impaired readers. In Proc. EACL Poster, p. 269. 
Chall, J.S., Dale, E., 1995. Readability Revisited: The 
New Dale-Chall Readability Formula. Brookline 
Books, Cambridge, MA. 
Charniak, E. 2000. A maximum-entropy-inspired 
parser.  In Proc. NAACL, pp. 132-139. 
Collins-Thompson, K., and Callan, J.  2004.  A lan-
guage modeling approach to predicting reading dif-
ficulty.  In Proc. NAACL, pp. 193-200. 
Dale, E. and J. S. Chall.  1949.  The concept of reada-
bility.  Elementary English 26(23). 
236
Davison, A., and Kantor, R.  1982.  On the failure of 
readability formulas to define readable texts: A case 
study from adaptations.  Reading Research Quar-
terly, 17(2):187-209. 
Drew, C.J., and Hardman, M.L. 2004.  Mental retar-
dation: A lifespan approach to people with intellec-
tual disabilities (8th ed.).  Columbus, OH: Merrill. 
Flesch, R.  1948.  A new readability yardstick.  Jour-
nal of Applied Psychology, 32:221-233. 
Fowler, A.E.  1998.  Language in mental retardation.  
In Burack, Hodapp, and Zigler (Eds.), Handbook of 
Mental Retardation and Development.  Cambridge, 
UK: Cambridge Univ. Press, pp. 290-333. 
Frazier, L.  1985.  Natural Language Parsing: Psy-
chological, Computational, and Theoretical Pers-
pectives, chapter Syntactic complexity, pp. 129-
189.  Cambridge University Press. 
Galley, M., McKeown, K. 2003. Improving Word 
Sense Disambiguation in Lexical Chaining. In 
Proc. IJCAI, pp. 1486-1488.  
Gunning, R. 1952.  The Technique of Clear Writing. 
McGraw-Hill. 
Heilman, M., Collins-Thompson, K., Callan, J., and 
Eskenazi, M.  2007.  Combining lexical and gram-
matical features to improve readability measures for 
first and second language texts.  In Proc. NAACL, 
pp. 460-467. 
Hickson-Bilsky, L.  1985.  Comprehension and men-
tal retardation.  International Review of Research in 
Mental Retardation, 13: 215-246. 
Katims, D.S.  2000.  Literacy instruction for people 
with mental retardation: Historical highlights and 
contemporary analysis.  Education and Training in 
Mental Retardation and Developmental Disabili-
ties, 35(1): 3-15. 
Kincaid, J. P., Fishburne, R. P., Rogers, R. L., and 
Chissom, B. S.  1975.  Derivation of new readabili-
ty formulas for Navy enlisted personnel, Research 
Branch Report 8-75, Millington, TN. 
Kincaid, J., Fishburne, R., Rodgers, R., and Chisson, 
B.  1975.  Derivation of new readability formulas 
for navy enlisted personnel.  Technical report, Re-
search Branch Report 8-75, U.S.  Naval Air Station. 
McLaughlin, G.H.  1969.  SMOG grading - a new 
readability formula.  Journal of Reading, 
12(8):639-646. 
McNamara, D.S., Ozuru, Y., Graesser, A.C., & Lou-
werse, M. (2006) Validating Coh-Metrix., In Proc. 
Conference of the Cognitive Science Society, pp. 
573.   
Miller, G., and Chomsky, N.  1963.  Handbook of 
Mathematical Psychology, chapter Finatary models 
of language users, pp. 419-491.  Wiley. 
Perfetti, C., and Lesgold, A.  1977.  Cognitive 
Processes in Comprehension, chapter Discourse 
Comprehension and sources of individual differ-
ences.  Erlbaum. 
Petersen, S.E., Ostendorf, M. 2009. A machine learn-
ing approach to reading level assessment. Computer 
Speech and Language, 23: 89-106. 
R Development Core Team. 2008. R: A Language 
and Environment for Statistical Computing. Vienna, 
Austria: R Foundation for Statistical Computing. 
http://www.R-project.org 
Roark, B., Mitchell, M., and Hollingshead, K.  2007.  
Syntactic complexity measures for detecting mild 
cognitive impairment.  In Proc. ACL Workshop on 
Biological, Translational, and Clinical Language 
Processing (BioNLP'07), pp. 1-8. 
Schwarm, S., and Ostendorf, M.  2005.  Reading level 
assessment using support vector machines and sta-
tistical language models.  In Proc. ACL, pp. 523-
530. 
Si, L., and Callan, J.  2001.  A statistical model for 
scientific readability.  In Proc. CIKM, pp. 574-576. 
Stenner, A.J. 1996. Measuring reading comprehension 
with the Lexile framework.  4th North American 
Conference on Adolescent/Adult Literacy. 
U.S. Census Bureau.  2000.  Projections of the total 
resident population by five-year age groups and 
sex, with special age categories: Middle series 
2025-2045.  Washington: U.S. Census Bureau, Po-
pulations Projections Program, Population Division. 
Weekly Reader, 2008. http://www.weeklyreader.com 
(Accessed Oct., 2008). 
Western/Pacific Literacy Network / Literacyworks, 
2008. CNN SF learning resources. 
http://literacynet.org/cnnsf/ (Accessed Oct., 2008). 
Williams, S., Reiter, E. 2005. Generating readable 
texts for readers with low basic skills. In Proc. Eu-
ropean Workshop on Natural Language Genera-
tion, pp. 140-147. 
Yngve, V.  1960.  A model and a hypothesis for lan-
guage structure.  American Philosophical Society, 
104: 446-466. 
237
Coling 2010: Poster Volume, pages 276?284,
Beijing, August 2010
A Comparison of Features for Automatic Readability Assessment
Lijun Feng
City University of New York
lijun7.feng@gmail.com
Martin Jansche
Google, Inc.
jansche@acm.org
Matt Huenerfauth
City University of New York
matt@cs.qc.cuny.edu
Noe?mie Elhadad
Columbia University
noemie@dbmi.columbia.edu
Abstract
Several sets of explanatory variables ? in-
cluding shallow, language modeling, POS,
syntactic, and discourse features ? are com-
pared and evaluated in terms of their im-
pact on predicting the grade level of read-
ing material for primary school students.
We find that features based on in-domain
language models have the highest predic-
tive power. Entity-density (a discourse fea-
ture) and POS-features, in particular nouns,
are individually very useful but highly cor-
related. Average sentence length (a shal-
low feature) is more useful ? and less ex-
pensive to compute ? than individual syn-
tactic features. A judicious combination
of features examined here results in a sig-
nificant improvement over the state of the
art.
1 Introduction
1.1 Motivation and Method
Readability Assessment quantifies the difficulty
with which a reader understands a text. Automatic
readability assessment enables the selection of ap-
propriate reading material for readers of varying
proficiency. Besides modeling and understanding
the linguistic components involved in readability, a
readability-prediction algorithm can be leveraged
for the task of automatic text simplification: as sim-
plification operators are applied to a text, the read-
ability is assessed to determine whether more sim-
plification is needed or a particular reading level
was reached.
Identifying text properties that are strongly cor-
related with text complexity is itself complex. In
this paper, we explore a broad range of text proper-
ties at various linguistic levels, ranging from dis-
course features to language modeling features, part-
of-speech-based grammatical features, parsed syn-
tactic features and well studied shallow features,
many of which are inspired by previous work.
We use grade levels, which indicate the number
of years of education required to completely under-
stand a text, as a proxy for reading difficulty. The
corpus in our study consists of texts labeled with
grade levels ranging from grade 2 to 5. We treat
readability assessment as a classification task and
evaluate trained classifiers in terms of their predic-
tion accuracy. To investigate the contributions of
various sets of features, we build prediction models
and examine how the choice of features influences
the model performance.
1.2 Related Work
Many traditional readability metrics are linear mod-
els with a few (often two or three) predictor vari-
ables based on superficial properties of words, sen-
tences, and documents. These shallow features
include the average number of syllables per word,
the number of words per sentence, or binned word
frequency. For example, the Flesch-Kincaid Grade
Level formula uses the average number of words
per sentence and the average number of syllables
per word to predict the grade level (Flesch, 1979).
The Gunning FOG index (Gunning, 1952) uses av-
erage sentence length and the percentage of words
with at least three syllables. These traditional met-
rics are easy to compute and use, but they are not
reliable, as demonstrated by several recent stud-
ies in the field (Si and Callan, 2001; Petersen and
Ostendorf, 2006; Feng et al, 2009).
276
With the advancement of natural language pro-
cessing tools, a wide range of more complex text
properties have been explored at various linguis-
tic levels. Si and Callan (2001) used unigram
language models to capture content information
from scientific web pages. Collins-Thompson and
Callan (2004) adopted a similar approach and used
a smoothed unigram model to predict the grade lev-
els of short passages and web documents. Heilman
et al (2007) continued using language modeling
to predict readability for first and second language
texts. Furthermore, they experimented with vari-
ous statistical models to test their effectiveness at
predicting reading difficulty (Heilman et al, 2008).
Schwarm/Petersen and Ostendorf (Schwarm and
Ostendorf, 2005; Petersen and Ostendorf, 2006)
used support vector machines to combine features
from traditional reading level measures, statistical
language models and automatic parsers to assess
reading levels. In addition to lexical and syntactic
features, several researchers started to explore dis-
course level features and examine their usefulness
in predicting text readability. Pitler and Nenkova
(2008) used the Penn Discourse Treebank (Prasad
et al, 2008) to examine discourse relations. We
previously used a lexical-chaining tool to extract
entities that are connected by certain semantic re-
lations (Feng et al, 2009).
In this study, we systematically evaluate all
above-mentioned types of features, as well as a
few extensions and variations. A detailed descrip-
tion of the features appears in Section 3. Section
4 discusses results of experiments with classifiers
trained on these features. We begin with a descrip-
tion of our data in the following section.
2 Corpus
We contacted the Weekly Reader1 corporation, an
on-line publisher producing magazines for elemen-
tary and high school students, and were granted
access in October 2008 to an archive of their ar-
ticles. Among the articles retrieved, only those
for elementary school students are labeled with
grade levels, which range from 2 to 5. We selected
only this portion of articles (1629 in total) for the
1http://www.weeklyreader.com
Table 1: Statistics for the Weekly Reader Corpus
Grade docs. words/document words/sentence
mean std. dev. mean std. dev.
2 174 128.27 106.03 9.54 2.32
3 289 171.96 106.05 11.39 2.42
4 428 278.03 187.58 13.67 2.65
5 542 335.56 230.25 15.28 3.21
study.2 These articles are intended to build chil-
dren?s general knowledge and help them practice
reading skills. While pre-processing the texts, we
found that many articles, especially those for lower
grade levels, consist of only puzzles and quizzes,
often in the form of simple multiple-choice ques-
tions. We discarded such texts and kept only 1433
full articles. Some distributional statistics of the
final corpus are listed in Table 1.
3 Features
3.1 Discourse Features
We implement four subsets of discourse fea-
tures: entity-density features, lexical-chain fea-
tures, coreference inference features and entity grid
features. The coreference inference features are
novel and have not been studied before. We pre-
viously studied entity-density features and lexical-
chain features for readers with intellectual disabili-
ties (Feng et al, 2009). Entity-grid features have
been studied by Barzilay and Lapata (2008) in a
stylistic classification task. Pitler and Nenkova
(2008) used the same features to evaluate how well
a text is written. We replicate this set of features
for grade level prediction task.
3.1.1 Entity-Density Features
Conceptual information is often introduced in a
text by entities, which consist of general nouns
and named entities, e.g. people?s names, locations,
organizations, etc. These are important in text
comprehension, because established entities form
basic components of concepts and propositions, on
which higher level discourse processing is based.
Our prior work illustrated the importance of en-
tities in text comprehension (Feng et al, 2009).
2A corpus of Weekly Reader articles was previously used
in work by Schwarm and Ostendorf (2005). However, the two
corpora are not identical in size nor content.
277
Table 2: New Entity-Density Features
1 percentage of named entities per document
2 percentage of named entities per sentences
3 percentage of overlapping nouns removed
4 average number of remaining nouns per sentence
5 percentage of named entities in total entities
6 percentage of remaining nouns in total entities
We hypothesized that the number of entities in-
troduced in a text relates to the working memory
burden on their targeted readers ? individuals with
intellectual disabilities. We defined entities as a
union of named entities and general nouns (nouns
and proper nouns) contained in a text, with over-
lapping general nouns removed. Based on this, we
implemented four kinds of entity-density features:
total number of entity mentions per document, total
number of unique entity mentions per document,
average number of entity mentions per sentence,
and average number of unique entity mentions per
sentence.
We believe entity-density features may also re-
late to the readability of a text for a general au-
dience. In this paper, we conduct a more re-
fined analysis of general nouns and named entities.
To collect entities for each document, we used
OpenNLP?s3 name-finding tool to extract named
entities; general nouns are extracted from the out-
put of Charniak?s Parser (see Section 3.3). Based
on the set of entities collected for each document,
we implement 12 new features. We list several of
these features in in Table 2.
3.1.2 Lexical Chain Features
During reading, a more challenging task with enti-
ties is not just to keep track of them, but to resolve
the semantic relations among them, so that infor-
mation can be processed, organized and stored in
a structured way for comprehension and later re-
trieval. In earlier work (Feng et al, 2009), we
used a lexical-chaining tool developed by Galley
and McKeown (2003) to annotate six semantic re-
lations among entities, e.g. synonym, hypernym,
hyponym, etc. Entities that are connected by these
semantic relations were linked through the text to
form lexical chains. Based on these chains, we
implemented six features, listed in Table 3, which
3http://opennlp.sourceforge.net/
Table 3: Lexical Chain Features
1 total number of lexical chains per document
2 avg. lexical chain length
3 avg. lexical chain span
4 num. of lex. chains with span ? half doc. length
5 num. of active chains per word
6 num. of active chains per entity
Table 4: Coreference Chain Features
1 total number of coreference chains per document
2 avg. num. of coreferences per chain
3 avg. chain span
4 num. of coref. chains with span ? half doc. length
5 avg. inference distance per chain
6 num. of active coreference chains per word
7 num. of active coreference chains per entity
we use in our current study. The length of a chain
is the number of entities contained in the chain,
the span of chain is the distance between the index
of the first and last entity in a chain. A chain is
defined to be active for a word or an entity if this
chain passes through its current location.
3.1.3 Coreference Inference Features
Relations among concepts and propositions are of-
ten not stated explicitly in a text. Automatically re-
solving implicit discourse relations is a hard prob-
lem. Therefore, we focus on one particular type,
referential relations, which are often established
through anaphoric devices, e.g. pronominal refer-
ences. The ability to resolve referential relations is
important for text comprehension.
We use OpenNLP to resolve coreferences. En-
tities and pronominal references that occur across
the text and refer to the same person or object
are extracted and formed into a coreference chain.
Based on the chains extracted, we implement seven
features as listed in Table 4. The chain length,
chain span and active chains are defined in a sim-
ilar way to the lexical chain features. Inference
distance is the difference between the index of the
referent and that of its pronominal reference. If the
same referent occurs more than once in a chain,
the index of the closest occurrence is used when
computing the inference distance.
3.1.4 Entity Grid Features
Coherent texts are easier to read. Several computa-
tional models have been developed to represent and
278
measure discourse coherence (Lapata and Barzilay,
2005; Soricut and Marcu, 2006; Elsner et al, 2007;
Barzilay and Lapata, 2008) for NLP tasks such as
text ordering and text generation. Although these
models are not intended directly for readability re-
search, Barzilay and Lapata (2008) have reported
that distributional properties of local entities gen-
erated by their grid models are useful in detecting
original texts from their simplified versions when
combined with well studied lexical and syntactic
features. This approach was subsequently pursued
by Pitler and Nenkova (2008) in their readability
study. Barzilay and Lapata?s entity grid model is
based on the assumption that the distribution of
entities in locally coherent texts exhibits certain
regularities. Each text is abstracted into a grid
that captures the distribution of entity patterns at
the level of sentence-to-sentence transitions. The
entity grid is a two-dimensional array, with one di-
mension corresponding to the salient entities in the
text, and the other corresponding to each sentence
of the text. Each grid cell contains the grammatical
role of the specified entity in the specified sentence:
whether it is a subject (S), object (O), neither of
the two (X), or absent from the sentence (-).
We use the Brown Coherence Toolkit (v0.2) (El-
sner et al, 2007), based on (Lapata and Barzilay,
2005), to generate an entity grid for each text in
our corpus. The distribution patterns of entities
are traced between each pair of adjacent sentences,
resulting in 16 entity transition patterns4. We then
compute the distribution probability of each entity
transition pattern within a text to form 16 entity-
grid-based features.
3.2 Language Modeling Features
Our language-modeling-based features are inspired
by Schwarm and Ostendorf?s (2005) work, a study
that is closely related to ours. They used data
from the same data ? the Weekly Reader ? for
their study. They trained three language mod-
els (unigram, bigram and trigram) on two paired
complex/simplified corpora (Britannica and Litera-
cyNet) using an approach in which words with high
information gain are kept and the remaining words
4These 16 transition patterns are: ?SS?, ?SO?, ?SX?, ?S-?,
?OS?, ?OO?, ?OX?, ?O-?, ?XS?, ?XO?, ?XX?, ?X-?, ?-S?,
?-O?, ?-X?, ?- -?.
are replaced with their parts of speech. These lan-
guage models were then used to score each text
in the Weekly Reader corpus by perplexity. They
reported that this approach was more successful
than training LMs on text sequences of word la-
bels alone, though without providing supporting
statistics.
It?s worth pointing out that their LMs were not
trained on the Weekly Reader data, but rather on
two unrelated paired corpora (Britannica and Lit-
eracyNet). This seems counter-intuitive, because
training LMs directly on the Weekly Reader data
would provide more class-specific information for
the classifiers. They justified this choice by stating
that splitting limited Weekly Reader data for train-
ing and testing purposes resulted in unsuccessful
performance.
We overcome this problem by using a hold-
one-out approach to train LMs directly on our
Weekly Reader corpus, which contains texts rang-
ing from Grade 2 to 5. We use grade levels to
divide the whole corpus into four smaller subsets.
In addition to implementing Schwarm and Osten-
dorf?s information-gain approach, we also built
LMs based on three other types of text sequences
for comparison purposes. These included: word-
token-only sequence (i.e., the original text), POS-
only sequence, and paired word-POS sequence.
For each grade level, we use the SRI Language
Modeling Toolkit5 (with Good-Turing discounting
and Katz backoff for smoothing) to train 5 lan-
guage models (1- to 5-gram) using each of the four
text sequences, resulting in 4?5?4= 80 perplex-
ity features for each text tested.
3.3 Parsed Syntactic Features
Schwarm and Ostendorf (2005) studied four parse
tree features (average parse tree height, average
number of SBARs, noun phrases, and verb phrases
per sentences). We implemented these and addi-
tional features, using the Charniak parser (Char-
niak, 2000). Our parsed syntactic features focus on
clauses (SBAR), noun phrases (NP), verb phrases
(VP) and prepositional phrases (PP). For each
phrase, we implement four features: total num-
ber of the phrases per document, average number
of phrases per sentence, and average phrase length
5http://www.speech.sri.com/projects/srilm/
279
measured by number of words and characters re-
spectively. In addition to average tree height, we
implement two non-terminal-node-based features:
average number of non-terminal nodes per parse
tree, and average number of non-terminal nodes
per word (terminal node).
3.4 POS-based Features
Part-of-speech-based grammatical features were
shown to be useful in readability prediction (Heil-
man et al, 2007; Leroy et al, 2008). To extend
prior work, we systematically studied a number of
common categories of words and investigated to
what extent they are related to a text?s complex-
ity. We focus primarily on five classes of words
(nouns, verbs, adjectives, adverbs, and preposi-
tions) and two broad categories (content words,
function words). Content words include nouns,
verbs, numerals, adjectives, and adverbs; the re-
maining types are function words. The part of
speech of each word is obtained from examining
the leaf node based on the output of Charniak?s
parser, where each leaf node consists of a word and
its part of speech. We group words based on their
POS labels. For each class of words, we imple-
ment five features. For example, for the adjective
class, we implemented the following five features:
percent of adjectives (tokens) per document, per-
cent of unique adjectives (types) per document,
ratio of unique adjectives per total unique words
in a document, average number of adjectives per
sentence and average number of unique adjectives
per sentence.
3.5 Shallow Features
Shallow features refer to those used by traditional
readability metrics, such as Flesch-Kincaid Grade
Level (Flesch, 1979), SMOG (McLaughlin, 1969),
Gunning FOG (Gunning, 1952), etc. Although
recent readability studies have strived to take ad-
vantage of NLP techniques, little has been revealed
about the predictive power of shallow features.
Shallow features, which are limited to superficial
text properties, are computationally much less ex-
pensive than syntactic or discourse features. To en-
able a comparison against more advanced features,
we implement 8 frequently used shallow features
as listed in Table 5.
Table 5: Shallow Features
1 average number of syllables per word
2 percentage of poly-syll. words per doc.
3 average number of poly-syll. words per sent.
4 average number of characters per word
5 Chall-Dale difficult words rate per doc.
6 average number of words per sentence
7 Flesch-Kincaid score
8 total number of words per document
3.6 Other Features
For comparison, we replicated 6 out-of-vocabulary
features described in Schwarm and Ostendorf
(2005). For each text in the Weekly Reader corpus,
these 6 features are computed using the most com-
mon 100, 200 and 500 word tokens and types based
on texts from Grade 2. We also replicated the 12
perplexity features implemented by Schwarm and
Ostendorf (2005) (see Section 3.2).
4 Experiments and Discussion
Previous studies on reading difficulty explored vari-
ous statistical models, e.g. regression vs. classifica-
tion, with varying assumptions about the measure-
ment of reading difficulty, e.g. whether labels are
ordered or unrelated, to test the predictive power
of models (Heilman et al, 2008; Petersen and Os-
tendorf, 2009; Aluisio et al, 2010). In our re-
search, we have used various models, including
linear regression; standard classification (Logis-
tic Regression and SVM), which assumes no rela-
tion between grade levels; and ordinal regression/
classification (provided by Weka, with Logistic
Regression and SMO as base function), which as-
sumes that the grade levels are ordered. Our exper-
iments show that, measured by mean squared error
and classification accuracy, linear regression mod-
els perform considerably poorer than classification
models. Measured by accuracy and F-measure,
ordinal classifiers perform comparable or worse
than standard classifiers. In this paper, we present
the best results, which are obtained by standard
classifiers. We use two machine learning packages
known for efficient high-quality multi-class classi-
fication: LIBSVM (Chang and Lin, 2001) and the
Weka machine learning toolkit (Hall et al, 2009),
from which we choose Logistic Regression as clas-
sifiers. We train and evaluate various prediction
280
Table 6: Comparison of discourse features
Feature Set LIBSVM Logistic Regress.
Entity-Density 59.63?0.632 57.59?0.375
Lexical Chain 45.86?0.815 42.58?0.241
Coref. Infer. 40.93?0.839 42.19?0.238
Entity Grid 45.92?1.155 42.14?0.457
all combined 60.50?0.990 58.79?0.703
models using the features described in Section 3.
We evaluate classification accuracy using repeated
10-fold cross-validation on the Weekly Reader cor-
pus. Classification accuracy is defined as the per-
centage of texts predicted with correct grade levels.
We repeat each experiment 10 times and report the
mean accuracy and its standard deviation.
4.1 Discourse Features
We first discuss the improvement made by extend-
ing our earlier entity-density features (Feng et al,
2009). We used LIBSVM to train and test mod-
els on the Weekly Reader corpus with our earlier
features and our new features respectively. With
earlier features only, the model achieves 53.66%
accuracy. With our new features added, the model
performance is 59.63%.
Table 6 presents the classification accuracy of
models trained with discourse features. We see
that, among four subsets of discourse features,
entity-density features perform significantly better
than the other three feature sets and generate the
highest classification accuracy (LIBSVM: 59.63%,
Logistic Regression: 57.59%). While Logistic Re-
gression results show that there is not much perfor-
mance difference among lexical chain, coreference
inference, and entity grid features, classification
accuracy of LIBSVM models indicates that lexical
chain features and entity grid features are better
in predicting text readability than coreference in-
ference features. Combining all discourse features
together does not significantly improve accuracy
compared with models trained only with entity-
density features.
4.2 Language Modeling Features
Table 7 compares the performance of models gen-
erated using our approach and our replication of
Schwarm and Ostendorf?s (2005) approach. In our
approach, features were obtained from language
Table 7: Comparison of lang. modeling features
Feature Set LIBSVM Logistic Regress.
IG 62.52?1.202 62.14?0.510
Text-only 60.17?1.206 60.31?0.559
POS-only 56.21?2.354 57.64?0.391
Word/POS pair 60.38?0.820 59.00?0.367
all combined 68.38?0.929 66.82?0.448
IG by Schwarm 52.21?0.832 51.89?0.405
Table 8: Comparison of parsed syntactic features
Feature Set # Feat. LIBSVM
Original features 4 50.68?0.812
Expanded features 21 57.79?1.023
models trained on the Weekly Reader corpus. Not
surprisingly, these are more effective than LMs
trained on the Britannica and LiteracyNet corpora,
in Schwarm and Ostendorf?s approach. Our results
support their claim that LMs trained with infor-
mation gain outperform LMs trained with POS la-
bels. However, we also notice that training LMs on
word labels alone or paired word/POS sequences
achieved similar classification accuracy to the IG
approach, while avoiding the complicated feature
selection of the IG approach.
4.3 Parsed Syntactic Features
Table 8 compares a classifier trained on the four
parse features of Schwarm and Ostendorf (2005) to
a classifier trained on our expanded set of parse fea-
tures. The LIBSVM classifier with the expanded
feature set scored 7 points higher than the one
trained on only the original four features, improv-
ing from 50.68% to 57.79%. Table 9 shows a
detailed comparison of particular parsed syntactic
features. The two non-terminal-node-based fea-
tures (average number of non-terminal nodes per
tree and average number of non-terminal nodes
per word) have higher discriminative power than
average tree height. Among SBARs, NPs, VPs and
PPs, our experiments show that VPs and NPs are
the best predictors.
4.4 POS-based Features
The classification accuracy generated by models
trained with various POS features is presented
in Table 10. We find that, among the five word
classes investigated, noun-based features gener-
281
Table 9: Detailed comp. of syntactic features
Feature Set LIBSVM Logistic Regress.
Non-term.-node ratios 53.02?0.571 51.80?0.171
Average tree height 44.26?0.914 43.45?0.269
SBARs 44.42?1.074 43.50?0.386
NPs 51.56?1.054 48.14?0.408
VPs 53.07?0.597 48.67?0.484
PPs 49.36?1.277 46.47?0.374
all combined 57.79?1.023 54.11?0.473
Table 10: Comparison of POS features
Feature Set LIBSVM Logistic Regress.
Nouns 58.15?0.862 57.01?0.256
Verbs 54.40?1.029 55.10?0.291
Adjectives 53.87?1.128 52.75?0.427
Adverbs 52.66?0.970 50.54?0.327
Prepositions 56.77?1.278 54.13?0.312
Content words 56.84?1.072 56.18?0.213
Function words 52.19?1.494 50.95?0.298
all combined 59.82?1.235 57.86?0.547
ate the highest classification accuracy, which is
consistent with what we have observed earlier
about entity-density features. Another notable ob-
servation is that prepositions demonstrate higher
discriminative power than adjectives and adverbs.
Models trained with preposition-based features per-
form close to those trained with noun-based fea-
tures. Among the two broader categories, content
words (which include nouns) demonstrate higher
predictive power than function words (which in-
clude prepositions).
4.5 Shallow Features
We present some notable findings on shallow fea-
tures in Table 11. Experimental results generated
by models trained with Logistic Regression show
that average sentence length has dominating predic-
tive power over all other shallow features. Features
based on syllable counting perform much worse.
The Flesch-Kincaid Grade Level score uses a fixed
linear combination of average words per sentence
and average syllables per word. Combining those
two features (without fixed coefficients) results in
the best overall accuracy, while using the Flesch-
Kincaid score as a single feature is significantly
worse.
Table 11: Comparison of shallow features
Feature Set Logistic Regress.
Avg. words per sent. 52.17?0.193
Avg. syll. per word 42.51?0.264
above two combined 53.04?0.514
Flesch-Kincaid score 50.83?0.144
Avg. poly-syll. words per sent. 45.70?0.306
all 8 features combined 52.34?0.242
4.6 Comparison with Previous Studies
A trivial baseline of predicting the most frequent
grade level (grade 5) predicts 542 out of 1433 texts
(or 37.8%) correctly. With this in mind, we first
compare our study with the widely-used Flesch-
Kincaid Grade Level formula, which is a linear
function of average words per sentence and average
syllables per word that aims to predict the grade
level of a text directly. Since this is a fixed formula
with known coefficients, we evaluated it directly
on our entire Weekly Reader corpus without cross-
validation. We obtain the predicted grade level
of a text by rounding the Flesch-Kincaid score
to the nearest integer. For only 20 out of 1433
texts the predicted and labeled grade levels agree,
resulting in a poor accuracy of 1.4%. By contrast,
using the Flesch-Kincaid score as a feature of a
simple logistic regression model achieves above
50% accuracy, as discussed in Section 4.5.
The most closely related previous study is the
work of Schwarm and Ostendorf (2005). How-
ever, because their experiment design (85/15 train-
ing/test data split) and machine learning tool
(SV Mlight) differ from ours, their results are not
directly comparable to ours. To make a compar-
ison, we replicated all the features used in their
study and then use LIBSVM and Weka?s Logistic
Regression to train two models with the replicated
features and evaluate them on our Weekly Reader
corpus using 10-fold cross-validation.
Using the same experiment design, we train clas-
sifiers with three combinations of our features as
listed in Table 12. ?All features? refers to a naive
combination of all features. ?AddOneBest? refers
to a subset of features selected by a group-wise
add-one-best greedy feature selection. ?WekaFS?
refers to a subset of features chosen by Weka?s
feature selection filter.
?WekaFS? consists of 28 features selected au-
282
Table 12: Comparison with previous work
baseline accuracy (majority class) 37.8
Flesch-Kincaid Grade Level 1.4
Feature Set # Feat. LIBSVM Logistic Reg.
Schwarm 25 63.18?1.664 60.50?0.477
All features 273 72.21?0.821 63.71?0.576
AddOneBest 122 74.01?0.847 69.22?0.411
WekaFS 28 70.06?0.777 65.46?0.336
tomatically by Weka?s feature selection filter us-
ing a best-first search method. The 28 features
include language modeling features, syntactic fea-
tures, POS features, shallow features and out-of-
vocabulary features. Aside from 4 shallow features
and 5 out-of-vocabulary features, the other 19 fea-
tures are novel features we have implemented for
this paper.
As Table 12 shows, a naive combination of all
features results in classification accuracy of 72%,
which is much higher than the current state of the
art (63%). This is not very surprising, since we are
considering a greater variety of features than any
previous individual study. Our WekaFS classifier
uses roughly the same number of features as the
best published result, yet it has a higher accuracy
(70.06%). Our best results were obtained by group-
wise add-one-best feature selection, resulting in
74% classification accuracy, a big improvement
over the state of the art.
5 Conclusions
We examined the usefulness of features at various
linguistic levels for predicting text readability in
terms of assigning texts to elementary school grade
levels. We implemented a set of discourse features,
enriched previous work by creating several new
features, and systematically tested and analyzed
the impact of these features.
We observed that POS features, in particular
nouns, have significant predictive power. The high
discriminative power of nouns in turn explains the
good performance of entity-density features, based
primarily on nouns. In general, our selected POS
features appear to be more correlated to text com-
plexity than syntactic features, shallow features
and most discourse features.
For parsed syntactic features, we found that verb
phrases appear to be more closely correlated with
text complexity than other types of phrases. While
SBARs are commonly perceived as good predic-
tors for syntactic complexity, they did not prove
very useful for predicting grade levels of texts in
this study. In future work, we plan to examine this
result in more detail.
Among the 8 shallow features, which are used
in various traditional readability formulas, we iden-
tified that average sentence length has dominating
predictive power over all other lexical or syllable-
based features.
Not surprisingly, among language modeling
features, combined features obtained from LMs
trained directly on the Weekly Reader corpus show
high discriminative power, compared with features
from LMs trained on unrelated corpora.
Discourse features do not seem to be very use-
ful in building an accurate readability metric. The
reason could lie in the fact that the texts in the cor-
pus we studied exhibit relatively low complexity,
since they are aimed at primary-school students. In
future work, we plan to investigate whether these
discourse features exhibit different discriminative
power for texts at higher grade levels.
A judicious combination of features examined
here results in a significant improvement over the
state of the art.
References
Sandra Aluisio, Lucia Specia, Caroline Gasperin,
and Carolina Scarton. 2010. Readability assess-
ment for text simplification. In NAACL-HLT
2010: The 5th Workshop on Innovative Use of
NLP for Building Educational Applications.
Regina Barzilay and Mirella Lapata. 2008. Model-
ing local coherence: An entity-based approach.
Computational Linguistics, 34(1):1?34.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIB-
SVM: A Library for Support Vector Machines.
Software available at http://www.csie.ntu.
edu.tw/~cjlin/libsvm.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st Con-
ference of the North American Chapter of the
ACL, pages 132?139.
283
Kevyn Collins-Thompson and Jamie Callan. 2004.
A language modeling approach to predicting
reading difficulty. In Proceedings of the Hu-
man Language Technology Conference of the
North American Chapter of the Association for
Computational Linguistics (HLT-NAACL 2004).
Micha Elsner, Joseph Austerweil, and Eugene
Charniak. 2007. A unified local and global
model for discourse coherence. In Proceed-
ings of the Conference on Human Language
Technology and North American chapter of the
Association for Computational Linguistics (HLT-
NAACL 2007).
Lijun Feng, Noe?mie Elhadad, and Matt Huener-
fauth. 2009. Cognitively motivated features for
readability assessment. In The 12th Conference
of the European Chapter of the Association for
Computational Linguistics (EACL 2009).
Rudolf Flesch. 1979. How to write plain English.
Harper and Brothers, New York.
Michel Galley and Kathleen McKeown. 2003. Im-
proving word sense disambiguation in lexical
chaining. In Proceedings of the 18th Inter-
national Joint Conference on Artificial Intelli-
gence.
Robert Gunning. 1952. The Technique of Clear
Writing. McGraw-Hill.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bern-
hard Pfahringer, Peter Reutemann, and Ian H.
Witten. 2009. The WEKA data mining software:
An update. SIGKDD Explorations, 11(1):10?18.
Michael J. Heilman, Kevyn Collins-Thompson,
Jamie Callan, and Maxine Eskenazi. 2007. Com-
bining lexical and grammatical features to im-
prove readability measures for first and second
language texts. In Human Language Technolo-
gies 2007: The Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics.
Michael J. Heilman, Kevyn Collins-Thompson,
and Maxine Eskenazi. 2008. An analysis of sta-
tistical models and features for reading difficulty
prediction. In ACL 2008: The 3rd Workshop on
Innovative Use of NLP for Building Educational
Applications.
Mirella Lapata and Regina Barzilay. 2005. Auto-
matic evaluation of text coherence: Models and
representations. In Proceedings of the Interna-
tional Joint Conference on Artificial Intelligence
(IJCAI?05), pages 1085?1090.
Gondy Leroy, Stephen Helmreich, James R. Cowie,
Trudi Miller, and Wei Zheng. 2008. Evaluating
online health information: Beyond readability
formulas. In AMIA 2008 Symposium Proceed-
ings.
G. Harry McLaughlin. 1969. Smog grading a
new readability formula. Journal of Reading,
12(8):639?646.
Sarah E. Petersen and Mari Ostendorf. 2006. A
machine learning approach to reading level as-
sessment. Technical report, University of Wash-
ington CSE Technical Report.
Sarah E. Petersen and Mari Ostendorf. 2009. A ma-
chine learning approach to reading level assess-
ment. Computer Speech and Language, 23:89?
106.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predict-
ing text quality. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki,
L. Robaldo, A. Joshi, and B. Webber. 2008. The
Penn discourse treebank. In The Sixth Interna-
tional Conference on Language Resources and
Evaluation (LREC?08).
Sarah E. Schwarm and Mari Ostendorf. 2005.
Reading level assessment using support vector
machines and statistical language models. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics.
Luo Si and Jamie Callan. 2001. A statistical model
for scientific readability. In Proceedings of the
Tenth International Conference on Information
and Knowledge Management.
Radu Soricut and Daniel Marcu. 2006. Discourse
generation using utility-trained coherence mod-
els. In Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics.
284
Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 89?97,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Collecting a Motion-Capture Corpus of American Sign Language  for Data-Driven Generation Research 
Pengfei Lu Department of Computer Science Graduate Center City University of New York (CUNY) 365 Fifth Ave, New York, NY 10016 pengfei.lu@qc.cuny.edu 
Matt Huenerfauth Department of Computer Science Queens College and Graduate Center City University of New York (CUNY) 65-30 Kissena Blvd, Flushing, NY 11367 matt@cs.qc.cuny.edu 
 
 
Abstract 
American Sign Language (ASL) generation software can improve the accessibility of in-formation and services for deaf individuals with low English literacy.  The understand-ability of current ASL systems is limited; they have been constructed without the benefit of annotated ASL corpora that encode detailed human movement.  We discuss how linguistic challenges in ASL generation can be ad-dressed in a data-driven manner, and we de-scribe our current work on collecting a motion-capture corpus. To evaluate the qual-ity of our motion-capture configuration, cali-bration, and recording protocol, we conducted an evaluation study with native ASL signers. 1 Introduction American Sign Language (ASL) is the primary means of communication for about one-half mil-lion deaf people in the U.S. (Mitchell et al, 2006).  ASL has a distinct word-order, syntax, and lexicon from English; it is not a representation of English using the hands.  Although reading is part of the curriculum for deaf students, lack of auditory ex-posure to English during the language-acquisition years of childhood leads to lower literacy for many adults.  In fact, the majority of deaf high school graduates in the U.S. have only a fourth-grade (age 10) English reading level (Traxler, 2000).   1.1 Applications of ASL Generation Research Most technology used by the deaf does not address this literacy issue; many deaf people find it diffi-
cult to read the English text on a computer screen or on a television with closed-captioning. Software to present information in the form of animations of ASL could make information and services more accessible to deaf users, by displaying an animated character performing ASL, rather than English text.  While writing systems for ASL have been proposed (Newkirk, 1987; Sutton, 1998), none is widely used in the Deaf community.  Thus, an ASL generation system cannot produce text output; the system must produce an animation of a human character performing sign language.  Coordinating the simultaneous 3D movements of parts of an animated character?s body is challenging, and few researchers have attempted to build such systems.   Prior work can be divided into two areas: scripting and generation/translation. Scripting sys-tems allow someone who knows sign language to ?word process? an animation by assembling a se-quence of signs from a lexicon and adding facial expressions.  The eSIGN project created tools for content developers to build sign databases and as-semble scripts of signing for web pages (Ken-naway et al, 2007).  Sign Smith Studio (Vcom3D, 2010) is a commercial tool for scripting ASL (dis-cussed in section 4).  Others study generation or machine translation (MT) of sign language (Chiu et al, 2007; Elliot & Glauert, 2008; Fotinea et al, 2008; Huenerfauth, 2006; Karpouzis et al, 2007; Marshall & Safar, 2005; Shionome et al, 2005; Sumihiro et al, 2000; van Zijl & Barker, 2003). Experimental evaluations of the understandabil-ity of state-of-the-art ASL animation systems have shown that native signers often find animations difficult to understand (as measured by compre-
89
hension questions) or unnatural (as measured by subjective evaluation questions) (Huenerfauth et al, 2008).  Errors include a lack of smooth inter-sign transitions, lack of grammatically-required facial expressions, and inaccurate sign perform-ances related to morphological inflection of signs. While current ASL animation systems have limitations, there are several advantages in present-ing sign language content in the form of animated virtual human characters, rather than videos: ? Generation or MT software planning ASL sen-tences cannot just concatenate videos of ASL.  Using video clips, it is difficult to produce smooth transitions between signs, subtle mo-tion variations in sign performances, or proper combinations of facial expressions with signs. ? If content must be frequently modified or up-dated, then a video performance would need to be largely re-recorded for each modification.  Whereas, an animation (scripted by a human author) could be further edited or modified. ? Because the face is used to indicate important information in ASL, a human must reveal his or her identity when producing an ASL video. Instead, a virtual human character could per-form sentences scripted by a human author. ? For wiki-style applications in which multiple authors are collaborating on information con-tent, ASL videos would be distracting: the per-son performing each sentence may differ.  A virtual human would be more uniform. ? Animations can be appealing to children for use in educational applications.  ? Animations allow ASL to be viewed at differ-ent angles, at different speeds, or by different virtual humans ? depending on the preferences of the user.  This can enable education applica-tions in which students learning ASL can prac-tice their ASL comprehension skills. 1.2 ASL is Challenging for NLP Research Natural Language Processing (NLP) researchers often apply techniques originally designed for one language to another, but research is not commonly ported to sign languages. One reason is that with-out a written form for ASL, NLP researchers must produce animation and thus address several issues: ? Timing: An ASL performance?s speed consists of: the speed of individual sign performances, 
the transitional time between signs, and the in-sertion of pauses during signing ? all of which are based on linguistic factors such as syntactic boundaries, repetition of signs in a discourse, and the part-of-speech of signs (Grosjean et al, 1979). ASL animations whose speed and paus-ing are incorrect are significantly less under-standable to ASL signers (Huenerfauth, 2009). ? Spatial Reference: Signers arrange invisible placeholders in the space around their body to represent objects or persons under discussion (Meier, 1990). To perform personal, posses-sive, or reflexive pronouns that refer to these entities, signers later point to these locations. Signers may not repeat the identity of these en-tities again; so, their conversational partner must remember where they have been placed.  An ASL generator must select which entities should be assigned 3D locations (and where). ? Inflection: Many verbs change their motion paths to indicate the 3D location where a spa-tial reference point has been established for their subject, object, or both (Padden, 1988). Generally, the motion paths of these inflecting verbs change so that their direction goes from the subject to the object (Figure 1); however, their paths are more complex than this.  Each verb has a standard motion path that is affected by the subject?s and the object?s 3D locations.  When a verb is inflected in this way, the signer does not need to overtly state the subject/object of a sentence. An ASL generator must produce appropriately inflected verb paths based on the layout of the spatial reference points. 
(a.)  
(b.)  Figure 1: An ASL inflecting verb ?BLAME?:  (a.) (person on left) blames (person on right),  (b.) (person on right) blames (person on left). 
90
? Coarticulation: As in speech production, the surrounding signs in a sentence affect finger, hand, and body movements.  ASL generators that use overly simple interpolation rules to produce these coarticulation effects yield un-natural and non-fluent ASL animation output. ? Non-Manuals: Head-tilt and eye-gaze indicate the 3D location of a verb?s subject and object (or other information); facial expressions also indicate negation, questions, topicalization, and other essential syntactic phenomena not conveyed by the hands (Neidle et al, 2000). Animations without proper facial expressions (and proper timing relative to manual signs) cannot convey the proper meaning of ASL sen-tences in a fluent and understandable manner. ? Evaluation: With no standard written form for ASL, string-based metrics cannot be used to evaluate ASL generation output automatically. User-based experiments are necessary, but it is difficult to accurately: screen for native sign-ers, prevent English environmental influences (that affect signer?s linguistic judgments), and design questions that measure comprehension of ASL animations (Huenerfauth et al, 2008). 1.3 Need for Data-Driven ASL Generation Due to these challenges, most prior sign language generation or MT projects have been short-lived, producing few example outputs (Zhao et al, 2000; Veale et al, 1998). Further developed systems also have limited coverage; e.g., Marshall and Safar (2005) hand-built translation transfer rules from English to British Sign Language. Huenerfauth (2006) surveys several rule-based systems and dis-cusses how they generally: have limited coverage; often merely concatenate signs; and do not address the Coarticulation, Spatial Reference, Timing, Non-Manuals, or Inflection issues (section 1.2).  Unfortunately, most prior work is not ?data-driven,? i.e. not based on statistical modeling of corpora, the dominant successful modern NLP ap-proach. The sign language generation research that has thus far been the most data-driven includes: ? Some researchers have used motion-capture (see section 3) to build lexicons of animations of individual signs, e.g. (Cox et al, 2002). However, their focus is recording a single cita-tion form of each sign, not creating annotated corpora of full sentences or discourse. Single-
sign recordings do not enable researchers to examine the Timing, Coarticulation, Spatial Reference, Non-Manuals, or Inflection phe-nomena (section 1.2), which operate over mul-tiple signs or sentences in an ASL discourse. ? Other researchers have examined how statisti-cal MT techniques could be used to translate from a written language to a sign language. Morrissey and Way (2005) discuss an exam-ple-based MT architecture for Irish Sign Lan-guage, and Stein et al (2006) apply simple statistical MT approaches to German Sign Language. Unfortunately, the sign language ?corpora? used in these studies consist of tran-scriptions of the sequence of signs performed, not recordings of actual human performances.  A transcription does not capture subtleties in the 3D movements of the hands, facial move-ments, or speed of an ASL performance.  Such information is needed in order to address the Spatial Reference, Inflection, Coarticulation, Timing, or Non-Manuals issues (section 1.2). ? Seguoat and Braffort (2009) derive models of coarticulation for French Sign Language based on a semi-automated ?rotoscoping? annotation of hand location from videos of signing. 1.4 Prior Sign Language Corpora Resources The reason why most prior ASL generation re-search has not been data-driven is that sufficiently detailed and annotated sign language corpora are in short supply and are time-consuming to construct. Without a writing system in common use, it is not possible to harvest some naturally arising source of ASL ?text?; instead, it is necessary to record the performance of a signer (through video or a mo-tion-capture suit).  Human signers must then tran-scribe and annotate this data by adding time-stamped linguistic details. For ASL (Neidle et al, 2000) and European sign languages (Bungeroth et al, 2006; Crasborn et al, 2004, 2006; Efthimiou & Fotinea, 2007), signers have been videotaped and experts marked time spans when events occur ? e.g. the right hand is performing the sign ?CAT? during time index 250-300 milliseconds, and the eyebrows are raised during time index 270-300. Such annotation is time-consuming to add; the largest ASL corpus has a few thousand sentences.   In order to learn how to control the movements of an animated virtual human based on a corpus, 
91
we need precise hand locations and joint angles of the human signer?s body throughout the perform-ance.  Asking humans to write down 3D angles and coordinates is time-consuming and inexact; some researchers have used computer vision techniques to model the signers? movements (see survey in (Loeding et al, 2004)).  Unfortunately, the com-plex shape of hands/face, rapid speed, and frequent occlusion of parts of the body during ASL limit the accuracy of vision-based recognition; it is not yet a reliable way to build a 3D model of a signer for a corpus.  Motion-capture technology (discussed in section 3) is required for this level of detail.   2 Research Goals & Focus of This Paper To address the lack of sufficiently detailed and linguistically annotated ASL corpora, we have be-gun a multi-year project to collect and annotate a motion-capture corpus of ASL (section 3). Digital 3D body movement and handshape data collected from native signers will become a permanent re-search resource for study by NLP researchers and ASL linguists. This corpus will allow us to create new ASL generation technologies in a data-driven manner by analyzing the subtleties in the motion data and its relationship to the linguistic structure. Specifically, we plan to model where signers tend to place spatial reference points around them in space. We also plan to uncover patterns in the mo-tion paths of inflecting verbs and model how they relate to layout of spatial references points. These models could be used in ASL generation software or could be used to partially automate with work of humans using ASL-scripting systems. To evaluate our ASL models, native signers will be asked to judge ASL animations produced using them. There are several unique aspects of our research: ? We use a novel combination of hand, body, head, and eye motion-tracking technologies and simultaneous video recordings (section 3). ? We collect multi-sentence single-signer ASL discourse, and we annotate novel linguistic in-formation (relevant to spatial reference points). ? We involve ASL signers in the research in several ways: as evaluators of our generation software, as research assistants conducting evaluation studies, and as corpus annotators. This paper will focus on the first of these as-pects of our project. Specifically, section 4 will 
examine the following research question: Have we successfully configured and calibrated our motion-capture equipment so that we are recording good-quality data that will be useful for NLP research?   Since the particular combination of motion-capture equipment we are using is novel and be-cause there have not been prior motion-capture-based ASL corpora projects, section 4 will evaluate whether the data we are collecting is of sufficient quality to drive ASL animations of a virtual human character.  In corpus-creation projects for tradi-tional written/spoken languages, researchers typi-cally gather text, audio, or (sometimes) video of human performances.  The quality of the gathered recordings is typically easier to verify and evalu-ate; for motion-capture data collected with a com-plex configuration of equipment, a more complex experimental design is necessary (section 4). 3 Our Motion-Capture Configuration The first stage of our research is to accurately and efficiently record 3D motion-capture data from ASL signers.  Assuming an ASL signer?s pelvis bone is stationary in 3D space, we want to record movement data for the upper body.  We are inter-ested in the shapes of each hand; the 3D location of the hands; the 3D orientation of the palms; joint angles for the wrists, elbows, shoulders, clavicle, neck, and waist; and a vector representing the eye-gaze aim.  We are using a customized configura-tion of several commercial motion-capture devices (as shown in Figure 2, worn by a human signer): ? Two Immersion CyberGloves?: The 22 flexi-ble sensor strips sewn into each of these spandex gloves record finger joint angles so that we can record the signer?s handshapes.  These gloves are ideal for recording ASL be-cause they are flexible and lightweight.  Hu-mans viewing a subject wearing the gloves are able to discern ASL fingerspelling and signing. ? Applied Science Labs H6 eye-tracker: This lightweight head-mounted eye-tracker with a near-eye camera records a signer?s eye gaze di-rection. A camera on the headband aims down, and a small clear plastic panel in front of the cheek reflects the image of the subject?s eye. When combined with the head tracking infor-mation from the IS-900 system below, the H6 identifies a 3D vector of eye-gaze in a room. 
92
? Intersense IS-900: This acoustical/intertial mo-tion-capture system uses a ceiling-mounted ul-trasonic speaker array and a set of directional microphones on a small sensor to record the location and orientation of the signer?s head.  A sensor sits atop the helmet shown in Figure 2a. IS-900 data is used to compensate for head movement when calculating eye-gaze direction with the Applied Science Labs H6 eye-tracker.  ? Animazoo IGS-190: This spandex bodysuit is covered with soft Velcro to which small sen-sors attach.  A sensor placed on each segment of the human?s body records inertial and mag-netic information.  Subjects wearing the suit stand facing north with their arms down at their sides at the beginning of the recording session; given this known starting pose and di-rection, the system calculates joint angles for the wrists, elbows, shoulders, clavicle, neck, and waist. We do not record leg/foot informa-tion in our corpus. Prior to recording data, we photograph subjects standing in a cube-shaped rig of known size; this allows us to identify bone lengths of the human subject, which are needed for the IGS-190 system to accurately calculate joint angles from the sensor data. Motion-capture recording sessions are video-taped to facilitate later linguistic analysis and an-notation. Videotaping the session also facilitates the ?clean up? of the motion-capture data in post-processing, during which algorithms are applied to adjust synchronization of different sensors or re-move ?jitter? or other noise artifacts from the re-cording.  Three digital high-speed video cameras 
film front view, facial close-up, and side views of the signer ? a setup that has been used in video-based ASL-corpora-building projects (Neidle et al, 2000). The front view is similar to Figure 2a (but wider). The facial close-up view is useful when later identifying specific non-manual facial expres-sions during ASL performances, which are essen-tial to correctly understanding and annotating the collected data. To facilitate synchronizing the three video files during post-processing, a strobe is flashed once at the start of the recording session. A ?blue screen? curtain hangs on the back and side walls of the motion-capture studio.  If future computer vision researchers wish to use this corpus to study ASL recognition from video, it is useful to have solid color walls for ?chroma key? back-ground removal.  Photographic studio lighting with spectra compatible with the eye-tracking system is used to support high-quality video recording.   During data collection, a native ASL signer (called the ?prompter?) sits directly behind the front-view camera to engage the participant wear-ing the suit (the ?performer?) in natural conversa-tion. While the corpus we are collecting consists of unscripted single-signer discourse, prior ASL cor-pora projects have identified the importance of sur-rounding signers with an ASL-centric environment during data collection (Neidle et al, 2000). English influence in the studio must be minimized to pre-vent signers from inadvertently code-switching to an English-like form of signing.  Thus, it is impor-tant that a native signer acts as the prompter, who conversationally suggests topics for the performer to discuss (to be recorded as part of the corpus).   
a.  b.  c.  Figure 2: (a) Motion-capture equipment configuration, (b) animation produced from motion-capture data (shown in evaluation study), and (c) animation produced using Sign Smith (shown in evaluation study). 
93
In our first year, we have collected and anno-tated 58 passages from 6 signers (40 minutes). We prefer to collect multi-sentence passages discuss-ing varied numbers of topics and with few ?classi-fier predicates,? phenomena that aren?t our current research focus.  In (Huenerfauth & Lu, 2010), we discuss details of: the genre of discourse we re-cord, our target linguistic phenomena to capture (spatial reference points and inflected verbs), the types of linguistic annotation added to the corpus, and the effectiveness of different ?prompts? used to elicit the desired type of spontaneous discourse.  This paper focuses on verifying the quality of the motion-capture data we can record using our current equipment configuration and protocols. We want to measure how well we have compensated for several possible sources of error in recordings: ? If a sensor connection is temporarily lost, then data gaps occur. We have selected equipment that does not require line-of-sight connections and tried to arrange the studio to avoid fre-quent dropping of any wireless connections. ? We ask subjects to perform a quick head movement and distinctive eye blink pattern at the beginning of the recording session to facili-tate ?synchronization? of the various motion-capture data streams during post-processing. ? Electronic and physical properties of sensors can lead to ?noise? in the data, which we at-tempt to remove with smoothing algorithms. ? Differences between the bone lengths of the human and the ?virtual skeleton? of the ani-mated character being recorded could lead to ?retargeting? errors, in which the body poses of the human do not match the recording.  We must be careful in the measurement of the bone lengths of the human participant and in the design of the virtual animation skeleton. ? To compensate for differences in how equip-ment sits on the body on different occasions or on different humans, we must set ?calibration? values; e.g., we designed a novel protocol for efficiently and accurately calibrating gloves for ASL signers (Lu & Huenerfauth, 2009).   4 Evaluating Our Collected Motion Data If a speech synthesis researcher were using a novel microphone technology to record audio perform-ances from human speakers to build a corpus, that 
researcher would want to experimentally confirm that the audio recordings were of high enough quality for research.  Even when perfectly clear audio recordings of human speech are recorded in a corpus, the automatic speech synthesis models trained on this data are not perfect.  Degradations in the quality of the corpus would yield even lower quality speech synthesis systems.  In the same way, it is essential that we evaluate the quality of the ASL motion-capture data we are collecting. In an earlier study, we sought to collect motion-data from humans and directly produce animations from them as an ?upper baseline? for an experi-mental study (Huenerfauth, 2006). We were not analyzing the collected data or using it for data-driven generation, we merely wanted the data to directly drive an animation of a virtual human character as a ?virtual puppet.? This earlier project used a different configuration of motion-capture equipment, including an earlier version of Cyber-Gloves? and an optical motion-capture system that required line-of-sight connections between infrared emitters on the signer?s body and cameras around the room.  Unfortunately, the data collected was so poor that the animations produced from the mo-tion-capture were not an ?upper? baseline ? in fact, they were barely understandable to native signers.  Errors arose from dropped connections, poor cali-bration, and insufficient removal of data noise. We have selected different equipment and have designed better protocols for recording high quality ASL data since that earlier study ? to compensate for the ?noise,? ?retargeting,? ?synchronization,? and ?calibration? issues mentioned in section 3.  However, we know that under some recording conditions, the quality of collected motion-capture data is so poor that ?virtual puppet? animations synthesized from it are not understandable. We expect that an even higher level of data quality is needed for a motion-capture corpus, which will be analyzed and manipulated in order to synthesize novel ASL animations from it.  Therefore, we con-ducted a study (discussed below) to evaluate the quality of our current motion-capture configura-tion.  As in our past study, we use the motion-capture data to directly control the body move-ments of a virtual human ?puppet.?  We then ask native ASL signers to evaluate the understandabil-ity and naturalness of the resulting animations (and compare them to some baseline animations pro-duced using ASL-animation scripting software).   
94
In our prior work, a native ASL signer designed a set of ASL stories and corresponding compre-hension questions for use in evaluation studies (Huenerfauth, 2009). The stories? average length is approximately 70 signs, and they consist of news stories, encyclopedia articles, and short narratives. We produced animations of each using Sign Smith Studio (SSS), commercial ASL-animation script-ing software (Vcom3D, 2010). Signs from SSS?s lexicon are placed on a timeline, and linguistically appropriate facial expressions are added. The soft-ware synthesizes an animation of a virtual human performing the story (Figure 2c). In earlier work, we designed algorithms for determining sign-speed and pause-insertion in ASL animations based on linguistic features of the sentence. We conducted a study to compare animations with default timing settings (uniform pauses and speed) and anima-tions governed by our timing algorithm ? at vari-ous speeds. The use of our timing algorithm yielded ASL animations that native signers found more understandable (Huenerfauth, 2009). We are reusing these stories and animations as baselines for comparison in a new evaluation study (below).   While we are collecting unscripted passages in our corpus, it is easier to compare the quality of different versions of animations when using a common set of scripted stories. Thus, we used the script from 10 of the stories above, and each was performed by a native signer, a 22-year-old male who learned ASL prior to age 2. He wore the full set of motion-capture equipment, and we followed the same calibration process and protocols as we do when recording ASL passages for our corpus. The signer rehearsed and memorized each story; ?cue cards? were also available when recording. 
Autodesk MotionBuilder software was used to produce a virtual human whose movements were driven by the motion-capture data (see Figure 2b). While our corpus contains video of facial expres-sion, our motion-capture equipment does not digit-ize it; so, the virtual human character has no facial movements. The recorded signer moved at an av-erage speed of 1.12 signs/second, and so for com-parison, we selected the version of the scripted ASL animations with the closest speed from our earlier study: 1.2 signs/second. (Since the scripted animations are slightly slower and include linguis-tic facial expressions, we expected them to receive higher understandability scores than our motion-capture animations.)  In our earlier work, we pro-duced two versions of each scripted story: one with default timing and one with our novel timing algo-rithm. Both versions are used as baselines for comparison in this new study; thus, we compare three versions of the same set of 10 ASL stories. Using questions designed to screen for native ASL signers developed in prior work (Huenerfauth et al, 2008), we recruited 12 participants to evalu-ate the ASL animations. A native ASL signer con-ducted the studies, in which participants viewed an animation and were then asked two types of ques-tions after each: (1) ten-point Likert-scale ques-tions about the ASL animation?s grammatical correctness, understandability, and naturalness of movement and (2) multiple-choice comprehension questions about basic facts from the story. The comprehension questions were presented in the form of scripted ASL animations (produced in SSS), and answer choices were presented in the form of clip-art images (so that strong English lit-eracy was not necessary). Identical questions were 
 Figure 3: Evaluation and comprehension scores (asterisks mark significant pairwise differences). 
95
used to evaluate the motion-capture animations and the scripted animations.  Examples of the questions are included in (Huenerfauth, 2009). Figure 3 displays results of the Likert-scale sub-jective questions and comprehension-question suc-cess scores for the three types of animations evaluated in this study. The scripted animations using our timing algorithm have higher compre-hension scores, but the motion-capture animations have higher naturalness scores.  All of the other scores for the animations are quite similar. Statisti-cally significant differences are marked with an asterisk (p<0.05, Mann-Whitney pairwise compari-sons with Bonferroni-corrected p-values). Non-parameteric tests were selected because the Likert-scale responses were not normally distributed. 5 Conclusion and Future Research Goals The research question addressed by this paper was whether our motion-capture configuration and re-cording protocols enabled us to collect motion-data of sufficient quality for data-driven ASL genera-tion research. In our study, the evaluation scores of the animations driven by the motion-capture data were similar to those of animations produced using state-of-the-art ASL animation scripting software.  This is a promising result, especially considering the slightly faster speed and lack of facial expres-sion information in the motion-capture animations.  While this suggests that the data we are collecting is of good quality, the real test will be when this corpus is used in future research.  If we can build useful ASL-animation generation software based on analysis of this corpus, then we will know that we have sufficient quality of motion-capture data. 5.1 Our Long-Term Research Goal: Making ASL Accessible to More NLP Researchers It is our goal to produce high-quality broad-coverage ASL generation software, which would benefit many deaf individuals with low English literacy.  However, this ambition is too large for any one team; for this technology to become real-ity, ASL must become a language commonly stud-ied by NLP researchers.  For this reason, we seek to build ASL software, models, and experimental techniques to serve as a resource for other NLP researchers.  Our goal is to make ASL ?accessible? to the NLP community.  By developing tools to address some of the modality-specific and spatial 
aspects of ASL, we can make it easier for other researchers to transfer their new NLP techniques to ASL. The goal is to ?normalize? ASL in the eyes of the NLP community.  Bridging NLP and ASL research will not only benefit deaf users: ASL will push the limits of current NLP techniques and will thus benefit other work in the field of NLP.  Sec-tion 1.2 listed six challenges for ASL NLP re-search; we address several of these in our research: We have conducted many experimental studies in which signers evaluate the understandability and naturalness of ASL animations (Huenerfauth et al, 2008; Huenerfauth, 2009).  To begin to address the Evaluation issue (section 1.2), we have published best-practices, survey materials, and experimental protocols for effectively evaluating ASL animation systems through the participation of native signers. We have also published baseline comprehension scores for ASL animations.  We will continue to produce such resources in future work. Our earlier work on timing algorithms for ASL animations (mentioned in section 4) was based on data reported in the linguistics literature (Grosjean et al, 1979).  In future work, we want to learn tim-ing models directly from our collected corpus ? to further address the Timing issue (section 1.2). To address the issues of Spatial Reference and Inflection (section 1.2), we plan on analyzing our ASL corpus to build models that can predict where in 3D space signers establish spatial reference points.  Further, we will analyze our corpus to ana-lyze how certain ASL verbs are inflected based on the 3D location of their subject and object. We want to build a parameterized lexicon of ASL verbs: given a 3D location for subject and object, we want to predict a 3D motion-path for the char-acter?s hands for a specific performance of a verb. While addressing the issues of Coarticulation and Non-Manuals (section 1.2) are not immediate research priorities, we believe our ASL corpus may also be useful in building computational models of these phenomena for data-driven ASL generation. Acknowledgments This material is based upon work supported by the National Science Foundation (Award #0746556), Siemens (Go PLM Academic Grant), and Visage Technologies AB (free academic license for soft-ware).  Jonathan Lamberton, Wesley Clarke, Kel-sey Gallagher, Amanda Krieger, and Aaron Pagan assisted with ASL data collection and experiments. 
96
References J. Bungeroth, D. Stein, P. Dreuw, M. Zahedi, H. Ney. 2006. A German sign language corpus of the domain weather report. Proc. LREC 2006 workshop on rep-resentation & processing of sign languages. Y.H. Chiu, C.H. Wu, H.Y. Su, C.J. Cheng. 2007. Joint optimization of word alignment and epenthesis gen-eration for Chinese to Taiwanese sign synthesis. IEEE Trans Pattern Anal Mach Intell 29(1):28-39. S. Cox, M. Lincoln, J. Tryggvason, M. Nakisa, M. Wells, M. Tutt, S. Abbott. 2002. Tessa, a system to aid communication with deaf people. Proc. ASSETS. O. Crasborn, E. van der Kooij, D. Broeder, H. Brugman. 2004. Sharing sign language corpora online: propos-als for transcription and metadata categories. Proc. LREC 2004 workshop on representation & process-ing of sign languages, pp. 20-23. O. Crasborn, H. Sloetjes, E. Auer, and P. Wittenburg. 2006. Combining video and numeric data in the analysis of sign languages within the ELAN annota-tion software. Proc. LREC 2006 workshop on repre-sentation & processing of sign languages, 82-87. E. Efthimiou, S.E. Fotinea. 2007. GSLC: creation and annotation of a Greek sign language corpus for HCI. Proc. HCI International. R. Elliot, J. Glauert. 2008. Linguistic modeling and lan-guage-processing technologies for avatar-based sign language presentation. Universal Access in the In-formation Society 6(4):375-391. S.E. Fotinea, E. Efthimiou, G. Caridakis, K. Karpouzis. 2008. A knowledge-based sign synthesis architecture. Univ. Access in Information Society 6(4):405-418. F. Grosjean, L. Grosjean, H. Lane. 1979. The patterns of silence: Performance structures in sentence produc-tion. Cognitive Psychology 11:58-81. M. Huenerfauth. 2006. Generating American sign lan-guage classifier predicates for English-to-ASL ma-chine translation, dissertation, U. of Pennsylvania. M. Huenerfauth, L. Zhao, E. Gu, J. Allbeck. 2008. Evaluation of American sign language generation by native ASL signers. ACM Trans Access Comput 1(1):1-27. M. Huenerfauth. 2009. A linguistically motivated model for speed and pausing in animations of American sign language. ACM Trans Access Comput 2(2):1-31. M. Huenerfauth, P. Lu. 2010. Annotating spatial refer-ence in a motion-capture corpus of American sign language discourse. Proc. LREC 2010 workshop on representation & processing of sign languages. K. Karpouzis, G. Caridakis, S.E. Fotinea, E. Efthimiou. 2007. Educational resources and implementation of a Greek sign language synthesis architecture. Comput-ers & Education 49(1):54-74. J. Kennaway, J. Glauert, I. Zwitserlood. 2007. Providing signed content on Internet by synthesized animation. ACM Trans Comput-Hum Interact 14(3):15. 
B. Loeding, S. Sarkar, A. Parashar, A. Karshmer. 2004. Progress in automated computer recognition of sign language, Proc. ICCHP, 1079-1087. P. Lu, M. Huenerfauth. 2009. Accessible motion-capture glove calibration protocol for recording sign language data from deaf subjects. Proc. ASSETS. I. Marshall, E. Safar. 2005. Grammar development for sign language avatar-based synthesis. Proc. UAHCI. R. Meier. 1990. Person deixis in American sign lan-guage. In: S. Fischer & P. Siple (eds.), Theoretical issues in sign language research, vol. 1: Linguistics.  Chicago: University of Chicago Press, 175-190. R. Mitchell, T. Young, B. Bachleda, M. Karchmer. 2006. How many people use ASL in the United States? Sign Language Studies 6(3):306-335. S. Morrissey, A. Way. 2005. An example-based ap-proach to translating sign language. Proc. Workshop on Example-Based Machine Translation, 109-116. C. Neidle, D. Kegl, D. MacLaughlin, B. Bahan, & R.G. Lee. 2000. The syntax of ASL: functional categories and hierarchical structure. Cambridge: MIT Press. D. Newkirk. 1987. SignFont Handbook. San Diego: Emerson and Associates. C. Padden. 1988. Interaction of morphology & syntax in American sign language. Outstanding dissertations in linguistics, series IV. New York: Garland Press. J. Segouat, A. Braffort. 2009. Toward the study of sign language coarticulation: methodology proposal. Proc Advances in Comput.-Human Interactions, 369-374. T. Shionome, K. Kamata, H. Yamamoto, S. Fischer. 2005. Effects of display size on perception of Japa-nese sign language---Mobile access in signed lan-guage. Proc. Human-Computer Interaction, 22-27. D. Stein, J. Bungeroth, H. Ney. 2006. Morpho-syntax based statistical methods for sign language transla-tion. Proc. European Association for MT, 169-177. K. Sumihiro, S. Yoshihisa, K. Takao. 2000. Synthesis of sign animation with facial expression and its effects on understanding of sign language. IEIC Technical Report 100(331):31-36. V. Sutton. 1998. The Signwriting Literacy Project. In Impact of Deafness on Cognition AERA Conference. C. Traxler. 2000. The Stanford achievement test, ninth edition: national norming and performance standards for deaf and hard-of-hearing students. J. Deaf Studies and Deaf Education 5(4):337-348. L. van Zijl, D. Barker. 2003. South African sign lan-guage MT system. Proc. AFRIGRAPH, 49-52.  VCom3D. 2010. Sign Smith Studio.  http://www.vcom3d.com/signsmith.php T. Veale, A. Conway, B. Collins. 1998. Challenges of cross-modal translation: English to sign translation in ZARDOZ system. Machine Translation 13:81-106. L. Zhao, K. Kipper, W. Schuler, C. Vogler, N. Badler, M. Palmer. 2000. A machine translation system from English to American sign language. Proc. AMTA. 
97
NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 66?74,
Montre?al, Canada, June 7?8, 2012. c?2012 Association for Computational Linguistics
 
 
Learning a Vector-Based Model of American Sign Language  Inflecting Verbs from Motion-Capture Data 
Pengfei Lu Department of Computer Science Graduate Center City University of New York (CUNY) 365 Fifth Ave, New York, NY 10016 pengfei.lu@qc.cuny.edu 
Matt Huenerfauth Department of Computer Science Queens College and Graduate Center City University of New York (CUNY) 65-30 Kissena Blvd, Flushing, NY 11367 matt@cs.qc.cuny.edu  Abstract 
American Sign Language (ASL) synthesis software can improve the accessibility of in-formation and services for deaf individuals with low English literacy. The synthesis com-ponent of current ASL animation generation and scripting systems have limited handling of the many ASL verb signs whose movement path is inflected to indicate 3D locations in the signing space associated with discourse refer-ents. Using motion-capture data recorded from human signers, we model how the mo-tion-paths of verb signs vary based on the lo-cation of their subject and object.  This model yields a lexicon for ASL verb signs that is pa-rameterized on the 3D locations of the verb?s arguments; such a lexicon enables more real-istic and understandable ASL animations.  A new model presented in this paper, based on identifying the principal movement vector of the hands, shows improvement in modeling ASL verb signs, including when trained on movement data from a different human signer. 1 Introduction American Sign Language (ASL) is a primary means of communication for over 500,000 people in the U.S. (Mitchell et al, 2006).  As a natural language that is not merely an encoding of English, ASL has a distinct syntax, word order, and lexicon. Someone can be fluent in ASL yet have significant difficulty reading English; in fact, due to various educational factors, the majority of deaf high school graduates (age 18+) in the U.S. have a fourth-grade (age 10) English reading level or low-er (Traxler, 2000).  This leads to accessibility chal-lenges for deaf adults when faced with English text on computers, video captions, or other sources.  
Technologies for automatically generating com-puter animations of ASL can make information and services accessible to deaf people with lower English literacy. While videos of sign language are feasible to produce in some contexts, animated avatars are more advantageous than video when the information content is often modified, the con-tent is generated or translated automatically, or signers scripting a message in ASL wish to pre-serve anonymity.  This paper focuses on ASL and producing accessible sign language animations for people who are deaf in the U.S., but many of the linguistic issues, literacy rates, and animation tech-nologies discussed within are also applicable to other sign languages used internationally.   2 Use Of Space, Inflected Verbs ASL signers can associate entities or concepts they are discussing with arbitrary locations in space (Liddle, 2003; Lillo-Martin, 1991; McBurney, 2002; Meier, 1990).  After an entity is first men-tioned, a signer may point to a 3D location in space around his/her body; to refer to this entity again, the signer (or his/her conversational partner) can point to this location. Many linguists have studied this pronominal use of space (Klima et al 1979; Liddell, 2003; McBurney, 2002; Meier, 1990). Some argue that signers tend to pick 3D locations on a semi-circular arc floating at chest height in front of their torso (McBurney, 2002; Meier, 1990); others argue that signers pick 3D locations at dif-ferent heights and distances from their body (Lid-dell, 2003).  Regardless, there are an infinite number of locations where entities may be associ-ated for pronominal reference; as discussed below, this also means that there are a potentially infinite number of ways for some verbs to be performed: a finite fixed lexicon for ASL is not sufficient. 
66
  
While ASL verbs have a standard citation form, many can be inflected to indicate the 3D location in space at which their subject and/or object have been associated (Liddell, 2003; Neidle et al, 2000; Padden, 1988).  Linguists refer to such verbs as ?inflecting? (Padden, 1988), ?indicating? (Liddell, 2003), or ?agreeing? verbs (Cormier, 2002). We use the term ?inflecting verbs? in this paper.  When they appear in a sentence, their standard motion path may be modified such that the movement or orientation goes from the 3D location of their sub-ject and toward the 3D location of their object (or more complex effects).  The resulting performance is a synthesis of the verb?s standard lexical motion path and the 3D locations associated with the sub-ject and object. Because the verb sign indicates its subject and/or object, the names of the subject and object may not be otherwise expressed in the sen-tence.  If the signer chooses to mention them in the sentence, it is legal to use the citation-form (unin-flected) version of the verb, but the resulting sen-tences tend to appear less fluent.  In prior studies, we have found that native ASL signers who view ASL animations report that those that include spa-tially inflected verbs and entities associated with locations in space are easier to understand (than those which lack spatial pronominal reference and lack verb inflection) (Huenerfauth and Lu, 2012).  Fig. 1 shows the ASL verb EMAIL, which in-flects for its subject and object locations.  Some ASL verbs do not inflect or inflect for their ob-ject?s location only (Liddell, 2003; Padden, 1988).  There are other categories of ASL verbs (e.g., ?de-picting,? ?locative,? or ?classifier?) whose move-ments convey complex spatial information and other forms of verb inflection (e.g., for temporal aspect); these are not the focus of this paper.  
 Fig. 1. Two inflected versions of the ASL verb EMAIL: (top) subject associated with location on left and object on right, (bottom) subject on right and object on left. 
3 Related Work on Sign Animation Given how the association of entities with loca-tions in space affects how signs are performed, it is not possible to pre-store all possible combinations of all the signs the system may need.  For pointing signs, inflecting verbs, and other space-affected signs, successful ASL systems must synthesize a specific instance of the sign as needed. Few sign language animation researchers have studied spa-tial inflection of verbs. There are two major types of ASL animation research: scripting software (El-liott et al, 2008; Traxler, 2000) or generation software (e.g., Fotinea et al, 2008; Huenerfauth, 2006; Marshall and Safar, 2005; VCom3D, 2012) as surveyed previously by (Huenerfauth and Han-son, 2009). Unfortunately, current generation and scripting systems for sign language animations typically do not make extensive use of spatial loca-tions to represent entities under discussion, the output of these systems looks much like the anima-tions without space use and without verb inflection that we evaluated in (Huenerfauth and Lu, 2012). For instance, Sign Smith Studio (VCom3D, 2012), a commercially available scripting system for ASL, contains a single uninflected version of most ASL verbs in its dictionary. To produce an inflected form of a verb, a user must use an ac-companying piece of software to precisely pose a character?s hands to produce a verb sign; this sig-nificantly slows down the process of scripting an ASL animation. One British Sign Language anima-tion generator (Marshall and Safar, 2005) can as-sociate entities under discussion with a finite number of locations in space (approximately 6). Its repertoire also includes a few verbs whose sub-ject/object are positioned at these locations. How-ever, most of the verbs handled by their system involved relatively simple motion paths for the hands from subject to object locations, and the sys-tem did not allow for the arrangement of pronomi-nal reference points at arbitrary locations in space.   Toro (Toro, 2004; 2005) focused on ASL in-flected verbs; they analyzed the videos of human signers to note the 2D hand locations in the image for different verbs. Next, they wrote animation code for planning motion paths for the hands based on their observations. A limitation of this work is that asking humans to look for hand locations in a video and write down angles and coordinates is 
67
  
inexact; further, a human looked for patterns in the data ? machine learning approaches were not used.  There are some sign language animation re-searchers who have used modeling techniques ap-plied to human motion data.  Researchers studying coarticulation for French Sign Language (LSF) animations (Segouat & Braffort, 2009) digitally analyzed the movements of human signers in video and trained mathematical models of the move-ments between signs, which could be used during animation synthesis.  Because collecting data from human via video requires researchers to estimate movements from a 2D image, it is more accurate and efficient to use motion-capture sensors.  Du-arte et al collected data via motion capture in their SignCom project for LSF (Duarte and Gibet, 2011), and they reassembled elements of the recordings to synthesize novel animations.  4 Our Prior Modeling Research The goal of our research is to construct computa-tional models of ASL verbs that can automate the work of human users of scripting software or be used within generation. Given the name of the verb, the location in space associated with verb?s subject, and the location associated with the object, our software should access its parameterized lexi-con of ASL verb signs to synthesize the specific inflected form needed for a sentence.  Our tech-nique for building these parameterized lexicon en-tries for each verb is data-driven: based on samples of sign language motion from human signers. Spe-cifically, we record a list of examples of each verb for a variety of arrangements of the verb?s subject and object around the signer?s body.  Fig. 2. shows how we identified 7 locations on an arc around the signer; we then collected examples of each verb for all possible combinations of these seven locations for subject and object. Table 1 lists the ASL verbs modeled in our prior work (Huenerfauth and Lu, 2010; Lu and Huenerfauth, 2011).   
 Fig. 2. Front & top view of arc positions around the signer. 
Table 1: Five ASL Verbs We Have Modeled Verb Inflection Type Description ASK Subject & Object The signer moves an extended index finger from the ?asker? (subject) to the ?person being asked? (object).  During the movement, the finger bends into a hooked shape. (ASL ?1? to ?X? handshape.) GIVE Subject & Object In this two-handed version of the sign, the signer moves two hands as a pair from the ?giver? (subject) toward the ?recipient? (object). (Both hands have an ASL ?flat-O? handshape.) MEET Subject & Object Signer moves two index fingers towards each other (pointing upward) to ?meet? at some point in the middle. (ASL ?1? handshape.) SCOLD Object Only The signer ?wags? (bounces up and down while pointing) an extended index finger at the ?person being scolded? (object).  (ASL ?1? handshape.) TELL Object Only The signer moves an extended index finger from the mouth/chin toward the ?person being told? (object). (ASL ?1? handshape.)  For verbs inflected for both subject and object location (MEET, GIVE), our training data con-tained 42 examples for all non-reflexive combina-tions of the 7 arc positions.  For verbs inflected for object location only (TELL, SCOLD, ASK), 7 ex-amples were collected.  While we focused on these five verbs as examples, we intend for our lexicon building methodology to be generalizable to other verbs and other sign languages. In our early work (Huenerfauth and Lu, 2010), we collected samples of inflected verbs by asking a native ASL signer with animation experience to produce these verbs using the Gesture Builder sign creation software (VCom3D, 2012).  In later work, we collected more natural/accurate data by using motion-capture equipment to record a human signer per-forming a verb for various arrangements of sub-ject/object in space (Lu and Huenerfauth, 2011).   Regardless of the data source, we extracted the hand position for each keyframe for each verb.  (A keyframe is an important moment for a movement; a straight-line path can be represented merely by its beginning and end.)  Thus, for a two-handed verb (e.g., GIVE) that is inflected for both subject and object, we collected 504 location values: 42 examples x 2 keyframes x 2 hands x 3 (x, y, z) val-ues.  Next, we fit third-order polynomial models for each dimension (x, y, z) of the hand position at each keyframe ? parameterized on the arc locations of the verb?s subject and object for that instance in the training data (Huenerfauth and Lu, 2010).   At this point, we could use the model to synthe-size novel ASL verb sign instances (properly in-flected for different locations of subject and object, 
68
  
including combinations not present in the training data) by predicting the location of the hand for each of the keyframes of a verb, given the location of the verb?s subject and object on the arc.  Our animation software is keyframe based, and it uses inverse kinematics and motion interpolation to syn-thesize a full animation from a list of hand location targets for specific keyframe times during the ani-mation. Additional details appear in (Huenerfauth and Lu, 2010; Lu and Huenerfauth, 2011).   To evaluate our models in prior work, we con-ducted a variety of user-based and distance-metric-based evaluations. For instance, we showed native ASL signer participants animations of short ASL stories that contained verbs (some versions pro-duced by our model, and some produced by a hu-man animator) to measure whether the stories containing our modeled verbs were easily under-stood, as measured on comprehension questions or side-by-side subjective evaluations (Huenerfauth and Lu, 2010).  No significant differences in com-prehension or evaluation scores were observed in these prior studies, indicating that the ASL anima-tions synthesized from our model had similar qual-ity to verb signs produced by a human animator. 5 Collecting More Verb Examples In prior work, we used motion-capture data from only a single human signer performing many in-flected forms of five ASL verbs.  For this paper, we asked two additional signers to perform exam-ples of each inflected form of the five verbs.  This section summarizes the collection methodology, described in detail in (Lu and Huenerfauth, 2011).  During a videotaped 90-minute recording session, each native ASL signer wore a set of motion-capture sensors while performing a set of ASL verb signs, for various given arrangements of the subject and object in the signing space. We use an Intersense IS-900 motion capture system with an overhead ultrasonic speaker array and hand, head, and torso mounted sensors with directional micro-phones and gyroscope to record location (x, y, z) and orientation (roll, pitch, yaw) data for the hands, torso, and head of the signer during the study. We placed colored targets around the perim-eter of the laboratory at precise angles, relative to where the signer was seated, corresponding to the points on the arc in Fig. 2.  Fig. 4. shows how we set up the laboratory during the data collection 
with 10cm colored paper squares were attached to the walls; the two squares visible in Fig. 4 corre-spond to arc positions 0.9 and 0.6 in Fig. 2. These squares served as ?targets? for the signer to use as ?subject? and ?object? when performing various inflected verb forms.  
 Fig. 4. This three-quarter view illustrates the layout of the laboratory during the motion capture data collection; the signer is facing a camera (off-screen to the right). Sitting behind the camera is another signer conversing with him. Another native ASL signer sitting behind the video camera prompted the performer to produce each inflected verb form by pointing to the colored squares for the subject and the object for each of the 42 samples we wanted to record for each verb. At the beginning of the session, the signer was asked to make several large arm movements and hand claps (Fig. 5) to facilitate the later synchroni-zation of the motion capture stream with the video data and scaling the data from the recorded human to match the body size of the VCom3D avatar.  
 Fig. 5. Arm movements the signer was asked to perform to facilitate calibration of the collected motion capture data. 
  Fig. 6. The signer signed the number that corresponded to each verb example being performed (left) and a close-up view of the hand-mounted sensor used in the study (right). 
69
  
Occasionally during the recording session (and whenever the signer made a mistake and needed to repeat a sign), the signer was asked to sign the se-quence number of the verb example being recorded (Fig. 6); this facilitated later analysis of the video.  We needed to identify timecodes in the motion capture data stream that correspond to the begin-ning and ending keyframes of each verb recorded. We asked a native ASL signer to view the video after the recording session to identify the time in-dex (video frame number) that corresponded to the start and end movement of each verb sign that we recorded. (If we had modeled signs with more complex motion paths, we might have needed more than two keyframes.) These time codes were used to extract hand location (x, y, z) data from the motion capture stream for each hand for each keyframe for each verb example that was recorded.  6 Modeling the Verb Path as a Vector Although experimental evaluations of verb models produced in prior work based on motion-capture data from a single human signer were positive (Lu and Huenerfauth, 2011), this may not have been a realistic test.  When constructing a large-scale sign language animation system, it may not be possible to gather all of the needed training examples for all of the verbs for a large lexicon from a single signer.  For instance, if you wish to learn performances of a verb from examples of the inflected form of that verb that happen to appear in a corpus, then you would likely need to mix data recorded from mul-tiple signers to produce your training data set for learning the inflected verb animation model. The challenge of using data from multiple sign-ers is that an ASL verb performance consists of: (1) non-meaningful/idiosyncratic variation in how dif-ferent people perform a verb (or how one person performs a verb on different occasions) and (2) meaningful/essential aspects of how a verb should be performed (that should be rather invariant across different signers or different occasions).  We prefer a model that captures the essential na-ture of the verb but not the signer-specific elements; models attuned too much to the specifics of a sin-gle human?s performance may overfit to that one individual?s version of the verb (or that one occa-sion when the signer performed).  Further, while motion-capture data recorded from humans with different body proportions can be somewhat re-
scaled to fit the animated character?s body size to be used by the sign language animation system, no ?retargeting? algorithm is perfect. If signer-specific idiosyncrasies are captured in the verb animation model, then the variation in data sources used when building a large-scale sign language anima-tion project may be apparent in its output.  Our prior modeling technique explicitly learned the starting and ending location of the hands for each instance of a verb based on a human signer?s movements.  However, when different signers per-form a verb (e.g., GIVE with subject at arc position -0.6 and object at 0.3), they may not select exactly the same point in 3D space for their hands to start and stop.  What is common across all of the varia-tions in the performance is the overall direction that the hands move through the signing space.  We can find empirical evidence for this intuition if we compare motion-capture data of the three dif-ferent signers we recorded (section 5) performing the same ASL inflecting verbs.  When we calculate Euclidean distance between different signer?s start-ing location and their ending locations of the hands for identical verb examples, we see inter-signer variability (Fig. 7).  If we instead calculate the Eu-clidean distance between the vector (direction and magnitude) of the hand movement from the start to the ending location between signers, we see much smaller inter-signer variability (Fig. 7). Section 7 explains the scale and formula used for the dis-tance metrics in Fig. 7 and elsewhere in this paper. 
 Fig. 7.  Inter-signer variability in ASL verb signs, re-ported using a ?point? or ?vector? distance metric. 
70
  
Using these results as intuition, we present a new model of ASL inflecting verbs in this paper, based on this ?vector? approach to modeling the movement of the signer?s hands through space.  We assume that what is essential to a human?s per-formance of an inflected ASL verb is the direction that the hands travel through space, not the specific starting and ending locations in space.  Thus, we model each verb example as a tuple of values: the difference between the x-, the y-, and the z-axis values for the starting and ending location of the hand.  (The model has three parameters for a one-handed sign and six parameters for a two-handed sign.)  Using this model, we followed a similar polynomial fitting technique summarized in sec-tion 4 ? except that we are now modeling a smaller number of parameters ? our new ?vector? model uses only three values per hand (deltax, deltay, deltaz), instead of six per hand in our prior ?point? model, which represented start and end location of the hand as (xstart, ystart, zstart, xend, yend, zend).  This new model can then be used to synthesize animations of ASL verb signs for given subject and object arc positions around the signer ? the differ-ence from our prior work is that these new models only represent the movement vector for the hands, not their specific starting and ending locations.   The purpose of building a model of a verb is that we wish to use it as a parameterized lexical entry in a sign language animation synthesis sys-tem; thus, we must explain how the model can be used to synthesize a novel verb example, given its input parameters (the arc position of the subject and the object of the verb).  While our new vector model predicts the motion vector for the hands, this is not enough; we need starting and ending locations for the hands (an infinite number of which are possible for a given vector).  Thus, we need a way to select a starting location for the hands for a specific verb instance (and then based on the vector, we would know the ending location).   We observe that, for a given verb, there are some locations in the signing space that are likely for the signer?s hands to occupy and some regions that are less likely.  Some motion paths through the signing space travel through high-likelihood ?pop-ular? regions of the signing space, and some, through less likely regions.  Thus, we can build a Gaussian mixture model of the likelihood that a hand might occupy a specific location in the sign-ing space during a particular ASL verb.  For a giv-
en motion vector, one possible starting point in the signing space will lead to a path that travels through a maximally likely region of the signing space.  Thus, we can search possible starting points for the hands for a given vector and identify an optimal path for the hands given a Gaussian mix-ture model of hand location likelihood.   Fig. 8 shows a (two-dimensional) illustration of our approach for selecting a starting location for the hand when synthesizing a verb.  The concentric groups of ovals in the image represent the compo-nent Gaussians in the mixture model, which was fit on the data from the locations that one hand occu-pied during a signer?s performances of a verb.  Given the vector (direction and magnitude) for the hand?s motion path for a verb (predicted by our model), we can systematically search the signing space for all possible starting locations for the hand ? to identify the starting location that yields a path through the signing space with maximum probabil-ity (as predicted by the Gaussian model).  The ar-rows shown in Fig. 8 represent a few possible paths for the hand given several possible starting locations, and one of these arrows travels a path through the model with maximum probability. 
 Fig. 8. This 2D diagram illustrates how the starting lo-cation for the hand can be selected that yields a path through the mixture model with maximum probability.   Specifically, for each signer, for each hand, for each verb, we used the recorded motion-capture data stream between the start-times and end-times of all of the verb examples as training data, and then we fit a 3D Gaussian mixture model for each, to represent the probability that the hand would occupy each location in the signing space during that verb.  We used a model with 6 component Gaussians for modeling the signing space for each of the verbs SCOLD, GIVE, ASK, and MEET. Due to the fast movement (and thus short clips of recorded motion-capture data) for the verb TELL, we only had sufficient data to fit a 5-component 
71
  
Gaussian model for the locations of the hand dur-ing this verb (TELL is a one-handed verb).  When we need to synthesize a verb, then we use our vec-tor model to predict a movement vector for the hands, and then we perform a grid search through the signing space (in the x, y, and z dimensions) to identify an optimal starting location for the hand.  If run-time efficiency is a concern, optimization or estimation methods could be applied to this search. In summary, the vector direction and magnitude of the hands are based on a model that is parame-terized on: the verb, the location of the subject on an arc around the signer, and the location of the object on this arc.  When a specific instance of a verb must be synthesized, a starting point for the hand is selected that maximizes the probability of the entire trajectory of the hands through space, based on a Gaussian mixture model specific to that verb (but not parameterized on any specific sub-ject/object locations in space). All instances of the verb in the training data were used to train the mix-ture model, due to data sparseness considerations. 7 Distance Metric Evaluation  Because the premise of this paper is that models of ASL verbs based on a motion vector representation would do a better job of capturing the essential aspects of a verb?s motion path across signers, we conducted an inter-signer cross-validation of our new model. We built separate models on the data from each of our three signers, and then we com-pared the resulting model?s predictions for all 42 verb instances collected from the other two signers.  For comparison purposes, we also trained three models (one per signer) using the ?point?-based model from our prior work (Lu and Huenerfauth, 2011).   Fig. 9 presents the results; the values of each bar are the average ?error? for each synthe-sized verb example for all five ASL verbs in Table 1.  The error score for a verb example is the aver-age of four values: (1) Euclidean distance between the start location of the right hand as predicted by the model and the start location of the right hand of the human signer data being used for evaluation, (2) same for the end location for the right hand, (3) same for the start location for the left hand, and (4) same for end location for the left hand. Fig. 9 shows that the new ?vector? model has lower error scores than our older ?point? model presented in prior work.  To interpret the Euclidean 
distance value, it is useful to know that the scale of the coordinate space used for the verb model is set such that shoulder width of a signer would be 1.0.  As a baseline for comparison, the average inter-signer variation (based on the values shown in Fig. 7) is also plotted in Fig. 9. 
 Fig. 9. Evaluation of the ?Point? and ?Vector? models for all five ASL verbs listed in Table 1. Next, we wanted to compare the two models under two assumptions: (1) it may not be possible to gather a large number of examples of a verb from a single signer and (2) it may be necessary to mix data from multiple signers when assembling a training data set for a verb model.  For instance, these conditions would hold if a researcher were using examples of a verb performance extracted from a multi-signer corpus to assemble a training set.  Due to the limited size of most sign language corpora (and the many possible combinations of subject and object position in the signing space), a training set gathered in this manner would likely contain a relatively small number of training ex-amples ? possibly gathered from multiple signers. To test the models under these conditions, we assembled three training data sets ? using the data from our three recorded signers.  Each data set in-cluded 22 examples of the performance of an ASL inflected verb for a subset of the various possible combinations of subject and object locations in the signing space ? with half of the examples from one signer and half from another.  After training a model on each data set, then the model was evalu-ated against the 42 examples of each verb perfor-mance recorded from the third signer (who was not part of the training data used for that model).  This process was repeated for a total of three times (for all combinations of the data from the three sign-
72
  
ers).  For comparison purposes, we also trained three models (one based on each of the three two-signer data sets) using the ?point?-based model from our prior work (Lu and Huenerfauth, 2011). Fig. 10 shows the results for two of the verbs in Table 1 (ASK and GIVE); the ?vector? model has lower error scores than our older ?point? model. 
 Fig. 10. Evaluation of the ?Point? and ?Vector? models trained on a small ?mixed? data set from two signers. Examples of animations of the ASL verbs syn-thesized using each of these models are on our lab website: http://latlab.cs.qc.cuny.edu/slpat2012/ 8 Conclusion And Future Work This paper presented and evaluated a new method of constructing a lexicon of ASL verb signs whose motion path depends on the location in the signing space associated with the verb?s subject and object.  We used motion capture data from multiple signers to evaluate whether our new models do a better job of capturing the signer-invariant and occasion-invariant aspect of an ASL inflected verb?s move-ment, compared to our prior modeling approach.  The parameterized models of ASL verb move-ments produced in this paper could be used to syn-thesize a desired verb instance for a potentially infinite number of arrangements of the subject and object of the verb in the signing space ? based on the collection of a finite number of examples of a verb performance from a human signer.  
Using this technique, generation software could include flexible lexicons that can be used to syn-thesize an infinite variety of inflecting verb in-stances, and scripting software could more easily enable users to include inflecting verbs in a sen-tence (without requiring the user to create a custom animations of a body movement for a particular inflected verb sign). While this paper demonstrates our method on five ASL verbs, this technique should be applicable to more ASL verbs, more ASL signs parameterized on spatial locations, and signs in other sign languages used internationally.  In this paper, we studied a set of ASL verbs with relatively simple motion-paths (consisting of straight line movements, which therefore only re-quired two keyframes per verb); in future work, we may analyze verbs with more complex movements of the hands.  Further, our vector models represent the magnitude (length) of the hands? motion path through space; in future work, we may explore techniques for rescaling these vector lengths.  In future work, we will also use hand orientation data from our motion capture sessions to synthesize hand orientation for sign animations.  We also plan to experiment with modeling how the timing of keyframes varies with subject/object positions.   Finally, we also plan on conducting a user-based evaluation study using animations synthe-sized by the models presented in this paper ? to determine if native ASL signers who view anima-tions containing such verbs find them to be more grammatical, understandable, and natural.   Acknowledgments This material is based upon work supported in part by the US. National Science Foundation under award number 0746556 and award number 1065009, by The City University of New York PSC-CUNY Research Award Program, by Sie-mens A&D UGS PLM Software through a Go PLM Academic Grant, and by Visage Technolo-gies AB through a free academic license for char-acter animation software. Jonathan Lamberton assisted with the recruitment of participants and the conduct of experimental sessions. Kenya Bry-ant, Wesley Clarke, Kelsey Gallagher, Amanda Krieger, Giovanni Moriarty, Aaron Pagan, Jaime Penzellna, Raymond Ramirez, and Meredith Turtletaub have also assisted with data collection and contributed their ASL expertise to the project. 
73
  
References  Cormier, K. 2002. Grammaticalization of Indexic Signs: How American Sign Language Expresses Numerosi-ty. Ph.D. Dissertation, University of Texas at Austin. Cox, S., M. Lincoln, J. Tryggvason, M. Nakisa, M. Wells, M. Tutt, S. Abbott. 2002. Tessa, a system to aid communication with deaf people. In Proceedings of Assets '02, 205-212.  Duarte, K., and Gibet, S. Presentation of the SignCom Project. In Proceedings of the First International Workshop on Sign Language Translation and Avatar Technology, Berlin, Germany, 10-11 Jan 2011. Elliott, R., Glauert, J., Kennaway, J., Marshall, I., Safar, E. 2008. Linguistic modeling and language-processing technologies for avatar-based sign lan-guage presentation. Univ Access Inf Soc 6(4), 375-391. Berlin: Springer. Fotinea, S.E., Efthimiou, E., Caridakis, G., Karpouzis K. 2008. A knowledge-based sign synthesis architec-ture. Univ Access Inf Soc 6(4):405-418. Berlin: Springer. Huenerfauth, M. 2006. Generating American Sign Lan-guage classifier predicates for English-to-ASL ma-chine translation, dissertation, U. of Pennsylvania. Huenerfauth, M., Hanson, V. 2009. Sign language in the interface: access for deaf signers. In C. Stephanidis (ed.), Universal Access Handbook. NJ: Erlbaum. 38.1-38.18. Huenerfauth, M., Zhao, L., Gu, E., Allbeck, J.  2008. Evaluation of American sign language generation by native ASL signers. ACM Trans Access Comput 1(1):1-27. Huenerfauth, M., Lu, P. 2010. Annotating spatial refer-ence in a motion-capture corpus of American Sign Language discourse. In Proc. LREC 2010 workshop on representation & processing of sign languages. Huenerfauth, M., Lu, P. 2010. Modeling and synthesiz-ing spatially inflected verbs for American sign lan-guage animations. In Proceedings of the 12th international ACM SIGACCESS conference on Computers and accessibility (ASSETS '10). ACM, New York, NY, USA, 99-106. Huenerfauth, M, P. Lu. (2012. in press). Effect of spa-tial reference and verb inflection on the usability of American sign language animation. In Univ Access Inf Soc. Berlin: Springer. Klima, E., U. Bellugi. 1979. The Signs of Language. Harvard University Press, Cambridge, MA. Liddell, S. 2003. Grammar, Gesture, and Meaning in American Sign Language. UK: Cambridge U. Press. Lillo-Martin, D. 1991. Universal Grammar and Ameri-can Sign Language: Setting the Null Argument Pa-rameters. Kluwer Academic Publishers, Dordrecht. Lu, P., Huenerfauth, M. 2011. Synthesizing American Sign Language Spatially Inflected Verbs from Mo-
tion-Capture Data. Second International Workshop on Sign Language Translation and Avatar Technolo-gy (SLTAT), in conjunction with ASSETS 2011, Dundee, Scotland. Marshall, I., E. Safar. 2005. Grammar development for sign language avatar-based synthesis. In Proc. UAHCI?05. McBurney, S.L. 2002. Pronominal reference in signed and spoken language. In R.P. Meier, K. Cormier, D. Quinto-Pozos (eds.) Modality and Structure in Signed and Spoken Languages. UK: Cambridge U. Press, 329-369. Meier, R. 1990. Person deixis in American sign lan-guage. In S. Fischer, P. Siple (eds.) Theoretical issues in sign language research. Chicago: University of Chicago Press, 175-190. Mitchell, R., Young, T., Bachleda, B., & Karchmer, M. 2006. How many people use ASL in the United States? Why estimates need updating. Sign Lang Studies, 6(3):306-335. Neidle, C., D. Kegl, D. MacLaughlin, B. Bahan, R.G. Lee. 2000. The syntax of ASL: functional categories and hierarchical structure. Cambridge: MIT Press. Padden, C. 1988. Interaction of morphology & syntax in American Sign Language. New York: Garland Press. Segouat, J., A. Braffort. 2009. Toward the study of sign language coarticulation: methodology proposal. In Proc. Advances in Computer-Human Interactions, 369-374. Toro, J. 2004. Automated 3D animation system to in-flect agreement verbs. Proc. 6th High Desert Linguis-tics Conf. Toro, J. 2005. Automatic verb agreement in computer synthesized depictions of American Sign Language. Ph.D. dissertation, Depaul University, Chicago, IL. Traxler, C. 2000. The Stanford achievement test, 9th edition: national norming and performance standards for deaf & hard-of-hearing students. J Deaf Stud & Deaf Educ 5(4):337-348. VCom3D. 2012. Homepage. http://www.vcom3d.com/ Zhao, L., Kipper, K., Schuler, W., Vogler, C., Badler, N., Palmer, M. 2000. A machine translation system from English to American Sign Language. In Proc. AMTA?00, pp. 293-300. 
74
