Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 13?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Exploring Topic Continuation Follow-up Questions using Machine Learning
Manuel Kirschner
KRDB Center
Faculty of Computer Science
Free University of Bozen-Bolzano, Italy
kirschner@inf.unibz.it
Raffaella Bernardi
KRDB Center
Faculty of Computer Science
Free University of Bozen-Bolzano, Italy
bernardi@inf.unibz.it
Abstract
Some of the Follow-Up Questions (FU Q) that
an Interactive Question Answering (IQA) sys-
tem receives are not topic shifts, but rather
continuations of the previous topic. In this pa-
per, we propose an empirical framework to ex-
plore such questions, with two related goals in
mind: (1) modeling the different relations that
hold between the FU Q?s answer and either the
FU Q or the preceding dialogue, and (2) show-
ing how this model can be used to identify the
correct answer among several answer candi-
dates. For both cases, we use Logistic Regres-
sion Models that we learn from real IQA data
collected through a live system. We show that
by adding dialogue context features and fea-
tures based on sequences of domain-specific
actions that represent the questions and an-
swers, we obtain important additional predic-
tors for the model, and improve the accuracy
with which our system finds correct answers.
1 Introduction
Interactive Question Answering (IQA) can be de-
scribed as a fusion of the QA paradigm with di-
alogue system capabilities. While classical QA is
concerned with questions posed in isolation, its in-
teractive variant is intended to support the user in
finding the correct answer via natural-language dia-
logue. In an IQA setting, both the system and the
user can pose Follow-Up Questions (FU Q). In the
second case, whenever an IQA system receives an
additional user question (note that this is what we
call a Follow-Up Question throughout this work), it
can either interpret it as being thematically related to
a previous dialogue segment (topic continuation), or
as a shift to some new, unrelated topic (topic shift).
A definition of thematic relatedness of FU Qs might
rely on the elements of the attentional state, i.e., on
the objects, properties and relations that are salient
before and after processing the user question. Topic
continuation FU Qs should be interpreted within the
context, whereas topic shift FU Qs have to be treated
as first questions and can thus be processed with
standard QA technologies. Therefore, a first task
in IQA is to detect whether a FU Q is a topic shift or
a topic continuation (Yang et al, 2006).
To help answering topic continuation FU Qs, an
IQA system would need to fuse the FU Q with cer-
tain information from the dialogue context (cf. (van
Schooten et al, 2009)). Thus, a second task in IQA
is to understand which turns in the dialogue context
are possible locations of such information, and ex-
actly what kind of information should be considered.
Knowing that a FU Q concerns the same topic as the
previous question or answer, we thus want to study
in more detail the way the informational content of
questions and answers evolves before/after the FU Q
is asked. A model of these so-called informational
transitions would provide insights into what a user is
likely to ask about next in natural coherent human-
machine dialogue.
In order to tackle any of the two IQA tasks men-
tioned above we need IQA dialogues. Most current
work on IQA uses the TREC QA data; the TREC
QA tracks in 2001 and 2004 included series of con-
text questions, where FU Qs always depended on the
context set by an earlier question from the same se-
ries. However, these data were constructed artifi-
cially and are not representative of actual dialogues
from an IQA system (for instance, system answers
are not considered at all). Real IQA data yield chal-
13
lenges for an automatic processing approach (Yang
et al, 2006). Our work is based on collecting and
analyzing IQA dialogues from users of a deployed
system.
In this paper, we address the second task intro-
duced above, namely the study of common relations
between the answer to a topic continuation FU Q and
other turns in the dialogue context. Our collected di-
alogue data are from the ?library help desk? domain.
In many of the dialogues, library users request in-
formation about a specific library-related action; we
are thus dealing with task-oriented dialogues. This
work is based on two hypotheses regarding relations
holding between the FU Q?s answer and the dialogue
context. For studying such relations, we want to ex-
plore the usefulness of (1) a representation of the
library-related action underlying questions and an-
swers, and (2) a representation of the dialogue con-
text of the FU Q.
2 Background
In order to understand what part of the history of
the dialogue is important for processing FU Qs,
significant results come from Wizard-of-Oz stud-
ies, like (Dahlba?ck and Jo?nsson, 1989; Bertomeu
et al, 2006; Kirschner and Bernardi, 2007), from
which it seems that the immediate linguistic context
(i.e., the last user initiative plus the last system re-
sponse) provides the most information for resolving
any context-dependency of the FU Qs. These studies
analyzed one particular case of topic continuation
FU Q, namely those questions containing reference-
related discourse phenomena (ellipsis, definite de-
scription or anaphoric pronoun); we assume that the
results could be extended to fully specified ques-
tions, too.
Insights about the informational transitions within
a dialogue come from Natural Language Genera-
tion research. (McCoy and Cheng, 1991) provide
a list of informational transitions (they call them fo-
cus shifts) that we can interpret as transitions based
on certain thematic relations. Depending on the con-
versation?s current focus type, they list specific focus
shift candidates, i.e., the items that should get focus
as a coherent conversation moves along. Since we
are interested in methods for interpreting FU Qs au-
tomatically, we decided to restrict ourselves to use
Node type Informational transition targets
Action Actor, object, etc., of the action ?
any participant (Fillmore) role; pur-
pose (goal) of action, next action in
some sequence, subactions, special-
izations of the action
Table 1: Possible informational transition targets for ?ac-
tion? node type (McCoy and Cheng, 1991)
only the ?action? focus type to represent the focus
of questions and answers in IQA dialogues. We con-
jecture that actions form a suitable and robust basis
for describing the (informational) meaning of utter-
ances in our class of task-based ?help desk? IQA di-
alogues. Table 1 shows the focus shift candidates
for a current focus of type ?action?. In this work
we concentrate on the informational transitions in-
volving two actions (i.e., including one of the focus
targets listed in bold face in the table).
3 Exploring topic continuation FU Qs
using Machine Learning
We base our study of topic continuation FU Qs on
the two main results described in Section 2: We
study snippets of dialogues consisting of four turns,
viz. a user question (Q?1), the corresponding sys-
tem answer (A?1), the FU Q and its system answer
(A0); we use Logistic Regression Models to learn
from these snippets (1) which informational (action-
action) transitions hold between A0 and the FU Q
or the preceding dialogue, and (2) how to predict
whether a specific answer candidate A0 is correct for
a given dialogue snippet.
3.1 Machine learning framework: Logistic
Regression
Logistic regression models (Agresti, 2002) are gen-
eralized linear models that describe the relationship
between features (predictors) and a binary outcome
(in our case: answer correctness). We estimate the
model parameters (the beta coefficients ?1, . . . , ?k)
that represent the contribution of each feature to the
total answer correctness score using maximum like-
lihood estimation. Note that there is a close rela-
tionship to Maximum Entropy models, which have
performed well in many tasks. A major advantage
of using logistic regression as a supervised machine
14
learning framework (as opposed to other, possibly
better performing approaches) is that the learned co-
efficients are easy to interpret. The logistic regres-
sion equation which predicts the probability for a
particular answer candidate A0 being correct, de-
pending on the learned intercept ?0, the other beta
coefficients and the feature values x1, . . . , xk (which
themselves depend on a combination of Q?1, A?1,
FU Q or A0) is:
Prob{answerCorrect} = 11 + exp(?X??) , where
X?? = ?0 + (?1x1 + . . .+ ?kxk)
3.2 Dialogue data collection
We have been collecting English human-computer
dialogues using BoB, an IQA system which is pub-
licly accessible on the Library?s web-site of our
university1. We see the availability of dialogue
data from genuinely motivated visitors of the library
web-site as an interesting detail of our approach; our
data are less constrained and potentially more dif-
ficult to interpret than synthesized dialogues (e.g.,
TREC context track data), but should on the other
hand provide insights into the structure of actual
IQA dialogues that IQA systems might encounter.
We designed BoB as a simple chatbot-inspired ap-
plication that robustly matches user questions using
regular expression-based question patterns, and re-
turns an associated canned-text answer from a repos-
itory of 529. The question patterns and answers
have been developed by a team of librarians, and
cover a wide range of library information topics,
e.g., opening time, lending procedures and different
library services. In the context of this work, we use
BoB merely as a device for collecting real human-
computer IQA dialogues.
As a preliminary step towards automatically mod-
eling action-based informational transitions trig-
gered by FU Qs, we annotated each of the 529 an-
swers in our IQA system?s repository with the ?li-
brary action? that we considered to best represent
its (informational) meaning. For this, we had de-
vised a (flat) list of 25 library-related actions by an-
alyzing the answer repository (e.g.: access, borrow,
change, deliver). We also added synonymous verbs
1www.unibz.it/library
to our action list, like ?obtain? for ?borrow?. If we
did not find any action to represent a system an-
swer, we assigned it a special ?generic-information?
tag, e.g. for answers to questions like ?What are the
opening times??.
We base our current study on the dialogues col-
lected during the first four months of the IQA sys-
tem being accessible via the Library?s web site. Af-
ter a first pass of manually filtering out dialogues
that consisted only of a single question, or where the
question topics were only non-library-related, the
collected corpus consists of 948 user questions (first
or FU Qs) in 262 dialogue sessions (i.e., from differ-
ent web sessions). We hand-annotated the user FU
Qs in these dialogues as either ?topic continuation?
(248 questions), or ?topic shift? (150 questions).
The remaining FU Qs are user replies to system-
initiative clarification questions, which we do not
consider here. For each user question, we marked
whether the answer given by the IQA system was
correct; in the case of wrong answers, we asked our
library domain experts to provide the correct answer
that BoB should have returned. However, we only
corrected the system answer in those cases where
the user did not ask a further FU Q afterwards, as
we must not change on-going dialogues.
To get the actual training/test data, we had to fur-
ther constrain the set of 248 topic continuation FU
Qs. We removed all FU Qs that immediately follow
a system answer that we considered incorrect; this is
because any further FU Q is then uttered in a situa-
tion where the user is trying to react to the problem-
atic answer, which clearly influences the topic of the
FU Q. Of the then remaining 76 FU Qs, we keep the
following representation of the dialogue context: the
previous user question Q?1 and the previous system
answer A?1. We also keep the FU Q itself, and its
corresponding correct answer A0.
Finally, we automatically annotated each question
with one or more action tags. This was done by sim-
ply searching the stemmed question string for any
verb stem from our list of 25 actions (or one of their
synonyms); if no action stem is found, we assigned
the ?generic-information? tag to the question. Note
that this simple action detection algorithm for ques-
tions fails in case of context-dependent questions
where the verb is elided or if the question contains
still unknown action synonyms.
15
3.3 Features
In the machine learning framework introduced
above, the model is intended to predict the correct-
ness of a given system answer candidate, harnessing
information from the local dialogue context: Q?1,
A?1, FU Q and the particular answer candidate A0.
We now introduce different features that relate A0 to
either the FU Q or some other preceding turn of the
dialogue. The features describe specific aspects of
how the answer candidate relates to the current dia-
logue. Note that we do not list features relating Q?1
and A0, since our experiments showed no evidence
for including them in our models.
tfIdfSimilarityQA, tfIdfSimilarityAA: TF/IDF-
based proximity scores (ranging from 0 to 1) be-
tween two strings, namely FU Q and A0, or A?1
and A0, respectively. Based on vector similarity (us-
ing the cosine measure of angular similarity) over
dampened and discriminatively weighted term fre-
quencies. Definition of the TF/IDF distance: two
strings are more similar if they contain many of the
same tokens with the same relative number of occur-
rences of each. Tokens are weighted more heavily if
they occur in few documents2, hence we used a sub-
set of the UK English version of the Web-as-Corpus
data3 to train the IDF scores.
Features based on action sequences. To describe
the action-related informational transitions we ob-
serve between the FU Q and A0 and between A?1
and A0, we use two sets of features, both of which
are based on hand-annotated actions for answers
and automatically assigned actions for questions.
actionContinuityQA, actionContinuityAA: sim-
ple binary features indicating whether the same li-
brary action (or one of its synonyms) was identi-
fied between the FU Q and A0, or A?1 and A0, re-
spectively. lmProbQA, lmProbAA: encode Statis-
tical Language Model probabilities for action tag se-
quences, i.e., the probability for A0 having a certain
action, given the action associated with FU Q, or the
action of A?1, respectively. The underlying Statis-
tical Language Models are probability distributions
2Cf. Alias-i?s LingPipe documentation http:
//alias-i.com/lingpipe/demos/tutorial/
stringCompare/read-me.html
3http://wacky.sslmit.unibo.it
over action-action sequences that reflect how likely
certain action sequences occur in our IQA dialogues,
thus capturing properties of salient action sequences.
More technically, we use Witten-Bell smoothed 2-
gram statistical language models, which we trained
on our action-tagged FU Q data.
4 Results
For the evaluation of the logistic regression model,
we proceed as follows. Applying a cross-validation
scheme, we split our 76 FU Q training examples
randomly into five non-intersecting partitions of 15
(or 16) FU Q (with corresponding Q?1, A?1, and
correct A0) each. To train the logistic regression
model, we need training data consisting of a vec-
tor of independent variables (the various feature val-
ues), along with the binary dependent variable, i.e.,
?answer correct? or ?answer false?. We generate
these training data by ?multiplying out? each train-
ing partition?s 61 FU Qs (76 minus the held-out test
set of 15) with all 529 answer candidates; for each
FU Q dialogue snippet used for training, this results
in one positive training example (where A0 is the 1
correct out 529 answer candidates), and 528 nega-
tive training examples (for all other answer candi-
dates).
For each of the five training/test partitions, we
train a different model. We then evaluate each of
these models on their corresponding held-out test
set. Following the cross-validation idea through, we
also train separate Statistical Language Models on
sequences of action tags for each of the five training
splits; this ensures that the language model proba-
bilities were never trained on test data. We perform
the evaluation in terms of the mean rank that the cor-
rect answer A0 is assigned after ranking all 529 an-
swer candidates (by evaluating the logistic regres-
sion equation to yield answer scores).
In the following, we give details of different lo-
gistic regression models we experimented with. Ini-
tially, we chose a subset from the list of features
introduced above. Our goal was to retain as few
features as needed to explore our two hypotheses,
i.e., whether we can make use of (1) a representa-
tion of the FU Q?s underlying library action, and/or
(2) a representation of the immediate dialogue con-
text. By dropping uninformative features, the result-
16
ing models become simpler and easier to interpret.
With this goal in mind, we applied a fast backwards
elimination routine that drops uninformative predic-
tors (cf. (Baayen, 2008, p.204)) on the five training
data splits. In all five splits, both TF/IDF features
turned out to be important predictors; in four of the
splits, also lmProbQA was retained. lmProbAA was
dropped as superfluous in all but two splits, and ac-
tionSimilarityAA was retained only in one. With
these results, the set of features we retain for our
modeling experiments is: tfIdfSimilarityQA, tfIdf-
SimilarityAA and lmProbQA.
?Complete? model: tfIdfSimilarityQA, tfIdfSim-
ilarityAA and lmProbQA We estimated logistic
regression models on the five cross evaluation train-
ing sets using all three features as predictors. Table 2
shows the mean ranks of the correct answer for the
five evaluation runs, and an overall mean rank with
the average across the five splits.
To illustrate the contribution of each of the three
predictors towards the score of an answer candi-
date, we provide the (relevant linear part of) the
learned logistic regression equation for the ?com-
plete? model (trained on split 1 of the data). Note
that the ?answer ranker? evaluates this equation to
get a score for an answer candidate A0.
X?? = ?8.4 + (9.5 ? tfIdfSimilarityQA +
4.6 ? tfIdfSimilarityAA +
1.7 ? lmProbQA)
Reduced model 1: No representation of dialogue
context Only the features concerning the FU Q
and the answer A0 (tfIdfSimilarityQA, lmProbQA)
are used as predictors in building the logistic re-
gression model. The result is a model that treats
every FU Q as a stand-alone question. Across the
five models, the coefficient for tfIdfSimilarityQA is
roughly five times the size of that for lmProbQA.
Reduced model 2: No action sequences We
keep only the two TF/IDF features (tfIdfSimilari-
tyQA, tfIdfSimilarityAA). This model thus does not
use any features that depend on human annotation,
but only fully automatic features. The coefficient
learned for tfIdfSimilarityQA is generally twice as
large as that for tfIdfSimilarityAA.
Reduced model 3: No dialogue context, no action
sequences Considered as a baseline, this model
uses a single feature (tfIdfSimilarityQA) to predict
answer correctness, favoring those answer candi-
dates that have the highest lexical similarity wrt. the
FU Q.
5 Discussion
In order to better understand the relatively high
mean ranks of the correct answer candidates across
Table 2, we scrutinized the results of the answer
ranker (based on all tests on the ?complete? model).
The distribution of the ranks of correct answers is
clearly skewed; in around half of the 76 cases, the
correct answer was actually ranked among the top
20 of the 529 answer candidates. However, the mean
correct rank deteriorates badly due to the lowest-
ranking third of cases. Analyzing these lowest-
ranking cases, it appears that they are often instances
of two sub-classes of topic continuation FU Qs: (i)
the FU Q is context-dependent, i.e., underspecified
or exhibiting reference-related discourse phenom-
ena; (ii) the FU Q is a slight variation of the pre-
vious question (e.g. only the wh-phrase changes, or
only the object changes). This error analysis seems
to suggest that it should be worthwhile to distin-
guish between sub-classes of topic-continuation FU
Qs, and to improve specifically how answers for the
?difficult? sub-classes are ranked.
The relatively high mean ranks are also due to the
fact that in our approach of acquiring dialogue data,
for each FU Q we marked only one answer from the
whole repository as ?correct?. Again for the ?com-
plete? model, we checked the top 20 answer can-
didates that ranked higher than the actual ?correct?
one. We found that in over half of the cases an an-
swer that could be considered correct was among the
top 20.
Looking at the ranking results across the differ-
ent models in Table 2, the fact that the ?complete?
model seems to outperform each of the three re-
duced models (although no statistical significance
could be attained from comparing the rank num-
bers) confirms our two hypotheses proposed earlier.
Firstly, identifying the underlying actions of ques-
tions/answers and modeling action-based sequences
yield important information for identifying correct
17
Reduced m. 3 Reduced m. 2 Reduced m. 1 Complete model
Predictors tfIdfSimilarityQA tfIdfSimilarityQA, tfIdfSimilarityQA, tfIdfSimilarityQA,
in model tfIdfSimilarityAA tfIdfSimilarityAA,
lmProbQA lmProbQA
Split 1 141.2 108.4 112.5 96.2
Split 2 102.7 97.4 53.8 57.7
Split 3 56.7 63.7 50.5 52.7
Split 4 40.5 26.2 37.9 35.7
Split 5 153.1 105.3 129.6 89.1
Mean 98.8 80.2 76.7 66.3
Table 2: Mean ranks of correct A0 out of 529 answer candidates, across models and training/test splits
answers to topic continuation FU Qs. Secondly, as
for the role of the immediate dialogue context for
providing additional clues for identifying good an-
swers to FU Qs, our data show that a high lexical
similarity score between A?1 and A0 indicates a cor-
rect answer candidate. While (Yang et al, 2006)
point out the importance of Q?1 to provide context
information, in our experiments it was generally su-
perseded by A?1.
As for the two features relating the underlying
actions of A?1 and A0 (actionContinuityAA, lm-
ProbAA), the picture seems less clear; in our current
modeling experiments, we had not enough evidence
to keep these features. However, we plan to explore
the underlying idea of action-action sequences in the
future, and conjecture that such information should
come into its own for context-dependent FU Qs.
6 Future work
Besides annotating and using more dialogue data as
more people talk to our IQA system, we plan to
implement a state-of-the-art topic-shift detection al-
gorithm as proposed in (Yang et al, 2006), train-
ing and testing it on our own FU Q data. We will
attempt to improve this system by adding action-
based features, and then extend it to distinguish
three classes: topic shifts, (topic continuation) FU
Qs that are fully specified, and (topic continuation)
context-dependent FU Qs. We then plan to build
dedicated logistic regression models for the differ-
ent sub-classes of topic continuation FU Qs. If each
model uses a specific set of predictors, we hope to
improve the overall rank of correct answers across
the different classes of FU Qs. Also, from compar-
ing the different models, we are interested in study-
ing the specific properties of different FU Q types.
References
[Agresti2002] Alan Agresti. 2002. Categorical Data
Analysis. Wiley-Interscience, New York.
[Baayen2008] R. Harald Baayen. 2008. Analyzing Lin-
guistic Data. Cambridge University Press.
[Bertomeu et al2006] Nu?ria Bertomeu, Hans Uszkoreit,
Anette Frank, Hans-Ulrich Krieger, and Brigitte Jo?rg.
2006. Contextual phenomena and thematic relations
in database QA dialogues: results from a wizard-of-oz
experiment. In Proc. of the Interactive Question An-
swering Workshop at HLT-NAACL 2006, pages 1?8,
New York, NY.
[Dahlba?ck and Jo?nsson1989] Nils Dahlba?ck and Arne
Jo?nsson. 1989. Empirical studies of discourse repre-
sentations for natural language interfaces. In Proc. of
the 4th Conference of the European Chapter of the
ACL (EACL?89), pages 291?298, Manchester, UK.
[Kirschner and Bernardi2007] Manuel Kirschner and
Raffaella Bernardi. 2007. An empirical view on
iqa follow-up questions. In Proc. of the 8th SIGdial
Workshop on Discourse and Dialogue, Antwerp,
Belgium.
[McCoy and Cheng1991] Kathleen F. McCoy and Jean-
nette Cheng. 1991. Focus of attention: Constraining
what can be said next. In Cecile L. Paris, William R.
Swartout, and William C. Mann, editors, Natural Lan-
guage Generation in Artificial Intelligence and Com-
putational Linguistics, pages 103?124. Kluwer Aca-
demic Publishers, Norwell, MA.
[van Schooten et al2009] Boris van Schooten, R. op den
Akker, R. Rosset, O. Galibert, A. Max, and G. Illouz.
2009. Follow-up question handling in the IMIX and
Ritel systems: A comparative study. Journal of Natu-
ral Language Engineering, 15(1):97?118.
[Yang et al2006] Fan Yang, Junlan Feng, and Giuseppe
Di Fabbrizio. 2006. A data driven approach to rele-
vancy recognition for contextual question answering.
In Proc. of the Interactive Question Answering Work-
shop at HLT-NAACL 2006, pages 33?40, New York
City, NY.
18
Categorial Type Logic meets Dependency Grammar to annotate
an Italian Corpus
R. Bernardi
KRDB,
Free University of Bolzano Bozen,
P.zza Domenicani, 3
39100 Bolzano Bozen,
Italy,
bernardi@inf.unibz.it
A. Bolognesi and F. Tamburini
CILTA,
University of Bologna,
P.zza San Giovanni in Monte, 4,
I-40124, Bologna,
Italy,
{bolognesi,tamburini}@cilta.unibo.it
M. Moortgat
UiL OTS,
Utrecht University,
Trans 10,
3512 JK, Utrecht,
The Netherlands
Moortgat@phil.uu.nl
Abstract
In this paper we present work in progress on the
annotation of an Italian Corpus (CORIS) devel-
oped at CILTA (University of Bologna). We in-
duce categorial type assignments from a depen-
dency treebank (Torino University treebank,
TUT) and use the obtained categories with an-
notated dependency relations to study the dis-
tributional behavior of Italian words and reach
an empirically founded part-of-speech classifica-
tion.
1 Introduction
The work reported on here is part of a project1
aimed at annotating the CORIS/CODIS 100-
million-words synchronic corpus of contempo-
rary Italian with linguistic information: first
part-of-speech tagging for the complete corpus,
and in a later stage syntactic analysis for a sub-
corpus.
We have been investigating existing Italian
treebanks in order to assess their potential use-
fulness for bootstrapping the CORIS/CODIS an-
notation tasks. We are aware of two such tree-
banks that are relevant to our purposes: the
TUT corpus developed at Torino University, and
ISST (Italian Syntactic-Semantic treebank) de-
veloped under the national program SI-TAL by
a consortium of companies and research centers
coordinated by the ?Consorzio Pisa Ricerche?
(CPR)2.
ISST is a multi-layered corpus, annotated at
the syntactic and lexico-semantic levels. A user
interface is provided to explore the corpus. The
ISST corpus is rather competitive in terms of
size: it counts 305,547 word tokens (Monte-
magni et al, 2003). A drawback is that the
corpus is not publicly available yet. The TUT
1The project is funded under the FIRB 2001 action.
2The parnters of the consortium were: ILC-
CNR/CPR, Venice University/CVR, ITC-IRST, ?Tor
Vergata? University/CERTIA, and Synthema.
corpus is rather small, consisting only of 38,653
words. There is no user interface for TUT but
the corpus is downloadable (http://www.di.
unito.it/~tutreeb/). Despite its small size,
TUT can serve as a training corpus for creating
larger annotated resources.
Our goal is to annotate CORIS with part-of-
speech (PoS) tags and semi-automatically build
a treebank for a fragment of it. To achieve this
two-fold task we start from TUT exploiting its
information on dependency relations. We are
still in a preliminary phase of the project and
so far attention has been focused on the first
task. However, the work done in this phase is
expected to play a role in our second task too.
In Section 2, we will describe in more detail
the problems which arise when working out the
first task. In Section 3, we briefly introduce the
formalisms we work with. In Section 4, we ex-
plain how we encode dependency relations into
categorial type assignments (CTAs), and how we
automatically induce these types from TUT de-
pendency structures. Finally, in Section 5 we
draw some preliminary conclusions and briefly
describe our action list.
2 PoS tagging for Italian
Before embarking on our first task, we have
studied the current situation with respect to
PoS tagging for Italian. Italian is one of the
languages for which a set of annotation guide-
lines has been developed in the context of the
EAGLES project (Expert Advisory Group on
Language Engineering Standards (Monachini,
1995)). Several research groups have worked on
PoS annotation in practice (for example, Torino
University, Xerox and Venice University).
We have compared the tag sets used by these
groups with Monachini?s guidelines. From this
comparison, it results that though there is a
general agreement on the main parts of speech
to be used3, considerable divergence exists when
it comes to the actual classification of Italian
words with respect to these main PoS classes.
The classes for which differences of opinion are
most evident are adjectives, determiners and
adverbs. For instance, words like molti (tr.
many) have been classified as ?indefinite deter-
miners? by Monachini, ?plural quantifiers? by
Xerox, ?indefinite adjectives? by the Venice and
Turin groups. This simple example shows that
the choice of PoS tags is already influenced by
the linguistic theory adopted in the background.
This theoretical bias will then influence the kind
of conclusions one can draw from the annotated
corpus.
Our aim is to derive an empirically founded
PoS classification, making no a priori assump-
tions about the PoS classes to be distinguished.
Our background assumptions are minimal and,
we hope, uncontroversial: we assume that
we have access to head-dependent (H-D) and
functor-argument (F-A) relations in our mate-
rial. We encode the H-D and F-A information
into categorial type formulas. These formulas
then serve as ?labels/tags? from which we ob-
tain the desired empirically founded PoS classi-
fication by means of a clustering algorithm.
To bootstrap the process of type induction,
we transform the TUT corpus into a simpli-
fied dependency treebank. The transformation
keeps the bare dependency relations but re-
moves the more theory-laden annotation. In
Section 4, we describe how we use the simpli-
fied dependency treebank for our distributional
study of Italian PoS classification. First, we
briefly look at H-D and F-A relations as they
occur in the TUT corpus and in Categorial Type
Logic (CTL).
3 Dependency and
functor-argument relations
3.1 Dependency structures in TUT
The Turin University Treebank (TUT) is a cor-
pus of Italian sentences annotated by specifying
relational structures augmented with morpho-
syntactic information and semantic role (hence-
forth ARS) in a monostratal dependency-based
representation. The treebank in its current re-
lease includes 38,653 words and 1,500 sentences
3The standard classification consists of nouns, verbs,
adjectives, determiners, articles, adverbs, prepositions,
conjunctions, numerals, interjections, punctuation and
a class of residual items which differs from project to
project.
from the Italian civil law code, the national
newspapers La Stampa and La Repubblica, and
from various reviews, newspapers, novels, and
academic papers.
The ARS schema consists of i) morpho-
syntactic, ii) functional-syntactic and iii) se-
mantic components, specifying part-of-speech,
grammatical relations, and thematic role infor-
mation, respectively. The reader is referred
to (Bosco, 2003) for a detailed description of the
TUT annotation schema. An example is given
below (tr. ?The first steps have not been encour-
aging?). In this example, the node TOP-VERB is
the root of the whole structure4.
************** FRASE ALB-71 **************
1 I (IL ART DEF M PL)
[6;VERB-SUBJ]
2 primi (PRIMO ADJ ORDIN M PL)
[3;ADJC+ORDIN-RMOD]
3 approcci (APPROCCIO NOUN COMMON M PL)
[1;DET+DEF-ARG]
4 non (NON ADV NEG)
[6;ADVB-RMOD]
5 sono (ESSERE VERB AUX IND PRES INTR 3 PL)
[6;AUX+TENSE]
6 stati (ESSERE VERB MAIN PART PAST INTR PL M)
[0;TOP-VERB]
7 esaltanti (ESALTANTE ADJ QUALIF ALLVAL PL)
[6;VERB-PREDCOMPL+SUBJ]
8 . (#\. PUNCT) [6;END]
Because we are interested in extracting
dependency relations, we can focus on the
functional-syntactic component of the TUT an-
notation, where information relating to gram-
matical relations (heads and dependents) is en-
coded.
The TUT annotation schema for depen-
dents makes a primary distinction between
(a) functional and (b) non-functional tags,
for dependents that can and that can-
not be assigned thematic roles, respectively.
These two classes are further divided into
(a?) arguments (ARG) and modifiers (MOD)
and (b?), AUX, COORDINATOR, INTERJECTION,
CONTIN, EMPTYCOMPL, EMPTYLOC, SEPARATOR
and VISITOR5; and furthermore, the arguments
4The top nodes used in TUT are TOP-VERB,
TOP-NOUN, TOP-CONJ, TOP-ART, TOP-NUM, TOP-PRON,
TOP-PHRAS and TOP-PREP
5The labels that require some explanation are: (i)
CONTIN, (ii) EMPTYCOMPL, (iii) EMPTYLOC and (iv) VISITOR.
They are used for expressions that (i) introduce a part
of an expression with a non-compositional interpreta-
tion (e.g. locative or idiomatic expressions and denom-
inative structures: ?Arrivo` [prima]H [de]D ll?alba?, lit.
tr. ?(She) arrived ahead of the daybreak?); (ii) link a re-
(ARG) and modifiers (MOD) are sub-divided as fol-
lowing
ARG
SUBJ OBJ INDOBJ INDCOMPL PREDCOMPL
MODIFIER
RMOD
RELCLR RMODPRED
APPOSITION
RELCLA
3.2 Categorial functor-argument
structures
Categorial Type Logic (CTL) (Moortgat, 1997)
is a logic-based formalism belonging to the fam-
ily of Categorial Grammars (CG). In CTL, the
type-forming operations of CG are viewed as
logical connectives. As the slogan ?Parsing-
as-Deduction? suggests, such a view makes it
possible to do away with combinatory syn-
tactic rules altogether; establishing the well-
formedness of an expression becomes a process
of deduction in the logic of the type-forming
connectives.
The basic distinction expressed by the cat-
egorial type formulas is the Fregean opposi-
tion between complete and incomplete expres-
sions. Complete expressions are categorized by
means of atomic type formulas; grammatical-
ity judgements for expressions with an atomic
type do not require further contextual informa-
tion. Typical examples of atomic types would
be ?sentence? (s) and ?noun? (n). Incomplete ex-
pressions are categorized by means of fractional
type formulas; the denominators of these frac-
tions indicate the material that has to be found
in the context in order to obtain a complete ex-
pression of the type of the numerator.
Definition 3.1 (Fractional type formulas)
Given a set of basic types ATOM, the set of
types TYPE is the smallest set such that:
i. if A ? ATOM, then A ? TYPE;
ii. if A and B ? TYPE, then A/B and
B\A ? TYPE.
There are different ways of presenting valid
type computations. In a Natural Deduction for-
mat, we write ? ` A for a demonstration that
flexive personal pronoun with particular verbal head (e.g.
?La porta [si]D [apre]H?, lit. tr. ?the door (it) opens?);
(iii) link a pronoun with a verbal head introducing a sort
of metaphorical location of the head (e.g. ?In Albania
[ci]D [sono]H molti problemi?, lit. tr. ?In Albany there
are many problems?.); (iv) mark the extraction of a part
of a structure (e.g. ?Cos`? devi vedere questo argomento?,
lit. tr. ?This way (you) must see this topic?).
the structure ? has the type A. The statement
A ` A is axiomatic. Each of the connectives /
and \ has an Elimination rule and an Introduc-
tion rule. Below, we give these inference rules
for / (incompleteness to the right). The cases
for \ (incompleteness to the left) are symmetric.
Given structures ? and ? of types A/B and B
respectively, the Elimination rule builds a com-
pound structure ??? of type A. The Introduc-
tion rule allows one to take apart a compound
structure ??B into its immediate substructures.
? ` A/B ? ` B
? ?? ` A /E
? ?B ` A
? ` A/B /I
Notice that the language of fractional types
is essentially higher-order: the denominator of
a fraction does not have to be atomic, but can
itself be a fraction. The Introduction rules are
indispensable if one is interested in capturing
the full set of theorems of the type calculus.
Classical CG (in the style of Ajdukiewicz and
Bar-Hillel) uses only the Elimination rules, and
hence has restricted inferential capacities. It is
impossible in classical CG to obtain the validity
A ` B/(A\B), for example. Still, the classi-
cal CG perspective will be useful to realize our
aim of automatically inducing type assignments
from structured data obtained from the TUT
corpus thanks to the type resolution algorithm
explained below.
Type inference algorithms for classical CG
have been studied by (Buszkowski and Penn,
1990). The structured data needed by their
type inference algorithms are so-called functor-
argument structures (fa-structures). An fa-
structure for an expression is a binary branching
tree; the leaf nodes are labeled by lexical expres-
sions (words), the internal nodes by one of the
symbols J (for structures with the functor as
the left daughter) or I (for structures with the
functor as the right daughter).
To assign types to the leaf nodes of an fa-
structure, one proceeds in a top-down fashion.
The type of the root of the structure is fixed (for
example: s). Compound structures are typed as
follows:
- to type a structure ? J ? as A, type ? as
A/B and ? as B;
- to type a structure ? I ? as A, type ? as
B and ? as B\A.
If a word occurs in different structural environ-
ments, the typing algorithm will produce dis-
tinct types. The set of type assignments to a
word can be reduced by factoring : one identi-
fies type assignments that can be unified. For
an example, compare the structured input be-
low:
a. Claudia I parla
b. Claudia I (parla I bene)
Assuming a goal type s, from (a) we obtain the
assignments
Claudia : A,parla : A\s
and from (b)
Claudia : C,parla : B,bene : B\(C\s)
Factoring leads to the identifications A = C,
B = (A\s), producing for ?bene? the modifier
type (A\s)\(A\s).
3.3 From TUT dependency structures
to categorial types
To accomplish our aims, we will have an oc-
casion to use two extensions of the basic cate-
gorial machinery outlined in the section above:
a generalization of the type language to multi-
ple modes of composition, and the addition of
structural rules of inference to the logical rules
of slash Elimination and Introduction.
Multimodal composition The intuitions
underlying the distinction between heads and
dependents in Dependency Grammars (DG) and
between functors and arguments in CG often
coincide, but there are also cases where they
diverge (Venneman, 1977). In the particu-
lar case of the TUT annotation schema, we
see that for all instances of dependents la-
beled as ARG (or one of its sublabels), the
DG head/dependent articulation coincides with
the CG functor/argument asymmetry. But for
DG modifiers, or dependents without thematic
roles of the class AUX (auxiliary)6 there is a
mismatch between dependency structure and
functor-argument structure. Modifiers would be
functors in terms of their categorial type: func-
tors where the numerator and the denominator
are identical. This makes them into ?identities?
for the fractional multiplication, which explains
their optionality and the possibility of iteration.
AUX elements in DG would count as morpholog-
ical modifiers of the head verbs. From the CG
point of view, they would be typed as functors
6And also COORDINATOR, INTERJECTION.
with non-identical numerator and denomina-
tor, distinguishing them that way from optional
modifiers, and capturing the fact that they are
indispensable to build a complete grammatical
structure.
To reconcile the competing demands of the
head-dependent and functor-argument classifi-
cation, we make use of the type calculus pro-
posed in (Moortgat and Morrill, 1991), which
treats dependency and functor-argument rela-
tions as two orthogonal dimensions of linguistic
organization. Instead of one composition oper-
ation ?, the system of (Moortgat and Morrill,
1991) has two: ?l for structures where the left
daughter is the head, and ?r for right-headed
structures. The two composition operations
each have their slash and backslash operations
for the typing of incomplete expressions:
- A/lB: a functor looking for a B to the right
to form an A; the functor is the head, the
argument the dependent;
- A/rB: a functor looking for a B to the right
to form an A; the argument is the head, the
functor the dependent;
- B\lA: a functor looking for a B to the left
to form an A; the argument is the head,
the functor the dependent;
- B\rA: a functor looking for a B to the left
to form an A; the functor is the head, the
argument the dependent.
The type inference algorithm of (Buszkowski
and Penn, 1990) can be straightforwardly
adapted to the multimodal situation. The inter-
nal nodes of the fa-structures now are labeled
with a fourfold distinction: as before, the tri-
angle points to the functor daughter of a con-
stituent; in the case of the black triangle, the
functor daughter is the head constituent, in the
case of the white triangle, the functor daughter
is the dependent.
ad ah fd fh
fh J
fd C
ah B
ad I
The type-inference clauses can be adapted ac-
cordingly.
- to type a structure ? J ? as A, type ? as
A/lB and ? as B;
- to type a structure ? C ? as A, type ? as
A/rB and ? as B.
- to type a structure ? I ? as A, type ? as
B\rA and ? as B;
- to type a structure ? B ? as A, type ? as
B\lA and ? as B.
Structural reasoning The dependency rela-
tions in the TUT corpus abstract from surface
word order. When we induce categorial type
formulas from these dependency relations, as we
will see in Section 4.1, the linear order imposed
by ?/? and ?\? in the obtained formulas will not
always be compatible with the observable sur-
face order. Incompatibilities will arise, specif-
ically, in the case of non-projective dependen-
cies. Where such mismatches occur, the induced
types will not be immediately useful for parsing
? the longer term subtask of the project dis-
cussed here.
To address this issue, we can extend the in-
ference rules of our categorial logic with struc-
tural rules. The general pattern of these rules
is: infer ?? ` A from ? ` A, where ?? is some
rearrangement of the constituents of ?. These
rules, in other words, characterize the structural
deformations under which type assignment is
preserved. Structural rules can be employed
in two ways in CTL (see (Moortgat, 2001) for
discussion). In an on-line use, they actually
manipulate structural configurations during the
parsing process. Such on-line use can be very
expensive computationally. Used off-line, they
play a role complementary to the factoring op-
eration, producing a number of derived lexical
type-assignments from some canonical assign-
ment. With the derived assignments, parsing
can then proceed without altering the surface
structure.
As indicated in the introduction, the use of
CTL in the construction of a treebank for a part
of the CILTA corpus belongs to a future phase
of our project. For the purposes of this paper
we must leave the exact nature of the required
structural rules, and the trade-off between off-
line and on-line uses, as a subject for further
research.
4 A distributional study of Italian
part-of-speech tagging
In order to annotate the CORIS corpus with a
theory-neutral set of PoS tags, we plan to carry
out a distributional study of its lexicon.
Early approaches to this problem were based
on the hypothesis that if two words are syn-
tactically and semantically different, they will
appear in different contexts. There are a num-
ber of studies that, starting from this hypoth-
esis, have built automatic or semi-automatic
procedures for clustering words (Brill and Mar-
cus, 1992; Pereira et al, 1993; Martin et al,
1998), especially in the field of cognitive sci-
ences (Redington et al, 1998; Gobet and Pine,
1997; Clark, 2000). They examine the distribu-
tional behaviour of some target words, compar-
ing the lexical distribution of their respective
collocates using quantitative measures of distri-
butional similarity (Lee, 1999).
In (Brill and Marcus, 1992) it is given a semi-
automatic procedure that, starting from lexical
statistical data collected from a large corpus,
aims to arrange target words in a tree (more
precisely a dendrogram), instead of clustering
them automatically. This procedure requires a
linguistic examination of the resulting tree, in
order to identify the word classes that are most
appropriate to describe the phenomenon under
investigation. In this sense, they use a semi-
automatic word-class generator method.
A similar procedure has been applied on Ital-
ian in (Tamburini et al, 2002). The novelty of
this work is that it derives the distributional in-
formation on words from a very basic set of PoS
tags, namely nouns, verbs and adjectives. This
method, completely avoiding the sparseness of
the data affecting Brill and Marcus? method,
uses general information about the distribution
of lexical words to study the internal subdivi-
sions of the set of grammatical words, and re-
sults more stable than the method based only
on lexical co-occurrence.
The main drawback of these techniques is the
limited context of analysis. Collecting informa-
tion from a defined context, typically two or
three words will invariably miss syntactic de-
pendencies longer than the context interval. To
overcome this problem we propose to exploit the
expressivity of CTAs (with encoded core depen-
dency relations, as we saw in the section above)
by applying the clustering algorithms on them.
Below we sketch how we intend to induce CTAs
from the TUT dependency treebank, and the
clustering method we plan to implement. The
whole procedure can be summarized by the pic-
ture below.
Treebank conversion???????? CTL structures
? type resolution
PoS tagset clustering??????? Categorial Types
4.1 Inducing categorial types from TUT
The first step is to reduce the distinctions
encoded in the TUT treebank to bare head-
dependent relations: the ARG type on the one
hand, and the MOD and AUX types on the other.
These relations are converted into fa-structures
built by means of the dependency-sensitive op-
erators J, I, C , B .
By means of example, we consider some sim-
ple sentences exemplifying the different rela-
tions.
Figure 1 shows a head-dependent structure
in which edges represent head-dependent rela-
tions and each edge points to the dependent of
each relation. In this example, each H-D rela-
tion agrees with the F-A relation, i.e. each head
corresponds to a functor and the dependents are
all labeled as arguments (or sub-tags of it).7
Alan
0
Alan
SUBJ
mangia
1
eats
TOP_VERB
la
2
the
OBJ
mela
3
apple
ARG
SUBJ OBJ ARG
Figure 1: ARG: Functor and Head coincide
Figure 2 adds to the example from figure 1
the use of qualifying adjectives, which is an ex-
ample of a modifier, and past tense auxiliaries.
Considering the relation between ?mela?(apple)
and ?rossa? (red), and between ?ha? (has) and
?mangiato? (eaten), we have the dependency
trees in Figure 2.
In the first case, the noun is the head and
the adjective is the dependent, but from the
functor-argument perspective, the adjective (in
general, the modifier) is the incomplete functor
component. A similar discrepancy is observed
for the auxiliary and the main verb, where the
auxiliary should be classified as the incomplete
functor, but as the dependent element with re-
spect to the main verb. In this case the absence
7The example follows TUT practice in designating the
determiner as the head of the noun phrases. We are
aware of the fact that this is far from controversial in
the dependency community. In preprocessing TUT be-
fore type inference, we have the occasion to adjust such
debatable decisions, and representational issues such as
the use of empty categories, for which there is no need
in a CTL framework.
Alan
0
Alan
SUBJ
ha
1
AUX
mangiato
2
ate
TOP_VERB
la
3
the
OBJ
mela
4
apple
ARG
SUBJ AUX OBJ ARG
Figure 2: MOD and AUX: Functors as Dependents
of the auxiliary would result in an ungrammat-
ical sentence. The relations of MOD and AUX ex-
hibit a different behavior than ARG, and hence
are depicted with different arcs.
Our simple example sentences could be con-
verted into the following fa-structures:
- Allen I (mangia J (la J mela)
- Allen I (mangia J (la J (mela B rossa))
- Allen I ((ha C mangiato) J (la J mela))
The second step is to run the Buszkowski-
Penn type-inference algorithm (in its extended
form, discussed above) on the fa-structures ob-
tained from TUT, and to reduce the lexicon
by factoring (identification of unifiable assign-
ments) and (in a later phase) structural deriv-
ability. Fixing the goal type for these examples
as s, we obtain the following type assignments
from the fa-structures given above:
Allan A
mangia (A\rs)/lB
la B/lC
mela C
rossa C\lC
ha ((A\rs)/lB)/rD
mangiato D
Notice that from the output in our tiny sam-
ple, we have no information allowing us to iden-
tify the argument assignments A and B. No-
tice also that from an fa-structure which takes
together ?ha mangiato? in a constituent, we
obtain a type assignment for ?mangiato? that
does not express its incompleteness anymore
? instead, the combination with the auxil-
iary expresses this. This is already an exam-
ple where structural reasoning can play a role:
compare the above analysis with the type so-
lution one would obtain by starting from an
fa-structure which takes ?mangiato la mela?
as a constituent, which yields a type solution
(A\rs)/rE for the auxiliary, and E/lB for the
head verb. We are currently experimenting with
the effect of different constituent groupings on
the size of the induced type lexicon.
4.2 Clustering Algorithms
Once we have induced the categorial type as-
signments for the TUT lexicon, the last step of
our first task is to divide it into clusters so to
study the distributional behavior of the corre-
sponding lexical entries. The advantage of us-
ing categorial types as objects of the clustering
algorithm is that they represent long distance
dependencies as well as limited distributional
information. Thus the categorial types become
the basic elements of syntactic information as-
sociated with lexical entries and the basic ?dis-
tributional fingerprints? used in the clustering
process.
Every clustering process is based on a no-
tion of ?distance? between the objects involved
in the process. We should define an appropri-
ate metric among categorial types. We believe
that a crucial role will be played by the depen-
dency relation encoded into the types by means
of compositional modes.
Currently, we are studying the application of
proper distance measures considering types as
trees and adapting the theoretical results on
tree metrics to our problem. The algorithm for
computing the tree-edit distance (Shasha and
Zhang, 1997), designed for generic trees, ap-
pears to be a good candidate for clustering in
categorial-type domain. What remains to be
done is to experiment the algorithm and fine-
tune the metrics to our purpose.
5 Conclusions and Further Research
In this paper we have presented work in progress
devoted to the syntactic annotation of a large
Italian corpus. We have just started working in
this direction and the biggest part of the work
has still to be done. We are currently evaluating
the TUT encoding of dependency information,
and identifying areas that allow optimization
from the point of view of CTL type induction.
A case in point is the heavy reliance of TUT
on empty elements and/or traces, which con-
flicts with our desire for an empirically-based
and theory-neutral representation of linguistic
dependencies. It seems that the trace artifact
can be avoided if one properly exploits the more
expressive category concept of CTL, allowing
product types for asyndetic constructions, and
higher-order types for multiple dependencies. In
parallel, we are looking for other sources of de-
pendency information for Italian, in order to
complement the rather small TUT database we
have at our disposal now.
6 Acknowledgments
Our thanks go to FIRB 2001 project
RBNE01H8RS coordinated by prof. R. Rossini
Favretti for the funding supports. Thanks to
L. Surace and C. Seidenari for the detailed
comparison on Italian PoS classifications.
References
C. Bosco. 2003. A grammatical relation system
for treebank annotation. Ph.D. thesis, Com-
puter Science Department, Turin University.
E. Brill and M. Marcus. 1992. Tagging an un-
familiar text with minimal human supervi-
sion. In Proceedings of the Fall Symposium
on Probabilistic Approaches to Natural Lan-
guage, pages 10?16, Cambridge. MA: Ameri-
can Association for Artificial Intelligence.
W. Buszkowski and G. Penn. 1990. Categorial
grammars determined from linguistic data by
unification. Studia Logica, 29:431?454.
A. Clark. 2000. Inducing syntactic categories
by context distribution clustering. In Pro-
ceedings of CoNLL-2000 and LLL-2000 Con-
ference, pages 94?91, Lisbon, Portugal.
F. Gobet and J. Pine. 1997. Modelling the ac-
quisition of syntactic categories. In Proceed-
ings of the 19th Annual Meeting of the Cog-
nitive Science Society, pages 265?270.
L. Lee. 1999. Measures of distributional simi-
larity. In Proceedings of the 37th ACL, pages
25?32, College Park, MD.
S. Martin, J. Liermann, and H. Ney. 1998. Al-
gorithms for bigram and trigram word clus-
tering. Speech Communication, 24:19?37.
M. Monachini. 1995. ELM-IT: An Italian in-
carnation of the EAGLES-TS. definition of
lexicon specification and classification guide-
lines. Technical report, Pisa.
S. Montemagni, F. Barsotti, M. Battista,
N. Calzolari, O. Corazzari, A. Lenci, A. Zam-
polli, F. Fanciulli, M. Massetani, R. Raf-
faelli, R. Basili, M. T. Pazienza, D. Saracino,
F. Zanzotto, N. Mana, F. Pianesi, and R. Del-
monte, 2003. Building and using parsed cor-
pora, chapter Building the Italian Syntactic-
Semantic Treebank, pages 189?210. Lan-
guage and Speech series. Kluwer, Dordrecht.
M. Moortgat and G. Morrill. 1991. Heads
and phrases. Type calculus for dependency
and constituent structure. Technical report,
Utrecht.
M. Moortgat. 1997. Categorial type logics.
In J. van Benthem and A. ter Meulen, edi-
tors, Handbook of Logic and Language, pages
93?178. The MIT Press, Cambridge, Mas-
sachusetts.
Michael Moortgat. 2001. Structural equa-
tions in language learning. In P. de Groote,
G. Morrill, and C. Retore?, editors, Logical As-
pects of Computational Linguistics, volume
2099 of Lecture Notes in Artificial Intelli-
gence, pages 1?16, Berlin. Springer.
F. Pereira, T. Tishby, and L. Lee. 1993. Dis-
tributional clustering of English words. In
Proceedings of the 31st ACL, pages 183?190,
Columbus, Ohio.
M. Redington, N. Chater, and S. Finch. 1998.
Distributional information: a powerful cue for
acquiring syntactic categories. Cognitive Sci-
ence, 22(4):425?469.
D. Shasha and D. Zhang. 1997. Approximate
tree pattern matching. In A. Apostolico and
Z. Galig, editors, Pattern matching algo-
rithms. Oxford University Press.
F. Tamburini, C. De Santis, and Zamuner E.
2002. Identifying phrasal connectives in Ital-
ian using quantitative methods. In S. Nuc-
corini, editor, Phrases and Phraseology -Data
and Description. Berlin: Peter Land.
T. Venneman. 1977. Konstituenz und Depen-
denz in einigen neueren Grammatiktheorien.
Sprachwissenschaft, 2:259?301.
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 769?779,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Exploiting language models for visual recognition
Dieu-Thu Le
DISI, University of Trento
Povo, 38123, Italy
dle@disi.unitn.it
Jasper Uijlings
DISI, University of Trento
Povo, 38123, Italy
jrr@disi.unitn.it
Raffaella Bernardi
DISI, University of Trento
Povo, 38123, Italy
bernardi@disi.unitn.it
Abstract
The problem of learning language models
from large text corpora has been widely stud-
ied within the computational linguistic com-
munity. However, little is known about the
performance of these language models when
applied to the computer vision domain. In this
work, we compare representative models: a
window-based model, a topic model, a distri-
butional memory and a commonsense knowl-
edge database, ConceptNet, in two visual
recognition scenarios: human action recog-
nition and object prediction. We examine
whether the knowledge extracted from texts
through these models are compatible to the
knowledge represented in images. We de-
termine the usefulness of different language
models in aiding the two visual recognition
tasks. The study shows that the language
models built from general text corpora can be
used instead of expensive annotated images
and even outperform the image model when
testing on a big general dataset.
1 Introduction
Computational linguistics have created many tools
for automatic knowledge acquisition which have
been successfully applied in many tasks inside the
language domain, such as question answering, ma-
chine translation, semantic web, etc. In this paper
we ask whether such knowledge generalizes to the
observed reality outside the language domain, where
we use well-known image datasets as a proxy for ob-
served reality.
In particular, we aim to determine which language
model yields knowledge that is most suitable for use
in Computer Vision. Therefore we test a variety of
language models and a linguistically mined knowl-
edge base within two computer vision scenarios:
Human action recognition : Recognizing
<subject, verb, object> triples based on
objects (e.g., car, horse) and scenes (the place
that the actions occur, e.g., countryside, forest,
office) recognized in images. In this scenario,
we only consider images with human actions
so the ?human? subject is always present.
Objects in context : Predicting the most likely
identity of an object given its context as ex-
pressed in terms of co-occurring objects.
Computer vision can greatly benefit from natural
language processing as learning from images re-
quires a prohibitively expensive annotation effort. A
major goal of natural language processing is to ob-
tain general knowledge from text and in this paper
we test which model provides the best knowledge
for use in the visual domain.
Within the two visual scenarios, we compare three
state-of-the-art language models and a knowledge
base: (1) A window-based model, which counts
co-occurrence frequencies within a fixed window;
(2) R-LDA (Se?aghdha, 2010), an extension of LDA
that enables generation of joint probabilities; (3)
TypeDM (Baroni and Lenci, 2010), a strong Distri-
butional Memory model; (4) ConceptNet (Speer and
Havasi, 2013), an automatically generated semantic
graph containing concepts with their relations.
We test the language models in two ways: (1) We
directly compare the statistics of the linguistic mod-
els with statistics extracted from the visual domain.
769
(2) We compare the linguistic models inside the two
computer vision applications, leading to a direct es-
timation of their usefulness.
To summarize, our main research questions are:
(1) Is the knowledge from language compatible with
the knowledge from vision? (2) Can the knowl-
edge extracted from language help in computer vi-
sion scenarios?
2 Related Work
Using high level knowledge to aid image under-
standing has become a recent interest in the com-
puter vision community. Objects, actions and scenes
are detected and localized in images using low-
level features. This detection and localization pro-
cess is guided by reasoning and knowledge. Such
knowledge is employed to disambiguate locations
between objects in (Gupta and Davis, 2008). From
the defined relationships between nouns (e.g., above,
below, brighter, smaller), the system constrains
which region in an image corresponds to which ob-
ject/noun. Similarly, (Srikanth et al, 2005) ex-
ploit ontologies extracted from WordNet to asso-
ciate words and images and image regions. (Yu
et al, 2011) employ relations between scenes and
objects introducing an active model to recognize
scenes through objects. The reasoning knowledge
limits the detector to search for an object within a
particular region rather than on the whole image.
Language models have also been employed to
generate descriptive sentences for images. (Ushiku
et al, 2012) introduce an online learning method for
multi-keyphrase estimation to generate a sentence
using a grammar model to describe an image. Simi-
larly, from objects and scenes detected in an image,
(Yang et al, 2011) estimated a sentence structure to
generate a sentence description composed of a noun,
verb, scene and preposition.
The studies most similar to ours are (Teo et al,
2012) and (Lampert et al, 2009). In (Teo et al,
2012), the Gigaword corpus is used to extract rela-
tionships between tools and actions (e.g., knife - cut,
cup - drink) by counting their co-occurences. These
relationships are used to constrain and select the
most plausible actions within a predefined set of ac-
tions in cooking videos. Instead of using this knowl-
edge as a guidance during recognition, we compare
different language models and build a general frame-
work that is able to detect unseen actions through
their components (verb - object - scene), hence our
method does not limit the number of actions in im-
ages. (Lampert et al, 2009) use attributes of nouns
(e.g., an animal: white, eat fish, water, etc.). They
can detect animals without having seen training ex-
amples by manually defining the attributes of the tar-
get animal. In this work, rather than relying on man-
ual definitions, our aim is to find the best language
models built automatically from available corpora to
extract relations from natural language.
Currently, human action recognition is popular
and mostly studied in video using the Bag-of-Visual-
Words method (Delaitre et al, 2010; Everts et al,
2013; Kuehne et al, 2012; Reddy and Shah, 2012;
Wang et al, 2013). In this method one extracts small
local visual patches of, say, 24 by 24 pixels by 10
frames at every 12th pixel at every 5th frame. For
each patch local gradients or local movement (opti-
cal flow) histograms are calculated. Then these local
visual features are mapped to abstract, predefined
?visual words?, previously obtained using k-means
clustering on a set of random features. While results
are good, there are two main drawbacks with this
approach. First of all, human actions are semantic
and more naturally recognized through their compo-
nents (human, objects, scene) rather than through a
bag of local gradient/motion patterns. Hence we use
a component-based method for human action recog-
nition. Second, the number of possible human ac-
tions is huge (the number of objects times the num-
ber of verbs). Obtaining annotated visual examples
for each action is therefore prohibitively expensive.
So we learn from language models how components
combine into human actions.
3 Two Visual Recognition Scenarios
We now describe the two computer vision scenarios:
human action recognition and objects in context.
3.1 Human Action Recognition
We want to identify a human action, defined as a
<subject, verb, object> triple. We do this by recog-
nizing the human, the object, and the scene and then
determine the most likely verb based on these com-
ponents. Scenes are only used here as features for
770
predicting/disambiguating the human action and the
final task is to define the human action triple. As in
most work in human action recognition, we simplify
the problem by considering only images in which
human actions occur. This means that a human is
always present, leaving the problem of predicting
the verb given the object and the scene. While this
may seem like a strong assumption, the possibility
of having no action in the image at all is largely un-
explored in computer vision due to its difficulty.
	

	
		


 



	

Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 23?32,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Entailment above the word level in distributional semantics
Marco Baroni
Raffaella Bernardi
University of Trento
name.surname@unitn.it
Ngoc-Quynh Do
Free University of Bozen-Bolzano
quynhdtn.hut@gmail.com
Chung-chieh Shan
Cornell University
University of Tsukuba
ccshan@post.harvard.edu
Abstract
We introduce two ways to detect entail-
ment using distributional semantic repre-
sentations of phrases. Our first experiment
shows that the entailment relation between
adjective-noun constructions and their head
nouns (big cat |= cat), once represented as
semantic vector pairs, generalizes to lexical
entailment among nouns (dog |= animal).
Our second experiment shows that a classi-
fier fed semantic vector pairs can similarly
generalize the entailment relation among
quantifier phrases (many dogs|=some dogs)
to entailment involving unseen quantifiers
(all cats|=several cats). Moreover, nominal
and quantifier phrase entailment appears to
be cued by different distributional corre-
lates, as predicted by the type-based view
of entailment in formal semantics.
1 Introduction
Distributional semantics (DS) approximates lin-
guistic meaning with vectors summarizing the
contexts where expressions occur. The success
of DS in lexical semantics has validated the hy-
pothesis that semantically similar expressions oc-
cur in similar contexts (Landauer and Dumais,
1997; Lund and Burgess, 1996; Sahlgren, 2006;
Schu?tze, 1997; Turney and Pantel, 2010). For-
mal semantics (FS) represents linguistic mean-
ings as symbolic formulas and assemble them via
composition rules. FS has successfully modeled
quantification and captured inferential relations
between phrases and between sentences (Mon-
tague, 1970; Thomason, 1974; Heim and Kratzer,
1998). The strengths of DS and FS have been
complementary to date: On one hand, DS has in-
duced large-scale semantic representations from
corpora, but it has been largely limited to the
lexical domain. On the other hand, FS has pro-
vided sophisticated models of sentence meaning,
but it has been largely limited to hand-coded mod-
els that do not scale up to real-life challenges by
learning from data.
Given these complementary strengths, we nat-
urally ask if DS and FS can address each other?s
limitations. Two recent strands of research are
bringing DS closer to meeting core FS chal-
lenges. One strand attempts to model compo-
sitionality with DS methods, representing both
primitive and composed linguistic expressions
as distributional vectors (Baroni and Zamparelli,
2010; Grefenstette and Sadrzadeh, 2011; Gue-
vara, 2010; Mitchell and Lapata, 2010). The
other strand attempts to reformulate FS?s notion
of logical inference in terms that DS can cap-
ture (Erk, 2009; Geffet and Dagan, 2005; Kotler-
man et al 2010; Zhitomirsky-Geffet and Dagan,
2010). In keeping with the lexical emphasis of
DS, this strand has focused on inference at the
word level, or lexical entailment, that is, discover-
ing from distributional vectors of hyponyms (dog)
that they entail their hypernyms (animal).
This paper brings these two strands of research
together by demonstrating two ways in which the
distributional vectors of composite expressions
bear on inference. Here we focus on phrasal vec-
tors harvested directly from the corpus rather than
obtained compositionally. In a first experiment,
we exploit the entailment properties of a class
of composite expressions, namely adjective-noun
constructions (ANs), to harvest training data for
an entailment recognizer. The recognizer is then
successfully applied to detect lexical entailment.
In short, since almost all ANs entail the noun they
contain (red car entails car), the distributional
vectors of AN-N pairs can train a classifier to de-
tect noun pairs that stand in the same relation (dog
23
entails animal). With almost no manual effort,
we achieve performance nearly identical with the
state-of-the-art balAPinc measure that Kotlerman
et al(2010) crafted, which detects feature inclu-
sion between the two nouns? occurrence contexts.
Our second experiment goes beyond lexical in-
ference. We look at phrases built from a quanti-
fying determiner1 and a noun (QNs) and use their
distributional vectors to recognize entailment re-
lations of the form many dogs |= some dogs, be-
tween two QNs sharing the same noun. It turns
out that a classifier trained on a set of Q1N |=Q2N
pairs can recognize entailment in pairs with a new
quantifier configuration. For example, we can
train on many dogs |= some dogs then correctly
predict all cats|=several cats. Interestingly, on the
QN entailment task, neither our classifier trained
on AN-N pairs nor the balAPinc method beat
baseline methods. This suggests that our success-
ful QN classifiers tap into vector properties be-
yond such relations as feature inclusion that those
methods for nominal entailment rely upon.
Together, our experiments show that corpus-
harvested DS representations of composite ex-
pressions such as ANs and QNs contain suffi-
cient information to capture and generalize their
inference patterns. This result brings DS closer
to the central concerns of FS. In particular, the
QN study is the first to our knowledge to show
that DS vectors capture semantic properties not
only of content words, but of an important class of
function words (quantifying determiners) deeply
studied in FS but of little interest until now in DS.
Besides these theoretical implications, our re-
sults are of practical import. First, our AN study
presents a novel, practical method for detect-
ing lexical entailment that reaches state-of-the-
art performance with little or no manual interven-
tion. Lexical entailment is in turn fundamental
for constructing ontologies and other lexical re-
sources (Buitelaar and Cimiano, 2008). Second,
our QN study demonstrates that phrasal entail-
ment can be automatically detected and thus paves
the way to apply DS to advanced NLP tasks such
as recognizing textual entailment (Dagan et al
2009).
1In the sequel we will simply refer to a ?quantifying de-
terminer? as a ?quantifier?.
2 Background
2.1 Distributional semantics above the word
level
DS models such as LSA (Landauer and Dumais,
1997) and HAL (Lund and Burgess, 1996) ap-
proximate the meaning of a word by a vector that
summarizes its distribution in a corpus, for exam-
ple by counting co-occurrences of the word with
other words. Since semantically similar words
tend to share similar contexts, DS has been very
successful in tasks that require quantifying se-
mantic similarity among words, such as synonym
detection and concept clustering (Turney and Pan-
tel, 2010).
Recently, there has been a flurry of interest
in DS to model meaning composition: How can
we derive the DS representation of a composite
phrase from that of its constituents? Although the
general focus in the area is to perform algebraic
operations on word semantic vectors (Mitchell
and Lapata, 2010), some researchers have also di-
rectly examined the corpus contexts of phrases.
For example, Baldwin et al(2003) studied vec-
tor extraction for phrases because they were inter-
ested in the decomposability of multiword expres-
sions. Baroni and Zamparelli (2010) and Gue-
vara (2010) look at corpus-harvested phrase vec-
tors to learn composition functions that should de-
rive such composite vectors automatically. Ba-
roni and Zamparelli, in particular, showed qual-
itatively that directly corpus-harvested vectors for
AN constructions are meaningful; for example,
the vector of young husband has nearest neigh-
bors small son, small daughter and mistress. Fol-
lowing up on this approach, we show here quanti-
tatively that corpus-harvested AN vectors are also
useful for detecting entailment. We find moreover
distributional vectors informative and useful not
only for phrases made of content words (such as
ANs) but also for phrases containing functional
elements, namely quantifying determiners.
2.2 Entailment from formal to distributional
semantics
Entailment in FS To characterize the condi-
tions under which a sentence is true, FS begins
with the lexical meanings of the words in the sen-
tence and builds up the meanings of larger and
larger phrases until it arrives at the meaning of the
whole sentence. The meanings throughout this
24
compositional process inhabit a variety of seman-
tic domains, depending on the syntactic category
of the expressions: typically, a sentence denotes a
truth value (true or false) or truth conditions,
a noun such as cat denotes a set of entities, and a
quantifier phrase (QP) such as all cats denotes a
set of sets of entities.
The entailment relation (|=) is a core notion of
logic: it holds between one or more sentences and
a sentence such that it cannot be that the former
(antecedent) are true and the latter (consequent)
is false. FS extends this notion from formal-logic
sentences to natural-language expressions. By as-
signing meanings to parts of a sentence, FS allows
defining entailment not only among sentences but
also among words and phrases. Each semantic
domain A has its own entailment relation |=A.
The entailment relation |=S among sentences is
the logical notion just described, whereas the en-
tailment relations |=N and |=QP among nouns
and quantifier phrases are the inclusion relations
among sets of entities and sets of sets of entities
respectively. Our results in Section 5 show that
DS needs to treat |=N and |=QP differently as well.
Empirical, corpus-based perspectives on en-
tailment Until recently, the corpus-based re-
search tradition has studied entailment mostly at
the word level, with applied goals such as clas-
sifying lexical relations and building taxonomic
WordNet-like resources automatically. The most
popular approach, first adopted by Hearst (1992),
extracts lexical relations from patterns in large
corpora. For instance, from the pattern N1 such
as N2 one learns that N2 |=N1 (from insects such
as beetles, derive beetles |= insects). Several stud-
ies have refined and extended this approach (Pan-
tel and Ravichandran, 2004; Snow et al 2005;
Snow et al 2006; Turney, 2008).
While empirically very successful, the pattern-
based method is mostly limited to single content
words (or frequent content-word phrases). We are
interested in entailment between phrases, where it
is not obvious how to use lexico-syntactic patterns
and cope with data sparsity. For instance, it seems
hard to find a pattern that frequently connects one
QP to another it entails, as in all beetles PATTERN
many beetles. Hence, we aim to find a more gen-
eral method and investigate whether DS vectors
(whether corpus-harvested or compositionally de-
rived) encode the information needed to account
for phrasal entailment in a way that can be cap-
tured and generalized to unseen phrase pairs.
Rather recently, the study of sentential entail-
ment has taken an empirical turn, thanks to the de-
velopment of benchmarks for entailment systems.
The FS definition of entailment has been modified
by taking common sense into account. Instead of
a relation from the truth of the consequent to the
truth of the antecedent in any circumstance, the
applied view looks at entailment in terms of plau-
sibility: ? |= ? if a human who reads (and trusts)
? would most likely infer that ? is also true. En-
tailment systems have been compared under this
new perspective in various evaluation campaigns,
the best known being the Recognizing Textual En-
tailment (RTE) initiative (Dagan et al 2009).
Most RTE systems are based on advanced NLP
components, machine learning techniques, and/or
syntactic transformations (Zanzotto et al 2007;
Kouleykov and Magnini, 2005). A few systems
exploit deep FS analysis (Bos and Markert, 2006;
Chambers et al 2007). In particular, the FS re-
sults about QP properties that affect entailment
have been exploited by Chambers et alwho com-
plement a core broad-coverage system with a Nat-
ural Logic module to trade lower recall for higher
precision. For instance, they exploit the mono-
tonicity properties of no that cause the follow-
ing reversal in entailment direction: some bee-
tles |= some insects but no insects |= no beetles.
To investigate entailment step by step, we ad-
dress here a much simpler and clearer type of
entailment than the more complex notion taken
up by the RTE community. While RTE is out-
side our present scope, we do focus on QP entail-
ment as Natural Logic does. However, our eval-
uation differs from Chambers et als, since we
rely on general-purpose DS vectors as our only
resource, and we look at phrase pairs with differ-
ent quantifiers but the same noun. For instance,
we aim to predict that all beetles |= many beetles
but few beetles 6|=all beetles. QPs, of course, have
many well-known semantic properties besides en-
tailment; we leave their analysis to future study.
Entailment in DS Erk (2009) suggests that it
may not be possible to induce lexical entailment
directly from a vector space representation, but it
is possible to encode the relation in this space af-
ter it has been derived through other means. On
the other hand, recent studies (Geffet and Dagan,
25
2005; Kotlerman et al 2010; Weeds et al 2004)
have pursued the intuition that entailment is the
asymmetric ability of one term to ?substitute? for
another. For example, baseball contexts are also
sport contexts but not vice versa, hence baseball
is ?narrower? than sport and baseball |=sport. On
this view, entailment between vectors corresponds
to inclusion of contexts or features, and can be
captured by asymmetric measures of distribution
similarity. In particular, Kotlerman et al(2010)
carefully crafted the balAPinc measure (see Sec-
tion 3.5 below). We adopt this measure because
it has been shown to outperform others in several
tasks that require lexical entailment information.
Like Kotlerman et al we want to capture the
entailment relation between vectors of features.
However, we are interested in entailment not only
between words but also between phrases, and we
ask whether the DS view of entailment as fea-
ture inclusion, which captures entailment between
nouns, also captures entailment between QPs. To
this end, we complement balAPinc with a more
flexible supervised classifier.
3 Data and methods
3.1 Semantic space
We construct distributional semantic vectors from
the 2.83-billion-token concatenation of the British
National Corpus (http://www.natcorp.
ox.ac.uk/), WackyPedia and ukWaC (http:
//wacky.sslmit.unibo.it/). We tok-
enize and POS-tag this corpus, then lemmatize
it with TreeTagger (Schmid, 1995) to merge sin-
gular and plural instances of words and phrases
(some dogs is mapped to some dog).
We process the corpus in two steps to compute
semantic vectors representing our phrases of in-
terest. We use phrases of interest as a general
term to refer to both multiword phrases and sin-
gle words, and more precisely to: those AN and
QN sequences that are in the data sets (see next
subsections), the adjectives, quantifiers and nouns
contained in those sequences, and the most fre-
quent (9.8K) nouns and (8.1K) adjectives in the
corpus. The first step is to count the content
words (more precisely, the most frequent 9.8K
nouns, 8.1K adjectives, and 9.6K verbs in the cor-
pus) that occur in the same sentence as phrases
of interest. In the second step, following standard
practice, the co-occurrence counts are converted
into pointwise mutual information (PMI) scores
(Church and Hanks, 1990). The result of this step
is a sparse matrix (with both positive and negative
entries) with 48K rows (one per phrase of interest)
and 27K columns (one per content word).
3.2 The AN |= N data set
To characterize entailment between nouns using
their semantic vectors, we need data exemplifying
which noun entails which. This section introduces
one cheap way to collect such a training data set
exploiting semantic vectors for composed expres-
sions, namely AN sequences. We rely on the lin-
guistic fact that ANs share a syntactic category
and semantic type with plain common nouns (big
cat shares syntactic category and semantic type
with cat). Furthermore, most adjectives are re-
strictive in the sense that, for every noun N, the
AN sequence entails the N alone (every big cat
is a cat). From a distributional point of view, the
vector for an N should by construction include the
information in the vector for an AN, given that the
contexts where the AN occurs are a subset of the
contexts where the N occurs (cat occurs in all the
contexts where big cat occurs). This ideal inclu-
sion suggests that the DS notion of lexical entail-
ment as feature inclusion (see Section 2.2 above)
should be reflected in the AN |= N pattern.
Because most ANs entail their head Ns, we can
create positive examples of AN |= N without any
manual inspection of the corpus: simply pair up
the semantic vectors of ANs and Ns. Furthermore,
because an AN usually does not entail another N,
we can create negative examples (AN1 6|=N2) just
by randomly permuting the Ns. Of course, such
unsupervised data would be slightly noisy, espe-
cially because some of the most frequent adjec-
tives are not restrictive.
To collect cleaner data and to be sure that we
are really examining the phenomenon of entail-
ment, we took a mere few moments of man-
ual effort to select the 256 restrictive adjectives
from the most frequent 300 adjectives in the cor-
pus. We then took the Cartesian product of these
256 adjectives with the 200 concrete nouns in the
BLESS data set (Baroni and Lenci, 2011). Those
nouns were chosen to avoid highly polysemous
words. From the Cartesian product, we obtain a
total of 1246 AN sequences, such as big cat, that
occur more than 100 times in the corpus. These
AN sequences encompass 190 of the 256 adjec-
26
tives and 128 of the 200 nouns.
The process results in 1246 positive instances
of AN |= N entailment, which we use as training
data. To create a comparable amount of negative
data, we randomly permuted the nouns in the pos-
itive instances to obtain pairs of AN1 6|= N2 (e.g.,
big cat 6|=dog). We manually double-checked that
all positive and negative examples are correctly
classified (2 of 1246 negative instances were re-
moved, leaving 1244 negative training examples).
3.3 The lexical entailment N1 |= N2 data set
For testing data, we first listed all WordNet nouns
in our corpus, then extracted hyponym-hypernym
chains linking the first synsets of these nouns. For
example, pope is found to entail leader because
WordNet contains the chain pope ? spiritual
leader ? leader. Eliminating the 20 hypernyms
with more than 180 hyponyms (mostly very ab-
stract nouns such as entity, object, and quality)
yields 9734 hyponym-hypernym pairs, encom-
passing 6402 nouns. Manually double-checking
these pairs leaves us with 1385 positive instances
of N1 |= N2 entailment.
We created the negative instances of again 1385
pairs by inverting 33% of the positive instances
(from pope|=leader to leader 6|=pope), and by ran-
domly shuffling the words across the positive in-
stances. We also manually double-checked these
pairs to make sure that they are not hyponym-
hypernym pairs.
3.4 The Q1N |= Q2N data set
We study 12 quantifiers: all, both, each, either,
every, few, many, most, much, no, several, some.
We took the Cartesian product of these quantifiers
with the 6402 WordNet nouns described in Sec-
tion 3.3. From this Cartesian product, we obtain
a total of 28926 QN sequences, such as every cat,
that occur at least 100 times in the corpus. These
are our QN phrases of interest to which the proce-
dure in Section 3.1 assigns a semantic vector.
Also, from the set of quantifier pairs (Q1,Q2)
where Q1 6= Q2, we identified 13 clear cases
where Q1 |=Q2 and 17 clear cases where Q1 6|=Q2.
These 30 cases are listed in the first column of
Table 1. For each of these 30 quantifier pairs
(Q1,Q2), we enumerate those WordNet nouns N
such that semantic vectors are available for both
Q1N and Q2N (that is, both sequences occur in
at least 100 times). Each such noun then gives
Quantifier pair Instances Correct
all |= some 1054 1044 (99%)
all |= several 557 550 (99%)
each |= some 656 647 (99%)
all |= many 873 772 (88%)
much |= some 248 217 (88%)
every |= many 460 400 (87%)
many |= some 951 822 (86%)
all |= most 465 393 (85%)
several |= some 580 439 (76%)
both |= some 573 322 (56%)
many |= several 594 113 (19%)
most |= many 463 84 (18%)
both |= either 63 1 (2%)
Subtotal 7537 5804 (77%)
some 6|= every 484 481 (99%)
several 6|= all 557 553 (99%)
several 6|= every 378 375 (99%)
some 6|= all 1054 1043 (99%)
many 6|= every 460 452 (98%)
some 6|= each 656 640 (98%)
few 6|= all 157 153 (97%)
many 6|= all 873 843 (97%)
both 6|= most 369 347 (94%)
several 6|= few 143 134 (94%)
both 6|= many 541 397 (73%)
many 6|= most 463 300 (65%)
either 6|= both 63 39 (62%)
many 6|= no 714 369 (52%)
some 6|= many 951 468 (49%)
few 6|= many 161 33 (20%)
both 6|= several 431 63 (15%)
Subtotal 8455 6690 (79%)
Total 15992 12494 (78%)
Table 1: Entailing and non-entailing quantifier pairs
with number of instances per pair (Section 3.4) and
SVMpair-out performance breakdown (Section 5).
rise to an instance of entailment (Q1N |= Q2N if
Q1 |=Q2; example: many dogs |= several dogs) or
non-entailment (Q1N 6|=Q2N if Q1 6|=Q2; example:
many dogs 6|=most dogs). The number of QN pairs
that each quantifier pair gives rise to in this way is
listed in the second column of Table 1. As shown
there, we have a total of 7537 positive instances
and 8455 negative instances of QN entailment.
3.5 Classification methods
We consider two methods to classify candidate
pairs as entailing or non-entailing, the balAPinc
measure of Kotlerman et al(2010) and a standard
Support Vector Machine (SVM) classifier.
27
balAPinc As discussed in Section 2.2, balAP-
inc is optimized to capture a relation of feature
inclusion between the narrower (entailing) and
broader (entailed) terms, while capturing other in-
tuitions about the relative relevance of features.
balAPinc averages two terms, APinc and LIN.
APinc is given by:
APinc(u |= v) =
?|Fu|
r=1
(
P (r) ? rel?(fr)
)
|Fu|
APinc is a version of the Average Precision
measure from Information Retrieval tailored to
lexical inclusion. Given vectors Fu and Fv rep-
resenting the dimensions with positive PMI val-
ues in the semantic vectors of the candidate pair
u |= v, the idea is that we want the features (that
is, vector dimensions) that have larger values in
Fu to also have large values in Fv (the opposite
does not matter because it is u that should be in-
cluded in v, not vice versa). The Fu features are
ranked according to their PMI value so that fr
is the feature in Fu with rank r, i.e., r-th high-
est PMI. Then the sum of the product of the two
terms P (r) and rel?(fr) across the features in Fu
is computed. The first term is the precision at r,
which is higher when highly ranked u features are
present in Fv as well. The relevance term rel?(fr)
is higher when the feature fr in Fu also appears
in Fv with a high rank. (See Kotlerman et alfor
how P (r) and rel?(fr) are computed.) The result-
ing score is normalized by dividing by the entail-
ing vector size |Fu| (in accordance with the idea
that having more v features should not hurt be-
cause the u features should be included in the v
features, not vice versa).
To balance the potentially excessive asymmetry
of APinc towards the features of the antecedent,
Kotlerman et alaverage it with LIN, the widely
used symmetric measure of distributional similar-
ity proposed by Lin (1998):
LIN(u, v) =
?
f?Fu?Fv [wu(f) + wv(f)]?
f?Fu wu(f) +
?
f?Fv wv(f)
LIN essentially measures feature vector overlap.
The positive PMI values wu(f) and wv(f) of a
feature f in Fu and Fv are summed across those
features that are positive in both vectors, normal-
izing by the cumulative positive PMI mass in both
vectors. Finally, balAPinc is the geometric aver-
age of APinc and LIN:
balAPinc(u|=v) =
?
APinc(u |= v) ? LIN(u, v)
To adapt balAPinc to recognize entailment, we
must select a threshold t above which we classify
a pair as entailing. In the experiments below, we
explore two approaches. In balAPincupper, we op-
timize the threshold directly on the test data, by
setting t to maximize the F-measure on the test
set. This gives us an upper bound on how well bal-
APinc could perform on the test set (but note that
optimizing F does not necessarily translate into a
good accuracy performance, as clearly illustrated
by Table 3 below). In balAPincAN |= N, we use the
AN |= N data set as training data and pick the t
that maximizes F on this training set.
We use the balAPinc measure as a refer-
ence point because, on the evidence provided by
Kotlerman et al it is the state of the art in various
tasks related to lexical entailment. We recognize
however that it is somewhat complex and specifi-
cally tuned to capturing the relation of feature in-
clusion. Consequently, we also experiment with
a more flexible classifier, which can detect other
systematic properties of vectors in an entailment
relation. We present this classifier next.
SVM Support vector machines are widely used
high-performance discriminative classifiers that
find the hyperplane providing the best separation
between negative and positive instances (Cristian-
ini and Shawe-Taylor, 2000). Our SVM classifiers
are trained and tested using Weka 3 and LIBSVM
2.8 (Chang and Lin, 2011). We use the default
polynomial kernel ((u ?v/600)3) with  (tolerance
of termination criterion) set to 1.6. This value was
tuned on the AN |=N data set, which we never use
for testing. In the same initial tuning experiments
on the AN |=N data set, SVM outperformed deci-
sion trees, naive Bayes, and k-nearest neighbors.
We feed each potential entailment pair to SVM
by concatenating the two vectors representing the
antecedent and consequent expressions.2 How-
ever, for efficiency and to mitigate data sparse-
ness, we reduce the dimensionality of the seman-
tic vectors to 300 columns using Singular Value
Decomposition (SVD) before feeding them to the
classifier.3 Because the SVD-reduced semantic
2We have tried also to represent a pair by subtracting and
by dividing the two vectors. The concatenation operation
gave more successful results.
3To keep a manageable parameter space, we picked 300
columns without tuning. This is the best value reported in
many earlier studies, including classic LSA. Since SVD
sometimes improves the semantic space (Landauer and Du-
28
vectors occupy a 300-dimensional space, the en-
tailment pairs occupy a 600-dimensional space.
An SVM with a polynomial kernel takes into
account not only individual input features but also
their interactions (Manning et al 2008, chapter
15). Thus, our classifier can capture not just prop-
erties of individual dimensions of the antecedent
and consequent pairs, but also properties of their
combinations (e.g., the product of the first dimen-
sions of the antecedent and the consequent). We
conjecture that this property of SVMs is funda-
mental to their success at detecting entailment,
where relations between the antecedent and the
consequent should matter more than their inde-
pendent characteristics.
4 Predicting lexical entailment from
AN |= N evidence
Since the contexts of AN must be a subset of the
contexts of N, semantic vectors harvested from
AN phrases and their head Ns are by construc-
tion in an inclusion relation. The first experiment
shows that these vectors constitute excellent train-
ing data to discover entailment between nouns.
This suggests that the vector pairs representing
entailment between nouns are also in an inclusion
relation, supporting the conjectures of Kotlerman
et al(2010) and others.
Table 2 reports the results we obtained with
balAPincupper, balAPincAN |= N (Section 3.5) and
SVMAN |= N (the SVM classifier trained on the
AN |= N data). As an upper bound for meth-
ods that generalize from AN |= N, we also re-
port the performance of SVM trained with 10-fold
cross-validation on the N1 |= N2 data themselves
(SVMupper). Finally, we tried two baseline classi-
fiers. The first baseline (fq(N1)< fq(N2)) guesses
entailment if the first word is less frequent than
the second. The second (cos(N1, N2)) applies a
threshold (determined on the test set) to the co-
sine similarity of the pair. The results of these
baselines shown in Table 2 use SVD; those with-
out SVD are similar. Both baselines outperformed
more trivial methods such as random guessing or
fixed response, but they performed significantly
worse than SVM and balAPinc.
Both methods that generalize entailment from
AN |= N to N1 |= N2 perform well, with 70%
mais, 1997; Rapp, 2003; Schu?tze, 1997), we tried balAPinc
on the SVD-reduced vectors as well, but results were consis-
tently worse than with PMI vectors.
P R F Accuracy
(95% C.I.)
SVMupper 88.6 88.6 88.5 88.6 (87.3?89.7)
balAPincAN |= N 65.2 87.5 74.7 70.4 (68.7?72.1)
balAPincupper 64.4 90.0 75.1 70.1 (68.4?71.8)
SVMAN |= N 69.3 69.3 69.3 69.3 (67.6?71.0)
cos(N1, N2) 57.7 57.6 57.5 57.6 (55.8?59.5)
fq(N1)< fq(N2) 52.1 52.1 51.8 53.3 (51.4?55.2)
Table 2: Detecting lexical entailment. Results ranked
by accuracy and expressed as percentages. 95% con-
fidence intervals around accuracy calculated by bino-
mial exact tests.
accuracy on the test set, which is balanced be-
tween positive and negative instances. Interest-
ingly, the balAPinc decision thresholds tuned on
the AN |= N set and on the test data are very
close (0.26 vs. 0.24), resulting in very similar per-
formance for balAPincAN |= N and balAPincupper.
This suggests that the relation captured by bal-
APinc on the phrasal entailment training data is
indeed the same that the measure captures when
applied to lexical entailment data.
The success of this first experiment shows that
the entailment relation present in the distribu-
tional representation of AN phrases and their
head Ns transfers to lexical entailment (entailment
among Ns). Most importantly, this result demon-
strates that the semantic vectors of composite ex-
pressions (such as ANs) are useful for lexical en-
tailment. Moreover, the result is in accordance
with the view of FS, that ANs and Ns have the
same semantic type, and thus they enter entail-
ment relations of the same kind. Finally, the hy-
pothesis that entailment among nouns is reflected
by distributional inclusion among their semantic
vectors (Kotlerman et al 2010) is supported both
by the successful generalization of the SVM clas-
sifier trained on AN |= N pairs and by the good
performance of the balAPinc measure.
5 Generalizing QN entailment
The second study is somewhat more ambitious,
as it aims to capture and generalize the entailment
relation between QPs (of shape QN) using only
the corpus-harvested semantic vectors represent-
ing these phrases as evidence. We are thus first
and foremost interested in testing whether these
vectors encode information that can help a power-
29
P R F Accuracy
(95% C.I.)
SVMpair-out 76.7 77.0 76.8 78.1 (77.5?78.8)
SVMquantifier-out 70.1 65.3 68.0 71.0 (70.3?71.7)
SVMQpair-out 67.9 69.8 68.9 70.2 (69.5?70.9)
SVMQquantifier-out 53.3 52.9 53.1 56.0 (55.2?56.8)
cos(QN1, QN2) 52.9 52.3 52.3 53.1 (52.3?53.9)
balAPincAN |= N 46.7 5.6 10.0 52.5 (51.7?53.3)
SVMAN |= N 2.8 42.9 5.2 52.4 (51.7?53.2)
fq(QN1)<fq(QN2) 51.0 47.4 49.1 50.2 (49.4?51.0)
balAPincupper 47.1 100 64.1 47.2 (46.4?47.9)
Table 3: Detecting quantifier entailment. Results
ranked by accuracy and expressed as percentages.
95% confidence intervals around accuracy calculated
by binomial exact tests.
ful classifier, such as SVM, to detect entailment.
To abstract away from lexical or other effects
linked to a specific quantifier, we consider two
challenging training and testing regimes. In the
first (SVMpair-out), we hold out one quantifier pair
as testing data and use the other 29 pairs in Table 1
as training data. Thus, for example, the classifier
must discover all dogs |= some dogs without see-
ing any all N |= some N instance in the training
data. In the second (SVMquantifier-out), we hold out
one of the 12 quantifiers as testing data (that is,
hold out every pair involving a certain quantifier)
and use the rest as training data. For example,
the quantifier must guess all dogs |= some dogs
without ever seeing all in the training data. We
expect the second training regime to be more dif-
ficult, not just because there is less training data,
but also because the trained classifier is tested on
a quantifier that it has never encountered within
any training QN sequence.4
Table 3 reports the results for SVMpair-out and
SVMquantifier-out, as well as for the methods we
tried in the lexical entailment experiments. (As
in the first study, the frequency- and cosine-based
4In our initial experiments, we added negative entail-
ment instances by blindly permuting the nouns, under the
assumption that Q1N1 typically does not entail Q2N2 when
Q1 6= Q2 and N1 6= N2. These additional instances turned
out to be much easier to classify: adding an equal proportion
of them to the training data and testing data, such that the
number of instances where N1 = N2 and where N1 6= N2
is equal, reduced every error rate roughly by half. The re-
ported results do not involve these additional instances.
baselines are only slightly better overall than more
trivial baselines.) We consider moreover an alter-
native approach that ignores the noun altogether
and uses vectors for the quantifiers only (e.g., the
decision about all dogs |=some dogs considers the
corpus-derived all and some vectors only). The
models resulting from this Q-only strategy are
marked with the superscript Q in the table.
The results confirm clearly that semantic vec-
tors for QNs contain enough information to allow
a classifier to detect entailment: SVMquantifier-out
performs as well as the lexical entailment classi-
fiers of our first study, and SVMpair-out does even
better. This success is especially impressive given
our challenging training and testing regimes.
In contrast to the first study, now SVMAN |= N,
the classifier trained on the AN |= N data set,
and balAPinc perform no better than the base-
lines. (Here balAPincupper and balAPincAN |= N
pick very different thresholds: the first settling
on a very low t = 0.01, whereas for the sec-
ond t = 0.26.) As predicted by FS (see Section
2.2 above), noun-level entailment does not gen-
eralize to quantifier phrase entailment, since the
two structures have different semantic types, cor-
responding to different kinds of entailment rela-
tions. Moreover, the failure of balAPinc suggests
that, whatever evidence the SVMs rely upon, it is
not simple feature inclusion.
Interestingly, even the Q vectors alone encode
enough information to capture entailment above
chance. Still, the huge drop in performance from
SVMQpair-out to SVM
Q
quantifier-out suggests that the Q-
only method learned ad-hoc properties that do not
generalize (e.g., ?all entails every Q2?).
Tables 1 and 4 break down the SVM results by
(pairs of) quantifiers. We highlight the remark-
able dichotomy in Table 4 between the good per-
formance on the universal-like quantifiers (each,
every, all, much) and the poor performance on the
existential-like ones (some, no, both, either).
In sum, the QN experiments show that seman-
tic vectors contain enough information to detect
a logical relation such as entailment not only be-
tween words, but also between phrases contain-
ing quantifiers that determine their entailment re-
lation. While a flexible classifier such as SVM
performs this task well, neither measuring fea-
ture inclusion nor generalizing nominal entail-
ment works. SVMs are evidently tapping into
other properties of the vectors.
30
Quantifier Instances Correct
|= 6|= |= 6|=
each 656 656 649 637 (98%)
every 460 1322 402 1293 (95%)
much 248 0 216 0 (87%)
all 2949 2641 2011 2494 (81%)
several 1731 1509 1302 1267 (79%)
many 3341 4163 2349 3443 (77%)
few 0 461 0 311 (67%)
most 928 832 549 511 (60%)
some 4062 3145 1780 2190 (55%)
no 0 714 0 380 (53%)
both 636 1404 589 303 (44%)
either 63 63 2 41 (34%)
Total 15074 16910 9849 12870 (71%)
Table 4: Breakdown of results with leaving-one-
quantifier-out (SVMquantifier-out) training regime.
6 Conclusion
Our main results are as follows.
1. Corpus-harvested semantic vectors repre-
senting adjective-noun constructions and
their heads encode a relation of entailment
that can be exploited to train a classifier
to detect lexical entailment. In particular,
a relation of feature inclusion between the
narrower antecedent and broader consequent
terms captures both AN |= N and N1 |= N2
entailment.
2. The semantic vectors of quantifier-noun con-
structions also encode information sufficient
to learn an entailment relation that general-
izes to QNs containing quantifiers that were
not seen during training.
3. Neither the entailment information encoded
in AN |= N vectors nor the balAPinc mea-
sure generalizes well to entailment detection
in QNs. This result suggests that QN vectors
encode a different kind of entailment, as also
suggested by type distinctions in Formal Se-
mantics.
In future work, we want first of all to conduct
an analysis of the features in the Q1N |=Q2N vec-
tors that are crucially exploited by our success-
ful entailment recognizers, in order to understand
which characteristics of entailment are encoded in
these vectors.
Very importantly, instead of extracting vectors
representing phrases directly from the corpus, we
intend to derive them by compositional operations
proposed in the literature (see Section 2.1 above).
We will look for composition methods producing
vector representations of composite expressions
that are as good as (or better than) vectors directly
extracted from the corpus at encoding entailment.
Finally, we would like to evaluate our entail-
ment detection strategies for larger phrases and
sentences, possibly containing multiple quanti-
fiers, and eventually embed them as core compo-
nents of an RTE system.
Acknowledgments
We thank the Erasmus Mundus EMLCT Program
for the student and visiting scholar grants to the
third and fourth author, respectively. The first
two authors are partially funded by the ERC 2011
Starting Independent Research Grant supporting
the COMPOSES project (nr. 283554). We are
grateful to Gemma Boleda, Louise McNally, and
the anonymous reviewers for valuable comments,
and to Ido Dagan for important insights into en-
tailment from an empirical point of view.
References
Timothy Baldwin, Colin Bannard, Takaaki Tanaka,
and Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Pro-
ceedings of the ACL 2003 Workshop on Multiword
Expressions, pages 89?96.
Marco Baroni and Alessandro Lenci. 2011. How
we BLESSed distributional semantic evaluation. In
Proceedings of the Workshop on Geometrical Mod-
els of Natural Language Semantics.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Johan Bos and Katja Markert. 2006. When logical
inference helps determining textual entailment (and
when it doesn?t. In Proceedings of the Second PAS-
CAL Challenges Workshop on Recognising Textual
Entailment.
Paul Buitelaar and Philipp Cimiano. 2008. Bridging
the Gap between Text and Knowledge. IOS, Ams-
terdam.
Nathanael Chambers, Daniel Cer, Trond Grenager,
David Hall, Chloe Kiddon, Bill MacCartney, Marie-
Catherine de Marneffe, Daniel Ramage, Eric Yeh,
31
and Christopher D. Manning. 2007. Learning
alignments and leveraging natural logic. In ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2(3):27:1?27:27.
Kenneth Church and Peter Hanks. 1990. Word associ-
ation norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22?29.
Nello Cristianini and John Shawe-Taylor. 2000. An
introduction to Support Vector Machines and other
kernel-based learning methods. Cambridge Univer-
sity Press, Cambridge.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: ratio-
nal, evaluation and approaches. Natural Language
Engineering, 15:459?476.
Katrin Erk. 2009. Supporting inferences in semantic
space: representing words as regions. In Proceed-
ings of IWCS, pages 104?115, Tilburg, Netherlands.
Maayan Geffet and Ido Dagan. 2005. The distribu-
tional inclusion hypotheses and lexical entailment.
In Proceedings of ACL, pages 107?114, Ann Arbor,
MI.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011. Experimental support for a categorical com-
positional distributional model of meaning. In Pro-
ceedings of EMNLP, pages 1395?1404, Edinburgh.
Emiliano Guevara. 2010. A regression model
of adjective-noun compositionality in distributional
semantics. In Proceedings of the ACL GEMS Work-
shop, pages 33?37, Uppsala, Sweden.
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
COLING, pages 539?545, Nantes, France.
Irene Heim and Angelika Kratzer. 1998. Semantics in
Generative Grammar. Blackwell, Oxford.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and
Maayan Zhitomirsky-Geffet. 2010. Directional
distributional similarity for lexical inference. Natu-
ral Language Engineering, 16(4):359?389.
Milen Kouleykov and Bernardo Magnini. 2005. Tree
edit sistance for textual entailment. In Proceed-
ings of RALNP-2005, International Conference on
Recent Advances in Natural Language Processing,
pages 271?278.
Thomas Landauer and Susan Dumais. 1997. A
solution to Plato?s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological Review,
104(2):211?240.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of ICML, pages
296?304, Madison, WI, USA.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, 28:203?
208.
Chris Manning, Prabhakar Raghavan, and Hinrich
Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, Cambridge.
Jeff Mitchell and Mirella Lapata. 2010. Composi-
tion in distributional models of semantics. Cogni-
tive Science, 34(8):1388?1429.
Richard Montague. 1970. Universal Grammar. Theo-
ria, 36:373?398.
Patrick Pantel and Deepak Ravichandran. 2004. Au-
tomatically labeliing semantic classes. In Proceed-
ings of HLT-NAACL 2004, pages 321?328.
Reinhard Rapp. 2003. Word sense discovery based on
sense descriptor dissimilarity. In Proceedings of the
9th MT Summit, pages 315?322, New Orleans, LA.
Magnus Sahlgren. 2006. The Word-Space Model.
Dissertation, Stockholm University.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to German.
In Proceedings of the EACL-SIGDAT Workshop,
Dublin, Ireland.
Hinrich Schu?tze. 1997. Ambiguity Resolution in Nat-
ural Language Learning. CSLI, Stanford, CA.
Rion Snow, Daniel Juravsky, and Andrew Y. Ng.
2005. Learning syntactic patterns for automatic hy-
pernym discovery. In Proceedings of NIPS 17.
Rion Snow, Daniel Juravsky, and Andrew Y. Ng.
2006. Semantic taxonomy induction from het-
erogenous evidence. In Proceedings of ACL 2006,
pages 801?808.
Richmond H. Thomason, editor. 1974. Formal Phi-
losophy: Selected Papers of Richard Montague.
Yale University Press, New York.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Peter Turney. 2008. A uniform approach to analogies,
synonyms, antonyms and associations. In Proceed-
ings of COLING, pages 905?912, Manchester, UK.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th Interna-
tional Conference of Computational Linguistics,
COLING-2004, pages 1015?1021.
Fabio M. Zanzotto, Marco Pennacchiotti, and Alessan-
dro Moschitti. 2007. Shallow semantics in fast tex-
tual entailment rule learners. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing.
Maayan Zhitomirsky-Geffet and Ido Dagan. 2010.
Bootstrapping distributional feature vector quality.
Computational Linguistics, 35(3):435?461.
32
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 53?57,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A relatedness benchmark to test the role of determiners
in compositional distributional semantics
Raffaella Bernardi and Georgiana Dinu and Marco Marelli and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, Italy)
first.last@unitn.it
Abstract
Distributional models of semantics cap-
ture word meaning very effectively, and
they have been recently extended to ac-
count for compositionally-obtained rep-
resentations of phrases made of content
words. We explore whether compositional
distributional semantic models can also
handle a construction in which grammat-
ical terms play a crucial role, namely de-
terminer phrases (DPs). We introduce a
new publicly available dataset to test dis-
tributional representations of DPs, and we
evaluate state-of-the-art models on this set.
1 Introduction
Distributional semantics models (DSMs) approx-
imate meaning with vectors that record the dis-
tributional occurrence patterns of words in cor-
pora. DSMs have been effectively applied to in-
creasingly more sophisticated semantic tasks in
linguistics, artificial intelligence and cognitive sci-
ence, and they have been recently extended to
capture the meaning of phrases and sentences via
compositional mechanisms. However, scaling up
to larger constituents poses the issue of how to
handle grammatical words, such as determiners,
prepositions, or auxiliaries, that lack rich concep-
tual content, and operate instead as the logical
?glue? holding sentences together.
In typical DSMs, grammatical words are treated
as ?stop words? to be discarded, or at best used
as context features in the representation of content
words. Similarly, current compositional DSMs
(cDSMs) focus almost entirely on phrases made
of two or more content words (e.g., adjective-noun
or verb-noun combinations) and completely ig-
nore grammatical words, to the point that even
the test set of transitive sentences proposed by
Grefenstette and Sadrzadeh (2011) contains only
Tarzan-style statements with determiner-less sub-
jects and objects: ?table show result?, ?priest say
mass?, etc. As these examples suggest, however,
as soon as we set our sight on modeling phrases
and sentences, grammatical words are hard to
avoid. Stripping off grammatical words has more
serious consequences than making you sound like
the Lord of the Jungle. Even if we accept the
view of, e.g., Garrette et al (2013), that the log-
ical framework of language should be left to other
devices than distributional semantics, and the lat-
ter should be limited to similarity scoring, still ig-
noring grammatical elements is going to dramat-
ically distort the very similarity scores (c)DSMs
should provide. If we want to use a cDSM for
the classic similarity-based paraphrasing task, the
model shouldn?t conclude that ?The table shows
many results? is identical to ?the table shows no
results? since the two sentences contain the same
content words, or that ?to kill many rats? and ?to
kill few rats? are equally good paraphrases of ?to
exterminate rats?.
We focus here on how cDSMs handle determin-
ers and the phrases they form with nouns (deter-
miner phrases, or DPs).1 While determiners are
only a subset of grammatical words, they are a
large and important subset, constituting the natu-
ral stepping stone towards sentential distributional
semantics: Compositional methods have already
been successfully applied to simple noun-verb and
noun-verb-noun structures (Mitchell and Lapata,
2008; Grefenstette and Sadrzadeh, 2011), and de-
terminers are just what is missing to turn these
skeletal constructions into full-fledged sentences.
Moreover, determiner-noun phrases are, in super-
ficial syntactic terms, similar to the adjective-noun
phrases that have already been extensively studied
from a cDSM perspective by Baroni and Zampar-
1Some linguists refer to what we call DPs as noun phrases
or NPs. We say DPs simply to emphasize our focus on deter-
miners.
53
elli (2010), Guevara (2010) and Mitchell and Lap-
ata (2010). Thus, we can straightforwardly extend
the methods already proposed for adjective-noun
phrases to DPs.
We introduce a new task, a similarity-based
challenge, where we consider nouns that are
strongly conceptually related to certain DPs and
test whether cDSMs can pick the most appropri-
ate related DP (e.g., monarchy is more related to
one ruler than many rulers).2 We make our new
dataset publicly available, and we hope that it will
stimulate further work on the distributional seman-
tics of grammatical elements.3
2 Composition models
Interest in compositional DSMs has skyrocketed
in the last few years, particularly since the influ-
ential work of Mitchell and Lapata (2008; 2009;
2010), who proposed three simple but effective
composition models. In these models, the com-
posed vectors are obtained through component-
wise operations on the constituent vectors. Given
input vectors u and v, the multiplicative model
(mult) returns a composed vector p with: pi =
uivi. In the weighted additive model (wadd), the
composed vector is a weighted sum of the two in-
put vectors: p = ?u+?v, where ? and ? are two
scalars. Finally, in the dilation model, the output
vector is obtained by first decomposing one of the
input vectors, say v, into a vector parallel to u and
an orthogonal vector. Following this, the parallel
vector is dilated by a factor ? before re-combining.
This results in: p = (?? 1)?u,v?u+ ?u,u?v.
A more general form of the additive model
(fulladd) has been proposed by Guevara (2010)
(see also Zanzotto et al (2010)). In this approach,
the two vectors to be added are pre-multiplied by
weight matrices estimated from corpus-extracted
examples: p = Au+Bv.
Baroni and Zamparelli (2010) and Coecke et
al. (2010) take inspiration from formal semantics
to characterize composition in terms of function
application. The former model adjective-noun
phrases by treating the adjective as a function from
nouns onto modified nouns. Given that linear
functions can be expressed by matrices and their
application by matrix-by-vector multiplication, a
2Baroni et al (2012), like us, study determiner phrases
with distributional methods, but they do not model them com-
positionally.
3Dataset and code available from clic.cimec.
unitn.it/composes.
functor (such as the adjective) is represented by a
matrix U to be multiplied with the argument vec-
tor v (e.g., the noun vector): p = Uv. Adjective
matrices are estimated from corpus-extracted ex-
amples of noun vectors and corresponding output
adjective-noun phrase vectors, similarly to Gue-
vara?s approach.4
3 The noun-DP relatedness benchmark
Paraphrasing a single word with a phrase is a
natural task for models of compositionality (Tur-
ney, 2012; Zanzotto et al, 2010) and determin-
ers sometimes play a crucial role in defining the
meaning of a noun. For example a trilogy is com-
posed of three works, an assemblage includes sev-
eral things and an orchestra is made of many
musicians. These examples are particularly in-
teresting, since they point to a ?conceptual? use
of determiners, as components of the stable and
generic meaning of a content word (as opposed to
situation-dependent deictic and anaphoric usages):
for these determiners the boundary between con-
tent and grammatical word is somewhat blurred,
and they thus provide a good entry point for testing
DSM representations of DPs on a classic similarity
task. In other words, we can set up an experiment
in which having an effective representation of the
determiner is crucial in order to obtain the correct
result.
Using regular expressions over WordNet
glosses (Fellbaum, 1998) and complementing
them with definitions from various online dic-
tionaries, we constructed a list of more than 200
nouns that are strongly conceptually related to a
specific DP. We created a multiple-choice test set
by matching each noun with its associated DP
(target DP), two ?foil? DPs sharing the same noun
as the target but combined with other determiners
(same-N foils), one DP made of the target deter-
miner combined with a random noun (same-D
foil), the target determiner (D foil), and the target
noun (N foil). A few examples are shown in Table
1. After the materials were checked by all authors,
two native speakers took the multiple-choice test.
We removed the cases (32) where these subjects
provided an unexpected answer. The final set,
4Other approaches to composition in DSMs have been re-
cently proposed by Socher et al (2012) and Turney (2012).
We leave their empirical evaluation on DPs to further work,
in the first case because it is not trivial to adapt their complex
architecture to our setting; in the other because it is not clear
how Turney would extend his approach to represent DPs.
54
noun target DP same-N foil 1 same-N foil 2 same-D foil D foil N foil
duel two opponents various opponents three opponents two engineers two opponents
homeless no home too few homes one home no incision no home
polygamy several wives most wives fewer wives several negotiators several wives
opulence too many goods some goods no goods too many abductions too many goods
Table 1: Examples from the noun-DP relatedness benchmark
characterized by full subject agreement, contains
173 nouns, each matched with 6 possible answers.
The target DPs contain 23 distinct determiners.
4 Setup
Our semantic space provides distributional repre-
sentations of determiners, nouns and DPs. We
considered a set of 50 determiners that include all
those in our benchmark and range from quanti-
fying determiners (every, some. . . ) and low nu-
merals (one to four), to multi-word units analyzed
as single determiners in the literature, such as a
few, all that, too much. We picked the 20K most
frequent nouns in our source corpus considering
singular and plural forms as separate words, since
number clearly plays an important role in DP se-
mantics. Finally, for each of the target determiners
we added to the space the 2K most frequent DPs
containing that determiner and a target noun.
Co-occurrence statistics were collected from the
concatenation of ukWaC, a mid-2009 dump of the
English Wikipedia and the British National Cor-
pus,5 with a total of 2.8 billion tokens. We use
a bag-of-words approach, counting co-occurrence
with all context words in the same sentence with
a target item. We tuned a number of parameters
on the independent MEN word-relatedness bench-
mark (Bruni et al, 2012). This led us to pick the
top 20K most frequent content word lemmas as
context items, Pointwise Mutual Information as
weighting scheme, and dimensionality reduction
by Non-negative Matrix Factorization.
Except for the parameter-free mult method, pa-
rameters of the composition methods are esti-
mated by minimizing the average Euclidean dis-
tance between the model-generated and corpus-
extracted vectors of the 20K DPs we consider.6
For the lexfunc model, we assume that the deter-
miner is the functor and the noun is the argument,
5wacky.sslmit.unibo.it; www.natcorp.ox.
ac.uk
6All vectors are normalized to unit length before compo-
sition. Note that the objective function used in estimation
minimizes the distance between model-generated and corpus-
extracted vectors. We do not use labeled evaluation data to
optimize the model parameters.
method accuracy method accuracy
lexfunc 39.3 noun 17.3
fulladd 34.7 random 16.7
observed 34.1 mult 12.7
dilation 31.8 determiner 4.6
wadd 23.1
Table 2: Percentage accuracy of composition
methods on the relatedness benchmark
and estimate separate matrices representing each
determiner using the 2K DPs in the semantic space
that contain that determiner. For dilation, we treat
direction of stretching as a parameter, finding that
it is better to stretch the noun.
Similarly to the classic TOEFL synonym detec-
tion challenge (Landauer and Dumais, 1997), our
models tackle the relatedness task by measuring
cosines between each target noun and the candi-
date answers and returning the item with the high-
est cosine.
5 Results
Table 2 reports the accuracy results (mean ranks
of correct answers confirm the same trend). All
models except mult and determiner outperform the
trivial random guessing baseline, although they
are all well below the 100% accuracy of the hu-
mans who took our test. For the mult method we
observe a very strong bias for choosing a single
word as answer (>60% of the times), which in
the test set is always incorrect. This leads to its
accuracy being below the chance level. We sus-
pect that the highly ?intersective? nature of this
model (we obtain very sparse composed DP vec-
tors, only ?4% dense) leads to it not being a re-
liable method for comparing sequences of words
of different length: Shorter sequences will be con-
sidered more similar due to their higher density.
The determiner-only baseline (using the vector of
the component determiner as surrogate for the DP)
fails because D vectors tend to be far from N vec-
tors, thus the N foil is often preferred to the correct
response (that is represented, for this baseline, by
its D). In the noun-only baseline (use the vector
of the component noun as surrogate for the DP),
55
the correct response is identical to the same-N and
N foils, thus forcing a random choice between
these. Not surprisingly, this approach performs
quite badly. The observed DP vectors extracted di-
rectly from the corpus compete with the top com-
positional methods, but do not surpass them.7
The lexfunc method is the best compositional
model, indicating that its added flexibility in mod-
eling composition pays off empirically. The ful-
ladd model is not as good, but also performs well.
The wadd and especially dilation models perform
relatively well, but they are penalized by the fact
that they assign more weight to the noun vectors,
making the right answer dangerously similar to the
same-N and N foils.
Taking a closer look at the performance of the
best model (lexfunc), we observe that it is not
equally distributed across determiners. Focusing
on those determiners appearing in at least 4 cor-
rect answers, they range from those where lexfunc
performance was very significantly above chance
(p<0.001 of equal or higher chance performance):
too few, all, four, too much, less, several; to
those on which performance was still significant
but less impressively so (0.001<p< 0.05): sev-
eral, no, various, most, two, too many, many, one;
to those where performance was not significantly
better than chance at the 0.05 level: much, more,
three, another. Given that, on the one hand, per-
formance is not constant across determiners, and
on the other no obvious groupings can account
for their performance difference (compare the ex-
cellent lexfunc performance on four to the lousy
one on three!), future research should explore the
contextual properties of specific determiners that
make them more or less amenable to be captured
by compositional DSMs.
6 Conclusion
DSMs, even when applied to phrases, are typically
seen as models of content word meaning. How-
ever, to scale up compositionally beyond the sim-
plest constructions, cDSMs must deal with gram-
matical terms such as determiners. This paper
started exploring this issue by introducing a new
and publicly available set testing DP semantics in
a similarity-based task and using it to systemati-
cally evaluate, for the first time, cDSMs on a con-
7The observed method is in fact at advantage in our ex-
periment because a considerable number of DP foils are not
found in the corpus and are assigned similarity 0 with the tar-
get.
struction involving grammatical words. The most
important take-home message is that distributional
representations are rich enough to encode infor-
mation about determiners, achieving performance
well above chance on the new benchmark.
Theoretical considerations would lead one to
expect a ?functional? approach to determiner rep-
resentations along the lines of Baroni and Zampar-
elli (2010) and Coecke et al (2010) to outperform
those approaches that combine vectors separately
representing determiners and nouns. This predic-
tion was largely borne out in the results, although
the additive models, and particularly fulladd, were
competitive rivals.
We attempted to capture the distributional se-
mantics of DPs using a fairly standard, ?vanilla?
semantic space characterized by latent dimensions
that summarize patterns of co-occurrence with
content word contexts. By inspecting the con-
text words that are most associated with the var-
ious latent dimensions we obtained through Non-
negative Matrix Factorization, we notice how they
are capturing broad, ?topical? aspects of meaning
(the first dimension is represented by scripture, be-
liever, resurrection, the fourth by fever, infection,
infected, and so on). Considering the sort of se-
mantic space we used (which we took to be a rea-
sonable starting point because of its effectiveness
in a standard lexical task), it is actually surpris-
ing that we obtained the significant results we ob-
tained. Thus, a top priority in future work is to ex-
plore different contextual features, such as adverbs
and grammatical terms, that might carry informa-
tion that is more directly relevant to the semantics
of determiners.
Another important line of research pertains to
improving composition methods: Although the
best model, at 40% accuracy, is well above chance,
we are still far from the 100% performance of hu-
mans. We will try, in particular, to include non-
linear transformations in the spirit of Socher et al
(2012), and look for better ways to automatically
select training data.
Last but not least, in the near future we
would like to test if cDSMs, besides dealing with
similarity-based aspects of determiner meaning,
can also help in capturing those formal properties
of determiners, such as monotonicity or definite-
ness, that theoretical semanticists have been tradi-
tionally interested in.
56
7 Acknowledgments
This research was supported by the ERC 2011
Starting Independent Research Grant n. 283554
(COMPOSES).
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung-Chieh Shan. 2012. Entailment above
the word level in distributional semantics. In Pro-
ceedings of EACL, pages 23?32, Avignon, France.
Elia Bruni, Gemma Boleda, Marco Baroni, and
Nam Khanh Tran. 2012. Distributional semantics
in Technicolor. In Proceedings of ACL, pages 136?
145, Jeju Island, Korea.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345?384.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Dan Garrette, Katrin Erk, and Ray Mooney. 2013. A
formal approach to linking logical form and vector-
space lexical semantics. In H. Bunt, J. Bos, and
S. Pulman, editors, Computing Meaning, Vol. 4. In
press.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of EMNLP, pages 1394?1404, Edinburgh, UK.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33?37,
Uppsala, Sweden.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104(2):211?
240.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2009. Language
models based on semantic composition. In Proceed-
ings of EMNLP, pages 430?439, Singapore.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
Peter Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533?
585.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263?
1271, Beijing, China.
57
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 1?8,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 1: Evaluation of Compositional Distributional
Semantic Models on Full Sentences through Semantic Relatedness and
Textual Entailment
Marco Marelli
(1)
Luisa Bentivogli
(2)
Marco Baroni
(1)
Raffaella Bernardi
(1)
Stefano Menini
(1,2)
Roberto Zamparelli
(1)
(1)
University of Trento, Italy
(2)
FBK - Fondazione Bruno Kessler, Trento, Italy
{name.surname}@unitn.it, {bentivo,menini}@fbk.eu
Abstract
This paper presents the task on the evalu-
ation of Compositional Distributional Se-
mantics Models on full sentences orga-
nized for the first time within SemEval-
2014. Participation was open to systems
based on any approach. Systems were pre-
sented with pairs of sentences and were
evaluated on their ability to predict hu-
man judgments on (i) semantic relatedness
and (ii) entailment. The task attracted 21
teams, most of which participated in both
subtasks. We received 17 submissions in
the relatedness subtask (for a total of 66
runs) and 18 in the entailment subtask (65
runs).
1 Introduction
Distributional Semantic Models (DSMs) approx-
imate the meaning of words with vectors sum-
marizing their patterns of co-occurrence in cor-
pora. Recently, several compositional extensions
of DSMs (CDSMs) have been proposed, with the
purpose of representing the meaning of phrases
and sentences by composing the distributional rep-
resentations of the words they contain (Baroni and
Zamparelli, 2010; Grefenstette and Sadrzadeh,
2011; Mitchell and Lapata, 2010; Socher et al.,
2012). Despite the ever increasing interest in the
field, the development of adequate benchmarks for
CDSMs, especially at the sentence level, is still
lagging. Existing data sets, such as those intro-
duced by Mitchell and Lapata (2008) and Grefen-
stette and Sadrzadeh (2011), are limited to a few
hundred instances of very short sentences with a
fixed structure. In the last ten years, several large
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
data sets have been developed for various com-
putational semantics tasks, such as Semantic Text
Similarity (STS)(Agirre et al., 2012) or Recogniz-
ing Textual Entailment (RTE) (Dagan et al., 2006).
Working with such data sets, however, requires
dealing with issues, such as identifying multiword
expressions, recognizing named entities or access-
ing encyclopedic knowledge, which have little to
do with compositionality per se. CDSMs should
instead be evaluated on data that are challenging
for reasons due to semantic compositionality (e.g.
context-cued synonymy resolution and other lexi-
cal variation phenomena, active/passive and other
syntactic alternations, impact of negation at vari-
ous levels, operator scope, and other effects linked
to the functional lexicon). These issues do not oc-
cur frequently in, e.g., the STS and RTE data sets.
With these considerations in mind, we devel-
oped SICK (Sentences Involving Compositional
Knowledge), a data set aimed at filling the void,
including a large number of sentence pairs that
are rich in the lexical, syntactic and semantic phe-
nomena that CDSMs are expected to account for,
but do not require dealing with other aspects of
existing sentential data sets that are not within
the scope of compositional distributional seman-
tics. Moreover, we distinguished between generic
semantic knowledge about general concept cate-
gories (such as knowledge that a couple is formed
by a bride and a groom) and encyclopedic knowl-
edge about specific instances of concepts (e.g.,
knowing the fact that the current president of the
US is Barack Obama). The SICK data set contains
many examples of the former, but none of the lat-
ter.
2 The Task
The Task involved two subtasks. (i) Relatedness:
predicting the degree of semantic similarity be-
tween two sentences, and (ii) Entailment: detect-
ing the entailment relation holding between them
1
(see below for the exact definition). Sentence re-
latedness scores provide a direct way to evalu-
ate CDSMs, insofar as their outputs are able to
quantify the degree of semantic similarity between
sentences. On the other hand, starting from the
assumption that understanding a sentence means
knowing when it is true, being able to verify
whether an entailment is valid is a crucial chal-
lenge for semantic systems.
In the semantic relatedness subtask, given two
sentences, systems were required to produce a re-
latedness score (on a continuous scale) indicating
the extent to which the sentences were expressing
a related meaning. Table 1 shows examples of sen-
tence pairs with different degrees of semantic re-
latedness; gold relatedness scores are expressed on
a 5-point rating scale.
In the entailment subtask, given two sentences
A and B, systems had to determine whether the
meaning of B was entailed by A. In particular, sys-
tems were required to assign to each pair either
the ENTAILMENT label (when A entails B, viz.,
B cannot be false when A is true), the CONTRA-
DICTION label (when A contradicted B, viz. B is
false whenever A is true), or the NEUTRAL label
(when the truth of B could not be determined on
the basis of A). Table 2 shows examples of sen-
tence pairs holding different entailment relations.
Participants were invited to submit up to five
system runs for one or both subtasks. Developers
of CDSMs were especially encouraged to partic-
ipate, but developers of other systems that could
tackle sentence relatedness or entailment tasks
were also welcome. Besides being of intrinsic in-
terest, the latter systems? performance will serve
to situate CDSM performance within the broader
landscape of computational semantics.
3 The SICK Data Set
The SICK data set, consisting of about 10,000 En-
glish sentence pairs annotated for relatedness in
meaning and entailment, was used to evaluate the
systems participating in the task. The data set
creation methodology is outlined in the following
subsections, while all the details about data gen-
eration and annotation, quality control, and inter-
annotator agreement can be found in Marelli et al.
(2014).
3.1 Data Set Creation
SICK was built starting from two existing data
sets: the 8K ImageFlickr data set
1
and the
SemEval-2012 STS MSR-Video Descriptions data
set.
2
The 8K ImageFlickr dataset is a dataset of
images, where each image is associated with five
descriptions. To derive SICK sentence pairs we
randomly chose 750 images and we sampled two
descriptions from each of them. The SemEval-
2012 STS MSR-Video Descriptions data set is a
collection of sentence pairs sampled from the short
video snippets which compose the Microsoft Re-
search Video Description Corpus. A subset of 750
sentence pairs were randomly chosen from this
data set to be used in SICK.
In order to generate SICK data from the 1,500
sentence pairs taken from the source data sets, a 3-
step process was applied to each sentence compos-
ing the pair, namely (i) normalization, (ii) expan-
sion and (iii) pairing. Table 3 presents an example
of the output of each step in the process.
The normalization step was carried out on the
original sentences (S0) to exclude or simplify in-
stances that contained lexical, syntactic or seman-
tic phenomena (e.g., named entities, dates, num-
bers, multiword expressions) that CDSMs are cur-
rently not expected to account for.
The expansion step was applied to each of the
normalized sentences (S1) in order to create up to
three new sentences with specific characteristics
suitable to CDSM evaluation. In this step syntac-
tic and lexical transformations with predictable ef-
fects were applied to each normalized sentence, in
order to obtain (i) a sentence with a similar mean-
ing (S2), (ii) a sentence with a logically contradic-
tory or at least highly contrasting meaning (S3),
and (iii) a sentence that contains most of the same
lexical items, but has a different meaning (S4) (this
last step was carried out only where it could yield
a meaningful sentence; as a result, not all normal-
ized sentences have an (S4) expansion).
Finally, in the pairing step each normalized
sentence in the pair was combined with all the
sentences resulting from the expansion phase and
with the other normalized sentence in the pair.
Considering the example in Table 3, S1a and S1b
were paired. Then, S1a and S1b were each com-
bined with S2a, S2b,S3a, S3b, S4a, and S4b, lead-
1
http://nlp.cs.illinois.edu/HockenmaierGroup/data.html
2
http://www.cs.york.ac.uk/semeval-
2012/task6/index.php?id=data
2
Relatedness score Example
1.6
A: ?A man is jumping into an empty pool?
B: ?There is no biker jumping in the air?
2.9
A: ?Two children are lying in the snow and are making snow angels?
B: ?Two angels are making snow on the lying children?
3.6
A: ?The young boys are playing outdoors and the man is smiling nearby?
B: ?There is no boy playing outdoors and there is no man smiling?
4.9
A: ?A person in a black jacket is doing tricks on a motorbike?
B: ?A man in a black jacket is doing tricks on a motorbike?
Table 1: Examples of sentence pairs with their gold relatedness scores (on a 5-point rating scale).
Entailment label Example
ENTAILMENT
A: ?Two teams are competing in a football match?
B: ?Two groups of people are playing football?
CONTRADICTION
A: ?The brown horse is near a red barrel at the rodeo?
B: ?The brown horse is far from a red barrel at the rodeo?
NEUTRAL
A: ?A man in a black jacket is doing tricks on a motorbike?
B: ?A person is riding the bicycle on one wheel?
Table 2: Examples of sentence pairs with their gold entailment labels.
ing to a total of 13 different sentence pairs.
Furthermore, a number of pairs composed of
completely unrelated sentences were added to the
data set by randomly taking two sentences from
two different pairs.
The result is a set of about 10,000 new sen-
tence pairs, in which each sentence is contrasted
with either a (near) paraphrase, a contradictory or
strongly contrasting statement, another sentence
with very high lexical overlap but different mean-
ing, or a completely unrelated sentence. The ra-
tionale behind this approach was that of building
a data set which encouraged the use of a com-
positional semantics step in understanding when
two sentences have close meanings or entail each
other, hindering methods based on individual lex-
ical items, on the syntactic complexity of the two
sentences or on pure world knowledge.
3.2 Relatedness and Entailment Annotation
Each pair in the SICK dataset was annotated to
mark (i) the degree to which the two sentence
meanings are related (on a 5-point scale), and (ii)
whether one entails or contradicts the other (con-
sidering both directions). The ratings were col-
lected through a large crowdsourcing study, where
each pair was evaluated by 10 different subjects,
and the order of presentation of the sentences was
counterbalanced (i.e., 5 judgments were collected
for each presentation order). Swapping the order
of the sentences within each pair served a two-
fold purpose: (i) evaluating the entailment rela-
tion in both directions and (ii) controlling pos-
sible bias due to priming effects in the related-
ness task. Once all the annotations were collected,
the relatedness gold score was computed for each
pair as the average of the ten ratings assigned by
participants, whereas a majority vote scheme was
adopted for the entailment gold labels.
3.3 Data Set Statistics
For the purpose of the task, the data set was ran-
domly split into training and test set (50% and
50%), ensuring that each relatedness range and en-
tailment category was equally represented in both
sets. Table 4 shows the distribution of sentence
pairs considering the combination of relatedness
ranges and entailment labels. The ?total? column
3
Original pair
S0a: A sea turtle is hunting for fish S0b: The turtle followed the fish
Normalized pair
S1a: A sea turtle is hunting for fish S1b: The turtle is following the fish
Expanded pairs
S2a: A sea turtle is hunting for food S2b: The turtle is following the red fish
S3a: A sea turtle is not hunting for fish S3b: The turtle isn?t following the fish
S4a: A fish is hunting for a turtle in the sea S4b: The fish is following the turtle
Table 3: Data set creation process.
indicates the total number of pairs in each range
of relatedness, while the ?total? row contains the
total number of pairs in each entailment class.
SICK Training Set
relatedness CONTRADICT ENTAIL NEUTRAL TOTAL
1-2 range 0 (0%) 0 (0%) 471 (10%) 471
2-3 range 59 (1%) 2 (0%) 638 (13%) 699
3-4 range 498 (10%) 71 (1%) 1344 (27%) 1913
4-5 range 155 (3%) 1344 (27%) 352 (7%) 1851
TOTAL 712 1417 2805 4934
SICK Test Set
relatedness CONTRADICT ENTAIL NEUTRAL TOTAL
1-2 range 0 (0%) 1 (0%) 451 (9%) 452
2-3 range 59 (1%) 0 (0%) 615(13%) 674
3-4 range 496 (10%) 65 (1%) 1398 (28%) 1959
4-5 range 157 (3%) 1338 (27%) 326 (7%) 1821
TOTAL 712 1404 2790 4906
Table 4: Distribution of sentence pairs across the
Training and Test Sets.
4 Evaluation Metrics and Baselines
Both subtasks were evaluated using standard met-
rics. In particular, the results on entailment were
evaluated using accuracy, whereas the outputs on
relatedness were evaluated using Pearson correla-
tion, Spearman correlation, and Mean Squared Er-
ror (MSE). Pearson correlation was chosen as the
official measure to rank the participating systems.
Table 5 presents the performance of 4 base-
lines. The Majority baseline always assigns
the most common label in the training data
(NEUTRAL), whereas the Probability baseline
assigns labels randomly according to their rela-
tive frequency in the training set. The Overlap
baseline measures word overlap, again with
parameters (number of stop words and EN-
TAILMENT/NEUTRAL/CONTRADICTION
thresholds) estimated on the training part of the
data.
Baseline Relatedness Entailment
Chance 0 33.3%
Majority NA 56.7%
Probability NA 41.8%
Overlap 0.63 56.2%
Table 5: Performance of baselines. Figure of merit
is Pearson correlation for relatedness and accuracy
for entailment. NA = Not Applicable
5 Submitted Runs and Results
Overall, 21 teams participated in the task. Partici-
pants were allowed to submit up to 5 runs for each
subtask and had to choose the primary run to be in-
cluded in the comparative evaluation. We received
17 submissions to the relatedness subtask (for a
total of 66 runs) and 18 for the entailment subtask
(65 runs).
We asked participants to pre-specify a pri-
mary run to encourage commitment to a
theoretically-motivated approach, rather than
post-hoc performance-based assessment. Inter-
estingly, some participants used the non-primary
runs to explore the performance one could reach
by exploiting weaknesses in the data that are not
likely to hold in future tasks of the same kind
(for instance, run 3 submitted by The Meaning
Factory exploited sentence ID ordering informa-
tion, but it was not presented as a primary run).
Participants could also use non-primary runs to
test smart baselines. In the relatedness subtask
six non-primary runs slightly outperformed the
official winning primary entry,
3
while in the
entailment task all ECNU?s runs but run 4 were
better than ECNU?s primary run. Interestingly,
the differences between the ECNU?s runs were
3
They were: The Meaning Factory?s run3 (Pearson
0.84170) ECNU?s runs2 (0.83893) run5 (0.83500) and Stan-
fordNLP?s run4 (0.83462) and run2 (0.83103).
4
due to the learning methods used.
We present the results achieved by primary runs
against the Entailment and Relatedness subtasks in
Table 6 and Table 7, respectively.
4
We witnessed
a very close finish in both subtasks, with 4 more
systems within 3 percentage points of the winner
in both cases. 4 of these 5 top systems were the
same across the two subtasks. Most systems per-
formed well above the best baselines from Table
5.
The overall performance pattern suggests that,
owing perhaps to the more controlled nature of
the sentences, as well as to the purely linguistic
nature of the challenges it presents, SICK entail-
ment is ?easier? than RTE. Considering the first
five RTE challenges (Bentivogli et al., 2009), the
median values ranged from 56.20% to 61.75%,
whereas the average values ranged from 56.45%
to 61.97%. The entailment scores obtained on
the SICK data set are considerably higher, being
77.06% for the median system and 75.36% for
the average system. On the other hand, the re-
latedness task is more challenging than the one
run on MSRvid (one of our data sources) at STS
2012, where the top Pearson correlation was 0.88
(Agirre et al., 2012).
6 Approaches
A summary of the approaches used by the sys-
tems to address the task is presented in Table 8.
In the table, systems in bold are those for which
the authors submitted a paper (Ferrone and Zan-
zotto, 2014; Bjerva et al., 2014; Beltagy et al.,
2014; Lai and Hockenmaier, 2014; Alves et al.,
2014; Le?on et al., 2014; Bestgen, 2014; Zhao et
al., 2014; Vo et al., 2014; Bic?ici and Way, 2014;
Lien and Kouylekov, 2014; Jimenez et al., 2014;
Proisl and Evert, 2014; Gupta et al., 2014). For the
others, we used the brief description sent with the
system?s results, double-checking the information
with the authors. In the table, ?E? and ?R? refer
to the entailment and relatedness task respectively,
and ?B? to both.
Almost all systems combine several kinds of
features. To highlight the role played by com-
position, we draw a distinction between compo-
sitional and non-compositional features, and di-
vide the former into ?fully compositional? (sys-
4
ITTK?s primary run could not be evaluated due to tech-
nical problems with the submission. The best ITTK?s non-
primary run scored 78,2% accuracy in the entailment task and
0.76 r in the relatedness task.
ID Compose ACCURACY
Illinois-LH run1 P/S 84.6
ECNU run1 S 83.6
UNAL-NLP run1 83.1
SemantiKLUE run1 82.3
The Meaning Factory run1 S 81.6
CECL ALL run1 80.0
BUAP run1 P 79.7
UoW run1 78.5
Uedinburgh run1 S 77.1
UIO-Lien run1 77.0
FBK-TR run3 P 75.4
StanfordNLP run5 S 74.5
UTexas run1 P/S 73.2
Yamraj run1 70.7
asjai run5 S 69.8
haLF run2 S 69.4
RTM-DCU run1 67.2
UANLPCourse run2 S 48.7
Table 6: Primary run results for the entailment
subtask. The table also shows whether a sys-
tem exploits composition information at either the
phrase (P) or sentence (S) level.
tems that compositionally computed the meaning
of the full sentences, though not necessarily by as-
signing meanings to intermediate syntactic con-
stituents) and ?partially compositional? (systems
that stop the composition at the level of phrases).
As the table shows, thirteen systems used compo-
sition in at least one of the tasks; ten used compo-
sition for full sentences and six for phrases, only.
The best systems are among these thirteen sys-
tems.
Let us focus on such compositional methods.
Concerning the relatedness task, the fine-grained
analyses reported for several systems (Illinois-
LH, The Meaning Factory and ECNU) shows that
purely compositional systems currently reach per-
formance above 0.7 r. In particular, ECNU?s
compositional feature gives 0.75 r, The Meaning
Factory?s logic-based composition model 0.73 r,
and Illinois-LH compositional features combined
with Word Overlap 0.75 r. While competitive,
these scores are lower than the one of the best
5
ID Compose r ? MSE
ECNU run1 S 0.828 0.769 0.325
StanfordNLP run5 S 0.827 0.756 0.323
The Meaning Factory run1 S 0.827 0.772 0.322
UNAL-NLP run1 0.804 0.746 0.359
Illinois-LH run1 P/S 0.799 0.754 0.369
CECL ALL run1 0.780 0.732 0.398
SemantiKLUE run1 0.780 0.736 0.403
RTM-DCU run1 0.764 0.688 0.429
UTexas run1 P/S 0.714 0.674 0.499
UoW run1 0.711 0.679 0.511
FBK-TR run3 P 0.709 0.644 0.591
BUAP run1 P 0.697 0.645 0.528
UANLPCourse run2 S 0.693 0.603 0.542
UQeResearch run1 0.642 0.626 0.822
ASAP run1 P 0.628 0.597 0.662
Yamraj run1 0.535 0.536 2.665
asjai run5 S 0.479 0.461 1.104
Table 7: Primary run results for the relatedness
subtask (r for Pearson and ? for Spearman corre-
lation). The table also shows whether a system ex-
ploits composition information at either the phrase
(P) or sentence (S) level.
purely non-compositional system (UNAL-NLP)
which reaches the 4th position (0.80 r UNAL-NLP
vs. 0.82 r obtained by the best system). UNAL-
NLP however exploits an ad-hoc ?negation? fea-
ture discussed below.
In the entailment task, the best non-
compositional model (again UNAL-NLP)
reaches the 3rd position, within close reach of the
best system (83% UNAL-NLP vs. 84.5% obtained
by the best system). Again, purely compositional
models have lower performance. haLF CDSM
reaches 69.42% accuracy, Illinois-LH Word
Overlap combined with a compositional feature
reaches 71.8%. The fine-grained analysis reported
by Illinois-LH (Lai and Hockenmaier, 2014)
shows that a full compositional system (based
on point-wise multiplication) fails to capture
contradiction. It is better than partial phrase-based
compositional models in recognizing entailment
pairs, but worse than them on recognizing neutral
pairs.
Given our more general interest in the distri-
butional approaches, in Table 8 we also classify
the different DSMs used as ?Vector Space Mod-
els?, ?Topic Models? and ?Neural Language Mod-
els?. Due to the impact shown by learning methods
(see ECNU?s results), we also report the different
learning approaches used.
Several participating systems deliberately ex-
ploit ad-hoc features that, while not helping a true
understanding of sentence meaning, exploit some
systematic characteristics of SICK that should be
controlled for in future releases of the data set.
In particular, the Textual Entailment subtask has
been shown to rely too much on negative words
and antonyms. The Illinois-LH team reports that,
just by checking the presence of negative words
(the Negation Feature in the table), one can detect
86.4% of the contradiction pairs, and by combin-
ing Word Overlap and antonyms one can detect
83.6% of neutral pairs and 82.6% of entailment
pairs. This approach, however, is obviously very
brittle (it would not have been successful, for in-
stance, if negation had been optionally combined
with word-rearranging in the creation of S4 sen-
tences, see Section 3.1 above).
Finally, Table 8 reports about the use of external
resources in the task. One of the reasons we cre-
ated SICK was to have a compositional semantics
benchmark that would not require too many ex-
ternal tools and resources (e.g., named-entity rec-
ognizers, gazetteers, ontologies). By looking at
what the participants chose to use, we think we
succeeded, as only standard NLP pre-processing
tools (tokenizers, PoS taggers and parsers) and rel-
atively few knowledge resources (mostly, Word-
Net and paraphrase corpora) were used.
7 Conclusion
We presented the results of the first task on the
evaluation of compositional distributional seman-
tic models and other semantic systems on full sen-
tences, organized within SemEval-2014. Two sub-
tasks were offered: (i) predicting the degree of re-
latedness between two sentences, and (ii) detect-
ing the entailment relation holding between them.
The task has raised noticeable attention in the
community: 17 and 18 submissions for the relat-
edness and entailment subtasks, respectively, for a
total of 21 participating teams. Participation was
not limited to compositional models but the major-
ity of systems (13/21) used composition in at least
one of the subtasks. Moreover, the top-ranking
systems in both tasks use compositional features.
However, it must be noted that all systems also ex-
6
Participant ID Non composition features Comp features Learning Methods External Resources 
Ve
cto
r S
em
an
tic
s M
od
el 
To
pic
 M
od
el 
Ne
ura
l L
an
gu
ag
e M
od
el 
De
no
tat
ion
al 
Mo
de
l 
Wo
rd 
Ov
erl
ap
 
Wo
rd 
Sim
ila
rit
y 
Sy
nta
cti
c F
ea
tur
es
 
Se
nte
nc
e d
iffe
ren
ce
 
Ne
ga
tio
n F
ea
tur
es
 
Se
nte
nc
e C
om
po
sit
ion
 
Ph
ras
e c
om
po
sit
ion
  
SV
M 
an
d K
ern
el 
me
tho
ds
 
K-
Ne
are
st 
Ne
igh
bo
urs
 
Cla
ssi
fie
r C
om
bin
ati
on
 
Ra
nd
om
 Fo
res
t 
Fo
L/P
rob
ab
ilis
tic
 Fo
L 
Cu
rri
cu
lum
 ba
se
d l
ea
rni
ng
 
Ot
he
r 
Wo
rdN
et 
Pa
rap
hra
se
s D
B 
Ot
he
r C
orp
ora
 
Im
ag
eF
lick
er 
 ST
S M
SR
-V
ide
o 
De
scr
ipt
ion
 
ASAP R R R R R R R R R 
ASJAI B B B B B B B B E B R B 
BUAP B B B B E B E B 
UEdinburgh B B B B B E R B 
CECL B B B B B B 
ECNU B B B B B B B B B B B B B 
FBK-TR R R R E B E E B R E R R E 
haLF E E E E 
IITK B B B B B B B B B 
Illinois-LH B B B B B B B B B B B B 
RTM-DCU B B B B B 
SemantiKLUE B B B B B B B B 
StandfordNLP B B R R R B E 
The Meaning Factory R R R R R R B E R E B B R 
UANLPCourse B B B B B 
UIO-Lien E E 
UNAL-NLP B B B B R B B 
UoW B B B B B B 
UQeRsearch R R R R R R R 
UTexas B B B B B B B 
Yamarj B B B B 
Table 8: Summary of the main characteristics of the participating systems on R(elatedness), E(ntailment)
or B(oth)
ploit non-compositional features and most of them
use external resources, especially WordNet. Al-
most all the participating systems outperformed
the proposed baselines in both tasks. Further anal-
yses carried out by some participants in the task
show that purely compositional approaches reach
accuracy above 70% in entailment and 0.70 r for
relatedness. These scores are comparable with the
average results obtained in the task.
Acknowledgments
We thank the creators of the ImageFlickr, MSR-
Video, and SemEval-2012 STS data sets for grant-
ing us permission to use their data for the task. The
University of Trento authors were supported by
ERC 2011 Starting Independent Research Grant
n. 283554 (COMPOSES).
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-
lot on semantic textual similarity. In Proceedings of
the Sixth International Workshop on Semantic Eval-
uation (SemEval 2012), volume 2.
Ana O. Alves, Adirana Ferrugento, Mariana Lorenc?o,
and Filipe Rodrigues. 2014. ASAP: Automatica se-
mantic alignment for phrases. In Proceedings of Se-
mEval 2014: International Workshop on Semantic
Evaluation.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Islam Beltagy, Stephen Roller, Gemma Boleda, Katrin
Erk, and Raymon J. Mooney. 2014. UTexas: Nat-
ural language semantics using distributional seman-
tics and probablisitc logic. In Proceedings of Se-
mEval 2014: International Workshop on Semantic
Evaluation.
7
Luisa Bentivogli, Ido Dagan, Hoa T. Dang, Danilo Gi-
ampiccolo, and Bernardo Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge.
In The Text Analysis Conference (TAC 2009).
Yves Bestgen. 2014. CECL: a new baseline and a non-
compositional approach for the Sick benchmark. In
Proceedings of SemEval 2014: International Work-
shop on Semantic Evaluation.
Ergun Bic?ici and Andy Way. 2014. RTM-DCU: Ref-
erential translation machines for semantic similar-
ity. In Proceedings of SemEval 2014: International
Workshop on Semantic Evaluation.
Johannes Bjerva, Johan Bos, Rob van der Goot, and
Malvina Nissim. 2014. The Meaning Factory: For-
mal Semantics for Recognizing Textual Entailment
and Determining Semantic Similarity. In Proceed-
ings of SemEval 2014: International Workshop on
Semantic Evaluation.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Machine learning challenges. Evalu-
ating predictive uncertainty, visual object classifica-
tion, and recognising textual entailment, pages 177?
190. Springer.
Lorenzo Ferrone and Fabio Massimo Zanzotto. 2014.
haLF:comparing a pure CDSM approach and a stan-
dard ML system for RTE. In Proceedings of Se-
mEval 2014: International Workshop on Semantic
Evaluation.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of EMNLP, pages 1394?1404, Edinburgh, UK.
Rohit Gupta, Ismail El Maarouf Hannah Bechara, and
Costantin Oras?an. 2014. UoW: NLP techniques de-
veloped at the University of Wolverhampton for Se-
mantic Similarity and Textual Entailment. In Pro-
ceedings of SemEval 2014: International Workshop
on Semantic Evaluation.
Sergio Jimenez, George Duenas, Julia Baquero, and
Alexander Gelbukh. 2014. UNAL-NLP: Combin-
ing soft cardinality features for semantic textual sim-
ilarity, relatedness and entailment. In Proceedings
of SemEval 2014: International Workshop on Se-
mantic Evaluation.
Alice Lai and Julia Hockenmaier. 2014. Illinois-lh: A
denotational and distributional approach to seman-
tics. In Proceedings of SemEval 2014: International
Workshop on Semantic Evaluation.
Sa?ul Le?on, Darnes Vilarino, David Pinto, Mireya To-
var, and Beatrice Beltr?an. 2014. BUAP:evaluating
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings of SemEval 2014: Inter-
national Workshop on Semantic Evaluation.
Elisabeth Lien and Milen Kouylekov. 2014. UIO-
Lien: Entailment recognition using minimal recur-
sion semantics. In Proceedings of SemEval 2014:
International Workshop on Semantic Evaluation.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. A SICK cure for the evaluation of
compositional distributional semantic models. In
Proceedings of LREC, Reykjavik.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Thomas Proisl and Stefan Evert. 2014. SemantiK-
LUE: Robust semantic similarity at multiple levels
using maximum weight matching. In Proceedings of
SemEval 2014: International Workshop on Semantic
Evaluation.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
An N. P. Vo, Octavian Popescu, and Tommaso Caselli.
2014. FBK-TR: SVM for Semantic Relatedness and
Corpus Patterns for RTE. In Proceedings of Se-
mEval 2014: International Workshop on Semantic
Evaluation.
Jiang Zhao, Tian Tian Zhu, and Man Lan. 2014.
ECNU: One Stone Two Birds: Ensemble of Het-
erogenous Measures for Semantic Relatedness and
Textual Entailment. In Proceedings of SemEval
2014: International Workshop on Semantic Evalu-
ation.
8
Coling 2008: Proceedings of the workshop on Knowledge and Reasoning for Answering Questions, pages 25?32
Manchester, August 2008
Context Modeling for IQA: The Role of Tasks and Entities
Raffaella Bernardi and Manuel Kirschner
KRDB, Faculty of Computer Science
Free University of Bozen-Bolzano, Italy
{bernardi, kirschner}@inf.unibz.it
Abstract
In a realistic Interactive Question Answer-
ing (IQA) setting, users frequently ask
follow-up questions. By modeling how the
questions? focus evolves in IQA dialogues,
we want to describe what makes a partic-
ular follow-up question salient. We intro-
duce a new focus model, and describe an
implementation of an IQA system that we
use for exploring our theory. To learn prop-
erties of salient focus transitions from data,
we use logistic regression models that we
validate on the basis of predicted answer
correctness.
1 Questions within a Context
Question Answering (QA) systems have reached a
high level of performance within the scenario orig-
inally described in the TREC competitions, and
are ready to tackle new challenges as shown by
the new tracks proposed in recent instantiations
(Voorhees, 2004). To answer these challenges, at-
tention is moving towards adding semantic infor-
mation at different levels. Our work is about con-
text modeling for Interactive Question Answering
(IQA) systems. Our research hypothesis is that a)
knowledge about the dialogue history, and b) lexi-
cal knowledge about semantic arguments improve
an IQA system?s ability to answer follow-up ques-
tions. In this paper we use logistic regression mod-
eling to verify our claims and evaluate how the per-
formance of our Q?A mapping algorithm varies
based on whether such knowledge is taken into ac-
count.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Actual IQA dialogues often exhibit ?context-
dependent? follow-up questions (FU Qs) contain-
ing anaphoric devices, like Q2 below. Such ques-
tions are potentially difficult to process by means
of standard QA techniques, and it is for these cases
that we claim that predicting the FU question?s fo-
cus (here, the entity ?library card?) will help a sys-
tem find the correct answer (cf. Sec. 6 for empirical
backup).
Q1: Can high-school students use the library?
A1: Yes, if they got a library card.
Q2: So, how do I get it?
Following (Stede and Schlangen, 2004), we re-
fer to the type of IQA dialogues we are studying
as ?information-seeking chat?, and conjecture that
this kind of dialogue can be handled by means of a
simple model of discourse structure. Our assump-
tion is that in general the user engages in a coherent
dialogue with the system. As proposed in (Ahren-
berg et al, 1995), we model the dialogues in terms
of pairs of initiatives (questions) and responses
(answers), ignoring other intentional acts.
The approach we adopt aims at answering the
following questions: (a) In what way does infor-
mation about the previous user questions and pre-
vious system answers help in predicting the next
FU Q? (b) Does the performance of an IQA sys-
tem improve if it has structure/history-based infor-
mation? (c) Which is the role that each part of this
information plays for determining the correct an-
swer to a FU Q?
This paper is structured as follows. Section 2
gives an overview of some theories of focus used in
dialogue and IQA. Section 3 then gives a detailed
account of our theory, explaining what a question
can focus on, and what patterns of focus change
we expect a FU Q will trigger. Hence, this first
25
part answers our question (a) above. We then move
to more applied issues in Sec. 4, where we show
how questions and answers were annotated with
focus information. The next Section 5 explains the
Q?A algorithm we use to test our theory so as to
answer (b), while Section 6 covers the logistic re-
gression models with which we learn optimal val-
ues for the algorithm from data, addressing ques-
tion (c).
2 Coherence in IQA dialogues
In the area of Discourse processing, much work
has been devoted to formulating rules that account
for the coherence of dialogues. This coherence
can often be defined in terms of focus and focus
shifts. In the following, we adopt the definition
from (Lec?uche et al, 1999): focus stands for the
?set of all the things to which participants in a di-
alogue are attending to at a certain point in a dia-
logue?.
1
In general, all theories of dialogue focus
considered by Lec?uche et al claim that the focus
changes according to some specific and well de-
fined patterns, following the rules proposed by the
respective theory. The main difference between
these theories lies in how these rules are formu-
lated.
A major distinguishing feature of different fo-
cus theories has been the question whether they ad-
dress global or local focus. While the latter explain
coherence between consecutive sentences, the for-
mer are concerned with how larger parts of the di-
alogue can be coherent. We claim that in ?infor-
mation seeking dialogue? this distinction is moot,
and the two kinds of foci collapse into one. Fur-
thermore, our empirical investigation shows that it
suffices to consider a rather short history of the di-
alogue, i.e. the previous user question and previous
system answer, when looking for relations between
previous dialogue and a FU Q.
Salient transitions between two consecutive
questions are defined in (Chai and Jin, 2004) un-
der the name of ?informational transitions?. The
authors aim to describe how the topic within a di-
1
This definition is in line with how focus has been used in
Computational Linguistics and Artificial Intelligence (hence,
?AI focus?), originating in the work of Grosz and Sidner on
discourse entity salience. We follow Lec?uche et al in that
focused elements could also be actions/tasks. We see the most
salient focused element (corresponding to the ?Backward-
looking center? in Centering Theory) as the topic of the ut-
terance. Accordingly, in the following we will use the terms
focus and topic interchangeably; cf. (Vallduvi, 1990) for a sur-
vey of these rather overloaded terms.
alogue evolves. They take ?entities? and ?activi-
ties? as the main possible focus of a dialogue. A
FU Q can be used to ask (i) a similar question as
the previous one but with different constraints or
different participants (topic extension); (ii) a ques-
tion concerning a different aspect of the same topic
(topic exploration); (iii) a question concerning a
related activity or a related entity (topic shift). We
take this analysis as our starting point, extend it
and propose an algorithm to automatically detect
the kind of focus transition a user performs when
asking a FU Q, and evaluate our extended theory
with real dialogue data. Following (Bertomeu et
al., 2006) we consider also the role of the system
answer, and we analyze the thematic relations be-
tween the current question and previous question,
and the current question and previous answer. Un-
like (Bertomeu et al, 2006), we attempt to learn a
model of naturally occurring thematic relations in
relatively unconstrained IQA dialogues.
3 Preliminary Observations
3.1 What ?things? do users focus on?
For all forthcoming examples of dialogues, ques-
tions and answers, we will base our discussion
on an actual prototype IQA system we have been
developing; this system is supposed to provide
library-related information in a university library
setting.
In the dialogues collected via an earlier Wizard-
of-Oz (WoZ) experiment (Kirschner and Bernardi,
2007), we observed that users either seem to
have some specific library-related task (action, e.g.
?search?) in mind that they want to ask the system
about, or they want to retrieve information on some
specific entity (e.g., ?guided tour?). People tend
to use FU Qs to ?zoom into? (i.e., find out more
about) either of the two. In line with this analysis,
the focus of a FU Q might move from the task (ac-
tion/verb) to the entities that are possible fillers of
the verb?s semantic argument slots.
Based on these simple observations, we pro-
pose a task/entity-based model for describing the
focus of questions and answers in our IQA set-
ting. Our theory of focus structure is related to the
task-based theory of (Grosz, 1977). Tasks corre-
spond to verbs, which are inherently connected to
an argument structure defining the verb?s semantic
roles. By consulting lexical resources like Prop-
Bank (Palmer et al, 2005), we can use existing
knowledge about possible semantic arguments of
26
the tasks we have identified.
We claim that actions/verbs form a suitable
and robust basis for describing the (informational)
meaning of utterances in IQA. Taking the main
verb along with its semantic arguments to repre-
sent the core meaning of user questions seems to
be a more feasible alternative to deep semantic ap-
proaches that still lack the robustness for dealing
with unconstrained user input.
Further, we claim that analyzing user questions
on the basis of their task/entity structure provides a
useful level of abstraction and granularity for em-
pirically studying informational transitions in IQA
dialogues. We back up this claim in Section 6.
Along the lines of (Kirschner and Bernardi, 2007),
we aim for a precise definition of focus structure
for IQA questions. Our approach is similar in spirit
to (Chai and Jin, 2004), whereas we need to re-
duce the complexity of their discourse representa-
tion (i.e., their number of possible question ?top-
ics?) so that we arrive at a representation of focus
structure that lends itself to implementation in a
practical IQA system.
3.2 How focus evolves in IQA
We try to formulate our original question, ?Given
a user question and a system response, what does
a salient FU Q focus on?? more precisely. We
want to know whether the FU Q initiates one of
the following three transitions:
2
Topic zoom asking about a different aspect of
what was previously focused
1. asking about the same task and same ar-
gument, but different question type (e.g.,
search for books: Q: where, FU Q: how)
2. asking about the same entity (e.g.,
guided tour: Q: when, FU Q: where)
3. asking about the same task but different
argument (e.g., Q: search for books, FU
Q: search for journals)
4. asking about an entity introduced in the
previous system answer
Coherent shift to a ?related? (semantically, or:
verb?its semantic argument) focus
1. from task to semantically related task
2. from task to related entity: entity is a se-
mantic argument of the task
2
Comparing our points to (Chai and Jin, 2004), Topic
zoom: 1. and 2. are cases of topic exploration, 3. of topic
extension, and 4. is new. Coherent shift: 1. and 2. are cases of
topic shift, and 3. and 4. are new.
3. from entity to semantically related entity
4. from entity to related task: entity is a se-
mantic argument of the task
Shift to an unrelated focus
From the analysis of our WoZ data we get cer-
tain intuitions about salient focus flow between
some preceding dialogue and a FU Q. First of all,
we learn that a dialogue context of just one previ-
ous user question and one previous system answer
generally provides enough information to resolve
context-dependent FU Qs. In the remainder of this
section, we describe the other intuitions by propos-
ing alternative ways of detecting the focus of a FU
Q that follows a salient relation (?Topic zoom? or
?Coherent shift?). Later in this paper we show how
we implement these intuitions as features, and how
we use a regression model to learn the importance
of these features from data.
Exploiting task/entity structure Knowing
which entities are possible semantic arguments
of a library-related task can help in detecting the
focused task. Even if the task is not expressed
explicitly in the question, the fact that a number of
participant entities are found in the question could
help identify the task at hand.
Exploiting (immediate) dialogue context: pre-
vious user question It might prove useful to
know the things that the immediately preceding
user question focused on. If users tend to con-
tinue focusing on the same task, entity or question
type, this focus information can help in ?complet-
ing? context-dependent FU Qs where the focused
things cannot be detected easily since they are not
mentioned explicitly. This way of using dialogue
context has been used in previous IQA systems,
e.g., the Ritel system (van Schooten et al, forth-
coming).
Exploiting (immediate) dialogue context: pre-
vious system answer Whereas the role of the
system answer has been ignored in some pre-
vious accounts of FU Qs (e.g., (Chai and Jin,
2004) and even in the highly influential TREC task
(Voorhees, 2004)), our data suggest that the system
answer does play a role for predicting what a FU
Q will focus on: it seems that the system answer
can introduce entities that a salient FU Q will ask
more information about. (van Schooten and op den
Akker, 2005) and (Bertomeu et al, 2006) describe
IQA systems that also consider the previous sys-
tem answer.
27
Exploiting task/entity structure combined with
dialogue context It might be useful to com-
bine knowledge about the task/entity structure with
knowledge about the previously focused task or
entity. E.g., a previously focused task might make
a ?coherent shift? to a participant entity likely;
likewise, a previously focused entity might enable
a coherent shift to a task in which that entity could
play a semantic role.
The questions to be addressed in the remain-
der of the paper now are the following. Does the
performance of an IQA system improve if it has
structure/history-based information as mentioned
above? Which is the role that each part of this in-
formation plays for determining the correct answer
to a FU Q?
4 Tagging focus on three levels
Following the discussion in Section 3.1, and hav-
ing studied the user dialogues from our WoZ data,
we propose to represent the (informational) mean-
ing of a user question by identifying the task
and/or entity that the question is about (focuses
on). Besides task and entity, we have Question
Type (QType) as a third level on which to describe
a question?s focus. The question type relates to
what type of information the user asks about the
focused task/entity, and equivalently describes the
exact type of answer (e.g., why, when, how) that
the user hopes to get about the focused task/entity.
Thus, we can identify the focus of a question with
the triple <Task, Entity, QType>.
We have been manually building a small
domain-dependent lexical resource that in the fol-
lowing we will call ?task/entity structure?. We
see it as a miniature version of the PropBank, re-
stricted to the small number of verbs/tasks that we
have identified to be relevant in our domain, but
extended with some additional semantic argument
slots if required. Most importantly, the argument
slots have been assigned to possible filler entities,
each of which can be described with a number of
synonymous names.
Tasks By analyzing a previously acquired exten-
sive list of answers to frequently-asked library-
related questions, we identified a list of 11 tasks
that library users might ask about (e.g. search, re-
serve, pick up, browse, read, borrow, etc.). Our
underlying assumption is that the focus (as identi-
fied by the focus triple) of a question is identical to
that of the corresponding answer. Thus, we assume
the focus triple describing a user question also de-
scribes its correct answer. For example, in Table 1,
A1 would share the same focus triple as Q1.
We think of the tasks as abstract descriptions of
actions that users can perform in the library con-
text. A user question focuses on a specific task if it
either explicitly contains that verb (or a synonym),
or implicitly refers to the same ?action frame? that
the verb instantiates.
Entities Starting from the information about se-
mantic arguments of these verbs available in
PropBank, and extending it when necessary for
domain-specific use of the verbs, for each task we
determined its argument slots. Again by inspect-
ing our list of FAQ answers, we started assign-
ing library-related entities to these argument slots,
when we found that the answer focuses on both
the task and the semantic argument entity. We
found that many answers focus on some library-
related entity without referring to any task. Thus,
we explicitly provide for the possibility of a ques-
tion/answer being about just an entity, e.g.: ?What
are the opening times??. A user question focuses
on a specific entity if it refers to it explicitly or
via some reference phenomenon (anaphora, ellip-
sis, etc.) linked to the dialogue history.
Question Types We compiled a list of question
(or answer) types by inspecting our FAQ answers
list, and thinking about the types of questions that
could have given rise to these answers. We aimed
for a compromise between potentially more fine-
grained distinctions of question semantics, and
better distinguishability of the resulting set of la-
bels (for a human annotator or a computer pro-
gram).
We defined each question type by providing a
typical question template, e.g.: ?where: where
can I find $Entity??, ?whatis: what is $Entity??,
?yesno: can I $Task $Entity??, ?howto: how do I
$Task $Entity??. Note how some question types
capture questions that focus on some task along
with some participant entity, while others focus on
just an entity. We also devised some question types
for questions focusing on just a task, where we as-
sume an implicit semantic argument which is not
expressed, e.g., ?how can I borrow?? (where in the
specific context of our application we can imply a
semantic argument like ?item?). A question has a
specific question type if it can be paraphrased with
the corresponding question template. An answer
28
has a specific type if it is the correct answer to that
question template.
4.1 A repository of annotated answers
From our original collection of answers to library
FAQs, we have annotated around 200 with focus
triples. The triples we selected include all poten-
tial answers to the FU Qs from the free FU Q elic-
itation experiment described in the next section.
Some of the actual answers were annotated with
more than one focus triple, e.g., often the answer
corresponded to more than one question type. The
total of 207 focus triples include all 11 tasks and
23 different question types (where the 4 most fre-
quent types were the ones mentioned as examples
above, accounting for just over 50% of all focus
triples).
For instance, the answer: ?You can restrict your
query in the OPAC on individual Library locations.
The search will then be restricted e.g. to the Li-
brary of Bressanone-Brixen or the library of the
?Museion?.? is marked by: <Task: search, Entity:
specific library location, QType: yesno>.
The algorithm we introduce in Section 5 uses
this answer repository as the setA of potential can-
didates from which it chooses the answer to a new
user question. Again, we assume that if we can de-
termine the correct focus triple of a user question,
the answer from our collection that has been an-
notated with that same triple will correctly answer
the question.
4.2 Annotated user questions
Having created an answer repository annotated
with focus triples, we need user questions anno-
tated on the same three levels, which we can then
use for training and evaluating the Q?A algorithm
that we introduce in Section 5. We acquired these
data in two steps: 1. eliciting free FU Qs from sub-
jects in a web-based experiment, 2. annotating the
questions with focus triples.
Dialogue Collection Experiment We set up a
web-based experiment to collect genuine FU Qs.
We adopted the experimental setup proposed in
(van Schooten and op den Akker, 2005)), in that
we presented to our subjects short dialogues con-
sisting of a first library-related question, and a cor-
responding correct answer, as exemplified by ?Q1?
and ?A1? in Table 1.
We asked the subjects to provide a FU Q ?Q2?
such that it will help further serve their information
need in the situation defined by the given previous
question-answer exchange. In this way, we col-
lected 88 FU Qs from 8 subjects and 11 contexts
(first questions and answers).
3
Annotating the questions We annotated these
88 FU Qs, along with the 11 first questions that
were presented to the subjects, with focus triples.
By (informally) analyzing the differences between
different annotators? results, we continuously tried
to disambiguate and improve the annotation in-
structions. As a result, we present a pre-compiled
list of entities from which the annotator selects the
one they consider to be in focus, and that of all
possible candidates is the one least ?implied? by
the context. Table 1 shows one example annota-
tion of one of the 11 first user questions and two of
the 8 corresponding FU Qs.
5 A feature-based Q?A algorithm
We now present an algorithm for mapping a user
question to a canned-text answer from our answer
repository. The decision about which answer to se-
lect is based on a score that the algorithm assigns to
each answer, which in turn depends on the values
of the features we have introduced in the previous
section. Thus, the purpose of the algorithm is to
select the best answer focus triple from the repos-
itory, based on feature values. In this way, we can
use the algorithm as a test bed for identifying fea-
tures that are good indicators for a correct answer.
Our goal is to evaluate the algorithm based on its
accuracy in finding correct focus triples (which are
the ?keys? to the actual system answers) for user
questions (see Section 5.2).
For each new user question q that is entered, the
algorithm iterates through all focus triples a in the
annotated answer repository A (cf. Section 4.1).
For each combination of q and a, all 10 features
x
1,q,a
. . . x
10,q,a
are evaluated. Each feature that
evaluates to true (? = 1) or some positive value,
contributes with this score ? towards the overall
score of a. The algorithm then returns the highest-
scoring answer a?.
a? = argmax
a?A
(?
1
x
1,q,a
+ ? ? ?+ ?
10
x
10,q,a
)
3
In the future, we plan to collect real FU Qs from users of
our online IQA system, which will solve the potential prob-
lem of these questions being somewhat artificial due to the
experimental setting. However, we still expect our current
data to be highly relevant for studying what users would ask
about next.
29
ID Q/A Task Entity QType
Q1 Can I get search results for a specific search specific library location yesno
library location?
A1 You can restrict your query in the OPAC
on individual Library locations. (...)
Q2a How can I do that? search specific library location howto
Q2b How long is my book reserved there if I reserve my book howlong
want to get it?
Table 1: Example annotation of one first question and two corresponding FU Qs
5.1 Features
Based on the intuitions presented in Section 3.2,
we now describe the 10 features x
1,q,a
, . . . , x
10,q,a
that our algorithm uses as predictors for answer
correctness. All Task and Entity matching is done
using string matching over word stems. QType
matching uses regular expression matching with
a set of simple regex patterns we devised for our
QTypes.
3 surface-based features x
1,q,a
, . . . , x
3,q,a
:
whether {Task
a
,Entity
a
,QType
a
} are
matched in q. Entity feature returns the
length in tokens of the matched entity.
1 task/entity structure-based feature x
4,q,a
:
how many of the participant entities of Task
a
(as encoded in our task/entity structure) are
matched in q.
4 focus continuity features x
5,q,a
, . . . , x
8,q,a
:
whether {Task
a
,Entity
a
,QType
a
} are con-
tinued in q, wrt. previous dialogue as fol-
lows:
4
? Task, Entity, QType continuity wrt. pre-
vious user question.
? Entity continuity wrt. previous system
answer.
2 task/entity structure + focus continuity fea-
tures x
9,q,a
, x
10,q,a
:
? Focused Task of previous user question
has Entity
a
as a participant.
? Task
a
has focused Entity of previous
question as a participant.
5.2 First Evaluation
Table 2 shows manually set feature scores
?
1
, . . . , ?
10
we used for a first evaluation of the al-
4
Both entity continuity features evaluate to ?2? when ex-
actly the same entity is used again, but to ?1? when a synonym
of the first entity is used.
k x
k,q,a
range(x
k,q,a
) ?
k
1 qTypeMatch 0,1 4
2 taskMatch 0,1 3
3 lenEntityMatch n 2
4 nEntitiesInTask n 1
5 taskContinuity 0,1 1
6 entityContinuity 0,1,2 1
7 qTypeContinuity 0,1 1
8 entityInPrevAnsw 0,1,2 2
9 entityInPrevTask 0,1 1
10 prevEntityInTask 0,1 1
Table 2: Manually set feature scores
gorithm; we chose these particular scores after in-
specting our WoZ data. With these scores, we ran
the Q?A algorithm on the annotated questions of
annotator 1, who had provided a ?gold standard?
annotation for 78 of the 99 user questions (the re-
mainder of the questions are omitted because the
annotator did not know how to assign a focus triple
to them). For 24 out of 78 questions, the algorithm
found the exact focus triple (from a total of 207
focus triples in the answer repository), yielding an
accuracy of 30.8%.
6 Logistic Regression Model
To improve the accuracy of the Q?A algorithm
and to learn about the importance of the single
features for predicting whether an answer from
A is correct, we want to learn optimal scores
?
1
, . . . , ?
10
from data. We use a logistic regression
model (cf. (Agresti, 2002)). Logistic regression
models describe the relationship between some
predictors (i.e., our features) and an outcome (an-
swer correctness).
We use the logit ? coefficients ?
1
, . . . , ?
k
that
the logistic regression model estimates (from train-
ing data, using maximum likelihood estimation)
30
Coeff. 95% C.I.
lenEntityMatch 6.76 5.26?8.26
qTypeMatch 2.54 2.02?3.06
taskContinuity 2.17 1.39?2.94
entityInPrevAnsw 1.78 1.06?2.49
taskMatch 1.37 0.80?1.94
prevEntityInTask -1.24 -2.06? -0.43
Table 3: Model M
2
: Magnitudes of significant ef-
fects
for the predictors as empirically motivated scores.
In contrast to other supervised machine learn-
ing techniques, regression models yield human-
readable coefficients that show the individual ef-
fect of each predictor on the outcome variable.
6.1 Generating Training data
We generate the training data for learning the lo-
gistic regression model from our annotated answer
repository A (Sec. 4.1) and annotated questions
(Sec. 4.2) as follows. For each human-annotated
question q and each candidate answer focus triple
from our repository (a ? A), we evaluate our fea-
tures x
1,q,a
, . . . , x
10,q,a
. If the focus triples of q
and a are identical, we take the particular feature
values as a training instance for a correct answer; if
the focus triples differ, we have a training instance
for a wrong answer.
5
6.2 Results and interpretation
We fit model M
1
based on the annotation of anno-
tator 2 using all 10 features.
6
We then fit a second
model M
2
, this time including only the 6 features
that correspond to coefficients from modelM
1
that
are significantly different from zero. Table 3 shows
the resulting logit ? coefficients with their 95%
confidence intervals. Using these coefficients as
new scores in our Q?A algorithm (and setting all
non-significant coefficients? feature scores to 0), it
finds the correct focus triple for 47 out of 78 test
questions (as before, annotated by annotator 1);
answer accuracy now reaches 60.3%.
We interpret the results in Table 3 as follows.
All three surface-based features are significant pre-
dictors of a correct answer. The length of the
5
Although in this way we get imbalanced data sets with
|A| ? 1 negative training instances for each positive one, we
have not yet explored this issue further.
6
We use annotator 2?s data for training, and annotator 1?s
for testing throughout this paper.
matched entity contributes more than the other
two; we attribute this to the fact that there are
more cases where our simple implementations of
qTypeMatch and taskMatch fail to detect the cor-
rect QType or task. While the task/entity structure-
based nEntitiesInTask clearly misses to reach sig-
nificance, the history-based features taskContinu-
ity and entityInPrevAnsw are useful indicators for
a correct answer. The first is evidence for ?Topic
zoom?, with the FU Q asking about a different as-
pect of the previously focused task, while the sec-
ond shows the influence of the previous answer in
shaping the entity focus of the FU Q. From the two
?task/entity structure + focus continuity? features,
we find that if a FU Q focuses on a task that in
our task/entity structure has an argument slot filled
with the previously focused entity, it actually indi-
cates a false answer; the implications of this find-
ing will have to be explored in future work.
Finally, to pinpoint the important contributions
of structure- and/or focus continuity features, we
fit a new model M
3
, this time including only the 3
(significant) surface-based features. Evaluating the
resulting coefficients in the same way as above, we
get only 24 out of 78 correct answer focus triples,
an accuracy of 30.8%. This result supports our ini-
tial claim that an IQA system improves if it has a
way of predicting the focus of a FU Q.
7 Conclusion
Our original hypothesis was that a) knowledge
about the dialogue history, and b) lexical knowl-
edge about semantic arguments could improve an
IQA system?s ability to answer FU Qs. We opera-
tionalized these notions by formulating a set of 10
features that evaluate whether a candidate answer
is the correct one given a new (FU) user question.
We then used regression modeling to investigate
the usefulness of each individual feature by learn-
ing from annotated IQA dialogue data, showing
that certain knowledge about the dialogue history
(the previously focused task, and the entities men-
tioned in the previous system answer) and about
semantic arguments are useful for distinguishing
correct from wrong answers to a FU Q. Finally,
we evaluated these results by showing how our
Q?A mapping algorithm?s answer accuracy im-
proved by using the empirically learned scores for
all statistically significant predictors/features. The
features and the Q?A algorithm as a whole are
based on a simple way to describe IQA questions
31
in terms of focus triples. By showing how we
have improved an actual system with learned fea-
ture scores, we demonstrated this representation?s
viability for implementation and for empirically
studying informational transitions in IQA.
Although the IQA system used in our project is
in several ways limited, our findings about how
focus evolves in real IQA dialogues should scale
up to any new or existing IQA system that allows
users to ask context-dependent FU Qs in a type of
?information seeking? paradigm. It would be in-
teresting to see how this type of knowledge could
be added to other IQA or dialogue systems in gen-
eral.
We see several directions for future work. Re-
garding coherent focus transitions, we have to look
into which transitions to different tasks/entities are
more coherent than others, possibly based on se-
mantic similarity. A major desideratum for show-
ing the scaleability of our work is to explore the
influence of the subjects on our data annotation.
We are currently working on getting an objective
inter-annotator agreement measure, using external
annotators. Finally, we plan to collect a large cor-
pus of IQA dialogues via a publicly accessible IQA
system, and have these dialogues annotated. With
more data, coming from genuinely interested users
instead of experimental subjects, and having these
data annotated by external annotators, we expect
to have more power to find significant and gener-
ally valid patterns of how focus evolves in IQA di-
alogues.
Acknowledgments
We thank Marco Baroni, Oliver Lemon, Massimo
Poesio and Bonnie Webber for helpful discussions.
References
Agresti, Alan. 2002. Categorical Data Analysis.
Wiley-Interscience, New York.
Ahrenberg, L., N. Dahlb?ack, and A. J?onsson. 1995.
Coding schemes for studies of natural language dia-
logue. In Working Notes from AAAI Spring Sympo-
sium, Stanford.
Bertomeu, N?uria, Hans Uszkoreit, Anette Frank, Hans-
Ulrich Krieger, and Brigitte J?org. 2006. Contex-
tual phenomena and thematic relations in database
QA dialogues: results from a wizard-of-oz experi-
ment. In Proc. of the Interactive Question Answer-
ing Workshop at HLT-NAACL 2006, pages 1?8, New
York, NY.
Chai, Joyce Y. and Rong Jin. 2004. Discourse structure
for context question answering. In Proc. of the HLT-
NAACL 2004 Workshop on Pragmatics in Question
Answering, Boston, MA.
Grosz, Barbara Jean. 1977. The representation and
use of focus in dialogue understanding. Ph.D. thesis,
University of California, Berkeley.
Kirschner, Manuel and Raffaella Bernardi. 2007. An
empirical view on iqa follow-up questions. In Proc.
of the 8th SIGdial Workshop on Discourse and Dia-
logue, Antwerp, Belgium.
Lec?uche, Renaud, Chris Mellish, Catherine Barry,
and Dave Robertson. 1999. User-system dia-
logues and the notion of focus. Knowl. Eng. Rev.,
13(4):381?408.
Palmer, Martha, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus
of semantic roles. Comput. Linguist., 31(1):71?106.
Stede, Manfred and David Schlangen. 2004.
Information-seeking chat: Dialogue management by
topic structure. In Proc. of SemDial?04 (Catalog),
Barcelona, Spain.
Vallduvi, Enric. 1990. The Informational Component.
Ph.D. thesis, University of Pennsylvania, Philadel-
phia, PA.
van Schooten, Boris and Rieks op den Akker. 2005.
Follow-up utterances in QA dialogue. Traitement
Automatique des Langues, 46(3):181?206.
van Schooten, Boris, R. op den Akker, R. Rosset,
O. Galibert, A. Max, and G. Illouz. forthcoming.
Follow-up question handling in the IMIX and Ritel
systems: A comparative study. Journal of Natural
Language Engineering.
Voorhees, Ellen M. 2004. Overview of the TREC 2004
question answering track. In Proc. of the 13th Text
REtrieval Conference.
32
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 322?331,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Towards an Empirically Motivated Typology of Follow-Up Questions:
The Role of Dialogue Context
Manuel Kirschner and Raffaella Bernardi
KRDB Centre, Faculty of Computer Science
Free University of Bozen-Bolzano, Italy
{kirschner,bernardi}@inf.unibz.it
Abstract
A central problem in Interactive Ques-
tion Answering (IQA) is how to answer
Follow-Up Questions (FU Qs), possibly
by taking advantage of information from
the dialogue context. We assume that FU
Qs can be classified into specific types
which determine if and how the correct
answer relates to the preceding dialogue.
The main goal of this paper is to propose
an empirically motivated typology of FU
Qs, which we then apply in a practical
IQA setting. We adopt a supervised ma-
chine learning framework that ranks an-
swer candidates to FU Qs. Both the an-
swer ranking and the classification of FU
Qs is done in this framework, based on a
host of measures that include shallow and
deep inter-utterance relations, automati-
cally collected dialogue management meta
information, and human annotation. We
use Principal Component Analysis (PCA)
to integrate these measures. As a result,
we confirm earlier findings about the ben-
efit of distinguishing between topic shift
and topic continuation FU Qs. We then
present a typology of FU Qs that is more
fine-grained, extracted from the PCA and
based on real dialogue data. Since all our
measures are automatically computable,
our results are relevant for IQA systems
dealing with naturally occurring FU Qs.
1 Introduction
When real users engage in written conversations
with an Interactive Question Answering (IQA)
system, they typically do so in a sort of dia-
logue rather than asking single shot questions.
The questions? context, i.e., the preceding interac-
tions, should be useful for understanding Follow-
Up Questions (FU Qs) and helping the system
pinpoint the correct answer. In previous work
(Kirschner et al, 2009; Bernardi et al, 2010;
Kirschner, 2010), we studied how dialogue con-
text should be considered to answer FU Qs. We
have used Logistic Regression Models (LRMs),
both for learning which aspects of dialogue struc-
ture are relevant to answering FU Qs, and for com-
paring the accuracy with which the resulting IQA
systems can correctly answer these questions. Un-
like much of the related research in IQA, which
used artificial collections of user questions, our
work has been based on real user-system dialogues
we collected via a chatbot-inspired help-desk IQA
system deployed on the web site of our University
Library.
Previously, our experiments used a selection
of shallow (Kirschner et al, 2009) and deep
(Bernardi et al, 2010) features, all of which de-
scribe specific relations holding between two ut-
terances (i.e., user questions or system answers).
In this paper we present additional features derived
from automatically collected dialogue meta-data
from our chatbot?s dialogue management com-
ponent. We use Principal Component Analysis
(PCA) to combine the benefits of all these infor-
mation sources, as opposed to using only certain
hand-selected features as in our previous work.
The main goal of this paper is to learn from data
a new typology of FU Qs; we then compare it to an
existing typology based on hand-annotated FU Q
types, as proposed in the literature. We show how
this new typology is effective for finding the cor-
rect answer to a FU Q. We produce this typology
by analyzing the main components of the PCA.
This paper presents two main results. A new,
empirically motivated typology of FU Qs confirms
earlier results about the practical benefit of dis-
tinguishing between topic continuation and topic
shift FU Qs, which are typically based on hand
annotation. We then show that we can do without
such hand annotations, in that our fully automatic,
322
on-line measures ? which include automatically
collected dialogue meta-data from our chatbot?s
dialogue manager ? lead to better performance in
identifying correct answers to FU Qs.
In the remainder of this paper, we first review
relevant previous work concerning FU Q typolo-
gies in IQA. Section 3 then introduces our col-
lection of realistic IQA dialogues which we will
use in all our experiments; the section includes
descriptions of meta information in the form of
dialogue management features and post-hoc hu-
man annotations. In Section 4 we introduce our
experimental framework, based on inter-utterance
features and LRMs. Our experimental results are
presented in Section 5, which is followed by our
conclusions.
2 Related work
Much of previous work on dialogue processing in
the domain of contextual or interactive Question
Answering (QA) (Bertomeu, 2008; van Schooten
et al, 2009; Chai and Jin, 2004; Yang et al, 2006)
has been based on (semi-)artificially devised sets
of context questions. However, the importance of
evaluating IQA against real user questions and the
need to consider preceding system answers has al-
ready been emphasized (Bernardi and Kirschner,
2010). The corpus of dialogues we deal with con-
sists of real logs in which actual library users were
conversing (by typing) with a chat-bot to obtain
information in a help-desk scenario.
(Yang et al, 2006) showed that shallow simi-
larity features between a FU Q and the preceding
utterances are useful to determine whether the FU
Q is a continuation of the on-going topic (?topic
continuation?), or it is a ?topic shift?. The authors
showed that recognizing these two basic types of
FU Qs is important for deciding which context
fusion strategies to employ for retrieving the an-
swer to the FU Q. (Kirschner et al, 2009) showed
how shallow measures of lexical similarity be-
tween questions and answers in IQA dialogues are
as effective as manual annotations for distinguish-
ing between these basic FU Q types. However,
that earlier work was based on a much smaller set
of dialogue data than we use in this paper, mak-
ing for statistically weaker results. (Bernardi et
al., 2010) improved on this approach by increas-
ing the data set, and adding ?deep? features that
quantify text coherence based on different theories
of dialogue and discourse structure. However, FU
Q classification was performed using either single,
hand-selected shallow or deep features, or a hand-
selected combination of one shallow and one deep
feature. In this paper, we adopt the most promising
measures of similarity and coherence from the two
aforementioned papers, add new features based
on automatically collected dialogue management
meta-data, and combine all this information via
Principal Component Analysis (PCA). By using
PCA, we circumvent the theoretical problem that
potentially multicollinear features pose to our sta-
tistical models, and at the same time we have a
convenient means for inducing a new typology of
FU Qs from our data, by analyzing the composi-
tion of the principal components of the PCA.
More fine-grained typologies of FU Qs have
been suggested, and different processing strategies
have been proposed for the identified types. In
this paper, we start from our own manual annota-
tion of FU Qs into four basic classes, as suggested
by the aforementioned literature (Bertomeu, 2008;
van Schooten et al, 2009; Sun and Chai, 2007).
We then compare it to our new PCA-based FU Q
typology.
3 Data
We now introduce the set of IQA dialogue data
which we will use in our experiments. For the pur-
pose of calculating inter-utterance features within
these user-system interactions ? as described in
Section 4.4 ? we propose to represent utterances
in terms of dialogue snippets. A dialogue snip-
pet, or snippet for short, contains a FU Q, along
with a 2-utterance window of the preceding dia-
logue context. In this paper we use a supervised
machine learning approach for evaluating the cor-
rectness of a particular answer to a FU Q; we thus
represent also the answer candidate as part of the
snippet. Introducing the naming convention we
use throughout this paper, a snippet consists of the
following four successive utterances: Q1, A1, Q2,
and A2. The FU Q is thus referred to as Q2.
The data consists of 1,522 snippets of 4-turn
human-machine interactions in English: users ask
questions and the system answers them. The data
set was collected via the Bolzano Bot (BoB) web
application that has been working as an on-line
virtual help desk for the users of our University
Library since October 2008.1 The snippets were
1www.unibz.it/library. More information on the
BoB dialogue corpus: bob.iqa-dialogues.net.
323
extracted from 916 users? interactions.
Table 3 shows three example dialogue snippets
with correct A1 and A2; these examples are meant
to give an idea of the general shape of the BoB
dialogue data. In the third example snippet, A1
and A2 actually contain clickable hyperlinks that
open an external web-site. We represent them here
as dots in parentheses.
Our library domain experts manually checked
that each FU Q was either correctly answered in
the first place by BoB, or they corrected BoB?s an-
swer by hand, by assigning to it the correct answer
from BoB?s answer repository. In this way, the di-
alogue data contain 1,522 FU Qs, along with their
respective contexts (Q1 and A1) and their correct
answers (A2). The resulting set of correct A2s
contains 306 unique answers.2
The BoB dialogue data also contain two levels
of meta information that we will use in this paper.
On the one hand, we have automatically collected
dialogue meta-data from BoB?s dialogue manager
that describe the internal state of the BoB system
when a FU Q was asked; this information is de-
scribed in Section 4.2. On the other hand, 417 of
the 1,522 FU Qs were hand-annotated regarding
FU Q type, as described in Section 4.3.
4 Model
Our goal is, given a FU Q (Q2 in our dialogue
snippets), to pick the best answer from the fixed
candidate set of 306 A2s, by assigning a score to
each candidate, and ranking them by this score.
Different FU Q types might require different an-
swer picking strategies. Thus, we specify both
A2 (identification) features, aiming at selecting the
correct A2 among candidates, and context (iden-
tification) features, that aim at characterizing the
context. The A2 identification features measure
the similarity or coherence between an utterance
in the context (e.g., Q2) and a candidate A2. Con-
text features measure the similarity or coherence
between pairs of utterances in the context (e.g.,
Q1 and Q2). They do not provide direct infor-
mation about A2, but might cue a special context
(say, an instance of topic shift) where we should
pay more attention to different A2 identification
features (say, less attention to the relation between
2Many of the 306 answer candidates overlap semantically.
This is problematic, given that our evaluation approach as-
sumes exactly one candidate to be correct, while all other 305
answers to be wrong. In this paper, we shall accept this fact,
for the merit of simplicity.
Q2 and A2, and more to the one between A1 and
A2).
We implement these ideas by estimating a gen-
eralized linear model from training data to predict
the probability that a certain A2 is correct given
the context. In this model, we enter A2 features as
main effects, and context features in interactions
with the former, allowing for differential weight
assignment to the same A2 features depending on
the values of the context features.
4.1 Logistic Regression
Logistic regression models (LRMs) are general-
ized linear models that describe the relationship
between features (independent variables) and a bi-
nary outcome (Agresti, 2002). LRMs are closely
related to Maximum Entropy models, which have
performed well in many NLP tasks. A major ad-
vantage of using logistic regression as a super-
vised machine learning framework (as opposed to
other, possibly better performing approaches) is
that the learned coefficients are easy to interpret
and assess in terms of their statistical significance.
The logistic regression equations specify the prob-
ability for a particular answer candidate A2 being
correct, depending on the ? coefficients (repre-
senting the contribution of each feature to the total
answer correctness score), and the feature values
x1, . . . , xk. In our setting, we are only interested
in the rank of each A2 among all answer candi-
dates, which can be easily and efficiently calcu-
lated through the linear part of the LRM: score
= ?1x1 + . . .+ ?kxk.
FU Q typology is implicitly modeled by inter-
action terms, given by the product of an A2 fea-
ture and a context feature. An interaction term
provides an extra ? to assign a differential weight
to an A2 feature depending on the value(s) of a
context feature. In the simplest case of interaction
with a binary 0-1 feature, the interaction ? weight
is only added when the binary feature has the 1-
value.
As described in (Kirschner, 2010), we esti-
mate the model parameters (the beta coefficients
?1, . . . , ?k) using maximum likelihood estima-
tion. Moreover, we put each model we construct
under trial by using an iterative backward elimina-
tion procedure that keeps removing the least sig-
nificant predictor from the model until a specific
stopping criterion that takes into account the sta-
tistical goodness of fit is satisfied. All the results
324
we report below are obtained with models that un-
derwent this trimming procedure.
There is a potential pitfall when using multi-
ple regression models such as LRMs with multi-
collinear predictors, i.e., predictors that are inter-
correlated, such as our alternative implementa-
tions of inter-utterance string similarity. In such
situations, the model may not give valid results
about the importance of the individual predictors.
In this paper, we use PCA to circumvent the prob-
lem by combining potentially multicollinear pre-
dictors to completely uncorrelated PC-based pre-
dictors.
In the following three sections, we describe the
different types of information that are the basis for
our features.
4.2 BoB dialogue management meta-data
When BoB interacts with a user, it keeps log files
of the IQA dialogue. First of all, these logs in-
clude a timed protocol of user input and BoB?s
responses: the user and system utterances are the
literal part of the information. On the other hand,
BoB also logs two dimensions of meta informa-
tion, both of which are based on BoB?s internal
status of its dialogue management routine. This
routine is based on a main initiative-response loop,
mapping user input to some canned-text answer,
where the user input should be matched by (at
least) one of a set of hand-devised regular expres-
sion question patterns.
Sub-dialogues Whenever BoB asks a system-
initiated question, the main loop is suspended, and
the system goes into a sub-dialogue state, where it
waits for a specific response from the user ? typ-
ically a short answer indicating the user?s choice
about one of the options suggested by BoB. The
next user input is then matched against a small
number of regular expression patterns specifically
designed for the particular system-intiative ques-
tion at hand. Depending on this user input, the
sub-dialogue can:
Continue: the user input matched one of the
regular expression patterns intended to capture
possible user choices
Break: the user broke the sub-dialogue by en-
tering something unforeseen, e.g., a new question
The first two parts of Table 4 give an overview
of the statistics of BoB?s dialogue management-
based meta information concerned with sub-
dialogue status. Besides continue and break, for
Q1 we consider also a third, very common case
that a user question was not uttered in a sub-
dialogue setting at all. Note that we excluded from
our data collection all those cases where Q2 con-
tinues a sub-dialogue from our collection of IQA
dialogues, since we do not consider such Q2s as
FU Qs, as they are highly constrained by the pre-
vious dialogue.
Apology responses The third part of Table 4
gives statistics of whether a particular system re-
sponse A1 was an apology message stating that
BoB did not understand the user?s input, i.e., none
of BoB?s question patterns matched the user ques-
tion.
4.3 Manual dialogue annotation
We now turn to the meta information in BoB dia-
logue data that stems from post-hoc human anno-
tation. For a portion of BoB?s log files, we added
up to two additional levels of meta information, by
annotating the log files after they were collected.3
The following paragraphs explain the individ-
ual levels of annotation by giving the correspond-
ing annotator instructions; Table 5 contains an
overview of the corresponding features. First of
all, we annotated FU Qs with their FU Q type.
Our choice of the particular four levels of the
FUQtype feature was influenced by the following
literature literature: from (De Boni and Manand-
har, 2005) and (Yang et al, 2006) we adopted the
distinction between topic shift and topic continua-
tion, while from (Bertomeu et al, 2006) we took
the notions of rephrases and context dependency.
Our annotation scheme is described in Figure 1;
note that topic continuations have three sub-types,
which are spelled out below.
FUQtype = isTopicShift: marks a FU Q
as a topic shift based on an intuitive notion of
whether the FU Q ?switches to something com-
pletely different?.
FUQtype = isRephrase: marks whether
the FU Q is an attempt to re-formulate the same
question. The FU Q could be a literal repetition of
the previous question, or it could be a rephrasing.
FUQtype = isContextDepentFUQ:
marks whether the FU Q needs to be consid-
ered along with some information provided by
3All annotations were performed by either one of the au-
thors.
325
the dialogue context in order to be correctly
understood.
FUQtype = isFullySpecifiedFUQ:
marks whether the FU Q does not need any
information from the dialogue context in order to
be correctly understood.
The second level of hand-annotation concerns a
manual check of the correctness of A1. It is avail-
able for 1,179 of our 1,522 snippets.
A1.isAnswer.correct: marks whether the
system response is correct for the given question.
A1.isApology.correct: marks whether
BoB?s apology message is correct for the given
question.
4.4 Shallow/deep inter-utterance relations
We exploit shallow features, which measure the
similarity between two utterances within a snip-
pet, and deep features, which encode coherence
between two utterances based on linguistic the-
ory. For each feature we will use names encoding
the utterances involved; e.g., distsim.A1.Q2
stands for the Distributional Similarity feature cal-
culated between A1 and Q2.
Shallow features The detailed description of all
the shallow features we used in our experiments
can be found in (Kirschner et al, 2009). The in-
tuition is that a high similarity between Q and A
tends to indicate a correct answer, while in the
case of high similarity between the dialogue con-
text and the FU Q, it indicates a ?topic continua-
tion? FU Q (as opposed to a ?topic shift? FU Q),
and thus helps discriminating these two classes of
FU Qs.
Lexical Similarity (lexsim): If
two utterances share some terms, they are simi-
lar; the more discriminative the terms they share,
the more similar the utterances. Implements a TF-
IDF-based similarity metric. Distributional
Similarity (distsim.svd): Two utter-
ances are similar not only if they share the same
terms, but also if they share similar terms (e.g.,
book and journal). Term similarity is estimated
on a corpus, by representing each content word
(noun, verb, adjective) as a vector that records
its corpus co-occurrence with other content words
within a 5-word span. Action sequence
(action): Based on the notion that in our help-
desk setting we are dealing with task-based dia-
logues, which revolve around library-related ac-
tions (e.g., ?borrow?, ?search?). The action fea-
ture indicates whether two utterances contain the
same action.
Deep features These features encode different
theories of discourse and dialogue coherence. Re-
fer to (Bernardi et al, 2010) for a full description
of all deep features we used experimentally, along
with more details on the underlying linguistic the-
ories, and our implementation choices for these
features.
We introduce a four-level feature, center,
that encodes the four transitions holding between
adjacent utterances that Centering Theory de-
scribes (Brennan et al, 1987; Grosz et al, 1995).
Somewhat differently from that classic theory,
(Sun and Chai, 2007) define the transitions de-
pending on whether both the head and the modi-
fier of the Noun Phrases (NP) representing the pre-
ferred centers4 are continued (cont) or switched
(rough shift: roughSh) betweenQ1 andQ2. The
remaining two transitions are defined in similar
terms.
4.5 PCA-based context classification features
Principal Component Analysis (PCA) (Manly,
2004) is a statistical technique for finding patterns
in high-dimensional data, or for reducing their di-
mensionality. Intuitively, PCA rotates the axes of
the original data dimensions in such a way that
few of the new axes already cover a large portion
of the variation in the data. These few new axes
are represented by the so-called principal compo-
nents (PCs). We employ this technique as a tool
for combining a multitude of potentially multi-
collinear predictors for context classification, i.e.,
all predictors that involve Q2 and some preceding
utterance. In our experiments we will also want to
look at the correlations of each of the top PCs with
the original context classification features; these
correlations are called loadings in PCA. We exper-
iment with the following three versions of PCA:
PCAA: without BoB dialogue management
meta-data features PCA performed over all
context classification features of the shallow and
deep types described in Section 4.4.
4Centers are noun phrases. The syntactic structure of a
noun phrase comprises a head noun, and possibly a modi-
fier, e.g., an adjective. We use a related approach, described
in (Ratkovic, 2009), to identify the preferred center of each
question.
326
PCAB: with BoB dialogue management meta-
data features PCAA plus BoB?s dialogue-
management meta-data features (Section 4.2).
PCAC: with BoB dialogue management meta-
data features and manual A1 correctness check
PCAB plus additional manual annotation of A1
correctness (Section 4.3).
5 Evaluation
We employ a standard 10-fold cross-validation
scheme for splitting training and prediction data.
We assess our LRMs by comparing the ranks that
the models assign to the gold-standard correct A2
candidate (i.e., the single A2 that our library do-
main experts had marked as correct for each of the
1,522 FU Qs). To determine whether differences
inA2 ranking performance are significant, we con-
sult both the paired t-test and the Wilcoxon signed
rank test about the difference of the 1,522 ranks.
5.1 Approximating hand-annotated FU Q
types with PCA-based features
We begin the evaluation of our approach by ex-
ploring the value of the hand-annotation-based FU
Q type as cues for expressing the relevance and
topical relatedness of that particular FU Q?s dia-
logue context.
For this purpose, we use the subset of 417
dialogue snippets which we annotated with the
FUQtype feature described in the first half of Ta-
ble 5. Figure 1 depicts our FU Q type taxonomy,
and the distribution of the four types in our data.
First of all, for this hand-annotated subset of di-
alogue snippets, we try to improve the A2 ranking
results of a ?main effects only? baseline LRM,
i.e., a model which does not distinguish between
different FU Q types. This baseline model was
proposed in earlier work (Kirschner et al, 2009).
We tried the following features as interaction
term(s) in our models, one after the other: whether
the hand-annotated FUQType feature indicates a
topic shift or not; the full four levels of FUQType;
a linear combination of the top five PCs of each of
the three PCA feature sets introduced in Section
4.5. After applying our automatic predictor elimi-
nation routine described in Section 4.1 and evalu-
ating the A2 ranking results of each of these mod-
els, none of the interactive models significantly
outperform our baseline. PCA-based context clas-
sification using only fully automatic BoB meta in-
formation features (PCAB in Section 4.5) results
in the largest improvement over baseline; however,
this improvement does not reach statistical signifi-
cance, most likely due to the small data set of only
417 cases. Still, using the hand-annotated FU Q
type feature FUQType, we can visualize how the
top PCs cluster the 417 FU Qs, and how this clus-
tering mirrors some of the distinctions of manually
assigned FU Q types: see Figure 2. E.g., plotting
the FU Qs along their PC1 and PC2 values seems
to mimic the annotator?s distinction between topic
shift FU Qs and the other three FU Q types. The
other pairs of PCs also appear to show certain clus-
ters. Overall, the automatic context classification
features that served as input to the PCA are useful
for describing different context-related behaviors
of different FU Qs.
5.2 Optimizing A2 ranking scores using
PCA-based features
Having shown the usefulness (in terms of assign-
ing high ranks to the gold-standard correct A2) of
FU Q classification via a PCA-based combination
of purely automatic context classification features,
we can now consider the full sample of 1,522 di-
alogue snippets described in Section 3, for which
we do not in general possess manual FU Q type
annotations.
The first row of Table 1 shows the A2 ranking
results of our baseline LRM. In the remainder of
the table, we compare this baseline model to three
different models which use a linear combination
of different versions of the top five PCs as interac-
tion terms. The three versions (A, B and C) were
introduced in Section 4.5.
5.3 Analysis of PC-based context features
The main goal of this paper is to devise an empiri-
cally motivated typology of FU Qs, under consid-
eration of automatically collected dialogue man-
agement meta information. We then want to show
how this new typology is effective for finding the
correct answer to a specific FU Q, in that for the
given FU Q it indicates the relevance and top-
ical relatedness of the question?s particular dia-
logue context. In Section 5.2 we have seen how
all PCA-based context classification features per-
form clearly better than a non-interactive baseline
model; more specifically, the top five PCs from
the PCAB scheme yield significantly better A2
ranking results than the PCAA scheme which does
not consider BoB dialogue management meta-data
features. Based on these results, we now look in
327
Model ID Interaction terms Mean rank Median rank Standard p (Paired p (Wilcoxon
correct A2 correct A2 dev. t-test) signed rank)
baseline none 48.72 14 69.35
PCAA PC1 + . . .+ PC5 44.25 12 64.58 < 0.0001 < 0.0001
PCAB PC1 + . . .+ PC5 42.72 12 62.53 0.0006 0.0087
PCAC PC1 + . . .+ PC5 42.87 12 62.94 not sig. not sig.
Table 1: Improving ranking of correct A2 (out of 306 answer candidates) with different PCA-based
interaction terms. Significance tests of rank differences wrt. result in preceding row.
more detail at the relevance of the top five PC fea-
tures in PCAB , and at their most important load-
ings, i.e., the original context classification fea-
tures that are most highly correlated with the value
of each particular PC. After running our predic-
tor elimination routine, the corresponding LRM
has kept three of these five top PCs as interaction
terms: PC1, PC2 and PC5. Table 2 describes the
top three positive and top three negative loadings
of these PCs. The table also shows how in model
PCAB , each of the interaction terms correspond-
ing to the three PCs influences the score that is cal-
culated for everyA2 candidate, either positively or
negatively.
Interpreting the results of Table 2 on a high,
dialogue-specific level, we draw the following
conclusions:
PC1 seems to capture a rather general distinc-
tion of topic shift versus topic continuation. A
FU Q with high lexical similarity to the preced-
ing utterances (i.e., a ?topic continuation?) should
preferably get an A2 with higher lexical similar-
ity with respect to both A1 and Q2. In this con-
text, ?topic shift? is partly described by a feature
from Centering Theory, and two of BoB?s dia-
logue management meta-data features.
PC2 shows relatively weak positive correlations
with any context classification features. On the
negative end, PC2 seems to describe a class of FU
Qs that are uttered after a Q1 that did neither con-
tinue nor exit a sub-dialogue. Also,A1 was a regu-
lar system answer (as opposed to an apology mes-
sage by BoB). Such FU Qs can thus be interpreted
as ?single shot? questions that a user poses after
their previous question was already dealt with in
A1. Because of the negative loadings, the value of
PC2 becomes negative, resulting in the avoidance
of any A2 that is highly similar to the preceding
A1.
PC5 distinguishes FU Qs that are mostly related
to the previous answer from those that are more
related to the previous question. Depending on
whether PC5 turns positive or negative, A2s are
preferred that are more similar to A1 or Q2, re-
spectively. Q1.Q2 similarity is determined by both
lexical similarity and Centering Theory features.
6 Conclusion
In this paper we have experimentally explored the
problem of FU Q types and their corresponding
answer identification strategies. The first result is
that our hand-annotated FU Q types did not sig-
nificantly improveQ2 answering performance (for
the annotated sub-set of 417 snippets). We at-
tribute this negative result in part to the difficulty
of the 4-level FU Q type annotation task. On the
other hand, we believe it is encouraging that with
purely automatic features for context classifica-
tion, combined through PCA, we significantly out-
performed our baseline. Adding BoB?s dialogue
management meta information ? which is also au-
tomatically available when using our dialogue col-
lection scheme ? for context classification helped
improve the scores even further. We analyzed the
top loadings of three PCs that our best-performing
LRM uses for FU Q type classification. We used
PCA both for circumventing the problem of mul-
ticollinear predictors in LRM, and as a diagnostic
tool to analyze the most important components of
automatically combined FU Q classification fea-
tures. Finally, a potentially difficult and cumber-
some manual annotation of the correctness of the
previous system answer A1 did not improve A2
ranking performance.
References
Alan Agresti. 2002. Categorical Data Analysis.
Wiley-Interscience, New York.
Raffaella Bernardi and Manuel Kirschner. 2010. From
328
LOADINGS
PC1 PC2 PC5
0.33 distsim.Q1.Q2 0.05 Q1.bob.contSubdial 0.45 distsim.A1.Q2
0.26 distsim.A1.Q2 0.04 Q2.center.roughSh 0.31 A1.bob.isApology
0.26 action.Q1.Q2 0.02 Q2.bob.breakSubdial 0.29 lexsim.A1.Q2
...
...
...
?0.13 A1.bob.isApology ?0.22 A1.bob.isAnswer ?0.18 lexsim.Q1.Q2
?0.15 Q2.bob.noSubdial ?0.30 Q2.bob.noSubdial ?0.23 Q2.center.cont
?0.22 Q2.center.roughSh ?0.31 Q1.bob.noSubdial ?0.26 A1.bob.isAnswer
INFLUENCE ON A2 SELECTION IN MODEL PCAB
pos for each A2 similar to Q2 pos for each A2 similar to A1 pos for each A2 similar to A1
pos for each A2 similar to A1 neg for each A2 similar to Q2
Table 2: Strongest loadings for the three PCs retained as interaction terms in Model PCAB , and indication
of each PC?s positive/negative influence on lexical similarity-based A2 selection features
artificial questions to real user interaction logs: Real
challenges for interactive question answering sys-
tems. In Proc. of Workshop on Web Logs and Ques-
tion Answering (WLQA?10), Valletta, Malta.
Raffaella Bernardi, Manuel Kirschner, and Zorana
Ratkovic. 2010. Context fusion: The role of dis-
course structure and centering theory. In Proceed-
ings of the Seventh conference on International Lan-
guage Resources and Evaluation (LREC?10), Val-
letta, Malta. European Language Resources Associ-
ation (ELRA).
Nu?ria Bertomeu, Hans Uszkoreit, Anette Frank, Hans-
Ulrich Krieger, and Brigitte Jo?rg. 2006. Contextual
phenomena and thematic relations in database QA
dialogues. In Proc. of the Interactive Question An-
swering Workshop at HLT-NAACL 2006, pages 1?8,
New York, NY.
Nuria Bertomeu. 2008. A Memory and Attention-
Based Approach to Fragment Resolution and its Ap-
plication in a Question Answering System. Ph.D.
thesis, Department of Computational Linguistics,
Saarland University.
Susan E. Brennan, Marilyn W. Friedman, and Carl J.
Pollard. 1987. A centering approach to pronouns.
In Proceedings of the 25th annual meeting on Asso-
ciation for Computational Linguistics, pages 155?
162, Stanford, California.
Joyce Y. Chai and Rong Jin. 2004. Discourse structure
for context question answering. In Proc. of the HLT-
NAACL 2004 Workshop on Pragmatics in Question
Answering, Boston, MA.
Marco De Boni and Suresh Manandhar. 2005. Im-
plementing clarification dialogues in open domain
question answering. Journal of Natural Language
Engineering, 11(4):343?361.
Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-
stein. 1995. Centering: A framework for model-
ing the local coherence of discourse. Computational
Linguistics, 21(2):203?225.
Manuel Kirschner, Raffaella Bernardi, Marco Baroni,
and Le Thanh Dinh. 2009. Analyzing Interactive
QA dialogues using Logistic Regression Models. In
Proc. of XIth International Conference of the Italian
Association for Artificial Intelligence (AI*IA), Reg-
gio Emilia, Italy.
Manuel Kirschner. 2010. The Structure of Real
User-System Dialogues in Interactive Question An-
swering. Ph.D. thesis, Free University of Bozen-
Bolzano, Italy.
Bryan F. J. Manly. 2004. Multivariate Statistical
Methods: A Primer. Chapman & Hall/CRC, 3rd
edition.
Zorana Ratkovic. 2009. Deep analysis in iqa: evalua-
tion on real users? dialogues. Master?s thesis, Euro-
pean Masters Program in Language and Communi-
cation Technologies.
Mingyu Sun and Joyce Y. Chai. 2007. Discourse pro-
cessing for context question answering based on lin-
guistic knowledge. Know.-Based Syst., 20(6):511?
526.
Boris van Schooten, R. op den Akker, R. Rosset,
O. Galibert, A. Max, and G. Illouz. 2009. Follow-
up question handling in the IMIX and Ritel systems:
A comparative study. Journal of Natural Language
Engineering, 15(1):97?118.
Fan Yang, Junlan Feng, and Giuseppe Di Fabbrizio.
2006. A data driven approach to relevancy recogni-
tion for contextual question answering. In Proc. of
the Interactive Question Answering Workshop at
HLT-NAACL 2006, pages 33?40, New York City,
NY.
329
Q1 where in the library can i find books
A1 Most of our holdings are accessible in the open-shelves area. In the repository we only keep
some older volumes of journals or the outsize stuff.
Q2 shelf mark
A2 Shelf marks are a combination of letters and numbers, which exactly define the location of a
book in a Library. If you find a book or a journal in the catalogue, you also see the shelf mark.
The shelf marks of the University Library consist of an alphanumeric code (e.g. ES 560 K19).
Would you like to know how you search them?
Q1 When can I return a book?
A1 Books and other media can be consigned at the information desk during the regular opening
hours of the Library. You can also leave them at the book drop located in front of the main
entrance of the university building - 24 hours a day.
Q2 I want to read the newspaper on the computer
A2 If you haven?t got a Library Card you can visit the Library to read books, journals or newspapers.
If you want to borrow something or use the other services we offer you should get a Library Card.
Do you want to know how it works?
Q1 i need the opac
A1 Here (. . . ) is the page with our OPAC for you. You can search the complete holdings of the
Library.
Q2 where is the advisory service
A2 The University?s Advisory Service provides an information service for prospective students who
would like to know more about which degree course to study. It also provides a support service
for enrolled students during their entire time at the University. You can get further information
from this (. . . ) site.
Table 3: Example dialogue snippets with correctly identified A2
Feature name Freq. Description
Q1.bob.contSubdial 7.6% Q1 continues system-initiated sub-dialogue
Q1.bob.breakSubdial 9.6% Q1 breaks out of system-initiated sub-dialogue
Q1.bob.noSubdial 82.9% BoB not in sub-dialogue mode when Q1 was uttered
Q2.bob.breakSubdial 13.6% Q2 breaks out of system-initiated sub-dialogue
Q2.bob.noSubdial 86.4% BoB not in sub-dialogue mode when Q2 was uttered
A1.bob.isAnswer 75.6% A1 is regular answer retrieved by BoB
A1.bob.isApology 24.4% A1 is apology message: BoB did not understand
Table 4: BoB dialogue management meta information. Proportions out of those 1,441 of total 1,522
snippets for which this information was logged.
Feature name Freq. Description
FUQtype=isTopicShift 40.0% (of 417) Q2 is topic shift
FUQtype=isRephrase 19.2% (of 417) Q2 is rephrasing of Q1
FUQtype=isContextDepentFUQ 6.5% (of 417) Q2 is context dependent
FUQtype=isFullySpecifiedFUQ 34.3% (of 417) Q2 is not context dependent
A1.isAnswer.correct 66.5% (of 1,179) BoB?s regular answer A1 is correct
A1.isAnswer.false 19.0% (of 1,179) BoB?s regular answer A1 is false
A1.isApology.correct 1.3% (of 1,179) BoB?s apology message A1 is correct
A1.isApology.false 13.2% (of 1,179) BoB?s apology message A1 is false
Table 5: Manual annotation meta information. Proportions out of those sub-sets of total 1,522 snippets
with available annotation.
330
Topic 
continuation
Topic shift
Related/
salient 
transition 
FU Q       
Rephrase
Context-
dependent
Fully 
specified
143
80
250
27
170
417
167
Figure 1: Manual FU Q type annotation scheme, with counts of FU Q types
Scatter Plot Matrix
PC1
2
4
6
2 4 6
-4
-2
0
-4 -2 0
PC2
2
4
6
2 4 6
-4
-2
0
-4 -2 0
PC3
0
2
4
0 2 4
-6
-4
-2
-6 -4 -2
PC4
2
4
6
2 4 6
-2
0
2
-2 0 2
PC5
0
2
4
0 2 4
-4
-2
0
-4 -2 0
FU Q types in 'context classification features' space
isContextDependent
isFullySpecified
isRephrase
isTopicShift
Figure 2: Distribution of hand-annotated FU Q types in PC-based feature space (PCAB)
331
Proceedings of the 2012 Student Research Workshop, pages 19?24,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Query classification using topic models and support vector machine
Dieu-Thu Le
University of Trento, Italy
dieuthu.le@disi.unitn.it
Raffaella Bernardi
University of Trento, Italy
bernardi@disi.unitn.it
Abstract
This paper describes a query classification
system for a specialized domain. We take as
a case study queries asked to a search engine
of an art, cultural and history library and clas-
sify them against the library cataloguing cate-
gories. We show how click-through links, i.e.,
the links that a user clicks after submitting a
query, can be exploited for extracting informa-
tion useful to enrich the query as well as for
creating the training set for a machine learn-
ing based classifier. Moreover, we show how
Topic Model can be exploited to further enrich
the query with hidden topics induced from the
library meta-data. The experimental evalua-
tions show that this system considerably out-
performs a matching and ranking classifica-
tion approach, where queries (and categories)
were also enriched with similar information.
1 Introduction
Query classification (QC) is the task of automati-
cally labeling user queries into a given target tax-
onomy. Providing query classification can help the
information providers understand users? needs based
on the categories that the users are searching for.
The main challenges of this task come from the na-
ture of user queries, which are usually very short and
ambiguous. Since queries contain only several to a
dozen words, a QC system often requires either a
rather large training set or an enrichment of queries
with other information (Shen et al, 2006a), (Broder
et al, 2007).
This study will focus on QC in art, culture and
history domain, using the Bridgeman art library1, al-
though our framework is general enough to be used
in different domains. Manually creating a training
1http://www.bridgemanart.com/
set of queries to build a classifier in a specific do-
main is very time-consuming. In this study, we
will describe our method of automatically creating
a training set based on the click-through links and
how we build an SVM (Support Vector Machine)
classifier with the integration of enriched informa-
tion. In (Le et al, 2011), it has been shown that
click-through information and topic models are use-
ful for query enrichment when the ultimate goal is
query classification. We will follow this enrichment
step, but integrate this information into a SVM clas-
sifier instead of using matching and ranking between
queries and categories as in (Le et al, 2011).
The purpose of this paper is to determine (1)
whether the query enrichment with click-though in-
formation and hidden topics is useful for a machine
learning query classification system using SVM; and
(2) whether integrating this enriched information
into a machine learning classifier can perform bet-
ter than the matching and ranking system.
In the next section, we will briefly review the
main streams of related work in QC. In section 3,
we will describe the Bridgeman art library. Sec-
tion 4 accounts for our proposed query classifica-
tion framework. In section 5, we will present our
experiment and evaluation. Section 6 concludes by
discussing our main achievements and proposing fu-
ture work.
2 Related work
Initial studies in QC classify queries into several
different types based on the information needed by
the user. (Broder, 2002) considered three different
types of queries: informational queries, navigational
queries and transactional queries. This stream of
study focuses on the type of the queries, rather than
topical classification of the queries.
Another stream of work deals with the problem
19
of classifying queries into a more complex taxon-
omy containing different topics. Our study falls into
this second stream. To classify queries consider-
ing their meaning, some work considered only in-
formation available in queries (e.g., (Beitzel et al,
2005) only used terms in queries). Some other work
has attempted to enrich queries with information
from external online dataset, e.g., web pages (Shen
et al, 2006a; Broder et al, 2007) and web direc-
tories (Shen et al, 2006b). Our work is similar
to their in the idea of exploiting additional dataset.
However, instead of using search engines as a way
of collecting relevant documents, we use the meta-
data of the library itself as a reference set. Further-
more, we employ topic models to analyze topics for
queries, rather than enriching queries with words se-
lected from those webpages directly as in (Shen et
al., 2006a; Broder et al, 2007).
The context of a given query can provide use-
ful information to determine its categories. Previ-
ous studies have confirmed the importance of search
context in QC. (Cao et al, 2009) considered the con-
text to be both previous queries within the same ses-
sion and pages of the clicked urls. In our approach,
we will also consider click through information to
enrich the queries and analyze topics.
In (Le et al, 2011), queries and categories are en-
riched with both information mined from the click-
through links as well as topics derived from a topic
model estimated from the library metadata. Sub-
sequently, the queries are mapped to the categories
based on their cosine similarity. Our proposed ap-
proach differs from (Le et al, 2011) in three re-
spects: (1) we enrich the queries, but not the cat-
egories (2) we employ a machine learning system
and integrate this enriched information as features to
learn an SVM classifier (3) we assume that the cate-
gory of a query is closely related to the category of
the corresponding click-through link, hence we au-
tomatically create a training data for the SVM clas-
sifier by analyzing the query log.
3 Bridgeman Art Library
Bridgeman Art Library (BAL)2 is one of the world?s
top image libraries for art, culture and history. It
contains images from over 8,000 collections and
2http://www.bridgemanart.com
more than 29,000 artists, providing a central source
of fine art for image users.
Works of art in the library have been annotated
with titles and keywords. Some of them are catego-
rized into a two-level taxonomy, a more fine-grained
classification of the Bridgeman browse menu. In our
study, we do not use the image itself but only the in-
formation associated with it, i.e., the title, keywords
and categories. We will take the 55 top-level cate-
gories from this taxonomy, which have been orga-
nized by a domain expert, as our target taxonomy.
4 Building QC using topic models and
SVM
Following (Le et al, 2011), we enrich queries both
with the information mined from the library via
click-through links and the information collected
from the library metadata via topic modeling. To
perform the query enrichment with topics derived
from the library metadata, there are several impor-
tant steps:
? Collecting and organizing the library metadata as
a reference set: the library metadata contains the in-
formation about artworks that have been annotated
by experts. To take advantage of this information
automatically, we collected all annotated artworks
and organized them by their given categories.
? Estimating a topic model for this reference set:
This step is performed using hidden topic analysis
models. In this framework, we choose to use latent
dirichlet alocation, LDA (Blei et al, 2003b).
? Analyzing topics for queries and integrating topics
into data for both the training set and new queries:
After the reference set has been analyzed using topic
models, it will be used to infer topics for queries.
The topic model will then be integrated into the data
to build a classifier.
4.1 Query enrichment via click-through links
We automatically extracted click-through links from
the query log (which provides us with the title of
the image that the user clicks) to enrich the query,
represented as a vector ??qi , with the title of one
randomly-chosen click-through associated with it.
To further exploit the click-through link, we find
the corresponding artwork and extract its keywords:
??qi ?
??
ti ?
???
kwi, where
??
ti ,
???
kwi are the vectors of words
20
in the title and keywords respectively.
4.2 Hidden Topic Models
The underlying idea is based upon a probabilis-
tic procedure of generating a new set of artworks,
where each set refers to titles and keywords of
all artworks in a category: First, each set ??wm
= (wm,n)
Nm
n=1 is generated by sampling a distribu-
tion over topics
??
?m from a Dirichlet distribution
(Dir(??? )), where Nm is the number of words in
that set m. After that, the topic assignment for
each observed word wm,n is performed by sam-
pling a word place holder zm,n from a multino-
mial distribution (Mult(
??
?m)). Then a word wm,n is
picked by sampling from the multinomial distribu-
tion (Mult(??? zm,n)). This process is repeated until
all K topics have been generated for the whole col-
lection.
Table 1: Generation process for LDA
?M : the total number of artwork sets
?K: the number of (hidden/latent) topics
? V : vocabulary size
? ??? ,
??
? : Dirichlet parameters
?
??
?m: topic distribution for document m
? ??? k: word distribution for topic k
? Nm: the length of document m
? zm,n: topic index of nth word in document m
? wm,n: a particular word for word placeholder [m, n]
? ? = {
??
?m}Mm=1: a M ?K matrix
? ? = {??? k}Kk=1: a K ? V matrix
In order to estimate parameters for LDA (i.e.,
the set of topics and their word probabilities ?
and the particular topic mixture of each document
?), different inference techniques can be used,
such as variational Bayes (Blei et al, 2003b), or
Gibbs sampling (Heinrich, 2004). In this work,
we will use Gibbs sampling following the descrip-
tion given in (Heinrich, 2004). Generally, the topic
assignment of a particular word t is computed as:
p(zi=k|
??z ?i,
??w)=
n(t)k,?i + ?t
[
?V
v=1 n
(v)
k +?v]?1
n(k)m,?i + ?k
[
?K
j=1 n
(j)
m +?j ]?1
(1)
where n(t)k,?i is the number of times the word t is
assigned to topic k except the current assignment;
?V
v=1 n
(v)
k ?1 is the total number of words assigned
to topic k except the current assignment; n(k)m,?i is the
number of words in set m assigned to topic k except
the current assignment; and
?K
j=1 n
(j)
m ? 1 is the
total number of words in set m except the current
word t. In normal cases, Dirichlet parameters ??? ,
and
??
? are symmetric, that is, all ?k (k = 1..K) are
the same, and similarly for ?v (v = 1..V ).
4.3 Hidden topic analysis of the Bridgeman
metadata
The Bridgeman metadata contains information
about artworks in the library that have been anno-
tated by the librarians. We extracted titles and key-
words of each artwork, those for which we had a
query with a click-through link corresponding to it,
and grouped them together by their sub-categories.
Each group is considered as a document ??wm =
(wm,n)
Nm
n=1, with the number of total documents M
= 732 and the vocabulary size V = 136K words. In
this experiment, we fix the number of topics K =
100. We used the GibbsLDA++ implementation3 to
estimate this topic model.
4.4 Building query classifier with hidden topics
Let Q? = {??qi ?}i=Ni=1 be the set of all queries en-
riched via the click-through links, where each en-
riched query is ??qi ? =
??qi ?
??
ti ?
???
kwi. We also per-
formed Gibbs sampling for all ??qi ? in order to esti-
mate its topic distribution
??
? i = {?i,1, . . . , ?i,K}
where the probability ?i,k of topic k in
??qi ? is com-
puted as:
?i,k =
n(k)i + ?k
?K
j=1 n
(j)
i + ?j
(2)
where n(k)i is the number of words in query i as-
signed to topic k and n(j)i is the total number of
words appearing in the enriched query i.
In order to integrate the topic distribution
??
?i =
{?i,1, . . . , ?i,K} into the vector of words
??qi ?
= {wi,1, wi,2, . . . , wi,Ni}, following (Phan et al,
2010), we only keep topics whose ?i,k is larger than
a threshold cut-off and use a scale parameter to do
the discretization for topics: the number of times
topic k integrated to ??qi ? is round(?i? scale). After
that, we build a Support Vector Machine classifier
using SVM light V2.204.
3http://gibbslda.sourceforge.net/
4http://svmlight.joachims.org/
21
5 Evaluation
In this section, we will describe our training set, gold
standard and the performance of our system in com-
parison with the one in (Le et al, 2011).
5.1 Training set
Manually annotating queries to create a training set
in this domain is a difficult task (e.g., it requires the
expert to search the query and look at the picture cor-
responding to the query, etc.). Therefore, we have
automatically generated a training set by exploiting
a 6-month query log as follow.
First, each query has been mapped to its click-
through information to extract the sub-category as-
sociated to the corresponding image. Then, from
this sub-category, we obtained its corresponding
top-cateogry (among the 55 we consider) as defined
in BAL taxonomy. The distribution of queries in
different categories varies quite a lot among the 55
target categories reflecting the artwork distribution
(e.g., there are many more artworks in the library be-
longing to the category ?Religion and Belief? than
to the category?Costume and Fashion?). We have
preserved such distribution over the target categories
when selecting randomly the 15,490 queries to build
our training set. After removing all punctuations and
stop words, we obtained a training set containing
50,337 words in total. Each word in this set serves
as a feature for the SVM classifier.
5.2 Test set
We used the test set of 1,049 queries used in (Le et
al., 2011), which is separate from the training set.
These queries have been manually annotated by a
BAL expert (up to 3 categories per query). Note that
these queries have also been selected automatically
while preserving the distribution over the target cat-
egories observed in the 6-month query log. We call
this the ?manual? gold standard. In addition, we also
made use of another gold standard obtained by map-
ping the click-through information of these queries
with their categories, similar to the way in which we
obtain the training set. We call this the ?via-CT?
gold standard.
5.3 Experimental settings
To evaluate the impact of click-though information
and topics in the classifier, we designed the follow-
ing experiments, where QR is the method without
any enrichment andQR-CT -HT is with the enrich-
ment via both click-through and hidden topics.
Setting Query enrichment
QR ??q
QR-HT ??q ?HT
QR-CT ??q ? = ??q +
??
t +
??
kw
QR-CT -HT ??q ? ?HT
? ??q : query
? ??q ?: query enriched with click-through information
?
??
t : click-through image?s title
?
??
kw: click-through image?s keywords
?HT : hidden topics from Bridgeman metadata
Table 2: Experimental Setting
Setting
Hits
Manual GS via-CT
# 1 # 2 # 3
?
Top 3 GS
QR 207 80 24 311 231
QR-HT 212 81 25 318 235
QR - CT 243 107 38 388 266
QR - CT - HT 289 136 49 474 323
Table 3: Results of query classification: number of cor-
rect categories found (for 1,049 queries)
Figure 1: The impact of click-through information with
matching-ranking (mr) and our approach (svm)
To answer our first research question, namely
whether click-through information and hidden top-
ics are useful for this query classifier, we examine
the number of correct categories found by the classi-
fier built both with and without the enrichment. The
results of the experiment are reported in Table 3. As
can be seen from the table, we notice that the click-
through information plays an important role. In par-
22
ticular, it increases the number of correct categories
found from 311 to 388 (compared with the manual
GS) and from 231 to 266 (using the via-CT GS).
To answer our second research question, namely
whether integrating the enriched information into a
machine learning classifier can perform better than
the matching and ranking method, we also compare
the results of our approach with the one in (Le et
al., 2011). Figure 1 shows the impact of the click-
through information for the SVM classifier (svm) in
comparison with the matching and ranking approach
(mr). Figure 2 shows the impact of the hidden topics
in both cases. We can see that in both cases our clas-
sifier outperforms the matching-ranking one con-
siderably (e.g., from 183 to 388 correct categories
found in the QR-CT-HT method).
Figure 2: The impact of hidden topics with matching-
ranking (mr) and our approach (svm)
However, in the case where we use only queries
without click-through information, we can see that
hidden topics do not bring a very strong impact (the
number of correct categories found only slightly in-
creases by 7 - using the ?manual? gold standard).
The result might come from the fact that this topic
model was built from the metadata, using only click-
through information, but has not been learned with
queries.
6 Conclusion
In this study, we have presented a machine learn-
ing classifier for query classification in an art im-
age archive. Since queries are usually very short,
thus difficult to classify, we first extend them with
their click-through information. Then, these queries
are further enriched with topics learned from the
BAL metadata following (Le et al, 2011). The re-
sult from this study has confirmed again the effect
of click-through information and hidden topics in
the query classification task using SVM. We have
also described our method of automatically creat-
ing a training set based on the selection of queries
mapped to the click-through links and their corre-
sponding available categories using a 6-month query
log. The result of this study has shown a consid-
erable increase in the performance of this approach
over the matching-ranking system reported in (Le et
al., 2011).
7 Future work
For future work, we are in the process of enhancing
our experimentation in several directions:
Considering more than one click-through image
per query: In this work, we have considered only
one category per query to create the training set,
while it might be more reasonable to take into ac-
count all click-through images of a given query. In
future work, we plan to enrich the queries with either
all click-through images or with the most relevant
one instead of randomly picking one click-through
image. In many cases, a click-through link is not
necessarily related to the meaning of a query (e.g.,
when users just randomly click on an image that they
find interesting). Thus, it might be useful to filter out
those click-through images that are not relevant.
Enriching queries with top hits returned by the
BAL search engine: In the query logs, there are
many queries that do not have an associated click-
through link. Hence, we plan to exploit other en-
richment method that do not rely on those links, in
particular we will try to exploit the information com-
ing from the top returned hits given by the library
search engine.
Analyzing queries in the same session: It has been
shown in some studies (Cao et al, 2009) that analyz-
ing queries in the same session can help determine
their categories. Our next step is to enrich a new
query with the information coming from the other
previous queries in the same session.
Optimizing LDA hyperparameters and topic
number selection: Currently, we fixed the num-
ber of topics K = 100, the Dirichlet hyperparame-
ters ? = 50/K = 0.5 and ? = 0.1 as in (Griffiths and
23
Steyvers, 2004). In the future, we will explore ways
to optimize these input values to see the effect of dif-
ferent topic models in our query classification task.
Exploiting visual features from the BAL images:
The BAL dataset provides an interesting case study
in which we plan to further analyze images to enrich
queries with their visual features. Combining text
and visual features has drawn a lot of attention in the
IR research community. We believe that exploiting
visual features from this art archive could lead to in-
teresting results in this specific domain. A possible
approach would be extracting visual features from
the click-through images and representing them to-
gether with textual features in a joint topic distribu-
tion (e.g., (Blei et al, 2003a; Li et al, 2010)).
Comparing system with other approaches: In the
future, we plan to compare our system with other
query classification systems and similar techniques
for query expansion in general. Furthermore, the
evaluation phase has not been carried out thoroughly
since it was difficult to compare the one-class output
with the gold-standard, where the number of correct
categories per query is not fixed. In the future, we
plan to exploit the output of our multi-class classi-
fier to assign up to three categories for each query
and compute the precision at n.
Acknowledgments
This work has been partially supported by the
GALATEAS project (http://www.galateas.eu/ ?
CIP-ICT PSP-2009-3-25430) funded by the Euro-
pean Union under the ICT PSP program.
References
Steven M. Beitzel, Eric C. Jensen, Ophir Frieder, and
David Grossman. 2005. Automatic web query clas-
sification using labeled and unlabeled training data. In
In Proceedings of the 28th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 581?582. ACM Press.
David M. Blei, Michael I, David M. Blei, and Michael I.
2003a. Modeling annotated data. In In Proc. of the
26th Intl. ACM SIGIR Conference.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003b. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Andrei Z. Broder, Marcus Fontoura, Evgeniy
Gabrilovich, Amruta Joshi, Vanja Josifovski, and
Tong Zhang. 2007. Robust classification of rare
queries using web knowledge. In Proceedings of the
30th annual international ACM SIGIR conference on
Research and development in information retrieval,
SIGIR ?07, pages 231?238, New York, NY, USA.
ACM.
Andrei Broder. 2002. A taxonomy of web search. SIGIR
Forum, 36:3?10, September.
Huanhuan Cao, Derek Hao Hu, Dou Shen, Daxi Jiang,
Jian-Tao Sun, Enhong Chen, and Qiang Yang. 2009.
Context-aware query classification. In SIGIR?09, The
32nd Annual ACM SIGIR Conference.
Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of America,
101 Suppl 1(Suppl 1):5228?5235.
Gregor Heinrich. 2004. Parameter estimation for text
analysis. Technical report.
Dieu-Thu Le, Raffaella Bernardi, and Edwin Vald. 2011.
Query classification via topic models for an art im-
age archive. In Recent Advances in Natural Language
Processing, RANLP, Bulgaria.
Li-Jia Li, Chong Wang, Yongwhan Lim, David Blei, and
Li Fei-Fei. 2010. Building and using a semantivisual
image hierarchy. In The Twenty-Third IEEE Confer-
ence on Computer Vision and Pattern Recognition, San
Francisco, CA, June.
Xuan-Hieu Phan, Cam-Tu Nguyen, Dieu-Thu Le, Le-
Minh Nguyen, Susumu Horiguchi, and Quang-Thuy
Ha. 2010. A hidden topic-based framework towards
building applications with short web documents. IEEE
Transactions on Knowledge and Data Engineering,
99(PrePrints).
Dou Shen, Rong Pan, Jian-Tao Sun, Jeffrey Junfeng Pan,
Kangheng Wu, Jie Yin, and Giang Yang. 2006a.
Query enrichment for web-query classification. ACM
Transactions on Information Systems, 24(3):320?352.
Dou Shen, Jian-Tao Sun, Qiang Yang, and Zheng Chen.
2006b. Building bridges for web query classification.
In SIGIR?06.
24
Proceedings of the 25th International Conference on Computational Linguistics, pages 17?24,
Dublin, Ireland, August 23-29 2014.
TUHOI: Trento Universal Human Object Interaction Dataset
Dieu-Thu Le
DISI, University of Trento
Povo, 38123, Italy
dle@disi.unitn.it
Jasper Uijlings
University of Trento, Italy
University of Edinburgh, Scotland
jrr.uijlings@ed.ac.uk
Raffaella Bernardi
DISI, University of Trento
Povo, 38123, Italy
bernardi@disi.unitn.it
Abstract
This paper describes the Trento Universal Human Object Interaction dataset, TUHOI, which is
dedicated to human object interactions in images.
1
Recognizing human actions is an important
yet challenging task. Most available datasets in this field are limited in numbers of actions and
objects. A large dataset with various actions and human object interactions is needed for training
and evaluating complicated and robust human action recognition systems, especially systems
that combine knowledge learned from language and vision. We introduce an image collection
with more than two thousand actions which have been annotated through crowdsourcing. We
review publicly available datasets, describe the annotation process of our image collection and
some statistics of this dataset. Finally, experimental results on the dataset including human action
recognition based on objects and an analysis of the relation between human-object positions in
images and prepositions in language are presented.
1 Introduction
Visual action recognition is generally studied on datasets with a limited number of predefined actions
represented in many training images or videos (Ikizler et al., 2008; Delaitre et al., 2011; Yao and Li,
2010; Yao et al., 2011). Common methods using holistic image or video representation such as Bag-
of-Words have achieved successful results in retrieval settings (Ayache and Quenot, 2008). Though
these predefined lists of actions are good for many computer vision problems, this cannot work when
one wants to recognize all possible actions. Firstly, the same action can be phrased in several ways.
Secondly, the number of actions that such systems would have to recognize in real life data is huge: the
number of possible interactions with all possible objects is bounded by the cartesian product of numbers
of verbs and objects. Therefore, the task of collecting images or videos of each individual action becomes
infeasible with this growing number. By necessity this means that for some actions only few examples
will be available. In this paper we want to enable studies in the direction of recognizing all possible
actions, for which we provide a new, suitable human-object interaction dataset.
A human action can be defined as a human, object, and the relation between them. Therefore, an action
is naturally recognized through its individual components. Recent advances in computer vision have led
to reasonable accuracy for object and human recognition, which makes recognizing the components
feasible. Additionally, language can help determining how components are combined. Furthermore,
the relative position between human and object can be used to disambiguate different human actions.
Perhaps prepositions in natural language can be linked to this relative position between the object and
human (e.g., step out of a car). To transfer this knowledge from language to vision, it is important that
the distribution of the visual actions are sampled similarly as the language data. This requirement is
fulfilled when the action frequencies in the dataset mirror the frequencies in which they occur in real life.
To sum up, we aim at building an image dataset which can (1) capture the distribution of human
interactions with objects in reality (if an action is more common that the other actions, that action is
also observed more frequently in the dataset than the others), (2) provide different ways of describing
1
Our dataset is available to download at http://disi.unitn.it/ dle/dataset/TUHOI.html
This work is licensed under a Creative Commons Attribution 4.0 International Licence. License details:
http://creativecommons.org/licenses/by/4.0/
17
an action for each image (there are many actions that can be phrased in several ways, for example: fix a
bike or repair a bike), (3) help with identifying different verb meanings (for example, the word ?riding?
has different implications for ?riding a horse?, ?riding a car?, and ?riding a skateboard?).
2 Available image datasets for human action recognition
A common approach to human action recognition is to exploit visual features using bag-of-features or
part-based representation and treat action recognition as a general classification problem (Delaitre et al.,
2010; Yao and Li, 2010; Wang et al., ; Laptev, 2005). For common actions, it has been shown that
learning the joint appearance of the human-object interaction can be beneficial (Sadeghi and Farhadi,
2011). Other studies recognize actions by their components such as objects, human poses, scenes (Gupta
et al., 2009; Yao et al., 2011): (Yao et al., 2011) jointly models attributes and parts, where attributes
are verbs and parts are objects and local body parts. These studies rely on suitable training data for
a set of predefined actions: (Gupta et al., 2009) tests on a 6 sport action dataset, (Yao and Li, 2010)
attempts to distinguish images where a human plays a musical instrument from images where he/she
does not, (Delaitre et al., 2010) classifies images to one of the seven every day actions, and (Yao et al.,
2011) introduces a dataset containing 40 human actions. Most of these datasets were obtained using web
search results such as Google, Bing, Flickr, etc. The number of images varies from 300 to more than 9K
images. A comparison of the publicly available datasets with respect to the number of actions and their
related objects is given in Table 1.
Dataset #images #objects #actions Examples of actions
Ikirler (Ikizler et al., 2008) 467 0 6 running, walking, throwing, crouching and kicking
Willow (Delaitre et al., 2011) 968 5 7 interaction with computer, photographing, riding bike
Sport dataset (Gupta et al., 2009) 300 4 6 tennis-forehand, tennis-serve, cricket bowling
Stanford 40 (Yao et al., 2011) 9532 31 40 ride horse, row boat, ride bike, cut vegetables
PPMI (Yao and Li, 2010) 4800 7 7 play violin, play guitar, play flute, play french horn
PASCAL (Everingham et al., 2012) 1221 6 10 jumping, playing instrument, riding horse
89 action dataset (Le et al., 2013) 2038 19 89 drive bus, sail boat, ride bike, fix bike, watch TV
TUHOI dataset 10805 189 2974 sit on chair, use computer, ride horse, play with dog
Table 1: A comparison of available human action datasets in terms of number of objects and actions
As can be seen in Table 1, the Stanford 40 action dataset contains quite a big number of images with
40 different actions. This dataset is good for visually training action recognizers since there are enough
images collected for each actions divided into training and test sets. There are some dataset in which
human action does not involved any object, these actions are for instance running, walking, or actions
where objects are not specified such as catching, throwing. These types of actions are not the target
domain of our dataset. We aim at recognizing the human object interactions based on objects. With the
same object, some actions are also more common than other actions: for example, sitting on a chair is
more commonly observed than standing on a chair. We want to capture such information in our dataset
which can reflect the human action distributions on common objects, aiming to sample human actions
related to objects in the visual world. Furthermore, how actions can be phrased in different ways, or how
verbs can have different meanings when interacting with different objects should also be considered.
Some actions can only be performed on some particular objects and are not applicable to some other
objects: a person can ride a horse, ride a bike, can feed a horse, but cannot feed a bike. This problem
of ambiguity and different word uses have been widely studied in computational linguistics, but have
received little attention from the computer vision community.
With the aim of creating a dataset that covers these requirements, we collect our dataset starting from
images where humans and objects co-occur together and define the actions we observe in each image
instead of collecting images for some predefined human actions. This way of annotating actions in
images is more natural and helps creating a more realistic dataset with various human actions that can
occur in images generally.
Recently, some good works attempted to generate descriptive sentences from images (Farhadi et al.,
2010; Kulkarni et al., 2011). In our dataset we focus on human actions, which, if present, are often the
main topic of interest within an image. As such, our dataset can be used as an important stepping stone
18
for generating full image descriptions as it allows for more rigorous evaluation than free-form text.
3 TUHOI, the new human action dataset
ImageNet is a hierarchical image database built upon the WordNet structure. The DET dataset in the
ImageNet large scale object recognition challenge 2013
2
contains 200 objects for training and evaluation.
With the idea of starting from images with humans and common objects, we chose to use this DET dataset
as a starting point to build our human action data.
3.1 The DET dataset: Object categories and labels
The 200 objects in the DET dataset are general, basic-level categories (e.g., monitor, waffle iron, sofa,
spatula, starfish). Each object corresponds to a synset (set of synonymous nouns) in WordNet. The
DET training set consists of single topic images where only the target object is annotated. As such,
most images only contain primarily the object of interest and few actions. It is good for learning object
classifiers but is not suitable for learning action recognition. In contrast, the validation dataset contains
various images where all object instances are annotated with a bounding box. Many of these images
contain actions. Therefore we start the annotation from the validation set.
Dataset #images #images #object #instances/object #?person?
having ?person? instances (min-max-median) instances
Training 395,909 9,877 345,854 438 - 73,799 - 660 18,258
Validation 20,121 5,791 55,502 31 - 12,823 - 111 12,823
Table 2: The statistics of the DET dataset
As can be seen in Table 2, there are 15,668 images having human and 31,081 human instances in
these images. We select only images having human since we want to annotate this dataset with human
object interactions. Objects related to clothes such as bathing cap, miniskirt, tie, etc. are not interesting
for human actions (most of the time, the action associated with these objects is ?to wear?). Therefore,
we excluded all these objects from the list of 200 objects above, which are: bathing cap, bow tie, bow,
brassiere, hat with a wide brim, helmet, maillot, miniskirt, neck brace, sunglasses, tie.
3.2 Human action annotation
Goal Our goal is to annotate these selected images containing humans and objects with their interac-
tions. Each human action is required to be associated with at least one of the given 200 object categories.
We used the Crowdflower, a crowdsourcing service for annotating these images. The Crowdflower anno-
tators are required to be English native speakers and they can use any vocabulary to describe the actions
as they wish. Every action is composed of a verb and an object (possibly with a preposition).
Annotation guideline For each image, given all object instances appearing in that image (together
with their bounding boxes), the annotator has been be asked to assign all human actions associated to
each of the object instance in the image (where ?no action? is also possible). Every human actions need
to have as object one of the object instances given in that image. For example, if the image has a bike
and a dog, the annotator will assign every human actions associated to ?bike? and ?dog?. Every image
has been annotated by at least 3 annotators, so that each action in the image can be described differently
by different people. Some examples of annotated images in our dataset are given in Figure 1.
3.3 Results of the annotation and some statistics
In total, there are 10,805 images, which have been annotated with 58,808 actions, of which 6,826 times
it has been annotated with ?no action? (11.6%). On average, there are 4.8 actions annotated for each
image (excluding ?no action?), of which there are 1.97 unique action/image. Some other statistics of the
dataset are given in Table 3: The number of unique verbs per object ranges from 1 (starfish, otter) to 158
(dog). As dogs occur very often in this image dataset (4,671 times), the number of actions associated to
it is also larger than other objects.
2
http://www.image-net.org/challenges/LSVRC/2013/
19
Figure 1: Examples of annotated images: Left: (1) play ping-pong, hold racket; (2) use laptop, hold computer mouse; (3)
use microphone, play accordion, play guitar, play violin; (4) talk on microphone, sit on sofa, pour pitcher; (5) play trombone;
(6) eat/suck popsicle; (7) listen/use/hear stethoscope; (8) ride bicycle, wear backpack; (9) swing/hold racket, hit tennis ball;
Right: (1) sit on chair, play violin; (2) wear diaper, sit on chair, squeeze/apply cream; (3) sit on chair, play cello; (4) hold/shake
maraca; (5) ride watercraft, wear swimming trunks; (6) cook/use stove, stir mushroom, hold spatula; (7) drive/row watercraft;
(8) sit on chair, pet dog, lay on sofa; (9) click/type on computer keyboard
Number of unique actions (verb + object): 2,974 actions
Number of unique verbs: 860 verbs
Verbs that are used most frequently (verb (#occurrences)): play (13043), hold (7731), ride (4765), sit (3535), sit on (1501)
drive (1491), wear (1441), eat (1175), hit (1168), pet (970), use (897), walk (787), stand (756)
touch (509), carry (507), blow (384), sail (323), kick (297), lead (290), throw (246), strum (239)
stand on (223), run (223)
Verbs that are used least frequently (occur only once): dirty, swing over, twist, beats, walks, ay, curl
face, shit, sail in, n?, see by, forge, draw, tag10, sling, rides, walk across, no image available, waving
drag, award, preform, strumb, died, land, unload, tricks, cooked, time, fasten, fall over, holed, leap over, pull up
Objects go with the largest number of verbs (object (#unique verbs)): dog (158), car (80), table (79), watercraft (68)
horizontal bar (56), chair (54), cart (52), whale (50), bicycle (48), cattle (42), soccer ball (41), balance beam (38)
band aid (38), motorcycle (37), flower pot (35), ladle (35), guitar (35), horse (35), ski (34), bus (34)
Objects that go with the least number of verbs (object (#unique verbs)): milk can (5), pitcher (5), scorpion (4), bear (4),
pretzel (4), sheep (4), frog (4), mushroom (4), printer (4), pineapple (4), ruler (3), guacamole (3), isopod (3), chime (3),
plate (rack (3), strawberry (3), porcupine (3), ant (3), toaster (3), bagel (3), jellyfish (3), dragonfly (2), lion (2), zebra (2),
goldfish (2), hamster (2), fig (2), squirrel (2), bee (2), centipede (2), koala (bear (2), snail (2), pomegranate (2), armadillo
(2), otter (1), starfish (1)
Table 3: Some statistics of the human action dataset
For some images, the annotators find many different ways to describe the action in the image. In our
data, a set of images was selected to be annotated by more than three people in order to facilitate sanity
checks. An example of such image which has been annotated by many people is given in Figure 2.
The annotators have found many verbs to describe the action: feeding, leading, running with, touching,
giving a treat to, etc.
Splitting training and test set For each object in our human action dataset, we split half of the images
for training and the other half is used for testing. The splitting process is done such that actions that
occur in test set also occur in training set to guarantee that the training set contains at least one image for
each action occurring in the test set.
Figure 2: Many different ways to describe an action in an image
Evaluating human action classification in our dataset To evaluate the performance of the human
action classification on this dataset, we use two different measurements: the accuracy and the traditional
20
precision, recall and F1 score. The accuracy reflects the percentage of predictions that are correct. We
calculate within how many images, the classifier assigns the correct actions for a given object i:
Accuracy
i
=
number of images that the classifier predicts correctly
total number of images
(1)
If the output of the classifier is one of the three annotated actions by human, then the action predicted
is considered to be correct. The accuracy of the whole system is the average accuracy over all objects,
with n is the total number of objects.
Accuracy =
?
n
i=1
Accuracy
i
n
(2)
This metric gives us the general performance of the system and easy to interpret. However, it gives
higher weights to actions that occur more often in the dataset. For example, if there are many actions
?ride bike? occurring in the dataset, the accuracy of the whole system depends mostly on the performance
of the class ?ride bike?. For actions that occur more rarely such as ?fix bike?, then the accuracy of the
class ?fix bike? will have little effect to the accuracy of the whole system.
To better analyze the results of the system and evaluate each action individually, we use the precision,
recall and F1 score for each class in the classifier. More specifically, as this classifier is the multi-class
classifier, these metrics are computed using a confusion matrix:
Precision
i
=
M
ii
?
j
M
ji
;Recall
i
=
M
ii
?
j
M
ij
(3)
where M
ij
is the value of the row i, column j in the confusion matrix. The confusion matrix is oriented
such that a given row of the matrix corresponds to the value of the ?truth?, i.e., correct actions assigned
by human, and a given column corresponds to the value of action assigned by the classifier. Finally, the
precision, recall and F1 score of the whole system are calculated as the average score over all actions.
4 Experiments
In this part, we use our newly collected dataset for building a general human action classifier based
on objects. We analyze the relative positions between humans and objects in each image and use this
information to help classifying human actions. Finally, we discuss the relations between human-object
positions with prepositions that are used in language for describing human actions.
4.1 Classifying human actions based on human-object positions
In this experiment, we used Forest Random classification method to classify an image to an action given
an object. The features used for this classifier are positions of the object and the person appearing in that
image. We compare this classifier when using position with a classifier using no position information to
see whether position information helps in classifying human actions and in which cases.
Extracting features To extract the features of objects and persons? positions in the images, we take
the bounding box of the first object instance annotated in that image. There are images with more than
one object instance (for example, there are several ?bike? in an image, so we do not know what ?bike?
we are talking about). We use the four coordinates of the bounding boxes of the object and person in the
image as features for the classifier.
Results of the classifier To compare whether position information can help in recognizing actions or
not, we design a naive classifier which learns from the probability of a verb given an object to assign an
action for each image from the training image dataset.
Accuracy Precision Recall F1
Without position 74.2% 0.40 0.26 0.29
With position 72.1% 0.65 0.29 0.36
Table 4: Results of the classifier with and without position information
21
Object Without position With position Object Without position With position
baseball 0.36 0.52 bus 0.57 0.73
face powder 0.33 1 hair spray 0.73 0.74
harmonica 0.07 0.97 horizontal bar 0.42 0.45
hotdog 0.29 0.57 motorcycle 0.80 0.82
turtle 0.43 0.71 water bottle 0.56 0.65
Table 5: Objects with higher accuracy when using position information
The results of the systems with and without position are report in Table 4. It shows that the accuracy of
the classifier without position is higher than when including the position (74.2% in compared to 72.1%).
However the precision, recall and F1 of the classifier using position are all higher than without position.
It?s due to the fact that the classifier without position blindly assigns each image to the most probable
action (i.e., actions that occurs most often with a given object learned from the training set), so it obtains
better overall accuracy when testing on all images. However, for other possible actions, this classifier
is unable to disambiguate actions and the performance of this classifier on less frequent actions is worst
than when including position information into the classifier. Generally, when taking into account all
possible actions, the position-based classifier has better average precision, recall and F1 score (28.6%
without position in compared with 35.8% using position).
To further analyze which objects and actions, the position information helps better, we compare the
accuracy of each individual objects. Table 5 reports main objects that have higher accuracy when using
position. We want to be able to predict which kind of actions that positions will help in recognizing them
through the knowledge we learn from language. This prediction will help us to learn how to include the
position information inside our human action recognizer since not all actions can be disambiguated by
positions. We divide the actions into two groups: one group for which we found position information
increase the classification results. Another group for which we found position information to decrease
the classification results.
4.2 From prepositions in language to relative positions between human and object in images
In this section, we want to learn how prepositions in language can be used to determine which positions
are useful in action classification, i.e., if they belong to the first group or the second group in the previous
experiment.
The relative positions between human and object in images are useful in analyzing their interactions.
For example, when a person is riding a horse, the person is usually on the top of the horse, and when
a person is feeding a horse, then the person is usually standing next to the horse. In spoken English,
sometimes prepositions can be used as an indicator to the relations between human and object positions.
We want to exploit the connection between human-object positions in images and prepositions that
link human, verb and object in language. Intuitively, if an action implies a strong positional relation
between the human and the object, we expect to find specific, distinguishing prepositions in language.
For example, in language you usually say ?sit on chair?, where the preposition on suggests a specific
spatial relation between the human and the chair. When an action does not imply a strong positional
relation, such as ?play?, we expect no specific prepositions.
Links in language models To test this hypothesis, we use TypeDM (Baroni and Lenci, 2010), a dis-
tributional memory that has been built from large scale text corpora. This model contains weighted
<word-link-word> tuples extracted from a dependency parse of corpora. The relations between words
are characterized by their ?link?. Some of these links are prepositions that connect verbs and objects
together. Examples of some tuples with word-link-word and their weights are provided in Figure 3.
Number of links and link entropy We want to determine whether there is any correlation between
human-object relative positions in images and the associated prepositions from language models. To do
this, we record two metrics: the number of links, where we count how many different links that connect
verbs and objects in the language model; and the entropy of each action A
i
verb-object pair (where the
human is implicit) is H(A
i
) defined by: H(A
i
) = ?
?
l
j
?L
i
p(l
j
)? log p(l
j
)
where L
i
is the set of all links that occur between verb and object of action i; p(l
j
) is the probability
of the link l
j
of the action A
i
:
22
Figure 3: Examples of word-link-word and their weights in the distributional memory
p(l
j
) =
weight(l
j
)
?
l
k
?L
i
weight(l
k
)
(4)
where weight(l
j
) is the weight given by the TypeDM of link j in action i.
Generally, the entropy for each action allows seeing whether a link is predictable for a given pair of
verb-object or not: when a link is predictable, the entropy is expected to be low (contain little infor-
mation), which might correspond to the case that the position information will be useful in predicting
actions and the other way around.
Number of links Entropy
Group 1 (position helps) 8 1.05
Group 2 (position doesn?t help) 15.3 1.36
Table 6: Actions that can be disambiguated by positions (Group 1) vs. actions that cannot be disam-
biguated by positions (Group 2) and their links in the language model
Results The result shown in Table 6: for the first group (with position is better), the average number
of links per relation (verb - object) is 8 and the average entropy is 1.05; the average number of links
per relation for the second group is almost twice more, 15.3, and their average entropy is also higher,
1.36. It shows that verbs which have many different ways of linking to an object might not have a
representative relative position between the person and object, hence more difficult to be classified based
on their positions. Verbs that have less links to an object tend to have more fixed relative positions
between persons and objects, hence it might be helpful to use position information in classification.
A qualitative analysis We further examine actions where this statement does not apply, i.e., actions
with high number of links and high entropy but belong to group 1 (position information helps) and
actions with low number of links and entropy belonging to group 2. For the first case, typical actions
which have high number of links/entropy are: ride car, ride bus, ride train, pull cart, light lamp. The large
number of links of these actions seem to come from relations which do not describe the human/object
interaction itself. For example, the links associated with ?ride bus? do not all actually refer to ?ride a
bus? but to ride another object in a position with respect to the bus: ride after bus, ride behind bus,
ride before bus. These cause extra links which are not related to the action itself. Similarly, actions
pull of/around/behind/below/on cart, there is another object which is moved to a specific position with
regards to the cart.
For the second case, examples of typical actions with low links but for which positions information
doesn?t help are hold harmonica, wear diaper, hold ladle, spread cream, hold racket, apply lipstick.
These actions are related to objects, for which their positions depends a lot on the human pose (e.g., hold
something). These actions in the language model do not contain many links as we expected: the most
possible link between hold, harmonica is in, which probably means hold harmonica in your hand.
Instead of looking at actions, we look into typical verbs where position information helps in classifying
actions and verbs where position information doesn?t help. For the first group, the most frequent verbs
are: chop, cut, drink, feed, lean, sit on, sleep, look at, put on, shake, shoot, wash, catch. For the second
group, the most frequent verbs are: clean, cook, lift, punch, sing, spray, spread. It can be observed that
verbs related to some particular poses or relative positions between human and object are better with
23
the position information (chop, drink, sit on, sleep), and verbs related to more various human poses and
unspecific are not helped by the position information (cook, sing, spray, clean).
Generally, there is a relation between prepositions in language and the relative positions between
human-object in images. Although this statement does not hold in every cases, for example when the
prepositions refer to the positions between another action (e.g., ride) and that object (e.g., after a bike),
this can be potentially solved by better NLP parsing and analyses of verb phrases. Furthermore, actions
that cannot be disambiguated by positions are usually related to different human poses, while actions that
have some particular human poses can be classified using position information.
5 Conclusion
In this paper, we have introduced the Trento Universal Human Object Interaction image dataset, TUHOI.
This dataset contains more than two thousand human actions associated with 189 common objects in
images. The main characteristics of this dataset are that it follows the actual human action distribution
observed in images, it captures different ways of describing an action and it enables the study of how
verbs are used differently with different objects in images. Additionally, we performed some prelimi-
nary experiments in which we show that action recognition can benefit from using position information.
Finally, we showed that this position information is related to prepositions that can be extracted from a
general language model.
References
Stephane Ayache and Georges Quenot. 2008. Video Corpus Annotation using Active Learning. In European
Conference on Information Retrieval (ECIR), pages 187?198, Glasgow, Scotland, mar.
Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based
semantics. Computational Linguistics.
Vincent Delaitre, Ivan Laptev, and Josef Sivic. 2010. Recognizing human actions in still images: a study of
bag-of-features and part-based representations. In BMVC. BMVA Press.
Vincent Delaitre, Josef Sivic, and Ivan Laptev. 2011. Learning person-object interactions for action recognition in
still images. In NIPS.
M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. 2012. The PASCAL Visual Object
Classes Challenge 2012 (VOC2012) Results.
Ali Farhadi, Mohsen Hejrati, Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and David
Forsyth. 2010. Every picture tells a story: Generating sentences for images. In ECCV.
Abhinav Gupta, Aniruddha Kembhavi, and Larry S. Davis. 2009. Observing human-object interactions: Using
spatial and functional compatibility for recognition. IEEE Trans. Pattern Anal. Mach. Intell., 31(10), October.
Nazli Ikizler, Ramazan Gokberk Cinbis, Selen Pehlivan, and Pinar Duygulu. 2008. Recognizing actions from still
images. In ICPR, pages 1?4. IEEE.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander Berg, and Tamara Berg. 2011.
Babytalk: Understanding and generating simple image descriptions. In CVPR.
Ivan Laptev. 2005. On space-time interest points. Int. J. Comput. Vision, 64(2-3):107?123, September.
Dieu Thu Le, Raffaella Bernardi, and Jasper Uijlings. 2013. Exploiting language models to recognize unseen
actions. In ICMR.
Amin Sadeghi and Ali Farhadi. 2011. Recognition using visual phrases. In CVPR.
Heng Wang, Alexander Kl?aser, Cordelia Schmid, and Cheng-Lin Liu. Dense trajectories and motion boundary
descriptors for action recognition.
Bangpeng Yao and Fei-Fei Li. 2010. Grouplet: A structured image representation for recognizing human and
object interactions. In CVPR, pages 9?16.
Bangpeng Yao, Xiaoye Jiang, Aditya Khosla, Andy Lai Lin, Leonidas J. Guibas, and Li Fei-Fei. 2011. Action
recognition by learning bases of action attributes and parts. In ICCV.
24
Proceedings of the 25th International Conference on Computational Linguistics, pages 112?114,
Dublin, Ireland, August 23-29 2014.
Coloring Objects: Adjective-Noun Visual Semantic Compositionality
Dat Tien Nguyen
(1,2)
Angeliki Lazaridou
(2)
Raffaella Bernardi
(2)
(1)
EM LCT,
(2)
University of Trento/ Italy
name.surname@unitn.it
Abstract
This paper reports preliminary experiments aiming at verifying the conjecture that semantic com-
positionality is a general process irrespective of the underlying modality. In particular, we model
compositionality of an attribute with an object in the visual modality as done in the case of an ad-
jective with a noun in the linguistic modality. Our experiments show that the concept topologies
in the two modalities share similarities, results that strengthen our conjecture.
1 Language and Vision
Recently, fields like computational linguistics and computer vision have converged to a common way of
capturing and representing the linguistic and visual information of atomic concepts, through vector space
models. At the same time, advances in computational semantics have lead to effective and linguistically
inspired approaches of extending such methods from single concepts to arbitrary linguistic units (e.g.
phrases), through means of vector-based semantic composition (Mitchell and Lapata, 2010).
Compositionality is not to be considered only an important component from a linguistic perspective,
but also from a cognitive perspective and there has been efforts to validate it as a general cognitive
process. However, in computer vision so far compositionality has received limited attention. Thus, in
this work, we study the phenomenon of visual compositionality and we complement limited previous
literature that has focused on event compositionality (St?ottinger et al., 2012) or general image struc-
ture (Socher et al., 2011), by studying models of attribute-object semantic composition.
In a nutshell, our work consists of learning vector representations of attribute-object (e.g., ?red car?,
?cute dog? etc.) and objects (e.g., ?car?, ?dog?, ?truck?, ?cat? etc.) and by using those compute the
representation of new objects having similar attributes (?red truck?, ?cute cat? etc.). This question has
both theoretical and applied impact. The possibility of developing a visual compositional model of
attribute-object, on the one hand, could shed light on the acquisition of such ability in humans; how we
learn attribute representation and compose them with different objects is still an open question within the
cognitive science community (Mintz and Gleitman, 2002). On the other hand, computer vision systems
could become generative and be able to recognize unseen attribute-object combinations, a component
especially useful for object recognition and image retrieval.
2 Visual Compositional Model
As our source of inspiration regarding the type of compositionality, we use the Lexical Functional model
(LF) (Baroni and Zamparelli, 2010), under which adjectives, in linguistic compositionality, are repre-
sented as linear functions (i.e., matrix of weights). Concretely, each adjective function f
W
adj
is induced
from corpus-observed vectors of adjective-noun phrases w
i
? W
phrase
and noun w
j
? W
noun
, e.g.,
?(w
red car
, w
car
), (w
red flag
, w
flag
), . . .?, by solving the least-squares regression problem:
arg min
f
W
adj
?R
d?d
||W
phrase
? f
W
adj
W
noun
||
This work is licensed under a Creative Commons Attribution 4.0 International Licence. License details:
http://creativecommons.org/licenses/by/4.0/
112
In this work, we propose to import the LF method in the visual modality, aiming at develop-
ing a Visual Compositional Model. Similarly to the case of linguistic compositionality, each at-
tribute function f
V
attr
is induced from image-harvested vector representations of attribute-object v
i
?
V
phrase
and object v
j
? V
object
, e.g. for training the function f
V
red
the following data can be used
?(v
red car
, v
car
), (v
red flag
, v
flag
), . . .?.
3 Experiments
The visual representations of attribute-objects and objects are created with the PHOW-color fea-
tures (Bosch et al., 2007) and SIFT color-agnostic features (Lowe, 2004) respectively. The linguistic
representations for the adjective-noun W
phrase
and noun W
noun
are built with the word2vec toolkit
1
using a corpus of 3 billion tokens.
2
Both visual and linguistic representations consist of 300 dimensions.
In this work, we focus on attributes related to 10 colors (Russakovsky and Fei-Fei, 2012) for a
total number of 9699 images depicting 202 unique objects/nouns and 886 unique phrases (attribute-
object/adjective-noun). Our experiments are conducted with aggregated attribute-object representations
obtained by summing the visual vectors extracted from images representing the same attribute-object,
The same pipeline is followed for the objects to obtain aggregated object vectors.
This work aims at comparing the behavior of the semantically-driven compositionality process across
the two modalities. For this reason, we report results on the intersection of V
phrase
and W
phrase
, a
process that results in 266 attribute-object/adjective-noun items. Furthermore, although the training data
for the two modalities are different, the size of the training data is identical, i.e., the f
V
attr
is trained using
the remaining 620 attribute-object items, whereas for the f
W
adj
, we randomly sample 620 adjective-noun
items from the language space.
3.1 Analysis of Language and Visual Semantic Spaces
This experiment aims at assessing the degree to which language and vision share commonalities. To this
end, we compute the cosine similarities between all possible combination of objects (resp., nouns) and
perform a correlation analysis of the similarity of the corresponding pairs in the two lists resulting in 0.45
Spearman correlation ? e.g., we correlate the similarity between v
cat
and v
dog
with that between w
cat
and
w
dog
. For instance, ?goat? and ?sheep? are highly similar in both spaces, whereas ?whale? and ?bird?
are similar only linguistically, whereas ?blackboard? and ?chair? are similar only visually. The same
experiment is performed between all possible combinations of attribute-object/adjective-noun items, e.g.
we correlate the similarity between v
white cat
and v
black dog
with that between w
white cat
and w
black dog
,
resulting in 0.33 Spearman correlation (see Table 1).
Overall, our results suggest that the topologies of the semantic spaces are similar in the two modalities.
Furthermore, since this phenomenon is also apparent in the cases of attribute-object and adjective-noun
pairs, this alludes to the possibility of transferring approaches of semantic compositionality from the
linguistic to the visual modality.
High Visual Low Visual
High Linguistic goat-sheep, jaguar- lion baboon-transporter, bird-whale
black bag - brown bag, brown bear - yellow dog blue grass - blue van, gray whale - white deer
Low Linguistic ball-horse, blackboard-chair baboon-sofa, backboard-panda
red strawberry - white ball, white bear - yellow dog black bag - green bridge, green table - yellow stick
Table 1: Similar and dissimilar concepts in the language and vision space.
3.2 Semantically-driven composition for attribute-object representations
The findings of the previous experiment suggest a high correlation between the visual attribute-attribute
representations and the corpus-harvested adjective-noun representations. An interesting question that
arises is whether we could approximate such visual representations of complex visual units, similarly to
1
https://code.google.com/p/word2vec/
2
http://wacky.sslmit.unibo.it, http://www.natcorp.ox.ac.uk
113
how is done in Computational Linguistics for approximating the text-based representations of adjective-
noun phrases. Thus, this experiment is designed in order to assess the validity of the semantically-driven
compositionality approach in the visual domain. Results are reported in Table 2. Since we expect that
the quality of the aggregated vectors depends on the numbers of available images, we report results for
subsets of the original data set that differ on the number of images per phrase.
By means of the LF composition method sketched in Section 2, we obtain the compositional represen-
tations of attribute-object (V
comp
phrase
) and adjective-noun (W
comp
phrase
) items. We then perform the correlation
analyses between the similarities obtained in the composed visual space V
comp
phrase
with: 1) the equiva-
lent image-harvested representations V
phrase
, 2) the equivalent corpus-derived linguistic representations
W
phrase
, 3) the equivalent compositionally-derived linguistic representations W
comp
phrase
.
Overall, the correlation between V
comp
space
and V
space
suggests that the visual compositionality of
attribute-object can account, to some extend, for the visual semantics of the respective image, and it
further improves with the number of images we consider for obtaining the aggregated vectors of the vi-
sual phrases. Finally, as expected, the correlations between V
comp
space
although lower than the ones reported
in Section 3.1, i.e., 0.22 vs 0.32, are still non negligible.
all phrases > 10 images > 20 images > 30 images
V
comp
phrase
- V
phrase
0.24 0.40 0.53 0.58
V
comp
phrase
- W
phrase
0.10 0.22 0.19 0.23
V
comp
phrase
- W
comp
phrase
0.04 0.05 0.18 0.10
Table 2: Spearman correlations between the similarities in the V
comp
phrase
and other semantic spaces.
4 Conclusions
In this work, we have experimented with semantically-driven compositionality of attributes with objects
in the visual modality, by adopting an out-of-the-box composition method from the computational se-
mantics literature. Our preliminary results have shown that the visual representations of attribute-objects
when obtained compositionally reflect properties similar not only to the ones found in representations
harvested automatically from images, but also from those extracted from text corpora. These results
show that semantic compositionality might be a general process irrespective of the underlying modality.
We have just scratched the surface on this topic and in the future we plan to experiment with a larger
variety of attributes and use and design alternative visual compositional models.
Acknowledgements
The second and third author acknowledge ERC 2011 Starting Independent Research Grant n. 283554
(COMPOSES). We thank the 3 anonymous reviewers for their comments, Marco Baroni and Elia Bruni
for their constant and useful feedback.
References
[Baroni and Zamparelli2010] Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are
matrices: Representing adjective-noun constructions in semantic space. In Proceedings of EMNLP, 1183?1193.
[Bosch et al.2007] Anna Bosch, Andrew Zisserman, and Xavier Munoz. 2007. Image classification using random
forests and ferns. In Proceedings of ICCV, 1?8.
[Lowe2004] David G Lowe. 2004. Distinctive image features from scale-invariant keypoints. International
Journal of Computer Vision, 60:91?110.
[Mintz and Gleitman2002] Toben H. Mintz and Lila R. Gleitman. 2002. Adjectives really do modify nouns: the
incremental and restricted nature of early adjective acquisition. Cognition, 84:267?293.
[Mitchell and Lapata2010] Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of
semantics. Cognitive Science, 34(8):1388?1429.
[Russakovsky and Fei-Fei2012] Olga Russakovsky and Li Fei-Fei. 2012. Attribute learning in large-scale datasets.
In Trends and Topics in Computer Vision, 1?14. Springer.
[Socher et al.2011] Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. 2011. Parsing natural scenes
and natural language with recursive neural networks. In Proceedings of ICML, 129?136.
[St?ottinger et al.2012] J. St?ottinger, J.R.R. Uijlings, A.K. Pandey, N. Sebe, and F. Giunchiglia. 2012. (unseen)
event recognition via semantic compositionality. In CVPR.
114
