Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 77?80,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Recent Improvements in the
CMU Large Scale Chinese-English SMT System
Almut Silja Hildebrand, Kay Rottmann, Mohamed Noamany, Qin Gao,
Sanjika Hewavitharana, Nguyen Bach and Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
silja, kayrm, mfn, qing, sanjika, nbach, vogel+@cs.cmu.edu
Abstract
In this paper we describe recent improvements
to components and methods used in our statis-
tical machine translation system for Chinese-
English used in the January 2008 GALE eval-
uation. Main improvements are results of
consistent data processing, larger statistical
models and a POS-based word reordering ap-
proach.
1 Introduction
Building a full scale Statistical Machine Transla-
tion (SMT) system involves many preparation and
training steps and it consists of several components,
each of which contribute to the overall system per-
formance. Between 2007 and 2008 our system im-
proved by 5 points in BLEU from 26.60 to 31.85
for the unseen MT06 test set, which can be mainly
attributed to two major points.
The fast growth of computing resources over
the years make it possible to use larger and larger
amounts of data in training. In Section 3 we show
how parallelizing model training can reduce training
time by an order of magnitude and how using larger
training data as well as more extensive models im-
prove translation quality.
Word reordering is still a difficult problem in
SMT. In Section 4 we apply a Part Of Speech (POS)
based syntactic reordering model successfully to our
large Chinese system.
1.1 Decoder
Our translation system is based on the CMU
SMT decoder as described in (Hewavitharana et
al., 2005). Our decoder is a phrase-based beam
search decoder, which combines multiple models
e.g. phrase tables, several language models, a dis-
tortion model ect. in a log-linear fashion. In order
to find an optimal set of weights, we use MER train-
ing as described in (Venugopal et al, 2005), which
uses rescoring of the top n hypotheses to maximize
an evaluation metric like BLEU or TER.
1.2 Evaluation
In this paper we report results using the BLEU met-
ric (Papineni et al, 2002), however as the evaluation
criterion in GALE is HTER (Snover et al, 2006), we
also report in TER (Snover et al, 2005).
We used the test sets from the NIST MT evalua-
tions from the years 2003 and 2006 as development
and unseen test data.
1.3 Training Data
In translation model training we used the Chinese-
English bilingual corpora relevant to GALE avail-
able through the LDC1. After sentence alignment
these sources add up to 10.7 million sentences with
301 million running words on the English side. Our
preprocessing steps include tokenization on the En-
glish side and for Chinese: automatic word segmen-
tation using the revised version of the Stanford Chi-
nese Word Segmenter2 (Tseng et al, 2005) from
2007, replacement of traditional by simplified Chi-
nese characters and 2-byte to 1-byte ASCII charac-
ter normalization. After data cleaning steps like e.g.
removal of sentence pairs with very unbalanced sen-
1http://projects.ldc.upenn.edu/gale/data/catalog.html
2http://nlp.stanford.edu/software/segmenter.shtml
77
tence length etc., we used the remaining 10 million
sentences with 260 million words (English) in trans-
lation model training (260M system).
2 Number Tagging
Systematic tagging and pre-translation of numbers
had shown significant improvements for our Arabic-
English system, so we investigated this for Chinese-
English. The baseline for these experiments was a
smaller system with 67 million words (67M) bilin-
gual training data (English) and a 500 million word
3-gram LM with a BLEU score of 27.61 on MT06.
First we pre-translated all numbers in the testdata
only, thus forcing the decoder to treat the numbers as
unknown words. Probably because the system could
not match longer phrases across the pre-translated
numbers, the overall translation quality degraded by
1.6 BLEU to 26.05 (see Table 1).
We then tagged all numbers in the training corpus,
replaced them with a placeholder tag and re-trained
the translation model. This reduced the vocabu-
lary and enabled the decoder to generalize longer
phrases across numbers. This strategy did not lead to
the expected result, the BLEU score for MT06 only
reached 25.97 BLEU.
System MT03 MT06
67M baseline 31.45/60.93 27.61/62.18
test data tagged ? 26.06/63.36
training data tagged 29.07/62.52 25.97/63.39
Table 1: Number tagging experiments, BLEU/TER
Analysing this in more detail, we found, the rea-
son for this degradation in translation quality could
be the unbalanced occurrence of number tags in the
training data. From the bilingual sentence pairs,
which contain number tags, 66.52% do not contain
the same number of tags on the Chinese and the En-
glish side. As a consequence 52% of the phrase pairs
in the phrase table, which contain number tags had
to be removed, because the tags were unbalanced.
This hurts system performance considerably.
3 Scaling up to Large Data
3.1 Language Model
Due to the availability of more computing resources,
we were able to extend the language model history
from 4- to 5-gram, which improved translation qual-
ity from 29.49 BLEU to 30.22 BLEU for our large
scale 260M system (see Table 2). This shows, that
longer LM histories help if we are able to use enough
data in model training.
System MT03 MT06
260M, 4gram 31.20/61.00 29.49/61.00
260M, 5gram 32.20/60.59 30.22/60.81
Table 2: 4- and 5-gram LM,260M system, BLEU/TER
The language model was trained on the sources
from the English Gigaword Corpus V3, which con-
tains several newspapers for the years between 1994
to 2006. We also included the English side of the
bilingual training data, resulting in a total of 2.7 bil-
lion running words after tokenization.
We trained separate open vocabulary language
models for each source and interpolated them using
the SRI Language Modeling Toolkit (Stolcke, 2002).
Table 3 shows the interpolation weights for the dif-
ferent sources. Apart from the English part of the
bilingual data, the newswire data from the Chinese
Xinhua News Agency and the Agence France Press
have the largest weights. This reflects the makeup of
the test data, which comes in large parts from these
sources. Other sources, as for example the UN par-
lamentary speeches or the New York Times, differ
significantly in style and vocabulary from the test
data and therefore get small weights.
xin 0.30 cna 0.06 nyt 0.03
bil 0.26 un 0.07 ltw 0.01
afp 0.21 apw 0.05
Table 3: LM interpolation weights per source
3.2 Speeding up Model Training
To accelerate the training of word alignment
models we implemented a distributed version of
GIZA++ (Och and Ney, 2003), based on the latest
version of GIZA++ and a parallel version developed
at Peking University (Lin et al, 2006). We divide the
bilingual training data in equal parts and distribute it
over several processing nodes, which perform align-
ment independently. In each iteration the nodes read
the model from the previous step and output all nec-
essary counts from the data for the models, e.g. the
78
co-occurrence or fertility model. A master process
collects the counts from the nodes, normalizes them
and outputs the intermediate model for each itera-
tion.
This distributed GIZA++ version finished training
the word alignment up to IBM Model 4 for both lan-
guage directions on the full bilingual corpus (260
million words, English) in 39 hours. On average
about 11 CPUs were running concurrently. In com-
parison the standard GIZA++ implementation fin-
ished the same training in 169 hours running on 2
CPUs, one for each language direction.
We used the Pharaoh/Moses package (Koehn et
al., 2007) to extract and score phrase pairs using the
grow-diag-final extraction method.
3.3 Translation Model
We trained two systems, one on the full data and one
without the out-of-domain corpora: UN parlament,
HK hansard and HK law parallel texts. These parla-
mentary sessions and law texts are very different in
genre and style from the MT test data, which con-
sists mainly of newspaper texts and in recent years
also of weblogs, broadcast news and broadcast con-
versation. The in-domain training data had 3.8 mil-
lion sentences and 67 million words (English). The
67 million word system reached a BLEU score of
29.65 on the unseeen MT06 testset. Even though the
full 260M system was trained on almost four times
as many running words, the baseline score for MT06
only increased by 0.6 to 30.22 BLEU (see Table 4).
System MT03 MT06
67M in-domain 32.42/60.26 29.65/61.22
260M full 32.20/60.59 30.22/60.81
Table 4: In-domain only or all training data, BLEU/TER
The 67M system could not translate 752 Chinese
words out of 38937, the number of unknown words
decreased to 564 for the 260M system. To increase
the unigram coverage of the phrase table, we added
the lexicon entries that were not in the phrase table
as one-word translations. This lowered the number
of unknown words further to 410, but did not effect
the translation score.
4 POS-based Reordering
As Chinese and English have very different word
order, reordering over a rather limited distance dur-
ing decoding is not sufficient. Also using a simple
distance based distortion probability leaves it essen-
tially to the language model to select among dif-
ferent reorderings. An alternative is to apply auto-
matically learned reordering rules to the test sen-
tences before decoding (Crego and Marino, 2006).
We create a word lattice, which encodes many re-
orderings and allows long distance reordering. This
keeps the translation process in the decoder mono-
tone and makes it significantly faster compared to
allowing long distance reordering at decoding time.
4.1 Learning Reordering Rules
We tag both language sides of the bilingual corpus
with POS information using the Stanford Parser3
and extract POS based reordering patterns from
word alignment information. We use the context in
which a reordering pattern is seen in the training data
as an additional feature. Context refers to the words
or tags to the left or to the right of the sequence for
which a reordering pattern is extracted.
Relative frequencies are computed for every rule
that has been seen more than n times in the training
corpus (we observed good results for n > 5).
For the Chinese system we used only 350k bilin-
gual sentence pairs to extract rules with length of
up to 15. We did not reorder the training corpus
to retrain the translation model on modified Chinese
word order.
4.2 Applying Reordering Rules
To avoid hard decisions, we build a lattice struc-
ture for each source sentence as input for our de-
coder, which contains reordering alternatives consis-
tent with the previously extracted rules.
Longer reordering patterns are applied first.
Thereby shorter patterns can match along new paths,
creating short distance reordering on top of long dis-
tance reordering. Every outgoing edge of a node is
scored with the relative frequency of the pattern used
on the following sub path (For details see (Rottmann
and Vogel, 2007)). These model scores give this re-
3http://nlp.stanford.edu/software/lex-parser.shtml
79
ordering approach an advantage over a simple jump
model with a sliding window.
System MT03 MT06
260M, standard 32.20/60.59 30.22/60.81
260M, lattice 33.53/59.74 31.74/59.59
Table 5: Reordering lattice decoding in BLEU/TER
The system with reordering lattice input outper-
forms the system with a reordering window of 4
words by 1.5 BLEU (see Table 5).
5 Summary
The recent improvements to our Chinese-English
SMT system (see Fig. 1) can be mainly attributed to
a POS based word reordering method and the possi-
bility to work with larger statistical models.
We used the lattice translation functionality of our
decoder to translate reordering lattices. They are
built using reordering rules extracted from tagged
and aligned parallel data. There is further potential
for improvement in this approach, as we did not yet
reorder the training corpus and retrain the translation
model on modified Chinese word order.Improvements in BLEU
242526
272829
303132
33
2007 67M+3gr 260M+3gr 260M+4gr 260M+5gr 260M+RO
Figure 1: Improvements for MT06 in BLEU
We modified GIZA++ to run in parallel, which en-
abled us to include especially longer sentences into
translation model training. We also extended our de-
coder to use 5-gram language models and were able
to train an interpolated LM from all sources of the
English GigaWord Corpus.
Acknowledgments
This work was partly funded by DARPA under
the project GALE (Grant number #HR0011-06-2-
0001).
References
Josep M. Crego and Jose B. Marino. 2006. Reordering
Experiments for N-Gram-Based SMT. Spoken Lan-
guage Technology Workshop, Palm Beach, Aruba.
Sanjika Hewavitharana, Bing Zhao, Almut Silja Hilde-
brand, Matthias Eck, Chiori Hori, Stephan Vogel and
Alex Waibel. 2005. The CMU Statistical Machine
Translation System for IWSLT 2005. IWSLT 2005,
Pittsburgh, PA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. ACL 2007,
Demonstration Session, Prague, Czech Republic.
Xiaojun Lin, Xinhao Wang, and Xihong Wu. 2006.
NLMP System Description for the 2006 NIST MT
Evaluation. NIST 2006 MT Evaluation.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Poukos, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. ACL 2002, Philadel-
phia, USA.
Kay Rottmann and Stephan Vogel. 2007. Word Reorder-
ing in Statistical Machine Translation with a POS-
based Distortion Model. TMI-2007: 11th Interna-
tional Conference on Theoretical and Methodological
Issues in MT, Skvde, Sweden.
Mathew Snover, Bonnie Dorr, Richard Schwartz, John
Makhoul, Linnea Micciula and Ralph Weischedel.
2005. A Study of Translation Error Rate with Tar-
geted Human Annotation. LAMP-TR-126, University
of Maryland, College Park and BBN Technologies.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. 7th Conference of AMTA, Cambridge, Mas-
sachusetts, USA.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. ICSLP, Denver, Colorado.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky and Christopher Manning. 2005. A Con-
ditional Random Field Word Segmenter. Fourth
SIGHAN Workshop on Chinese Language Processing.
Ashish Venugopal, Andreas Zollman and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. ACL 2005,
WPT-05, Ann Arbor, MI
80
Proceedings of the Second Workshop on Statistical Machine Translation, pages 197?202,
Prague, June 2007. c?2007 Association for Computational Linguistics
The ISL Phrase-Based MT System for the 2007 ACL Workshop on
Statistical Machine Translation
M. Paulik1,2, K. Rottmann2, J. Niehues2, S. Hildebrand 1,2 and S. Vogel1
1Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA, USA
2Institut fu?r Theoretische Informatik, Universita?t Karlsruhe (TH), Karlsruhe, Germany
{paulik|silja|vogel}@cs.cmu.edu ; {jniehues|rottmann}@ira.uka.de
Abstract
In this paper we describe the Interactive Sys-
tems Laboratories (ISL) phrase-based ma-
chine translation system used in the shared
task ?Machine Translation for European
Languages? of the ACL 2007 Workshop on
Statistical Machine Translation. We present
results for a system combination of the
ISL syntax-augmented MT system and the
ISL phrase-based system by combining and
rescoring the n-best lists of the two systems.
We also investigate the combination of two
of our phrase-based systems translating from
different source languages, namely Spanish
and German, into their common target lan-
guage, English.
1 Introduction
The shared task of the ACL 2007 Workshop on Sta-
tistical Machine Translation focuses on the auto-
matic translation of European language pairs. The
workshop provides common training sets for trans-
lation model training and language model training
to allow for easy comparison of results between the
participants.
Interactive Systems Laboratories participated in the
English ? Spanish Europarl and News Commen-
tary task as well as in the English ? German Eu-
roparl task. This paper describes the phrase-based
machine translation (MT) system that was applied
to these tasks. We also investigate the feasibility
of combining the ISL syntax-augmented MT system
(Zollmann et al, 2007) with our phrase-based sys-
tem by combining and rescoring the n-best lists pro-
duced by both systems for the Spanish ? English
Europarl task. Furthermore, we apply the same com-
bination technique to combine two of our phrase-
based systems that operate on different source lan-
guages (Spanish and German), but share the same
target language (English).
The paper is organized as follows. In section 2 we
give a general description of our phrase-based sta-
tistical machine translation system. Section 3 gives
an overview of the data and of the final systems
used for the English ? Spanish Europarl and News
Commentary tasks, along with corresponding per-
formance numbers. Section 4 shows the data, final
systems and results for the English ? German Eu-
roparl task. In Section 5, we present our experiments
involving a combination of the syntax-augmented
MT system with the phrase-based MT system and a
combination of the Spanish ? English and German
? English phrase-based systems.
2 The ISL Phrase-Based MT System
2.1 Word and Phrase Alignment
Phrase-to-phrase translation pairs are extracted by
training IBM Model-4 word alignments in both di-
rections, using the GIZA++ toolkit (Och and Ney,
2000), and then extracting phrase pair candidates
which are consistent with these alignments, start-
ing from the intersection of both alignments. This
is done with the help of phrase model training
code provided by University of Edinburgh during
the NAACL 2006 Workshop on Statistical Machine
Translation (Koehn and Monz, 2006). The raw rel-
197
ative frequency estimates found in the phrase trans-
lation tables are then smoothed by applying modi-
fied Kneser-Ney discounting as explained in (Foster
et al, 2006). The resulting phrase translation tables
are pruned by using the combined translation model
score as determined by Minimum Error Rate (MER)
optimization on the development set.
2.2 Word Reordering
We apply a part-of-speech (POS) based reordering
scheme (J. M. Crego et al, 2006) to the POS-tagged
source sentences before decoding. For this, we use
the GIZA++ alignments and the POS-tagged source
side of the training corpus to learn reordering rules
that achieve a (locally) monotone alignment. Fig-
ure 1 shows an example in which three reordering
rules are extracted from the POS tags of an En-
glish source sentence and its corresponding Span-
ish GIZA++ alignment. Before translation, we con-
struct lattices for every source sentence. The lattices
include the original source sentence along with all
the reorderings that are consistent with the learned
rules. All incoming edges of the lattice are anno-
tated with distortion model scores. Figure 2 gives an
example of such a lattice. In the subsequent lattice
decoding step, we apply either monotone decoding
or decoding with a reduced local reordering window,
typically of size 2.
2.3 Decoder and MER Training
The ISL beam search decoder (Vogel, 2003) com-
bines all the different model scores to find the best
translation. Here, the following models were used:
? The translation model, i.e. the phrase-to-
phrase translations extracted from the bilingual
corpus, annoted with four translation model
scores. These four scores are the smoothed for-
ward and backward phrase translation proba-
bilities and the forward and backward lexical
weights.
? A 4-gram language model. The SRI language
model toolkit was used to train the language
model and we applied modified Kneser-Ney
smoothing.
? An internal word reordering model in addition
to the already described POS-based reordering.
  
We all agree on thatPRP DT VB IN DTEn {4} esto {5} estamos {1} todos {2} de {} acuerdo {3}
? PRP DT VB IN DT :   4 ? 5 ? 1 ? 2 ? 3? PRP DT VB:   2 ? 3 ? 1 ? PRP DT VB IN:   3 ? 4 ? 1 ? 2
Figure 1: Rule extraction for the POS-based reorder-
ing scheme.
This internal reordering model assigns higher
costs to longer distance reordering.
? Simple word and phrase count models. The
former is essentially used to compensate for
the tendency of the language model to prefer
shorter translations, while the latter can give
preference to longer phrases, potentially im-
proving fluency.
The ISL SMT decoder is capable of loading
several language models (LMs) at the same time,
namely n-gram SRI language models with n up to
4 and suffix array language models (Zhang and Vo-
gel, 2006) of arbitrary length. While we typically
see gains in performance for using suffix array LMs
with longer histories, we restricted ourselves here to
one 4-gram SRI LM only, due to a limited amount
of available LM training data. The decoding process
itself is organized in two stages. First, all available
word and phrase translations are found and inserted
into a so-called translation lattice. Then the best
combination of these partial translations is found
by doing a best path search through the translation
lattice, where we also allow for word reorderings
within a predefined local reordering window.
To optimize the system towards a maximal BLEU
or NIST score, we use Minimum Error Rate (MER)
Training as described in (Och, 2003). For each
model weight, MER applies a multi-linear search
on the development set n-best list produced by the
system. Due to the limited numbers of translations
in the n-best list, these new model weights are sub-
optimal. To compensate for this, a new full trans-
lation is done. The resulting new n-best list is then
merged with the old n-best list and the optimization
process is repeated. Typically, the translation quality
converges after three iterations.
198
1
20 3
honourable1.0000
Members1.0000 honourable0.3299
Members0.6701 6
75 8
we1.0000
have1.0000
have0.9175
a0.08254,1.0000 a1.0000
?
?Honourable Members, we have a challenging agenda?
Figure 2: Example for a source sentence lattice from
the POS-based reordering scheme.
English Spanish
sentence pairs 1259914
unique sent. pairs 1240151
sentence length 25.3 26.3
words 31.84 M 33.16 M
vocabulary 266.9 K 346.3 K
Table 1: Corpus statistics for the English/Spanish
Europarl corpus.
3 Spanish? English Europarl and News
Commentary Task
3.1 Data and Translation Tasks
The systems for the English ? Spanish translation
tasks were trained on the sentence-aligned Europarl
corpus (Koehn, 2005). Detailed corpus statistics can
be found in Table 1. The available parallel News
Commentary training data of approximately 1 mil-
lion running words for both languages was only
used as additional language model training data, to
adapt our in-domain (Europarl) system to the out-of-
domain (News Commentary) task.
The development sets consist of 2000 Europarl
sentences (dev-EU) and 1057 News Commentary
sentences (dev-NC). The available development-
test data consists of 2 x 2000 Europarl sentences
(devtest-EU and test06-EU) and 1064 News Com-
mentary sentences (test06-NC). All development
and development-test sets have only one reference
translation per sentence.
3.2 Data Normalization
The ACL shared task is very close in form and con-
tent to the Final Text Editions (FTE) task of the TC-
STAR (TC-STAR, 2004) evaluation. For this rea-
son, we decided to apply a similar normalization
scheme to the training data as was applied in our TC-
STAR verbatim SMT system. Although trained on
?verbatimized? data that did not contain any num-
bers, but rather had all numbers and dates spelled
out, it yielded consistently better results than our
TC-STAR FTE SMT system. When translating FTE
content, the verbatim system treated all numbers as
unknown words, i.e. they were left unchanged dur-
ing translation. To compensate for this, we applied
extended postprocessing to the translations that con-
ducts the necessary conversions between Spanish
and English numbers, e.g. the conversion of deci-
mal comma in Spanish to decimal point in English.
Other key points which we adopted from this nor-
malization scheme were the tokenization of punc-
tuation marks, the true-casing of the first word of
each sentence, as well as extended cleaning of the
training data. The latter mainly consisted of the re-
moval of sections with a highly unbalanced source
to target words ratio and the removal of unusual
string combinations and document references, like
for example ?B5-0918/2000?, ?(COM(2000) 335 -
C5-0386/2000 - 2000/0143(CNS))?, etc.
Based on this normalization scheme, we trained and
optimized a baseline in-domain system on accord-
ingly normalized source and reference sentences.
For optimization, we combined the available de-
velopment sets for the Europarl task and the News
Commentary task. In order to further improve
the applied normalization scheme, we experimented
with replacing all numbers with the string ?NMBR?,
rather than spelling them out and by replacing all
document identifiers with the string ?DCMNT?,
rather than deleting them. This was first done for
the language model training data only, and then for
all data, i.e. for the bilingual training data and for
the development set source and reference sentences.
In the latter case, the respective tags were again re-
placed by the correct numbers and document identi-
fiers during postprocessing. Table 2 shows the case
sensitive BLEU scores for the three normalization
approaches on the English ? Spanish Europarl and
News Commentary development sets. These scores
were computed with the official NIST scoring script
against the original (not normalized) references.
3.3 In-domain System
As mentioned above, we combined the Europarl and
News Commentary development sets when optimiz-
ing the in-domain system. This resulted in only one
199
Task baseline LM only all data
Europarl 30.94 31.20 31.26
News Com. 31.28 31.39 31.73
Table 2: Case sensitive BLEU scores on the in-
domain and out-of-domain development sets for the
three different normalization schemes.
Task Eng ? Spa Spa ? Eng
dev-EU 31.29 31.77
dev-NC 31.81 31.12
devtest-EU 31.01 31.40
test06-EU 31.87 31.76
test06-NC 30.23 29.22
Table 3: Case sensitive BLEU scores for the final
English ? Spanish in-domain systems.
set of scaling factors, i.e. the in-domain system
applies the same scaling factors for translating in-
domain data as for translating out-of-domain data.
Our baseline system applied only monotone lattice
decoding. For our final in-domain system, we used a
local reordering window of length 2, which accounts
for the slightly higher scores when compared to the
baseline system. The BLEU scores for both trans-
lation directions on the different development and
development-test sets can be found in Table 3.
3.4 Out-of-domain System
In order to adapt our in-domain system towards the
out-of-domain News Commentary task, we consid-
ered two approaches based on language model adap-
tation. First, we interpolated the in-domain LM
with an out-of-domain LM computed on the avail-
able News Commentary training data. The inter-
polation weights were chosen such as to achieve a
minimal LM perplexity on the out-of-domain de-
velopment set. For both languages, the interpo-
lation weights were approximately 0.5. Our sec-
ond approach was to simply load the out-of-domain
LM as an additional LM into our decoder. In both
cases, we optimized the translation system on the
out-of-domain development data only. For the sec-
ond approach, MER optimization assigned three to
four times higher scaling factors to the consider-
ably smaller out-domain LM than to the original in-
domain LM. Table 4 shows the results in BLEU on
the out-of-domain development and development-
test sets for both translation directions. While load-
Eng ? Spa Spa ? Eng
Task interp 2 LMs interp 2 LMs
dev-NC 33.31 33.28 32.61 32.70
test06-NC 32.55 32.15 30.73 30.55
Table 4: Case sensitive BLEU scores for the final
English ? Spanish out-of-domain systems.
ing a second LM gives similar or slightly better re-
sults on the development set during MER optimiza-
tion, we see consistently worse results on the unseen
development-test set. This, in the context of the rela-
tively small amount of development data, can be ex-
plained by stronger overfitting during optimization.
4 English? German Europarl Task
The systems for the English ? German translation
tasks were trained on the sentence-aligned Europarl
corpus only. The complete corpus consists of ap-
proximately 32 million English and 30 million Ger-
man words.
We applied a similar normalization scheme to the
training data as for the English ? Spanish system.
The main difference was that we did not replace
numbers and that we removed all document refer-
ences. In the translation process, the document ref-
erences were treated as unknown words and there-
fore left unchanged. As above, we trained and op-
timized a first baseline system on the normalized
source and reference sentences. However, we used
only the Europarl task development set during opti-
mization. To achieve further improvements on the
German ? English task, we applied a compound
splitting technique. The compound splitting was
based on (Koehn and Knight, 2003) and was applied
on the lowercased source sentences. The words gen-
erated by the compound splitting were afterwards
true-cased. Instead of replacing a compound by
its separate parts, we added a parallel path into the
source sentence lattices used for translation. The
source sentence lattices were augmented with scores
on their edges indicating whether each edge repre-
sents a word of the original text or if it was gener-
ated during compound splitting.
Table 5 shows the case-sensitive BLEU scores for
the final German ? English systems. In contrast
to the English ? Spanish systems, we used only
monotonous decoding on the lattices containing the
200
task Eng ? Ger Ger ? Eng
dev-EU 18.58 23.85
devtest-EU 18.50 23.87
test06-EU 18.39 23.88
Table 5: Case sensitive BLEU scores for the final
English ? German in-domain systems.
syntactical reorderings.
5 System Combination via n-best List
Combination and Rescoring
5.1 N-best List Rescoring
For n-best list rescoring we used unique 500-best
lists, which may have less than 500 entries for
some sentences. In this evaluation, we used sev-
eral features computed from different information
sources such as features from the translation sys-
tem, additional language models, IBM-1 word lex-
ica and the n-best list itself. We calculated 4 fea-
tures from the IBM-1 word lexica: the word proba-
bility sum as well as the maximum word probabil-
ity in both language directions. From the n-best list
itself, we calculated three different sets of scores.
A position-dependent word agreement score as de-
scribed in (Ueffing and Ney, 2005) with a position
window instead of the Levenshtein alignment, the
n-best list n-gram probability as described in (Zens
and Ney, 2006) and a position-independent n-gram
agreement, which is a variation on the first two. To
tune the feature combination weights, we used MER
optimization.
Rescoring the n-best lists from our individual sys-
tems did not give significant improvements on the
available unseen development-test data. For this rea-
son, we did not apply n-best list rescoring to the indi-
vidual systems. However, we investigated the feasi-
bility of combining two different systems by rescor-
ing the joint n-best lists of both systems. The corre-
sponding results are described in the following sec-
tions.
5.2 Combining Syntax-Augmented MT and
Phrase-Based MT
On the Spanish ? English in-domain task, we par-
ticipated not only with the ISL phrase-based SMT
system as described in this paper, but also with
the ISL syntax-augmented system. The syntax-
task PHRA SYNT COMB
dev-EU 31.77 32.48 32.77
test06-EU 31.76 32.15 32.27
Table 6: Results for combining the syntax-
augmented system (SYNT) with the phrase-based
system (PHRA).
augmented system was trained on the same normal-
ized data as the phrase-based system. However, it
was optimized on the in-domain development set
only. More details on the syntax-augmented system
can be found in (Zollmann et al, 2007). Table 6
lists the respective BLEU scores of both systems as
well as the BLEU score achieved by combining and
rescoring the individual 500-best lists.
5.3 Combining MT Systems with Different
Source Languages
(Och and Ney, 2001) describes methods for trans-
lating text given in multiple source languages into a
single target language. The ultimate goal is to im-
prove the translation quality when translating from
one source language, for example English into mul-
tiple target languages, such as Spanish and German.
This can be done by first translating the English doc-
ument into German and then using the translation as
an additional source, when translating to Spanish.
Another scenario where a multi-source translation
becomes desirable was described in (Paulik et al,
2005). The goal was to improve the quality of au-
tomatic speech recognition (ASR) systems by em-
ploying human-provided simultaneous translations.
By using automatic speech translation systems to
translate the speech of the human interpreters back
into the source language, it is possible to bias the
source language ASR system with the additional
knowledge. Having these two frameworks in mind,
we investigated the possibility of combining our in-
domain German ? English and Spanish ? English
translation systems using n-best list rescoring. Ta-
ble 7 shows the corresponding results. Even though
the German ? English translation performance was
approximately 8 BLEU below the translation perfor-
mance of the Spanish ? English system, we were
able to improve the final translation performance by
up to 1 BLEU.
201
task Spa ? Eng Ger ? Eng Comb.
dev-EU 31.77 23.85 32.76
devtest-EU 31.40 23.87 32.41
test06-EU 31.76 23.88 32.51
Table 7: Results for combining the Spanish ? En-
glish and German ? English phrase-based systems
on the in-domain tasks.
6 Conclusion
We described the ISL phrase-based statistical ma-
chine translation systems that were used for the 2007
ACL Workshop on Statistical Machine Translation.
Using the available out-of-domain News Commen-
tary task training data for language model adapta-
tion, we were able to significantly increase the per-
formance on the out-of-domain task by 2.3 BLEU
for English ? Spanish and by 1.3 BLEU for Span-
ish ? English. We also showed the feasibility of
combining different MT systems by combining and
rescoring their resprective n-best lists. In particular,
we focused on the combination of our phrase-based
and syntax-augmented systems and the combination
of two phrase-based systems operating on different
source languages. While we saw only a minimal im-
provement of 0.1 BLEU for the phrase-based and
syntax-augmented combination, we gained up to 1
BLEU, in case of the multi-source translation.
References
G. Foster, R. Kuhn, and H. Johnson. 2006. Phrasetable
Smoothing for Statistical Machine Translation. In
Proc. of Empirical Methods in Natural Language Pro-
cessing, Sydney, Australia.
J. M. Crego et al 2006. N-gram-based SMT System
Enhanced with Reordering Patterns. In Proc. of the
Workshop on Statistical Machine Translation, pages
162?165, New York, USA.
P. Koehn and K. Knight. 2003. Empirical Methods for
Compound Splitting. In Proc. of the tenth conference
on European chapter of the Association for Computa-
tional Linguistics, pages 187?193, Budapest, Hungary.
P. Koehn and C. Monz. 2006. Manual and Automatic
Evaluation of Machine Translation between European
Langauges. In Proc. of the Workshop on Statisti-
cal Machine Translation, pages 102?121, New York,
USA.
P. Koehn. 2005. Europarl: A Parallel Corpus for Statis-
tical Machine Translation. In Proc. of Machine Trans-
lation Summit.
F.J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In Proc. of the 38th Annual Meet-
ing of the Association for Computational Linguistics,
Hongkong, China.
F. J. Och and H. Ney. 2001. Statistical Multi-Source
Translation. In Proc. of Machine Translation Summit,
pages 253?258, Santiago de Compostela, Spain.
F. J. Och. 2003. Minimum Error Rate Training in Statis-
tical Machine Translation. In Proc. of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 160 ? 167, Sapporo, Japan.
M. Paulik, S. Stueker, C. Fuegen, T. Schultz, T. Schaaf,
and A. Waibel. 2005. Speech Translation Enhanced
Automatic Speech Recognition. In Proc. of the Work-
shop on Automatic Speech Recognition and Under-
standing, San Juan, Puerto Rico.
TC-STAR. 2004. Technology and Corpora for Speech to
Speech Translation. http://www.tc-star.org.
N. Ueffing and H. Ney. 2005. Word-Level Con-
fidence Estimation for Machine Translation using
Phrase-Based Translation Models. In Proc. of HLT
and EMNLP, pages 763?770, Vancouver, British
Columbia, Canada.
S. Vogel. 2003. SMT Decoder Dissected: Word Re-
ordering. In Proc. of Int. Conf. on Natural Lan-
guage Processing and Knowledge Engineering, Bei-
jing, China.
R. Zens and H. Ney. 2006. N-gram Posterior Proba-
bilities for Statistical Machine Translation. In Proc.
of the Workshop on Statistical Machine Translation,
pages 72?77, New York, USA.
Y. Zhang and S. Vogel. 2006. Suffix Array and its Ap-
plications in Empirical Natural Language Processing.
In the Technical Report CMU-LTI-06-010, Pittsburgh,
USA.
A. Zollmann, A. Venugopal, M. Paulik, and S. Vogel.
2007. The Syntax Augmented MT (SAMT) system
at the Shared Task for the 2007 ACL Workshop on
Statistical Machine Translation. In Proc. of ACL 2007
Workshop on Statistical MachineTranslation, Prague,
Czech Republic.
202
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 184?187,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Tools for Collecting Speech Corpora via Mechanical-Turk 
  
Ian Lane1,2, Alex Waibel1,2 
1Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213, USA 
{ianlane,ahw}@cs.cmu.edu 
Matthias Eck2, Kay Rottmann2 
2Mobile Technologies LLC 
Pittsburgh, PA, USA 
matthias.eck@jibbigo.com 
kay.rottmann@jibbigo.com
 
 
Abstract 
To rapidly port speech applications to 
new languages one of the most difficult 
tasks is the initial collection of sufficient 
speech corpora. State-of-the-art automatic 
speech recognition systems are typical 
trained on hundreds of hours of speech 
data. While pre-existing corpora do exist 
for major languages, a sufficient amount 
of quality speech data is not available for 
most world languages. While previous 
works have focused on the collection of 
translations and the transcription of audio 
via Mechanical-Turk mechanisms, in this 
paper we introduce two tools which ena-
ble the collection of speech data remotely. 
We then compare the quality of audio col-
lected from paid part-time staff and unsu-
pervised volunteers, and determine that 
basic user training is critical to obtain us-
able data.  
1 Introduction 
In order to port a spoken language application to a 
new language, first an automatic speech recogni-
tion (ASR) system must be developed. For many 
languages pre-existing corpora do not exist and 
thus speech data must be collected before devel-
opment can begin. The collection of speech corpo-
ra is an expensive undertaking and obtaining this 
data rapidly, for example in response to a disaster, 
cannot be done using the typical methodology in 
which corpora are collected in controlled environ-
ments. 
 
To build an ASR system for a new language, two 
sets of data are required; first, a text corpus con-
sisting of written transcriptions of utterances users 
are likely to speak to the system, this is used to 
train the language model (LM) applied during 
ASR; and second, a corpora of recordings of 
speech, which are used to train an acoustic model 
(AM). Text corpora for a new language can be 
created by manually translating a pre-existing cor-
pus (or a sub-set of that corpus) into the new lan-
guage and crowd-sourcing methodologies can be 
used to rapidly perform this task. Rapidly creating 
corpora of speech data, however, is not trivial. 
Generally speech corpora are collected in con-
trolled environments where speakers are super-
vised by experts to ensure the equipment is setup 
correctly and recordings are performed adequately. 
However, for most languages performing this task 
on-site, where developers are located, is impractic-
al as there may not be a local community of speak-
ers of the required language. An alternative is to 
perform the data collection remotely, allowing 
speakers to record speech on their own PCs or mo-
bile devices in their home country or wherever 
they are located. While previous works have fo-
cused on the generation of translations (Razavian, 
2009) and transcribing of audio (Marge, 2010) via 
Mechanical-Turk, in this paper we focus on the 
collection of speech corpora using a Mechanical-
Turk type framework. 
 
Previous works (Voxforge), (Gruenstein, 2009), 
(Schultz, 2007) have developed solutions for col-
lecting speech data remotely via web-based inter-
faces. A web-based system for the collection of 
open-source speech corpora has been developed by 
the group at www.voxforge.org. Speech recordings 
are collected for ten major European languages and 
speakers can either record audio directly on the 
website or they can call in on a dedicated phone 
line. In (Gruenstein, 2009) spontaneous speech 
(US English) was collected via a web-based mem-
ory game. In this system speech prompts were not 
provided, but rather a voice-based memory game 
was used to gather and partially annotate 
184
             
Figure 1: Screenshots from Speech Collection iPhone App 
 
spontaneous speech. In comparison to the above 
works which focus on the collection of data for 
major languages, the SPICE project (Schultz, 
2007) provides a set of web-based tools to enable 
developers to create voice-based applications for 
less-common languages. In addition to tools for 
defining the phonetic units of a language and creat-
ing pronunciation dictionaries, this system also 
includes tools to create prompts and collect speech 
data from volunteers over the web. 
 
In this paper, we describe two tools we have de-
veloped to collect speech corpora remotely. The 
first, a Mobile smart-phone based system which 
allows speakers to record prompted speech directly 
on their phones and second, a web-based system 
which allows recordings to be collected remotely 
on PCs. We compare the quality of audio collected 
from paid part-time staff and unsupervised volun-
teers and determine that basic user training and 
automatic feedback mechanisms are required to 
obtain usable data. 
2 Collection of Speech on Mobile Devices 
Today?s smart-phones are able to record quality 
audio onboard and generally have the ability to 
connect to the internet via a fast wifi-connection. 
This makes them an ideal platform for collecting 
speech data in the field. Speech data can be col-
lected by a user at any time in any location, and the 
data can be uploaded at a later time when a wire-
less connection is available. At Mobile Technolo-
gies we have developed an iPhone application to 
perform this task. 
 
The collection procedure consists of three steps. 
First, on start-up a small amount of personal in-
formation, namely, gender and age, are requested 
from the user. They then select the language for 
which they intend to provide speech data. The mo-
bile-device ID, personal information and language 
selected is used as an identifier for individual 
speakers. Next, collection of speech data is per-
formed. Collection is performed offline, enabling 
data to be collected in the field where there may 
not be a persistent internet connection. A prompt is 
randomly selected from an onboard database of 
sentences and is presented to the user, who reads 
the sentence aloud holding down a push-to-talk 
button while speaking. During the speech collec-
tion stage, the system automatically proceeds to the 
following prompt when the current recording is 
complete. The user however has the ability to go 
back to previous recordings, listen to it and re-
speak the sentence if any issues are found. Finally, 
the speech data is uploaded using a wireless collec-
tion. Data is uploaded one utterance at a time to an 
FTP server. Uploading each utterance individually 
allows the user to halt the upload and continue it at 
a later time if required. 
  
185
 
Figure 2: Java applet for Web-based recording 
3 Collection via Web-based Recording 
One of the most popular websites for crowd-
sourcing is Amazon Mechanical Turk (AMT). 
?Requesters? post Human Intelligence Tasks 
(HITs) to this website and ?Workers? browse the 
HITs, perform tasks and get paid a predefined 
amount after submitting their work. It has been 
reported that over 100,000 workers from 100 coun-
tries are using AMT (Pontin, 2007). 
 
AMT allows two general types of HITs. A Ques-
tion Form HIT is based on a provided XML tem-
plate and only allows certain elements in the HIT. 
However, it is possible to integrate an external 
JAVA applet within a Question Form HIT which 
allows for some flexibility. Questions can also be 
hosted on an external website which increases flex-
ibility for the HIT developer while remaining 
tightly integrated in the AMT environment. 
 
For collection of audio data Amazon does not offer 
any integrated tools. We thus designed and imple-
mented a Java applet for web based speech collec-
tion. The Java applet can easily be incorporated in 
the AMT Question-Form mechanism and could 
also be used as part of an External-Question HIT.  
Currently the Java applet provides the same basic 
functionality as outlined for the iPhone application. 
The applet sequentially shows a number of 
prompts to record. The user can skip a sentence, 
playback a recording to check the quality and also 
redo the recording for the current sentence (see 
screenshot in Figure 2).  
 
After the user is finished, the recorded sentences 
are uploaded to a web-server using an HTTP Post 
request. An important difference is the necessity to 
be online during the speech recordings. 
4 Evaluation of Recorded Audio 
One issue when collecting speech data remotely is 
the quality of the resulting audio. When collection  
Table 1: Details of Evaluated Corpora 
 
Table 2: Annotations used to label poor quality 
recordings 
is performed in a controlled environment, the de-
veloper can ensure that the recording equipment is 
setup correctly, background noise is kept to a min-
imum and the speaker is adequately trained to use 
the recording equipment. However, the same is not 
guaranteed when collecting speech remotely via 
mechanical-turk frameworks.  
When recording prompted speech there are three 
types of issues that result in unsuitable data: 
? Garbage Audio: recordings that are emp-
ty, clipped, have insufficient power, or are 
incorrectly segmented. 
? Low quality recordings: low Signal-to-
Noise recordings due to poor equipment or 
large background noise 
? Speaker errors: Misspeaking of prompts, 
both accidental and malicious 
To verify the quality of audio recorded in unsuper-
vised environments we compared two sets of 
speech data. First, in an earlier data collection task 
we collected 445 prompted utterances from 10 US-
English speakers. This data collection was per-
formed in a quiet office environment with technic-
al supervision. Speakers were paid a fee for their 
time. As a comparison a similar collection of Hai-
tian Creole was performed. In this case data was 
collected on a volunteer basis and supervision was 
limited. Details of the collected data are shown in 
Table 1.  
  
Paid Employees 
Language English 
Number of Speakers 10  
Utterances Evaluated 445 
  
Volunteers 
Language Haitian Creole 
Number of Speakers 3 
Utterances Evaluated 167 
1 Recorded utterance is empty 
2 Utterance is not segmented correctly 
3 Recording is clipped 
4 Recording contains audible echo 
5 Recording contains audible noise 
186
 Figure 3: Percentage of recorded utterances de-
termined to be inadequate for acoustic model 
training. Annotations limited to five issues 
listed in Table 1. 
To determine the frequency of the quality issues 
listed above, we manually verified the two sets of 
collected speech. The recording of each utterance 
was listened to and if the audio file was determined 
to be of low quality it was annotated with one of 
the tags listed in Table 2. The percentage of utter-
ances labeled with each annotation is shown for the 
English and volunteer Haitian Creole cases in Fig-
ure 3. 
 
Around 10% of the English recordings were found 
to have issues. Clipping occurred in approximately 
5% and a distinct echo was present in the record-
ings for one speaker. For the Haitian Creole case 
the yield of useable audio was significantly lower 
than that obtained for English. For all three speak-
ers clipping was more prevalent and the level of 
background noise was higher. We discovered that 
due to lack of training, one of the volunteers had 
significant issues with the push-to-talk interface in 
our system. This led to many empty or incorrectly 
segmented recordings. In both cases, prompts were 
generally spoken accurately and technical prob-
lems caused poor quality recordings. 
 
We believe the large difference in the yield of high 
quality recordings, 90% for English compared to 
65% for Haitian Creole case, is directly due to the 
lack of training speakers received and the volun-
teer nature of the Haitian Creole task. By incorpo-
rating a basic tutorial when users first start our 
tools and an explicit feedback mechanism which 
automatically detects quality issues and prompts 
users to correct them we expect the yield of high 
quality recordings to increase significantly. In the 
near future we plan to use the tools to collect data 
from large communities of remote users.  
5 Conclusions and Future Work 
In this work, we have described two applications 
that allow speech corpora to be collected remotely, 
either directly on Mobile smart-phones or on a PC 
via a web-based interface. We also investigated the 
quality of recordings made by unsupervised volun-
teers and found that although prompts were gener-
ally read accurately, lack of training led to a 
significantly lower yield of high quality record-
ings. 
 
In the near future we plan to use the tools to collect 
data from large communities of remote users. We 
will also investigate the user of tutorials and feed-
back to improve the yield of high quality data. 
 
Acknowledgements 
 
We would like to thank the Haitian volunteers who 
gave their time to help with this data collection. 
References  
N. S. Razavian, S Vogel, "The Web as a Platform 
to Build Machine Translation Resources", 
IWIC2009 
M. Marge, S. Banerjee and A. Rudnicky, "Using 
the Amazon Mechanical Turk for Transcription 
of Spoken Language", IEEE-ICASSP, 2010 
Voxforge, www.voxforge.org 
A. Gruenstein, I. McGraw, and A. Sutherland, "A 
self-transcribing speech corpus: collecting con-
tinuous speech with an online educational 
game," Submitted to the Speech and Language 
Technology in Education (SLaTE) Workshop, 
2009. 
T. Schultz, et. al, "SPICE: Web-based Tools for 
Rapid Language Adaptation in Speech 
Processing Systems", In the Proceedings of 
INTERSPEECH, Antwerp, Belgium, 2007. 
J. Pontin, ?Artificial Intelligence, With Help From 
the Humans?, The New York Times, 25 March 
2007 
187
