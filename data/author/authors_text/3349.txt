331
332
333
334
335
336
337
338
Example Selection for Bootstrapping Statistical Parsers
Mark Steedman?, Rebecca Hwa?, Stephen Clark?, Miles Osborne?, Anoop Sarkar?
Julia Hockenmaier?, Paul Ruhlen? Steven Baker?, Jeremiah Crim?
?School of Informatics, University of Edinburgh
{steedman,stephenc,julia,osborne}@cogsci.ed.ac.uk
?Institute for Advanced Computer Studies, University of Maryland
hwa@umiacs.umd.edu
?School of Computing Science, Simon Fraser University
anoop@cs.sfu.ca
?Center for Language and Speech Processing, Johns Hopkins University
jcrim@jhu.edu,ruhlen@cs.jhu.edu
?Department of Computer Science, Cornell University
sdb22@cornell.edu
Abstract
This paper investigates bootstrapping for statis-
tical parsers to reduce their reliance on manu-
ally annotated training data. We consider both
a mostly-unsupervised approach, co-training,
in which two parsers are iteratively re-trained
on each other?s output; and a semi-supervised
approach, corrected co-training, in which a
human corrects each parser?s output before
adding it to the training data. The selection of
labeled training examples is an integral part of
both frameworks. We propose several selection
methods based on the criteria of minimizing er-
rors in the data and maximizing training util-
ity. We show that incorporating the utility cri-
terion into the selection method results in better
parsers for both frameworks.
1 Introduction
Current state-of-the-art statistical parsers (Collins, 1999;
Charniak, 2000) are trained on large annotated corpora
such as the Penn Treebank (Marcus et al, 1993). How-
ever, the production of such corpora is expensive and
labor-intensive. Given this bottleneck, there is consider-
able interest in (partially) automating the annotation pro-
cess.
To overcome this bottleneck, two approaches from ma-
chine learning have been applied to training parsers. One
is sample selection (Thompson et al, 1999; Hwa, 2000;
Tang et al, 2002), a variant of active learning (Cohn et al,
1994), which tries to identify a small set of unlabeled sen-
tences with high training utility for the human to label1.
Sentences with high training utility are those most likely
to improve the parser. The other approach, and the fo-
cus of this paper, is co-training (Sarkar, 2001), a mostly-
unsupervised algorithm that replaces the human by hav-
ing two (or more) parsers label training examples for each
other. The goal is for both parsers to improve by boot-
strapping off each other?s strengths. Because the parsers
may label examples incorrectly, only a subset of their out-
put, chosen by some selection mechanism, is used in or-
der to minimize errors. The choice of selection method
significantly affects the quality of the resulting parsers.
We investigate a novel approach of selecting training
examples for co-training parsers by incorporating the idea
of maximizing training utility from sample selection. The
selection mechanism is integral to both sample selection
and co-training; however, because co-training and sam-
ple selection have different goals, their selection methods
focus on different criteria: co-training typically favors se-
lecting accurately labeled examples, while sample selec-
tion typically favors selecting examples with high train-
ing utility, which often are not sentences that the parsers
already label accurately. In this work, we investigate se-
lection methods for co-training that explore the trade-off
between maximizing training utility and minimizing er-
rors.
Empirical studies were conducted to compare selection
methods under both co-training and a semi-supervised
framework called corrected co-training (Pierce and
Cardie, 2001), in which the selected examples are man-
ually checked and corrected before being added to the
1In the context of training parsers, a labeled example is a
sentence with its parse tree. Throughout this paper, we use the
term ?label? and ?parse? interchangeably.
                                                               Edmonton, May-June 2003
                                                             Main Papers , pp. 157-164
                                                         Proceedings of HLT-NAACL 2003
training data. For co-training, we show that the benefit of
selecting examples with high training utility can offset the
additional errors they contain. For corrected co-training,
we show that selecting examples with high training util-
ity reduces the number of sentences the human annotator
has to check. For both frameworks, we show that selec-
tion methods that maximize training utility find labeled
examples that result in better trained parsers than those
that only minimize error.
2 Co-training
Blum and Mitchell (1998) introduced co-training to
bootstrap two classifiers with different views of the data.
The two classifiers are initially trained on a small amount
of annotated seed data; then they label unannotated data
for each other in an iterative training process. Blum and
Mitchell prove that, when the two views are conditionally
independent given the label, and each view is sufficient
for learning the task, co-training can boost an initial
weak learner using unlabeled data.
The theory underlying co-training has been extended
by Dasgupta et al (2002) to prove that, by maximizing
their agreement over the unlabeled data, the two learn-
ers make few generalization errors (under the same in-
dependence assumption adopted by Blum and Mitchell).
Abney (2002) argues that this assumption is extremely
strong and typically violated in the data, and he proposes
a weaker independence assumption.
Goldman and Zhou (2000) show that, through care-
ful selection of newly labeled examples, co-training can
work even when the classifiers? views do not satisfy
the independence assumption. In this paper we investi-
gate methods for selecting labeled examples produced by
two statistical parsers. We do not explicitly maximize
agreement (along the lines of Abney?s algorithm (2002))
because it is too computationally intensive for training
parsers.
The pseudocode for our co-training framework is given
in Figure 1. It consists of two different parsers and a cen-
tral control that interfaces between the two parsers and
the data. At each co-training iteration, a small set of sen-
tences is drawn from a large pool of unlabeled sentences
and stored in a cache. Both parsers then attempt to label
every sentence in the cache. Next, a subset of the newly
labeled sentences is selected to be added to the train-
ing data. The examples added to the training set of one
parser (referred to as the student) are only those produced
by the other parser (referred to as the teacher), although
the methods we use generalize to the case in which the
parsers share a single training set. During selection, one
parser first acts as the teacher and the other as the student,
and then the roles are reversed.
A and B are two different parsers.
M iA and M iB are the models of A and B at step i.
U is a large pool of unlabeled sentences.
U i is a small cache holding a subset of U at step i.
L is the manually labeled seed data.
LiA and LiB are the labeled training examples for A and B
at step i.
Initialize:
L0A ? L
0
B ? L.
M0A ? Train(A,L
0
A)
M0B ? Train(B,L
0
B)
Loop:
U i ? Add unlabeled sentences from U .
M iA and M iB parse the sentences in U i and
assign scores to them according to their scoring
functions fA and fB .
Select new parses {PA} and {PB} according to some
selection method S, which uses the scores
from fA and fB .
Li+1A is L
i
A augmented with {PB}
Li+1B is L
i
B augmented with {PA}
M i+1A ? Train(A,L
i+1
A )
M i+1B ? Train(B,L
i+1
B )
Figure 1: The pseudo-code for the co-training algorithm
3 Selecting Training Examples
In each iteration, selection is performed in two steps.
First, each parser uses some scoring function, f , to assess
the parses it generated for the sentences in the cache.2
Second, the central control uses some selection method,
S, to choose a subset of these labeled sentences (based on
the scores assigned by f ) to add to the parsers? training
data. The focus of this paper is on the selection phase, but
to more fully investigate the effect of different selection
methods we also consider two possible scoring functions.
3.1 Scoring functions
The scoring function attempts to quantify the correctness
of the parses produced by each parser. An ideal scor-
ing function would give the true accuracy rates (e.g., F-
score, the combined labeled precision and recall rates).
In practice, accuracy is approximated by some notion
of confidence. For example, one easy-to-compute scor-
ing function measures the conditional probability of the
(most likely) parse. If a high probability is assigned, the
parser is said to be confident in the label it produced.
In our experimental studies, we considered the selec-
tion methods? interaction with two scoring functions: an
oracle scoring function fF-score that returns the F-score
of the parse as measured against a gold standard, and a
2In our experiments, both parsers use the same scoring func-
tion.
practical scoring function fprob that returns the condi-
tional probability of the parse.3
3.2 Selection methods
Based on the scores assigned by the scoring function,
the selection method chooses a subset of the parser la-
beled sentences that best satisfy some selection criteria.
One such criterion is the accuracy of the labeled exam-
ples, which may be estimated by the teacher parser?s con-
fidence in its labels. However, the examples that the
teacher correctly labeled may not be those that the stu-
dent needs. We hypothesize that the training utility of
the examples for the student parser is another important
criterion.
Training utility measures the improvement a parser
would make if that sentence were correctly labeled and
added to the training set. Like accuracy, the utility of
an unlabeled sentence is difficult to quantify; therefore,
we approximate it with values that can be computed from
features of the sentence. For example, sentences contain-
ing many unknown words may have high training util-
ity; so might sentences that a parser has trouble parsing.
Under the co-training framework, we estimate the train-
ing utility of a sentence for the student by comparing the
score the student assigned to its parse (according to its
scoring function) against the score the teacher assigned
to its own parse.
To investigate how the selection criteria of utility and
accuracy affect the co-training process, we considered a
number of selection methods that satisfy the requirements
of accuracy and training utility to varying degrees. The
different selection methods are shown below. For each
method, a sentence (as labeled by the teacher parser) is
selected if:
? above-n (Sabove-n): the score of the teacher?s parse
(using its scoring function) ? n.
? difference (Sdiff-n): the score of the teacher?s parse
is greater than the score of the student?s parse by
some threshold n.
? intersection (Sint-n): the score of the teacher?s parse
is in the set of the teacher?s n percent highest-
scoring labeled sentences, and the score of the stu-
dent?s parse for the same sentence is in the set of
the student?s n percent lowest-scoring labeled sen-
tences.
Each selection method has a control parameter, n, that
determines the number of labeled sentences to add at each
co-training iteration. It also serves as an indirect control
3A nice property of using conditional probability,
Pr(parse|sentence), as the scoring function is that it
normalizes for sentence length.
of the number of errors added to the training set. For ex-
ample, the Sabove-n method would allow more sentences
to be selected if n was set to a low value (with respect to
the scoring function); however, this is likely to reduce the
accuracy rate of the training set.
The above-n method attempts to maximize the accu-
racy of the data (assuming that parses with higher scores
are more accurate). The difference method attempts to
maximize training utility: as long as the teacher?s label-
ing is more accurate than that of the student, it is cho-
sen, even if its absolute accuracy rate is low. The inter-
section method attempts to maximize both: the selected
sentences are accurately labeled by the teacher and incor-
rectly labeled by the student.
4 Experiments
Experiments were performed to compare the effect of
the selection methods on co-training and corrected co-
training. We consider a selection method, S1, superior
to another, S2, if, when a large unlabeled pool of sen-
tences has been exhausted, the examples selected by S1
(as labeled by the machine, and possibly corrected by the
human) improve the parser more than those selected by
S2. All experiments shared the same general setup, as
described below.
4.1 Experimental Setup
For two parsers to co-train, they should generate com-
parable output but use independent statistical models.
In our experiments, we used a lexicalized context free
grammar parser developed by Collins (1999), and a lex-
icalized Tree Adjoining Grammar parser developed by
Sarkar (2002). Both parsers were initialized with some
seed data. Since the goal is to minimize human annotated
data, the size of the seed data should be small. In this pa-
per we used a seed set size of 1, 000 sentences, taken from
section 2 of the Wall Street Journal (WSJ) Penn Tree-
bank. The total pool of unlabeled sentences was the re-
mainder of sections 2-21 (stripped of their annotations),
consisting of about 38,000 sentences. The cache size is
set at 500 sentences. We have explored using different
settings for the seed set size (Steedman et al, 2003).
The parsers were evaluated on unseen test sentences
(section 23 of the WSJ corpus). Section 0 was used as
a development set for determining parameters. The eval-
uation metric is the Parseval F-score over labeled con-
stituents: F-score = 2?LR?LPLR+LP , where LP and LR
are labeled precision and recall rate, respectively. Both
parsers were evaluated, but for brevity, all results reported
here are for the Collins parser, which received higher Par-
seval scores.
80
80.5
81
81.5
82
82.5
83
83.5
84
0 2000 4000 6000 8000 10000 12000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Training Sentences
above-70%diff-10%int-60%No selection(Human annotated)
80
80.5
81
81.5
82
82.5
83
83.5
84
0 2000 4000 6000 8000 10000 12000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Training Sentences
above-90%diff-10%int-30%No selection(Human annotated)
(a) (b)
Figure 2: A comparison of selection methods using the oracle scoring function, fF-score, controlling for the label
quality of the training data. (a) The average accuracy rates are about 85%. (b) The average accuracy rates (except for
those selected by Sdiff-10%) are about 95%.
4.2 Experiment 1: Selection Methods and
Co-Training
We first examine the effect of the three selection meth-
ods on co-training without correction (i.e., the chosen
machine-labeled training examples may contain errors).
Because the selection decisions are based on the scores
that the parsers assign to their outputs, the reliability of
the scoring function has a significant impact on the per-
formance of the selection methods. We evaluate the ef-
fectiveness of the selection methods using two scoring
functions. In Section 4.2.1, each parser assesses its out-
put with an oracle scoring function that returns the Par-
seval F-score of the output (as compared to the human
annotated gold-standard). This is an idealized condition
that gives us direct control over the error rate of the la-
beled training data. By keeping the error rates constant,
our goal is to determine which selection method is more
successful in finding sentences with high training utility.
In Section 4.2.2 we replace the oracle scoring function
with fprob, which returns the conditional probability of
the best parse as the score. We compare how the selection
methods? performances degrade under the realistic con-
dition of basing selection decisions on unreliable parser
output assessment scores.
4.2.1 Using the oracle scoring function, fF-score
The goal of this experiment is to evaluate the selection
methods using a reliable scoring function. We therefore
use an oracle scoring function, fF-score, which guaran-
tees a perfect assessment of the parser?s output. This,
however, may be too powerful. In practice, we expect
even a reliable scoring function to sometimes assign high
scores to inaccurate parses. We account for this effect by
adjusting the selection method?s control parameter to af-
fect two factors: the accuracy rate of the newly labeled
training data, and the number of labeled sentences added
at each training iteration. A relaxed parameter setting
adds more parses to the training data, but also reduces
the accuracy of the training data.
Figure 2 compares the effect of the three selection
methods on co-training for the relaxed (left graph) and
the strict (right graph) parameter settings. Each curve in
the two graphs charts the improvement in the parser?s ac-
curacy in parsing the test sentences (y-axis) as it is trained
on more data chosen by its selection method (x-axis).
The curves have different endpoints because the selection
methods chose a different number of sentences from the
same 38K unlabeled pool. For reference, we also plotted
the improvement of a fully-supervised parser (i.e., trained
on human-annotated data, with no selection).
For the more relaxed setting, the parameters are chosen
so that the newly labeled training data have an average
accuracy rate of about 85%:
? Sabove-70% requires the labels to have an F-score ?
70%. It adds about 330 labeled sentences (out of the
500 sentence cache) with an average accuracy rate
of 85% to the training data per iteration.
? Sdiff-10% requires the score difference between the
teacher?s labeling and the student?s labeling to be at
least 10%. It adds about 50 labeled sentences with
an average accuracy rate of 80%.
? Sint-60% requires the teacher?s parse to be in the
top 60% of its output and the student?s parse for the
same sentence to be in its bottom 60%. It adds about
150 labeled sentences with an average accuracy rate
of 85%.
Although none rivals the parser trained on human an-
notated data, the selection method that improves the
parser the most is Sdiff-10%. One interpretation is that
the training utility of the examples chosen by Sdiff-10%
outweighs the cost of errors introduced into the training
data. Another interpretation is that the other two selection
methods let in too many sentences containing errors. In
the right graph, we compare the same Sdiff-10% with the
other two selection methods using stricter control, such
that the average accuracy rate for these methods is now
about 95%:
? Sabove-90% now requires the parses to be at least
90% correct. It adds about 150 labeled sentences
per iteration.
? Sint-30% now requires the teacher?s parse to be in
the top 30% of its output and the student?s parse for
the same sentence in its bottom 30%. It adds about
15 labeled sentences.
The stricter control on Sabove-90% improved the
parser?s performance, but not enough to overtake
Sdiff-10% after all the sentences in the unlabeled pool
had been considered, even though the training data of
Sdiff-10% contained many more errors. Sint-30% has a
faster initial improvement4, closely tracking the progress
of the fully-supervised parser. However, the stringent re-
quirement exhausted the unlabeled data pool before train-
ing the parser to convergence. Sint-30% might continue
to help the parser to improve if it had access to more un-
labeled data, which is easier to acquire than annotated
data5.
Comparing the three selection methods under both
strict and relaxed control settings, the results suggest that
training utility is an important criterion in selecting train-
ing examples, even at the cost of reduced accuracy.
4.2.2 Using the fprob scoring function
To determine the effect of unreliable scores on the se-
lection methods, we replace the oracle scoring function,
fF-score, with fprob, which approximates the accuracy
of a parse with its conditional probability. Although this
is a poor estimate of accuracy (especially when computed
from a partially trained parser), it is very easy to compute.
The unreliable scores also reduce the correlation between
the selection control parameters and the level of errors in
the training data. In this experiment, we set the parame-
ters for all three selection methods so that approximately
4A fast improvement rate is not a central concern here, but
it will be more relevant for corrected co-training.
5This oracle experiment is bounded by the size of the anno-
tated portion of the WSJ corpus.
79.8
80
80.2
80.4
80.6
80.8
81
81.2
1000 1500 2000 2500 3000 3500 4000 4500 5000
Par
sin
g A
ccu
rac
y o
n T
est
 Da
ta (F
scor
e)
Number of Training Sentences
above-70%diff-30%int-30%
Figure 3: A comparison of selection methods using the
conditional probability scoring function, fprob.
30-50 sentences were added to the training data per iter-
ation. The average accuracy rate of the training data for
Sabove-70% was about 85%, and the rate for Sdiff-30%
and Sint-30% was about 75%.
As expected, the parser performances of all three selec-
tion methods using fprob (shown in Figure 3) are lower
than using fF-score (see Figure 2). However, Sdiff-30%
and Sint-30% helped the co-training parsers to improve
with a 5% error reduction (1% absolute difference) over
the parser trained only on the initial seed data. In con-
trast, despite an initial improvement, using Sabove-70%
did not help to improve the parser. In their experiments on
NP identifiers, Pierce and Cardie (2001) observed a sim-
ilar effect. They hypothesize that co-training does not
scale well for natural language learning tasks that require
a huge amount of training data because too many errors
are accrued over time. Our experimental results suggest
that the use of training utility in the selection process can
make co-training parsers more tolerant to these accumu-
lated errors.
4.3 Experiment 2: Selection Methods and
Corrected Co-training
To address the problem of the training data accumulating
too many errors over time, Pierce and Cardie proposed
a semi-supervised variant of co-training called corrected
co-training, which allows a human annotator to review
and correct the output of the parsers before adding it to
the training data. The main selection criterion in their
co-training system is accuracy (approximated by confi-
dence). They argue that selecting examples with nearly
correct labels would require few manual interventions
from the annotator.
We hypothesize that it may be beneficial to consider
the training utility criterion in this framework as well.
We perform experiments to determine whether select-
ing fewer (and possibly less accurately labeled) exam-
80
81
82
83
84
85
86
87
2000 4000 6000 8000 10000 12000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Training Sentences
above-90%diff-10%int-30%No selection
80
81
82
83
84
85
86
87
0 5000 10000 15000 20000 25000 30000 35000 40000 45000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Constituents to Correct in the Training Data
above-90%diff-10%int-30%No selection
(a) (b)
Figure 4: A comparison of selection methods for corrected co-training using fF-score (a) in terms of the number of
sentences added to the training data; (b) in terms of the number of manually corrected constituents.
ples with higher training utility would require less effort
from the annotator. In our experiments, we simulated
the interactive sample selection process by revealing the
gold standard. As before, we compare the three selection
methods using both fF-score and fprob as scoring func-
tions.6
4.3.1 Using the oracle scoring function, fF-score
Figure 4 shows the effect of the three selection meth-
ods (using the strict parameter setting) on corrected co-
training. As a point of reference, we plot the improve-
ment rate for a fully supervised parser (same as the one
in Figure 2). In addition to charting the parser?s perfor-
mance in terms of the number of labeled training sen-
tences (left graph), we also chart the parser?s performance
in terms of the the number of constituents the machine
mislabeled (right graph). The pair of graphs indicates the
amount of human effort required: the left graph shows
the number of sentences the human has to check, and the
right graph shows the number of constituents the human
has to correct.
Comparing Sabove-90% and Sdiff-10%, we see that
Sdiff-10% trains a better parser than Sabove-90% when all
the unlabeled sentences have been considered. It also im-
proves the parser using a smaller set of training exam-
ples. Thus, for the same parsing performance, it requires
the human to check fewer sentences than Sabove-90% and
the reference case of no selection (Figure 4(a)). On the
other hand, because the labeled sentences selected by
Sdiff-10% contain more mistakes than those selected by
Sabove-90%, Sdiff-10% requires slightly more corrections
6The selection control parameters are the same as the previ-
ous set of experiments, using the strict setting (i.e., Figure 2(b))
for fF-score.
than Sabove-90% for the same level of parsing perfor-
mance; though both require fewer corrections than the
reference case of no selection (Figure 4(b)). Because
the amount of effort spent by the annotator depends on
the number of sentences checked as well as the amount
of corrections made, whether Sdiff-10% or Sabove-90% is
more effort reducing may be a matter of the annotator?s
preference.
The selection method that improves the parser at the
fastest rate is Sint-30%. For the same parser performance
level, it selects the fewest number of sentences for a hu-
man to check and requires the human to make the least
number of corrections. However, as we have seen in the
earlier experiment, very few sentences in the unlabeled
pool satisfy its stringent criteria, so it ran out of data be-
fore the parser was trained to convergence. At this point
we cannot determine whether Sint-30% might continue to
improve the parser if we used a larger set of unlabeled
data.
4.3.2 Using the fprob scoring function
We also consider the effect of unreliable scores in the
corrected co-training framework. A comparison between
the selection methods using fprob is reported in Figure
5. The left graph charts parser performance in terms of
the number of sentences the human must check; the right
charts parser performance in terms of the number of con-
stituents the human must correct. As expected, the unreli-
able scoring function degrades the effectiveness of the se-
lection methods; however, compared to its unsupervised
counterpart (Figure 3), the degradation is not as severe.
In fact, Sdiff-30% and Sint-30% still require fewer train-
ing data than the reference parser. Moreover, consistent
with the other experiments, the selection methods that at-
tempt to maximize training utility achieve better parsing
performance than Sabove-70%. Finally, in terms of reduc-
ing human effort, the three selection methods require the
human to correct comparable amount of parser errors for
the same level of parsing performance, but for Sdiff-30%
and Sint-30%, fewer sentences need to be checked.
4.3.3 Discussion
Corrected co-training can be seen as a form of active
learning, whose goal is to identify the smallest set of un-
labeled data with high training utility for the human to
label. Active learning can be applied to a single learner
(Lewis and Catlett, 1994) and to multiple learners (Fre-
und et al, 1997; Engelson and Dagan, 1996; Ngai and
Yarowsky, 2000). In the context of parsing, all previ-
ous work (Thompson et al, 1999; Hwa, 2000; Tang et
al., 2002) has focussed on single learners. Corrected co-
training is the first application of active learning for mul-
tiple parsers. We are currently investigating comparisons
to the single learner approaches.
Our approach is similar to co-testing (Muslea et al,
2002), an active learning technique that uses two classi-
fiers to find contentious examples (i.e., data for which the
classifiers? labels disagree) for a human to label. There is
a subtle but significant difference, however, in that their
goal is to reduce the total number of labeled training ex-
amples whereas we also wish to reduce the number of
corrections made by the human. Therefore, our selection
methods must take into account the quality of the parse
produced by the teacher in addition to how different its
parse is from the one produced by the student. The inter-
section method precisely aims at selecting sentences that
satisfy both requirements. Exploring different selection
methods is part of our on-going research effort.
5 Conclusion
We have considered three selection methods that have dif-
ferent priorities in balancing the two (often competing)
criteria of accuracy and training utility. We have em-
pirically compared their effect on co-training, in which
two parsers label data for each other, as well as corrected
co-training, in which a human corrects the parser labeled
data before adding it to the training set. Our results sug-
gest that training utility is an important selection criterion
to consider, even at the cost of potentially reducing the ac-
curacy of the training data. In our empirical studies, the
selection method that aims to maximize training utility,
Sdiff-n, consistently finds better examples than the one
that aims to maximize accuracy, Sabove-n. Our results
also suggest that the selection method that aims to maxi-
mize both accuracy and utility, Sint-n, shows promise in
improving co-training parsers and in reducing human ef-
fort for corrected co-training; however, a much larger un-
labeled data set is needed to verify the benefit of Sint-n.
The results of this study indicate the need for scor-
ing functions that are better estimates of the accuracy of
the parser?s output than conditional probabilities. Our
oracle experiments show that, by using effective selec-
tion methods, the co-training process can improve parser
peformance even when the newly labeled parses are
not completely accurate. This suggests that co-training
may still be beneficial when using a practical scoring
function that might only coarsely distinguish accurate
parses from inaccurate parses. Further avenues to ex-
plore include the development of selection methods to
efficiently approximate maximizing the objective func-
tion of parser agreement on unlabeled data, following the
work of Dasgupta et al (2002) and Abney (2002). Also,
co-training might be made more effective if partial parses
were used as training data. Finally, we are conducting ex-
periments to compare corrected co-training with other ac-
tive learning methods. We hope these studies will reveal
ways to combine the strengths of co-training and active
learning to make better use of unlabeled data.
Acknowledgments
This work has been supported, in part, by NSF/DARPA
funded 2002 Human Language Engineering Workshop
at JHU, EPSRC grant GR/M96889, the Department of
Defense contract RD-02-5700, and ONR MURI Con-
tract FCPO.810548265. We would like to thank Chris
Callison-Burch, Michael Collins, John Henderson, Lil-
lian Lee, Andrew McCallum, and Fernando Pereira for
helpful discussions; to Ric Crabbe, Adam Lopez, the par-
ticipants of CS775 at Cornell University, and the review-
ers for their comments on this paper.
References
Steven Abney. 2002. Bootstrapping. In Proceedings of the
40th Annual Meeting of the Association for Computational
Linguistics, pages 360?367, Philadelphia, PA.
Avrim Blum and Tom Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In Proceedings of the
11th Annual Conference on Computational Learning Theory,
pages 92?100, Madison, WI.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st Annual Meeting of the NAACL.
David Cohn, Les Atlas, and Richard Ladner. 1994. Improv-
ing generalization with active learning. Machine Learning,
15(2):201?221.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of Penn-
sylvania.
Sanjoy Dasgupta, Michael Littman, and David McAllester.
2002. PAC generalization bounds for co-training. In T. G.
Dietterich, S. Becker, and Z. Ghahramani, editors, Advances
80
81
82
83
84
85
86
87
2000 4000 6000 8000 10000 12000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Training Sentences
above-70%diff-30%int-30%No selection
80
81
82
83
84
85
86
87
0 5000 10000 15000 20000 25000 30000 35000 40000 45000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Constituents to Correct in the Training Data
above-70%diff-30%int-30%No selection
(a) (b)
Figure 5: A comparison of selection methods for corrected co-training using fprob (a) in terms of the number of
sentences added to the training data; (b) in terms of the number of manually corrected constituents.
in Neural Information Processing Systems 14, Cambridge,
MA. MIT Press.
Sean P. Engelson and Ido Dagan. 1996. Minimizing manual
annotation cost in supervised training from copora. In Pro-
ceedings of the 34th Annual Meeting of the ACL, pages 319?
326.
Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naftali
Tishby. 1997. Selective sampling using the query by com-
mittee algorithm. Machine Learning, 28(2-3):133?168.
Sally Goldman and Yan Zhou. 2000. Enhancing supervised
learning with unlabeled data. In Proceedings of the 17th In-
ternational Conference on Machine Learning, Stanford, CA.
Rebecca Hwa. 2000. Sample selection for statistical grammar
induction. In Proceedings of the 2000 Joint SIGDAT Confer-
ence on EMNLP and VLC, pages 45?52, Hong Kong, China,
October.
David D. Lewis and Jason Catlett. 1994. Heterogeneous un-
certainty sampling for supervised learning. In Proceedings
of the Eleventh International Conference on Machine Learn-
ing, pages 148?156.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Ion Muslea, Steve Minton, and Craig Knoblock. 2002. Selec-
tive sampling with redundant views. In Proceedings of the
Seventeenth National Conference on Artificial Intelligence,
pages 621?626.
Grace Ngai and David Yarowsky. 2000. Rule writing or an-
notation: Cost-efficient resource usage for base noun phrase
chunking. In Proceedings of the 38th Annual Meeting of the
ACL, pages 117?125, Hong Kong, China, October.
David Pierce and Claire Cardie. 2001. Limitations of co-
training for natural language learning from large datasets. In
Proceedings of the Empirical Methods in NLP Conference,
Pittsburgh, PA.
Anoop Sarkar. 2001. Applying co-training methods to statisti-
cal parsing. In Proceedings of the 2nd Annual Meeting of the
NAACL, pages 95?102, Pittsburgh, PA.
Anoop Sarkar. 2002. Statistical Parsing Algorithms for Lexi-
calized Tree Adjoining Grammars. Ph.D. thesis, University
of Pennsylvania.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen Clark,
Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen, Steven
Baker, and Jeremiah Crim. 2003. Bootstrapping statistical
parsers from small datasets. In The Proceedings of the An-
nual Meeting of the European Chapter of the ACL. To ap-
pear.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002. Active
learning for statistical natural language parsing. In Proceed-
ings of the 40th Annual Meeting of the ACL, pages 120?127,
July.
Cynthia A. Thompson, Mary Elaine Califf, and Raymond J.
Mooney. 1999. Active learning for natural language pars-
ing and information extraction. In Proceedings of ICML-99,
pages 406?414, Bled, Slovenia.
Building Deep Dependency Structures with a Wide-Coverage CCG Parser
Stephen Clark, Julia Hockenmaier and Mark Steedman
Division of Informatics
University of Edinburgh
Edinburgh EH8 9LW, UK
 
stephenc, julia, steedman  @cogsci.ed.ac.uk
Abstract
This paper describes a wide-coverage sta-
tistical parser that uses Combinatory Cat-
egorial Grammar (CCG) to derive de-
pendency structures. The parser differs
from most existing wide-coverage tree-
bank parsers in capturing the long-range
dependencies inherent in constructions
such as coordination, extraction, raising
and control, as well as the standard local
predicate-argument dependencies. A set
of dependency structures used for train-
ing and testing the parser is obtained from
a treebank of CCG normal-form deriva-
tions, which have been derived (semi-) au-
tomatically from the Penn Treebank. The
parser correctly recovers over 80% of la-
belled dependencies, and around 90% of
unlabelled dependencies.
1 Introduction
Most recent wide-coverage statistical parsers have
used models based on lexical dependencies (e.g.
Collins (1999), Charniak (2000)). However, the de-
pendencies are typically derived from a context-free
phrase structure tree using simple head percolation
heuristics. This approach does not work well for the
long-range dependencies involved in raising, con-
trol, extraction and coordination, all of which are
common in text such as the Wall Street Journal.
Chiang (2000) uses Tree Adjoining Grammar
as an alternative to context-free grammar, and
here we use another ?mildly context-sensitive? for-
malism, Combinatory Categorial Grammar (CCG,
Steedman (2000)), which arguably provides the
most linguistically satisfactory account of the de-
pendencies inherent in coordinate constructions and
extraction phenomena. The potential advantage
from using such an expressive grammar is to facili-
tate recovery of such unbounded dependencies. As
well as having a potential impact on the accuracy of
the parser, recovering such dependencies may make
the output more useful.
CCG is unlike other formalisms in that the stan-
dard predicate-argument relations relevant to inter-
pretation can be derived via extremely non-standard
surface derivations. This impacts on how best to de-
fine a probability model for CCG, since the ?spuri-
ous ambiguity? of CCG derivations may lead to an
exponential number of derivations for a given con-
stituent. In addition, some of the spurious deriva-
tions may not be present in the training data. One
solution is to consider only the normal-form (Eis-
ner, 1996a) derivation, which is the route taken in
Hockenmaier and Steedman (2002b).1
Another problem with the non-standard surface
derivations is that the standard PARSEVAL per-
formance measures over such derivations are unin-
formative (Clark and Hockenmaier, 2002). Such
measures have been criticised by Lin (1995) and
Carroll et al (1998), who propose recovery of head-
dependencies characterising predicate-argument re-
lations as a more meaningful measure.
If the end-result of parsing is interpretable
predicate-argument structure or the related depen-
dency structure, then the question arises: why build
derivation structure at all? A CCG parser can
directly build derived structures, including long-
1Another, more speculative, possibility is to treat the alter-
native derivations as hidden and apply the EM algorithm.
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 327-334.
                         Proceedings of the 40th Annual Meeting of the Association for
range dependencies. These derived structures can
be of any form we like?for example, they could
in principle be standard Penn Treebank structures.
Since we are interested in dependency-based parser
evaluation, our parser currently builds dependency
structures. Furthermore, since we want to model
the dependencies in such structures, the probability
model is defined over these structures rather than the
derivation.
The training and testing material for this CCG
parser is a treebank of dependency structures, which
have been derived from a set of CCG deriva-
tions developed for use with another (normal-form)
CCG parser (Hockenmaier and Steedman, 2002b).
The treebank of derivations, which we call CCG-
bank (Hockenmaier and Steedman, 2002a), was in
turn derived (semi-)automatically from the hand-
annotated Penn Treebank.
2 The Grammar
In CCG, most language-specific aspects of the gram-
mar are specified in the lexicon, in the form of syn-
tactic categories that identify a lexical item as either
a functor or argument. For the functors, the category
specifies the type and directionality of the arguments
and the type of the result. For example, the follow-
ing category for the transitive verb bought specifies
its first argument as a noun phrase (NP) to its right
and its second argument as an NP to its left, and its
result as a sentence:
(1) bought :=  S  NP  NP
For parsing purposes, we extend CCG categories
to express category features, and head-word and de-
pendency information directly, as follows:
(2) bought :=  S  dcl  bought  NP1  NP2
The feature  dcl specifies the category?s S result as a
declarative sentence, bought identifies its head, and
the numbers denote dependency relations. Heads
and dependencies are always marked up on atomic
categories (S, N, NP, PP, and conj in our implemen-
tation).
The categories are combined using a small set of
typed combinatory rules, such as functional applica-
tion and composition (see Steedman (2000) for de-
tails). Derivations are written as follows, with under-
lines indicating combinatory reduction and arrows
indicating the direction of the application:
(3) Marks bought Brooks
NPMarks 	 S 
 dcl  bought  NP1  NP2 NPBrooks
S 
 dcl  bought  NP1 
S 
 dcl  bought
Formally, a dependency is defined as a 4-tuple:

h f  f  s  ha  , where h f is the head word of the func-
tor,2 f is the functor category (extended with head
and dependency information), s is the argument slot,
and ha is the head word of the argument?for exam-
ple, the following is the object dependency yielded
by the first step of derivation (3):
(4)  bought

 S  dcl  bought  NP1  NP2  2  Brooks 
Variables can also be used to denote heads, and
used via unification to pass head information from
one category to another. For example, the expanded
category for the control verb persuade is as follows:
(5) persuade :=  S  dcl  persuade  NP1  S  to  2  NPX  NPX,3
The head of the infinitival complement?s subject is
identified with the head of the object, using the vari-
able X. Unification then ?passes? the head of the ob-
ject to the subject of the infinitival, as in standard
unification-based accounts of control.3
The kinds of lexical items that use the head pass-
ing mechanism are raising, auxiliary and control
verbs, modifiers, and relative pronouns. Among the
constructions that project unbounded dependencies
are relativisation and right node raising. The follow-
ing category for the relative pronoun category (for
words such as who, which, that) shows how heads
are co-indexed for object-extraction:
(6) who :=  NPX  NPX,1  S  dcl  2  NPX 
The derivation for the phrase The company that
Marks wants to buy is given in Figure 1 (with the
features on S categories removed to save space, and
the constant heads reduced to the first letter). Type-
raising (  ) and functional composition (  ), along
2Note that the functor does not always correspond to the lin-
guistic notion of a head.
3The extension of CCG categories in the lexicon and the la-
belled data is simplified in the current system to make it entirely
automatic. For example, any word with the same category (5)
as persuade gets the object-control extension. In certain rare
cases (such as promise) this gives semantically incorrect depen-
dencies in both the grammar and the data (promise Brooks to go
has a structure meaning promise Brooks that Brooks will go).
The company that Marks wants to buy
NPx

Nx,1 Nc
	
NPx  NPx,1

	
S2

NPx

NPm
	
Sw  NPx,1

	
S2  NPx

	
Sy  NPx,1

	
Sy,2  NPx

	
Sb  NP1

NP2

 ffGenerative Models for Statistical Parsing with Combinatory Categorial
Grammar
Julia Hockenmaier and Mark Steedman
Division of Informatics
University of Edinburgh
Edinburgh EH8 9LW, United Kingdom
fjulia, steedmang@cogsci.ed.ac.uk
Abstract
This paper compares a number of gen-
erative probability models for a wide-
coverage Combinatory Categorial Gram-
mar (CCG) parser. These models are
trained and tested on a corpus obtained by
translating the Penn Treebank trees into
CCG normal-form derivations. According
to an evaluation of unlabeled word-word
dependencies, our best model achieves a
performance of 89.9%, comparable to the
figures given by Collins (1999) for a lin-
guistically less expressive grammar. In
contrast to Gildea (2001), we find a signif-
icant improvement from modeling word-
word dependencies.
1 Introduction
The currently best single-model statistical parser
(Charniak, 1999) achieves Parseval scores of over
89% on the Penn Treebank. However, the grammar
underlying the Penn Treebank is very permissive,
and a parser can do well on the standard Parseval
measures without committing itself on certain se-
mantically significant decisions, such as predicting
null elements arising from deletion or movement.
The potential benefit of wide-coverage parsing with
CCG lies in its more constrained grammar and its
simple and semantically transparent capture of ex-
traction and coordination.
We present a number of models over syntac-
tic derivations of Combinatory Categorial Grammar
(CCG, see Steedman (2000) and Clark et al (2002),
this conference, for introduction), estimated from
and tested on a translation of the Penn Treebank
to a corpus of CCG normal-form derivations. CCG
grammars are characterized by much larger category
sets than standard Penn Treebank grammars, distin-
guishing for example between many classes of verbs
with different subcategorization frames. As a re-
sult, the categorial lexicon extracted for this purpose
from the training corpus has 1207 categories, com-
pared with the 48 POS-tags of the Penn Treebank.
On the other hand, grammar rules in CCG are lim-
ited to a small number of simple unary and binary
combinatory schemata such as function application
and composition. This results in a smaller and less
overgenerating grammar than standard PCFGs (ca.
3,000 rules when instantiated with the above cate-
gories in sections 02-21, instead of >12,400 in the
original Treebank representation (Collins, 1999)).
2 Evaluating a CCG parser
Since CCG produces unary and binary branching
trees with a very fine-grained category set, CCG
Parseval scores cannot be compared with scores
of standard Treebank parsers. Therefore, we also
evaluate performance using a dependency evalua-
tion reported by Collins (1999), which counts word-
word dependencies as determined by local trees and
their labels. According to this metric, a local tree
with parent node P, head daughter H and non-head
daughter S (and position of S relative to P, ie. left
or right, which is implicit in CCG categories) de-
fines a hP;H;Si dependency between the head word
of S, wS, and the head word of H , wH. This measure
is neutral with respect to the branching factor. Fur-
thermore, as noted by Hockenmaier (2001), it does
not penalize equivalent analyses of multiple modi-
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 335-342.
                         Proceedings of the 40th Annual Meeting of the Association for
Pierre Vinken ; 61 years old ; will join the board as a nonexecutive director Nov 29
N=N N ; N=N N (S[adj]nNP)nNP ; (S[dcl]nNP)=(S[b]nNP) ((S[b]nNP)=PP)=NP NP=N N PP=NP NP=N N=N N ((SnNP)n(SnNP))=N N
> > > > >
N N N N (SnNP)n(SnNP)
>
NP NP NP NP
< > >
NP S[adj]nNP (S[b]nNP)=PP PP
>
NPnNP S[b]nNP
< <
NP S[b]nNP
>
NP S[dcl]nNP
<
S[dcl]
Figure 1: A CCG derivation in our corpus
fiers. In the unlabeled case hi (where it only matters
whether word a is a dependent of word b, not what
the label of the local tree is which defines this depen-
dency), scores can be compared across grammars
with different sets of labels and different kinds of
trees. In order to compare our performance with the
parser of Clark et al (2002), we also evaluate our
best model according to the dependency evaluation
introduced for that parser. For further discussion we
refer the reader to Clark and Hockenmaier (2002) .
3 CCGbank?a CCG treebank
CCGbank is a corpus of CCG normal-form deriva-
tions obtained by translating the Penn Tree-
bank trees using an algorithm described by
Hockenmaier and Steedman (2002). Almost all
types of construction?with the exception of gap-
ping and UCP (?Unlike Coordinate Phrases?) are
covered by the translation procedure, which pro-
cesses 98.3% of the sentences in the training corpus
(WSJ sections 02-21) and 98.5% of the sentences
in the test corpus (WSJ section 23). The grammar
contains a set of type-changing rules similar to the
lexical rules described in Carpenter (1992). Figure
1 shows a derivation taken from CCGbank. Cate-
gories, such as ((S[b]nNP)=PP)=NP, encode unsat-
urated subcat frames. The complement-adjunct dis-
tinction is made explicit; for instance as a nonexec-
utive director is marked up as PP-CLR in the Tree-
bank, and hence treated as a PP-complement of join,
whereas Nov. 29 is marked up as an NP-TMP and
therefore analyzed as VP modifier. The -CLR tag
is not in fact a very reliable indicator of whether a
constituent should be treated as a complement, but
the translation to CCG is automatic and must do the
best it can with the information in the Treebank.
The verbal categories in CCGbank carry fea-
tures distinguishing declarative verbs (and auxil-
iaries) from past participles in past tense, past par-
ticiples for passive, bare infinitives and ing-forms.
There is a separate level for nouns and noun phrases,
but, like the nonterminal NP in the Penn Treebank,
noun phrases do not carry any number agreement.
The derivations in CCGbank are ?normal-form? in
the sense that analyses involving the combinatory
rules of type-raising and composition are only used
when syntactically necessary.
4 Generative models of CCG derivations
Expansion HeadCat NonHeadCat
P(exp j : : : ) P(H j : : : ) P(S j : : : )
Baseline P P;exp P;exp;H
+ Conj P;con jP P;exp;con jP P;exp;H ;con jP
+ Grandparent P;GP P;GP;exp P;GP;exp;H
+ ? P#?L;RP P;exp#?L;RP P;exp;H#?L;RP
Table 1: The unlexicalized models
The models described here are all extensions of
a very simple model which models derivations by a
top-down tree-generating process. This model was
originally described in Hockenmaier (2001), where
it was applied to a preliminary version of CCGbank,
and its definition is repeated here in the top row of
Table 1. Given a (parent) node with category P,
choose the expansion exp of P, where exp can be
leaf (for lexical categories), unary (for unary ex-
pansions such as type-raising), left (for binary trees
where the head daughter is left) or right (binary
trees, head right). If P is a leaf node, generate its
head word w. Otherwise, generate the category of
its head daughter H . If P is binary branching, gen-
erate the category of its non-head daughter S (a
complement or modifier of H).
The model itself includes no prior knowledge spe-
cific to CCG other than that it only allows unary and
binary branching trees, and that the sets of nontermi-
nals and terminals are not disjoint (hence the need to
include leaf as a possible expansion, which acts as a
stop probability).
All the experiments reported in this section were
conducted using sections 02-21 of CCGbank as
training corpus, and section 23 as test corpus. We
replace all rare words in the training data with their
POS-tag. For all experiments reported here and in
section 5, the frequency threshold was set to 5. Like
Collins (1999), we assume that the test data is POS-
tagged, and can therefore replace unknown words in
the test data with their POS-tag, which is more ap-
propriate for a formalism like CCG with a large set
of lexical categories than one generic token for all
unknown words.
The performance of the baseline model is shown
in the top row of table 3. For six out of the 2379
sentences in our test corpus we do not get a parse.1
The reason is that a lexicon consisting of the word-
category pairs observed in the training corpus does
not contain all the entries required to parse the test
corpus. We discuss a simple, but imperfect, solution
to this problem in section 7.
5 Extending the baseline model
State-of-the-art statistical parsers use many other
features, or conditioning variables, such as head
words, subcategorization frames, distance measures
and grandparent nodes. We too can extend the
baseline model described in the previous section
by including more features. Like the models of
Goodman (1997), the additional features in our
model are generated probabilistically, whereas in
the parser of Collins (1997) distance measures are
assumed to be a function of the already generated
structure and are not generated explicitly.
In order to estimate the conditional probabilities
of our model, we recursively smooth empirical es-
timates e?i of specific conditional distributions with
(possible smoothed) estimates of less specific distri-
butions e?i 1, using linear interpolation:
e?i = ?e?i +(1 ?)e?i 1
? is a smoothing weight which depends on the par-
ticular distribution.2
When defining models, we will indicate a back-
off level with a # sign between conditioning vari-
ables, eg. A;B # C # D means that we interpolate
?P(::: j A;B;C;D) with ?P(::: j A;B;C), which is an in-
terpolation of ?P(::: j A;B;C) and ?P(::: j A;B).
1We conjecture that the minor variations in coverage among
the other models (except Grandparent) are artefacts of the beam.
2We compute ? in the same way as Collins (1999), p. 185.
5.1 Adding non-lexical information
The coordination feature We define a boolean
feature, conj, which is true for constituents which
expand to coordinations on the head path.
S, +conj
S=NP, +conj
S=NP,  conj
S=(SnNP)
NP
IBM
(SnNP)=NP
buys
S=NP[c], +conj
conj
but
S=NP[c],  conj
S=(SnNP)
NP
Lotus
(SnNP)=NP
sells
NP
shares
This feature is generated at the root of the sentence
with P(conj j TOP). For binary expansions, conjH
is generated with P(conjH j H;S;con jP) and conjS is
generated with P(conjS j S # P;expP;H;conjP). Ta-
ble 1 shows how conj is used as a conditioning vari-
able. This is intended to allow the model to cap-
ture the fact that, for a sentence without extraction,
a CCG derivation where the subject is type-raised
and composed with the verb is much more likely in
right node raising constructions like the above.
The impact of the grandparent feature
Johnson (1998) showed that a PCFG estimated
from a version of the Penn Treebank in which
the label of a node?s parent is attached to the
node?s own label yields a substantial improvement
(LP/LR: from 73.5%/69.7% to 80.0%/79.2%).
The inclusion of an additional grandparent feature
gives Charniak (1999) a slight improvement in the
Maximum Entropy inspired model, but a slight
decrease in performance for an MLE model. Table
3 (Grandparent) shows that a grammar transfor-
mation like Johnson?s does yield an improvement,
but not as dramatic as in the Treebank-CFG case.
At the same time coverage is reduced (which might
not be the case if this was an additional feature in
the model rather than a change in the representation
of the categories). Both of these results are to be
expected?CCG categories encode more contextual
information than Treebank labels, in particular
about parents and grandparents; therefore the his-
tory feature might be expected to have less impact.
Moreover, since our category set is much larger,
appending the parent node will lead to an even more
fine-grained partitioning of the data, which then
results in sparse data problems.
Distance measures for CCG Our distance mea-
sures are related to those proposed by Goodman
(1997), which are appropriate for binary trees (un-
like those of Collins (1997)). Every node has a left
distance measure, ?L, measuring the distance from
the head word to the left frontier of the constituent.
There is a similar right distance measure ?R. We
implemented three different ways of measuring dis-
tance: ?Adjacency measures string adjacency (0, 1 or
2 and more intervening words); ?Verb counts inter-
vening verbs (0 or 1 and more); and ?Pct counts in-
tervening punctuation marks (0, 1, 2 or 3 and more).
These ?s are generated by the model in the follow-
ing manner: at the root of the sentence, generate ?L
with P(?L j TOP), and ?R with P(?R j TOP;?L).
Then, for each expansion, if it is a unary expan-
sion, ?LH = ?LP and ?RH = ?RP with a probabil-
ity of 1. If it is a binary expansion, only the ? in
the direction of the sister changes, with a probability
of P(?LH j ?LPH#P;S) if exp = right, and analo-
gously for exp=left. ?LS and ?RS are conditioned
on S and the ? of H and P in the direction of S:
P(?LS j S#?RP;?RH) and P(?RS j S;?LS#?RP;?RH).
They are then used as further conditioning variables
for the other distributions as shown in table 1.
Table 3 also gives the Parseval and dependency
scores obtained with each of these measures. ?Pct
has the smallest effect. However, our model does
not yet contain anything like the hard constraint on
punctuation marks in Collins (1999).
5.2 Adding lexical information
Gildea (2001) shows that removing the lexical de-
pendencies in Model 1 of Collins (1997) (that is,
not conditioning on wh when generating ws) de-
creases labeled precision and recall by only 0.5%.
It can therefore be assumed that the main influence
of lexical head features (words and preterminals) in
Collins? Model 1 is on the structural probabilities.
In CCG, by contrast, preterminals are lexical cat-
egories, encoding complete subcategorization infor-
mation. They therefore encode more information
about the expansion of a nonterminal than Treebank
POS-tags and thus are more constraining.
Generating a constituent?s lexical category c at its
maximal projection (ie. either at the root of the tree,
TOP, or when generating a non-head daughter S),
and using the lexical category as conditioning vari-
able (LexCat) increases performance of the baseline
model as measured by hP;H;Si by almost 3%. In
this model, cS, the lexical category of S depends on
the category S and on the local tree in which S is
generated. However, slightly worse performance is
obtained for LexCatDep, a model which is identical
to the original LexCat model, except that cS is also
conditioned on cH , the lexical category of the head
node, which introduces a dependency between the
lexical categories.
Since there is so much information in the lexical
categories, one might expect that this would reduce
the effect of conditioning the expansion of a con-
stituent on its head word w. However, we did find a
substantial effect. Generating the head word at the
maximal projection (HeadWord) increases perfor-
mance by a further 2%. Finally, conditioning wS
on wH , hence including word-word dependencies,
(HWDep) increases performance even more, by an-
other 3.5%, or 8.3% overall. This is in stark contrast
to Gildea?s findings for Collins? Model 1.
We conjecture that the reason why CCG benefits
more from word-word dependencies than Collins?
Model 1 is that CCG allows a cleaner parametriza-
tion of these surface dependencies. In Collins?
Model 1, wS is conditioned not only on the local
tree hP;H;Si, cH and wH , but also on the distance ?
between the head and the modifier to be generated.
However, Model 1 does not incorporate the notion
of subcategorization frames. Instead, the distance
measure was found to yield a good, if imperfect, ap-
proximation to subcategorization information.
Using our notation, Collins? Model 1 generates wS
with the following probability:
PCollins1(wS j cS;?;P;H;S;cH;wH) =
?1 ?P(wS j cS;?;P;H;S;cH ;wH)
+(1 ?1)

?2 ?P(wS j cS;?;P;H;S;cH)+(1 ?2) ?P(wS j cS)

?whereas the CCG dependency model generates
wS as follows:
PCCGdep(wS j cS;P;H;S;cH ;wH) =
? ?P(wS j cS;P;H;S;cH ;wH)+(1 ?) ?P(wS j cS)
Since our P, H , S and cH are CCG categories, and
hence encode subcategorization information, the lo-
cal tree always identifies a specific argument slot.
Therefore it is not necessary for us to include a dis-
tance measure in the dependency probabilities.
Expansion HeadCat NonHeadCat LexCat Head word
P(exp j :::) P(H j :::) P(S j :::) P(cS j :::) P(cTOPj:::) P(wS j :::) P(wTOP j:::)
LexCat P;cP P;exp;cP P;exp;H#cP S#H;exp;P P=TOP ? ?
LexCatDep P;cP P;exp;cP P;exp;H#cP S#H;exp;P#cP P=TOP ? ?
HeadWord P;cP#wP P;exp;cP#wP P;exp;H#cP#wP S#H;exp;P P=TOP cS cP
HWDep P;cP#wP P;exp;cP#wP P;exp;H#cP#wP S#H;exp;P P=TOP cS#P;H;S;wP cP
HWDep? P;cP#?L;RP#wP P;exp;cP#?L;RP#wP P;exp;H#?L;RP#cP#wP S#H;exp;P P=TOP cS#P;H;S;wP cP
HWDepConj P;cP;conjP#wP P;exp;cP;conjP#wP P;exp;H;conjP#cP#wP S#H;exp;P P=TOP cS#P;H;S;wP cP
Table 2: The lexicalized models
Model NoParse LexCat LP LR BP BR hP;H;Si hSi hi CM on hi 2 CD
Baseline 6 87.7 72.8 72.4 78,3 77.9 75.7 81.1 84.3 23.0 51.1
Conj 9 87.8 73.8 73.9 79.3 79.3 76.7 82.0 85.1 24.3 53.2
Grandparent 91 88.8 77.1 77.6 82.4 82.9 79.9 84.7 87.9 30.9 63.8
?Pct 6 88.1 73.7 73.1 79.2 78.6 76.5 81.8 84.9 23.1 53.2
?Verb 6 88.0 75.9 75.5 81.6 81.1 76.9 82.3 85.3 25.2 55.1
?Adjacency 6 88.6 77.5 77.3 82.9 82.8 78.9 83.8 86.9 24.8 59.6
LexCat 9 88.5 75.8 76.0 81.3 81.5 78.6 83.7 86.8 27.4 57.8
LexCatDep 9 88.5 75.7 75.9 81.2 81.4 78.4 83.5 86.6 26.3 57.9
HeadWord 8 89.6 77.9 78.0 83.0 83.1 80.5 85.2 88.3 30.4 63.0
HWDep 8 92.0 81.6 81.9 85.5 85.9 84.0 87.8 90.1 37.9 69.2
HWDep? 8 90.9 81.4 81.6 86.1 86.3 83.0 87.0 89.8 35.7 68.7
HWDepConj 9 91.8 80.7 81.2 84.8 85.3 83.6 87.5 89.9 36.5 68.6
HWDep (+ tagger) 7 91.7 81.4 81.8 85.6 85.9 83.6 87.5 89.9 38.1 69.1
Table 3: Performance of the models: LexCat indicates accuracy of the lexical categories; LP, LR, BP and
BR (the standard Parseval scores labeled/bracketed precision and recall) are not commensurate with other
Treebank parsers. hP;H;Si, hSi, and hi are as defined in section 2. CM on hi is the percentage of sentences
with complete match on hi, and 2 CD is the percentage of sentences with under 2 ?crossing dependencies?
as defined by hi.
The hP;H;Si labeled dependencies we report are
not directly comparable with Collins (1999), since
CCG categories encode subcategorization frames.
For instance, if the direct object of a verb has been
recognized as such, but a PP has been mistaken as
a complement (whereas the gold standard says it
is an adjunct), the fully labeled dependency eval-
uation hP;H;Si will not award a point. Therefore,
we also include in Table 3 a more comparable eval-
uation hSi which only takes the correctness of the
non-head category into account. The reported fig-
ures are also deflated by retaining verb features like
tensed/untensed. If this is done (by stripping off
all verb features), an improvement of 0.6% on the
hP;H;Si score for our best model is obtained.
5.3 Combining lexical and non-lexical
information
When incorporating the adjacency distance mea-
sure or the coordination feature into the dependency
model (HWDep? and HWDepConj), overall per-
formance is lower than with the dependency model
alone. We conjecture that this arises from data
sparseness. It cannot be concluded from these re-
sults alone that the lexical dependencies make struc-
tural information redundant or superfluous. Instead,
it is quite likely that we are facing an estimation
problem similar to Charniak (1999), who reports
that the inclusion of the grandparent feature worsens
performance of an MLE model, but improves per-
formance if the individual distributions are modelled
using Maximum Entropy. This intuition is strength-
ened by the fact that, on casual inspection of the
scores for individual sentences, it is sometimes the
case that the lexicalized models perform worse than
the unlexicalized models.
5.4 The impact of tagging errors
All of the experiments described above use the POS-
tags as given by CCGbank (which are the Treebank
tags, with some corrections necessary to acquire cor-
rect features on categories). It is reasonable to as-
sume that this input is of higher quality than can
be produced by a POS-tagger. We therefore ran the
dependency model on a test corpus tagged with the
POS-tagger of Ratnaparkhi (1996), which is trained
on the original Penn Treebank (see HWDep (+ tag-
ger) in Table 3). Performance degrades slightly,
which is to be expected, since our approach makes
so much use of the POS-tag information for un-
known words. However, a POS-tagger trained on
CCGbank might yield slightly better results.
5.5 Limitations of the current model
Unlike Clark et al (2002), our parser does not al-
ways model the dependencies in the logical form.
For example, in the interpretation of a coordinate
structure like ?buy and sell shares?, shares will head
an object of both buy and sell. Similarly, in examples
like ?buy the company that wins?, the relative con-
struction makes company depend upon both buy as
object and wins as subject. As is well known (Ab-
ney, 1997), DAG-like dependencies cannot in gen-
eral be modeled with a generative approach of the
kind taken here3.
5.6 Comparison with Clark et al (2002)
Clark et al (2002) presents another statistical CCG
parser, which is based on a conditional (rather
than generative) model of the derived depen-
dency structure, including non-surface dependen-
cies. The following table compares the two parsers
according to the evaluation of surface and deep
dependencies given in Clark et al (2002). We
use Clark et al?s parser to generate these de-
pendencies from the output of our parser (see
Clark and Hockenmaier (2002)) 4.
LP LR UP UR
Clark 81.9% 81.8% 89.1% 90.1%
Hockenmaier 83.7% 84.2% 90.5% 91.1%
6 Performance on specific constructions
One of the advantages of CCG is that it provides a
simple, surface grammatical analysis of extraction
and coordination. We investigate whether our best
3It remains to be seen whether the more restricted reentran-
cies of CCG will ultimately support a generative model.
4Due to the smaller grammar and lexicon of Clark et al, our
parser can only be evaluated on slightly over 94% of the sen-
tences in section 23, whereas the figures for Clark et al (2002)
are on 97%.
model, HWDep, predicts the correct analyses, using
the development section 00.
Coordination There are two instances of argu-
ment cluster coordination (constructions like cost
$5,000 in July and $6,000 in August) in the devel-
opment corpus. Of these, HWDep recovers none
correctly. This is a shortcoming in the model, rather
than in CCG: the relatively high probability both of
the NP modifier analysis of PPs like in July and of
NP coordination is enough to misdirect the parser.
There are 203 instances of verb phrase coordina-
tion (S[:]nNP, with [:] any verbal feature) in the de-
velopment corpus. On these, we obtain a labeled re-
call and precision of 67.0%/67.3%. Interestingly, on
the 24 instances of right node raising (coordination
of (S[:]nNP)=NP), our parser achieves higher per-
formance, with labeled recall and precision of 79.2%
and 73.1%. Figure 2 gives an example of the output
of our parser on such a sentence.
Extraction Long-range dependencies are not cap-
tured by the evaluation used here. However, the ac-
curacy for recovering lexical categories for words
with ?extraction? categories, such as relative pro-
nouns, gives some indication of how well the model
detects the presence of such dependencies.
The most common category for subject relative
pronouns, (NPnNP)=(S[dcl]nNP), has been recov-
ered with precision and recall of 97.1% (232 out of
239) and 94.3% (232/246).
Embedded subject extraction requires the special
lexical category ((S[dcl]nNP)=NP)=(S[dcl]nNP)
for verbs like think. On this category, the model
achieves a precision of 100% (5/5) and recall of
83.3% (5/6). The case the parser misanalyzed is due
to lexical coverage: the verb agree occurs in our lex-
icon, but not with this category.
The most common category for object relative
pronouns, (NPnNP)=(S[dcl]=NP), has a recall of
76.2% (16 out of 21) and precision of 84.2% (16/19).
Free object relatives, NP=(S[dcl]=NP), have a
recall of 84.6% (11/13), and precision of 91.7%
(11/12). However, object extraction appears more
frequently as a reduced relative (the man John saw),
and there are no lexical categories indicating this ex-
traction. Reduced relative clauses are captured by a
type-changing rule NPnNP ! S[dcl]=NP. This rule
was applied 56 times in the gold standard, and 70
S[dcl]
NP
the suit
S[dcl]nNP
S[dcl]nNP
(S[dcl]nNP)=NP
seeks
NP
a court order
(SnNP)n(SnNP)
S[ng]nNP
(S[ng]nNP)=PP
((S[ng]nNP)=PP)=NP
preventing
NP
the guild
PP
PP=(S[ng]nNP)
from
S[ng]nNP
(S[ng]nNP)=NP
(S[ng]nNP)=NP
punishing
(S[ng]nNP)=NP[c]
conj
or
(S[ng]nNP)=NP
(S[ng]nNP)=PP
retaliating
PP=NP
against
NP
Mr: Trudeau
Figure 2: Right node raising output produced by our parser. Punishing and retaliating are unknown words.
times by the parser, out of which 48 times it corre-
sponded to a rule in the gold standard (or 34 times,
if the exact bracketing of the S[dcl]=NP is taken into
account?this lower figure is due to attachment de-
cisions made elsewhere in the tree).
These figures are difficult to compare with stan-
dard Treebank parsers. Despite the fact that the
original Treebank does contain traces for move-
ment, none of the existing parsers try to gener-
ate these traces (with the exception of Collins?
Model 3, for which he only gives an overall score
of 96.3%/98.8% P/R for subject extraction and
81.4%/59.4% P/R for other cases). The only ?long
range? dependency for which Collins gives numbers
is subject extraction hSBAR, WHNP, SG, Ri, which
has labeled precision and recall of 90.56% and
90.56%, whereas the CCG model achieves a labeled
precision and recall of 94.3% and 96.5% on the most
frequency subject extraction dependency hNPnNP,
(NPnNP)=(S[dcl]nNP), S[dcl]nNPi, which occurs
262 times in the gold standard and was produced
256 times by our parser. However, out of the
15 cases of this relation in the gold standard that
our parser did not return, 8 were in fact analyzed
as subject extraction of bare infinitivals hNPnNP,
(NPnNP)=(S[b]nNP), S[b]nNPi, yielding a com-
bined recall of 97.3%.
7 Lexical coverage
The most serious problem facing parsers like the
present one with large category sets is not so much
the standard problem of unseen words, but rather the
problem of words that have been seen, but not with
the necessary category.
For standard Treebank parsers, the latter problem
does not have much impact, if any, since the Penn
Treebank tagset is fairly small, and the grammar un-
derlying the Treebank is very permissive. However,
for CCG this is a serious problem: the first three
rows in table 4 show a significant difference in per-
formance for sentences with complete lexical cover-
age (?No missing?) and sentences with missing lex-
ical entries (?Missing?).
Using the POS-tags in the corpus, we can estimate
the lexical probabilities P(w j c) using a linear in-
terpolation between the relative frequency estimates
?P(w j c) and the following approximation:5
?Ptags(w j c) = ? t2tags ?P(w j t) ?P(t j c)
We smooth the lexical probabilities as follows:
?P(w j c) = ? ?P(w j c)+(1 ?) ?Ptags(w j c)
Table 4 shows the performance of the baseline
model with a frequency cutoff of 5 and 10 for rare
words and with a smoothed and non-smoothed lexi-
con.6 This frequency cutoff plays an important role
here - smoothing with a small cutoff yields worse
performance than not smoothing, whereas smooth-
ing with a cutoff of 10 does not have a significant
impact on performance. Smoothing the lexicon in
this way does make the parser more robust, result-
ing in complete coverage of the test set. However, it
does not affect overall performance, nor does it alle-
viate the problem for sentences with missing lexical
entries for seen words.
5We compute ? in the same way as Collins (1999), p. 185.
6Smoothing was only done for categories with a total fre-
quency of 100 or more.
Baseline, Cutoff = 5 Baseline, Cutoff = 10 HWDep, Cutoff = 10
(Missing = 463 sentences) (Missing = 387 sentences) (Missing = 387 sentences)
Non-smoothed Smoothed Non-smoothed Smoothed Smoothed
Parse failures 6 ? 5 ? ?
hP;H;Si, All 75.7 73.2 76.2 76.3 83.9
hP;H;Si, Missing 66.4 64.2 67.0 67.1 75.1
hP;H;Si, No missing 78.5 75.9 78.5 78.6 86.6
Table 4: The impact of lexical coverage, using a different cutoff for rare words and smoothing (section 23)
8 Conclusion and future work
We have compared a number of generative probabil-
ity models of CCG derivations, and shown that our
best model recovers 89.9% of word-word dependen-
cies on section 23 of CCGbank. On section 00, it
recovers 89.7% of word-word dependencies. These
figures are surprisingly close to the figure of 90.9%
reported by Collins (1999) on section 00, given that,
in order to allow a direct comparison, we have used
the same interpolation technique and beam strategy
as Collins (1999), which are very unlikely to be as
well-tuned to our kind of grammar.
As is to be expected, a statistical model of a CCG
extracted from the Treebank is less robust than a
model with an overly permissive grammar such as
Collins (1999). This problem seems to stem mainly
from the incomplete coverage of the lexicon. We
have shown that smoothing can compensate for en-
tirely unknown words. However, this approach does
not help on sentences which require previously un-
seen entries for known words. We would expect a
less naive approach such as applying morphologi-
cal rules to the observed entries, together with better
smoothing techniques, to yield better results.
We have also shown that a statistical model of
CCG benefits from word-word dependencies to a
much greater extent than a less linguistically moti-
vated model such as Collins? Model 1. This indi-
cates to us that, although the task faced by a CCG
parser might seem harder prima facie, there are
advantages to using a more linguistically adequate
grammar.
Acknowledgements
Thanks to Stephen Clark, Miles Osborne and the
ACL-02 referees for comments. Various parts of the
research were funded by EPSRC grants GR/M96889
and GR/R02450 and an EPSRC studentship.
References
Steven Abney. 1997. Stochastic Attribute-Value Grammars.
Computational Linguistics, 23(4).
Bob Carpenter. 1992. Categorial Grammars, Lexical Rules,
and the English Predicative. In R. Levine, ed., Formal
Grammar: Theory and Implementation. OUP.
Eugene Charniak. 1999. A Maximum-Entropy-Inspired Parser.
TR CS-99-12, Brown University.
David Chiang. 2000. Statistical Parsing with an Automatically-
Extracted Tree Adjoining Grammar 38th ACL, Hong Kong,
pp. 456-463.
Stephen Clark and Julia Hockenmaier. 2002. Evaluating a
Wide-Coverage CCG Parser. LREC Beyond PARSEVAL
workshop, Las Palmas, Spain.
Stephen Clark, Julia Hockenmaier, and Mark Steedman.
2002. Building Deep Dependency Structures Using a Wide-
Coverage CCG Parser. 40th ACL, Philadelphia.
Michael Collins. 1997. Three Generative Lexicalized Models
for Statistical Parsing. 35th ACL, Madrid, pp. 16?23.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania.
Daniel Gildea. 2001. Corpus Variation and Parser Perfor-
mance. EMNLP, Pittsburgh, PA.
Julia Hockenmaier. 2001. Statistical Parsing for CCG with
Simple Generative Models. Student Workshop, 39th ACL/
10th EACL, Toulouse, France, pp. 7?12.
Julia Hockenmaier and Mark Steedman 2002. Acquiring Com-
pact Lexicalized Grammars from a Cleaner Treebank. Third
LREC, Las Palmas, Spain.
Joshua Goodman. 1997. Probabilistic Feature Grammars.
IWPT, Boston.
Mark Johnson. 1998. PCFG Models of Linguistic Tree Repre-
sentations. Computational Linguistics, 24(4).
Adwait Ratnaparkhi. 1996. A Maximum Entropy Part-Of-
Speech Tagger. EMNLP, Philadelphia, pp. 133?142.
Mark Steedman. 2000. The Syntactic Process. The MIT Press,
Cambridge Mass.
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 308?316,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Priming Effects in Combinatory Categorial Grammar
David Reitter
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK
dreitter@inf.ed.ac.uk
Julia Hockenmaier
Inst. for Res. in Cognitive Science
University of Pennsylvania
3401 Walnut Street
Philadelphia PA 19104, USA
juliahr@cis.upenn.edu
Frank Keller
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK
keller@inf.ed.ac.uk
Abstract
This paper presents a corpus-based ac-
count of structural priming in human sen-
tence processing, focusing on the role that
syntactic representations play in such an
account. We estimate the strength of struc-
tural priming effects from a corpus of
spontaneous spoken dialogue, annotated
syntactically with Combinatory Catego-
rial Grammar (CCG) derivations. This
methodology allows us to test a range of
predictions that CCG makes about prim-
ing. In particular, we present evidence
for priming between lexical and syntactic
categories encoding partially satisfied sub-
categorization frames, and we show that
priming effects exist both for incremental
and normal-form CCG derivations.
1 Introduction
In psycholinguistics, priming refers to the fact that
speakers prefer to reuse recently encountered lin-
guistic material. Priming effects typically man-
ifest themselves in shorter processing times or
higher usage frequencies for reused material com-
pared to non-reused material. These effects are at-
tested both in language comprehension and in lan-
guage production. Structural priming occurs when
a speaker repeats a syntactic decision, and has
been demonstrated in numerous experiments over
the past two decades (e.g., Bock, 1986; Branigan
et al, 2000). These experimental findings show
that subjects are more likely to choose, e.g., a
passive voice construction if they have previously
comprehended or produced such a construction.
Recent studies have used syntactically anno-
tated corpora to investigate structural priming.
The results have demonstrated the existence of
priming effects in corpus data: they occur for spe-
cific syntactic constructions (Gries, 2005; Szm-
recsanyi, 2005), consistent with the experimen-
tal literature, but also generalize to syntactic rules
across the board, which repeated more often than
expected by chance (Reitter et al, 2006b; Dubey
et al, 2006). In the present paper, we build on
this corpus-based approach to priming, but focus
on the role of the underlying syntactic represen-
tations. In particular, we use priming to evaluate
claims resulting from a particular syntactic theory,
which is a way of testing the representational as-
sumptions it makes.
Using priming effects to inform syntactic the-
ory is a novel idea; previous corpus-based priming
studies have simply worked with uncontroversial
classes of constructions (e.g., passive/active). The
contribution of this paper is to overcome this limi-
tation by defining a computational model of prim-
ing with a clear interface to a particular syntac-
tic framework. The general assumption we make
is that priming is a phenomenon relating to gram-
matical constituents ? these constituents determine
the syntactic choices whose repetition can lead to
priming. Crucially, grammatical frameworks dif-
fer in the grammatical constituents they assume,
and therefore predict different sets of priming ef-
fects.
We require the following ingredients to pursue
this approach: a syntactic theory that identifies
a set of constituents, a corpus of linguistic data
annotated according to that syntactic theory, and
a statistical model that estimates the strength of
priming based on a set of external factors. We can
then derive predictions for the influence of these
factors from the syntactic theory, and test them
using the statistical model. In this paper, we use
regression models to quantify structural priming
effects and to verify predictions made by Com-
binatory Categorial Grammar (CCG, Steedman
(2000)), a syntactic framework that has the theo-
retical potential to elegantly explain some of the
phenomena discovered in priming experiments.
308
CCG is distinguished from most other gram-
matical theories by the fact that its rules are
type-dependent, rather than structure-dependent
like classical transformations. Such rules adhere
strictly to the constituent condition on rules, i.e.,
they apply to and yield constituents. Moreover,
the syntactic types that determine the applicability
of rules in derivations are transparent to (i.e., are
determined, though not necessarily uniquely, by)
the semantic types that they are associated with.
As a consequence, syntactic types are more ex-
pressive and more numerous than standard parts of
speech: there are around 500 highly frequent CCG
types, against the standard 50 or so Penn Treebank
POS tags. As we will see below, these properties
allow CCG to discard a number of traditional as-
sumptions concerning surface constituency. They
also allow us to make a number of testable pre-
dictions concerning priming effects, most impor-
tantly (a) that priming effects are type-driven and
independent of derivation, and, as a corollary;
(b) that lexical and derived constituents of the
same type can prime each other. These effects are
not expected under more traditional views of prim-
ing as structure-dependent.
This paper is organized as follows: Section 2
explains the relationship between structural prim-
ing and CCG, which leads to a set of specific pre-
dictions, detailed in Section 3. Sections 4 and 5
present the methodology employed to test these
predictions, describing the corpus data and the sta-
tistical analysis used. Section 6 then presents the
results of three experiments that deal with priming
of lexical vs. phrasal categories, priming in incre-
mental vs. normal form derivations, and frequency
effects in priming. Section 7 provides a discussion
of the implications of these findings.
2 Background
2.1 Structural Priming
Previous studies of structural priming (Bock,
1986; Branigan et al, 2000) have made few the-
oretical assumptions about syntax, regardless of
whether the studies were based on planned exper-
iments or corpora. They leverage the fact that al-
ternations such as He gave Raquel the car keys vs.
He gave the car keys to Raquel are nearly equiva-
lent in semantics, but differ in their syntactic struc-
ture (double object vs. prepositional object). In
such experiments, subjects are first exposed to a
prime, i.e., they have to comprehend or produce
either the double object or the prepositional ob-
ject structure. In the subsequent trial, the target,
they are the free to produce or comprehend either
of the two structures, but they tend to prefer the
one that has been primed. In corpus studies, the
frequencies of the alternative constructions can be
compared in a similar fashion (Gries, 2005; Szm-
recsanyi, 2005).
Reitter et al (2006b) present a different method
to examine priming effects in the general case.
Rather than selecting specific syntactic alterna-
tions, general syntactic units are identified. This
method detects syntactic repetition in corpora and
correlates its probability with the distance between
prime and target, where at great distance, any rep-
etition can be attributed to chance. The size of
the priming effect is then estimated as the differ-
ence between the repetition probability close to
the prime and far away from the prime. This is
a way of factoring out chance repetition (which
is required if we do not deal with syntactic alter-
nations). By relying on syntactic units, the prim-
ing model includes implicit assumptions about the
particular syntactic framework used to annotate
the corpus under investigation.
2.2 Priming and Lexicalized Grammar
Previous work has demonstrated that priming ef-
fects on different linguistic levels are not indepen-
dent (Pickering and Branigan, 1998). Lexical rep-
etition makes repetition on the syntactic level more
likely. For instance, suppose we have two verbal
phrases (prime, target) produced only a few sec-
onds apart. Priming means that the target is more
likely to assume the same syntactic form (e.g., a
passive) as the prime. Furthermore, if the head
verbs in prime and target are identical, experi-
ments have demonstrated a stronger priming ef-
fect. This effect seems to indicate that lexical and
syntactic representations in the grammar share the
same information (e.g., subcategorization infor-
mation), and therefore these representations can
prime each other.
Consequently, we treat subcategorization as
coterminous with syntactic type, rather than as a
feature exclusively associated with lexemes. Such
types determine the context of a lexeme or phrase,
and are determined by derivation. Such an anal-
ysis is exactly what categorial grammars suggest.
The rich set of syntactic types that categories af-
ford may be just sufficient to describe all and only
309
the units that can show priming effects during
syntactic processing. That is to say that syntac-
tic priming is categorial type-priming, rather than
structural priming.
Consistent with this view, Pickering and Brani-
gan (1998) assume that morphosyntactic features
such as tense, aspect or number are represented in-
dependently from combinatorial properties which
specify the contextual requirements of a lexical
item. Property groups are represented centrally
and shared between lexicon entries, so that they
may ? separately ? prime each other. For ex-
ample, the pre-nominal adjective red in the red
book primes other pre-nominal adjectives, but not
a post-nominal relative clause (the book that?s red)
(Cleland and Pickering, 2003; Scheepers, 2003).
However, if a lexical item can prime a phrasal
constituent of the same type, and vice versa, then
a type-driven grammar formalism like CCG can
provide a simple account of the effect, because
lexical and derived syntactic types have the same
combinatory potential, which is completely spec-
ified by the type, whereas in structure-driven the-
ories, this information is only implicitly given in
the derivational process.
2.3 Combinatory Categorial Grammar
CCG (Steedman, 2000) is a mildly context-
sensitive, lexicalized grammar formalism with a
transparent syntax-semantics interface and a flex-
ible constituent structure that is of particular in-
terest to psycholinguistics, since it allows the con-
struction of incremental derivations. CCG has also
enjoyed the interest of the NLP community, with
high-accuracy wide-coverage parsers(Clark and
Curran, 2004; Hockenmaier and Steedman, 2002)
and generators1 available (White and Baldridge,
2003).
Words are associated with lexical categories
which specify their subcategorization behaviour,
eg. ((S[dcl]\NP)/NP)/NP is the lexical category
for (tensed) ditransitive verbs in English such as
gives or send, which expect two NP objects to
their right, and one NP subject to their left. Com-
plex categories X/Y or X\Y are functors which
yield a constituent with category X, if they are ap-
plied to a constituent with category Y to their right
(/Y) or to their left (\Y).
Constituents are combined via a small set of
combinatory rule schemata:
Forward Application: X/Y Y ?> X
1http://opennlp.sourceforge.net/
Backward Application: Y X\Y ?> X
Forward Composition: X/Y Y/Z ?B X/Z
Backward Composition: Y\Z X\Y ?B X\Z
Backw. Crossed Composition: Y/Z X\Y ?B X/Z
Forward Type-raising: X ?T T/(T\X)
Coordination: X conj X ?? X
Function application is the most basic operation
(and used by all variants of categorial grammar):
I saw the man
NP (S\NP)/NP NP
>
S\NP
<
S
Composition (B) and type-raising (T) are neces-
sary for the analysis of long-range dependencies
and for incremental derivations. CCG uses the
same lexical categories for long-range dependen-
cies that arise eg. in wh-movement or coordina-
tion as for local dependencies, and does not re-
quire traces:
the man that I saw
NP (NP\NP)/(S/NP) NP (S\NP)/NP
>T
S/(S\NP)
>B
S/NP
>
NP\NP
I saw and you heard the man
NP (S\NP)/NP conj NP (S\NP)/NP
>T >T
S/(S\NP) S/(S\NP)
>B >B
S/NP S/NP
<?>
S/NP
>
S
The combinatory rules of CCG allow multiple,
semantically equivalent, syntactic derivations of
the same sentence. This spurious ambiguity is
the result of CCG?s flexible constituent structure,
which can account for long-range dependencies
and coordination (as in the above example), and
also for interaction with information structure.
CCG parsers often limit the use of the combi-
natory rules (in particular: type-raising) to obtain
a single right-branching normal form derivation
(Eisner, 1996) for each possible semantic inter-
pretation. Such normal form derivations only use
composition and type-raising where syntactically
necessary (eg. in relative clauses).
3 Predictions
3.1 Priming Effects
We expect priming effects to apply to CCG cat-
egories, which describe the type of a constituent
including the arguments it expects. Under our as-
sumption that priming manifests itself as a ten-
dency for repetition, repetition probability should
be higher for short distances from a prime (see
Section 5.2 for details).
310
3.2 Terminal and Non-terminal Categories
In categorial grammar, lexical categories specify
the subcategorization behavior of their heads, cap-
turing local and non-local arguments, and a small
set of rule schemata defines how constituents can
be combined.
Phrasal constituents may have the same cate-
gories as lexical items. For example, the verb saw
might have the (lexical) category (S\NP)/NP,
which allows it to combine with an NP to the right.
The resulting constituent for saw Johanna would
be of category S\NP ? a constituent which expects
an NP (the subject) to its left, and also the lexi-
cal category of an intransitive verb. Similarly, the
constituent consisting of a ditransitive verb and its
object, gives the money, has the same category as
saw. Under the assumption that priming occurs for
these categories, we proceed to test a hypothesis
that follows from the fact that categories merely
encode unsatisfied subcategorized arguments.
Given that a transitive verb has the same cat-
egory as the constituent formed by a ditransitive
verb and its direct object, we would expect that
both categories can prime each other, if they are
cognitive units. More generally, we would expect
that lexical (terminal) and phrasal (non-terminal)
categories of the same syntactic type may prime
each other. The interaction of such conditions with
the priming effect can be quantified in the statisti-
cal model.
3.3 Incrementality of Analyses
Type-raising and composition allow derivations
that are mostly left-branching, or incremental.
Adopting a left-to-right processing order for a sen-
tence is important, if the syntactic theory is to
make psycholinguistically viable predictions (Niv,
1994; Steedman, 2000).
Pickering et al (2002) present priming experi-
ments that suggest that, in production, structural
dominance and linearization do not take place in
different stages. Their argument involves verbal
phrases with a shifted prepositional object such
as showed to the mechanic a torn overall. At a
dominance-only level, such phrases are equivalent
to non-shifted prepositional constructions (showed
a torn overall to the mechanic), but the two vari-
ants may be differentiated at a linearization stage.
Shifted primes do not prime prepositional objects
in their canonical position, thus priming must oc-
cur at a linearized level, and a separate dominance
level seems unlikely (unless priming is selective).
CCG is compatible with one-stage formulations of
syntax, as no transformation is assumed and cate-
gories encode linearization together with subcate-
gorization.
CCG assumes that the processor may produce
syntactically different, but semantically equivalent
derivations.2 So, while neither the incremental
analysis we generate, nor the normal-form, rep-
resent one single correct derivation, they are two
extremes of a ?spectrum? of derivations. We hy-
pothesize that priming effects predicted on the ba-
sis of incremental CCG analyses will be as strong
than those predicted on the basis of their normal-
form equivalents.
4 Corpus Data
4.1 The Switchboard Corpus
The Switchboard (Marcus et al, 1994) corpus con-
tains transcriptions of spoken, spontaneous con-
versation annotated with phrase-structure trees.
Dialogues were recorded over the telephone
among randomly paired North American speak-
ers, who were just given a general topic to talk
about. 80,000 utterances of the corpus have been
annotated with syntactic structure. This portion,
included in the Penn Treebank, has been time-
aligned (per word) in the Paraphrase project (Car-
letta et al, 2004).
Using the same regression technique as em-
ployed here, Reitter et al (2006b) found a marked
structural priming effect for Penn-Treebank style
phrase structure rules in Switchboard.
4.2 Disfluencies
Speech is often disfluent, and speech repairs are
known to repeat large portions of the preceding
context (Johnson and Charniak, 2004). The orig-
inal Switchboard transcripts contains these disflu-
encies (marked up as EDITED):
( (S >>>(EDITED
(RM (-DFL- \bs [) )
(EDITED
(RM (-DFL- \bs [) )
(CC And)
(, ,)
(IP (-DFL- \bs +) ))
(CC and)
(, ,)
(RS (-DFL- \bs ]) )
(IP (-DFL- \bs +) ))<<<
2Selectional criteria such as information structure and in-
tonation allow to distinguish between semantically different
analyses.
311
(CC and)
>>>(RS (-DFL- \bs ]) )<<<
(NP-SBJ (PRP I) )
(VP (VBP guess)
(SBAR (-NONE- 0)
(S (NP-SBJ (DT that) )
(VP (BES ?s)
(SBAR-NOM-PRD
(WHNP-1 (WP what) )
(S (NP-SBJ (PRP I) )
(ADVP (RB really) )
(VP (VBP like)
(NP (-NONE- *T*-1) ))))))))
(. .) (-DFL- E_S) ))
It is unclear to what extent these repetitions
are due to priming rather than simple correc-
tion. In disfluent utterances, we therefore elimi-
nate reparanda and only keep repairs (the portions
marked with >...< are removed). Hesitations (uh,
etc.), and utterances with unfinished constituents
are also ignored.
4.3 Translating Switchboard to CCG
Since the Switchboard annotation is almost iden-
tical to the one of the Penn Treebank, we use a
similar translation algorithm to Hockenmaier and
Steedman (2005). We identify heads, arguments
and adjuncts, binarize the trees, and assign cat-
egories in a recursive top-down fashion. Nonlo-
cal dependencies that arise through wh-movement
and right node raising (*T* and *RNR* traces) are
captured in the resulting derivation. Figure 1 (left)
shows the rightmost normal form CCG derivation
we obtain for the above tree. We then transform
this normal form derivation into the most incre-
mental (i.e., left-branching) derivation possible, as
shown in Figure 1 (right).
This transformation is done by a top-down re-
cursive procedure, which changes each tree of
depth two into an equivalent left-branching anal-
ysis if the combinatory rules allow it. This pro-
cedure is run until no further transformation can
be executed. The lexical categories of both deriva-
tions are identical.
5 Statistical Analysis
5.1 Priming of Categories
CCG assumes a minimal set of combinatory rule
schemata. Much more than in those rules, syntac-
tic decisions are evident from the categories that
occur in the derivation.
Given the categories for each utterance, we can
identify their repeated use. A certain amount
of repetition will obviously be coincidental. But
structural priming predicts that a target category
will occur more frequently closer to a potential
prime of the same category. Therefore, we can
correlate the probability of repetition with the dis-
tance between prime and target. Generalized Lin-
ear Mixed Effects Models (GLMMs, see next sec-
tion) allow us to evaluate and quantify this corre-
lation.
Every syntactic category is counted as a poten-
tial prime and (almost always) as a target for prim-
ing. Because interlocutors tend to stick to a topic
during a conversation for some time, we exclude
cases of syntactic repetition that are a results of
the repetition of a whole phrase.
Previous work points out that priming is sensi-
tive to frequency (Scheepers (2003) for high/low
relative clause attachments, (Reitter et al, 2006a)
for phrase structure rules). Highly frequent items
do not receive (as much) priming. We include
the logarithm of the raw frequency of the syntac-
tic category in Switchboard (LNFREQ) to approx-
imate the effect that frequency has on accessibility
of the category.
5.2 Generalized Linear Mixed Effects
Regression
We use generalized linear mixed effects regression
models (GLMM, Venables and Ripley (2002)) to
predict a response for a number of given categorial
(?factor?) or continuous (?predictor?) explanatory
variables (features). Our data is made up of in-
stances of repetition examples and non-repetition
examples from the corpus. For each target in-
stance of a syntactic category c occurring in a
derivation and spanning a constituent that begins
at time t, we look back for possible instances of
constituents with the same category (the prime)
in a time frame of [t ? d ? 0.5; t ? d + 0.5] sec-
onds. If such instances can be found, we have a
positive example of repetition. Otherwise, c is in-
cluded as a data point with a negative outcome.
We do so for a range of different distances d, com-
monly 1 ? d ? 15 seconds.3 For each data point,
we include the logarithm of the distance d between
priming period and target as an explanatory vari-
able LNDIST. (See Reitter et al (2006b) for a
worked example.)
In order to eliminate cases of lexical repeti-
tion of a phrase, e.g., names or lexicalized noun
3This approach uses a number of data points per target,
looking backwards for primes. The opposite way ? looking
forwards for targets ? would make similar predictions.
312
Normal form derivation Incremental derivation
S[dcl]
S/S
and
S[dcl]
S/(S\NP)
NP
I
S[dcl]\NP
(S[dcl]\NP)/S[dcl]
guess
S[dcl]
S/(S\NP)
NP
that
S[dcl]\NP
(S[dcl]\NP)/NP
?s
NP
NP/(S[dcl]/NP)
what
S[dcl]/NP
S/(S\NP)
NP
I
(S[dcl]\NP)/NP
(S\NP)/(S\NP)
really
(S[dcl]\NP)/NP
like
S[dcl]
S[dcl]/(S[dcl]/NP)
S[dcl]/NP
S[dcl]/(S\NP)
S[dcl]/S[dcl]
S/(S\NP)
S/S
and
S/(S\NP)
NP
I
(S[dcl]\NP)/S[dcl]
guess
S/(S\NP)
NP
that
(S[dcl]\NP)/NP
?s
NP/(S[dcl]/NP)
what
S[dcl]/NP
S/(S\NP)
S/(S\NP)
NP
I
(S\NP)/(S\NP)
really
(S[dcl]\NP)/NP
like
Figure 1: Two derivations (normal form: left), incremental: right) for the sentence fragment and I guess
that?s what I really like from Switchboard.
phrases, which we consider topic-dependent or in-
stances of lexical priming, we only collect syntac-
tic repetitions with at least one differing word.
Without syntactic priming, we would assume
that there is no correlation between the probabil-
ity that a data point is positive (repetition occurs)
and distance d. With priming, we would expect
that the probability is inversely proportional to d.
Our model uses lnd as predictor LNDIST, since
memory effects usually decay exponentially.
The regression model fitted is then simply a
choice of coefficients ?i, among them one for each
explanatory variable i. ?i expresses the contribu-
tion of i to the probability of the outcome event,
that is, in our case, successful priming. The coeffi-
cient of interest is the one for the time correlation,
i.e. ?lnDist . It specifies the strength of decay of
repetition probability over time. If no other vari-
ables are present, a model estimates the repetition
probability for a data point i as
p?i = ?0 +?lnDist ln DISTi
Priming is present if the estimated parameter is
negative, i.e. the repetition probability decreases
with increasing distance between prime and target.
Other explanatory variables, such as ROLE,
which indicates whether priming occurs within a
speaker (production-production priming, PP) or
in between speakers (comprehension-production
priming, CP), receive an interaction coefficient
that adds linearly to ?lnDist . Additional interac-
tion variables are included depending on the ex-
perimental question.4
4Lastly, we identify the target utterance in a random fac-
tor in our model, grouping the several measurements (15 for
the different distances from each target) as repeated measure-
ments, since they depend on the same target category occur-
rence and are partially inter-dependent.
From the data produced, we include all cases
of reptition and a an equal number of randomly
sampled non-repetition cases.5
6 Experiments
6.1 Experiment 1: Priming in Incremental
and Normal-form Derivations
Hypothesis CCG assumes a multiplicity of se-
mantically equivalent derivations with different
syntactic constituent structures. Here, we in-
vestigate whether two of these, the normal-form
and the most incremental derivation, differ in the
strength with which syntactic priming occurs.
Method A joint model was built containing rep-
etition data from both types of derivations. Since
we are only interested in cases where the two
derivations differ, we excluded all constituents
where a string of words was analyzed as a con-
stituent in both derivations. This produced a data
set where the two derivations could be contrasted.
A factor DERIVATION in the model indicates
whether the repetition occurred in a normal-form
(NF) or an incremental derivation (INC).
Results Significant and substantial priming is
present in both types of derivations, for both PP
and CP priming. There is no significant difference
in priming strength between normal-form and
incremental derivations (?lnDist:NF = 0.008, p =
0.95). The logarithm of the raw category fre-
quency is negatively correlated with the priming
strength (?lnDist:lnFreq = 0.151, p < 0.0001. Note
that a negative coefficient for LNDIST indicates
5We trained our models using Penalized Quasi-
Likelihood (Venables and Ripley, 2002). This technique
works best if data is balanced, i.e. we avoid having very rare
positive examples in the data. Experiment 2 was conducted
on a subset of the data.
313
CP:NormalForm
PP:NormalForm
CP:Incremental
PP:Incremental
1.0 1.2 1.4 1.6
- - - -
Figure 2: Decay effect sizes in Experiment 1
for combinations of comprehension-production or
production-production priming and in incremental
or normal-form derivations. Error bars show (non-
simultaneous) 95% confidence intervals.
decay. The lower this coefficient, the more decay,
hence priming).
If there was no priming of categories for incre-
mentally formed constituents, we would expect to
see a large effect of DERIVATION. In the contrary,
we see no effect at a high p, where the that the
regression method used is demonstrably powerful
enough to detect even small changes in the prim-
ing effect. We conclude that there is no detectable
difference in priming between the two derivation
types. In Fig. 2, we give the estimated priming
effect sizes for the four conditions.6
The result is compatible with CCG?s separation
of derivation structure and the type of the result
of derivation. It is not the derivation structure that
primes, but rather the type of the result. It is also
compatible with the possibility of a non-traditional
constituent structure (such as the incremental anal-
ysis), even though it is clear that neither incremen-
tal nor normal-form derivations necessarily repre-
sent the ideal analysis.
The category sets occurring in both derivation
variants was largely disjunct, making testing for
actual overlap between different derivations im-
possible.
6.2 Experiment 2: Priming between Lexical
and Phrasal Categories
Hypothesis Since CCG categories simply en-
code unsatisfied subcategorization constraints,
constituents which are very different from a tradi-
tional linguistic perspective can receive the same
category. This is, perhaps, most evident in phrasal
6Note that Figures 2 and 3 stem from nested models that
estimate the effect of LNDIST within the four/eight condi-
tions. Confidence intervals will be larger, as fewer data-
points are available than when the overall effect of a single
factor is compared.
CP:lex?lex
PP:lex?lex
CP:lex?phr
PP:lex?phr
CP:phr?lex
PP:phr?lex
CP:phr?phr
PP:phr?phr
?1.0 ?1.2 ?1.4 ?1.6 ?1.8 ?2.0
Figure 3: Decay effect sizes in Experiment 2,
for combinations of comprehension-production
or production-production priming and lexical or
phrasal primes and targets, e.g. the third bar
denotes the decay in repetition probability of a
phrasal category as prime and a lexical one as
target, where prime and target occurred in utter-
ances by the same speaker. Error bars show (non-
simultaneous) 95% confidence intervals.
and lexical categories (where, e.g., an intransitive
verb is indistinguishable from a verb phrase).
Bock and Loebell (1990)?s experiments suggest
that priming effects are independent of the subcat-
egorization frame. There, an active voice sentence
primed a passive voice one with the same phrase
structure, but a different subcategorization. If we
find priming from lexical to phrasal categories,
then our model demonstrates priming of subcat-
egorization frames.
From a processing point of view, phrasal cat-
egories are distinct from lexical ones. Lexical
categories are bound to the lemma and thereby
linked to the lexicon, while phrasal categories
are the result of a structural composition or de-
composition process. The latter ones represent
temporary states, encoding the syntactic process.
Here, we test whether lexical and phrasal cate-
gories can prime each other, and if so, contrast the
strength of these priming effects.
Method We built a model which allowed lex-
ical and phrasal categories to prime each other.
A factor, STRUCTURAL LEVEL was introduced
314
to distinguish the four cases: priming in between
phrasal categories and in between lexical ones,
from lexical ones to phrasal ones and from phrasal
ones to lexical ones.
Recall that each data point encodes a possibility
to repeat a CCG category, referring to a particular
instance of a target category at time t and a time
span of duration of one second [t?d?0.5, t?d +
0.5] in which a priming instance of the same cate-
gory could occur. If it occurred at least once, the
data point was counted as a possible example of
priming (response variable: true), otherwise it was
included as a counter-example (response variable:
false). For the target category, its type (lexical or
phrasal) was clear. For the category of the prime,
we included two data points, one for each type,
with a response indicating whether a prime of the
category of such a type occurred in the time win-
dow. We built separate models for incremental and
normal form derivations. Models were fitted to
a balanced subset, including all repetitions and a
randomly sampled subset of non-repetitions.
Results Both the normal-form and the incre-
mental model show qualitatively the same re-
sults. STRUCTURALLEVEL has a significant
influence on priming strength (LN DIST) for
the cases where a lexical item serves as prime
(e.g., normal-form PP: ?lnDist:lex?lex = 0.261,
p < 0.0001; ?lnDist:lex?phr = 0.166, p < 0.0001;
?lnDist:phr?lex = 0.056, p < 0.05; as compared to
the baseline phr? phr. N.B. higher values denote
less decay & priming). Phrasal categories prime
other phrasal and lexical categories, but there is a
lower priming effect to be seen from lexical cate-
gories. Figure 3 presents the resulting effect sizes.
Albeit significant, we assume the effect of prime
type is attributable to processing differences rather
than the strong difference that would indicate that
there is no priming of, e.g., lexical subcategoriza-
tion frames. As the analysis of effect sizes shows,
we can see priming from and in between both lex-
ical and phrasal categories.
Additionally, there is no evidence suggesting
that, once frequency is taken into account, syntac-
tic processes happening high up in derivation trees
show more priming (see Scheepers 2003).
7 Discussion
We can confirm the syntactic priming effect for
CCG categories. Priming occurs in incremental
as well as in normal-form CCG derivations, and at
different syntactic levels in those derivations: we
demonstrated that priming effects persists across
syntactic stages, from the lowest one (lexical cate-
gories) up to higher ones (phrasal categories). This
is what CCG predicts if priming of categories is
assumed.
Linguistic data is inherently noisy. Annotations
contain errors, and conversions such as the one to
CCG may add further error. However, since noise
is distributed across the corpus, it is unlikely to af-
fect priming effect strength or its interaction with
the factors we used: priming, in this study, is de-
fined as decay of repetition probability. We see
the lack of control in the collection of a corpus like
Switchboard not only as a challenge, but also as an
advantage: it means that realistic data is present in
the corpus, allowing us to conduct a controlled ex-
periments to validate a claim about a specific the-
ory of competence grammar.
The fact that CCG categories prime could be
explained in a model that includes a basic form
of subcategorization. All categories, if lexical or
phrasal, contain a subcategorization frame, with
only those categories present that have yet to be
satisfied. Our CCG based models make predic-
tions for experimental studies, e.g., that specific
heads with open subcategorization slots (such as
transitive verbs) will be primed by phrases that re-
quire the same kinds of arguments (such as verbal
phrases with a ditransitive verb and an argument).
The models presented take the frequency of the
syntactic category into account, reducing noise,
especially in the conditions with lower numbers
of (positive) reptition examples (e.g., CP and in-
cremental derivations in Experiment 1). Whether
there are significant qualitative and quantitative
differences of PP and CP priming with respect to
choice of derivation type ? which would point out
processing differences in comprehension vs. pro-
duction priming ? will be a matter of future work.
At this point, we do not explicitly discriminate
different syntactic frameworks. Comparing prim-
ing effects in a corpus annotated in parallel accord-
ing to different theories will be a matter of future
work.
8 Conclusions
We have discussed an empirical, corpus-based ap-
proach to use priming effects in the validation of
general syntactic models. The analysis we pre-
sented is compatible with the reality of a lexical-
315
ized, categorial grammar such as CCG as a com-
ponent of the human sentence processor. CCG is
unusual in allowing us to compare different types
of derivational analyses within the same grammar
framework. Focusing on CCG allowed us to con-
trast priming under different conditions, while still
making a statistical and general statement about
the priming effects for all syntactic phenomena
covered by the grammar.
Acknowledgements
We would like to thank Mark Steedman, Roger Levy, Jo-
hanna Moore and three anonymous reviewers for their com-
ments. The authors are grateful for being supported by the
following grants: DR by The Edinburgh Stanford Link, JH
by NSF ITR grant 0205456, FK by The Leverhulme Trust
(grant F/00 159/AL ? Syntactic Parallelism).
References
J. Kathryn Bock. 1986. Syntactic persistence in language pro-
duction. Cognitive Psychology, 18:355?387.
J. Kathryn Bock and Helga Loebell. 1990. Framing sen-
tences. Cognition, 35:1?39.
Holly P. Branigan, Martin J. Pickering, and Alexandra A. Cle-
land. 2000. Syntactic co-ordination in dialogue. Cogni-
tion, 75:B13?25.
Jean Carletta, S. Dingare, Malvina Nissim, and T. Nikitina.
2004. Using the NITE XML toolkit on the Switchboard
corpus to study syntactic choice: a case study. In Proc. 4th
Language Resources and Evaluation Conference. Lisbon,
Portugal.
Stephen Clark and James R. Curran. 2004. Parsing the WSJ
using CCG and log-linear models. In Proc. of the 42nd
Annual Meeting of the Association for Computational Lin-
guistics. Barcelona, Spain.
A. A. Cleland and M. J. Pickering. 2003. The use of lexi-
cal and syntactic information in language production: Ev-
idence from the priming of noun-phrase structure. Journal
of Memory and Language, 49:214?230.
Amit Dubey, Frank Keller, and Patrick Sturt. 2006. Inte-
grating syntactic priming into an incremental probabilistic
parser, with an application to psycholinguistic modeling.
In Proc. of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Mtg of the Association
for Computational Linguistics. Sydney, Australia.
Jason Eisner. 1996. Efficient normal-form parsing for com-
binatory categorial grammar. In Proceedings of the 34th
Annual Meeting of the Association for Computational Lin-
guistics, pages 79?86. Santa Cruz,CA.
Stefan Th. Gries. 2005. Syntactic priming: A corpus-
based approach. Journal of Psycholinguistic Research,
34(4):365?399.
Julia Hockenmaier and Mark Steedman. 2002. Generative
models for statistical parsing with Combinatory Catego-
rial Grammar. In Proc. 40th Annual Meeting of the Asso-
ciation for Computational Linguistics. Philadelphia, PA.
Julia Hockenmaier and Mark Steedman. 2005. CCGbank:
Users? manual. Technical Report MS-CIS-05-09, Com-
puter and Information Science, University of Pennsylva-
nia.
Mark Johnson and Eugene Charniak. 2004. A tag-based noisy
channel model of speech repairs. In Proc. 42nd Annual
Meeting of the Association for Computational Linguistics,
pages 33?39. Barcelona, Spain.
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre,
A. Bies, M. Ferguson, K. Katz, and B. Schasberger. 1994.
The Penn treebank: Annotating predicate argument struc-
ture. In Proc. ARPA Human Language Technology Work-
shop. Plainsboro, NJ.
Michael Niv. 1994. A psycholinguistically motivated parser
for CCG. In Mtg. of the Association for Computational
Linguistics, pages 125?132.
Martin J. Pickering and Holly P. Branigan. 1998. The rep-
resentation of verbs: Evidence from syntactic priming in
language production. Journal of Memory and Language,
39:633?651.
Martin J. Pickering, Holly P. Branigan, and Janet F. McLean.
2002. Constituent structure is formulated in one stage.
Journal of Memory and Language, 46:586?605.
David Reitter, Frank Keller, and Johanna D. Moore. 2006a.
Computational modelling of structural priming in dia-
logue. In Proc. Human Language Technology conference
- North American chapter of the Association for Compu-
tational Linguistics annual mtg. New York City.
David Reitter, Johanna D. Moore, and Frank Keller. 2006b.
Priming of syntactic rules in task-oriented dialogue and
spontaneous conversation. In Proc. 28th Annual Confer-
ence of the Cognitive Science Society.
Christoph Scheepers. 2003. Syntactic priming of relative
clause attachments: Persistence of structural configuration
in sentence production. Cognition, 89:179?205.
Mark Steedman. 2000. The Syntactic Process. MIT Press.
Benedikt Szmrecsanyi. 2005. Creatures of habit: A corpus-
linguistic analysis of persistence in spoken english. Cor-
pus Linguistics and Linguistic Theory, 1(1):113?149.
William N. Venables and Brian D. Ripley. 2002. Modern
Applied Statistics with S. Fourth Edition. Springer.
Mike White and Jason Baldridge. 2003. Adapting chart re-
alization to CCG. In Proc. 9th European Workshop on
Natural Language Generation. Budapest, Hungary.
316
Wide-Coverage Semantic Representations from a CCG Parser
Johan Bos, Stephen Clark, Mark Steedman
School of Informatics, University of Edinburgh
  jbos,stevec,steedman  @inf.ed.ac.uk
James R. Curran
School of Information Technologies, University of Sydney
james@it.usyd.edu.au
Julia Hockenmaier
Institute for Research in Cognitive Science, University of Pennsylvania
juliahr@linc.cis.upenn.edu
Abstract
This paper shows how to construct semantic
representations from the derivations produced
by a wide-coverage CCG parser. Unlike the de-
pendency structures returned by the parser it-
self, these can be used directly for semantic in-
terpretation. We demonstrate that well-formed
semantic representations can be produced for
over 97% of the sentences in unseen WSJ text.
We believe this is a major step towards wide-
coverage semantic interpretation, one of the key
objectives of the field of NLP.
1 Introduction
The levels of accuracy and robustness recently
achieved by statistical parsers (e.g. Collins (1999),
Charniak (2000)) have led to their use in a num-
ber of NLP applications, such as question-answering
(Pasca and Harabagiu, 2001), machine transla-
tion (Charniak et al, 2003), sentence simplifica-
tion (Carroll et al, 1999), and a linguist?s search
engine (Resnik and Elkiss, 2003). Such parsers
typically return phrase-structure trees in the style
of the Penn Treebank, but without traces and co-
indexation. However, the usefulness of this output
is limited, since the underlying meaning (as repre-
sented in a predicate-argument structure or logical
form) is difficult to reconstruct from such skeletal
parse trees.
In this paper we demonstrate how a wide-
coverage statistical parser using Combinatory Cat-
egorial Grammar (CCG) can be used to generate se-
mantic representations. There are a number of ad-
vantages to using CCG for this task. First, CCG
provides ?surface compositional? analysis of certain
syntactic phenomena such as coordination and ex-
traction, allowing the logical form to be obtained for
such cases in a straightforward way. Second, CCG is
a lexicalised grammar, and only uses a small num-
ber of semantically transparent combinatory rules to
combine CCG categories. Hence providing a com-
positional semantics for CCG simply amounts to as-
signing semantic representations to the lexical en-
tries and interpreting the combinatory rules. And
third, there exist highly accurate, efficient and ro-
bust CCG parsers which can be used directly for
this task (Clark and Curran, 2004b; Hockenmaier,
2003).
The existing CCG parsers deliver predicate argu-
ment structures, but not semantic representations
that can be used for inference. The present paper
seeks to extend one of these wide coverage parsers
by using it to build logical forms suitable for use in
various NLP applications that require semantic in-
terpretation.
We show how to construct first-order represen-
tations from CCG derivations using the ?-calculus,
and demonstrate that semantic representations can
be produced for over 97% of the sentences in unseen
WSJ text. The only other deep parser we are aware
of to achieve such levels of robustness for the WSJ
is Kaplan et al (2004). The use of the ?-calculus
is integral to our method. However, first-order rep-
resentations are simply used as a proof-of-concept;
we could have used DRSs (Kamp and Reyle, 1993)
or some other representation more tailored to the ap-
plication in hand.
There is some existing work with a similar mo-
tivation to ours. Briscoe and Carroll (2002) gen-
erate underspecified semantic representations from
their robust parser. Toutanova et al (2002) and Ka-
plan et al (2004) combine statistical methods with a
linguistically motivated grammar formalism (HPSG
and LFG respectively) in an attempt to achieve levels
of robustness and accuracy comparable to the Penn
Treebank parsers (which Kaplan et al do achieve).
However, there is a key difference between these
approaches and ours. In our approach the creation
of the semantic representations forms a completely
It could cost taxpayers 15 million to install and residents 1 million a year to maintain
NP   S  NP  VP     VP  VP NP   NP  NP NP NP VP  NP conj NP NP VP  NP
it  ?p?x  could    p x ?x?y?p?z  cost    p zx  yxz taxpayers  15M  install  ?p?q  q p residents  1Mpa  maintain 
 T  T  T  B  B 	
    VP  VP NP   NP       VP
  VP NP   NP  NP  VP  VP NP  
    VP  VP NP   NP VP   VP  VP NP  VP       VP  VP NP   NP  NP
?q?y?p?z  q taxpayers  y p z ?r?p?z  r 15M  p z ?s?z  s install  z ?l?q?z  l qz  q residents  1Mpa  maintain  z
 B
  VP  VP NP  
      VP  VP NP   NP  NP
?q?p?z  q taxpayers  15M  p z
 B
VP       VP  VP NP   NP  NP
?q?z  q taxpayers  15M  install  z
 ? 	
VP       VP  VP NP   NP  NP
?q?z  q taxpayers  15M  install  z  q residents  1Mpa  maintain  z

VP
?z  cost    install  z taxpayers   15M  taxpayers  z  cost   maintain  z residents   1Mpa  residents  z
	
S  NP
?z  could    cost    install  z taxpayers   15M  taxpayers  z  could    cost    maintain  z residents   1Mpa  residents  z 

S
could    cost    install  it  taxpayers   15M  taxpayers  it   could    cost    maintain  it  residents   1Mpa  residents  it  
Figure 1: An example CCG derivation with a provisional semantics using predicate-argument structures
separate module to the syntax, whereas in the LFG
and HPSG approaches the semantic representation
forms an integral part of the grammar. This means
that, in order for us to work with another seman-
tic formalism, we simply have to modify the lexical
entries with respect to the semantic component.
2 Combinatory Categorial Grammar
We assume familiarity with CCG (Steedman, 2000),
an entirely type-driven lexicalized theory of gram-
mar based on categorial grammar. CCG lexical en-
tries pair a syntactic category (defining syntactic va-
lency and directionality) with a semantic interpre-
tation. For example, one of the categories for the
verb cost can be written as follows, with a provi-
sional Montague-style semantics expressed in terms
of predicate-argument structure:1

VP 

VP  NP  NP  NP :
?x?y?p?z  cost   p zx  yxz
Combinatory rules project such lexical category-
interpretation pairs onto derived category-
interpretation pairs. The specific involvement
in CCG of rules of functional composition (indexed
 B and  B in derivations) and type-raising (in-
dexed  T and  T) allows very free derivation of
non-standard constituents. This results in semantic
interpretations that support the ?surface composi-
tional? analysis of relativization and coordination,
as in Figure 1 for the sentence It could cost tax-
payers ?15 million to install and BPC residents 1
million a year to maintain.2
1This semantic notation uses x  y  z and p  q  r s as variables,
identifies constants with primes and uses concatenation a b to
indicate application of a to b. Application is ?left-associative,?
so abc is equivalent to  ab  c. The order of arguments in the
predication is ?wrapped?, consistent with the facts of reflexive
binding.
2Some details of the derivation and of the semantics of
noun phrases are suppressed, since these are developed be-
While the proliferation of surface constituents al-
lowed by CCG adds to derivational ambiguity (since
the constituent taxpayers ?15 million to install is
also allowed in the non-coordinate sentence It could
cost taxpayers ?15 million to install), previous work
has shown that standard techniques from the statisti-
cal parsing literature can be used for practical wide-
coverage parsing with state-of-the-art performance.
3 The Parser
A number of statistical parsers have recently been
developed for CCG (Clark et al, 2002; Hocken-
maier and Steedman, 2002b; Clark and Curran,
2004b). All of these parsers use a grammar de-
rived from CCGbank (Hockenmaier and Steedman,
2002a; Hockenmaier, 2003), a treebank of normal-
form CCG derivations derived semi-automatically
from the Penn Treebank. In this paper we use the
Clark and Curran (2004b) parser, which uses a log-
linear model of normal-form derivations to select an
analysis.
The parser takes a POS tagged sentence as in-
put with a set of lexical categories assigned to
each word. A CCG supertagger (Clark and Cur-
ran, 2004a) is used to assign the categories. The
supertagger uses a log-linear model of the target
word?s context to decide which categories to assign.
Clark and Curran (2004a) shows how dynamic use
of the supertagger ? starting off with a small num-
ber of categories assigned to each word and gradu-
ally increasing the number until an analysis is found
? can lead to a highly efficient and robust parser.
The lexical category set used by the parser con-
sists of those category types which occur at least 10
times in sections 2-21 of CCGbank, which results
in a set of 409 categories. Clark and Curran (2004a)
demonstrates that this relatively small set has high
coverage on unseen data and can be used to create
low. Some categories and interpretations are split across lines
to save space.
a robust and accurate parser. The relevance of a rel-
atively small category set is that, in order to obtain
semantic representations for a particular formalism,
only 409 categories have to be annotated.
The parser uses the CKY chart-parsing algorithm
from Steedman (2000). The combinatory rules
used by the parser are functional application (for-
ward and backward), generalised forward composi-
tion, backward composition, generalised backward-
crossed composition, and type raising. There is also
a coordination rule which conjoins categories of the
same type.
The parser also uses a number of unary
type-changing rules (Hockenmaier and Steedman,
2002a) and punctuation rules taken from CCGbank.
An example of a type-changing rule used by the
parser is the following, which takes a passive form
of a verb and creates a nominal modifier:
S   pss  NP  NP  NP (1)
This rule is used to create NPs such as the role
played by Kim Cattrall. An example of a comma
rule is the following:
S  S  S  S (2)
This rule takes a sentential modifier followed by a
comma and returns a sentential modifier of the same
type.
Type-raising is applied to the categories NP, PP
and S   adj  NP (adjectival phrase), and is imple-
mented by adding the relevant set of type-raised
categories to the chart whenever an NP, PP or
S   adj  NP is present. The sets of type-raised cate-
gories are based on the most commonly used type-
raising rule instantiations in sections 2-21 of CCG-
bank, and currently contain 8 type-raised categories
for NP and 1 each for PP and S   adj  NP.
For a given sentence, the automatically extracted
grammar can produce a very large number of deriva-
tions. Clark and Curran (2003) and Clark and Cur-
ran (2004b) describe how a packed chart can be used
to efficiently represent the derivation space, and also
efficient algorithms for finding the most probable
derivation. The parser uses a log-linear model over
normal-form derivations.3 Features are defined in
terms of the local trees in the derivation, including
lexical head information and word-word dependen-
cies. The normal-form derivations in CCGbank pro-
vide the gold standard training data.
For a given sentence, the output of the parser is
a set of syntactic dependencies corresponding to the
3A normal-form derivation is one which only uses type-
raising and function composition when necessary.
most probable derivation. However, for this paper
the parser has been modified to simply output the
derivation in the form shown in Figure 2, which is
the input for the semantic component.
4 Building Semantic Representations
4.1 Semantic Formalism
Our method for constructing semantic representa-
tions can be used with many different semantic for-
malisms. In this paper we use formulas of first-order
logic with a neo-Davidsonian analysis of events. We
do not attempt to cover all semantic phenomena;
for example, we do not currently deal with the res-
olution of pronouns and ellipsis; we do not give
a proper analysis of tense and aspect; we do not
distinguish between distributive and collective read-
ings of plural noun phrases; and we do not handle
quantifier scope ambiguities.
The following first-order formula for the sentence
A spokesman had no comment demonstrates the rep-
resentation we use:

x(spokesman(x) 
	 y(comment(y) 


e(have(e)  agent(e,x)  patient(e,y)))).
The tool that we use to build semantic representa-
tions is based on the lambda calculus. It can be used
to mark missing semantic information from natural
language expressions in a principled way using ?,
an operator that binds variables ranging over vari-
ous semantic types. For instance, a noun phrase like
a spokesman can be given the ?-expression
?p.  x(spokesman(x)  (p@x))
where the @ denotes functional application, and the
variable p marks the missing information provided
by the verb phrase. This expression can be com-
bined with the ?-expression for lied, using func-
tional application, yielding the following expres-
sion:
?p.  x(spokesman(x)  (p@x))@
?y.  e(lie(e)  agent(e,y)).
?-conversion is the process of eliminating all oc-
currences of functional application by substituting
the argument for the ?-bound variables in the func-
tor. ?-conversion turns the previous expression into
a first-order translation for A spokesman lied:

x(spokesman(x)   e(lie(e)  agent(e,x))).
The resulting semantic formalism is very sim-
ilar to the type-theoretic language L? (Dowty et
al., 1981). However, we merely use the lambda-
calculus as a tool for constructing semantic rep-
resentations, rather as a formal tool for model-
theoretic interpretation. As already mentioned, we
can use the same method to obtain, for exam-
ple, Discourse Representation Structures (Kuschert,
1999), or underspecified semantic representations
(Bos, 2004) to deal with quantifier scope ambigu-
ities.
4.2 Method and Algorithm
The output of the parser is a tree representing a
CCG derivation, where the leaves are lexical items
and the nodes correspond to one of the CCG com-
binatory rules, a unary type-changing rule, a type-
raising rule, or one of the additional miscellaneous
rules discussed earlier. Mapping the CCG deriva-
tion into a semantic representation consists of the
following tasks:
1. assigning semantic representations to the lexi-
cal items;
2. reformulating the combinatory rules in terms
of functional application;
3. dealing with type-raising and type-changing
rules;
4. applying ?-conversion to the resulting tree
structure.
Lexical items are ordered pairs consisting of the
CCG category and a lemmatised wordform. This in-
formation is used to assign a ?-expression to the leaf
nodes in the tree. For most open-class lexical items
we use the lemma to instantiate the lexical seman-
tics, as illustrated by the following two examples
(intransitive verbs and adjectives):
 
S[dcl]\NP, walk 
?q?u.q@?x.  e(walk(e)  agent(e,x)  u@e)
 
N/N, big 
?p?x.(big(x)  p@x)
For closed-class lexical items, the lexical seman-
tics is spelled out for each lemma individually, as in
the following two examples:
 
(S[X]\NP)\(S[X]\NP), not 
?v?q?f.  ((v@q)@f)
 
NP[nb]/N, all 
?p?q.	 x(p@x  q@x)
The second task deals with the combinatory rules.
The rules we currently use are forward and back-
ward application (FAPP, BAPP), generalised for-
ward composition (FCOMP), backward composition
(BCOMP), and generalised backward-crossed com-
position (BCROSS).
FAPP

x  y 

x@y 
BAPP

x  y 

y@x 
FCOMP

x  y  ?u   x@  u@y 
BCOMP

x  y  ?u   y@  u@x 
BCROSS

x  y  ?u   y@  x@u 
The type-raising and type-changing rules are
dealt with by looking up the specific rule and replac-
ing it with the resulting semantics. For instance, the
rule that raises category NP to S[X]/(S[X]\NP)
converts the semantics as follows:
TYPERAISE(NP, S[X]/(S[X]\NP), x)
= ?v?e.((v@x)@e)
The following type-changing rule applies to the
lexical semantics of categories of type N and con-
verts them to NP:
TYPECHANGE(N, NP, y)
= ?p.  x(y@x  p@x)
Tasks 1?3 are implemented using a recursive al-
gorithm that traverses the derivation and returns a
?-expression. Note that the punctuation rules used
by the parser do not contribute to the compositional
semantics and are therefore ignored.
Task 4 reduces the ?-expression to the target rep-
resentation by applying ?-conversion. In order to
maintain correctness of this operation, the functor
undergoes ?-conversion (renaming all bound vari-
ables for new occurrences) before substitution takes
place. ?-conversion is implemented using the tools
provided by Blackburn and Bos (2003).
4.3 Results
There are a number of possible ways to evaluate the
semantic representations output by our system. The
first is to calculate the coverage ? that is, the per-
centage of syntactic parses which can be given some
analysis by the semantic component. The second is
to evaluate the accuracy of the semantic representa-
tions; the problem is that there is not yet an accepted
evaluation metric which can be applied to such rep-
resentations.
There is, however, an accepted way of evaluat-
ing the syntactic component of the system, namely
to calculate precision and recall figures for labelled
syntactic dependencies (Clark et al, 2002). Given
bapp(?S[dcl]?,
bapp(?NP?,
fapp(?NP[nb]?,
leaf(?NP[nb]/N?,?the?),
fapp(?N?,
leaf(?N/N?,school-board?),
leaf(?N?,?hearing?))),
fapp(?NP\NP?,
bapp(?(NP\NP)/S[dcl]?,
leaf(?(NP\NP)/NP?,?at?),
leaf(?((NP\NP)/S[dcl])\((NP\NP)/NP)?,?which?)),
bapp(?S[dcl]?,
leaf(?NP?,?she?),
fapp(?S[dcl]\NP?,
leaf(?(S[dcl]\NP)/(S[pss]\NP)?,?was?),
leaf(?S[pss]\NP?,?dismissed?)))),
fapp(?S[dcl]\NP?,
leaf(?(S[dcl]\NP)/(S[pss]\NP)?,?was?),
fapp(?S[pss]\NP?,
leaf(?(S[pss]\NP)/PP?,?crowded?),
fapp(?PP?,
leaf(?PP/NP?,?with?),
bapp(?NP?,
lex(?NP?,leaf(?N?,?students?)),
conj(?conj?,?NP?,?NP\NP?,
leaf(?conj?,?and?),
lex(?NP?,leaf(?N?,?teachers?)))))))).
some A ((school-board[A] & hearing[A]) & some B (female[B] & some C
(dismiss[C] & (patient[C,B] & (at[A,C] & some D (crowd[D] & (patient[D,A]
& ((some E (student[E] & with[D,E]) & some F (teacher[F] & with[D,F])) &
event[D]))))))))
Figure 2: Parser output and semantic representation for the example sentence:
The school-board hearing at which she was dismissed was crowded with students and teachers
that the CCG parser produces dependencies which
are essentially predicate-argument dependencies,
the accuracy of the syntactic component should be
a good indication of the accuracy of the semantics,
especially given the transparent interface between
syntax and semantics used by our system. Hence
we report coverage figures in this paper, and repeat
figures for dependency recovery from an earlier pa-
per.
We do not evaluate the accuracy of the system
output directly, but we do have a way of check-
ing the well-formedness of the semantic represen-
tations. (The well-formedness of the representation
does not of course guarantee the correctness of the
output.) If the semantic representation fails to ?-
convert, we know that there are type conflicts re-
sulting from either: incorrect semantics assigned to
some lexical entries; incorrect interpretation of one
of the combinatory rules; or an inconsistency in the
output of the syntactic component.
We assigned lexical semantics to the 245 most
frequent categories from the complete set of 409,
and implemented 4 of the type-raising rules, and the
10 unary type-changing rules, used by the parser.
We used section 00 from CCGbank for development
purposes; section 23 (2,401 sentences) was used as
the test set. The parser provides a syntactic analysis
for 98.6% of the sentences in section 23. The ac-
curacy of the parser is reported in Clark and Curran
(2004b): 84.6% F-score over labelled dependencies
for section 23. Of the sentences the parser analyses,
92.3% were assigned a semantic representation, all
of which were well-formed. The output of the sys-
tem for an example sentence is given in Figure 2.
The reason for the lack of complete coverage is
that we did not assign semantic representations to
the complete set of lexical categories. In future
work we will cover the complete set, but as a simple
remedy we have implemented the following robust-
ness strategy: we assign a semantic template to parts
of the tree that could not be analysed. For example,
the template for the NP category is ?p.  x(p@x).
This was done for the 10 most frequent categories
and results in a coverage of 98.6%.
Although we expect the accuracy of the seman-
tic representations to mirror those of the syntactic
component, and therefore be useful in NLP applica-
tions, there is still a small number of errors arising
from different sources. First, some constructions are
incorrectly analysed in CCGbank; for example, ap-
positives in CCGbank are represented as coordinate
constructions (Hockenmaier, 2003). Second, errors
are introduced by the semantic construction com-
ponent; for example, the non-head nouns in a noun-
noun compound are currently treated as modifiers of
the head noun, in the same way as adjectives. And
finally, the parser introduces errors because of in-
complete coverage of the lexicon, and mistakes due
to the parsing model. We expect general improve-
ments in statistical parsing technology will further
improve the accuracy of the parser, and we will fur-
ther develop the semantic component.
5 Conclusions and Future Work
This paper has demonstrated that we can construct
semantic representations using a wide-coverage
CCG parser, with a coverage of over 97% on un-
seen WSJ sentences. We believe this is a major step
towards wide-coverage semantic interpretation, one
of the key objectives of the field of NLP.
The advantages of our approach derive largely
from the use of CCG. The lexicalised nature of the
formalism means that our system has a high degree
of modularity, with separate syntactic and semantic
components.
We have shown how to construct simple first-
order semantic representations from CCG deriva-
tions. We have not dealt with all semantic phe-
nomena, such as quantifier scope ambiguities and
anaphora resolution. In future work we will in-
vestigate using underspecified semantic representa-
tions. The utility of our system for NLP applications
will be tested by integration with an existing open-
domain Question-Answering system (Leidner et al,
2003).
We will also investigate the construction of a tree-
bank of semantic representations derived automati-
cally from CCGbank. Previous work, such as Li-
akata and Pulman (2002) and Cahill et al (2003),
has attempted to generate semantic representations
from the Penn Treebank. Cahill et al use a transla-
tion of the Treebank to LFG F-structures and quasi-
logical forms. An advantage of our approach is
that our system for constructing semantic represen-
tations, whatever semantic formalism is used, can
be applied directly to the derivations in CCGbank.
Acknowledgements
This research was partially supported by EPSRC
grant GR/M96889.
References
Patrick Blackburn and Johan Bos. 2003. Represen-
tation and Inference for Natural Language. A First
Course in Computational Semantics. Draft available
at http://www.comsem.org, June.
Johan Bos. 2004. Computational semantics in dis-
course: Underspecification, resolution, and inference.
Journal of Logic, Language and Information, 12(2).
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of the 3rd LREC Conference, pages 1499?1504, Las
Palmas, Gran Canaria.
Aoife Cahill, Mairead McCarthy, Josef van Genabith,
and Andy Way. 2003. Quasi-Logical Forms from
F-Structures for the Penn Treebank. In Harry Bunt,
Ielka van der Sluis, and Roser Morante, editors, Pro-
ceedings of the Fifth International Workshop on Com-
putational Semantics (IWCS-5), pages 55?71. Tilburg
University.
J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. De-
vlin, and J. Tait. 1999. Simplifying text for language-
impaired readers. In Proceedings of the 9th Meeting
of EACL, pages 269?270, Bergen, Norway.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for machine
translation. In Proceedings of the MT Summit IX, New
Orleans, Louisiana.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Meeting of the
NAACL, pages 132?139, Seattle, WA.
Stephen Clark and James R. Curran. 2003. Log-linear
models for wide-coverage CCG parsing. In Proceed-
ings of the EMNLP Conference, pages 97?104, Sap-
poro, Japan.
Stephen Clark and James R. Curran. 2004a. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In Proceedings of the 20th International Confer-
ence on Computational Linguistics (COLING-04) (to
appear), Geneva, Switzerland.
Stephen Clark and James R. Curran. 2004b. Parsing
the WSJ using CCG and log-linear models. In Pro-
ceedings of the 42nd Meeting of the ACL (to appear),
Barcelona, Spain.
Stephen Clark, Julia Hockenmaier, and Mark Steedman.
2002. Building deep dependency structures with a
wide-coverage CCG parser. In Proceedings of the
40th Meeting of the ACL, pages 327?334, Philadel-
phia, PA.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
David R. Dowty, Robert E. Wall, and Stanley Peters.
1981. Introduction to Montague Semantics. Studies
in Linguistics and Philosophy. D. Reidel Publishing
Company.
Julia Hockenmaier and Mark Steedman. 2002a. Acquir-
ing compact lexicalized grammars from a cleaner tree-
bank. In Proceedings of the Third LREC Conference,
pages 1974?1981, Las Palmas, Spain.
Julia Hockenmaier and Mark Steedman. 2002b. Gen-
erative models for statistical parsing with Combina-
tory Categorial Grammar. In Proceedings of the 40th
Meeting of the ACL, pages 335?342, Philadelphia, PA.
Julia Hockenmaier. 2003. Data and Models for Statis-
tical Parsing with Combinatory Categorial Grammar.
Ph.D. thesis, University of Edinburgh.
Hans Kamp and Uwe Reyle. 1993. From Discourse
to Logic; An Introduction to Modeltheoretic Seman-
tics of Natural Language, Formal Logic and DRT.
Kluwer, Dordrecht.
Ronald M. Kaplan, Stefan Riezler, Tracy H. King,
John T. Maxwell III, Alexander Vasserman, and
Richard Crouch. 2004. Speed and accuracy in shal-
low and deep stochastic parsing. In Proceedings of
the HLT/NAACL Conference, Boston, MA.
Susanna Kuschert. 1999. Dynamic Meaning and Ac-
commodation. Ph.D. thesis, Universita?t des Saarlan-
des.
Jochen L. Leidner, Johan Bos, Tiphaine Dalmas,
James R. Curran, Stephen Clark, Colin J. Bannard,
Mark Steedman, and Bonnie Webber. 2003. The
QED open-domain answer retrieval system for TREC
2003. In Proceedings of the Twelfth Text Retrieval
Conference (TREC 2003), pages 595?599, Gaithers-
burg, MD.
Maria Liakata and Stephen Pulman. 2002. From trees to
predicate-argument structures. In Shu-Chuan Tseng,
editor, COLING 2002. Proceedings of the 19th In-
ternational Conference on Computational Linguistics,
pages 563?569. Taipei, Taiwan.
Marius Pasca and Sanda Harabagiu. 2001. High per-
formance question/answering. In Proceedings of the
ACL SIGIR Conference on Research and Development
in Information Retrieval, pages 366?374, New Or-
leans LA.
Philip Resnik and Aaron Elkiss. 2003. The lin-
guist?s search engine: Getting started guide. Tech-
nical Report LAMP-TR-108/CS-TR-4541/UMIACS-
TR-2003-109, University of Maryland, College Park,
MA.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, MA.
Kristina Toutanova, Christopher Manning, Stuart
Shieber, Dan Flickinger, and Stephan Oepen. 2002.
Parse disambiguation for a rich HPSG grammar. In
Proceedings of the First Workshop on Treebanks
and Linguistic Theories, pages 253?263, Sozopol,
Bulgaria.
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 505?512,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Creating a CCGbank and a wide-coverage CCG lexicon for German
Julia Hockenmaier
Institute for Research in Cognitive Science
University of Pennsylvania
Philadelphia, PA 19104, USA
juliahr@cis.upenn.edu
Abstract
We present an algorithm which creates a
German CCGbank by translating the syn-
tax graphs in the German Tiger corpus into
CCG derivation trees. The resulting cor-
pus contains 46,628 derivations, covering
95% of all complete sentences in Tiger.
Lexicons extracted from this corpus con-
tain correct lexical entries for 94% of all
known tokens in unseen text.
1 Introduction
A number of wide-coverage TAG, CCG, LFG and
HPSG grammars (Xia, 1999; Chen et al, 2005;
Hockenmaier and Steedman, 2002a; O?Donovan
et al, 2005; Miyao et al, 2004) have been ex-
tracted from the Penn Treebank (Marcus et al,
1993), and have enabled the creation of wide-
coverage parsers for English which recover local
and non-local dependencies that approximate the
underlying predicate-argument structure (Hocken-
maier and Steedman, 2002b; Clark and Curran,
2004; Miyao and Tsujii, 2005; Shen and Joshi,
2005). However, many corpora (Bo?homva? et al,
2003; Skut et al, 1997; Brants et al, 2002) use
dependency graphs or other representations, and
the extraction algorithms that have been developed
for Penn Treebank style corpora may not be im-
mediately applicable to this representation. As a
consequence, research on statistical parsing with
?deep? grammars has largely been confined to En-
glish. Free-word order languages typically pose
greater challenges for syntactic theories (Rambow,
1994), and the richer inflectional morphology of
these languages creates additional problems both
for the coverage of lexicalized formalisms such
as CCG or TAG, and for the usefulness of de-
pendency counts extracted from the training data.
On the other hand, formalisms such as CCG and
TAG are particularly suited to capture the cross-
ing dependencies that arise in languages such as
Dutch or German, and by choosing an appropriate
linguistic representation, some of these problems
may be mitigated.
Here, we present an algorithm which translates
the German Tiger corpus (Brants et al, 2002) into
CCG derivations. Similar algorithms have been
developed by Hockenmaier and Steedman (2002a)
to create CCGbank, a corpus of CCG derivations
(Hockenmaier and Steedman, 2005) from the Penn
Treebank, by C?ak?c? (2005) to extract a CCG lex-
icon from a Turkish dependency corpus, and by
Moortgat and Moot (2002) to induce a type-logical
grammar for Dutch.
The annotation scheme used in Tiger is an ex-
tension of that used in the earlier, and smaller,
German Negra corpus (Skut et al, 1997). Tiger
is better suited for the extraction of subcatego-
rization information (and thus the translation into
?deep? grammars of any kind), since it distin-
guishes between PP complements and modifiers,
and includes ?secondary? edges to indicate shared
arguments in coordinate constructions. Tiger also
includes morphology and lemma information.
Negra is also provided with a ?Penn Treebank?-
style representation, which uses flat phrase struc-
ture trees instead of the crossing dependency
structures in the original corpus. This version
has been used by Cahill et al (2005) to extract a
German LFG. However, Dubey and Keller (2003)
have demonstrated that lexicalization does not
help a Collins-style parser that is trained on this
corpus, and Levy and Manning (2004) have shown
that its context-free representation is a poor ap-
proximation to the underlying dependency struc-
ture. The resource presented here will enable
future research to address the question whether
?deep? grammars such as CCG, which capture the
underlying dependencies directly, are better suited
to parsing German than linguistically inadequate
context-free approximations.
505
1. Standard main clause
Peter gibt Maria das Buch
                   
      
 	
                         

       

    
 
 	

2. Main clause with fronted adjunct 3. Main clause with fronted complement
dann gibt Peter Maria das Buch
                     
 
 	
          
 
    
 
 

 	

Maria gibt Peter das Buch
                   
  
 
 
 	
                    
 
    
 
 	

Figure 1: CCG uses topicalization (1.), a type-changing rule (2.), and type-raising (3.) to capture the
different variants of German main clause order with the same lexical category for the verb.
2 German syntax and morphology
Morphology German verbs are inflected for
person, number, tense and mood. German nouns
and adjectives are inflected for number, case and
gender, and noun compounding is very productive.
Word order German has three different word
orders that depend on the clause type. Main
clauses (1) are verb-second. Imperatives and ques-
tions are verb-initial (2). If a modifier or one of
the objects is moved to the front, the word order
becomes verb-initial (2). Subordinate and relative
clauses are verb-final (3):
(1) a. Peter gibt Maria das Buch.
Peter gives Mary the book.
b. ein Buch gibt Peter Maria.
c. dann gibt Peter Maria das Buch.
(2) a. Gibt Peter Maria das Buch?
b. Gib Maria das Buch!
(3) a. dass Peter Maria das Buch gibt.
b. das Buch, das Peter Maria gibt.
Local Scrambling In the so-called ?Mittelfeld?
all orders of arguments and adjuncts are poten-
tially possible. In the following example, all 5!
permutations are grammatical (Rambow, 1994):
(4) dass [eine Firma] [meinem Onkel] [die Mo?bel] [vor
drei Tagen] [ohne Voranmeldung] zugestellt hat.
that [a company] [to my uncle] [the furniture] [three
days ago] [without notice] delivered has.
Long-distance scrambling Objects of embed-
ded verbs can also be extraposed unboundedly
within the same sentence (Rambow, 1994):
(5) dass [den Schrank] [niemand] [zu reparieren] ver-
sprochen hat.
that [the wardrobe] [nobody] [to repair] promised
has.
3 A CCG for German
3.1 Combinatory Categorial Grammar
CCG (Steedman (1996; 2000)) is a lexicalized
grammar formalism with a completely transparent
syntax-semantics interface. Since CCG is mildly
context-sensitive, it can capture the crossing de-
pendencies that arise in Dutch or German, yet is
efficiently parseable.
In categorial grammar, words are associ-
ated with syntactic categories, such as    or
     for English intransitive and transitive
verbs. Categories of the form   or   are func-
tors, which take an argument  to their left or right
(depending on the the direction of the slash) and
yield a result . Every syntactic category is paired
with a semantic interpretation (usually a  -term).
Like all variants of categorial grammar, CCG
uses function application to combine constituents,
but it also uses a set of combinatory rules such as
composition ( ) and type-raising (). Non-order-
preserving type-raising is used for topicalization:
Application:     
    
Composition:     
 
 
    
 
 
    
 
 
    
 
 
Type-raising:  

   
Topicalization:  

   
Hockenmaier and Steedman (2005) advocate
the use of additional ?type-changing? rules to deal
with complex adjunct categories (e.g.     
 	  for ing-VPs that act as noun phrase mod-
ifiers). Here, we also use a small number of such
rules to deal with similar adjunct cases.
506
3.2 Capturing German word order
We follow Steedman (2000) in assuming that the
underlying word order in main clauses is always
verb-initial, and that the sententce-initial subject is
in fact topicalized. This enables us to capture dif-
ferent word orders with the same lexical category
(Figure 1). We use the features  
 and  
 to
distinguish verbs in main and subordinate clauses.
Main clauses have the feature  , requiring ei-
ther a sentential modifier with category    
,
a topicalized subject (    
 ), or a
type-raised argument (    
 ), where 
can be any argument category, such as a noun
phrase, prepositional phrase, or a non-finite VP.
Here is the CCG derivation for the subordinate
clause ( ) example:
dass Peter Maria das Buch gibt
   
           
         

 
      

 
   

 

 
 
For simplicity?s sake our extraction algorithm
ignores the issues that arise through local scram-
bling, and assumes that there are different lexical
category for each permutation.1
Type-raising and composition are also used to
deal with wh-extraction and with long-distance
scrambling (Figure 2).
4 Translating Tiger graphs into CCG
4.1 The Tiger corpus
The Tiger corpus (Brants et al, 2002) is a pub-
licly available2 corpus of ca. 50,000 sentences (al-
most 900,000 tokens) taken from the Frankfurter
Rundschau newspaper. The annotation is based
on a hybrid framework which contains features of
phrase-structure and dependency grammar. Each
sentence is represented as a graph whose nodes
are labeled with syntactic categories (NP, VP, S,
PP, etc.) and POS tags. Edges are directed and la-
beled with syntactic functions (e.g. head, subject,
accusative object, conjunct, appositive). The edge
labels are similar to the Penn Treebank function
tags, but provide richer and more explicit infor-
mation. Only 72.5% of the graphs have no cross-
ing edges; the remaining 27.5% are marked as dis-
1Variants of CCG, such as Set-CCG (Hoffman, 1995) and
Multimodal-CCG (Baldridge, 2002), allow a more compact
lexicon for free word order languages.
2http://www.ims.uni-stuttgart.de/projekte/TIGER
continuous. 7.3% of the sentences have one or
more ?secondary? edges, which are used to indi-
cate double dependencies that arise in coordinated
structures which are difficult to bracket, such as
right node raising, argument cluster coordination
or gapping. There are no traces or null elements to
indicate non-local dependencies or wh-movement.
Figure 2 shows the Tiger graph for a PP whose
NP argument is modified by a relative clause.
There is no NP level inside PPs (and no noun level
inside NPs). Punctuation marks are often attached
at the so-called ?virtual? root (VROOT) of the en-
tire graph. The relative pronoun is a dative object
(edge label DA) of the embedded infinitive, and
is therefore attached at the VP level. The relative
clause itself has the category S; the incoming edge
is labeled RC (relative clause).
4.2 The translation algorithm
Our translation algorithm has the following steps:
translate(TigerGraph g):
TigerTree t = createTree(g);
preprocess(t);
if (t  null)
CCGderiv d = translateToCCG(t);
if (d  null);
if (isCCGderivation(d))
return d;
else fail;
else fail;
else fail;
1. Creating a planar tree: After an initial pre-
processing step which inserts punctuation that is
attached to the ?virtual? root (VROOT) of the
graph in the appropriate locations, discontinuous
graphs are transformed into planar trees. Starting
at the lowest nonterminal nodes, this step turns
the Tiger graph into a planar tree without cross-
ing edges, where every node spans a contiguous
substring. This is required as input to the actual
translation step, since CCG derivations are pla-
nar binary trees. If the first to the th child of a
node  span a contiguous substring that ends in
the th word, and the  th child spans a sub-
string starting at   , we attempt to move
the first  children of  to its parent  (if the
head position of  is greater than ). Punctuation
marks and adjuncts are simply moved up the tree
and treated as if they were originally attached to
 . This changes the syntactic scope of adjuncts,
but typically only VP modifiers are affected which
could also be attached at a higher VP or S node
without a change in meaning. The main exception
507
1. The original Tiger graph:
  an
  in
APPR
einem
  a
ART
H?chsten
 Highest
   NN
 dem
whom
PRELS
sich
refl.
PRF 
 fraglos
 without
questions
 ADJD 
 habe
 have
VAFIN
HD
HDMODA
SB OC
NKNKAC RC
PP
VP
 der
 the
ART
Mensch
human
  NN
kleine
small
ADJA
NK NK NK
NP
 S
     zu
     to
PTKZU
unterwerfen
   submit
  VVVIN
PM HD
VZ
OA
 ,
$, 
2. After transformation into a planar tree and preprocessing:
PP
APPR-AC
an
NP-ARG
ART-HD
einem
NOUN-ARG
NN-NK
Ho?chsten
PKT
,
SBAR-RC
PRELS-EXTRA-DA
dem
S-ARG
NP-SB
ART-NK
der
NOUN-ARG
ADJA-NK
kleine
NN-HD
Mensch
VP-OC
PRF-ADJ
sich
ADJD-MO
fraglos
VZ-HD
PTKZU-PM
zu
VVINF
unterwerfen
VAFIN-HD
habe
3. The resulting CCG derivation

   
an
  
  
  
     
einem
  
Ho?chsten

,
   
     
   
dem
 
   
 
  
   
  
     
der
  
   
kleine
  
Mensch
 
      
      
     
sich
      
     
fraglos
      
       
zu
      
unterwerfen
 
       
habe
Figure 2: From Tiger graphs to CCG derivations
are extraposed relative clauses, which CCG treats
as sentential modifiers with an anaphoric depen-
dency. Arguments that are moved up are marked
as extracted, and an additional ?extraction? edge
(explained below) from the original head is intro-
duced to capture the correct dependencies in the
CCG derivation. Discontinuous dependencies be-
tween resumptive pronouns (?place holders?, PH)
and their antecedents (?repeated elements?, RE)
are also dissolved.
2. Additional preprocessing: In order to obtain
the desired CCG analysis, a certain amount of pre-
processing is required. We insert NPs into PPs,
nouns into NPs3, and change sentences whose
first element is a complementizer (dass, ob, etc.)
into an SBAR (a category which does not ex-
ist in the original Tiger annotation) with S argu-
3The span of nouns is given by the NK edge label.
ment. This is necessary to obtain the desired CCG
derivations where complementizers and preposi-
tions take a sentential or nominal argument to their
right, whereas they appear at the same level as
their arguments in the Tiger corpus. Further pre-
processing is required to create the required struc-
tures for wh-extraction and certain coordination
phenomena (see below).
In figure 2, preprocessing of the original Tiger
graph (top) yields the tree shown in the middle
(edge labels are shown as Penn Treebank-style
function tags).4
We will first present the basic translation algo-
rithm before we explain how we obtain a deriva-
tion which captures the dependency between the
relative pronoun and the embedded verb.
4We treat reflexive pronouns as modifiers.
508
3. The basic translation step Our basic transla-
tion algorithm is very similar to Hockenmaier and
Steedman (2005). It requires a planar tree with-
out crossing edges, where each node is marked as
head, complement or adjunct. The latter informa-
tion is represented in the Tiger edge labels, and
only a small number of additional head rules is re-
quired. Each individual translation step operates
on local trees, which are typically flat.
N
C
 
C

... C

... C
  
C

Assuming the CCG category of  is , and its
head position is , the algorithm traverses first the
left nodes 	
 
...	
  
from left to right to create a
right-branching derivation tree, and then the right
nodes (	

...	
  
) from right to left to create a
left-branching tree. The algorithm starts at the root
category and recursively traverses the tree.
N
C
 
L
 C

L
... R
R
R
H

...
C
  
C

The CCG category of complements and of the
root of the graph is determined from their Tiger
label. VPs are   , where the feature  dis-
tinguishes bare infinitives, zu-infinitives, passives,
and (active) past participles. With the exception
of passives, these features can be determined from
the POS tags alone.5 Embedded sentences (under
an SBAR-node) are always  
. NPs and nouns
( and ) have a case feature, e.g. .6 Like
the English CCGbank, our grammar ignores num-
ber and person agreement.
Special cases: Wh-extraction and extraposition
In Tiger, wh-extraction is not explicitly marked.
Relative clauses, wh-questions and free relatives
are all annotated as S-nodes,and the wh-word is
a normal argument of the verb. After turning the
graph into a planar tree, we can identify these
constructions by searching for a relative pronoun
in the leftmost child of an S node (which may
be marked as extraposed in the case of extrac-
tion from an embedded verb). As shown in fig-
ure 2, we turn this S into an SBAR (a category
which does not exist in Tiger) with the first edge
as complementizer and move the remaining chil-
5Eventive (?werden?) passive is easily identified by con-
text; however, we found that not all stative (?sein?) passives
seem to be annotated as such.
6In some contexts, measure nouns (e.g. Mark, Kilometer)
lack case annotation.
dren under a new S node which becomes the sec-
ond daughter of the SBAR. The relative pronoun
is the head of this SBAR and takes the S-node as
argument. Its category is  
, since all clauses
with a complementizer are verb-final. In order to
capture the long-range dependency, a ?trace? is
introduced, and percolated down the tree, much
like in the algorithm of Hockenmaier and Steed-
man (2005), and similar to GPSG?s slash-passing
(Gazdar et al, 1985). These trace categories are
appended to the category of the head node (and
other arguments are type-raised as necessary). In
our case, the trace is also associated with the verb
whose argument it is. If the span of this verb
is within the span of a complement, the trace is
percolated down this complement. When the VP
that is headed by this verb is reached, we assume
a canonical order of arguments in order to ?dis-
charge? the trace.
If a complement node is marked as extraposed,
it is also percolated down the head tree until the
constituent whose argument it is is found. When
another complement is found whose span includes
the span of the constituent whose argument the ex-
traposed edge is, the extraposed category is perco-
lated down this tree (we assume extraction out of
adjuncts is impossible).7 In order to capture the
topicalization analysis, main clause subjects also
introduce a trace. Fronted complements or sub-
jects, and the first adjunct in main clauses are ana-
lyzed as described in figure 1.
Special case: coordination ? secondary edges
Tiger uses ?secondary edges? to represent the de-
pendencies that arise in coordinate constructions
such as gapping, argument cluster coordination
and right (or left) node raising (Figure 3). In right
(left) node raising, the shared elements are argu-
ments or adjuncts that appear on the right periph-
ery of the last, (or left periphery of the first) con-
junct. CCG uses type-raising and composition to
combine the incomplete conjuncts into one con-
stituent which combines with the shared element:
liest immer und beantwortet gerne jeden Brief.
always reads and gladly replies to every letter.
      	        
 
     
 
   
7In our current implementation, each node cannot have
more than one forward and one backward extraposed element
and one forward and one backward trace. It may be preferable
to use list structures instead, especially for extraposition.
509
Complex coordinations: a Tiger graph with secondary edges
MO
w?hrend
  while
 KOUS
  78
  78
CARD
Prozent
percent
   NN
und
and
KON
sich
refl.
PRF 
 aussprachen
    argued
     VVFIN
HDSBCP
  f?r
  for
APPR
Bush
Bush
  NE
 S
OA
 
 vier
 vier
CARD
Prozent
percent
   NN
  f?r
  for
APPR
Clinton
Clinton
    NE
NKAC
PP
NKAC
PP
NKNK
NP
NKNK
NP
SBMO
 S
CDCJ CJ
CS
The planar tree after preprocessing:
SBAR
KOUS-HD
wa?hrend
S-ARG
ARGCLUSTER
S-CJ
NP-SB
78 Prozent
PRF-MO
sich
PP-MO
fu?r Bush
KON-CD
und
S-CJ
NP-SB
vier Prozent
PP-MO
fu?r Clinton
VVFIN-HD
aussprachen
The resulting CCG derivation:
 
   	

wa?hrend
 

 
  
   
 
  
   
 
  
   
 
  
   
  
78 Prozent
     
sich
     
fu?r Bush
 
  
    	
	
und
 
  
   
 
  
   
  
vier Prozent
     
fu?r Clinton
 
   
aussprachen
Figure 3: Processing secondary edges in Tiger
In order to obtain this analysis, we lift such
shared peripheral constituents inside the conjuncts
of conjoined sentences CS (or verb phrases, CVP)
to new S (VP) level that we insert in between the
CS and its parent.
In argument cluster coordination (Figure 3), the
shared peripheral element (aussprachen) is the
head.8 In CCG, the remaining arguments and ad-
juncts combine via composition and typeraising
into a functor category which takes the category of
the head as argument (e.g. a ditransitive verb), and
returns the same category that would result from
a non-coordinated structure (e.g. a VP). The re-
sult category of the furthest element in each con-
junct is equal to the category of the entire VP (or
sentence), and all other elements are type-raised
and composed with this to yield a category which
takes as argument a verb with the required subcat
frame and returns a verb phrase (sentence). Tiger
assumes instead that there are two conjuncts (one
of which is headless), and uses secondary edges
8Wa?hrend has scope over the entire coordinated structure.
to indicate the dependencies between the head and
the elements in the distant conjunct. Coordinated
sentences and VPs (CS and CVP) that have this
annotation are rebracketed to obtain the CCG con-
stituent structure, and the conjuncts are marked as
argument clusters. Since the edges in the argu-
ment cluster are labeled with their correct syntac-
tic functions, we are able to mimic the derivation
during category assignment.
In sentential gapping, the main verb is shared
and appears in the middle of the first conjunct:
(6) Er trinkt Bier und sie Wein.
He drinks beer and she wine.
As in the English CCGbank, we ignore this con-
struction, which requires a non-combinatory ?de-
composition? rule (Steedman, 1990).
5 Evaluation
Translation coverage The algorithm can fail at
several stages. If the graph cannot be turned into a
tree, it cannot be translated. This happens in 1.3%
(647) of all sentences. In many cases, this is due
510
to coordinated NPs or PPs where one or more con-
juncts are extraposed. We believe that these are
anaphoric, and further preprocessing could take
care of this. In other cases, this is due to verb top-
icalization (gegeben hat Peter Maria das Buch),
which our algorithm cannot currently deal with.9
For 1.9% of the sentences, the algorithm cannot
obtain a correct CCG derivation. Mostly this is
the case because some traces and extraposed el-
ements cannot be discharged properly. Typically
this happens either in local scrambling, where an
object of the main verb appears between the aux-
iliary and the subject (hat das Buch Peter...)10, or
when an argument of a noun that appears in a rel-
ative clause is extraposed to the right. There are
also a small number of constituents whose head is
not annotated. We ignore any gapping construc-
tion or argument cluster coordination that we can-
not get into the right shape (1.5%), 732 sentences).
There are also a number of other constructions
that we do not currently deal with. We do not pro-
cess sentences if the root of the graph is a ?virtual
root? that does not expand into a sentence (1.7%,
869). This is mostly the case for strings such as
Frankfurt (Reuters)), or if we cannot identify a
head child of the root node (1.3%, 648; mostly
fragments or elliptical constructions).
Overall, we obtain CCG derivations for 92.4%
(46,628) of all 54,0474 sentences, including
88.4% (12,122) of those whose Tiger graphs are
marked as discontinuous (13,717), and 95.2%
of all 48,957 full sentences (excluding headless
roots, and fragments, but counting coordinate
structures such as gapping).
Lexicon size There are 2,506 lexical category
types, but 1,018 of these appear only once. 933
category types appear more than 5 times.
Lexical coverage In order to evaluate coverage
of the extracted lexicon on unseen data, we split
the corpus into segments of 5,000 sentences (ig-
noring the last 474), and perform 10-fold cross-
validation, using 9 segments to extract a lexicon
and the 10th to test its coverage. Average cover-
age is 86.7% (by token) of all lexical categories.
Coverage varies between 84.4% and 87.6%. On
average, 92% (90.3%-92.6%) of the lexical tokens
9The corresponding CCG derivation combines the rem-
nant complements as in argument cluster coordination.
10This problem arises because Tiger annotates subjects as
arguments of the auxiliary. We believe this problem could be
avoided if they were instead arguments of the non-finite verb.
that appear in the held-out data also appear in the
training data. On these seen tokens, coverage is
94.2% (93.5%-92.6%). More than half of all miss-
ing lexical entries are nouns.
In the English CCGbank, a lexicon extracted
from section 02-21 (930,000 tokens) has 94% cov-
erage on all tokens in section 00, and 97.7% cov-
erage on all seen tokens (Hockenmaier and Steed-
man, 2005). In the English data set, the proportion
of seen tokens (96.2%) is much higher, most likely
because of the relative lack of derivational and in-
flectional morphology. The better lexical coverage
on seen tokens is also to be expected, given that the
flexible word order of German requires case mark-
ings on all nouns as well as at least two different
categories for each tensed verb, and more in order
to account for local scrambling.
6 Conclusion and future work
We have presented an algorithm which converts
the syntax graphs in the German Tiger corpus
(Brants et al, 2002) into Combinatory Catego-
rial Grammar derivation trees. This algorithm is
currently able to translate 92.4% of all graphs in
Tiger, or 95.2% of all full sentences. Lexicons
extracted from this corpus contain the correct en-
tries for 86.7% of all and 94.2% of all seen to-
kens. Good lexical coverage is essential for the
performance of statistical CCG parsers (Hocken-
maier and Steedman, 2002a). Since the Tiger cor-
pus contains complete morphological and lemma
information for all words, future work will address
the question of how to identify and apply a set of
(non-recursive) lexical rules (Carpenter, 1992) to
the extracted CCG lexicon to create a much larger
lexicon. The number of lexical category types is
almost twice as large as that of the English CCG-
bank. This is to be expected, since our gram-
mar includes case features, and German verbs re-
quire different categories for main and subordinate
clauses. We currently perform only the most es-
sential preprocessing steps, although there are a
number of constructions that might benefit from
additional changes (e.g. comparatives, parentheti-
cals, or fragments), both to increase coverage and
accuracy of the extracted grammar.
Since Tiger corpus is of comparable size to the
Penn Treebank, we hope that the work presented
here will stimulate research into statistical wide-
coverage parsing of free word order languages
such as German with deep grammars like CCG.
511
Acknowledgments
I would like to thank Mark Steedman and Aravind
Joshi for many helpful discussions. This research
is supported by NSF ITR grant 0205456.
References
Jason Baldridge. 2002. Lexically Specified Derivational
Control in Combinatory Categorial Grammar. Ph.D. the-
sis, School of Informatics, University of Edinburgh.
Alena Bo?homva?, Jan Hajic?, Eva Hajic?ova?, and Barbora
Hladka?. 2003. The Prague Dependency Treebank: Three-
level annotation scenario. In Anne Abeille?, editor, Tree-
banks: Building and Using Syntactially Annotated Cor-
pora. Kluwer.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lexius, and George Smith. 2002. The TIGER tree-
bank. In Workshop on Treebanks and Linguistic Theories,
Sozpol.
Aoife Cahill, Martin Forst, Mairead McCarthy, Ruth
O?Donovan, Christian Rohrer, Josef van Genabith, and
Andy Way. 2005. Treebank-based acquisition of multilin-
gual unification-grammar resources. Journal of Research
on Language and Computation.
Ruken C?ak?c?. 2005. Automatic induction of a CCG gram-
mar for Turkish. In ACL Student Research Workshop,
pages 73?78, Ann Arbor, MI, June.
Bob Carpenter. 1992. Categorial grammars, lexical rules,
and the English predicative. In Robert Levine, editor, For-
mal Grammar: Theory and Implementation, chapter 3.
Oxford University Press.
John Chen, Srinivas Bangalore, and K. Vijay-Shanker. 2005.
Automated extraction of Tree-Adjoining Grammars from
treebanks. Natural Language Engineering.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In Proceedings
of the 42nd Annual Meeting of the Association for Com-
putational Linguistics, Barcelona, Spain.
Amit Dubey and Frank Keller. 2003. Probabilistic parsing
for German using Sister-Head dependencies. In Erhard
Hinrichs and Dan Roth, editors, Proceedings of the 41st
Annual Meeting of the Association for Computational Lin-
guistics, pages 96?103, Sapporo, Japan.
Gerald Gazdar, Ewan Klein, Geoffrey K. Pullum, and Ivan A.
Sag. 1985. Generalised Phrase Structure Grammar.
Blackwell, Oxford.
Julia Hockenmaier and Mark Steedman. 2002a. Acquir-
ing compact lexicalized grammars from a cleaner Tree-
bank. In Proceedings of the Third International Con-
ference on Language Resources and Evaluation (LREC),
pages 1974?1981, Las Palmas, Spain, May.
Julia Hockenmaier and Mark Steedman. 2002b. Generative
models for statistical parsing with Combinatory Categorial
Grammar. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics, pages 335?
342, Philadelphia, PA.
Julia Hockenmaier and Mark Steedman. 2005. CCGbank:
Users? manual. Technical Report MS-CIS-05-09, Com-
puter and Information Science, University of Pennsylva-
nia.
Beryl Hoffman. 1995. Computational Analysis of the Syntax
and Interpretation of ?Free? Word-order in Turkish. Ph.D.
thesis, University of Pennsylvania. IRCS Report 95-17.
Roger Levy and Christopher Manning. 2004. Deep depen-
dencies from context-free statistical parsers: correcting
the surface dependency approximation. In Proceedings
of the 42nd Annual Meeting of the Association for Com-
putational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Linguis-
tics, 19:313?330.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilistic dis-
ambiguation models for wide-coverage HPSG parsing. In
Proceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 83?90, Ann Ar-
bor, MI.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii. 2004.
Corpus-oriented grammar development for acquiring a
Head-driven Phrase Structure Grammar from the Penn
Treebank. In Proceedings of the First International Joint
Conference on Natural Language Processing (IJCNLP-
04).
Michael Moortgat and Richard Moot. 2002. Using the Spo-
ken Dutch Corpus for type-logical grammar induction.
In Proceedings of the Third International Conference on
Language Resources and Evaluation (LREC).
Ruth O?Donovan, Michael Burke, Aoife Cahill, Josef van
Genabith, and Andy Way. 2005. Large-scale induc-
tion and evaluation of lexical resources from the Penn-
II and Penn-III Treebanks. Computational Linguistics,
31(3):329 ? 365, September.
Owen Rambow. 1994. Formal and Computational Aspects
of Natural Language Syntax. Ph.D. thesis, University of
Pennsylvania, Philadelphia PA.
Libin Shen and Aravind K. Joshi. 2005. Incremental LTAG
parsing. In Proceedings of the Human Language Tech-
nology Conference / Conference of Empirical Methods in
Natural Language Processing (HLT/EMNLP).
Wojciech Skut, Brigitte Krenn, Thorsten Brants, and Hans
Uszkoreit. 1997. An annotation scheme for free word
order languages. In Fifth Conference on Applied Natural
Language Processing.
Mark Steedman. 1990. Gapping as constituent coordination.
Linguistics and Philosophy, 13:207?263.
Mark Steedman. 1996. Surface Structure and Interpretation.
MIT Press, Cambridge, MA. Linguistic Inquiry Mono-
graph, 30.
Mark Steedman. 2000. The Syntactic Process. MIT Press,
Cambridge, MA.
Fei Xia. 1999. Extracting Tree Adjoining Grammars from
bracketed corpora. In Proceedings of the 5th Natural Lan-
guage Processing Pacific Rim Symposium (NLPRS-99).
512
Identifying Semantic Roles Using Combinatory Categorial Grammar
Daniel Gildea and Julia Hockenmaier
University of Pennsylvania
{dgildea,juliahr}@cis.upenn.edu
Abstract
We present a system for automatically
identifying PropBank-style semantic roles
based on the output of a statistical parser
for Combinatory Categorial Grammar.
This system performs at least as well as
a system based on a traditional Treebank
parser, and outperforms it on core argu-
ment roles.
1 Introduction
Correctly identifying the semantic roles of sentence
constituents is a crucial part of interpreting text, and
in addition to forming an important part of the infor-
mation extraction problem, can serve as an interme-
diate step in machine translation or automatic sum-
marization. Even for a single predicate, semantic
arguments can have multiple syntactic realizations,
as shown by the following paraphrases:
(1) John will meet with Mary.
John will meet Mary.
John and Mary will meet.
(2) The door opened.
Mary opened the door.
Recently, attention has turned to creating cor-
pora annotated with argument structures. The
PropBank (Kingsbury and Palmer, 2002) and the
FrameNet (Baker et al, 1998) projects both doc-
ument the variation in syntactic realization of the
arguments of predicates in general English text.
Gildea and Palmer (2002) developed a system to
predict semantic roles (as defined in PropBank) from
sentences and their parse trees as determined by the
statistical parser of Collins (1999). In this paper, we
examine how the syntactic representations used by
different statistical parsers affect the performance
of such a system. We compare a parser based on
Combinatory Categorial Grammar (CCG) (Hocken-
maier and Steedman, 2002b) with the Collins parser.
As the CCG parser is trained and tested on a cor-
pus of CCG derivations that have been obtained by
automatic conversion from the Penn Treebank, we
are able to compare performance using both gold-
standard and automatic parses for both CCG and the
traditional Treebank representation. The Treebank-
parser returns skeletal phrase-structure trees with-
out the traces or functional tags in the original Penn
Treebank, whereas the CCG parser returns word-
word dependencies that correspond to the under-
lying predicate-argument structure, including long-
range dependencies arising through control, raising,
extraction and coordination.
2 Predicate-argument relations in
PropBank
The Proposition Bank (Kingsbury and Palmer,
2002) provides a human-annotated corpus of
semantic verb-argument relations. For each verb
appearing in the corpus, a set of semantic roles is
defined. Roles for each verb are simply numbered
Arg0, Arg1, Arg2, etc. As an example, the entry-
specific roles for the verb offer are given below:
Arg0 entity offering
Arg1 commodity
Arg2 price
Arg3 benefactive or entity offered to
These roles are then annotated for every instance
of the verb appearing in the corpus, including the
following examples:
(3) [ARG0 the company] to offer [ARG1 a 15% stake]
to [ARG2 the public].
(4) [ARG0 Sotheby?s] ... offered [ARG2 the Dorrance
heirs] [ARG1 a money-back guarantee]
(5) [ARG1 an amendment] offered by [ARG0 Rep.
Peter DeFazio]
(6) [ARG2 Subcontractors] will be offered [ARG1 a
settlement]
A variety of additional roles are assumed
to apply across all verbs. These secondary
roles can be thought of as being adjuncts,
rather than arguments, although no claims are
made as to optionality or other traditional argu-
ment/adjunct tests. The secondary roles include:
Location in Tokyo, outside
Time last week, on Tuesday, never
Manner easily, dramatically
Direction south, into the wind
Cause due to pressure from Washington
Discourse however, also, on the other hand
Extent 15%, 289 points
Purpose to satisfy requirements
Negation not, n?t
Modal can, might, should, will
Adverbial (none of the above)
and are represented in PropBank as ?ArgM? with an
additional function tag, for example ArgM-TMP for
temporal. We refer to PropBank?s numbered argu-
ments as ?core? arguments. Core arguments repre-
sent 75% of the total labeled roles in the PropBank
data. Our system predicts all the roles, including
core arguments as well as the ArgM labels and their
function tags.
3 Predicate-argument relations in CCG
Combinatory Categorial Grammar (CCG) (Steed-
man, 2000), is a grammatical theory which provides
a completely transparent interface between surface
syntax and underlying semantics, such that each
syntactic derivation corresponds directly to an in-
terpretable semantic representation which includes
long-range dependencies that arise through control,
raising, coordination and extraction.
In CCG, words are assigned atomic cate-
gories such as NP, or functor categories like
(S[dcl]\NP)/NP (transitive declarative verb) or S/S
(sentential modifier). Adjuncts are represented
as functor categories such as S/S which expect
and return the same type. We use indices to
number the arguments of functor categories, eg.
(S[dcl]\NP
1
)/NP
2
, or S/S
1
, and indicate the word-
word dependencies in the predicate-argument struc-
ture as tuples ?w
h
, c
h
, i, w
a
?, where c
h
is the lexical
category of the head word w
h
, and w
a
is the head
word of the constituent that fills the ith argument of
c
h
.
Long-range dependencies can be projected
through certain types of lexical categories or
through rules such as coordination of functor
categories. For example, in the lexical category of a
relative pronoun, (NP\NP
i
)/(S[dcl]/NP
i
), the head
of the NP that is missing from the relative clause
is unified with (as indicated by the indices i) the
head of the NP that is modified by the entire relative
clause.
Figure 1 shows the derivations of an ordinary
sentence, a relative clause and a right-node-raising
construction. In all three sentences, the predicate-
argument relations between London and denied and
plans and denied are the same, which in CCG is
expressed by the fact that London fills the first (ie.
subject) argument slot of the lexical category of de-
nied, (S[dcl]\NP
1
)/NP
2
, and plans fills the second
(object) slot. The relations extracted from the CCG
derivation for the sentence ?London denied plans on
Monday? are shown in Table 1.
The CCG parser returns the local and long-range
word-word dependencies that express the predicate-
argument structure corresponding to the derivation.
These relations are recovered with an accuracy of
around 83% (labeled recovery) or 91% (unlabeled
recovery) (Hockenmaier, 2003). By contrast, stan-
dard Treebank parsers such as (Collins, 1999) only
return phrase-structure trees, from which non-local
dependencies are difficult to recover.
S[dcl]
NP
1
N
London
S[dcl]\NP
1
S[dcl]\NP
1
(S[dcl]\NP
1
)/NP
2
denied
NP
2
N
plans
(S\NP)\(S\NP)
((S\NP)\(S\NP))/NP
on
NP
N
Monday
NP
NP
2
N
plans
NP\NP
2
(NP\NP
i
)/(S[dcl]/NP
i
)
that
S[dcl]/NP
2
S/(S\NP
1
)
NP
N
London
(S[dcl]\NP
1
)/NP
2
denied
S[dcl]
S[dcl]/NP
2
S[dcl]/NP
2
S/(S\NP
1
)
NP
N
London
(S[dcl]\NP
1
)/NP
2
denied
S[dcl]/NP[conj]
conj
but
S[dcl]/NP
S/(S\NP)
NP
N
Paris
(S[dcl]\NP)/NP
admitted
NP
2
N
plans
Figure 1: CCG derivation trees for three clauses containing the same predicate-argument relations.
w
h
c
h
i w
a
denied (S[dcl]\NP
1
)/NP
2
1 London
denied (S[dcl]\NP
1
)/NP
2
2 plans
on ((S\NP
1
)\(S\NP)
2
)/NP
3
2 denied
on ((S\NP
1
)\(S\NP)
2
)/NP
3
3 Monday
Table 1: CCG predicate-argument relations for the
sentence ?London denied plans on Monday?
The CCG parser has been trained and tested on
CCGbank (Hockenmaier and Steedman, 2002a), a
treebank of CCG derivations obtained from the Penn
Treebank, from which we also obtain our training
data.
4 Mapping between PropBank and
CCGbank
Our aim is to use CCG derivations as input to a sys-
tem for automatically producing the argument labels
of PropBank. In order to do this, we wish to cor-
relate the CCG relations above with PropBank ar-
guments. PropBank argument labels are assigned
to nodes in the syntactic trees from the Penn Tree-
bank. While the CCGbank is derived from the Penn
Treebank, in many cases the constituent structures
do not correspond. That is, there may be no con-
stituent in the CCG derivation corresponding to the
same sequence of words as a particular constituent
in the Treebank tree. For this reason, we compute
the correspondence between the CCG derivation and
the PropBank labels at the level of head words. For
each role label for a verb?s argument in PropBank,
we first find the head word for its constituent accord-
ing to the the head rules of (Collins, 1999). We then
look for the label of the CCG relation between this
head word and the verb itself.
5 The Experiments
In previous work using the PropBank corpus,
Gildea and Palmer (2002) developed a system to
predict semantic roles from sentences and their
parse trees as determined by the statistical parser of
Collins (1999). We will briefly review their proba-
bility model before adapting the system to incorpo-
rate features from the CCG derivations.
5.1 The model of Gildea and Palmer (2002)
For the Treebank-based system, we use the proba-
bility model of Gildea and Palmer (2002). Proba-
bilities of a parse constituent belonging to a given
semantic role are calculated from the following fea-
tures:
The phrase type feature indicates the syntactic
type of the phrase expressing the semantic roles: ex-
amples include noun phrase (NP), verb phrase (VP),
and clause (S).
The parse tree path feature is designed to capture
the syntactic relation of a constituent to the pred-
icate. It is defined as the path from the predicate
through the parse tree to the constituent in question,
represented as a string of parse tree nonterminals
linked by symbols indicating upward or downward
movement through the tree, as shown in Figure 2.
Although the path is composed as a string of sym-
bols, our systems will treat the string as an atomic
value. The path includes, as the first element of the
string, the part of speech of the predicate, and, as the
last element, the phrase type or syntactic category of
the sentence constituent marked as an argument.
S
NP VP
NP
He ate some pancakes
PRP
DT NN
VB
Figure 2: In this example, the path from the predi-
cate ate to the argument NP He can be represented as
VB?VP?S?NP, with ? indicating upward movement
in the parse tree and ? downward movement.
The position feature simply indicates whether the
constituent to be labeled occurs before or after the
predicate. This feature is highly correlated with
grammatical function, since subjects will generally
appear before a verb, and objects after. This feature
may overcome the shortcomings of reading gram-
matical function from the parse tree, as well as errors
in the parser output.
The voice feature distinguishes between active
and passive verbs, and is important in predicting se-
mantic roles because direct objects of active verbs
correspond to subjects of passive verbs. An instance
of a verb was considered passive if it is tagged as
a past participle (e.g. taken), unless it occurs as a
descendent verb phrase headed by any form of have
(e.g. has taken) without an intervening verb phrase
headed by any form of be (e.g. has been taken).
The head word is a lexical feature, and provides
information about the semantic type of the role filler.
Head words of nodes in the parse tree are determined
using the same deterministic set of head word rules
used by Collins (1999).
The system attempts to predict argument roles
in new data, looking for the highest probabil-
ity assignment of roles r
i
to all constituents i
in the sentence, given the set of features F
i
=
{pt
i
, path
i
, pos
i
, v
i
, h
i
} at each constituent in the
parse tree, and the predicate p:
argmax
r
1..n
P (r
1..n
|F
1..n
, p)
We break the probability estimation into two
parts, the first being the probability P (r
i
|F
i
, p) of
a constituent?s role given our five features for the
consituent, and the predicate p. Due to the sparsity
of the data, it is not possible to estimate this proba-
bility from the counts in the training data. Instead,
probabilities are estimated from various subsets of
the features, and interpolated as a linear combina-
tion of the resulting distributions. The interpolation
is performed over the most specific distributions for
which data are available, which can be thought of as
choosing the topmost distributions available from a
backoff lattice, shown in Figure 3.
P(r | h)
P(r | h, pt, p)
P(r | pt, p)
P(r | p)
P(r | pt, path, p)
P(r | h, p)
P(r | pt, pos, v, p)
P(r | pt, pos, v)
Figure 3: Backoff lattice with more specific distri-
butions towards the top.
The probabilities P (r
i
|F
i
, p) are combined with
the probabilities P ({r
1..n
}|p) for a set of roles ap-
pearing in a sentence given a predicate, using the
following formula:
P (r
1..n
|F
1..n
, p) ? P ({r
1..n
}|p)
?
i
P (r
i
|F
i
, p)
P (r
i
|p)
This approach, described in more detail in
Gildea and Jurafsky (2002), allows interaction be-
tween the role assignments for individual con-
stituents while making certain independence as-
sumptions necessary for efficient probability estima-
tion. In particular, we assume that sets of roles ap-
pear independent of their linear order, and that the
features F of a constituents are independent of other
constituents? features given the constituent?s role.
5.2 The model for CCG derivations
In the CCG version, we replace the features above
with corresponding features based on both the sen-
tence?s CCG derivation tree (shown in Figure 1)
and the CCG predicate-argument relations extracted
from it (shown in Table 1).
The parse tree path feature, designed to capture
grammatical relations between constituents, is re-
placed with a feature defined as follows: If there is
a dependency in the predicate-argument structure of
the CCG derivation between two words w and w?,
the path feature from w to w? is defined as the lexical
category of the functor, the argument slot i occupied
by the argument, plus an arrow (? or?) to indicate
whether w or w? is the categorial functor. For exam-
ple, in our sentence ?London denied plans on Mon-
day?, the relation connecting the verb denied with
plans is (S[dcl]\NP)/NP.2.?, with the left arrow
indicating the lexical category included in the rela-
tion is that of the verb, while the relation connecting
denied with on is ((S\NP)\(S\NP))/NP.2.?, with
the right arrow indicating the the lexical category in-
cluded in the relation is that of the modifier.
If the CCG derivation does not define a predicate-
argument relation between the two words, we use
the parse tree path feature described above, defined
over the CCG derivation tree. In our training data,
77% of PropBank arguments corresponded directly
to a relation in the CCG predicate-argument repre-
sentation, and the path feature was used for the re-
maining 23%. Most of these mismatches arise be-
cause the CCG parser and PropBank differ in their
definition of head words. For instance, the CCG
parser always assumes that the head of a PP is
the preposition, whereas PropBank roles can be as-
signed to the entire PP (7), or only to the NP argu-
ment of the preposition (8), in which case the head
word comes from the NP:
(7) ... will be offered [PPARGM-LOC in the U.S].
(8) to offer ...[PP to [NPARG2 the public]].
In embedded clauses, CCG assumes that the head is
the complementizer, whereas in PropBank, the head
comes from the embedded sentence itself. In com-
plex verb phrases (eg. ?might not have gone?), the
CCG parser assumes that the first auxiliary (might)
is head, whereas PropBank assumes it is the main
verb (gone). Therefore, CCG assumes that not mod-
ifies might, whereas PropBank assumes it modi-
fies gone. Although the head rules of the parser
could in principle be changed to reflect more di-
rectly the dependencies in PropBank, we have not
attempted to do so yet. Further mismatches occur
because the predicate-argument structure returned
by the CCG parser only contains syntactic depen-
dencies, whereas the PropBank data also contain
some anaphoric dependencies, eg.:
(9) [ARG0 Realist ?s] negotiations to acquire
Ammann Laser Technik AG...
(10) When properly applied, [ARG0 the adhesive] is
designed to...
Such dependencies also do not correspond to a rela-
tion in the predicate-argument structure of the CCG
derivation, and cause the path feature to be used.
The phrase type feature is replaced with the lex-
ical category of the maximal projection of the Prop-
Bank argument?s head word in the CCG derivation
tree. For example, the category of plans is N, and
the category of denied is (S[dcl]\NP)/NP.
The voice feature can be read off the CCG cate-
gories, since the CCG categories of past participles
carry different features in active and passive voice
(eg. sold can be (S[pt]\NP)/NP or S[pss]\NP).
The head word of a constituent is indicated in the
derivations returned by the CCG parser.
SARG0
NP
NNP
London
VP
VBD
denied
ARG1
NP
NNS
plans
ARGM-TMP
PP
IN
on
NP
NNP
Monday
S[dcl]ARG0
NP
N
London
S[dcl]\NP
S[dcl]\NP
(S[dcl]\NP)/NP
denied
ARG1
NP
N
plans
ARGM-TMP
(S\NP)\(S\NP)
((S\NP)\(S\NP))/NP
on
NP
N
Monday
Figure 4: A sample sentence as produced by the Treebank parser (left) and by the CCG parser (right). Nodes
are annotated with PropBank roles ARG0, ARG1 and ARGM-TMP.
Treebank-based CCG-based
Features extracted from Args Precision Recall F-score Precision Recall F-score
Automatic parses core 75.9 69.6 72.6 76.1 73.5 74.8
all 72.6 61.2 66.4 71.0 63.1 66.8
Gold-standard parses core 85.5 81.7 83.5 82.4 78.6 80.4
all 78.8 69.9 74.1 76.3 67.8 71.8
Gold-standard w/o traces core 77.6 75.2 76.3
all 74.4 66.5 70.2
Table 2: Accuracy of semantic role prediction
5.3 Data
We use data from the November 2002 release of
PropBank. The dataset contains annotations for
72,109 predicate-argument structures with 190,815
individual arguments (of which 75% are core, or
numbered, arguments) and has includes examples
from 2462 lexical predicates (types). Annotations
from Sections 2 through 21 of the Treebank were
used for training; Section 23 was the test set. Both
parsers were trained on Sections 2 through 21.
6 Results
Because of the mismatch between the constituent
structures of CCG and the Treebank, we score both
systems according to how well they identify the head
words of PropBank?s arguments. Table 2 gives the
performance of the system on both PropBank?s core,
or numbered, arguments, and on all PropBank roles
including the adjunct-like ArgM roles. In order to
analyze the impact of errors in the syntactic parses,
we present results using features extracted from both
automatic parser output and the gold standard parses
in the Penn Treebank (without functional tags) and
in CCGbank. Using the gold standard parses pro-
vides an upper bound on the performance of the sys-
tem based on automatic parses. Since the Collins
parser does not provide trace information, its up-
per bound is given by the system tested on the
gold-standard Treebank representation with traces
removed. In Table 2, ?core? indicates results on
PropBank?s numbered arguments (ARG0...ARG5)
only, and ?all? includes numbered arguments as well
as the ArgM roles. Most of the numbered argu-
ments (in particular ARG0 and ARG1) correspond
to arguments that the CCG category of the verb di-
rectly subcategorizes for. The CCG-based system
outperforms the system based on the Collins parser
on these core arguments, and has comparable perfor-
mance when all PropBank labels are considered. We
believe that the superior performance of the CCG
system on this core arguments is due to its ability to
recover long-distance dependencies, whereas we at-
tribute its lower performance on non-core arguments
mainly to the mismatches between PropBank and
CCGbank.
The importance of long-range dependencies for
our task is indicated by the fact that the performance
on the Penn Treebank gold standard without traces
Treebank-based CCG-based
Scoring Precision Recall F-score Precision Recall F-score
Automatic parses Head word 72.6 61.2 66.4 71.0 63.1 66.8
Boundary 68.6 57.8 62.7 55.7 49.5 52.4
Gold-standard parses Head word 77.6 75.2 76.3 76.3 67.8 71.8
(Treebank: w/o traces) Boundary 74.4 66.5 70.2 67.5 60.0 63.5
Table 3: Comparison of scoring regimes, using automatic parser output and gold standard parses. The first
row in this table corresponds to the second row in Table 2.
is significantly lower than that on the Penn Treebank
with trace information. Long-range dependencies
are especially important for core arguments, shown
by the fact that removing trace information from the
Treebank parses results in a bigger drop for core
arguments (83.5 to 76.3 F-score) than for all roles
(74.1 to 70.2). The ability of the CCG parser to re-
cover these long-range dependencies accounts for its
higher performance, and in particular its higher re-
call, on core arguments.
The CCG gold standard performance is below
that of the Penn Treebank gold standard with traces.
We believe this performance gap to be caused by
the mismatches between the CCG analyses and the
PropBank annotations described in Section 5.2. For
the reasons described, the head words of the con-
stituents that have PropBank roles are not necessar-
ily the head words that stand in a predicate-argument
relation in CCGbank. If two words do not stand in a
predicate-argument relation, the CCG system takes
recourse to the path feature. This feature is much
sparser in CCG: since CCG categories encode sub-
categorization information, the number of categories
in CCGbank is much larger than that of Penn Tree-
bank labels. Analysis of our system?s output shows
that the system trained on the Penn Treebank gold
standard obtains 55.5% recall on those relations that
require the CCG path feature, whereas the system
using CCGbank only achieves 36.9% recall on these.
Also, in CCG, the complement-adjunct distinction
is represented in the categories for the complement
(eg. PP) or adjunct (eg. (S\NP)\(S\NP) and in
the categories for the head (eg. (S[dcl]\NP)/PP
or S[dcl]\NP). In generating the CCGbank, various
heuristics were used to make this distinction. In par-
ticular, for PPs, it depends on the ?closely-related?
(CLR) function tag, which is known to be unreli-
able. The decisions made in deriving the CCGbank
often do not match the hand-annotated complement-
adjunct distinctions in PropBank, and this inconsis-
tency is likely to make our CCGbank-based features
less predictive. A possible solution is to regenerate
the CCGbank using the Propbank annotations.
The impact of our head-word based scoring is an-
alyzed in Table 3, which compares results when only
the head word must be correctly identified (as in Ta-
ble 2) and to results when both the beginning and
end of the argument must be correctly identified in
the sentence (as in Gildea and Palmer (2002)). Even
if the head word is given the correct label, the bound-
aries of the entire argument may be different from
those given in the PropBank annotation. Since con-
stituents in CCGbank do not always match those in
PropBank, even the CCG gold standard parses ob-
tain comparatively low scores according to this met-
ric. This is exacerbated when automatic parses are
considered.
7 Conclusion
Our CCG-based system for automatically labeling
verb arguments with PropBank-style semantic roles
outperforms a system using a traditional Treebank-
based parser for core arguments, which comprise
75% of the role labels, but scores lower on adjunct-
like roles such as temporals and locatives. The CCG
parser returns predicate-argument structures that in-
clude long-range dependencies; therefore, it seems
inherently better suited for this task. However, the
performance of our CCG system is lowered by the
fact that the syntactic analyses in its training corpus
differ from those that underlie PropBank in impor-
tant ways (in particular in the notion of heads and the
complement-adjunct distinction). We would expect
a higher performance for the CCG-based system if
the analyses in CCGbank resembled more closely
those in PropBank.
Our results also indicate the importance of recov-
ering long-range dependencies, either through the
trace information in the Penn Treebank, or directly,
as in the predicate-argument structures returned by
the CCG parser. We speculate that much of the
performance improvement we show could be ob-
tained with traditional (ie. non-CCG-based) parsers
if they were designed to recover more of the infor-
mation present in the Penn Treebank, in particular
the trace co-indexation. An interesting experiment
would be the application of our role-labeling sys-
tem to the output of the trace recovery system of
Johnson (2002). Our results also have implications
for parser evaluation, as the most frequently used
constituent-based precision and recall measures do
not evaluate how well long-range dependencies can
be recovered from the output of a parser. Measures
based on dependencies, such as those of Lin (1995)
and Carroll et al (1998), are likely to be more rele-
vant to real-world applications of parsing.
Acknowledgments This work was supported by the In-
stitute for Research in Cognitive Science at the University of
Pennsylvania, the Propbank project (DoD Grant MDA904-00C-
2136), an EPSRC studentship and grant GR/M96889, and NSF
ITR grant 0205 456. We thank Mark Steedman, Martha Palmer
and Alexandra Kinyon for their comments on this work.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of COLING/ACL, pages 86?90, Montreal.
John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998.
Parser evaluation: a survey and a new proposal. In
Proceedings of the 1st International Conference on
Language Resources and Evaluation, pages 447?454,
Granada, Spain.
Michael John Collins. 1999. Head-driven Statistical
Models for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Daniel Gildea and Martha Palmer. 2002. The necessity
of syntactic parsing for predicate argument recogni-
tion. In Proceedings of the 40th Annual Conference of
the Association for Computational Linguistics (ACL-
02), Philadelphia, PA.
Julia Hockenmaier and Mark Steedman. 2002a. Acquir-
ing Compact Lexicalized Grammars from a Cleaner
Treebank. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 1974?1981, Las Palmas.
Julia Hockenmaier and Mark Steedman. 2002b. Gener-
ative models for statistical parsing with Combinatory
Categorial Grammar. In Proceedings of the 40th An-
nual Conference of the Association for Computational
Linguistics (ACL-02), Philadelphia, PA.
Julia Hockenmaier. 2003. Data and models for statis-
tical parsing with Combinatory Categorial Grammar.
Ph.D. thesis, School of Informatics, University of Ed-
inburgh.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th Annual Confer-
ence of the Association for Computational Linguistics
(ACL-02), Philadelphia, PA.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to PropBank. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2002), Las Palmas.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings of
the 19th International Joint Conference on Artificial
Intelligence (IJCAI-95), pages 1420?1425, Montreal.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge Mass.
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 293?300,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Protein folding and chart parsing
Julia Hockenmaier Aravind K. Joshi
Institute for Research in Cognitive Science
University of Pennsylvania
Philadelphia, PA 19104, USA
 	

Proceedings of the 10th Conference on Parsing Technologies, pages 36?38,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
The Impact of Deep Linguistic Processing on Parsing Technology
Timothy Baldwin
University of Melbourne
tim@csse.unimelb.edu.au
Mark Dras
Macquarie University
madras@ics.mq.edu.au
Julia Hockenmaier
University of Pennsylvania
juliahr@cis.upenn.edu
Tracy Holloway King
PARC
thking@parc.com
Gertjan van Noord
University of Groningen
vannoord@let.rug.nl
Abstract
As the organizers of the ACL 2007 Deep
Linguistic Processing workshop (Baldwin et
al., 2007), we were asked to discuss our per-
spectives on the role of current trends in
deep linguistic processing for parsing tech-
nology. We are particularly interested in
the ways in which efficient, broad coverage
parsing systems for linguistically expressive
grammars can be built and integrated into
applications which require richer syntactic
structures than shallow approaches can pro-
vide. This often requires hybrid technolo-
gies which use shallow or statistical methods
for pre- or post-processing, to extend cover-
age, or to disambiguate the output.
1 Introduction
Our talk will provide a view on the relevance of deep
linguistic processing for parsing technologies from
the perspective of the organizers of the ACL 2007
Workshop on Deep Linguistic Processing (Baldwin
et al, 2007). The workshop was conceived with the
broader aim of bringing together the different com-
putational linguistic sub-communities which model
language predominantly by way of theoretical syn-
tax, either in the form of a particular theory (e.g.
CCG, HPSG, LFG, TAG, the Prague School) or a
more general framework which draws on theoretical
and descriptive linguistics. These ?deep linguistic
processing? approaches differ from shallower meth-
ods in that they yield richer, more expressive, struc-
tural representations which capture long-distance
dependencies or the underlying predicate-argument
structure directly.
Aspects of this research have often had their own
separate fora, such as the ACL 2005 workshop on
deep lexical acquisition (Baldwin et al, 2005), as
well as the TAG+ (Kallmeyer and Becker, 2006),
Alpino (van der Beek et al, 2005), ParGram (Butt
et al, 2002) and DELPH-IN (Oepen et al, 2002)
projects and meetings. However, the fundamental
approaches to building a linguistically-founded sys-
tem and many of the techniques used to engineer
efficient systems are common across these projects
and independent of the specific grammar formal-
ism chosen. As such, we felt the need for a com-
mon meeting in which experiences could be shared
among a wider community, similar to the role played
by recent meetings on grammar engineering (Wint-
ner, 2006; Bender and King, 2007).
2 The promise of deep parsing
Deep linguistic processing has traditionally been
concerned with grammar development (for use in
both parsing and generation). However, the linguis-
tic precision and complexity of the grammars meant
that they had to be manually developed and main-
tained, and were computationally expensive to run.
In recent years, machine learning approaches
have fundamentally altered the field of natural lan-
guage processing. The availability of large, manu-
ally annotated, treebanks (which typically take years
of prior linguistic groundwork to produce) enabled
the rapid creation of robust, wide-coverage parsers.
However, the standard evaluation metrics for which
such parsers have been optimized generally ignore
36
much of the rich linguistic information in the orig-
inal treebanks. It is therefore perhaps only natural
that deep processing methods, which often require
substantial amounts of manual labor, have received
considerably less attention during this period.
But even if further work is required for deep
processing techniques to fully mature, we believe
that applications that require natural language under-
standing or inference, among others, will ultimately
need detailed syntactic representations (capturing,
e.g., bounded and unbounded long-range dependen-
cies) from which semantic interpretations can eas-
ily be built. There is already some evidence that
our current deep techniques can, in some cases, out-
perform shallow approaches. There has been work
demonstrating this in question answering, targeted
information extraction and the recent textual entail-
ment recognition task, and perhaps most notably in
machine translation: in this latter field, after a period
of little use of linguistic knowledge, deeper tech-
niques are beginning to lead to better performance,
e.g. by redefining phrases by syntactic ?treelets?
rather than contiguous word sequences, or by explic-
itly including a syntactic component in the probabil-
ity model, or by syntactic preprocessing of the data.
3 Closing the divide
In the past few years, the divide between ?deep?,
rule-based, methods and ?shallow?, statistical, ap-
proaches, has begun to close from both sides. Re-
cent advances in using the same treebanks that have
advanced shallow techniques to extract more expres-
sive grammars or to train statistical disambiguators
for them, and in developing framework-specific tree-
banks, have made it possible to obtain similar cov-
erage, robustness, and disambiguation accuracy for
parsers that use richer structural representations. As
witnessed by many of the papers in our workshop
(Baldwin et al, 2007), a large proportion of current
deep systems have statistical components to them,
e.g., as pre- or post-processing to control ambigu-
ity, as means of acquiring and extending lexical re-
sources, or even use machine learning techniques
to acquire deep grammars automatically. From the
other side of the divide, many of the purely statistical
approaches are using progressively richer linguistic
features and are taking advantage of these more ex-
pressive features to tackle problems that were tradi-
tionally thought to require deep systems, such as the
recovery of traces or semantic roles.
4 The continued need for research on deep
processing
Although statistical techniques are becoming com-
monplace even for systems built around hand-
written grammars, there is still a need for further
linguistic research and manual grammar develop-
ment. For example, supervised machine-learning
approaches rely on large amounts of manually anno-
tated data. Where such data are available, develop-
ers of deep parsers and grammars can exploit them
to determine frequency of certain constructions, to
bootstrap gold standards for their systems, and to
provide training data for the statistical components
of their systems such as parse disambiguators. But
for the majority of the world?s languages, and even
for many languages with large numbers of speakers,
such corpora are unavailable. Under these circum-
stances, manual grammar development is unavoid-
able, and recent progress has allowed the underlying
systems to become increasingly better engineered,
allowing for more rapid development of any given
grammar, as well as for overlay grammars that adapt
to particular domains and applications and for port-
ing of grammars from one language to another.
Despite recent work on (mostly dependency
grammar-based) multilingual parsing, it is still the
case that most research on statistical parsing is done
on English, a fixed word-order language where sim-
ple context-free approximations are often sufficient.
It is unclear whether our current models and al-
gorithms carry over to morphologically richer lan-
guages with more flexible word order, and it is possi-
ble that the more complex structural representations
allowed by expressive formalisms will cease to re-
main a luxury.
Further research is required on all aspects of
deep linguistic processing, including novel linguis-
tic analyses and implementations for different lan-
guages, formal comparisons of different frame-
works, efficient parse and learning algorithms, better
statistical models, innovative uses of existing data
resources, and new evaluation tools and methodolo-
gies. We were fortunate to receive so many high-
37
quality submissions on all of these topics for our
workshop.
5 Conclusion and outlook
Deep linguistic processing brings together a range of
perspectives. It covers current approaches to gram-
mar development and issues of theoretical linguis-
tic and algorithmic properties, as well as the appli-
cation of deep linguistic techniques to large-scale
applications such as question answering and dialog
systems. Having industrial-scale, efficient parsers
and generators opens up new application domains
for natural language processing, as well as inter-
esting new ways in which to approach existing ap-
plications, e.g., by combining statistical and deep
processing techniques in a triage process to pro-
cess massive data quickly and accurately at a fine
level of detail. Notably, several of the papers ad-
dressed the relationship of deep linguistic process-
ing to topical statistical approaches, in particular in
the area of parsing. There is an increasing inter-
est in deep linguistic processing, an interest which
is buoyed by the realization that new, often hybrid,
techniques combined with highly engineered parsers
and generators and state-of-the-art machines opens
the way towards practical, real-world application of
this research. We look forward to further opportu-
nities for the different computational linguistic sub-
communities who took part in this workshop, and
others, to continue to come together in the future.
References
Timothy Baldwin, Anna Korhonen, and Aline Villavicen-
cio, editors. 2005. Proceedings of the ACL-SIGLEX
Workshop on Deep Lexical Acquisition. Ann Arbor,
USA.
Timothy Baldwin, Mark Dras, Julia Hockenmaier,
Tracy Holloway King, and Gertjan van Noord, editors.
2007. Proceedings of the ACL Workshop on Deep Lin-
guistic Processing, Prague, Czech Republic.
Emily Bender and Tracy Holloway King, editors. 2007.
Grammar Engineering Across Frameworks, Stanford
University. CSLI On-line Publications. to appear.
Miriam Butt, Helge Dyvik, T. H. King, Hiroshi Masuichi,
and Christian Rohrer. 2002. The parallel grammar
project. In COLING Workshop on Grammar Engi-
neering and Evaluation, Taipei, Taiwan.
Laura Kallmeyer and Tilman Becker, editors. 2006. Pro-
ceedings of the Eighth International Workshop on Tree
Adjoining Grammar and Related Formalisms (TAG+),
Sydney, Australia.
Stephan Oepen, Dan Flickinger, J. Tsujii, and Hand
Uszkoreit, editors. 2002. Collaborative Language En-
gineering: A Case Study in Efficient Grammar-based
Processing. CSLI Publications.
Leonoor van der Beek, Gosse Bouma, Jan Daciuk, Tanja
Gaustad, Robert Malouf, Mark-Jan Nederhof, Gert-
jan van Noord, Robbert Prins, and Bego na Vil-
lada Moiro?n. 2005. Algorithms for linguistic pro-
cessing. NWO Pionier final report. Technical report,
University of Groningen.
Shuly Wintner. 2006. Large-scale grammar development
and grammar engineering. Research workshop of the
Israel Science Foundation.
38
Parsing with generative models of predicate-argument structure
Julia Hockenmaier
IRCS, University of Pennsylvania, Philadelphia, USA
and
Informatics, University of Edinburgh, Edinburgh, UK
juliahr@linc.cis.upenn.edu
Abstract
The model used by the CCG parser
of Hockenmaier and Steedman (2002b)
would fail to capture the correct bilexical
dependencies in a language with freer
word order, such as Dutch. This paper
argues that probabilistic parsers should
therefore model the dependencies in the
predicate-argument structure, as in the
model of Clark et al (2002), and defines
a generative model for CCG derivations
that captures these dependencies, includ-
ing bounded and unbounded long-range
dependencies.
1 Introduction
State-of-the-art statistical parsers for Penn
Treebank-style phrase-structure grammars (Collins,
1999), (Charniak, 2000), but also for Categorial
Grammar (Hockenmaier and Steedman, 2002b),
include models of bilexical dependencies defined
in terms of local trees. However, this paper
demonstrates that such models would be inadequate
for languages with freer word order. We use the
example of Dutch ditransitives, but our argument
equally applies to other languages such as Czech
(see Collins et al (1999)). We argue that this
problem can be avoided if instead the bilexical
dependencies in the predicate-argument structure
are captured, and propose a generative model for
these dependencies.
The focus of this paper is on models for Combina-
tory Categorial Grammar (CCG, Steedman (2000)).
Due to CCG?s transparent syntax-semantics inter-
face, the parser has direct and immediate access
to the predicate-argument structure, which includes
not only local, but also long-range dependencies
arising through coordination, extraction and con-
trol. These dependencies can be captured by our
model in a sound manner, and our experimental re-
sults for English demonstrate that their inclusion im-
proves parsing performance. However, since the
predicate-argument structure itself depends only to
a degree on the grammar formalism, it is likely
that parsers that are based on other grammar for-
malisms could equally benefit from such a model.
The conditional model used by the CCG parser of
Clark et al (2002) also captures dependencies in the
predicate-argument structure; however, their model
is inconsistent.
First, we review the dependency model proposed
by Hockenmaier and Steedman (2002b). We then
use the example of Dutch ditransitives to demon-
strate its inadequacy for languages with a freer word
order. This leads us to define a new generative model
of CCG derivations, which captures word-word de-
pendencies in the underlying predicate-argument
structure. We show how this model can capture
long-range dependencies, and deal with the pres-
ence of multiple dependencies that arise through the
presence of long-range dependencies. In our current
implementation, the probabilities of derivations are
computed during parsing, and we discuss the dif-
ficulties of integrating the model into a probabilis-
tic chart parsing regime. Since there is no CCG
treebank for other languages available, experimen-
tal results are presented for English, using CCGbank
(Hockenmaier and Steedman, 2002a), a translation
of the Penn Treebank to CCG. These results demon-
strate that this model benefits greatly from the inclu-
sion of long-range dependencies.
2 A model of surface dependencies
Hockenmaier and Steedman (2002b) define a sur-
face dependency model (henceforth: SD) HWDep
which captures word-word dependencies that are de-
fined in terms of the derivation tree itself. It as-
sumes that binary trees (with parent category P )
have one head child (with category H) and one non-
head child (with category D), and that each node
has one lexical head h=hc; wi. In the following tree,
P=S[dcl]nNP, H=(S[dcl]nNP)=NP, D=NP, h
H
=
h(S[dcl]nNP)=NP; openedi, and h
D
=hN; doorsi.
S[dcl]nNP
(S[dcl]nNP)=NP
opened
NP
its doors
The model conditions w
D
on its own lexical cate-
gory c
D
, on h
H
= hc
H
; w
H
i and on the local tree 
in which the D is generated (represented in terms of
the categories hP;H;Di):
P (w
D
jc
D
;  = hP;H;Di; h
H
= hc
H
; w
H
i)
3 Predicate-argument structure in CCG
Like Clark et al (2002), we define predicate-
argument structure for CCG in terms of the depen-
dencies that hold between words with lexical func-
tor categories and their arguments. We assume that
a lexical head is a pair hc; wi, consisting of a word
w and its lexical category c. Each constituent has
at least one lexical head (more if it is a coordinate
construction). The arguments of functor categories
are numbered from 1 to n, starting at the innermost
argument, where n is the arity of the functor, eg.
(S[dcl]nNP
1
)=NP
2
, (NPnNP
1
)=(S[dcl]=NP)
2
. De-
pendencies hold between lexical heads whose cat-
egory is a functor category and the lexical heads
of their arguments. Such dependencies can be ex-
pressed as 3-tuples hhc; wi; i; hc0 ; w0ii, where c is a
functor category with arity  i, and hc0; w0i is a lex-
ical head of the ith argument of c.
The predicate-argument structure that corre-
sponds to a derivation contains not only local,
but also long-range dependencies that are projected
from the lexicon or through some rules such as the
coordination of functor categories. For details, see
Hockenmaier (2003).
4 Word-word dependencies in Dutch
Dutch has a much freer word order than English.
The analyses given in Steedman (2000) assume that
this can be accounted for by an extended use of
composition. As indicated by the indices (which
are only included to improve readability), in the
following examples, hij is the subject (NP
3
) of
geeft, de politieman the indirect object (NP
2
), and
een bloem the direct object (NP
1
).1
Hij geeft de politieman een bloem
(He gives the policeman a flower)
S=(S=NP
3
) ((S=NP
1
)=NP
2
)=NP
3
Tn(T=NP
2
) Tn(T=NP
1
)
<B
Tn((T=NP
1
)=NP
2
)
<B

S=NP
3
>
S
Een bloem geeft hij de politieman
S=(S=NP
1
) ((S=NP
1
)=NP
2
)=NP
3
Tn(T=NP
3
) Tn(T=NP
2
)
<
(S=NP
1
)=NP
2
<
S=NP
1
>
S
De politieman geeft hij een bloem
S=(S=NP
2
) ((S=NP
1
)=NP
2
)=NP
3
Tn(T=NP
3
) Tn(T=NP
1
)
<
(S=NP
1
)=NP
2
<B

S=NP
2
>
S
A SD model estimated from a corpus containing
these three sentences would not be able to capture
the correct dependencies. Unless we assume that
the above indices are given as a feature on the NP
categories, the model could not distinguish between
the dependency relations of Hij and geeft in the
first sentence, bloem and geeft in the second sen-
tence and politieman and geeft in the third sentence.
Even with the indices, either the dependency be-
tween politieman and geeft or between bloem and
geeft in the first sentence could not be captured by a
model that assumes that each local tree has exactly
one head. Furthermore, if one of these sentences oc-
curred in the training data, all of the dependencies in
the other variants of this sentence would be unseen
to the model. However, in terms of the predicate-
argument structure, all three examples express the
same relations. The model we propose here would
therefore be able to generalize from one example to
the word-word dependencies in the other examples.
1The variables T are uninstantiated for reasons of space.
The cross-serial dependencies of Dutch are one
of the syntactic constructions that led people to
believe that more than context-free power is re-
quired for natural language analysis. Here is an
example together with the CCG derivation from
Steedman (2000):
dat ik Cecilia de paarden zag voeren
(that I Cecilia the horses saw feed)
NP
1
NP
2
NP
3
((SnNP
1
)nNP
2
)=VP VPnNP
3
>B

((SnNP
1
)nNP
2
)nNP
3
<
(SnNP
1
)nNP
2
<
SnNP
1
<
S
Again, a local dependency model would systemat-
ically model the wrong dependencies in this case,
since it would assume that all noun phrases are ar-
guments of the same verb.
However, since there is no Dutch corpus that is
annotated with CCG derivations, we restrict our at-
tention to English in the remainder of this paper.
5 A model of predicate-argument
structure
We first explain how word-word dependencies in the
predicate-argument structure can be captured in a
generative model, and then describe how these prob-
abilities are estimated in the current implementation.
5.1 Modelling local dependencies
We first define the probabilities for purely local de-
pendencies without coordination. By excluding non-
local dependencies and coordination, at most one
dependency relation holds for each word. Consider
the following sentence:
S[dcl]
NP
N
Smith
S[dcl]nNP
S[dcl]nNP
resigned
(SnNP)n(SnNP)
yesterday
This derivation expresses the following depen-
dencies:
hhS[dcl]nNP; resignedi; 1; hN; Smithii
hh(SnNP)n(SnNP); yesterdayi; 2; hS[dcl]nNP; resignedii
We assume again that heads are generated before
their modifiers or arguments, and that word-word
dependencies are expressed by conditioning modi-
fiers or arguments on heads. Therefore, the head
words of arguments (such as Smith) are generated
in the following manner:
P (w
a
jc
a
; hhc
h
;w
h
i; i; hc
a
; w
a
ii)
The head word of modifiers (such as yesterday) are
generated differently:
P (w
m
jc
m
; hhc
m
;w
m
i; i; hc
h
;w
h
i)
Like Collins (1999) and Charniak (2000), the SD
model assumes that word-word dependencies can be
defined at the maximal projection of a constituent.
However, as the Dutch examples show, the argument
slot i can only be determined if the head constituent
is fully expanded. For instance, if S[dcl] expands
to a non-head S=(S=NP) and to a head S[dcl]=NP,
it is necessary to know how the S[dcl]=NP expands
to determine which argument is filled by the non-
head, even if we already know that the lexical cate-
gory of the head word of S[dcl]=NP is a ditransitive
((S[dcl]=NP)=NP)=NP. Therefore, we assume that
the non-head child of a node is only expanded after
the head child has been fully expanded.
5.2 Modelling long-range dependencies
The predicate-argument structure that corresponds
to a derivation contains not only local, but also long-
range dependencies that are projected from the lex-
icon or through some rules such as the coordination
of functor categories. In the following derivation,
Smith is the subject of resigned and of left:
S[dcl]
NP
N
Smith
S[dcl]nNP
S[dcl]nNP
resigned
S[dcl]nNP[conj]
conj
and
S[dcl]nNP
left
In order to express both dependencies, Smith has
to be conditioned on resigned and on left:
P (w=Smithj N;hhS[dcl]nNP; resignedi; 1; hN; wii;
hhS[dcl]nNP; lefti; 1; hN; wii)
In terms of the predicate-argument structure,
resigned and left are both lexical heads of this
sentence. Since neither fills an argument slot of
the other, we assume that they are generated inde-
pendently. This is different from the SD model,
which conditions the head word of the second
and subsequent conjuncts on the head word of
the first conjunct. Similarly, in a sentence such
as Miller and Smith resigned, the current model as-
sumes that the two heads of the subject noun phrase
are conditioned on the verb, but not on each other.
Argument-cluster coordination constructions
such as give a dog a bone and a policeman a flower
are another example where the dependencies in the
predicate-argument structure cannot be expressed
at the level of the local trees that combine the
individual arguments. Instead, these dependencies
are projected down through the category of the
argument cluster:
SnNP
1
((SnNP
1
)=NP
2
)=NP
3
give
(SnNP
1
)n(((SnNP
1
)=NP
2
)=NP
3
)
Lexical categories that project long-range depen-
dencies include cases such as relative pronouns, con-
trol verbs, auxiliaries, modals and raising verbs.
This can be expressed by co-indexing their argu-
ments, eg. (NPnNP
i
)=(S[dcl]nNP
i
) for relative pro-
nouns. Here, Smith is also the subject of resign:
S[dcl]
NP
N
Smith
S[dcl]nNP
(S[dcl]nNP)=(S[b]nNP)
will
S[b]nNP
resign
Again, in order to capture this dependency, we as-
sume that the entire verb phrase is generated before
the subject.
In relative clauses, there is a dependency between
the verbs in the relative clause and the head of the
noun phrase that is modified by the relative clause:
NP
NP
N
Smith
NPnNP
(NPnNP)=(S[dcl]nNP)
who
S[dcl]nNP
resigned
Since the entire relative clause is an adjunct, it is
generated after the noun phrase Smith. Therefore,
we cannot capture the dependency between Smith
and resigned by conditioning Smith on resigned. In-
stead, resigned needs to be conditioned on the fact
that its subject is Smith. This is similar to the way
in which head words of adjuncts such as yesterday
are generated. In addition to this dependency, we
also assume that there is a dependency between who
and resigned. It follows that if we want to capture
unbounded long-range dependencies such as object
extraction, words cannot be generated at the max-
imal projection of constituents anymore. Consider
the following examples:
NP
NP
The woman
NPnNP
(NPnNP)=(S[dcl]=NP)
that
S[dcl]=NP
S=(SnNP)
NP
I
(S[dcl]nNP)=NP
saw
NP
NP
The woman
NPnNP
(NPnNP)=(S[dcl]=NP)
that
S[dcl]=NP
S=(SnNP)
NP
I
(S[dcl]nNP)=NP
(S[dcl]nNP)=NP
saw
NP=NP
NP=PP
a picture
PP=NP
of
In both cases, there is a S[dcl]=NP with lexical head
(S[dcl]nNP)=NP; however, in the second case, the
NP argument is not the object of the transitive verb.
This problem can be solved by generating
words at the leaf nodes instead of at the maxi-
mal projection of constituents. After expanding
the (S[dcl]nNP)=NP node to (S[dcl]nNP)=NP and
NP=NP, the NP that is co-indexed with woman can-
not be unified with the object of saw anymore.
These examples have shown that two changes to
the generative process are necessary if word-word
dependencies in the predicate-argument structure
are to be captured. First, head constituents have to
be fully expanded before non-head constituents are
generated. Second, words have to be generated at
the leaves of the tree, not at the maximal projection
of constituents.
5.3 The word probabilities
Not all words have functor categories or fill argu-
ment slots of other functors. For instance, punctu-
ation marks, conjunctions, and the heads of entire
sentences are not conditioned on any other words.
Therefore, they are only conditioned on their lexical
categories. Therefore, this model contains the fol-
lowing three kinds of word probabilities:
1. Argument probabilities:
P (wjc;hhc
0
; w
0
i; i; hc; wii)
The probability of generating word w, given
that its lexical category is c and that hc; wi is
head of the ith argument of hc0; w0i.
2. Functor probabilities:
P (wjc;hhc; wi; i; hc
0
; w
0
ii)
The probability of generating word w, given
that its lexical category is c and that hc0; w0i is
head of the ith argument of hc; wi.
3. Other word probabilities: P (wjc)
If a word does not fill any dependency relation,
it is only conditioned on its lexical category.
5.4 The structural probabilities
Like the SD model, we assume an underlying pro-
cess which generates CCG derivation trees starting
from the root node. Each node in a derivation tree
has a category, a list of lexical heads and a (possi-
bly empty) list of dependency relations to be filled
by its lexical heads. As discussed in the previous
section, head words cannot in general be generated
at the maximal projection if unbounded long-range
dependencies are to be captured. This is not the case
for lexical categories. We therefore assume that a
node?s lexical head category is generated at its max-
imal projection, whereas head words are generated
at the leaf nodes. Since lexical categories are gen-
erated at the maximal projection, our model has the
same structural probabilities as the LexCat model of
Hockenmaier and Steedman (2002b).
5.5 Estimating word probabilities
This model generates words in three different
ways?as arguments of functors that are already
generated, as functors which have already one (or
more) arguments instantiated, or independent of the
surrounding context. The last case is simple, as this
probability can be estimated directly, by counting
the number of times c is the lexical category of w in
the training corpus, and dividing this by the number
of times c occurs as a lexical category in the training
corpus:
^
P (wjc) =
C(w; c)
C(c)
In order to estimate the probability of an argument
w, we count the number of times it occurs with lex-
ical category c and is the ith argument of the lexical
functor hc0; w0i in question, divided by the number
of times the ith argument of hc0; w0i is instantiated
with a constituent whose lexical head category is c:
^
P (wjc; hhc
0
; w
0
i; i; hc; wii) =
C(hhc
0
; w
0
i; i; hc; wii)
P
w
00
C(hhc
0
; w
0
i; i; hc; w
00
ii)
The probability of a functor w, given that its ith ar-
gument is instantiated by a constituent whose lexical
head is hc0; w0i can be estimated in a similar manner:
^
P (wjc; hhc; wi; i; hc
0
; w
0
ii) =
C(hhc; wi; i; hc
0
; w
0
ii)
P
w
00
C(hhc; w
00
i; i; hc
0
; w
0
ii)
Here we count the number of times the ith argu-
ment of hc; wi is instantiated with hc0; w0i, and di-
vide this by the number of times that hc0; w0i is the
ith argument of any lexical head with category c.
For instance, in order to compute the probability
of yesterday modifying resigned as in the previous
section, we count the number of times the transitive
verb resigned was modified by the adverb yesterday
and divide this by the number of times resigned was
modified by any adverb of the same category.
We have seen that functor probabilities are not
only necessary for adjuncts, but also for certain
types of long-range dependencies such as the rela-
tion between the noun modified by a relative clause
and the verb in the relative clause. In the case of zero
or reduced relative clauses, some of these dependen-
cies are also captured by the SD model. However, in
that model, only counts from the same type of con-
struction could be used, whereas in our model, the
functor probability for a verb in a zero or reduced
relative clause can be estimated from all occurrences
of the head noun. In particular, all instances of the
noun and verb occurring together in the training data
(with the same predicate-argument relation between
them, but not necessarily with the same surface con-
figuration) are taken into account by the new model.
To obtain the model probabilities, the relative fre-
quency estimates of the functor and argument prob-
abilities are both interpolated with the word proba-
bilities ^P (wjc).
5.6 Conditioning events on multiple heads
In the presence of long-range dependencies and co-
ordination, the new model requires the conditioning
of certain events on multiple heads. Since it is un-
likely that such probabilities can be estimated di-
rectly from data, they have to be approximated in
some manner.
If we assume that all dependencies dep
i
that hold
for a word are equally likely, we can approximate
P (wjc; dep
1
; :::; dep
n
) as the average of the individ-
ual dependency probabilities:
P (wjc; dep
1
; :::; dep
n
) 
1
n
n
X
i=1
P (wjc; dep
i
)
This approximation is has the advantage that it is
easy to compute, but might not give a good estimate,
since it averages over all individual distributions.
6 Dynamic programming and beam search
This section describes how this model is integrated
into a CKY chart parser. Dynamic programming and
effective beam search strategies are essential to guar-
antee efficient parsing in the face of the high ambi-
guity of wide-coverage grammars. Both use the in-
side probability of constituents. In lexicalized mod-
els where each constituent has exactly one lexical
head, and where this lexical head can only depend
on the lexical head of one other constituent, the in-
side probability of a constituent is the probability
that a node with the label and lexical head of this
constituent expands to the tree below this node. The
probability of generating a node with this label and
lexical head is given by the outside probability of the
constituent.
In the model defined here, the lexical head of
a constituent can depend on more than one other
word. As explained in section 5.2, there are in-
stances where the categorial functor is conditioned
on its arguments ? the example given above showed
that verbs in relative clauses are conditioned on the
lexical head of the noun which is modified by the
relative clause. Therefore, the inside probability of
a constituent cannot include the probability of any
lexical head whose argument slots are not all filled.
This means that the equivalence relation defined
by the probability model needs to take into account
not only the head of the constituent itself, but also
all other lexical heads within this constituent which
have at least one unfilled argument slot. As a conse-
quence, dynamic programming becomes less effec-
tive. There is a related problem for the beam search:
in our model, the inside probabilities of constituents
within the same cell cannot be directly compared
anymore. Instead, the number of unfilled lexical
heads needs to be taken into account. If a lexical
head hc; wi is unfilled, the evaluation of the proba-
bility of w is delayed. This creates a problem for the
beam search strategy.
The fact that constituents can have more than one
lexical head causes similar problems for dynamic
programming and the beam search.
In order to be able to parse efficiently with our
model, we use the following approximations for dy-
namic programming and the beam search: Two con-
stituents with the same span and the same category
are considered equivalent if they delay the evalua-
tion of the probabilities of the same words and if
they have the same number of lexical heads, and if
the first two elements of their lists of lexical heads
are identical (the same words and lexical categories).
This is only an approximation to true equivalence,
since we do not check the entire list of lexical heads.
Furthermore, if a cell contains more than 100 con-
stituents, we iteratively narrow the beam (by halv-
ing it in size)2 until the beam search has no further
effect or the cell contains less than 100 constituents.
This is a very aggressive strategy, and it is likely to
adversely affect parsing accuracy. However, more
lenient strategies were found to require too much
space for the chart to be held in memory. A better
way of dealing with the space requirements of our
model would be to implement a packed shared parse
forest, but we leave this to future work.
7 An experiment
We use sections 02-21 of CCGbank for training, sec-
tion 00 for development, and section 23 for test-
ing. The input is POS-tagged using the tagger of
Ratnaparkhi (1996). However, since parsing with
the new model is less efficient, only sentences  40
tokens only are used to test the model. A fre-
quency cutoff of  20 was used to determine rare
words in the training data, which are replaced with
their POS-tags. Unknown words in the test data
are also replaced by their POS-tags. The models
are evaluated according to their Parseval scores and
to the recovery of dependencies in the predicate-
argument structure. Like Clark et al (2002), we
do not take the lexical category of the dependent
into account, and evaluate hhc; wi; i; h ; w0ii for la-
belled, and hh ; wi; ; h ; w0ii for unlabelled recov-
ery. Undirectional recovery (UdirP/UdirR) evalu-
ates only whether there is a dependency between w
and w0. Unlike unlabelled recovery, this does not pe-
2Beam search is as in Hockenmaier and Steedman (2002b).
nalize the parser if it mistakes a complement for an
adjunct or vice versa.
In order to determine the impact of capturing dif-
ferent kinds of long-range dependencies, four differ-
ent models were investigated: The baseline model is
like the LexCat model of (2002b), since the struc-
tural probabilities of our model are like those of
that model. Local only takes local dependencies
into account. LeftArgs only takes long-range de-
pendencies that are projected through left arguments
(nX) into account. This includes for instance long-
range dependencies projected by subjects, subject
and object control verbs, subject extraction and left-
node raising. All takes all long-range dependen-
cies into account, in particular it extends LeftArgs
by capturing also the unbounded dependencies aris-
ing through right-node-raising and object extraction.
Local, LeftArgs and All are all tested with the ag-
gressive beam strategy described above.
In all cases, the CCG derivation includes all long-
range dependencies. However, with the models that
exclude certain kinds of dependencies, it is possible
that a word is conditioned on no dependencies. In
these cases, the word is generated with P (wjc).
Table 1 gives the performance of all four mod-
els on section 23 in terms of the accuracy of lexical
categories, Parseval scores, and in terms of the re-
covery of word-word dependencies in the predicate-
argument structure. Here, results are further bro-
ken up into the recovery of local, all long-range,
bounded long-range and unbounded long-range de-
pendencies.
LexCat does not capture any word-word de-
pendencies. Its performance on the recovery of
predicate-argument structure can be improved by
3% by capturing only local word-word dependen-
cies (Local). This excludes certain kinds of depen-
dencies that were captured by the SD model. For in-
stance, the dependency between the head of a noun
phrase and the head of a reduced relative clause (the
shares bought by John) is captured by the SD model,
since shares and bought are both heads of the local
trees that are combined to form the complex noun
phrase. However, in the SD model the probability of
this dependency can only be estimated from occur-
rences of the same construction, since dependency
relations are defined in terms of local trees and not
in terms of the underlying predicate-argument struc-
LexCat Local LeftArgs All
Lex. cats: 88.2 89.9 90.1 90.1
Parseval
LP: 76.3 78.4 78.5 78.5
LR: 75.9 78.5 79.0 78.7
UP: 82.0 83.4 83.6 83.2
UR: 81.6 83.6 83.8 83.4
Predicate-argument structure (all)
LP: 77.3 80.8 81.6 81.5
LR: 78.2 80.6 81.5 81.4
UP: 86.4 88.3 88.9 88.7
UR: 87.4 88.1 88.8 88.6
UdirP: 88.0 89.7 90.2 90.0
UdirR: 89.0 89.5 90.1 90.0
Non-long-range dependencies
LP: 78.9 82.5 83.0 82.9
LR: 79.5 82.3 82.7 82.6
UP: 87.5 89.7 89.9 89.8
UR: 88.1 89.4 89.6 89.4
All long-range dependencies
LP: 60.8 62.6 67.1 66.3
LR: 64.4 63.0 68.5 68.8
UP: 75.3 74.2 78.9 78.1
UR: 80.2 74.9 80.5 80.9
Bounded long-range dependencies
LP: 63.9 64.8 69.0 69.2
LR: 65.9 64.1 70.2 70.0
UP: 79.8 77.1 81.4 81.4
UR: 82.4 76.7 82.6 82.6
Unbounded long-range dependencies
LP: 46.0 50.4 55.6 52.4
LR: 54.7 55.8 58.7 61.2
UP: 54.1 58.2 63.8 61.1
UR: 66.5 63.7 66.8 69.9
Table 1: Evaluation (sec. 23,  40 words).
ture. By including long-range dependencies on left
arguments (such as subjects) (LeftArgs), a further
improvement of 0.7% on the recovery of predicate-
argument structure is obtained. This model captures
the dependency between shares and bought. In con-
trast to the SD model, it can use all instances of
shares as the subject of a passive verb in the train-
ing data to estimate this probability. Therefore, even
if shares and bought do not co-occur in this partic-
ular construction in the training data, the event that
is modelled by our dependency model might not be
unseen, since it could have occurred in another syn-
tactic context.
Our results indicate that in order to perform well
on long-range dependencies, they have to be in-
cluded in the model, since Local, the model that
captures only local dependencies performs worse on
long-range dependencies than LexCat, the model
that captures no word-word dependencies. How-
ever, with more than 5% difference on labelled pre-
cision and recall on long-range dependencies, the
model which captures long-range dependencies on
left arguments performs significantly better on re-
covering long-range dependencies than Local. The
greatest difference in performance between the mod-
els which do capture long-range dependencies and
the models which do not is on long-range dependen-
cies. This indicates that, at least in the kind of model
considered here, it is very important to model not
just local, but also long-range dependencies. It is not
clear why All, the model that includes all dependen-
cies, performs slightly worse than the model which
includes only long-range dependencies on subjects.
On the Wall Street Journal task, the overall per-
formance of this model is lower than that of the
SD model of Hockenmaier and Steedman (2002b).
In that model, words are generated at the maxi-
mal projection of constituents; therefore, the struc-
tural probabilities can also be conditioned on words,
which improves the scores by about 2%. It is also
very likely that the performance of the new models
is harmed by the very aggressive beam search.
8 Conclusion and future work
This paper has defined a new generative model for
CCG derivations which captures the word-word de-
pendencies in the corresponding predicate-argument
structure, including bounded and unbounded long-
range dependencies. In contrast to the conditional
model of Clark et al (2002), our model captures
these dependencies in a sound and consistent man-
ner. The experiments presented here demonstrate
that the performance of a simple baseline model
can be improved significantly if long-range depen-
dencies are also captured. In particular, our re-
sults indicate that it is important not to restrict the
model to local dependencies. Future work will ad-
dress the question whether these models can be
run with a less aggressive beam search strategy, or
whether a different parsing algorithm is more suit-
able. The problems that arise due to the overly
aggressive beam search strategy might be over-
come if we used an n-best parser with a simpler
probability model (eg. of the kind proposed by
Hockenmaier and Steedman (2002b)) and used the
new model as a re-ranker. The current implemen-
tation uses a very simple method of estimating the
probabilities of multiple dependencies, and more so-
phisticated techniques should be investigated.
We have argued that a model of the kind proposed
in this paper is essential for parsing languages with
freer word order, such as Dutch or Czech, where the
model of Hockenmaier and Steedman (2002b) (and
other models of surface dependencies) would sys-
tematically capture the wrong dependencies, even if
only local dependencies are taken into account. For
English, our experimental results demonstrate that
our model benefits greatly from modelling not only
local, but also long-range dependencies, which are
beyond the scope of surface dependency models.
Acknowledgements
I would like to thank Mark Steedman and Stephen Clark for
many helpful discussions, and gratefully acknowledge support
from an EPSRC studentship and grant GR/M96889, the School
of Informatics, and NSF ITR grant 0205 456.
References
Eugene Charniak. 2000. A Maximum-Entropy-Inspired Parser.
In Proceedings of the First Meeting of the NAACL, Seattle.
Stephen Clark, Julia Hockenmaier, and Mark Steedman.
2002. Building Deep Dependency Structures using a Wide-
Coverage CCG Parser. In Proceedings of the 40th Annual
Meeting of the ACL.
Michael Collins, Jan Hajic, Lance Ramshaw, and Christoph
Tillmann. 1999. A Statistical Parser for Czech. In Pro-
ceedings of the 37th Annual Meeting of the ACL.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania.
Julia Hockenmaier and Mark Steedman. 2002a. Acquiring
Compact Lexicalized Grammars from a Cleaner Treebank.
In Proceedings of the Third LREC, pages 1974?1981, Las
Palmas, May.
Julia Hockenmaier and Mark Steedman. 2002b. Generative
Models for Statistical Parsing with Combinatory Categorial
Grammar. In Proceedings of the 40th Annual Meeting of the
ACL.
Julia Hockenmaier. 2003. Data and Models for Statistical
Parsing with CCG. Ph.D. thesis, School of Informatics, Uni-
versity of Edinburgh.
Adwait Ratnaparkhi. 1996. A Maximum Entropy Part-Of-
Speech Tagger. In Proceedings of the EMNLP Conference,
pages 133?142, Philadelphia, PA.
Mark Steedman. 2000. The Syntactic Process. The MIT Press,
Cambridge Mass.
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 465?473,
Beijing, August 2010
Normal-form parsing for Combinatory Categorial Grammars
with generalized composition and type-raising
Julia Hockenmaier Yonatan Bisk
University of Illinois at Urbana-Champaign
{juliahmr, bisk1}@illinois.edu
Abstract
We propose and implement a modifica-
tion of the Eisner (1996) normal form to
account for generalized composition of
bounded degree, and an extension to deal
with grammatical type-raising.
1 Introduction
Combinatory Categorial Grammar (Steedman,
2000) is a linguistically expressive grammar for-
malism that has been used for many NLP appli-
cations, including wide-coverage parsing (Clark
and Curran, 2007; Hockenmaier, 2003) and se-
mantic interpretation (Curran et al, 2007), se-
mantic role-labeling (Gildea and Hockenmaier,
2003; Boxwell et al, 2009), semantic parsing
(Zettlemoyer and Collins, 2005) and natural lan-
guage generation (Espinosa et al, 2008).
An essential feature of CCG is its flexible
constituent structure, licensed by type-raising
and composition rules which can create ?non-
standard? constituents such as ?John saw?, or
?Mary talked to?, required in constructions in-
volving non-local dependencies, such as wh-
extraction (Fig. 1) or right-node raising. Since
?John saw? can now also be a constituent in
?John saw Mary?, this leads to a combinato-
rial explosion of spurious ambiguities, i.e. mul-
tiple syntactic derivations of the same seman-
tic interpretation (Wittenburg, 1986). This can
create problems for applications based on CCG,
e.g. for the induction of stochastic CCGs from
text annotated with logical forms (Zettlemoyer
and Collins, 2007), where spreading probabil-
ity mass over equivalent derivations should be
avoided. A number of normal-form (NF) parsing
algorithms that aim to produce only one deriva-
tion per interpretation have been proposed (Wit-
tenburg, 1986; Niv, 1994; Pareschi and Steed-
man, 1987; Hepple and Morrill, 1989; Eis-
ner, 1996). Computationally, such algorithms
are very attractive since they do not require
costly semantic equivalence checks (Karttunen,
1989; Komagata, 2004) during parsing. Eis-
ner?s (1996) normal form is the most devel-
oped and well-known of these approaches, but
is only defined for a variant of CCG where
type-raising is a lexical operation and where the
degree of composition is unbounded. There-
fore, it and its equivalent reformulation by Hoyt
and Baldridge (2008) in a multimodal variant of
CCG are not safe (preserve all interpretations)
and complete (remove all spurious ambiguities)
for more commonly used variants of CCG. In
particular, this NF is not safe when the degree
of composition is bounded,1 and not complete
when type-raising is a grammatical operation.
This paper defines a NF for CCG with bounded
composition and grammatical type-raising.
2 Combinatory Categorial Grammar
In CCG, every constituent (?John saw?) has a
syntactic category (S/NP) and a semantic in-
terpretation (?x.saw(john?, x)).2 Constituents
combine according to a small set of language-
1Although Eisner (1996, section 5) also provides a safe
and complete parsing algorithm which can return non-NF
derivations when necessary to preseve an interpretation if
composition is bounded or the grammar is restricted in
other (arbitrary) ways.
2More complex representations than simple predicate-
argument structures are equally possible.
the man that John saw
NP (NP\NP)/(S/NP) NP (S\NP)/NP
>T
S/(S\NP)
>B1
S/NP
>B0
NP\NP
<B0NP
Figure 1: CCG derivations for wh-extraction
465
Application (>) X/Y : ?x.f(x) Y : a ? X : f(a)
(<) Y : a X\Y : ?x.f(x) ? X : f(a)
Composition (>B1) X/Y : ?x.f(x) Y/Z : ?y.g(y) ? X/Y : ?z.f(g(z))
(<B1) Y\Z : ?y.g(y) X\Y : ?x.f(x) ? X\Y : ?z.f(g(z))
(>B 1?) X/Y : ?x.f(x) Y\Z : ?y.g(y) ? X\Y : ?z.f(g(z))
(<B 1?) Y/Z : ?y.g(y) X\Y : ?x.f(x) ? X/Y : ?z.f(g(z))
(>Bn) X/Y : ?x.f(x) Y|Z1|...|Zn : ?zn..z1.g(z1...zn) ? X|Z1|...|Zn : ?zn...z1.f(g(z1...zn))
(<Bn) Y|Z1|...|Zn : ?zn..z1.g(z1...zn) X\Y : ?x.f(x) ? X|Z1|...|Zn : ?zn...z1.f(g(z1...zn))
Typeraising (>T ) For X ? Carg : X : a ? T/i(T\iX) : ?f.f(a)
(<T ) For X ? Carg : X : a ? T\i(T/iX) : ?f.f(a)
Figure 2: CCG?s combinatory rules.
independent combinatory rules (Fig. 2). The lex-
icon pairs words with categories and interpreta-
tions and is language-specific.
Syntax We distinguish atomic (S, NP, PP,
etc.) from complex categories ((S\NP)/NP,
N/N, etc.). A complex category of the form X/Y
(or X\Y) represents a function which returns a
result of type X when applied to an argument
of type Y, which, in the case of a forward slash
(/) has to follow the functor, and in the case of
a backslash (\) has to preceed it. X and Y can
themselves be complex again. We will use cat-
egories with vertical slashes when the direction
of the slash does not matter, and may omit un-
necessary parentheses (so X|Y|Z will represent
(X\Y)/Z, (X\Y)\Z, ...). We will also use the
shorthand X|Y1..n (or X|?) to refer to a category
with (possibly complex) result X and arguments
Y1...Yn (or an unspecified, possibly empty, list
of arguments ? = Y0...n, where |?| = n) that
can each appear with either type of slash.
Semantics If the category of a constituent is
atomic (NP; S), its interpretation will also be
atomic (kim?; sleeps?(kim?)), and if the category
is a functor of arity n (X|Y1..n), the interpretation
is a ?-expression ?yn..?y1?(y1...yn) of arity n.
The lexicon Each language defines a finite set
of lexical category types Clex (e.g. (S\NP)/NP
is in the English lexicon, but (S\NP)\NP is not)
with maximal arity NL. This defines a set of
lexical argument category types Carg , consist-
ing of all categories Y that are the argument
of some lexical category (X|Y)|? ? Clex (with
|?| ? 0). Since Clex is finite, Carg is strictly
smaller than Clex (and usually consists of basic
categories such as NP, S, S\NP).
Combinatory Rules In addition to function
application (>,<), CCG has three kinds of com-
binatory rules (Fig. 2): harmonic function com-
position (>B(1), <B(1)), crossing function com-
position (>B ?,<B ?) and type-raising (>T ,
<T ). All rules take one or two input categories
and yield one output category, and consist of a
syntactic and a corresponding semantic opera-
tion. Composition also has generalized variants
>Bn, <Bn up to a fixed degree NB .3 Compo-
sition of unbounded degree increases the genera-
tive capacity of CCG (Weir, 1988), and should be
disallowed. Application (>,<) can be seen as a
special case of composition (>B0,<B0). When
composing X|Y with Y|Z to X|Z, we call X|Y
the primary input and Y|Z the secondary in-
put. Harmonic composition allows associativ-
ity: the string A/B B/C C now has an alter-
native derivation where A/B and B/C compose
into A/C, whereas crossing composition enables
novel permutations, such as C A/B B\C.
Type-raising swaps the functor-argument rela-
tion. Although it is often assumed to take place
in the lexicon, we will distinguish lexical cate-
gories (e.g. for quantifiers) that have the syn-
tactic type of type-raised categories, but seman-
tics that could not be obtained by type-raising a
simple category from grammatically type-raised
categories. We follow the common definition
of CCG (Steedman, 2000) and allow only cat-
egories X ? Carg to be type-raised.4 Instantia-
3In X|Y1..n or X|?=X|Y1...|?|, we do not assume the
slash variable | ? {/, \} to be instantiated the same way for
all Yi. We will therefore only distinguish between forward
and backward generalized composition Bn>1.
4We stipulate that it may be further necessary to only
allow those argument categories to type-raise that are not
used to project unbounded dependencies, such as S/NP in
466
tions of the variable T should also be restricted
to categories of finite arity NT in oder to pre-
vent an increase in generative capacity (Hoff-
man, 1995; Komagata, 1997). We refer to the
arity of T as the degree of any particular instan-
tation of T . We follow Steedman (2000) and
assume NT = NB .
Coordination requires a ternary rule (?) which
can be binarized (?>, ?<) to simplify parsing:5
(?) X conj X ? X
(?>) X X[conj] ? X
(?<) conj X ? X[conj]
Uses of type-raising and composition In En-
glish, type-raising and composition are required
for wh-extraction and right node raising of argu-
ments as well as so-called argument cluster co-
ordination. In other languages, they are needed
for scrambling and cross-serial dependencies.
It is important to note that when type-raising is
required, it always occurs in tandem with com-
position. Since type-raising an argument Y to
X/(X\Y) and applying it to the functor X\Y is
semantically equivalent to applying X\Y directly
to Y, type-raising is never required when func-
tion application can be used instead. That is, in
all cases, a type-raised argument must be com-
posed with another constituent, usually the orig-
inal functor (head). Only in argument-cluster co-
ordination will the type-raised element be com-
posed with a non-head constituent. In the lat-
ter case, coordination will be required before
the argument cluster can be combined with the
head. Composition without type-raising may oc-
cur, e.g. for adjuncts, which have categories X|X,
but may modify a constituent with category X|?.
Restrictions on type-raising and composition
In order to prevent overgenerations of the form
?John speaks because Chinese, he enjoys Bei-
jing.?, we assume a variant of CCG in which
forward crossing composition >B 1? (e.g. of be-
cause:(S/S)/S) into the result of backward type-
raising <T (e.g. Chinese:S\(S/NP), and, simi-
larly, <Bx into the result of >T , are disallowed.
(NP\NP)/(S/NP) for English object relative pronouns.
5Here, X needs to be restricted to a finite set of cate-
gories (Weir, 1988). In multimodal CCG, conjunction have
categories of the form (X?\?X)/?X, i.e. must apply to their
argument
Punctuation and Type-changing rules CCG-
bank (Hockenmaier and Steedman, 2007) uses
special punctuation rules such as S . ? S or
, NP\NP ? NP\NP, and a small number of
(non-recursive) type-changing rules (with id-
iosyncratic semantics) such as N ? NP (for
determiner-less NPs) or S[pss]\NP ? NP\NP
(for complex adjuncts, here passive VPs being
used as NP postmodifiers):
Punctuation (>P) X:? [., ; ] ? X:?
(<P) [., ; ] X:? ? X:?
TypeChanging (TCR) X:? ? Y:?(?)
CCG parsing CCG can be parsed with a
bottom-up CKY-like algorithm (Shieber et al,
1995; Steedman, 2000), which differs from stan-
dard CKY in that it requires one (or two) unary
completion steps in each cell to deal with type-
raising (and type changing).6 Chart items are of
the form ?X, i, j?, where X is a category, and the
indices i and j represent the span of the item.
Interpretations need only to be constructed for
complete derivations when unpacking the chart.
3 The Eisner normal form
The Eisner normal form Eisner (1996)
presents a normal-form parsing algorithm for
CCG without grammatical type raising (where
the lexicon may still contain categories like
S/(S\NP), but there is no combinatory rule
that changes a complex (derived) NP to e.g.
S/(S\NP)). He proves that his algorithm finds
only one canonical derivation for each semantic
interpretation of an input string consisting of a
sequence of words and their lexical categories.
Since the presence of both pre- and postmodi-
fiers (as in ?intentionally knock twice?7) intro-
duces a genuine ambiguity, Eisner proves that
the only kind of spurious ambiguity that can
arise in his variant of CCG is due to associative
chains of composition such as A/B B/C C/D or
A/B B/C C\D, which can be derived as either
6Since composition allows the arity of derived (? non-
terminal) CCG categories to grow with the length of the
input string, worst-case complexity of this naive algorithm
is exponential. (Vijay-Shanker and Weir, 1993)?s O(n6)
algorithm has a more compact representation of categories.
7This can mean ?x.intentionally ?(twice ?(knock ?(x)))
or ?x.twice ?(intentionally ?(knock ?(x))).
467
Eisner NF Not Eisner NF
(A|B1..b)/C (C|D1..d)/E (E|F1..f)/G G|H1..h
>Bh
(E|F1..f)|H1..h
>Bf+h
((C|D1..d)|F1..f)|H1..h
>Bd+f+h
(((A|B1..b)|D1..d)|F1..f)|H1..h
(A|B1..b)/C (C|D1..d)/E (E|F1..f)/G G|H1..h
>Bd+1
((A|B1..b)|D1..d)/E
>Bf+1
(((A|B1..b)|D1..d)|F1..f)|G
>Bh
(((A|B1..b)|D1..d)|F1..f)|H1..h
Figure 3: Eisner NF and generalized composition Bn>1
Left branching Right branching
>B0(>Bm+1,...)?>Bm?0(...,>B0) A/B (B|D0..m)/C C m ? 0
>B1(>Bm?1,...)?>Bm?1(...,>B1) A/B (B|C1...m?1)/D D/E m ? 1
>Bn?1(>B1,...) ?>Bn(...,>Bm=n) A/B B/C C/D1..n m = n ? 1
? :>Bn>1(...,>Bm>n) A/(B|D1..k) B/C ((C|D1..k)|E1..n m > n > 1
>Bm(>Bk,...) ?>Bn>1(...,>B1<m<n) A/B (B|C1..k?1)/D D|E1..m n > m > 1
Figure 4: Associative composition chains: our NF disallows the grayed-out derivations.
>B (..., >B ) or >B (>B , ). This is eliminated
by the following constraint:
Eisner NF Constraint 1. The output X|? of
forward composition >Bn>0 cannot be the pri-
mary input to forward application or composi-
tion >Bm?0. The output of <Bn>0 cannot be
the primary input to <Bm?0.
This can be implemented by a ternary feature
HE ? {>Bn, <Bn, ?} and chart items of the
form ?X, HE, i, j? where HE =>Bn (or <Bn)
if X was produced by the corresponding compo-
sition rule (for any n > 0) and ? otherwise.
4 A new normal form for CCG
4.1 Generalized composition
Eisner NF and generalized composition Un-
boundedly long sequences of generalized com-
position are required e.g. for Dutch verb clus-
ters that give rise to cross-serial dependen-
cies (N1...NnV1...Vn with Ni the argument of
Vi). These can be obtained through standard
bounded-degree compositions, but the Eisner NF
produces a derivation that requires compositions
of unbounded degree (Fig. 3). Although this is
allowed in the variant of CCG Eisner considers,
compositions of unbounded degree are usually
disallowed because they increase the generative
capacity of CCG (Weir, 1988). We stipulate that
the NF of any derivation ? should not require
composition rules of higher degree than ? itself.
Note that the output of function application (B0)
always has lower arity than its functor; the output
of regular composition (B1) has the same arity as
its primary functor, but the output of generalized
composition (Bn>1) has an arity that is n ? 1
higher than that of the primary functor. Bn>1
therefore requires a different treatment.
Our reformulation of the Eisner NF As-
sociative composition chains for constituents
A B C can lead to spurious ambiguity if both a
left-branching >Bn(>Bm(A B) C) and a right-
branching >Bn?(A >Bm?(B C)) are possible and
lead to the same interpretation. Figure 4 il-
lustrates all possible cases consisting of three
constituents. In most cases, the right-branching
(Eisner NF) derivation is to be preferred. For
generalized composition >Bn>1, >Bm>1, left-
branching >Bn>1(>Bm>1, ...) is always al-
lowed, but right-branching >Bn(..., >Bm) is
only allowed if m ? n.
NF Constraint 1 (B0 and Bn?1). The output of
>Bn?1 (resp. <Bn?1) cannot be primary func-
tor for >Bn?1 (resp. <Bn?1).
NF Constraint 2 (B1 and Bn?1). The output of
>B1 (resp. <B1) cannot be primary functor for
>Bn?1 (resp. <Bn?1).
NF Constraint 3 (Bn>1 and Bm>1). The out-
put of >Bm (resp. <Bm) cannot be secondary
functor for >Bn>m (resp. <Bn>m).
4.2 Grammatical type-raising
Eisner NF and type-raising Figure 5 illus-
trates a spurious ambiguity arising through type-
468
which Sue ate happily
NP : (S\NP)/NP : S\S :
s? ?y.?x.ate?(x, y) ?z.happily?(z)
>T
S/(S\NP) :
?f.f(s?)
>B1
S/NP : ?y.ate?(s?, y)
<B1?
S/NP : ?y.happily?(ate?(s?, y))
which Sue ate happily
NP : (S\NP)/NP : S\S :
s? ?y.?x.ate?(x, y) ?z.happily?(z)
>T
S/(S\NP) :
?f.f(s?)
<B2?
(S\NP)/NP :
?y.?x.happily?(ate?(x, y))
>B1
S/NP : ?y.happily?(ate?(s?, y))
Figure 5: The Eisner NF allows spurious ambiguity arising due to type-raising
raising that the Eisner NF does not exclude.8
Here two derivations can be obtained because
the result of combining the adverb with the
subject-verb cluster is no longer the output of
a forward composition, and can therefore ap-
ply to the object. The derivations are semanti-
cally equivalent: although type-raising reverses
the syntactic functor-argument relation, a type-
raised argument applied to a predicate returns
the same interpretation as when the predicate
is applied directly to the original. But Eis-
ner treats S/(S\NP) as a category with se-
mantics ?x.?(x), in which case the derivations
yield indeed different scope relations. Eis-
ner?s analyis is correct for certain classes of
words which have lexical categories that ap-
pear like type-raised categories, but have a dif-
ferent interpretation from that of categories ob-
tained by type-raising. These are usually scope-
bearing elements, such as the universal quantifer
every ((S/(S\NP))/N : ?P?Q?xP(x) ? Q(x)),
and there may not be a single derivation which
captures all semantic interpretations. Lexical-
ized pseudo-type-raising therefore needs to be
distinguished from grammatical type-raising.
Our extension of the (modified) Eisner NF
In Fig. 5, Eisner NF licenses two derivations.
Both contain an instance of composition in
which the type-raised argument is the primary
component. In the analysis in which this is the
second derivation step, the canceled part of this
<B2 composition (boxed) contains a category
(\NP) that was part of the argument output of
the first >B1 composition (bold-faced):
8We have chosen a slighly unusual adverb category to
illustrate a general problem.
which Sue ate happily
S/ (S\NP) (S\NP)/NP S\S
<B2?
S\NP /NP
>B1
S/NP
Our NF will eliminate derivations of this type
and prefer the other, lower-degree derivation.
We stipulate that the spurious ambiguities that
arise through type-raising and composition can
be eliminated through the following rule:
NF Constraint 4 (T and Bn>0). The output of
>T cannot be primary input to >Bn>0 if the
secondary input is the output of <Bm>n. The
output of <T cannot be primary input in <Bn>0
if the secondary input is the output of >Bm>n.
We also stipulate that a type-raised T/(T\X)
cannot be used as a functor in application (since
T\X could always apply directly to X).
NF Constraint 5 (T and B0). The output of for-
ward (or backward) type-raising >T (resp. <T )
cannot be the functor in application > (resp. <).
Additional spurious ambiguities arise through
the interaction of type-raising and coordination:
Since any category can be coordinated, we can
either coordinate X and then type-raise the co-
ordinated X to T/(T\X), or we can first type-
raise each conjunct to T/(T\X) and then con-
join. Since nonsymmetric coordinations of an
argument-adjunct cluster and a single argument
(as in eats ((pizza for lunch) and pasta)) require
type-raising before coordination, we formulate
the following rule to eliminate interactions be-
tween type-raising and coordination:
NF Constraint 6 (T and ?). The result of coor-
dination ? cannot be type-raised.
469
NF Derivation A NF Derivation B
A B C
X/X : (X|?a)|?b : (X|?a)\(X|?a) :
?Pa(P ) ?xbxab(xaxb) ?Q?zac(Q(za))
<Bb
(X|?a)|?b : ?xbxac(b(xaxb))
>Ba+b?
(X|?a)|?b : ?xbxaa(c(b(xaxb)))
A B C
X/X : (X|?a)|?b : (X|?a)\(X|?a) :
?Pa(P ) ?xbxab(xaxb) ?Q?zac(Q(za))
>Ba+b?
(X|?a)|?b : ?xbxaa(b(xaxb))
<Bb?
(X|?a)|?b : ?xbxac(a(b(xaxb)))
Figure 6: Constituents with pre- and postmodifiers have two semantically distinct derivations
Punctuation and Type-changing rules Punc-
tuation results in spurious ambiguities, either
when a constituent X has both an initial and a fi-
nal punctuation mark (e.g. a comma), or when it
has an initial (final) punctuation mark and a final
(initial) modifier. The first case is easy to fix by
disallowing the output of , X ? X to be the in-
put of X ,? X. The latter could be eliminated by
disallowing the output X of right-recursive (left-
recursive) punctuation rule to be secondary input
to any left-recursive (right-recursive) application
or composition rule (e.g. X X\X ? X).9
Implementation Our normal-form constraints
can be implemented in a bottom-up parser with
items of the form ?X, C, i, j?, with
C ? {>, >B 1, >B 2, ..., >Bn; <, <B 1, <B 2, ..., <Bn;
>T , <T , >Pct,<Pct, ?>, ?<, TCR}
4.3 Is our normal form safe and complete?
Here we sketch the beginnings of a proof that
our algorithm allows one and only one syntac-
tic derivation per semantic interpretation for the
version of CCG we consider. We first examine
all cases of two adjacent constituents A, B which
must combine into a category C:
Functor X/Y and argument Y combine to X
The functor must apply to the argument. The ar-
gument could type-raise, but then cannot apply.
Functor X/Y|? and argument Y combine to
X|? The functor cannot apply to the argument.
The argument must type-raise to X\(X/Y), and
can then backward-compose into the functor.
Functor X/X and X\X can combine to X/X or
X\X This is not a spurious ambiguity, since the
output categories are different.
9If punctuation can be used both with X and Y, it also
interacts with type-changing rules X ? Y. Our current
implementation does not deal with this case.
Functor X|Y and Y|Z combine to X|Z Our re-
formulation of Eisner?s NF eliminates spurious
ambiguities that are due to such associative com-
position chains. This covers not only argument
clusters (which must compose), but also ambigu-
ous cases where one constituent (e.g. Y/Z with
? = ) is the argument of the first (X/Y), and ei-
ther takes the third (Z) as its own argument or is
modified by the third Y\Y (there are, of course,
other arrangements of such categories which are
not ambiguous, e.g. X/Y Z Y\Z.
We now focus our attention on the ternary
cases in which one of the constituents is a head
(predicate), and the other two are either its argu-
ments or modifiers. The counterexample to Eis-
ner?s normal-form algorithm shows that there is
at least one additional kind of spurious ambigu-
ity that arises when there are three adjacent con-
stituents A, B, C and both A and C can compose
into B. There are three cases: 1) A and C are
both modifiers of B, 2) one of A or C is a mod-
ifier of B, the other is an argument of B, and 3)
A and C are both arguments of B. Only 1) is a
real ambiguity, but the other cases are instances
of spurious ambiguity which our NF eliminates.
Argument Y, head (X\Y)/Z and argument Z
combine to X In the NF derivation, the head
applies first to the Z, than to Y. All other deriva-
tions are blocked, either because type-raised cat-
egories cannot apply, or because the output of
composition cannot apply.
Modifier X/X, head (X|?)|? and modifier
(X|?)\(X|?) combine to (X|?)|? (Fig. 4.2).
This is the ?intentionally knock twice? example.
The derivations have different semantics.
Argument Y, head ((X|?)\Y)|?, and modifier
X\X combine to (X|?)|? (Fig. 7). If there is
an ambiguity, B must have a category of the form
470
Normal form Not normal form
A B C
Y ((X|?a)\Y)|?b : X\X
a ?xbxixab(xaxixb) ?Q?zac(Q(za))
>T
(X|?a)/((X|?a)\Y) :
?P?yaP (aya)
>Bb?
(X|?a)|?b : ?xbxab(xaaxb)
<Ba+b?
(X|?a)|?b : ?xbxac(b(xaaxb))
A B C
Y ((X|?a)\Y)|?b : X\X
a ?xbxixab(xaxixb) ?Q?zac(Q(za))
>T <Ba+b+1?
(X|?a)/((X|?a)\Y) : ((X|?a)\Y)|?b :
?P?yaP (aya) ?xbxixac(b(xaxixb))
>Bb?
(X|?a)|?b : ?xbxac(b(xaaxb))
Figure 7: Argument Y, head ((X|?a)\Y)|?b, and modifier X\X combine to (X|?a)|?b
Normal form Not normal form
A B C
Y (((X\Y)|?a)/Z)|?b Z
a ?xbxjxaxib(xixaxjxb) c
>T <T
X/(X\Y) ((X\Y)|?a)\(((X\Y)|?a)/Z)
?P?yaP (aya) ?Q?zazizaQ(czaziza)
<Bb?
((X\Y)|?a)|?b : ?xbxaxib(xixacxb)
>Ba+b?
(X|?a)|?b : ?xbxab(axacxb)
A B C
Y (((X\Y)|?a)/Z)|?b : Z
a ?xbxjxaxib(xixaxjxb) c
>T <T
X/(X\Y) (X|?a)\((X|?a)/Z)
?P?yaP (aya) ?Q?zaQ(cza)
>Ba+b+1?
((X|?a)/Z)|?b : ?xbxjxab(axaxjxb)
<Bb?
(X|?a)|?b : ?xbxab(axacxb)
Figure 8: Argument Y, head (((X\Y)|?)/Z)|? and argument Z combine to (X|?)|?
((X|?)\Yi)|? (with X possibly complex and ?, ?
possibly empty), and C must have a category of
the form X\X. We obtain the NF derivation by
first combining head and argument, followed by
the modifier. The other derivation violates the
NF constraints.
Argument Y, head (((X\Y)|?)/Z)|? and ar-
gument Z combine to (X|?)|? (Fig. 8) The
derivation in which Z composes first is in NF.
The derivation in which the Y combines first
with the head is blocked.
Arguments YA, YB, head (((X\Y1)|?)\Y2)|?
combine to (X|?)|? There are two readings:
standard (YA:=Y1, YB:=Y2), and scrambled
(YA:=Y2, YB:=Y1). If ? and ? are empty, func-
tion application is sufficient for the standard
reading, and our NF constraint 1 excludes the
?argument cluster? derivation in which both YA
and YB type-raise, compose and then apply to the
head. Otherwise, at least one of the arguments
has to type-raise and compose into the head. If
both ? and ? are non-empty, each interpretation
has only one derivation in which the type-raised
YA composes into the output of the composition
of the type-raised YB with the head. Since the
degree of the second composition is lower than
the first, this is allowed by our NF constraint 2.
Argument YA and heads (((X\Y1)|?)/Z and
((Z|?)\Y2)|? combine to (((X|?)|?)\Y2)|? or
to (((X|\Y1?)|?)|? There are two readings:
standard (YA:=Y1) or scrambled (YA:=Y2). De-
pending on the maximal degree n of Bn allowed
by the grammar, the standard reading one can ei-
ther be obtained by type-raising YA and compos-
ing into the first head (allowed by our NF) or by
first composing the two heads and then compos-
ing the type-raised YA into the cluster (allowed
by Eisner, but not by us). The second reading
requires the heads to compose and then YA to
apply or compose (depending on the arity of ?),
which is allowed by our NF constraint 2 because
the degree of this second composition is lower
than that of the first.
Our NF and the bound NT on type-raising
If X\X in Fig. 7 is replaced with a (non-type-
raised) category Z\X (for Z 
= X), the non-NF
derivation requires T|Z|+a, whereas the NF-
derivation requires T|X|+a. If we stipulate a fi-
nite bound NT on the degree of type-raising,
and if |X| > |Z| and |X| + a > NT , our
NF cannot be derived anymore. If such Z\X
(with X ? Carg ) can be derived from the lexi-
con, our NF requires therefore a potentially un-
bounded degree of type-raising. The T-degree
471
Sentence length l=15...30
15 20 25 30
No NF (total #derivs) 4.13E6 5.66E8 3.06E11 1.59E14
Eisner B 18.92% 9.05% 3.63% 2.14%
Our B 18.38% 8.97% 3.60% 2.02%
Our B , T 2.92% 1.22% 0.37% 0.10%
Our full NF 2.60% 0.93% 0.33% 0.09%
(a) Median % of allowed derivations
Sentence length l= 30
Min Mean Median Max
No NF 5.99E9 8.19E15 1.59E14 2.61E17
Eisner B 1.60% 2.68% 2.14% 2.76%
Our B 1.57% 2.49% 2.02% 2.69%
Our B ,T 0.64% 0.07% 0.10% 0.05%
Our full NF 0.53% 0.06% 0.09% 0.05%
(b) Statistics on the % of allowed derivations
Figure 9: Experimental results: the effect of different normal forms on the number of derivations
of the non-NF derivation in Fig. 8 is also one less
than that of the NF derivation, but its B-degree is
increased by one, so for NT = NB either both
derivations are possible or neither.
What remains to be proven is that we have
considered all cases of spurious ambiguity in-
volving three constituents, and that all cases of
spurious ambiguity that arise for more than three
constituents reduce to these cases.
5 The effects of normal form parsing
We now illustrate the impact of the different nor-
mal form variants on a small, restricted, gram-
mar. We define a set of atomic categories, a set of
lexical categories (up to a fixed arity NLex), and
compile out all possible rule instantiations (in-
cluding compositions up to a fixed degree N|B)
that generate categories up to a fixed arity Ncat10
The effect of different normal forms This
experiment is intended to examine how nor-
mal form parsing might reduce spurious ambi-
guity for actual grammars, e.g. for unsuper-
vised estimation of stochastic CCGs. We cre-
ated a small English grammar with atomic cat-
egories S,NP,N, conj, ., , ; and 47 lexical cate-
gories using NLex = 3, NB = 3, NCat = 15.
There are two type-changing rules (N ? NP
and S/NP ? NP\NP ). We accept deriva-
tions of S, NP and S\NP. The T|X in T has
to be a lexical category. Our lexical categories
are divided into disjoint sets of adjuncts of the
form X|X and (X|X)|Y, head (both atomic and
complex), and punctuation and conjunction cat-
egories. The comma can act as a conjunction or
to set off modifiers (requiring punctuation rules
10The restriction of categories to a fixed arity
means that we could generate cross-serial dependencies
N1...NnV1...Vn only up to n = Acat .
of the form X|X , ? X|X and , X|X ? X|X).
We furthermore define coarse-grained parts of
speech (noun, verb, function word, conj, other)
and decide for each part of speech which lexical
categories it can take. We compare different NF
settings for sentences of lengths 15?30 from Eu-
roparl (Koehn, 2005). At each length, we com-
pare 100 sentences that our grammar can parse.
All NFs can parse all sentences the full grammar
can parse. Results (Fig. 9(a)) show that our NF
reduces the number of derivations significantly
over Eisner?s NF, even though our (full) gram-
mar only allows a restricted set of type-raising
rules. Fig. 9(b) illustrates the combinatorial ex-
plosion of spurious derivations as the sentence
length increases.
6 Conclusions
We have proposed a modification and extension
of Eisner (1996)?s normal form that is more ap-
propriate for commonly used variants of CCG
with grammatical type-raising and generalized
composition of bounded degree, as well as some
non-combinatory extensions to CCG. Our exper-
iments indicate that incorporating normal form
constraints to deal with grammatical type-raising
drastically reduces the number of derivations.
We have sketched the outline of a proof that our
normal form is safe and complete for the variant
of CCG we consider, althoug we have seen that
under certain circumstances, type-raising of un-
bounded degree may be required. Future work
will investigate this issue further, and will also
aim to turn our informal arguments about the ad-
equacy of our approach into a full proof, and pro-
vide more experiments on a wider range of gram-
mars and languages.
472
References
Boxwell, Stephen, Dennis Mehay, and Chris Brew.
2009. Brutus: A semantic role labeling system in-
corporating CCG, CFG, and dependency features.
In Proceedings of the 47th ACL/4th IJCNLP, pages
37?45.
Clark, Stephen and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Curran, James, Stephen Clark, and Johan Bos.
2007. Linguistically motivated large-scale NLP
with C&C and boxer. In Proceedings of the 45th
ACL Companion Volume (Demo and Poster Ses-
sions), pages 33?36, Prague, Czech Republic.
Eisner, Jason. 1996. Efficient normal-form pars-
ing for Combinatory Categorial Grammar. In Pro-
ceedings of the 34th ACL, pages 79?86, Santa
Cruz, CA.
Espinosa, Dominic, Michael White, and Dennis
Mehay. 2008. Hypertagging: Supertagging for
surface realization with CCG. In Proceedings of
ACL-08: HLT, pages 183?191, Columbus, Ohio.
Gildea, Daniel and Julia Hockenmaier. 2003. Iden-
tifying semantic roles using Combinatory Catego-
rial Grammar. In Proceedings of EMNLP, Sap-
poro, Japan.
Hepple, Mark and Glyn Morrill. 1989. Parsing and
derivational equivalence. In Proceedings of the
Fourth EACL, pages 10?18, Manchester, UK.
Hockenmaier, Julia and Mark Steedman. 2007.
CCGbank: A corpus of CCG derivations and de-
pendency structures extracted from the penn tree-
bank. Computational Linguistics, 33(3):355?396.
Hockenmaier, Julia. 2003. Data and models for
statistical parsing with Combinatory Categorial
Grammar. Ph.D. thesis, School of Informatics,
University of Edinburgh.
Hoffman, Beryl. 1995. Computational Analysis of
the Syntax and Interpretation of ?Free? Word-order
in Turkish. Ph.D. thesis, University of Pennsylva-
nia. IRCS Report 95-17.
Hoyt, Frederick and Jason Baldridge. 2008. A log-
ical basis for the D combinator and normal form
in CCG. In Proceedings of ACL-08: HLT, pages
326?334, Columbus, Ohio.
Karttunen, Lauri. 1989. Radical lexicalism. In
Baltin, M.R. and A.S. Kroch, editors, Alternative
Conceptions of Phrase Structure. Chicago Univer-
sity Press, Chicago.
Koehn, Philipp. 2005. Europarl: A parallel cor-
pus for statistical machine translation. In 10th MT
Summit, pages 79?86, Phuket, Thailand.
Komagata, Nobo. 1997. Generative power of
CCGs with generalized type-raised categories. In
ACL35/EACL8 (Student Session), pages 513?515.
Komagata, Nobo. 2004. A solution to the spurious
ambiguity problem for practical combinatory cate-
gorial grammar parsers. Computer Speech & Lan-
guage, 18(1):91 ? 103.
Niv, Michael. 1994. A psycholinguistically moti-
vated parser for CCG. In Proceedings of The 32nd
ACL, Las Cruces, NM, pages 125?132.
Pareschi, Remo and Mark Steedman. 1987. A lazy
way to chart parse with categorial grammars. In
Proceedings of the 25th ACL, pages 81?88, Stan-
ford, CA.
Shieber, Stuart M., Yves Schabes, and Fernando
C. N. Pereira. 1995. Principles and implemen-
tation of deductive parsing. Journal of Logic Pro-
gramming, 24(1?2):3?36, July?August.
Steedman, Mark. 2000. The Syntactic Process. MIT
Press, Cambridge, MA.
Vijay-Shanker, K and David J Weir. 1993. Parsing
some constrained grammar formalisms. Compu-
tational Linguistics, 19(4):591?636.
Weir, David. 1988. Characterising Mildly Context-
sensitive Grammar Formalisms. Ph.D. thesis, Uni-
versity of Pennsylvania. Tech. Report CIS-88-74.
Wittenburg, Kent B. 1986. Natural Language Pars-
ing with Combinatory Categorial Grammar in a
Graph-Unification Based Formalism. Ph.D. the-
sis, University of Texas at Austin.
Zettlemoyer, Luke S. and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of the 21st UAI, pages
658?666, Edinburgh, UK.
Zettlemoyer, Luke and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for pars-
ing to logical form. In Proceedings of EMNLP-
CoNLL, pages 678?687, Prague, Czech Republic.
Acknowledgements
We would like to thank Mark Steedman for help-
ful discussions, and Jason Eisner for his very
generous feedback which helped to greatly im-
prove this paper. All remaining errors and omis-
sions are our own responsibility. J.H is supported
by NSF grant IIS 08- 03603 INT2-Medium.
473
Coling 2010: Poster Volume, pages 1158?1166,
Beijing, August 2010
Shallow Information Extraction from Medical Forum Data
Parikshit Sondhi and Manish Gupta and ChengXiang Zhai and Julia Hockenmaier
Department of Computer Science
University of Illinois at Urbana Champaign
{sondhi1, gupta58, czhai, juliahmr}@illinois.edu
Abstract
We study a novel shallow information ex-
traction problem that involves extracting
sentences of a given set of topic cate-
gories from medical forum data. Given
a corpus of medical forum documents,
our goal is to extract two related types
of sentences that describe a biomedical
case (i.e., medical problem descriptions
and medical treatment descriptions). Such
an extraction task directly generates med-
ical case descriptions that can be useful
in many applications. We solve the prob-
lem using two popular machine learning
methods Support Vector Machines (SVM)
and Conditional Random Fields (CRF).
We propose novel features to improve the
accuracy of extraction. Experiment results
show that we can obtain an accuracy of up
to 75%.
1 Introduction
Conventional information extraction tasks gener-
ally aim at extracting finer granularity semantic
information units such as entities and relations.
While such detailed information is no doubt very
useful, extraction of such information also tends
to be difficult especially when the mentions of the
entities to be extracted do not conform to regular
syntactic patterns.
In this paper, we relax this conventional goal
of extraction and study an easier extraction task
where we aim at extracting sentences that belong
to a set of predefined semantic categories. That is,
we take a sentence as a unit for extraction. Specif-
ically, we study this problem in the context of ex-
tracting medical case description from medical fo-
rums.
A variety of medical health forums exist online.
People use them to post their problems, get ad-
vices from experienced patients, get second opin-
ions from other doctors, or merely to vent out their
frustration.
Compared with well-structured sources such as
Wikipedia, forums are more valuable in the sense
that they contain first hand patient experiences
with richer information in terms of what treat-
ments are better than others and why. Besides
this, on forums, patients explain their symptoms
much more freely than those mentioned on rela-
tively formal sources like Wikipedia. And hence,
forums are much more easier to understand for a
na??ve user.
However, even on targeted forums (which fo-
cus on a single disease), data is quite unstruc-
tured. There is therefore a need to structure out
this information and present it in a form that can
directly be used for a variety of other information
extraction applications like the collecting of med-
ical case studies pertaining to a particular disease,
mining frequently discussed symptoms, identify-
ing correlation between symptoms and treatments,
etc.
A typical medical case description tends to con-
sist of two aspects:
? Physical Examination/Symptoms (PE):
This covers current conditions and includes
any condition that is the focus of current
discussion. Note that if a drug causes an
allergy, then we consider it as a PE and
not a medication. Any condition that is the
focus of conversation, i.e. around which
1158
treatments are being proposed or questions
are being asked is considered PE even if the
user is recounting their past experience.
? Medications (MED): Includes medications
the person is currently taking, or is intend-
ing to take, or any medication on which the
question is targeted. Medications do not nec-
essarily mean drugs. Any measures (includ-
ing avoiding of substances) taken to treat or
avoid the symptoms are considered as medi-
cation. Sometimes, users also mention other
things like constituents of the drug, how
much of the drug to consume at a time, how
to get access to a medication, how much it
costs, side effects of medications, other qual-
ities of medications etc.
Figure 1 shows an example of PE and MED la-
belings.
Figure 1: Example of PE and MED labelings
We thus frame the problem of extracting med-
ical case descriptions as extracting sentences that
describe any of these two aspects. Specifically,
the task is to identify sentences in each of the two
related categories (i.e., PE and MED) from forum
posts. As an extraction task, this task is ?shal-
lower? than conventional information extraction
tasks such as entity extraction in the sense that
we extract a sentence as a unit, which makes the
extraction task more tractable. Indeed, the task
is more similar to sentence categorization. How-
ever, it also differs from a regular sentence cat-
egorization task (e.g., sentiment analysis) in that
the multiple categories are usually closely related
and categorization of multiple sentences may be
dependent in the sense that knowing the category
of one sentence may influence our decision about
the category of another sentence nearby. For ex-
ample, knowing that a sentence is in the category
PE should increase our belief that the next sen-
tence is of category of PE or MED.
We solve the problem using two popular ma-
chine learning methods, Support Vector Machines
(SVM) and Conditional Random Fields (CRF).
We define and study a large set of features, includ-
ing two kinds of novel features: (1) novel features
based on semantic generalization of terms, and (2)
novel features specific to forums.
Since this is a novel task, there is no existing
data set that we can use for evaluation. We thus
create a new data set for evaluation. Experiment
results show that both groups of novel features
are effective and can improve extraction accuracy.
With the best configurations, we can obtain an ac-
curacy of up to 75%, demonstrating feasibility of
automatic extraction of medical case descriptions
from forums.
2 Related work
Medical data mining has been looked atleast since
the early 2000s. Cios and Moore (2002) em-
phasize the uniqueness of medical data mining.
They stress that data mining in medicine is dis-
tinct from that in other fields, because the data
are heterogeneous, and special ethical, legal, and
social constraints apply to private medical infor-
mation. Treatment recommendation systems have
been built that use the structured data to diag-
nose based on symptoms (Lazarus et al, 2001)
and recommend treatments. Holt et al(2005) pro-
vide references to medical systems that use case
based reasoning methodologies for medical diag-
nosis. Huge amounts of medical data stored in
clinical data warehouses can be used to detect pat-
terns and relationships, which could provide new
medical knowledge (Lazarus et al, 2001). In con-
trast, we look at the problem of converting some
of the unstructured medical text data present in fo-
rum threads into structured symptoms and treat-
ments. This data can then be used by all of the
above mentioned applications.
Structuring of unstructured text has been stud-
ied by many works in the literature. Auto-
matic information extraction (Aone and Ramos-
Santacruz, 2000; Buttler et al, 2001) and wrap-
per induction techniques have been used for struc-
turing web data. Sarawagi (2008) and Laen-
1159
der et al (2002) offer comprehensive overviews
of information extraction and wrapper induction
techniques respectively. The main difference be-
tween our work and main stream work on extrac-
tion is that we extract sentences as units, which
is shallower but presumably more robust. Heinze
et al (2002) state that the current state-of-the-
art in NLP is suitable for mining information of
moderate content depth across a diverse collec-
tion of medical settings and specialties. Zhou
et al (2006), the authors perform information ex-
traction from clinical medical records using a de-
cision tree based classifier using resources such as
WordNet 1, UMLS 2 etc. They extract past medi-
cal history and social behaviour from the records.
In other related works, sentiment classifica-
tion (Pang et al, 2002; Prabowo and Thelwall,
2009; Cui et al, 2006; Dave et al, 2003) attempts
to categorize text based on polarity of sentiments
and is often applied at the sentence level (Kim and
Zhai, 2009). Some work has also been done on
extracting content from forum data. This includes
finding question answer pairs (Cong et al, 2008)
from online forums, auto-answering queries on a
technical forum (Feng et al, 2006), ranking an-
swers (Harabagiu and Hickl, 2006) etc. To the
best of our knowledge, this is the first work on
shallow extraction from medical forum data.
3 Problem formulation
Let P = (s1, ...sn) be a sequence of sentences
in a forum post. Given a set of interesting cate-
gories C = {c1, ..., ck} that describe a medical
case, our task is to extract sentences in each cat-
egory from the post P . That is, we would like to
classify each sentence si into one of the categories
ci or Background, which we treat as a special cat-
egory meaning that the sentence is irrelevant to
our extraction task. Depending on specific appli-
cations, a sentence may belong to more than one
category.
In this paper, we focus on extracting sen-
tences of two related categories describing a med-
ical case: (1) Physical Examination (PE), which
includes sentences describing the condition of
1http://wordnet.princeton.edu/
2http://www.nlm.nih.gov/research/umls
a patient (i.e., roughly symptoms) (2) Medica-
tions (MED), which includes sentences mention-
ing medications (i.e., roughly treatment). These
sentences provide a basic description of a medi-
cal case and can already be very useful if we can
extract them.
We chose to analyze at the sentence level be-
cause a sentence provides enough context to de-
tect the category accurately. For example, de-
tecting the categories at word level will not help
us to mark a sentence like ?I get very uncom-
fortable after eating cheese? as PE or mark a
sentence like ?It?s best to avoid cheese in that
case? as MED. Here the problem is loosely repre-
sented by a combination of ?uncomfortable eating
cheese? and the solution is represented loosely by
?avoid cheese?. Indeed, in preliminary analysis,
we found that most of the times, the postings con-
sist of PE and MED type sentences.
4 Methods
We use SVMs and CRFs to learn classifiers
to solve our problem. SVMs represent ap-
proaches that solve the problem as a classifi-
cation/categorization task while CRFs solve the
problem as a sequence labeling task. In this sec-
tion, we provide the basics of SVMs and CRFs.
4.1 Support Vector Machines
SVM first introduced in (Boser et al, 1992), are
a binary classifier that constructs a hyperplane
which separates the training instances belonging
to the two classes. SVMs maximize the separa-
tion margin between this hyperplane and the near-
est training datapoints of any class. The larger the
margin, the lower the generalization error of the
classifier. SVMs have been used to classify both
linearly and non-linearly seperable data, and have
been shown to outperform other popular classi-
fiers like decision trees, Na??ve Bayes classifiers,
k-nearest neighbor classifiers, etc. We use SVMs
as a representative classifier that does not consider
dependencies between the predictions on multiple
sentences.
4.2 Conditional Random Fields
Each of the sentences in the postings can itself
contain features which help us to categorize it.
1160
Besides this, statistical dependencies exist be-
tween sentences. Intuitively, a MED sentence will
follow a PE sentence with high probability, but the
probability of a PE sentence following an MED
sentence would be low. Conditional random fields
are graphical models that can capture such depen-
dencies among input sentences. A CRF model de-
fines a conditional distribution p(y|x) where y is
the predicted category (label) and x is the set of
sentences (observations). CRF is an undirected
graphical model in which each vertex represents
a random variable whose distribution is to be in-
ferred, and each edge represents a dependency be-
tween two random variables. The observation x
can be dependent on the current hidden label y,
previous n hidden labels and on any of the other
observations in a n order CRF. CRFs have been
shown to outperform other probabilistic graphical
models like Hidden Markov Models (HMMs) and
Maximum Entropy Markov Models (MeMMs).
Sutton and McCallum (2006) provide an excellent
tutorial on CRFs.
5 Features
To perform our categorization task, we use the fol-
lowing features.
? Word based features: This includes uni-
grams, bigrams and trigrams in the current
sentence. Each of the n-grams is mapped to a
separate boolean feature per sentence where
value is 1 if it appears in sentence and 0 oth-
erwise.
? Semantic features: This includes Unified
Medical Language System (UMLS3) seman-
tic groups of words in the current sentence.
UMLS is a prominent bio-medical domain
ontology. It contains approximately a mil-
lion bio-medical concepts grouped under 135
semantic groups. MMTX4 is a tool that al-
lows mapping of free text into UMLS con-
cepts and groups. We use these 135 semantic
groups as our semantic features. In order to
generate these features, we first process this
sentence through MMTX API which pro-
vides all the semantic groups that were found
3http://www.nlm.nih.gov/research/umls/
4http://mmtx.nlm.nih.gov/
in the sentence. Each of the semantic groups
becomes a boolean feature.
? Position based features: We define two
types of position based features: position of
the current sentence in the post and position
of the current post in the thread. These fea-
tures are specific to the forum data. We in-
clude these features based on the observa-
tions that first post usually contains condition
related sentences while subsequent posts of-
ten contain treatment measures for the cor-
responding condition. Each of the position
number of a sentence in a post and a post
in a thread is mapped to a boolean feature
which gets fired for a sentence at a partic-
ular position. E.g. For a sentence at po-
sition i in a post, POSITION IN POST i
would be set to 1 while other features PO-
SITION IN POST j where j 6= i would be
set to 0.
? User based features: We include a boolean
feature which gets fired when the sentence
is a part of a post by the thread creator.
This feature is important because most of the
posts by a thread creator have a high proba-
bility of being a PE.
? Tag based features(Edge features): We de-
fine features on tags (PE/MED/Backgnd) of
previous two sentences to capture local de-
pendencies between sentences. E.g., a set
of medication related tags often follow a de-
scription of a condition. We use these fea-
tures only for CRF based experiments.
? Morphological features: These include one
boolean feature each for presence of
? a capitalized word in the sentence
? an abbreviation in the sentence
? a number in the sentence
? a question mark in the sentence
? an exclamation mark in the sentence
? Length based features: We also consider the
number of words in a sentence as a separate
type of feature. Feature LENGTH i becomes
true for a sentence containing i words.
1161
Category Labeler 1 Labeler 2
PE 513 517
MED 286 280
Background 695 697
Table 1: Labeling results
6 Experiments
6.1 Dataset
Evaluation of this new extraction task is chal-
lenging as no test set is available. To solve
this problem, we opted to created our own test
set. HealthBoards5 is a medical forum web por-
tal that allows patients to discuss their ailments.
We scraped 175 posts contained in 50 threads on
allergy i.e., an average of 3.5 posts per thread
and around 2 posts per user with a maximum
of 9 posts by a particular user. Two humans
were asked to tag this corpus as conditions (i.e.,
PE category) or treatments (i.e., MED category)
or none on a per sentence basis. The corpus
consists of 1494 sentences. Table 1 shows the
labeling results. The data set is available at
(http://timan.cs.uiuc.edu/downloads.html). Also
the labeling results match quite well (82.86%)
with a Kappa statistic value of 0.73. Occasion-
ally (around 3%) PE and MED both occur in the
same sentence and the labelers chose to mark such
sentences as PE. In the case when the two label-
ers disagree, we manually analyzed the results and
further chose one of them for our experiments.
6.2 Evaluation methodology
For evaluation, we use 5-fold cross validation.
For CRFs, we used the Mallet6 toolkit and for
SVM, we used SVM-Light7. We experimented
by varying the size of the training set, with differ-
ent feature sets, using two machine learning mod-
els: SVMs and CRFs. Our aim is to accurately
classify any sentence in a post as PE or MED
or background. First we explore and identify the
feature sets that help us in attaining higher accu-
racy. Next, we identify the setting (sequence la-
beling by CRFs or independent classification by
SVMs) that works better to model our problem.
5http://www.healthboards.com
6http://mallet.cs.umass.edu/
7http://svmlight.joachims.org/
We present most of our results using four metrics:
precision, recall, F1 measure and average accu-
racy which is the ratio of correctly labeled sen-
tences to the total sentences.
We considered the following features: all the
2647 words in the vocabulary (no stop-word re-
moval or any other type of selection), 10858 bi-
grams, 135 semantic groups from UMLS, two po-
sition based features, one user based feature, two
tag based features, four morphological features
and one length based feature as described in the
previous section. Thus our feature set is quite
rich. Note that other than the usual features, se-
mantic, position-based and user-based features are
specific to the medical domain or to forum data.
6.3 Basic Results
First we considered word features, and learned a
linear chain CRF model. We added other sets of
features one by one, and observed variations in ac-
curacy. Table 2 shows the accuracy in terms of
precision, recall and F1. Note that these results are
for an Order 1 linear-chain CRF. Accuracy is mea-
sured as ratio of the number of correct labelings of
PE, MED and background to the total number of
sentences in our dataset. Notice that the MED ac-
curacy values are in general quite low compared
to those of PE. As we will discuss later, accuracy
is low for MED because our word-based features
are not discriminative enough for the MED cate-
gory.
From Table 2, we see that the accuracy keeps
increasing as we add semantic UMLS based fea-
tures, position based features and morphological
features. However, length based features (word
count), user-based faetures, and bigrams do not re-
sult in any improvements. We also tried trigrams,
but did not observe any accuracy gains. Thus we
find that semantic features and position-based fea-
tures which are specific to the medical domain
and the forum data respectively are helpful when
added on top of word features, while generic fea-
tures such as length-based features tend to not add
value.
We also trained an order 2 CRF using the same
set of features. Results obtained were similar to
order 1 CRFs and so we do not report them here.
This shows that local dependencies are more im-
1162
Feature set PE Prec MED Prec PE Recall MED Recall PE F1 MED F1 Accuracy %
Word 0.60 0.49 0.65 0.36 0.62 0.42 63.43
+Semantic 0.61 0.52 0.68 0.37 0.64 0.43 65.05?
+Position 0.63 0.54 0.7 0.34 0.66 0.42 65.45
+Morphological 0.64 0.52 0.69 0.36 0.66 0.42 65.70
+WordCount 0.62 0.51 0.70 0.33 0.66 0.40 65.23
+Thread Creator 0.62 0.51 0.71 0.34 0.66 0.41 65.49
+Bigrams 0.62 0.51 0.69 0.34 0.66 0.41 64.82
Table 2: Order 1 Linear Chain CRF. ?Improvement over only word features significant at 0.05-level,
using Wilcoxon?s signed-rank test
portant in medical forum data and global depen-
dencies do not add further signal.
Further, we perform experiments using SVMs
using the same set of features. Table 3 shows
accuracy results on SVM. Again PE is detected
with higher accuracy compared to MED. Unlike
CRFs, SVMs do not incorporate the notion of lo-
cal dependencies between sentences. However,
we observe that SVMs outperform CRFs, as is ev-
ident from the results in Table 3. This is interest-
ing, since it suggests that the SVM accuracy can
potentially be further enhanced by incorporating
such dependency information (e.g. in the form
of new features). We leave this as part of future
work.
Figure 2 shows an example of a forum post
(which talks about allergy to dogs) being tagged
using our CRF model.
Figure 2: Tagging example of a forum post
6.4 Feature selection
Incremental addition of different feature types did
not lead to substantial improvement in perfor-
mance. This suggests that none of the feature
classes contains all ?good? features. We there-
fore perform feature selection based on informa-
tion gain and choose the top 4253 features from
among all the features discussed earlier, based on
a threshold for the gain. This results in improve-
ment in the accuracy values over the previous best
results (Table 4).
Among the word feature set, we found that
important features were allergy, alergies, food,
hives, allergic, sinus, bread. Among bigrams, al-
lergic to, ear infections, my throat, are allergic,
to gluten, food allergies have high information
gain values. Among the UMLS based se-
mantic groups, we found that patf (Pathologic
Function), dsyn (Disease or Syndrome), orch
(Organic Chemical), phsu (Pharmacologic Sub-
stance), sosy (Sign or Symptom) have high in-
formation gain values. Also looking at the word
count feature, we notice that background sen-
tences are generally short sentences. All these fea-
tures are clearly highly discriminative.
6.5 Variation in training data size
We varied the amount of training data used for
learning the models to observe the variation in
performance with size of training data. Table 5
shows the variation in accuracy (PE F1, MED
F1 and average accuracy) for different sizes of
training data using CRFs. In general, we observe
that accuracy improves as we increase the training
data, but the degree varies with the feature sets
used. We see similar trends in SVM also. These
results show that it is possible to further improve
prediction accuracy by obtaining additional train-
ing data.
6.6 Probing into the low MED accuracy
As observed in Tables 2 and 3, MED accuracy
is quite low compared to PE accuracy. We wish
to gain a deeper insight into why the MED ac-
curacy suffers. Therefore, we plot the frequency
of words in sentences marked as PE or MED ver-
sus the rank of the word as shown in the figure 3.
We removed the stop words. Observe that for PE
the curve is quite steep. This indicates that there
1163
Feature set PE Prec MED Prec PE Recall MED Recall PE F1 MED F1 Accuracy %
Word 0.65 0.52 0.71 0.28 0.68 0.36 66.13
+Semantic 0.73 0.54 0.73 0.38 0.73 0.45 71.02?
+Position 0.71 0.52 0.71 0.35 0.71 0.42 69.61
+Morphological 0.72 0.53 0.72 0.38 0.72 0.44 70.28
+WordCount 0.74 0.54 0.72 0.37 0.73 0.44 71.55
+Thread Creator 0.74 0.56 0.72 0.39 0.73 0.46 72.02
+Bigrams 0.75 0.54 0.72 0.40 0.74 0.46 71.69
Table 3: SVM results. ?Improvement over only word features significant at 0.05-level, using
Wilcoxon?s signed-rank test
Classifier PE Prec PE Recall PE F1 MED Prec MED Recall MED F1 Accuracy %
SVM (all* features) 0.72 0.53 0.72 0.38 0.72 0.44 70.28
SVM (selected features) 0.75 0.75 0.75 0.61 0.33 0.44 75.08?
CRF (all* features) 0.64 0.52 0.69 0.36 0.66 0.42 65.70
CRF (selected features) 0.60 0.77 0.67 0.58 0.37 0.45 65.93?
Table 4: Accuracy using the best feature set. (*Word +Semantic +Position +Morphological features).
?Improvement over all* features significant at 0.05-level, using Wilcoxon?s signed-rank test
are some discriminative words which have very
high frequency and so the word features observed
in the training set alo get fired for sentences in
the test set with high probability. While for MED,
we observe that most of the words have very low
frequencies. This basically means that discrimi-
native words for MED may not occur with good
enough frequency. So, many of the word features
that show up in the training set may not appear in
the test data. Hence, MED accuracy suffers.
50
60
70
80
f t
he
 te
rm
PE
MED
0
10
20
30
40
Fr
eq
ue
nc
y 
of
1 31 61 91 12
1
15
1
18
1
21
1
24
1
27
1
30
1
33
1
36
1
39
1
42
1
45
1
Rank of the term
Figure 3: Freq of words vs rank for PE and MED
6.7 Multi-class vs Single class categorization
Note that our task is quite different from plain sen-
tence categorization task. We observe that there is
a dependence between the categories (PE/MED)
that we are trying to predict per sentence. For ex-
ample, considering 100% training data, Table 6
compares the precision, recall and F1 values when
PE MED Backgnd EOP
PE 0.54 0.13 0.28 0.05
MED 0.15 0.51 0.30 0.04
Backgnd 0.18 0.08 0.54 0.20
BOP 0.40 0.07 0.53 0.0
Table 7: Transition probability values
SVM and CRF are trained as single class classi-
fiers using word+semantic features with the multi-
class results obtained previously. Results are gen-
erally better when we do multi-class categoriza-
tion versus single-class categorization. This trend
was reflected for other featuresets also.
6.8 Analysis of transition probabilities
Table 7 shows the transition probabilities from
one category to another as calculated based on our
labelled dataset. BOP is beginning of posting and
EOP is end of posting. Note that posts often start
with a PE or a background sentence and often end
with a background sentence. Also, consecutive
sentences within a posting tend to belong to the
same category.
6.9 Error analysis
We also perform some error analysis on results us-
ing the best feature set. Table 8 shows the confu-
sion matrix for CRF/SVM. We observe many of
the MED errors are because an MED sentence of-
ten gets marked as PE. This basically happens be-
cause some sentences contain both PE and MED.
1164
Feature set 25% 50% 75% 100%
Word 0.59/0.21/0.57 0.6/0.36/0.60 0.61/0.39/0.62 0.62/0.42/0.63
+Semantic 0.61/0.17/0.59 0.63/0.32/0.61 0.64/0.38/0.63 0.64/0.43/0.65
+Position 0.59/0.18/0.56 0.64/0.29/0.60 0.65/0.33/0.62 0.66/0.42/0.65
+Morphological 0.6/0.19/0.57 0.64/0.32/0.61 0.65/0.37/0.63 0.66/0.42/0.65
Best 0.61/0.18/0.65 0.66/0.28/0.64 0.66/0.38/0.66 0.69/0.43/0.68
Table 5: Precision, recall, and F value for various sizes of training data set.
Classifier Type PE Prec PE Recall PE F1 MED Prec MED Recall MED F1
SVM PE vs BKG 0.79 0.64 0.71 - - -
SVM MED vs BKG - - - 0.6 0.28 0.39
SVM Multi-class 0.73 0.73 0.73 0.54 0.38 0.45
CRF PE vs BKG 0.68 0.64 0.66 - - -
CRF MED vs BKG - - - 0.53 0.3 0.39
CRF Multi-class 0.61 0.68 0.64 0.52 0.37 0.43
Table 6: Multi-class vs Single-class categorization with word+semantic features
PE MED Backgnd
PE 424/404 37/37 81/101
MED 102/70 107/95 81/125
Backgnd 164/62 55/21 618/754
Table 8: Confusion matrix showing counts of
actual vs predicted labels for (Best CRF Classi-
fier/Best SVM Classifier)
Other than that some of the PE keywords are also
present in MED sentences, and since the few dis-
criminative MED keywords are quite low in fre-
quency, MED accuracy suffers. E.g. The sen-
tence ?i?m still on antibiotics for the infection but
they don?t seem to be doing any good anymore.?
was labeled as MED but marked as PE by the
CRF. The sentence clearly talks about a medica-
tion. However, the keyword ?infection? is often
observed in PE sentences and so the CRF marks
the sentence as PE.
7 Conclusion
In this paper, we studied a novel shallow infor-
mation extraction task where the goal is to extract
relevant sentences to a predefined set of categories
that describe a medical case. We proposed to
solve the problem using supervised learning and
explored two representative approaches (i.e., CRF
and SVM). We proposed and studied two different
types of novel features for this task, including gen-
eralized terms and forum structure features. We
also created the first test set for evaluating this
problem. Our experiment results show that (1) the
proposed new features are effective for improving
the extraction accuracy, and (2) it is feasible to au-
tomatically extract medical cases in this way, with
the best prediction accuracy above 75%.
Our work can be further extended in several
ways. First, since constructing a test set is labor-
intensive, we could only afford experimenting
with a relatively small data set. It would be in-
teresting to further test the proposed features on
larger data set. Second, while in CRF, we have
shown adding dependency features improves per-
formance, it is unclear how to evaluate this po-
tential benefit with SVM. Since SVM generally
outperforms CRF for this task, it would be very
interesting to further explore how we can extend
SVM to incorporate dependency.
8 Acknowledgement
We thank the anonymous reviewers for their use-
ful comments. This paper is based upon work sup-
ported in part by an IBM Faculty Award, an Alfred
P. Sloan Research Fellowship, an AFOSR MURI
Grant FA9550-08-1-0265, and by the National
Science Foundation under grants IIS-0347933,
IIS-0713581, IIS-0713571, and CNS-0834709.
References
Aone, Chinatsu and Mila Ramos-Santacruz. 2000.
Rees: a large-scale relation and event extraction sys-
tem. In ANLP.
Boser, Bernhard E., Isabelle Guyon, and Vladimir Vap-
nik. 1992. A training algorithm for optimal mar-
1165
gin classifiers. In Computational Learing Theory,
pages 144?152.
Buttler, David, Ling Liu, and Calton Pu. 2001. A fully
automated object extraction system for the world
wide web. In ICDCS.
Cios, Krzysztof J. and William Moore. 2002. Unique-
ness of medical data mining. Artificial Intelligence
in Medicine, 26:1?24.
Cong, Gao, Long Wang, Chin-Yew Lin, Young-In
Song, and Yueheng Sun. 2008. Finding question-
answer pairs from online forums. In SIGIR ?08:
Proceedings of the 31st annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 467?474, New York,
NY, USA. ACM.
Cui, Hang, Vibhu Mittal, and Mayur Datar. 2006.
Comparative Experiments on Sentiment Classifica-
tion for Online Product Reviews. In Proc. of the Na-
tional Conf. on Artificial Intelligence, pages 1265?
1270.
Dave, Kushal, Steve Lawrence, and David M. Pen-
nock. 2003. Mining the Peanut Gallery: Opinion
Extraction and Semantic Classification of Product
Reviews. In Proc. of WWW, pages 519?528.
Feng, Donghui, Erin Shaw, Jihie Kim, and Eduard
Hovy. 2006. An intelligent discussion-bot for an-
swering student queries in threaded discussions. In
IUI ?06: Proceedings of the 11th international con-
ference on Intelligent user interfaces, pages 171?
177, New York, NY, USA. ACM.
Harabagiu, Sanda and Andrew Hickl. 2006. Methods
for using textual entailment in open-domain ques-
tion answering. In ACL-44: Proceedings of the 21st
International Conference on Computational Lin-
guistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 905?
912, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Heinze, Daniel T., Mark L. Morsch, and John Hol-
brook. 2002. Mining free-text medical records. In
Proceedings of the AMIA Annual Symposium.
Holt, Alec, Isabelle Bichindaritz, Rainer Schmidt, and
Petra Perner. 2005. Medical applications in case-
based reasoning. Knowl. Eng. Rev., 20(3):289?292.
Kim, Hyun Duk and ChengXiang Zhai. 2009. Gener-
ating comparative summaries of contradictory opin-
ions in text. In CIKM, pages 385?394.
Laender, Alberto H. F., Berthier A. Ribeiro-neto, Alti-
gran S. da Silva, and Juliana S. Teixeira. 2002. A
brief survey of web data extraction tools. SIGMOD
Record.
Lazarus, R, K P Kleinman, I Dashevsky, A DeMaria,
and R Platt. 2001. Using automated medical
records for rapid identification of illness syndromes
(syndromic surveillance): the example of lower res-
piratory infection. BMC Public Health, 1:9.
Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment Classification using
Machine Learning techniques. In Proc. of EMNLP,
pages 79?86.
Prabowo, Rudy and Mike Thelwall. 2009. Sentiment
analysis: A combined approach. Journal of Infor-
metrics, 3(2):143?157, April.
Sarawagi, Sunita. 2008. Information extraction.
Foundations and Trends in Databases, 1.
Sutton, Charles and Andrew Mccallum, 2006. In-
troduction to Conditional Random Fields for Rela-
tional Learning. MIT Press.
Zhou, Xiaohua, Hyoil Han, Isaac Chankai, Ann Pre-
strud, and Ari Brooks. 2006. Approaches to text
mining for clinical medical records. In SAC ?06:
Proceedings of the 2006 ACM symposium on Ap-
plied computing, pages 235?239, New York, NY,
USA. ACM.
1166
Coling 2010: Poster Volume, pages 1265?1273,
Beijing, August 2010
Citation Author Topic Model in Expert Search
Yuancheng Tu, Nikhil Johri, Dan Roth, Julia Hockenmaier
University of Illinois at Urbana-Champaign
{ytu,njohri2,danr,juliahmr}@illinois.edu
Abstract
This paper proposes a novel topic model,
Citation-Author-Topic (CAT) model that
addresses a semantic search task we define
as expert search ? given a research area as
a query, it returns names of experts in this
area. For example, Michael Collins would
be one of the top names retrieved given the
query Syntactic Parsing.
Our contribution in this paper is two-fold.
First, we model the cited author informa-
tion together with words and paper au-
thors. Such extra contextual information
directly models linkage among authors
and enhances the author-topic association,
thus produces more coherent author-topic
distribution. Second, we provide a prelim-
inary solution to the task of expert search
when the learning repository contains ex-
clusively research related documents au-
thored by the experts. When compared
with a previous proposed model (Johri
et al, 2010), the proposed model pro-
duces high quality author topic linkage
and achieves over 33% error reduction
evaluated by the standard MAP measure-
ment.
1 Introduction
This paper addresses the problem of searching for
people with similar interests and expertise, given
their field of expertise as the query. Many existing
people search engines need people?s names to do a
?keyword? style search, using a person?s name as
a query. However, in many situations, such infor-
mation is insufficient or impossible to know be-
forehand. Imagine a scenario where the statistics
department of a university invited a world-wide
known expert in Bayesian statistics and machine
learning to give a keynote speech; how can the
organizer notify all the people on campus who
are interested without spamming those who are
not? Our paper proposes a solution to the afore-
mentioned scenario by providing a search engine
which goes beyond ?keyword? search and can re-
trieve such information semantically. The orga-
nizer would only need to input the research do-
main of the keynote speaker, i.e. Bayesian statis-
tics, machine learning, and all professors and stu-
dents who are interested in this topic will be re-
trieved and an email agent will send out the infor-
mation automatically.
Specifically, we propose a Citation-Author-
Topic (CAT) model which extracts academic re-
search topics and discovers different research
communities by clustering experts with similar in-
terests and expertise. CAT assumes three steps of
a hierarchical generative process when producing
a document: first, an author is generated, then that
author generates topics which ultimately generate
the words and cited authors. This model links
authors to observed words and cited authors via
latent topics and captures the intuition that when
writing a paper, authors always first have topics
in their mind, based on which, they choose words
and cite related works.
Corpus linguists or forensic linguists usually
1265
identify authorship of disputed texts based on
stylistic features, such as vocabulary size, sen-
tence length, word usage that characterize a spe-
cific author and the general semantic content is
usually ignored (Diederich et al, 2003). On the
other hand, graph-based and network based mod-
els ignore the content information of documents
and only focus on network connectivity (Zhang
et al, 2007; Jurczyk and Agichtein, 2007). In
contrast, the model we propose in this paper fully
utilizes the content words of the documents and
combines them with the stylistic flavor contex-
tual information to link authors and documents to-
gether to not only identify the authorship, but also
to be used in many other applications such as pa-
per reviewer recommendation, research commu-
nity identification as well as academic social net-
work search.
The novelty of the work presented in this pa-
per lies in the proposal of jointly modeling the
cited author information and using a discrimi-
native multinomial distribution to model the co-
author information instead of an artificial uni-
form distribution. In addition, we apply and eval-
uate our model in a semantic search scenario.
While current search engines cannot support in-
teractive and exploratory search effectively, our
model supports search that can answer a range of
exploratory queries. This is done by semantically
linking the interests of authors to the topics of the
collection, and ultimately to the distribution of the
words in the documents.
In the rest of this paper, we first present some
related work on author topic modeling and expert
search in Sec. 2. Then our model is described in
Sec. 3. Sec. 4 introduces our expert search system
and Sec. 5 presents our experiments and the evalu-
ation. We conclude this paper in Sec. 6 with some
discussion and several further developments.
2 Related Work
Author topic modeling, originally proposed
in (Steyvers et al, 2004; Rosen-Zvi et al, 2004),
is an extension of Latent Dirichlet Allocation
(LDA) (Blei et al, 2003), a probabilistic genera-
tive model that can be used to estimate the proper-
ties of multinomial observations via unsupervised
learning. LDA represents each document as a
mixture of probabilistic topics and each topic as
a multinomial distribution over words. The Au-
thor topic model adds an author layer over LDA
and assumes that the topic proportion of a given
document is generated by the chosen author.
Author topic analysis has attracted much atten-
tion recently due to its broad applications in ma-
chine learning, text mining and information re-
trieval. For example, it has been used to pre-
dict authors for new documents (Steyvers et al,
2004), to recommend paper reviewers (Rosen-Zvi
et al, 2004), to model message data (Mccallum et
al., 2004), to conduct temporal author topic anal-
ysis (Mei and Zhai, 2006), to disambiguate proper
names (Song et al, 2007), to search academic so-
cial networks (Tang et al, 2008) and to generate
meeting status analyses for group decision mak-
ing (Broniatowski, 2009).
In addition, there are many related works on
expert search at the TREC enterprise track from
2005 to 2007, which focus on enterprise scale
search and discovering relationships between enti-
ties. In that setting, the task is to find the experts,
given a web domain, a list of candidate experts
and a set of topics 1. The task defined in our paper
is different in the sense that our topics are hid-
den and our document repositories are more ho-
mogeneous since our documents are all research
papers authored by the experts. Within this set-
ting, we can explore in depth the influence of the
hidden topics and contents to the ranking of our
experts. Similar to (Johri et al, 2010), in this pa-
per we apply CAT in a semantic retrieval scenario,
where searching people is associated with a set of
hidden semantically meaningful topics instead of
their personal names.
In recent literature, there are three main lines of
work that extend author topic analyses. One line
of work is to relax the model?s ?bag-of-words?
assumption by automatically discovering multi-
word phrases and adding them into the original
model (Johri et al, 2010). Similar work has also
been proposed for other topic models such as
Ngram topic models (Wallach, 2006; Wang and
McCallum, 2005; Wang et al, 2007; Griffiths et
al., 2007).
1http://trec.nist.gov/pubs.html
1266
Another line of work models authors informa-
tion as a general contextual information (Mei and
Zhai, 2006) or associates documents with network
structure analysis (Mei et al, 2008; Serdyukov et
al., 2008; Sun et al, 2009). This line of work
aims to propose a general framework to deal with
collections of texts with an associated networks
structure. However, it is based on a different topic
model than ours; for example, Mei?s works (Mei
and Zhai, 2006; Mei et al, 2008) extend proba-
bilistic latent semantic analysis (PLSA), and do
not have cited author information explicitly.
Our proposal follows the last line of work
which extends author topic modeling with spe-
cific contextual information and directly captures
the association between authors and topics to-
gether with this contextual information (Tang et
al., 2008; Mccallum et al, 2004). For exam-
ple, in (Tang et al, 2008), publication venue is
added as one extra piece of contextual informa-
tion and in (Mccallum et al, 2004), email recip-
ients, which are treated as extra contextual infor-
mation, are paired with email authors to model an
email message corpus. In our proposed method,
the extra contextual information consists of the
cited authors in each documents. Such contextual
information directly captures linkage among au-
thors and cited authors, enhances author-topic as-
sociations, and therefore produces more coherent
author-topic distributions.
3 The Citation-Author-Topic (CAT)
Model
CAT extends previously proposed author topic
models by explicitly modelling the cited author
information during the generative process. Com-
pared with these models (Rosen-Zvi et al, 2004;
Johri et al, 2010), whose plate notation is shown
in Fig. 1, CAT (shown in Fig. 2) adds cited au-
thor information and generates authors according
to the observed author distribution.
Four plates in Fig. 1 represent topic (T ), au-
thor (A), document (D) and words in each doc-
ument (Nd) respectively. CAT (Fig. 2) has one
more plate, cited-author topic plate, in which each
topic is represented as a multinomial distribution
over all cited authors (?c).
Within CAT, each author is associated with a
 
D
A
N d
Figure 1: Plate notation of the previously pro-
posed author topic models (Rosen-Zvi et al,
2004; Johri et al, 2010).

D
A
N d

Figure 2: Plate notation of our current model:
CAT generates words W and cited authors C in-
dependently given the topic.
multinomial distribution over all topics, ~?a, and
each topic is a multinomial distribution over all
words, ~?t, as well as a multinomial distribution
over all cited authors ~?c. Three symmetric Dirich-
let conjugate priors, ?, ? and ?, are defined for
each of these three multinomial distributions in
CAT as shown in Fig. 2.
The generative process of CAT is formally de-
fined in Algorithm 1. The model first samples
the word-topic, cited author-topic and the author-
topic distributions according to the three Dirich-
let hyperparameters. Then for each word in each
document, first the author k is drawn from the
observed multinomial distribution and that author
chooses the topic zi, based on which word wi and
cited author ci are generated independently.
CAT differs from previously proposed MAT
(Multiword-enhanced Author Topic) model (Johri
et al, 2010) in two aspects. First of all, CAT uses
1267
Algorithm 1: CAT: A, T ,D,N are four
plates as shown in Fig. 2. The generative pro-
cess of CAT modeling.
Data: A, T ,D,N
for each topic t ? T do
draw a distribution over words:
~?t ? DirN (?) ;
draw a distribution over cited authors:
~?c ? DirC(?) ;
for each author a ? A do
draw a distribution over topics:
~?a ? DirT (?) ;
for each document d ? D and k authors ? d
do
for each word w ? d do
choose an author
k ? Multinomial(Ad) ;
assign a topic i given the author:
zk,i|k ? Multinomial(?a) ;
draw a word from the chosen topic:
wd,k,i|zk,i ? Multinomial(?zk,i) ;
draw a cited author from the topic:
cd,k,i|zk,i ? Multinomial(?zk,i)
cited author information to enhance the model
and assumes independence between generating
the words and cited authors given the topic. Sec-
ondly, instead of an artificial uniform distribution
over all authors and co-authors, CAT uses the ob-
served discriminative multinomial distribution to
generate authors.
3.1 Parameter Estimation
CAT includes three sets of parameters. The T
topic distribution over words, ?t which is similar
to that in LDA. The author-topic distribution ?a as
well as the cited author-topic distribution ?c. Al-
though CAT is a relatively simple model, finding
its posterior distribution over these hidden vari-
ables is still intractable due to their high dimen-
sionality. Many efficient approximate inference
algorithms have been used to solve this problem
including Gibbs sampling (Griffiths and Steyvers,
2004; Steyvers and Griffiths, 2007; Griffiths et al,
2007) and mean-field variational methods (Blei et
al., 2003). Gibbs sampling is a special case of
Markov-Chain Monte Carlo (MCMC) sampling
and often yields relatively simple algorithms for
approximate inference in high dimensional mod-
els.
In our CAT modeling, we use a collapsed Gibbs
sampler for our parameter estimation. In this
Gibbs sampler, we integrated out the hidden vari-
ables ?, ? and ? using the Dirichlet delta func-
tion (Heinrich, 2009). The Dirichlet delta func-
tion with an M dimensional symmetric Dirichlet
prior ? is defined as:
?M (?) =
?
(
?M
)
? (M?)
Based on the independence assumptions de-
fined in Fig. 2, the joint distribution of topics,
words and cited authors given all hyperparame-
ters which originally represented by integrals can
be transformed into the delta function format and
formally derived in Equation 1.
P (~z, ~w,~c|?, ?, ?) (1)
= P (~z|?, ?, ?)P (~w,~c|~z, ?, ?, ?)
= P (~z)P (~w|~z)P (~c|~z)
=
A?
a=1
?(nA+?)
?(?)
T?
z=1
?(nZw+?)
?(?)
T?
z=1
?(nZc+?)
?(?)
The updating equation from which the Gibbs
sampler draws the hidden variable for the current
state j, i.e., the conditional probability of drawing
the kth author Kkj , the ith topic Zij , and the cth
cited author Ccj tuple, given all the hyperparame-
ters and all the observed documents and authors,
cited authors except the current assignment (the
exception is denoted by the symbol ??j), is de-
fined in Equation 2.
P (Zij ,Kkj , Ccj |Wwj ,??j, Ad, ?, ?, ?) (2)
? ?(nZ+?)?(nZ,?j+?)
?(nK+?)
?(nK,?j+?)
?(nC+?)
?(nC,?j+?)
= n
w
i,?j+?w
V
P
w=1
nwi,?j+V ?w
nik,?j+?i
T
P
i=1
nik,?j+T?i
nci,?j+?c
C
P
c=1
nci,?j+C?c
The parameter sets ? and ?, ? can be interpreted
as sufficient statistics on the state variables of
the Markov Chain due to the Dirichlet conjugate
priors we used for the multinomial distributions.
1268
These three sets of parameters are estimated based
on Equations 3 , 4 and 5 respectively, in which nwi
is defined as the number of times the word w is
generated by topic i; nik is defined as the number
of times that topic i is generated by author k and
nic is defined as the number of times that the cited
author c is generated by topic i. The vocabulary
size is V , the number of topics is T and the cited-
author size is C.
?w,i =
nwi + ?w
V?
w=1
nwi + V ?w
(3)
?k,i =
nik + ?i
T?
i=1
nik + T?i
(4)
?c,i =
nci + ?c
C?
c=1
nci + C?c
(5)
The Gibbs sampler used in our experiments is
adapted from the Matlab Topic Modeling Tool-
box 2.
4 Expert Search
In this section, we describe a preliminary re-
trieval system that supports expert search, which
is intended to identify groups of research experts
with similar research interests and expertise by in-
putting only general domain key words. For ex-
ample, we can retrieve Michael Collins via search
for natural language parsing.
Our setting is different from the standard TREC
expert search in that we do not have a pre-defined
list of experts and topics, and our documents are
all research papers authored by experts. Within
this setting, we do not need to identify the status of
our experts, i.e., a real expert or a communicator,
as in TREC expert search. All of our authors and
cited authors are experts and the task amounts to
ranking the experts according to different topics
given samples of their research papers.
The ranking function of this retrieval model is
derived through the CAT parameters. The search
2http://psiexp.ss.uci.edu/research/programs data/
aims to link research topics with authors to by-
pass the proper names of these authors. Our re-
trieval function ranks the joint probability of the
query words (W ) and the target author (a), i.e.,
P (W,a). This probability is marginalized over all
topics, and the probability that an author is cited
given the topic is used as an extra weight in our
ranking function. The intuition is that an author
who is cited frequently should be more prominent
and ranked higher. Formally, we define the rank-
ing function of our retrieval system in Equation 6.
ca denotes when the author is one of the cited au-
thors in our corpus. CAT assumes that words and
authors, and cited authors are conditionally inde-
pendent given the topic, i.e., wi ? a ? ca.
P (W,a) =
?
wi
?i
?
t
P (wi, a|t, ca)P (t, ca)
=
?
wi
?i
?
t
P (wi|t)P (a|t)P (ca|t)P (t)
(6)
W is the input query, which may contain one or
more words. If a multiword is detected within the
query, it is added into the query. The final score
is the sum of all words in this query weighted by
their inverse document frequency ?i.
In our experiments, we chose ten queries which
cover several popular research areas in computa-
tional linguistics and natural language processing
and run the retrieval system based on three mod-
els: the original author topic model (Rosen-Zvi
et al, 2004), the MAT model (Johri et al, 2010)
and the CAT model. In the original author topic
model, query words are treated token by token.
Both MAT and CAT expand the query terms with
multiwords if they are detected inside the original
query. For each query, top 10 authors are returned
from the system. We manually label the relevance
of these 10 authors based on the papers collected
in our corpus.
Two standard evaluation metrics are used to
measure the retrieving results. First we evaluate
the precision at a given cut-off rank, namely pre-
cision at rank k with k ranging from 1 to 10. We
then calculate the average precision (AP) for each
query and the mean average precision (MAP) for
1269
the queries. Unlike precision at k, MAP is sensi-
tive to the ranking and captures recall information
since it assumes the precision of the non-retrieved
documents to be zero. It is formally defined as
the average of precisions computed at the point of
each of the relevant documents in the ranked list
as shown in Equation 7.
AP =
?n
r=1(Precision(r)? rel(r))
| relevant documents | (7)
To evaluate the recall of our system, we col-
lected a pool of authors for six of our queries re-
turned from an academic search engine, Arnet-
Miner (Tang et al, 2008)3 as our reference author
pool and evaluate our recall based on the number
of authors we retrieved from that pool.
5 Experiments and Analysis
In this section, we describe the empirical evalua-
tion of our model qualitatively and quantitatively
by applying our model to the expert search we de-
fined in Sec. 4. We compare the retrieving results
with two other models: Multiword- enhanced Au-
thor Topic (MAT) model (Johri et al, 2010) and
the original author topic model (Rosen-Zvi et al,
2004).
5.1 Data set and Pre-processing
We crawled the ACL anthology website and col-
lected papers from ACL, EMNLP and CONLL
over a period of seven years. The ACL anthol-
ogy website explicitly lists each paper together
with its title and author information. Therefore,
the author information of each paper can be ob-
tained accurately without extracting it from the
original paper. However, many author names are
not represented consistently. For example, the
same author may have his/her middle name listed
in some papers, but not in others. We therefore
normalized all author names by eliminating mid-
dle names from all authors.
Cited authors of each paper are extracted from
the reference section and automatically identified
by a named entity recognizer tuned for citation ex-
traction (Ratinov and Roth, 2009). Similar to reg-
ular authors, all cited authors are also normalized
3http://www.arnetminer.org
Conf. Year Paper Author uni. Vocab.
ACL 03-09 1,326 2,084 34,012 205,260
EMNLP 93-09 912 1,453 40,785 219,496
CONLL 97-09 495 833 27,312 123,176
Total 93-09 2,733 2,911 62,958 366,565
Table 1: Statistics about our data set. Uni. denotes
unigram words and Vocab. denotes all unigrams
and multiword phrases discovered in the data set.
with their first name initial and their full last name.
We extracted about 20,000 cited authors from our
corpus. However, for the sake of efficiency, we
only keep those cited authors whose occurrence
frequency in our corpus is above a certain thresh-
old. We experimented with thresholds of 5, 10 and
20 and retained the total number of 2,996, 1,771
and 956 cited authors respectively.
We applied the same strategy to extract mul-
tiwords from our corpus and added them into
our vocabulary to implement the model described
in (Johri et al, 2010). Some basic statistics about
our data set are summarized in Table 1 4.
5.2 Qualitative Coherence Analysis
As shown by other previous works (Wallach,
2006; Griffiths et al, 2007; Johri et al, 2010),
our model also demonstrates that embedding mul-
tiword tokens into the model can achieve more co-
hesive and better interpretable topics. We list the
top 10 words from two topics of CAT and compare
them with those from the unigram model in Ta-
ble 2. Unigram topics contain more general words
which can occur in every topic and are usually less
discriminative among topics.
Our experiments also show that CAT achieves
better retrieval quality by modeling cited authors
jointly with authors and words. The rank of an
author is boosted if that author is cited more fre-
quently. We present in Table 3 the ranking of one
of our ten query terms to demostrate the high qual-
ity of our proposed model. When compared to the
model without cited author information, CAT not
only retrieves more comprehensive expert list, its
ranking is also more reasonable than the model
without cited author information.
Another observation in our experiments is that
4Download the data and the software package at:
http://L2R.cs.uiuc.edu/?cogcomp/software.php.
1270
Query term: parsing
Proposed CAT Model Model without cited authors
Rank Author Prob. Author Prob.
1 J. Nivre 0.125229 J. Nivre 0.033200
2 C. Manning 0.111252 R. Barzilay 0.023863
3 M. Johnson 0.101342 M. Johnson 0.023781
4 J. Eisner 0.063528 D. Klein 0.018937
5 M. Collins 0.047347 R. McDonald 0.017353
6 G. Satta 0.042081 L. Marquez 0.016003
7 R. McDonald 0.041372 A. Moschitti 0.015781
8 D. Klein 0.041149 N. Smith 0.014792
9 K. Toutanova 0.024946 C. Manning 0.014040
10 E. Charniak 0.020843 K. Sagae 0.013384
Table 3: Ranking for the query term: parsing. CAT achieves more comprehensive and reasonable rank
list than the model without cited author information.
CAT Uni. AT Model
TOPIC 49 Topic 27
pronoun resolution anaphor
antecedent antecedents
coreference resolution anaphoricity
network anphoric
resolution is
anaphor anaphora
pronouns soon
anaphor antecedent determination
semantic knowledge pronominal
proper names salience
TOPIC 14 Topic 95
translation quality hypernym
translation systems seeds
source sentence taxonomy
word alignments facts
paraphrases hyponym
decoder walk
parallel corpora hypernyms
translation system page
parallel corpus logs
translation models extractions
Table 2: CAT with embedded multiword com-
ponents achieves more interpretable topics com-
pared with the unigram Author Topic (AT) model.
some experts who published many papers, but on
heterogeneous topics, may not be ranked at the
very top by models without cited author infor-
mation. However, with cited author information,
those authors are ranked higher. Intuitively this
makes sense since many of these authors are also
the most cited ones.
5.3 Quantitative retrieval results
One annotator labeled the relevance of the re-
trieval results from our expert search system. The
annotator was also given all the paper titles of each
Precision@K
K CAT Model Model w/o Cited Authors
1 0.80 0.80
2 0.80 0.70
3 0.73 0.60
4 0.70 0.50
5 0.68 0.48
6 0.70 0.47
7 0.69 0.40
8 0.68 0.45
9 0.73 0.44
10 0.70 0.44
Table 4: Precision at K evaluation of our proposed
model and the model without cited author infor-
mation.
corresponding retrieved author to help make this
binary judgment. We experiment with ten queries
and retrieve the top ten authors for each query.
We first used the precision at k for evaluation.
We calculate the precision at k for both our pro-
posed CAT model and the MAT model, which
does not have the cited author information. The
results are listed in Table 4. It can be observed
that at every rank position, our CAT model works
better. In order to focus more on relevant retrieval
results, we also calculated the mean average pre-
cision (MAP) for both models. For the given ten
queries, the MAP score for the CAT model is 0.78,
while the MAT model without cited author infor-
mation has a MAP score of 0.67. The CAT model
with cited author information achieves about 33%
error reduction in this experiment.
1271
Query ID Query Term
1 parsing
2 machine translation
3 dependency parsing
4 transliteration
5 semantic role labeling
6 coreference resolution
7 language model
8 Unsupervised Learning
9 Supervised Learning
10 Hidden Markov Model
Table 5: Queries and their corresponding ids we
used in our experiments.
Recall for each query
Query ID CAT Model Model w/o Cite
1 0.53 0.20
2 0.13 0.20
3 0.27 0.13
4 0.13 0.2
5 0.27 0.20
6 0.13 0.26
Average 0.24 0.20
Table 6: Recall comparison between our proposed
model and the model without cited author infor-
mation.
Since we do not have a gold standard experts
pool for our queries, to evaluate recall, we col-
lected a pool of authors returned from an aca-
demic search engine, ArnetMiner (Tang et al,
2008) as our reference author pool and evaluated
our recall based on the number of authors we re-
trieved from that pool. Specifically, we get the
top 15 returned persons from that website for each
query and treat them as the whole set of relevant
experts for that query and our preliminary recall
results are shown in Table 6.
In most cases, the CAT recall is better than that
of the compared model, and the average recall is
better as well. All the queries we used in our ex-
periments are listed in Table 5. And the average
recall value is based on six of the queries which
have at least one overlap author with those in our
reference recall pool.
6 Conclusion and Further Development
This paper proposed a novel author topic model,
CAT, which extends the existing author topic
model with additional cited author information.
We applied it to the domain of expert retrieval
and demonstrated the effectiveness of our model
in improving coherence in topic clustering and au-
thor topic association. The proposed model also
provides an effective solution to the problem of
community mining as shown by the promising re-
trieval results derived in our expert search system.
One immediate improvement would result from
extending our corpus. For example, we can ap-
ply our model to the ACL ARC corpus (Bird et
al., 2008) to check the model?s robustness and en-
hance the ranking by learning from more data. We
can also apply our model to data sets with rich
linkage structure, such as the TREC benchmark
data set or ACL Anthology Network (Radev et al,
2009) and try to enhance our model with the ap-
propriate network analysis.
Acknowledgments
The authors would like to thank Lev Ratinov for
his help with the use of the NER package and the
three anonymous reviewers for their helpful com-
ments and suggestions. The research in this pa-
per was supported by the Multimodal Information
Access & Synthesis Center at UIUC, part of CCI-
CADA, a DHS Science and Technology Center of
Excellence.
References
Bird, S., R. Dale, B. Dorr, B. Gibson, M. Joseph,
M. Kan, D. Lee, B Powley, D. Radev, and Y. Tan.
2008. The acl anthology reference corpus: A refer-
ence dataset for bibliographic research in computa-
tional linguistics. In Proceedings of LREC?08.
Blei, D., A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research.
Broniatowski, D. 2009. Generating status hierar-
chies from meeting transcripts using the author-
topic model. In In Proceedings of the Workshop:
Applications for Topic Models: Text and Beyond.
Diederich, J., J. Kindermann, E. Leopold, and
G. Paass. 2003. Authorship attribution with support
vector machines. Applied Intelligence, 19:109?123.
1272
Griffiths, T. and M. Steyvers. 2004. Finding scientific
topic. In Proceedings of the National Academy of
Science.
Griffiths, T., M. Steyvers, and J. Tenenbaum. 2007.
Topics in semantic representation. Psychological
Review.
Heinrich, G. 2009. Parameter estimation for text anal-
ysis. Technical report, Fraunhofer IGD.
Johri, N., D. Roth, and Y. Tu. 2010. Experts? retrieval
with multiword-enhanced author topic model. In
Proceedings of NAACL-10 Semantic Search Work-
shop.
Jurczyk, P. and E. Agichtein. 2007. Discovering au-
thorities in question answer communities by using
link analysis. In Proceedings of CIKM?07.
Mccallum, A., A. Corrada-emmanuel, and X. Wang.
2004. The author-recipient-topic model for topic
and role discovery in social networks: Experiments
with enron and academic email. Technical report,
University of Massachusetts Amherst.
Mei, Q. and C. Zhai. 2006. A mixture model for con-
textual text mining. In Proceedings of KDD-2006,
pages 649?655.
Mei, Q., D. Cai, D. Zhang, and C. Zhai. 2008. Topic
modeling with network regularization. In Proceed-
ing of WWW-08:, pages 101?110.
Radev, D., M. Joseph, B. Gibson, and P. Muthukrish-
nan. 2009. A Bibliometric and Network Analysis
of the field of Computational Linguistics. Journal
of the American Society for Information Science and
Technology.
Ratinov, L. and D. Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proc. of the Annual Conference on Computational
Natural Language Learning (CoNLL).
Rosen-Zvi, M., T. Griffiths, M. Steyvers, and P. Smyth.
2004. the author-topic model for authors and docu-
ments. In Proceedings of UAI.
Serdyukov, P., H. Rode, and D. Hiemstra. 2008. Mod-
eling multi-step relevance propagation for expert
finding. In Proceedings of CIKM?08.
Song, Y., J. Huang, and I. Councill. 2007. Efficient
topic-based unsupervised name disambiguation. In
Proceedings of JCDL-2007, pages 342?351.
Steyvers, M. and T. Griffiths. 2007. Probabilistic topic
models. In Handbook of Latent Semantic Analysis.
Lawrence Erlbaum Associates.
Steyvers, M., P. Smyth, and T. Griffiths. 2004. Proba-
bilistic author-topic models for information discov-
ery. In Proceedings of KDD.
Sun, Y., J. Han, J. Gao, and Y. Yu. 2009. itopicmodel:
Information network-integrated topic modeling. In
Proceedings of ICDM-2009.
Tang, J., J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su.
2008. Arnetminer: Extraction and mining of aca-
demic social networks. In Proceedings of KDD-
2008, pages 990?998.
Wallach, H. 2006. Topic modeling; beyond bag of
words. In International Conference on Machine
Learning.
Wang, X. and A. McCallum. 2005. A note on topi-
cal n-grams. Technical report, University of Mas-
sachusetts.
Wang, X., A. McCallum, and X. Wei. 2007. Topical
n-grams: Phrase and topic discoery with an appli-
cation to information retrieval. In Proceedings of
ICDM.
Zhang, J., M. Ackerman, and L. Adamic. 2007. Ex-
pertise networks in online communities: Structure
and algorithms. In Proceedings of WWW 2007.
1273
CCGbank: A Corpus of CCG Derivations and
Dependency Structures Extracted from the
Penn Treebank
Julia Hockenmaier?
University of Pennsylvania
Mark Steedman??
University of Edinburgh
This article presents an algorithm for translating the Penn Treebank into a corpus of Combina-
tory Categorial Grammar (CCG) derivations augmented with local and long-range word?word
dependencies. The resulting corpus, CCGbank, includes 99.4% of the sentences in the Penn
Treebank. It is available from the Linguistic Data Consortium, and has been used to train wide-
coverage statistical parsers that obtain state-of-the-art rates of dependency recovery.
In order to obtain linguistically adequate CCG analyses, and to eliminate noise and incon-
sistencies in the original annotation, an extensive analysis of the constructions and annotations
in the Penn Treebank was called for, and a substantial number of changes to the Treebank were
necessary. We discuss the implications of our findings for the extraction of other linguistically
expressive grammars from the Treebank, and for the design of future treebanks.
1. Introduction
In order to understand a newspaper article, or any other piece of text, it is necessary to
construct a representation of its meaning that is amenable to some form of inference.
This requires a syntactic representation which is transparent to the underlying seman-
tics, making the local and long-range dependencies between heads, arguments, and
modifiers explicit. It also requires a grammar that has sufficient coverage to deal with
the vocabulary and the full range of constructions that arise in free text, together with
a parsing model that can identify the correct analysis among the many alternatives that
such a wide-coverage grammar will generate even for the simplest sentences. Given our
current machine learning techniques, such parsing models typically need to be trained
on relatively large treebanks?that is, text corpora hand-labeled with detailed syntactic
structures. Because such annotation requires linguistic expertise, and is therefore diffi-
cult to produce, we are currently limited to at most a few treebanks per language.
One of the largest and earliest such efforts is the Penn Treebank (Marcus, Santorini,
and Marcinkiewicz 1993; Marcus et al 1994), which contains a one-million word
? Institute for Research in Cognitive Science, University of Pennsylvania, 3401 Walnut Street, Suite 400A,
Philadelphia, PA 19104-6228, USA. E-mail: juliahr@cis.upenn.edu.
?? School of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail:
steedman@inf.ed.ac.uk.
Submission received: 16 July 2005; revised submission received: 24 January 2007; accepted for publication:
21 February 2007.
? 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 3
subcorpus of Wall Street Journal text that has become the de facto standard training
and test data for statistical parsers. Its annotation, which is based on generic phrase-
structure grammar (with coindexed traces and other null elements indicating non-local
dependencies) and function tags on nonterminal categories providing (a limited degree
of) syntactic role information, is designed to facilitate the extraction of the underlying
predicate?argument structure. Statistical parsing on the Penn Treebank has made great
progress by focusing on the machine-learning or algorithmic aspects (Magerman 1994;
Ratnaparkhi 1998; Collins 1999; Charniak 2000; Henderson 2004; McDonald, Crammer,
and Pereira 2005). However, this has often resulted in parsing models and evaluation
measures that are both based on reduced representations which simplify or ignore the
linguistic information represented by function tags and null elements in the original
Treebank. (One exception is Collins 1999, whose Model 2 includes a distinction between
arguments and adjuncts, and whose Model 3 additionally captures wh-movement in
relative clauses with a GPSG-like ?slash-feature-passing? mechanism.)
The reasons for this shift away from linguistic adequacy are easy to trace. The very
healthy turn towards quantitative evaluation interacts with the fact that just about every
dimension of linguistic variation exhibits a Zipfian distribution, where a very small
proportion of the available alternatives accounts for most of the data. This creates a
temptation to concentrate on capturing the few high-frequency cases at the top end of
the distribution, and to ignore the ?long tail? of rare events such as non-local dependen-
cies. Despite the fact that these occur in a large number of sentences, they affect only a
small number of words, and have thus a small impact on overall dependency recovery.
Although there is now a sizable literature on trace and function-tag insertion al-
gorithms (Blaheta and Charniak 2000; Johnson 2002; Campbell 2004), and integrated
parsing with function tags or null elements (Dienes and Dubey 2003a, 2003b; Merlo and
Musillo 2005; Gabbard, Kulick, and Marcus 2006), such approaches typically require ad-
ditional pre- or postprocessing steps that are likely to add further noise and errors to the
parser output. A completely integrated approach that is based on a syntactic representa-
tion which allows direct recovery of the underlying predicate?argument structure might
therefore be preferable. Such representations are provided by grammar formalisms
that are more expressive than simple phrase-structure grammar, like Lexical-Functional
Grammar (LFG) (Kaplan and Bresnan 1982), Head-driven Phrase-Structure Grammar
(HPSG) (Pollard and Sag 1994), Tree-Adjoining Grammar (TAG) (Joshi and Schabes
1992), Minimalist Program?related Grammars (Stabler 2004), or Combinatory Catego-
rial Grammar (CCG) (Steedman 1996, 2000). However, until very recently, only hand-
written grammars, which lack the wide coverage and robustness of Treebank parsers,
were available for these formalisms (Butt et al 1999; XTAG-group 1999; Copestake and
Flickinger 2000; OpenCCG1 [White and Baldridge 2003; White 2006]).
Because treebank annotation for individual formalisms is prohibitively expensive,
there have been a number of efforts to extract TAGs, LFGs, and, more recently, HPSGs,
from the Penn Treebank (Xia 1999; Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi
2000; Xia 2001; Cahill et al 2002; Miyao, Ninomiya, and Tsujii 2004; O?Donovan et al
2005; Shen and Joshi 2005; Chen, Bangalore, and Vijay-Shanker 2006). Statistical parsers
that are trained on these TAG and HPSG corpora have been presented by Chiang (2000)
and Miyao and Tsujii (2005), whereas the LFG parsing system of Cahill et al (2004) uses
1 Open CCG, the successor of Grok (Hockenmaier et al 2004), is available from http://openccg.
sourceforge.net.
356
Hockenmaier and Steedman CCGbank
a postprocessing step on the output of a Treebank parser to recover predicate?argument
dependencies.
In this article we present an algorithmic method for obtaining a corpus of CCG
derivations and dependency structures from the Penn Treebank, together with some
observations that we believe carry wider implications for similar attempts with other
grammar formalisms and corpora. Earlier versions of the resulting corpus, CCGbank,
have already been used to build a number of wide-coverage statistical parsers (Clark,
Hockenmaier, and Steedman 2002; Hockenmaier and Steedman 2002; Hockenmaier
2003b, 2003a; Clark and Curran 2004, 2007), which recover both local and long-range
dependencies directly and in a single pass.
CCG is a linguistically expressive, but efficiently parseable, lexicalized grammar
formalism that was specifically designed to provide a base-generative account of coor-
dinate and relativized constructions like the following:
a. pay HealthVest $5 million right away and additional amounts in the future (1)
b. the parched Franco years, the everyday poverty and stagnant atmosphere of
which he described in brutally direct, vivid prose
c. Who is, and who should be, making the criminal law here?
CCG directly captures the non-local dependencies involved in these and other
constructions, including control and raising, via an enriched notion of syntactic types,
without the need for syntactic movement, null elements, or traces. It also provides
a ?surface-compositional? syntax?semantics interface, in which monotonic rules of
semantic composition are paired one-to-one with rules of syntactic composition. The
corresponding predicate?argument structure or logical form can therefore be directly
obtained from any derivation if the semantic interpretation of each lexical entry is
known. In this article and in CCGbank, we approximate such semantic interpretations
with dependency graphs that include most semantically relevant non-anaphoric local
and long-range dependencies. Although certain decisions taken by the builders of the
original Penn Treebank mean that the syntactic derivations that can be obtained from
the Penn Treebank are not always semantically correct (as we will discuss), subsequent
work by Bos et al (2004) and Bos (2005) has demonstrated that the output of parsers
trained on CCGbank can also be directly translated into logical forms such as Discourse
Representation Theory structures (Kamp and Reyle 1993), which can then be used as in-
put to a theorem prover in applications like question answering and textual entailment
recognition.
Translating the Treebank into this more demanding formalism has revealed certain
sources of noise and inconsistency in the original annotation that have had to be cor-
rected in order to permit induction of a linguistically correct grammar. Because of this
preprocessing, the dependency structures in CCGbank are likely to be more consistent
than those extracted directly from the Treebank via heuristics such as those given by
Magerman (1994) and Collins (1999), and therefore may also be of immediate use for
dependency-based approaches. However, the structure of certain constructions, such
as compound nouns or fragments, is deliberately underspecified in the Penn Treebank.
Although we have attempted to semi-automatically restore the missing structure wher-
ever possible, in many cases this would have required additional manual annotation,
going beyond the scope of our project. We suspect that these properties of the original
Treebank will affect any similar attempt to extract dependency structures or grammars
for other expressive formalisms. The Penn Treebank is the earliest (and still the largest)
357
Computational Linguistics Volume 33, Number 3
corpus of its kind; we hope that our experiences will extend its useful life, and help in
the design of future treebanks.
2. Combinatory Categorial Grammar
Combinatory Categorial Grammar (CCG) was originally developed as a ?near-context-
free? theory of natural language grammar, with a very free definition of derivational
structure adapted to the analysis of coordination and unbounded dependency without
movement or deletion transformations. It has been successfully applied to the analysis
of coordination, relative clauses and related constructions, intonation structure, binding
and control, and quantifier scope alternation, in a number of languages?see Steedman
and Baldridge (2006) for a recent review. Extensions of CCG to other languages and
word-orders are discussed by Hoffman (1995), Kang (1995), Bozsahin (1998), Komagata
(1999), Steedman (2000), Trechsel (2000), Baldridge (2002), and C?ak?c? (2005). The deriva-
tions in CCGbank follow the analyses of Steedman (1996, 2000), except where noted.
2.1 Lexical Categories
Categorial Grammars are strongly lexicalized, in the sense that the grammar is entirely
defined by a lexicon in which words (and other lexical items) are associated with one
or more specific categories which completely define their syntactic behavior. The set of
categories consists of basic categories (e.g., S, NP, PP) and complex categories of the
form X/Y or X\Y, representing functors with (basic or complex) argument category Y
and result category X. Functor categories of the form X/Y expect their argument Y to its
right, whereas those of the form X\Y expect Y to their left.2 These functor categories en-
code subcategorization information, that is, the number and directionality of expected
arguments. English intransitive verbs and verb phrases have the category S\NP: they
take a (subject) NP to their left as argument and yield a sentence. English transitive
verbs have the category (S\NP)/NP: they take an (object) NP to their right to yield a
verb phrase (S\NP), which in turn takes a (subject) NP to its left to form a sentence S.
Each syntactic category also has a corresponding semantic interpretation (here given as
a ?-expression). Hence, the lexical entry for ditransitive give can be written as follows:3
give := ((S\NP)/NP)/NP:?x?y?z.give?yxz (2)
In our translation algorithm, we use simple word?word dependency structures to ap-
proximate the underlying semantic interpretation.
2.2 Derivations
A universal set of syntactic combinatory rules defines how constituents can be com-
bined. All variants of categorial grammar since Ajdukiewicz (1935) and Bar-Hillel (1953)
include function application, where a functor X/Y or X\Y is applied to an argument Y:
Forward Application: X/Y : f Y : a ? X : fa
Backward Application: Y : a X\Y : f ? X : fa
(3)
2 This is the ?result-leftmost? notation, which is easiest to mentally translate into the corresponding
semantic type. There is an alternative ?result-on-top? notation.
3 This category embodies a lexicalized ?Wrap? relation between the rightward NP arguments and the
corresponding variables x and y in the semantic interpretation (Bach 1976; Dowty 1978; Jacobson 1992),
reflecting the binding-theoretic relations between these arguments at the level of logical form rather than
surface derivation.
358
Hockenmaier and Steedman CCGbank
These rules give rise to derivations like the following:4
I give them money
NP :I? ((S\NP)/NP)/NP : ?x?y?z.give?yxz NP : them? NP : money?
>
(S\NP)/NP : ?y?z.give?y them?z
>
S\NP : ?z.give?money?them?z
<
S : give?money?them?I?
(4)
This derivation is isomorphic to a traditional context-free derivation tree like the fol-
lowing (the semantics is omitted):
S
NP
I
S\NP
(S\NP)/NP
((S\NP)/NP)/NP
give
NP
them
NP
money
CCG additionally introduces a set of rule schemata based on the combinators of
combinatory logic (Curry and Feys 1958), which enable succinct analyses of extraction
and coordination constructions. It is a distinctive property of CCG that all syntactic rules
are purely type-driven, unlike traditional structure-dependent transformations. Compo-
sition and substitution allow two functors to combine into another functor, whereas
type-raising is a unary rule that exchanges the roles of functor and argument:
Forward Composition: X/Y: f Y/Z:g ?B X/Z:?z.f (gz) (5)
Backward Composition: Y\Z:g X\Y: f ?B X\Z:?z.f (gz)
Backward Crossed Composition: Y/Z:g X\Y: f ?B X/Z:?z.f (gz)
Forward Type-raising: X:a ?T T/(T\X):?f.fa
Backward Crossed Substitution: Y\Z:g (X\Y)\Z: f ?S X\Z:?z.fz(gz)
Generalized Forward Composition: X/Y: f (Y/W)/Z:g ?B2 (X/W)/Z:?z.?w.f (gzw)
For example, the following is the derivation of a relative clause related to (4):
money that I give them
N (N\N)/(S/NP) NP ((S\NP)/NP)/NP NP
: money? : ?p?q?x.px?qx : I? : ?x?y?z.give?yxz : them?
>T
S/(S\NP) : ?f .f I?
>B2
(S/NP)/NP : ?x?y.give?yxI?
>
S/NP : ?y.give?y them?I?
>
N\N : ?q?x.give?x them?I? ? qx
<
N : ?x.give?x them?I? ? money?x
(6)
We will see further examples of their use later. Such rules induce additional de-
rivational ambiguity, even in canonical sentences like (4). However, our translation
4 Application of the two rules is indicated by underlines distinguished as < and >, respectively.
359
Computational Linguistics Volume 33, Number 3
algorithm yields normal form derivations (Hepple and Morrill 1989; Wittenburg and
Wall 1991; Ko?nig 1994; Eisner 1996), which use composition and type-raising only
when syntactically necessary. For coordination, we will use a binarized version of the
following ternary rule schema:5
Coordination: X: f conj X: g ?& X : ?x.fx ? gx (7)
For further explanation and linguistics and computational motivation for this the-
ory of grammar, the reader is directed to Steedman (1996, 2000).
2.3 Head-Dependency Structure in CCGbank
The syntactic derivations in CCGbank are accompanied with bilexical head-dependency
structures, which are defined in terms of the lexical heads of functor categories and their
arguments. The derivation in (6) corresponds to the following dependency structure,
which includes the long-range dependency between give and money:
?that,(N\N)/(S[dcl]/NP),1,money?, (8)
?that,(N\N)/(S/NP),2,give?,
?give,((S[dcl]\NP1)/NP2)/NP3,1,I?,
?give,((S[dcl]\NP1)/NP2)/NP3,2,money?,
?give,((S[dcl]\NP1)/NP2)/NP3,3,them?
The dependency structures in CCGbank are intended to include all non-anaphoric local
and long-range dependencies relevant to determining semantic predicate?argument
relations, and hence approximate more fine-grained semantic representations. In
this, they differ crucially from the bilexical surface dependencies used by the par-
sing models of Collins (1999) and Charniak (2000) and returned by the depen-
dency parser of McDonald, Crammer, and Pereira (2005). In order to obtain such
non-local dependencies, certain types of lexical category such as relative pronouns
or raising and control verbs require additional coindexation information (described
subsequently).
We believe that CCGbank?s extensive annotation of non-local predicate?argument
dependencies is one of its most useful features for researchers using other expressive
grammar formalisms, including LFG, HPSG, and TAG, facilitating comparisons in
terms of error analyses of particular constructions or types of dependency, such as
non-subject extracted relative clauses. Because these dependency structures provide
a suitable approximation of the underlying semantics, and because each interpreta-
tion unambiguously corresponds to one dependency structure (but may be obtained
from multiple, equivalent, derivations), we furthermore follow Lin (1998) and Carroll,
Minnen, and Briscoe (1999) in regarding them as a fairer, and ultimately more useful,
5 In the semantics of this rule, ? stands for the usual pointwise recursion over logical conjunction (Rooth
and Partee 1982). Baldridge (2002), following Hepple (1990), Morrill (1994), and Moortgat (1997),
advocates a variant of CCG where categories and rules are further specified by modalities that limit the
applicability of rules such as composition. This variant allows a more elegant account in which
conjunctions have categories of the form (X\X)/X, in which the  modality prevents the rules in (5)
from applying, because they are variously restricted by ? (?crossed?) and  (?harmonic?) modalities. In
the current version of CCGbank, such modalities are ignored, at some cost to generative soundness.
360
Hockenmaier and Steedman CCGbank
standard against which to evaluate the output of parsers trained on CCGbank than the
syntactic derivations themselves.
3. The Penn Treebank
The Wall Street Journal subcorpus of the Penn Treebank contains about 50,000 sentences,
or 1 million words, annotated with part-of-speech tags and phrase-structure trees:
(S (NP-SBJ (PRP He)) (9)
(VP (VBZ is)
(VP (ADVP (RB just))
(VBG passing)
(NP (DT the) (NN buck))
(PP-DIR (TO to)
(NP (JJ young) (NNS people)))))
(. .))
These trees are relatively flat: modals and auxiliaries introduce a new VP level, whereas
verb modifiers and arguments typically appear all at the same level, as sisters of the
main verb. A similarly flat annotation style is adopted at the sentence level. NPs are
flat as well, with all complex modifiers appearing at the same NP level, and compound
nouns typically lacking any internal structure.
The translation algorithm needs to identify syntactic heads, and has to distinguish
between complements and modifiers. In the Treebank, this information is not explicit.
Although some non-terminal nodes carry additional function tags, such as -SBJ (sub-
ject) or -TMP (temporal modifier), truly problematic cases such as prepositional phrases
are often marked with tags such as -CLR (?closely related?) or -DIR (?direction?), which
are not always reliable or consistent indicators that a constituent is a modifier or an
argument.
The Treebank uses various types of null elements and traces to encode non-local
dependencies. These are essential for our algorithm since they make it possible to obtain
correct CCG derivations for relative clauses, wh-questions, and coordinate constructions
such as right node raising. Their treatment is discussed in Sections 6.2 and 6.3.
4. The Basic Translation Algorithm
In order to obtain CCG derivations from the Penn Treebank, we need to define a
mapping from phrase structure trees to CCG derivations, including a treatment of the
null elements in the Treebank. We also need to modify the Treebank where its syntactic
analyses differ from CCG, and clean up certain sources of noise that would otherwise
result in incorrect CCG derivations.
We will begin by ignoring null elements, and assume that Penn Treebank trees
are entirely consistent with CCG analyses. The basic algorithm then consists of four
steps:
foreach tree ?: (10)
determineConstituentTypes(?);
makeBinary(?);
assignCategories(?);
assignDependencies(?);
361
Computational Linguistics Volume 33, Number 3
Similar algorithms for phrase-structure trees without traces or other null elements have
been suggested by Buszkowski and Penn (1990) and Osborne and Briscoe (1998).
We illustrate this basic algorithm using the previous example (9). Then we will
extend this algorithm to deal with coordination, and introduce a modification to cope
with the fact that certain word classes, such as participials, can act as modifiers of
a large number of constituent types. Section 5 summarizes the most important pre-
processing steps that were necessary to obtain the desired CCG analyses from the
Treebank trees. Section 6 extends this basic algorithm to deal with the null elements
in the Treebank.
4.1 Determining Constituent Types: Heads, Complements, and Adjuncts
First, the constituent type of each node (head (h), complement (c), or adjunct (a)) is
determined, using heuristics adapted from Magerman (1994) and Collins (1999), which
take the label of a node and its parent into account.6 We assume that NP daughters of VPs
are complements, unless they carry a function tag such as -LOC, -DIR, -TMP, and so on,
but treat all PPs as adjuncts unless they carry the -CLR function tag. In our example,
we therefore treat passing as transitive, even though it should subcategorize for the
PP:
S:h
NP-SBJ:c
He
VP:h
VBZ:h
is
VP:c
ADVP:a
just
VBG:h
passing
NP:c
the buck
PP-DIR:a
to young people
4.2 Binarizing the Tree
Next, the tree is binarized:
S:h
NP-SBJ:c
He
VP:h
VBZ:h
is
VP:c
ADVP:a
just
VP:h
VP:h
VBG:h
passing
NP:c
the buck
PP-DIR:a
to young people
This binarization process inserts dummy nodes into the tree such that all children to the
left of the head branch off in a right-branching tree, and then all children to the right of
the head branch off in a left-branching tree.7
6 We had to modify these heuristics in minor ways. A complete list of our rules is in the CCGbank manual
(Hockenmaier and Steedman 2005).
7 In order to guarantee that sentence-final punctuation marks (omitted in the example for space reasons)
modify the sentence rather than the VP, we first insert an additional S node that spans everything but the
sentence-final punctuation mark.
362
Hockenmaier and Steedman CCGbank
4.3 Assigning Categories
We assign CCG categories to the nodes in this binary tree in the following manner:
4.3.1 The Root Node. The category of the root node is determined by the label of the root
of the Treebank tree (e.g., {VP} ? S\NP, {S, SINV, SQ} ? S).8 If the root node has the
category S, it typically carries a feature that distinguishes different types of sentences,
such as declaratives (S[dcl]), wh-questions (S[wq]), yes?no questions (S[q]), or fragments
(S[frg]). In our running example, the root is S[dcl], because its Treebank label is S, and its
head word, the auxiliary, has the POS tag VBZ.
4.3.2 Head and Complement. The category of a complement child is defined by a similar
mapping from Treebank labels to categories, for example, {NP} ? NP, {PP} ? PP.9 The
CCG category of the head is a function which takes the category of the complement as
argument and returns the category of the parent node. The direction of the slash is given
by the position of the complement relative to the head:
S[dcl]
NP
He
S[dcl]\NP
is just passing the buck to young people
The VP that is headed by the main verb passing is a complement of the auxiliary. Because
the POS tag of passing is VBG, the CCG category of the complement VP is S[ng]\NP (present
participle) and the lexical category of is is therefore (S[dcl]\NP)/(S[ng]\NP):
S[dcl]\NP
(S[dcl]\NP)/(S[ng]\NP)
is
S[ng]\NP
just passing the buck to young people
Other VP features include [to] (to infinitival), [b] (bare infinitival), S[pt] (past participle),
[pss] (passive), or [ng] (present participle).
4.3.3 Head and Adjunct. According to the Treebank annotation and the assumptions of
the algorithm, our example has two VP adjuncts: the adverb just, and, because of its
-DIR function tag, the PP to young people. In both cases, the adjunct category depends on
the category of the parent, and the category of the head child is copied from the parent:
S[ng]\NP
(S\NP)/(S\NP)
just
S[ng]\NP
S[ng]\NP
passing the buck
(S\NP)\(S\NP)
to young people
Given a parent category C, the category of an adjunct child is a unary functor C?/C? if
the adjunct child is to the left of the head child (a premodifier), or C?\C? if it is to the right
8 Every Treebank tree is in fact rooted in an unlabeled node. This node is kept in the translation and
assigned the label TOP.
9 Generally, the Treebank label determines the category of complements. Function tags, such as -SBJ, are
currently not reflected in the categories. However, we use *EXP* null elements and the EX POS-tag to
assign categories NP[expl] and NP[thr] to expletive it or there.
363
Computational Linguistics Volume 33, Number 3
Figure 1
Function composition reduces the number of lexical categories of adjuncts.
of the head (a postmodifier). In most cases, the category C? is equal to the parent category
C without any features such as [dcl], [ng], and so forth, and the modifier combines with
the head via simple function application. As shown in Figure 1, in many cases, a more
elegant (and general) analysis can be obtained if we allow modifiers to compose with the
head. For example, regularly has the category (S\NP)\(S\NP) in sentences such as I visit
certain places regularly, because it modifies the verb phrase visit certain places, which has
the category S[dcl]\NP. But in the corresponding relative clause places that I visit regularly
or with heavy NP shift (I visit regularly certain places in Europe), regularly modifies visit,
that is, a constituent with category (S[dcl]\NP)/NP. Without function composition, the cat-
egory of regularly would have to be ((S\NP)/NP)\((S\NP)/NP), but (crossed) composition
allows the ordinary category (S\NP)\(S\NP) to also work in this case.
Therefore, if the parent (and head) category C is of the form X/$, the algorithm strips
off all outermost forward arguments /$ (and syntactic features) from C to obtain C?.
Similarly, if C is of the form X\$, all outermost backward arguments \$ (and syntactic
features) are stripped off from C to obtain C?.
4.3.4 Head and Punctuation Mark. With the exception of some dashes and parentheses
(see Section 4), the category of a punctuation mark is identical to its POS tag, and the
head has the same category as its parent.
4.3.5 The Final Derivation. Figure 2 shows the complete CCG derivation of our ex-
ample. The category assignment procedure corresponds to a top-down normal-form
derivation, which almost always uses function application. In the basic case presented
here, composition is only used to provide a uniform analysis of adjuncts. Long-range
dependencies represented in the Penn Treebank by traces such as *T* and *RNR* require
extensions to the basic algorithm, which result in derivations that make use of type-
raising, composition, and (occasionally) substitution rules like those in (5) wherever
syntactically necessary. We defer explanation of these rules until Section 6, which
presents the constructions that motivate them.
4.4 Assigning the Dependency Structure
Finally, we need to obtain the word?word dependencies which approximate the un-
derlying predicate?argument structure. This is done by a bottom-up procedure, which
simply retraces the steps in the CCG derivation that we have now obtained.
364
Hockenmaier and Steedman CCGbank
Figure 2
The CCG derivation with corresponding dependencies and dependency graph for example (9).
All categories in CCGbank, including results and arguments of complex categories,
are associated with a corresponding list of lexical heads. This list can be empty (in the
case of yet uninstantiated arguments of functor categories), or it can consist of one or
more tokens. Lexical categories have one lexical head, the word itself?for example,
He for the first NP, and is for the (S[dcl]\NP)/(S[b]\NP). All dependencies are defined
in terms of the heads of lexical functor categories and of their arguments. In order to
distinguish the slots filled by different arguments, we number the arguments of complex
lexical categories from left to right in the category notation (that is, from innermost to
outermost argument in a purely applicative derivation), for example, (S[ng]\NP1)/NP2, or
((S[b]\NP1)/(S[to]\NP)2)/NP3.
In lexical functor categories such as that of the auxiliary, (S[dcl]\NP)/(S[b]\NP), the
lexical head of all result categories (S[dcl]\NP and S[dcl]) is identical to the lexical head of
the entire category (i.e., is). But in functor categories that represent modifiers, such as the
adverb (S\NP)/(S\NP), the head of the result (the modified verb phrase) comes from the
argument (the unmodified verb phrase). We use indices on the categories to represent
this information: (S\NP)i/(S\NP)i. In CCGbank, modifier categories are easily identified
by the fact that they are of the form X|X or (X|X)| . . . (with | either / or \), where X does not
have any of the features described previously, such as [dcl], [b]. Similarly, determiners
(the) take a noun (N, buck) as argument to form a (non-bare) noun phrase whose lexical
head comes from the noun: NP[nb]i/Ni. Thus, the lexical head of the noun phrase the buck
is buck, not the.
We also use this coindexation mechanism for lexical categories that project non-
local dependencies. For instance, the category of the auxiliary, (S[dcl]\NP)/(S[ng]\NP),
mediates a dependency between the subject (He) and the main verb (passing). Like
all lexical categories of auxiliaries, modals and subject-raising verbs, the head of
the subject NP is coindexed with the head of subject inside the VP argument:
(S[dcl]\NPi)/(S[ng]\NPi). The set of categories that project such dependencies is not ac-
quired automatically, but is given (as a list of category templates) to the algorithm
which creates the actual dependency structures. A complete list of the lexical entries in
sections 02?21 of the Treebank which use this coindexation mechanism to project non-
local dependencies is given in the CCGbank manual (Hockenmaier and Steedman 2005).
We believe that in practice this mechanism is largely correct, even though it is based on
the (fundamentally flawed) assumption that all lexical categories that have the same
365
Computational Linguistics Volume 33, Number 3
syntactic type project the same dependencies. It may be possible to use the indices on
the PRO-null elements (*-1) in the Treebank to identify and resolve ambiguous cases;
we leave this to future research.10
Function application and composition typically result in the instantiation of the
lexical head of an argument of some functor category, and therefore create new de-
pendencies, whereas coordination creates a new category whose lexical head lists are
concatenations of the head lists of the conjuncts.
When the (S[ng]\NP1)/NP2 passing is combined with the NP the buck, the lexical head
of the NP2 is instantiated with buck. Similarly, when the adverb just (S\NP1)/(S\NP)2 is
applied to passing the buck, a dependency between just and passing is created:
?passing,(S[ng]\NP)/NP,2,buck?, ? just,(S\NP)/(S\NP),2,passing? (11)
However, because (S\NP1 )/(S\NP)2 is a modifier category, the head of the resulting
S[ng]\NP is passing, not just (and no dependency is established between just and its NP1).
In the next step, this S[ng]\NP is combined with the auxiliary (S[dcl]\NP1 )/(S[ng]\NP)2.
The NP in the (S[ng]\NP)2 argument of the auxiliary unifies with the (uninstantiated) NP1
argument of passing. Because the NP in the (S[ng]\NP)2 is also coindexed with the subject
NP1 of the auxiliary, the NP of the resulting S[dcl]\NP now has two unfilled dependencies
to the subject NP1 of is and passing. When the entire verb phrase is combined with the
subject, He fills both slots:
?passing,(S[ng]\NP)/NP,1,He?
?is,(S[dcl]\NP)/(S[ng]\NP,1,He? (12)
Figure 2 shows the resulting CCG derivation and the corresponding list of word?
word dependencies for our example sentence. It is the latter structure that we claim
approximates for present purposes the predicate?argument structure or interpretation
of the sentence, and provides the gold standard against which parsers can be evaluated.
4.5 Coordination
In order to deal with coordination, both the tree binarization and the category assign-
ment have to be modified.
In CCGbank, coordination is represented by the following binary rule schemata,
rather than the ternary rule (7)?compare to Steedman (1989):11
a. conj X ? X[conj] (13)
b., X ? X[conj]
c. X X[conj] ? X
In order to obtain this analysis from Treebank trees, a separate node that spans only the
conjuncts and the conjunction or punctuation marks (comma, semicolon) is inserted if
necessary. Identifying the conjuncts often requires a considerable amount of preprocess-
ing. These trees are then transformed into strictly right-branching binary trees. The
10 That our assumption that coindexation can be determined deterministically from the syntactic types
alone is incorrect is most obvious in the case of control verbs: Both promise and persuade have the syntactic
category ((S\NP)/(S[to]\NP))/NP, yet for persuade, the subject of the to-VP should be coindexed with the
object NP, whereas for promise, it should be coindexed with the subject. However, all control verbs that
we identified in CCGbank are object control.
11 As noted earlier, in Baldridge?s (2002) modal version of CCG, conjunctions have categories of the form
(X\X)/X, but without modalities this category leads to overgeneralization.
366
Hockenmaier and Steedman CCGbank
dummy nodes inserted during binarization receive the same category as the conjuncts,
but additionally carry a feature [conj]:
NP
NP
Japan
NP[conj]
,
,
NP
NP
South Korea
NP[conj]
conj
and
NP
Taiwan
An additional modification of the grammar is necessary to deal with ?unlike co-
ordinate phrases? (UCP), namely, coordinate constructions where the conjuncts do not
belong to the same syntactic category:
(VP (VBP are) (14)
(UCP-PRD (ADJP risky)
(CC and)
(PP not in the best interest of the investing public)))
Such constructions are difficult for any formalism. This phenomenon could be handled
elegantly with a feature hierarchy over categories as proposed by Copestake (2002),
Villavicencio (2002), and McConville (2007). Because the induction of such a hierarchy
was beyond the scope of our project, we modify our grammar slightly, and allow the
algorithm to use instantiations of a special coordination rule schema, such as:
conj PP ? S[adj]\NP[conj] (15)
This enables us to analyze the previous example as:
S[dcl]\NP
(S[dcl]\NP)/(S[adj]\NP)
are
S[adj]\NP
S[adj]\NP
risky
S[adj]\NP[conj]
conj
and
PP
not in the best interest...
4.6 Type-Changing Rules for Clausal Adjuncts
In CCG, all language-specific information is associated with the lexical categories of
words. There are many syntactic regularities associated with word classes, however,
which may potentially generate a large number of lexical entries for each item in that
class. One particularly frequent example of this is clausal adjuncts.
Figure 3 illustrates how the basic algorithm described above leads to a prolifera-
tion of adjunct categories. For example, a past participle such as used would receive
a different category in a reduced relative like Figure 3(a) from its standard category
(S[pss]\NP)/(S[to]\NP). As a consequence, modifiers of used would also receive different
categories depending on what occurrence of used they modify. This is undesirable,
because we are only guaranteed to acquire a complete lexicon if we have seen all
participles (and their possible modifiers) in all their possible surface positions. Similar
regularities have been recognized and given a categorial analysis by Carpenter (1992),
who advocates lexical rules to account for the use of predicatives as adjuncts. In a statis-
tical model, the parameters for such lexical rules are difficult to estimate. We therefore
follow the approach of Aone and Wittenburg (1990) and implement these type-changing
367
Computational Linguistics Volume 33, Number 3
Figure 3
Type-changing rules reduce the number of lexical category types required for complex adjuncts.
operations in the derivational syntax, where these generalizations are captured in a
few rules. If these rules apply recursively to their own output, they can generate an
infinite set of category types, leading to a shift in generative power from context-free to
recursively enumerable (Carpenter 1991, 1992). Like Aone and Wittenburg, we therefore
consider only a finite number of instantiations of these type-changing rules, namely
those which arise when we extend the category assignment procedure in the following
way: For any sentential or verb phrase modifier (an adjunct with label S or SBAR with
null complementizer, or VP) to which the original algorithm assigns category X|X, apply
the following type-changing rule (given in bottom-up notation) in reverse:
S$ ? X|X (16)
where S$ is the category that this constituent obtains if it is treated like a head node by
the basic algorithm. S$ has the appropriate verbal features, and can be S\NP or S/NP.
Some of the most common type-changing rules are the following, for various types of
reduced relative modifier:
a. S[pss]\NPi ? NPi\NPi (17)
?workers [exposed to it]?
b. S[adj]\NPi ? NPi\NPi
?a forum [likely to bring attention to the problem]?
c. S[ng]\NPi ? NPi\NPi
?signboards [advertising imported cigarettes]?
d. S[ng]\NPi ? (S\NPi)\(S\NPi )
?become chairman, [succeeding Ian Butler]?
e. S[dcl]/NPi ? NPi\NPi
?the millions of dollars [it generates]?
368
Hockenmaier and Steedman CCGbank
In order to obtain the correct predicate?argument structure, the heads of corresponding
arguments in the input and output category are unified (as indicated by coindexation).
In written English, certain types of NP-extraposition require a comma before or after
the extraposed noun phrase:
Factories booked $236.74 billion in orders in September, [NP nearly the same (18)
as the $236.79 billion in August]
Because any predicative noun phrase could be used in this manner, this construction is
also potentially problematic for the coverage of our grammar and lexicon. However, the
fact that a comma is required allows us to use a small number of binary type-changing
rules (which do not project any dependencies), such as:
NP , ? S/S
, NP ? S\S
, NP ? (S\NP)\(S\NP)
5. Necessary Preprocessing Steps
The translation algorithm presumes that the trees in the Penn Treebank map directly
to the desired CCG derivations. However, this is not always the case, either because
of noise in the Treebank annotation, differences in linguistic analysis, or because CCG,
like any other expressive linguistic formalism, requires information that is not present
in the Treebank analysis. Before translation, a number of preprocessing steps are there-
fore required. Disregarding the most common preprocessing step (the insertion of a
noun level, which is required in virtually all sentences), preprocessing affects almost
43% of all sentences. Here we summarize the most important preprocessing steps for
those constructions that do not involve non-local dependencies. Preprocessing steps
required for constructions involving non-local dependencies (i.e., traces or null elements
in the Treebank) are mentioned in Section 6. Remaining problems are discussed in
Section 7. More detailed and complete descriptions can be found in the CCGbank
manual.
5.1 Dealing with Noise in the Treebank
Annotation errors and inconsistencies in the Treebank affect the quality of any extracted
grammar or lexicon. This is especially true for formalisms with an extended domain
of locality, such as TAG or CCG, where a single elementary tree or lexical category
may contain information that is distributed over a number of distinct phrase-structure
rules.
Part-of-Speech Tagging Errors. Ratnaparkhi (1996) estimates a POS tagging error rate of
3% in the Treebank. The translation algorithm is sensitive to these errors and incon-
sistencies, because POS tagging errors can lead to incorrect categories or to incorrect
features on verbal categories (e.g., when a past participle is wrongly tagged as past
tense). For instance, if a simple past tense form occurs in a verb phrase which itself is
the daughter of a verb phrase whose head is an inflected verb, it is highly likely that
it should be a past participle instead. Using the verb form itself and the surrounding
369
Computational Linguistics Volume 33, Number 3
context, we have attempted to correct such errors automatically. In 7% of all sentences,
our algorithm modifies at least one POS tag.
Quotation Marks. Although not strictly coming under the heading of noise, quotation
marks cause a number of problems for the translation algorithm. Although it is tempting
to analyze them similarly to parentheticals, quotations often span sentence boundaries,
and consequently quotation marks appear to be unbalanced at the sentence level. We
therefore decided to eliminate them during the preprocessing stage.
5.2 Adding Structure to the Treebank Analyses
Unlike a hand-written grammar, the grammar that is implicit in a treebank has to cover
all constructions that occur in the corpus. Expressive formalisms such as CCG provide
explicit analyses that contain detailed linguistic information. For example, CCG deriva-
tions assign a lexical head to every constituent and define explicit functor?argument
relations between constituents. In a phrase-structure grammar, analyses can be much
coarser, and may omit more fine-grained structures if they are assumed to be implicit in
the given analysis. Furthermore, constructions that are difficult to analyze do not need
to be given a detailed analysis. In both cases, the missing information has to be added
before a Treebank tree can be translated into CCG. If the missing structure is implicit
in the Treebank analysis, this step is relatively straightforward, but constructions such
as parentheticals, multiword expressions, and fragments require careful reanalysis in
order to avoid lexical coverage problems and overgeneration.
Detecting Coordination. Although the Treebank does not explicitly indicate coordination,
it can generally be inferred from the presence of a conjunction. However, in list-like
nominal coordinations, the conjuncts are only separated by commas or semicolons, and
may be difficult to distinguish from appositives. There are also a number of verb-phrase
or sentential coordinations in the Treebank where shared arguments or modifiers simply
appear at the same level as conjuncts and the conjunction:12
(VP (VBP meet) (19)
(CC or)
(VBP exceed)
(NP their 1989 spending))
In CCG, the conjuncts and conjunction form a separate constituent. In 1.8% of all sen-
tences, additional preprocessing is necessary to obtain this structure.
Noun Phrases and Quantifier Phrases. In the Penn Treebank, non-recursive noun phrases
have remarkably little internal structure:
(NP (DT the) (NNP Dutch) (VBG publishing) (NN group)) (20)
Some, but not all, of the structure that is required to obtain a linguistically adequate
analysis can be inferred (semi-)automatically. The CCGbank grammar distinguishes
noun phrases, NP, from nouns, N, and treats determiners (the) as functions from nouns
12 Other examples include adverbial expressions such as therefore, so, even that appear between the
conjunction and the second conjunct.
370
Hockenmaier and Steedman CCGbank
to noun phrases (NP[nb]/N). Therefore, we need to insert an additional noun level, which
also includes the adjuncts Dutch and publishing, which receive both the category N/N:
(NP (DT the) (21)
(NOUN (NNP Dutch) (VBG publishing) (NN group)))
However, because nominal compounds in the Treebank have no internal bracketing,
we always assume a right-branching analysis, and are therefore not able to obtain the
correct dependencies for cases such as (lung cancer) deaths.
QPs (?quantifier phrases?) are another type of constituent where the Treebank anno-
tation lacks internal structure:
(QP (IN between) (CD 3) (NN %) (CC and) (CD 5) (NN %)) (22)
We use a number of heuristics to identify the internal structure of these constituents?
for example, to detect conjuncts and prepositions. The above example is then
re-bracketed:
(QP (IN between) (23)
(QP (QP (CD 3) (NN %))
(CC and)
(QP (CD 5) (NN %)))
Fragments. 1.24% of the sentences in the Penn Treebank correspond to or contain frag-
mentary utterances (labeled FRAG), for which no proper analysis could be given:
a. (FRAG (NP The next province) (. ?)) (24)
b. (SBARQ (WRB how) (RP about) (FRAG (NP the New Guinea Fund)) (. ?)
FRAGs are often difficult to analyze, and the annotation is not very consistent. The CCG-
bank manual lists heuristics that we used to infer additional structure. For example, if a
node is labeled FRAG, and there is only one daughter (and potentially an end-of-sentence
punctuation mark), as in the first example, we treat the tree as if it was labeled with the
label of its daughter (NP in this case).
Parentheticals. Parentheticals are insertions that are often enclosed in parentheses, or
preceded by a dash. Unless the parenthetical element itself is of a type that could be a
modifier by itself (e.g., a PP), we assume that the opening parenthesis or first dash takes
the parenthetical element as argument and yields a modifier of the appropriate type:
(PRN (: --) (25)
(NP (NP the third-highest)
(PP-LOC in the developing world)))
This results in the following derivation, which ignores the fact that parentheses are
usually balanced (Nunberg 1990):
NP\NP
(NP\NP)/NP
?
NP
the third-highest in the developing world
371
Computational Linguistics Volume 33, Number 3
We use a similar treatment for other constituents that appear after colons and dashes,
such as sentence-final appositives, or parentheticals that are not marked as PRN. Overall,
these changes affect 8.7% of all sentences.
Multi-Word Expressions. Under the assumption that every constituent has a lexical head
that corresponds to an individual orthographic word, multi-word expressions require
an analysis where one of the items subcategorizes for a specific syntactic type that can
only correspond to the other lexical item. We only attempted an analysis for expres-
sions that are either very frequent or where the multi-word expression has a different
subcategorization behavior from the head word of the expression. This includes some
closed-class items (described in the CCGbank manual), including connectives (e.g., as if,
as though, because of ), comparatives (so ADJ that, too ADJ to, at least/most/ . . . X), mone-
tary expressions, and dates, affecting 23.8% of all sentences.
5.3 Changing the Treebank Analyses
Additionally, there are a number of constructions whose Treebank annotation differs
from the standard CCG analysis for linguistic reasons. This includes small clauses, as
well as pied-piping, subject extraction from embedded sentences and argument cluster
coordination (discussed in Section 6).
Small Clauses. The Treebank treats constructions such as the following as small clauses:
(S (NP-SBJ that volume) (26)
(VP (VBZ makes)
(S (NP-SBJ it)
(NP-PRD the largest supplier...in Europe))))
Pollard and Sag (1992) and Steedman (1996) argue against this analysis on the basis
of extractions like what does the country want forgiven, which suggest that these cases
should rather be treated as involving two complements. We eliminate the small clause,
and transform the trees such that the verb takes both NP children of the small clause
as complements, thereby obtaining the lexical category ((S[dcl]\NP)/NP)/NP for makes.
Because our current grammar treats predicative NPs like ordinary NPs, we are not able
to express the relationship between it and supplier, or between pool and hostage. A correct
analysis would assign a functor category S[nom]\NP (or perhaps NP[prd]\NP) to predica-
tive NP arguments of verbs like makes, not only in these examples, but also in copular
sentences and appositives. The other case where small clauses are used in the Treebank
includes absolute with and though constructions (with the limit in effect). Here, we also
assume that the subordinating conjunction takes the individual constituents in the small
clause as complements, and with obtains therefore the category ((S/S)/PP)/NP. Again, a
predicative analysis of the PP might be desirable in order to express the dependencies
between limit and in effect. Eliminating small clauses affects 8.2% of sentences.
6. Long-Range Dependencies in the Treebank
The treatment of non-local dependencies is one of the most important points of dif-
ference between grammar formalisms. The Treebank uses a large inventory of null
element types and traces, including coindexation to represent long-range dependencies.
372
Hockenmaier and Steedman CCGbank
Because standard Treebank parsers use probabilistic versions of context-free grammar,
they are generally trained and tested on a version of the Treebank in which these null
elements and indices are deleted or ignored, or, in the case of Collin?s (1999) Model 3,
only partially captured. Non-local dependencies are therefore difficult to recover from
their output. In CCG, long-range dependencies are represented without null elements or
traces, and coindexation is restricted to arguments of the same lexical functor category.
Although this mechanism is less expressive than the potentially unrestricted coin-
dexation used in the Treebank, it allows parsers to recover non-anaphoric long-range
dependencies directly, without the need for further postprocessing or trace insertion.
6.1 Passive, Control, Raising and Extraposition
Passive. In the Treebank, the surface subject of a passive sentence is coindexed with a ?
null element in direct object position:
(S (NP-SBJ-1 accountants) (27)
(VP (VBP are)
(RB n?t)
(VP (VBN noted)
(NP-2 (-NONE- *-1))
(PP-CLR as being deeply emotional))))
Our translation algorithm uses the presence of the ? null element to identify passive
mode, but ignores it otherwise, assigning the CCG category S[pss]\NP to noted.13 The
dependency between the subject and the participial is mediated through the lexical
category of the copula, (S[dcl]\NPi )/(S[pss]\NPi) (with the standard semantics ?p?x.px).14
In order to reduce lexical ambiguity and deal with data sparseness, we treat optional
by-PPs which contain the ?logical? subject (NP-LGS) as adjuncts rather than arguments
of the passive participle.15
Here is the resulting CCG derivation, together with its dependency structure:
S[dcl]
NP
accountants
S[dcl]\NP
(S[dcl]\NP)/(S[pss]\NP)
(S[dcl]\NP)/(S[pss]\NP)
are
(S\NP)\(S\NP)
n?t
S[pss]\NP
(S[pss]\NP)/PP
noted
PP
as being deeply emotional
?are, (S[dcl]\NP)/(S[pss]\NP), 1, accountants?, ?are, (S[dcl]\NP)/(S[pss]\NP), 2, noted?
?n?t, (S\NP)\(S\NP), 2, are?, ?noted, (S[pss]\NP)/PP, 1, accountants?, ?noted, (S[pss]\NP)/PP, 2, as?
13 In the case of verbs like pay for, which take a PP argument, the null element appears within the PP. In
order to obtain the correct lexical category of paid, (S[pss]\NP)/(PP/NP), we treat the null element like
an argument of the preposition and percolate it up to the PP level.
14 We assume that the fact that the subject NP argument of passive participials with category S[pss]\NP
identifies the patient, rather than agent, is represented in the semantic interpretation of noted, for
example, ?x.noted?x one?, where one? is simply a placeholder for a bindable argument, like the relational
grammarians? cho?meur relation.
15 Extractions such as Who was he paid by require the by-PP to be treated as an argument, and it would in fact
be better to use a lexical rule to generate (S[pss]\NP)/PP[by] from S[pss]\NP and vice versa.
373
Computational Linguistics Volume 33, Number 3
Infinitival and Participial VPs, Gerunds. In the Treebank, participial phrases, gerunds,
imperatives, and to-VP arguments are annotated as sentences with a ? null subject:
(PP (IN over) (28)
(S-NOM (NP-SBJ (-NONE- *))
(VP (VBG cutting)
(NP capital-gains taxes))))
We treat these like verb phrases (S\NP) with the appropriate feature ([b], [to], [ng], or [pt]),
depending on the part-of-speech tag of the verb.
Control and Raising. CCGbank does not distinguish between control and raising. In the
Treebank, subject-control and subject-raising verbs (e.g., want and seem) also take an S
complement with a null subject that is coindexed with the subject of the main clause:
(S (NP-SBJ-1 Every Japanese under 40) (29)
(VP (VBZ seems)
(S (NP-SBJ (-NONE- *-1))
(VP to be fluent in Beatles lyrics))))
Because an S with an empty subject NP has category S\NP, we obtain the correct syntactic
category (S[dcl]\NPi)/(S[to]\NPi) for seems:
S[dcl]
NP
Every
Japanese
under 40
S[dcl]\NP
(S[dcl]\NP)/(S[to]\NP)
seems
S[to]\NP
(S[to]\NP)/(S[b]\NP)
to
S[b]\NP
(S[b]\NP)/(S[adj]\NP)
be
S[adj]\NP
(S[adj]\NP)/PP
fluent
PP
in Beatles lyrics
We ignore the coindexation in the Treebank, and treat all control verbs as non-arbitrary
control. As indicated by the index i, we assume that all verbs which subcategorize for a
verb phrase complement and take no direct object mediate a dependency between their
subject and their complement. Because the copula and to mediate similar dependencies
between their subjects and complements, but do not fill their own subject dependencies,
Japanese has the following dependencies:
?seems,(S[dcl]\NP)/(S[to]\NP),1,Japanese?, (30)
? fluent,(S[adj]\NP)/PP,1,Japanese?
In the Treebank, object-raising verbs (wants half the debt forgiven) take a small clause argu-
ment with non-empty subject. Following our treatment of small clauses (see Section 5.3)
we modify this tree so that we obtain the lexical category (((S[dcl]\NP)/(S[pss]\NPi))/NPi)
for wanted, which mediates the dependency between debt and forgiven.16
16 The English lexicon also has a very small number of subject control verbs like promise, bearing the
category ((S[dcl]\NPi)/(S[to]\NP))/NPi, which have to be treated specially. However, we did not find
any such subject control verbs in the Wall Street Journal corpus.
374
Hockenmaier and Steedman CCGbank
Extraposition of Appositives. Appositive noun phrases can be extraposed out of a sentence
or verb phrase, resulting in an anaphoric dependency. The Penn Treebank analyzes
these as adverbial small clauses with a coindexed null subject:
(S (S-ADV (NP-SBJ *-1) (NP-PRD No dummies)) (31)
(, ,)
(NP-SBJ-1 the drivers)
(VP pointed out they still had space ...)
We also treat these appositives as sentential modifiers. However, the corresponding
CCG derivation deliberately omits the dependency between dummies and drivers:17
S[dcl]
S/S
NP
NP[nb]/N
No
N
dummies
,
,
S[dcl]
the drivers pointed out...
This derivation uses one of the special binary type-changing rules (see Section 4.6) that
takes into account that these appositives can only occur adjacent to commas.
6.2 Long-Range Dependencies Through Extraction
The Penn Treebank analyzes wh-questions, relative clauses, topicalization of comple-
ments, tough movement, cleft, and parasitic gaps in terms of movement. These construc-
tions are frequent: The entire Treebank contains 16,056 *T* traces, including 8,877 NP
traces, 4,120 S traces, 2,465 ADVP traces, 422 PP traces, and 210 other *T* traces. Sections
02?21 (39,604 sentences) contain 5,288 full subject relative clauses, as well as 459 full and
873 reduced object relative clauses. The dependencies involved in these constructions,
however, are difficult to obtain from the output of standard parsers such as Collins
(1999) or Charniak (2000), and require additional postprocessing that may introduce
further noise and errors. In those cases where the trace corresponds to a ?moved?
argument, the corresponding long-range dependencies can be recovered directly from
the correct CCG derivation.
In the Treebank, the ?moved? constituent is coindexed with a trace (*T*), which is
inserted at the extraction site:
(NP-SBJ (NP Brooks Brothers)) (32)
(, ,)
(SBAR (WHNP-1 (WDT which))
(S (NP-SBJ NNP Marks))
(VP (VBD bought)
(NP (-NONE- *T*-1))
(NP-TMP last year))))))
17 We regard this type of dependency as anaphoric rather than syntactic, on the basis of its immunity to
such syntactic restrictions as subject islands.
375
Computational Linguistics Volume 33, Number 3
CCG has a similarly uniform analysis of these constructions, albeit one that does not
require syntactic movement. In the CCG derivation of the example, the relative pronoun
has the category (NPi\NPi )/(S[dcl]/NPi) whereas the verb bought just bears the standard
transitive category (S[dcl]\NP)/NP. The subject NP and the incomplete VP combine via
type-raising and forward composition into an S[dcl]/NP, which the relative pronoun then
takes as its argument:
NP
NP
Brooks Brothers,
NP\NP
(NP\NP)/(S[dcl]/NP)
which
S[dcl]/NP
S/(S\NP)
NP
Marks
(S[dcl]\NP)/NP
(S[dcl]\NP)/NP
bought
(S\NP)\(S\NP)
last year
The coindexation on the lexical category of the relative pronoun guarantees that the
missing object unifies with the modified NP, and we obtain the desired dependencies:
?which,(NP\NP)/(S[dcl]/NP),1,Brothers?, ?which,(NP\NP)/(S[dcl]/NP),1,bought?, (33)
?bought,(S[dcl]\NP)/NP,1,Marks?, ?bought,(S[dcl]\NP)/NP,2,Brothers?
This analysis of movement in terms of functors over incomplete constituents allows
CCG to use the same category for the verb when its arguments are extracted as when
they are in situ. This includes not only relative clauses and wh-questions, but also pied-
piping, tough movement, topicalization, and clefts.
For our translation algorithm, the *T* traces are essential: They indicate the pres-
ence of a long-range dependency for a particular argument of the verb, and allow us
to use a mechanism similar to GPSG?s slash-feature passing (Gazdar et al 1985), so
that long-range dependencies are represented in the gold-standard dependency struc-
tures of the test and training data. This is crucial to correctly inducing and evaluating
grammars and parsers for any expressive formalism, including TAG, GPSG, HPSG,
LFG, and MPG. A detailed description of this mechanism and of our treatment of other
constructions that use *T* traces can be found in the CCGbank manual.
This algorithm works also if there is a coordinate structure within the relative clause
such that there are two *T* traces (the interest rates they pay *T* on their deposits and charge
*T* on their loans), resulting in the following long-range dependencies:
?pay,((S[dcl]\NP)/PP)/NP,3,rates?, ?charge,((S[dcl]\NP)/PP)/NP,3,rates? (34)
6.2.1 Subject Extraction from Embedded Sentences. In CCG, verbs which take a bare sen-
tential complement have the category ((S\NP)/NPi )/(S\NPi) if the subject of the sentential
complement is extracted (Steedman 1996). In order to obtain these categories from the
Treebank (where the corresponding subject trace is in its canonical position), we assume
376
Hockenmaier and Steedman CCGbank
that the verb takes the VP and the NP argument in reversed order and change the tree
accordingly before translation, resulting in the correct CCG analysis:
NP
NP
the sort
of measures
NP\NP
(NP\NP)/(S[dcl]/NP)
that
S[dcl]/NP
S/(S\NP)
economists
(S[dcl]\NP)/NP
((S[dcl]\NP)/NP)/(S[dcl]\NP)
say
S[dcl]\NP
are necessary
We obtain the following long-range dependencies:
?are,((S[dcl]\NP)/(S[adj]\NP)),1,sort?, ?necessary,S[adj]\NP,1,sort? (35)
Because our grammar does not use Baldridge?s (2002) modalities or Steedman?s (1996)
equivalent rule-based restrictions, which prohibit this category from applying to
in situ NPs, this may lead to overgeneralization. However, such examples are relatively
frequent: There are 97 instances of ((S[.]\NP)/NP)/(S[dcl]\NP) in sections 02?21, and to
omit this category would reduce coverage and recovery of long-range extractions.
6.2.2 Wh-Questions. *T* traces are also used for wh-questions:
(SBARQ (WHNP-1 (WDT Which) (NNS cars)) (36)
(SQ (VBP do)
(NP-SBJ Americans)
(VP (VB favor)
(NP (-NONE- *T*-1))
(ADVP most)
(NP-TMP these days)))
(. ?)))
By percolating the *T* trace up to the SQ-level in a similar way to relative clauses and
treating Which as syntactic head of the WHNP, we obtain the desired CCG analysis:
S[wq]
S[wq]/(S[q]/NP)
(S[wq]/(S[q]/NP))/N
Which
N
cars
S[q]/NP
S[q]/(S[b]\NP)
(S[q]/(S[b]\NP))/NP
do
NP
Americans
(S[b]\NP)/NP
(S[b]\NP)/NP
(S[b]\NP)/NP
favor
(S\NP)\(S\NP)
most
(S\NP)\(S\NP)
these days
We coindex the head of the extracted NP with that of the noun (cars): (S[wq]/(S[q]/
NPi))/Ni, and the subject of do with the subject of its complement ((S[q]/(S[b]\NP1))/NPi)
to obtain the following dependencies:
377
Computational Linguistics Volume 33, Number 3
?do,((S[q]/(S[b]\NP))/NP),1,favor?, ?do,((S[q]/(S[b]\NP))/NP),2,Americans?, (37)
? favor,((S[b]\NP)/NP),2,Americans?, ? favor,((S[b]\NP)/NP),2,cars?
6.2.3 Pied-Piping. *T* traces are also used for pied-piping:
(NP-SBJ (NP the swap) (38)
(, ,)
(SBAR (WHNP-2 (WHNP (NNS details))
(WHPP (IN of) (WHNP (WDT which))))
(S *T*-2 were disclosed)))
In this example, we need to rebracket the Treebank tree so that details of forms a
constituent,18 apply a special rule to assign the category (NP\NP)/NP to the preposition,
and combine it via type-raising and composition with details. This constituent is then
treated as an argument of the relative pronoun:
NP
NP
the swap,
NP\NP
(NP\NP)/(S[dcl]\NP)
NP/NP
NP/(NP\NP)
NP
details
(NP\NP)/NP
of
((NP\NP)/(S[dcl]\NP))\(NP/NP)
which
S[dcl]\NP
were disclosed
With appropriate coindexation ((NP\NPi)/(S[dcl]\NPj))\(NP/NPi)j, we obtain the following
non-local dependencies:19
?of,(NP\NP)/NP,1,details?, ?of,(NP\NP)/NP,2,swap?, (39)
?were,(S[dcl]\NP)/(S[pss]\NP),1,details?, ?disclosed,S[pss]\NP,1,details?
6.2.4 Extraction of Adjuncts. *T* traces can also stand for extracted adjuncts:
(S (SBAR-TMP (WHADVP-1 (WRB When)) (40)
(S (NP-SBJ the stock market)
(VP (VBD dropped)
(ADVP-TMP (-NONE- *T*-1)))))
(S the Mexico fund plunged about 18%))
Because adjuncts generally do not extract unboundedly,20 the corresponding traces
(which account for 20% of all *T* traces) can be ignored by the translation procedure.
18 In cases where the WHPP is not enclosed in another noun phrase (the swap, in which details were disclosed),
no preprocessing is required to obtain the desired CCG derivation.
19 The index j requires that the lexical head of the NP/NP (details of ) is details.
20 Unbounded extractions are only allowed in combination with certain verbs, for example: The year in
which Ms Fazool says that she was born. The current lexicon for these verbs does not support the correct
analysis of such extractions.
378
Hockenmaier and Steedman CCGbank
Instead, the dependency between when and dropped is directly established by the fact
that dropped is the head of the complement S[dcl]:
S[dcl]
S/S
(S/S)/S[dcl]
When
S[dcl]
the stock market dropped
S[dcl]
the Mexico fund plunged...
This results in the following set of dependencies of when:
?When,(S/S)/S[dcl],1,plunged?, ?When,(S/S)/S[dcl],2,dropped? (41)
6.3 Long-Range Dependencies Through Coordination
6.3.1 Right Node Raising. Just as composition and type-raising permit CCG analyses of
wh-extraction, which use the same lexical categories as for in situ complements, they
also provide an analysis of right node raising constructions without introducing any
new lexical categories.
In the Treebank analysis of right node raising, the shared constituent is coindexed
with two *RNR* traces in both of its canonical positions:
(SBARQ (SBARQ (WHNP-5 (WP Who)) (42)
(SQ (NP-SBJ (-NONE- *T*-5))
(VP (VBZ is)
(VP (-NONE- *RNR*-4)))))
(CC and)
(SBARQ (WHNP-6 (WP who))
(SQ (NP-SBJ (-NONE- *T*-6))
(VP (MD should)
(VP (VB be)
(VP (-NONE- *RNR*-4))))))
(VP-4 (VBG making)
(NP the criminal law)
(ADVP-LOC (RB here)))
(. ?))
We need to alter the translation algorithm slightly to deal with *RNR* traces in a man-
ner essentially equivalent to the earlier treatment of *T* wh-traces. Details are in the
CCGbank manual. The CCG derivation for the above example is as follows:
S[wq]
S[wq]/(S[ng]\NP)
S[wq]/(S[ng]\NP)
Who is
S[wq]/(S[ng]\NP)[conj]
conj
and
S[wq]/(S[ng]\NP)
who should be
S[ng]\NP
making the
criminal law
here
The right node raising dependencies are as follows:
?is,(S[dcl]\NP)/(S[ng]\NP),2,making?, (43)
?be,(S[b]\NP)/(S[ng]\NP),2,making?
379
Computational Linguistics Volume 33, Number 3
Our algorithm works also if the shared constituent is an adjunct, or if two conjoined
noun phrases share the same head, which is also annotated with *RNR* traces.
Although there are only 209 sentences with *RNR* traces in the entire Treebank, right
node raising is actually far more frequent, because *RNR* traces are not used when the
conjuncts consist of single verb tokens. The Treebank contains 349 VPs in which a verb
form (/VB/) is immediately followed by a conjunction (CC) and another verb form, and
has an NP sister (without any coindexation or function tag). In CCGbank, sections 02?21
alone contain 444 sentences with verbal or adjectival right node raising.
6.4 Right Node Raising Parasitic Gaps
Right node raising is also marked in the Penn Treebank using *RNR* traces for ?parasitic
gap? constructions such as the following:
a. (VP (VBN held) (44)
(S (NP-SBJ (-NONE- *-2))
(VP (TO to)
(VP (VP (VB cause)
(NP (-NONE- *RNR*-1)))
(PRN (, ,)
(PP (RB rather)
(IN than)
(VP (VB resolve)
(NP (-NONE- *RNR*-1))))
(, ,))
(NP-1 (NN conflict))))))
b. (S (NP-SBJ These first magnitude wines)
(VP (VBD ranged)
(PP-CLR-LOC in price)
(PP-DIR (PP (IN from)
(NP (NP $40)
(NP-ADV (-NONE- *RNR*-1))))
(PP (TO to)
(NP (NP $125)
(NP-ADV (-NONE- *RNR*-1))))
(NP-ADV-1 a bottle))))
These sentences require rules based on the substitution combinator S (Steedman 1996).
Our treatment of right node raising traces deals with the first case correctly, via the back-
ward crossing rule <S?, and allows us to obtain the following correct dependencies:
?cause,((S[b]\NP)/NP),1,system?,1 ?cause,((S[b]\NP)/NP),2,conflict?, (45)
?resolve((S[b]\NP)/NP),1,system?, ?resolve((S[b]\NP)/NP),2,conflict?
The second type of parasitic gap, (44b), would be handled equally correctly by the
forward substitution rule >S, since the PPs are both arguments. Unfortunately, as we
saw in Section 3, the Treebank classifies such PPs as directional adverbials, hence we
380
Hockenmaier and Steedman CCGbank
translate them as adjuncts and lose such examples, of which there are at least three
more, all also involving from and to:
a. Home purchase plans have ranged monthly from 2.9% to 3.7% of respondents. (46)
b. They?ll go from being one of the most leveraged to one of the least leveraged
casino companies.
c. the decline in average gold price realization to $367 from $429 per ounce
As in the case of leftward extraction, including such long-range dependencies in the
dependency structure is crucial to correct induction and evaluation of all expressive
grammar formalisms. Although no leftward-extracting parasitic gaps appear to occur
in the Treebank, our grammar and model predicts examples like the following, and will
cover them when encountered:
Conflict which the system was held to cause, rather than resolve. (47)
6.4.1 Argument Cluster Coordination. If two VPs with the same head are conjoined,
the second verb can be omitted. The Treebank encodes these constructions as a VP-
coordination in which the second VP lacks a verb. The daughters of the second conjunct
are coindexed with the corresponding elements in the first conjunct using a = index:
(VP (VP (VB pay) (48)
(NP HealthVest)
(NP-2 $ 5 million)
(ADVP-TMP-3 right away))
(CC and)
(VP (NP=2 additional amounts)
(PP-TMP=3 in the future))
In the CCG account of this construction, $5 million right away and additional amounts in the
future form constituents (?argument clusters?), which are then coordinated. These con-
stituents are obtained by type-raising and composing the arguments in each conjunct,
yielding a functor which takes a verb with the appropriate category to its left to yield
a verb phrase (Dowty 1988; Steedman 1985). Then the argument clusters are conjoined,
and combine with the verb via function application:21
pay HealthWest $5 million right away and additional amounts in the future
DTV NP NP VP\VP conj NP VP\VP
> >T >T
TV VP\TV VP\TV
<B <B
VP\TV VP\TV
<?>
VP\TV
<
VP
21 We use the following abbreviations: VP for S\NP, TV for transitive (S\NP)/NP, and DTV for ditransitive
((S\NP)/NP/NP).
381
Computational Linguistics Volume 33, Number 3
This construction is one in which the CCGbank head-dependency structure (shown
subsequently) fails to capture the full set of predicate?argument structure relations that
would be implicit in a full logical form:
?pay,((S[b]\NP)/NP)/NP,3,$?, ?pay,((S[b]\NP)/NP)/NP,3,amounts?, (50)
?away,(S\NP)\(S\NP),2,pay?, ?in,((S\NP)\(S\NP))/NP,2,pay?
That is, the dependency structure does not express the fact that right away takes scope
over $5 million and in future over additional amounts, rather than the other way around.
However, this information is included in the full surface-compositional semantic inter-
pretation that is built by the combinatory rules.
Because the Treebank constituent structure does not correspond to the CCG analy-
sis, we need to transform the tree before we can translate it. During preprocessing,
we create a copy of the entire argument cluster which corresponds to the constituent
structure of the CCG analysis. During normal category assignment, we use the first
conjunct in its original form to obtain the correct categories of all constituents. In a later
stage, we use type-raising and composition to combine the constituents within each
argument cluster. For a detailed description of this algorithm and a number of variations
on the original Treebank annotation that we did not attempt to deal with, the interested
reader is referred to the CCGbank manual.
There are 226 instances of argument-cluster coordination in the entire Penn Tree-
bank. The algorithm delivers a correct CCG derivation for 146 of these. Translation fail-
ures are due to the fact that the algorithm can at present only deal with this construction
if the two conjuncts are isomorphic in structure, which is not always the case. This is un-
fortunate, because CCG is particularly suited for this construction. However, we believe
that it would be easier to manually reannotate those sentences that are not at present
translated than to try to adapt the algorithm to deal with all of them individually.
6.4.2 Gapping. For sentential gapping, the Treebank uses annotation similar to argument
cluster coordination:
(S (S (NP-SBJ-1 Only the assistant manager) (51)
(VP (MD can)
(VP (VB talk)
(PP-CLR-2 to the manager))))
(CC and)
(S (NP-SBJ=1 the manager)
(PP-CLR=2 to the general manager)))
This construction cannot be handled with the standard combinatory rules of CCG that
are assumed for English. Instead, Steedman (2000) proposes an analysis of gapping
that uses a unification-based ?decomposition? rule. Categorial decomposition allows
a category type to be split apart into two subparts, and is used to yield an analysis of
gapping that is very similar to that of argument cluster coordination:22
22 It is only the syntactic types that are decomposed or recovered in this way: the corresponding semantic
entities and in particular the interpretation for the gapped verb group can talk must be available from the
left conjunct?s information structure, via anaphora. That is, decomposition adds very little to the
categorial information available from the right conjunct, except to make the syntactic types yield an S.
The real work is done in the semantics.
382
Hockenmaier and Steedman CCGbank
Only the assistant... can talk to the manager and the manager to the general manager
S conj NP PP. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<dcomp <T <T
(S/PP)/NP S\((S/PP)/NP) (S/PP)\((S/PP)/NP) S\(S/PP)
<B
S\((S/PP)/NP)
<?>
S\((S/PP)/NP)
<
S
(52)
Because the derivation is not a tree anymore, and the decomposed constituents do
not correspond to actual constituents in the surface string, this analysis is difficult to
represent in a treebank. The 107 sentences that contain sentential gapping are therefore
omitted in the current version of CCGbank, even though special coordination rules that
mimic the decomposition analysis are conceivable.
6.5 Other Null Elements in the Treebank
Besides the cases discussed herein, the Treebank contains further kinds of null elements,
all of which the algorithm ignores. The null element *ICH* (?Insert Constituent Here?),
which appears 1,240 times, is used for extraposition of modifiers. Like ellipsis, this
is a case of a semantic dependency which we believe to be anaphoric, and therefore
not reflected in the syntactic category. For this reason we treat any constituent that is
coindexed with an *ICH* as an adjunct. The null element *PPA* (?Permanent Predictable
Ambiguity,? 26 occurrences) is used for genuine attachment ambiguities. Since the
Treebank manual states that the actual constituent should be attached at the more likely
attachment site, we chose to ignore any *PPA* null element. Our algorithm also ignores
the null element *?*, which occurs 582 times, and indicates ?a missing predicate or a
piece thereof? (Marcus, Santorini, and Marcinkiewicz 1993). It is used for VP ellipsis,
and can also occur in conjunction with a VP pro-form do (You either believe he can do it or
you don?t *?*), or in comparatives (the total was far higher than expected *?*).23
6.6 The Complete Translation Algorithm
We can now define the complete translation algorithm, including the modifications
necessary to deal with traces and argument clusters:
foreach tree ?:
preprocessTree(?);
determineConstituentTypes(?);
makeBinary(?);
percolateTraces(?);
assignCategories(?);
treatArgumentClusters(?);
cutTracesAndUnaryRules(?);
verifyDerivation(?);
assignDependencies(?);
23 We believe that both conjuncts in the first example are complete sentences which are related
anaphorically. Therefore, the syntactic category of do is S[dcl]\NP, not (S[dcl]\NP)/VP. In the second
example, *?* indicates a semantic argument of expected that we do not reflect in the syntactic category.
383
Computational Linguistics Volume 33, Number 3
The successive steps have the following more detailed character:
preprocessTree: Correct tagging errors, ensure the constituent structure conforms to the
CCG analysis. Eliminate quotes. Create copies of coordinated argument clusters
that correspond to the CCG analysis.
determineConstituentTypes: For each node, determine its constituent type (head, com-
plement, adjunct, conjunction, a constituent that is coindexed with a *RNR* trace,
spurious null element, or argument cluster).
makeBinary: Binarize the tree.
percolateTraces: Determine the CCG category of *T* and *RNR* traces in complement
position, and percolate them up to the appropriate level in the tree.
assignCategories: Assign CCG categories to nodes in the tree, starting at the root
node. Nodes that are coindexed with *RNR* traces receive the category of the
corresponding traces. Argument clusters are ignored in this step.
treatArgumentClusters: Assign categories to argument clusters.
cutTracesAndUnaryRules: Cut out constituents that are not part of the CCG derivation,
such as traces, null elements, and the copy of the first conjunct in argument cluster
coordination. Eliminate resulting unary projections of the form X ? X.
verifyDerivation: Discard those trees for which the algorithm does not produce a valid
CCG derivation. In most cases, this is due to argument cluster coordination that is
not annotated in a way that our algorithm can deal with.
assignDependencies: coindex specific classes of lexical categories to project non-local
dependencies, and generate the word?word dependencies that constitute the un-
derlying predicate?argument structure.
7. Remaining Problems for the Translation Algorithm
In a number of cases, missing structure or a necessary distinction between different
constructions needed to inform the translation is missing, and cannot be inferred de-
terministically from the Treebank analysis without further manual re-annotation. We
discuss these residual problems here, because they are likely to present obstacles to the
extraction of linguistically adequate grammars in any formalism.
7.1 Complement/Adjunct Distinction
Our translation algorithm requires a distinction between complements and adjuncts.
In many cases, this distinction is easily read off the Treebank annotation, but it is in
general an open linguistic problem (McConnell-Ginet 1982). Because the Treebank
annotation does not explicitly distinguish between complements and adjuncts,
researchers typically develop their own heuristics?see, for example, Kinyon and
Prolo (2002). For prepositional phrases, we rely on the -CLR (?closely related?)
function tag to identify complements, although it is unclear whether the Treebank
annotators were able to use this tag consistently. Not all PP arguments seem to have this
function tag, and some PPs that have this tag may have been better considered adjuncts:
a. (VP (VBN replaced) (53)
(NP (-NONE- *-1))
(PP with a different kind of filter))
384
Hockenmaier and Steedman CCGbank
b. (VP (VB redeploy)
(NP their money)
(PP-CLR at lower rates))
For TAG, Chen, Bangalore, and Vijay-Shanker (2006) show that different heuristics yield
grammars that differ significantly in size, coverage, and linguistic adequacy. We have
not attempted such an investigation. In a future version of CCGbank, it may be possible
to follow Shen and Joshi (2005) in using the semantic roles of the Proposition Bank
(Palmer, Gildea, and Kingsbury 2005) to distinguish arguments and adjuncts.
7.2 Phrasal Verbs
Particle-verb constructions are difficult to identify in the Treebank, because particles can
be found as PRT, ADVP-CLR, and ADVP. Therefore, verbs in the CCGbank grammar do not
subcategorize for particles, which are instead treated as adverbial modifiers.
7.3 Compound Nouns
Compound nouns are often inherently ambiguous, and in most cases, the Treebank
does not specify their internal structure:
(NP (JJ only) (JJ French) (NN history) (NNS questions)) (54)
In order to obtain the correct analysis, manual re-annotation would be required. Because
this was not deemed feasible within our project, compound nouns are simply translated
into strictly right-branching binary trees, which yields the correct analysis in some, but
not all, cases. This eschews the computational problem that a grammar for compound
nouns induces all possible binary bracketings, but is linguistically incorrect.
7.4 Coordinate Nouns
A similar problem arises in compound nouns that involve internal coordination:
(NP (NN cotton) (CC and) (NN acetate) (NNS fibers)) (55)
We include the following (linguistically incorrect) rule in our grammar, which yields a
default dependency structure corresponding to N/N coordination:
conj N ? N (56)
This rule allows us to translate the above tree as follows:
N
N/N
cotton
N
conj
and
N
N/N
acetate
N
fibers
385
Computational Linguistics Volume 33, Number 3
7.5 Appositives and Lists
The Treebank markup of NP appositives is indistinguishable from that of NP lists:
(NP (NP Elsevier N.V.) (57)
(, ,)
(NP the Dutch publishing group))
Therefore, our current grammar does not distinguish between appositives and NP
coordination, even though appositives should be analyzed as predicative modifiers.
This leads to a reduction of ambiguity in the grammar, but is semantically incorrect:
NP
NP
Elsevier N.V.
NP[conj]
,
,
NP
the Dutch publishing group
7.6 Lack of Number Agreement
Our current grammar does not implement number agreement (which is, however,
represented in the POS tags). One problem that prevented us from including number
agreement is the above-mentioned inability to distinguish NP lists and appositives.
7.7 Attachment of Noun Phrase Modifiers
In the Penn Treebank, all relative clauses are attached at the noun phrase level. This is se-
mantically undesirable, because a correct interpretation of restrictive relative clauses can
only be obtained if they modify the noun, whereas non-restrictive relative clauses are
noun phrase modifiers. Because this distinction requires manual inspection on a case-
by-case basis, we were unable to modify the Treebank analysis. Thus, all CCGbank rel-
ative pronouns have categories of the form (NPi\NPi )/(S/NPi), rather than (Ni\Ni )/(S/NPi).
This will make life difficult for those trying to provide a Montague-style semantics for
relative modifiers. Like most other problems that we were not able to overcome, this
limitation of the Treebank ultimately reflects the sheer difficulty of providing a consis-
tent and reliable annotation for certain linguistic phenomena, such as modifier scope.
7.7.1 Heavy NP Shift. In English, noun phrase arguments can be shifted to the end of
the sentence if they become too ?heavy.? This construction was studied extensively by
Ross (1967). The CCG analysis (Steedman 1996) uses backward crossed composition to
provide an analysis where brings has its canonical lexical category (VP/PP)/NP:
brings to nearly 50 the number of country funds that are listed in New York....
(VP/PP)/NP VP\(VP/PP) NP
<B?
VP/NP
>
VP
(58)
386
Hockenmaier and Steedman CCGbank
Because the Penn Treebank does not indicate heavy NP shift, the corresponding
CCGbank derivation does not conform to the desired analysis, and requires additional
lexical categories which may lead to incorrect overgeneralizations:24
S[dcl]
NP
The surge
S[dcl]\NP
(S[dcl]\NP)/NP
((S[dcl]\NP)/NP)/PP
brings
PP
to nearly 50
NP
the number of
country funds
that are listed...
This will also be a problem in using the Penn Treebank or CCGbank for any theory of
grammar that treats heavy NP shift as extraction or movement.
8. Coverage, Size, and Evaluation
Here we first examine briefly the coverage of the translation algorithm on the entire
Penn Treebank. Then we examine the CCG grammar and lexicon that are obtained from
CCGbank. Although the grammar of CCG is usually thought of as consisting only of the
combinatory rule schemata such as (3) and (5), we are interested here in the instantiation
of these rules, in which the variables X and Y are bound to values such as S and NP,
because statistical parsers such as Hockenmaier and Steedman?s (2002) or Clark and
Curran?s (2004) are trained on counts of such instantiations. We report our results on
sections 02?21, the standard training set for Penn Treebank parsers, and use section 00
to evaluate coverage of the training set on unseen data. Sections 02?21 contains 39,604
sentences (929,552 words/tokens), whereas section 00 consists of 1,913 sentences (45,422
words/tokens).
8.1 Coverage of the Translation Algorithm
CCGbank contains 48,934 (99.44%) of the 49,208 sentences in the entire Penn Treebank.
The missing 274 sentences could not be automatically translated to CCG. This includes
107 instances of sentential gapping, a construction our algorithm does not cover (see
Section 6.4.2), and 66 instances of non-sentential gapping, or argument-cluster coordi-
nation (see Section 6.4.1).
The remaining translation failures include trees that consist of sequences of NPs that
are not separated by commas, some fragments, and a small number of constructions
involving long-range dependencies, such as wh-extraction, parasitic gaps, or argument
cluster coordinations where the translation did not yield a valid CCG derivation be-
cause a complement had been erroneously identified as an adjunct.
24 Backward crossed composition is also used by Steedman (1996, 2000) and Baldridge (2002) to account for
constraints on preposition stranding in English. Because this rule in its unrestricted form leads to
overgeneralization, Baldridge restricts crossing rules via the ? modality. The current version of CCGbank
does not implement modalities, but because the grammar that is implicit in CCGbank only consists of
particular seen rule instantiations, it may not be affected by such overgeneration problems.
387
Computational Linguistics Volume 33, Number 3
Table 1
The 20 tokens with the highest number of lexical categories and their frequency (sections 02-21).
Word #Cats. Freq. Word #Cats. Freq.
as 130 4237 of 59 22782
is 109 6893 that 55 7951
to 98 22056 -LRB- 52 1140
than 90 1600 not 50 1288
in 79 15085 are 48 3662
? 67 2001 with 47 4214
?s 67 9249 so 47 620
for 66 7912 if 47 808
at 63 4313 on 46 5112
was 61 3875 from 46 4437
8.2 The Lexicon
A CCG lexicon specifies the lexical categories of words, and therefore contains the entire
language-specific grammar. Here, we examine the size and coverage of the lexicon that
consists of the word?category pairs that occur in CCGbank. This lexicon could be used
by any CCG parser, although morphological generalization (which is beyond the scope
of the present paper) and ways to treat unknown words are likely to be necessary to
obtain a more complete lexicon.
Number of Entries. The lexicon extracted from sections 02?21 has 74,669 entries for
44,210 word types (or 929,552 word tokens). Many words have only a small number
of categories, but because a number of frequent closed-class items have a large number
of categories (see Table 1), the expected number of lexical categories per token is 19.2.
Number and Growth of Lexical Category Types. How likely is it that we have observed the
complete inventory of category types in the English language? There are 1,286 lexical
category types in sections 02?21. Figure 4 examines the growth of the number of lexical
category types as a function of the amount of data translated into CCG. The log?log plot
Figure 4
The growth of lexical category types and rule instantiations (sections 02?21).
388
Hockenmaier and Steedman CCGbank
Figure 5
A log?log plot of the rank order and frequency of the lexical category types (left) and
instantiations of combinatory rules (right) in CCGbank.
of the rank order and frequency of the lexical categories in Figure 5 indicates that the
underlying distribution is roughly Zipfian, with a small number of very frequent cate-
gories and a long tail of rare categories. We note 439 categories that occur only once, and
only 556 categories occur five times or more. Inspection suggests that although some of
the category types that occur only once are due to noise or annotation errors, most are
correct and are in fact required for certain constructions. Typical examples of rare but
correct and necessary categories are relative pronouns in pied-piping constructions, or
verbs which take expletive subjects.
Lexical Coverage on Unseen Data. The lexicon extracted from sections 02?21 contains
the necessary categories (as determined by our translation algorithm) for 94.0%
of all tokens in section 00 (42,707 out of 45,422). The missing entries that would be
required for the remaining 6% of tokens fall into two classes: 1,728, or 3.8%, correspond
to completely unknown words that do not appear at all in section 02?21, whereas the
other 2.2% of tokens do appear in the training set, but not with the categories required
in section 00.
All statistical parsers have to be able to accept unknown words in their input,
regardless of the underlying grammar formalism. Typically, frequency information for
rare words in the training data is used to estimate parameters for unknown words (and
when these rare or unknown words are encountered during parsing, additional infor-
mation may be obtained from a POS-tagger (Collins 1997)). However, in a lexicalized
formalism such as CCG, there is the additional problem of missing lexical entries for
known words. Because lexical categories play such an essential role in CCG, even a
small fraction of missing lexical entries can have a significant effect on coverage, since
the parser will not be able to obtain the correct analysis for any sentence that contains
such a token. Hockenmaier and Steedman (2002) show that this lexical coverage prob-
lem does in practice have a significant impact on overall parsing accuracy. However,
because many of the known words with missing entries do not appear very often in
the training data, Hockenmaier (2003a) demonstrates that this problem can be partially
alleviated if the frequency threshold below which rare words are treated as unseen is
set to a much higher value than for standard Treebank parsers. An alternative approach,
advocated by Clark and Curran (2004), is to use a supertagger which predicts lexical
CCG categories in combination with a discriminative parsing model.
389
Computational Linguistics Volume 33, Number 3
8.3 The Syntactic Component
Size and Growth of Instantiated Syntactic Rule Set. Statistical CCG parsers such as
Hockenmaier and Steedman (2002) or Clark and Curran (2004) are trained on counts
of specific instantiations of combinatory rule schemata by category-types. It is therefore
instructive to consider the frequency distribution of these category-instantiated rules.
The grammar for sections 02-21 has 3,262 instantiations of general syntactic com-
binatory rules like those in (3) with specific categories. Of these, 1146 appear only
once, and 2,027 appear less than five times. Although there is some noise, many of the
CCG rules that appear only once are linguistically correct and should be used by the
parser. They include certain instantiations of type-raising, coordination, or punctuation
rules, or rules involved in argument cluster coordinations, pied-piping constructions, or
questions, all of which are rare in the Wall Street Journal. As can be seen from Figure 5,
the distribution of rule frequencies is again roughly Zipfian, with the 10 most frequent
rules accounting for 59.2% of all rule instantiations (159 rules account for 95%; 591 rules
for 99%). The growth of rule instantiations is shown in Figure 4. If function tags are
ignored, the grammar for the corresponding sections of the original Treebank contains
12,409 phrase-structure rules, out of which 6,765 occur only once (Collins 1999). These
rules also follow a Zipfian distribution (Gaizauskas 1995). The fact that both category
types and rule instances are also Zipfian for CCGbank, despite its binarized rules, shows
that the phenomenon is not just due to the Treebank annotation with its very flat rules.
Syntactic Rule Coverage on Unseen Data. Syntactic rule coverage for unseen data is almost
perfect: 51,932 of the 51,984 individual rule instantiations in section 00 (corresponding
to 844 different rule types) have been observed in section 02?21. Out of the 52 missing
rule instantiation tokens (corresponding to 38 rule types, because one rule appears 13
times in one sentence), six involve coordination, and three punctuation. One missing
rule is an instance of substitution (caused by a parasitic gap). Two missing rules are
instances of type-raised argument types combining with a verb of a rare type.
9. Conclusion
This paper has presented an algorithm which translates Penn Treebank phrase-structure
trees into CCG derivations augmented with word?word dependencies that approxi-
mate the underlying predicate?argument structure. In order to eliminate some of the
noise in the original annotation and to obtain linguistically adequate derivations that
conform to the ?correct? analyses proposed in the literature, considerable preprocessing
was necessary. Even though certain mismatches between the syntactic annotations in
the Penn Treebank and the underlying semantics remain, and will affect any similar
attempt to obtain expressive grammars from the Treebank, we believe that CCGbank,
the resulting corpus, will be of use to the computational linguistics community in the
following ways.
CCGbank has already enabled the creation of several robust and accurate
wide-coverage CCG parsers, including Hockenmaier and Steedman (2002), Clark,
Hockenmaier, and Steedman (2002), Hockenmaier (2003b), and Clark and Curran (2004,
2007). Although the construction of full logical forms was beyond the scope of this
project, CCGbank can also be seen as a resource which may enable the automatic con-
struction of full semantic interpretations by wide-coverage parsers. Unlike most Penn
Treebank parsers, such as Collins (1999) or Charniak (2000), these CCGbank parsers re-
turn not only syntactic derivations, but also local and long-range dependencies, includ-
390
Hockenmaier and Steedman CCGbank
ing those that arise under relativization and coordination. Although these dependencies
are only an approximation of the full semantic interpretation that can in principle be
obtained from a CCG, they may prove useful for tasks such as summarization and ques-
tion answering (Clark, Steedman, and Curran 2004). Furthermore, Bos et al (2004) and
Bos (2005) have demonstrated that the output of CCGbank parsers can be successfully
translated into Kamp and Reyle?s (1993) Discourse Representation Theory structures, to
support question answering and the textual entailment task (Bos and Markert 2005).
We hope that these results can be ported to other corpora and other similarly
expressive grammar formalisms. We also hope that our experiences will be useful in
designing guidelines for future treebanks. Although implementational details will differ
across formalisms, similar problems and questions to those that arose in our work will
be encountered in any attempt to extract expressive grammars from annotated corpora.
Because CCGbank preserves most of the linguistic information in the Treebank in a
somewhat less noisy form, we hope that others will find it directly helpful for inducing
grammars and statistical parsing models for other linguistically expressive formalisms.
There are essentially three ways in which this might work.
For lexicalized grammars, it may in some cases be possible to translate the subcat-
egorization frames in the CCG lexicon directly into the target theory. For type-logical
grammars (Moortgat 1988; Morrill 1994; Moot 2003), this is little more than a matter
of transducing the syntactic types for the lexicon into the appropriate notation. For
formalisms like LTAG, the relation is more complex, but the work of Joshi and Kulick
(1996), who ?unfold? CCG categories into TAG elementary trees via partial proof trees,
and Shen and Joshi (2005), who define LTAG ?spines? that resemble categories, suggest
that this is possible. Transduction into HPSG signs is less obvious, but also seems
possible in principle.
A second possibility is to transduce CCGbank itself into a form appropriate to the
target formalism. There seems to be a similar ordering over alternative formalisms from
straightforward to less straightforward for this approach. We would also expect that de-
pendency grammars Mel?c?uk and Pertsov 1987; Hudson 1984) and parsers (McDonald,
Crammer, and Pereira 2005) could be trained and tested with little extra work on the
dependencies in CCGbank.
Finally, we believe that existing methods for translating the Penn Treebank from
scratch into other grammar formalisms will benefit from including preprocessing simi-
lar to that described here.
As some indication of the relative ease with which these techniques transfer, we
offer the observation that the 900K-word German Tiger dependency corpus has recently
been translated into CCG using very similar techniques by Hockenmaier (2006), and
C?ak?c? (2005) has derived a Turkish lexicon from the a similarly preprocessed version of
the METU-Sabanc?? Turkish dependency treebank (Oflazer et al 2003).
A fundamental assumption behind attempts at the automatic translation of syn-
tactically annotated corpora into different grammatical formalisms such as CCG, TAG,
HPSG, or LFG is that the analyses that are captured in the original annotation can
be mapped directly (or, at least, without too much additional work) into the desired
analyses in the target formalism. This can only hold if all constructions that are treated
in a similar manner in the original corpus are also treated in a similar manner in the
target formalism. For the Penn Treebank, our research and the work of others (Xia 1999;
Chen and Vijay-Shanker 2004; Chiang 2000; Cahill et al 2002) have shown that such a
correspondence exists in most cases.
Although the output of most current Treebank parsers is linguistically impover-
ished, the Treebank annotation itself is not. It is precisely the linguistic richness and
391
Computational Linguistics Volume 33, Number 3
detail of the original annotation?in particular, the additional information present in
the null elements and function tags that are ignored by most other parsers?that has
made the creation of CCGbank possible. The translation process would have been
easier if some of the annotation had been more explicit and precise (as in the case of
VP coordination, where preprocessing was required to identify the conjuncts, or in
NP coordination, where we were not able to distinguish NP lists from appositives)
and consistent (most importantly in identifying adjuncts and arguments). An impor-
tant conclusion that follows for the builders of future treebanks is that the tradition
established by the Penn Treebank of including all linguistically relevant dependencies
should be continued, with if anything even closer adherence to semantically informed
linguistic insights into predicate?argument structural relations. Our results also indicate
that corpora of at least the order of magnitude of the Penn Treebank are necessary
to obtain grammars and parsers that are sufficiently expressive, robust, and wide in
coverage to recover these relations completely.
Acknowledgments
We would like to thank our colleagues in
Edinburgh and Philadelphia?in particular
Jason Baldridge, Johan Bos, Stephen Clark,
James Curran, Michael White, Mitch Marcus,
Ann Bies, Martha Palmer, and Aravind
Joshi?for numerous conversations and
feedback on the corpus. We would also like
to thank the Linguistic Data Consortium for
their help in publishing CCGbank, and the
Computational Linguistics reviewers for their
extensive comments on earlier versions of
this paper.
We gratefully acknowledge the financial
support provided by EPSRC grant
GR/M96889. JH also acknowledges support
by an EPSRC studentship and the Edinburgh
Language Technology Group, and by NSF
ITR grant 0205456 at the University of
Pennsylvania. MJS acknowledges support
from the Scottish Enterprise
Edinburgh?Stanford Link (NSF IIS-041628
(R39058)) and EU IST grant PACOPLUS
(FP6-2004-IST-4-27657).
References
Ajdukiewicz, Kazimierz. 1935. Die
syntaktische Konnexita?t. In Storrs McCall,
editor, Polish Logic 1920?1939. Oxford
University Press, Oxford, pages 207?231.
Translated from Studia Philosophica, 1, 1?27.
Aone, Chinatsu and Kent Wittenburg. 1990.
Zero morphemes in Unification-based
Combinatory Categorial Grammar. In
Proceedings of the 28th Annual Meeting of the
Association for Computational Linguistics,
pages 188?193, Pittsburgh, PA.
Bach, Emmon. 1976. An extension of classical
transformational grammar. In Problems in
Linguistic Metatheory: Proceedings of the
1976 Conference at Michigan State
University, pages 183?224, Lansing, MI.
Baldridge, Jason. 2002. Lexically Specified
Derivational Control in Combinatory
Categorial Grammar. Ph.D. thesis, School of
Informatics, University of Edinburgh.
Bar-Hillel, Yehoshua. 1953. A
quasi-arithmetical notation for syntactic
description. Language, 29:47?58.
Blaheta, Don and Eugene Charniak. 2000.
Assigning function tags to parsed text. In
Proceedings of the First Meeting of the North
American Chapter of the Association for
Computational Linguistics, pages 234?240,
Seattle.
Bos, Johan. 2005. Towards wide-coverage
semantic interpretation. In Proceedings
of Sixth International Workshop on
Computational Semantics IWCS-6,
pages 42?53, Tilburg, The Netherlands.
Bos, Johan, Stephen Clark, Mark Steedman,
James R. Curran, and Julia Hockenmaier.
2004. Wide-coverage semantic
representations from a CCG parser. In
Proceedings of the 20th International
Conference on Computational Linguistics
(COLING?04), pages 1240?1246, Geneva,
Switzerland.
Bos, Johan and Katja Markert. 2005.
Recognising textual entailment with
logical inference. In Proceedings of Human
Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing, pages 628?635,
Vancouver, Canada.
Bozsahin, Cem. 1998. Deriving the
predicate-argument structure for a free
word order language. In Proceedings of
COLING-ACL ?98, Montreal, pages 167?173,
Cambridge, MA.
Buszkowski, Wojciech and Gerald Penn.
1990. Categorial grammars determined
392
Hockenmaier and Steedman CCGbank
from linguistic data by unification. Studia
Logica, 49:431?454.
Butt, Miriam, Tracy Holloway King,
Maria-Eugenia Nino, and Frederique
Segond. 1999. A Grammar Writer?s
Cookbook. CSLI Publications, Stanford, CA.
Cahill, Aoife, Michael Burke, Ruth
O?Donovan, Josef Van Genabith, and Andy
Way. 2004. Long-distance dependency
resolution in automatically acquired
wide-coverage PCFG-based LFG
approximations. In Proceedings of the 42nd
Meeting of the Association for Computational
Linguistics (ACL?04), Main Volume,
pages 319?326, Barcelona, Spain.
Cahill, Aoife, Mairead McCarthy, Josef van
Genabith, and Andy Way. 2002. Automatic
annotation of the Penn Treebank with LFG
F-structure information. In LREC 2002
Workshop on Linguistic Knowledge
Acquisition and Representation -
Bootstrapping Annotated Language Data,
pages 8?15, Las Palmas, Spain.
C?ak?c?, Ruken. 2005. Automatic induction of
a CCG grammar for Turkish. In ACL 2005
Student Research Workshop, pages 73?78,
Ann Arbor, MI.
Campbell, Richard. 2004. Using linguistic
principles to recover empty categories. In
Proceedings of the 42nd Meeting of the
Association for Computational Linguistics
(ACL?04), Main Volume, pages 645?652,
Barcelona, Spain.
Carpenter, Bob. 1991. The generative power
of Categorial Grammars and Head-driven
Phrase Structure Grammars with lexical
rules. Computational Linguistics,
17(3):301?314.
Carpenter, Bob. 1992. Categorial grammars,
lexical rules, and the English predicative.
In Robert Levine, editor, Formal
Grammar: Theory and Implementation.
Oxford University Press, Oxford,
chapter 3.
Carroll, John, G. Minnen, and E. Briscoe.
1999. Corpus annotation for parser
evaluation. In Proceedings of the EACL-99
Workshop on Linguistically Interpreted
Corpora (LINC-99), pages 35?41, Bergen,
Norway.
Charniak, Eugene. 2000. A
Maximum-Entropy-inspired parser. In
Proceedings of the First Meeting of the North
American Chapter of the Association for
Computational Linguistics, pages 132?139,
Seattle, WA.
Chen, John, Srinivas Bangalore, and
K. Vijay-Shanker. 2006. Automated
extraction of Tree-Adjoining Grammars
from treebanks. Natural Language
Engineering, 12(03):251?299.
Chen, John and K. Vijay-Shanker. 2004.
Extraction of TAGs from Treebank. In H.
Bunt, J. Caroll, and G. Satta, editors, New
Developments in Parsing Technology.
Springer, Berlin, pages 73?90.
Chiang, David. 2000. Statistical parsing with
an automatically extracted Tree Adjoining
Grammar. In Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics, pages 456?463, Hong Kong.
Clark, Stephen and James R. Curran. 2004.
Parsing the WSJ using CCG and log-linear
models. In Proceedings of the 42nd Annual
Meeting of the Association for Computational
Linguistics, pages 103?110, Barcelona,
Spain.
Clark, Stephen and James R. Curran. 2007.
Formalism-Independent Parser Evaluation
with CCG and DepBank. In Proceedings of
the 45th Annual Meeting of the Association of
Computational Linguistics, pages 248?255,
Prague, Czech Republic.
Clark, Stephen, Julia Hockenmaier, and
Mark Steedman. 2002. Building deep
dependency structures using a
wide-coverage CCG parser. In Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics,
pages 327?334, Philadelphia, PA.
Clark, Stephen, Mark Steedman, and
James R. Curran. 2004. Object-extraction
and question-parsing using CCG. In
Proceedings of the 2004 Conference on
Empirical Methods in Natural Language
Processing (EMNLP?04), pages 111?118,
Barcelona, Spain.
Collins, Michael. 1997. Three generative
lexicalized models for statistical parsing.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics,
pages 16?23, Madrid, Spain.
Collins, Michael. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, Computer and Information Science,
University of Pennsylvania.
Copestake, Ann. 2002. Implementing Typed
Feature Structure Grammars. CSLI
Publications, Stanford, CA.
Copestake, Ann and Dan Flickinger. 2000.
An open-source grammar development
environment and broad-coverage English
grammar using HPSG. In Proceedings of the
Second International Conference on Language
Resources and Evaluation (LREC),
pages 591?600, Athens, Greece.
Curry, Haskell B. and Robert Feys. 1958.
Combinatory Logic, volume I.
393
Computational Linguistics Volume 33, Number 3
North-Holland, Amsterdam.
Dienes, Peter and Amit Dubey. 2003a.
Antecedent recovery: Experiments with a
trace tagger. In Proceedings of the 2003
Conference on Empirical Methods in Natural
Language Processing (EMNLP?03),
pages 33?40, Sapporo, Japan.
Dienes, Peter and Amit Dubey. 2003b. Deep
syntactic processing by combining shallow
methods. In Proceedings of the 41st Annual
Meeting of the Association for Computational
Linguistics, pages 431?438, Sapporo, Japan.
Dowty, David. 1978. Governed
transformations as lexical rules in a
Montague grammar. Linguistic Inquiry,
9:393?426.
Dowty, David. 1988. Type-raising, functional
composition, and non-constituent
coordination. In Richard T. Oehrle,
Emmon Bach, and Deirdre Wheeler,
editors, Categorial Grammars and Natural
Language Structures. Reidel, Dordrecht,
pages 153?198.
Eisner, Jason. 1996. Efficient normal-form
parsing for Combinatory Categorial
Grammar. In Proceedings of the 34th Annual
Meeting of the Association for Computational
Linguistics, pages 79?86, Santa Cruz, CA.
Gabbard, Ryan, Seth Kulick, and Mitchell
Marcus. 2006. Fully parsing the Penn
Treebank. In Proceedings of the Human
Language Technology Conference of the
NAACL, Main Conference, pages 184?191,
New York, NY.
Gaizauskas, Robert. 1995. Investigations into
the grammar underlying the Penn
Treebank. Technical Report CS-95-25,
Department of Computer Science,
University of Sheffield.
Gazdar, Gerald, Ewan Klein, Geoffrey K.
Pullum, and Ivan A. Sag. 1985. Generalised
Phrase Structure Grammar. Blackwell,
Oxford.
Henderson, James. 2004. Discriminative
training of a neural network statistical
parser. In Proceedings of the 42nd Meeting of
the Association for Computational Linguistics
(ACL?04), Main Volume, pages 95?102,
Barcelona, Spain.
Hepple, Mark. 1990. The Grammar and
Processing of Order and Dependency: a
Categorial Aproach. Ph.D. thesis, University
of Edinburgh.
Hepple, Mark and Glyn Morrill. 1989.
Parsing and derivational equivalence. In
Proceedings of the Fourth Conference of the
European Chapter of the Association for
Computational Linguistics, pages 10?18,
Manchester, UK.
Hockenmaier, Julia. 2003a. Data and Models
for Statistical Parsing with Combinatory
Categorial Grammar. Ph.D. thesis,
School of Informatics, University of
Edinburgh.
Hockenmaier, Julia. 2003b. Parsing with
generative models of predicate-argument
structure. In Proceedings of the 41st Annual
Meeting of the ACL, pages 359?366,
Sapporo, Japan.
Hockenmaier, Julia. 2006. Creating a
CCGbank and a wide-coverage CCG
lexicon for German. In Proceedings of the
21st International Conference on
Computational Linguistics and 44th Annual
Meeting of the Association for Computational
Linguistics, pages 505?512, Sydney,
Australia.
Hockenmaier, Julia, Gann Bierner, and Jason
Baldridge. 2004. Extending the coverage of
a CCG System. Research in Language and
Computation, 2:165?208.
Hockenmaier, Julia and Mark Steedman.
2002. Generative models for statistical
parsing with Combinatory Categorial
Grammar. In Proceedings of the 40th Annual
Meeting of the Association for Computational
Linguistics, pages 335?342, Philadelphia,
PA.
Hockenmaier, Julia and Mark Steedman.
2005. CCGbank: Users? Manual. Department
of Computer and Information Science
Technical Report MS-CIS-05-09. University
of Pennsylvania, Philadelphia, PA.
Hoffman, Beryl. 1995. Computational Analysis
of the Syntax and Interpretation of ?Free?
Word-order in Turkish. Ph.D. thesis,
University of Pennsylvania. IRCS
Report 95-17.
Hudson, Richard. 1984. Word Grammar.
Blackwell, Oxford.
Jacobson, Pauline. 1992. Flexible Categorial
grammars: Questions and prospects. In
Robert Levine, editor, Formal Grammar.
Oxford University Press, Oxford,
pages 129?167.
Johnson, Mark. 2002. A simple
pattern-matching algorithm for recovering
empty nodes and their antecedents. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics,
pages 136?143, Philadelphia, PA.
Joshi, Aravind and Seth Kulick. 1996. Partial
proof trees as building blocks for a
Categorial grammar. Linguistics and
Philosophy, 20(6):637?667.
Joshi, Aravind and Yves Schabes. 1992. Tree
adjoining grammars and lexicalized
grammars. In M. Nivat and M. Podelski,
394
Hockenmaier and Steedman CCGbank
editors, Tree Automata and Languages.
North-Holland, pages 409?432.
Kamp, Hans and Uwe Reyle. 1993. From
Discourse to Logic. Kluwer, Dordrecht.
Kang, Beom-Mo. 1995. On the treatment of
complex predicates in categorial grammar.
Linguistics and Philosophy, 18:61?81.
Kaplan, Ronald and Joan Bresnan. 1982.
Lexical-Functional Grammar: A formal
system for grammatical representation. In
The Mental Representation of Grammatical
Relations. MIT Press, Cambridge, MA,
pages 173?281.
Kinyon, Alexandra and Carlos Prolo. 2002.
Identifying verb arguments and their
syntactic function in the Penn Treebank. In
Proceedings of the Third International
Conference on Language Resources and
Evaluation (LREC), pages 1982?1987,
Las Palmas, Spain.
Komagata, Nobo. 1999. Information Structure
in Texts: A Computational Analysis of
Contextual Appropriateness in English and
Japanese. Ph.D. thesis, Computer and
Information Science, University of
Pennsylvania.
Ko?nig, Esther. 1994. A hypothetical
reasoning algorithm for linguistic
analysis. Journal of Logic and Computation,
4:1?19.
Lin, Dekang. 1998. Dependency-based
evaluation of MINIPAR. In Workshop on the
Evaluation of Parsing Systems, Granada,
Spain.
Magerman, David M. 1994. Natural Language
Parsing as Statistical Pattern Recognition.
Ph.D. thesis, Department of Computer
Science, Stanford University.
Marcus, M., G. Kim, M. A. Marcinkiewicz,
R. MacIntyre, A. Bies, M. Ferguson,
K. Katz, and B. Schasberger. 1994. The
Penn Treebank: Annotating predicate?
argument structure. In Proceedings of the
Human Language Technology Workshop,
pages 114?119, Princeton, NJ.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19:313?330.
McConnell-Ginet, Sally. 1982. Adverbs and
logical form. Language, 58:144?184.
McConville, Mark. 2007. Inheritance and the
Categorial Lexicon. Ph.D. thesis, University
of Edinburgh.
McDonald, Ryan, Koby Crammer, and
Fernando Pereira. 2005. Online
large-margin training of dependency
parsers. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, pages 91?98, Ann Arbor, MI.
Mel?c?uk, Igor and Nicolaj Pertsov. 1987.
Surface Syntax of English. John Benjamins,
Amsterdam.
Merlo, Paola and Gabriele Musillo. 2005.
Accurate function parsing. In Proceedings of
Human Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing, pages 620?627,
Vancouver, Canada.
Miyao, Yusuke, Takashi Ninomiya, and
Jun?ichi Tsujii. 2004. Corpus-oriented
grammar development for acquiring a
Head-driven Phrase Structure Grammar
from the Penn Treebank. In Proceedings of
the First International Joint Conference on
Natural Language Processing (IJCNLP-04),
pages 684?693, Hainan Island, China.
Miyao, Yusuke and Jun?ichi Tsujii. 2005.
Probabilistic disambiguation models for
wide-coverage HPSG parsing. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics,
pages 83?90, Ann Arbor, MI.
Moortgat, Michael. 1988. Categorial
Investigations. Ph.D. thesis, Universiteit
van Amsterdam. Published by Foris,
Dordrecht, 1989.
Moortgat, Michael. 1997. Categorial type
logics. In Johan van Benthem and Alice ter
Meulen, editors, Handbook of Logic and
Language. North Holland, Amsterdam,
pages 93?177.
Moot, Richard. 2002. Parsing corpus-induced
type-logical grammars. In Proceedings of the
CoLogNet/ElsNet Workshop on Linguistic
Corpora and Logic Based Grammar
Formalisms, pages 70?85, Utrecht,
Netherlands.
Morrill, Glyn. 1994. Type-Logical Grammar.
Kluwer, Dordrecht.
Nunberg, Geoffrey. 1990. The Linguistics
of Punctuation. Number 18 in CSLI
Lecture Notes. CSLI Publications,
Stanford, CA.
O?Donovan, Ruth, Michael Burke, Aoife
Cahill, Josef van Genabith, and Andy Way.
2005. Large-scale induction and evaluation
of lexical resources from the Penn-II and
Penn-III Treebanks. Computational
Linguistics, 31(3):329?365.
Oflazer, Kemal, Bilge Say, Dilek Zeynep
Hakkani-Tu?r, and Go?khan Tu?r. 2003.
Building a Turkish treebank. In Anne
Abeille?, editor, Treebanks. Building and
using syntactically annotated corpora. Kluwer
Academic Publishers, Amsterdam,
pages 261?277.
395
Computational Linguistics Volume 33, Number 3
Osborne, Miles and Ted Briscoe. 1998.
Learning Stochastic Categorial Grammars.
In Proceedings of CoNLL97: Computational
Natural Language Learning, pages 80?87,
Somerset, NJ.
Palmer, Martha, Daniel Gildea, and Paul
Kingsbury. 2005. The Proposition
Bank: An annotated corpus of semantic
roles. Computational Linguistics,
31(1):71?106.
Pollard, Carl and Ivan Sag. 1992. Anaphors
in English and the scope of binding theory.
Linguistic Inquiry, 23:261?303.
Pollard, Carl and Ivan Sag. 1994. Head Driven
Phrase Structure Grammar. CSLI/Chicago
University Press, Chicago, IL.
Ratnaparkhi, Adwait. 1996. A maximum
entropy part-of-speech tagger. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing,
pages 133?142, Philadelphia, PA.
Ratnaparkhi, Adwait. 1998. Maximum
Entropy Models for Natural Language
Ambiguity Resolution. Ph.D. thesis,
Computer and Information Science,
University of Pennsylvania.
Rooth, Mats and Barbara Partee. 1982.
Conjunction, type-ambiguity, and
wide-scope ?or?. In Proceedings of the First
West Coast Conference on Formal Linguistics,
pages 353?362, Stanford CA.
Ross, John Robert. 1967. Constraints on
Variables in Syntax. Ph.D. thesis, MIT.
Published as ?Infinite Syntax!?, Ablex,
Norton, NJ. 1986.
Shen, Libin and Aravind Joshi. 2005.
Building an LTAG Treebank. Technical
Report MS-CIS-05-15, CIS, University of
Pennsylvania, Philadelphia, PA.
Stabler, Edward. 2004. Varieties of crossing
dependencies: Structure-dependence and
mild context sensitivity. Cognitive Science,
28:699?720.
Steedman, Mark. 1985. Dependency and
coordination in the grammar of Dutch and
English. Language, 61:523?568.
Steedman, Mark. 1989. Constituency and
coordination in a Combinatory grammar.
In Mark Baltin and Anthony Kroch,
editors, New Conceptions of Phrase Structure.
Chicago University Press, Chicago IL,
pages 201?306.
Steedman, Mark. 1996. Surface Structure and
Interpretation. MIT Press, Cambridge, MA.
Steedman, Mark. 2000. The Syntactic Process.
MIT Press, Cambridge, MA.
Steedman, Mark and Jason Baldridge. 2006.
Combinatory Categorial Grammar. In
Keith Brown, editor, Encyclopedia of
Language and Linguistics, volume 2.
Elsevier, Oxford, 2nd edition,
pages 610?622.
Trechsel, Frank. 2000. A CCG approach to
Tzotzil pied-piping. Natural Language and
Linguistic Theory, 18:611?663.
Villavicencio, Aline. 2002. The Acquisition
of a Unification-Based Generalised Categorial
Grammar. Ph.D. thesis, Computer
Laboratory, University of Cambridge.
White, Michael. 2006. Efficient Realization of
Coordinate Structures in Combinatory
Categorial Grammar. Research on Language
and Computation, 4(1):39?75.
White, Michael and Jason Baldridge. 2003.
Adapting Chart Realization to CCG. In
Proceedings of the 9th European Workshop
on Natural Language Generation,
pages 119?126, Budapest, Hungary.
Wittenburg, Kent and Robert Wall. 1991.
Parsing with categorial grammar in
predictive normal form. In Masaru Tomita,
editor, Current Issues in Parsing Technology.
Kluwer, Dordrecht, pages 65?83. Revised
selected papers from International
Workshop on Parsing Technology (IWPT)
1989, Carnegie Mellon University.
Xia, Fei. 1999. Extracting Tree Adjoining
Grammars from bracketed corpora. In
Proceedings of the 5th Natural Language
Processing Pacific Rim Symposium
(NLPRS-99), pages 398?403, Beijing, China.
Xia, Fei. 2001. Automatic Grammar Generation
from two different perspectives. Ph.D. thesis,
University of Pennsylvania.
Xia, Fei, Martha Palmer, and Aravind Joshi.
2000. A uniform method of grammar
extraction and its applications. In
Proceedings of the 2000 Conference on
Empirical Methods in Natural Language
Processing, pages 53?62, Hong Kong.
XTAG-group. 1999. A Lexicalized Tree
Adjoining Grammar for English. Technical
Report IRCS-98-18, University of
Pennsylvania.
396
Tutorial Abstracts of ACL 2010, page 1,
Uppsala, Sweden, 11 July 2010. c?2010 Association for Computational Linguistics
Wide-coverage NLP with Linguistically Expressive Grammars
Julia Hockenmaier
Department of Computer Science,
University of Illinois
juliahmr@illinois.edu
Yusuke Miyao
National Institute of Informatics
yusuke@nii.ac.jp
Josef van Genabith
Centre for Next Generation Localisation,
School of Computing,
Dublin City University
josef@computing.dcu.ie
1 Introduction
In recent years, there has been a lot of research
on wide-coverage statistical natural language
processing with linguistically expressive gram-
mars such as Combinatory Categorial Grammars
(CCG), Head-driven Phrase-Structure Grammars
(HPSG), Lexical-Functional Grammars (LFG)
and Tree-Adjoining Grammars (TAG). But al-
though many young researchers in natural lan-
guage processing are very well trained in machine
learning and statistical methods, they often lack
the necessary background to understand the lin-
guistic motivation behind these formalisms. Fur-
thermore, in many linguistics departments, syntax
is still taught from a purely Chomskian perspec-
tive. Additionally, research on these formalisms
often takes place within tightly-knit, formalism-
specific subcommunities. It is therefore often dif-
ficult for outsiders as well as experts to grasp the
commonalities of and differences between these
formalisms.
2 Content Overview
This tutorial overviews basic ideas of TAG/
CCG/LFG/HPSG, and provides attendees with a
comparison of these formalisms from a linguis-
tic and computational point of view. We start
from stating the motivation behind using these ex-
pressive grammar formalisms for NLP, contrast-
ing them with shallow formalisms like context-
free grammars. We introduce a common set of
examples illustrating various linguistic construc-
tions that elude context-free grammars, and reuse
them when introducing each formalism: bounded
and unbounded non-local dependencies that arise
through extraction and coordination, scrambling,
mappings to meaning representations, etc. In the
second half of the tutorial, we explain two key
technologies for wide-coverage NLP with these
grammar formalisms: grammar acquisition and
parsing models. Finally, we show NLP applica-
tions where these expressive grammar formalisms
provide additional benefits.
3 Tutorial Outline
1. Introduction: Why expressive grammars
2. Introduction to TAG
3. Introduction to CCG
4. Introduction to LFG
5. Introduction to HPSG
6. Inducing expressive grammars from corpora
7. Wide-coverage parsing with expressive
grammars
8. Applications
9. Summary
References
Aoife Cahill, Michael Burke, Ruth O?Donovan, Stefan
Riezler, Josef van Genabith and Andy Way. 2008.
Wide-Coverage Deep Statistical Parsing using Au-
tomatic Dependency Structure Annotation. Compu-
tational Linguistics, 34(1). pp.81-124, MIT Press.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature For-
est Models for Probabilistic HPSG Parsing. Compu-
tational Linguistics, 34(1). pp.35-80, MIT Press.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
Computational Linguistics, 33(3). pp.355-396, MIT
Press.
1
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 135?139,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Beefmoves: Dissemination, Diversity, and Dynamics of English Borrowings
in a German Hip Hop Forum
Matt Garley
Department of Linguistics
University of Illinois
707 S Mathews Avenue
Urbana, IL 61801, USA
mgarley2@illinois.edu
Julia Hockenmaier
Department of Computer Science
University of Illinois
201 N Goodwin Avenue
Urbana, IL 61801, USA
juliahmr@illinois.edu
Abstract
We investigate how novel English-derived
words (anglicisms) are used in a German-
language Internet hip hop forum, and what
factors contribute to their uptake.
1 Introduction
Because English has established itself as something
of a global lingua franca, many languages are cur-
rently undergoing a process of introducing new loan-
words borrowed from English. However, while the
motivations for borrowing are well studied, includ-
ing e.g. the need to express concepts that do not have
corresponding expressions in the recipient language,
and the social prestige associated with the other lan-
guage (Hock and Joseph, 1996), the dynamics of this
process are poorly understood. While mainstream
political debates often frame borrowing as evidence
of cultural or linguistic decline, it is particularly per-
vasive in youth culture, which is often heavily influ-
enced by North American trends. In many countries
around the globe, hip hop fans form communities in
which novel, creative uses of English are highly val-
ued (Pennycook, 2007), indicative of group mem-
bership, and relatively frequent. We therefore study
which factors contribute to the uptake of (hip hop-
related) anglicisms in an online community of Ger-
man hip hop fans over a span of 11 years.
2 The MZEE and Covo corpora
We collected a ?12.5M word corpus (MZEE) of fo-
rum discussions from March 2000 to March 2011
on the German hip hop portal MZEE.com. A man-
ual analysis of 10K words identified 8.2% of the
tokens as anglicisms, contrasting with only 1.1%
anglicisms in a major German news magazine, the
Spiegel (Onysko, 2007, p.114). These anglicisms
include uninflected English stems (e.g., battle, rap-
per, flow) as well as English stems with English in-
flection (e.g., battled, rappers, flows), English stems
with German inflection (e.g., gebattlet, rappern,
flowen ?battled, rappers, to flow?), and English stems
with German derivational affixes (e.g., battlema?ssig,
rapperische, flowendere ?battle-related, rapper-like,
more flowing?), as well as compounds with one
or more English parts (e.g., battleraporientierter,
hiphopgangstaghettorapper, maschinengewehrflow
?someone oriented towards battle-rap, hip hop-
gangsta-ghetto-rapper, machinegun flow?). We also
collected a ?20M word corpus (Covo) of English-
language hip hop discussion (May 2003 - November
2011) from forums at ProjectCovo.com.
3 Identification of novel anglicisms
In order to identify novel anglicisms in the
MZEE corpus, we have developed a classifier
which can identify anglicism candidates, includ-
ing those which incorporate German material (e.g.,
mo?chtegerngangsterstyle ?wannabe gangster style?),
with very high recall. Since we are not interested in
well-established anglicisms (e.g., Baby, OK), non-
English words, or placenames, our goal is quite
different from the standard language identification
problem, including Alex (2008)?s inclusion classi-
fier, which sought to identify ?foreign words? in
general, including internationalisms, homographic
135
Baseline n-gram classifier accuracy for n=
1 2 3 4 5 6 7
87.54 94.80 97.74 99.35 99.85 99.96 99.98
Figure 1: Accuracy of the baseline classifer on word lists;
10-fold CV; std. deviations ? 0.02 for all cases
words, and non-German placenames, but ignored
hybrid/bilingual compounds and English words with
German morphology during evaluation. Our final
system consists of a binary classifier augmented
with dictionary lookup for known words and two
routines to deal with German morphology (affixa-
tion and compounding).
The baseline classifier We used MALLET (Mc-
Callum, 2002) to train a maximum entropy classi-
fier, using character 1- through 6-grams (including
word boundaries) as features. Since we could not
manually annotate a large portion of the MZEE cor-
pus, the training data consisted of the disjoint sub-
sets of the English and German CELEX wordlists
(Baayen et al, 1995), as well as the words used
in Covo (to obtain coverage of hip hop English).
We tested the classifier using 10-fold cross valida-
tion on the training data and on a manually anno-
tated development set of 10K consecutive tokens
from MZEE. All data was lowercased (this improved
performance). We excluded from both data sets
4,156 words shared by the CELEX wordlists (such
as Greek/Latin loanwoards common to both lan-
guages and homographs such as hat), 100 common
German and 50 common English stop words, all 3-
character words without vowels and 1,019 hip hop
artists/label names, which reduced the development
set from 10K tokens, or 3,380 distinct types, to 4,651
tokens and 2,741 types.
Affix-stripping Since German is a moderately in-
flected language, anglicisms are often ?hidden? by
German morphology: in geflowt ?flowed?, the En-
glish stem flow takes German participial affixes. We
therefore included a template-based affix-stripping
preprocessing step, removing common German af-
fixes before feature extraction. Because of the
possibility of multiple prefixation or suffixation
(e.g. rum-ge-battle (?battling around?) or deep-er-en
(?deeper?)), we stripped sequences of two prefixes
and/or three suffixes. Our list of affixes was built
Precision
All tokens All types OOVtyp.
Affix Comp. nodict dict nodict dict nodict
no no 0.63 0.64 0.58 0.62 0.26
no yes 0.66 0.69 0.58 0.62 0.27
yes no 0.59 0.69 0.60 0.66 0.29
yes yes 0.60 0.70 0.60 0.67 0.32
Table 1: Type- and token-based precision at recall=95
from commonly-affixed stems in the MZEE corpus
and a German grammar (Fagan, 2009).
Compound-cutting Nominal and adjectival com-
pounding is common in German, and loanword
compounds are commonly found in MZEE:
(1) a. chart|tauglich (?suitable for the charts?)
b. flow|maschine|ma?ssig (?like a flow ma-
chine?)
c. Rap|vollpfosten (?rap dumbasses?)
Since these contain features that are highly indica-
tive of German (e.g. -lich#, a?, and pf ), we devised a
compound-cutting procedure for words over length
l (=7): if the word is initially classified as German,
it is divided several ways according to the param-
eters n (=3), the number of cuts in each direction
from the center, and m (=2), the minimum length of
each part. Both halves are classified separately, and
if the maximum anglicism classifier score out of all
splits exceeds a target confidence c (=0.7), the orig-
inal word is labeled a candidate anglicism. Parame-
ter values were optimized on a subset of compounds
from the development set.
Dictionary classification When applying the clas-
sifier to the MZEE corpus, words which occur ex-
clusively in one of the German and English CELEX
wordlists are automatically classified as such. This
improved classifier results over tokens and types, as
seen in Table 1 in the comparison of token and type
precision for the dict/nodict conditions.
Evaluation We evaluated our system by adjusting
the classifier threshold to obtain a recall level of 95%
or higher on anglicism tokens in the development set
(see Table 1). The final classifier achieved a per-
token precision of 70% (per type: 67%) at 95% re-
call, a gain of 7% (9%) over the baseline.
Our system identified 1,415 anglicism candidate
types with a corpus frequency of 100 or greater, out
136
of which we identified 851 (57.5%) for further in-
vestigation; 441 (31.1%) were either established an-
glicisms, place names, artist names, and other loan-
words, and 123 (8.7%) were German words.
4 Predicting the fate of anglicisms
We examine here factors hypothesized to play a role
in the establishment (or decline) of anglicisms.
Frequency in the English Covo corpus We first
examine whether a word?s frequency in the English-
speaking hip hop community influences whether
it becomes more frequently used in the German
hip hop community. We aligned four large (>1M
words each) 12-month time windows of the Covo
and MZEE corpora, spanning the period 11-2003
through 11-2007. We used the 851 most fre-
quent anglicisms identified in our system to find
106 English stems commonly used in German
anglicisms, and compute their relative frequency
(aggregated over all word forms) in each Covo
and MZEE time window. We then measure cor-
relation coefficients r between the frequency of
a stem in Covo at time Tt, fEt (stem), and the
change in log frequency of the corresponding an-
glicisms in MZEE between Tt and a later time Tu,
? log10 f
G
t:u(w) = log10 f
G
u (w) ? log10 f
G
t (w),
as well as the corresponding p-values, and coeffi-
cients of determination R2 (Table 2). There is a sig-
nificant positive correlation between the variables,
especially for change over a two-year time span.
Covo log10 ft(stem) vs. MZEE ? log10 ft:u(stem)
r p t R2 N
u = t + 1 year 0.1891 0.0007 3.423 3.6% 318
u = t + 2 year 0.3130 0.0001 4.775 9.8% 212
u = t + 3 year 0.2327 0.0164 2.440 5.4% 106
Table 2: Correlations between stem frequency in Covo
during year t and frequency change in MZEE between t
and year u = t + i
Initial frequency and dissemination in MZEE
In studying the fate of all words in two En-
glish Usenet corpora, Altmann, Pierrehumbert and
Motter (2011, p.5) found that the measures DU
(dissemination over users) and DT (dissemina-
tion over threads) predict changes in word fre-
quency (? log10 f ) better than initial word fre-
Figure 2: Correlation coefficient comparison of DU , DT ,
log10 f with ? log10 f
quency (log10 f ). D
U = Uw
U?w
is defined as the ratio
of the actual number of users of word w (Uw) over
the expected number of users of w (U?w), and DT =
Tw
T?w
is calculated analogously fo the actual/expected
number of threads in which w is used. U?w and T?w
are estimated from a bag-of-words model approxi-
mating a Poisson process.
We apply Altmann et al?s model to study the dif-
ference in word dynamics between anglicisms and
native words. Since we are not able to lemma-
tize the entire MZEE corpus, this study uses the
851 most common anglicism word forms identified
by our system, treating all word forms as distinct.
We split the MZEE corpus into six non-overlapping
windows of 2M words each (T1 through T6), cal-
culate DUt (w), D
T
t (w) and log10 ft(w) within each
time window Tt. We again measure how well
these variables predict the change in log frequency
? log10 ft:u(w) = log10 fu(w) ? log10 ft(w) be-
tween the initial time Tt and a later time Tu, with
u = t + 1, ..., t + 3.
When measured over all words excluding angli-
cisms, log10 ft, D
U
t , and D
T
t at an initial time are
very weakly (0.0309 < r < 0.0692), but sig-
nificantly (p < .0001) positively correlated with
? log10 ft:u. However, in contrast to Altmann et
al.?s findings that DU and DT serve better than fre-
quency as predictors of word fate, for the set of an-
glicisms (Table 3), all correlations were both nega-
tive and stronger, and initial frequency log10 ft (not
dissemination) is the best predictor, especially as the
time spans increase in length. That is, while most
words? frequency change cannot generally be pre-
dicted from earlier frequency, we find that, for an-
glicisms, a high frequency is more likely to lead to a
decline, and vice versa.1.
1A set of 337 native German words frequency-matched to
the most common 337 anglicisms in our data set patterns with
the superset of all words (i.e., is not well predicted by any of the
137
? log10 ft:t+1(w)
r p t R2 N
log10 ft -0.2919 <.0001 -19.641 8.5% 4145
DUt -0.0814 .0001 -5.258 0.7% 4145
DTt -0.0877 .0001 -5.668 0.8% 4145
? log10 ft:t+2(w)
log10 ft -0.3580 <.0001 -22.042 12.8% 3306
DUt -0.1207 .0001 -6.987 1.5% 3306
DTt -0.1373 .0001 -7.97 1.9% 3306
? log10 ft:t+3(w)
log10 ft -0.4329 <.0001 -23.864 18.7% 2471
DUt -0.1634 .0001 -8.229 2.7% 2471
DTt -0.1755 .0001 -8.858 3.1% 2471
Table 3: Correlations between initial frequency and dis-
semination over users and threads and a change in fre-
quency for the 851 most common anglicisms in MZEE.
Finally, from the comparison of timespans in Ta-
ble 3, we see that the predictive ability (R2) of
the three measures increases as the timespan for
? log10 f becomes longer, i.e., frequency and dis-
semination effects on frequency change do not oper-
ate as strongly in immediate time scales.2.
5 Conclusion
In this study, we examined factors hypothesized to
influence the propagation of words through a com-
munity of speakers, focusing on anglicisms in a Ger-
man hip hop discussion corpus. The first analysis
presented here sheds light on the lexical dynamics
between the English and German hip hop commu-
nities, demonstrating that English frequency corre-
lates positively with change in a borrowed word?s
frequency in the German community?this result is
not shocking, as the communities are exposed to
shared inputs (e.g., hip hop lyrics), but the strength
of this correlation is highest in a two-year timespan,
suggesting a time lag from the frequency of hip hop
terms in English to the effects on those terms in Ger-
man. Future research here could profitably focus on
this relationship, especially for terms whose success
in the English and German hip hop communities is
highly disparate. Investigation of those terms could
suggest non-frequency factors which affect a word?s
variables) in this regard.
2An analysis which truncated the forms in the first two
timespans to match the N of the third confirm that this increase
is not simply an effect of the number of cases considered.
success or failure.
The second analysis, which compared three mea-
sures used by Altmann, Pierrehumbert, and Mot-
ter (2011) to predict lexical frequency change, found
that log10 f , D
U , and DT did not predict frequency
change well for non-anglicism words in the MZEE
corpus, but that log10 f in particular does predict fre-
quency change for anglicisms, though this correla-
tion is inverse; this finding relates to another analysis
of loanwords. In a diachronic study of loanword fre-
quencies in two French newspaper corpora, Chesley
and Baayen (2010, p.1364-5) found that high initial
frequency was ?a bad omen for a borrowing? and
found an interaction effect between frequency and
dispersion (roughly equivalent to dissemination in
the present study): ?As dispersion and frequency in-
crease, the number of occurrences at T2 decreases.?
A view of language as a stylistic resource (Cou-
pland, 2007) provides some explanation for these
counter-intuitive findings: An anglicism which is
used less often initially but survives is likely to in-
crease in frequency as other speakers adopt it for
?cred? or in-group prestige. However, a highly
frequent anglicism seems to become increasingly
undesirable?after all, if everyone is using it, it loses
its capacity to distinguish in-group members (con-
sider, e.g., the widespread adoption of the term bling
outside hip hop culture in the US). This circum-
stance is reflected by a drop in frequency as the word
becomes passe?. This view is supported by ethno-
graphic interviews with members of the German hip
hop community: ?Yeah, [the use of anglicisms is]
naturally overdone, for the most part. It?s targeted
at these 15, 14-year-old kids, that think this is cool.
The crowd! Ah, cool! Yeah, it?s true?the crowd, even
I say that, but not seriously.? -?Peter?, 22, beatboxer
and student at the Hip Hop Academy Hamburg.
In summary, the analyses discussed here lever-
age the opportunities provided by large-scale cor-
pus analysis and by the uniquely language-focused
nature of the hip hop community to investigate is-
sues of sociohistorical linguistic concern: what sort
of factors are at work in the process of linguis-
tic change through contact, and more specifically,
which word-extrinsic properties of stems and word-
forms condition the success and failure of borrowed
English words in the German hip hop community.
138
Acknowledgements
Matt Garley was supported by the Cognitive Sci-
ence/Artificial Intelligence Fellowship from the
University of Illinois and a German Academic Ex-
change Service (DAAD) Graduate Research Grant.
Julia Hockenmaier is supported by the National Sci-
ence Foundation through CAREER award 1053856
and award 0803603. The authors would like to thank
Dr. Marina Terkourafi of the University of Illinois at
Urbana-Champaign Linguistics Department for her
insights and contributions to this research project.
References
Beatrice Alex. 2008. Automatic detection of English
inclusions in mixed-lingual data with an application
to parsing. Ph.D. thesis, Institute for Communicat-
ing and Collaborative Systems, School of Informatics,
University of Edinburgh.
Eduardo G. Altmann, Janet B. Pierrehumbert, and Adil-
son E. Motter. 2011. Niche as a determinant of word
fate in online groups. PLoS ONE, 6(5):e19009, 05.
R.H. Baayen, R. Piepenbrock, and L. Gulikers. 1995.
The CELEX lexical database. CD-ROM.
Paula Chesley and R.H. Baayen. 2010. Predicting
new words from newer words: Lexical borrowings in
french. Linguistics, 45(4):1343?1374.
Nikolas Coupland. 2007. Style: Language variation
and identity. Cambridge, UK: Cambridge University
Press.
Sarah M.B. Fagan. 2009. German: A linguistic introduc-
tion. Cambridge, UK: Cambridge University Press.
Hans Henrich Hock and Brian D. Joseph. 1996. Lan-
guage history, language change, and language rela-
tionship: An introduction to historical and compara-
tive linguistics. Berlin, New York: Mouton de Gruyter.
Andrew Kachites McCallum. 2002. Mallet: A
machine learning for language toolkit. Web:
http://mallet.cs.umass.edu.
Alexander Onysko. 2007. Anglicisms in German: Bor-
rowing, lexical productivity, and written codeswitch-
ing. Berlin: Walter de Gruyter.
Alastair Pennycook. 2007. Global Englishes and tran-
scultural flows. New York, London: Routledge.
139
Transactions of the Association for Computational Linguistics, 1 (2013) 75?88. Action Editor: Sharon Goldwater.
Submitted 11/2012; Revised 1/2013; Published 3/2013. c?2013 Association for Computational Linguistics.
An HDP Model for Inducing Combinatory Categorial Grammars
Yonatan Bisk and Julia Hockenmaier
Department of Computer Science
The University of Illinois at Urbana-Champaign
201 N Goodwin Ave Urbana, IL 61801
{bisk1,juliahmr}@illinois.edu
Abstract
We introduce a novel nonparametric Bayesian
model for the induction of Combinatory Cat-
egorial Grammars from POS-tagged text. It
achieves state of the art performance on a
number of languages, and induces linguisti-
cally plausible lexicons.
1 Introduction
What grammatical representation is appropriate for
unsupervised grammar induction? Initial attempts
with context-free grammars (CFGs) were not very
successful (Carroll and Charniak, 1992; Charniak,
1993). One reason may be that CFGs require the
specification of a finite inventory of nonterminal cat-
egories and rewrite rules, but unless one adopts lin-
guistic principles such as X-bar theory (Jackendoff,
1977), these nonterminals are essentially arbitrary
labels that can be combined in arbitrary ways. While
further CFG-based approaches have been proposed
(Clark, 2001; Kurihara and Sato, 2004), most re-
cent work has followed Klein and Manning (2004)
in developing models for the induction of projec-
tive dependency grammars. It has been shown that
more sophisticated probability models (Headden III
et al, 2009; Gillenwater et al, 2011; Cohen and
Smith, 2010) and learning regimes (Spitkovsky et
al., 2010), as well as the incorporation of prior lin-
guistic knowledge (Cohen and Smith, 2009; Berg-
Kirkpatrick and Klein, 2010; Naseem et al, 2010)
can lead to significant improvement over Klein and
Manning?s baseline model. The use of dependency
grammars circumvents the question of how to obtain
an appropriate inventory of categories, since depen-
dency parses are simply defined by unlabeled edges
between the lexical items in the sentence. But de-
pendency grammars make it also difficult to cap-
ture non-local structures, and Blunsom and Cohn
(2010) show that it may be advantageous to refor-
mulate the underlying dependency grammar in terms
of a tree-substitution grammar (TSG) which pairs
words with treelets that specify the number of left
and right dependents they have. In this paper, we
explore yet another option: instead of dependency
grammars, we use Combinatory Categorial Gram-
mar (CCG, Steedman (1996; 2000)), a linguistically
expressive formalism that pairs lexical items with
rich categories that capture all language-specific in-
formation. This may seem a puzzling choice, since
CCG requires a significantly larger inventory of cat-
egories than is commonly assumed for CFGs. How-
ever, unlike CFG nonterminals, CCG categories are
not arbitrary symbols: they encode, and are deter-
mined by, the basic word order of the language and
the number of arguments each word takes. CCG is
very similar to TSG in that it also pairs lexical items
with rich items that capture all language-specific in-
formation. Like TSG and projective dependency
grammars, we restrict ourselves to a weakly context-
free fragment of CCG. But while TSG does not dis-
tinguish between argument and modifier dependen-
cies, CCG makes an explicit distinction between the
two. And while the elementary trees of Blunsom
and Cohn (2010)?s TSG and their internal nodel la-
bels have no obvious linguistic interpretation, the
syntactic behavior of any CCG constituent can be
directly inferred from its category. To see whether
75
the algorithm has identified the basic syntactic prop-
erties of the language, it is hence sufficient to in-
spect the induced lexicon. Conversely, Boonkwan
and Steedman (2011) show that knowledge of these
basic syntactic properties makes it very easy to cre-
ate a language-specific lexicon for accurate unsu-
pervised CCG parsing. We have recently proposed
an algorithm for inducing CCGs (Bisk and Hocken-
maier, 2012b) that has been shown to be competitive
with other approaches even when paired with a very
simple probability model (Gelling et al, 2012). In
this paper, we pair this induction algorithm with a
novel nonparametric Bayesian model that is based
on a different factorization of CCG derivations, and
show that it outperforms our original model and
many other approaches on a large number of lan-
guages. Our results indicate that the use of CCG
yields grammars that are significantly more robust
when dealing with longer sentences than most de-
pendency grammar-based approaches.
2 Combinatory Categorial Grammar
Combinatory Categorial Grammar (Steedman,
2000) is a linguistically expressive, lexicalized
grammar formalism that associates rich syntactic
types with words and constituents. For simplicity,
we restrict ourselves to the standard two atomic
types S (sentences) and N (encompassing both
nouns and noun phrases) from which we recursively
build categories. Complex categories are of the
form X/Y or X\Y, and represent functions which
return a result of type X when combined with an
argument of type Y. The directionality of the slash
indicates whether the argument precedes or follows
the functor. We write X|Y when the direction of the
slash does not matter.
The CCG lexicon encodes all language-specific
information. It pairs every word with a set of cate-
gories that define both its specific syntactic behavior
as well as the overall word order of the language:
N : {he, girl , lunch, ...} N/N : {good , the, eating , ...}
S\N : {sleeps, ate, eating , ...} (S\N)/N : {sees, ate, ...}
S\S : {quickly , today ...} (S\N)/(S\N) : {good , the, ...}
To draw a simple contrast, in Spanish we would
expect adjectives to take the category N\N because
Spanish word ordering dictates that the adjective fol-
low the noun. The lexical categories also capture
word-word dependencies: head-argument relations
are captured by the lexical category of the head (e.g.
(S\N)/N), whereas head-modifier relations are cap-
tured by the lexical category of the modifier, which
is of the form X\X or X/X, and may take further
arguments of its own. Our goal will be to automati-
cally learn these types of lexicons for a language. In
Figure 3, we juxtapose several such lexicons which
were automatically discovered by our system.
The rules of CCG are defined by a small set of
of combinatory rules, which are traditionally writ-
ten as schemas that define how constituents can be
combined in a bottom-up fashion (although genera-
tive probability models for CCG view them in a top-
down manner, akin to CFG rules). The first, and
most obvious, of these rules is function application:
X/Y Y ? X (B0>)
Y X\Y ? X (B0<)
Here the functor X/Y or X\Y is applied to an
argument Y resulting in X. While standard CCG
has a number of additional combinatory rules (type-
raising, generalized variants of composition and
substitution) that increase its generative capacity be-
yond context-free grammars and allow an elegant
treatment of non-local dependencies arising in ex-
traction, coordination and scrambling, we follow
Bisk and Hockenmaier (2012b) and use a restricted
fragment, without type-raising, that allows only ba-
sic composition and is context-free:
X/Y Y/Z ? X/Z (B1> )
X/Y Y\Z ? X\Z (B1X>)
Y\Z X\Y ? X\Z (B1< )
Y/Z X\Y ? X/Z (B1X<)
The superscript 1 denotes the arity of the compo-
sition which is too low to recover non-projective de-
pendencies, and our grammar is thus weakly equiva-
lent to the dependency grammar representations that
are commonly used for grammar induction. The
main role of composition in our fragment is that it
allows sentential and verb modifiers to both take cat-
egories of the form S\S and S/S. Composition in-
76
troduces spurious ambiguities, which we eliminate
by using Eisner (1996)?s normal form.1
Coordinating conjunctions have a special cate-
gory conj, and we binarize coordination as follows
(Hockenmaier and Steedman, 2007):
X X[conj] ?&1 X (&1)
conj X ?&2 X[conj] (&2)
3 Category induction
Unlike dependency grammars, CCG requires an in-
ventory of lexical categories. Given a set of lexical
categories, the combinatory rules define the set of
parses for each sentence. We follow the algorithm
proposed by Bisk and Hockenmaier (2012b) to au-
tomatically induce these categories. The lexicon is
initialized by pairing all nominal tags (nouns, pro-
nouns and determiners) with the category N, all verb
tags with the category S, and coordinating conjunc-
tions with the category conj:
CONJ ? conj
DET, NOUN, NUM, PRON ? N
VERB ? S
Although our lexicons are defined over corpus-
specific POS tags, we use a slightly modified version
of Petrov et al (2012)?s Universal POS tagset to cat-
egorize them into these broad classes. The primary
changes we make to their mappings are the addition
of a distinction (where possible) between subordi-
nating and coordinating conjunctions and between
main and auxiliary verbs2.
Since the initial lexicon consists only of atomic
categories, it cannot parse any complex sentences:
The man ate quickly
DT NNS VBD RB
- N S -
Complex lexical categories are induced by con-
sidering the local context in which tokens appear.
Given an input sentence, and a current lexicon which
assigns categories to at least some of the tokens in
the sentence, we apply the following two rules to
add new categories to the lexicon: The argument
rule allows any lexical tokens that have categories
other than N and conj to take immediately adjacent
1The normal-form of Hockenmaier and Bisk (2010) is not
required for this fragment of CCG.
2This distinction was suggested by the authors (p.c.)
Ns as arguments. The modifier rule allows any to-
ken (other than coordinating conjunctions that ap-
pear in the middle of sentences) to modify an imme-
diate neighbor that has the category S or N or is a
modifier (S|S or N|N) itself.
The man ate quickly
DT NNS VBD RB
N/N N, S/S S, N\N S\S
S\N
These rules can be applied iteratively to form
more complex categories. We restrict lexical cate-
gories to a maximal arity of 2, and disallow the cat-
egory (S/N)\N, since it is equivalent to (S\N)/N.
The man ate quickly
DT NNS VBD RB
N/N, N, S/S S, N\N, S\S,
(S/S)/(S/S)(N\N)/(N\N) S\N (N\N)\(N\N)
(N/N)\(N/N) (S/S)\(S/S)
(S\S)/(S\S)
The resultant, overly general, lexicon is then used
to parse the training data. Each complete parse has
to be of category S or N, with the constraint that
sentences that contain a main verb can only form
parses of category S.
4 A new probability model for CCG
Generative models define the probability of a parse
tree ? as the product of individual rule probabili-
ties. Our previous work (Bisk and Hockenmaier,
2012b) uses the most basic model of Hockenmaier
and Steedman (2002), which first generates the head
direction (left, right, unary, or lexical), followed by
the head category, and finally the sister category. 3
This factorization does not take advantage of the
unique functional nature of CCG. We therefore in-
troduce a new factorization we call the Argument
Model. It exploits the fact that CCG imposes strong
constraints on a category?s left and right children,
since these must combine to create the parent type
via one of the combinators. In practice this means
that given the parent X/Z, the choice of combinator4
c and an argument Y we can uniquely determine the
categories of the left and right children:
3Huang et al (2012) present a (deficient) variant and
Bayesian extension of the Bisk and Hockenmaier (2012b)
model without k-best smoothing that both underperform our
published results.
4If X is an atomic category, only application is possible.
77
Parent c ? Left Right
X/Z B0> (X/Z)/Y Y
B0< Y (X/Z)\Y
B1> X/Y Y/Z
B1< Y/Z X\Y
and correspondingly for X\Z:
Parent c ? Left Right
X\Z B0> (X\Z)/Y Y
B0< Y (X\Z)\Y
B1> X/Y Y\Z
B1< Y\Z X\Y
While type-changing and raising are not used in
this work the model?s treatment of root productions
extends easily to handle these other unary cases. We
simply treat the argument Y as the unary outcome so
that the parent, combinator and argument uniquely
specify every detail of the unary rule:
Parent c ? Y
TOP TOP ? {S,N}
S/(S\N) T< N
S\(S/N) T> N
We still distinguish the same rule types as before
(lexical, unary, binary with head left/right), leading
us to the following model definition:
Given: P := X/Z
where type(t) ? {Left,Right,Unary,Lex}
p(t|P)?
{
p(w|P, t) Lex
p(Y|P, t)? p(c|P, t,Y) o.w.
Argument Combinator
Note that this model generates only one CCG cat-
egory but uniquely defines the two children of a par-
ent node. We will see below that this greatly simpli-
fies the development of non-parametric extensions.
5 HDP-CCG: a nonparametric model
Simple generative models such as PCFGs or Bisk
and Hockenmaier (2012b)?s CCG model are not
robust in the face of sparsity, since they assign
zero probability to any unseen event. Sparsity is
a particular problem for formalisms like CCG that
have a rich inventory of object types. Nonpara-
metric Bayesian models, e.g. Dirichlet Processes
(Teh, 2010) or their hierarchical variants (Teh et
al., 2006) and generalizations (Teh, 2006) overcome
this problem in a very elegant manner, and are used
by many state-of-the-art grammar induction systems
(Naseem et al, 2010; Blunsom and Cohn, 2010;
Boonkwan and Steedman, 2011). They also im-
pose a rich-getting-richer behavior that seems to be
advantageous in many modeling applications. By
contrast, Bisk and Hockenmaier (2012b) propose a
weighted top-k scheme to address these issues in an
ad-hoc manner.
The argument model introduced above lends it-
self particularly well to nonparametric extensions
such as the standard Hierarchical Dirichlet Pro-
cesses (HDP). In this work the size of the grammar
and the number of productions are fixed and small,
but we present the formulation as infinite to allow for
easy extension in the future. Specifically, this frame-
work allows for extensions which grow the grammar
during parsing/training or fully lexicalize the pro-
ductions. Additionally, while our current work uses
only a restricted fragment of CCG that has only a
finite set of categories, the literature?s generalized
variants of composition make it possible to gener-
ate categories of unbounded arity. We therefore be-
lieve that this is a very natural probabilistic frame-
work for CCG, since HDPs make it possible to con-
sider a potentially infinite set of categories that can
instantiate the Y slot, while allowing the model to
capture language-specific preferences for the set of
categories that can appear in this position.
The HDP-CCG model In Bayesian models,
multinomials are drawn from a corresponding n-
dimensional Dirichlet distribution. The Dirichlet
Process (DP) generalizes the Dirichlet distribution
to an infinite number of possible outcomes, allowing
us to deal with a potentially infinite set of categories
or words. DPs are defined in terms of a base dis-
tribution H that corresponds to the mean of the DP,
and a concentration or shape parameter ?. In a Hi-
erarchical Dirichlet Process (Teh et al, 2006), there
is a hierarchy of DPs, such that the base distribution
of a DP at level n is a DP at level n? 1.
The HDP-CCG (Figure 1) is a reformulation of
the Argument Model introduced above in terms of
Hierarchical Dirichlet Processes.5 At the heart of
the model is a distribution over CCG categories. By
combining a stick breaking process with a multino-
mial over categories we can define a DP over CCG
5An alternative HDP model for semantic parsing with CCG
is proposed by Kwiatkowski et al (2012).
78
HDP-CCG
1) Draw global parameters
Define MLE root parameter ?TOP
Draw top-level symbol weights ?Y ? GEM(?Y )
Draw top-level lexical weights ?L ? GEM(?L)
For each grammar symbol z ? {1, 2, ...}:
Define MLE rule type parameters ?Tz
Draw argument parameters ?Yz ? DP(?Y, ?Y )
Draw lexical emission parameters ?Lz ? DP(?L, ?L)
For each grammar symbol y ? {1, 2, ...}:
Define MLE combinator parameters ?Cz,y
2) For each parse tree:
Generate root node zTOP ? Binomial(?TOP )
For each node i in the parse tree:
Choose rule type ti ?Multinomial(?Tzi )If ti == Lex:
Emit terminal symbol xi ?Multinomial(?Lzi )If ti == Left/Right/Unary:
Generate argument category yi ?Multinomial(?Yzi )Generate combinator ci ?Multinomial(?Czi,yi )Deterministically create zL(i) (and zR(i) if binary)
zi
yi ci
zL(i) zR(i)
xL(i) xR(i)z ?
?y
?Y
?T
?C
?L
?Y
?L
Because we are working with CCG, the
parent zi, argument yi and combinator ci
uniquely define the two children categories
(zL(i), zR(i)). The dashed arrows here rep-
resent the deterministic process used to
generate these two categories.
Figure 1: The HDP-CCG has two base distributions, one over the space of categories and the other over
words (or tags). For every grammar symbol, an argument distribution and emission distribution is drawn
from the corresponding Dirichlet Processes. In addition, there are several MLE distributions tied to a given
symbol for generating rule types, combinators and lexical tokens.
categories whose stick weights (?Y ) correspond to
the frequency of the category in the corpus. Next we
build the hierarchical component of our model by
choosing an argument distribution (?Y ), again over
the space of categories, for every parent X/Z. This
argument distribution is drawn from the previously
defined base DP, allowing for an important level of
parameter tying across all argument distributions.
While the base DP does define the mean around
which all argument distributions are drawn, we also
require a notion of variance or precision which de-
termines how similar individual draws will be. This
precision is determined by the magnitude of the hy-
perparameter ?Y . This hierarchy is paralleled for
lexical productions which are drawn from a unigram
base DP over terminal symbols controlled by ?L.
For simplicity we use the same scheme for setting
the values for ?L as ?Y . We present experimen-
tal results in which we vary the value of ?Y as a
function of the number of outcomes allowed by the
grammar for argument categories or the corpus in
the case of terminal symbols. Specifically, we set
?Y = np for conditioning contexts with n out-
comes. Since Liang et al (2009) found that the ideal
value for alpha appears to be superlinear but sub-
quadratic in n, we present results where p takes the
values 0, 1.0, 1.5, and 2.0 to explore the range from
uniform to quadratic. This setting for ? is the only
free parameter in the model. By controlling preci-
sion we can tell the model to what extent global cor-
pus statistics should be trusted. We believe this has
a similar effect to Bisk and Hockenmaier (2012b)?s
top-k upweighting and smoothing scheme.
One advantage of the argument model is that it
only requires a single distribution over categories for
each binary tree. In contrast to similar proposals for
CFGs (Liang et al, 2007), which impose no formal
restrictions on the nonterminals X, Y, Z that can ap-
pear in a rewrite rule X? Y Z, this greatly sim-
plifies the modeling problem (yielding effectively a
model that is more akin to nonparametric HMMs),
since it avoids the need to capture correlations be-
79
tween different base distributions for Y and Z.
Variational Inference HDPs need to be estimated
with approximate techniques. As an alternative to
Gibbs sampling (Teh et al, 2006), which is exact,
but typically very slow and has no clear convergence
criteria, variational inference algorithms (Bishop,
2006; Blei and Jordan, 2004) estimate the parame-
ters of a truncated model to maximize a lower bound
of the likelihood of the actual model. This allows for
factorization of the model and a training procedure
analogous to the Inside-Outside algorithm (Lari and
Young, 1991), allowing training to run very quickly
and in a trivially parallelizable manner.
To initialize the base DP?s stick weights, we fol-
low the example of Kurihara et al (2007) and use
an MLE model initialized with uniform distributions
to compute global counts for the categories in our
grammar. When normalized these provide a better
initialization than a uniform set of weights. Updates
to the distributions are then performed in a coordi-
nate descent manner which includes re-estimation of
the base DPs.
In variational inference, multinomial weights W
take the place of probabilities. The weights for an
outcome Y with conditioning variable P are com-
puted by summing pseudocounts with a scaled mean
vector from the base DP. The computation involves
moving in the direction of the gradient of the Dirich-
let distribution which results in the following differ-
ence of Digammas (?):
WP (Y ) = ?(C(P, Y ) + ?P?Y )
??(C(P, ?) + ?P )
Importantly, the Digamma and multinomial
weights comprise a righ-get-richer scheme, biasing
the model against rare outcomes. In addition, since
variational inference is done by coordinate descent,
it is trivially parallelizeable. In practice, training and
testing our models on the corpora containing sen-
tences up to length 15 used in this paper takes be-
tween one minute to at most three hours on a single
12-core machine depending on their size.
6 Evaluation
As is standard for this task, we evaluate our systems
against a number of different dependency treebanks,
and measure performance in terms of the accuracy of
directed dependencies (i.e. the percentage of words
in the test corpus that are correctly attached). We use
the data from the PASCAL challenge for grammar
induction (Gelling et al, 2012), the data from the
CoNLL-X shared task (Buchholz and Marsi, 2006)
and Goldberg (2011)?s Hebrew corpus.
Converting CCG derivations into dependencies is
mostly straightforward, since the CCG derivation
identifies the root word of each sentence, and head-
argument and head-modifier dependencies are easily
read off of CCG derivations, since the lexicon de-
fines them explicitly. Unlike dependency grammar,
CCG is designed to recover non-local dependencies
that arise in control and binding constructions as
well as in wh-extraction and non-standard coordi-
nation, but since this requires re-entrancies, or co-
indexation of arguments (Hockenmaier and Steed-
man, 2007), within the lexical categories that trigger
these constructions, our current system returns only
local dependencies. But since dependency gram-
mars also captures only local dependencies, this has
no negative influence on our current evaluation.
However, a direct comparison between depen-
dency treebanks and dependencies produced by
CCG is more difficult (Clark and Curran, 2007),
since dependency grammars allow considerable
freedom in how to analyze specific constructions
such as verb clusters (which verb is the head?)
prepositional phrases and particles (is the head the
noun or the preposition/particle?), subordinating
conjunctions (is the conjunction a dependent of the
head of the main clause and the head of the embed-
ded clause a dependent of the conjunction, or vice
versa?) and this is reflected in the fact that the tree-
banks we consider often apply different conventions
for these cases. Although remedying this issue is
beyond the scope of this work, these discrepancies
very much hint at the need for a better mechanism to
evaluate linguistically equivalent structures or tree-
bank standardization.
The most problematic construction is coordina-
tion. In standard CCG-to-dependency schemes, both
conjuncts are independent, and the conjunction itself
is not attached to the dependency graph, whereas de-
pendency grammars have to stipulate that either one
of the conjuncts or the conjunction itself is the head,
with multiple possibilities of where the remaining
80
constituents attach. In addition to the standard CCG
scheme, we have identified five main styles of con-
junction in our data (Figure 2), although several cor-
pora distinguish multiple types of coordinating con-
junctions which use different styles (not all shown
here). Since our system has explicit rules for coordi-
nation, we transform its output into the desired target
representation that is specific to each language.
7 Experiments
We evaluate our system on 13 different languages.
In each case, we follow the test and training regimes
that were used to obtain previously published results
in order to allow a direct comparison. We com-
pare our system to the results presented at the PAS-
CAL Challenge on Grammar Induction (Gelling et
al., 2012)6, as well as to Gillenwater et al (2011)
and Naseem et al (2012). We use Nivre (2006)?s
Penn2Malt implementation of Collins (2003)?s head
rules to translate the WSJ Penn Treebank (Marcus
et al, 1993) into dependencies. Finally, when train-
ing the MLE version of our model we use a simple
smoothing scheme which defines a small rule proba-
bility (e?15) to prevent any rule used during training
from going to zero.
7.1 PASCAL Challenge on Grammar
Induction
In Table 1, we compare the performance of the ba-
sic Argument model (MLE), of our HDP model with
four different settings of the hyperparameters (as ex-
plained above) and of the systems presented in the
PASCAL Challenge on Grammar Induction (Gelling
et al, 2012). The systems in this competition were
instructed to train over the full dataset, including the
unlabelled test data, and include Bisk and Hocken-
maier (2012a)?s CCG-based system (BH) to Cohn et
al. (2010)?s reimplementation of Klein and Manning
(2004)?s DMV model in a tree-substitution gram-
mar framework (BC), as well as three other de-
pendency based systems which either incorporate
Naseem et al (2010)?s rules in a deterministic fash-
ion (S?gaard, 2012), rely on extensive tuning on
6Numbers are from personal correspondence with the orga-
nizers. The previously published numbers are not comparable
to literature due to an error in the evaluation. http://wiki.
cs.ox.ac.uk/InducingLinguisticStructure/
ResultsDepComparable
the development set (Tu, 2012) or incorporate mil-
lions of additional tokens from Wikipedia to esti-
mate model parameters (Marecek and Zabokrtsky,
2012). We ignore punctuation for all experiments
reported in this paper, but since the training data
(but not the evaluation) includes punctuation marks,
participants were free to choose whether to include
punctuation or ignore it.
While BH is the only other system with directly
interpretable linguistic output, we also include a di-
rect comparison with BC, whose TSG representa-
tion is equally expressive to ours. Finally we present
a row with the maximum performance among the
other three models. As we have no knowledge of
how much data was used in the training of other sys-
tems we simply present results for systems trained
on length 15 (not including punctuation) sentences
and then evaluated at lengths 10 and 15.
The MLE version of our model shows rather vari-
able performance: although its results are particu-
larly bad on Basque (Eu), it outperforms both BH
and BC on some other settings. By contrast, the
HDP system is always better than the MLE model.
It outperforms all other systems on half of the cor-
pora. On average, it outperforms BH and BC by
10.3% and 9.3% on length 10, or 9.7% and 7.8 %
on length 15 respectively. The main reason why our
system does not outperform BC by an even higher
margin is the very obvious 11.4%/11.5% deficit on
Slovene. However, the Slovene dependency tree-
bank seems to follow a substantially different anno-
tation scheme. In particular, the gold standard an-
notation of the 1,000 sentences in the Slovene de-
velopment set treats many of them as consisting of
independent sentences (often separated by punctua-
tion marks that our system has no access to), so that
the average number of roots per sentence is 2.7:
>>
?
verjeti
believe
ti
I
,
,
??
?
je
is
mehko
soft
rekla
said
When our system is presented with these short
components in isolation, it oftentimes analyzes them
correctly, but since it has to return a tree with a sin-
gle root, its performance degrades substantially.
We believe the HDP performs so well as com-
pared to the MLE model because of the influence
of the shared base distribution, which allows the
81
Ar, Eu, Cs, Nl,
WSJ, Ch, He Da, He Es, Bg, De, Pt Sv, Sl Ja
noun conj noun noun conj noun noun conj noun noun conj noun noun conj noun
Figure 2: In the treebanks used for evaluation different standards exist for annotating coordination. While
not exhaustive, this table demonstrates five of the most common schemes used in the literature. Syntactically
these are identical and traditionally CCG draws arcs only to the arguments without attaching the conjunction.
For the purposes of comparison with the literature we have implemented these five translation schemes.
Arabic Danish Slovene Swedish Dutch Basque Portuguese WSJ Childes Czech
# Tokens 5,470 25,341 54,032 61,877 78,737 81,345 158,648 163,727 290,604 436,126
# Tags 20 24 36 30 304 14 23 36 69 62
PA
SC
AL BC 60.8/58.4 44.7/39.4 62.6/57.9 63.2/56.6 51.8/52.0 53.0/48.9 52.4/50.2 68.6/63.3 47.4/46.1 47.9/43.1Max 67.2/66.8 60.1/56.0 65.6/61.8 72.8/63.4 51.1/47.6 53.7/47.8 67.0/61.8 71.2/64.8 56.0/54.5 58.3/54.4
BH 41.6/43.7 46.4/43.8 49.6/43.9 63.7/57.0 49.7/43.6 45.1/39.6 70.8/67.2 68.2/59.6 61.4/59.8 45.0/38.9
Th
isw
ork
MLE 41.6/42.9 43.4/39.2 46.1/41.1 70.1/59.7 52.2/47.2 29.6/26.5 62.2/59.7 59.5/52.4 53.3/51.9 50.5/45.8
HDP0.0 48.0/50.0 63.9/58.5 44.8/39.8 67.6/62.1 45.0/33.9 41.6/39.1 71.0/66.0 59.8/52.9 56.3/55.2 54.0/49.0
HDP1.0 45.6/47.1 45.7/42.3 53.9/46.9 74.5/66.9 58.5/54.4 50.1/44.6 65.1/60.6 64.3/56.5 71.5/70.3 55.8/50.7
HDP1.5 49.6/50.4 58.7/54.4 53.2/48.2 74.3/67.1 57.4/54.5 50.6/45.0 70.0/64.7 65.5/57.2 69.6/68.6 55.6/50.3
HDP2.0 66.4/65.1 56.5/49.5 54.2/46.4 71.6/64.1 51.7/48.3 49.4/43.3 76.3/70.5 70.7/62.9 74.1/73.3 54.4/48.5
+/? -0.8/-1.7 +3.8/+2.5 -11.4/-15.4 +1.7/+3.5 +6.7/+2.4 -3.1/-3.9 +5.5/+3.3 -0.5/-1.9 +12.7/+13.5 -2.5/-3.7
Table 1: A comparison of the basic Argument model (MLE) and four hyper-parameter settings of the HDP-
CCG against two syntactic formalisms that participated in the PASCAL Challenge (Gelling et al, 2012),
BH (Bisk and Hockenmaier, 2012a) and BC (Blunsom and Cohn, 2010), in addition to a max over all other
participants. We trained on length 15 data (punctuation removed), including the test data as recommended
by the organizers. The last row indicates the difference between our best system and the competition.
global category distribution to influence each of the
more specific distributions. Further, it provides a
very simple knob in the choice of hyperparame-
ters, which has a substantial effect on performance.
A side effect of the hyperparameters is that their
strength also determines the rate of convergence.
This may be one of the reasons for the high vari-
ance seen in the four settings tested, although we
note that since our initialization is always uniform,
and not random, consecutive runs do not introduce
variance in the model?s performance.
7.2 Comparison with systems that capture
linguistic constraints
Since our induction algorithm is based on the knowl-
edge of which POS tags are nouns and verbs, we
compare in Table 2 our system to Naseem et al
(2010), who present a nonparametric dependency
model that incorporates thirteen universal linguistic
constraints. Three of these constraints correspond
to our rules that verbs are the roots of sentences and
may take nouns as dependents, but the other ten con-
straints (e.g. that adjectives modify nouns, adverbs
modify adjectives or verbs, etc.) have no equivalent
in our system. Although our system has less prior
knowledge, it still performs competitively.
On the WSJ, Naseem et al demonstrate the im-
portance and effect of the specific choice of syntactic
rules by comparing the performance of their system
with hand crafted universal rules (71.9), with En-
glish specific rules (73.8), and with rules proposed
by Druck et al (2009) (64.9). The performance of
Naseem et al?s system drops very significantly as
sentence length (and presumable parse complexity)
82
Sl Es Da Pt Sv
?#Tokens 3.8K 4.2K 9.5K 15K 24K
N10 50.9 67.2 51.9 71.5 63.3
HDP 56.6 62.1 51.5 74.7 69.8
Table 2: A comparison of our system with Naseem
et al (2010), both trained and tested on the length 10
training data from the CoNLL-X Shared Task.
increases, whereas our system shows significantly
less decline, and outperforms their universal system
by a significant margin.7
? 10 ? 20
Naseem Universal Rules 71.9 50.4
Naseem English Rules 73.8 66.1
HDP-CCG 68.2 64.2
HDP-CCG (train ? 20) 71.9
In contrast to Spitkovsky et al (2010), who reported
that performance of their dependency based system
degrades when trained on longer sentences, our per-
formance on length 10 sentences increases to 71.9
when we train on sentences up to length 20.
Another system that is also based on CCG, but
captures significantly more linguistic knowledge
than ours, was presented by Boonkwan and Steed-
man (2011), who achieve an accuracy of 74.5 on
WSJ10 section 23 (trained on sections 02-22). Us-
ing the same settings, our system achieves an accu-
racy of 68.4. Unlike our approach, Boonkwan and
Steedman do not automatically induce an appropri-
ate inventory of lexical category, but use an exten-
sive questionnaire that defines prototype categories
for various syntactic constructions, and requires sig-
nificant manual engineering of which POS tags are
mapped to what categories to generate a language-
specific lexicon. However, their performance de-
grades significantly when only a subset of the ques-
tions are considered. Using only the first 14 ques-
tions, covering facts about the ordering of subjects,
verbs and objects, adjectives, adverbs, auxiliaries,
adpositions, possessives and relative markers, they
achieve an accuracy of 68.2, which is almost iden-
7Our earlier generative model showed similar behavior, al-
though the results in Bisk and Hockenmaier (2012b) are not
directly comparable due to differences in the data.
Sl Es Da Pt Sv
#Tokens 3,857 4,230 9,549 15,015 24,021
G10 51.2 62.4 47.2 54.3 48.6
HDP 57.9 65.4 49.3 73.5 73.2
Bg WSJ Nl Ja De
#Tokens 38,220 42,442 43,405 43,501 77,705
G10 59.8 64.4 47.5 60.2 47.4
HDP 66.1 70.3 56.2 64.1 68.4
Table 3: A comparison of our system with Gillenwa-
ter et al (2010), both trained on the length 10 train-
ing data, and tested on the length 10 test data, from
the CoNLL-X Shared task.
tical to ours, even though we use significantly less
initial knowledge. However, the lexicons we present
below indicate that we are in fact learning many of
the very exact details that in their system are con-
structed by hand. The remaining 14 questions in
Boonkwan and Steedman?s questionnaire cover less
frequent phenomena such as the order of negative
markers, dative shift, and pro-drop. The obvious ad-
vantage of this approach is that this allows them to
define a much more fine-grained inventory of lexical
categories than our system can automatically induce.
We also stipulate that for certain languages knowl-
edge of pro-drop could play a significant role in the
success of their approach: if complete sentences are
allowed to be of the form S\N or S/N, the same lex-
ical category can be used for the verb regardless of
whether the subject is present or has been dropped.
7.3 Additional Languages
In order to provide results on additional languages,
we present in Table 3 a comparison to the work of
Gillenwater et al (2010) (G10), using the ConLL-X
Shared Task data (Buchholz and Marsi, 2006). Fol-
lowing Gillenwater et al, we train only on sentences
of length 10 from the training set and evaluate on the
test set. Since this is a different training regime, and
these corpora differ for many languages from that of
the PASCAL challenge, numbers from Table 1 can-
not be compared directly with those in Table 3. We
have also applied our model to Goldberg (2011)?s
Hebrew corpus, where it achieves an accuracy of
62.1 (trained and tested on all sentences length 10;
7,253) and 59.6 (length 15; 21,422 tokens).
83
Arabic % Swedish % WSJ % Childes % Japanese % Czech %
VERB (S\N)/N 56 S 45 S\N 52 S/N 44 S 84 S 26
(S/N)/N 29 S\N 20 (S\N)/N 19 S 37 S\N 25
ADP N\N 68 (S\S)/N 49 (S\S)/N 46 (S\S)/N 45 (S/S)\N 44 (S\S)/N 42
N/N 21 (N\N)/N 25 (N\N)/N 20 N/N 25 N\N 23 (S/S)/N 26
NOUN N\N 50 N 91 N 79 N 89 N 73 N 74
N 35 N/N 12
ADJ N\N 82 N/N 50 N/N 70 N/N 46 S/S 64 N/N 55
Figure 3: Partial lexicons demonstrating language specific knowledge learned automatically for five lan-
guages. For ease of comparison between languages, we use the universal tag label (Verb, Adposition, Noun
and Adjective). Shown are the most common categories and the fraction of occurrences of the tag that are
assigned this category (according to the Viterbi parses).
8 The Induced Lexicons
Since our approach is based on a lexicalized for-
malism such as CCG, our system automatically in-
duces lexicons that pair words (or, in our case, POS-
tags) with language-specific categories that capture
their syntactic behavior. If our approach is success-
ful, it should learn the basic syntactic properties of
each language, which will be reflected in the corre-
sponding lexicon. In Figure 3 one sees how verbs
subcategorize differently, how word ordering differs
by language, and how the attachment structures of
prepositions are automatically discovered and differ
across languages. In Arabic, for example, the sys-
tem learns that word order is variable and therefore
the verb must allow for both SVO and VOS style
constructions. We generally learn that adpositions
(prepositions or postpositions) take nouns as argu-
ments. In Czech, PPs can appear before and after the
verb, leading to two different categories ((S\S)/N
and (S/S)/N). Japanese has postpositions that ap-
pear in preverbal position ((S/S)\N), but when this
category is assigned to nominal particles that cor-
respond to case markers, it effectively absorbs the
noun, leading to a preference for verbs that do not
take any arguments (S), and to a misanalysis of ad-
jectives as verb modifiers (S/S). Our lexicons also
reflect differences in style: while Childes and the
WSJ are both English, they represent very different
registers. We learn that subjects are mostly absent in
the informal speech and child-directed instructions
contained in Childes, while effectively mandatory in
the Wall Street Journal.
9 Conclusions
This paper has introduced a novel factorization for
CCG models and showed how when combined with
non-parametric Bayesian statistics it can compete
with every other grammar induction system cur-
rently available, including those that capture a sig-
nificant amount of prior linguistic knowledge. The
use of a powerful syntactic formalism proves ben-
eficial both in terms of requiring very limited uni-
versal knowledge and robustness at longer sentence
lengths. Unlike standard grammar induction sys-
tems that are based on dependency grammar, our
system returns linguistically interpretable lexicons
for each language that demonstrate it has discov-
ered their basic word order. Of particular note is the
simplicity of the model both algorithmically and in
terms of implementation. By not faltering on longer
sentences or requiring extensive tuning, the system
can be easily and quickly deployed on a new lan-
guage and return state of the art performance and
easily interpretable lexicons. In this paper, we have
applied this model only to a restricted fragment of
CCG, but future work will address the impact of lex-
icalization and the inclusion of richer combinators.
10 Acknowledgements
This work is supported by NSF CAREER award
1053856 (Bayesian Models for Lexicalized Gram-
mars).
84
References
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phyloge-
netic Grammar Induction. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1288?1297, Uppsala, Sweden, July.
Christopher Bishop. 2006. Pattern Recognition and Ma-
chine Learning. Springer-Verlag, August.
Yonatan Bisk and Julia Hockenmaier. 2012a. Induction
of Linguistic Structure with Combinatory Categorial
Grammars. In NAACL HLT Workshop on Induction of
Linguistic Structure, pages 90?95, Montre?al, Canada,
June.
Yonatan Bisk and Julia Hockenmaier. 2012b. Simple
Robust Grammar Induction with Combinatory Cate-
gorial Grammars. In Proceedings of the Twenty-Sixth
Conference on Artificial Intelligence (AAAI-12), pages
1643?1649, Toronto, Canada, July.
David M Blei and Michael I Jordan. 2004. Variational
Methods for the Dirichlet Process. In Proceedings of
the Twenty-First International Conference on Machine
Learning (ICML 2004), Banff, Alberta, Canada, July.
Phil Blunsom and Trevor Cohn. 2010. Unsupervised
Induction of Tree Substitution Grammars for Depen-
dency Parsing. Proceedings of the 2010 Conference
on Empirical Methods of Natural Language Process-
ing, pages 1204?1213, October.
Prachya Boonkwan and Mark Steedman. 2011. Gram-
mar Induction from Text Using Small Syntactic Proto-
types. In Proceedings of 5th International Joint Con-
ference on Natural Language Processing, pages 438?
446, Chiang Mai, Thailand, November.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing. In
Proceedings of the 10th Conference on Computational
Natural Language Learning (CoNLL-X), pages 149?
164, New York City, June.
Glenn Carroll and Eugene Charniak. 1992. Two Exper-
iments on Learning Probabilistic Dependency Gram-
mars from Corpora. Working Notes of the Workshop
Statistically-Based NLP Techniques, pages 1?13.
Eugene Charniak. 1993. Statistical Language Learning.
The MIT Press, Cambridge, Massachusetts.
Stephen Clark and James R Curran. 2007. Formalism-
Independent Parser Evaluation with CCG and Dep-
Bank. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
248?255, Prague, Czech Republic, June.
Alex Clark. 2001. Unsupervised Language Acquisition:
Theory and Practice. Ph.D. thesis, University of Sus-
sex, September.
Shay B Cohen and Noah A Smith. 2009. Variational
Inference for Grammar Induction with Prior Knowl-
edge. Proceedings of the ACL-IJCNLP 2009 Confer-
ence Short Papers, pages 1?4.
Shay B Cohen and Noah A Smith. 2010. Covariance
in Unsupervised Learning of Probabilistic Grammars.
The Journal of Machine Learning Research, pages
3117?3151, November.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing Tree-Substitution Grammars. The
Journal of Machine Learning Research, 11:3053?
3096, November.
Michael Collins. 2003. Head-Driven Statistical Mod-
els for Natural Language Parsing. Computational Lin-
guistics, 29(4):589?637, December.
Gregory Druck, Gideon Mann, and Andrew McCal-
lum. 2009. Semi-supervised Learning of Dependency
Parsers using Generalized Expectation Criteria. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 360?368, Suntec, Singapore, Au-
gust.
Jason Eisner. 1996. Efficient Normal-Form Parsing for
Combinatory Categorial Grammar. In Proceedings of
the 34th Annual Meeting of the Association for Com-
putational Linguistics, pages 79?86, Santa Cruz, Cali-
fornia, USA, June.
Douwe Gelling, Trevor Cohn, Phil Blunsom, and Joa?o V
Graca. 2012. The PASCAL Challenge on Grammar
Induction. In NAACL HLT Workshop on Induction of
Linguistic Structure, pages 64?80, Montre?al, Canada,
June.
Jennifer Gillenwater, Kuzman Ganchev, Joa?o V Graca,
Fernando Pereira, and Ben Taskar. 2010. Sparsity in
Dependency Grammar Induction. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics, pages 194?199, Uppsala, Swe-
den, July.
Jennifer Gillenwater, Kuzman Ganchev, Joa?o V Graca,
Fernando Pereira, and Ben Taskar. 2011. Posterior
Sparsity in Unsupervised Dependency Parsing. The
Journal of Machine Learning Research, 12:455?490,
February.
Yoav Goldberg. 2011. Automatic Syntactic Processing of
Modern Hebrew. Ph.D. thesis, Ben-Gurion University
of the Negev, November.
William P Headden III, Mark Johnson, and David Mc-
Closky. 2009. Improving Unsupervised Dependency
Parsing with Richer Contexts and Smoothing. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
101?109, Boulder, Colorado, June.
Julia Hockenmaier and Yonatan Bisk. 2010. Normal-
form parsing for Combinatory Categorial Grammars
with generalized composition and type-raising. In
85
Proceedings of the 23rd International Conference on
Computational Linguistics (Coling 2010), pages 465?
473, Beijing, China, August. Coling 2010 Organizing
Committee.
Julia Hockenmaier and Mark Steedman. 2002. Gener-
ative Models for Statistical Parsing with Combinatory
Categorial Grammar. In Proceedings of 40th Annual
Meeting of the Association for Computational Lin-
guistics, pages 335?342, Philadelphia, Pennsylvania,
USA, July.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Dependency
Structures Extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396, September.
Yun Huang, Min Zhang, and Chew Lim Tan. 2012.
Improved Combinatory Categorial Grammar Induc-
tion with Boundary Words and Bayesian Inference.
In Proceedings of the 24rd International Conference
on Computational Linguistics (Coling 2012), Mumbai,
India, December.
Ray Jackendoff. 1977. X-Bar Syntax: A Study of Phrase
Structure. The MIT Press.
Dan Klein and Christopher D Manning. 2004. Corpus-
Based Induction of Syntactic Structure: Models of De-
pendency and Constituency. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL?04), Main Volume, pages 478?485,
Barcelona, Spain, July.
Kenichi Kurihara and Taisuke Sato. 2004. An Appli-
cation of the Variational Bayesian Approach to Prob-
abilistic Context-Free Grammars. International Joint
Conference on Natural Language Language Process-
ing Workshop Beyond Shallow Analyses, March.
Kenichi Kurihara, Max Welling, and Yee-Whye Teh.
2007. Collapsed Variational Dirichlet Process Mix-
ture Models. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence (IJCAI07),
pages 2796?2801, Hyderabad, India, January.
Tom Kwiatkowski, Sharon Goldwater, Luke Zettlemoyer,
and Mark Steedman. 2012. A probabilistic model of
syntactic and semantic acquisition from child-directed
utterances and their meanings. In Proceedings of the
13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 234?
244, Avignon, France, April. Association for Compu-
tational Linguistics.
Karim Lari and Steve J Young. 1991. Applications
of stochastic context-free grammars using the Inside-
Outside algorithm. Computer speech & language,
5(3):237?257, January.
Percy Liang, Slav Petrov, Michael I Jordan, and Dan
Klein. 2007. The Infinite PCFG Using Hierarchical
Dirichlet Processes. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 688?697,
Prague, Czech Republic.
Percy Liang, Michael I Jordan, and Dan Klein. 2009.
Probabilistic Grammars and Hierarchical Dirichlet
Processes. In The Oxford Handbook of Applied
Bayesian Analysis. Oxford University Press.
Mitchell P Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330, June.
David Marecek and Zdenek Zabokrtsky. 2012. Unsu-
pervised Dependency Parsing using Reducibility and
Fertility features. In NAACL HLT Workshop on Induc-
tion of Linguistic Structure, pages 84?89, Montre?al,
Canada, June.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using Universal Linguistic Knowl-
edge to Guide Grammar Induction. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1234?1244, Cambridge,
MA, October.
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective Sharing for Multilingual Dependency
Parsing. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 629?637, Jeju, Republic
of Korea, July.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A Universal Part-of-Speech Tagset. In Proceedings
of the 8th International Conference on Language Re-
sources and Evaluation (LREC-2012), pages 2089?
2096, Istanbul, Turkey, May.
Anders S?gaard. 2012. Two baselines for unsuper-
vised dependency parsing. In NAACL HLT Work-
shop on Induction of Linguistic Structure, pages 81?
83, Montre?al, Canada, June.
Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky. 2010. From Baby Steps to Leapfrog: How ?Less
is More? in Unsupervised Dependency Parsing. In Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 751?759,
Los Angeles, California, June.
Mark Steedman. 1996. Surface Structure and Interpre-
tation. The MIT Press, January.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, September.
Yee-Whye Teh, Michael I Jordan, Matthew J Beal, and
David M Blei. 2006. Hierarchical Dirichlet Pro-
86
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566?1581.
Yee-Whye Teh. 2006. A Hierarchical Bayesian Lan-
guage Model based on Pitman-Yor Processes. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages
985?992, Sydney, Australia, July.
Yee-Whye Teh. 2010. Dirichlet Process. In Encyclope-
dia of Machine Learning, pages 280?287. Springer.
Kewei Tu. 2012. Combining the Sparsity and Unambi-
guity Biases for Grammar Induction. In NAACL HLT
Workshop on Induction of Linguistic Structure, pages
105?110, Montre?al, Canada, June.
87
88
From image descriptions to visual denotations:
New similarity metrics for semantic inference over event descriptions
Peter Young Alice Lai Micah Hodosh Julia Hockenmaier
Department of Computer Science
University of Illinois at Urbana-Champaign
{pyoung2, aylai2, mhodosh2, juliahmr}@illinois.edu
Abstract
We propose to use the visual denotations of
linguistic expressions (i.e. the set of images
they describe) to define novel denotational
similarity metrics, which we show to be at
least as beneficial as distributional similarities
for two tasks that require semantic inference.
To compute these denotational similarities, we
construct a denotation graph, i.e. a subsump-
tion hierarchy over constituents and their de-
notations, based on a large corpus of 30K im-
ages and 150K descriptive captions.
1 Introduction
The ability to draw inferences from text is a prereq-
uisite for language understanding. These inferences
are what makes it possible for even brief descrip-
tions of everyday scenes to evoke rich mental im-
ages. For example, we would expect an image of
people shopping in a supermarket to depict aisles
of produce or other goods, and we would expect
most of these people to be customers who are either
standing or walking around. But such inferences
require a great deal of commonsense world knowl-
edge. Standard distributional approaches to lexical
similarity (Section 2.1) are very effective at iden-
tifying which words are related to the same topic,
and can provide useful features for systems that per-
form semantic inferences (Mirkin et al., 2009), but
are not suited to capture precise entailments between
complex expressions. In this paper, we propose a
novel approach for the automatic acquisition of de-
notational similarities between descriptions of ev-
eryday situations (Section 2). We define the (visual)
denotation of a linguistic expression as the set of im-
ages it describes. We create a corpus of images of
everyday activities (each paired with multiple cap-
tions; Section 3) to construct a large scale visual de-
notation graph which associates image descriptions
with their denotations (Section 4). The algorithm
that constructs the denotation graph uses purely syn-
tactic and lexical rules to produce simpler captions
(which have a larger denotation). But since each
image is originally associated with several captions,
the graph can also capture similarities between syn-
tactically and lexically unrelated descriptions. We
apply these similarities to two different tasks (Sec-
tions 6 and 7): an approximate entailment recogni-
tion task for our domain, where the goal is to decide
whether the hypothesis (a brief image caption) refers
to the same image as the premises (four longer cap-
tions), and the recently introduced Semantic Textual
Similarity task (Agirre et al., 2012), which can be
viewed as a graded (rather than binary) version of
paraphrase detection. Both tasks require semantic
inference, and our results indicate that denotational
similarities are at least as effective as standard ap-
proaches to similarity. Our code and data set, as
well as the denotation graph itself and the lexical
similarities we define over it are available for re-
search purposes at http://nlp.cs.illinois.edu/
Denotation.html.
2 Towards Denotational Similarities
2.1 Distributional Similarities
The distributional hypothesis posits that linguistic
expressions that appear in similar contexts have a
67
Transactions of the Association for Computational Linguistics, 2 (2014) 67?78. Action Editor: Lillian Lee.
Submitted 6/2013; Revised 10/2013; Published 2/2014. c?2014 Association for Computational Linguistics.
Gray haired man in black suit and yellow tie working in a financial environment.
A graying man in a suit is perplexed at a business meeting.
A businessman in a yellow tie gives a frustrated look.
A man in a yellow tie is rubbing the back of his neck.
A man with a yellow tie looks concerned.
A butcher cutting an animal to sell.
A green-shirted man with a butcher?s apron uses a knife to carve out the hanging carcass of a cow.
A man at work, butchering a cow.
A man in a green t-shirt and long tan apron hacks apart the carcass of a cow
while another man hoses away the blood.
Two men work in a butcher shop; one cuts the meat from a butchered cow, while the other hoses the floor.
Figure 1: Two images from our data set and their five captions
similar meaning (Harris, 1954). This has led to the
definition of vector-based distributional similarities,
which represent each word w as a vector w derived
from counts of w?s co-occurrence with other words.
These vectors can be used directly to compute the
lexical similarities of words, either via the cosine
of the angle between them, or via other, more com-
plex metrics (Lin, 1998). More recently, asymmetric
similarities have been proposed as more suitable for
semantic inference tasks such as entailment (Weeds
and Weir, 2003; Szpektor and Dagan, 2008; Clarke,
2009; Kotlerman et al., 2010). Distributional word
vectors can also be used to define the compositional
similarity of longer strings (Mitchell and Lapata,
2010). To compute the similarity of two strings, the
lexical vectors of the words in each string are first
combined into a single vector (e.g. by element-wise
addition or multiplication), and then an appropriate
vector similarity (e.g. cosine) is applied to the re-
sulting pair of vectors.
2.2 Visual Denotations
Our approach is inspired by truth-conditional se-
mantic theories in which the denotation of a declar-
ative sentence is assumed to be the set of all situa-
tions or possible worlds in which the sentence is true
(Montague, 1974; Dowty et al., 1981; Barwise and
Perry, 1980). Restricting our attention to visually
descriptive sentences, i.e. non-negative, episodic
(Carlson, 2005) sentences that can be used to de-
scribe an image (Figure 1), we propose to instantiate
the abstract notions of possible worlds or situations
with concrete sets of images. The interpretation
function J?K maps sentences to their visual denota-
tions JsK, which is the set of images i ? Us ? U in
a ?universe? of images U that s describes:
JsK = {i ? U | s is a truthful description of i} (1)
Similarly, we map nouns and noun phrases to the
set of images that depict the objects they describe,
and verbs and verb phrases to the set of images that
depict the events they describe.
2.3 Denotation Graphs
Denotations induce a partial ordering over descrip-
tions: if s (e.g. ?a poodle runs on the beach?) en-
tails a description s? (e.g. ?a dog runs?), its denota-
tion is a subset of the denotation of s? (JsK ? Js?K),
and we say that s? subsumes the more specific s
(s? v s). In our domain of descriptive sentences,
we can obtain more generic descriptions by simple
syntactic and lexical operations ? ? O ? S ? S
that preserve upward entailment, so that if ?(s) =
s?, JsK ? Js?K. We consider three types of oper-
ations: the removal of optional material (e.g PPs
like on the beach), the extraction of simpler con-
stituents (NPs, VPs, or simple Ss), and lexical sub-
stitutions of nouns by their hypernyms (poodle ?
dog). These operations are akin to the atomic ed-
its of MacCartney and Manning (2008)?s NatLog
system, and allow us to construct large subsump-
tion hierarchies over image descriptions, which we
call denotation graphs. Given a set of (upward
entailment-preserving) operations O ? S ? S, the
denotation graph DG = ?E, V ? of a set of images I
and a set of strings S represents a subsumption hier-
archy in which each node V = ?s, JsK? corresponds
to a string s ? S and its denotation JsK ? I . Di-
rected edges e = (s, s?) ? E ? V ? V indicate a
subsumption relation s v s? between a more generic
expression s and its child s?. An edge from s to s?
68
exists if there is an operation ? ? O that reduces the
string s? to s (i.e. ?(s?) = s) and its inverse ??1
expands the string s to s? (i.e. ??1(s) = s?).
2.4 Denotational Similarities
Given a denotation graph over N images, we esti-
mate the denotational probability of an expression s
with a denotation of size |JsK| as PJK(s) = |JsK|/N ,
and the joint probability of two expressions analo-
gously as PJK(s, s?) = |JsK ? Js?K|/N . The condi-
tional probability PJK(s | s?) indicates how likely
s is to be true when s? holds, and yields a simple
directed denotational similarity. The (normalized)
pointwise mutual information (PMI) (Church and
Hanks, 1990) defines a symmetric similarity:
nPMI JK(s, s?) =
log
( PJK(s,s?)
PJK(s)PJK(s?)
)
? log(PJK(s, s?))
We set PJK(s|s) = nPMI JK(s, s) = 1, and, if s or
s? are not in the denotation graph, nPMI JK(s, s?) =
PJK(s, s?) = 0.
3 Our Data Set
Our data set (Figure 1) consists of 31,783 pho-
tographs of everyday activities, events and scenes
(all harvested from Flickr) and 158,915 captions
(obtained via crowdsourcing). It contains and ex-
tends Hodosh et al. (2013)?s corpus of 8,092 im-
ages. We followed Hodosh et al. (2013)?s approach
to collect images. We also use their annotation
guidelines, and use similar quality controls to cor-
rect spelling mistakes, eliminate ungrammatical or
non-descriptive sentences. Almost all of the im-
ages that we add to those collected by Hodosh et
al. (2013) have been made available under a Cre-
ative Commons license. Each image is described in-
dependently by five annotators who are not familiar
with the specific entities and circumstances depicted
in them, resulting in captions such as ?Three people
setting up a tent?, rather than the kind of captions
people provide for their own images (?Our trip to
the Olympic Peninsula?). Moreover, different an-
notators use different levels of specificity, from de-
scribing the overall situation (performing a musical
piece) to specific actions (bowing on a violin). This
variety of descriptions associated with the same im-
age is what allows us to induce denotational similari-
ties between expressions that are not trivially related
by syntactic rewrite rules.
4 Constructing the Denotation Graph
The construction of the denotation graph consists
of the following steps: preprocessing and linguistic
analysis of the captions, identification of applicable
transformations, and generation of the graph itself.
Preprocessing and Linguistic Analysis We use
the Linux spell checker, the OpenNLP tok-
enizer, POS tagger and chunker (http://opennlp.
apache.org), and the Malt parser (Nivre et al.,
2006) to analyze the captions. Since the vocabulary
of our corpus differs significantly from the data these
tools are trained on, we resort to a number of heuris-
tics to improve the analyses they provide. Since
some heuristics require us to identify different entity
types, we developed a lexicon of the most common
entity types in our domain (people, clothing, bodily
appearance (e.g. hair or body parts), containers of
liquids, food items and vehicles).
After spell-checking, we normalize certain words
and compounds with several spelling variations, e.g.
barbecue (barbeque, BBQ), gray (grey), waterski
(water ski), brown-haired (brown haired), and to-
kenize the captions using the OpenNLP tokenizer.
The OpenNLP POS tagger makes a number of sys-
tematic errors on our corpus (e.g. mistagging main
verbs as nouns). Since these errors are highly sys-
tematic, we are able to correct them automatically
by applying deterministic rules (e.g. climbs is never
a noun in our corpus, stand is a noun if it is pre-
ceded by vegetable but a verb when preceded by a
noun that refers to people). These fixes apply to
27,784 (17% of the 158,915 image captions). Next,
we use the OpenNLP chunker to create a shallow
parse. Fixing its (systematic) errors affects 28,587
captions. We then analyze the structure of each
NP chunk to identify heads, determiners and pre-
nominal modifiers. The head may include more than
a single token if WordNet (or our hypernym lexi-
con, described below) contains a corresponding en-
try (e.g. little girl). Determiners include phrases
such as a couple or a few. Although we use the
Malt parser (Nivre et al., 2006) to identify subject-
verb-object dependencies, we have found it more ac-
curate to develop deterministic heuristics and lexi-
69
cal rules to identify the boundaries of complex (e.g.
conjoined) NPs, allowing us to treat ?a man with red
shoes and a white hat? as an NP followed by a sin-
gle PP, but ?a man with red shoes and a white-haired
woman? as two NPs, and to transform e.g. ?stand-
ing by a man and a woman? into ?standing? and not
?standing and a woman? when dropping the PP.
Hypernym Lexicon We use our corpus and Word-
Net to construct a hypernym lexicon that allows us
to replace head nouns with more generic terms. We
only consider hypernyms that occur themselves with
sufficient frequency in the original captions (replac-
ing ?adult? with ?person?, but not with ?organ-
ism?). Since the language in our corpus is very
concrete, each noun tends to have a single sense, al-
lowing us to always replace it with the same hyper-
nyms.1 But since WordNet provides us with mul-
tiple senses for most nouns, we first have to iden-
tify which sense is used in our corpus. To do this,
we use the heuristic cross-caption coreference algo-
rithm of Hodosh et al. (2010) to identify coreferent
NP chunks among the original five captions of each
image.2 For each ambiguous head noun, we con-
sider every non-singleton coreference chains it ap-
pears in, and reduce its synsets to those that stand
in a hypernym-hyponym relation with at least one
other head noun in the chain. Finally, we apply a
greedy majority voting algorithm to iteratively nar-
row down each term?s senses to a single synset that
is compatible with the largest number of coreference
chains it occurs in.
Caption Normalization In order to increase the
recall of the denotations we capture, we drop all
punctuation marks, and lemmatize nouns, verbs, and
adjectives that end in ?-ed? or ?-ing? before gener-
1Descriptions of people that refer to both age and gen-
der (e.g. ?man?) can have multiple distinct hypernyms
(?adult?/??male?). Because our annotators never describe
young children or babies as ?persons?, we only allow terms
that are likely to describe adults or teenagers (including occu-
pations) to be replaced by the term ?person?. This means that
the term ?girl? has two senses: a female child (the default) or a
younger woman. We distinguish the two senses in a preprocess-
ing step: if the other captions of the same image do not mention
children, but refer to teenaged or adult women, we assign girl
the woman-sense. Some nouns that end in -er (e.g. ?diner?,
?pitcher? also violate our monosemy assumption.
2Coreference resolution has also been used for word sense
disambiguation by Preiss (2001) and Hu and Liu (2011).
ating the denotation graph. In order to distinguish
between frequently occurring homonyms where the
noun is unrelated to the verb, we change all forms of
the verb dress to dressed, all forms of the verb stand
to standing and all forms of the verb park to park-
ing. Finally, we drop sentence-initial there/here/this
is/are (as in there is a dog splashing in the water),
and normalize the expressions in X and dressed (up)
in X (where X is an article of clothing or a color) to
wear X. We reduce plural determiners to {two, three,
some}, and drop singular determiners except for no.
4.1 Rule Templates
The denotation graph contains a directed edge from
s to s? if there is a rule ? that reduces s? to s, with an
inverse ??1 that expands s to s?. Reduction rules can
drop optional material, extract simpler constituents,
or perform lexical substitutions.
Drop Pre-Nominal Modifiers: ?red shirt? ?
?shirt? In an NP of the form ?X Y Z?, where
X and Y both modify the head Z, we only allow
X and Y to be dropped separately if ?X Z? and
?Y Z? both occur elsewhere in the corpus. Since
?white building? and ?stone building? occur else-
where in the corpus, we generate both ?white build-
ing? and ?stone building? from the NP ?white stone
building?. But since ?ice player? is not used,
we replace ?ice hockey player? only with ?hockey
player? (which does occur) and then ?player?.
Drop Other Modifiers ?run quickly? ? ?run?
We drop ADVP chunks and adverbs in VP chunks.
We also allow a prepositional phrase (a preposi-
tion followed by a possibly conjoined NP chunk)
to be dropped if the preposition is locational
(?in?, ?on?, ?above?, etc.), directional (?towards?,
?through?, ?across?, etc.), or instrumental (?by?,
?for?, ?with?). Similarly, we also allow the drop-
ping of all ?wear NP? constructions. Since the dis-
tinction between particles and prepositions is often
difficult, we also use a predefined list of phrasal
verbs that commonly occur in our corpus to identify
constructions such as ?climb up a mountain?, which
is transformed into ?climb a mountain? or ?walk
down a street?, which is transformed into ?walk?.
Replace Nouns by Hypernyms: ?red shirt? ?
?red clothing? We iteratively use our hypernym
70
GENERATEGRAPH():
Q,Captions,Rules? ?
for all c ? ImageCorpus do
Rules(c)? GenerateRules(sc)
pushAll(Q, {c} ? RootNodes(sc,Rules(c)))
while ?empty(Q) do
(c, s)? pop(Q)
Captions(s)? Captions(s) ? {c}
if |Captions(s)| = 2 then
for all c? ? Captions(s) do
pushAll(Q, {c?} ? Children(s,Rules(c?)))
else if |Captions(s)| > 2 then
pushAll(Q, {c} ? Children(s,Rules(c)))
Figure 2: Generating the graph
lexicon to make head nouns more generic. We only
allow head nouns to be replaced by their hypernyms
if any age based modifiers have already been re-
moved: ?toddler? can be replaced with ?child?, but
not ?older toddler? with ?older child?.
Handle Partitive NPs: cup of tea? ?cup?, ?tea?
In most partitive NP1-of-NP2 constructions (?cup of
tea?, ?a team of football players?) the correspond-
ing entity can be referred to by both the first or the
second NP. Exceptions include the phrase ?body of
water?, and expressions such as ?a kind/type/sort
of?, which we treat similar to determiners.
Handle VP1-to-VP2 Cases Depending on the first
verb, we replace VPs of the form X to Y with both X
and Y if X is a movement or posture (jump to catch,
etc.). Otherwise we distinguish between cases we
can only replace with X (wait to jump) and those we
can only replace with Y (seem to jump).
Extract Simpler Constituents Any noun phrase
or verb phrase can also be used as a node in the
graph and simplified further. We use the Malt de-
pendencies (and the person terms in the entity type
lexicon) to identify and extract subject-verb-object
chunks which correspond to simpler sentences that
we would otherwise not be able to obtain: from
?man laugh(s) while drink(ing)?, we extract ?man
laugh? and ?man drink?, and then further split those
into ?man?, ?laugh(s)?, and ?drink?.
4.2 Graph Generation
The naive approach to graph generation would be to
generate all possible strings for each caption. How-
ever, this would produce far more strings than can be
processed in a reasonable amount of time, and most
of these strings would have uninformative denota-
tions, consisting of only a single image. To make
graph generation tractable, we use a top-down al-
gorithm which generates the graph from the most
generic (root) nodes, and stops at nodes that have a
singleton denotation (Figure 2). We first identify the
set of rules that can apply to each original caption
(GenerateRules). These rules are then used to re-
duce each caption as much as possible. The resulting
(maximally generic) strings are added as root nodes
to the graph (RootNodes), and added to the queue
Q. Q keeps track of all currently possible node ex-
pansions. It contains items ?c, s?, which pair the ID
of an original caption and its image (c) with a string
(s) that corresponds to an existing node in the graph
and can be derived from c?s caption. When ?c, s? is
processed, we check how many captions have gen-
erated s so far (Captions(s)). If s has more than a
single caption, we use each of the applicable rewrite
rules of c?s caption to create new strings s? that cor-
respond to the children of s in the graph, and push
all resulting ?c, s?? onto Q. If c is the second caption
of s, we also use all of the applicable rewrite rules
from the first caption c? to create its children.
A post-processing step (not shown in Figure 2)
attaches each original caption to all leaf nodes of the
graph to which it can be reduced. Finally, we obtain
the denotation of each node s from the set of images
whose captions are in Captions(s).
5 The Denotation Graph
Size and Coverage On our corpus of 158,439
unique captions and 31,783 images, the denotation
graph contains 1,749,097 captions, out of which
230,811 describe more than a single image. Ta-
ble 1 provides the distribution of the size of deno-
tations. It is perhaps surprising that the 161 cap-
tions which describe each over 1,000 images do
not just consist of nouns such as person, but also
contain simple sentences such as woman standing,
adult work, person walk street, or person play in-
strument. Since the graph is derived from the origi-
nal captions by very simple syntactic operations, the
denotations it captures are most likely incomplete:
Jsoccer playerK contains 251 images, Jplay soccerK
contains 234 images, and Jsoccer gameK contains
71
Size of denotations |JsK| ? 1 |JsK| ? 2 |JsK| ? 5 |JsK| ? 10 |JsK| ? 100 |JsK| ? 1000
Nr. of captions 1,749,096 230,811 53,341 22,683 1,921 161
Table 1: Distribution of the size of denotations in our graph
119 images. We have not yet attempted to iden-
tify variants in word order (?stick tongue out? vs.
?stick out tongue?) or equivalent choices of prepo-
sition (?look into mirror? vs. ?look in mirror?). De-
spite this brittleness, the current graph already gives
us a large number of semantic associations.
Denotational Similarities The following exam-
ples of the similarities found by nPMI JK and PJK
show that denotational similarities do not simply
find topically related events, but instead find events
that are related by entailment:
PJK(x|y) x y
0.962 sit eat lunch
0.846 play guitar strum
0.811 surf catch wave
0.800 ride horse rope calf
0.700 listen sit in classroom
If someone is eating lunch, it is likely that they
are sitting, and people who sit in a classroom are
likely to be listening to somebody. These entail-
ments can be very precise: ?walk up stair? entails
?ascend?, but not ?descend?; the reverse is true for
?walk down stair?:
PJK(x|y) x =ascend x =descend
y =walk up stair 32.0 0.0
y =walk down stair 0.0 30.8
nPMI JK captures paraphrases as well as closely
related events: people look in a mirror when shav-
ing their face, and baseball players may try to tag
someone who is sliding into base:
nPMI JK x y
0.835 open present unwrap
0.826 lasso try to rope
0.791 get ready to kick run towards ball
0.785 try to tag slide into base
0.777 shave face look in mirror
Comparing the expressions that are most similar
to ?play baseball? or ?play football? according to
the denotational nPMI JK and the compositional ?
similarities reveals that the denotational similarity
finds a number of actions that are part of the partic-
ular sport, while the compositional similarity finds
events that are similar to playing baseball (football):
play baseball
nPMI JK ?
0.674 tag him 0.859 play softball
0.637 hold bat 0.782 play game
0.616 try to tag 0.768 play ball
0.569 slide into base 0.741 play catch
0.516 pitch ball 0.739 play cricket
play football
nPMI JK ?
0.623 tackle person 0.826 play game
0.597 hold football 0.817 play rugby
0.545 run down field 0.811 play soccer
0.519 wear white jersey 0.796 play on field
0.487 avoid 0.773 play ball
6 Task 1: Approximate Entailment
A caption never provides a complete description of
the depicted scene, but commonsense knowledge
often allows us to draw implicit inferences: when
somebody mentions a bride, it is quite likely that the
picture shows a woman in a wedding dress; a pic-
ture of a parent most likely also has a child or baby,
etc. In order to compare the utility of denotational
and distributional similarities for drawing these in-
ferences, we apply them to an approximate entail-
ment task, which is loosely modeled after the Rec-
ognizing Textual Entailment problem (Dagan et al.,
2006), and consists of deciding whether a brief cap-
tion h (the hypothesis) can describe the same image
as a set of captions P = {p1, ...,pN} known to de-
scribe the same image (the premises).
Data We generate positive and negative items
?P,h,?? (Figure 3) as follows: Given an image,
any subset of four of its captions form a set of
premises. A hypothesis is either a short verb phrase
or sentence that corresponds to a node in the deno-
tation graph. By focusing on short hypotheses, we
minimize the possibility that they contain extrane-
ous details that cannot be inferred from the premises.
Positive examples are generated by choosing a node
h as hypothesis and an image i ? JhK such that ex-
actly one caption of i generates h and the other four
captions of i are not descendants of h and hence
do not trivially entail h, giving an unfair advantage
to denotational approaches. Negative examples are
generated by choosing a node h as hypothesis and
selecting four of the captions of an image i 6? JhK.
72
Premises: A woman with dark hair in bending, open mouthed, towards the back of a dark headed toddler?s head.
A dark-haired woman has her mouth open and is hugging a little girl while sitting on a red blanket.
A grown lady is snuggling on the couch with a young girl and the lady has a frightened look.
A mom holding her child on a red sofa while they are both having fun.
VP Hypothesis: make face
Premises: A man editing a black and white photo at a computer with a pencil in his ear.
A man in a white shirt is working at a computer.
A guy in white t-shirt on a mac computer.
A young main is using an Apple computer.
S Hypothesis: man sit
Figure 3: Positive examples from the Approximate Entailment tasks.
Since our items are created automatically, a posi-
tive hypothesis is not necessarily logically entailed
by its premises. We have performed a small-scale
human evaluation on 300 items (200 positive, 100
negative), each judged independently by the same
three judges (inter-annotator agreement: Fleiss-? =
0.74). Our results indicate that over half (55%) of
the positive hypotheses can be inferred from their
premises alone without looking at the original im-
age, while almost none of the negative hypotheses
(100% for sentences, 96% for verb phrases) can be
inferred from their premises. The training items are
generated from the captions of 25,000 images, and
the test items are generated from a disjoint set of
3,000 images. The VP data set consists of 290,000
training items and 16,000 test items, while the S data
set consists of 400,000 training items and 22,000 test
items. Half of the items in each set are positive, and
the other half are negative.
Models All of our models are binary MaxEnt clas-
sifiers, trained using MALLET (McCallum, 2002).
We have two baseline models: a plain bag-of-words
model (BOW) and a bag-of-words model where we
add all hypernyms in our lexicon to the captions be-
fore computing their overlap (BOW-H). This is in-
tended to minimize the advantage the denotational
features obtain from the hypernym lexicon used to
construct the denotation graph. In both cases, a
global BOW feature captures the fraction of tokens
in the hypothesis that are contained in the premises.
Word-specific BOW features capture the product of
the frequencies of each word in h and P. All other
models extend the BOW-H model.
Denotational Similarity Features We compute
denotational similarities nPMI JK and PJK (Sec-
tion 2.4) over the pairs of nodes in a denotation
graph that is restricted to the training images. We
only consider pairs of nodes n,n? if their denota-
tions contain at least 10 images and their intersection
contains at least 2 images.
To map an item ?P,h? to denotational simi-
larity features, we represent the premises as the
set of all nodes P that are ancestors of its cap-
tions. A sentential hypothesis is represented as
the set of nodes H = {hS , hsbj , hV P , hv, hdobj}
that correspond to the sentence (h itself), its sub-
ject, its VP and its direct object. A VP hypothe-
sis has only the nodes H = {hV P , hv, hdobj}. In
both cases, hdobj may be empty. Both of the de-
notational similarities nPMI JK(h, p) and PJK(h|p)
for h ? H , p ? P lead to two constituent-
specific features, sumx and maxx, (e.g. sumsbj =?
p sim(hsbj , p), maxdobj = maxp sim(hdobj , p))
and two global features sump,h = ?p,h sim(h, p)
and maxp,h = maxp,h sim(h, p). Each constituent
type also has a set of node-specific sumx,s and
maxx,s features that are on when constituent x in
h is equal to the string s and whose value is equal
to the constituent-based feature. For PJK, each con-
stituent (and each constituent-node pair) has an ad-
ditional feature P (h|P ) = 1 ??n(1 ? PJK(h|pn))
that estimates the probability that h is generated by
some node in the premise.
Lexical Similarity Features We use two sym-
metric lexical similarities: standard cosine distance
(cos), and Lin (1998)?s similarity (Lin):
cos(w,w?) = w?w??w??w??
Lin(w,w?) =
?
i:w(i)>0?w?(i)>0w(i)+w
?(i)?
iw(i)+
?
iw?(i)
73
We use two directed lexical similarities: Clarke
(2009)?s similarity (Clk), and Szpektor and Dagan
(2008)?s balanced precision (Bal), which builds on
Lin and on Weeds and Weir (2003)?s similarity (W):
Clk(w | w?) =
?
i:w(i)>0?w?(i)>0 min(w(i),w?(i))?
iw(i)
Bal(w | w?) =
?
W(w | w?)? Lin(w,w?)
W(w | w?) =
?
i:w(i)>0?w?(i)>0 w(i)?
iw(i)
We also use two publicly available resources that
provide precomputed similarities, Kotlerman et al.
(2010)?s DIRECT noun and verb rules and Chklovski
and Pantel (2004)?s VERBOCEAN rules. Both are
motivated by the need for numerically quantifiable
semantic inferences between predicates. We only
use entries that correspond to single tokens (ignor-
ing e.g. phrasal verbs).
Each lexical similarity results in the follow-
ing features: words in the output are represented
by a max-simw feature which captures its max-
imum similarity with any word in the premises
(max-simw = maxw??P sim(w,w?)) and by a
sum-simw feature which captures the sum of its sim-
ilarities to the words in the premises (sum-simw =?
w??P sim(w,w?)). Global max sim and sum sim
features capture the maximal (resp. total) similarity
of any word in the hypothesis to the premise.
We compute distributional and compositional
similarities (cos, Lin, Bal, Clk, ?, ?) on our im-
age captions (?cap?), the BNC and Gigaword. For
each corpus C, we map each word w that appears
at least 10 times in C to a vector wC of the non-
negative normalized pointwise mutual information
scores (Section 2.4) of w and the 1,000 words (ex-
cluding stop words) that occur in the most sentences
of C. We generally define P (w) (and P (w,w?)) as
the fraction of sentences in C in which w (and w?)
occur. To allow a direct comparison between dis-
tributional and denotational similarities, we first de-
fine P (w) (and P (w,w?)) over individual captions
(?cap?), and then, to level the playing field, we rede-
fine P (w) (and P (w,w?)) as the fraction of images
in whose captions w (and w?) occur (?img?), and
then we use our lexicon to augment captions with
all hypernyms (?+hyp?). Finally, we include BNC
and Gigaword similarity features (?all?).
VP task S task
Baseline 1: BoW 58.7 71.2
Baseline 2: BoW-H 59.0 73.6
External 1: DIRECT 59.2 73.5
External 2: VerbOcean 60.8 74.0
Cap All Cap All
Distributional cos 67.5 71.9 76.1 78.9
Distributional Lin 62.6 70.2 75.4 77.8
Distributional Bal 62.3 69.6 74.7 75.3
Distributional Clk 62.4 69.2 75.4 77.5
Compositional ? 68.4 70.3 75.3 77.3
Compositional ? 67.8 71.4 76.9 79.2
Compositional ?,? 69.8 72.7 77.0 79.6
Denotational nPMI JK 74.9 80.2
Denotational PJK 73.8 79.5
nPMI JK, PJK 75.5 81.2
Combined cos, ?,? 71.1 72.6 77.4 79.2
nPMI JK, PJK, ?,? 75.6 75.9 80.2 80.7
nPMI JK, PJK, cos 75.6 75.7 80.2 81.2
nPMI JK, PJK, cos, ?,? 75.8 75.9 81.2 80.5
Table 2: Test accuracy on Approximate Entailment.
Compositional Similarity Features We use two
standard compositional baselines to combine the
word vectors of a sentence into a single vector: ad-
dition (s? = w1 + ... + wn, which can be inter-
preted as a disjunctive operation), and element-wise
(Hadamard) multiplication (s? = w1  ...  wn,
which can be seen as a conjunctive operation). In
both cases, we represent the premises (which con-
sist of four captions) as a the sum of each caption?s
vector p = p1 + ...p4. This gives two composi-
tional similarity features: ? = cos(p?,h?), and
? = cos(p?,h?).
6.1 Experimental Results
Table 2 provides the test accuracy of our mod-
els on the VP and S tasks. Adding hypernyms
(BOW-H) yields a slight improvement over the ba-
sic BOW model. Among the external resources,
VERBOCEAN is more beneficial than DIRECT, but
neither help as much as in-domain distributional
similarities (this may be due to sparsity).
Table 2 shows only the simplest (?Cap?) and
the most complex (?all?) distributional and com-
positional models, but Table 3 provides accuracies
of these models as we go from standard sentence-
based co-occurrence counts towards more denota-
tion graph-like co-occurrence counts that are based
on all captions describing the same image (?Img?),
74
VP task S task
Cap Img +Hyp All Cap Img +Hyp All
cos 67.5 69.3 69.8 71.9 76.1 76.8 77.5 78.9
Lin 62.6 63.4 61.3 70.0 75.4 74.8 75.2 77.8
Bal 62.3 61.9 62.8 69.6 74.7 75.5 75.1 75.3
Clk 62.4 67.3 68.0 69.2 75.4 75.5 76.0 77.5
? 68.4 70.5 70.5 70.3 75.3 76.6 77.1 77.3
? 67.8 71.4 71.6 71.4 76.9 78.1 79.1 79.2
?,? 69.8 72.7 72.9 72.7 77.0 78.6 79.3 79.6
nPMI JK 74.9 80.2
PJK 73.8 79.5
nPMI JK, PJK 75.5 81.2
Table 3: Accuracy on hypotheses as various additions are
made to the vector corpora. Cap is the image corpus with
caption co-occurrence. Img is the image corpus with im-
age co-occurrence. +Hyp augments the image corpus
with hypernyms and uses image co-occurrence. All adds
the BNC and Gigaword corpora to +Hyp.
VP task S task
Words in h 1 2 3+ 2 3 4+
% of items 72.8 13.9 13.3 65.3 22.8 11.9
BoW-H 52.0 75.0 80.1 69.1 80.8 84.4
cos (All) 68.8 79.4 81.1 75.9 83.9 85.7? (All) 68.1 80.8 79.5 76.5 83.9 85.1
nPMI JK 72.0 82.9 82.2 77.3 85.4 86.2
Table 4: Accuracy on hypotheses of varying length.
include hypernyms (?+Hyp?), and add informa-
tion from other corpora (?All?). The ?+Hyp? col-
umn in Table 3 shows that the denotational metrics
clearly outperform any distributional metric when
both have access to the same information. Al-
though the distributional models benefit from the
BNC and Gigaword-based similarities (?All?), their
performance is still below that of the denotational
models. Among the distributional model, the simple
cos performs better than Lin, or the directed Clk and
Bal similarities. In all cases, giving models access to
different similarity features improves performance.
Table 4 shows the results by hypothesis length.
As the length of h increases, classifiers that use sim-
ilarities between pairs of words (BOW-H and cos)
continue to improve in performance relative to the
classifiers that use similarities between phrases and
sentences (? and nPMI JK). Most likely, this is due
to the lexical similarities having a larger set of fea-
tures to work with for longer h. nPMI JK does espe-
cially well on shorter h, likely due to the shorter h
having larger denotations.
7 Task 2: Semantic Textual Similarity
To assess how the denotational similarities perform
on a more established task and domain, we apply
them to the 1500 sentence pairs from the MSR Video
Description Corpus (Chen and Dolan, 2011) that
were annotated for the SemEval 2012 Semantic Tex-
tual Similarity (STS) task (Agirre et al., 2012). The
goal of this task is to assign scores between 0 and 5
to a pair of sentences, where 5 indicates equivalence,
and 0 unrelatedness. Since this is a symmetric task,
we do not consider directed similarities. And be-
cause the goal of this experiment is not to achieve
the best possible performance on this task, but to
compare the effectiveness of denotational and more
established similarities, we only compare the impact
of denotational similarities with compositional sim-
ilarities computed on our own corpus. Since the
MSR Video corpus associates each video with mul-
tiple sentences, it is in principle also amenable to a
denotational treatment, but the STS task description
explicitly forbids its use.
7.1 Models
Baseline and Compositional Features Our start-
ing point is Ba?r et al. (2013)?s DKPro Similarity,
one of the top-performing models from the 2012
STS shared task, which is available and easily mod-
ified. It consists of a log-linear regression model
trained on multiple text features (word and charac-
ter n-grams, longest common substring and longest
common subsequence, Gabrilovich and Markovitch
(2007)?s Explicit Semantic Analysis, and Resnik
(1995)?s WordNet-based similarity). We investigate
the effects of adding compositional (computed on
the vectors obtained from the image-caption train-
ing data) and denotational similarity features to this
state-of-the-art system.
Denotational Features Since the STS task is
symmetric, we only consider nPMI JK similari-
ties. We again represent each sentence s by fea-
tures based on 5 types of constituents: S =
{sS , ssbj , sV P , sv, sdobj}. Since sentences might be
complex, they might contain multiple constituents
of the same type, and we therefore think of each
feature as a feature over sets of nodes. For each
constituent C we consider two sets of nodes in the
denotation graph: C itself (typically leaf nodes),
75
DKPro +?,? (img) +nPMI JK +both
Pearson r 0.868 0.880 0.888 0.890
Table 5: Performance on the STS MSRvid task: DKPro
(Ba?r et al., 2013) plus compositional (?,?) and/or deno-
tational similarities (nPMI JK) from our corpus
and Canc, their parents and grandparents. For
each pair of sentences, C-C similarities compute
the similarity of the constituents of the same type,
while C-all similarities compute the similarity of
a C constituent in one sentence against all con-
stituents in the other sentence. For each pair of
constituents we consider three similarity features:
sim(C1, C2), max(sim(C1Canc2 ), sim(Canc1 , C2)),
sim(Canc1 , Canc2 ). The similarity of two sets of
nodes is determined by the maximal similarity
of any pair of their elements: sim(C1, C2) =
maxc1?C1,c2?C2 nPMI JK(c1, c2). This gives us 15
C-C features and 15 C-all features.
7.2 Experiments
We use the STS 2012 train/test data, normalized in
the same way as the image captions for the deno-
tation graph (i.e. we re-tokenize, lemmatize, and
remove determiners). Table 5 shows experimental
results for four models: DKPro is the off-the-shelf
DKProSimilarity model (Ba?r et al., 2013). From
our corpus, we either add additive and multiplicative
compositional features (?,?) from Section 6 (img),
the C-C and C-All denotational features based on
nPMI JK, or both compositional and denotational
features. Systems are evaluated by the Pearson cor-
relation (r) of their predicted similarity scores to the
human-annotated ones. We see that the denotational
similarities outperform the compositional similari-
ties, and that including compositional similarity fea-
tures in addition to denotational similarity features
has little effect. For additional comparison, the
published numbers for the TakeLab Semantic Text
Similarity System (S?aric? et al., 2012), another top-
performing model from the 2012 shared task, are
r = 0.880 on this dataset.
8 Conclusion
Summary of Contributions We have defined
novel denotational metrics of linguistic similarity
(Section 2), and have shown them to be at least
competitive with, if not superior to, distributional
similarities for two tasks that require simple se-
mantic inferences (Sections 6, 7), even though our
current method of computing them is somewhat
brittle (Section 5). We have also introduced two
new resources: a large data set of images paired
with descriptive captions, and a denotation graph
that pairs generalized versions of these captions
with their visual denotations, i.e. the sets of im-
ages they describe. Both of these resources are
freely available (http://nlp.cs.illinois.edu/
Denotation.html) Although the aim of this paper
is to show their utility for a purely linguistic task,
we believe that they should also be of great interest
for people who aim to build systems that automat-
ically associate image with sentences that describe
them (Farhadi et al., 2010; Kulkarni et al., 2011; Li
et al., 2011; Yang et al., 2011; Mitchell et al., 2012;
Kuznetsova et al., 2012; Gupta et al., 2012; Hodosh
et al., 2013).
Related Work and Resources We believe that the
work reported in this paper has the potential to open
up promising new research directions. There are
other data sets that pair images or video with de-
scriptive language, but we have not yet applied our
approach to them. Chen and Dolan (2011)?s MSR
Video Description Corpus (of which the STS data
is a subset) is most similar to ours, but its curated
part is significantly smaller. Instead of several in-
dependent captions, Grubinger et al. (2006)?s IAPR
TC-12 data set contains longer descriptions. Or-
donez et al. (2011) harvested 1 million images and
their user-generated captions from Flickr to create
the SBU Captioned Photo Dataset. These captions
tend to be less descriptive of the image. The de-
notation graph is similar to Berant et al. (2012)?s
?entailment graph?, but differs from it in two ways:
first, entailment relations in the denotation graph
are defined extensionally in terms of the images de-
scribed by the expressions at each node, and sec-
ond, nodes in Berant et al.?s entailment graph corre-
spond to generic propositional templates (X treats
Y), whereas nodes in our denotation graph corre-
spond to complete propositions (a dog runs).
76
Acknowledgements
We gratefully acknowledge the support of the
National Science Foundation under NSF awards
0803603 ?INT2-Medium: Understanding the mean-
ing of images?, 1053856 ?CAREER: Bayesian Mod-
els for Lexicalized Grammars?, and 1205627 ?CI-
P:Collaborative Research: Visual entailment data
set and challenge for the Language and Vision Com-
munity?, as well as via an NSF Graduate Research
Fellowship to Alice Lai.
References
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 task 6: a pilot
on semantic textual similarity. In Proceedings of the
First Joint Conference on Lexical and Computational
Semantics - Volume 1: Proceedings of the main confer-
ence and the shared task, and Volume 2: Proceedings
of the Sixth International Workshop on Semantic Eval-
uation, SemEval ?12, pages 385?393.
Daniel Ba?r, Torsten Zesch, and Iryna Gurevych. 2013.
DKPro Similarity: An Open Source Framework for
Text Similarity. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguis-
tics: System Demonstrations, pages 121?126, Sofia,
Bulgaria, August.
Jon Barwise and John Perry. 1980. Situations and atti-
tudes. Journal of Philosophy, 78:668?691.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2012. Learning entailment relations by global graph
structure optimization. Computational Linguistics,
38(1):73?111.
Greg Carlson, 2005. The Encyclopedia of Language and
Linguistics, chapter Generics, Habituals and Iteratives.
Elsevier, 2nd edition.
David Chen and William Dolan. 2011. Collecting
highly parallel data for paraphrase evaluation. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 190?200, Portland, Oregon, USA,
June.
Timothy Chklovski and Patrick Pantel. 2004. Verbo-
cean: Mining the web for fine-grained semantic verb
relations. In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 33?40, Barcelona, Spain, July.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22?29.
Daoud Clarke. 2009. Context-theoretic semantics for
natural language: an overview. In Proceedings of
the Workshop on Geometrical Models of Natural Lan-
guage Semantics, pages 112?119, Athens, Greece,
March.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entailment
challenge. In Machine Learning Challenges, volume
3944 of Lecture Notes in Computer Science, pages
177?190. Springer.
David Dowty, Robert Wall, and Stanley Peters. 1981. In-
troduction to Montague Semantics. Reidel, Dordrecht.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi,
Peter Young, Cyrus Rashtchian, Julia Hockenmaier,
and David Forsyth. 2010. Every picture tells a
story: Generating sentences from images. In Proceed-
ings of the European Conference on Computer Vision
(ECCV), Part IV, pages 15?29, Heraklion, Greece,
September.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based ex-
plicit semantic analysis. In Proceedings of the 20th
international joint conference on Artifical intelligence,
IJCAI?07, pages 1606?1611.
Michael Grubinger, Paul Clough, Henning Mu?ller, and
Thomas Deselaers. 2006. The IAPR benchmark: A
new evaluation resource for visual information sys-
tems. In OntoImage 2006, Workshop on Language
Resources for Content-based Image Retrieval during
LREC 2006, pages 13?23, Genoa, Italy, May.
Ankush Gupta, Yashaswi Verma, and C. Jawahar. 2012.
Choosing linguistics over vision to describe images.
In Proceedings of the Twenty-Sixth AAAI Conference
on Artificial Intelligence, Toronto, Ontario, Canada,
July.
Zellig S Harris. 1954. Distributional structure. Word,
10:146?162.
Micah Hodosh, Peter Young, Cyrus Rashtchian, and Julia
Hockenmaier. 2010. Cross-caption coreference reso-
lution for automatic image understanding. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning, pages 162?171, Uppsala,
Sweden, July.
Micah Hodosh, Peter Young, and Julia Hockenmaier.
2013. Framing image description as a ranking task:
Data, models and evaluation metrics. Journal of Arti-
ficial Intelligence Research (JAIR), 47:853?899.
Shangfeng Hu and Chengfei Liu. 2011. Incorporating
coreference resolution into word sense disambigua-
tion. In Alexander F. Gelbukh, editor, Computational
Linguistics and Intelligent Text Processing, volume
6608 of Lecture Notes in Computer Science, pages
265?276. Springer Berlin Heidelberg.
77
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distributional
similarity for lexical inference. Natural Language En-
gineering, 16(4):359?389.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and generat-
ing simple image descriptions. In Proceedings of the
2011 IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), pages 1601?1608.
Polina Kuznetsova, Vicente Ordonez, Alexander Berg,
Tamara Berg, and Yejin Choi. 2012. Collective gener-
ation of natural image descriptions. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), pages
359?368, Jeju Island, Korea, July.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing sim-
ple image descriptions using web-scale n-grams. In
Proceedings of the Fifteenth Conference on Compu-
tational Natural Language Learning (CoNLL), pages
220?228, Portland, OR, USA, June.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning (ICML),
pages 296?304, Madison, WI, USA, July.
Bill MacCartney and Christopher D. Manning. 2008.
Modeling semantic containment and exclusion in nat-
ural language inference. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (Coling 2008), pages 521?528, Manchester, UK,
August.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.
Shachar Mirkin, Ido Dagan, and Eyal Shnarch. 2009.
Evaluating the inferential utility of lexical-semantic
resources. In Proceedings of the 12th Conference of
the European Chapter of the ACL (EACL 2009), pages
558?566, Athens, Greece, March.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Stratos, Xufeng Han, Alyssa Mensch,
Alex Berg, Tamara Berg, and Hal Daume III. 2012.
Midge: Generating image descriptions from computer
vision detections. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL), pages 747?756,
Avignon, France, April.
Richard Montague. 1974. Formal philosophy: papers
of Richard Montague. Yale University Press, New
Haven. Edited by Richmond H. Thomason.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of the International Confer-
ence on Language Resources and Evaluation (LREC),
pages 2216?2219.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2text: Describing images using 1 million
captioned photographs. In Advances in Neural Infor-
mation Processing Systems 24, pages 1143?1151.
Judita Preiss. 2001. Anaphora resolution with word
sense disambiguation. In Proceedings of SENSEVAL-
2 Second International Workshop on Evaluating
Word Sense Disambiguation Systems, pages 143?146,
Toulouse, France, July.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceedings
of the 14th international joint conference on Artificial
intelligence - Volume 1, IJCAI?95, pages 448?453.
Idan Szpektor and Ido Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (Coling 2008), pages 849?856, Manchester, UK,
August. Coling 2008 Organizing Committee.
Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder,
and Bojana Dalbelo Bas?ic?. 2012. Takelab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montre?al, Canada, 7-8 June.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
the 2003 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 81?88.
Yezhou Yang, Ching Teo, Hal Daume III, and Yiannis
Aloimonos. 2011. Corpus-guided sentence genera-
tion of natural images. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 444?454, Edin-
burgh, UK, July.
78
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 329?334,
Dublin, Ireland, August 23-24, 2014.
Illinois-LH: A Denotational and Distributional Approach to Semantics
Alice Lai and Julia Hockenmaier
Department of Computer Science
University of Illinois at Urbana-Champaign
{aylai2, juliahmr}@illinois.edu
Abstract
This paper describes and analyzes our Se-
mEval 2014 Task 1 system. Its features
are based on distributional and denota-
tional similarities; word alignment; nega-
tion; and hypernym/hyponym, synonym,
and antonym relations.
1 Task Description
SemEval 2014 Task 1 (Marelli et al., 2014a) eval-
uates system predictions of semantic relatedness
(SR) and textual entailment (TE) relations on sen-
tence pairs from the SICK dataset (Marelli et al.,
2014b). The dataset is intended to test compo-
sitional knowledge without requiring the world
knowledge that is often required for paraphrase
classification or Recognizing Textual Entailment
tasks. SR scores range from 1 to 5. TE relations
are ?entailment,? ?contradiction,? and ?neutral.?
Our system uses features that depend on the
amount of word overlap and alignment between
the two sentences, the presence of negation, and
the semantic similarities of the words and sub-
strings that are not shared across the two sen-
tences. We use simple distributional similarities
as well as the recently proposed denotational sim-
ilarities of Young et al. (2014), which are intended
as more precise metrics for tasks that require en-
tailment. Both similarity types are estimated on
Young et al.?s corpus, which contains 31,783 im-
ages of everyday scenes, each paired with five de-
scriptive captions.
2 Our System
Our system combines different sources of seman-
tic similarity to predict semantic relatedness and
This work is licensed under a Creative Commons At-
tribution 4.0 International License. Page numbers and pro-
ceedings footer are added by the organizers. License de-
tails: http://creativecommons.org/licenses/
by/4.0/
textual entailment. We use distributional sim-
ilarity features, denotational similarity features,
and alignment features based on shallow syntac-
tic structure.
2.1 Preprocessing
We lemmatize all sentences with the Stanford
CoreNLP system
1
and extract syntactic chunks
with the Illinois Chunker (Punyakanok and Roth,
2001). Like Young et al. (2014), we use the Malt
parser (Nivre et al., 2006) to identify 5 sets of con-
stituents for each sentence: subject NPs, verbs,
VPs, direct object NPs, and other NPs.
For stopwords, we use the NLTK English stop-
word list of 127 high-frequency words. We re-
move negation words (no, not, and nor) from the
stopword list since their presence is informative
for this dataset and task.
2.2 Distributional Similarities
After stopword removal and lemmatization, we
compute vectors for tokens that appear at least 10
times in Young et al. (2014)?s image description
corpus. In the vector space, each dimension corre-
sponds to one of the 1000 most frequent lemmas
(contexts). The jth entry of the vector of w
i
is the
positive normalized pointwise mutual information
(pnPMI) between target w
i
and context w
j
:
pnPMI(w
i
, w
j
) = max
?
?
0,
log
(
P (w
i
,w
j
)
P (w
i
)P (w
j
)
)
? log (P (w
i
, w
j
))
?
?
We define P (w
i
) as the fraction of images with
at least one caption containing w
i
, and P (w
i
, w
j
)
as the fraction of images whose captions contain
both w
i
and w
j
. Following recent work that ex-
tends distributional similarities to phrases and sen-
tences (Mitchell and Lapata, 2010; Baroni and
Zamparelli, 2010; Grefenstette and Sadrzadeh,
1
http://nlp.stanford.edu/software/
corenlp.shtml
329
Features Description # of features
Negation True if either sentence contains explicit negation; False otherwise 1
Word overlap Ratio of overlapping word types to total word types in s
1
and s
2
1
Denotational constituent similarity Positive normalized PMI of constituent nodes in the denotation
graph
30
Distributional constituent similarity Cosine similarity of vector representations of constituent phrases 30
Alignment Ratio of number of aligned words to length of s
1
and s
2
; max, min,
average unaligned chunk length; number of unaligned chunks
23
Unaligned matching Ratio of number of matched chunks to unaligned chunks; max, min,
average matched chunk similarity; number of crossings in matching
31
Chunk alignment Number of chunks; number of unaligned chunk labels; ratio of un-
aligned chunk labels to number of chunks; number of matched la-
bels; ratio of matched to unmatched chunk labels
17
Synonym Number of matched synonym pairs (w
1
, w
2
) 1
Hypernym Number of matched hypernym pairs (w
1
, w
2
), number of matched
hypernym pairs (w
2
, w
1
)
2
Antonym Number of matched antonym pairs (w
1
, w
2
) 1
Table 1: Summary of features.
2011; Socher et al., 2012), we define a phrase vec-
tor p to be the pointwise multiplication product of
the vectors of the words in the phrase:
p = w
1
 ... w
n
where  is the multiplication of corresponding
vector components, i.e. p
i
= u
i
? v
i
.
2.3 Denotational Similarities
In Young et al. (2014), we introduce denotational
similarities, which we argue provide a more pre-
cise metric for semantic inferences. We use an
image-caption corpus to define the (visual) de-
notation of a phrase as the set of images it de-
scribes, and construct a denotation graph, i.e. a
subsumption hierarchy (lattice) of phrases paired
with their denotations. For example, the denota-
tion of the node man is the set of images in the
corpus that contain a man, and the denotation of
the node person is rock climbing is the set of im-
ages that depict a person rock climbing. We de-
fine the (symmetric) denotational similarity of two
phrases as the pnPMI between their correspond-
ing sets of images. We associate each constituent
in the SICK dataset with a node in the denotation
graph, but new nodes that are unique to the SICK
data have no quantifiable similarity to other nodes
in the graph.
2.4 Features
Table 1 summarizes our features. Since TE is a
directional task and SR is symmetric, we express
features that depend on sentence order twice: 1)
f
1
are the features of s
1
and f
2
are the features of
s
2
, 2) f
1
are the features of the longer sentence
and f
2
are the features of the shorter sentence.
These directional features are specified in the
following feature descriptions.
Negation In this dataset, contradictory sentence
pairs are often marked by explicit negation, e.g. s
1
= ?The man is stirring the sauce for the chicken?
and s
2
= ?The man is not stirring the sauce for
the chicken.? A binary feature is set to 1 if either
sentence contains not, no, or nobody, and set to 0
otherwise.
Word Overlap We compute
|W
1
?W
2
|
|W
1
?W
2
|
on lemma-
tized sentences without stopwords where W
i
is
the set of word types that appear in s
i
. Training
a MaxEnt or log-linear model using this feature
achieves better performance than the word overlap
baseline provided by the task organizers.
Denotational Constituent Similarity Denota-
tional similarity captures entailment-like relations
between events. For example, sit and eat lunch
have a high pnPMI, which follows our intuition
that a person who is eating lunch is likely to be
sitting. We use the same denotational constituent
features that Young et al. (2014) use for a textual
similarity task. C are original nodes, C
anc
are par-
ent and grandparent nodes, and sim(C
a
, C
b
) is the
maximum pnPMI of any pair of nodes a ? C
a
,
b ? C
b
.
C-C features compare constituents of the same
type. These features express how often we expect
corresponding constituents to describe the same
situation. For example, s
1
= ?Girls are doing
backbends and playing outdoors? and s
2
= ?Chil-
330
dren are doing backbends? have subject nodes
{girl} and {child}. Girls are sometimes de-
scribed as children, so sim(girl, child) = 0.498.
In addition, child is a parent node of girl, so
max(sim(anc(girl), child)) = 1. There are 15
C-C features: sim(C
1
, C
2
), max(sim(C
1
, C
anc
2
),
sim(C
anc
1
, C
2
)), sim(C
anc
1
, C
anc
2
) for each con-
stituent type.
C-all features compare different constituent
types. These features express how often we
expect any pair of constituents to describe the
same scene. For example, s
1
= ?Two teams are
competing in a football match? and s
2
= ?A
player is throwing a football? are topically related
sentences. Comparing constituents of different
types like player and compete or player and
football match gives us more information about
the similarity of the sentences. There are 15 C-all
features: the maximum, minimum, and sum of
sim(C
t
1
, C
2
) and sim(C
1
, C
t
2
) for each constituent
type.
Distributional Constituent Similarity Distribu-
tional vector-based similarity may alleviate the
sparsity of the denotation graph. For example,
for subject NP C-C features, we have non-zero
distributional similarity for 87% of instances in
the trial data, but non-zero denotational simi-
larity for only 56% of the same instances. The
football and team nodes may have no common
images in the denotation graph, but we still
have distributional vectors for football and for
team. The 30 distributional similarity features are
the same as the denotational similarity features
except sim(a, b) is the cosine similarity between
constituent phrase vectors.
Alignment Since contradictory and entailing sen-
tences have limited syntactic variation in this
dataset, aligning sentences can help to predict se-
mantic relatedness and textual entailment. We use
the Needleman-Wunsch algorithm (1970) to com-
pute an alignment based on exact word matches
between two lemmatized sentences. The similar-
ity between two lemmas is 1.0 if the words are
identical and 0.0 otherwise, and we do not penal-
ize gaps. This gives us the longest subsequence of
matching lemmas.
The alignment algorithm results in a sentence
pair alignment and 2 unaligned chunk sets defined
by syntactic chunks. For example, s
1
= ?A brown
and white dog is running through the tall grass?
and s
2
= ?A brown and white dog is moving
through the wild grass? are mostly aligned, with
the remaining chunks u
1
= {[VP run], [NP tall]}
and u
2
= {[VP move], [NP wild]}.
There are 23 alignment features. Directional
features per sentence are the number of words
(2 features), the number of aligned words (2
features), and the ratio between those counts (2
features). These features are expressed twice,
once according to the sentence order in the dataset
and once ordered by longer sentence before
shorter sentence, for a total of 12 directional fea-
tures. Non-directional features are the maximum,
minimum, and average unaligned chunk length for
each sentence and for both sentences combined (9
features), and the number of unaligned chunks in
each sentence (2 features).
Unaligned Chunk Matching We want to know
the similarity of the remaining unaligned chunks
because when two sentences have a high overlap,
their differences are very informative. For exam-
ple, in the case that two sentences are identical
except for a single word in each sentence, if we
know that the two words are synonymous, then we
should predict that the two sentences are highly
similar. However, if the two words are antonyms,
the sentences are likely to be contradictory.
We use phrase vector similarity to compute the
most likely matches between unaligned chunks.
We repeat the matching process twice: for sim-
ple matching, any 2 chunks with non-zero phrase
similarity can be matched across sentences, while
for strict matching, chunks can match only if they
have the same type, e.g. NP or VP. This gives us
two sets of features.
For s
1
= ?A brown and white dog is running
through the tall grass? and s
2
= ?A brown and
white dog is moving through the wild grass,? the
unaligned chunks are u
1
= {[VP run], [NP tall]}
and u
2
= {[VP move], [NP wild]}. For strict
matching, the only valid matches are [VP run]?
[VP move] and [NP tall]?[NP wild]. For simple
matching, [NP tall] could also match [VP move]
instead and [VP run] could match [NP wild].
There are a total of 31 unaligned chunk match-
ing features. Directional features per sentence
include the number of unaligned chunks (2
features) and the ratio of the number of matched
chunks to the total number of chunks (2 fea-
331
tures). These features are expressed twice, once
according to the sentence order in the dataset and
once ordered by longer sentence before shorter
sentence, for a total of 8 directional features.
Non-directional features per sentence pair include
the maximum, minimum, and average similarity
of the matched chunks (3 features); the maximum,
minimum, and average length of the matched
chunks (3 features); and the number of matched
chunks (1 feature). We extract these 15 features
for both simple matching and strict matching. In
addition, we also count the number of crossings
that result from matching the unaligned chunks in
place (1 feature). This penalizes matched sets that
contain many crossings or long-distance matches.
Chunk Label Alignment and Matching Since
similar sentences in this dataset often have
similar syntax, we compare their chunk label
sequences, e.g. [NP A brown and white dog]
[VP is running] [PP through] [NP the tall grass]
becomes NP VP PP NP. We compute 17 features
based on aligning and matching these chunk
label sequences. Directional features are the total
number of labels in the sequence (2 features),
the number of unaligned labels (2 features), the
ratio of the number of unaligned labels to the
total number of labels (2 features), and the ratio
of the number of matched labels to the number of
unaligned labels (2 features). These features are
expressed twice, once according to the sentence
order in the dataset and once ordered by longer
sentence before shorter sentence, for a total of 16
directional features. We also count the number of
matched labels for the sentence pair (1 feature).
Synonyms and Hypernyms We count the num-
ber of synonyms and hypernyms in the matched
chunks for each sentence pair. Synonyms are
words that share a WordNet synset, and hyper-
nyms are words that have a hypernym relation
in WordNet. There are two hypernym features
because hypernymy is directional: num hyp
1
is
the number of words in s
1
that have a hypernym
in s
2
, while num hyp
2
is the number of words
in s
2
that have a hypernym in s
1
. For example,
s
1
= ?A woman is cutting a lemon? and s
2
= ?A
woman is cutting a fruit? have num hyp
1
= 1.
For synonyms, num syn is the number of word
pairs in s
1
and s
2
that are synonyms. For example,
s
1
= ?A brown and white dog is running through
the tall grass? and s
2
= ?A brown and white
dog is moving through the wild grass? have
num syn = 1.
Antonyms When we match unaligned chunks, the
highest similarity pair are sometimes antonyms,
e.g. s
1
= ?Some people are on a crowded street?
and s
2
= ?Some people are on an empty street.?
In other cases, they are terms that we think of as
mutually exclusive, e.g. man and woman. In both
cases, the sentences are unlikely to be in an en-
tailing relationship. Since resources like WordNet
will fail to identify the mutually exclusive pairs
that are common in this dataset, e.g. bike and car
or piano and guitar, we use the training data to
build a list of these pairs. We identify the matched
chunks that occur in contradictory or neutral sen-
tences but not entailed sentences. We exclude syn-
onyms and hypernyms and apply a frequency filter
of n = 2. Commonly matched chunks in neutral
or contradictory sentences include sit?stand, boy?
girl, and cat?dog. These are terms with differ-
ent and often mutually exclusive meanings. Com-
monly matched chunks in entailed sentences in-
clude man?person, and lady?woman. These are
terms that could easily be used to describe the
same situation. However, cut?slice is a common
pair in both neutral and entailed sentences and we
do not want to count it as an antonym pair. There-
fore, we consider frequent pairs that occur in con-
tradictory or neutral but not entailed sentences to
be antonyms.
The feature num ant is the number of matched
antonyms in a sentence pair. We identify an
antonym if c
a
and c
b
are on the antonym list or
occur in one of these patterns: X?not X, X?no X,
X?no head-noun(X) (e.g. blue hat?no hat), X?
no hypernym(X) (e.g. poodle?no dog), X?no syn-
onym(X) (e.g. kid?no child). For each antonym
pair, we set the similarity score of that match to
0.0.
For example, num ant = 1 for s
1
= ?A small
white dog is running across a lawn? and s
2
= ?A
big white dog is running across a lawn.? In addi-
tion, num ant = 1 for s
1
= ?A woman is leaning
on the ledge of a balcony? and s
2
= ?A man is
leaning on the ledge of a balcony.?
2.5 Models
For the SR task, we implement a log-linear regres-
sion model using Weka (Hall et al., 2009). Specif-
332
Accuracy Pearson ?
Chance baseline 33.3 ?
Majority baseline 56.7 ?
Probability baseline 41.8 ?
Overlap baseline 56.2 0.627
Submitted system 84.5 0.799
Table 2: TE and SR results on test data.
Model Accuracy Pearson ?
Overlap baseline 56.8 0.646
Negation 61.0 0.093
Word overlap 65.0 0.694
(+Vector composition) 66.4 0.697
+Denotational similarity 74.4 0.751
+Distributional similarity 71.8 0.756
+Den +Dist 77.0 0.782
+Alignment 70.4 0.697
+Unaligned chunk matching 75.8 0.719
+Align +Match 75.2 0.728
+Synonyms 65.2 0.696
+Hypernyms 66.8 0.716
+Antonyms 71.0 0.704
All features 84.2 0.802
Table 3: TE and SR results on trial data.
ically, under Weka?s default settings, we train a
ridge regression model with regularization param-
eter ? = 1?10
?8
. For the TE task, we use a Max-
Ent model implemented with MALLET (McCal-
lum, 2002). The MaxEnt model is optimized with
L-BFGS, using the default settings. Both models
use the same set of features.
3 Results
Our submitted system was trained on the full train-
ing and trial data (5000 sentences). Table 2 shows
our results on the test data. We substantially out-
perform all baselines.
3.1 Feature Ablation
We train models on the training data and test on
the trial data. Models marked with + include our
word overlap feature. We also examine a single
compositional feature (vector composition): the
cosine similarity of two sentence vectors. A sen-
tence vector is the pointwise multiplication prod-
uct of component word vectors.
Table 3 compares performance on both tasks.
For TE, unaligned chunk matching outperforms
other features. Denotational constituent similarity
also does well. For SR, distributional and deno-
tational features have the highest correlation with
gold scores. Combining them further improves
performance.
% Accuracy
Model N E C
Overlap baseline 77.3 44.8 0.0
Negation 85.4 0.0 86.4
Word overlap 82.9 63.8 0.0
(+Vector composition) 84.7 64.5 0.0
+Denotational similarity 83.6 67.3 52.7
+Distributional similarity 86.5 60.4 37.8
+Den +Dist 85.4 68.7 60.8
+Alignment 87.9 50.6 41.8
+Unaligned chunk matching 90.4 66.6 37.8
+Align +Match 88.6 61.8 50.0
+Synonyms 82.2 65.2 0.0
+Hypernyms 84.0 68.0 0.0
+Antonyms 83.6 82.6 0.0
All features 86.5 83.3 77.0
Table 4: TE accuracy on trial data by entailment
type (Neutral, Entailment, Contradiction).
Table 4 shows TE accuracy of each model by
entailment label. On contradictions, the negation
model has 86.0% accuracy while our final system
has only 77.0% accuracy. However, the negation
model cannot identify entailment. Its performance
is due to the high proportion of contradictions that
can be identified by explicit negation.
We expected antonyms to improve classifica-
tion of contradictions, but the antonym feature
actually has the highest accuracy of any feature
on entailed sentences. The dataset contains few
contradictions, and most involve explicit negation,
not antonyms. The antonym feature indicates that
when two sentences have high word overlap and
no antonyms, one is likely to entail the other. Neu-
tral sentences often contain word pairs that are
mutually exclusive, so the antonym feature distin-
guishes between neutral and entailed sentences.
4 Conclusion
Our system combines multiple similarity metrics
to predict semantic relatedness and textual entail-
ment. A binary negation feature and similarity
comparisons based on chunking do very well, as
do denotational constituent similarity features. In
the future, we would like to focus on multiword
paraphrases and prepositional phrases, which our
current system has trouble analyzing.
Acknowledgements
We gratefully acknowledge support of the Na-
tional Science Foundation under grants 1053856
and 1205627, as well as an NSF Graduate Re-
search Fellowship to Alice Lai.
333
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193, Cambridge, MA, October.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing, pages 1394?1404,
Edinburgh, Scotland, UK., July.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An up-
date. SIGKDD Explorations, 11(1):10?18.
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faela Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014a. SemEval-2014 task 1: Evaluation
of compositional distributional semantic models on
full sentences through semantic relatedness and tex-
tual entailment. In Proceedings of SemEval 2014:
International Workshop on Semantic Evaluation.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaela Bernardi, and Roberto Zampar-
elli. 2014b. A SICK cure for the evaluation of com-
positional distributional semantic models. In Pro-
ceedings of LREC.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of LREC, vol-
ume 6, pages 2216?2219.
Vasin Punyakanok and Dan Roth. 2001. The use of
classifiers in sequential inference. In NIPS, pages
995?1001. MIT Press.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201?1211, Jeju Island, Korea, July.
Peter Young, Alice Lai, Micah Hodosh, and Julia
Hockenmaier. 2014. From image descriptions to
visual denotations: New similarity metrics for se-
mantic inference over event descriptions. Transac-
tions of the Association for Computational Linguis-
tics, 2:67?78.
334
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 139?147,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Collecting Image Annotations Using Amazon?s Mechanical Turk
Cyrus Rashtchian Peter Young Micah Hodosh Julia Hockenmaier
Department of Computer Science
University of Illinois at Urbana-Champaign
201 North Goodwin Ave, Urbana, IL 61801-2302
{crashtc2, pyoung2, mhodosh2, juliahmr}@illinois.edu
Abstract
Crowd-sourcing approaches such as Ama-
zon?s Mechanical Turk (MTurk) make it pos-
sible to annotate or collect large amounts of
linguistic data at a relatively low cost and high
speed. However, MTurk offers only limited
control over who is allowed to particpate in
a particular task. This is particularly prob-
lematic for tasks requiring free-form text en-
try. Unlike multiple-choice tasks there is no
correct answer, and therefore control items
for which the correct answer is known can-
not be used. Furthermore, MTurk has no ef-
fective built-in mechanism to guarantee work-
ers are proficient English writers. We de-
scribe our experience in creating corpora of
images annotated with multiple one-sentence
descriptions on MTurk and explore the effec-
tiveness of different quality control strategies
for collecting linguistic data using Mechani-
cal MTurk. We find that the use of a qualifi-
cation test provides the highest improvement
of quality, whereas refining the annotations
through follow-up tasks works rather poorly.
Using our best setup, we construct two image
corpora, totaling more than 40,000 descriptive
captions for 9000 images.
1 Introduction
Although many generic NLP applications can be de-
veloped by using existing corpora or text collections
as test and training data, there are many areas where
NLP could be useful if there was a suitable corpus
available. For example, computer vision researchers
are becoming interested in developing methods that
can predict not just the presence and location of cer-
tain objects in an image, but also the relations be-
tween objects, their attributes, or the actions and
events they participate in. Such information can
neither be obtained from standard computer vision
data sets such as the COREL collection nor from
the user-provided keyword tag annotations or cap-
tions on photo-sharing sites such as Flickr. Simi-
larly, although the text near an image on a website
may provide cues about the entities depicted in the
image, an explicit description of the image content
itself is typically only provided if it is not immedi-
ately obvious to a human what is depicted (in which
case we may not expect a computer vision system
to be able to recognize the image content either).
We therefore set out to collect a corpus of images
annotated with simple full-sentence descriptions of
their content. To obtain these descriptions, we used
Amazon?s Mechanical Turk (MTurk).1 MTurk is
an online framework that allows researchers to post
annotation tasks, called HITs (?Human Intelligence
Task?), then, for a small fee, be completed by thou-
sands of anonymous non-expert users (Turkers). Al-
though MTurk has been used for a variety of tasks in
NLP, our use of MTurk differs from other research
in NLP that uses MTurk mostly for annotation of
existing text. Similar to crowdsourcing-based an-
notation, quality control is an essential component
of crowdsourcing-based data collection efforts, and
needs to be factored into the overall costs. For us,
the quality of the text produced by the Turkers is
particularly important since we are interested in us-
1All of our experiments on Mechanical Turk were adminis-
tered and paid for through the services offered by Dolores Labs.
139
ing this corpus for future research at the intersection
of computer vision and natural language processing.
However, MTurk provides limited ways to imple-
ment such quality control directly. For example, our
initial experiments yielded a data set that contained
many sentences that were clearly not written by na-
tive speakers. We learned that several steps must be
taken to ensure that Turkers both understand the task
and produce quality data.
This paper describes our experiences with Turk
(based on data collection efforts in spring and sum-
mer 2009), comparing two different approaches to
quality control. Although we did not set out to run a
scientific experiment comparing different strategies
of how to collect linguistic data on Turk, our expe-
rience points towards certain recommendations for
how to collect linguistic data on Turk.
2 The core task: image annotation
The PASCAL Data Set Every year, the Pat-
tern Analysis, Statistical Modeling, and Computa-
tional Learning (PASCAL) organization hosts the
Visual Object Classes Challenge (Everingham et al,
2008). This is a competition similar to the shared
tasks familiar to the ACL community, where a com-
mon data set of images with classification and de-
tection information is released, and computer vision
researchers compete to create the best classification,
detection, and segmentation systems. We chose to
use this collection of images because it is a standard
resource for computer vision, and will therefore fa-
ciliate further research.
The VOC2008 development and training set con-
tains around 6000 images. It is categorized by ob-
jects that appear in the image, with some images ap-
pearing in multiple categories.2. The images con-
tain a wide variety of actions and scenery. Our cor-
pus consists of 1000 of these images, fifty randomly
chosen from each of the twenty categories.
MTurk setup We asked Turkers to write one de-
scriptive sentence for each of ten images. An ex-
ample annotation screen is shown in Figure 1. We
2The twenty categories include people, various animals,
vehicles and other objects: person, bird, cat, cow,
dog, horse, sheep, aeroplane, bicycle,
boat, bus, car, motorbike, train, bottle,
chair, dining table, potted plant, sofa,
tv/monitor
Figure 1: Screenshot of the image annotation task.
first showed the Turkers a list of instructive guide-
lines describing the task (Figure 6). The instruc-
tions told them to write ten complete but simple sen-
tences, to include adjectives if possible, to describe
the main characters, the setting, or the relation of
the objects in the image, to pay attention to gram-
mar and spelling, and to try to be concise. These
instructions were meant to both explain the task and
to prepare Turkers to write quality sentences. We
then showed each Turker a set of ten images, chosen
randomly from the 1000 total images, and displayed
one at a time. The Turkers navigated using ?Next?
buttons through the ten annotation screens, each dis-
playing one image and one text-box. We allowed
Turkers ten minutes to complete one task.3 We re-
stricted the task to Turkers who have previously had
at least 95% of their results approved. We paid $0.10
to complete one task. The total cost for all 5000 de-
scriptions was $50 (plus Amazon?s 10% fee).
2.1 Results
On average, Turkers wrote the ten sentences in a to-
tal of four minutes. The average pay rate was $1.30
per hour, and the whole experiment finished in under
two days. Five different people described each im-
age, and in the end, most of the Turkers completed
the task successfully, although 2.5% of the 5000 sen-
tences were empty strings. Turkers varied in the time
they took to complete the experiment, in the length
of their sentences, and in the level of detail they in-
cluded about the image. An example captioned im-
age is shown in Figure 2.
Problems with the data The quality of descrip-
tions varied greatly. We were hoping to collect sim-
ple sentences, written in correct English, describ-
ing the entities and actions in the images. More-
3This proved to be more than enough time for the task.
140
Figure 2: An image along with the five captions that were written by Turkers.
over, these are explicitly the types of descriptions we
asked for in the MTurk task instructions. Although
we found the descriptions acceptable more than half
of the time, a large number of the remaining descrip-
tions had at least one of the following two problems:
1. Some descriptions did not mention the salient
entities in the image, some were simply noun
phrases (or less), and some were humorous or
speculative.4 We find all of these to be prob-
lems because future computer vision and nat-
ural language processing research will require
accurate and consistent image captions.
2. A number of Turkers were not sufficiently pro-
ficient in English. Many descriptions contained
grammar and spelling errors, and some in-
cluded very awkward constructions. For exam-
ple, the phrase ?X giving pose? showed up sev-
eral times in descriptions of images containing
people (e.g. ?The lady and man giving pose.?).
Such spelling and grammar errors will pose dif-
ficulties for any standard text-processing algo-
rithms trained on native English.
Spell checking Due to the large number of mis-
spellings in in the initial data set, we first ran the sen-
tences first through our spell checker before putting
them up on Turk to assess their quality. We tok-
enized the captions with OpenNLP, and first checked
a manually created list of spelling corrections for
each token. These included canonicalizations (cor-
recting ?surf board? as ?surfboard?), words our au-
tomatic spell checker did not recognize (?mown?),
and the most common misspellings in our data set
4For example, some Turkers commented on the feelings of
animals (e.g. ?the dog is not very happy next to the dumpster?),
and others made jokes about the content of the image (e.g. ?The
goat is ready for hair cut?)
(?shepard? to ?shepherd?). If the token was not in
our manual list, we passed the word to aspell. From
aspell?s candidate corrections, we selected the most
frequent word that appeared either in other captions
of the same image, of images of the same topic, or
any caption in our data set.
3 Post-hoc quality control
Because our initial data collection efforts resulted in
relatively noisy data, we created a new set of MTurk
tasks designed to provide post-hoc quality control.
Our aim was to filter out captions containing mis-
spellings and incorrect grammar.
MTurk setup Each HIT consisted of fifty differ-
ent image descriptions and asked Turkers to decide
for each of them whether they contained correct
grammar and spelling or not. At the beginning of
each HIT, we included a brief training phase, where
we showed the Turkers five example descriptions la-
beled as ?correct? or ?incorrect? (Figure 7). In the
HIT itself, the fifty descriptions were displayed in
blocks of five (albeit not for the same image) , and
each description was followed by two radio buttons
labeled ?correct? and ?incorrect?. We did not show
the corresponding images. A screenshot is shown in
Figure 3. Each block of five captions contained one
control item that we use for later assessment of the
Turkers? spell-checking ability. We wrote these con-
trol captions ourselves, modeling them after actual
image descriptions. We paid $0.08 for one task, and
three people completed each task.
3.1 Results
On average, Turkers completed a HIT (judging fifty
sentences) in four minutes, at an average hourly rate
of $1.04. Each sentence in our data set was judged
by three Turkers. The whole experiment finished
141
Figure 3: Screenshot from the grammar/spelling checking task. This is a block of five sentences that Turkers had
to label as using correct or incorrect grammar and spelling. The first sentence is a control item that we included to
monitor the Turkers? performance, and the other four are captions generated by other Turkers in a previous task.
Data set Quality control % Votes for ?correct English?
produced by... performed by... 0 1 2 3
Unqualified writers three Turkers 18.9% 31.2% 26.4% 23.5%
Unqualified writers three experts 11.8% 12.7% 15.3% 60.2%
Qualified writers three experts 0.5% 2.5% 15.0% 82.0%
Table 1: Quality control by Turkers and Experts. The three experts judged 600 sentences from each data set. 565
sentences produced by unqualified workers were also judged by three Turkers.
in under two days, at a total cost of $28.80 (plus
Amazon?s 10% fee). We also selected randomly
600 spell-checked sentences for expert annotation.
Three members of our team (all native speakers of
English) judged each of these sentences in the same
manner as the Turkers. Each sentence could there-
fore get between 0 and 3 Turker votes and between
0 and 3 expert votes for good English. The top two
rows of Table 1 show the distribution of votes in
each of the two groups. We also assess whether the
judgments of the Turkers correlate with our own ex-
pert judgments. Table 2(a) shows the overall agree-
ment between Turkers and expert annotators. The
rest of Table 2 shows how performance of the Turk-
ers on the control items affected agreement with ex-
pert judgments. We define the performance of a
Turker in terms of the average the number of con-
trol items that they got right in each HIT they took.
For each threshold in Tables 2(a)-(d), we considered
only those images for which we have three quality
judgments by workers whose performance is above
the specified threshold.
Our results show that the effectiveness of using
Turkers to filter for grammar and spelling issues is
limited. Overall, the Turker judgments were overly
harsh. The majority Turker vote agrees with the ma-
jority vote of the trained annotators on only 65.1%
of the sentences. Manual inspection of the differ-
ences reveals that the Turkers marked many per-
fectly grammatical English sentences as incorrect
(although they also marked a few which we had
missed). Agreement with experts decreases among
those Turkers that performed better on the control
sentences, with only 56.7% agreement for Turkers
that got all the controls right. In addition, the Turk-
ers are significantly more likely to report false nega-
tives over false positives and this also increases with
performance on the control sentences. (Overall, the
Turkers marked 29.9% of the sentences as false neg-
atives, whereas the Turkers that scored perfectly on
the controls marked 39.3% as false negatives.) Ex-
amination of the areas of high disagreement reveal
that the Turkers were much more likely to vote down
noun phrases than the experts were. The correct ex-
ample captions provided in the instructions of the
quality control test were complete sentences. Some
of the control captions were noun phrases, but all
of the noun phrase controls had some other error
in them. Thus it was possible to either believe that
noun phrases were correct or incorrect, and still be
consistent with the provided examples, and provide
correct judgments on the control sentences.
142
(a) ? 0 controls correct: 565 sentences
Turk Expert votes
votes 0 1 2 3
0 6.9% 4.4% 3.7% 3.9%
1 3.2% 5.7% 5.0% 17.3%
2 1.8% 2.8% 3.5% 18.2%
3 0.0% 0.4% 2.5% 20.7%
(b) ? 5 controls correct: 553 sentences
Turk Expert votes
votes 0 1 2 3
0 6.9% 4.5% 3.8% 4.0%
1 3.1% 5.4% 5.1% 17.5%
2 1.8% 2.7% 3.6% 18.4%
3 0.0% 0.4% 2.5% 20.3%
(c) ? 7 controls correct: 331 sentences
Turk Expert votes
votes 0 1 2 3
0 6.9% 6.3% 3.9% 5.1%
1 3.0% 4.5% 5.1% 24.5%
2 1.8% 1.8% 2.4% 15.1%
3 0.0% 0.0% 2.1% 17.2%
(d) ? 9 controls correct: 127 sentences
Turk Expert votes
votes 0 1 2 3
0 7.9% 6.3% 3.1% 6.3%
1 1.6% 4.7% 6.3% 23.6%
2 0.8% 3.1% 1.6% 15.7%
3 0.0% 0.0% 1.6% 17.3%
Table 2: Quality control: Agreement between Turker and Expert votes, depending on the average number of control
items the Turker voters got right.
4 Quality control through pre-screening
Quality control can also be imposed through a pre-
screening of the Turkers allowed to take the HIT. We
collected another set of five descriptions per image,
but restricted participation to Turkers residing in the
US5, and created a brief qualification test to check
their English. We would like to be able to restrict our
tasks to Turkers who are native speakers and com-
petent spellers and writers of English, regardless of
their country of residence. However, this seems to
be difficult to verify within the current MTurk setup.
Qualification Test Design The qualification test
consists of forty binary questions: fifteen testing
spelling, fifteen testing grammar, and ten testing the
ability to identify good image descriptions.
In all three cases, we started the section with a
set of instructions displaying examples of positive
and negative answers to the tasks. Each spelling
question consisted of a single sentence, and Turk-
ers were asked to determine if all of the words in
the sentence were spelled correctly and if the correct
word was being used (?lose? versus ?loose?). Each
grammar question consisted of a single sentence that
was either correct or included a grammatical error.
Both spelling and grammar checking questions were
based on common mistakes made by foreign English
5As of March 2010, 46.80% of Turkers reside in the U.S
(http://behind-the-enemy-lines.blogspot.
com/ 03/09/2010)
Figure 4: Average caption length (5000 images)
speakers and on grammatical or spelling errors that
occurred in our initial set of image captions. The
grammar and spelling questions are listed in Table
3. The image description questions consisted of one
image shown with two actual captions, and the Turk-
ers were asked which caption better described the
image. In order to pass the qualification test, we
required each annotator to correctly answer at least
twenty-four spelling and grammar questions and at
least eight image description questions. To prevent
Turkers from using the number of question they got
correct to do a brute force search for the correct an-
swers, we simply told them if they passed (?1?) or
failed (?0?). Currently, 1504 people have taken the
qualification test, with a 67.2% passing rate. Since
this qualification test was only required for our HITs
that were restricted to US residents, we assume (but
are not able to verify) that most, if not all, of the
people who took this test are actually US residents.
143
MTurk Set-up We use the same MTurk set-up as
before, but to encourage Turkers to complete the
task even though they first have to pass a qualifica-
tion test, we pay them $0.10 to annotate five images.
4.1 Results
We found that the Turkers who passed the qualifica-
tion provided much better captions for the images.
The average time spent on each image was longer
(four minutes per ten images for the non-qualified
workers versus five minutes per ten images for the
qualified workers). On average, qualified Turk-
ers produced slightly longer sentences (avg. 10.7
words) than non-qualified workers (avg. 10.0 words)
(Figure 4), and the awkward constructions produced
by the unqualified workers were mostly absent. The
entire corpus was annotated in 253 hours at a cost of
$100.00 (plus Amazon?s 10% fee).
We also looked at the rate of misspellings (ap-
proximated by how often our spell-checker indicated
a misspelling). Without the qualification test, Out
of the 600 sentences produced without the qualifica-
tion test, 78 contained misspellings, whereas only 25
sentences out of the 600 produced by the qualified
workers contained misspellings. Furthermore, mis-
spellings in the no-qualification group include many
genuine errors (?the boys are playing in tabel?,
?bycycles?, ?eatting?), whereas misspellings in the
qualification group are largely typos (e.g. Ywo for
Two, tableclothe, chari for chair). Furthermore, the
spell checker corrected all 25 misspellings in the
qualified data set to the intended word, but 27 out of
the 78 misspellings in the data produced by the un-
qualified workers got changed to some other word.
The same three members of our team rated again
the English of 600 randomly selected sentences writ-
ten by Turkers residing in the US who passed our
test. We found a significant improvement in quality
(Table 1, bottom row), with the majority expert vote
accepting over 97% of the sentences. This is also
corroborated by qualitative analysis of the data (see
Figure 5 for examples). Inspection reveals that sen-
tences that are deemed ungrammatical by the experts
typically contain some undetected typo, and would
be correct if these typos could be fixed. Without a
qualification test, there is a significantly greater per-
centage of nonsensical responses such as: ?Is this a
bird squirrel?? and ?thecentury?. In addition, gram-
matically correct but useless fragments such as ?very
dark? and ?peace? only appear without a test. After
requiring the qualification test, the major reasons for
rejection by Turkers are typos such as in ?The two
dogs blend in with the stuff animals? or missing de-
terminers such as in ?a train on tracks in town?.
Overall cost effectiveness Using the no qualifica-
tion test approach, we first paid $50.00 to get 5000
sentences written by unqualified Turkers (which re-
sulted in 4851 non-empty sentences). This resulted
in low-quality data which required further verifica-
tion. Since this is too time-consuming for expert an-
notators, we then paid another $28.80 to get each of
these sentences subsequently checked by three Turk-
ers for grammaticality, resulting in 2222 sentences
which received at least two positive votes for gram-
maticality. With the qualification test approach, we
paid $100.00 to get 5000 sentences written. Based
on our experiments on the set of 600 sentences, ex-
perts would judge over 97% of these sentences as
correct, thus obviating the immediate need for fur-
ther control. That is, it effectively costs more for
non-qualified Turkers to produce sentences that are
judged to be good than for qualified Turkers. Fur-
thermore, their sentences will probably be of lower
quality even after they have been judged acceptable.
5 A corpus of captions for Flickr photos
Encouraged by the success of the qualification test
approach, we extended our corpus to contain 8000
images collected from Flickr. We again paid the
Turkers $0.10 to annotate five images. Our data set
consists of 8108 hand-selected images from Flickr,
depicting actions and events (rather than images de-
picting scenery and mood). These images are more
likely to require full sentence descriptions than the
PASCAL images. We chose six large Flickr groups6
and downloaded a few thousand images from each,
giving us a total of 15,000 candidate images. We re-
moved all black and white or sepia images as well as
images containing photographer signatures or seals.
Next, we manually identified pictures that depicted
the actions of people or animals. For example, we
kept images of people walking in parks, but not of
6The groups: strangers!, Wild-Child (Kids in Action), Dogs
in Action (Read the Rules), Outdoor Activities, Action Photog-
raphy and Flickr-Social (two or more people in the photo)
144
Without qualification test
(1) lady with birds
(2) Some parrots are have speaking skill.
(3) A lady in their dining table with birds on her shoulder and head.
(4) Asian woman with two cockatiels, on shoulder
head, room with oak cabinets.,
(5) The lady loves the parrot
With qualification test
(1) A woman has a bird on her shoulder, and another bird on her head
(2) A woman with a bird on her head and a bird on her shoulder.
(3) A women sitting at a dining table with two small birds sitting on her.
(4) A young Asian woman sitting at a kitchen
table with a bird on her head and another on her shoulder.
(5) Two birds are perched on a woman sitting in a kitchen.
Figure 5: Comparison of captions written by Turkers with and without qualification test
empty parks; we kept several people posing, but not
a close-up of a single person.7 Each HIT asked
Turkers to describe five images. We required the
qualification test and US residency. Average com-
pletion time was a little above 3 minutes for 5 sen-
tences. The corpus was annotated in 284 hours8, at
a total cost of $812.00 (plus Amazon?s 10% fee).
6 Related work and conclusions
Related work MTurk has been used for many dif-
ferent NLP and vision tasks (Tietze et al, 2009;
Zaidan and Callison-Burch, 2009; Snow et al, 2008;
Sorokin and Forsyth, 2008). Due to the noise in-
herent in non-expert annotations, many other at-
tempts at quality control have been made. Kit-
tur et al (2008) solicit ratings about different as-
pects of Wikipedia articles. At first they receive
very noisy results, due to Turkers? not paying at-
tention when completing the task or specifically try-
ing to cheat the requester. They remade the task,
this time starting by asking the Turkers verifiable
questions, speculating that the users would produce
better quality responses when they suspect their an-
swers will be checked. They also added a question
that required the Turkers to comprehend the con-
tent of the Wikipedia article. With this new set-
up, they find that the quality greatly increases and
carelessness is reduced. Kaisser and Lowe (2008)
7Our final data set consists of 1482 pictures from action pho-
tography, 1904 from dogs, 776 from flickr-social, 916 from out-
door, 1257 from strangers and 1773 from wild-child.
8Note that the annotation process scaled pretty well, con-
sidering that annotating more than eight times the number of
images took only 31 hours longer.
collected question and answer pairs by presenting
Turkers with a question and telling them to copy and
paste from a document of text they know to contain
the answer. They achieve a good but far from per-
fect interannotator agreement based on the extracted
answers. We speculate that the quality would be
much worse if the Turkers wrote the sentences them-
selves. Callison-Burch (2009) asks Turkers to pro-
duce translations when given reference sentences in
other languages. Overall, he finds find that Turk-
ers produce better translations than machine transla-
tion systems. To eliminate translations from Turkers
who simply put the reference sentence into an online
translation website, he performs a follow-up task,
where he asks other Turkers to vote on if they believe
that sentences were generated using an online trans-
lation system. Mihalcea and Strapparava (2009) ask
Turkers to produce 4-5 sentence opinion paragraphs
about the death penalty, about abortion and describ-
ing a friend. They report that aside from a small
number of invalid responses, all of the paragraphs
were of good quality and followed their instructions.
Their success is surprising to us because they do not
report using a qualification test, and when we did
this our responses contained a large amount of in-
correct English spelling and grammar.
The TurKit toolkit (Little et al, 2009) provides
another approach to improving the quality of MTurk
annotations. Their iterative framework allows the
requester to set up a series of tasks that first solic-
its text annotations from Turkers and then asks other
Turkers to improve the annotations. They report suc-
cessful results using this methodology, but we chose
145
to stick with simply using the qualification test be-
cause it achieves the desired results already. Fur-
thermore, although using TurKit would have proba-
bly done away with our few remaining grammar and
spelling mistakes, it may have caused the captions
for an image to be a little too similar, and we value
a diversity in the use of words and points of view.
Our experiences We have described our experi-
ences in using Amazon?s Mechanical Turk in the
first half of 2009 to create a corpus of images anno-
tated with descriptive sentences. We implemented
two different approaches to quality control: first, we
did not impose any restrictions on who could write
image descriptions. This was then followed by a sec-
ond set of MTurk tasks where Turkers had to judge
the quality of the sentences generated in our initial
Turk experiments. This approach to quality control
would be cost-effective if the initial data were not
too noisy and the subsequent judgments were ac-
curate and cheap. However, this was not the case,
and quality control on the judgments in the form of
control items turned out to result in even lower ac-
curacy. We then repeated our data collection effort,
but required that Turkers live in the US and take a
brief qualification test that we created to test their
English. This is cost-effective if English proficiency
can be accurately assessed in such a brief qualifica-
tion test. We found that the latter approach was in-
deed far cheaper, and produced significantly better
data. We did not set out to run a scientific experi-
ment comparing different strategies of how to col-
lect linguistic data on Turk, and therefore there may
be multiple explanations for the effects we observe.
Nevertheless, our experience indicates strongly that
even very simple prescreening measures can provide
very effective quality control.
We also extended our corpus to include 8000 im-
ages collected from Flickr. We hope to release this
data to the public for future natural language pro-
cessing and computer vision research.
Recommended practices for usingMTurk in NLP
Our experience indicates that with simple prescreen-
ing, linguistic data can be elicited fairly cheaply and
rapidly from crowd-sourcing services such as Me-
chanical Turk. However, many applications may re-
quire more control over where the data comes from.
Even though NLP data collection differs fundamen-
tally from psycholinguistic experiments that may
elicit production data, our community will typically
also need to know whether data was produced by na-
tive speakers or not. Until MTurk provides a better
mechanism to check the native language of its work-
ers, linguistic data collection on MTurk will have to
rely on potentially very noisy input.
Acknowledgements
This research was funded by NSF grant IIS 08-
03603 INT2-Medium: Understanding the Meaning
of Images. We are grateful for David Forsyth?s ad-
vice and for Alex Sorokin?s support with MTurk.
References
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In Proceedings of EMNLP 2009.
M. Everingham, L. Van Gool, C. K. I. Williams,
J. Winn, and A. Zisserman. 2008. The
PASCAL Visual Object Classes Challenge
2008 (VOC2008) Results. http://www.pascal-
network.org/challenges/VOC/voc2008/workshop/.
Michael Kaisser and John Lowe. 2008. Creating a re-
search collection of question answer sentence pairs
with amazons mechanical turk. In LREC 2008.
Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008.
Crowdsourcing user studies with mechanical turk. In
Proceedings of SIGCHI 2008.
Greg Little, Lydia B. Chilton, Max Goldman, and
Robert C. Miller. 2009. Turkit: tools for iterative tasks
on mechanical turk. In HCOMP ?09: Proceedings of
the ACM SIGKDD Workshop on Human Computation.
Rada Mihalcea and Carlo Strapparava. 2009. The lie
detector: Explorations in the automatic recognition
of deceptive language. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast ? but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of EMNLP 2008.
Alexander Sorokin and David Forsyth. 2008. Utility data
annotation with amazon mechanical turk. In Com-
puter Vision and Pattern Recognition Workshop.
Martin I. Tietze, Andi Winterboer, and Johanna D.
Moore. 2009. The effect of linguistic devices in infor-
mation presentation messages on comprehension and
recall. In Proceedings of ENLG 2009.
Omar F. Zaidan and Chris Callison-Burch. 2009. Feasi-
bility of human-in-the-loop minimum error rate train-
ing. In Proceedings of EMNLP 2009.
146
Are all of the words correctly spelled and correctly used? Is the sentence grammatically correct?
A group of children playing with thier toys (N) A man giving pose to camera. (N)
He accepts the crowd?s praise graciously. (Y) The white sheep walks on the grass. (Y)
The coffee is kept at a very hot temperture. (N) She is good woman. (N)
A green car is parked in front of a resturant. (N) He should have talk to him. (N)
An orange cat sleeping with a dog that is much larger then it. (N) He has many wonderful toy. (N)
I ate a tasty desert after lunch. (N) He sended the children home to their parents. (N)
A group of people getting ready for a surprise party. (Y) The passage through the hills was narrow. (Y)
A small refrigerator filled with colorful fruits and vegetables. (Y) A sleeping dog. (Y)
Two men fly by in a red plain. (N) The questions on the test was difficult. (N)
A causal picture of a man and a woman. (N) In Finland, we are used to live in a cold climate. (N)
Three men are going out for a special occasion. (Y) Three white sheeps graze on the grassy field. (N)
Woman eatting lots of food. (N) Between you and me, this is wrong. (Y)
Dyning room with chairs. (N) They are living there during six months. (N)
A woman recieving a package. (N) I was given lots of advices about buying new furnitures. (N)
This is a relatively uncommon occurance. (Y) A horse being led back to it?s stall. (N)
Table 3: The spelling and grammar portions of the qualification test. The test may be found on MTurk by searching
for the qualification entitled ?Image Annotation Qualification?.
Figure 6: Screenshot of the image annotation instruc-
tions: guidelines (top) and examples (bottom).
Figure 7: Screenshot of the quality control test instruc-
tions: guidelines (top) and examples (bottom).
147
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 162?171,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Cross-caption coreference resolution for automatic image understanding
Micah Hodosh Peter Young Cyrus Rashtchian Julia Hockenmaier
Department of Computer Science
University of Illinois at Urbana-Champaign
{mhodosh2, pyoung2, crashtc2, juliahmr}@illinois.edu
Abstract
Recent work in computer vision has aimed
to associate image regions with keywords
describing the depicted entities, but ac-
tual image ?understanding? would also re-
quire identifying their attributes, relations
and activities. Since this information can-
not be conveyed by simple keywords, we
have collected a corpus of ?action? photos
each associated with five descriptive cap-
tions. In order to obtain a consistent se-
mantic representation for each image, we
need to first identify which NPs refer to
the same entities. We present three hierar-
chical Bayesian models for cross-caption
coreference resolution. We have also cre-
ated a simple ontology of entity classes
that appear in images and evaluate how
well these can be recovered.
1 Introduction
Many photos capture a moment in time, telling a
brief story of people, animals and objects, their at-
tributes, and their relationship to each other. Al-
though different people may give different inter-
pretations to the same picture, people can read-
ily interpret photos and describe the entities and
events they perceive in complex sentences. This
level of image understanding still remains an elu-
sive goal for computer vision: although current
methods may be able to identify the overall scene
(Quattoni and Torralba, 2009) or some specific
classes of entities (Felzenszwalb et al, 2008), they
are only starting to be able to identify attributes
of entities (Farhadi et al, 2009), and are far from
recovering a complete semantic interpretation of
the depicted situation. Like natural language pro-
cessing, computer vision requires suitable training
data, and there are currently no publicly available
data sets that would enable the development of
such systems.
Photo sharing sites such as Flickr allow users
to annotate images with keywords and other de-
scriptions, and vision researchers have access to
large collections of images annotated with key-
words (e.g. the Corel collection). A lot of recent
work in computer vision has been aimed at pre-
dicting these keywords (Blei et al, 2003; Barnard
et al, 2003; Feng and Lapata, 2008; Deschacht
and Moens, 2007; Jeon et al, 2003). But key-
words alone are not expressive enough to capture
relations between entities. Some research has used
the text that surrounds an image in a news arti-
cle as a proxy (Feng and Lapata, 2008; Deschacht
and Moens, 2007). However, in many cases, the
surrounding text or a user-provided caption does
not simply describe what is depicted in the image
(since this is usually obvious to the human reader
for which this text is intended), but provides ad-
ditional information. We have collected a corpus
of 8108 images associated with several simple de-
scriptive captions. In contrast to the text near an
image on the web, the captions in our corpus pro-
vide direct, if partial and slightly noisy, descrip-
tions of the image content. Our data set differs
from paraphrase corpora (Barzilay and McKeown,
2001; Dolan et al, 2004) in that the different cap-
tions of an image are produced independently by
different writers. There are many ways of describ-
ing the same image, because it is often possible
to focus on different aspects of the depicted situ-
ation, and because certain aspects of the situation
may be unclear to the human viewer.
One of our goals is to use these captions to
obtain a semantic representation of each image
that is consistent with all of its captions. In or-
der to obtain such a representation, it is neces-
sary to identify the entities that appear in the im-
age, and to perform cross-caption coreference res-
olution, i.e. to identify all mentions of the same
entity in the five captions associated with an im-
age. In this paper, we compare different meth-
162
A golden retriever (ANIMAL) is playing with a smaller black and brown dog(ANIMAL) in a pink collar (CLOTHING).
A smaller black dog (ANIMAL) is fighting with a larger brown dog (ANIMAL) in a forest (NAT_BACKGROUND).
A smaller black and brown dog (ANIMAL) is jumping on a large orange dog (ANIMAL).
Brown dog (ANIMAL) with mouth (BODY_PART) open near head(BODY_PART) of black and tan dog (ANIMAL).
Two dogs (ANIMAL) playing near the woods (NAT_BACKGROUND).
Figure 1: An image with five captions from our corpus. Coreference chains and ontological classes are
indicated in color.
ods of cross-caption coreference resolution on our
corpus. In order to facilitate further computer vi-
sion research, we have also defined a set of coarse-
grained ontological classes that we use to automat-
ically categorize the entities in our data set.
2 A corpus of action images and captions
Image collection and sentence annotation We
have constructed a corpus consisting of 8108 pho-
tographs from Flickr.com, each paired with five
one-sentence descriptive captions written by Ama-
zon?s Mechanical Turk1 workers. We downloaded
a few thousand images from each of six selected
Flickr groups2. To facilitate future computer vi-
sion research on our data, we filtered out images in
black-and-white or sepia, as well as images with
watermarks, signatures, borders or other obvious
editing. Since our collection focuses on images
depicting actions, we then filtered out images of
scenery, portraits, and mood photography. This
was done independently by two members of our
group and adjudicated by a third.
We paid Turk workers $0.10 to write one de-
scriptive sentence for each of five distinct and ran-
domly chosen images that were displayed one at
a time. We required a small qualification test
that examined the workers? English grammar and
spelling and we restricted the task to U.S. work-
ers (see Rashtchian et al (2010) for more details).
Our final corpus contains five sentences for each
of our 8108 images, totaling 478,317 word tokens,
and an average sentence length of 11.8 words.
We first spell-checked3 these sentences, and used
OpenNLP4 to POS-tag them. We identified NPs
using OpenNLP?s chunker, followed by a semi-
1https://www.mturk.com
2The groups:?strangers!?, ?Wild-Child (Kids in Action)?,
?Dogs in Action (Read the Rules)?, ?Outdoor Activities?,
?Action Photography?, ?Flickr-Social (two or more people in
the photo)?.
3We used Unix?s aspell to generate possible correc-
tions and chose between them based on corpus frequencies.
4http://opennlp.sourceforge.net
automatic procedure to correct for a number of
systematic chunking errors that could easily be
corrected. We randomly selected 200 images for
further manual annotation, to be used as test and
development data in our experiments.
Gold standard coreference annotation We
manually annotated NP chunks, ontological
classes, and cross-caption coreference chains for
each of the 200 images in our test and development
data. Each image was annotated independently by
two annotators and adjudicated by a third.5 The
development set contains 1604 mentions. On av-
erage, each caption has 3.2 mentions, and each im-
age has 5.9 coreference chains (distinct entities).
Ontological annotation of entities In order to
understand the role entities mentioned in the sen-
tences play in the image, we have defined a simple
ontology of entity classes (Table 1). We distin-
guish entities that constitute the background of an
image from those that appear in the foreground.
These entities can be animate (people or animals)
or inanimate. For inanimate objects, we distin-
guish static objects from ?movable? objects. We
also distinguish man-made from natural objects
and backgrounds, since this matters for computer
vision algorithms. We have labeled the entity
mentions in our test and development data with
classes from this ontology. Again, two of us an-
notated each image?s mentions, and adjudication
was performed by a single person. Our ontology
is similar to, but smaller than the one proposed
by Hollink and Worring (2005) for video retrieval,
which in turn is based on Hoogs et al (2003) and
Hunter (2001).
3 Predicting image entities from captions
Figure 1 shows an image from our corpus. Dif-
ferent captions use different words to refer to the
5We used MMAX2 (Mu?ller and Strube, 2006) both for
annotation and adjudication.
163
Ontological Class Examples
animal dog, horse, cow
background man-made street, pool, carpet
background natural ocean, field, air
body part hair, mouth, arms
clothing shirt, hat, sunglasses
event trick, sunset, game
fixed object man-made furniture, statue, ramp
fixed object natural rock, puddle, bush
image attribute camera, picture, closeup
material man-made paint, frosting
material natural water, snow, dirt
movable man-made ball, toy, bowl
movable natural leaves, snowball
nondepictable something, Batman
orientation front, top, [the] distance
part of edge, side, top, tires
person family, skateboarder
property of shadow, shade, theme
vehicle surfboard, bike, boat
writing graffiti, map
Table 1: Our ontology for entities in images.
same entity, or even seemingly contradictory mod-
ifiers (?orange? vs. ?brown? dog). In order to
predict what entities appear in an image from its
captions, we need to identify how many entities
each sentence describes, and what role these enti-
ties play in the image (e.g. person, animal, back-
ground). Because we have five sentences asso-
ciated with each image, we also need to identify
which noun phrases in the different captions of
the same image refer to the same entity. Because
the captions were generated independently, there
are no discourse cues such as anaphora to identify
coreference. This creates problems for standard
coreference resolution systems trained on regular
text. Our data also differs from standard corefer-
ence data sets in that entities are rarely referred to
by proper nouns.
Our first task is to identify which noun phrases
may refer to the same entity. We do this by identi-
fying the set of entity types that each NP may refer
to. We use WordNet (Fellbaum, 1998) to iden-
tify the possible entity types (WordNet synsets) of
each head noun. Since the salient entities in each
image are likely to be mentioned by more than one
caption writer, we then aim to restrict those types
to those that may be shared by some head nouns in
the other captions of the same image. This gives
us an inventory of entity types for each mention,
which we use to identify coreferences, restricted
by the constraint that all coreferent mentions refer
to an entity of the same type.
4 Using WordNet to identify entity types
WordNet (Fellbaum, 1998) provides a rich ontol-
ogy of entity types that facilitates our coreference
task.6 We use WordNet to obtain a lexicon of pos-
sible entity types for each mention (based on their
lexical heads, assumed to be the last word with a
nominal POS tag7). We first generate a set of can-
didate synsets based solely on the lexical heads,
and then generate lexicon entries based on rela-
tions between the candidates.
WordNet synsets provide us with synonyms,
and hypernym/hyponym relations. For each men-
tion, we generate a list of candidate synsets.
We require that the candidates are one of the
first four synsets reported and that their fre-
quency is to be at least one-tenth of the most
frequent synset. We limit candidates to ones
with ?physical entity#n#1?, ?event#n#1?, or ?vi-
sual property#n#1? as a hypernym, in order to en-
sure that the synset describes something that is de-
pictable. To avoid word senses that refer to a per-
son in a metaphorical fashion, (e.g. pig meaning
slovenly person or red meaning communist), we
ignore synsets that refer to people if the word has
a synset that is an animal or color.8
In general, we would like for mentions to be
able to take on more specific word senses. For ex-
ample, we would like to be able to identify that
?woman? and ?person? may refer to the same
entity, whereas ?man? and ?woman? typically
would not. However, we also do not want a type
inventory that is too large or too fine-grained.
Once the candidate synsets are generated, we
consider all pairs of nouns (n1, n2) that occur in
different captions of the same image and exam-
ine all corresponding pairs of candidate synsets
(s1, s2). If s2 is a synonym or hypernym of s1, it
is possible that two captions have different words
describing the same entity, so we add s1 and s29
to the lexicon of n1. Adding s2 to n1?s lexicon al-
lows it to act as an umbrella sense covering other
nouns describing the same entity.10 We add s2 to
6For the prediction of ontological classes, we use our own
ontology because WordNet is too fine-grained for this pur-
pose.
7If there are two NP chunks that form a ?[NP ... group] of
[NP... ]? construction, we only use the second NP chunk.
8An exception list handles cases (diver, blonde), where the
human sense is more likely than the animal or color sense.
9We don?t add s2 if it is ?object#n#1? or ?clothing#n#1?.
10This is needed when captions use different aspects of
the entity to describe it (for example, ?skier? and ?a skiing
man?).
164
the lexicon of n2 (since if n1 is using the sense s1,
then n2 must be using the sense s2) and if n1 oc-
curs at least five times in the corpus, we add s1 to
the lexicon of n2.
5 A heuristic coreference algorithm
Based on WordNet candidate synsets, we define
a heuristic algorithm that finds the optimal entity
assignment for the mentions associated with each
image. This algorithm is based on the principles
driving our generative model described below, and
on the observation that salient entities will be men-
tioned in many captions and that captions tend to
use similar words to describe the same entity.
Simple heuristic algorithm:
1. For each noun, choose the synset that appears
in the most number of captions of an image,
and break ties by choosing the synset that
covers the fewest distinct lemmatized nouns.
2. Group all of the noun phrase chunks that
share a synset into a single coreference chain.
6 Bayesian coreference models
Since we cannot afford to manually annotate our
entire data set with coreference information, we
follow Haghighi and Klein (2007)?s work on un-
supervised coreference resolution, and develop a
series of generative Bayesian models for our task.
6.1 Model 0: Simple Mixture Model
In our first model, based on Haghighi and Klein?s
baseline Dirichlet Process model, each image i
corresponds to the set of observed mentions wi
from across its captions. Image i has a hidden
global topic Ti, drawn from a distribution with a
GEM prior with hyperparameter ? as explained by
Teh et al (2006). In a Dirichlet process, the GEM
distribution is an infinite analog of the Dirich-
let distribution, allowing for a potentially infinite
number of mixture components. P (Ti = t) is pro-
portional to ? if t is a new component, or to the
number of times t has been drawn before other-
wise. Given a topic choice Ti = t, entity type
assignments Zj for all mentions wj in image i
are in turn drawn from a topic-specific multino-
mial ?t over all possible entity types E that was
drawn from a Dirichlet prior with hyperparameter
?. Similarly, given an entity type Zi = z, each
corresponding (observed) head word wj is drawn
from an entity type-specific multinomial ?z over
all possible words V, drawn from a finite Dirich-
let prior with hyperparameter ?. The set of all im-
ages belonging to the same topic is analogous to
an individual document in Haghighi and Klein?s
baseline model.11 All headwords of the same en-
tity type are assumed to be coreferent, similar to
Haghighi and Klein?s model. As described in sec-
tion 4, we use WordNet to identify the subset of
types that can actually produce the given words.
Therefore, similar to the way Andrzejewski and
Zhu (2009) handled a priori knowledge of topics,
we will define an indicator variable ?ij that is 1
iff the WordNet information allows word i to be
produced from entity set j and 0 otherwise.
6.1.1 Sampling Model 0
We find argmaxZ,TP (Z,T|X) with Gibbs sam-
pling. Here, Z and T are the collection of type
and topic assignments, with Z?j = Z? {Zj} and
T?i = T ? {Ti}. This style of notation will be
extended analogously to other variables. Let ne,x
represent the number of times word x is produced
from entity e across all topics and let pj be the
number of images assigned to topic j. Let mt,e
represent the number of times entity type e is
generated by topic t. Each iteration consists of
two steps: first, each Zi is resampled, fixing T;
and then each Ti is resampled based on Z12.
1. Sampling Zj:
P (Zj=e|wj ? w
i,Z?j ,T) ? P (wj |Zj=e)P (Zj=e|Ti)
P (wj = x|Zj = e) ?
?
n?je,x + ?
P
x? n
?j
e,x? + ?
?
?xe
P (Zj = e|Ti = t) =
m?jt,e + ?
P
e? m
?j
t,e? + ?
2. Sampling Ti:
P (Ti=j|w,Z,T
?i) ? P (Ti=j|T
?i)P (Z|Ti=j,T
?i)
? P (Ti = j|T
?i)
Y
k?wi
P (Zk|Ti = j)
= P (Ti = j|T
?i)
Y
k?wi
m?ij,Zk + ?P
e? m
?i
j,e? + ?
P (Ti = j|T
?i) ?
(
?, If its a new topic
pj Otherwise
11Since we do not have multiple images of the same well-
known people or places, referred to by their names, we do not
perform any cross-image coreference
12Sampling on the exponentiated posterior to find the mode
as Haghighi and Klein (2007) did was found to not signifi-
cantly affect results on our tasks
165
Caption 1: {x21,1:a golden retriever; x21,2 :a smaller black and brown dog; x21,3:a pink collar}
Caption 2: {x21,4:a smaller black dog; x21,5:a larger brown dog; x21,6:a forest}
Caption 3: {x21,7:small black and brown dog; x21,8:a large orange dog}
Caption 4: {x21,9:brown dog; x21,10:mouth; x21,11:head; x21,12:black and tan dog}
Caption 5: {x21,13:two dogs; x21,14:the woods}
DOG
attr:5
DOG
attr:3
CLOTHING
attr:2
FOREST
attr:8
Image 21:
x21,1
x21,5
x21,8
x21,9
MOUTH
attr:0 ...
x21,2 
x21,4 
x21,7 
x21,12 
x21,3 x21,6
x21,14
x21,10 Restaurant 211
Figure 2: Models 1 and 2 as Chinese restaurant franchises: each image topic is a franchise, each image
is a restaurant, each entity is a table, each mention is a customer. Model 2 adds attributes (in italics).
6.2 Model 1: Explicit Entities
Model 0 does not have an explicit representation
of entities beyond their type and thus cannot dis-
tinguish multiple entities of the same type in an
image. Although Model 1 also represents men-
tions only by their head words (and thus cannot
distinguish black dog from brown dog), it creates
explicit entities based on the Chinese restaurant
franchise interpretation of the hierarchical Dirich-
let Process model (Teh et al, 2006). Figure 2 (ig-
noring the modifiers / attributes for now) illustrates
the Chinese restaurant franchise interpretation of
our model. Using this metaphor, there are a se-
ries of restaurants (= images), each consisting of
a potentially infinite number of tables (= entities),
that are frequented by customers (= entity men-
tions) who will be seated at the tables. Restau-
rants belong to franchises (= image topics). Each
table is served one dish (= entity type, e.g. DOG,
CLOTHING) shared by all the customers. The head
word of a mention xi,j is generated in the follow-
ing manner: customer j enters restaurant i (be-
longing to franchise Ti) and sits down at one of
the existing tables with probability proportional to
the number of other customers there, or sits at a
new table with probability proportional to a con-
stant. A dish eia (DOG) from a potentially infinite
menu is served to each new table a, with probabil-
ity proportional to the total number of tables it is
served at in the franchise Ti (or to a constant if it
is a new dish). The (observed) head word of the
mention xj,i (dog, retriever) is then drawn from
the multinomial distribution over words defined by
the entity type (DOG) at the table. The menu (set
of dishes) available to each restaurant and table is
restricted by our lexicon of WordNet synsets for
each mention. More formally, each image topic
t defines a distribution over entities drawn from a
global GEM prior with hyperparameter ?. There-
fore, the probability of an entity a is proportional
to the number of its existing mentions in images of
the same topic, or to ?, if it is previoulsy unmen-
tioned. The type of each entity, ea, is drawn from
a topic-dependent multinomial with global Dirich-
let prior. The head words of mentions are gener-
ated by their entity type as in Model 0. Mentions
assigned to the same entity are considered to be
coreferent. Based on the nature of our corpus, we
again assume that two words cannot be coreferent
within a sentence, restrict the distribution to not
allow inter-sentence coreference and renormalize
the values accordingly.
6.2.1 Sampling Model 1
There are three parts to our resampling procedure:
resampling the entity assignment for each word,
resampling the entity type for each entity, and re-
sampling the topic of each image. The kth word
of image i, sentence j, will now be wij,k; e
i
a is the
entity type of entity a in image i; aij,k is the en-
tity that word k of sentence j is produced from in
image i, and Zij,k represents that entity?s type. a
is the set of all current entity assignments and e
are the type assignments for entities. m is now de-
fined as the number of entities of a certain type be-
ing drawn for an image, n is defined as before and
ci,a is the number of times entity a is expressed in
image i. Topics are resampled as in Model 0.
Entity Assignment Resampling Entity assign-
ments for words are resampled one sentence at a
time in the order the headwords appear in the sen-
tence. For each word in the sentence, entity as-
signments are defined by the distribution of Fig-
ure 3. The headword is assigned to an existing
entity with probability proportional to the number
of entities already assigned to that entity and the
probability that the entity emits that word. The
word is assigned to a new entity with a newly
166
Model 1:
P (eia=e|a,w, Ti = t, e
?i,a) ? (m?e
i,a
t,e + ?)
Q
{wij,k=x|a
i
j,k=a}
n?e
i,a
e,x + ?
P
y(n
?ei,a
e,y + ?)
?x,e
P (aij,k=a|a
?(i,j,k?|k??k), e,T) ?
(
c?j,k
?
i,a P (w
i
j,k|Z
i
j,k = e
i
a,a
?(i,j,k?|k??k))?ij,a, if a is not new
?P (eia|e
?i,a, Ti)P (wij,k|Z
i
j,k = e
i
a,a
?(i,j,k?|k??k)), o/w
With:
P (wij,k=x|Z
i
j,k=e,a
?(i,j,k?|k??k)) ?
n?(i,j,k
?|k??k)
e,x + ?
P
y(n
?(i,j,k?|k??k)
e,y + ?)
?x,e
P (eia=e|e
?i,a, Ti = t) ?
m?e
i,a
t,e + ?
P
e?(m
?ei,a
t,e? + ?)
Model 2:
P (aij,k=a|a
?(i,j,k?|k??k), e,T,b) ?
(
c?j,k
?
i,a P (w
i
j,k|Z
i
j,k = e
i
a,a
?(i,j,k?|k??k))?ij,aP (d
i
j,k|b
i
a), if a is not new
?P (eia|e
?i,a, Ti)P (wij,k|Z
i
j,k = e
i
a,a
?(i,j,k?|k??k))P (bia)P (d
i
a|b
i
a), o/w
P (bia=b|D
i
a,b
?i,a)?P (bia=b|b
?i,a)
Q
d?Dia
(s?i,ab,d + ?)
P
d?(s
?i,a
b,d? + ?)
Figure 3: Sampling equations for Models 1 and 2
drawn entity type with probability proportional ?,
the probability that the entity type is for an im-
age of the given topic (normalized over WordNet?s
possible entities for the word), and the probability
the drawn type produces the word. ?ij,a = 1 iff
entity a of image i does not appear in sentence j
and ?ij,a = 0 otherwise. a
?(i,j,k?|k??k) represents
removing the kth or later words in sentence j of
image i
Entity Type Resampling Fixing the assign-
ments, the type of each entity is redrawn based
on the distribution in Figure 3. It is proportional
to the probability that a certain entity type is in an
image of a given topic and, independently for each
of the words, the probability that the given word
expresses the type. n?e
i,a
e,x is the number of times
entity type e is expressed as word x not counting
the words attached to the currently entity being re-
sampled and m?e
i,a
t,e is the number of times an en-
tity of type e appears in an image of topic t not
counting the current entity being resampled. The
probability of a given image belonging to a topic is
proportional the number of images already in the
topic (or ?) followed by the probability that each
of the entities in the image were drawn from that
topic.
6.3 Model 2: Explicit Entities and Modifiers
Certain entities cannot be distinguished simply by
head word alone, such in the example in Figure 2.
Model 2 augments Model 1 with the ability to gen-
erate modifiers. In addition to an entity type, each
entity draws an attribute from a global distribution
drawn from a GEM distribution with hyperparam-
eter ?. An attribute is a multinomial distribution,
on possible modifier words, drawn from a Dirich-
let prior with parameter ?. From the attribute, each
modifier word is drawn independently. There-
fore given an attribute b and a set of modifiers d:
P (d|b) ?
?
d?d(sd + ?) where sd is number of
times modifier d is produced by attribute b. In ad-
dition, the probability of a certain attribute b given
all other assignments is given by:
P (bia = b|b
?i,a) ?
(
?, If its a new attribute
rb, Otherwise
where rb is the number of entities with at-
tribute b. As in Model 1, mentions assigned to
the same entity are considered coreferent. Con-
sider the ?smaller black dog? mention in Figure 2.
When the mention is being resampled, the at-
tribute choice for each table will bias the probabil-
ity distribution towards the table whose attribute
is more likely to produce ?smaller? and ?black?.
Therefore, the model can now better distinguish
the two dogs in the image.
6.3.1 Sampling Model 2
The addition of modifiers only directly effects the
distribution when resampling entity assignments
since attributes are independent of entity types,
image topics, and headwords of noun phrases. The
sampling distribution are again shown in Figure 3.
In a separate sampling step, it is now necessary to
resample the attribute assigned to each entity: The
probability of drawing a certain attribute is illus-
trated in Figure 3 with Dia as the set of all the mod-
ifiers of all the noun phrases currently assigned to
167
entity a of image i, and s?i,ab,d as the number of
times attribute b produces modifier d without the
current assignment of entity a of image i.
6.4 Implementation
The topic assignments for each image are initial-
ized to correspond to the Flickr groups the images
were taken from. Each mention was initialized as
its own entity with type and attribute sampled from
a uniform distribution.
As our training is unsupervised, each of the
models were ran on the entire dataset. For Model
0, after burn-in, 20 samples of Z were taken
spaced 200 iterations apart, while for Model 1
samples were taken spaced 100 apart, and 25 apart
for Model 2. The implementation of Model 2 ran
too slow to effectively judge when burn in oc-
curred, impacting the results.
The values of parameters ?, ?, ?, ?, ?, ?, and
the number of initial attributes were hand-tuned
based on the average performance on our anno-
tated development subset of 100 images.13
7 Evaluation of coreference resolution
We evaluate each of the generative models and the
heuristic coreference algorithm on the annotated
test subset of our corpus consisting of 100 images
with both the OpenNLP chunking and the gold
standard chunking. We report our scores based
on the MUC evaluation metric. The results are
reported in Table 2 as the average scores across
all the samples of two independent runs of each
model. We also present results on Model 0 with-
out using WordNet where every word can be an
expression of one of 200 fake entity sets. The
same table also shows the performance of a base-
line model and the upper bound on performance
imposed by WordNet.
A baseline model: In our baseline model, two
noun phrases in captions of the same image are
coreferent if they share the same head noun.
Upper bound on performance: Although
WordNet synsets provide a good indication of
whether two mentions can refer to the same
entity or not, they may also be overly restrictive
in other cases. We measure the upper bound
on performance that our reliance on WordNet
imposes by finding the best-scoring coreference
assignment that is consistent with our lexicon.
13(0.1, 0.1, 100, 0.001875, 100, 0.0002, 20) respectively.
This achieves an F-score of 90.2 on the test data
with gold chunks.
Performance increases in each subsequent
model. The heuristic beats each of the models, but
in some sense it is an extreme version of Model
1. Both it and Model 1 attempt to produce en-
tity sets that cover as many captions as possible,
while minimizing the number of distinct words in-
volved. The heuristic locally forces this case, at
the expense of no longer being a generative model.
8 Ontological Class Prediction
As a further step towards understanding the se-
mantics of images, we develop a model that labels
each entity with one of the ontological classes de-
fined in section 2. The immediate difficulty of this
task is that our ontology includes not only seman-
tic distinctions, but also spatial and visual ones.
While it may be easy to tell which words are an-
imals and which are people, there is only a fine
distinction at the language level whether an object
is movable, fixed, or part of the background.14
8.1 Model and Features
We define our task as a classification problem,
where each entity must be assigned to one of
twenty classes defined by our ontology. We use a
Maximum Entropy classifier, implemented in the
MALLET toolkit (McCallum, 2002), and define
the following text features:
NP Chunk: We include all the words in the NP
chunk, unfiltered.
WordNet Synsets and Hypernyms: The most
likely synset is either the first one that appears in
WordNet or one of the ones predicted by our coref-
erence system. For each of these possibilities, we
include all of that synset?s hypernyms.
Syntactic Role: We parsed our captions with the
C&C parser (Clark and Curran, 2007), and record
whether the word appears as a direct object of a
verb, as the object of a preposition, as the subject
of the sentence, or as a modifier. If it is a modi-
fier, we also add the head word of the phrase being
modified.
14For example, we deem bowls and silverware to be mov-
able objects; furniture, fixed; and carpets, background. More-
over, in all three cases, we must correctly distinguish that
these objects are man-made and not found in nature.
168
Model OpenNLP chunks Gold chunks
Rec. Prec. F1 Rec. Prec. F1
Baseline 57.3 89.5 69.9 64.1 96.2 77.0
Upper bound 82.1 100 90.2
WN Heuristic 70.6 84.8 77.0 80.4 90.6 85.2
Model 0 w/o WN 79.7 59.8 68.4 85.1 62.7 72.2
Model 0 66.8 83.1 74.1 75.9 90.3 82.5
Model 1 69.6 83.8 76.0 78.0 90.8 83.9
Model 2 69.2 84.4 76.1 77.9 91.5 84.1
Table 2: Coreference resolution results (MUC scores; Models 0-2 are averaged over all samples)
8.2 Experiments
We use two baselines. The naive baseline catego-
rizes words by selecting the most frequent class
of the word. If no instances of the word have oc-
curred, it uses the overall most frequent class. The
WordNet baseline works by finding the most fre-
quent class amongst the most relevant synsets for
a word. It calculates the class frequency for each
synset by assuming each word has the sense of its
first synset and incrementing the frequency of the
first synset and its hypernyms. When categorizing
a word, it finds the set of closest hypernyms of the
word that have a non-zero frequency, and chooses
the class with the greatest sum of frequency counts
amongst those hypernyms.
We train the MaxEnt classifier using semi-
supervised learning. Initially, we train a classifier
using the 500 sentence gold standard development
set. For each class, we use the top 5%15 of the la-
bels to label the unlabeled data and provide addi-
tional training data. We then retrain the classifier
on the newly labeled examples and the develop-
ment set, and run it on the test set. For each coref-
erence chain in the test set, we relabel all of the
mentions in the chain to use the majority class, if
a clear majority exists. If no such majority exists,
we leave the labels as is. The MaxEnt classifier
experiments were conducted by varying the source
of the synset assigned to each word. For each of
our coreference systems, we report two scores (Ta-
ble 3). The first is the average accuracy when us-
ing the output from two runs of each model with
about 20 samples per run, and the second uses the
output that performs best on the coreference task
when scored on the development data.
Discussion Although we use WordNet to clas-
sify our entity mentions, we designed our ontology
by considering only the images and their captions,
with no particular mapping to WordNet in mind.
15This was tuned using 10-fold cross-validation of the de-
velopment set.
Classifier (synset prediction) Accuracy (gold chunks)
Naive Baseline 72.0
WordNet Baseline 81.0
MaxEnt (1st-synset) 84.4
MaxEnt (WN heuristic) 82.7
Avg. ? Best-Coref
MaxEnt (Model 1) 83.9 0.5 84.5
MaxEnt (Model 2) 84.1 0.4 85.3
Table 3: Prediction of ontological classes
Therefore, these experiments provide of a proof of
concept for the semi-supervised labeling of a cor-
pus using any semantic/visual ontology.
Overall, Model 2 had the best performance for
this task. This demonstrates that the additional
features of Model 2 force synset selections that are
consistent across the entire corpus, and are sen-
sitive to the modifiers appearing with them. The
WordNet heuristic selects synsets in a fairly ar-
bitrary manner - all other things being equal, the
synsets are chosen without reference to what other
synsets are chosen by similar clusters of nouns.
9 Evaluating entity prediction
Together, the coreference resolution algorithm and
ontology classification model provide us with a set
of distinct, ontologically-categorized entities ap-
pearing in each image. We perform a final experi-
ment to evaluate how well our models can recover
the mentioned entities and their ontological types
for each image. We now represent each entity as a
tuple (L, c), where L is its coreference chain, and
c is the ontological class of these mentions. 16
We compute the precision and recall between
the predicted and gold standard tuples for each im-
age. We consider a tuple (L?, c?) correctly pre-
dicted only when a copy of (L?, c?) occurs both
in the set of predicted tuples and the set of gold
standard tuples.17 Then, as usual, for precision we
16Note that for each image, the tuples of all entities corre-
spond to a partition of the set of the head-word mentions in
an image.
17We assign no partial credit because incorrect typing or
169
Model Recall Precision F-score
Baseline 28.4 20.6 23.9
WordNet Heuristic 48.3 43.9 46.0
Model 1 (avg) 51.7 42.8 46.8
Model 1 (best-coref) 50.9 45.4 48.0
Model 2 (avg) 52.2 42.7 47.0
Model 2 (best-coref) 52.3 46.0 49.0
Table 4: Overall entity recovery. We measure
how many entities we identify correctly (requiring
complete recovery of their coreference chains and
correct prediction of their ontological class.
normalize the number of overlapping tuples by the
number of predicted tuples, and for recall, by the
number of gold standard tuples. We report average
precision and recall over all images in our test set.
We report scores for four different pairs of on-
tological class and coreference chain predictions.
As a baseline, we use the ontological classes pre-
dicted by the our naive baseline and the chains pre-
dicted by the ?same-words-are-coreferent? coref-
erence resolution baseline.
We also report results using the classes and
chains predicted by Model 1, Model 2, and the
WordNet Heuristic Algorithm. The influence of
the different coreference algorithms comes from
the entity types that are used to determine corefer-
ence chains, and that also correspond to WordNet
candidate synsets. In other words, although the
final coreference chain may be predicted by two
different models, the synsets they use to do so may
differ, affecting the synset and hypernym features
used for ontological prediction. We present results
in Table 4 for these four different set-ups.
The synsets chosen by the different corefer-
ence algorithms clearly have different applicabil-
ity when it comes to ontological class prediction.
Although Model 2 performs comparably to Model
1 and does worse than the WordNet heuristic al-
gorithm for coreference chain prediction, it cer-
tainly does better on this task. Since our end goal
is creating a unified semantic representation, this
final task judges the effectiveness of our models to
capture the most detailed entity information. The
success of Model 2 means that the incorporation
of adjectives informs the proper choice of synsets
that are useful in predicting ontological classes.
10 Conclusion
As a first step towards automatic image under-
standing, we have collected a corpus of images as-
incomplete coreference chaining both completely change the
semantics of an image.
sociated with several simple descriptive captions,
which provide more detailed information about
the image than simple keywords. We plan to make
this data set available for further research in com-
puter vision and natural language processing. In
order to enable the creation of a semantic repre-
sentation of the image content that is consistent
with the captions in our data set, we use Word-
Net and a series of Bayesian models to perform
cross-caption coreference resolution. Similar to
Haghighi and Klein (2009), who find that linguis-
tic heuristics can provide very strong baselines for
standard coreference resultion, relatively simple
heuristics based on WordNet alne perform sur-
prisingly well on our task, although they are out-
performed by our Bayesian models for overall en-
tity prediction. Since our generative models are
based on Dirichlet Process priors, they are de-
signed to favor a small number of unique entities
per image. In the heuristic algorithm, this bias
is built in explicitly, resulting in slightly higher
performance on the coreference resolution task.
However, while the generative models can use
global information to learn what entity type each
word is likely to represent, the heuristic is unable
to capture any non-local information about the en-
tities, and thus provides less useful input for the
prediction of ontological classes.
Future work will aim to improve on these re-
sults by overcoming the upper bound on perfor-
mance imposed by WordNet, and through a more
sophisticated model of modifiers. We will also in-
vestigate how image features can be incorporated
into our model to improve performance on entity
detection. Ultimately, identifying the depicted en-
tities from multiple image captions will require
novel ways to correctly handle the semantics of
plural NPs (i.e. that one caption?s ?two dogs? con-
sist of another?s ?golden retreiver? and ?smaller
black dog?). We foresee similar challenges when
dealing with verbs and events.
The creation of an actual semantic representa-
tion of the image content is a challenging problem
in itself, since the different captions often focus
on different aspects of the depicted situation, or
provide different interpretation of ambiguous sit-
uations. We believe that this poses many inter-
esting challenges for natural language processing,
and will ultimately require ways to integrate the
information conveyed in the caption with visual
features extracted from the image.
170
Acknowledgements
This research was funded by NSF grant IIS 08-
03603 INT2-Medium: Understanding the Mean-
ing of Images. We are grateful for David Forsyth
and Dan Roth?s advice, and for Alex Sorokins sup-
port with MTurk.
References
David Andrzejewski and Xiaojin Zhu. 2009. Latent
Dirichlet alocation with topic-in-set knowledge. In
NAACL HLT 2009 Workshop on Semi-Supervised
Learning for Natural Language Processing, pages
43?48.
Kobus Barnard, Pinar Duygulu, David Forsyth,
Nando De Freitas, David M. Blei, and Michael I.
Jordan. 2003. Matching words and pictures. Jour-
nal of Machine Learning Research, 3:1107?1135.
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting paraphrases from a parallel corpus. In
Proceedings of the 39th annual meeting of the Asso-
ciation for Computational Linguistics, pages 50?57,
Toulouse, France, July.
David M. Blei, Michael I, David M. Blei, and Michael
I. 2003. Modeling annotated data. In Proceedings
of the 26th International ACM SIGIR Conference,
pages 127?134.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Koen Deschacht and Marie-Francine Moens. 2007.
Text analysis for automatic image annotation. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of Coling 2004, pages 350?356,
Geneva, Switzerland, August. COLING.
A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. 2009.
Describing objects by their attributes. In IEEE Con-
ference on Computer Vision and Pattern Recogni-
tion, pages 1778?1785, June.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press, Cam-
bridge, MA ; London, May.
P. Felzenszwalb, D. McAllester, and D. Ramanan.
2008. A discriminatively trained, multiscale, de-
formable part model. In IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 1?8,
June.
Yansong Feng and Mirella Lapata. 2008. Automatic
image annotation using auxiliary text information.
In Proceedings of ACL-08: HLT, pages 272?280,
Columbus, Ohio, June.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric Bayesian
model. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 848?855, Prague, Czech Republic.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1152?1161, Singapore, August. Associ-
ation for Computational Linguistics.
L. Hollink and M. Worring. 2005. Building a vi-
sual ontology for video retrieval. In MULTIMEDIA
?05: Proceedings of the 13th annual ACM interna-
tional conference on Multimedia, pages 479?482,
New York, NY, USA. ACM.
A. Hoogs, J. Rittscher, G. Stein, and J. Schmiederer.
2003. Video content annotation using visual anal-
ysis and a large semantic knowledgebase. In IEEE
Computer Society Conference on Computer Vision
and Pattern Recognition, volume 2, pages II?327 ?
II?334 vol.2, June.
Jane Hunter. 2001. Adding multimedia to the semantic
web - building an mpeg-7 ontology. In In Interna-
tional Semantic Web Working Symposium (SWWS,
pages 261?281.
Lavrenko Manmatha Jeon, V. Lavrenko, R. Manmatha,
and J. Jeon. 2003. A model for learning the seman-
tics of pictures. In Seventeenth Annual Conference
on Neural Information Processing Systems (NIPS).
MIT Press.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Christoph Mu?ller and Michael Strube. 2006. Multi-
level annotation of linguistic data with MMAX2. In
Sabine Braun et al editor, Corpus Technology and
Language Pedagogy, pages 197?214. Peter Lang,
Frankfurt a.M., Germany.
Ariadna Quattoni and Antonio B. Torralba. 2009.
Recognizing indoor scenes. In IEEE Conference
on Computer Vision and Pattern Recognition, pages
413?420. IEEE.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image anno-
tations using amazons mechanical turk. In NAACL
Workshop Creating Speech and Language Data With
Amazons Mechanical Turk.
Yee Whye Teh, Michael I Jordan, Matthew J Beal, and
David M Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566?1581.
171
NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 90?95,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Induction of Linguistic Structure with Combinatory Categorial Grammars
Yonatan Bisk and Julia Hockenmaier
Department of Computer Science
University of Illinois at Urbana-Champaign
201 N Goodwin Ave. Urbana IL, 61801
{bisk1,juliahmr}@illinois.edu
Abstract
Our system consists of a simple, EM-based
induction algorithm (Bisk and Hockenmaier,
2012), which induces a language-specific
Combinatory Categorial grammar (CCG) and
lexicon based on a small number of linguistic
principles, e.g. that verbs may be the roots of
sentences and can take nouns as arguments.
1 Introduction
Much of the recent work on grammar induction has
focused on the development of sophisticated statisti-
cal models that incorporate expressive priors (Cohen
and Smith, 2010) or linguistic universals (Naseem et
al., 2010; Boonkwan and Steedman, 2011) that have
all been shown to be very helpful. But, with some
notable exceptions, such as (Cohn et al, 2011),
the question of what underlying linguistic represen-
tation to use has received considerably less atten-
tion. Our induction algorithm is based on Com-
binatory Categorial Grammar (Steedman, 2000), a
linguistically expressive, lexicalized grammar for-
malism which associates words with rich syntactic
categories that capture language-specific facts about
basic word order and subcategorization. While
Boonkwan and Steedman (2011) have shown that
linguists can easily devise a language-specific in-
ventory of such categories that allows a parser to
achieve high performance in the absence of anno-
tated training data, our algorithm automatically dis-
covers the set of categories it requires to parse the
sentences in the training data.
2 Combinatory Categorial Grammar
(CCG)
The set of CCG categories is built recursively from
two atomic types, S (sentence) and N (noun). Com-
plex types are of the form X/Y or X\Y, and repre-
sent functions which combine with an argument of
type Y to yield a constituent of type X as result. The
slash indicates whether the Y precedes (\) or follows
(/) the functor. An English lexicon should contain
categories such as S\N and (S\N)/N for verbs: both
transitive and intransitive verbs subcategorize for a
preceding subject, and the transitive verb addition-
ally takes an object to its right. In this manner,
the argument slots of lexical categories also define
word-word dependencies between heads and their
arguments (Clark and Hockenmaier, 2002; Hocken-
maier and Steedman, 2007). Modifiers are gener-
ally of the form X|X: in English, pre-nominal adjec-
tives are N/N, whereas adverbs may be (N/N)/(N/N),
S/S, or S\S, and prepositions can have categories
such as (N\N)/N or (S\S)/N. That is, CCG assumes
that the direction of the corresponding dependency
goes from the modifier to the head. This discrep-
ancy between CCG and most other analyses can eas-
ily be removed under the assumption that all cate-
gories of the form X|X are modifiers whose depen-
dencies should be reversed when comparing against
other frameworks.
Adjacent constituents can be combined according
to a small, universal set of combinatory rules. For
the purposes of this work we restrict ourselves to
function application and B1 composition:
X/Y Y ? X (>)
90
Y X\Y ? X (<)
X/Y Y|iZ ? X|iZ (B1>)
Y|iZ X\Y ? X|iZ (B1<)
Here the slash variable |i can be instantiated with
either the forward or backward slash.
These rules allow derivations (parses) such as:
The man ate quickly
DT NNS VBD RB
N/N N S\N S\S
> <B
N S\N
<
S
CCG also has unary type-raising rules of the form
X ? T/(T\X) ( >T)
X ? T\(T/X) ( <T)
We only allow nouns to be type-raised, and impose
the restriction that the argument T\N (or T/N) of the
type-raised category has to already be present in the
lexicon of the language.
This restricted set of combinatory rules provides
sufficient power for reasonable parse accuracy but
does not allow us to capture non-projective (cross-
ing) dependencies.
Coordination is handled by a ternary rule
X conj X ? X (>)
which we binarize as:
X X[conj] ? X (< &)
conj X ? X[conj] (> &)
Punctuation, when present, can be absorbed by
rules of the form
X Pct ? X (< p)
Pct X ? X (> p)
The iterative combination of these categories re-
sulting in S or N is considered a successful parse. In
order to avoid spurious ambiguities, we restrict our
derivations to be normal-form (Hockenmaier and
Bisk, 2010).
3 An algorithm for unsupervised CCG
induction
We now describe our induction algorithm, which
consists of two stages: category induction (creation
of the grammar), followed by parameter estimation
for the probability model.
3.1 Category induction
We assume there are two atomic categories, N (nouns
or noun phrases) and S (sentences), a special con-
junction category conj, and a special start symbol
TOP. We assume that all strings we encounter are
either nouns or sentences:
N? TOP S? TOP
We also assume that we can group POS-tags into
four groups: nominal tags, verbal tags, conjunctions,
and others. This allows us to create an initial lexicon
L(0), which only contains entries for atomic cate-
gories, e.g. for the English Penn Treebank tag set
(Marcus et al, 1993):
N : {NN,NNS,NNP,PRP,DT}
S : {MD,VB,VBZ,VBG,VBN,VBD}
conj : {CC}
We force any string that contains one or more verbs
(besides VBG in English), to be parsed with the S?
TOP rule.
Since the initial lexicon would only allow us
to parse single word utterances (or coordinations
thereof), we need to induce complex functor cat-
egories. The lexicon entries for atomic categories
remain, but all POS-tags, including nouns and con-
junctions, will be able to acquire complex categories
during induction. We impose the following con-
straints on the lexical categories we induce:
1. Nouns (N) do not take any arguments.
2. The heads of sentences (S|...) and modifiers
(X|X, (X|X)|(X|X)) may take N or S as arguments.
3. Sentences (S) may only take nouns (N) as argu-
ments.
(We assume S\S and S/S are modifiers).
4. Modifiers (X/X or X\X) can be modified
by categories of the form (X/X)|(X/X) or
(X\X)|(X\X).
5. The maximal arity of any lexical category is 3.
6. Since (S\N)/N is completely equivalent to
(S/N)\N, we only allow the former category.
Induction is an iterative process. At each stage,
we aim to parse all sentences Si in our training cor-
pus D = {S1, ...., SD} with the current lexicon
91
L(t). In order to parse a sentence S = w0...wn, all
words wi ? S need to have lexical categories that al-
low a complete parse (resulting in a constituent TOP
that spans the entire sentence). Initially, only some
words will have lexical categories:
The man ate quickly
DT NNS VBD RB
- N S -
We assume that any word may modify adjacent con-
stituents:
The man ate quickly
DT NNS VBD RB
N/N N, S/S S, N\N S\S
We also assume that any word that previously had
a category other than N (which we postulate does
not take any arguments) can take any adjacent non-
modifier category as argument, leading us here to
introduce S\N for the verb:
The man ate quickly
DT NNS VBD RB
N/N N, S/S S, N\N, S\N S\S
With these categories, we obtain the correct parse:
The man ate quickly
DT NNS VBD RB
N/N N S\N S\S
> <B
N S\N
<
S
We then update the lexicon with all new tag-category
pairs that have been found, excluding those that did
not lead to a successful parse:
N/N : {DT} S\N : {VBD,VBZ} S\S : {RB,NNS,IN}
The first stage of induction can only introduce func-
tors of arity 1, but many words, such as prepositions
or transitive verbs, require more complex categories,
leading us to complete, but incorrect parses such as
The man eats with friends
DT NNS VBZ IN NNS
N/N N S\N S\S S\S
> <B
N S\N
<B
S\N
<
S
During the second iteration, we can discover addi-
tional simple, as well as more complex, categories.
We now discover transitive verb categories:
The man ate chips
DT NNS VBD NNS
N/N N (S\N)/N N
> >
N S\N
<
S
The second stage also introduces a large number
of complex modifiers of the form (X/X)|(X/X) or
(X\X)|(X\X), e.g.:
The man ate very quickly
DT NNS VBD RB RB
N/N, N, S/S S, N\N, S\S, S\S,
(S/S)/(S/S) (N\N)/(N\N) S\N (S\S)/(S\S) (S\S)\(S\S)
(N/N)\(N/N) (S/S)\(S/S) (N\N)\(N\N)
(S\S)/(S\S)
The final induction step takes adjacent constituents
that can be derived from the existing lexicon into
account. This allows us to induce (S\S)/N for IN,
since we can combine a and friend to N.
3.2 Parameter estimation
After constructing the lexicon, we parse the training
corpus, and use the Inside-Outside algorithm (Lari
and Young, 1991), a variant of the Expectation-
Maximization algorithm for probabilistic context-
free grammars, to estimate model parameters. We
use the baseline model of Hockenmaier and Steed-
man (2002), which is a simple generative model that
is equivalent to an unlexicalized PCFG. In a CFG,
the set of terminals and non-terminals is disjoint, but
in CCG, not every category can be lexical. Since
this model is also the basis of a lexicalized model
that captures dependencies, it distinguishes between
lexical expansions (which produce words), unary ex-
pansions (which are the result of type-raising or the
TOP rules), binary expansions where the head is the
left child, and binary expansions whose head is the
right child. Each tree is generated top-down from the
start category TOP. For each (parent) node, first its
expansion type exp ? {Lex,Unary,Left,Right} is
generated. Based on the expansion type, the model
then produces either the word w, or the category of
the head child (H), and, possibly the category of the
non-head sister category (S):
92
Lexical pe(exp=Lex | P)? pw(w | P, exp=Lex)
Unary pe(exp=Unary | P)? pH(H | P, exp=Unary)
Left pe(exp=Left | P)? pH(H | P, exp=Left)
? pS(S | P,H, exp=Left)
Right pe(exp=Right | P)? pH(H | P, exp=Right)
? pS(S | P,H, exp=Right)
3.3 Dependency generation
We use the following regime for generating depen-
dencies from the resulting CCG derivations:
1. Arguments Y are dependents of their heads X|Y
2. Modifiers X|X are dependents of their heads X
or X|Y.
3. The head of the entire string is a dependent of
the root node (0)
4. Following the CoNLL-07 shared task represen-
tation (Johansson and Nugues, 2007), we ana-
lyze coordinations (X1 conj X2) as creating a
dependency from the first conjunct, X1, to the
conjunction conj, and from conj to the sec-
ond conjunct X2.
In the case of parse failures we return a right-
branching dependency tree.
3.4 Training details
The data provided includes fine, coarse and univer-
sal part-of-speech tags. Additionally, the data was
split into train, test and development sets though the
organizers encouraged merging the data for train-
ing. Finally, while punctuation was present, it was
not evaluated but potentially provided an additional
source of signal during training and test. We chose
from among these options and maximum sentence
length based on performance on the development
set. We primarily focused on training with shorter
sentences but grew the dataset if necessary or if, as
is the case in Arabic, there was very little short sen-
tence data. Our final training settings were:
Language Tags Max Len Punc
Arabic Fine 40 X
Basque Coarse 20
Childes Fine 20 X
Czech Fine 10
Danish Fine 20 X
Dutch Fine 10 X
Slovene Fine 10 X
Swedish Fine 15
PTB Fine 10
Portuguese Fine 10
In the case of Czech, we only trained on the test-
set because the data set was so large and the results
from randomly downsampling the merged dataset
were equivalent to simply using the previously de-
fined test-set.
3.5 Future directions
Since our current system is so simple, there is ample
space for future work. We plan to investigate the
effect of more complex statistical models and priors
that have been shown to be helpful in dependency
grammar-based systems. We also wish to relax the
assumption that we know in advance which part-of-
speech tags are nouns, verbs, or conjunctions.
4 Final observations regarding evaluation
Although the analysis of constructions involving ba-
sic head-argument and head-modifier dependencies
is generally uncontroversial, many common con-
structions allow a number of plausible analyses.
This makes it very difficult to evaluate and compare
different unsupervised approaches for grammar in-
duction. The corpora used in this workshop also
assume different conventions for a number of con-
structions. Figure 1 shows the three different types
of analysis for coordination adopted by the corpora
used in this shared task (as well as the standard
CCG analysis). The numbers to the side indicate
for each corpus what percentage of our system?s er-
ror rate is due to missed dependencies within coor-
dinated structures (i.e between a conjunction and a
conjunct, or between two conjuncts). It is important
to note that the way in which we extract dependen-
cies from coordinations is somewhat arbitrary (and
completely independent of the underlying probabil-
ity model, which currently captures no explicit de-
93
WILS-12
Ar 25.5%
Eu 22.6%
??? ????? ??? ???????
Childes 7.7%
Cz 21.4%
Da 13.1%
Nl 15.3%
PTB 18.1%
??? ????? ??? ???????
WILS-12
Sl 17.2%
Sv 11.1%
??? ????? ??? ???????
WILS-12 & CoNLL-07
Pt 7.8%
??? ????? ??? ???????
Standard CCG
Figure 1: Different analyses of coordination in the
various corpora used in this shared task. Our sys-
tem adopts the CoNLL-07 convention, instead of the
standard CCG analysis. For the development set of
each corpus, we also indicate what percentage of the
errors our system makes is due to missed coordina-
tion dependencies.
pendencies). These systematic differences of anal-
ysis are also reflected in our final results. The only
exception is the Childes corpus, where coordination
is significantly rarer.
However, this is a general problem. There are
many other constructions for which no agreed-upon
standard exists. For example, the Wall Street Journal
data used in this shared task assumes a dependency
between the verb of the main clause and the verb of
a subordinate clause, whereas the CoNLL-07 anal-
ysis stipulates a dependency between the main verb
and the subordinating conjunction:
??????????????????????????????????????
??????????????????????????????????????
(a) CoNLL-07
??????????????????????????????????????
??????????????????????????????????????
(b) WILS-12
We therefore believe that much further work is
required to address the problems surrounding eval-
uation and comparison of unsupervised induction
systems adequately. Even if the community can-
not agree on a single gold standard, systems should
not be penalized for producing one kind of linguisti-
cally plausible analysis over another. The systematic
divergences that arise with coordination for our ap-
proach are relatively easy to fix, since we only need
to change the way in which we read off dependen-
cies. But this points to a deeper underlying problem
that affects the entire field.
Acknowledgements
This research is supported by the National Science
Foundation through CAREER award 1053856 and
award 0803603.
References
Yonatan Bisk and Julia Hockenmaier. 2012. Simple Ro-
bust Grammar Induction with Combinatory Categorial
Grammars. In Association for the Advancement of Ar-
tificial Intelligence.
Prachya Boonkwan and Mark Steedman. 2011. Gram-
mar Induction from Text Using Small Syntactic Pro-
totypes. In International Joint Conference on Natural
Language Processing, pages 438?446, November.
Stephen Clark and Julia Hockenmaier. 2002. Evaluating
a wide-coverage CCG parser. In Proceedings of the
LREC Beyond PARSEVAL workshop, page 2002, Las
Palmas, Spain.
S. B. Cohen and N. A. Smith. 2010. Covariance in unsu-
pervised learning of probabilistic grammars. Journal
of Machine Learning Research, 11:3017?3051.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2011. Inducing tree-substitution grammars. Jour-
nal of Machine Learning Research, pages 3053?3096,
November.
Julia Hockenmaier and Yonatan Bisk. 2010. Normal-
form parsing for Combinatory Categorial Grammars
with generalized composition and type-raising. In
COLING.
Julia Hockenmaier and Mark Steedman. 2002. Gen-
erative models for statistical parsing with Combina-
tory Categorial Grammar. In Association for Compu-
tational Linguistics, pages 335?342.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Compu-
tational Linguistics, pages 355?396, January.
94
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
Proceedings of NODALIDA 2007, Tartu, Estonia.
K Lari and S Young. 1991. Applications of stochastic
context-free grammars using the inside-outside algo-
rithm. Computer speech & language, 5(3):237?257,
January.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowledge
to guide grammar induction. In Empirical Methods in
Natural Language Processing, pages 1234?1244, Oc-
tober.
Mark Steedman. 2000. The syntactic process. MIT
Press, January.
95
