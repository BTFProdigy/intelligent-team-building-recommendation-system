Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 185?192,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning More Effective Dialogue Strategies Using Limited Dialogue
Move Features
Matthew Frampton and Oliver Lemon
HCRC, School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, UK
M.J.E.Frampton@sms.ed.ac.uk, olemon@inf.ed.ac.uk
Abstract
We explore the use of restricted dialogue
contexts in reinforcement learning (RL)
of effective dialogue strategies for infor-
mation seeking spoken dialogue systems
(e.g. COMMUNICATOR (Walker et al,
2001)). The contexts we use are richer
than previous research in this area, e.g.
(Levin and Pieraccini, 1997; Scheffler and
Young, 2001; Singh et al, 2002; Pietquin,
2004), which use only slot-based infor-
mation, but are much less complex than
the full dialogue ?Information States? ex-
plored in (Henderson et al, 2005), for
which tractabe learning is an issue. We
explore how incrementally adding richer
features allows learning of more effective
dialogue strategies. We use 2 user simu-
lations learned from COMMUNICATOR
data (Walker et al, 2001; Georgila et al,
2005b) to explore the effects of differ-
ent features on learned dialogue strategies.
Our results show that adding the dialogue
moves of the last system and user turns
increases the average reward of the auto-
matically learned strategies by 65.9% over
the original (hand-coded) COMMUNI-
CATOR systems, and by 7.8% over a base-
line RL policy that uses only slot-status
features. We show that the learned strate-
gies exhibit an emergent ?focus switch-
ing? strategy and effective use of the ?give
help? action.
1 Introduction
Reinforcement Learning (RL) applied to the prob-
lem of dialogue management attempts to find op-
timal mappings from dialogue contexts to sys-
tem actions. The idea of using Markov Deci-
sion Processes (MDPs) and reinforcement learn-
ing to design dialogue strategies for dialogue sys-
tems was first proposed by (Levin and Pierac-
cini, 1997). There, and in subsequent work such
as (Singh et al, 2002; Pietquin, 2004; Scheffler
and Young, 2001), only very limited state infor-
mation was used in strategy learning, based al-
ways on the number and status of filled informa-
tion slots in the application (e.g. departure-city is
filled, destination-city is unfilled). This we refer to
as low-level contextual information. Much prior
work (Singh et al, 2002) concentrated only on
specific strategy decisions (e.g. confirmation and
initiative strategies), rather than the full problem
of what system dialogue move to take next.
The simple strategies learned for low-level def-
initions of state cannot be sensitive to (sometimes
critical) aspects of the dialogue context, such as
the user?s last dialogue move (DM) (e.g. request-
help) unless that move directly affects the status of
an information slot (e.g. provide-info(destination-
city)). We refer to additional contextual infor-
mation such as the system and user?s last di-
alogue moves as high-level contextual informa-
tion. (Frampton and Lemon, 2005) learned full
strategies with limited ?high-level? information
(i.e. the dialogue move(s) of the last user utter-
ance) and only used a stochastic user simulation
whose probabilities were supplied via common-
sense and intuition, rather than learned from data.
This paper uses data-driven n-gram user simula-
tions (Georgila et al, 2005a) and a richer dialogue
context.
On the other hand, increasing the size of the
state space for RL has the danger of making
the learning problem intractable, and at the very
least means that data is more sparse and state ap-
proximation methods may need to be used (Hen-
derson et al, 2005). To date, the use of very
large state spaces relies on a ?hybrid? super-
vised/reinforcement learning technique, where the
reinforcement learning element has not yet been
shown to significantly improve policies over the
purely supervised case (Henderson et al, 2005).
185
The extended state spaces that we propose are
based on theories of dialogue such as (Clark, 1996;
Searle, 1969; Austin, 1962; Larsson and Traum,
2000), where which actions a dialogue participant
can or should take next are not based solely on
the task-state (i.e. in our domain, which slots are
filled), but also on wider contextual factors such
as a user?s dialogue moves or speech acts. In
future work we also intend to use feature selec-
tion techniques (e.g. correlation-based feature sub-
set (CFS) evaluation (Rieser and Lemon, 2006))
on the COMMUNICATOR data (Georgila et al,
2005a; Walker et al, 2001) in order to identify ad-
ditional context features that it may be effective to
represent in the state.
1.1 Methodology
To explore these issues we have developed a Re-
inforcement Learning (RL) program to learn di-
alogue strategies while accurate simulated users
(Georgila et al, 2005a) converse with a dialogue
manager. See (Singh et al, 2002; Scheffler and
Young, 2001) and (Sutton and Barto, 1998) for a
detailed description of Markov Decision Processes
and the relevant RL algorithms.
In dialogue management we are faced with the
problem of deciding which dialogue actions it is
best to perform in different states. We use (RL) be-
cause it is a method of learning by delayed reward
using trial-and-error search. These two proper-
ties appear to make RL techniques a good fit with
the problem of automatically optimising dialogue
strategies, because in task-oriented dialogue of-
ten the ?reward? of the dialogue (e.g. successfully
booking a flight) is not obtainable immediately,
and the large space of possible dialogues for any
task makes some degree of trial-and-error explo-
ration necessary.
We use both 4-gram and 5-gram user sim-
ulations for testing and for training (i.e. train
with 4-gram, test with 5-gram, and vice-versa).
These simulations also simulate ASR errors since
the probabilities are learned from recognition hy-
potheses and system behaviour logged in the
COMMUNICATOR data (Walker et al, 2001) fur-
ther annotated with speech acts and contexts by
(Georgila et al, 2005b). Here the task domain is
flight-booking, and the aim for the dialogue man-
ager is to obtain values for the user?s flight infor-
mation ?slots? i.e. departure city, destination city,
departure date and departure time, before making
a database query. We add the dialogue moves of
the last user and system turns as context features
and use these in strategy learning. We compare
the learned strategies to 2 baselines: the original
COMMUNICATOR systems and an RL strategy
which uses only slot status features.
1.2 Outline
Section 2 contains a description of our basic ex-
perimental framework, and a detailed description
of the reinforcement learning component and user
simulations. Sections 3 and 4 describe the experi-
ments and analyse our results, and in section 5 we
conclude and suggest future work.
2 The Experimental Framework
Each experiment is executed using the DIPPER
Information State Update dialogue manager (Bos
et al, 2003) (which here is used to track and up-
date dialogue context rather than deciding which
actions to take), a Reinforcement Learning pro-
gram (which determines the next dialogue action
to take), and various user simulations. In sections
2.3 and 2.4 we give more details about the rein-
forcement learner and user simulations.
2.1 The action set for the learner
Below is a list of all the different actions that the
RL dialogue manager can take and must learn to
choose between based on the context:
1. An open question e.g. ?How may I help you??
2. Ask the value for any one of slots 1...n.
3. Explicitly confirm any one of slots 1...n.
4. Ask for the nth slot whilst implicitly confirm-
ing1 either slot value n? 1 e.g. ?So you want
to fly from London to where??, or slot value
n + 1
5. Give help.
6. Pass to human operator.
7. Database Query.
There are a couple of restrictions regarding
which actions can be taken in which states: an
open question is only available at the start of the
dialogue, and the dialogue manager can only try
to confirm non-empty slots.
2.2 The Reward Function
We employ an ?all-or-nothing? reward function
which is as follows:
1. Database query, all slots confirmed: +100
2. Any other database query: ?75
1Where n = 1 we implicitly confirm the final slot andwhere n = 4 we implicitly confirm the first slot. This action
set does not include actions that ask the nth slot whilst im-plicitly confirming slot value n ? 2. These will be added infuture experiments as we continue to increase the action andstate space.
186
3. User simulation hangs-up: ?100
4. DIPPER passes to a human operator: ?50
5. Each system turn: ?5
To maximise the chances of a slot value be-
ing correct, it must be confirmed rather than just
filled. The reward function reflects the fact that
a successful dialogue manager must maximise its
chances of getting the slots correct i.e. they must
all be confirmed. (Walker et al, 2000) showed
with the PARADISE evaluation that confirming
slots increases user satisfaction.
The maximum reward that can be obtained for
a single dialogue is 85, (the dialogue manager
prompts the user, the user replies by filling all four
of the slots in a single utterance, and the dialogue
manager then confirms all four slots and submits a
database query).
2.3 The Reinforcement Learner?s Parameters
When the reinforcement learner agent is initial-
ized, it is given a parameter string which includes
the following:
1. Step Parameter: ? = decreasing
2. Discount Factor: ? = 1
3. Action Selection Type = softmax (alternative
is ?-greedy)
4. Action Selection Parameter: temperature =
15
5. Eligibility Trace Parameter: ? = 0.9
6. Eligibility Trace = replacing (alternative is
accumulating)
7. Initial Q-values = 25
The reinforcement learner updates its Q-values
using the Sarsa(?) algorithm (see (Sutton and
Barto, 1998)). The first parameter is the step-
parameter ? which may be a value between 0 and
1, or specified as decreasing. If it is decreasing, as
it is in our experiments, then for any given Q-value
update ? is 1k where k is the number of times thatthe state-action pair for which the update is be-
ing performed has been visited. This kind of step
parameter will ensure that given a sufficient num-
ber of training dialogues, each of the Q-values will
eventually converge. The second parameter (dis-
count factor) ? may take a value between 0 and 1.
For the dialogue management problem we set it to
1 so that future rewards are taken into account as
strongly as possible.
Apart from updating Q-values, the reinforce-
ment learner must also choose the next action
for the dialogue manager and the third parameter
specifies whether it does this by ?-greedy or soft-
max action selection (here we have used softmax).
The fifth parameter, the eligibility trace param-
eter ?, may take a value between 0 and 1, and the
sixth parameter specifies whether the eligibility
traces are replacing or accumulating. We used re-
placing traces because they produced faster learn-
ing for the slot-filling task. The seventh parameter
is for supplying the initial Q-values.
2.4 N-Gram User Simulations
Here user simulations, rather than real users, inter-
act with the dialogue system during learning. This
is because thousands of dialogues may be neces-
sary to train even a simple system (here we train
on up to 50000 dialogues), and for a proper explo-
ration of the state-action space the system should
sometimes take actions that are not optimal for the
current situation, making it a sadistic and time-
consuming procedure for any human training the
system. (Eckert et al, 1997) were the first to
use a user simulation for this purpose, but it was
not goal-directed and so could produce inconsis-
tent utterances. The later simulations of (Pietquin,
2004) and (Scheffler and Young, 2001) were to
some extent ?goal-directed? and also incorporated
an ASR error simulation. The user simulations in-
teract with the system via intentions. Intentions
are preferred because they are easier to generate
than word sequences and because they allow er-
ror modelling of all parts of the system, for exam-
ple ASR error modelling and semantic errors. The
user and ASR simulations must be realistic if the
learned strategy is to be directly applicable in a
real system.
The n-gram user simulations used here (see
(Georgila et al, 2005a) for details and evaluation
results) treat a dialogue as a sequence of pairs of
speech acts and tasks. They take as input the n?1
most recent speech act-task pairs in the dialogue
history, and based on n-gram probabilities learned
from the COMMUNICATOR data (automatically
annotated with speech acts and Information States
(Georgila et al, 2005b)), they then output a user
utterance as a further speech-act task pair. These
user simulations incorporate the effects of ASR er-
rors since they are built from the user utterances
as they were recognized by the ASR components
of the original COMMUNICATOR systems. Note
that the user simulations do not provide instanti-
ated slot values e.g. a response to provide a des-
tination city is the speech-act task pair ?[provide
info] [dest city]?. We cannot assume that two such
responses in the same dialogue refer to the same
187
destination cities. Hence in the dialogue man-
ager?s Information State where we record whether
a slot is empty, filled, or confirmed, we only up-
date from filled to confirmed when the slot value
is implicitly or explicitly confirmed. An additional
function maps the user speech-act task pairs to a
form that can be interpreted by the dialogue man-
ager. Post-mapping user responses are made up of
one or more of the following types of utterance:
(1) Stay quiet, (2) Provide 1 or more slot values,
(3) Yes, (4) No, (5) Ask for help, (6) Hang-up, (7)
Null (out-of-domain or no ASR hypothesis).
The quality of the 4 and 5-gram user sim-
ulations has been established through a variety
of metrics and against the behaviour of the ac-
tual users of the COMMUNICATOR systems, see
(Georgila et al, 2005a).
2.4.1 Limitations of the user simulations
The user and ASR simulations are a fundamen-
tally important factor in determining the nature of
the learned strategies. For this reason we should
note the limitations of the n-gram simulations used
here. A first limitation is that we cannot be sure
that the COMMUNICATOR training data is suffi-
ciently complete, and a second is that the n-gram
simulations only use a window of n moves in
the dialogue history. This second limitation be-
comes a problem when the user simulation?s cur-
rent move ought to take into account something
that occurred at an earlier stage in the dialogue.
This might result in the user simulation repeating a
slot value unnecessarily, or the chance of an ASR
error for a particular word being independent of
whether the same word was previously recognised
correctly. The latter case means we cannot sim-
ulate for example, a particular slot value always
being liable to misrecognition. These limitations
will affect the nature of the learned strategies. Dif-
ferent state features may assume more or less im-
portance than they would if the simulations were
more realistic. This is a point that we will return to
in the analysis of the experimental results. In fu-
ture work we will use the more accurate user sim-
ulations recently developed following (Georgila et
al., 2005a) and we expect that these will improve
our results still further.
3 Experiments
First we learned strategies with the 4-gram user
simulation and tested with the 5-gram simula-
tion, and then did the reverse. We experimented
with different feature sets, exploring whether bet-
ter strategies could be learned by adding limited
context features. We used two baselines for com-
parison:
? The performance of the original COMMUNI-
CATOR systems in the data set (Walker et al,
2001).
? An RL baseline dialogue manager learned
using only slot-status features i.e. for each
of slots 1? 4, is the slot empty, filled or con-
firmed?
We then learned two further strategies:
? Strategy 2 (UDM) was learned by adding the
user?s last dialogue move to the state.
? Strategy 3 (USDM) was learned by adding
both the user and system?s last dialogue
moves to the state.
The possible system and user dialogue moves
were those given in sections 2.1 and 2.4 respec-
tively, and the reward function was that described
in section 2.2.
3.1 The COMMUNICATOR data baseline
We computed the scores for the original hand-
coded COMMUNICATOR systems as was done
by (Henderson et al, 2005), and we call this the
?HLG05? score. This scoring function is based
on task completion and dialogue length rewards as
determined by the PARADISE evaluation (Walker
et al, 2000). This function gives 25 points for
each slot which is filled, another 25 for each that
is confirmed, and deducts 1 point for each sys-
tem action. In this case the maximum possible
score is 197 i.e. 200 minus 3 actions, (the sys-
tem prompts the user, the user replies by filling all
four of the slots in one turn, and the system then
confirms all four slots and offers the flight). The
average score for the 1242 dialogues in the COM-
MUNICATOR dataset where the aim was to fill
and confirm only the same four slots as we have
used here was 115.26. The other COMMUNICA-
TOR dialogues involved different slots relating to
return flights, hotel-bookings and car-rentals.
4 Results
Figure 1 tracks the improvement of the 3 learned
strategies for 50000 training dialogues with the 4-
gram user simulation, and figure 2 for 50000 train-
ing dialogues with the 5-gram simulation. They
show the average reward (according to the func-
tion of section 2.2) obtained by each strategy over
intervals of 1000 training dialogues.
Table 1 shows the results for testing the strate-
gies learned after 50000 training dialogues (the
baseline RL strategy, strategy 2 (UDM) and strat-
egy 3 (USDM)). The ?a? strategies were trained
with the 4-gram user simulation and tested with
188
Features Av. Score HLG05 Filled Slots Conf. Slots Length
4 ? 5 gram = (a)
RL Baseline (a) Slots status 51.67 190.32 100 100 ?9.68
RL Strat 2, UDM (a) + Last User DM 53.65** 190.67 100 100 ?9.33
RL Strat 3, USDM (a) + Last System DM 54.9** 190.98 100 100 ?9.02
5 ? 4 gram = (b)
RL Baseline (b) Slots status 51.4 190.28 100 100 ?9.72
RL Strat 2, UDM (b) + Last User DM 54.46* 190.83 100 100 ?9.17
RL Strat 3, USDM (b) + Last System DM 56.24** 191.25 100 100 ?8.75
RL Baseline (av) Slots status 51.54 190.3 100 100 ?9.7
RL Strat 2, UDM (av) + Last User DM 54.06** 190.75 100 100 ?9.25
RL Strat 3, USDM (av) + Last System DM 55.57** 191.16 100 100 ?8.84
COMM Systems 115.26 84.6 63.7 ?33.1
Hybrid RL *** Information States 142.6 88.1 70.9 ?16.4
Table 1: Testing the learned strategies after 50000 training dialogues, average reward achieved per dia-
logue over 1000 test dialogues. (a) = strategy trained using 4-gram and tested with 5-gram; (b) = strategy
trained with 5-gram and tested with 4-gram; (av) = average; * significance level p < 0.025; ** signifi-
cance level p < 0.005; *** Note: The Hybrid RL scores (here updated from (Henderson et al, 2005))
are not directly comparable since that system has a larger action set and fewer policy constraints.
the 5-gram, while the ?b? strategies were trained
with the 5-gram user simulation and tested with
the 4-gram. The table also shows average scores
for the strategies. Column 2 contains the average
reward obtained per dialogue by each strategy over
1000 test dialogues (computed using the function
of section 2.2).
The 1000 test dialogues for each strategy were
divided into 10 sets of 100. We carried out t-tests
and found that in both the ?a? and ?b? cases, strat-
egy 2 (UDM) performs significantly better than
the RL baseline (significance levels p < 0.005
and p < 0.025), and strategy 3 (USDM) performs
significantly better than strategy 2 (UDM) (signif-
icance level p < 0.005). With respect to average
performance, strategy 2 (UDM) improves over the
RL baseline by 4.9%, and strategy 3 (USDM) im-
proves by 7.8%. Although there seem to be only
negligible qualitative differences between strate-
gies 2(b) and 3(b) and their ?a? equivalents, the
former perform slightly better in testing. This sug-
gests that the 4-gram simulation used for testing
the ?b? strategies is a little more reliable in filling
and confirming slot values than the 5-gram.
The 3rd column ?HLG05? shows the average
scores for the dialogues as computed by the re-
ward function of (Henderson et al, 2005). This is
done for comparison with that work but also with
the COMMUNICATOR data baseline. Using the
HLG05 reward function, strategy 3 (USDM) im-
proves over the original COMMUNICATOR sys-
tems baseline by 65.9%. The components making
up the reward are shown in the final 3 columns
of table 1. Here we see that all of the RL strate-
gies are able to fill and confirm all of the 4 slots
when conversing with the simulated COMMUNI-
CATOR users. The only variation is in the aver-
age length of dialogue required to confirm all four
slots. The COMMUNICATOR systems were of-
ten unable to confirm or fill all of the user slots,
and the dialogues were quite long on average. As
stated in section 2.4.1, the n-gram simulations do
not simulate the case of a particular user goal ut-
terance being unrecognisable for the system. This
was a problem that could be encountered by the
real COMMUNICATOR systems.
Nevertheless, the performance of all the learned
strategies compares very well to the COMMUNI-
CATOR data baseline. For example, in an average
dialogue, the RL strategies filled and confirmed all
four slots with around 9 actions not including of-
fering the flight, but the COMMUNICATOR sys-
tems took an average of around 33 actions per di-
alogue, and often failed to complete the task.
With respect to the hybrid RL result of (Hen-
derson et al, 2005), shown in the final row of the
table, Strategy 3 (USDM) shows a 34% improve-
ment, though these results are not directly compa-
rable because that system uses a larger action set
and has fewer constraints (e.g. it can ask ?how may
I help you?? at any time, not just at the start of a
dialogue).
Finally, let us note that the performance of the
RL strategies is close to optimal, but that there is
some room for improvement. With respect to the
HLG05 metric, the optimal system score would be
197, but this would only be available in rare cases
where the simulated user supplies all 4 slots in the
189
-120
-100
-80
-60
-40
-20
 0
 20
 40
 0  5  10 15 20 25 30 35 40 45 50
Av
er
ag
e 
Re
wa
rd
Number of Dialogues (Thousands)
Training With 4-gram
Baseline
Strategy 2
Strategy 3
Figure 1: Training the dialogue strategies with the
4-gram user simulation
first utterance. With respect to the metric we have
used here (with a ?5 per system turn penalty), the
optimal score is 85 (and we currently score an av-
erage of 55.57). Thus we expect that there are
still further improvments that can be made to more
fully exploit the dialogue context (see section 4.3).
4.1 Qualitative Analysis
Below are a list of general characteristics of the
learned strategies:
1. The reinforcement learner learns to query the
database only in states where all four slots
have been confirmed.
2. With sufficient exploration, the reinforce-
ment learner learns not to pass the call to a
human operator in any state.
3. The learned strategies employ implicit confir-
mations wherever possible. This allows them
to fill and confirm the slots in fewer turns than
if they simply asked the slot values and then
used explicit confirmation.
4. As a result of characteristic 3, which slots
can be asked and implicitly confirmed at the
same time influences the order in which the
learned strategies attempt to fill and confirm
each slot, e.g. if the status of the third slot is
?filled? and the others are ?empty?, the learner
learns to ask for the second or fourth slot
-120
-100
-80
-60
-40
-20
 0
 20
 40
 0  5  10 15 20 25 30 35 40 45 50
Av
er
ag
e 
Re
wa
rd
Number of Dialogues (Thousands)
Training With 5-gram
Baseline
Strategy 2
Strategy 3
Figure 2: Training the dialogue strategies with the
5-gram user simulation
rather than the first, since it can implicitly
confirm the third while it asks for the second
or fourth slots, but it cannot implicitly con-
firm the third while it asks for the first slot.
This action is not available (see section 2.1).
4.2 Emergent behaviour
In testing the UDM strategy (2) filled and con-
firmed all of the slots in fewer turns on aver-
age than the RL baseline, and strategy 3 (USDM)
did this in fewer turns than strategy 2 (UDM).
What then were the qualitative differences be-
tween the three strategies? The behaviour of the
three strategies only seems to really deviate when
a user response fails to fill or confirm one or more
slots. Then the baseline strategy?s state has not
changed and so it will repeat its last dialogue
move, whereas the state for strategies 2 (UDM)
and 3 (USDM) has changed and as a result, these
may now try different actions. It is in such circum-
stances that the UDM strategy seems to be more
effective than the baseline, and strategy 3 (USDM)
more effective than the UDM strategy. In figure 3
we show illustrative state and learned action pairs
for the different strategies. They relate to a sit-
uation where the first user response(s) in the di-
alogue has/have failed to fill a single slot value.
NB: here ?emp? stands for ?empty? and ?fill? for
?filled? and they appear in the first four state vari-
ables, which stand for slot states. For strategy 2
(UDM), the fifth variable represents the user?s last
190
dialogue move, and the for strategy 3 (USDM), the
fifth variable represents the system?s last dialogue
move, and the sixth, the user?s last dialogue move.
BASELINE STRATEGY
State:
[emp,emp,emp,emp]
Action: askSlot2
STRATEGY 2 (UDM)
State:
[emp,emp,emp,emp,user(quiet)]
Action: askSlot3
State:
[emp,emp,emp,emp,user(null)]
Action: askSlot1
STRATEGY 3 (USDM)
State:
[emp,emp,emp,emp,askSlot3,user(quiet)]
Action: askSlot3
State:
[emp,emp,emp,emp,askSlot3,user(null)]
Action: giveHelp
State:
[emp,emp,emp,emp,giveHelp,user(quiet)]
Action: askSlot3
State:
[emp,emp,emp,emp,giveHelp,user(null)]
Action: askSlot3
Figure 3: Examples of the different learned strate-
gies and emergent behaviours: focus switching
(for UDM) and giving help (for USDM)
Here we can see that should the user responses
continue to fail to provide a slot value, the base-
line?s state will be unchanged and so the strategy
will simply ask for slot 2 again. The state for strat-
egy 2 (UDM) does change however. This strategy
switches focus between slots 3 and 1 depending on
whether the user?s last dialogue move was ?null? or
?quiet? NB. As stated in section 2.4, ?null? means
out-of-domain or that there was no ASR hypothe-
sis. Strategy 3 (USDM) is different again. Knowl-
edge of the system?s last dialogue move as well
as the user?s last move has enabled the learner to
make effective use of the ?give help? action, rather
than to rely on switching focus. When the user?s
last dialogue move is ?null? in response to the sys-
tem move ?askSlot3?, then the strategy uses the
?give help? action before returning to ask for slot 3
again. The example described here is not the only
example of strategy 2 (UDM) employing focus
switching while strategy 3 (USDM) prefers to use
the ?give help? action when a user response fails
to fill or confirm a slot. This kind of behaviour in
strategies 2 and 3 is emergent dialogue behaviour
that has been learned by the system rather than ex-
plicitly programmed.
4.3 Further possibilities for improvement
over the RL baseline
Further improvements over the RL baseline might
be possible with a wider set of system actions.
Strategies 2 and 3 may learn to make more ef-
fective use of additional actions than the baseline
e.g. additional actions that implicitly confirm one
slot whilst asking another may allow more of the
switching focus described in section 4.1. Other
possible additional actions include actions that ask
for or confirm two or more slots simultaneously.
In section 2.4.1, we highlighted the fact that the
n-gram user simulations are not completely real-
istic and that this will make certain state features
more or less important in learning a strategy. Thus
had we been able to use even more realistic user
simulations, including certain additional context
features in the state might have enabled a greater
improvement over the baseline. Dialogue length
is an example of a feature that could have made
a difference had the simulations been able to sim-
ulate the case of a particular goal utterance being
unrecognisable for the system. The reinforcement
learner may then be able to use the dialogue length
feature to learn when to give up asking for a par-
ticular slot value and make a partially complete
database query. This would of course require a
reward function that gave some reward to partially
complete database queries rather than the all-or-
nothing reward function used here.
5 Conclusion and Future Work
We have used user simulations that are n-gram
models learned from COMMUNICATOR data to
explore reinforcement learning of full dialogue
strategies with some ?high-level? context infor-
mation (the user and and system?s last dialogue
moves). Almost all previous work (e.g. (Singh
et al, 2002; Pietquin, 2004; Scheffler and Young,
2001)) has included only low-level information
in state representations. In contrast, the explo-
ration of very large state spaces to date relies on a
?hybrid? supervised/reinforcement learning tech-
nique, where the reinforcement learning element
has not been shown to significantly improve poli-
cies over the purely supervised case (Henderson et
al., 2005).
We presented our experimental environment,
the reinforcement learner, the simulated users,
and our methodology. In testing with the sim-
ulated COMMUNICATOR users, the new strate-
gies learned with higher-level (i.e. dialogue move)
information in the state outperformed the low-
level RL baseline (only slot status information)
191
by 7.8% and the original COMMUNICATOR sys-
tems by 65.9%. These strategies obtained more
reward than the RL baseline by filling and con-
firming all of the slots with fewer system turns on
average. Moreover, the learned strategies show
interesting emergent dialogue behaviour such as
making effective use of the ?give help? action and
switching focus to different subtasks when the cur-
rent subtask is proving problematic.
In future work, we plan to use even more realis-
tic user simulations, for example those developed
following (Georgila et al, 2005a), which incorpo-
rate elements of goal-directed user behaviour. We
will continue to investigate whether we can main-
tain tractability and learn superior strategies as we
add incrementally more high-level contextual in-
formation to the state. At some stage this may
necessitate using a generalisation method such as
linear function approximation (Henderson et al,
2005). We also intend to use feature selection
techniques (e.g. CFS subset evaluation (Rieser and
Lemon, 2006)) on in order to determine which
contextual features this suggests are important.
We will also carry out a more direct comparison
with the hybrid strategies learned by (Henderson
et al, 2005). In the slightly longer term, we will
test our learned strategies on humans using a full
spoken dialogue system. We hypothesize that the
strategies which perform the best in terms of task
completion and user satisfaction scores (Walker et
al., 2000) will be those learned with high-level di-
alogue context information in the state.
Acknowledgements
This work is supported by the ESRC and the TALK
project, www.talk-project.org.
References
John L. Austin. 1962. How To Do Things With Words.Oxford University Press.
Johan Bos, Ewan Klein, Oliver Lemon, and TetsushiOka. 2003. Dipper: Description and formalisationof an information-state update dialogue system ar-
chitecture. In 4th SIGdial Workshop on Discourse
and Dialogue, Sapporo.
Herbert H. Clark. 1996. Using Language. CambridgeUniversity Press.
Weiland Eckert, Esther Levin, and Roberto Pieraccini.1997. User modeling for spoken dialogue system
evaluation. In IEEE Workshop on Automatic Speech
Recognition and Understanding.
Matthew Frampton and Oliver Lemon. 2005. Rein-forcement Learning Of Dialogue Strategies UsingThe User?s Last Dialogue Act. In IJCAI workshop
on Knowledge and Reasoning in Practical Dialogue
Systems.
Kallirroi Georgila, James Henderson, and OliverLemon. 2005a. Learning User Simulations for In-formation State Update Dialogue Systems. In In-
terspeech/Eurospeech: the 9th biennial conference
of the International Speech Communication Associ-
ation.
Kallirroi Georgila, Oliver Lemon, and James Hender-son. 2005b. Automatic annotation of COMMUNI-
CATOR dialogue data for learning dialogue strate-gies and user simulations. In Ninth Workshop on the
Semantics and Pragmatics of Dialogue (SEMDIAL:
DIALOR).
James Henderson, Oliver Lemon, and Kallirroi
Georgila. 2005. Hybrid Reinforcement/SupervisedLearning for Dialogue Policies from COMMUNI-CATOR data. In IJCAI workshop on Knowledge and
Reasoning in Practical Dialogue Systems,.
Staffan Larsson and David Traum. 2000. Information
state and dialogue management in the TRINDI Dia-logue Move Engine Toolkit. Natural Language En-
gineering, 6(3-4):323?340.
Esther Levin and Roberto Pieraccini. 1997. Astochastic model of computer-human interactionfor learning dialogue strategies. In Eurospeech,
Rhodes,Greece.
Olivier Pietquin. 2004. A Framework for Unsuper-
vised Learning of Dialogue Strategies. Presses Uni-versitaires de Louvain, SIMILAR Collection.
Verena Rieser and Oliver Lemon. 2006. Using ma-chine learning to explore human multimodal clarifi-cation strategies. In Proc. ACL.
Konrad Scheffler and Steve Young. 2001. Corpus-based dialogue simulation for automatic strategy
learning and evaluation. In NAACL-2001 Work-
shop on Adaptation in Dialogue Systems, Pittsburgh,USA.
John R. Searle. 1969. Speech Acts. Cambridge Uni-versity Press.
Satinder Singh, Diane Litman, Michael Kearns, andMarilyn Walker. 2002. Optimizing dialogue man-
agement with reinforcement learning: Experimentswith the NJFun system. Journal of Artificial Intelli-
gence Research (JAIR).
Richard Sutton and Andrew Barto. 1998. Reinforce-
ment Learning. MIT Press.
Marilyn A. Walker, Candace A. Kamm, and Diane J.Litman. 2000. Towards Developing General Mod-els of Usability with PARADISE. Natural Lan-
guage Engineering, 6(3).
Marilyn A. Walker, Rebecca J. Passonneau, and
Julie E. Boland. 2001. Quantitative and Qualita-tive Evaluation of Darpa Communicator Spoken Di-alogue Systems. In Meeting of the Association for
Computational Linguistics, pages 515?522.
192
Automatic Generation of Translation Dictionaries Using Intermediary
Languages
Kisuh Ahn and Matthew Frampton
ICCS, School of Informatics
Edinburgh University
K.Ahn@sms.ed.ac.uk,M.J.E.Frampton@sms.ed.ac.uk
Abstract
We describe a method which uses one or more
intermediary languages in order to automati-
cally generate translation dictionaries. Such
a method could potentially be used to effi-
ciently create translation dictionaries for lan-
guage groups which have as yet had little in-
teraction. For any given word in the source
language, our method involves first translating
into the intermediary language(s), then into
the target language, back into the intermediary
language(s) and finally back into the source
language. The relationship between a word
and the number of possible translations in an-
other language is most often 1-to-many, and so
at each stage, the number of possible transla-
tions grows exponentially. If we arrive back at
the same starting point i.e. the same word in
the source language, then we hypothesise that
the meanings of the words in the chain have
not diverged significantly. Hence we back-
track through the link structure to the target
language word and accept this as a suitable
translation. We have tested our method by us-
ing English as an intermediary language to au-
tomatically generate a Spanish-to-Germandic-
tionary, and the results are encouraging.
1 Introduction
In this paper we describe a method which uses one or
more intermediary languages to automatically generate
a dictionary to translate from one language,
 
, to an-
other,  . The method relies on using dictionaries that
can connect
 
to  and back to
 
via the intermediary
language(s), e.g.
    
,
  
 , 
  
,
  
 
, where
 
is an intermediary language such as En-
glish. The resources required to exploit the method
are not difficult to find since dictionaries already ex-
ist that translate between English and a vast number of
other languages. Whereas at present the production of
translation dictionaries is manual (e.g. (Serasset1994)),
our method is automatic. We believe that projects such
as (Boitet et al2002) and (Wiktionary), which are cur-
rently generating translation dictionaries by hand could
benefit greatly from using our method. Translation dic-
tionaries are useful not only for end-user consumption
but also for various multilingual tasks such as cross-
language question answering (e.g. (Ahn et al2004))
and information retrieval (e.g. (Argaw et al2004)). We
have applied our method to automatically generate a
Spanish-to-German dictionary. We chose this language
pair because we were able to find an online Spanish-to-
German dictionary which could be used to evaluate our
result.
The structure of the paper is as follows. In sec-
tion 2.1, we describe how if we translate a word from
a source language into an intermediary language, and
then into a target language, the number of possible
translations may grow drastically. Some of these trans-
lations will be ?better? than others, and in section 2.2
we give a detailed description of our method for iden-
tifying these ?better? translations. Having identified the
?better? translations we can then automatically generate
a dictionary that translates directly from the source to
the target language. In section 3 we describe how we
used our method to automatically generate a Spanish-
to-German dictionary, and in section 3.3, we evaluate
the result. Finally, in section 4, we conclude and sug-
gest future work.
2 Translating Via An Intermediary
Language
2.1 The Problem
Consider the problem of finding the different possible
translations for a word  from language
 
in language
 when there is no available
  
 dictionary. Let
us assume that there are dictionaries which allow us
to connect from
 
to  and back to
 
via an inter-
mediary language
 
i.e. dictionaries for
    
,
  
 , 
  
and
    
, as shown in figure 1.
If there was only ever  suitable translation for any
given word in another language, then it would be triv-
ial to use dictionaries
    
and
  
 in order
to obtain a translation of  in language  . However,
this is not the case - for any given word  in language
 
the
    
dictionary will usually give multiple
possible translations    fi ffi ffi ffi   # % , some of which diverge
more than others in meaning from  . The
  
 dic-
tionary will then produce multiple possible translations
for each of    fi ffi ffi ffi   # % to give  ( fi ffi ffi ffi ( * % where , . 0 2 .
Again, some of  ( fi ffi ffi ffi ( * % will diverge more than oth-
41
DictionaryDictionary
  X ?> IL
Dictionary
  IL ?> Y
  Y ?> IL
Dictionary
  IL ?> X
Figure 1: The cycle of dictionaries
ers in meaning from their source words in     fi ffi ffi ffi     .
Hence we have  possible translations of the word 
from language

in language  . Some of    fi ffi ffi ffi    will
have diverged less in meaning than others from  , and
so can be considered ?better? translations. The problem
then is how to identify these ?better? translations.
2.2 Using The Link Structure To Find ?Better?
Translations
Our method for identifying the ?better? translations is
to first use dictionary 
  
to produce    
 
fi ffi ffi ffi  
  
 ,
the multiple possible translations of each of    fi ffi ffi ffi    ,
where     . Next we use dictionary
   
to
give   
 
fi ffi ffi ffi 
  	
 , the multiple translations of each of
   
 
fi ffi ffi ffi  
  
 , where 
    . We then select each of the
members of the set   
 
fi ffi ffi ffi 
  	
 which are equal to the
original word  . We hypothesise that to have returned
to the same starting word, the meanings of the words
that have formed a chain through the link structure can-
not have diverged significantly, and so we retrace two
steps to the word in    fi ffi ffi ffi    and accept this as a suit-
able translation of  . Figure 2 represents a hypotheti-
cal case in which two members of the set   
 
fi
ffi ffi ffi 
  	

are equal to the original word  . We retrace our route
from these through the links to  fi and   , and we ac-
cept these as suitable translations.
X IL Y IL X
x1
x1
x1
?> ?> ?> ?>
y1
y2
Figure 2: Translating from
    

   

. Nodes are possible translations.
If we apply the method described here to a large
number of words from language

then we can auto-
matically generate a language

-to-language  dictio-
nary. Here we have considered using just one interme-
diary language, but provided we have the dictionaries
to complete a cycle from

to  and back to

, then
we can use any number of intermediary languages, e.g.
   
,
    
 
,
 
 
  
,
  
 , where
 
 
is a second intermediary language.
3 The Experiment
We have applied the method described in section 2 in
order to automatically generate a Spanish-to-German
dictionary using Spanish-to-English, English-to-
German, German-to-English and English-to-Spanish
dictionaries. We chose Spanish and German because
we were able to find an online Spanish-to-German
dictionary which could be used to evaluate our
automatically-generated dictionary.
3.1 Obtaining The Data
We first collected large lists of German and English
lemmas from the Celex Database, ((Baayen and Gu-
likers1995)). We also gathered a short list of Span-
ish lemmas, all starting with the letter ?a? from the
Wiktionary website (Wiktionary) to use as our start-
ing terms. We created our own dictionaries by mak-
ing use of online dictionaries. In order to obtain the
English translations for the German lemmas and vice
versa, we queried ?The New English-German Dictio-
nary? site of The Technical Universiy of Dresden 1.
To obtain the English translations for the Spanish lem-
mas and vice versa, we queried ?The Spanish Dict?
website 2. Finally, we wanted to compare the per-
formance of our automatically-generated Spanish-to-
German dictionary with that of a manually-generated
Spanish-to-German dictionary, and for this we used a
website called ?DIX: Deutsch-Spanisch Woerterbuch?
3. Table  gives information about the four dictionar-
ies which we created in order to automatically gener-
ate our Spanish-to-German dictionary. The fifth is the
manually-generated dictionary used for evaluation.
Dicts Ents Trans Trans/term
S to E           ffi 
E to S             ffi 
G to E               ffi 
E to G               ffi 
S to G?          ffi  
Table 1: Dictionaries; S = Spanish, E = English, G =
German, S to G? is the dictionary used for evaluation.
1http://www.iee.et.tu-dresden.de/cgi-
bin/cgiwrap/wernerr/search.sh
2http://www.spanishdict.com/
3http://dix.osola.com/
42
3.2 Automatically Generating The Dictionary
For our experiment, we used the method described in
section 2 to automatically construct a scaled-down ver-
sion of a Spanish-to-German dictionary. It contained
     Spanish terms, all starting with the letter ?a?. To
store and operate on the data, we used the open source
database program PostgresSQL, version

ffi  ffi  . Start-
ing with the Spanish-to-English dictionary, at each of
stages    , we produced a new dictionary table with
an additional column to the right for the new lan-
guage. We did this by using the appropriate dictio-
nary to look up the translations for the terms in the
old rightmost column, before inserting these transla-
tions into a new rightmost column. For example, to
create the Spanish-to-English-to-German (SEG) table,
we used the English-to-German dictionary to find the
translations for the English terms in the Spanish-to-
English (SE) table, and then inserted these translations
into a new rightmost column. We kept producing new
tables in this fashion until we had generated a Spanish-
to-English-to-German-to-English-to-Spanish (SEGES)
table. In stage  , the final stage, we selected only those
rows in which the starting and ending Spanish terms
were the same. Important characteristics of these dic-
tionary tables are given in table

.
Stages Dicts Ents Trans Trans/term
0 SE           ffi 
1 SEG       	       ffi 
2 SEGE 
           ffi 
3 SEGES 
             ffi 
4 SEGES 
        ffi 
Table 2: Constructing Dictionary; Ents = number of
entries, Trans = number of translations, Trans/term =
average number of translations given per entry.
Table

shows that the number of translations-per-
term grew and grew from  ffi  translations in the start-
ing Spanish-to-English dictionary to an enormous  

ffi 
translations per term in the SEGES table after stage  .
However, after stage  , having selected only those rows
with matching first and last entries for Spanish, we re-
duced the number of translations back to  ffi  per term.
3.3 Evaluation
Having automatically generated the Spanish-to-
German dictionary containing 
   unique Spanish
terms, we then compared it to the manually-generated
Spanish-to-German dictionary (see section 3.1).
We gave the same initial      Spanish terms to the
manually-generated dictionary but received transla-
tions for only   

.
The results are summarised in table  . We observe
that when we regard the manually-generated dictionary
as the Gold-standard, our automatically-generated dic-
tionary managed to produce a relatively adequate cov-
erage of some

 ffi  

( 
	 
out of 
   ) with respect
Auto SG Man SG Overlap
Entries 
        	     ffi  


Total Trans      

   

 
Trans/Entry  ffi  
 ffi   ffi     ffi 


Table 3: Result: SG automatic vs SG manual
to main entries overlap between the two dictionaries.
When we look at the number of translations per term,
we find that our dictionary covered most of the transla-
tions found in the manually-generated dictionary (  ffi

out of 
 ffi  average or

 ffi 

) for which there was a
corresponding entry in our dictionary. In fact, our dic-
tionary produced more translations-per-term than the
manually-generated one. An extra translation may be
an error or it may not appear in the manually-generated
dictionary because the manually-generated dictionary
is too sparse. Further evaluation is required in order to
assess how many of the extra translations were errors.
In conclusion, we find that our automatically-
generated dictionary has an adequate but not perfect
coverage and very good recall for each term covered
within our dictionary. As for the precision of the trans-
lations found, we need more investigation and perhaps
a more complete manually-generated comparison dic-
tionary. The results might have been even better had
it not been for several problems with the four starting
dictionaries. For example, a translation for a particular
word could sometimes not be found as an entry in the
next dictionary. This might be because the entry sim-
ply wasn?t present, or because of different conventions
e.g. listing verbs as ?to Z? when another simply gives
?Z?. Another cause was differences in font encoding
e.g. with German umlauts. Results might also have
improved had the starting dictionaries provided more
translations per entry term, and had we used part-of-
speech information - this was impossible since not all
of the dictionaries listed part-of-speech. All in all given
the fact that the quality of data with which we started
was far from ideal, we believe that our method shows
great promise for saving human labour in the construc-
tion of translation dictionaries.
4 Conclusion
In this paper we have described a method using one
or more intermediary languages to automatically gen-
erate a dictionary to translate from one language,


,
to another,  . The method relies on using dictionar-
ies that can connect


to  and back to


via the in-
termediary language(s). We applied the method to au-
tomatically generate a Spanish-to-German dictionary,
and desptite the limitations of our starting dictionaries,
the result seems to be reasonably good. As was stated
in section  ffi  , we did not evaluate whether translations
we generated that were not in the gold-standard man-
ual dictionary were errors or good translations. This
is essential future work. We also intend to empirically
43
test what happens when further intermediary dictionar-
ies are introduced into the chain.
We believe that our method can make a great con-
tribution to the construction of translation dictionaries.
Even if a dictionary produced by ourmethod is not con-
sidered quite complete or accurate enough for general
use, it can serve as a very good starting point, thereby
saving a great deal of human labour - human labour
that requires a large amount of linguistic expertise. Our
method could be used to produce translation dictionar-
ies for relatively unconnected language groups, most
likely by using English as an intermediary language.
Such translation dictionaries could be important in pro-
moting communication between these language groups
and an ever more globalised and interconnected world.
A final point to make regards applying our method
more generally outside of the domain of translation
dictionary construction. We believe that our method,
which makes use of link structures, could be applied in
different areas involving graphs.
References
Kisuh Ahn, Beatrix Alex, Johan Bos, Tiphaine Dalmas,
Jochen L. Leidner, Matthew B. Smillie, and Bonnie
Webber. Cross-lingual question answering with qed.
2004.
Atelach Alemu Argaw, Lars Asker, Richard Coester,
and Jussi Kalgren. Dictionary based amharic - en-
glish information retrieval. 2004.
R.H. Baayen and L. Gulikers. The celex lexical
database (release 2). InDistriubted by the Linguistic
Data Consortium, 1995.
Christian Boitet, Mathieu Mangeot, and Gilles Seras-
set. The papillon project: Cooperatively build-
ing a multilingual lexical data-base to derive open
source dictionaries and lexicons. In 2nd Workshop
NLPXML, pages 93?96, Taipei, Taiwan, September
2002.
Gilles Serasset. Interlingual lexical organization for
multilingual lexical databases. In Proceedings of
15th International Conference on Computational
Linguistics, COLING-94, pages 5?9, Aug 1994.
Wiktionary. A wiki based open content dictionary. In
http://www.wiktionary.org/.
44
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1133?1141,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Real-time decision detection in multi-party dialogue
Matthew Frampton, Jia Huang, Trung Huu Bui and Stanley Peters
Center for the Study of Language and Information
Stanford University
Stanford, CA, 94305, USA
{frampton|jiahuang|thbui|peters}@stanford.edu
Abstract
We describe a process for automatically
detecting decision-making sub-dialogues
in multi-party, human-human meetings in
real-time. Our basic approach to decision
detection involves distinguishing between
different utterance types based on the roles
that they play in the formulation of a de-
cision. In this paper, we describe how this
approach can be implemented in real-time,
and show that the resulting system?s per-
formance compares well with other detec-
tors, including an off-line version.
1 Introduction
In collaborative and organized work environ-
ments, people share information and make de-
cisions through multi-party conversations, com-
monly referred to as meetings. The demand for
automatic methods that process, understand and
summarize information contained in audio and
video recordings of meetings is growing rapidly,
as evidenced by on-going projects which are fo-
cused on this goal, (Waibel et al, 2003; Janin et
al., 2004). Our research is part of a general effort
to develop a system that can automatically extract
and summarize information such as conversational
topics, action items, and decisions.
This paper concerns the development of a real-
time decision detector ? a system which can de-
tect and summarize decisions as they are made
during a meeting. Such a system could provide
a summary of all of the decisions which have been
made up until the current point in the meeting,
and this is something which we expect will help
users to enjoy more productive meetings. Cer-
tainly, good decision-making relies on access to
relevant information, and decisions made earlier
in a meeting often have a bearing on the current
topic of discussion, and so form part of this rele-
vant information. However, in a long and winding
meeting, participants might not have these earlier
decisions at the forefront of their minds, and so
an accurate and succinct reminder, as provided by
a real-time decision detector, could potentially be
very useful. A record of earlier decisions could
also help users to identify outstanding issues for
discussion, and to therefore make better use of the
remainder of the meeting.
Our approach to decision detection uses an an-
notation scheme which distinguishes between dif-
ferent types of utterance based on the roles which
they play in the decision-making process. Such a
scheme facilitates the detection of decision discus-
sions (Fern?andez et al, 2008), and by indicating
which utterances contain particular types of infor-
mation, it also aids their summarization. To auto-
matically detect decision discussions, we use what
we refer to as hierarchical classification. Here, in-
dependent binary sub-classifiers detect the differ-
ent decision dialogue acts, and then based on the
sub-classifier hypotheses, a super-classifier deter-
mines which dialogue regions are decision discus-
sions. In this paper then, we address the chal-
lenges for applying this approach in real-time, and
produce a system which is able to detect decisions
soon after they are made, (for example within a
minute). We conduct tests and compare this sys-
tem?s performance with other detectors, including
an off-line equivalent.
The remainder of the paper proceeds as follows.
Section 2 describes related work, and Section 3 de-
scribes our annotation scheme for decision discus-
sions, and our experimental data. Next, Section
4 explains the hierarchical classification approach
in more detail, and Section 5 considers how it can
be applied in real-time. Section 6 describes the
experiments in which we test the real-time detec-
tor, and finally, Section 7 presents conclusions and
ideas for future work.
1133
2 Related Work
Decisions are one of the most important meet-
ing outputs. User studies (Lisowska et al, 2004;
Banerjee et al, 2005) have confirmed that meeting
participants consider this to be the case, and Whit-
taker et al (2006) found that the development of
an automatic decision detection component is crit-
ical to the re-use of meeting archives. As a result,
with the new availability of substantial meeting
corpora such as the ISL (Burger et al, 2002), ICSI
(Janin et al, 2004) and AMI (McCowan et al,
2005) Meeting Corpora, recent years have seen an
increasing amount of research on decision-making
dialogue.
This recent research has tackled issues such
as the automatic detection of agreement and dis-
agreement (Hillard et al, 2003; Galley et al,
2004), and of the level of involvement of conver-
sational participants (Wrede and Shriberg, 2003;
Gatica-Perez et al, 2005). In addition, Verbree
et al (2006) created an argumentation scheme in-
tended to support automatic production of argu-
ment structure diagrams from decision-oriented
meeting transcripts. Only very recent research has
specifically investigated the automatic detection of
decisions, namely (Hsueh and Moore, 2007) and
(Fern?andez et al, 2008).
Hsueh and Moore (2007) used the AMI Meeting
Corpus, and attempted to automatically identify
dialogue acts (DAs) in meeting transcripts which
are ?decision-related?. Within any meeting, the
authors decided which DAs were decision-related
based on two different kinds of manually created
summary: the first was an extractive summary of
the whole meeting, and the second, an abstrac-
tive summary of the decisions which were made.
Those DAs in the extractive summary which sup-
port any of the decisions in the abstractive sum-
mary were manually tagged as decision-related.
Hsueh and Moore (2007) then trained a Maxi-
mum Entropy classifier to recognize this single
DA class, using a variety of lexical, prosodic, dia-
logue act and conversational topic features. They
achieved an F-score of 0.35, which gives an indi-
cation of the difficulty of this task.
Unlike Hsueh and Moore (2007), Fern?andez
et al (2008) made an attempt at modelling the
structure of decision-making dialogue. They de-
signed an annotation scheme that takes account of
the different roles which different utterances play
in the decision-making process ? for example,
their scheme distinguishes between decision DAs
(DDAs) which initiate a discussion by raising a
topic/issue, those which propose a resolution, and
those which express agreement for a proposed res-
olution and cause it to be accepted as a decision.
The authors applied the annotation scheme to a
portion of the AMI corpus, and then took what
they refer to as a hierarchical classification ap-
proach in order to automatically identify decision
discussions and their component DAs. Here, one
binary Support Vector Machine (SVM) per DDA
class hypothesized occurrences of that DDA class,
and then based on the hypotheses of these so-
called sub-classifiers, a super-classifier, (a further
SVM), determined which regions of dialogue rep-
resented decision discussions. This approach pro-
duced better results than the kind of ?flat classi-
fication? approach pursued by Hsueh and Moore
(2007) where a single classifier looks for exam-
ples of a single decision-related DA class. Using
manual transcripts, and a variety of lexical, utter-
ance, speaker, DA and prosodic features for the
sub-classifiers, the super-classifier?s F1-score was
0.58 according to a lenient match metric. Note that
(Purver et al, 2007) had previously pursued the
same basic approach as Fern?andez et al (2008) in
order to detect action items.
While both Hsueh and Moore (2007), and
Fern?andez et al (2008) attempted off-line decision
detection, in this paper, we attempt real-time deci-
sion detection. We take the same basic approach
as Fern?andez et al (2008), and make changes to
its implementation so that it can work effectively
in real-time.
3 Data
The AMI corpus (McCowan et al, 2005), is a
freely available corpus of multi-party meetings
containing both audio and video recordings, as
well as a wide range of annotated information
including dialogue acts and topic segmentation.
Conversations are all in English, but participants
can include non-native English speakers. All of
the meetings in our sub-corpus last around 30 min-
utes, and are scenario-driven, wherein four partic-
ipants play different roles in a company?s design
team: project manager, marketing expert, inter-
face designer and industrial designer. The discus-
sions concern how to design a remote control.
We used the off-line version of the Decipher
speech recognition engine (Stolcke et al, 2008) in
1134
order to obtain off-line ASR transcripts for these
17 meetings, and the real-time version, to ob-
tain real-time ASR transcripts. Decipher gener-
ates the transcripts by first producing Word Con-
fusion Networks (WCNs) and then extracting their
best paths. The real-time recognizer generates
?live? transcripts with 5 to 15 seconds of latency
for immediate display. In processing completed
meetings, the off-line system makes seven recog-
nition passes, including acoustic adaptation and
language model rescoring, in about 4.2 times real-
time (on a 4-score 2.6 GHz Opteron server). In
general usage with multi-party dialogue, the word
error rate (WER) for the off-line version of De-
cipher is approximately 23%, and for the real-
time version, approximately 35%
1
. Stolcke et al
(2008) report a WER of 26.9% for the off-line ver-
sion on AMI meetings.
The real-time ASR transcripts for the 17 meet-
ings contain a total of 8440 utterances/dialogue
acts, (around 496 per meeting), and the off-line
ASR transcripts, 7495 utterances/dialogue acts,
(around 441 per meeting).
3.1 Modelling Decision Discussions
We use the same annotation scheme as
(Fern?andez et al, 2008) in order to model
decision-making dialogue. As stated in Section 2,
this scheme distinguishes between a small number
of dialogue act types based on the role which they
perform in the formulation of a decision. Recall
that using this scheme in conjunction with hierar-
chical classification produced better decision de-
tection than a ?flat classification? approach with a
single ?decision-related? DA class. Since it indi-
cates which utterances contain particular types of
information, such a scheme also aids the summa-
rization of decision discussions.
The annotation scheme (see Table 1 for a sum-
mary) is based on the observation that a decision
discussion contains the following main structural
components: (a) a topic or issue requiring resolu-
tion is raised, (b) one or more possible resolutions
are considered, (c) a particular resolution is agreed
upon, that is, it becomes the decision. Hence the
scheme distinguishes between three correspond-
ing decision dialogue act (DDA) classes: Issue (I),
Resolution (R), and Agreement (A). Class R is fur-
ther subdivided into Resolution Proposal (RP) and
1
This information was obtained through personal commu-
nication.
Resolution Restatement (RR). Note that an utter-
ance can be assigned to more than one of these
DDA classes, and that within a decision discus-
sion, more than one utterance may correspond to a
particular DDA class.
Here we use the sample decision discussion
below in 1 in order to provide examples of the
different DDA types. I utterances introduce the
topic of the decision discussion, examples be-
ing ?Are we going to have a backup?? and ?But
would a backup really be necessary?? On the
other hand, R utterances specify the resolution
which is ultimately adopted as the decision. RP
utterances propose this resolution (e.g. ?I think
maybe we could just go for the kinetic energy. . . ?),
while RR utterances close the discussion by con-
firming/summarizing the decision (e.g. ?Okay,
fully kinetic energy?). Finally, A utterances agree
with the proposed resolution, so causing it to be
adopted as the decision, (e.g. ?Yeah?, ?Good?
and ?Okay?.
(1) A: Are we going to have a backup?
Or we do just?
B: But would a backup really be necessary?
A: I think maybe we could just go for the
kinetic energy and be bold and innovative.
C: Yeah.
B: I think? yeah.
A: It could even be one of our selling points.
C: Yeah ?laugh?.
D: Environmentally conscious or something.
A: Yeah.
B: Okay, fully kinetic energy.
D: Good.
2
3.2 Experimental data for real-time decision
detection
Originally, two individuals used the annotation
scheme described above in order to annotate the
manual transcripts of 9 and 10 meetings respec-
tively. The annotators overlapped on two meet-
ings, and their kappa inter-annotator agreement
ranged from 0.63 to 0.73 for the four DDA classes.
The highest agreement was obtained for class RP,
and the lowest for class A. Although these kappa
values are not extremely high, if we used a single,
less homogeneous ?decision-related? DA class
like Hsueh and Moore (2007), then its kappa score
2
This example was extracted from the AMI dialogue
ES2015c and has been modified slightly for presentation pur-
poses.
1135
key DDA class description
I issue utterances introducing the issue or topic under discussion
R resolution utterances containing the resolution adopted as the decision
RP ? proposal ? utterances where the decision is originally proposed
RR ? restatement ? utterances where the decision is confirmed or restated
A agreement utterances explicitly signalling agreement with the decision
Table 1: Set of decision dialogue act (DDA) classes
would probably be significantly lower. The de-
cision discussion annotations used by Hsueh and
Moore (2007) are part of the AMI corpus, and are
for the manual transcriptions. The reader can find
a comparison between these annotations and our
own manual transcript annotations in (Fern?andez
et al, 2008).
After obtaining the new off-line and real-time
ASR transcripts, we transferred the DDA annota-
tions from the manual transcripts. In both sets of
ASR transcripts, each meeting contains on aver-
age around 26 DAs tagged with one or more of the
DDA sub-classes in Table 1. DDAs are thus very
sparse, corresponding to only 5.3% of utterances
in the real-time transcripts, and 6.0% in the off-
line. In the real-time transcripts, Issue utterances
make up less than 1.2% of the total number of ut-
terances in a meeting, while Resolution utterances
are around 1.6%: 1.2% are RP and less than 0.4%
are RR on average. Almost half of DDA utterances
(slightly over 2.6% of all utterances on average)
are tagged as belonging to class Agreement. In the
off-line transcripts, the percentages are fairly sim-
ilar: 1.6% of utterances are Issue DDAs, 2.0% are
RP, 0.5% are RR, and 2.4% are A.
We now move on to describe the hierarchical
classification approach which we use to try to au-
tomatically detect decision sub-dialogues and their
component DDAs.
4 Hierarchical Classification
Hierarchical classification is designed to exploit
the fact that within decision discussions, our
DDAs can be expected to co-occur in particular
types of patterns. It involves two different types of
classifier:
1. Sub-classifier: One independent binary sub-
classifier per DDA class classifies each utter-
ance.
2. Super-classifier: A sliding window shifts
through the meeting one utterance at a time,
and following each shift, a binary super-
classifier determines whether the region of
dialogue within the window is part of a de-
cision discussion.
In our decision detectors, the sub-classifiers run
in parallel in order to reduce processing time.
For each utterance, the sub-classifiers use fea-
tures which are derived from the properties of
that utterance in context. On the other hand,
the super-classifier?s features are the hypothesized
class labels and confidence scores for the utter-
ances within the window. In various experiments,
we have found that a suitable size for the window,
is the average length of a decision discussion in
our data in utterances. The super-classifier also
?corrects? the sub-classifiers. This means that if a
DA is classified as positive by a sub-classifier, but
does not fall within a region classified as part of
a decision discussion by the super-classifier, then
the sub-classifier?s hypothesis is changed to nega-
tive.
We now move on to consider how this basic ap-
proach to decision detection can be implemented
in a real-time system.
5 Design considerations for our real-time
system
A real-time decision detector should detect deci-
sions as soon after they are made as possible. It is
for this reason that we have set our real-time de-
tector to automatically run at frequent and regular
intervals during a meeting. An alternative would
be to give the user (a meeting participant) respon-
sibility for instructing the detector when to run.
However, a user may sometimes leave substantial
gaps between giving run commands. When this
happens, the detector will have to process a large
number of utterances in a single run, and so the
user may wait some time before being presented
with any results. In addition, giving the user re-
sponsibility for instructing the detector when to
1136
Figure 1: Decision discussion regions hypothesized by
consecutive runs overlap (D
1
to D
3
and D
2
to D
4
) and so
are merged.
run means burdening the user with an extra task to
perform during the meeting, and this goes against
the general philosophy behind the system?s devel-
opment. The system is intended to be as unobtru-
sive as possible during the meeting, and to relieve
users of tasks which distract their attention away
from the current discussion (e.g. note-taking), not
to create new tasks, however small.
Obviously, on the first occasion that the detector
runs during a meeting, it can only process ?new?
(previously unprocessed) utterances, but on sub-
sequent runs, it has the option to reprocess some
number of ?old? utterances (utterances which it
has already processed in a previous run). Cer-
tainly, the detector should reprocess some of the
most recent old utterances because it is possible
that a decision discussion straddles these utter-
ances and new utterances. However, the number of
old utterances that are reprocessed should be lim-
ited. If the meeting has lasted a while already, then
the processing of a large portion of the earlier old
utterances is likely to be redundant ? it will sim-
ply produce the same results for these utterances
as the previous run.
The fact that the real-time detector processes re-
cent old utterances means that consecutive runs
can produce hypotheses for decision discussion re-
gions which overlap, or which are duplicates. Fig-
ure 1 gives an example of the former. We deal with
overlapping hypotheses by merging them into one,
so forming a larger single decision discussion re-
gion. Figure 2 gives an example of duplicate hy-
potheses. Here, on run n, the detector hypothe-
sizes decision discussion D
1
to D
2
, and then on
run n+1, since the bounds of this original hypoth-
esis are now wholly contained within the region of
Figure 2: Consecutive runs hypothesize the same decision
discussion region D
1
to D
2
, and so one of the duplicates is
discarded.
old reprocessed utterances, the detector hypothe-
sizes a duplicate. We deal with such cases by dis-
carding the duplicate.
6 Experiments
We conducted various experiments related to real-
time decision detection, our goal being to produce
a system which:
? relative to alternative versions, detects deci-
sion discussions accurately,
? generates results for any portion of dialogue
very soon after that portion of dialogue has
ended.
The current version of our real-time detector is set
to process the same number of old and new utter-
ances on each run. Here, we refer to this value as i,
and hence on each run the system processes a total
of 2i utterances (i old and i new). Another of the
system?s characteristics is that runs take place ev-
ery i utterances, meaning that as we decrease i, the
system provides new results more frequently and
is hence ?more real-time?. One of the things we
investigate here then, is what to set i to in order
to best satisfy the two design goals given above.
Having found this value, we compare the hierar-
chical real-time detector?s performance with alter-
native detectors, these being:
? an off-line detector applied to off-line ASR
transcripts,
? a flat real-time detector,
? an off-line detector applied to the real-time
ASR transcripts.
1137
Lexical unigrams after text normalization
Utterance length in words, duration in
word rate
Speaker speaker ID & AMI speaker role
Context features as above for utterances
u +/- 1. . .u +/- 5
Table 2: Features for decision DA detection
Note that the off-line detectors use hierarchical
classification, and that the flat real-time detec-
tor uses a single binary classifier which treats all
DDAs as members of a single merged DDA class.
6.1 Classifiers and features
All classifiers (sub and super-classifiers) in all de-
tectors are linear-kernel Support Vector Machines
(SVMs), produced using SVMlight (Joachims,
1999). For the sub-classifiers, we are obviously re-
stricted to using features which can be computed
in a very short period of time, and in the experi-
ments here, we use lexical, utterance and speaker
features. These are summarized in Table 2. An
utterance?s lexical features are the words in its
transcription, its utterance features are its dura-
tion, number of words, and word rate (number of
words divided by duration), and its speaker fea-
tures are the speaker?s role (see Section 3) and ID.
We also use lexical features for the previous and
where available, next utterances: the I, RP and RR
sub-classifiers use the lexical features for the pre-
vious/next utterance and the A sub-classifier, those
from the previous/next 5 utterances. These set-
tings produced the best results in preliminary ex-
periments. We do not use DA features because
we lack an automatic DA tagger, nor do we use
prosodic features because (Fern?andez et al, 2008)
was unable to derive any value from them with
SVMs.
6.2 Evaluation
We evaluate each of our decision detectors in 17-
fold cross validations, where in each fold, the de-
tector trains on 16 meetings and then tests on the
remaining one. Evaluation can be made at three
levels:
1. The sub-classifiers? detection of each of the
DDA classes.
2. The sub-classifiers? detection of each of the
DDA classes after correction by the super-
classifier.
Figure 3: The relationship between the number of old/new
utterances processed in a single run, and the super-classifier?s
F1-score. Here the sub-classifiers use only lexical features.
3. The super-classifier?s detection of decision
discussion regions.
For 1 and 2, we use the same lenient-match met-
ric as (Fern?andez et al, 2008; Hsueh and Moore,
2007), which allows a margin of 20 seconds pre-
ceding and following a hypothesized DDA. Note
that here we only give credit for hypotheses based
on a 1-1 mapping with the gold-standard labels.
For 3, we follow (Fern?andez et al, 2008; Purver et
al., 2007) and use a windowed metric that divides
the dialogue into 30-second windows and evalu-
ates on a per window basis.
6.3 Results and analysis
Here, Section 6.3.1 will present results for differ-
ent values of i, the number of old/new utterances
processed in a single run. Section 6.3.2 then com-
pares the performance of the real-time and off-line
systems, (and also real-time systems which use hi-
erarchical vs. flat classification), and Section 6.3.3
presents some feature analysis.
6.3.1 Varying the number of old/new
utterances processed in a run
Figure 3 shows the relationship between i, the set-
ting for the number of old/new utterances pro-
cessed in a single run, and the super-classifier?s
F1-score. Here, the sub-classifiers are using only
lexical features. We can see from the graph that
as i increases to 15, the super-classifier?s F1-score
also increases, but thereafter, it plateaus. Hence
15 is apparently the value which best satisfies the
two design goals given at the start of Section 6.
It should also be noted that 15 is the mean length
of a decision discussion in our data, and so per-
1138
sub-classifiers super
I RP RR A classifier
Re .73 .73 .84 .71 .82
Pr .08 .09 .03 .15 .40
F1 .15 .16 .06 .25 .54
Table 3: Results for the hierarchical real-time
decision detector, using lexical, utterance and
speaker features.
sub-classifiers super
I RP RR A classifier
Re .51 .51 .10 .63 .83
Pr .12 .11 .04 .15 .41
F1 .19 .19 .05 .24 .55
Table 4: Results for the hierarchical off-line de-
cision detector on off-line ASR transcripts, using
lexical, utterance and speaker features.
haps this is a transferable finding. The mean du-
ration of a run when i = 15 is approximately 4
seconds, while the mean duration of 15 utterances
in our data-set is approximately 60 seconds, mean-
ing that for the average case, the detector returns
the results for the current run, long before it is
due to make the next. Significant lee-way is per-
haps necessary here, because the final version of
the real-time detector will include a summariza-
tion component which extracts key phrases from
Issue/Resolution utterances, and its processing can
last some time, even for a single decision.
We should say then, that the system is not
strictly real-time because in general, it detects de-
cisions soon after they are made (for example
within a minute), rather than immediately after. In
the future we intend to modify the system so that
it can run more frequently than once every i ut-
terances. However it is important that runs do not
occur too frequently ? for example, if i = 15 and
the system runs after every utterance, then the ex-
tra processing will cause it to gradually fall further
and further behind the meeting.
6.3.2 Real-time vs. off-line results
Table 3 shows the results achieved by a hierarchi-
cal real-time decision detector whose run settings
are as described above, and whose sub-classifiers
3
use lexical, utterance and speaker features. These
results compare well with those of an equivalent
3
In Tables 3 to 6, sub-classifier results are post-correction
(see Section 6.2).
sub-classifiers super
I RP RR A classifier
Re .50 .51 .09 .63 .83
Pr .11 .11 .03 .14 .41
F1 .19 .18 .05 .23 .55
Table 5: Results for the hierarchical off-line de-
tector on real-time ASR transcripts, using lexical,
utterance and speaker features.
sub-classifiers super
I RP RR A classifier
Re .67 .74 .84 .66 .85
Pr .07 .08 .03 .14 .41
F1 .13 .15 .05 .24 .55
Table 6: Results for the hierarchical real-time de-
cision detector, using lexical features only.
off-line detector, which are shown in Table 4. The
F1-scores for the real-time and off-line decision
super-classifiers are .54 and .55 respectively, and
the difference is not statistically significant. This
may indicate that the hierarchical classification ap-
proach is fairly robust to increasing ASR Word
Error Rates (WERs). Combining the output from
each of the independent sub-classifiers might com-
pensate somewhat for any decreases in their indi-
vidual accuracy, as there was here for the I and RP
sub-classifiers.
The hierarchical real-time detector?s F1-score is
also 10 points higher than a flat classifier (.54 vs.
.44). Hence, while Fern?andez et al (2008) demon-
strated that the hierarchical classification approach
could improve off-line decision detection, we have
demonstrated here that it can also improve real-
time decision detection.
Table 5 shows the results when an off-line
detector is applied to real-time ASR transcripts.
Here, the super-classifier obtains an F1-score of
.55, one point higher than the real-time detector,
but again, the difference is not statistically signifi-
cant.
6.3.3 Feature analysis
We also investigated the contribution of the ut-
terance and speaker features. Table 6 shows the
results for the hierarchical real-time decision de-
tector when its sub-classifiers use only lexical fea-
tures. The sub-classifier F1-scores are all slightly
lower than when utterance and speaker features
are used (see Table 3), and the super-classifier
1139
score is only 1 point different. None of these dif-
ferences are statistically significant.
Since lexical features are important, we used in-
formation gain in order to investigate which words
are predictive of each DDA type. Due to differ-
ences in the transcripts, the predictive words for
the off-line and real-time systems are not the same,
but we can find commonalities, and these com-
monalities make sense given the DDA definitions.
Firstly in Resolution and particularly Issue DAs,
some of the most predictive words could be used
to define discussion topics, and so we might ex-
pect to find them in the meeting agenda. Exam-
ples are ?energy?, and ?color?. Predictive words
for Resolutions also include semantically-related
words which are key in defining the decision (?ki-
netic?,?green?). Additional predictive words for
RPs are the personal pronouns ?I? and ?we?,
and the verbs, ?think? and ?like?, and for RRs,
words which we would associate with summing
up (?consensus?, ?definitely?, and ?okay?). Un-
surprisingly, for Agreements, ?yeah? and ?okay?
both score very highly.
7 Conclusion
(Fern?andez et al, 2008) described an approach
to decision detection in multi-party meetings and
demonstrated how it could work relatively well in
an off-line system. The approach has two defining
characteristics. The first is its use of an annota-
tion scheme which distinguishes between differ-
ent utterance types based on the roles which they
play in the decision-making process. The second
is its use of hierarchical classification, whereby
binary sub-classifiers detect instances of each of
the decision DAs (DDAs), and then based on the
sub-classifier hypotheses, a super-classifier deter-
mines which regions of dialogue are decision dis-
cussions.
In this paper then, we have taken the same ba-
sic approach to decision detection as Fern?andez et
al. (2008), but changed the way in which it is im-
plemented so that it can work effectively in real-
time. Our implementation changes include run-
ning the detector at regular and frequent intervals
during the meeting, and reprocessing recent utter-
ances in case a decision discussion straddles these
and brand new utterances. The fact that the de-
tector reprocesses utterances means that on con-
secutive runs, overlapping and duplicate hypothe-
sized decision discussions are possible. We have
therefore added facilities to merge overlapping hy-
potheses and to remove duplicates.
In general, the resulting system is able to detect
decisions soon after they are made (for example
within a minute), rather than immediately after. It
has performed well in testing, achieving an F1-
score of .54, which is only one point lower than
an equivalent off-line system, and in any case, the
difference was not statistically significant. A flat
real-time detector achieved .44.
In future work, we plan to extend the decision
discussion annotation scheme and try to extract
supporting arguments for decisions. We will also
experiment with using sequential models in order
to try to exploit any sequential ordering patterns in
the occurrence of the DDAs.
Acknowledgements This material is based
upon work supported by the Defense Advanced
Research Projects Agency (DARPA) under Con-
tract No. FA8750-07-D-0185/0004, and by the
Department of the Navy Office of Naval Research
(ONR) under Grants No. N00014-05-1-0187 and
N00014-09-1-0106. Any opinions, findings and
conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the views of DARPA or ONR.
We are grateful to the three anonymous EMNLP
reviewers for their helpful comments and sugges-
tions, and to our partners at SRI International who
provided us with off-line and real-time transcripts
for our meeting data.
References
Satanjeev Banerjee, Carolyn Ros?e, and Alex Rudnicky.
2005. The necessity of a meeting recording and
playback system, and the benefit of topic-level anno-
tations to meeting browsing. In Proceedings of the
10th International Conference on Human-Computer
Interaction.
Susanne Burger, Victoria MacLaren, and Hua Yu.
2002. The ISL Meeting Corpus: The impact of
meeting type on speech style. In Proceedings of the
7th International Conference on Spoken Language
Processing (INTERSPEECH - ICSLP), Denver, Col-
orado.
Raquel Fern?andez, Matthew Frampton, Patrick Ehlen,
Matthew Purver, and Stanley Peters. 2008. Mod-
elling and detecting decisions in multi-party dia-
logue. In Proc. of the 9th SIGdial Workshop on Dis-
course and Dialogue.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
1140
ment and disagreement in conversational speech:
Use of Bayesian networks to model pragmatic de-
pendencies. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Daniel Gatica-Perez, Ian McCowan, Dong Zhang, and
Samy Bengio. 2005. Detecting group interest level
in meetings. In IEEE International Conference on
Acoustics, Speech, and Signal Processing (ICASSP).
Dustin Hillard, Mari Ostendorf, and Elisabeth
Shriberg. 2003. Detection of agreement vs. dis-
agreement in meetings: Training with unlabeled
data. In Companion Volume of the Proceedings of
HLT-NAACL 2003 - Short Papers, Edmonton, Al-
berta, May.
Pey-Yun Hsueh and Johanna Moore. 2007. Automatic
decision detection in meeting speech. In Proceed-
ings of MLMI 2007, Lecture Notes in Computer Sci-
ence. Springer-Verlag.
Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip
Dhillon, Jane Edwards, Javier Marc??as-Guarasa,
Nelson Morgan, Barbara Peskin, Elizabeth Shriberg,
Andreas Stolcke, Chuck Wooters, and Britta Wrede.
2004. The ICSI meeting project: Resources and re-
search. In Proceedings of the 2004 ICASSP NIST
Meeting Recognition Workshop.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Sch?olkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods ?
Support Vector Learning. MIT Press.
Agnes Lisowska, Andrei Popescu-Belis, and Susan
Armstrong. 2004. User query analysis for the spec-
ification and evaluation of a dialogue processing and
retrieval system. In Proceedings of the 4th Interna-
tional Conference on Language Resources and Eval-
uation.
Iain McCowan, Jean Carletta, W. Kraaij, S. Ashby,
S. Bourban, M. Flynn, M. Guillemot, T. Hain,
J. Kadlec, V. Karaiskos, M. Kronenthal, G. Lathoud,
M. Lincoln, A. Lisowska, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meeting Corpus. In
Proceedings of Measuring Behavior, the 5th Inter-
national Conference on Methods and Techniques in
Behavioral Research, Wageningen, Netherlands.
Matthew Purver, John Dowding, John Niekrasz,
Patrick Ehlen, Sharareh Noorbaloochi, and Stanley
Peters. 2007. Detecting and summarizing action
items in multi-party dialogue. In Proceedings of the
8th SIGdial Workshop on Discourse and Dialogue,
Antwerp, Belgium.
Andreas Stolcke, Xavier Anguera, Kofi Boakye,
?
Ozg?ur
C?etin, Adam Janin, Matthew Magimai-Doss, Chuck
Wooters, and Jing Zheng. 2008. The ICSI-SRI
Spring 2007 meeting and lecture recognition system.
In Proc. of CLEAR 2007 and RT2007.
Daan Verbree, Rutger Rienks, and Dirk Heylen. 2006.
First steps towards the automatic construction of
argument-diagrams from real discussions. In Pro-
ceedings of the 1st International Conference on
Computational Models of Argument, volume 144,
pages 183?194. IOS press.
A. Waibel, T. Schultz, M. Bett, M. Denecke, R. Malkin,
I. Rogina, R. Stiefelhagen, and J. Yang. 2003.
SMaRT: The smart meeting room task at ISL. In
ICASSP.
Steve Whittaker, Rachel Laban, and Simon Tucker.
2006. Analysing meeting records: An ethnographic
study and technological implications. In S. Renals
and S. Bengio, editors, Machine Learning for Multi-
modal Interaction: Second International Workshop,
MLMI 2005, Revised Selected Papers, volume 3869
of Lecture Notes in Computer Science, pages 101?
113. Springer.
Britta Wrede and Elizabeth Shriberg. 2003. Spotting
?hot spots? in meetings: Human judgements and
prosodic cues. In Proceedings of the 9th European
Conference on Speech Communication and Technol-
ogy, Geneva, Switzerland.
1141
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 273?281,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Who is ?You?? Combining Linguistic and Gaze Features to Resolve
Second-Person References in Dialogue?
Matthew Frampton1, Raquel Ferna?ndez1, Patrick Ehlen1, Mario Christoudias2,
Trevor Darrell2 and Stanley Peters1
1Center for the Study of Language and Information, Stanford University
{frampton, raquelfr, ehlen, peters}@stanford.edu
2International Computer Science Institute, University of California at Berkeley
cmch@icsi.berkeley.edu, trevor@eecs.berkeley.edu
Abstract
We explore the problem of resolving the
second person English pronoun you in
multi-party dialogue, using a combination
of linguistic and visual features. First, we
distinguish generic and referential uses,
then we classify the referential uses as ei-
ther plural or singular, and finally, for the
latter cases, we identify the addressee. In
our first set of experiments, the linguistic
and visual features are derived from man-
ual transcriptions and annotations, but in
the second set, they are generated through
entirely automatic means. Results show
that a multimodal system is often prefer-
able to a unimodal one.
1 Introduction
The English pronoun you is the second most fre-
quent word in unrestricted conversation (after I
and right before it).1 Despite this, with the ex-
ception of Gupta et al (2007b; 2007a), its re-
solution has received very little attention in the lit-
erature. This is perhaps not surprising since the
vast amount of work on anaphora and reference
resolution has focused on text or discourse - medi-
ums where second-person deixis is perhaps not
as prominent as it is in dialogue. For spoken di-
alogue pronoun resolution modules however, re-
solving you is an essential task that has an impor-
tant impact on the capabilities of dialogue summa-
rization systems.
?We thank the anonymous EACL reviewers, and Surabhi
Gupta, John Niekrasz and David Demirdjian for their com-
ments and technical assistance. This work was supported by
the CALO project (DARPA grant NBCH-D-03-0010).
1See e.g. http://www.kilgarriff.co.uk/BNC_lists/
Besides being important for computational im-
plementations, resolving you is also an interesting
and challenging research problem. As for third
person pronouns such as it, some uses of you are
not strictly referential. These include discourse
marker uses such as you know in example (1), and
generic uses like (2), where you does not refer to
the addressee as it does in (3).
(1) It?s not just, you know, noises like something
hitting.
(2) Often, you need to know specific button se-
quences to get certain functionalities done.
(3) I think it?s good. You?ve done a good review.
However, unlike it, you is ambiguous between sin-
gular and plural interpretations - an issue that is
particularly problematic in multi-party conversa-
tions. While you clearly has a plural referent in
(4), in (3) the number of its referent is ambigu-
ous.2
(4) I don?t know if you guys have any questions.
When an utterance contains a singular referen-
tial you, resolving the you amounts to identifying
the individual to whom the utterance is addressed.
This is trivial in two-person dialogue since the cur-
rent listener is always the addressee, but in conver-
sations with multiple participants, it is a complex
problem where different kinds of linguistic and vi-
sual information play important roles (Jovanovic,
2007). One of the issues we investigate here is
2In contrast, the referential use of the pronoun it (as well
as that of some demonstratives) is ambiguous between NP
interpretations and discourse-deictic ones (Webber, 1991).
273
how this applies to the more concrete problem of
resolving the second person pronoun you.
We approach this issue as a three-step prob-
lem. Using the AMI Meeting Corpus (McCowan
et al, 2005) of multi-party dialogues, we first dis-
criminate between referential and generic uses of
you. Then, within the referential uses, we dis-
tinguish between singular and plural, and finally,
we resolve the singular referential instances by
identifying the intended addressee. We use multi-
modal features: initially, we extract discourse fea-
tures from manual transcriptions and use visual in-
formation derived from manual annotations, but
then we move to a fully automatic approach, us-
ing 1-best transcriptions produced by an automatic
speech recognizer (ASR) and visual features auto-
matically extracted from raw video.
In the next section of this paper, we give a brief
overview of related work. We describe our data in
Section 3, and explain how we extract visual and
linguistic features in Sections 4 and 5 respectively.
Section 6 then presents our experiments with man-
ual transcriptions and annotations, while Section
7, those with automatically extracted information.
We end with conclusions in Section 8.
2 Related Work
2.1 Reference Resolution in Dialogue
Although the vast majority of work on reference
resolution has been with monologic text, some re-
cent research has dealt with the more complex
scenario of spoken dialogue (Strube and Mu?ller,
2003; Byron, 2004; Arstein and Poesio, 2006;
Mu?ller, 2007). There has been work on the iden-
tification of non-referential uses of the pronoun it:
Mu?ller (2006) uses a set of shallow features au-
tomatically extracted from manual transcripts of
two-party dialogue in order to train a rule-based
classifier, and achieves an F-score of 69%.
The only existing work on the resolution of you
that we are aware of is Gupta et al (2007b; 2007a).
In line with our approach, the authors first disam-
biguate between generic and referential you, and
then attempt to resolve the reference of the ref-
erential cases. Generic uses of you account for
47% of their data set, and for the generic vs. ref-
erential disambiguation, they achieve an accuracy
of 84% on two-party conversations and 75% on
multi-party dialogue. For the reference resolution
task, they achieve 47%, which is 10 points over
a baseline that always classifies the next speaker
as the addressee. These results are achieved with-
out visual information, using manual transcripts,
and a combination of surface features and manu-
ally tagged dialogue acts.
2.2 Addressee Detection
Resolving the referential instances of you amounts
to determining the addressee(s) of the utterance
containing the pronoun. Recent years have seen
an increasing amount of research on automatic
addressee detection. Much of this work focuses
on communication between humans and computa-
tional agents (such as robots or ubiquitous com-
puting systems) that interact with users who may
be engaged in other activities, including interac-
tion with other humans. In these situations, it
is important for a system to be able to recognize
when it is being addressed by a user. Bakx et
al. (2003) and Turnhout et al (2005) studied this
issue in the context of mixed human-human and
human-computer interaction using facial orienta-
tion and utterance length as clues for addressee
detection, while Katzenmaier et al (2004) inves-
tigated whether the degree to which a user utter-
ance fits the language model of a conversational
robot can be useful in detecting system-addressed
utterances. This research exploits the fact that hu-
mans tend to speak differently to systems than to
other humans.
Our research is closer to that of Jovanovic
et al (2006a; 2007), who studied addressing in
human-human multi-party dialogue. Jovanovic
and colleagues focus on addressee identification in
face-to-face meetings with four participants. They
use a Bayesian Network classifier trained on sev-
eral multimodal features (including visual features
such as gaze direction, discourse features such as
the speaker and dialogue act of preceding utter-
ances, and utterance features such as lexical clues
and utterance duration). Using a combination of
features from various resources was found to im-
prove performance (the best system achieves an
accuracy of 77% on a portion of the AMI Meeting
Corpus). Although this result is very encouraging,
it is achieved with the use of manually produced
information - in particular, manual transcriptions,
dialogue acts and annotations of visual focus of at-
tention. One of the issues we aim to investigate
here is how automatically extracted multimodal
information can help in detecting the addressee(s)
of you-utterances.
274
Generic Referential Ref Sing. Ref Pl.
49.14% 50.86% 67.92% 32.08%
Table 1: Distribution of you interpretations
3 Data
Our experiments are performed using the AMI
Meeting Corpus (McCowan et al, 2005), a collec-
tion of scenario-driven meetings among four par-
ticipants, manually transcribed and annotated with
several different types of information (including
dialogue acts, topics, visual focus of attention, and
addressee). We use a sub-corpus of 948 utterances
containing you, and these were extracted from 10
different meetings. The you-utterances are anno-
tated as either discourse marker, generic or refer-
ential.
We excluded the discourse marker cases, which
account for only 8% of the data, and of the refer-
ential cases, selected those with an AMI addressee
annotation.3 The addressee of a dialogue act can
be unknown, a single meeting participant, two
participants, or the whole audience (three partici-
pants in the AMI corpus). Since there are very few
instances of two-participant addressee, we distin-
guish only between singular and plural addressees.
The resulting distribution of classes is shown in
Table 1.4
We approach the reference resolution task as a
two-step process, first discriminating between plu-
ral and singular references, and then resolving the
reference of the singular cases. The latter task re-
quires a classification scheme for distinguishing
between the three potential addressees (listeners)
for the given you-utterance.
In their four-way classification scheme,
Gupta et al (2007a) label potential addressees in
terms of the order in which they speak after the
you-utterance. That is, for a given you-utterance,
the potential addressee who speaks next is labeled
1, the potential addressee who speaks after that is
2, and the remaining participant is 3. Label 4 is
used for group addressing. However, this results
in a very skewed class distribution because the
next speaker is the intended addressee 41% of
the time, and 38% of instances are plural - the
3Addressee annotations are not provided for some dia-
logue act types - see (Jovanovic et al, 2006b).
4Note that the percentages of the referential singular and
referential plural are relative to the total of referential in-
stances.
L1 L2 L3
35.17% 30.34% 34.49%
Table 2: Distribution of addressees for singular you
remaining two classes therefore make up a small
percentage of the data.
We were able to obtain a much less skewed class
distribution by identifying the potential addressees
in terms of their position in relation to the current
speaker. The meeting setting includes a rectangu-
lar table with two participants seated at each of
its opposite longer sides. Thus, for a given you-
utterance, we label listeners as either L1, L2 or
L3 depending on whether they are sitting opposite,
diagonally or laterally from the speaker. Table 2
shows the resulting class distribution for our data-
set. Such a labelling scheme is more similar to Jo-
vanovic (2007), where participants are identified
by their seating position.
4 Visual Information
4.1 Features from Manual Annotations
We derived per-utterance visual features from the
Focus Of Attention (FOA) annotations provided
by the AMI corpus. These annotations track meet-
ing participants? head orientation and eye gaze
during a meeting.5 Our first step was to use the
FOA annotations in order to compute what we re-
fer to as Gaze Duration Proportion (GDP) values
for each of the utterances of interest - a measure
similar to the ?Degree of Mean Duration of Gaze?
described by (Takemae et al, 2004). Here a GDP
value denotes the proportion of time in utterance u
for which subject i is looking at target j:
GDPu(i, j) =
?
j
T (i, j)/Tu
were Tu is the length of utterance u in millisec-
onds, and T (i, j), the amount of that time that i
spends looking at j. The gazer i can only refer to
one of the four meeting participants, but the tar-
get j can also refer to the white-board/projector
screen present in the meeting room. For each utter-
ance then, all of the possible values of i and j are
used to construct a matrix of GDP values. From
this matrix, we then construct ?Highest GDP? fea-
tures for each of the meeting participants: such
5A description of the FOA labeling scheme is avail-
able from the AMI Meeting Corpus website http://corpus.
amiproject.org/documentations/guidelines-1/
275
For each participant Pi
? target for whole utterance
? target for first third of utterance
? target for second third of utterance
? target for third third of utterance
? target for -/+ 2 secs from you start time
? ratio 2nd hyp. target / 1st hyp. target
? ratio 3rd hyp. target / 1st hyp. target
? participant in mutual gaze with speaker
Table 3: Visual Features
features record the target with the highest GDP
value and so indicate whom/what the meeting par-
ticipant spent most time looking at during the ut-
terance.
We also generated a number of additional fea-
tures for each individual. These include firstly,
three features which record the candidate ?gazee?
with the highest GDP during each third of the ut-
terance, and which therefore account for gaze tran-
sitions. So as to focus more closely on where par-
ticipants are looking around the time when you
is uttered, another feature records the candidate
with the highest GDP -/+ 2 seconds from the start
time of the you. Two further features give some
indication of the amount of looking around that
the speaker does during an utterance - we hypoth-
esized that participants (especially the speaker)
might look around more in utterances with plu-
ral addressees. The first is the ratio of the sec-
ond highest GDP to the highest, and the second
is the ratio of the third highest to the highest. Fi-
nally, there is a highest GDP mutual gaze feature
for the speaker, indicating with which other indi-
vidual, the speaker spent most time engaged in a
mutual gaze.
Hence this gives a total of 29 features: seven
features for each of the four participants, plus one
mutual gaze feature. They are summarized in Ta-
ble 3. These visual features are different to those
used by Jovanovic (2007) (see Section 2). Jo-
vanovic?s features record the number of times that
each participant looks at each other participant
during the utterance, and in addition, the gaze di-
rection of the current speaker. Hence, they are not
highest GDP values, they do not include a mutual
gaze feature and they do not record whether par-
ticipants look at the white-board/projector screen.
4.2 Automatic Features from Raw Video
To perform automatic visual feature extraction, a
six degree-of-freedom head tracker was run over
each subject?s video sequence for the utterances
containing you. For each utterance, this gave 4 se-
quences, one per subject, of the subject?s 3D head
orientation and location at each video frame along
with 3D head rotational velocities. From these
measurements we computed two types of visual
information: participant gaze and mutual gaze.
The 3D head orientation and location of each
subject along with camera calibration information
was used to compute participant gaze information
for each video frame of each sequence in the form
of a gaze probability matrix. More precisely, cam-
era calibration is first used to estimate the 3D head
orientation and location of all subjects in the same
world coordinate system.
The gaze probability matrix is a 4 ? 5 matrix
where entry i, j stores the probability that subject
i is looking at subject j for each of the four sub-
jects and the last column corresponds to the white-
board/projector screen (i.e., entry i, j where j = 5
is the probability that subject i is looking at the
screen). Gaze probability G(i, j) is defined as
G(i, j) = G0e
??i,j2/?2
where ?i,j is the angular difference between the
gaze of subject i and the direction defined by the
location of subjects i and j. G0 is a normalization
factor such that
?
j G(i, j) = 1 and ? is a user-
defined constant (in our experiments, we chose
? = 15 degrees).
Using the gaze probability matrix, a 4 ? 1 per-
frame mutual gaze vector was computed that for
entry i stores the probability that the speaker and
subject i are looking at one another.
In order to create features equivalent to those
described in Section 4.1, we first collapse the
frame-level probability matrix into a matrix of bi-
nary values. We convert the probability for each
frame into a binary judgement of whether subject
i is looking at target j:
H(i, j) = ?G(i, j)
? is a binary value to evaluate G(i, j) > ?, where
? is a high-pass thresholding value - or ?gaze prob-
ability threshold? (GPT) - between 0 and 1.
Once we have a frame-level matrix of binary
values, for each subject i, we compute GDP val-
ues for the time periods of interest, and in each
case, choose the target with the highest GDP as the
candidate. Hence, we compute a candidate target
for the utterance overall, for each third of the ut-
terance, and for the period -/+ 2 seconds from the
276
you start time, and in addition, we compute a can-
didate participant for mutual gaze with the speaker
for the utterance overall.
We sought to use the GPT threshold which pro-
duces automatic visual features that agree best
with the features derived from the FOA annota-
tions. Hence we experimented with different GPT
values in increments of 0.1, and compared the re-
sulting features to the manual features using the
kappa statistic. A threshold of 0.6 gave the best
kappa scores, which ranged from 20% to 44%.6
5 Linguistic Information
Our set of discourse features is a simplified ver-
sion of those employed by Galley et al (2004) and
Gupta et al (2007a). It contains three main types
(summarized in Table 4):
? Sentential features (1 to 13) encode structural,
durational, lexical and shallow syntactic patterns
of the you-utterance. Feature 13 is extracted us-
ing the AMI ?Named Entity? annotations and in-
dicates whether a particular participant is men-
tioned in the you-utterance. Apart from this fea-
ture, all other sentential features are automatically
extracted, and besides 1, 8, 9, and 10, they are all
binary.
? Backward Looking (BL)/Forward Looking (FL)
features (14 to 22) are mostly extracted from ut-
terance pairs, namely the you-utterance and the
BL/FL (previous/next) utterance by each listener
Li (potential addressee). We also include a few
extra features which are not computed in terms of
utterance pairs. These indicate the number of par-
ticipants that speak during the previous and next 5
utterances, and the BL and FL speaker order. All
of these features are computed automatically.
? Dialogue Act (DA) features (23 to 24) use the
manual AMI dialogue act annotations to represent
the conversational function of the you-utterance
and the BL/FL utterance by each potential ad-
dressee. Along with the sentential feature based
on the AMI Named Entity annotations, these are
the only discourse features which are not com-
puted automatically. 7
6The fact that our gaze estimator is getting any useful
agreement with respect to these annotations is encouraging
and suggests that an improved tracker and/or one that adapts
to the user more effectively could work very well.
7Since we use the manual transcripts of the meetings, the
transcribed words and the segmentation into utterances or di-
alogue acts are of course not given automatically. A fully
automatic approach would involve using ASR output instead
of manual transcriptions? something which we attempt in
(1) # of you pronouns
(2) you (say|said|tell|told| mention(ed)|mean(t)|
sound(ed))
(3) auxiliary you
(4) wh-word you
(5) you guys
(6) if you
(7) you know
(8) # of words in you-utterance
(9) duration of you-utterance
(10) speech rate of you-utterance
(11) 1st person
(12) general case
(13) person Named Entity tag
(14) # of utterances between you- and BL/FL utt.
(15) # of speakers between you- and BL/FL utt.
(16) overlap between you- and BL/FL utt. (binary)
(17) duration of overlap between you- and BL/FL utt.
(18) time separation between you- and BL/FL utt.
(19) ratio of words in you- that are in BL/FL utt.
(20) # of participants that speak during prev. 5 utt.
(21) # of participants that speak during next 5 utt.
(22) speaker order BL/FL
(23) dialogue act of the you-utterance
(24) dialogue act of the BL/FL utterance
Table 4: Discourse Features
6 First Set of Experiments & Results
In this section we report our experiments and re-
sults when using manual transcriptions and anno-
tations. In Section 7 we will present the results
obtained using ASR output and automatically ex-
tracted visual information. All experiments (here
and in the next section) are performed using a
Bayesian Network classifier with 10-fold cross-
validation.8 In each task, we give raw overall ac-
curacy results and then F-scores for each of the
classes. We computed measures of information
gain in order to assess the predictive power of the
various features, and did some experimentation
with Correlation-based Feature Selection (CFS)
(Hall, 2000).
6.1 Generic vs. Referential Uses of You
We first address the task of distinguishing between
generic and referential uses of you.
Baseline. A majority class baseline that classi-
fies all instances of you as referential yields an ac-
curacy of 50.86% (see Table 1).
Results. A summary of the results is given in Ta-
ble 5. Using discourse features only we achieve
an accuracy of 77.77%, while using multimodal
Section 7.
8We use the the BayesNet classifier implemented in the
Weka toolkit http://www.cs.waikato.ac.nz/ml/weka/.
277
Features Acc F1-Gen F1-Ref
Baseline 50.86 0 67.4
Discourse 77.77 78.8 76.6
Visual 60.32 64.2 55.5
MM 79.02 80.2 77.7
Dis w/o FL 78.34 79.1 77.5
MM w/o FL 78.22 79.0 77.4
Dis w/o DA 69.44 71.5 67.0
MM w/o DA 72.75 74.4 70.9
Table 5: Generic vs. referential uses
(MM) yields 79.02%, but this increase is not sta-
tistically significant.
In spite of this, visual features do help to dis-
tinguish between generic and referential uses -
note that the visual features alone are able to beat
the baseline (p < .005). The listeners? gaze is
more predictive than the speaker?s: if listeners
look mostly at the white-board/projector screen in-
stead of another participant, then the you is more
likely to be referential. More will be said on this
in Section 6.2.1 in the analysis of the results for
the singular vs. plural referential task.
We found sentential features of the you-
utterance to be amongst the best predictors, es-
pecially those that refer to surface lexical proper-
ties, such as features 1, 11, 12 and 13 in Table 4.
Dialogue act features provide useful information
as well. As pointed out by Gupta et al (2007b;
2007a), a you pronoun within a question (e.g.
an utterance tagged as elicit-assess or
elicit-inform) is more likely to be referen-
tial. Eliminating information about dialogue acts
(w/o DA) brings down performance (p < .005),
although accuracy remains well above the baseline
(p < .001). Note that the small changes in perfor-
mance when FL information is taken out (w/o FL)
are not statistically significant.
6.2 Reference Resolution
We now turn to the referential instances of you,
which can be resolved by determining the ad-
dressee(s) of the given utterance.
6.2.1 Singular vs. Plural Reference
We start by trying to discriminate singular vs. plu-
ral interpretations. For this, we use a two-way
classification scheme that distinguishes between
individual and group addressing. To our knowl-
edge, this is the first attempt at this task using lin-
guistic information.9
9But see e.g. (Takemae et al, 2004) for an approach that
uses manually extracted visual-only clues with similar aims.
Baseline. A majority class baseline that consid-
ers all instances of you as referring to an individual
addressee gives 67.92% accuracy (see Table 1).
Results. A summary of the results is shown in
Table 6. There is no statistically significant differ-
ence between the baseline and the results obtained
when visual features are used alone (67.92% vs.
66.28%). However, we found that visual informa-
tion did contribute to identifying some instances of
plural addressing, as shown by the F-score for that
class. Furthermore, the visual features helped to
improve results when combined with discourse in-
formation: using multimodal (MM) features pro-
duces higher results than the discourse-only fea-
ture set (p < .005), and increases from 74.24% to
77.05% with CFS.
As in the generic vs. referential task, the white-
board/projector screen value for the listeners? gaze
features seems to have discriminative power -
when listeners? gaze features take this value, it is
often indicative of a plural rather than a singular
you. It seems then, that in our data-set, the speaker
often uses the white-board/projector screen when
addressing the group, and hence draws the listen-
ers? gaze in this direction. We should also note
that the ratio features which we thought might be
useful here (see Section 4.1) did not prove so.
Amongst the most useful discourse features
are those that encode similarity relations between
the you-utterance and an utterance by a potential
addressee. Utterances by individual addressees
tend to be more lexically cohesive with the you-
utterance and so if features such as feature 19 in
Table 4 indicate a low level of lexical similarity,
then this increases the likelihood of plural address-
ing. Sentential features that refer to surface lexical
patterns (features 6, 7, 11 and 12) also contribute
to improved results, as does feature 21 (number of
speakers during the next five utterances) - fewer
speaker changes correlates with plural addressing.
Information about dialogue acts also plays a
role in distinguishing between singular and plu-
ral interpretations. Questions tend to be addressed
to individual participants, while statements show a
stronger correlation with plural addressees. When
no DA features are used (w/o DA), the drop in per-
formance for the multimodal classifier to 71.19%
is statistically significant (p < .05). As for the
generic vs. referential task, FL information does
not have a significant effect on performance.
278
Features Acc F1-Sing. F1-Pl.
Baseline 67.92 80.9 0
Discourse 71.19 78.9 54.6
Visual 66.28 74.8 48.9
MM* 77.05 83.3 63.2
Dis w/o FL 72.13 80.1 53.7
MM w/o FL 72.60 79.7 58.1
Dis w/o DA 68.38 78.5 40.5
MM w/o DA 71.19 78.8 55.3
Table 6: Singular vs. plural reference; * = with Correlation-
based Feature Selection (CFS).
6.2.2 Detection of Individual Addressees
We now turn to resolving the singular referential
uses of you. Here we must detect the individual
addressee of the utterance that contains the pro-
noun.
Baselines. Given the distribution shown in Ta-
ble 2, a majority class baseline yields an accu-
racy of 35.17%. An off-line system that has access
to future context could implement a next-speaker
baseline that always considers the next speaker to
be the intended addressee, so yielding a high raw
accuracy of 71.03%. A previous-speaker base-
line that does not require access to future context
achieves 35% raw accuracy.
Results. Table 7 shows a summary of the re-
sults, and these all outperform the majority class
(MC) and previous-speaker baselines. When all
discourse features are available, adding visual in-
formation does improve performance (74.48% vs.
60.69%, p < .005), and with CFS, this increases
further to 80.34% (p < .005). Using discourse or
visual features alone gives scores that are below
the next-speaker baseline (60.69% and 65.52% vs.
71.03%). Taking all forward-looking (FL) infor-
mation away reduces performance (p < .05), but
the small increase in accuracy caused by taking
away dialogue act information is not statistically
significant.
When we investigated individual feature contri-
bution, we found that the most predictive features
were the FL and backward-looking (BL) speaker
order, and the speaker?s visual features (including
mutual gaze). Whomever the speaker spent most
time looking at or engaged in a mutual gaze with
was more likely to be the addressee. All of the vi-
sual features had some degree of predictive power
apart from the ratio features. Of the other BL/FL
discourse features, features 14, 18 and 19 (see Ta-
ble 4) were more predictive. These indicate that
utterances spoken by the intended addressee are
Features Acc F1-L1 F1-L2 F1-L3
MC baseline 35.17 52.0 0 0
Discourse 60.69 59.1 60.0 62.7
Visual 65.52 69.1 63.5 64.0
MM* 80.34 80.0 82.4 79.0
Dis w/o FL 52.41 50.7 51.8 54.5
MM w/o FL 66.55 68.7 62.7 67.6
Dis w/o DA 61.03 58.5 59.9 64.2
MM w/o DA 73.10 72.4 69.5 72.0
Table 7: Addressee detection for singular references; * =
with Correlation-based Feature Selection (CFS).
often adjacent to the you-utterance and lexically
similar.
7 A Fully Automatic Approach
In this section we describe experiments which
use features derived from ASR transcriptions and
automatically-extracted visual information. We
used SRI?s Decipher (Stolcke et al, 2008)10 in or-
der to generate ASR transcriptions, and applied
the head-tracker described in Section 4.2 to the
relevant portions of video in order to extract the
visual information. Recall that the Named Entity
features (feature 13) and the DA features used in
our previous experiments had been manually an-
notated, and hence are not used here. We again
divide the problem into the same three separate
tasks: we first discriminate between generic and
referential uses of you, then singular vs. plural
referential uses, and finally we resolve the ad-
dressee for singular uses. As before, all exper-
iments are performed using a Bayesian Network
classifier and 10-fold cross validation.
7.1 Results
For each of the three tasks, Figure 7 compares
the accuracy results obtained using the fully-
automatic approach with those reported in Section
6. The figure shows results for the majority class
baselines (MCBs), and with discourse-only (Dis),
and multimodal (MM) feature sets. Note that the
data set for the automatic approach is smaller,
and that the majority class baselines have changed
slightly. This is because of differences in the ut-
terance segmentation, and also because not all of
the video sections around the you utterances were
processed by the head-tracker.
In all three tasks we are able to significantly
outperform the majority class baseline, but the vi-
sual features only produce a significant improve-
10Stolcke et al (2008) report a word error rate of 26.9% on
AMI meetings.
279
Figure 1: Results for the manual and automatic systems; MCB = majority class baseline, Dis = discourse features, MM =
multimodal, * = with Correlation-based Feature Selection (CFS), FL = forward-looking, man = manual, auto = automatic.
ment in the individual addressee resolution task.
For the generic vs. referential task, the discourse
and multimodal classifiers both outperform the
majority class baseline (p < .001), achieving
accuracy scores of 68.71% and 68.48% respec-
tively. In contrast to when using manual transcrip-
tions and annotations (see Section 6.1), removing
forward-looking (FL) information reduces perfor-
mance (p < .05). For the referential singular
vs. plural task, the discourse and multimodal with
CFS classifier improve over the majority class
baseline (p < .05). Multimodal with CFS does
not improve over the discourse classifier - indeed
without feature selection, the addition of visual
features causes a drop in performance (p < .05).
Here, taking away FL information does not cause
a significant reduction in performance. Finally,
in the individual addressee resolution task, the
discourse, visual (60.78%) and multimodal clas-
sifiers all outperform the majority class baseline
(p < .005, p < .001 and p < .001 respec-
tively). Here the addition of visual features causes
the multimodal classifier to outperform the dis-
course classifier in raw accuracy by nearly ten per-
centage points (67.32% vs. 58.17%, p < .05), and
with CFS, the score increases further to 74.51%
(p < .05). Taking away FL information does
cause a significant drop in performance (p < .05).
8 Conclusions
We have investigated the automatic resolution of
the second person English pronoun you in multi-
party dialogue, using a combination of linguistic
and visual features. We conducted a first set of
experiments where our features were derived from
manual transcriptions and annotations, and then a
second set where they were generated by entirely
automatic means. To our knowledge, this is the
first attempt at tackling this problem using auto-
matically extracted multimodal information.
Our experiments showed that visual informa-
tion can be highly predictive in resolving the ad-
dressee of singular referential uses of you. Visual
features significantly improved the performance of
both our manual and automatic systems, and the
latter achieved an encouraging 75% accuracy. We
also found that our visual features had predictive
power for distinguishing between generic and ref-
erential uses of you, and between referential sin-
gulars and plurals. Indeed, for the latter task,
they significantly improved the manual system?s
performance. The listeners? gaze features were
useful here: in our data set it was apparently the
case that the speaker would often use the white-
board/projector screen when addressing the group,
thus drawing the listeners? gaze in this direction.
Future work will involve expanding our data-
set, and investigating new potentially predictive
features. In the slightly longer term, we plan to
integrate the resulting system into a meeting as-
sistant whose purpose is to automatically extract
useful information from multi-party meetings.
280
References
Ron Arstein and Massimo Poesio. 2006. Identifying
reference to abstract objects in dialogue. In Pro-
ceedings of the 10th Workshop on the Semantics and
Pragmatics of Dialogue (Brandial?06), pages 56?
63, Potsdam, Germany.
Ilse Bakx, Koen van Turnhout, and Jacques Terken.
2003. Facial orientation during multi-party inter-
action with information kiosks. In Proceedings of
INTERACT, Zurich, Switzerland.
Donna Byron. 2004. Resolving pronominal refer-
ence to abstract entities. Ph.D. thesis, University
of Rochester, Department of Computer Science.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
ment and disagreement in conversational speech:
Use of Bayesian networks to model pragmatic de-
pendencies. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Surabhi Gupta, John Niekrasz, Matthew Purver, and
Daniel Jurafsky. 2007a. Resolving ?you? in multi-
party dialog. In Proceedings of the 8th SIGdial
Workshop on Discourse and Dialogue, Antwerp,
Belgium, September.
Surabhi Gupta, Matthew Purver, and Daniel Jurafsky.
2007b. Disambiguating between generic and refer-
ential ?you? in dialog. In Proceedings of the 45th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL).
Mark Hall. 2000. Correlation-based Feature Selection
for Machine Learning. Ph.D. thesis, University of
Waikato.
Natasa Jovanovic, Rieks op den Akker, and Anton Ni-
jholt. 2006a. Addressee identification in face-to-
face meetings. In Proceedings of the 11th Confer-
ence of the European Chapter of the ACL (EACL),
pages 169?176, Trento, Italy.
Natasa Jovanovic, Rieks op den Akker, and Anton Ni-
jholt. 2006b. A corpus for studying addressing
behaviour in multi-party dialogues. Language Re-
sources and Evaluation, 40(1):5?23. ISSN=1574-
020X.
Natasa Jovanovic. 2007. To Whom It May Concern -
Addressee Identification in Face-to-Face Meetings.
Ph.D. thesis, University of Twente, Enschede, The
Netherlands.
Michael Katzenmaier, Rainer Stiefelhagen, and Tanja
Schultz. 2004. Identifying the addressee in human-
human-robot interactions based on head pose and
speech. In Proceedings of the 6th International
Conference on Multimodal Interfaces, pages 144?
151, State College, Pennsylvania.
Iain McCowan, Jean Carletta, W. Kraaij, S. Ashby,
S. Bourban, M. Flynn, M. Guillemot, T. Hain,
J. Kadlec, V. Karaiskos, M. Kronenthal, G. Lathoud,
M. Lincoln, A. Lisowska, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meeting Corpus. In
Proceedings of Measuring Behavior, the 5th Inter-
national Conference on Methods and Techniques in
Behavioral Research, Wageningen, Netherlands.
Christoph Mu?ller. 2006. Automatic detection of non-
referential It in spoken multi-party dialog. In Pro-
ceedings of the 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), pages 49?56, Trento, Italy.
Christoph Mu?ller. 2007. Resolving it, this, and that
in unrestricted multi-party dialog. In Proceedings
of the 45th Annual Meeting of the Association for
Computational Linguistics, pages 816?823, Prague,
Czech Republic.
Andreas Stolcke, Xavier Anguera, Kofi Boakye, O?zgu?r
C?etin, Adam Janin, Matthew Magimai-Doss, Chuck
Wooters, and Jing Zheng. 2008. The icsi-sri spring
2007 meeting and lecture recognition system. In
Proceedings of CLEAR 2007 and RT2007. Springer
Lecture Notes on Computer Science.
Michael Strube and Christoph Mu?ller. 2003. A ma-
chine learning approach to pronoun resolution in
spoken dialogue. In Proceedings of ACL?03, pages
168?175.
Yoshinao Takemae, Kazuhiro Otsuka, and Naoki
Mukawa. 2004. An analysis of speakers? gaze
behaviour for automatic addressee identification in
multiparty conversation and its application to video
editing. In Proceedings of IEEE Workshop on Robot
and Human Interactive Communication, pages 581?
586.
Koen van Turnhout, Jacques Terken, Ilse Bakx, and
Berry Eggen. 2005. Identifying the intended
addressee in mixed human-humand and human-
computer interaction from non-verbal features. In
Proceedings of ICMI, Trento, Italy.
Bonnie Webber. 1991. Structure and ostension in
the interpretation of discourse deixi. Language and
Cognitive Processes, 6(2):107?135.
281
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 156?163,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Modelling and Detecting Decisions in Multi-party Dialogue
Raquel Ferna?ndez, Matthew Frampton, Patrick Ehlen, Matthew Purver, and Stanley Peters
Center for the Study of Language and Information
Stanford University
{raquel|frampton|ehlen|mpurver|peters}@stanford.edu
Abstract
We describe a process for automatically de-
tecting decision-making sub-dialogues in tran-
scripts of multi-party, human-human meet-
ings. Extending our previous work on ac-
tion item identification, we propose a struc-
tured approach that takes into account the dif-
ferent roles utterances play in the decision-
making process. We show that this structured
approach outperforms the accuracy achieved
by existing decision detection systems based
on flat annotations, while enabling the extrac-
tion of more fine-grained information that can
be used for summarization and reporting.
1 Introduction
In collaborative and organized work environments,
people share information and make decisions exten-
sively through multi-party conversations, usually in
the form of meetings. When audio or video record-
ings are made of these meetings, it would be valu-
able to extract important information, such as the
decisions that were made and the trains of reason-
ing that led to those decisions. Such a capability
would allow work groups to keep track of courses
of action that were shelved or rejected, and could al-
low new team members to get quickly up to speed.
Thanks to the recent availability of substantial meet-
ing corpora?such as the ISL (Burger et al, 2002),
ICSI (Janin et al, 2004), and AMI (McCowan et
al., 2005) Meeting Corpora?current research on the
structure of decision-making dialogue and its use for
automatic decision detection has helped to bring this
vision closer to reality (Verbree et al, 2006; Hsueh
and Moore, 2007b).
Our aim here is to further that research by ap-
plying a simple notion of dialogue structure to the
task of automatically detecting decisions in multi-
party dialogue. A central hypothesis underlying our
approach is that this task is best addressed by tak-
ing into account the roles that different utterances
play in the decision-making process. Our claim is
that this approach facilitates both the detection of
regions of discourse where decisions are discussed
and adopted, and also the identification of important
aspects of the decision discussions themselves, thus
opening the way to better and more concise report-
ing.
In the next section, we describe prior work on re-
lated efforts, including our own work on action item
detection (Purver et al, 2007). Sections 3 and 4 then
present our decision annotation scheme, which dis-
tinguishes several types of decision-related dialogue
acts (DAs), and the corpus used as data (in this study
a section of the AMI Meeting Corpus). Next, in Sec-
tion 5, we describe our experimental methodology,
including the basic conception of our classification
approach, the features we used in classification, and
our evaluation metrics. Section 6 then presents our
results, obtained with a hierarchical classifier that
first trains individual sub-classifiers to detect the dif-
ferent types of decision DAs, and then uses a super-
classifier to detect decision regions on the basis of
patterns of these DAs, achieving an F-score of 58%.
Finally, Section 7 presents some conclusions and di-
rections for future work.
2 Related Work
Recent years have seen an increasing interest in re-
search on decision-making dialogue. To a great
extent, this is due to the fact that decisions have
156
been shown to be a key aspect of meeting speech.
User studies (Lisowska et al, 2004; Banerjee et al,
2005) have shown that participants regard decisions
as one of the most important outputs of a meeting,
while Whittaker et al (2006) found that the develop-
ment of an automatic decision detection component
is critical to the re-use of meeting archives. Identify-
ing decision-making regions in meeting transcripts
can thus be expected to support development of a
wide range of applications, such as automatic meet-
ing assistants that process, understand, summarize
and report the output of meetings; meeting tracking
systems that assist in implementing decisions; and
group decision support systems that, for instance,
help in constructing group memory (Romano and
Nunamaker, 2001; Post et al, 2004; Voss et al,
2007).
Previously researchers have focused on the in-
teractive aspects of argumentative and decision-
making dialogue, tackling issues such as the detec-
tion of agreement and disagreement and the level
of emotional involvement of conversational partic-
ipants (Hillard et al, 2003; Wrede and Shriberg,
2003; Galley et al, 2004; Gatica-Perez et al, 2005).
From a perhaps more formal perspective, Verbree et
al. (2006) have created an argumentation scheme in-
tended to support automatic production of argument
structure diagrams from decision-oriented meeting
transcripts. Only Hsueh and Moore (2007a; 2007b),
however, have specifically investigated the auto-
matic detection of decisions.
Using the AMI Meeting Corpus, Hsueh and
Moore (2007b) attempt to identify the dialogue acts
(DAs) in a meeting transcript that are ?decision-
related?. The authors define these DAs on the ba-
sis of two kinds of manually created summaries: an
extractive summary of the whole meeting, and an
abstractive summary of the decisions made in the
meeting. Those DAs in the extractive summary that
support any of the decisions in the abstractive sum-
mary are then manually tagged as decision-related
DAs. They trained a Maximum Entropy classifier
to recognize this single DA class, using a variety of
lexical, prosodic, dialogue act and topical features.
The F-score they achieved was 0.35, which gives a
good indication of the difficulty of this task.
In our previous work (Purver et al, 2007), we at-
tempted to detect a particular kind of decision com-
mon in meetings, namely action items?public com-
mitments to perform a given task. In contrast to
the approach adopted by Hsueh and Moore (2007b),
we proposed a hierarchical approach where indi-
vidual classifiers were trained to detect distinct ac-
tion item-related DA classes (task description, time-
frame, ownership and agreement) followed by a
super-classifier trained on the hypothesized class la-
bels and confidence scores from the individual clas-
sifiers that would detect clusters of multiple classes.
We showed that this structured approach produced
better classification accuracy (around 0.39 F-score
on the task of detecting action item regions) than a
flat-classifier baseline trained on a single action item
DA class (around 0.35 F-score).
In this paper we extend this approach to the more
general task of detecting decisions, hypothesizing
that?as with action items?the dialogue acts in-
volved in decision-making dialogue form a rather
heterogeneous set, whose members co-occur in par-
ticular kinds of patterns, and that exploiting this
richer structure can facilitate their detection.
3 Decision Dialogue Acts
We are interested in identifying the main conver-
sational units in a decision-making process. We ex-
pect that identifying these units will help in detect-
ing regions of dialogue where decisions are made
(decision sub-dialogues), while also contributing to
identification and extraction of specific decision-
related bits of information.
Decision-making dialogue can be complex, often
involving detailed discussions with complicated ar-
gumentative structure (Verbree et al, 2006). Deci-
sion sub-dialogues can thus include a great deal of
information that is potentially worth extracting. For
instance, we may be interested in knowing what a
decision is about, what alternative proposals were
considered during the decision process, what argu-
ments were given for and against each of them, and
last but not least, what the final resolution was.
Extracting these and other potential decision com-
ponents is a challenging task, which we do not in-
tend to fully address in this paper. This initial study
concentrates on three main components we believe
constitute the backbone of decision sub-dialogues.
A typical decision sub-dialogue consists of three
main components that often unfold in sequence. (a)
157
key DDA class description
I issue utterances introducing the issue or topic under discussion
R resolution utterances containing the decision that is adopted
RP ? proposal ? utterances where the decision adopted is proposed
RR ? restatement ? utterances where the decision adopted is confirmed or restated
A agreement utterances explicitly signalling agreement with the decision made
Table 1: Set of decision dialogue act (DDA) classes
A topic or issue that requires some sort of conclu-
sion is initially raised. (b) One or more proposals are
considered. And (c) once some sort of agreement is
reached upon a particular resolution, a decision is
adopted.
Dialogue act taxonomies often include tags
that can be decision-related. For instance, the
DAMSL taxonomy (Core and Allen, 1997) in-
cludes the tags agreement and commit, as well
as a tag open-option for utterances that ?sug-
gest a course of action?. Similarly, the AMI
DA scheme1 incorporates tags like suggest,
elicit-offer-or-suggestion and assess.
These tags are however very general and do not cap-
ture the distinction between decisions and more gen-
eral suggestions and commitments.2 We therefore
devised a decision annotation scheme that classifies
utterances according to the role they play in the pro-
cess of formulating and agreeing on a decision. Our
scheme distinguishes among three main decision di-
alogue act (DDA) classes: issue (I), resolution (R),
and agreement (A). Class R is further subdivided into
resolution proposal (RP) and resolution restatement
(RR). A summary of the classes is given in Table 1.
Annotation of the issue class includes any utter-
ances that introduce the topic of the decision discus-
sion. For instance, in example (1) below, the utter-
ances ?Are we going to have a backup?? and ?But
would a backup really be necessary?? are tagged as
I. The classes RP and RR are used to annotate those
utterances that specify the resolution adopted?i.e.
the decision made. Annotation with the class RP
includes any utterances where the resolution is ini-
1A full description of the AMI Meeting Corpus DA scheme
is available at http://mmm.idiap.ch/private/ami/
annotation/dialogue acts manual 1.0.pdf, after
free registration.
2Although they can of course be used to aid the identification
process?see Section 5.3.
tially proposed (like the utterance ?I think maybe we
could just go for the kinetic energy. . . ?). Sometimes
decision discussions include utterances that sum up
the resolution adopted, like the utterance ?Okay,
fully kinetic energy? in (1). This kind of utterance
is tagged with the class RR. Finally, the agreement
class includes any utterances in which participants
agree with the (proposed) resolution, like the utter-
ances ?Yeah? and ?Good? as well as ?Okay? in di-
alogue (1).
(1) A: Are we going to have a backup?
Or we do just?
B: But would a backup really be necessary?
A: I think maybe we could just go for the
kinetic energy and be bold and innovative.
C: Yeah.
B: I think? yeah.
A: It could even be one of our selling points.
C: Yeah ?laugh?.
D: Environmentally conscious or something.
A: Yeah.
B: Okay, fully kinetic energy.
D: Good.3
Note that an utterance can be assigned to more
than one of these classes. For instance, the utter-
ance ?Okay, fully kinetic energy? is annotated both
as RR and A. Similarly, each decision sub-dialogue
may contain more than one utterance corresponding
to each class, as we saw above for issue. While
we do not a priori require each of these classes to
be present for a set of utterances to be considered
a decision sub-dialogue, all annotated decision sub-
dialogues in our corpus include the classes I, RP and
A. The annotation process and results are described
in detail in the next section.
3This example was extracted from the AMI dialogue
ES2015c and has been modified slightly for presentation pur-
poses.
158
4 Data: Corpus & Annotation
In this study, we use 17 meetings from the AMI
Meeting Corpus (McCowan et al, 2005), a pub-
licly available corpus of multi-party meetings con-
taining both audio recordings and manual transcrip-
tions, as well as a wide range of annotated infor-
mation including dialogue acts and topic segmenta-
tion. Conversations are all in English, but they can
include native and non-native English speakers. All
meetings in our sub-corpus are driven by an elicita-
tion scenario, wherein four participants play the role
of project manager, marketing expert, interface de-
signer, and industrial designer in a company?s de-
sign team. The overall sub-corpus makes up a total
of 15,680 utterances/dialogue acts (approximately
920 per meeting). Each meeting lasts around 30
minutes.
Two authors annotated 9 and 10 dialogues each,
overlapping on two dialogues. Inter-annotator
agreement on these two dialogues was similar to
(Purver et al, 2007), with kappa values ranging
from 0.63 to 0.73 for the four DDA classes. The
highest agreement was obtained for class RP and the
lowest for class A.4
On average, each meeting contains around 40
DAs tagged with one or more of the DDA sub-
classes in Table 1. DDAs are thus very sparse, cor-
responding to only 4.3% of utterances. When we
look at the individual DDA sub-classes this is even
more pronounced. Utterances tagged as issue make
up less than 0.9% of utterances in a meeting, while
utterances annotated as resolution make up around
1.4%?1% corresponding to RP and less than 0.4%
to RR on average. Almost half of DDA utterances
(slightly over 2% of all utterances on average) are
tagged as belonging to class agreement.
We compared our annotations with the annota-
tions of Hsueh and Moore (2007b) for the 17 meet-
ings of our sub-corpus. The overall number of ut-
terances annotated as decision-related is similar in
the two studies: 40 vs. 30 utterances per meeting on
average, respectively. However, the overlap of the
annotations is very small leading to negative kappa
scores. As shown in Figure 1, only 12.22% of ut-
4The annotation guidelines we used are available on-
line at http://godel.stanford.edu/twiki/bin/
view/Calo/CaloDecisionDiscussionSchema

  









  !"
##Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 235?243,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Extracting decisions from multi-party dialogue using directed graphical
models and semantic similarity
Trung H. Bui1, Matthew Frampton1, John Dowding2, and Stanley Peters1
1Center for the Study of Language and Information, Stanford University
{thbui|frampton|peters}@stanford.edu
2University of California/Santa Cruz
jdowding@ucsc.edu
Abstract
We use directed graphical models (DGMs)
to automatically detect decision discus-
sions in multi-party dialogue. Our ap-
proach distinguishes between different di-
alogue act (DA) types based on their role
in the formulation of a decision. DGMs
enable us to model dependencies, includ-
ing sequential ones. We summarize deci-
sions by extracting suitable phrases from
DAs that concern the issue under discus-
sion and its resolution. Here we use a
semantic-similarity metric to improve re-
sults on both manual and ASR transcripts.
1 Introduction
In work environments, people share information
and make decisions in multi-party conversations
known as meetings. The demand for systems that
can automatically process, understand and sum-
marize information contained in audio and video
recordings of meetings is growing rapidly. Our
own research, and that of other contemporary
projects (Janin et al, 2004), aim at meeting this
demand.
At present, we are focusing on the automatic
detection and summarization of decision discus-
sions. Our approach for detecting decision dis-
cussions involves distinguishing between differ-
ent dialogue act (DA) types based on their role
in the decision-making process. Two of these
types are DAs which describe the Issue under dis-
cussion, and DAs which describe its Resolution.
To summarize a decision discussion, we identify
words and phrases in the Issue and Resolution
DAs, which can be used to produce a concise, de-
scriptive summary.
This paper describes new experiments in both
detecting and summarizing decision discussions.
In the detection stage, we investigate the use of
Directed Graphical Models (DGMs). DGMs are
attractive because they can be used to model se-
quence and dependencies between predictor vari-
ables. In the summarization stage, we attempt to
improve phrase selection with a new feature that
measures the level of semantic similarity between
candidate Issue phrases and Resolution utterances,
and vice-versa. The feature is generated by a
semantic-similarity metric which uses WordNet as
a knowledge source. The motivation is that ordi-
narily, the Issue and Resolution components in a
decision summary should be semantically similar.
The paper proceeds as follows. Firstly, Sec-
tion 2 describes related work, and Section 3, our
data-set and annotation scheme for decision dis-
cussions. Section 4 then reports our decision de-
tection experiments using DGMs, and Section 5,
the summarization experiments. Finally, Section
6 draws conclusions and proposes ideas for future
work.
2 Related Work
User studies (Banerjee et al, 2005) have con-
firmed that meeting participants consider deci-
sions to be one of the most important meeting
outputs, and (Whittaker et al, 2006) found that
the development of an automatic decision detec-
tion component is critical to the re-use of meet-
ing archives. With the new availability of substan-
tial meeting corpora such as the AMI corpus (Mc-
Cowan et al, 2005), recent years have therefore
seen an increasing amount of research on decision-
making dialog. This research has tackled issues
such as the automatic detection of agreement and
disagreement (Galley et al, 2004), and of the
235
level of involvement of conversational participants
(Gatica-Perez et al, 2005). In addition, (Verbree
et al, 2006) created an argumentation scheme in-
tended to support automatic production of argu-
ment structure diagrams from decision-oriented
meeting transcripts. As yet, there has been rela-
tively little work which specifically addresses the
automatic detection and summarization of deci-
sions.
Decision discussion detection: (Hsueh and
Moore, 2007) used the AMI Meeting Corpus, and
attempted to automatically identify DAs in meet-
ing transcripts which are ?decision-related?. For
each meeting, two manually created summaries
were used to judge which DAs were decision-
related: an extractive summary of the whole meet-
ing, and an abstractive summary of its decisions.
Those DAs in the extractive summary which sup-
port any of the decisions in the abstractive sum-
mary were manually tagged as decision-related.
(Hsueh and Moore, 2007) then trained a Maxi-
mum Entropy classifier to recognize this single
DA class, using a variety of lexical, prosodic, DA
and conversational topic features. They achieved
an F-score of 0.35.
Unlike (Hsueh and Moore, 2007), (Ferna?ndez et
al., 2008b) made an attempt at modelling the struc-
ture of decision-making dialogue. The authors de-
signed an annotation scheme that takes account of
the different roles which utterances can play in the
decision-making process?for example it distin-
guishes between DDAs (decision DAs) which ini-
tiate a discussion by raising an issue, those which
propose a resolution, and those which express
agreement for a proposed resolution. The authors
annotated a portion of the AMI corpus, and then
applied what they refer to as ?hierarchical classi-
fication?. Here, one sub-classifier per DDA class
hypothesizes occurrences of that DDA class, and
then based on these hypotheses, a super-classifier
determines which regions of dialogue are deci-
sion discussions. All of the classifiers, (sub and
super), were linear kernel binary Support Vec-
tor Machines (SVMs). Results were better than
those obtained with (Hsueh and Moore, 2007)?s
approach?the F1-score for detecting decision dis-
cussions in manual transcripts was .58 vs. .50.
Note that (Purver et al, 2007) had previously pur-
sued the same basic approach as (Ferna?ndez et al,
2008b) in order to detect action items.
In this paper, we build on the promising results
of (Ferna?ndez et al, 2008b), by using Directed
Graphical Models (DGMs) in place of SVMs.
DGMs are attractive because they provide a natu-
ral framework for modelling sequence and depen-
dencies between variables including the DDAs.
We are especially interested in whether DGMs
better exploit non-lexical features. (Ferna?ndez et
al., 2008b) obtained much more value from lexi-
cal than non-lexical features (and indeed no value
at all from prosodic features), but lexical features
have disadvantages. In particular, they can be do-
main specific, increase the size of the feature space
dramatically, and deteriorate more than other fea-
tures in quality when ASR is poor.
Decision summarization: Recent years have
seen research on spoken dialogue summarization
(e.g. (Zechner, 2002)). Most has attempted to gen-
erate summaries of full dialogues, but some very
recent research has focused on specific dialogue
events, namely action items (Purver et al, 2007),
and decisions (Ferna?ndez et al, 2008a).
(Ferna?ndez et al, 2008a) used the DDA an-
notation scheme mentioned above, and began by
extracting the DDAs which raise issues or pro-
vide accepted resolutions. Only manual tran-
scripts were used and the DDAs were extracted
by hand rather than automatically. The next step
was to parse each DDA with a general rule-based
parser (Dowding et al, 1993), producing multi-
ple short fragments rather than one full utterance
parse. Then, for each DDA, an SVM regression
model used various features (including parse, se-
mantic and lexical features) to select the fragment
which was most likely to appear in a gold-standard
extractive decision summary. The entire manual
utterance transcriptions were used as the baseline,
and although the SVM?s precision was high, it was
not enough to offset the baseline?s perfect recall,
and so its F-score was lower. The ?Oracle?, which
always chooses the fragment with the highest F1-
score produced very good results. This motivates
deeper investigation into how to improve the frag-
ment/parse selection phase, and so we assess the
usefulness of a semantic-similarity feature for the
SVM. We conduct experiments with ASR as well
as manual transcripts.
3 Data
For the experiments reported in this study, we used
17 meetings from the AMI Meeting Corpus (Mc-
Cowan et al, 2005), a freely available corpus of
236
multi-party meetings with both audio and video
recordings, and a wide range of annotated in-
formation including DAs and topic segmentation.
Conversations are in English, but some partici-
pants are non-native English speakers. The meet-
ings last around 30 minutes each, and are scenario-
driven, wherein four participants play different
roles in a company?s design team: project man-
ager, marketing expert, interface designer and in-
dustrial designer.
3.1 Modelling Decision Discussions
We use the same annotation scheme as (Ferna?ndez
et al, 2008b) to model decision-making dialogue.
As stated in Section 2, this scheme distinguishes
between a small number of DA types based on the
role which they perform in the formulation of a de-
cision. Apart from improving the initial detection
of decision discussions (Ferna?ndez et al, 2008b),
such a scheme also aids their subsequent summa-
rization, because it indicates which utterances con-
tain particular types of information.
The annotation scheme is based on the observa-
tion that a decision discussion contains the follow-
ing main structural components: (a) a topic or is-
sue requiring resolution is raised, (b) one or more
possible resolutions are considered, (c) a particular
resolution is agreed upon and so becomes the de-
cision. Hence the scheme distinguishes between
three main decision dialogue act (DDA) classes:
issue (I), resolution (R), and agreement (A). Class
R is further subdivided into resolution proposal
(RP) and resolution restatement (RR). I utterances
introduce the topic of the decision discussion, ex-
amples being ?Are we going to have a backup??
and ?But would a backup really be necessary??
in Dialogue 1. On the other hand, R utterances
specify the resolution which is ultimately adopted
as the decision. RP utterances propose this reso-
lution (e.g. ?I think maybe we could just go for
the kinetic energy. . . ?), while RR utterances close
the discussion by confirming/summarizing the de-
cision (e.g. ?Okay, fully kinetic energy?) . Finally,
A utterances agree with the proposed resolution,
signalling that it is adopted as the decision, (e.g.
?Yeah?, ?Good? and ?Okay?). Note that an utter-
ance can be assigned to more than one DDA class,
and within a decision discussion, more than one
utterance can be assigned to the same DDA class.
We use both manual and ASR one-best tran-
scripts1 in the experiments described here. DDA
annotations were first made on the manual tran-
scripts, and then transferred onto the ASR tran-
scripts. Inter-annotator agreement was satisfac-
tory, with kappa values ranging from .63 to .73 for
the four DDA classes. Due to different segmen-
tation, the manual and ASR transcripts contain a
total of 15,680 and 8,357 utterances respectively,
and on average, 40 and 33 DDAs per meeting.
Hence DDAs are slightly less sparse in the ASR
transcripts: for all DDAs, 6.7% vs. 4.3% of the to-
tal number of utterances, for I, 1.6% vs. 0.9%, for
RP, 2% vs. 1%, for RR, 0.5% vs. 0.4%, and for A,
2.6% vs. 2%.
(1) A: Are we going to have a backup? Or we do
just?
B: But would a backup really be necessary?
A: I think maybe we could just go for the
kinetic energy and be bold and innovative.
C: Yeah.
B: I think? yeah.
A: It could even be one of our selling points.
C: Yeah ?laugh?.
D: Environmentally conscious or something.
A: Yeah.
B: Okay, fully kinetic energy.
D: Good.2
4 Decision Discussion Detection using
Directed Graphical Models
A directed graphical model (DGM) M, (see Mur-
phy (2002)), is a directed acyclic graph consisting
of nodes which represent random variables, arcs
which represent dependencies among these vari-
ables, and a probability distribution P over the
variables. Let X = {X1, X2, ..., Xn} be a set of
random variables that are associated with nodes in
a DGM and Pa(Xi) be parents of Xi. The proba-
bility distribution of the model M satisfies:
P (X1, X2, ..., Xn) =
n?
i=1
(P (Xi)|Pa(Xi))
When a DGM is used as a classifier, the goal is to
correctly infer the value of the class node Xc ? X
given a vector of values for the observed node(s)
1We used SRI?s Decipher for which (Stolcke et al, 2008)
reports a word error rate of 26.9% on AMI meetings.
2This example was extracted from the AMI dialogue
ES2015c and has been modified slightly for presentation pur-
poses.
237
Xo ? X\Xc. This is done by using M to find the
value of Xc which gives the highest conditional
probability P (Xc|Xo).
To detect each individual DDA class, we ex-
amined the four simple DGMs in Figure 1 (see
Appendix). The DDA node is binary where
value 1 indicates the presence of a DDA and 0
its absence. The evidence node (E) is a multi-
dimensional vector of observed values of non-
lexical features. These include utterance features
(UTT) such as length in words, duration in mil-
liseconds, position within the meeting (as percent-
age of elapsed time), manually annotated dialogue
act (DA) features3 such as inform, assess, suggest,
and prosodic features (PROS) such as energy and
pitch. These features are the same as the non-
lexical features used by Ferna?ndez et al (2008b).
The hidden component node (C) represents the
distribution of observable evidence E as a single
Gaussian in the -sim models, and a mixture in the
-mix models. For the -mix models, the number
of Gaussian components is hand-tuned during the
training phase.
More complex models are constructed from the
four simple models in Figure 1 to allow for depen-
dencies between different DDAs. For example, the
model in Figure 2 (see Appendix) generalizes Fig-
ure 1c with arcs connecting the DDA classes based
on analysis of the annotated AMI data.
4.1 Experiments
The DGM classifiers in Figures 1 and 2 were im-
plemented in Matlab using the BNT software4.
Since the current BNT version does not sup-
port multiple time series training for fully observ-
able Dynamic Bayesian Networks (DBNs), we ex-
tended the software for training models using this
structure (e.g., Figure 1c and Figure 2).
A DGM classifier is considered to have hy-
pothesized a DDA if the marginal probability of
its DDA node is above a hand-tuned threshold.
We tested the DGMs on manual and ASR tran-
scripts in a 17-fold cross-validation, and evaluated
their performance on both a per-utterance basis,
and also with the same lenient-match metric as
Ferna?ndez et al (2008b). This allows a margin
of 20 seconds preceding and following a hypoth-
esized DDA, and so we refer to it as the 40 sec-
ond metric. In addition, we hypothesized decision
3We use the AMI DA annotations. These are only avail-
able for manual transcripts.
4http://www.cs.ubc.ca/?murphyk/Software/BNT/bnt.html
discussion regions using the DGM output and the
following two simple rules:
? A decision discussion region begins with an
Issue DDA.
? A decision discussion region contains at least
one Issue DDA and one Resolution DDA.
To evaluate the accuracy of these hypothesized re-
gions, like Ferna?ndez et al (2008b), we divided
the dialogue into 30-second windows and evalu-
ated on a per window basis.
4.2 Results
Tables 1 and 2 show the F1-scores for each
DGM when using the best feature sets (I:
UTT+DA+PROS, RP: UTT+DA, RR: UTT, A:
UTT+DA). The BN-mix model gives the highest
F1-score for A on both evaluation metrics, and the
DBN-mix model, the highest for I, RP, and RR,
but there are no statistically significant differences
between any of the alternative DGMs.
Classifier I RP RR A
BN-mix .09 .09 .04 .19
DBN-mix .16 .14 .05 .17
BN-sim .12 .09 .04 .17
DBN-sim .15 .11 .04 .16
Table 1: F1-score (per utterance) of the DGMs us-
ing the best combination of non-lexical features.
Classifier I RP RR A
BN-mix .19 .24 .07 .38
DBN-mix .27 .24 .07 .32
BN-sim .23 .22 .06 .36
DBN-sim .25 .22 .06 .31
Table 2: F1-score (40 seconds) of the DGMs using
the best combination of non-lexical features.
To determine whether modeling dependencies
between DDAs improves performance, we exper-
imented with the DGMs that are generalized from
the DBN-sim (Figure 2) and DBN-mix models.
The F1-scores did not improve for I, RP, and RR,
while for A, the DGM generalized from DBN-sim
gave a .03 improvement according to the 40 sec-
onds metric, but this was not statistically signifi-
cant.
For each DDA, Table 3 compares the results of
the best DGM and the hierarchical SVM classi-
fication method of Ferna?ndez et al (2008b) (see
238
Section 2). The DGM performs better for all
DDAs on both evaluation metrics (p < 0.005).
Note that while prosodic features proved useless
to SVM classifiers (Ferna?ndez et al (2008b)), with
DGMs, they have some predictive power.
Per utterance 40 seconds
Classifier DDA Pr Re F1 Pr Re F1
SVM I .03 .62 .05 .04 .89 .08
DGM .11 .28 .16 .20 .44 .27
SVM RP .03 .60 .07 .05 .90 .10
DGM .09 .35 .14 .16 .57 .24
SVM RR .01 .49 .02 .01 .80 .03
DGM .02 .42 .05 .04 .58 .07
SVM A .05 .70 .10 .07 .90 .13
DGM .13 .31 .19 .29 .55 .38
Table 3: Performance of the DGM classifier vs.
the SVM classifier. Both use the best combination
of non-lexical features.
We also generated results without DA features.
Here, the best F1-scores for I, RP, and A degrade
between .07 and .09 (p < 0.05), but they are still
higher than the equivalent SVM results with DA
features. Since (Ferna?ndez et al, 2008b) report
that lexical features are the most useful for the
SVM classifiers, it will be interesting to see how
well the DGMs perform when they use lexical as
well as non-lexical features.
Detecting DDAs in ASR transcripts: Table 4
compares the DGM F1-scores when using ASR
one-best and manual transcripts. The DGMs per-
form well on ASR output. For I and RP, the results
on ASR are actually higher, perhaps because the
DDAs are less sparse. In the absence of DA fea-
tures, prosodic features improve the performance
for A in both sources.
UTT UTT+PROS
I RP RR A I RP RR A
ASR .20 .21 .06 .24 .16 .24 .07 .28
Man .18 .17 .07 .27 .16 .15 .05 .30
Table 4: F1-scores (40 seconds) computed using
ASR one-best vs. manual transcriptions.
Detecting decision discussion regions: Table 5
shows that according to the 30-second window
metric, rule-based classification with DGM output
compares well with hierarchical SVM classifica-
tion (Ferna?ndez et al, 2008b). In fact, even when
the latter uses lexical as well as non-lexical fea-
tures, its F1-score is still about the same as the
DGM-based classifier. Our future work will in-
volve dispensing with the rule-based approach and
designing a DGM which can detect decision dis-
cussion regions.
Classifier Pr Re F1
SVM .35 .88 .50
DGM .39 .93 .55
Table 5: Results in detecting decision discussion
regions for the SVM super-classifier and rule-
based DGM classifier, both using the best com-
bination of non-lexical features.
5 Decision Summarization
We now turn to the task of extracting useful
phrases for summarization. Since a summary of a
decision discussion should minimally contain the
issue under discussion, and its resolution, we leave
Agreement (A) utterances aside, and concentrate
on extracting phrases from Issues (I) and Resolu-
tions (R).
Our basic approach is the same taken in
(Ferna?ndez et al, 2008a): The WCN5 of each I
and R utterance is parsed by the Gemini parser
(Dowding et al, 1993) to produce multiple short
fragments, and then an SVM regression model
uses certain features in order to select the parse
that is most likely to match a gold-standard extrac-
tive summary. Our work is new in two respects:
summarizing from ASR output in addition to man-
ual transcriptions, and using a semantic-similarity
feature in the SVM. This new feature is generated
using Ted Pedersen?s semantic-similarity package
(Pedersen, 2002), and is motivated by the fact that
ordinarily the Issue summary should be semanti-
cally similar to the Resolution and vice versa.
The next section describes the lexical resources
used by Gemini, and Section 5.2, the metric for
calculating semantic similarity.
5.1 Open-Domain Semantic Parser
Since human-human spoken dialogue, especially
after being processed by an imperfect recognizer,
is likely to be highly ungrammatical, we have de-
veloped a semantic parser that only attempts to
find basic predicate-argument structures of the ma-
jor phrase types (S, VP, NP, and PP) and has access
to a broad-coverage lexicon. To build a broad-
coverage lexicon, we used publicly available lex-
ical resources for English, including COMLEX,
5When using manual transcripts, we create ?dummy
WCNs?: WCNs with a single path.
239
VerbNet, WordNet, and NOMLEX.
COMLEX provides detailed syntactic informa-
tion for the 40k most common words of En-
glish, and VerbNet, detailed semantic information
for verbs, including verb class, verb frames, the-
matic roles, mappings of syntactic position to the-
matic roles, and selection restrictions on thematic
role fillers. From WordNet we extracted another
15K nouns and the semantic class information for
all nouns. These semantic classes were hand-
aligned to the selectional classes used in Verb-
Net, based on the upper ontology of EuroWord-
Net. NOMLEX provides syntactic information for
event nominalizations, and information for map-
ping the noun arguments to the corresponding verb
syntactic positions.
These resources were combined and converted
to the Prolog-based format used in the Gemini
framework, which includes a fast bottom-up ro-
bust parser in which syntactic and semantic in-
formation is applied interleaved. Gemini can
compute parse probabilities on the context-free
skeleton of the grammar. In the experiments de-
scribed here these parse probabilities are trained
on Switchboard tree-bank data.
5.2 Semantic Similarity Metric: Normalized
Path Length
Ted Pedersen?s semantic similarity package (Ped-
ersen, 2002) can be used to apply a number of
different metrics that use WordNet as a knowl-
edge base. The metric used here, Normalized Path
Length (Leacock and Chodorow, 1998), defines
the semantic similarity sim between words w1 and
w2 as:
simc1,c2 = ? log
len(c1, c2)
2?D (1)
where c1 and c2 are concepts corresponding to w1
and w2, len(c1, c2) is the length of the shortest
path between them, and D is the maximum depth
of the taxonomy.
5.3 Experiments
Data: For the manual transcripts in our sub-
corpus, the average length in words of I and R ut-
terances is 12.2 and 11.9 respectively, and for the
ASR, 22.4 and 18.1. To provide a gold-standard,
phrases from I and R utterances in the man-
ual transcriptions were annotated as summary-
worthy. The aim was to select those phrases
which should appear in an extractive summary, or
could be the basis of a generated abstractive sum-
mary. As a general guideline, we tried to select
the phrase(s) which describe the issue/resolution
as succinctly as possible. This does not include
phrases which express the speaker?s attitude to-
wards the issue/resolution. Dialogue 2 is an exam-
ple where square brackets indicate which phrases
were selected as summary-worthy.
(2) A:(I) So we we?re looking at [sliders for both
volume and channel change]
B:(R)I was thinking kind of [just for the
volume]
Regression models: We use SVMlight
(Joachims, 1999) to learn separate SVM re-
gression models for Issues and Resolutions.
These rank the Gemini parses for each utterance
according to their likelihood of matching the
gold-standard summary. The top-ranked parse
is then entered into the automatically-generated
decision summary.
Features: We train the regression models with
various types of feature (see Table 6), including
properties of the WCN paths, parse, semantic and
lexical features. As lexical features are likely to be
more domain-specific, and they dramatically in-
crease size of the feature space, we prefer to avoid
them if possible.
To generate the semantic-similarity feature for
an I/R parse, we compute its semantic similarity
with the full transcripts of each of the R/I utter-
ances within the same decision discussion. The
feature?s value is then equal to the greatest of the
resulting semantic-similarity scores. Since Ted
Pedersen?s package operates on the noun portion
of WordNet, we must first extract all of the nouns
in the parse/utterance transcription. Next, we form
all of the possible pairs containing one noun from
the parse, and one from the utterance transcrip-
tion. Then we compute the semantic similarity
for each pair, and take their sum to be the level
of semantic similarity between the parse and the
utterance transcription. We experimented with av-
eraging rather than summing these scores, but the
resulting semantic-similarity feature was less pre-
dictive.
Evaluation: The models are evaluated in 10-
fold cross-validations using the same metric as
(Ferna?ndez et al, 2008a): Recall is the total pro-
portion of the gold-standard extractive summary
240
WCN phrase length (WCN arcs)
start/end point (absolute & percentage)
Parse parse probability
phrase type (S/VP/NP/PP)
Semantic main verb VerbNet class
head noun WordNet synset
Sem-sim Normalized Path Length
Lexical main verb, head noun
Table 6: Features for parse fragment ranking
Issue Resolution
Re Pr F1 Re Pr F1
Baseline 1.0 .50 .67 1.0 .60 .75
Oracle .77 .96 .85 .74 .99 .84
WCN,parse,sem .63 .69 .66 .61 .66 .64
+ sem-sim .65 .71 .68 .64 .69 .67
+ lexical .65 .67 .66 .65 .70 .67
Table 7: Parse ranking results for I & R Utterances
using manual transcriptions.
covered by the selected parse; precision is the to-
tal proportion of the chosen parse which overlaps
with the gold-standard summary. The baseline is
the entire transcription, and we also compare to
an ?oracle? that always chooses a parse with the
highest F1-score. Note that we use the extractive
summaries from the manual transcriptions as the
gold-standard for the evaluation of the results ob-
tained with ASR.
Results and analysis: Results with manual tran-
scriptions are shown in Table 7, and those with
ASR, in Table 8. In all cases, when starting with
a feature set containing WCN, parse and seman-
tic features, the F1-score is improved by adding
the semantic-similarity feature. For Issues, the F1-
score improves from .66 to .68 with manual tran-
scripts, and from .30 to .32 with ASR. The im-
provements for Resolutions are highly significant:
with manual transcripts, the F1 score increases
from .64 to .67 (p < 0.005), and with ASR, from
.33 to .37 (p < 0.005). Note that the further addi-
tion of lexical features only produces a significant
improvement in the case of I summarization with
ASR.
Compared to the full transcript baseline, we
achieve higher F1-scores for Issues?.68 vs. .67
with manual transcriptions, and .35 vs. .31 with
ASR?but slightly lower for Resolutions. There
remains a fairly large gap between our best scores
and their corresponding oracles (especially with
ASR), and so there may still be potential for sub-
stantial improvement.
Issue Resolution
Re Pr F1 Re Pr F1
Baseline .77 .20 .31 .80 .27 .40
Oracle .61 .87 .72 .59 .91 .72
WCN,parse,sem .28 .33 .30 .31 .35 .33
+ sem-sim .30 .34 .32 .35 .38 .36
+ lexical .35 .35 .35 .34 .39 .37
Table 8: Parse ranking results for I & R Utterances
using ASR.
6 Conclusions and Future Work
This paper has presented work on the detec-
tion and summarization of decision discussions
in multi-party dialogue. In the detection experi-
ments, we investigated the use of directed graph-
ical models (DGMs), and found that when us-
ing non-lexical features, the DGMs outperform
the hierarchical SVM classification method of
Ferna?ndez et al (2008b). The F1-score for the
four DDA classes increased between .04 and .19
(p < .005), and for identifying decision discus-
sion regions, by .05. This is encouraging because
lexical features have disadvantages?for example
they can be domain specific and greatly increase
the feature space. In addition, modelling the de-
pendencies between the DDA classes increased
performance for Agreement utterances, and the
DGMs were robust to ASR.
In the summarization experiments, we sum-
marized decision discussions by extracting key
words/phrases from their Issue (I) and Resolu-
tion (R) utterances. Each utterance?s Word Confu-
sion Network was parsed with an open-domain se-
mantic parser, thus producing multiple candidate
phrases, and then an SVM regression model se-
lected one of these phrases to enter into the sum-
mary. The experiments here investigated the use-
fulness of a new SVM feature which measures the
level of semantic similarity between candidate I
parses and R utterances, and vice-versa. This fea-
ture was generated with a semantic-similarity met-
ric which uses WordNet as a knowledge source.
It was found to improve performance with both
manual transcripts and ASR, and for R summa-
rization, the improvements were highly significant
(p < .005).
In future work, we plan to integrate lexical fea-
tures into our DGMs by using a switching Dy-
namic Bayesian Network similar to that reported
in (Ji and Bilmes, 2005). We also plan to extend
the decision discussion annotation scheme so that
we can try to automatically extract supporting ar-
241
guments for decisions.
Acknowledgements This material is based
upon work supported by the Defense Advanced
Research Projects Agency (DARPA) under Con-
tract No. FA8750-07-D-0185/0004, and by the
Department of the Navy Office of Naval Research
(ONR) under Grants No. N00014-05-1-0187 and
N00014-09-1-0106. Any opinions, findings and
conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the views of DARPA or ONR.
References
Satanjeev Banerjee, Carolyn Rose?, and Alex Rudnicky.
2005. The necessity of a meeting recording and
playback system, and the benefit of topic-level anno-
tations to meeting browsing. In Proceedings of the
10th International Conference on Human-Computer
Interaction.
John Dowding, Jean Mark Gawron, Doug Appelt, John
Bear, Lynn Cherny, Robert Moore, and Douglas
Moran. 1993. GEMINI: a natural language system
for spoken-language understanding. In Proceedings
of the 31st Annual Meeting of the Association for
Computational Linguistics (ACL).
Raquel Ferna?ndez, Matthew Frampton, John Dowding,
Anish Adukuzhiyil, Patrick Ehlen, and Stanley Pe-
ters. 2008a. Identifying relevant phrases to summa-
rize decisions in spoken meetings. In Proceedings
of Interspeech.
Raquel Ferna?ndez, Matthew Frampton, Patrick Ehlen,
Matthew Purver, and Stanley Peters. 2008b. Mod-
elling and detecting decisions in multi-party dia-
logue. In Proceedings of the 9th SIGdial Workshop
on Discourse and Dialogue.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
ment and disagreement in conversational speech:
Use of Bayesian networks to model pragmatic de-
pendencies. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Daniel Gatica-Perez, Ian McCowan, Dong Zhang, and
Samy Bengio. 2005. Detecting group interest level
in meetings. In Proceedings of ICASSP.
Pey-Yun Hsueh and Johanna Moore. 2007. Automatic
decision detection in meeting speech. In Proceed-
ings of MLMI 2007, Lecture Notes in Computer Sci-
ence. Springer-Verlag.
Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip
Dhillon, Jane Edwards, Javier Marc??as-Guarasa,
Nelson Morgan, Barbara Peskin, Elizabeth Shriberg,
Andreas Stolcke, Chuck Wooters, and Britta Wrede.
2004. The ICSI meeting project: Resources and re-
search. In Proceedings of the 2004 ICASSP NIST
Meeting Recognition Workshop.
Gang Ji and Jeff Bilmes. 2005. Dialog act tagging
using graphical models. In Proceedings of ICASSP.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods ?
Support Vector Learning. MIT Press.
Claudia Leacock and Martin Chodorow, 1998. Word-
Net: An Electronic Lexical Database, chapter Com-
bining local context and WordNet similarity for
word sense identification. University of Chicago
Press.
Iain McCowan, Jean Carletta, W. Kraaij, S. Ashby,
S. Bourban, M. Flynn, M. Guillemot, T. Hain,
J. Kadlec, V. Karaiskos, M. Kronenthal, G. Lathoud,
M. Lincoln, A. Lisowska, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meeting Corpus. In
Proceedings of Measuring Behavior, the 5th Inter-
national Conference on Methods and Techniques in
Behavioral Research, Wageningen, Netherlands.
Kevin Murphy. 2002. Dynamic Bayesian Networks:
Representation, Inference and Learning. Ph.D. the-
sis, University of California Berkeley.
Ted Pedersen. 2002. Semantic similarity package.
http:/www.d.umn.edu/ tpederse/similarity.
Matthew Purver, John Dowding, John Niekrasz,
Patrick Ehlen, Sharareh Noorbaloochi, and Stanley
Peters. 2007. Detecting and summarizing action
items in multi-party dialogue. In Proceedings of the
8th SIGdial Workshop on Discourse and Dialogue,
Antwerp, Belgium.
Andreas Stolcke, Xavier Anguera, Kofi Boakye, O?zgu?r
C?etin, Adam Janin, Matthew Magimai-Doss, Chuck
Wooters, and Jing Zheng. 2008. The ICSI-SRI
spring 2007 meeting and lecture recognition system.
In Proceedings of CLEAR 2007 and RT2007.
Daan Verbree, Rutger Rienks, and Dirk Heylen. 2006.
First steps towards the automatic construction of
argument-diagrams from real discussions. In Pro-
ceedings of the 1st International Conference on
Computational Models of Argument, volume 144,
pages 183?194. IOS press.
Steve Whittaker, Rachel Laban, and Simon Tucker.
2006. Analysing meeting records: An ethnographic
study and technological implications. In S. Renals
and S. Bengio, editors, Machine Learning for Multi-
modal Interaction: Second International Workshop,
MLMI 2005, Revised Selected Papers, volume 3869
of Lecture Notes in Computer Science, pages 101?
113. Springer.
Klaus Zechner. 2002. Automatic summarization of
open-domain multiparty dialogues in diverse genres.
Computational Linguistics, 28(4):447?485.
242
Appendix
DDA
E
a) BN-sim
DDA
E
b) BN-mix
C
DDA
time t-1 time t
E
DDA
E
c) DBN-sim
DDA
time t-1 time t
E
DDA
E
d) DBN-mix
CC
Figure 1: Simple DGMs for individual decision
detection. During training, the shaded nodes are
hidden, and the clear nodes are observable.
A
time t-1 time t
E
A
E
I I
RP RP
RR RR
Figure 2: A DGM that takes the dependencies be-
tween decisions into account.
243
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 306?309,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Cascaded Lexicalised Classifiers for Second-Person Reference Resolution
Matthew Purver
Department of Computer Science
Queen Mary University of London
London E1 4NS, UK
mpurver@dcs.qmul.ac.uk
Raquel Ferna?ndez
ILLC
University of Amsterdam
1098 XH Amsterdam, Netherlands
raquel.fernandez@uva.nl
Matthew Frampton and Stanley Peters
CSLI
Stanford University
Stanford, CA 94305, USA
frampton,peters@csli.stanford.edu
Abstract
This paper examines the resolution of the
second person English pronoun you in
multi-party dialogue. Following previous
work, we attempt to classify instances as
generic or referential, and in the latter case
identify the singular or plural addressee.
We show that accuracy and robustness can
be improved by use of simple lexical fea-
tures, capturing the intuition that different
uses and addressees are associated with
different vocabularies; and we show that
there is an advantage to treating referen-
tiality and addressee identification as sep-
arate (but connected) problems.
1 Introduction
Resolving second-person references in dialogue is
far from trivial. Firstly, there is the referentiality
problem: while we generally conceive of the word
you1 as a deictic addressee-referring pronoun, it
is often used in non-referential ways, including as
a discourse marker (1) and with a generic sense
(2). Secondly, there is the reference problem: in
addressee-referring cases, we need to know who
the addressee is. In two-person dialogue, this is
not so difficult; but in multi-party dialogue, the ad-
dressee could in principle be any one of the other
participants (3), or any group of more than one (4):
(1) It?s not just, you know, noises like something
hitting.
(2) Often, you need to know specific button
sequences to get certain functionalities done.
(3) I think it?s good. You?ve done a good review.
(4) I don?t know if you guys have any questions.
1We include your, yours, yourself, yourselves.
This paper extends previous work (Gupta et al,
2007; Frampton et al, 2009) in attempting to au-
tomatically treat both problems: detecting refer-
ential uses, and resolving their (addressee) refer-
ence. We find that accuracy can be improved by
the use of lexical features; we also give the first
results for treating both problems simultaneously,
and find that there is an advantage to treating them
as separate (but connected) problems via cascaded
classifiers, rather than as a single joint problem.
2 Related Work
Gupta et al (2007) examined the referentiality
problem, distinguishing generic from referential
uses in multi-party dialogue; they found that 47%
of uses were generic and achieved a classification
accuracy of 75%, using various discourse features
and discriminative classifiers (support vector ma-
chines and conditional random fields). They at-
tempted the reference-resolution problem, using
only discourse (non-visual) features, but accuracy
was low (47%).
Addressee identification in general (i.e. in-
dependent of the presence of you) has been ap-
proached in various ways. Traum (2004) gives
a rule-based algorithm based on discourse struc-
ture; van Turnhout et al (2005) used facial ori-
entation as well as utterance features; and more
recently Jovanovic (2006; 2007) combined dis-
course and gaze direction features using Bayesian
networks, achieving 77% accuracy on a portion of
the AMI Meeting Corpus (McCowan et al, 2005)
of 4-person dialogues.
In recent work, therefore, Frampton et al
(2009) extended Gupta et al?s method to in-
clude multi-modal features including gaze direc-
tion, again using Bayesian networks on the AMI
corpus. This gave a small improvement on the ref-
306
erentiality problem (achieving 79% accuracy), and
a large improvement on the reference-resolution
task (77% accuracy distinguishing singular uses
from plural, and 80% resolving singular individ-
ual addressee reference).
However, they treated the two tasks in isola-
tion, and also broke the addressee-reference prob-
lem into two separate sub-tasks (singular vs. plu-
ral reference, and singular addressee reference). A
full computational you-resolution module would
need to treat all tasks (either simultaneously as one
joint classification problem, or as a cascaded se-
quence) ? with inaccuracy at one task necessar-
ily affecting performance at another ? and we ex-
amine this here. In addition, we examine the ef-
fect of lexical features, following a similar insight
to Katzenmaier et al (2004); they used language
modelling to help distinguish between user- and
robot-directed utterances, as people use different
language for the two ? we expect that the same is
true for human participants.
3 Method
We used Frampton et al (2009)?s AMI corpus
data: 948 ?you?-containing utterances, manu-
ally annotated for referentiality and accompanied
by the AMI corpus? original addressee annota-
tion. The very small number of two-person ad-
dressee cases were joined with the three-person
(i.e. all non-speaker) cases to form a single ?plu-
ral? class. 49% of cases are generic; 32% of
referential cases are plural, and the rest are ap-
proximately evenly distributed between the singu-
lar participants. While Frampton et al (2009) la-
belled singular reference by physical location rel-
ative to the speaker (giving a 3-way classification
problem), our lexical features are more suited to
detecting actual participant identity ? we there-
fore recast the singular reference task as a 4-way
classification problem and re-calculate their per-
formance figures (giving very similar accuracies).
Discourse Features We use Frampton et al
(2009)?s discourse features. These include sim-
ple durational and lexical/phrasal features (includ-
ing mention of participant names); AMI dialogue
act features; and features expressing the simi-
larity between the current utterance and previ-
ous/following utterances by other participants. As
dialogue act features are notoriously hard to tag
automatically, and ?forward-looking? information
about following utterances may be unavailable in
an on-line system, we examine the effect of leav-
ing these out below.
Visual Features Again we used Frampton et al
(2009)?s features, extracted from the AMI corpus
manual focus-of-attention annotations which track
head orientiation and eye gaze. Features include
the target of gaze (any participant or the meet-
ing whiteboard/projector screen) during each ut-
terance, and information about mutual gaze be-
tween participants. These features may also not
always be available (meeting rooms may not al-
ways have cameras), so we investigate the effect
of their absence below.
Lexical Features The AMI Corpus simulates a
set of scenario-driven business meetings, with par-
ticipants performing a design task (the design of
a remote control). Participants are given specific
roles to play, for example that of project manager,
designer or marketing expert. It therefore seems
possible that utterances directed towards particular
individuals will involve the use of different vocab-
ularies reflecting their expertise. Different words
or phrases may also be associated with generic
and referential discussion, and extracting these au-
tomatically may give benefits over attempting to
capture them using manually-defined features. To
exploit this, we therefore added the use of lexical
features: one feature for each distinct word or n-
gram seen more than once in the corpus. Although
such features may be corpus- or domain-specific,
they are easy to extract given a transcript.
4 Results and Discussion
4.1 Individual Tasks
We first examine the effect of lexical features on
the individual tasks, using 10-way cross-validation
and comparing performance with Frampton et al
(2009). Table 1 shows the results for the referen-
tiality task in terms of overall accuracy and per-
class F1-scores; ?MC Baseline? is the majority-
class baseline; results labelled ?EACL? are Framp-
ton et al (2009)?s figures, and are presented for
all features and for reduced feature sets which
might be more realistic in various situations: ?-V?
removes visual features; ?-VFD? removes visual
features, forward-looking discourse features and
dialogue-act tag features.
As can be seen, adding lexical features
(?+words? adds single word features, ?+3grams?
adds n-gram features of lengths 1-3) improves the
307
Features Acc Fgen Fref
MC Baseline 50.9 0 67.4
EACL 79.0 80.2 77.7
EACL -VFD 73.7 74.1 73.2
+words 85.3 85.7 84.9
+3grams 87.5 87.4 87.5
+3grams -VFD 87.2 86.9 87.6
3grams only 85.9 85.2 86.4
Table 1: Generic vs. referential uses
Features Acc Fsing Fplur
MC Baseline 67.9 80.9 0
EACL 77.1 83.3 63.2
EACL -VFD 71.4 81.5 37.1
+words 83.1 87.8 72.5
+3grams 85.9 90.0 76.6
+3grams -VFD 87.1 91.0 77.6
3grams only 86.9 90.8 77.0
Table 2: Singular vs. plural reference.
performance significantly ? accuracy is improved
by 8.5% absolute above the best EACL results,
which is a 40% reduction in error. Robustness to
removal of potentially problematic features is also
improved: removing all visual, forward-looking
and dialogue act features makes little difference.
In fact, using only lexical n-gram features, while
reducing accuracy by 2.6%, still performs better
than the best EACL classifier.
Table 2 shows the equivalent results for the
singular-plural reference distinction task; in this
experiment, we used a correlation-based fea-
ture selection method, following Frampton et al
(2009). Again, performance is improved, this time
giving a 8.8% absolute accuracy improvement, or
38% error reduction; robustness to removing vi-
sual and dialogue act features is also very good,
even improving performance.
For the individual reference task (again using
feature selection), we give a further ?NS baseline?
of taking the next speaker; note that this performs
rather well, but requires forward-looking informa-
tion so should not be compared to ?-F? results.
Results are again improved (Table 3), but the im-
provement is smaller: a 1.4% absolute accuracy
improvement (7% error reduction); we conclude
from this that visual information is most impor-
tant for this part of the task. Robustness to feature
unavailability still shows some improvement: ex-
Features Acc FP1 FP2 FP3 FP4
MC baseline 30.7 0 0 0 47.0
NS baseline 70.7 71.6 71.1 72.7 68.2
EACL 80.3 82.8 79.7 75.9 81.4
EACL -V 73.8 79.2 70.7 74.1 71.4
EACL -VFD 56.6 58.9 55.5 64.0 47.3
+words 81.4 83.9 79.7 79.3 81.8
+3grams 81.7 83.9 80.3 79.3 82.5
+3grams -V 74.8 81.3 71.7 75.2 71.4
+3grams -VFD 60.7 66.3 55.9 66.2 53.0
3grams only 60.7 63.1 58.1 52.9 63.4
3grams +NS 74.5 76.7 73.8 75.0 72.7
Table 3: Singular addressee detection.
cluding all visual, forward-looking and dialogue-
act features has less effect than on the EACL sys-
tem (60.7% vs. 56.6% accuracy), and a system
using only n-grams and the next speaker identity
gives a respectable 74.5%.
Feature Analysis We examined the contribu-
tion of particular lexical features using Informa-
tion Gain methods. For the referentiality task, we
found that generic uses of you were more likely
to appear in utterances containing words related to
the main meeting topic, such as button, channel,
or volume (properties of the to-be-designed remote
control). In contrast, words related to meeting
management, such as presentation, email, project
and meeting itself, were predictive of referential
uses. The presence of first person pronouns and
discourse and politeness markers such as okay,
please and thank you was also indicative of refer-
entiality, as were n-grams capturing interrogative
structures (e.g. do you).
For the plural/singular distinction, we found
that the plural first person pronoun we correlated
with plural references of you. Other predictive n-
grams for this task were you mean and you know,
which were indicative of singular and plural refer-
ences, respectively. Finally, for the individual ref-
erence task, useful lexical features included par-
ticipant names, and items related to their roles.
For instance, the n-grams sales, to sell and make
money correlated with utterances addressed to the
?marketing expert?, while utterances containing
speech recognition and technical were addressed
to the ?industrial designer?.
Discussion The best F-score of the three sub-
tasks is for the generic/referential distinction; the
308
Features Acc Fgen Fplur FP1 FP2 FP3 FP4
MC baseline 49.1 65.9 0 0 0 0 0
EACL 58.3 73.3 24.3 57.6 57.0 36.0 51.1
+3grams 60.9 74.8 42.0 57.7 52.2 35.6 50.2
3grams only 67.5 84.8 61.6 39.1 39.3 30.6 38.6
Cascade +3grams 78.1 87.4 59.1 64.1 76.4 75.0 82.6
Table 4: Combined task: generic vs. plural vs. singular addressee.
worst is for the detection of plural reference (Fplur
in Table 2). This is not surprising: humans find the
former task easy to annotate ? Gupta et al (2007)
report good inter-annotator agreement (? = 0.84)
? but the latter hard. In their analysis of the AMI
addressee annotations, Reidsma et al (2008) ob-
serve that most confusions amongst annotators are
between the group-addressing label and the labels
for individuals; whereas if annotators agree that an
utterance is addressed to an individual, they also
reach high agreement on that addressee?s identity.
4.2 Combined Task
We next combined the individual tasks into one
combined task; for each you instance, a 6-way
classification as generic, group-referring or refer-
ring to one of the 4 participants. This was at-
tempted both as a single classification exercise us-
ing a single Bayesian network; and as a cascaded
pipeline of the three individual tasks; see Table 4.
Both used correlation-based feature selection.
For the single joint classifier, n-grams again im-
prove performance over the EACL features. Using
only n-grams gives a significant improvement, per-
haps due to the reduction in the size of the feature
space on this larger problem. Accuracy is reason-
able (67.5%), but while F-scores are good for the
generic class (above 80%), others are low.
However, use of three cascaded classifiers
improves performance to 78% and gives large
per-class F-score improvements, exploiting
the higher accuracy of the first two stages
(generic/referential, singular/plural), and the fact
that different features are good for different tasks.
5 Conclusions
We have shown that the use of simple lexical fea-
tures can improve performance and robustness for
all aspects of second-person pronoun resolution:
referentiality detection and reference identifica-
tion. An overall 6-way classifier is feasible, and
cascading individual classifiers can help. Future
plans include testing on ASR transcripts, and in-
vestigating different classification techniques for
the joint task.
References
M. Frampton, R. Ferna?ndez, P. Ehlen, M. Christoudias,
T. Darrell, and S. Peters. 2009. Who is ?you?? com-
bining linguistic and gaze features to resolve second-
person references in dialogue. In Proceedings of the
12th Conference of the EACL.
S. Gupta, J. Niekrasz, M. Purver, and D. Jurafsky.
2007. Resolving ?you? in multi-party dialog. In
Proceedings of the 8th SIGdial Workshop on Dis-
course and Dialogue.
N. Jovanovic, R. op den Akker, and A. Nijholt. 2006.
Addressee identification in face-to-face meetings. In
Proceedings of the 11th Conference of the EACL.
N. Jovanovic. 2007. To Whom It May Concern -
Addressee Identification in Face-to-Face Meetings.
Ph.D. thesis, University of Twente, The Netherlands.
M. Katzenmaier, R. Stiefelhagen, and T. Schultz. 2004.
Identifying the addressee in human-human-robot in-
teractions based on head pose and speech. In Pro-
ceedings of the 6th International Conference on
Multimodal Interfaces.
I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bour-
ban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec,
V. Karaiskos, M. Kronenthal, G. Lathoud, M. Lin-
coln, A. Lisowska, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meeting Corpus. In
Proceedings of the 5th International Conference on
Methods and Techniques in Behavioral Research.
D. Reidsma, D. Heylen, and R. op den Akker. 2008.
On the contextual analysis of agreement scores. In
Proceedings of the LREC Workshop on Multimodal
Corpora.
D. Traum. 2004. Issues in multi-party dialogues. In
F. Dignum, editor, Advances in Agent Communica-
tion, pages 201?211. Springer-Verlag.
K. van Turnhout, J. Terken, I. Bakx, and B. Eggen.
2005. Identifying the intended addressee in mixed
human-humand and human-computer interaction
from non-verbal features. In Proceedings of ICMI.
309
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 253?256,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Detection of time-pressure induced stress in speech via acoustic indicators ?
Matthew Frampton, Sandeep Sripada, Ricardo Augusto Hoffmann Bion and Stanley Peters
Center for the Study of Language and Information
Stanford University, Stanford, CA, 94305 USA
{frampton@,ssandeep@,ricardoh@,peters@csli.}stanford.edu
Abstract
We use automatically extracted acoustic
features to detect speech which is gener-
ated under stress, achieving 76.24% accu-
racy with a binary logistic regression. Our
data are task-oriented human-human dia-
logues in which a time-limit is unexpect-
edly introduced partway through. Anal-
ysis suggests that we can detect approxi-
mately when this event occurs. We also
consider the importance of normalizing
the acoustic features by speaker, and de-
tecting stress in new speakers.
1 Introduction
The term stressed speech can refer to speech
generated under psychological stress (Sigmund et
al., 2007). Stress alters an individual?s mental
and physiological state, which then affects their
speech. The ability to identify stressed speech
would be very valuable to Spoken Dialogue Sys-
tems (SDSs), especially in ?stressful? applications
such as search-and-rescue robots. Speech recog-
nizers are usually trained on normal speech, and
so can struggle badly on other speech. Tech-
niques exist for making ASR robust to noise/stress
(Hansen and Patil, 2007), but knowing when to ap-
ply them will in general require the ability to de-
tect stressed speech. This ability is clearly also
needed when the user?s stress level should affect
how the SDS responds. An SDS should some-
times generate stressed speech itself?for exam-
ple, to impart a sense of urgency on the user.
This paper investigates spectral-based acoustic
indicators of stress in human-human, task-oriented
?The research reported in this paper was sponsored by the
Department of the Navy, Office of Naval Research, under
grant number N00014-017-1-1049. Any opinions, findings
and conclusions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily reflect the
views of the Office of Naval Research.
dialogues in which stress is induced in the lat-
ter stages by the unexpected introduction of time-
pressure. Unlike previous studies, we detect stress
in whole utterances in the raw audio, which is
more realistic for applications. We also consider
the importance of normalizing the features, and
detection of both the introduction of the stressor,
and stress in new speakers.
2 Related work
Stressors and clip sizes: The stressors in pre-
vious studies include logical problems, images of
human bodies with skin diseases/severe accident
injuries (Tolkmitt and Scherer, 1986), loss of con-
trol of a helicopter (Protopapas and Liberman,
2001), university examinations (Sigmund et al,
2007), and an increasingly difficult air controller
simulation and verbal quiz (Scherer et al, 2008).
Sigmund et al (2007) detect stress in approxi-
mately 2000 voiced segments of 5 vowels. Tolk-
mitt and Scherer (1986), Protopapas and Liberman
(2001) and Scherer et al (2008) detect stress in
whole utterances, but these are respectively, read
from a card, quiz answers, and with verbal content
removed. Studies on the Speech under Simulated
and Actual Stress (SUSAS) corpus (Hansen and
Bou-Ghazale, 1997) detect stress in words. These
include (Hansen, 1996; Zhou, 1999; Hansen and
Womack, 1996; Zhou, 2001; Casale et al, 2007).
The SUSAS corpus contains aircraft communica-
tion words from a common highly confusable vo-
cabulary set of 35, and they are divided into differ-
ent speaking styles.
Acoustic cues: The most widely investigated
acoustic cues relate to fundamental frequency (F0,
also called pitch), formant frequencies and spec-
tral composition e.g. (Tolkmitt and Scherer, 1986;
Hansen, 1996; Zhou, 1999; Protopapas and Liber-
man, 2001; Sigmund et al, 2007; Scherer et
al., 2008). Mel-Frequency Cepstral Coefficients
253
Category Examples
F0-related Median, mean, minimum, time of minimum as % thr? clip, max, time of max as % thr? clip,
range (max-min), standard deviation, mean absolute slope,
mean slope without octave jumps, number of voiced frames.
Intensity-related Median, mean, minimum, time of minimum as % thr? clip, max, time of max as % thr? clip,
range (max-min), standard deviation.
Formant-related Mean, minimum, time of minimum as % through clip, max, time of max as % through clip,
(for F1-F3) range (max-min).
Spectral tilt-related Mean, minimum, maximum, range (max-min).
Table 1: The acoustic features which are extracted from the audio clips using Praat (Boersma and Weenink, 2010).
(MFCCs)1 and Teager Energy Operator (TEO)2
(Kaiser, 1990) based features have also been con-
sidered e.g. (Hansen and Womack, 1996; Zhou,
2001; Casale et al, 2007).
Features of all these types have proved useful
in detecting stressed speech. The classification
methods employed are various, including a tradi-
tional binary hypothesis detection-theory method
(Zhou, 1999) and neural networks (Hansen and
Womack, 1996; Scherer et al, 2008), while Casale
et al (2007) used genetic algorithms for feature
selection. Of the two more recent studies which
detected stress in whole utterances, Protopapas
and Lieberman found that mean and maximum F0
within an utterance correlate highly with subject
stress ratings, and Scherer et al?s neural network
outperformed a human baseline. Note that find-
ings/results in these and other previous studies are
not directly comparable with our own, because we
detect stress in whole utterances in raw audio.
3 Data
The original data (Eberhard et al, 2010) are 4
task-oriented dialogues between 2 native English-
speaking participants. Hence there are 8 speakers
in total (7 male, 1 female), and the dialogues con-
tain 263, 172, 228 and 210 utterances respectively.
During a dialogue, the participants (the direc-
tor and member) are on a floor with corridors and
rooms that contain various colored boxes. The di-
rector stays in one room, and gives task instruc-
tions via walkie-talkie to the member, providing
directions with a map which is partially complete
and accurate for box locations. The tasks are lo-
cating boxes which are unmarked on the map, and
transferring blocks between and retrieving speci-
fied boxes. Initial instructions do not mention a
1MFCCs model the human auditory system?s nonlinear
filtering in measuring spectral band energies.
2The TEO is a nonlinear operator which uses mechanical
and physical considerations to extract the signal energy.
time limit, but at the end of the 7th minute, the di-
rector is given a timer and told there are 3 minutes
to complete the current tasks, plus one new task.
We use the Nuance speech recognizer (V. 9.0)
to end-point each dialogue?s audio signal, and the
resulting clips are mostly 1 to 3 seconds. In
preliminary experiments (not reported), denoising
seemed to remove acoustic information which is
indicative of stress. Hence we use raw audio.
Stressed speech: For present purposes, we as-
sume that all speech after the introduction of the
time limit is stressed. Hence 448 of the 663 au-
dio clips in our experimental data are unstressed,
and 215 are stressed. In future we plan to use
the Amazon Mechanical Turk to obtain perceived
stress ratings on a scale with more gradations.
4 Experiments
Acoustic features: We use Praat (Boersma and
Weenink, 2010) to compute F0, intensity, formant
and spectral tilt-related features for each clip (Ta-
ble 1). F0 (pitch) corresponds to the rate of vocal
cord vibration in Hertz (Hz), and Intensity, to the
sound?s loudness in decibels (dB), (derived from
the amplitude or increase in air pressure). A for-
mant is a concentration of acoustic energy around
a particular frequency in the speech wave. There
are several, each corresponding to a resonance in
the vocal tract, and we consider the lowest three
(F1-F3). Spectral tilt measures the difference in
energy between the 1st and 2nd formants, and so
estimates the degree to which energy at the funda-
mental dominates in the glottal source waveform.
Comparing different normalization methods:
We evaluate binary logistic regression models with
10-fold cross-validation, and try the following 4
methods for normalizing each clip?s acoustic fea-
tures according to its speaker.
? Maximum normalization: Due to the possi-
bility of outliers, we divide each feature value
254
Normalization % Accuracy US %correct S %correct MCB
Maximum normalization 74.4 (74.25) 86.67 (85.05) 48.5 (54.3) 67.8 (67.1)
Z-score 73.5 (73.78) 84.89 (84.12) 49.53 (52.7) 67.8 (67.1)
US Average 75.61 (76.24) 86.63 (86.2) 53.5 (55.9) 67.8 (67.1)
S Average 75.31 (75) 84.67 (84.375) 55.6 (55.9) 67.8 (67.1)
No normalization 68.52 (70.45) 84.34 (82.8) 37.4 (45.2) 67.8 (67.1)
Table 2: Binary logistic regression 10-fold cross validation with different feature normalization approaches: Scores within
brackets are when the female speaker data is removed; S = Stressed, US = Unstressed, MCB = Majority Class Baseline.
by the 95th percentile value for that feature,
rather than the maximum.
? Z-score: Using the mean and standard devi-
ation for each feature, the feature vector is
converted to Z-scores3.
? Unstressed (US) average: Each feature is
normalized by its mean value in the un-
stressed region.
? Stressed (S) average: Each feature is normal-
ized by its mean value in the stressed region.
Table 2 shows the results. All those gener-
ated with feature normalization are significantly
better (p < 0.005) than the majority class base-
line (MCB), (i.e. classifying all utterances as un-
stressed). Without normalization, the overall accu-
racy drops about 5?6%, and the stressed speech
class about 11?18%. Different normalization
methods do not produce very different results,
but US average gives the best overall accuracy
(75.61%). When we remove the female speaker,
this increases to 76.24%, and feature normaliza-
tion remains important.
We also tested our assumption that the speech
before and after the introduction of time-pressure
is unstressed and stressed respectively, by check-
ing that they really are different. As before, we
considered 7 minutes unstressed, and 3 stressed,
and used US average normalization. However we
now assigned different minutes to the unstressed
and stressed categories: first we swapped the 6th
and 8th, then also the 5th and 9th, and then also the
7th and 10th. As a result, classification accuracy
dropped, (to 75%, then 68.71%, then 67.66%),
which supports our assumption.
Feature contribution analysis: Table 3 shows
the US average normalized features with informa-
tion gain greater than zero. Intensity and pitch
features are ranked most predictive (i.e. maximum
3A Z-score indicates the number of standard deviations
between an observation and the mean.
and mean intensity, and mean and median pitch),
but Spectral tilt mean and a couple of formant fea-
tures are also predictive. In general, higher values
for the most predictive pitch and intensity features
(e.g. Intensity max and Pitch mean) seem to indi-
cate stress. An interaction term for Intensity max
and Pitch mean caused a significant improvement
in the fit of the model?the ?2 value (or change in
the -2 Log likelihood) was 4.952 (p < 0.05).
Feature Info. Gain
Intensity max .101
Pitch mean .099
Intensity mean .099
Pitch median .088
Pitch max .059
Intensity min .046
Spectral tilt mean .042
Pitch min .041
F1 min .038
Intensity range .034
Intensity std. dev. .033
F3 range .033
Intensity median .031
Table 3: Unstressed average normalized features ranked by
information gain.
Detecting the introduction of the stressor:
Figure 1 shows the percentage of audio clips in
each minute that were classified as stressed. As we
would hope, there is a dramatic increase from the
7th to the 8th minute (around 20% to over 50%).
Such an increase could be used to detect the intro-
duction of the stressor, time-pressure.
Detecting stress in new speakers: To detect
stressed speech in new speakers, we evaluate the
logistic regression with an 8-fold cross-validation,
in each fold training on 7 speakers, and testing on
the other. We apply US average normalization, ini-
tially with the average values for the new speaker?s
unstressed speech, and then with the average val-
ues in unstressed speech across all ?seen? speakers
(speakers in the training set). Evaluation scores
(Table 4) are now lower, especially for the lat-
ter approach, but the former remains significantly
255
Figure 1: The percentage of clips in each minute of the
dialogues which our classifier marks as stressed, (note that
time-pressure is introduced at the end of minute 7).
better than the MCB. Since the female speaker?s
stress class F-score is 0, we tried normalizing the
7 male speakers based on only seen male data,
and then average accuracy for a male rose from
67.09% to 68.02% (not statistically significant).
Spkr % Accuracy F-unstress F-stress
1 62.5 (62.2) .77 (.74) .07 (0.34)
2 75 (75) .84 (.84) .09 (0.44)
3 57.6 (72.9) .62 (.81) .49 (0.5)
4 71.73 (74) .84 (.83) 0 (0.43)
5 71.62 (73.0) .77 (.77) .62 (0.68)
6 77.6 (80.4) .86 (.88) .51 (0.52)
7 64.36 (65.6) .74 (.77) .4 (0.35)
8 60.97 (71.7) .67 (.8) .49 (0.46)
Av. 67.67 (71.9) .76 (.80) .33 (0.46)
Table 4: Predicting stress in new speakers: New speaker
features are normalized based on unstressed speech for all
speakers in training set (unbracketed) and on their own un-
stressed speech (bracketed). Speaker 4 is the female.
5 Conclusion
For detecting stressed speech, we demonstrated
the importance of normalizing acoustic features by
speaker, and achieved 76.24% classification accu-
racy with a binary logistic regression model. The
most indicative features were maximum and mean
intensity within an utterance, and mean and me-
dian pitch. After the introduction of time-pressure,
the percentage of clips classified as stressed in-
creased dramatically, showing that it is possible
to detect approximately when this event occurs.
We also attempted to detect stressed speech in new
speakers, and as expected, results were poorer.
In future work we plan to expand our data-set
with more dialogues, and test accuracy for detect-
ing the introduction of the stressor. We want to use
MFCCs and TEO features, and also non-acoustic
features such as disfluency features. As mentioned
previously, we also hope to move beyond binary
classification, by acquiring perceived stress ratings
on a scale with more gradations.
References
P. Boersma and D. Weenink. 2010. Praat: doing pho-
netics by computer (version 5.1.29). Available from
http://www.praat.org/. [Computer program].
S. Casale, A. Russo, and S. Serrano. 2007. Multistyle classi-
fication of speech under stress using feature subset selec-
tion based on genetic algorithms. Speech Communication,
49:801?810.
K. Eberhard, H. Nicholson, S. Kubler, S. Gundersen, and M.
Scheutz. 2010. The Indiana ?Cooperative Remote Search
Task? (CReST) Corpus. In Proc. of LREC.
J.H.L. Hansen and S. Bou-Ghazale. 1997. Getting started
with SUSAS: a Speech Under Simulated and Actual Stress
database. In Eurospeech-97: International Conference on
Speech Communication and Technology.
J. Hansen and S. Patil, 2007. Speaker Classification I: Fun-
damentals, Features, and Methods, chapter Speech Under
Stress: Analysis, Modeling and Recognition, pages 108?
137. Springer-Verlag, Berlin, Heidelberg.
J. Hansen and B.Womack. 1996. Feature analysis and neural
network based classification of speech under stress. IEEE
Transactions on Speech & Audio Processing, 4(4):307?
313.
J. Hansen. 1996. Analysis and compensation of speech
under stress and noise for environmental robustness in
speech recognition. Speech Communications, Special Is-
sue on Speech Under Stress, 20(2):151?170.
J.F. Kaiser. 1990. On a simple algorithm to calculate the
energy of a signal. In Proc. of ICASSP.
A. Protopapas and P. Liberman. 2001. Fundamental fre-
quency of phonation and perceived emotional stress. Jour-
nal of the Acoustical Society of America, 101(4):2267?
2277.
S. Scherer, H. Hofmann, M. Lampmann, M. Pfeil, S. Rhinow,
F. Schwenker, and G. Palm. 2008. Emotion recognition
from speech: Stress experiment. In Proc. of LREC.
M. Sigmund, A. Prokes, and Z. Brabec. 2007. Statistical
analysis of glottal pulses in speech under psychological
stress. In Proc. of the 16th European Signal Processing
Conference.
F. J. Tolkmitt and K. R. Scherer. 1986. Effect of experi-
mentally induced stress on vocal parameters. Journal of
Experimental Psychology, 12(3):302?313.
G. Zhou. 1999. Nonlinear speech analysis and acoustic
model adaptation with applications to stress classification
and speech recognition. Ph.D. thesis, Duke University.
G. Zhou. 2001. Nonlinear feature based classification of
speech under stress. IEEE Transactions on Speech & Au-
dio Processing, 9:201?216.
256
