Extracting Causal Knowledge from a Medical Database
Using Graphical Patterns
Christopher S.G. Khoo, Syin Chan and Yun Niu
Centre for Advanced Information Systems, School of Computer Engineering
Blk N4, Rm2A-32, Nanyang Avenue
Nanyang Technological University
Singapore 639798
assgkhoo@ntu.edu.sg; asschan@ntu.edu.sg; niuy n@hotmail.com
Abstract
This paper reports the first part of a project
that aims to develop a knowledge extrac-
tion and knowledge discovery system that
extracts causal knowledge from textual da-
tabases. In this initial study, we develop a
method to identify and extract cause-effect
information that is explicitly expressed in
medical abstracts in the Medline database.
A set of graphical patterns were constructed
that indicate the presence of a causal rela-
tion in sentences, and which part of the
sentence represents the cause and which
part represents the effect. The patterns are
matched with the syntactic parse trees of
sentences, and the parts of the parse tree
that match with the slots in the patterns are
extracted as the cause or the effect.
1 Introduction
Vast amounts of textual documents and data-
bases are now accessible on the Internet and the
World Wide Web. However, it is very difficult
to retrieve useful information from this huge
disorganized storehouse. Programs that can
identify and extract useful information, and re-
late and integrate information from multiple
sources are increasingly needed. The World
Wide Web presents tremendous opportunities
for developing knowledge extraction and knowl-
edge discovery programs that automatically ex-
tract and acquire knowledge about a domain by
integrating information from multiple sources.
New knowledge can be discovered by relating
disparate pieces of information and by infer-
encing from the extracted knowledge.
This paper reports the first phase of a project
to develop a knowledge extraction and knowl-
edge discovery system that focuses on causal
knowledge. A system is being developed to
identify and extract cause-effect information
from the Medline database ? a database of ab-
stracts of medical journal articles and conference
papers. In this initial study, we focus on cause-
effect information that is explicitly expressed
(i.e. indicated using some linguistic marker) in
sentences. We have selected four medical areas
for this study ? heart disease, AIDS, depression
an  schizophrenia.
The medical domain was selected for two
reasons:
1. The causal relation is particular important in
medicine, which is concerned with devel-
oping treatments and drugs that can effect a
cure for some disease
2. Because of the importance of the causal re-
lation in medicine, the relation is more likely
to be explicitly indicated using linguistic
means (i.e. using words such as result, ef-
fect, cause, etc.).
2 Previous Studies
The goal of information extraction research is to
develop systems that can identify the passage(s)
in a document that contains information that is
relevant to a prescribed task, extract the infor-
mation and relate the pieces of information by
filling a structured template or a database record
(Cardie, 1997; Cowie & Lehnert, 1996; Gai-
zauskas & Wilks, 1998).
Information extraction research has been
influenced tremendously by the series of Mes-
sage Understanding Conferences (MUC-5,
MUC-6, MUC-7), organized by the U.S. Ad-
vanced Research Projects Agency (ARPA)
(http://www.muc.saic.com/proceedings/proceedi
ngs_index.html). Participants of the conferences
develop systems to perform common informa-
tion extraction tasks, defined by the conference
organizers.
For each task, a template is specified that
indicates the slots to be filled in and the type of
information to be extracted to fill each slot. The
set of slots defines the various entities, aspects
and roles relevant to a prescribed task or topic of
interest. Information that has been extracted can
be used for populating a database of facts about
entities or events, for automatic summarization,
for information mining, and for acquiring
knowledge to use in a knowledge-based system.
Information extraction systems have been devel-
oped for a wide range of tasks. However, few of
them have focused on extracting cause-effect
information from texts.
Previous studies that have attempted to ex-
tract cause-effect information from text have
mostly used knowledge-based inferences to infer
the causal relations. Selfridge, Daniell & Sim-
mons (1985) and Joskowsicz, Ksiezyk &
Grishman (1989) developed prototype computer
programs that extracted causal knowledge from
short explanatory messages entered into the
knowledge acquisition component of an expert
system. When there was an ambiguity whether a
causal relation was expressed in the text, the
systems used a domain model to check whether
such a causal relation between the events was
possible.
Kontos & Sidiropoulou (1991) and Kaplan
& Berry-Rogghe (1991) used linguistic patterns
to identify causal relations in scientific texts, but
the grammar, lexicon, and patterns for identify-
ing causal relations were hand-coded and devel-
oped just to handle the sample texts used in the
studies. Knowledge-based inferences were also
used. The authors pointed out that substantial
domain knowledge was needed for the system to
identify causal relations in the sample texts ac-
curately.
More recently, Garcia (1997) developed a
computer program to extract cause-effect infor-
mation from French technical texts without us-
ing domain knowledge. He focused on causative
verbs and reported a precision rate of 85%.
Khoo, Kornfilt, Oddy & Myaeng (1998) devel-
oped an automatic method for extracting cause-
effect information from Wall Street Journal texts
using linguistic clues and pattern matching.
Their system was able to extract about 68% of
the causal relations with an error rate of about
36%.
The emphasis of the current study is on ex-
tracting cause-effect information that is explic-
itly expressed in the text without knowledge-
based inferencing. It is hoped that this will result
in a method that is more easily portable to other
subject areas and document collections. We also
make use of a parser (Conexor?s FDG parser) to
construct syntactic parse trees for the sentences.
Graphical extraction patterns are constructed to
extract information from the parse trees. As a
result, a much smaller number of patterns need
be constructed. Khoo et al (1998) who used
only part-of-speech tagging and phrase bracket-
ing, but not full parsing, had to construct a large
number of extraction patterns.
3 Initial Analysis of the Medical Texts
200 abstracts were downloaded from the Med-
line database for use as our training sample of
texts. They are from four medical areas: depres-
sion, schizophrenia, heart disease and AIDs
(fifty abstracts from each area). The texts were
analysed to identify:
1. the different roles and attributes that are in-
volved in a causal situation. Causeand effect
are, of course, the main roles, but other roles
also exist including enabling conditions, size
of the effect, and size of the cause (e.g. dos-
age).
2. the various linguistic markers used by the
writers to explicitly signal the presence of a
causal relation, e.g. as a result, affect, re-
duce, etc.
3.1 Cause-effect template
The various roles and attributes of causal situ-
tions identified in the medical abstracts are
s ructured in the form of a template. There are
three levels in our cause-effect template, Level 1
giving the high-level roles and Level 3 giving
the most specific sub-roles. The first two levels
are given in Table 1. A more detailed description
i  provided in Khoo, Chan & Niu (1999).
The information extraction system devel-
oped in this initial study attempts to fill only the
m in slots of cause, effect and modality, without
attempting to divide the main slots into subslots.
Table 1. The cause-effect template
Level 1 Level 2
Object
State/EventCause
Size
Object
State/EventEffect
Size
Polarity (e.g. ?Increase?, ?Decrease?,
etc.)
Object
State/Event
Size
Duration
Condition
Degree of necessity
Modality (e.g. ?True?, ?False?,
?Probable?, ?Possible?, etc.)
Research method
Sample size
Significance level
Information source
Evidence
Location
Type of causal relation
Table 2. Common causal expressions for
depression & schizophrenia
Expression No. of
Occurrences
 causative verb  69
 effect (of) ?(on)  51
 associate with  35
 treatment of  31
 have effect on  28
 treat with  26
 treatment with  22
 effective (for)  14
 related to  10
Table 3. Common causal expressions for
AIDs & heart disease
 Expression  No. of
Occurrences
 causative verb  119
 have effect on  30
 effect (of)?(on)  25
 due to  20
 associate with  19
 treat with  15
 causative noun (including
nominalized verbs)
 12
 effective for  10
3.2 Causal expressions in medical texts
Causal relations are expressed in text in various
ways. Two common ways are by using causal
links and causative verbs. Causal links are words
used to link clauses or phrases, indicating a
causal relation between them. Altenburg (1984)
provided a comprehensive typology of causal
links. He classified them into four main types:
the adverbial link (e.g. hence, therefore), the
prepositional link (e.g. because of, on account
of), subordination (e.g. because, as, since, for,
so) and the clause-integrated line (e.g. that?s
why, the result was). Causative verbs are transi-
tive action verbs that express a causal relation
between the subject and object or prepositional
phrase of the verb. For example, the transitive
verb break can be paraphrased as to cause to
break, and the transitive verb kill can be para-
phrased as to cause to die.
We analyzed the 200 training abstracts to
identify the linguistic markers (such as causal
links and causative verbs) used to indicate causal
relations explicitly. The most common linguistic
expressions of cause-effect found in the Depres-
sion and Schizophrenia bstracts (occurring at
least 10 times in 100 abstracts) are listed in Ta-
ble 2. The common expressions found in the
AIDs and Heart Disease abstracts (with at least
10 occurrences) are listed in Table 3. The ex-
pressions listed in the two tables cover about
70% of the explicit causal expressions found in
the sample abstracts. Six expressions appear in
both tables, indicating a substantial overlap in
the two groups of medical areas. The most fr-
quent way of expressing cause and effect is by
using causative verbs.
4 Automatic Extraction of Cause-
Effect Information
The information extraction process used in this
study makes use of pattern matching. This is
similar to methods employed by other research-
ers for information extraction. Whereas most
studies focus on particular types of events or
topics, we are focusing on a particular type of
relation. Furthermore, the patterns used in this
study are graphical patterns that are matched
with syntactic parse trees of sentences. The pat-
terns represent different words and sentence
structures that indicate the presence of a causal
relation and which parts of the sentence repr-
sent which roles in the causal situation. Any part
of the sentence that matches a particular pattern
is considered to describe a causal situation, and
the words in the sentence that match slots in the
pattern are extracted and used to fill the appro-
priate slots in the cause-effect template.
4.1 Parser
The sentences are parsed using Conexor?s Func-
tional Dependency Grammar of English (FDG)
parser (http://www.conexor.fi), which generates
a representation of the syntactic structure of the
sentence (i.e. the parse tree). For the example
sentence
Paclitaxel was well tolerated and resulted in a
significant clinical response in this patient.
a graphical representation of the parser output is
given in Fig. 1. For easier processing, the syn-
tactic structure is converted to the linear con-
ceptual graph formalism (Sowa, 1984) given in
Fig. 2.
A conceptual graph is a graph with the
nodes representing concepts and the directed
arcs representing relations between concepts.
Although the conceptual graph formalism was
developed primarily for semantic representation,
we use it to represent the syntactic structure of
sentences. In the linear conceptual graph nota-
tion, concept labels are given within square
brackets and relations between concepts are
Fig. 1. Syntactic structure of a sentence
given within parentheses. Arrows indicate the
direction of the relations.
4.2 Construction of causality patterns
We developed a set of graphical patterns that
specifies the various ways a causal relation can
be explicitly expressed in a sentence. We call
them causality patterns. The initial set of pat-
terns was constructed based on the training set
of 200 abstracts mentioned earlier. Each abstract
was analysed by two of the authors to identify
the sentences containing causal relations, and the
p rts of the sentences representing the cause and
the effect. For each sentence containing a causal
relation, the words (causality identifiers) that
were used to signal the causal relation were also
iden ified. These are mostly causal links and
causative verbs described earlier.
Example sentence
Paclitaxel was well tolerated and resulted in a
significant clinical response in this patient.
Syntactic structure in linear conceptual
gr ph format
[tolerate]-
   (vch)->[be]->(subj)->[paclitaxel]
   (man)->[well]
   (cc)->[and]
   (cc)->[result]-
(loc)->[in]->(pcomp)->[response]-
(det)->[a]
(attr)->[clinical]->(attr)
->[significant],
(phr)->[in]->(pcomp)->[patient]
->(det)->[this],,.
Example causality pattern
[*]-
&(v-ch)->(subj)->[T:cause.object]
(cc|cnd)->[result]+-
(loc)+->[in]+->(pcomp)
->[T:effect.event]
(phr)->[in]->(pcomp)
->[T:effect.object],,.
Cause-effect template
Cause: paclitaxel
Effect: a significant clinical response in this
patient
Fig. 2. Sentence structure and causality
pattern in conceptual graph format
main
root
tolerate
be
v-ch
well and
man cc result
cc
in in
loc phr
response
pcomp
patient
pcomp
clinical
attr
a
det
this
det
significant
attr
We constructed the causality patterns for
each causality identifier, to express the different
sentence constructions that the causality identi-
fier can be involved in, and to indicate which
parts of the sentence represent the cause and the
effect. For each causality identifier, at least 20
sentences containing the identifier were ana-
lysed. If the training sample abstracts did not
have 20 sentences containing the identifier, ad-
ditional sentences were downloaded from the
Medline database. After the patterns were con-
structed, they were applied to a new set of 20
sentences from Medline containing the identi-
fier. Measures of precision and recall were cal-
culated. Each set of patterns are thus associated
with a precision and a recall figure as a rough
indication of how good the set of patterns is.
The causality patterns are represented in lin-
ear conceptual graph format with some exten-
sions. The symbols used in the patterns are as
follows:
1. Concept nodes take the following form:
[concept_label] or [concept_label:
role_indicator]. Concept_label can be:
? a character string in lower case, represent-
ing a stemmed word
? a character string in uppercase, refering to a
class of synonymous words that can occupy
that place in a sentence
? ?*?, a wildcard character that can match
any word
? ?T?, a wildcard character that can match
with any sub-tree.
Role_indicator refers to a slot in the cause-
effect template, and can take the form:
? role_label which is the name of a slot in the
cause-effect template
? role_label = ?value?, where value is a
character string that should be entered in
the slot in the cause-effect template (if
?value? is not specified, the part of the
sentence that matches the conc pt_label is
entered in the slot).
2. Relation nodes take the following form:
(set_of_relations). Set_of_relations can be:
? a relation_label, which is a character string
representing a syntactic relation (these are
the relation tags used by Conexor?s FDG
parser)
? relation_label | set of relations (?|? indi-
cates a logical ?or?)
3. &subpattern_label refers to a set of sub-
graphs.
Each node can also be followed by a ?+?
indicating that the node is mandatory. If the
mandatory nodes are not found in the sentence,
then the pattern is rejected and no information is
extracted from the sentence. All other nodes are
optional. An example of a causality pattern is
given in Fig. 2.
4.3 Pattern matching
The information extraction process involves
matching the causality patterns with the parse
trees of the sentences. The parse trees and the
ca sality patterns are both represented in the
linear conceptual graph notation. The pattern
matching for each sentence follows the follow-
ing procedure:
1. the causality identifiers that match with
keywords in the sentence are identified,
2. the causality patterns associated with each
matching causality identifier are shortlisted,
3. for each shortlisted pattern, a matching pro-
cess is carried out on the sentence.
The matching process involves a kind of
spreading activation in both the causality pattern
graph and the sentence graph, starting from the
node representing the causality identifier. If a
pattern node matches a sentence node, the
matching node in the pattern and the sentence
are activated. This activation spreads outwards,
with the causality identifier node as the center.
When a pattern node does not match a sentence
node, then the spreading activation stops for that
branch of the pattern graph. Procedures are at-
tached to the nodes to check whether there is a
match and to extract words to fill in the slots in
the cause-effect template. The pattern matching
program has been implemented in Java (JDK
1.2.1). An example of a sentence, matching pat-
tern and filled template is given in Fig. 2.
5 Evaluation
A total of 68 patterns were constructed for the
35 causality identifiers that occurred at least
twice in the training abstracts. The patterns were
appli d to two sets of new abstracts downloaded
from Medline: 100 new abstracts from the origi-
nal four medical areas (25 abstracts from each
area), and 30 abstracts from two new domains
(15 each) ? digestive system diseases and respi-
ratory tract diseases. Each test abstract was
analyzed by at least 2 of the authors to identify
?medically relevant? cause and effect. A fair
number of causal relations in the abstracts are
trivial and not medically relevant, and it was felt
that it would not be useful for the information
extraction system to extract these trivial causal
relations.
Of the causal relations manually identified
in the abstracts, about 7% are implicit (i.e. have
to be inferred using knowledge-based inferenc-
ing) or occur across sentences. Since the focus
of the study is on explicitly expressed cause and
effect within a sentence, only these are included
in the evaluation. The evaluation results are pre-
sented in Table 4. Recall is the percentage of the
slots filled by the human analysts that are cor-
rectly filled by the computer program. Precision
is the percentage of slots filled by the computer
program that are correct (i.e. the text entered in
the slot is the same as that entered by the human
analysts). If the text entered by the computer
program is partially correct, it is scored as 0.5
(i.e. half correct). The F-measure given in Table
4 is a combination of recall and precision
equally weighted, and is calculated using the
formula (MUC-7):
2*precision*recall / (precision + recall)
Table 4. Extraction results
Slot Recall
Preci-
sion
F-
Measure
Results for 100 abstracts from the
original 4 medical areas
Causality
Identifier
.759 .768 .763
Cause .462 .565 .508
Effect .549 .611 .578
Modality .410 .811 .545
Results for 30 abstracts from 2 new
medical areas
Causality
Identifier
.618 .759 .681
Cause .415 .619 .497
Effect .441 .610 .512
Modality .542 .765 .634
For the 4 medical areas used for building the
extraction patterns, the F-measure for the cause
and effect slots are 0.508 and 0.578 respectively.
If implicit causal relations are included in the
evaluation, the recall measures for cause and
effect are 0.405 and 0.481 respectively, yielding
an F-measure of 0.47 for cause and 0.54 for ef-
fect. The results are not very good, but not very
bad either for an information extraction task.
For the 2 new medical areas, we can see in
Table 4 that the precision is about the same as
for the original 4 medical areas, indicating that
the current extraction patterns work equally well
in th  new areas. The lower recall indicates that
n w causality identifiers and extraction patterns
need to be constructed.
The sources of errors were analyzed for the
set of 100 test abstracts and are summarized in
Table 5. Most of the spurious extractions (in-
formation extracted by the program as cause or
effect but not identified by human analysts) were
actually causal relations that were not medically
relevant. As mentioned earlier, the manual iden-
tification of causal relations focused on medi-
cally relevant causal relations. In the cases
wher  the program did not correctly extract
cause and effect information identified by the
analysts, half were due to incorrect parser out-
put, and in 20% of the cases, causality patterns
have not been constructed for the causality iden-
tifier found in the sentence.
We also analyzed the instances of implicit
causal relations in sentences, and found that
many of them can be identified using some
amount of semantic analysis. Some of them in-
volve words like when, after and with that indi-
cate a time sequence, for example:
? The results indicate that changes to 8-OH-
DPAT and clonidine-induced responses oc-
cur quicker with the combination treatment
than with either reboxetine or sertraline
treatments alone.
? There are also no reports of serious adverse
events when lithium is added to a monoam-
ine oxidase inhibitor.
? Four days after flupenthixol administration,
the patient developed orolingual dyskinetic
movements involving mainly tongue biting
and protrusion.
Table 5. Sources of Extraction Errors
A. Spurious errors (the program identified
cause or effect not identified by the hu-
man judges)
A1.The relations extraced are not relevant to medi-
cine or disease. (84.1%)
A2.Nominalized or adjectivized verbs are identified
as causative verbs by the program because of
parser error. (2.9%)
A3.Some words and sentence constructions that are
used to indicate cause-effect can be used to indi-
cate other kinds of relations as well. (13.0%)
B. Missing slots (cause or effect not ex-
tracted by program), incorrect text ex-
tracted, and partially correct extraction
B1.Complex sentence structures that are not in-
cluded in the pattern. (18.8%)
B2.The parser gave the wrong syntactic structure of
a sentence. (49.2%)
B3.Unexpected sentence structure resulting in the
program extracting information that is actually
not a cause or effect. (1.5%)
B4.Patterns for the causality identifier have not been
constructed. (19.6%)
B5.Sub-tree error. The program extracts the relevant
sub-tree (of the parse tree) to fill in the cause or
effect slot. However, because of the sentence
construction, the sub-tree includes both the cause
and effect resulting in too much text being ex-
tracted. (9.5%)
B6.Errors caused by pronouns that refer to a phrase
or clause within the same sentence. (1.3%)
In these cases, a treatment or drug is associated
with a treatment response or physiological event.
If noun phrases and clauses in sentences can be
classified accurately into treatments and treat-
ment responses (perhaps by using Medline?s
Medical Subject Headings), then such implicit
causal relations can be identified automatically.
Another group of words involved in implicit
causal relations are words like receive, get and
take, that indicate that the patient received a
drug or treatment, for example:
? The nine subjects who received p24-VLP
and zidovudine had an augmentation and/or
broadening of their CTL response compared
with baseline (p = 0.004).
Such causal relations can also be identified by
semantic analysis and classifying noun phrases
and clauses into treatments and treatment r-
sponses.
6. Conclusion
We have described a method for performing
automatic extraction of cause-effect information
from textual documents. We use Conexor?s FDG
parser to construct a syntactic parse tree for each
target sentence. The parse tree is matched with a
set of graphical causality patterns that indicate
the presence of a causal relation. When a match
is found, various attributes of the causal relation
(e.g. the cause, the effect, and the modality) can
then be extracted and entered in a cause-effect
template.
The accuracy of our extraction system is not
yet satisfactory, with an accuracy of about 0.51
(F-measure) for extracting the cause and 0.58
for extracting the effect that are explicitly ex-
pressed. If both implicit and explicit causal rela-
tions are included, the accuracy is 0.41 for cause
and 0.48 for effect. We were heartened to find
that when the extraction patterns were applied to
2 new medical areas, the extraction precision
was the same as for the original 4 medical areas.
Future work includes:
1. Constructing patterns to identify causal re-
lations across sentences
2. Expanding the study to more medical areas
3. Incorporating semantic analysis to extract
implicit cause-effect information
4. Incorporating discourse processing, includ-
ing anaphor and co-reference resolution
5. Developing a method for constructing ex-
traction patterns automatically
6. Investigating whether the cause-effect in-
formation extracted can be chained together
to synthesize new knowledge.
Two aspects of discourse processing is being
studied: co-reference resolution and hypothesis
confirmation. Co-reference resolution is impor-
t nt for two reasons. The first is the obvious rea-
son that to extract complete cause-effect infor-
mation, pronouns and references have to be
r solved and replaced with the information that
they refer to. The second reason is that quite of-
ten a causal relation between two events is ex-
pressed more than once in a medical abstract,
each time providing new information about the
causal situation. The extraction system thus
needs to be able to recognize that the different
causal expressions refer to the same causal
situation, and merge the information extracted
from the different sentences.
The second aspect of discourse processing
being investigated is what we refer to as hy-
pothesis confirmation. Sometimes, a causal rela-
tion is hypothesized by the author at the begin-
ning of the abstract. This hypothesis may be
confirmed or disconfirmed by another sentence
later in the abstract. The information extraction
system thus has to be able to link the initial hy-
pothetical cause-effect expression with the con-
firmation or disconfirmation expression later in
the abstract.
Finally, we hope eventually to develop a
system that not only extracts cause-effect infor-
mation from medical abstracts accurately, but
also synthesizes new knowledge by chaining the
extracted causal relations. In a series of studies,
Swanson (1986) has demonstrated that logical
connections between the published literature of
two medical research areas can provide new and
useful hypotheses. Suppose an article reports
that A causes B, and another article reports that
B causes C, then there is an implicit logical link
between A and C (i.e. A causes C). This relation
would not become explicit unless work is done
to extract it. Thus, new discoveries can be made
by analysing published literature automatically
(Finn, 1998; Swanson & Smalheiser, 1997).
References
Altenberg, B. (1984). Causal linking in spoken and
written English. Studia Linguistica, 38(1), 20-69.
Cardie, C. (1997). Empirical methods in information
extraction. AI Magazine, 18(4), 65-79.
Cowie, J., & Lehnert, W. (1996). Information extrac-
tion. Communications of the ACM, 39(1), 80-91.
Finn, R. (1998). Program Uncovers Hidden Connec-
tions in the Literature. Th  Scientist, 12( 0), 12-13.
Gaizauskas, R., & Wilks, Y. (1998). Information
extraction beyond document retrieval. Journ l of
Documentation, 54(1), 70-105.
Garcia, D. (1997). COATIS, an NLP system to locate
expressions of actions connected by causality links.
In Knowledge Acquisition, Modeling and Ma-
agement, 10th European Workshop, EKAW ?97
Proceedings (pp. 347-352). Berlin: Springer-
Verlag.
Joskowsicz, L., Ksiezyk, T., & Grishman, R. (1989).
Deep domain models for discourse analysis. In The
Annual AI Systems in Government Conference (pp.
195-200). Silver Spring, MD: IEEE Computer So-
ciety.
Kaplan, R. M., & Berry-Rogghe, G. (1991). Knowl-
edge-based acquisition of causal relationships in
text. Knowledge Acquisition, 3(3), 317-337.
Khoo, C., Chan, S., Niu, Y., & Ang, A. (1999). A
method for extracting causal knowledge from tex-
tual databases. Singapore Journal of Library &
Information Management, 28, 48-63.
Khoo, C.S.G., Kornfilt, J., Oddy, R.N., & Myaeng,
S.H. (1998). Automatic extraction of cause-effect
information from newspaper text without knowl-
edge-based inferencing. Literary and Linguistic
Computing, 13(4), 177-186.
Kontos, J., & Sidiropoulou, M. (1991). On the acqui-
sition of causal knowledge from scientific texts
with attribute grammars. Expert Systems for Infor-
mation Management, 4(1), 31-48.
MUC-5. (1993). Fifth Message Understanding Co-
fer nce (MUC-5). San Francisco: Morgan Kauf-
mann.
MUC-6. (1995). Sixth Message Understanding Con-
ference (MUC-6). San Francisco: Morgan Kauf-
mann.
MUC-7. (2000).  Message Understanding Confer-
e ce proceedings (MUC-7) [Online]. Available:
http://www.muc.saic.com/proceedings/muc_7_toc.
html.
Selfri ge, M., Daniell, J., & Simmons, D. (1985).
Learning causal models by understanding real-
world natural language explanations. In The Sec-
ond Conference on Artificial Intelligence Applica-
tions: The Engineering of Knowledge-Based Sys-
tems (pp. 378-383). Silver Spring, MD: IEEE
Computer Society.
Sowa, J.F. (1984). Conceptual structures: Informa-
 processing in man and machine. Reading,
MA: Addison-Wesley,.
Swanson, D.R. (1986). Fish oil, Raynaud?s Syn-
drome, and undiscovered public knowledge. Per-
spectives in Biology and Medicine, 30(1), 7-18.
Swanson, D.R., & Smalheiser, N.R. (1997). An inter-
active system for finding complementary litera-
tures: A stimulus to scientific discovery. Artificial
Intelligence, 91, 183-203.
Proceedings of the 14th European Workshop on Natural Language Generation, pages 125?135,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Deconstructing Human Literature Reviews ?  
A Framework for  Multi-Document Summar ization  
Kokil Jaidka, Chr istopher  S.G. Khoo, J in-Cheon Na 
Division of Information Studies 
Wee Kim Wee School of Communication and Information 
Nanyang Technological University, Singapore 
[kokil, chriskhoo]@pmail.ntu.edu.sg, tjcna@ntu.edu.sg 
Abstract 
This study is conducted in the area of multi-
document summarization, and develops a 
literature review framework based on a 
deconstruction of human-written literature 
review sections in information science research 
papers. The first part of the study presents the 
results of a multi-level discourse analysis to 
investigate their discourse and content 
characteristics. These findings were 
incorporated into a framework for literature 
reviews, focusing on their macro-level 
document structure and the sentence-level 
templates, as well as the information 
summarization strategies. The second part of 
this study discusses insights from this analysis, 
and how the framework can be adapted to 
automatic summaries resembling human written 
literature reviews. Summaries generated from a 
partial implementation are evaluated against 
human written summaries and assessors? 
comments are discussed to formulate 
recommendations for future work.  
1 Introduction 
This project proposes a framework for literature 
reviews, which has applications in automatic 
summarization of scientific papers. A literature 
review is the traditional multi-document summary 
of research papers which is constructed by a 
researcher to survey previous findings and its 
structure follows certain linguistic rules. Several 
studies have identified that literature reviews are 
used to achieve distinct rhetorical purposes (Hart, 
1998; Bourner, 1996; Boot & Beile, 2005; Jonsson, 
2006; Massey, 2006; Torraco, 2005; Hinchliffe, 
2003; Bruce, 1994), such as to: 
? Compare and contrast previous research. 
? Identify gaps in the literature 
? Identify new research questions 
? Define the proposed research contributions 
? Build the justification for the current work 
? Situate the work in the research literature 
? Reinterpret and critique previous results 
These rhetorical characteristics of literature 
reviews make it a challenging research problem in 
automatic multi-document summarization ? not 
only should the summarizer identify salient 
information, but it should also synthesize the 
summary in a way that achieves certain 
argumentative purposes. The problem of 
summarization in context was first identified by 
Sparck Jones and Endres-Niggemeyer (1995) and 
subsequently in Sparck Jones? follow-up article 
(2007), wherein they questioned the usefulness of 
state-of-the-art summarization methods in 
addressing users? information needs. As articulated 
by Sparck Jones (2007) and echoed by Nenkova 
and McKeown (2011), summarization needs to be 
viewed as a part of the larger discourse (academic 
writing) it belongs to, tailored to the purpose 
(literature review) of summarization, the reader (in 
this case, a researcher) and the genre being 
summarized (research papers). Motivated by this 
research gap, we outline the aims of our analyses: 
? To identify how to emulate the purpose of 
literature reviews, we conducted a 
discourse analysis to identify the macro-
level structure and the sentence-level 
linguistic expressions embedded in 
literature review sections. 
? To identify the relationship between 
research paper and literature review, we 
conducted an information analysis to 
identify rules for selecting and 
125
transforming information from research 
papers. 
The focus of the paper is to draw insights from the 
framework to propose strategies for automatic 
literature review generation. An automatic 
summary fashioned as a literature review can 
function as a tool to help literature review writers 
by pointing out ways in which information in the 
source papers can be compared and integrated. For 
information searchers, it can provide a 
customisable overview of a set of retrieval results 
that is more readable and more logical than a list of 
salient sentences. 
2 Previous Work 
This paper investigates the human summarization 
process through an extensive discourse analysis. 
Human summarization is a process comprising 
document exploration to investigate the document 
macrostructure, relevance assessment by 
constructing a mental representation and summary 
production by selecting and transforming text from 
the source(s) (Endres-Niggemeyer, Maier, and 
Sigel, 1995). The underlying principle is the theory 
of human synthesis of information, by Van Dijk 
and Kintsch (1983). 
This study proposes a linguistically motivated 
framework for summarization. In previous work, a 
summarization framework developed by Marcu 
(2000) compressed information from general texts 
by identifying rhetorical relationships between 
clauses and sentences, and extracting sentence 
nuclei. Shiyan, Khoo & Goh (2008) summarized 
social science dissertation abstracts by referencing 
a social science taxonomy to identify important 
information and a specially constructed knowledge 
bank to identify important inter-relationships. In 
earlier work, a summarization framework designed 
by Teufel and Moens (2002) identified 7 categories 
of scientific arguments and extracted single-
document summaries from chemistry and 
computational linguistics papers (Teufel, 
Siddharthan & Batchelor, 2009) based on user?s 
queries. However, it required large corpora of 
manually annotated papers to be applied to any 
field, and it generated only single-document 
summaries.  
Some other scientific summarization systems 
aim to model information relationships accurately 
without concerning themselves with summary 
structure. Centrifuser, a framework for 
summarizing medical literature (Elhadad, Kan, 
Klavans and McKeown, 2005) produced a multi-
document, query-focused indicative summary 
highlighting the similarities and differences 
between source documents. The topic tree for the 
final summary was constructed offline by 
clustering a large number of documents, thus it 
was not suitable for real-time user queries. In a 
related recent approach, Hoang and Kan (2010) 
presented preliminary results from automatically 
generating related work sections for a target paper 
by taking a hierarchical topic tree as an input; 
however, the requirement of a pre-conceived topic 
tree limits the scalability of this system. To sum 
up, these scientific summarization systems are 
typically delimited by their scalability and 
generalizability for multiple documents and 
domains. 
Newer approaches in scientific paper 
summarization rely on preselected information 
cited in other papers to judge whether information 
is influential or not, and generate a multi-document 
summary of a topic (Nanba, Kando & Okumura, 
2011) or a single document summary for a paper 
using its relevant cited information (Qazvinian & 
Radev, 2008). A system for generating literature 
surveys through citations was proposed by 
Mohammad et al (2009) which applied superficial 
analysis of research paper citation sentences to 
suggest model sentences; the present study 
describes parallel efforts to refine a summarization 
framework after extensive discourse analysis.  We 
consider providing not just a synopsis of 
information, but also integrating the synopsis with 
the contextual and rhetorical features which make 
a human written literature review a coherent, 
cohesive and useful reference. Our study thus 
addresses a different, and more challenging, set of 
objectives than the citation-based summarizers of 
recent work. 
3 Developing the Literature Review 
Framework  
Following the first research aim, we carried out an 
analysis of the discourse structure of a sample of 
30 literature review sections in research papers 
haphazardly selected from the Journal of the 
American Society for Information Science and 
Technology between the years 2000-2008, 2 or 3 
126
from each year. On average, a literature review 
section was 1146 words in length and it cited 17 
studies. The texts were analyzed at 3 levels of 
detail: 
? Macro-level document structure: to identify 
the different sections of the literature, the 
types of information they contain and how 
they are organized hierarchically. 
? Sentence-level rhetorical structure: to 
identify how sentences are framed according 
to the overall purpose of the literature 
review. 
? Summarization strategies: to identify how 
information was selected and synthesized 
for the literature review. 
Preliminary findings of these discourse analyses 
have been discussed in previous work by the 
authors, notably, in a discussion of the features of 
the macro-structure of information science 
literature reviews (Khoo, Na & Jaidka, 2011), 
rhetorical functions found in literature reviews 
(Jaidka, Khoo & Na, 2010) and associations 
between sections in source papers and their citing 
sentences in literature reviews (Jaidka, Khoo & 
Na, 2013). The current study applies the discourse 
characteristics thus identified to develop and test a 
literature review framework for multi-document 
summarization. 
3.1 Designing Document Structure Templates 
As noted in academic writing textbooks (Hart, 
1998), literature reviews are structured as a 
hierarchy of topics and each ?paragraph? fulfills 
certain functions. To identify these macro-
structures and their functions, we conducted this 
discourse analysis, proceeding with the assumption 
that a literature is structured as a set of topic 
elements, with each topic having a set of 
embedded study elements (i.e. descriptions of 
research studies relevant to the topic). An 
exploratory study was conducted to identify the 
structures within these topics and their hierarchical 
relationships. Two Research Assistants holding 
graduate degrees annotated every sentence with  
one or more of the following tags:  
? title tag: to provide a statement of the topic 
theme or study objective 
? description tag: to encapsulate the details of 
the topic or study 
? meta-summary tag: to provide the writers? 
comments as an overview summary of the 
research in the field  
? meta-critique tag: to contain the writers? 
critique or interpretation of cited studies, 
critical comparison of research or 
justification for the current study 
? current-study tag: to refers to and compare 
with the current work being described in the 
paper. 
? method and result tags: to provide a 
description of the research methods and 
research results reported in the cited papers. 
In this coding scheme, the meta-summary and 
meta-critique tags provide the writers? comments, 
citing one or more studies together.  The rest of the 
elements comprise descriptive text about 
individual studies. The average inter-coder 
reliability score (Cohen?s Kappa) obtained was 
high at 0.76. Disagreements between the coders 
were resolved through discussion until a mutually 
agreeable solution was reached. The analysis 
identified different types of literature reviews as 
well as different structures. In our literature review 
framework, these findings suggested rules for 
generating different types of literature reviews: 
? Integrative literature reviews should 
comprise a large proportion of meta-
summary and meta-critique elements. This 
is because they discuss and critique ideas 
from a number of studies in a high-level 
summary. 
? Descriptive literature reviews should 
report the results of individual studies in 
detail, outlining their methodology and 
recommendations. This is because they 
were found to comprise significantly more 
study elements.  
? Integrative literature reviews should be 
organized as a hierarchical structure with 
embedded topics. Comparatively, 
descriptive literature reviews should be 
organized as a flat structure, with many 
more topic elements per text but less 
embedded topics. This is because 
127
integrative literature reviews were found to 
comprise an average of 2.5 embedded 
topics, and descriptive literature reviews 
had an average of 1.4 embedded sub-
topics.  
These rules have been applied in designing several 
integrative and descriptive literature review 
templates. Fig 1 illustrates one of the template 
integrative literature reviews we designed. It 
comprises a level 1 starting topic which acts as the 
overall topic of the literature review. The topic has 
other sub-topic elements within it, each of which 
begins with a meta-summary element which 
introduces it, followed by study elements to 
illustrate it. The topic elements determine the 
logical organization of the literature review; meta-
summary are incorporated into the structure 
because they provide research overviews and 
highlight the similarities across related papers. The 
study elements highlight the unique features   for 
individual papers. These templates will be 
instantiated in the automatic literature review 
generation process. 
 
Figure 1. A template document structure in the  
literature review framework 
3.2 Designing Sentence Templates 
Previous studies of literature reviews (Bunton, 
2002; Kwan, 2006) have highlighted the broad 
rhetorical ?moves? which organize the text, but 
none have attempted to identify their linguistic 
structure or specific functions. In the clause-level 
analysis, we annotated linguistic expressions 
framing research descriptions, defined as discourse 
markers by Hyland (2004). Although discourse 
markers include generic logical connectives such 
as ?so?, ?therefore? and ?because?, we followed 
Teufel?s criteria (Teufel, 1999 pp. 76) to focus on 
only those discourse markers which are used in 
scientific discourse to perform one of the functions 
listed below: 
? Describe a topic: Present a broad overview 
of research (e.g., ?Previous research 
focused on?) or its context (e.g., ?Research 
in the area of?) 
? Describe a study: Cite an author (e.g., ?In 
a study by?) or describe research processes 
(e.g., ?X identified??, ?Y has conducted 
an experiment to??) 
? Compare studies: Highlight similarities or 
differences in research (e.g., ?Several 
studies have applied?). 
? Provide additional information: Frame 
examples or enumerate research studies 
(e.g., ?For example?, ?A list includes?). 
It was found that a total of 110 expressions were 
used in 1298 variations to frame different types of 
information in different ways and achieve different 
rhetorical functions. We have applied these 
findings in the literature review framework to 
develop sentence templates for text generation, and 
to formulate rules for selecting templates which are 
significantly associated with the type of literature 
review and discourse element to be populated: 
? In integrative literature reviews: apply regular 
expressions which describe research objectives 
in the description elements. In the meta-
summary elements in integrative literature 
reviews, apply expressions which ?state the 
common aims?.  
? In descriptive literature reviews: choose 
expressions which ?state the research method? 
and ?state the common approaches? in the 
description and meta-summary elements. 
Regular expressions are applied for text-to-text 
generation, serving as a means to extract 
information from source papers as well as to map 
them into appropriate sentence templates. Those 
applied to extract and instantiate research objective 
sentences within topics, studies and comparisons 
are illustrated in Table 1. 
3.3 Designing Information Selection and 
Summar ization Strategies 
In accordance with the second research aim, we 
conducted a content analysis to identify the 
relationship between the source papers and the 
final literature review. Similar work describing text 
editing strategies has been done by Jing and 
STUDY STUDY META-SUMMARY 
TOPIC 
TOPIC 
TOPIC META-SUMMARY 
128
McKeown (1999); however, in this analysis we 
extend their objectives to additionally identify: 
? The source sections of the paper from 
where information was selected (i.e., 
Abstract Introduction, Methodology, 
Results or Conclusion). 
? The types of transformations used to 
convert the source sentence to the 
referencing sentence (i.e., copy-paste, 
paraphrase, or higher-level summary). 
? Identifying the types of information 
selected from the source papers (i.e., 
objective, methodology, results and critical 
summary). 
? Analysis of the reasons for preference of 
one source sentence over another, despite 
providing similar information. This was 
inferred by comparing candidate source 
sentences against each other. 
The corpus for analysis was constructed by 
analyzing the 20 literature reviews line-by-line and 
retaining all the sentences referencing previous 
work, either explicitly (e.g., ?X and Y (1998) 
conducted experiments in transitive translation?) or 
implicitly by adding onto the details of a cited 
study (e.g., ?Studies have also focused on users' 
mental models of information seeking (X, 1989)?.  
A total of 349 references were collected from 
the twenty literature review sections. Sentence 
providing definitions, or citing sources other than 
research papers, were further discarded because 
they lay outside the scope of our analysis. The 
findings, revealed that more than a quarter of all 
selected information was from the Abstract of the 
source paper. The information selected by the 
reviewer is copy-pasted more often in descriptive 
as compared to integrative literature reviews. Some 
of these findings have been applied to suggest 
strategies for information selection and 
summarization in the literature review framework: 
? For research objective information: 
choose sentences from the Abstract and 
Introduction of source papers; copy-paste 
it into descriptive literature reviews, but 
paraphrase it in integrative literature 
reviews. 
? In descriptive literature reviews: provide 
detailed method information, copy-pasted 
from the Introduction or Method of source 
papers. 
? In integrative literature reviews: provide 
detailed result information, summarized at 
a higher level from the Results and 
Conclusions.  
When more than one sentence provides the same 
factual information, the sentence selection criteria 
listed in Table 2 should be followed to choose the 
more concise alternative. 
Function 
Type of Information 
Required 
Regular  Expression which map into Sentence Templates 
Describe 
a topic 
 
Introduce a topic through its 
research aspects 
Introduce a topic through its 
literature review 
Introduce area of research 
(Researchers | Research) (have |has) (in | are concerned with | 
have addressed |proposed | observed | investigated | focused on) 
The (literature review | prior work) (covered | dealt with | looked 
at | focused on )?  
research | studies | findings) in the (field | area | domain | 
context) of 
Describe 
a study 
State the study objective 
 
 
State the study motivation 
State the study hypothesis 
(the study | we | who) (conducted |explored | proposed | pursued 
| described | attempted to | represented | analyzed | examined | 
investigated |deals with | seeks to discover) 
(The | Their) underlying research (question | objective) (was |is) 
 (They) (argue | opine | hold |debate | believe) that 
Compare 
studies 
State the common aim of 
studies 
The (common)? (issue | motivation |aim | principle) (for | 
behind) (many | most| some| these| such | existing) studies 
(Many| Most |These | Some | Such | Existing | Various)? (studies 
| work) have (explored | focused on) 
Table 1. Regular expressions obtained from clause-level analysis
129
Type of Cr iter ia Order  of Pr ior ity 
Lexical 
? ?This article/paper...?  
? ?The aim/goal/objective is??  
? ?We present/ describe...?  
? ?Recent research into...?  
? Sentences with how/what/why questions 
Syntactic 
? Sentence having the main topic in its main clause 
? The sentence with fewer clauses 
? The sentence with no back-referencing 
Surface 
? Sentence from the first paragraphs of a section 
? The title of the source paper 
? The sentence which is the shortest  
Table 2. Criteria for selecting sentences
4 Evaluation 
To evaluate the framework, the objective was to 
compare its ?human-ness? represented by its 
Comprehensibility, Readability and Usefulness 
against human-written literature reviews and 
machine-generated sentence extracts. For this 
purpose, the framework was partially adapted in a 
summarization method focusing on comparing 
research objective information extracted from 
Abstracts and Introduction sections, and presenting 
a topical overview resembling a three-level 
literature review. The output generated is similar to 
the summaries generated by Centrifuser (Elhadad 
et al, 2005) ? sentences are extracted to provide a 
synopsis of similarities and unique features of 
studies are highlighted for individual papers; 
however our prototype does so without rely on 
external domain knowledge. The method was 
implemented in Java on the Eclipse IDE, and it 
comprised three stages: 
? Text pre-processing: to extract sentences 
from the Abstract and Introduction of the 
input source papers. Here the text is 
segmented, tokenized, parsed, stop-words 
are filtered and n-grams of noun phrases 
are created to represent concepts in the 
source papers. 
? Information selection and integration: to 
identify similarities and differences across 
the research objective sentences of source 
papers. It selects important concepts based 
on the document frequency of lexical 
concept chains (Barzilay and McKeown, 
2005), and applies the research objective 
sentence selection rules developed in the 
framework to select important information 
for summarization. 
? Text presentation: to produce text that has 
the characteristics of the literature review. 
It applies the document structure described 
in the framework, to organize the literature 
review, and sentence templates particular 
to research objective information in 
integrative literature reviews (the ones 
listed in Table 1). 
The resultant summaries resemble a human written 
literature review because they are laid out as a 
topic tree and present a comparative overview of 
similarities and unique features. However, some 
grammatical errors can be spotted, which would 
need a post-processing module to remove.  
30 sets of information science source papers 
were prepared by sampling topics from 30 
literature reviews from 2000-2008 issues of 
JASIST, Journal of Documentation and Journal of 
Information Science and downloading the papers 
they cited. Only 3-10 source papers were 
downloaded for every sampled topic; this was so 
that the task could be manageable for the 
researchers constructing the human summaries. An 
excerpt system summary is provided in Table 3.  
For each input set of related research papers, 
three types of summaries were generated, each 
with a different kind of method ? framework-based 
structure (by our method), sentence-extraction 
structure (by the baseline, MEAD) and a human-
written summary by a researcher:  
? MEAD: The MEAD summarization 
system (Radev, Jing, Stys, & Tam, 2004) 
was the baseline; it followed a sentence-
130
extraction approach to generate multi-
document extracts of information 
(generally news articles). 
? System: Our system based on the 
framework, and focusing on the 
similarities and differences between 
research objectives at the lexical and 
syntactic level.  
? Human: Five researchers from the School 
of Humanities and Social Sciences of our 
university summarized the research 
objective sentences from set of source 
papers in the context of a given (main) 
topic. 
This literature review presents research in 
relevance published by Barry (1994), Harter (1992), 
Tang and Solomon (1998), Vakkari and Hakala 
(2000) and Wang and Soergel (1998).  
Studies by Barry (1994) and Tang et al (1998) 
focus on retrieval mechanism.  
Researchers in relevance have also considered 
users (Harter, 1992; Vakkari et al, 2000; Wang et 
al., 1998).  
The study by Vakkari et al (2000) demonstrates that 
it is productive to study relevance as a task and 
process-oriented user construct.  
Studies by Wang et al (1998) and Tang et al (1998) 
focus on dynamic models.  
The study by Tang et al (1998) is a step in the 
empirical exploration of the evolutionary nature of 
relevance judgments. 
Table 3: Excerpt from a system summary 
In the human summaries, the coders selected an 
average of 3 sub-topics and 8 unique sub-topics in 
their summaries. Human summaries also had the 
highest compression rate of 18%, as compared to a 
compression rate of 25% by MEAD and our 
System. An inter-coder agreement was conducted 
over 10 summaries by taking the summaries done 
by one of the post-graduate researchers as 
reference and comparing each pair of summaries, 
considering each of the ?similarities? or 
?differences? as a ?common? or ?unique? sub-
topic. Comparisons revealed that the coders 
usually had the same idea of what constituted an 
important ?similarity? or common sub-topic 
(percent agreement= 70%) though they often chose 
different ?differences? or unique sub-topics in their 
summaries (percent agreement= 56%). 
Content evaluation of the 30 sets of summaries 
by the ROUGE-1 metric (Lin & Hovy, 2003) 
revealed that system summaries had a higher but 
not significantly different effectiveness or f-
measure of 0.38 as compared to the baseline 
(0.33). We developed our own version of ROUGE 
to measure information overlap by comparing the 
information concepts extracted from summaries. It 
was different from the standard ROUGE-1 in three 
ways: it filtered out ?research stopwords? such as 
?method?, ?experiment? and ?study?, which didn?t 
represent research information; it aggregated 
words which shared the same lemma; and it also 
conflated co-occurring adjacent words into the 
same information concepts. Consequently, we 
obtained real scores of effectiveness in terms of 
higher f-measure scores for both the system and 
the baseline. The system?s f-measure (0.57) was a 
significant improvement over the baseline (0.50) at 
the 0.01 level. The results are provided in Table 4. 
For the quality evaluation, 90 questionnaires 
were prepared from the 30 sets of summaries, 
using permutations of presentation orders to 
account for carry-over effects during assessment. 
To recruit assessors, a call for participation in the 
evaluation was broadcast over the internet, through 
postings in discussion boards, personal emails and 
library sciences mailing lists. The invitation was 
also personally extended to authors of other 
publications in JASIST, JDoc and JIS. The 
invitation for participation was restricted to only 
Library and Information Science and Computer 
Science researchers and PhD students who had 
passed their qualifying exam. It was anticipated 
that such assessors would be more familiar with 
the topics in the summary, and would be able to 
make meaningful comments about the summaries 
and their characteristics, such as lack of evident 
comparisons and generalizations, or incorrect 
comparisons and generalizations among unlike 
information. There were a total number of 35 
assessors with a mean research experience of 6 
years, who provided 67 responses, by filling out 1 
or 2 each, over a period of two months. The 
assessors were from reputable international 
universities in different countries. The highest 
degrees held by the assessors varied from 
Bachelors (for PhD students who had passed their 
qualifying exam) to PhD. They scored the 
131
summaries on their Comprehensibility, Readability 
and Usefulness and also provided qualitative 
comments to the following questions: 
? What did you like about this summary? 
? What did you find confusing about this 
summary? 
? How is this summary, a good/bad literature 
review? 
The quantitative results in Table 5 show that the 
System summary was significantly more readable 
and more useful than the baseline at the 0.05 level. 
The qualitative results (provided in Table 6) are 
equally interesting and show that researchers with 
different number of years of research liked or 
disliked different things about the System 
summary. Researchers with 0-4 years of 
experience did not have any specific preference of 
one type of summary over another. Researchers 
with 5-8 years of experience were more conscious 
of grammatical errors and repetition mistakes in 
the system summary. Researchers with 9-12 years 
of experience ignored the grammatical errors in 
Human summaries and System and instead 
criticized their lack of detail. Researchers with 13 
years or experience or more were sensitive to the 
overall ?context? and ?flow? of the summary. Most 
of the assessors were able to identify the main 
topic and its related sub-topics; however, they 
experienced the System as being more disjointed, 
lacking ?focus? as compared to the Human 
summaries. On the whole, researchers were 
satisfied with the overview provided as well as the 
hierarchical organization. It would be interesting to 
see whether these findings and differences would 
be replicated in a larger study. 
Measures System MEAD 
Recall 0.70 0.63 
Precision 0.49 0.44 
F-measure 0.57 0.50 
Table 4. Results from the content evaluation 
(N=30) 
 MEAD System Human 
Comprehensibility 5.6 5.6 6.2 
Readability 4.9 5.3 5.6 
Usefulness 5.7 6.4 6.3 
Table 5. Results from the quality evaluation 
(N=67) 
5 Conclusion and Future Work 
This study has analyzed how authors select 
information, transform it and organize it in a 
definite discourse structure as a literature review. 
Our findings identified two styles of literature 
reviews ? the integrative and descriptive literature 
reviews, with different profiles of discourse 
elements and rhetorical expressions. Integrative 
literature reviews present information from several 
studies in a condensed form as a critical summary, 
possibly complemented with a comparison, 
evaluation or comment on the research gap. The 
focus is on highlighting relationships amongst 
concepts or comparing studies against each other. 
Descriptive reviews present experimental detail 
about previous studies, such as the approach 
followed, their results and evaluation. The focus is 
on providing important details of previous studies 
in a concise form. 
From these findings, we conjecture that authors 
begin a literature review with an overall strategy in 
mind. They select and edit the information content 
based on the style of literature review. They may 
choose to write an integrative style of literature 
review to guide the reader along a critical survey 
of previous research. To support their argument, 
they paraphrase information selected from the 
Abstract and Conclusion sections, and integrate 
information from the Results sections into a high-
level overview of important findings. Accordingly, 
they choose the discourse structure and linguistic 
expressions to frame their argument. 
Our framework has since been validated on a 
larger sample size of 90 articles selected from 3 
top journals in information science. It is 
recommended for application in a complete 
automatic literature review generation system, 
wherein a user would be able to control the style of 
literature review, the level of detail and analysis 
required, as well as the structure of the layout and 
the number of topics. At the information selection 
stage, it would be able to apply different 
information selection and transformation strategies 
to generate different parts of a literature review. At 
the text generation stage, it would be able to 
introduce a topic and describe its context and core 
concepts, describe a study and its objectives, 
methods and findings, delineate a research gap and 
identify the common and different features among 
studies, and illustrate its argument with examples.
132
 Year   0-4 Year   5-8 Year   9-12 Year   13+ 
C
om
p
re
h
en
si
b
il
it
y 
- It gives a good 
overview on the 
topic and points 
 
- I liked the structure. 
-  It summarizes the 
research and connects 
the authors to the topic 
by the use of "these 
authors." 
- It's not too short nor 
too long. 
- Easy to read and 
understand. 
-  It is better review 
than the others 
because it tries to tie 
the literature together 
in some fashion. 
- There seemed to be no reason 
for the ordering of the 
sentences about the different 
research papers 
- Each individual statement in 
the summary seems relevant 
(of some objective value) by 
itself, but all together lacks 
uniformity in subject. 
- However it does seem to get 
the core issues. 
R
ea
d
ab
il
it
y 
- Continuity 
- Yet, the 
linking of 
sentence could 
be better. 
- Too many 
repetitions, but 
gives some 
information 
 
- This summary is 
neither readable nor 
informative. 
- The same studies are 
cited several times 
- It kept repeating all 
the studies. 
- It felt very disjointed, 
maybe because of all 
the small paragraphs. 
- Badly written, hard 
to read. 
- It flows well 
- Has some sentences 
seemingly unrelated 
to neighboring 
sentences 
- Generally easy to read. 
- There are a few mistakes in 
grammar, which is distracting. 
- Very readable. 
- Like:  seems to have a bit of 
flow. 
U
se
fu
ln
es
s 
- This summary 
seems quite 
good 
- I feel I got an 
overview over 
the research in 
the area. 
- The summary 
covered a good 
deal of literature 
- The overview 
is nice but still 
really flat. 
- This is the best 
summary of the 
sample. 
- Comprehensively 
covers the text  
- The summary 
provides information 
about groups of studies 
researching certain 
topics 
- This summary 
provides an overview  
of research in web 
search with more 
informative details 
- Comparison 
between studies is 
helpful.  
- More info required 
about study, 
including methods, 
findings. 
- It would be pretty 
useful for lit review. 
- While comparisons 
of different papers 
are well done, it 
would also be useful 
to have more 
description of each 
study. 
- Should give an indication of 
these trends in order to help 
the reader contextualize the 
research field. 
- There is an attempt at 
relating studies to each other 
so that one gets an overview of 
the research area. 
Table 6. Comments on System by assessors with different years of research experience
References 
Barzilay, R., & McKeown, K. R. (2005). Sentence 
fusion for multidocument news summarization. 
Computational Linguistics, 31(3), 297-328. 
Boote, D. N., & Beile, P. (2005). Scholars before 
researchers: On the centrality of the dissertation  
 
 
 
literature review in research preparation. 
Educational researcher, 34(6), 3-15. 
Bourner, T. (1996). The research process: four steps to 
success. Research methods: guidance for 
postgraduates, Arnold, London, 7-11. 
133
Bruce, C. S. (1994). Research students' early 
experiences of the dissertation literature review. 
Studies in Higher Education, 19(2), 217-229. 
Bunton, D. (2002) Generic moves in Ph.D Introduction 
chapters. In J. Flowerdew (Ed.), Academic 
Discourse. London: Longman. 
Cooper, H. M. (1988). The structure of knowledge 
synthesis. Knowledge in Society, 1, 104-126. 
Hoang, C., & Kan, M.Y.  2010. Towards automated 
related work summarization. In Proceedings of the 
23rd International Conference on Computational 
Linguistics (COLING?10): Posters (pp. 427?435). 
DUC. (2002). The Document Understanding 
Conference. Retrieved Oct 2010, from 
http://duc.nist.gov. 
Elhadad, N., Kan, M. Y., Klavans, J. L., & McKeown, 
K. R. (2005). Customization in a unified framework 
for summarizing medical literature.Artificial 
Intelligence in Medicine, 33(2), 179. 
Endres-Niggemeyer, B., Maier, E., & Sigel, A. (1995). 
How to implement a naturalistic model of 
abstracting: four core working steps of an expert 
abstractor. Information Processing & Management, 
31(5), 631-674. 
Guo, Q., & Li, C. (2007, August). The Research on the 
Application of Text Clustering and Natural 
Language Understanding in Automatic Abstracting. 
In Fuzzy Systems and Knowledge Discovery, 2007. 
FSKD 2007. Fourth International Conference on 
(Vol. 4, pp. 92-96). IEEE. 
Hart, C. (1998). Doing a literature review. London: 
Sage. 
Hinchliffe, L. (2003). Having your say in a scholarly 
way. Research Strategies, 19, 163? 164. 
Hyland, K. (2004). Disciplinary interactions: 
Metadiscourse in L2 postgraduate writing. Journal 
of Second Language Writing, 13(2), 133-151. 
Jing, H., & McKeown, K. R. (1999). The decomposition 
of human-written summary sentences. In 
Proceedings of the 22nd annual international ACM 
SIGIR conference on Research and development in 
information retrieval (pp. 129-136). ACM. 
Jaidka, K., Khoo, C., and Na, J.-C. (2010). Imitating 
Human Literature Review Writing: An Approach to 
Multi-Document Summarization. In Proceedings of 
the International Conference on Asian Digital 
Libraries (ICADL) (pp. 116-119). Australia: 
Springer-Verlag. 
Jaidka, K., Khoo, C., & Na, J. C. (2013). Literature 
Review Writing: How Information is Selected and 
Transformed. Aslib Proceedings, 65(3), 303-325.  
Khoo, C., Na, J. C., & Jaidka, K. (2011). Analysis of the 
macro-level discourse structure of literature reviews. 
Online Information Review, 35(2), 255-271. 
Kwan, B. S. (2006). The schematic structure of 
literature reviews in doctoral theses of applied 
linguistics. English for Specific Purposes, 25(1), 30-
55. 
Lin, C. Y., & Hovy, E. (2003, May). Automatic 
evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of the 2003 
Conference of the North American Chapter of the 
Association for Computational Linguistics on 
Human Language Technology-Volume 1 (pp. 71-78). 
Association for Computational Linguistics. 
Marcu, D. (1997, July). From discourse structures to 
text summaries. In Proceedings of the ACL (Vol. 97, 
pp. 82-88). 
Nenkova, A., & McKeown, K. (2011). Automatic 
summarization. Now Publishers Inc. 
Nanba, H., Kando, N., & Okumura, M. (2011). 
Classification of research papers using citation links 
and citation types: Towards automatic review article 
generation. Advances in Classification Research 
Online, 11(1), 117-134. 
Ou, S., Khoo, C. S. G., & Goh, D. H. (2008). Design 
and development of a concept-based multi-document 
summarization system for research abstracts. 
Journal of information science, 34(3), 308-326. 
Radev, D. R., Jing, H., Sty?, M., & Tam, D. (2004). 
Centroid-based summarization of multiple 
documents. Information Processing & Management, 
40(6), 919-938. 
Saggion, H., & Lapalme, G. (2002). Generating 
indicative-informative summaries with sumum. 
Computational linguistics, 28(4), 497-526. 
Mohammad, S., Dorr, B., Egan, M., Ahmed, H., 
Muthukrishan, P., Qazvinian, V., Radev, D., Zajic, 
D. (2009). Using citations to generate surveys of 
scientific paradigms. In Proceedings of Human 
Language Technologies: The 2009 Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics (pp. 584?
592). Association for Computational Linguistics. 
Sparck Jones, K., & Endres-Niggemeyer, B. (1995). 
Automatic summarizing. Information Processing & 
Management, 31(5), 625-630. 
134
Sparck Jones, K. (2007). Automatic summarising: The 
state of the art. Information Processing & 
Management, 43(6), 1449-1481. 
Teufel, S. (1999). Argumentative zoning: Information 
extraction from scientific text (Doctoral dissertation, 
University of Edinburgh). 
Teufel, S., & Moens, M. (2002). Summarizing scientific 
articles: experiments with relevance and rhetorical 
status. Computational linguistics, 28(4), 409-445. 
Teufel, S., Siddharthan, A., & Batchelor, C. (2009, 
August). Towards discipline-independent 
argumentative zoning: Evidence from chemistry and 
computational linguistics. In Proceedings of the 
2009 Conference on Empirical Methods in Natural 
Language Processing: Volume 3 (pp. 1493-1502). 
Association for Computational Linguistics. 
Torraco, R. J. (2005). Writing integrative literature 
reviews: Guidelines and examples. Human Resource 
Development Review, 4(3), 356-367. 
Van Dijk, T. A., & Kintsch, W. (1983). Strategies of 
discourse comprehension. New York: Academic 
Press. 
 
135
