Proceedings of the Linguistic Annotation Workshop, pages 109?112,
Prague, June 2007. c?2007 Association for Computational Linguistics
Combining Independent Syntactic and Semantic Annotation Schemes
Marc Verhagen, Amber Stubbs and James Pustejovsky
Computer Science Department
Brandeis University, Waltham, USA
{marc,astubbs,jamesp}@cs.brandeis.edu
Abstract
We present MAIS, a UIMA-based environ-
ment for combining information from var-
ious annotated resources. Each resource
contains one mode of linguistic annotation
and remains independent from the other re-
sources. Interactions between annotations
are defined based on use cases.
1 Introduction
MAIS is designed to allow easy access to a set of
linguistic annotations. It embodies a methodology
to define interactions between separate annotation
schemes where each interaction is based on a use
case. With MAIS, we adopt the following require-
ments for the interoperability of syntactic and se-
mantic annotations:
1. Each annotation scheme has its own philosophy
and is independent from the other annotations.
Simple and generally available interfaces pro-
vide access to the content of each annotation
scheme.
2. Interactions between annotations are not de-
fined a priori, but based on use cases.
3. Simple tree-based and one-directional merg-
ing of annotations is useful for visualization of
overlap between schemes.
The annotation schemes currently embedded in
MAIS are the Proposition Bank (Palmer et al,
2005), NomBank (Meyers et al, 2004) and Time-
Bank (Pustejovsky et al, 2003). Other linguis-
tics annotation schemes like the opinion annotation
(Wiebe et al, 2005), named entity annotation, and
discourse annotation (Miltsakaki et al, 2004) will
be added in the future.
In the next section, we elaborate on the first
two requirements mentioned above and present the
MAIS methodology to achieve interoperability of
annotations. In section 3, we present the XBank
Browser, a unified browser that allows researchers
to inspect overlap between annotation schemes.
2 Interoperability of Annotations
Our goal is not to define a static merger of all anno-
tation schemes. Rather, we avoid defining a poten-
tially complex interlingua and instead focus on how
information from different sources can be combined
pragmatically. A high-level schematic representa-
tion of the system architecture is given in figure 1.
PropBank NomBank TimeBank
PropBank NomBank TimeBank
annotation     initializers
interface interface interface
case-based 
interaction
case-based 
interaction
GUI GUI
Figure 1: Architecture of MAIS
109
The simple and extensible interoperability of
MAIS can be put in place using three components: a
unified environment that stores the annotations and
implements some common functionality, a set of an-
notation interfaces, and a set of case-based interac-
tions.
2.1 Unified Environment
All annotations are embedded as stand-off annota-
tions in a unified environment in which each annota-
tion has its own namespace. This unified environ-
ment takes care of some basic functionality. For
example, given a tag from one annotation scheme,
there is a method that returns tags from other anno-
tation schemes that have the same text extent or tags
that have an overlap in text extent. The unified envi-
ronment chosen for MAIS is UIMA, the open plat-
form for unstructured information analysis created
by IBM.1
UIMA implements a common data representation
named CAS (Common Analysis Structure) that pro-
vides read and write access to the documents being
analyzed. Existing annotations can be imported into
a CAS using CAS Initializers. UIMA also provides
a framework for Analysis Engines: modules that can
read from and write to a CAS and that can be com-
bined into a complex work flow.
2.2 Annotation Interfaces
In the unified environment, the individual annota-
tions are independent from each other and they are
considered immutable. Each annotation defines an
interface through which salient details of the anno-
tations can be retrieved. For example, annotation
schemes that encodes predicate-argument structure,
that is, PropBank and NomBank, define methods
like
args-of-relation(pred)
arg-of-relation(pred, arg)
relation-of-argument(arg)
Similarly, the interface for TimeBank includes
methods like
rel-between(eventi, eventj)
events-before(event)
event-anchorings(event)
1http://www.research.ibm.com/UIMA/
The arguments to these methods are not strings
but text positions, where each text position contains
an offset and a document identifier. Return values
are also text positions. All interfaces are required to
include a method that returns the tuples that match a
given string:
get-locations(string, type)
This method returns a set of text positions. Each
text position points to a location where the input
string occurs as being of the given type. For Time-
Bank, the type could be event or time, for Prop-
Bank and NomBank, more appropriate values are
rel or arg0.
2.3 Case-based Interactions
Most of the integration work occurs in the interac-
tion components. Specific interactions can be built
using the unified environment and the specified in-
terfaces of each annotation scheme.
Take for example, the use case of an entity chron-
icle (Pustejovsky and Verhagen, 2007). An entity
chronicle follows an entity through time, display-
ing what events an entity was engaged in, how these
events are anchored to time expressions, and how the
events are ordered relative to each other. Such an
application depends on three kinds of information:
identification of named entities, predicate-argument
structure, and temporal relations. Each of these de-
rive from a separate annotation scheme. A use case
can be built using the interfaces for each annotation:
? the named entity annotation returns the text
extents of the named entity, using the gen-
eral method get-locations(string,
type)
? the predicate-argument annotation (accessed
through the PropBank and NomBank inter-
faces) returns the predicates that go with a
named-entity argument, repeatedly using the
method relation-of-argument(arg)
? finally, the temporal annotation returns the tem-
poral relations between all those predicates,
calling rel-between(eventi, eventj)
on all pairs of predicates
110
Note that named entity annotation is not inte-
grated into the current system. As a stopgap mea-
sure we use a pre-compiled list of named entities
and feed elements of this list into the PropBank
and NomBank interfaces, asking for those text po-
sitions where the entity is expressed as an argu-
ment. This shows the utility of a general method
like get-locations(string, type).
Each case-based interaction is implemented using
one or more UIMA analysis engines. It should be
noted that the analysis engines used for the entity
chronicler do not add data to the common data repre-
sentation. This is not a principled choice: if adding
new data to the CAS is useful then it can be part of
the case-based interaction, but these added data are
not integrated into existing annotations, rather, they
are added as a separate secondary resource.2
The point of this approach is that applications can
be built pragmatically, using only those resources
that are needed. It does not depend on fully merged
syntactic and semantic representations. The entity
chronicle, for example, does not require discourse
annotation, opinion annotation or any other resource
except for the three discussed before. An a priori
requirement to have a unified representation intro-
duces complexities that go beyond what?s needed for
individual applications.
This is not to say that a unified representation is
not useful on its own, there is obvious theoretical
interest in thoroughly exploring how annotations re-
late to each other. But we feel that the unified repre-
sentation is not needed for most, if not all, practical
applications.
3 The XBank Browser
The unified browser, named the XBank Browser, is
intended as a convenience for researchers. It shows
the overlap between different annotations. Annota-
tions from different schemes are merged into one
XML representation and a set of cascading style
sheets is used to display the information.
2In fact, for the entity chronicle it would be useful to have
extra data available. The current implementation uses what?s
provided by the basic resources plus a few heuristics to super-
ficially merge data from separate documents. But a more in-
formative chronicle along the lines of (Pustejovsky and Verha-
gen, 2007) would require more temporal links than available in
TimeBank. These can be pre-compiled and added using a dedi-
cated analysis engine.
The XBank Browser does not adhere to the MAIS
philosophy that all resources are independent. In-
stead, it designates one syntactic annotation to pro-
vide the basic shape of the XML tree and requires
tags from other annotations to find landing spots in
the basic tree.
The Penn Treebank annotation (Marcus et al,
1993) was chosen to be the first among equals: it
is the starting point for the merger and data from
other annotations are attached at tree nodes. Cur-
rently, only one heuristic is used to merge in data
from other sources: go up the tree to find a Treebank
constituent that contains the entire extent of the tag
that is merged in, then select the head of this con-
stituent. A more sophisticated approach would con-
sist of two steps:
? first try to find an exact match of the imported
tag with a Treebank constituent,
? if that fails, find the constituent that contains
the entire tag that is merged in, and select this
constituent
In the latter case, there can be an option to select
the head rather than the whole constituent. In any
case, the attached node will be marked if its original
extent does not line up with the extent at the tree
node.
It should be noted that this merging is one-
directional since no attempt is made to change the
shape of the tree defined by the Treebank annota-
tion.
The unified browser currently displays markups
from the Proposition Bank, NomBank, TimeBank
and the Discourse Treebank. Tags from individual
schemes can be hidden as desired. The main prob-
lem with the XBank Browser is that there is only a
limited amount of visual clues that can be used to
distinguish individual components from each other
and cognitive overload restricts how many annota-
tion schemes can be viewed at the same time. Nev-
ertheless, the browser does show how a limited num-
ber of annotation schemes relate to each other.
All functionality of the browser can be accessed at
http://timeml.org/ula/. An idea of what
it looks like can be gleaned from the screenshot dis-
played in figure 2. In this figure, boxes represent
relations from PropBank or NomBank and shaded
111
Figure 2: A glimpse of the XBank Browser
backgrounds represent arguments. Superscripts are
indexes that identify relations, subscripts identify
what relation an argument belongs to. Red fonts
indicate events from TimeBank. Note that the real
browser is barely done justice by this picture be-
cause the browser?s use of color is not visible.
4 Conclusion
We described MAIS, an environment that imple-
ments interoperability between syntactic and seman-
tic annotation schemes. The kind of interoperabil-
ity proposed herein does not require an elaborate
representational structure that allows the interaction.
Rather, it relies on independent annotation schemes
with interfaces to the outside world that interact
given a specific use case. The more annotations
there are, the more interactions can be defined. The
complexity of the methodology is not bound by the
number of annotation schemes integrated but by the
complexity of the use cases.
5 Acknowledgments
The work reported in this paper was performed as
part of the project ?Towards a Comprehensive Lin-
guistic Annotation of Language?, and supported un-
der award CNS-0551615 of the National Science
Foundation.
References
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treeb. Computational
Linguistics, 19(2):313?330.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The nombank
project: An interim report. In A. Meyers, editor, HLT-
NAACL 2004 Workshop: Frontiers in Corpus Annota-
tion, pages 24?31, Boston, Massachusetts, USA, May
2 - May 7. Association for Computational Linguistics.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. The penn discourse treebank.
In Proceedings of the Language Resources and Evalu-
ation Conference, Lisbon, Portugal.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?106.
James Pustejovsky and Marc Verhagen. 2007. Con-
structing event-based entity chronicles. In Proceed-
ings of the IWCS-7, Tilburg, The Netherlands.
James Pustejovsky, Patrick Hanks, Roser Saur??, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro, and
Marcia Lazo. 2003. The timebank corpus. In Pro-
ceedings of Corpus Linguistics, pages 647?656.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165?210.
112
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tutorial Abstracts,
pages 1?2, Dublin, Ireland, August 23-29 2014.
Biomedical/clinical NLP
Ozlem Uzuner
Information Studies
University of Albany, SUNY
Albany, NY
ouzuner@albany.edu
Meliha Yetis?gen
Biomedical and
Health Informatics
University of Washington
Seattle, WA
melihay@uw.edu
Amber Stubbs
Library and
Information Science
Simmons College
Boston, MA
stubbs@simmons.edu
Introduction
Recent years have seen a rapid growth in the use of biomedical documents and narrative clinical records
for applications outside of direct patient care. Accordingly, recent years have also seen an increase in
the development of NLP technologies for concept and relation extraction, summarization, and question
answering on these data.
This tutorial will present an overview of the biomedical and clinical NLP data, tools, and methods
with the intent of providing the researchers with a jump-start into these domains. We will focus on the
demand for NLP in biomedical and clinical domains, the potential for impact, and the required NLP
tasks. We will introduce this information in the following categories:
Overview of biomedical/clinical NLP
Biomedical narratives are often dense with domain-specific jargon. Clinical narratives, in addition to be-
ing dense with domain-specific jargon, exhibit the complexities of a specialized sub-language. They are
written by the domain experts and for the domain experts. Their primary purpose is to assist in inform-
ing future decisions about the care of the patients. As a result, both biomedical and clinical narratives
present challenges for existing open-domain NLP technologies and require special considerations for
their accurate understanding and interpretation.
In this section, we will discuss the data sources currently available to researchers, as well as provide
an overview of the research questions both domains. On the clinical side, this includes using EHRs
for phenotyping and decision support systems. The biomedical side uses NLP to explore fields such as
literature-based discovery and literature searches.
Current research questions in biomedical and clinical NLP
NLP applications are generally built to answer specific questions about data. In this section, we will pro-
vide examples of the types of questions researchers are asking in the clinical and biomedical domains.
Additionally, we will discuss how different linguistic aspects of these data are addressed by looking at ex-
isting syntactic (part of speech tagging, parsing) and semantic (concept extraction, temporal information
extraction, coreference resolution) systems.
Datasets and the annotation process
Building annotated corpora for any task can be challenging, but the biomedical and clinical domains have
additional barriers that make creating these corpora difficult. In this section, we will discuss available
annotated resources in both domains, and discuss challenges in biomedical and clinical corpus building,
such as restrictions on data access and the need for domain experts to be part of the annotation process.
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Clinical Annotation Case Study
The 2014 i2b2 NLP Shared Task
1
involved two NLP challenges: 1) de-identification of medical records,
and 2) identification of risk factors for coronary artery disease in diabetic patients. Each of these tracks
required a separate annotation effort, and in this portion of the tutorial we will describe the end-to-end
process of creating this annotated resource, from data selection to writing the annotation guidelines to
creating the final gold standards.
NLP Methods
Just as there are many research questions in the biomedical and clinical domains, there are many existing
NLP systems that address these questions. In this portion of the tutorial, we will describe the three main
approaches (rule-based, statistical, and hybrid) commonly used to process biomedical and clinical text.
Additionally, we will present on-going research projects from our research labs including (1) extracting
structure and semantics from clinical text through section segmentation and assertion analysis and (2)
clinical applications such as phenotype modeling and specific examples of information extraction in the
radiology domain.
Open questions and future directions
Research in the fields of biomedical and clinical NLP is far from complete; the end of this tutorial will
look at current unsolved problems in these fields, as well as look ahead towards potential future research
questions.
Acknowledgements
This tutorial was supported in part by Informatics for Integrating Biology and the Bedside (i2b2) award
No. 2U54LM008748 from the National Institutes of Health (NIH)/National Q5 Library of Medicine
(NLM), by award No. 1R13LM01141101 from the NIH NLM, by award No. 1R21EB016872 from NIH
NIBIB, and by Institute of Translational Health Sciences award No. UL1TR000423 from NIH NCATS.
1
https://www.i2b2.org/NLP/HeartDisease/
2
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 141?143,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Applying the TARSQI Toolkit to augment text mining of EHRs
Amber Stubbs
Department of Computer Science
Brandeis University MS 018
Waltham, Massachusetts, 02454 USA
astubbs@cs.brandeis.edu
Benjamin Harshfield
Channing Laboratory
Brigham and Women?s Hospital
Boston, Massachusetts, 02115 USA
rebjh@channing.harvard.edu
Abstract
We present a preliminary attempt to ap-
ply the TARSQI Toolkit to the medi-
cal domain, specifically electronic health
records, for use in answering temporally
motivated questions.
1 Introduction
Electronic Health Records are often the most com-
plete records of a patient?s hospital stay, making
them invaluable for retrospective cohort studies.
However, the free text nature of these documents
makes it difficult to extract complex information
such as the relative timing of conditions or proce-
dures. While there have been recent successes in
this endeavor (Irvine et al, 2008; Mowery et al,
2009; Zhou et al, 2007), there is still much to be
done. We present work done to adapt the TARSQI
Toolkit (TTK) to the medical domain. Though the
use of the TTK and a set of auxiliary Perl scripts,
we perform information extraction over a set of
354 discharge summaries used in the R3i REAL-
IST study to answer the following question:
Which patients can be positively identi-
fied as being on statins at the time they
were admitted to the hospital?
2 TARSQI Toolkit
The TARSQI Toolkit, developed as a part of the
AQUAINT workshops, is a ?modular system for
automatic temporal and event annotation of nat-
ural language? in newswire texts (Verhagen and
Pustejovsky, 2008). The different modules prepro-
cess the data, label events and times, create links
between times and events (called ?tlinks?), and
mark subordination relationships. Output from the
TTK consists documents annotated in TimeML,
an XML specification for event and time annota-
tion (Pustejovsky et al, 2005). Of particular inter-
est for this project are EVITA, the module respon-
sible for finding events in text, and Blinker, the
module used to create syntactic rule-based links
between events and timexes.
3 Structure of EHRs
The bodies of the Electronic Health Records
used were segmented, with each section having a
header indicating the topic of that section (?Med-
ical History?, ?Course of Treatment?, ?Discharge
Medications?, etc). Header names and sections are
not standardized across EHRs, but often give im-
portant temporal information about when events
described took place (Denny et al, 2008).
4 Statin Extraction Methodology
As the purpose of this task was to discover what
changes to the TTK would be necessary to make
the transition from newswire to medical texts, over
the course of two weeks we filled in the gaps in the
toolkit?s abilities with a few auxiliary Perl scripts.
Specifically, these scripts were used to clean up in-
put so that it conformed to TTK expectations, la-
bel the statins as events, locate section headers and
associate temporal information with the headers.
A list of statins was acquired from an MD, and
then supplemented with information from web-
sites in order to get al currently marketed versions
of the drugs. This list was then used in conjunc-
tion with a Perl script to find mentions of statins
in the discharge summaries and create TimeML
event tags for them.
In order to identify and categorize section head-
ers we developed a program to automatically col-
lect header names from a separate set of approxi-
mately 700 discharge summaries. Then we gath-
ered statistics on word frequency and created sim-
ple rules for characterizing headers based on key-
words. Headers were divided into four simple cat-
egories: Past, Present, After, and Not (for cate-
141
gories that did not contain specific or relevant tem-
poral information).
The Blinker component of the TTK was then
modified to take into account temporal informa-
tion stored in the header in addition to the syntac-
tic information present in each individual sentence
for the creation of tlinks.
5 Results
Output from the modified TTK was compared
to the judgment of human annotators on the
same dataset. Two annotators, employees of
BWH/Harvard Medical involved in data manage-
ment and review for clinical trials, were asked
to label each file as yes for those patients taking
statins at the time they were admitted to the hos-
pital, and no for those that werent. Files where
statins were mentioned without clear temporal an-
chorings were categorized as ?unsure?.
Inter-annotator agreement was 85% (Cohen
kappa=.75), with 75% of the disagreements being
between ?no? and ?unsure?. The majority of these
ambiguous cases were discharge summaries where
a statin was listed under ?discharge? but admission
medications were not listed, nor were the statins
mentioned as being started at the hospital. The
annotation guidelines have been updated to reflect
how to annotate these cases in the future. Over-
all, 139 patients were identified as being on statins,
174 were not on statins, and 41 were unclear.
As the question was which patients could be
positively identified as being on statins at the time
of admission, the files labeled as ?unsure? were
considered to be ?no? for the purposes of evalua-
tion against the TTK, making the totals 139 yeses
to 215 noes. The comparison between human and
computer annotation are shown below:
Yes No
Human 139 215
TTK 129 225
Table 1: Distribution of statin classifications.
The TTK system had an accuracy of 84% overall,
with an accuracy of 95% on the files that the hu-
man annotators found to be unambiguous.
6 Limitations
While we were pleased by these results, a num-
ber of factors worked in the favor or the automated
system. The task itself, while requiring a mixture
of lexical and temporal knowledge, was greatly
simplified by a finite list of medications and a bi-
nary outcome variable. Obscure abbreviations or
misspellings could have prevented identification
of statin mentions for both the computer and hu-
mans, making the overall accuracy questionable.
Additionally, in the majority of documents the
statins were mentioned in lists under temporally
anchored headings rather than free text, thereby
minimizing the impact of uncertain times as de-
scribed in Hripcsak et al(2009).
7 Future work
Our work so far shows promising results for being
able to modify the TARSQI Toolkit for use in the
medical domain. In the future, we would like to in-
tegrate the functionality of the Perl scripts used in
this project into the TTK, in particular expanding
the vocabulary of the EVITA module to the medi-
cal domain, section header labeling, and the use of
the headers in tlink creation.
New annotation schemas will need to be added
to the project in order to get a more complete and
accurate view of medical records. Under consider-
ation is the Clinical E-Science Framework (CLEF)
(Roberts et al, 2007) for annotating medical enti-
ties, actions (which would overlap with TimeML
events), drugs, etc. Modifications to Blinker will
be more fully integrated with the existing rule li-
braries. At this point it is unclear whether the TTK
will remain a single program, or if it will split into
domain-specific versions.
Furthermore, the number of files labeled ?un-
sure? by human annotators highlights the need for
cross-document analysis abilities. Had previous
records for these patients been available, it seems
likely that there would have been fewer uncertain-
ties.
8 Conclusion
Modifying the TARSQI Toolkit, a newswire-
trained parser, for application in the medical do-
main provided accurate results for a very specific
time-sensitive query.
Acknowledgments
Partial support for the work described here was
provided by the Residual Risk Reduction Initiative
Foundation (r3i.org).
142
References
Joshua C Denny, Randolph A Miller, Kevin B John-
son, and Anderson Spickard. 2008. Development
and evaluation of a clinical note section header ter-
minology. AMIA Annual Symposium proceedings,
pages 156?60.
George Hripcsak, Noe?mie Elhadad, Yueh-Hsia Chen,
Li Zhou, and Frances P Morrison. 2009. Using em-
piric semantic correlation to interpret temporal as-
sertions in clinical texts. J Am Med Inform Assoc,
16(2):220?7.
Ann K Irvine, Stephanie W Haas, and Tessa Sullivan.
2008. Tn-ties: A system for extracting temporal in-
formation from emergency department triage notes.
AMIA Annual Symposium proceedings, pages 328?
32.
Danielle L. Mowery, Henk Harkema, John N. Dowling,
Jonathan L. Lustgarten, and Wendy W. Chapman.
2009. Distinguishing historical from current prob-
lems in clinical reports: which textual features help?
In BioNLP ?09: Proceedings of the Workshop on
BioNLP, pages 10?18, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
James Pustejovsky, Bob Ingria, and Roser Sauri et al,
2005. The Language of Time: A Reader, chapter
The Specification Language TimeML, pages 545?
558. Oxford University Press, Oxford.
Angus Roberts, Robert Gaizauskas, and Mark et al
Hepple. 2007. The clef corpus: semantic annotation
of clinical text. AMIA Annual Symposium proceed-
ings, pages 625?9.
Marc Verhagen and James Pustejovsky. 2008. Tem-
poral processing with the tarsqi toolkit. In Coling
2008: Companion volume - Posters and Demonstra-
tions, pages 189?192, Manchester, UK.
Li Zhou, Simon Parsons, and George Hripcsak. 2007.
The evaluation of a temporal reasoning system in
processing clinical discharge summaries. J Am Med
Inform Assoc, 15(1):99?106.
143
Proceedings of the Fifth Law Workshop (LAW V), pages 129?133,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
MAE and MAI: Lightweight Annotation and Adjudication Tools
Amber Stubbs
Department of Computer Science
Brandeis University MS 018
Waltham, Massachusetts, 02454 USA
astubbs@cs.brandeis.edu
Abstract
MAE and MAI are lightweight annotation and
adjudication tools for corpus creation. DTDs
are used to define the annotation tags and at-
tributes, including extent tags, link tags, and
non-consuming tags. Both programs are writ-
ten in Java and use a stand-alone SQLite
database for storage and retrieval of annota-
tion data. Output is in stand-off XML.
1 Introduction
The use of machine learning for natural language
processing tasks has been steadily increasing over
the years: text processing challenges such as those
associated with the SemEval workshops (Erk and
Strapparava, 2010) and the I2B2 medical informat-
ics shared tasks (i2b2 team, 2011) are well known,
and tools for training and testing algorithms on cor-
pora, such as the Natural Language Tool Kit (Bird
et al, 2009) and the WEKA tools (Hall et al, 2009)
are widely used.
However, a key component for training a machine
for a task is having sufficient data for the computer
to learn from. In order to create these corpora, hu-
man researchers must define the tasks that they wish
to accomplish and find ways to encode the necessary
information, usually in some form of XML, then
have relevant data annotated with XML tags.
The necessity of corpus annotation has led to a
number of useful tools, as well as assessments for
tool usability and standards for linguistic annotation.
A recent survey (Dipper et al, 2004) examined what
attributes an annotation tool should have for it to be
most useful, and the Linguistic Annotation Frame-
work (LAF) describes the desired properties of an
annotation framework to ensure interoperability and
utility (Ide and Romary, 2006).
The Multi-purpose Annotation Environment
(MAE), and the Multi-document Adjudication
Interface (MAI) were designed to be easy to begin
using, but have enough flexibility to provide a
starting point for most annotation tasks. Both
programs are written in Java and use a stand-
alone SQLite database1 for storage and retrieval
of annotation data, and output standoff XML
that is compliant with the abstract LAF model.
Both of these tools are available from http:
//pages.cs.brandeis.edu/?astubbs/
2 Related Work
As previously mentioned, there are already a num-
ber of annotation tools in use?Dipper et al exam-
ined five different programs; additionally Knowta-
tor (Ogren, 2006), GATE (Cunningham et al, 2010),
Callisto (MITRE, 2002), and BAT (Verhagen, 2010)
have been used for various annotation tasks; and the
list goes on2. However, as Kaplan et al noted in
a paper about their own annotation software, SLAT
2.0 (2010), much annotation software is not generic,
either because it was designed for a specific anno-
tation task, or designed to be used in a particular
way. BAT, for example, utilizes a layered anno-
tation framework, which allows for adjudication at
each step of the annotation process, but this makes
1http://www.zentus.com/sqlitejdbc/
2See http://annotation.exmaralda.org/index.php/Tools for a
reasonably up-to-date list of annotation software
129
tasks difficult to modify and is best suited for use
when the schema is not likely to change. GATE was
built primarily as a tool for automated annotation,
and Callisto, while excellent for annotating contigu-
ous portions of texts, cannot easily create links?
it requires the user to create an entire task-specific
plug-in. Knowtator provides links and extent tag-
ging, but comes as a plug-in for Prote?ge?3, a level
of overhead that users may find daunting. Similarly,
the Apache UIMA system (Apache, 2006) is well
developed and supported but presents a very steep
learning curve for task creation.
As for adjudication, while some software has
built-in judgment capabilities (GATE, BAT, Know-
tator, and SLAT, for example), that functionality
does not stand alone, but rather relies on the annota-
tions being done in the same environment.
All of the tools mentioned are well-suited for their
purposes, but it seems that there is room for an anno-
tation tool that allows for reasonably complex anno-
tation tasks without requiring a lot of time for setup.
3 Simple Task Creation
One of the defining factors that Dipper et al (2004)
identified in evaluating annotation tools is simplic-
ity of use?how long does it take to start annotating?
Upon examining various existing annotation tools,
they found that there was often a trade-off between
simplicity and data quality assurance: tools that have
an open interface and loose restrictions for tag sets
tended to have lower quality data output, while tools
that require a specification could output better data,
but took a little longer to get running.
MAE and MAI attempt to find a middle ground
between the two extremes: they require task defini-
tions in the form of slightly customized Document
Type Definition (DTD) files, which are used to de-
fine the tags and their attributes but are not difficult
to create or modify4.
There are two types of tags that are primarily used
in annotation: extent tags (sometimes called ?seg-
ments? (Noguchi et al, 2008)) which are used to
mark a contiguous portion of text as having a spe-
cific characteristic, and link tags, which are used to
3http://protege.stanford.edu/
4In the future, a GUI will be added to MAE that will make
the DTD creation process easier.
create a relationship between two extent tags. MAE
and MAI support both of these tag types, and addi-
tionally support non-consuming extent tags, which
can be useful for having an annotator mark explic-
itly whether or not a particular phenomena appears
in the document being annotated.
DTD creation is quite simple. If, for example, an
annotator wanted to look at nouns and mark their
types, they could define the following:
<!ELEMENT NOUN (#PCDATA)>
<!ATTLIST NOUN type
(person|place|thing|other)>
The ?#PCDATA? in the first line informs the soft-
ware that NOUN is an extent tag, and the second
line gives NOUN an attribute called ?type?, with the
possible values defined in the list in parenthesis.
Creating a link is equally simple:
<!ELEMENT ACTION EMPTY >
<!ATTLIST ACTION relationship
(performs|performed_by)>
The ?EMPTY? marker indicates that the tag is a
link, and the attributes and attribute values work the
same way as for extent tags.
4 MAE
Once the DTD is created and files are preprocessed,
the user loads the DTD and a file into MAE. The
text to be annotated appears in the window, and a
window at the bottom of the screen holds a table
for each tag (see Figure 1). When a user selects
an extent and creates a tag, some information about
the tag is automatically added to the table: the start
and end locations of the tag, and the text of the ex-
tent. Additionally, MAE will automatically generate
a document-unique ID number for that tag so that it
can easily be referenced in links.
The user can then add in any information about
the attributes by filling in the table at the bottom of
the screen. In the text window, the color of the ex-
tent that has been tagged is changed to the color as-
sociated with that tag. If there are multiple tags in a
location, the text is underlined as well. Highlighting
tagged text in the window will also highlight any ta-
ble rows associated with that tag, including link tags.
This makes it easy for the annotator to see what in-
formation has already been added about that text.
130
Figure 1: TimeML annotation in MAE.
Non-consuming tags are created from the menu at
the top of the screen. Links are created by holding
down the control key (or the command key on Macs)
and clicking on the two tags that will be linked. A
window pops up that allows the user to link either
to the tags at the specified locations, or to any non-
consuming tags that have been created in the docu-
ment.
5 Output
Once the user is done annotating, they can save their
work in an XML file. MAE outputs (and takes as in-
put) UTF-8 encoded files, so it can be used to anno-
tate any character set that is representable in UTF-8,
including Chinese. The output is compliant with the
LAF guidelines (Ide and Romary, 2006).
5.1 System testing
MAE is currently being used for a variety of annota-
tion tasks: medical record annotation, eligibility cri-
teria assessment, and for a university course on cor-
pus creation. Annotation tasks in that course range
from opinion annotation to tense and aspect in Chi-
nese verbs. It is currently being used on Windows,
Mac, and Linux.
6 MAI
MAI is built on the same back-end code as MAE,
making them easily compatible. Like MAE, using
MAI begins with loading a DTD. Then the adjudi-
cator can load each annotation of a text that they
would like to create a gold standard for. As each
new document is added, MAI loads the tag informa-
tion for each annotation into the database for quick
reference.
Once all the files are loaded, the adjudicator se-
lects the tag they want to review from the left part
of the screen. The text is then color-coded to reflect
the agreement of the annotators: blue if all the anno-
tators agree that a tag of the selected type should be
at a location, red if only a subset would place a tag
there, and black for locations where no tag is present
(see Figure 2).
When text is highlighted by the adjudicator, the
information about each annotator?s tag and attributes
for that location is filled in on a table to the right of
the screen. From there, the annotator can either fill
in the values for the gold standard by hand, or copy
the values from one annotator directly into the gold
standard column and modifying them as needed.
Once the adjudicator is satisfied with the gold stan-
dard they can add the annotation to the database by
131
Figure 2: The extent adjudication table in MAI
clicking the ?accept/modify? button at the bottom of
the gold standard column. At this point, MAI will
generate a new ID for that tag, and the color of the
adjudicated font will become green.
At the time of this writing, the algorithms for link
and non-consuming tag adjudication have not been
fully worked out for use inside of MAI. However,
once the extent tags have been adjudicated, the an-
notator can choose to export the non-consuming tags
and link tags that involve ?approved? extent tags
into an XML file, along with the adjudicated ex-
tents. This partially-judged file can then be loaded
into MAE, where it is easier to display and modify
all the relevant information.
6.1 System testing
As with MAE, MAI has been used for the various
annotation projects for a course on corpus creation,
as well as a medical record annotation task. This
program is still under development, but so far adju-
dications tasks with MAI have proved successful.
7 Conclusions and Future Work
While MAE and MAI do not represent a new fron-
tier in annotation software, I believe that their ease
of use, portability, and clean visualization will make
them useful tools for annotation projects that do not
want to invest in the time required to use other exist-
ing software, and for adjudicators that want an easy
way to fix discrepancies between annotators. Admit-
tedly, tasks involving heirarchical annotations would
require one of the more sophisticated tools that are
currently available, but there are still many tasks that
do not require that level of complexity that MAE and
MAI can be used for.
There is room for improvement in both of these
programs: fully implementing link adjudication in
MAI, allowing for more customization in the visu-
alizations would make them more enjoyable to use,
and expanding the functionality to make them more
useful for more tasks (for example, allowing links
with multiple anchors instead of just two). Both
MAE and MAI are under development, and im-
provements to both will be made over the coming
months.
Acknowledgments
Funding for this project development was provided
by NIH grant NIHR21LM009633-02, PI: James
Pustejovsky
Many thanks to the annotators who helped me
identify bugs in the software, particularly Cornelia
Parkes, Cheryl Keenan, BJ Harshfield, and all the
students in the Brandeis University Spring 2011
Computer Science 216 class.
132
References
Apache. 2006. Unstructured information management
architecture. http://uima.apache.org/.
Steven Bird, Ewan Klein, and Edward Loper. 2009. Nat-
ural Language Processing with Python. O?Reilly Me-
dia Inc, Sebastopol, CA, first edition edition.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, Valentin Tablan, Niraj Aswani, and
Ian Roberts, 2010. Developing Language Processing
Components with GATE, 5 edition, July.
Stefanie Dipper, Michael Go?tze, and Manfred Stede.
2004. Simple annotation tools for complex annota-
tion tasks: an evaluation. In Proceedings of the LREC
Workshop on XML-based Richly Annotated Corpora,
pages 54?62, Lisbon, Portugal.
Katrin Erk and Carlo Strapparava. 2010. Semeval-2.
http://semeval2.fbk.eu/semeval2.php.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explorations, 11(1).
i2b2 team. 2011. i2b2 shared task.
https://www.i2b2.org/NLP/Coreference/Main.php.
accessed Feb. 2011.
Nancy Ide and Laurent Romary. 2006. Representing lin-
guistic corpora and their annotations. In Proceedings
of the Fifth Language Resources and Evaluation Con-
ference (LREC.
Dain Kaplan, Ryu Iida, and Takenobu Tokunaga. 2010.
Slat 2.0: Corpus construction and annotation process
management. In Proceedings of the 16th Annual Meet-
ing of The Association for Natural Language Process-
ing, pages pp.510 ? 513.
MITRE. 2002. Callisto website.
http://callisto.mitre.org/index.html. accessed Dec. 17,
2010.
Masaki Noguchi, Kenta Miyoshi, Takenobu Tokunaga,
Ryu Iida, Mamoru Komachi, and Kentaro Inui. 2008.
Multiple purpose annotation using slat - segment and
link-based annotation tool -. In Proceedings of 2nd
Linguistic Annotation Workshop, pages pp.61 ? 64.
Philip V. Ogren. 2006. Knowtator: a prote?ge? plug-in
for annotated corpus construction. In Proceedings of
the 2006 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology, pages 273?275, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Marc Verhagen. 2010. The brandeis annotation tool.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk,
Stelios Piperidis, Mike Rosner, and Daniel Tapias,
editors, Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. European Language
Resources Association (ELRA).
133
Proceedings of the Fifth Law Workshop (LAW V), pages 152?160,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Increasing Informativeness in Temporal Annotation
James Pustejovsky
Department of Computer Science
Brandeis University MS 018
Waltham, Massachusetts, 02454 USA
jamesp@cs.brandeis.edu
Amber Stubbs
Department of Computer Science
Brandeis University MS 018
Waltham, Massachusetts, 02454 USA
astubbs@cs.brandeis.edu
Abstract
In this paper, we discuss some of the chal-
lenges of adequately applying a specification
language to an annotation task, as embodied
in a specific guideline. In particular, we dis-
cuss some issues with TimeML motivated by
error analysis on annotated TLINKs in Time-
Bank. We introduce a document level in-
formation structure we call a narrative con-
tainer (NC), designed to increase informative-
ness and accuracy of temporal relation identi-
fication. The narrative container is the default
interval containing the events being discussed
in the text, when no explicit temporal anchor
is given. By exploiting this notion in the cre-
ation of a new temporal annotation over Time-
Bank, we were able to reduce inconsistencies
and increase informativeness when compared
to existing TLINKs in TimeBank.
1 Introduction
In linguistic annotation projects, there is often a gap
between what the annotation schema is designed to
capture and how the guidelines are interpreted by the
annotators and adjudicators given a specific corpus
and task (Ide and Bunt, 2010; Ide, 2007). The dif-
ficulty in resolving these two aspects of annotation
is compounded when tasks are looked at in a poten-
tially incomplete annotation task; namely, where the
guideline is following a specification to a point, but
in fact human annotation is not even suggested as
complete because it would be infeasible. Creating
temporal links to represent the timeline of events in
a document is an example of this: human annota-
tion of every possible temporal relationship between
events and times in a narrative would be an over-
whelming task.
In this paper, we discuss how temporal rela-
tion annotation must be sensitive to two aspects of
the task that were not mentioned in the TimeBank
guideline (Pustejovsky et al, 2005): (a) sensitivity
to the genre and style of the text; and (b) the interac-
tion with discourse relations that explicitly reference
the flow of the narrative in the text. We believe that
making reference to both these aspects in the text
during the annotation process will increase overall
informativeness and accuracy of the annotation. In
the present paper, we focus primarily on the first of
these points, and introduce a document level infor-
mation structure we call a narrative container (NC).
Because of the impossibility of humans captur-
ing every relationship, it is vital that the annotation
guidelines describe an approach that will result in
maximally informative temporal links without rely-
ing on standards that are too difficult to apply. With
this in mind, we have been examining the TimeBank
corpus (Pustejovsky et al, 2003) and the annotation
guideline that created it, and have come to these re-
alizations:
(1) ? The guideline does not specify certain types
of annotations that should be performed;
? The guideline forces some annotations to be
performed when they should not always be.
Additionally, we have discovered some inconsisten-
cies in the TimeBank corpus related to temporal
links. Furthermore, upon examination, we have be-
come aware of the importance of the text style and
152
genre, and how readers interpret temporally unah-
chored events.
This gave rise, in examining the genres that are
most frequent in TimeBank (namely news and fi-
nance), to the possibility that readers of news ar-
ticles and narratives have possible default assump-
tions about when unanchored events take place. It
seems reasonable for a reader to assume in a sen-
tence such as: Oneida Ltd. declared a 10% stock
dividend, payable Dec. 15 to stock of record Nov.
17, that the ?declared? event took place soon before
the article?s Document Creation Time (DCT).
Exactly how soon before may be related to some
proximate interval of time associated with both the
publication time and frequency. That is, it appears
that just as importantly, if not more so, than the DCT,
is a related and dependent notion of the salient in-
terval surrounding the creation time, for interpreting
the events that are being reported or written about.
We will call this the Narrative Container. There
seems to be a default value for this container affected
by many variables. For example, a print newspaper
seems to associate in the content and style a nar-
rative container of approximately 24 hours, or one
business day. A newswire article, on the other hand,
has a narrative container of 2-10 hours. Conversely,
weekly and monthly publications would likely have
a narrative container of a much longer duration (a
week or more).
Along with the narrative container, there are two
related concepts that proved useful in framing this
new approach to temporal annotation. The Narra-
tive Scope describes the timespan described in the
document, with the left marker defined by the earli-
est event mentioned in the document, and the right
by the event furthest in the future. The other impor-
tant concept is that of Narrative Time. A Narrative
Time is essentially the current temporal anchor for
events in a document, and can change as the reader
moves through the narrative.
With these as initial assumptions we did some
cursory inspection of the TimeBank data to deter-
mine if there was a correlation between Narrative
Container length and genre, and found it to be a
compelling assumption. With that in mind, we de-
termined that TLINK creation should be focused on
relationships to the narrative container, rather than
to the DCT.
Our goal is, to the extent possible, to see how
we can use a container metaphor, albeit somewhat
underspecified, to left-delineate the container within
which unanchored events might be in relation to.
2 Identifying Temporal Relations
While low-level temporal annotation tasks such as
identifying events and time expressions are rela-
tively straightforward and can be marked up with
high consistency, high-level tasks such as arrang-
ing events in a document in a temporal order have
proved to be much more challenging. The tempo-
ral ordering of events in a document, for example, is
accomplished by identifying all distinct event-event
pairings. For a document that has n events, this
requires the annotation of
(n
2
)
events pairs. Ob-
viously, for general-purpose annotation, where all
possible events are considered, the number of event
pairs grows essentially quadratically to the number
of events, and the task quickly becomes unmanage-
able.
There are, however, strategies that we can adopt
to make this labeling task more tractable. First we
need to distinguish the domains over which ordering
relations are performed. Temporal ordering relations
in text are of three kinds:
(2) a. A relation between two events;
b. A relation between two times;
c. A relation between a time and an event.
TimeML, as a formal specification of the temporal
information conveyed in language, makes no dis-
tinction between these ordering types. But a human
reader of a text does make a distinction, based on
the discourse relations established by the author of
the narrative (Miltsakaki et al, 2004; Poesio, 2004).
Temporal expressions denoting the local Narrative
Container in the text act as embedding intervals
within which events occur. Within TimeML, these
are event-time anchoring relations (TLINKs). Dis-
course relations establish how events relate to one
another in the narrative, and hence should constrain
temporal relations between two events. Thus, one
of the most significant constraints we can impose is
to take advantage of the discourse structure in the
document before event-event ordering relations are
identified.
153
Although, in principle, during an annotation a
temporal relation can be specified between any two
events in the text, it is worth asking what informa-
tiveness a given temporal relation introduces to the
annotation. The informativeness of an annotation
will be characterized as a function of the information
contained in the individual links and their closure.
We can distinguish, somewhat informally for now,
two sources of informativeness in how events are
temporally ordered relative to each other in a text:
(a) externally and (b) internally. Consider first ex-
ternal informativeness. This is information derived
from relations outside the temporal relation con-
straint set, e.g., as coming from explicit discourse re-
lations between events (and hence is associated with
the relations in (2a) above). For example, we will
assume that, for two events, e1 and e2, in a text, the
temporal relation between them is more informative
if they are also linked through a discourse relation,
e.g., a PDTB relation (Prasad et al, 2008). Mak-
ing such an assumption will allow us to focus in on
the temporal relations that are most valuable without
having to exhaustively annotate all event pairs.
Now consider internal informativeness. This is
information derived from the nature of the relation
itself, as defined largely by the algebra of relations
(Allen, 1984; Vilain et al, 1986). First, we assume
that, for two events, e1 and e2, a temporal relation
R1 is more informative than R2 if R1 entails R2.
More significantly, however, as noted above, is to
capitalize on the relations that inhere between events
and the times that anchor them (i.e., (2c) above).
Hence, we will say that, given an event, e1 and a
time t1, a temporal relation R is more informative
the more it anchors e1 to t1. That is, a containment
relation is more informative than an ordering rela-
tion, and the smaller the container, the more infor-
mative the relation.1
The Document Creation Time (DCT) as designed
in TimeML is introduced as a reference time, against
which the mentioned events and time expressions in
the document can be ordered. Consider the text frag-
ment below.
1We defer discussion of the formal definition of informative-
ness for the present paper, as we are focusing on initial results
over re-annotated data in TimeBank.
4-10-2011
Local officials reported yesterday that a
car exploded in downtown Basra.
The TimeML annotation guideline (AG) suggests
identifying relations between the DCT and textual
events. Hence standard markup as in TimeBank re-
sults in the following sort of annotation:
(3) a. DCT= t1, val=10-04-2011
b. t2 = yesterday, val=09-04-2011
b. e1 = report
c. e2 = explode
d. TLINK1 = before(e1, t1)
e. TLINK2 = before(e2, t1)
f. TLINK3 = includes(t2, e1)
This is a prototypical annotation fragment. Notice
that by focusing on the link between events and the
DCT, the annotator is forced to engage in a kind of
periodic ?back-and-forth? evaluation of the events
in the text, relative to the DCT. While there is a con-
tainer TIMEX3 that bounds e1, there is no informa-
tion given grounding the actual time of the event of
interest, namely, the explosion, e2. By following the
AG literally and through no fault of their own, the
annotators have missed an opportunity to provide a
more informative markup; namely, the identification
of the TLINK below:
(4) TLINK4 = includes(t2, e2)
That is, the explosion occurred on the date valued
for yesterday, i.e., ?09-04-2011?.
The point of this paper is to discuss the difference
encountered when applying a specification given a
particular guideline for annotating a body of text.
The example we want to discuss is the manner in
which events are linked (related) to the Document
Creation Time (DCT) in TimeML. These consider-
ations have arisen in the context of new annotation
problems in different genre and domains, hoping to
apply the principles of TimeML.
3 Narrative Scope
As previously mentioned, the Narrative Scope of a
document is the temporal span over which the events
in a document occur, as defined by the timexes in a
154
document. While not every event in a document will
necessarily occur inside the Narrative Scope (some
may still occur before or after any dates that are
specifically mentioned), the Narrative Scope pro-
vides a useful container for describing when events
discussed most likely occurred. The narrative scope
was not considered as part of the annotation task,
but it did help to ground the concepts of Narrative
Containers and Narrative Times.
4 Narrative Time
As a reader moves through a document, the intro-
duction of a new TIMEX will often shift the tem-
poral focus of the events to be anchored to this new
time point (Smith, 2003). These temporal anchors
are what we refer to as Narrative Times, and func-
tion in much the same way as newly introduced lo-
cations in spatial annotation.
However, consider how we can use Narrative
Times to increase accuracy of the TLINKS over a
document in TimeML. As mentioned above, we dis-
tinguish three types of temporal orderings in a text:
time-time, event-time, and event-event. The first
identifies orderings between two TIMEX3 expres-
sions and is performed automatically. The second
identifies what the local Narrative Time for an event
is, i.e., how an EVENT is anchored to a TIMEX3.
Event-event pairings, for the purposes of this paper,
will not be discussed, though they are a vital and
complex component of temporal annotation, largely
involving discourse relations.
To illustrate our proposed strategy, consider the
news article text shown below.
April 25, 2010 7:04 p.m. EDT -t0
S1: President Obama paid-e1 tribute Sunday -t1
to 29 workers killed-e2 in an explosion -e3 at a
West Virginia coal mine earlier this month- t2,
saying-e4 they died-e5 ?in pursuit of the Amer-
ican dream.?
S2: The blast-e6 at the Upper Big Branch Mine
was the worst U.S. mine disaster in nearly 40
years.
There are three temporal expressions in the above
text: the Document Creation time, t0; and two
TIMEXes, t1 and t2. Each of these TIMEXes func-
tions as a Narrative Time, as they are clearly provid-
ing temporal anchors to nearby events. In this case,
all the events are located within the Narrative Time
appropriate to them. Hence, the number of order-
ings is linearly determined by the number of events
in the document, since each is identified with a sin-
gle Narrative Time. Knowing the narrative time as-
sociated with each event will allow us to perform
limited temporal ordering between events that are
associated with different narrative times, which, as
mentioned above, is significantly more informative
than if events were only given partial orderings to
the DCT or to each other.
5 Narrative Containers
So far we have examined sentences that contain
specific temporal anchors for the events discussed.
Consider, however, the following sentences from ar-
ticle wsj 1031.tml in TimeBank:
10-26-1989
1 Philip Morris Cos., New York, adopted a
defense measure designed to make a hostile
takeover prohibitively expensive.
2 The giant foods, tobacco and brewing company
said it will issue common-share purchase rights to
shareholders of record Nov. 8.
Aside from the DCT, the only TIMEX in these
two sentences is Nov. 8, which is only anchoring is-
sue and record. The other events in the sentences
can only be connected to the DCT, and presum-
ably only in a ?before? or ?after? TLINK?in the ab-
sence of other information, any reader would assume
from the past tenses of adopted and said that these
events occurred before the article was published, and
that any events associated with the future (make,
takeover) are intended to happen after the DCT.
However most readers, knowing that the Wall
Street Journal is published daily, will likely assume
that any event mentioned which is not specifically
associated with a date, occurred within a certain
time frame?it would be extremely unusual for a
newspaper to use the construction presented above
if the events actually occurred, for example, a year
or even a week prior to the publication date. We call
this assumed window the Narrative Container, as it
provides left and right boundaries for when unan-
155
t2 "earlier this month"
t1 
"Sunday"
e3 
explosion
e5 "died"e1 "paid" e2 "killed"
e4 
"saying"
t0 DCT e6 "blast"
t0 DCT
t1 "Sunday"
e2 "killed"
t2 earlier 
this month
e5 "died" e6 "blast" e1 "paid"
e4 
"saying"
e3 
explosion
A
B
Figure 1: A: Times and events as appearing in the text; B: events grouped into their appropriate Narrative Times.
chored events most likely occurred, where in pre-
vious TimeML annotations these events would usu-
ally be given one-sided relationships to the DCT. In
most cases, the right boundary of the Narrative Con-
tainer is the DCT. The left boundary, however, re-
quires other factors about the article to be taken into
account before it can be given a value. The primary
factor is how frequently the source of the document
is published, but other aspects of the article may also
determine the Narrative Container size.
5.1 Style, Genre, Channel, and Anchors
In order to determine what factors might influence
the interpretation of the size of a Narrative Con-
tainer, we asked an undergraduate researcher to cat-
egorize each of the articles in TimeBank according
to the following characteristics (Lee, 2001; Biber,
2009).
(5) ? Channel: is the document written or spoken?
? Production circumstances: how was the doc-
ument distributed? broadcast, newswire, daily
publication;
? Style: what format was used to present the
information?
? Presence of a temporal anchor: Whether an
article contained a Narrative Time in the first
sentence of the document.
In general, we felt that the production circum-
stances would be the most relevant in determining
the duration of the Narrative Container. The distri-
butions of the different categories in TimeBank are
shown in Table 1. There is a 100% overlap between
the ?broadcast? and ?spoken? subcategories?all of
those articles are word-for-word transcripts of tele-
vision news reports. The ?style? category proved the
most difficult to define?the ?quiz? article is a broad-
cast transcript of a geography question asked during
the evening news, while the ?biography? articles are
overviews of people?s lives. The editorials include a
letter to the editor of the Wall Street Journal and an
editorial column from the New York Times.
Category number percent
Production Circ.
broadcast 25 13.7%
daily paper 140 76.5%
newswire 18 9.8%
Channel
spoken 25 13.7%
written 158 86.3%
Style
biography 2 1.1%
editorial 2 1.1%
finance 135 73.8%
news 43 23.5%
quiz 1 0.5%
Temporal Anchor
no 138 75.4%
yes 45 24.6%
Table 1: Distributions of categories in TimeBank
6 Preliminary Studies
In order to assess the validity of our theories on Nar-
rative Containers, Time, and Scope, we asked three
undergraduate researchers to re-annotate TimeBank
using the Narrative Container theory as a guide.
Each annotator evaluated all of the events in
TimeBank by identifying the temporal constraint
that anchored the event. If the annotators felt that
the event was not specifically anchored, they could
156
place it within the Narrative Container for the docu-
ment, or they could give the event a simple ?before?
or ?after? value related to the Narrative Container or
Document Creation Time. We also asked them to
assign start and end times to the Narrative Container
for each document.
The annotation here was not intended to be as
complete as the TimeBank annotation task, or even
the TempEval tasks?rather, the goal was to deter-
mine if the Narrative Container theory could be ap-
plied in a way that resulted in an increase in infor-
mativeness, and whether the annotators could work
with the idea of a Narrative Container. Because
these annotations are not comprehensive in their
scope, the analysis provided here is somewhat pre-
liminary, but we believe it is clear that the use of a
Narrative Container in temporal annotations is both
informative and intuitive.
6.1 Narrative container agreement
Each annotator was asked to assign a value to the
narrative container of each document. They were
given limited directions as to what the size of an NC
might be: only some suggestions regarding possible
correlations between type and frequency of publica-
tion and size of the narrative container. For example,
it was suggested that a news broadcast might have a
narrative container of only a few hours, a daily news-
paper would have one of a day (or one that extended
to the previous business day), and a newswire article
would have a narrative container that extended back
24 hours from the time of publication.
All the annotators agreed that an NC would not
extend forward beyond the document creation time
(DCT), and that in most cases the NC would end at
the DCT. Because the annotators gave their data on
the size of the NC in free text (for example, an an-
notator would say ?1 day? to indicate that the NC
for an article began the day before the article was
published) the comparison of the narrative contain-
ers was performed manually by one of the authors to
determine if the annotators agreed on the size of the
NC.
Agreement was determined using a fairly strict
matching criterion?if the narrative containers given
were clearly referring to the same interval they were
interpreted to be the same. If, however, there was
ambiguity about the date or one annotator indicated
a smaller time period than another, then they were
judged to be different. A common example of am-
biguity was related to newspaper articles that were
written on Mondays?annotators could not always
determine if the events described occurred the day
before, or on the previous business day For eval-
uation purposes, the ambiguous cases were given
?maybe? values, but were not included in analysis
that relied on the NCs being the same.
Overall, using the strict agreement metric all the
annotators agreed on the size of the narrative con-
tainer in 95 out of 183 articles?slightly over 50% of
the time. However, the annotators only completely
disagreed on 6 of the 183 articles?in all other cases
there was some level of agreement between pairs of
annotators.
6.2 NCs and Document Classifications
We compared Narrative Container agreements
against the categories outlined above: style, channel,
production circumstances, and temporal anchorings
in order to determine if any of those attributes lent
themselves to agreement about the size of the Narra-
tive Container. We disregarded the biography, quiz,
and editorial classifications as those categories were
too small to provide useful data.
For the most part, no one category stood out as
lending itself to accuracy?newswire had the high-
est levels of agreement at 72%, while daily papers
came in at 58%. Written channels had 60% agree-
ment, and the finance style had 59%. Articles with
temporal anchors in the beginning of the document
were actually slightly less likely to have agreement
on the Narrative Container than those that didn?t?
48% and 53%, respectively.
While the higher disagreement levels over Nar-
rative Container size in the presence of a temporal
anchor seems counter-intuitive, it stems from a sim-
ple cause: if the temporal anchor overlapped with
the expected narrative container but was not exactly
the same size, sometimes one annotator would use
that anchor as the Narrative Container, while the oth-
ers would not. This sometimes also happened with
a Narrative Time that was not at the start of the
document or sometimes even the Narrative Scope
would be used as the Narrative Container. While
in some articles it is the case that a Narrative Time
anchors more events than the Narrative Container,
157
ll
ll
l
0.0
0.2
0.4
0.6
0.8
1.0
Fleiss Kappa by Article Category
Fleis
s Ka
ppa
l
l
ll
0.0
0.2
0.4
0.6
0.8
1.0
l
l
l
l
0.0
0.2
0.4
0.6
0.8
1.0
l
l
ll
l
0.0
0.2
0.4
0.6
0.8
1.0
Broadcast Daily Newswire Spoken Written Finance News No YesProduction Circ Channel Style Temporal Anchorings
Figure 2: Distributions of Fleiss Kappa scores over TimeBank categories
that does not make that Narrative Time the Nar-
rative Container for the document?the Narrative
Container is always the interval during which an
unanchored event would be assumed to have taken
place. This point of confusion can easily be clarified
in the guidelines.
Spoken/broadcast articles had the lowest agree-
ment on Narrative Container size, with none of those
articles having complete agreement between anno-
tators. This was largely caused by our annotators
not agreeing on how much time those categories
would encompass by default?two felt that the narra-
tive containers for broadcast news would extend to
only a few hours before going on air, and the other
felt that, like a daily paper, the entire previous day
would be included when dealing with unanchored
times.
As for the question of how large a Narrative Con-
tainer should be for broadcast articles, the size of all
Narrative Containers will need to be studied more
in depth in order to determine how widely they can
be applied? it is possible that in general, the actual
size is less important than the simple concept of the
Narrative Container.
6.3 Agreement over event anchors
The annotators were asked to read each article in
TimeBank and ?create links from each event to the
nearest timex or to the DNC.? They were asked
specifically to not link an event to another event,
only to find the time that would be used to anchor
each event in a timeline. The annotators were also
asked to use only three relationship types: before,
after, and is included (which also stood in for ?over-
lap?). This was done in order to keep the annotation
as simple as possible: we wanted to see if the narra-
tive container was a useful tool in temporal annota-
tion, not produce a full gold standard corpus.
This differs from the TimeML annotation guide-
lines, which suggested only that ?A TLINK has to
be created each time a temporal relationship hold-
ing between events or an event and a time needs to
be annotated.? (Saur?? et al, 2006) Examples given
were for sentences such as ?John drove to Boston on
Monday??cases where an event was specifically re-
lated to a time or another event. However, because
such examples were relatively rare, and temporal re-
lationships are not always so clearly expressed, this
annotation method resulted in a corpus that was not
optimally informative. TimeML also uses a fuller
set of temporal relations.
The NC annotations, on the other hand, are much
richer in terms of informativeness. Annotators most
often linked to the NC, often with an ?is included?
relationship (as in: e1 is included NC). In fact,
roughly 50% of the events were linked to the narra-
tive container and had ?is included? as the relation-
ship type. In previous TimeML annotations, most of
those events would have been annotated as simply
occurring before or overlapping with the document
creation time, which is a significantly less informa-
tive association. Clearly the narrative container was
an intuitive concept for the annotators, and one that
was relevant to their annotations.
6.3.1 Inter-annotator agreement
We used Fleiss? kappa (Fleiss, 1971) to obtain values
for agreement between the three annotators: first,
we compared the number of times they agreed what
the temporal anchor for an event should be, then we
compared whether those links that matched had the
same relation type. Data analysis was done in R with
the irr package (R Team, 2009; Gamer et al, 2010).
158
said (e1)
enable (e10)declared (e2)purchase (e12)issue (e18)
exercised (e14)
90 days(t31)
issued (e7)issue (e32)
10/26/89 (t30) (DCT)
said (e8)has (e5)said (e17)
10/25/89 ? 10/26/89 (NC)
10/26/89 (t30) (DCT)
said (e1)declared (e2)has (e5)issued (e7)said (e8)issue (e32)said (e17)
90 days (t31)
exercised (e14)
enable  (e10)
purchase(e12)
issue (e18)
TimeBank annotation Narrative Container annotation
Wall Street Journal - wsj_1042.tml
Figure 3: Visual depictions of the TLINK annotations in TimeBank and with the Narrative Container annotations.
Solid lines indicate events and times in the box have IS INCLUDED relationships with the timex at the top, and
dotted lines indicate events that were given IDENTITY relationships
When looking at the kappa scores for the tempo-
ral anchor, it should be noted that these scores do
not always accurately reflect the level of agreement
between annotators. Because of the lack of variabil-
ity, Fleiss? Kappa will interpret any article where an
annotator only linked events to the NC received neg-
ative agreement scores. These values have been left
in the tables as data points, but it should be noted
that these annotations are entirely valid?some ar-
ticles in TimeBank contain no temporal information
other than the document creation time (and by exten-
sion, the narrative container), making it only natural
for the annotators to annotate events only in rela-
tion to the narrative container. The average Fleiss?
Kappa scores for the temporal anchors was .74, with
a maximum of 1 and a minimum of -.04.
6.4 Informativeness in NC Annotation
As we previously described, Narrative Containers
are theoretically more informative than Document
Creation Times when trying to place unanchored
events on a timeline. In practice, they are as infor-
mative as we anticipated: compare the visualizations
of TLINK annotations between TimeBank and the
NC links in Figure 3. These were created from the
file wsj 1042.tml, one that had complete agreement
between annotators about both the size of the NC
(one day before the DCT through the DCT) and all
the temporal anchors and temporal relations.
Clearly, the NC task has resulted in a more in-
formative annotation?all the events have at least one
constraint, and most have both left and right con-
straints.
7 Conclusions and Future Work
Narrative Containers, Narrative Times, and Narra-
tive Scopes are important tools for temporal annota-
tion tasks. The analysis provided here clearly shows
that annotating with an NC increases informative-
ness, and that the concept is sufficiently intuitive for
it to not add confusion to the already complicated
task of temporal annotation. However, the work in
this area is far from complete. In the future we in-
tend to study where the left boundary of the NC
should be placed for different genres and publica-
tion frequencies. Another annotation task must be
performed, requiring a more comprehensive TLINK
creation guideline, using both event-time and event-
event links. Finally, the use of all three concepts for
automated annotation tasks should be examined, as
they may prove as useful to machines as they are to
humans.
Acknowledgements
This work has been supported by NSF grant
#0753069 to Co-PI James Pustejovsky. Many thanks
to Chiara Graf, Zac Pustejovsky, and Virginia Par-
tridge for their help creating the annotations, and to
BJ Harshfield for his R expertise. We would also
like to acknowledge Aravind Joshi, Nianwen Xue,
and Marc Verhagen for useful input.
159
References
James Allen. 1984. Towards a general theory of action
and time. Arificial Intelligence, 23:123?154.
Douglas Biber. 2009. Register, Genre, and Style.
J. L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
Matthias Gamer, Jim Lemon, and Ian Fellows Pus-
pendra Singh ?puspendra.pusp22@gmail.com?, 2010.
irr: Various Coefficients of Interrater Reliability and
Agreement. R package version 0.83.
Nancy Ide and Harry Bunt. 2010. Anatomy of annota-
tion schemes: Mappings to graf. In In Proceedings 4th
Linguistic Annotation Workshop (LAW IV).
Nancy Ide. 2007. Annotation science: From theory to
practice and use: Data structures for linguistics re-
sources and applications. In In Proceedings of the Bi-
enniel GLDV Conference.
David Lee. 2001. Genres, registers, text types, domains,
and styles: Clarifying the concepts and navigating a
path through the bnc jungle. Language Learning &
Technology, 5(3.3):37?72.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. The penn discourse treebank.
In In Proceedings of LREC 2004.
Massimo Poesio. 2004. Discourse annotation and se-
mantic annotation in the gnome corpus. In In Proceed-
ings of the ACL Workshop on Discourse Annotation.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
In Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC 2008).
James Pustejovsky, Patrick Hanks, Roser Saur?`, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro, and
Marcia Lazo. 2003. The timebank corpus. In
Dawn Archer, Paul Rayson, Andrew Wilson, and Tony
McEnery, editors, Proceedings of the Corpus Linguis-
tics 2003 conference, pages 647?656, Lancaster Uni-
versity. UCREL.
James Pustejovsky, Robert Knippen, Jessica Littman, and
Roser Saur??. 2005. Temporal and event information in
natural language text. Language Resources and Eval-
uation, 39:123?164, May.
R Team, 2009. R: A Language and Environment for Sta-
tistical Computing. R Foundation for Statistical Com-
puting, Vienna, Austria. ISBN 3-900051-07-0.
Roser Saur??, Jessica Littman, Bob Knippen, Robert
Gaizauskas, Andrea Setzer, and James Pustejovsky,
2006. TimeML Annotation Guidelines, version 1.2.1
edition, January.
Carlota Smith. 2003. Modes of Discourse. Cambridge
University Press, Cambridge, UK.
Marc Vilain, Henry Kautz, and Peter Beek. 1986. Con-
straint propagation algorithms for temporal reasoning.
In Readings in Qualitative Reasoning about Physical
Systems, pages 377?382. Morgan Kaufmann.
160
