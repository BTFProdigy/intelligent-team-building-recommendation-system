Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 657?669,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Finding Good Enough: A Task-Based Evaluation of Query Biased
Summarization for Cross Language Information Retrieval
Jennifer Williams, Sharon Tam, Wade Shen
MIT Lincoln Laboratory Human Language Technology Group
244 Wood Street, Lexington, MA 02420 USA
jennifer.williams@ll.mit.edu, sharontam@alum.mit.edu
swade@ll.mit.edu
Abstract
In this paper we present our task-based
evaluation of query biased summarization
for cross-language information retrieval
(CLIR) using relevance prediction. We de-
scribe our 13 summarization methods each
from one of four summarization strate-
gies. We show how well our methods
perform using Farsi text from the CLEF
2008 shared-task, which we translated to
English automtatically. We report preci-
sion/recall/F1, accuracy and time-on-task.
We found that different summarization
methods perform optimally for different
evaluation metrics, but overall query bi-
ased word clouds are the best summariza-
tion strategy. In our analysis, we demon-
strate that using the ROUGE metric on our
sentence-based summaries cannot make
the same kinds of distinctions as our evalu-
ation framework does. Finally, we present
our recommendations for creating much-
needed evaluation standards and datasets.
1 Introduction
Despite many recent advances in query biased
summarization for cross-language information re-
trieval (CLIR), there are no existing evaluation
standards or datasets to make comparisons among
different methods, and across different languages
(Tombros and Sanderson, 1998; Pingali et al.,
2007; McCallum et al., 2012; Bhaskar and Bandy-
opadhyay, 2012). Consider that creating this
kind of summary requires familiarity with tech-
niques from machine translation (MT), summa-
rization, and information retrieval (IR). In this
This work was sponsored by the Federal Bureau of Inves-
tigation under Air Force Contract FA8721-05-C-0002. Opin-
ions, interpretations, conclusions, and recommendations are
those of the authors and are not necessarily endorsed by the
United States Government.
paper, we arrive at the intersection of each of
these research areas. Query biased summariza-
tion (also known as query-focused, query-relevant,
and query-dependent) involves automatically cap-
turing relevant ideas and content from a document
with respect to a given query, and presenting it as a
condensed version of the original document. This
kind of summarization is mostly used in search en-
gines because when search results are tailored to a
user?s information need, the user can find texts that
they are looking for more quickly and more ac-
curately (Tombros and Sanderson, 1998; Mori et
al., 2004). Query biased summarization is a valu-
able research area in natural language processing
(NLP), especially for CLIR. Users of CLIR sys-
tems meet their information needs by submitting
their queries in L
1
to search through documents
that have been composed in L
2
, even though they
may not be familiar with L
2
(Hovy et al., 1999;
Pingali et al., 2007).
There are no standards for objectively evaluat-
ing summaries for CLIR ? a research gap that we
begin to address in this paper. The problem we
explore is two-fold: what kinds of summaries are
well-suited for CLIR applications, and how should
the summaries be evaluated. Our evaluation is ex-
trinsic, that is to say we are interested in how sum-
marization affects performance on a different task
(Mani et al., 2002; McKeown et al., 2005; Dorr
et al., 2005; Murray et al., 2009; McCallum et
al., 2012). We use relevance prediction as our ex-
trinsic task: a human must decide if a summary
for a given document is relevant to a particular in-
formation need, or not. Relevance prediction is
known to be useful as it correlates with some au-
tomatic intrinsic methods as well (President and
Dorr, 2006; Hobson et al., 2007). To the best of
our knowledge, we are the first to apply this eval-
uation framework to cross language query biased
summarization.
Each one of the summarization methods that we
657
present in this paper belongs to one of the fol-
lowing strategies: (1) unbiased full machine trans-
lated text, (2) unbiased word clouds, (3) query bi-
ased word clouds, and (4) query biased sentence
summaries. The methods and strategies that we
present are fast, cheap, and language-independent.
All of these strategies are extractive, meaning that
we used existing parts of a document to create the
condensed version, or summary.
We approach our task as an engineering prob-
lem: the goal is to decide if summaries are good
enough to help CLIR system users find what they
are looking for. We have simplified the task by as-
suming that a set of documents has already been
retrieved from a search engine, as CLIR tech-
niques are outside the scope of this paper. We
predict that showing the full MT English text as
a summarization strategy would not be particu-
larly helpful in our relevance prediction task be-
cause the words in the text could be mixed-up,
or sentences could be nonsensical, resulting in
poor readability. For the same reasons, we expect
that showing the full MT English text would take
longer to arrive at a relevance decision. Finally,
we predict that query biased summaries will result
in faster, more accurate decisions from the partic-
ipants (Tombros and Sanderson, 1998).
We treat the actual CLIR search engine as if it
were a black box so that we can focus on evaluat-
ing if the summaries themselves are useful. As a
starting point, we begin with some principles that
we expect to hold true when we evaluate. These
principles provide us with the kind of framework
that we need for a productive and judicious dis-
cussion about how well a summarization method
works. We encourage the NLP community to
consider the following concepts when developing
evaluation standards for this problem:
? End-user intelligiblity
? Query-salience
? Retrieval-relevance
Summaries should be presented to the end-user in
a way that is both concise and intelligible, even
if the machine translated text is difficult to under-
stand. Our notions of query-salience and retrieval-
relevance capture the expectation that good sum-
maries will be efficient enough to help end-users
fulfill their information needs. For query-salience,
we want users to positively identify relevant doc-
uments. Similarly, for retrieval-relevance we want
users to be able to find as many relevant docu-
ments as possible.
This paper is structured as follows: Section 2
presents related work; Section 3 describes our data
and pre-processing; Section 4 details our sum-
marization methods and strategies; Section 5 de-
scribes our experiments; Section 6 shows our re-
sults and analysis; and in Section 7, we conclude
and discuss some future directions for the NLP
community.
2 Related Work
Automatic summarization is generally a well-
investigated research area. Summarization is a
way of describing the relationships of words in
documents to the information content of that doc-
ument (Luhn, 1958; Edmunson, 1969; Salton and
Yang, 1973; Robertson and Walker, 1994; Church
and Gale, 1999; Robertson, 2004). Recent work
has looked at creating summaries of single and
multiple documents (Radev et al., 2004; Erkan and
Radev, 2004; Wan et al., 2007; Yin et al., 2012;
Chatterjee et al., 2012), as well as summary eval-
uation (Jing et al., 1998; Tombros and Sanderson
1998; Mani et al., 1998; Mani et al., 1999; Mani,
2001; Lin and Hovy, 2003; Lin, 2004; Nenkova
et al., 2007; Hobson et al., 2007; Owczarzak
et al., 2012), query and topic biased summariza-
tion (Berger and Mittal, 2000; Otterbacher et al.,
2005; Daume and Marcu, 2006; Chali and Joty,
2008; Otterbacher et al., 2009; Bando et al., 2010;
Bhaskar and Bandyopadhyay, 2012; Harwath and
Hazen, 2012; Yin et al., 2012), and summarization
across languages (Pingali et al., 2007; Or?asan and
Chiorean, 2008; Wan et al., 2010; Azarbonyad et
al., 2013).
2.1 Query Biased Summarization
Previous work most closely related to our own
comes from Pingali et al., (2007). In their work,
they present their method for cross-language
query biased summarization for Telugu and En-
glish. Their work was motivated by the need for
people to have access to foreign-language docu-
ments from a search engine even though the users
were not familiar with the foreign language, in
their case English. They used language model-
ing and translation probability to translate a user?s
query into L
2
, and then summarized each docu-
ment in L
2
with respect to the query. In their final
step, they translated the summary from L
2
back
658
to L
1
for the user. They evaluated their method
on the DUC 2005 query-focused summarization
shared-task with ROUGE scores. We compare our
methods to this work also on the DUC 2005 task.
Our work demonstrates the first attempt to draw at
a comparison between user-based studies and in-
trinsic evaluation with ROUGE. However, one of
the limitations with evaluating this way is that the
shared-task documents and queries are monolin-
gual.
Bhaskar and Bandyopadhyay (2012) tried a
subjective evaluation of extractive cross-language
query biased summarization for 7 different lan-
guages. They extracted sentences, then scored and
ranked the sentences to generate query dependent
snippets of documents for their cross lingual in-
formation access (CLIA) system. However, the
snippet quality was determined subjectively based
on scores on a scale of 0 to 1 (with 1 being best).
Each score indicated annotator satisfaction for a
given snippet. Our evaluation methodology is ob-
jective: we ask users to decide if a given document
is relevant to an information need, or not.
2.2 Machine Translation Effects
Machine translation quality can affect summa-
rization quality. Wan et al. (2010) researched
the effects of MT quality prediction on cross-
language document summarization. They gener-
ated 5-sentence summaries in Chinese using En-
glish source documents. To select sentences, they
used predicted translation quality, sentence posi-
tion, and sentence informativeness. In their eval-
uation, they employed 4 Chinese-speakers to sub-
jectively rate summaries on a 5-point scale (5 be-
ing best) along the dimensions of content, read-
ability, and overall impression. They showed that
their approach of using MT quality scores did im-
prove summarization quality on average. While
their findings are important, their work did not ad-
dress query biasing or objective evaluation of the
summaries. We attempt to overcome limitations of
machine translation quality by using word clouds
as one of our summarization strategies.
Knowing when to translate is another challenge
for cross-language query biased summarization.
Several options exist for when and what to trans-
late during the summarization process: (1) the
source documents can be translated, (2) the user?s
query can be translated, (3) the final summary can
be translated, or (4) some combination of these.
An example of translating only the summaries
themselves can be found in Wan et al., (2010).
On the other hand, Pingali et al. (2007) translated
the queries and the summaries. In our work, we
used gold-translated queries from the CLEF 2008
dataset, and machine translated source documents.
We briefly address this in our work, but note that a
full discussion of when and what to translate, and
those effects on summarization quality, is outside
of the scope of this paper.
2.3 Summarization Evaluation
There has been a lot of work towards developing
metrics for understanding what makes a summary
good. Evaluation metrics are either intrinsic or ex-
trinsic. Intrinsic metrics, such as ROUGE, mea-
sure the quality of a summary with respect to gold
human-generated summaries (Lin, 2004; Lin and
Hovy, 2003). Generating gold standard summaries
is expensive and time-consuming, a problem that
persists with cross-language query biased summa-
rization because those summaries must be query
biased as well as in a different language from the
source documents.
On the other hand, extrinsic metrics measure the
quality of summaries at the system level, by look-
ing at overall system performance on downstream
tasks (Jing et al, 1998; Tombros and Sanderson,
1998). One of the most important findings for
query biased summarization comes from Tombros
and Sanderson (1998). In their monolingual task-
based evaluation, they measured user speed and
accuracy at identifying relevant documents. They
found that query biased summarization improved
the user speed and accuracy when the user was
asked to make relevance judgements for IR tasks.
We also expect that our evaluation will demon-
strate that user speed and accuracy is better when
summaries are query biased.
3 Data and Pre-Processing
We used data from the Farsi CLEF 2008 ad hoc
task (Agirre et al., 2009). Each of the queries in-
cluded in this dataset consisted of a title, narrative,
and description. Figure 1 shows an example of the
elements of a CLEF 2008 query. All of the au-
tomatic query-biasing in this work was based on
the query titles. For our human relevance predic-
tion task on Mechanical Turk, we used the nar-
rative version. The CLEF 2008 dataset included
a ground-truth answer key indicating which docu-
659
ments were relevant to each query. For each query,
we randomly selected 5 documents that were rele-
vant as well as 5 documents that were not relevant.
The subset of CLEF 2008 data that we used there-
fore consisted of 500 original Farsi documents and
50 parallel English-Farsi queries. Next we will de-
scribe our text pre-processing steps for both lan-
guages as well as how we created our parallel En-
glish documents.
Figure 1: Full MT English summary and CLEF
2008 English query (title, description, narrative).
3.1 English Documents
All of our English documents were created auto-
matically by translating the original Farsi docu-
ments into English (Drexler et al., 2012). The
translated documents were sentence-aligned with
one sentence per line. For all of our summariza-
tion experiments (except unbised full MT text),
we processed the text as follows: removed extra
spaces, removed punctuation, folded to lowercase,
and removed digits. We also removed common
English stopwords
2
from the texts.
3.2 Farsi Documents
We used the original CLEF 2008 Farsi docu-
ments for two of our summarization methods. We
stemmed words in each document using automatic
morphological analysis with Morfessor CatMAP.
We note that within-sentence punctuation was re-
moved during this process (Creutz and Lagus,
2007). We also removed Farsi stopwords and dig-
its.
4 Summarization Strategies
All of our summarization methods were extrac-
tive except for unbiased full machine translated
text. In this section, we describe each of our
13 summarization methods which we have orga-
nized into one of the following strategies: (1) un-
biased full machine translated text, (2) unbiased
2
English and Farsi stopword lists from:
http://members.unine.ch/jacques.savoy/clef/index.html
word cloud summaries, (3) query biased word
cloud summaries, and (4) query biased sentence
summaries. Regardless of which summarization
method used, we highlighted words in yellow that
also appeard in the query. Let t be a term in
document d where d ? D
L
and D
L
is a collec-
tion of documents in a particular language. Note
that for our summarization methods, term weight-
ings were calculated separately for each language.
While |D| = 1000, we calculated term weightings
based on |D
E
| = 500 and |D
F
| = 500. Finally,
let q be a query where q ? Q and Q is our set of
50 parallel English-Farsi CLEF queries. Assume
that log refers to log
10
.
Figure 2: Full MT English summary and CLEF
2008 English query.
4.1 Unbiased Full Machine Translated
English
Our first baseline approach was to use all of the
raw machine translation output (no subsets of
the sentences were used). Each summary there-
fore consisted of the full text of an entire doc-
ument automatically translated from Farsi to En-
glish (Drexler et al., 2012). Figure 2 shows an ex-
ample full text document translated from Farsi to
English and a gold-standard English CLEF query.
Note that we use this particular document-query
pair as an example throughout this paper (docu-
ment: H-770622-42472S8, query: 10.2452/552-
AH). According to the CLEF answer key, the sam-
ple document is relevant to the sample query.
4.2 Unbiased Word Clouds
For our second baseline approach, we ranked
terms in a document and displayed them as word
clouds. Word clouds are one a way to arrange
a collection of words where each word can vary
660
in size. We used word clouds as a summariza-
tion strategy to overcome any potential disfluen-
cies from the machine translation output and also
to see if they are feasible at all for summarization.
All of our methods for word clouds used words
from machine translated English text. Each term-
ranking method below generates different ranked
lists of terms, which we used to create different
word clouds. We created one word cloud per doc-
ument using the top 12 ranked words. We used
the raw term scores to scale text font size, so that
words with a highter score appeared larger and
more prominent in a word cloud. Words were
shuffled such that the exact ordering of words was
at random.
I: Term Frequency (TF) Term frequency is
very commonly used for finding important terms
in a document. Given a term t in a document d,
the number of times that term occurs is:
tf
t,d
= |t ? d|
II: Inverse Document Frequency (IDF) The
idf term weighting is typically used in IR and
other text categorization tasks to make distinc-
tions between documents. The version of idf that
we used throughout our work came from Erkan
and Radev (2004) and Otterbacher et al. (2009),
in keeping consistent with theirs. Let N be the
number of documents in the collection, such that
N = |D| and n
t
is the number of documents that
contain term t, such that n
t
= |{d ? D : t ? d}|,
then:
idf
t
= log
N + 1
0.5? n
t
While idf is usually thought of as a type of
heuristic, there have been some discussions about
its theoretical basis (Robertson, 2004; Robertson
and Walker, 1994; Church and Gale, 1999; Salton
and Yang, 1973). An example of this summary is
shown in Figure 3.
III: Term Frequency Inverse Document Fre-
quency (TFIDF) We use tfidf
t,d
term weight-
ing to find terms which are both rare and impor-
tant for a document, with respect to terms across
all other documents in the collection:
tfidf
t,d
= tf
t,d
? idf
t
4.3 Query Biased Word Clouds
We generated query biased word clouds following
the same principles as our unbiased word clouds,
Figure 3: Word cloud summary for inverse docu-
ment frequency (IDF), for query ?Tehran?s stock
market?.
namely the text font scaling and highlighting re-
mained the same.
IV. Query Biased Term Frequency (TFQ) In
Figure 4 we show a sample word cloud summary
based on query biased term frequency. We define
query biased term frequency tfQ at the document
level, as:
tfQ
t,d,q
=
{
2tf
t,d
, if t ? q
tf
t,d
, otherwise
Figure 4: Word cloud summary for query biased
term frequency (TFQ), for query ?Tehran?s stock
market?.
V. Query Biased Inverse Document Frequency
(IDFQ) Since idf helps with identifying terms
that discriminate documents in a collection, we
would expect that query biased idf would help to
identify documents that are relevant to a query:
idfQ
t,q
=
{
2idf
t
, if t ? q
idf
t
, otherwise
VI. Query Biased TFIDF (TFIDFQ) We de-
fine query biased tf ? idf similarly to our TFQ
and IDFQ, at the document level:
tfidfQ
t,d,q
=
{
2tf
t,d
? idf
t
, if t ? q
tf
t,d
? idf
t
, otherwise
661
Figure 5: Word cloud summary for scaled query
biased term frequency (SFQ) for query ?Tehran?s
stock market?.
VII. Query Biased Scaled Frequency (SFQ)
This term weighting scheme, which we call scaled
query biased term frequency or sfQ, is a variant of
the traditional tf?idf weighting. First, we project
the usual term frequency into log-space, for a term
t in document d with:
tfS
t,d
= log(tf
t,d
)
We let tfS
t,d
? 0 when tf
t,d
= 1. We believe that
singleton terms in a document provide no indica-
tion that a document is query-relevant, and trea-
ment of singleton terms in this way would have the
potential to reduce false-positives in our relevance
prediction task. Note that scaled term frequency
differs from Robertson?s (2004) inverse total term
frequency in the sense that our method involves no
consideration of term position within a document.
Scaled query biased term frequency, shown in Fig-
ure 5, is defined as:
sfQ
t,d,q
=
{
2tfS
t,d
? idf
t
, if t ? q
tfS
t,d
? idf
t
, otherwise
VIII. Word Relevance (W) We adapted an
existing relevance weighting from Allan et al.,
(2003), that was originally formulated for ranking
sentences with respect to a query. However, we
modified their originaly ranking method so that we
could rank individual terms in a document instead
of sentences. Our method for word relevance, W
is defined as:
W
t,d,q
= log(tf
t,d
+ 1)? log(tf
t,q
+ 1)? idf
t
In W , term frequency values are smoothed by
adding 1. The smoothing could especially af-
fect rare terms and singletons, when tf
t,d
is very
low. All terms in a query or a document will
be weighted and each term could potentially con-
tribute to summary.
4.4 Query Biased Sentence Summaries
Sentences are a canonical unit to use in extractive
summaries. In this section we describe four differ-
ent sentence scoring methods that we used. These
methods show how to calculate sentence scores for
a given document with respect to a given query.
Sentences for a document were always ranked us-
ing the raw score value output generated from a
scoring method. Each document summary con-
tained the top 3 ranked sentences where the sen-
tences were simply listed out. Each of these meth-
ods used sentence-aligned English machine trans-
lated documents, and two of them also used the
original Farsi text.
IX. Sentence Relevance (REL) Our sentence
relevance scoring method comes from Allan et al.
(2003). The sentence weight is a summation over
words that appear in the query. We provide their
sentence scoring formula here. This calculates the
relevance score for a sentence s from document d,
to a query q:
rel
(s|q)
=
?
t?s
log(tf
t,s
+1)? log(tf
t,q
+1)? idf
t
Terms will occur in either the sentence or the
query, or both. We applied this method to machine
tranlsated English text. The output of this method
is a relevance score for each sentence in a given
document. We used those scores to rank sentences
in each document from our English machine trans-
lated text.
X. Query Biased Lexrank (LQ) We imple-
mented query biased LexRank, a well-known
graph-based summarization method (Otterbacher
et al., 2009). It is a modified version of the orig-
inal LexRank algorithm (Erkan and Radev, 2004;
Page et al., 1998). The similarity metric, sim
x,y
,
also known as idf-modified cosine similarity, mea-
sures the distance between two sentences x and y
in a document d, defined as:
sim
x,y
=
?
t?x,y
tf
t,x
? tf
t,y
? (idf
t
)
2
?
?
t?x
tfidf
2
t,x
?
?
t?y
tfidf
2
t,y
We used sim
x,y
to score the similarity of
sentence-to-sentence, resulting in a similarity
662
Figure 6: LQP - projecting Farsi sentence scores
onto parallel English sentences.
graph where each vertex was a sentence and each
edge was the cosine similarity between sentences.
We normalized the cosine matrix with a similarity
threshold (t = 0.05), so that sentences above this
threshold were given similarity 1, and 0 otherwise.
We used rel
(s|q)
to score sentence-to-query. The
LexRank score for each sentence was then calcu-
lated as:
LQ
s|q
=
d? rel
s|q
?
z?C
rel
z|q
+ (1? d)?
?
v?adj[s]
sim
s,v
?
r?adj[v]
sim
v,r
LQ
v|q
where C is the set of all sentences in a given doc-
ument. Here the parameter d is just a damper to
designate a probability of randomly jumping to
one of the sentences in the graph (d = 0.7). We
found the stationary distribution by applying the
power method ( = 5), which is guaranteed to
converge to a stationary distribution (Otterbacher
et al., 2009). The output of LQ is a score for each
sentence from a given document with respect to
a query. We used that score to rank sentences in
each document from our English machine trans-
lated text.
XI. Projected Cross-Language Query Biased
Lexrank (LQP) We introduce LQP to describe
a way of scoring and ranking sentences such that
the L
1
(English) summaries are biased from the
L
2
(Farsi) query and source document. Our gold-
standard Farsi queries were included with our
CLEF 2008 data, making them more reliable than
what we could get from automatic translation.
First, sentences from each Farsi document were
scored with Farsi queries using LQ, described
above. Then each LQ score was projected onto
sentence-aligned English. We demonstrate LQP
Figure 7: LQC - Farsi sentence scores are com-
bined with parallel English sentence scores to ob-
tain sentence re-ranking.
in Figure 6. By doing this, we simulated trans-
lating the user?s English query into Farsi with the
best possible query translation, before proceed-
ing with summarization. This approach to cross-
language summarization could be of interest for
CLIR systems that do query translation on-the-fly.
It is also of interest for summarization systems that
need to utilize previously translated source docu-
ments the capability is lacking to translate sum-
maries from L
2
to L
1
.
XII. Combinatory Query Biased Lexrank
(LQC) Another variation of LexRank that we
introduce in this work is LQC, which combines
LexRank scores from both languages to re-rank
sentences. A visual summary of this method is
shown in Figure 7. We accomplished our re-
ranking by first running LQ on Farsi and English
separately, then adding the two scores together.
This combination of Farsi and English scores pro-
vided us with a different way to score and rank
sentences, compared with LQ and LQP . The
idea behind combinatory query biased LexRank
is to take advantage of sentences which are high-
ranking in Farsi but not in English. The LQC
method exploits all available resources in our
dataset: L
1
and L
2
queries as well as L
1
and L
2
documents.
5 Experiments
We tested each of our summarization methods and
overall strategies in a task-based evaluation frame-
work using relevance prediction. We used Me-
chanical Turk for our experiments since it has been
shown to be useful for evaluating NLP systems
(Callison-Burch 2009; Gillick and Liu, 2010). We
obtained human judgments for whether or not a
document was considered relevant to a query, or
information need. We measured the relevance
663
judgements by precision/recall/F1, accuracy, and
also time-on-task based on the average response
time per Human Intelligence Task (HIT).
5.1 Mechanical Turk
In our Mechanical Turk experiment, we used ter-
minology from CLEF 2008 to describe a query
as an ?information need?. All of the Mechanical
Turk workers were presented with the following
for their individual HIT: instructions, an informa-
tion need and one summary for a document. Work-
ers were asked to indicate if the given summary
for a document was relevant to the given informa-
tion need (Hobson et al., 2007). Workers were
not shown the original Farsi source documents.
We paid workers $0.01 per HIT. We obtained 5
HITs for each information need and summary pair.
We used a built-in approval rate qualification pro-
vided by Mechanical Turk to restrict which work-
ers could work on our tasks. Each worker had an
approval rate of at least 95
Instructions: Each image below consists
of a statement summarizing the informa-
tion you are trying to find from a set
of documents followed by a summary
of one of the documents returned when
you query the documents. Based on the
summary, choose whether you think the
document returned is relevant to the in-
formation need. NOTE: It may be diffi-
cult to distinguish whether the document
is relevant as the text may be difficult
to understand. Just use your best judg-
ment.
6 Results and Analysis
We present our experiment results and additional
analysis. First, we report the results of our rel-
evance prediction task, showing performance for
individual summarization methods as well as per-
formance for the overall strategies. Then we
show analysis of our results from the monolin-
gual question-biased shared-task for DUC 2005,
as well as a comparison to previous work.
6.1 Results for Individual Methods
Our results are shown in Table 1. We report perfor-
mance for 13 individual methods as well as over-
all peformance on the 4 different summarization
strategies. To calculate the performance for each
strategy, we used the arithmetic mean of the corre-
sponding individual methods. We measured preci-
sion, recall and F1 to give us a sense of our sum-
maries might influence document retrieval in an
actual CLIR system. We also measured accuracy
and time-on-task. For these latter two metrics, we
distinguish between summaries that were relevant
(R) and non-relevant (NR).
All of the summarization-based methods fa-
vored recall over precision: documents were
marked ?relevant? more often than ?non-relevant?.
For many of the methods shown in Table 1, work-
ers spent more time correctly deciding ?relevant?
than correctly deciding ?non-relevant?. This sug-
gests some workers participated in our Mechanical
Turk task purposefully. For many of the summa-
rization methods, workers were able to positively
identify relevant documents.
From Table 1 we see that Full MT performed
better on precision than all of the other methods
and strategies, but we note that performance on
precision was generally very low. This might be
due to Mechanical Turk workers overgeneraliz-
ing by marking summaries as relevant when they
were not. Some individual methods preserve our
principle of retrieval-relevance, as indicated by
the higher recall scores for SQF, LQEF, and TFQ.
That is to say, these particular query biased sum-
marization methods can be used to assist users
with identifying more relevant documents. The ac-
curacy on relevant documents addresses our prin-
ciple of query-salience, and it is especially high
for our query-biased methods: LQEF, SQF, LQ,
and TFQ. The results also seem to fit our intuition
that the summary in Figure 3 seems less relevant
to the summaries shown in Figures 4 & 5 even
though these are the same documents biased on
the same query ?Tehran stock market?.
Overall, query biased word clouds outperform
the other summarization strategies for 5 out of
7 metrics. This could be due to the fact that
word clouds provide a very concise and overview
of a document, which is one of the main goals
for automatic summarization. Along these lines,
word clouds are probably not subject to the effects
of MT quality and we believe it is possible that
MT quality could have had a negative impact on
our query biased extracted sentence summaries, as
well as our full MT English texts.
664
Table 1: Individual method results: precision/recall/F1, time-on-task, and accuracy. Note that results for
time-on-task and accuracy scores are distinguished for relevant (R) and non-relevant (NR) documents.
Precision, Recall, F1 Time-on-Task Accuracy
Summarization Strategy Prec. Rec. F1 R NR R NR
Unbiased Full MT English 0.653 0.636 0.644 219.5 77.6 0.696 0.712
TF 0.615 0.777 0.686 33.5 34.6 0.840 0.508
IDF 0.537 0.470 0.501 84.7 45.8 0.444 0.700
TFIDF 0.647 0.710 0.677 33.2 38.2 0.772 0.656
Unbiased Word Clouds 0.599 0.652 0.621 50.5 39.5 0.685 0.621
TFQ 0.605 0.809 0.692 55.3 82.4 0.864 0.436
IDFQ 0.582 0.793 0.671 23.6 31.6 0.844 0.436
TFIDFQ 0.599 0.738 0.661 37.9 26.9 0.804 0.500
SFQ 0.591 0.813 0.685 55.7 49.4 0.876 0.504
W 0.611 0.738 0.669 28.2 28.9 0.840 0.564
Query Biased Word Clouds 0.597 0.778 0.675 36.4 34.2 0.846 0.488
REL 0.582 0.746 0.654 30.6 44.3 0.832 0.548
LQ 0.549 0.783 0.646 64.4 54.8 0.868 0.292
LQP 0.578 0.734 0.647 28.2 28.0 0.768 0.472
LQC 0.557 0.810 0.660 33.9 38.8 0.896 0.292
Query Biased Sentences 0.566 0.768 0.651 39.2 41.5 0.841 0.401
Table 2: Comparison of peer systems on DUC
2005 shared-task for monolingual question-biased
summarization, f-scores from ROUGE-2 and
ROUGE-SU4.
Peer ID ROUGE-2 ROUGE-SU4
17 0.07170 0.12970
8 0.06960 0.12790
4 0.06850 0.12770
Tel-Eng-Sum 0.06048 0.12058
LQ 0.05124 0.09343
REL 0.04914 0.09081
6.2 Analysis with DUC 2005
We analysed our summarization methods by
comparing two of our sentence-based methods
(LQ and REL) with peers from the monolin-
gual question-biased summarization shared-task
for DUC 2005. Even though DUC 2005 is a mono-
lingual task, we decided to use it as part of our
analysis for two reasons: (1) to see how well we
could do with query/question biasing while ignor-
ing the variables introduced by MT and cross-
language text, and (2) to make a comparison to
previous work. Pingali et al., (2007) also used this
the same DUC task to assess their cross-language
query biased summarization system. Systems
from the DUC 2005 question-biased summariza-
tion task were evaluated automatically against hu-
man gold-standard summaries using ROUGE (Lin
and Hovy, 2003) . Our results from the DUC
2005 shared-task are shown in Table 2, reported
as ROUGE-2 and ROUGE-SU4 f-scores, as these
two variations of ROUGE are the most helpful
(Dang, 2005; Pingali et al., 2007).
Table 2 shows scores for several top peer sys-
tems, as well as results for the Tel-Eng-Sum
method from Pingali et al., (2007). While we have
reported f-scores in our analysis, we also note that
our implementations of LQ and REL outperform
all of the DUC 2005 peer systems for precision, as
shown in Table 3. We also know that ROUGE can-
not be used for comparing sentence summaries to
ranked lists of words and there are no existing in-
trinsic methods to make that kind of comparison.
Therefore we were able to successfully compare
just 2 of our sentence-based methods to previous
work using ROUGE.
7 Discussion and Future Work
Cross-language query biased summarization is an
important part of CLIR, because it helps the user
decide which foreign-language documents they
might want to read. But, how do we know if
665
Table 3: Top 3 system precision scores for
ROUGE-2 and ROUGE-SU4.
Peer ID ROUGE-2 ROUGE-SU4
LQ 0.08272 0.15197
REL 0.0809 0.15049
15 0.07249 0.13129
a query biased summary is ?good enough? to be
used in a real-world CLIR system? We want to
be able to say that we can do query biased sum-
marization just as well for both monolingual and
cross-language IR systems. From previous work,
there has been some variability with regard to
when and what to translate - variables which have
no impact on monolingual summarization. We at-
tempted to address this issue with two of our meth-
ods: LQP and LQC. To fully exploit the MT vari-
able, we would need many more relevance pre-
diction experiments using humans who know L
1
and others who know L
2
. Unfortunately in our
case, we were not able to find Farsi speakers on
Mechanical Turk. Access to these speakers would
have allowed us to try further experiments as well
as other kinds of analysis.
Our results on the relevance prediction task
tell us that query biased summarization strategies
help users identify relevant documents faster and
with better accuracy than unbiased summaries.
Our findings support the findings of Tombros and
Sanderson (1998). Another important finding is
that now we can weigh tradeoffs so that different
summarization methods could be used to optimize
over different metrics. For example, if we want
to optimize for retrieval-relevance we might select
a summarization method that tends to have higher
recall, such as scaled query biased term frequency
(SFQ). Similarly, we could optimize over accu-
racy on relevant documents, and use Combinatory
LexRank (LQC) with Farsi and English together.
We have shown that the relevance prediction
tasks can be crowdsourced on Mechanical Turk
with reasonable results. The data we used from
the Farsi CLEF 2008 ad-hoc task included an an-
swer key, but there were no parallel English docu-
ments. However, in order for the NLP community
to make strides in evaluating cross-language query
biased summarization for CLIR, we will need star-
dards and data. Optimal data would be parallel
datasets consisting of documents in L
1
and L
2
with queries in L
1
and L
2
along with an answer
key specifying which documents are relevant to
the queries. Further we would also need sets of
human gold-standard query biased summaries in
L
1
and L
2
. These standards and data would al-
low us to compare method-to-method across dif-
ferent languages, while simultaneously allowing
us to tease apart other variables such as: when and
what to translate, translation quality, methods for
biasing, and type of summarization strategy (sen-
tences, words, etc). And of course it would be bet-
ter if this standard dataset was multilingual instead
of billingual, for obvious reasons.
We have approached cross-language query bi-
ased summarization as a stand-alone problem,
treating the CLIR system and document retrieval
as a black box. However, summaries need to pre-
serve query-salience: summaries should not make
it more difficult to positively identify relavant doc-
uments. And they should also preserve retrieval-
relevance: summaries should help users identify
as many relevant documents as possible.
Acknowledgments
We would like to express thanks to David Har-
wath at MIT Computer Science and Artificial In-
telligence Laboratory (CSAIL), who helped us de-
velop and implement ideas in this paper. We also
want to thank Terry Gleason from MIT Lincoln
Laboratory for providing machine translations.
References
Eneko Agirre, Giorgio Maria Di Nunzio, Nicola Ferro,
Thomas Mandl, and Carol Peters. CLEF 2008: Ad
hoc track overview. In Evaluating Systems for Mul-
tilingual and Multimodal Information Access, pp
15?37. Springer Berlin Heidelberg, 2009.
James Allan, Courtney Wade, and Alvaro Bolivar. Re-
trieval and Novelty Detection at the Sentence Level.
In Proceedings of the 26th Annual International
ACM SIGIR Conference on Research and Develop-
ment in Informaion Retrieval, (SIGIR ?03). ACM,
New York, NY, USA, 314-321.
Hosein Azarbonyad, Azadeh Shakery, and Heshaam
Faili. Exploiting Multiple Translation Resources for
English-Persian Cross Language Information Re-
trieval. In P. Forner, H. M?uller, R. Paredes, P. Rosso,
and B. Stein, editors, Information Access Evalua-
tion. Multilinguality, Multimodality, and Visualiza-
tion, volume 8138 of Lecture Notes in Computer Sci-
ence, pp 93?99. Springer Berlin Heidelberg, 2013.
Lorena Leal Bando, Falk Scholer, Andrew Turpin.
Constructing Query-biased Summaries: A Compar-
ison of Human and System Generated Snippets. In
666
Proceedings of the Third Symposium on Information
Interaction in Context (IIiX ?10), ACM 2010, New
York, NY, USA, 195-204.
Adam Berger and Vibhu O Mittal. Query-Relevant
Summarization Using FAQs. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, ACL 2000.
Pinaki Bhaskar and Sivaji Bandyopadhyay. Cross-
Lingual Query Dependent Snippet Generation. In-
ternational Journal of Computer Science and Infor-
mation Technology (IJCSIT), 3(4), 2012.
Pinaki Bhaskar and Sivaji Bandyopadhyay. Language
Independent Query Focused Snippet Generation. In
T. Catarci, P. Forner, D. Hiemstra, A. Pe?nas, and
G. Santucci, editors, Information Access Evaluation.
Multilinguality, Multimodality, and Visual Analytics,
volume 7488 of Lecture Notes in Computer Science,
pp 138?140. Springer Berlin Heidelberg, 2012.
Stephen P. Borgatti, Kathleen M. Carley, David Krack-
hardt. On the Robustness of Centrality Measures
Under Conditions of Imperfect Data. Social Net-
works, (28):124?136, 2006.
Florian Boudin, St?ephane Huet, and Juan-Manuel
Torres-Moreno. A Graph-Based Approach to Cross-
Language Multi-Document Summarization. Poli-
bits, (43):113?118, 2011.
Chris Callison-Burch. Fast, Cheap, and Creative: Eval-
uating Translation Quality Using Amazon?s Me-
chanical Turk. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pp 286?295, Singapore, ACL
2009.
Yllias Chali and Shafiq R. Joty. Unsupervised Ap-
proach for Selecting Sentences in Query-Based
Summarization. In Proceedings of the Twenty-First
International FLAIRS Conference, 2008.
Niladri Chatterjee, Amol Mittal, and Shubham Goyal.
Single Document Extractive Text Summarization
Using Genetic Algorithms. In Emerging Applica-
tions of Information Technology (EAIT), 2012 Third
International Conference, pp 19?23, 2012.
Kenneth W. Church and William A. Gale. Inverse Doc-
ument Frequency (IDF): A Measure of Deviations
From Poisson. In Natural language processing us-
ing very large corpora, pages 283?295. Springer,
1999.
Mathias Creutz and Krista Lagus. Unsupervised Mod-
els for Morpheme Segmentation and Morphology
Learning. ACM Transactions on Speech and Lan-
guage Processing, 4(1):3:1?3:34, February 2007.
Hoa Trang Dang. Overview of DUC 2005. In Pro-
ceedings of the Document Understanding Confer-
ence, 2005.
Hal Daum?e III, Daniel Marcu. Bayesian Query-
Focused Summarization. In Proceedings of the 21st
International Conference on Computational Lin-
guistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, ACL 2006.
Jennifer Drexler, Wade Shen, Terry P. Gleason, Timo-
thy R. Anderson, Raymond E. Slyh, Brian M. Ore,
and Eric G. Hansen. The MIT-LL/AFRL IWSLT-
2012 MT System. In Proceedings of the Interna-
tional Workshop on Spoken Language Translation
(IWSLT), Hong Kong, December 2012.
Bonnie J. Dorr, Christof Monz, Stacy President,
Richard Schwartz, and David Zajic. A Methodol-
ogy for Extrinsic Evaluation of Text Summarization:
Does ROUGE Correlate? In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for Machine Translation and/or Sum-
marization, pp 1-8. Ann Arbor, ACL 2005.
H. P. Edmundson. New Methods in Automatic Extract-
ing. In Journal of the ACM, 16(2):264?285, April
1969.
G?unes? Erkan and Dragomir R. Radev. Lexrank:
Graph-based lexical centrality as salience in text
summarization. Journal of Artificial Intelligence
Research, 22(1):457?479, December 2004.
Dan Gillick and Yang Liu. Non-Expert Evaluation
of Summarization Systems is Risky. In Proceed-
ings of NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk, pp 148-151, Los Angeles, California,
USA, June, 2010.
David Harwath and Timothy J. Hazen. Topic Identi-
fication Based Extrinsic Evaluation of Summariza-
tion Techniques Applied to Conversational Speech.
In Proceedings of ICASSP, 2012: 5073-5076.
Stacy P. Hobson, Bonnie J. Dorr, Christof Monz, and
Richard Schwartz. Task-Eased Evaluation of Text
Summarization Using Relevance Prediction. In In-
formation Processing Management, 43(6): 1482-
1499, 2007.
Hongyan Jing, Regina Barzilay, Kathleen McKeown,
and Michael Elhadad. Summarization Evaluation
Methods: Experiments and Analysis. In Proceed-
ings of American Association for Artificial Ingelli-
gence (AAAI), 1998.
Reza Karimpour, Amineh Ghorbani, Azadeh Pishdad,
Mitra Mohtarami, Abolfazl AleAhmad, Hadi Amiri,
and Farhad Oroumchian. Improving Persian Infor-
mation Retrieval Systems Using Stemming and Part
of Speech Tagging. In Proceedings of the 9th Cross-
language Evaluation Forum Conference on Evaluat-
ing Systems for Multilingual and Multimodal Infor-
mation Access, CLEF 2008, pp 89?96, Berlin, Hei-
delberg, 2009. Springer-Verlag.
667
Chin-Yew Lin. Looking For A Few Good Metrics:
Automatic Summarization Evaluation - How Many
Samples Are Enough? In Proceedings of NTCIR
Workshop 4, Tokyo, Japan, June 2004.
Annie Louis and Ani Nenkova. Automatic Summary
Evaluation without Human Models. In Proceedings
of Empirical Methods in Natural Language Process-
ing, EMNLP 2009.
H. P. Luhn. The Automatic Creation of Literature Ab-
stracts. IBM Journal of Research and Development,
2(2):159?165, April 1958.
Inderjeet Mani, Eric Bloedorn, and Barbara Gates. Us-
ing Cohesion and Coherence Models for Text Sum-
marization. In AAAI Symposium Technical Report
SS-989-06, AAAI Press, 69?76, 1998.
Inderjeet Mani, David House, Gary Klein, Lynette
Hirschman, Therese Firmin, and Beth Sundheim.
The TIPSTER SUMMAC Text Summarization
Evaluation. In Proceedings of European Associa-
tion for Coputational Linguistics, EACL 1999.
Inderjeet Mani. Summarization Evaluation: An
Overview. In Proceedings of the NTCIR Workshop,
Vol. 2, 2001.
Inderjeet Mani, Gary Klein, David House, Lynette
Hirschman, Therese Firmin, and Beth Sundheim.
SUMMAC: A Text Summarization Evaluation. Nat-
ural Language Engineering, 8(1) 43-68. March
2002.
Kathleen McKeown, Rebecca J. Passonneau, David K.
Elson, Ani Nenkova, and Julia Hirschberg. Do Sum-
maries Help? A Task-Based Evaluation of Multi-
Document Summarization. In Proceedings of the
28th Annual International ACM SIGIR Conference
on Research and Development in Information Re-
trieval, pp 210-217. ACM 2005.
Anthony McCallum, Gerald Penn, Cosmin Munteanu,
and Xiaodan Zhu. Ecological Validity and the Eval-
uation of Speech Summarization Quality. In Pro-
ceedings of Workshop on Evaluation Metrics and
System Comparison for Automatic Summarization.
2012 Association for Computational Linguistics,
Stroudsburg, PA, USA, 28-35.
Tatsunori Mori, Masanori Nozawa, and Yoshiaki
Asada. Multi-Answer Focused Multi-Document
Summarization Using a Question-Answering En-
gine. In Proceedings of the 20th International
Conference on Computational Linguistics, COLING
?04, Stroudsburg, PA, USA, ACL 2004.
Gabriel Murray, Thomas Kleinbauer, Peter Poller,
Tilman Becker, Steve Renals, and Jonathan Kilgour.
Extrinsic Summarization Evaluation: A Decision
Audit Task. ACM Transactions on Speech and Lan-
guage Processing, 6(2) Article 2, October 2009.
Ani Nenkova and Kathleen McKeown. A Survey of
Text Summarization Techniques. In C. C. Aggarwal
and C. Zhai, editors, Mining Text Data, pp 43?76.
Springer US, 2012.
Constantin Or?asan and Oana Andreea Chiorean. Eval-
uation of a Cross-Lingual Romanian-English Multi-
Document Summariser. In Proceedings of Lan-
guage Resources and Evaluation Conference, LREC
2008.
Jahna Otterbacher, G?unes? Erkan, and Dragomir R
Ravev. Using Random Walks for Question-focused
Sentence Retrieval. In Proceedings of Human Lan-
guage Technology Conference on Empirical Meth-
ods in Natural Language Processing, Vancouver,
Canada, pp 915-922, EMNLP 2005.
Jahna Otterbacher, G?unes? Erkan, and Dragomir R.
Ravev. Biased LexRank: Passage Retrieval Using
Random Walks With Question-Based Priors. In In-
formation Processing Management, 45(1), January
2009, pp 42-54.
Karolina Owczarzak, John M. Conroy, Hoa Trang
Dang, and Ani Nenkova. An Assessment of the Ac-
curacy of Automatic Evaluation in Summarization.
In Proceedings of the Workshop on Evaluation Met-
rics and System Comparison for Automatic Summa-
rization, pp 1-9, Montr?eal, Canada, ACL 2012.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. The Pagerank Citation Ranking:
Bringing Order to the Web. Technical report, Stan-
ford Digital Library Technologies Project, 1998.
Prasad Pingali, Jagadeesh Jagarlamudi, and Vasudeva
Varma. Experiments in Cross Language Query Fo-
cused Multi-Document Summarization In Work-
shop on Cross Lingual Information Access Address-
ing the Information Need of Multilingual Societies,
IJCAI 2007.
Stacy F. President and Bonnie J. Dorr. Text Sum-
marization Evaluation: Correlating Human Perfor-
mance on an Extrinsic Task With Automatic In-
trinsic Metrics. No. LAMP-TR-133. University of
Maryland College Park Language and Media Pro-
cessing Laboratory Institute for Advanced Computer
Studies (UMIACS), 2006.
Dragomir R. Radev, Hongyan Jing, Malgorzata Sty?s,
and Daniel Tam. Centroid-Based Summarization
of Multiple Documents. InProceedings of In-
formaion Processing Management, 40(6):919?938,
Nov. 2004.
Stephen Robertson. Understanding Inverse Docu-
ment Frequency: on Theoretical Arguments for IDF.
Journal of Documentation, 60(5):503?520, 2004.
Stephen E. Robertson and Steve Walker. Some Simple
Effective Approximations to the 2-Poisson Model
for Probabilistic Weighted Retrieval. In Proceed-
ings of the 17th annual international ACM SIGIR
668
conference on Research and development in infor-
mation retrieval, pp 232?241. Springer-Verlag New
York, Inc., 1994.
Gerard Salton and Chung-Shu Yang. On the Specifica-
tion of Term Values in Automatic Indexing. Journal
of Documentation, 29(4):351?372, 1973.
Anastasios Tombros and Mark Sanderson. Advantages
of Query Biased Summaries in Information Re-
trieval. In Proceedings of the 21st annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pp 2?10. ACM,
1998.
Xiaojun Wan and Jianguo Xiao. Graph-Based
Multi-Modality Learning for Topic-Focused Multi-
Document Summarization. In Proceedings of the
21st international jont conference on Artifical intel-
ligence (IJCAI?09), San Francisco, CA, USA, 1586-
1591.
Xiaojun Wan, Huiying Li, and Jianguo Xiao. Cross-
Language Document Summarization Based on Ma-
chine Translation Quality Prediction. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL ?10). Associ-
ation for Computational Linguistics, Stroudsburg,
PA, USA, 917-926.
Xiaojun Wan, Houping Jia, Shanshan Huang, and Jian-
guo Xiao. Summarizing the Differences in Multilin-
gual News. In Proceedings of the 34th International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, SIGIR ?11, pp 735?
744, New York, NY, USA, 2011. ACM.
Wenpeng Yin, Yulong Pei, Fan Zhang, and Lian?en
Huang. SentTopic-MultiRank: A Novel Ranking
Model for Multi-Document Summarization. In Pro-
ceedings of COLING, pages 2977?2992, 2012.
Junlin Zhang, Le Sun, and Jinming Min. Using
the Web Corpus to Translate the Queries in Cross-
Lingual Information Retrieval. In Proceedings in
2005 IEEE International Conference on Natural
Language Processing and Knowledge Engineering,
2005, IEEE NLP-KE ?05, pp 493?498, 2005.
669
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 223?227,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Extracting and modeling durations for habits and events from Twitter
Jennifer Williams Graham Katz
Department of Linguistics Department of Linguistics
Georgetown University Georgetown University
Washington, D.C., USA Washington, D.C., USA
jaw97@georgetown.edu egk7@georgetown.edu
Abstract
We seek to automatically estimate typical 
durations for  events  and  habits  described 
in Twitter tweets.  A corpus of more than 
14 million tweets containing temporal du-
ration  information  was  collected.  These 
tweets were classified as to their habituality 
status  using a bootstrapped, decision tree. 
For each verb lemma,  associated duration 
information was collected for episodic and 
habitual uses of the verb. Summary statis-
tics for  483 verb lemmas and their typical 
habit and episode durations has been com-
piled and made available.  This automati-
cally  generated  duration  information  is 
broadly comparable to hand-annotation.
1 Introduction
Implicit  information  about  temporal  durations  is 
crucial to any natural language processing task in-
volving  temporal  understanding  and  reasoning. 
This  information  comes  in  many  forms,  among 
them knowledge about typical durations for events 
and  knowledge  about  typical  times  at  which  an 
event occurs. We know that lunch lasts for half an 
hour  to  an  hour  and takes  place  around noon,  a 
game of chess lasts from a few minutes to a few 
hours and can occur any time, and so when we in-
terpret a text such as ?After they ate lunch, they 
played a game of chess and then went to the zoo? 
we can infer that the zoo visit probably took place 
in the early afternoon. In this paper we focus on 
duration. Hand-annotation of event durations is ex-
pensive slow (Pan et al, 2011), so it is desirable to 
automatically determine typical durations. This pa-
per describes a method for automatically extracting 
information about typical durations for events from 
tweets posted to the Twitter microblogging site.
Twitter is a rich resource for information about 
everyday events ? people post their tweets to Twit-
ter publicly in real-time as they conduct their activ-
ities throughout the day, resulting in a significant 
amount  of  mundane  information  about  common 
events. For example, (1) and (2) were used to pro-
vide information about how long a work event can 
last:
(1) Had  work for an hour and 30 mins now 
going to disneyland with my cousins :)
(2) I play in a loud rock band, I  worked at a 
night  club for  two years.  My ears  have  
never  hurt  so  much  @melaniemarnie  
@giorossi88 @CharlieHi11 
In this paper, we sought to use this kind informa-
tion to  determine likely durations  for  events  and 
habits  of  a  variety  of  verbs.  This  involved  two 
steps: extracting a wide range of tweets such as (1) 
and (2) and classifying these as to whether they re-
ferred to specific event (as in (1)) or a general habit 
(as in (2)), then summarizing the duration informa-
tion associated with each kind of use of a given 
verb.
This paper answers two investigative questions:
? How  well  can  we  automatically  extract 
fine-grain  duration information  for events 
and habits from Twitter?
? Can we effectively distinguish episode and 
habit duration distributions ?
The results presented here show that Twitter can be 
mined  for  fine-grain  event  duration  information 
223
with high precision using regular expressions. Ad-
ditionally, verb uses can be effectively categorized 
as  to  their  habituality,  and  duration  information 
plays an important role in this categorization. 
2 Prior Work
Past research on typical durations has made use of 
standard  corpora  with  texts  from  literature  ex-
cerpts, news stories, and full-length weblogs (Pan 
et al 2006;  2007;  2011; Kozareva & Hovy, 2011; 
Gusev et al, 2011). For example, Pan et al (2011) 
hand-annotated of  a  portion  of  the  TIMEBANK 
corpus that consisted of Wall Street  Journal  arti-
cles. For 58 non-financial articles, they annotated 
over 2,200 events with typical temporal duration, 
specifying the upper and lower bounds for the du-
ration of  each event.  In  addition they  used their 
corpus to automatically determine event durations 
with machine learning,  predicting features  of  the 
duration on the basis of the verb lemma, local tex-
tual  context.  and  other  information.  Their best 
(SVM) classifier  achieved  precision of 78.2% on 
the course-grained task of determining whether an 
event's duration was longer or shorter than one day 
(compared with 87.7% human agreement). For de-
termining the fine-grained task of determining the 
most  likely  temporal  unit?second,  minute,  hour, 
day,  week,  etc.?achieved  67.9%  (human  agree-
ment: 79.8%). This shows that lexical information 
can be effectively  leveraged for  duration predic-
tion.
To compile temporal duration information for a 
wider range of verbs, Gusev et al (2011) explored 
an automatic Web-based query method for harvest-
ing typical durations of events. Their data consist-
ed of search engine ?hit-counts? and they analyzed 
the distribution of durations associated with  1000 
frequent verbs in terms of whether the  event lasts 
for more or less than a day (course-grain task) or 
whether it lasts for seconds, minutes, hours, days, 
weeks,  months,  or  years  (fine-grain  task).  They 
note that many verbs have a two-peaked distribu-
tion and they suggest that the two-peaked distribu-
tion could be a result  of the usage referring to a 
habit or a single episode. (When used with a dura-
tion marker,  run,  for example, is used about 15% 
of the time with hour-scale and 38% with year-s-
cale duration markers). Rather than making a dis-
tinction between habits and episodes in their data, 
they apply a heuristic to focus on episodes only. 
Kozareva and Hovy (2011) also collected typi-
cal durations of events using Web query patterns. 
They proposed a six-way classification of ways in 
which events are related to time, but provided only 
programmatic analyses of a few verbs using We-
b-based  query  patterns.  They  have  proposed  a 
compilation  of  the  5,000  most  common  verbs 
along with their typical temporal durations. In each 
of  these  efforts,  automatically  collecting  a  large 
amount of reliable to cover a wide range of verbs 
has been noted as a difficulty. It is this task that we 
seek to take up.
3 Corpus Methodology
Our goal was to discover the duration distribution 
as well as typical habit and typical episode dura-
tions for each verb lemma that we found in our col-
lection.  A wide range of factors influence typical 
event durations. Among these are the character of a 
verb's arguments, the presence of negation and oth-
er embedding features. For this preliminary work, 
we ignored the effects of arguments, and focused 
only on generating duration information for verb 
lemmas. Also, tweets that were negated, condition-
al tweets, and tweets in the future tense  were put 
aside.
3.1 Data Collection
A corpus of tweets was collected from the Twitter 
web  service  API  using  an  open-source module 
called  Tweetstream  (Halvorsen  &  Schierkolk, 
2010). Tweets were collected that contained refer-
ence to  a  temporal  duration.  The data  collection 
task began on February 1, 2011 and ended on Sep-
tember 28, 2011.  Duplicate tweets were identified 
by their unique tweet ID provided by Twitter, and 
were  removed from the data set. Also tweets that 
were marked by Twitter as 'retweets' (tweets that 
have been reposted to Twitter) were removed. The 
following query terms (denoting temporal duration 
measure) were used to filter the Twitter stream for 
tweets containing temporal duration:
second,  seconds,  minute,  minutes,  hour,  
hours,  day,  days,  week,  weeks,  month,  
months, year, years, decade, decades, cen-
tury,  centuries,  sec,  secs,  min,  mins,  hr,  
hrs, wk, wks, yr, yrs
The number of tweets in  the  resulting  corpus was 
14,801,607 and the total number of words in the 
224
corpus was 224,623,447. Tweets were normalized, 
tokenized,  and  then  tagged  for  POS,  using  the 
NLTK Treebank Tagger (Bird & Loper, 2004). 
3.2 Extraction Frames
To associate each temporal duration with its event, 
events and durations were identified and extracted 
using  four  types  of  regular  expression  extraction 
frames.  The  patterns  applied  a heuristic  to  asso-
ciate each verb with a temporal expression, similar 
to  the  extraction  frames used  in  Gusev  et  al. 
(2011). The four types of extraction frames were:
? verb for duration
? verb in duration
? spend duration verbing 
? takes duration to verb 
where verb is the target verb and duration is a du-
ration-measure term. In (3), for example,  the verb 
work is associated with the temporal duration term 
44 years.
(3) Retired watchmaker worked for 44 years 
without a telephone, to avoid unnecessary  
interruptions, http://t.co/ox3mB6g
These four extraction frame types were also varied 
to  include different  tenses,  different  grammatical 
aspects,  and  optional  verb  arguments to  reach  a 
wide  range  of  event  mentions  and  ordering  be-
tween the verb and the duration clause. For each 
matched tweet  a  feature  vector  was created with 
the  following  features:  verb  lemma,  temporal 
bucket  (seconds,  minutes,  hours,  weeks,  days, 
months or years), tense (past or present), grammat-
ical aspect (simple, progressive, or perfect), dura-
tion in seconds, and the extraction frame type (for, 
in, spend, or take). For example, the features ex-
tracted from (3) were: 
[work, years, past, simple, 1387584000, FOR]
Tweets with verbal lemmas that occur fewer than 
100 times in the extracted corpus were filtered out. 
The  resulting  data  set contained  390,562 feature 
vectors covering 483 verb lemmas.
3.3 Extraction Precision
Extraction frame performance was estimated using 
precision on a random sample of 400 hand-labeled 
tweets. Each instance in the sample was labeled as 
correct if the extracted feature vector was correct 
in its entirety. The overall precision for extraction 
frames was estimated as 90.25%, calculated using 
a  two-tailed t-test  for  sample size  of  proportions 
with 95% confidence (p=0.05, n=400). 
3.4 Duration Results 
In order to summarize information about dura-
tion for each of the 483 verb lemmas, we calculat-
ed the frequency distribution of tweets by duration 
in seconds. This distribution can be represented in 
histogram form, as in Figure 1 for the verb lemma 
search,  with with bins corresponding to temporal 
units of measure (seconds, minutes, etc.). 
Figure 1: Frequency distribution for search 
This  histogram  shows the  characteristic  bi-
modal-distributions noted  by Pan et al, (2011) and 
Gusev et. al., (2011), an issue taken up in the next 
section.
4 Episodic/Habitual Classification 
Most verbs have both episodic and habitual uses, 
which clearly correspond to different typical dura-
tions. In order to draw this distinction we built a 
system to automatically classify our tweets  as to 
their  habituality.  The  extracted  feature  vectors 
were used in a machine learning task to label each 
tweet  in the collection as denoting a habit  or  an 
episode, broadly following Mathew & Katz (2009). 
This classification was done with bootstrapping, in 
a partially supervised manner.
4.1 Bootstrapping Classifier
First, a random sample of 1000 tweets from the ex-
tracted  corpus  was  hand-labeled  as  being  either 
225
habit  or  episode (236 habits;  764 episodes).  The 
extracted  feature  vectors  for  these  tweets  were 
used to train a C4.5 decision tree classifier (Hall et 
al., 2009). This classifier achieved an accuracy of 
83.6% during training. We used this classifier and 
the hand-labeled set to seed the generic Yarowsky 
Algorithm  (Abney,  2004), iteratively  inducing  a 
habit or episode label for all the tweets in the col-
lection,  using  the  WEKA output  for  confidence 
scoring and a confidence threshold of 0.96. 
The extracted corpus was classified into 94,643 
habitual tweets and  295,918 episodic tweets.  To 
estimate  the  accuracy  of  the  classifier,  400  ran-
domly  chosen  tweets  from  the  extracted  corpus 
were hand-labeled, giving an estimated accuracy of 
85% accuracy with 95% confidence, using the two-
tailed t-test for sample size of proportions (p=0.05, 
n=400).
4.2 Results 
Clearly the data in Figure 1 represents two com-
bined distributions: one for episodes and one for 
habits, as we illustrate in Figure 2. We see that the 
verb search describes episodes that most often last 
minutes or hours, while it describes habits that go 
on for years. 
Figure 2: Duration distribution for search
These two different uses are illustrated in (4) and 
(5). 
(4) Obviously I'm the one who found the tiny  
lost black Lego in 30 seconds after the 3 of  
them searched for 5 minutes. 
(5) @jaynecheeseman they've been searching  
for you for 11 years now. I'd look out if I  
were you.
In Table  1  we provide  summary information for 
several verb  lemmas, indicating the average dura-
tion  for  each  verb  and  the  temporal  unit  corre-
sponding to the largest bin for each verb.
Verb 
 Episodic Use  Habitual Use
Modal 
bin Mean
Modal 
bin Mean
snooze minutes 1.6 hrs decades 7.5 yrs
coach hours 10 days years 8.5 yrs
approve minutes 1.7 mon. years 1.4 yrs
eat minutes 5.3 wks days 5.7 yrs
kiss seconds 4.5 days weeks 1.8 yrs
visit weeks 7.2 wks. years 4.9 yrs
Table 1. Mean duration and mode for 6 of the verbs 
It is clear that the methodology  overestimates the 
duration of episodes somewhat ?  our estimates of 
typical durations are 2-3 times as long as those that 
come from the annotation in  Pan,  et.  al.  (2009). 
Nevertheless, the modal bin corresponds approxi-
mately to that the hand annotation in Pan, et. al., 
(2011) for nearly half (45%) of the verbs lemmas.
5 Conclusion
We have presented a hybrid approach for extract-
ing typical  durations  of  habits  and episodes.  We 
are able to extract high-quality information about 
temporal  durations  and  to  effectively  classify 
tweets as to their habituality. It is clear that Twitter 
tweets contain a lot of unique data about different 
kinds of events and habits, and mining this data for 
temporal duration information has turned out to be 
a fruitful avenue for collecting the kind of world-
knowledge that we need for robust temporal lan-
guage processing. Our verb lexicon is available at: 
https://sites.google.com/site/relinguistics/.
226
References 
Steven Abney. 2004. ?Understanding the Yarowsky Al-
gorithm?. Computational Linguistics 30(3): 365-395.
Steven Bird and Edward Loper. 2004. NLTK: The natu-
ral language toolkit.  In Proceedings of 42nd Annual  
Meeting  of  the  Association for  Computational  Lin-
guistics (ACL-04).
Andrey  Gusev,  Nathaniel  Chambers,  Pranav  Khaitan, 
Divye Khilnani, Steven Bethard, and Dan Jurafsky. 
2011. ?Using query patterns to learn the durations of 
events?. IEEE IWCS-2011, 9th International Confer-
ence on Web Services. Oxford, UK 2011.
Mark  Hall,  Eibe  Frank,  Geoffrey  Holmes,  Bernhard 
Pfahringer,  Peter  Reutemann,  and  Ian  H.  Witten. 
2009.  The WEKA  Data  Mining Software:  An Up-
date; SIGKDD Explorations, Volume 11, Issue 1.
Rune  Halvorsen,  and Christopher  Schierkolk.  2010. 
Tweetstream:  Simple  Twitter  Streaming  API  (Ver-
sion  0.3.5)  [Software].  Available  from  https://bit-
bucket.org/runeh/tweetstream/src/.
Jerry Hobbs and James Pustejovsky. 2003. ?Annotating 
and reasoning about time and events?.  In Proceed-
ings of the AAAI Spring Symposium on Logical For-
mulation of Commonsense Reasoning. Stanford Uni-
versity, CA 2003.
Zornitsa  Kozareva  and Eduard  Hovy. 2011. ?Learning 
Temporal  Information  for  States  and  Events?.  In 
Proceedings of  the Workshop on Semantic Annota-
tion for Computational Linguistic Resources (ICSC  
2011), Stanford.
Thomas  Mathew and  Graham  Katz. 2009. ?Supervised 
Categorization of Habitual and Episodic Sentences?. 
Sixth  Midwest  Computational  Linguistics  Colloqui-
um. Bloomington, Indiana: Indiana University. 
Marc Moens and Mark Steedman. 1988. ?Temporal On-
tology  and  Temporal  Reference?.  Computational  
Linguistics 14(2):15-28.
Feng  Pan,  Rutu  Mulkar-Mehta,  and  Jerry  R.  Hobbs. 
2006. ?An Annotated Corpus of Typical Durations of 
Events?.  In Proceedings  of  the  Fifth  International  
Conference on Language Resources and Evaluation 
(LREC), 77-82. Genoa, Italy.
Feng  Pan,  Rutu  Mulkar-Mehta,  and  Jerry  R.  Hobbs. 
2011. "Annotating and Learning Event Durations in 
Text." Computational Linguistics 37(4):727-752.
227
Proceedings of the 2012 Student Research Workshop, pages 49?54,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Extracting fine-grained durations for verbs from Twitter
Jennifer Williams
Department of Linguistics
Georgetown University
Washington, DC USA
jaw97@georgetown.edu
Abstract
This paper presents recent work on a  new 
method  to  automatically  extract fine-
grained duration  information for  common 
verbs using  a  large  corpus  of  Twitter 
tweets.  Regular  expressions were  used to 
extract verbs and durations from each tweet 
in a corpus of more than 14 million tweets 
with 90.38% precision  covering 486 verb 
lemmas. Descriptive statistics for each verb 
lemma  were  found  as  well  as  the  most 
typical  fine-grained  duration  measure. 
Mean  durations  were compared  with 
previous work by Gusev et al (2011) and it 
was  found  that  there  is  a  small  positive 
correlation.
1 Introduction
Implicit information about events is crucial to any 
natural  language  processing  task  involving 
temporal  understanding  and  reasoning.  This 
information  comes  in  many  forms,  among  them 
knowledge about typical durations for events and 
knowledge about typical times at which an event 
occurs.  We know that  lunch lasts  for perhaps an 
hour and takes place around noon, and so when we 
interpret a text such as ?After they ate lunch, they 
played a game of chess and then went to the zoo? 
we can infer that the  chess game probably  lasted 
for a few hours and not for several months. 
This  paper  describes  a  new  method for 
extracting information about typical durations for 
verbs from  tweets  posted  to  the  Twitter 
microblogging site.  Twitter is a rich resource for 
information about  everyday events ? people post 
their  'tweets'  to  Twitter  publicly  in  real-time  as 
they  conduct  their  activities  throughout  the  day, 
resulting  in  a  significant  amount  of  information 
about common events.  Data from Twitter is more 
diverse  than the data  found in news articles  that 
has  typically  been  used  for  looking  at  event 
durations (Pan et al, 2011). For example, consider 
that (1) was used find out that working can last for 
an hour and a half:
(1) Had work for an hour and 30 mins now 
going to disneyland with my cousins :)
I extracted and analyzed a large number of such 
tweets containing temporal duration  information. 
This  involved  identifying  relevant  tweets, 
extracting  the  temporal  phrases,  and  associating 
these with the verb they modified. The processes 
are  described  below.  Two  objectives were 
investigated in this paper: (1) how to automatically 
extract  duration  information  for  common  verbs 
from  Twitter,  and  (2)  to  discover  the  duration 
distributions for common  verbs. A wide range of 
factors  influence  typical  durations.  Among  these 
are  the  character  of  a  verb's  arguments,  the 
presence  of  negation  and  other  embedding 
features. For example,  eating a snack is different 
from  eating  a  meal since  these  events  have 
different durations. To simplify the task, I set aside 
tweets  wherein the  sentence-level verb  was 
negated,  or  in  the  conditional  or  future  tenses. 
Examining  the effect of verb arguments  was also 
set aside in this work.
49
The  problem  of  finding  typical  duration  for 
events can be viewed as a coarse-grained task or a 
fine-grained  task.  At  the  coarse-grained  level  it 
could be determined whether or not a chess game 
lasts for more or less than one day, whereas a fine- 
grained analysis would indicate that a chess game 
lasts for minutes or hours.
The results of this  work show that Twitter can 
be  mined  for  duration  information  with  high 
accuracy using regular expressions. Likewise, the 
typical durations for verbs can be summarized in 
terms  of  the  most  frequent  duration-measure 
(seconds,  minutes,  hours,  days,  weeks,  months, 
years, decades) as well as by descriptive statistics. 
2 Prior Work
Past research on typical durations has made use of 
standard  corpora  with  texts  from  literature 
excerpts,  news  stories,  and  full-length  weblogs 
(Pan et al, 2011; Kozareva & Hovy, 2011; Gusev 
et al, 2011). However, data from Twitter has been 
useful  for  other  NLP  tasks  such  as  detecting 
sarcasm (Gonz?lez-Ib??ez et al, 2011), as well as 
sentiment  for  Twitter  events  (Thelwall  et  al., 
2011).  The present  work used data  from Twitter 
because  it  is  readily  available  and diverse  in  its 
linguistic nature.
2.1 Hand-Annotation
The first to examine typical durations of events 
was Pan et al (2011).  They  describe a  method to 
annotate events  with  duration  information.  They 
hand-annotated  a  portion  of  the  TIMEBANK 
corpus  that  consisted  of  news  articles  and  non-
financial  articles  from  the  Wall  Street  Journal. 
They  did  this  for  48  news  articles  (for  2,132 
events) and 10 Wall Street Journal articles (for 156 
events). For each event, three annotators indicated 
a  lower-bound  duration  and  an  upper-bound 
duration  that  would  cover  80%  of  the  possible 
cases  provided  that  durations  are  normally 
distributed.  They converted  the  upper  and  lower 
bounds into distributions.  They defined annotator 
agreement  to  be the  average  overlap  of  all  the 
pairwise  overlapping  areas,  calculated  using  the 
kappa statistic. 
In their experiments, Pan et al (2011) examined 
their  annotation  guidelines  and  found  that 
annotator  agreement  was  significantly  improved 
after  annotators  were  instructed  to  use  their 
guidelines.  These  guidelines  took  into 
consideration information about event classes. The 
final  guidelines addressed the following kinds of 
classes:  actions  vs.  states,  aspectual  events, 
reporting events (quoted and unquoted reporting), 
multiple  events,  events  involving  negation, 
appearance  events,  and  positive  infinitive 
duration1.  Human  agreement  for  coarse-grained 
analysis  was  reported  to  be  87.7%  whereas 
agreement for fine-grained analysis was 79.8%. 
Hand-annotation  is  an expensive way  of 
acquiring  typical  duration  and  human  annotators 
do not always agree on how long events last. This 
paper presents a  way  to  extract  duration 
information  automatically  and  at  a  fine-grained 
scale  to  discover  the  kinds  of  distributions  of 
durations for different verbs as well as their typical 
durations. 
2.2 Web Extraction
To compile temporal duration information for a 
wider range of verbs, Gusev et al (2011) explored 
a Web-based query method for harvesting typical 
durations of events. They used five different kinds 
of  query  frames to  extract  events  and  their 
durations from the web  at  a coarse-grained level 
and  at  a  fine-grained  level.  They  compiled  a 
lexicon  of  10,000  events  and  their  duration 
distributions. 
In  the  work of  Gusev  et  al.  (2011),  they 
calculated the most likely duration for events at a 
fine-grained  scale.  To  obtain  each  of  the fine-
grained  duration  distributions,  they  first  binned 
durations into  their  temporal  unit  measures 
(seconds,  minutes,  hours,  etc.).  Next,  they 
discarded  data  that  was  extracted  using  patterns 
that  had  very  low ?hit-counts?  in  their  effort  to 
judge  the  reliability  of  their  extraction  frames. 
Finally, they normalized the distributions based on 
how  often  each pattern  occurs  in  general.  They 
note  that  many  verbs  have  a  two-peaked 
distribution.  When  used  with  a  duration  marker, 
run,  for example, is used about 15% of the time 
with hour-scale and 38% with year-scale duration 
markers. In the case of the event say, Gusev et al 
(2011)  chose  to  normalize  their  duration 
distributions  with a  heuristic  to  account  for  the 
possibility that all of the year-scale durations could 
1 Positive infinitive durations describe states that will last 
forever once they begin, such as being dead.
50
be attributed to the common phrase ?... for years?.
Kozareva  and  Hovy  (2011)  also  collected 
typical  durations  of  events  using  Web  query 
patterns. They proposed a six-way classification of 
ways  in  which  events  are  related  to  time,  but 
provided  only  programmatic  analyses  of  a  few 
verbs using Web-based query patterns. They have 
asked for a compilation of the 5,000 most common 
verbs along with their typical temporal durations. 
In each of these efforts,  automatically collecting a 
large amount of reliable data which covers a wide 
range of verbs has been noted as a difficulty. 
3 Methodology
3.1 Data Collection
For the present study, tweets were collected from 
the  Twitter web service API using an open-source 
Python  module called Tweetstream (Halvorsen & 
Schierkolk,  2010)2.  Specifically,  tweets  were 
collected  that  contained  reference  to  a  temporal 
duration.  The data  collection  task  began  on 
February  1,  2011  and  ended   on  September  28, 
2011. The total number of tweets in  the collected 
corpus  was 14,801,607  and  the  total  number  of 
words in the corpus was 224,623,447. 
 The following query terms (denoting temporal 
duration  measure)  were  used  to  extract  tweets 
containing  temporal  duration  from  the  Twitter 
stream:
second, seconds, minute, minutes, hour, hours,
day, days, week, weeks, month, months, year,
years, decade, decades, century, centuries, sec,
secs, min, mins, hr, hrs, wk, wks, yr, yrs
Tweets  were  normalized,  tokenized,  and  then 
tagged for POS, using the NLTK Treebank Tagger 
(Bird  &  Loper,  2004).  Each  tweet  came  with  a 
unique  tweet ID  number  provided by Twitter  and 
this ID was used to inform  whether or not  there 
were  duplicate  entries in  the dataset,  and  all 
duplicate entries were removed. The twitter stream 
was also filtered so that it did not include re-tweets 
(tweets that have been reposted to Twitter).
3.2 Extraction Frames
To associate a temporal duration with each  verb, 
the  verbs and  durations  were  matched  and 
2 This Python module is available open-source at: 
https://bitbucket.org/runeh/tweetstream/src/
extracted  using  four  types  of  regular  expression 
extraction frames. The patterns applied a heuristic 
to associate each verb with a temporal expression, 
similar to the extraction frames used  by Gusev et 
al.  (2011).  Unlike  Gusev  et  al.  (2011)  four 
different  extraction  frames  were  used (for,  in,  
spend, and  take)  with varied tense and aspect on 
each frame,  in an effort to widen the coverage of 
extractions  compared  with  that  of Gusev  et  al. 
(2011).  Each of  the  four  frames were associated 
with  a  set  of  regular  expressions  to match  and 
extract verbs for two tenses (past and present), and 
three  different  aspects  (simple,  perfect,  and 
progressive).  Durations  could  match  spelled  out 
numbers (one hour), hyphenated numbers (twenty-
one minutes), or digits (30 minutes).
FOR: The  for-adverbial extraction  frame  was 
designed to  match  two tenses  and three  aspects. 
The regular expressions accounted for variation in 
the  word  ordering.  Consider  some  simplified 
pattern examples below, which show varied word 
order and tense-aspect combinations: 
? John ran for 10 minutes
? for ten minutes Sally was running
IN: The in-adverbial extraction frame  is tricky 
for extracting durations because the in-adverbial is 
sometimes  used  to  describe  pending  events  or 
things that are about to happen, such as, ?Sally is 
going  to  the  store  in  5  minutes?.  However,  I 
wanted  to  avoid  collecting  durations  for  future 
events.  Therefore any verbs that  matched the in-
adverbial extraction frame were restricted to match 
the perfect aspect  with any tense or the past tense 
and with any aspect, to indicate that a given event 
has been completed. 
SPEND/TAKE: The tense and aspect were not 
restricted and the tweets  were matched for tense 
and aspect on  spend and  take.  In these cases the 
durations were syntactically associated with spend 
and take whereas semantically, the durations were 
associated with the verb in the complement clause 
(read,  work, etc.).  Variations  in  word order,  like 
that found in examples of the for extraction frame, 
were not  allowed for  tweets  matching  the  spend 
extraction frame.  We see in  the  examples below 
that  the  verb is  read and the tense and aspect in 
each  of  the  examples  were  found  to  be  past 
progressive:
? Susie was spending 30 minutes reading
51
? Susie was taking 5 minutes to read it
3.3 Post-Processing Extracted Tweets
There were several steps to the post-processing 
of tweets. First, I identified the verb lemmas using 
NLTK  WordNet  (Bird  and  Loper,  2004).  Verb 
lemmas that  occurred  less  than 100 times  were 
removed. 
Next,  all  of  the  durations-measures  were 
converted  into  seconds  using  a  separate  set  of 
regular expressions.  Instances where the duration 
lasted  for  longer  than  1  billion  seconds  were 
removed.  There  were  6,389  tweets  that  met  this 
condition.  These  tweets  were  removed  in  an 
attempt to avoid figurative speech that  can  occur 
on Twitter. So tweets such as the ones shown in (2) 
and (3) were removed:
(2) I hate when I order food and it takes  
2009779732 years to come
(3) I think my iTunes library is too big, it takes  
7987694564 years to open
Not  all  of  the  temporal  durations  that  were 
extracted were numerically  measured.  Tweets that 
contained  indefinite  determiners a or  an  were 
treated as having a value of 1 temporal unit so that 
the noun phrase  ?an hour? could be converted to 
3600  seconds.  There  were  51,806 such  tweets. 
Some of the tweets contained expressions such as: 
?some hours?, ?many hours?, and ?several hours?. 
In  cases  like  these, the  duration  was  treated  as 
having a value of based on its temporal unit so that 
durations like  ?many hours?  were treated as  one 
hour.  This  was  applied  to  all  of  the  temporal 
durations that were not numerically measured3.
In addition, tweets that matched more than one 
extraction  frame  were  removed.  After  the  post-
processing  stage  390,562 tweets  were  extracted 
that covered 486 verb lemmas. 
3.4 Extraction Frame Evaluation
Extraction frame precision was estimated for each 
frame by  hand-annotating  a  randomly  selected 
sample  and  labeling  each  extracted  tweet as 
relevant if the duration, tense, aspect and verb were 
identified. The extraction frames performed overall 
with 90.38%  precision,  estimated  from  a  sample 
size  determined  by  the  two-tailed  t-test  for 
proportions with 95% confidence (n=400, p=0.05). 
3There were 35,553 tweets matching this criteria.
The extraction frame precision is reported below in 
Table 1.
Extraction 
Frame Type
Estimated
Precision
# Tweets
for 91.25% 270,624
in 72.25% 83,061
spend 99.75% 2,593
take 98.25% 34,284
Overall 90.38% 390,562
Table 1. Number of extracted tweets
4 Analysis of Durations
4.1 Duration Distributions
Twitter is a lucrative resource for gathering typical 
durations associated with common verbs at a fine-
grained level.  Some  verbs were found to have a 
very  short  mean  duration  (consider  rain  and 
snooze)  while  some  had a  longer mean  duration 
(consider live and work), shown in Table 2. 
Short  durations Long durations
doze 32,721 grow 197,921,586
jog 405,550 smoke 246,557,468
cough 4,756,427 live 247,274,960
rain 4,994,776 marry 312,000,000
meet 40,503,127 exist 341,174,881
 Table 2. Mean durations (in seconds)
for a sample of verb lemmas 
The  following plots  (Figures  1-3)  show  the 
frequency distribution for  three different  lemmas: 
wrestle, say, and boil. Similar to the work done by 
Pan  et  al.  (2011)  and  Gusev  et  al.  (2011),  this 
research  also  shows that  some  of  the duration 
distributions are bimodal. Gusev et al (2011), Pan 
et  al.  (2011),  and  recent  work  by  Williams  and 
Katz (2012) show that  some bimodal distributions 
could  be  associated  with  iterative  events  or 
habituality.
52
Figure 1. Distribution for wrestle, typically takes 
minutes or years
Figure 2. Distribution for say, typically takes 
seconds or years
The  bimodal  distributions  for  wrestle and  say 
could  possibly  indicate  that  there  are  two 
phenomena present  in the distributions: durations 
for events, and durations for habits. Consider that 
the sentence ?John wrestled for half and hour with 
his kids? describes an event whereas the sentence 
?John wrestled for 30 years as a pro? describes a 
habit.  An  analysis  of  the  relationship  between 
bimodal  distributions  and  habituality  would 
provide more information in future work.
Not all of the distributions are bimodal, in fact 
we can see that is the case with the distribution for 
boil. Users  of  Twitter  are  not  usually reporting 
long durations for that verb, but they do in several 
rare  cases.  This  could  be  due  to  the  effects  of 
figurative speech, as in ?John has been making by 
blood boil for decades?.
Figure 3. Distribution for boil, typically takes 
minutes
4.2 Comparison of Previous Work
To compare my work with Gusev et al, (2011), 
I  found  the overlap of verb lemmas.  There were 
356  verb  lemmas  in  common.  I  calculated  the 
log10 of  each mean duration associated with each 
verb lemma, for my data and theirs. I plotted my 
means versus  their  means  and  I  used  linear 
regression  to  find  a  best  fit  line. The  Pearson 
correlation  value  was  0.46  (p  <  0.01),  which 
suggests a weak  positive  correlation.  Some of the 
outliers  that we  see in Figure 4 correspond to the 
following verb lemmas: freeze, judge, age, double,  
load, lock, revise, score, heat, remove, lose, meet,  
head, ring, skate, yell, and fall.
Figure 4. Mean durations vs. Gusev et al (2011) in 
log10 seconds
53
5 Discussion
This  paper  has  presented  a  new  method  to 
automatically  extract  duration  information  for 
verbs using data from Twitter. The four extraction 
frames used  here  were 90.25%  accurate.  This 
indicates that regular expressions can be applied to 
tweets  to  associate  an  event  with  its  duration. 
Comparison with previous work shows that there is 
a  positive  correlation,  and  this  indicates  that  the 
method  presented  here  is  nearly  comparable. 
Corpora,  extracted  tweets,  durations,  and  other 
materials used in this study will be made publicly 
available at the following website:
 https://sites.google.com/site/relinguistics/
 
6 Future Work
There were  several aspects of natural language 
that were put aside in this research. Future work 
should compare how the duration distributions are 
affected  by  modality,  negation,  and  the  future 
tense/aspect combinations. And, although I briefly 
addressed the presence of figurative language, this 
work could  benefit  from  knowing  which  tweets 
were  figurative,  since  this  may  affect  how  we 
examine typical durations.
Only four types of extraction frames were used 
in this study. More work is needed to find out if 
there are other extraction frames that can be used 
for  this  same task,  and  exactly  which  extraction 
frames  should  be  used  under  various 
circumstances. Future work could also address the 
combinatorial effects of modality, negation, future 
tenses, and  verb arguments  with typical duration. 
Events  like  ?John might  finish writing his email 
soon? and  ?John might finish writing his memoir 
soon?  will  have  different  kinds  of durations 
associated with them.
Looking at the distributions presented here, it is 
not  clear  where  the  boundary  might  be between 
single  episodes, iterative  events or  habits. This 
kind of distinction between habits and events could 
prove  to  be  important  because  an  event  such  as 
exist can go on for years, decades or centuries, and 
in  some cases  exist might  only  last  for  a  few 
seconds ?  but  we  would  not  say  that  exist is  a 
habit. At the same time, the frequency distribution 
for  wrestle in  Figure  1  indicates that  the  event 
wrestle lasts  for  hours,  but the  fact  that  it  is 
reported to last  for  years  suggests  that  there  are 
some habits in the collected data.
References 
Steven  Bird  and Edward  Loper.  2004.  NLTK:  The 
natural  language  toolkit.  In Proceedings of  42nd 
Annual  Meeting  of  the  Association  for  
Computational Linguistics (ACL-04).
Roberto Gonz?lez-Ib??ez, Smaranda Muresan, and Nina 
Wacholder. 2011. ?Identifying sarcasm in Twitter: a 
closer  look?.  In  Proceedings  of  the  49th  Annual  
Meeting  of  the  Association  for  Computational  
Linguistics (pp. 581?586),  Portland,  Oregon,  June 
19-24.
Andrey  Gusev,  Nathaniel  Chambers,  Pranav  Khaitan, 
Divye Khilnani, Steven Bethard, and Dan Jurafsky. 
2011. ?Using query patterns to learn the durations of 
events?.  IEEE  IWCS-2011,  9th  International  
Conference on Web Services. Oxford, UK 2011.
Rune  Halvorsen,  and Christopher  Schierkolk.  2010. 
Tweetstream:  Simple  Twitter  Streaming  API 
(Version  0.3.5)  [Software].  Available  from 
https://bitbucket.org/runeh/tweetstream/src/
Jerry Hobbs and James Pustejovsky. 2003. ?Annotating 
and  reasoning  about  time  and  events?.  In 
Proceedings  of  the  AAAI  Spring  Symposium  on  
Logical  Formulation  of  Commonsense  Reasoning. 
Stanford University, CA 2003.
Zornitsa  Kozareva  and Eduard  Hovy. 2011. ?Learning 
Temporal  Information  for  States  and  Events?.  In 
Proceedings  of  the  Workshop  on  Semantic  
Annotation for Computational Linguistic  Resources  
(ICSC 2011), Stanford.
Marc Moens  and  Mark Steedman.  1988.  ?Temporal 
Ontology and Temporal Reference?.  Computational  
Linguistics 14(2):15-28.
Feng  Pan,  Rutu  Mulkar-Mehta,  and  Jerry  R.  Hobbs. 
2011. "Annotating and Learning Event Durations in 
Text." Computational Linguistics. 37(4):727-752.
Mike  Thelwall,  Kevan  Buckley,  and  Georgios 
Paltoglou.  2011.  ?Sentiment  in  Twitter  events.? 
Journal  of  the  American  Society  of  Information  
Science  and  Technology,  62: 406?418. 
doi: 10.1002/asi.21462
Jennifer Williams and Graham Katz. 2012. ?Extracting 
and modeling durations  for habits and events  from 
Twitter?.  In  Proceedings  of  Association  for  
Computational Linguistics, ACL 2012. Jeju, Republic 
of Korea.
54
Proceedings of the 2nd Workshop on Predicting and Improving Text Readability for Target Reader Populations, pages 30?38,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Language-Independent Approach to Automatic Text DifficultyAssessment for Second-Language LearnersWade Shen1, Jennifer Williams1, Tamas Marius2, and Elizabeth Salesky ?1
1MIT Lincoln Laboratory Human Language Technology Group,
244 Wood Street Lexingon, MA 02420, USA
{swade,jennifer.williams,elizabeth.salesky}@ll.mit.edu
2DLI Foreign Language Center, Bldg. 420, Room 119 Monterey, CA 93944, USAtamas.g.marius.civ@mail.milAbstract
In this paper, we introduce a new base-
line for language-independent text diffi-
culty assessment applied to the Intera-
gency Language Roundtable (ILR) profi-
ciency scale. We demonstrate that reading
level assessment is a discriminative prob-
lem that is best-suited for regression. Our
baseline uses z-normalized shallow length
features and TF-LOG weighted vectors on
bag-of-words for Arabic, Dari, English,
and Pashto. We compare Support Vector
Machines and the Margin-Infused Relaxed
Algorithm measured by mean squared er-
ror. We provide an analysis of which fea-
tures are most predictive of a given level.1 Introduction
The ability to obtain new materials of an appro-
priate language proficiency level is an obstacle
for second-language learners and educators alike.
With the growth of publicly available Internet and
news sources, learners and instructors of foreign
languages should have ever-increasing access to
large volumes of foreign language text. How-
ever, sifting through this pool of foreign language
data poses a significant challenge. In this paper
we demonstrate two machine learning regression
methods which can be used to help both learn-
ers and course developers by automatically rat-
ing documents based on the text difficulty. These
methods can be used to automatically identify
documents at specific levels in order to speed
course or test development, providing learners
? This work was sponsored by the Department of De-
fense under Air Force Contract FA8721-05-C-0002. Opin-
ions, interpretations, conclusions, and recommendations are
those of the authors and are not necessarily endorsed by the
United States Government.
with custom-tailored materials that match their
learning needs.
ILR (Interagency Language Roundtable) levels
reflect differences in text difficulty for second-
language learners at different stages of their edu-
cation. A description of each level is shown in Ta-
ble 1 (Interagency Language Roundtable, 2013).
Some levels differ in terms of sentence structure,
length of document, type of communication, etc.,
while others, especially the higher levels, differ in
terms of the domain and style of writing. Given
these differences, we expect that both semantic
content and grammar-related features will be nec-
essary to distinguish between documents at differ-
ent levels.Level Description
0 No proficiency
0+ Memorized proficiency
1 Elementary proficiency
1+ Elementary proficiency, plus
2 Limited working proficiency
2+ Limited working proficiency, plus
3 General professional proficiency
3+ General professional proficiency, plus
4 Advanced professional proficienty
4+ Advanced professional proficiency, plus
5 Functionally native proficiency
Table 1: Description of ILR levels.
Automatically determining ILR levels from
documents is a research problem without known
solutions. We have developed and adapted a se-
ries of rating algorithms and a set of experiments
gauging the feasibility of automatic ILR level as-
signment for text documents. Using data provided
by the Defense Language Institute Foreign Lan-
guage Center (DLIFLC), we show that while the
problem is tractable, the performance of automatic
30
methods is not perfect.
Our general approach treats the ILR rating prob-
lem as one of text classification; given the contents
and structure of a document, which of the ILR lev-
els should this document be assigned to? This
differs from traditional topic classification tasks
where word-usage often uniquely defines topics,
since we are also interested in features of text com-
plexity that describe structure. Leveling text is a
problem better fit to regression because reading
level is a continuous scale. We want to know how
close a document is to a given level (or between
levels), so we measured performance using mean
squared error (MSE). We show that language-
independent features can be used for regression
with Support Vector Machines (SVMs) and the
Margin-Infused Relaxed Algorithm (MIRA), and
we present our results for this new baseline for
Arabic, Dari, English, and Pashto. To the best of
our knowledge, this is the first study to systemati-
cally examine a language-independent approach to
readability using the ILR rating scale for second-
language learners.
This paper is structured as follows: Section 2
describes previous work on reading level assess-
ment as a text classification problem, Section 3
describes the two algorithms that we used in our
present work, Section 4 describes our data and ex-
periments, Section 5 reports our results, Section 6
provides an analysis of our results, and Section 7
proposes different kinds of future work that can be
done to improve this baseline.2 Related Work
In this section we describe some work on the read-
ability problem that is most closely related to our
own.
One of the earliest formulas for reading level
assessment, called the Flesch Reading Ease For-
mula, measured readability based on shallow
length features (Flesch, 1948). This metric in-
cluded two measurements: the average number of
words per sentence and the average number of syl-
lables per word. Although these features appear to
be shallow at the offset, the number of syllables
per word could be taken as an abstraction of word
complexity. Those formulas, as well as their var-
ious revisions, have become popular because they
are easy to compute for a variety of applications,
including structuring highly technical text that is
comprehensible at lower reading levels (Kincaid
et al, 1975). Some of the revisions to the Flesch
Reading Ease Formula have included weighting
these shallow features in order to linearly regress
across different difficulty levels.
Much effort has been placed into automating
the scoring process, and recent work on this is-
sue has examined machine learning methods to
treat reading level as a text classification prob-
lem. Schwarm and Ostendorf (2005) worked on
automatically classifying text by grade level for
first-language learners. Their machine learning
approach was a one vs. all method using a set
of SVM binary classifiers that were constructed
for each grade level category: 2, 3, 4, and 5.
The following features were used for classfication:
average sentence length, average number of syl-
lables per word, Flesch-Kincaid score, 6 out-of-
vocabulary (OOV) rate scores, syntactic parse fea-
tures, and 12 language model perplexity scores.
Their data was taken from the Weekly Reader
newspaper, already separated by grade level. They
found that the error rate for misclassification by
more than one grade level was significantly lower
for the SVM classifier than for both Lexile and
Flesch-Kincaid. Petersen and Ostendorf (2009)
later replicated and expanded Schwarm and Os-
tendorf (2005), reaffirming that both classifica-
tion and regression with SVMs provided a better
approximation of readabilty by grade level when
compared with more traditional methods such as
the Flesch-Kincaid score. In the current work, we
also use SVM for regression, but have decided to
report mean squared error as a more meaningful
metric.
In an effort to uncover which features are the
most salient for discriminating among reading lev-
els, Feng et al, (2010) studied classification per-
formance using combinations of different kinds of
readability features using data from the Weekly
Reader newspaper. Their work examined the
following types of features: discourse, language
modeling, parsed syntactic features, POS fea-
tures, shallow length features, as well as some
features replicated from Schwarm and Ostendorf
(2005). They reported classifier accuracy and
mean squared error from two classifiers, SVM and
Logistic Regression, which were used to predict
grade level for grades 2 through 5. While they
found that POS features were the most predictive
overall, they also found that the average number of
words per sentence was the most predictive length
31
feature. This length feature alone achieved 52%
accuracy with the Logistic Regression classifier.
In the present work, we use the average number of
words per sentence as a length feature and show
that this metric has some correspondence with the
different ILR levels.
Another way to examine readability is to treat
it as a sorting problem; that is, given some collec-
tion of texts, to sort them from easiest to most dif-
ficult. Tanaka-Ishii et al, (2010) presented a novel
method for determining readibility based on sort-
ing texts using text from two groups: low difficulty
and high difficulty. They reported their results
in terms of the Spearman correlation coefficient
to compare performance of Flesch-Kincaid, Dale-
Chall, SVM regression, and their sorting method.
They showed that their sorting method was supe-
rior to the other methods, followed by SVM re-
gression. However, they call for a more mod-
ern and efficient approach to the problem, such as
online learning, that would estimate weights for
regression. We answer their call with an online
learning approach in this work.3 Algorithms
In this section, we describe two maximum margin
approaches that we used in our experiments. Both
are based on the principle of structural risk mini-
mization. We selected the SVM algorithm because
of its proven usefulness for automatic readability
assessment. In addition, the Margin-Infused Re-
laxed Algorithm is advantageous because it is an
online algorithm and therefore allows for incre-
mental training while still taking advantange of
structural risk minimization.3.1 Structural Risk Minimization
For many classification and regression problems,
maximum margin approaches are shown to per-
form well with minimal amounts of training data.
In general, these approaches involve linear dis-
criminative classifiers that attempt to learn hy-
perplane decision boundaries which separate one
class from another. Since multiple hyperplanes
that separate classes can exist, these methods add
an additional constraint: they attempt to learn hy-
perplanes while maximizing a region around the
boundary called the margin. We show an exam-
ple of this kind of margin in Figure 1, where the
margin represents the maximum distance between
the decision boundary and support vectors. The
maximum margin approach helps prevent overfit-
ting issues that can occur during training, a princi-
ple called structural risk minimization. Therefore
we experiment with two such margin-maximizing
algorithms, described below.
Figure 1: Graphical depiction of the maximum
margin principle.3.2 Support Vector Machines
For text classification problems, the most popular
maximum margin approach is the SVM algorithm,
introduced by Vapnik (1995). This approach uses
a quadratic programming method to find the sup-
port vectors that define the margin. This is a batch
training algorithm requiring all training data to be
present in order to perform the optimization pro-
cedure (Joachims, 1998a). We used LIBSVM to
implement our own SVM for regression (Chang
and Lin, 2001).
Discriminative methods seek to best divide
training examples in each class from out-of-class
examples. SVM-based methods are examples
of this approach and have been successfully ap-
plied to other text classification problems, includ-
ing previous work on reading level assessment
(Schwarm and Ostendorf, 2005; Petersen and Os-
tendorf, 2009; Feng et al, 2010). This approach
attempts to explicitly model the decision boundary
between classes. Discriminative methods build a
model for each class c that is defined by the bound-
ary between examples of class c and examples
from all other classes in the training data.
32
3.3 Margin-Infused Relaxed Algorithm
Online approaches have the advantage of allowing
incremental adaptation when new labeled exam-
ples are added during training. We implemented
a version of MIRA from Crammer and Singer
(2003), which we used for regression. Cram-
mer and Singer (2003) proved MIRA as an on-
line multiclass classifier that employs the prin-
ciple of structural risk minimization, and is de-
scribed as ultraconservative because it only up-
dates weights for misclassified examples. For
classification, MIRA is formulated as shown in
equation (1):
c? = argmax
c2C
fc(d) (1)
where
fc(d) = w ? d (2)
and w is the weight vector which defines the
model for class c. During training, examples are
presented to the algorithm in an online fashion (i.e.
one at a time) and the weight vector is updated
accourding to the update shown in equation (2):
wt = wt 1 + l(wt 1,dt 1)vt 1 (3)
l(wt 1,dt 1) = ||dt 1  wt 1||  ? (4)
vt 1 = (sign(||dt 1  wt 1||)  ?)dt 1 (5)
where l(?) is the loss function, ? corresponds to
the margin slack, and vt 1 is the negative gradient
of the loss vector for the previously seen example
||dt 1   wt 1||. This update forces the weight
vector towards erroneous examples during train-
ing. The magnitude of the change is proportional
to the l(?). For correct training examples, no up-
date is performed as l(?) = 0. In a binary classi-
fication task, MIRA attempts to minimize the loss
function in (4), such that the magnitude of the dis-
tance between a document vector and the weight
vector is also minimized.
However, unlike topic classification or classi-
fication of words based on their semantic class
where the classes are generally discrete, the ILR
levels lie on a continuum (i.e. level 2 >> level
1 >> level 0). Therefore we are more interested
in using MIRA for regression because we want
to compare the predicted value with the true real-
valued label, rather than a class label. For regres-
sion, we can redefine the MIRA loss function as
follows:
l(wt,dt) = |lt   dt ? wt|  ? (6)
In this case, lt is the correct value (in our case,
ILR level) for training document dt and dt ? wt is
the predicted value given the current weight vector
wt. We expect that minimizing this loss function
cumulatively over the entire training set will yield
a regression model that can predict ILR levels for
unseen documents.
This revised loss function results in a modi-
fied update equation for each online update of
the MIRA weight vector (generating a new set of
weights wt from the previously seen example):
wt = wt 1 + l(wt 1,dt 1)vt 1 (7)
vt 1 = (sign(|lt 1 dt 1 ?wt 1|) ?)dt 1 (8)
vt 1 defines the direction of loss and the mag-
nitude of the update relative to the current train-
ing example dt 1. Since this approach is online,
MIRA does not guarantee minimal loss or maxi-
mummargin constraints for all of the training data.
However, in practice, these methods perform as
well as their SVM counterparts without the need
for batch training (Crammer et al, 2006).4 Experiments4.1 Data
All of our experiments used data from four lan-
guages: Arabic (AR), Dari (DAR), English (EN),
and Pashto (PS). In Table 2, we show the distri-
bution of number of documents per ILR level for
each language. All of our data was obtained from
the Directorate of Language Science and Technol-
ogy (LST) and the Language Technology Evalua-
tion and Application Division (LTEA) at the De-
fense Language Institute Foreign Language Cen-
ter (DLIFLC). The data was compiled using an
online resource (Domino). Language experts (na-
tive speakers) used various texts from the Inter-
net which they considered to be authentic mate-
rial and they created the Global Language Online
Support System (GLOSS) system. The texts were
used to debug the GLOSS system and to see how
well GLOSS worked for the respective languages.
Each of the texts were labeled by two independent
linguists expertly trained in ILR level scoring. The
ratings from these two linguists were then adjudi-
cated by a third linguist. We used the resulting
adjudicated labels for our training and evaluation.
We preprocessed the data by doing the follow-
ing tokenization: removed extra whitespace, nor-
malized URIs, normalized currency, normalized
33
Level AR DAR EN PS
1 204 197 198 197
1+ 200 197 197 199
2 199 201 204 200
2+ 199 194 196 198
3 198 195 202 198
3+ 194 194 198 200
4 198 195 190 195
Overall 1394 1375 1390 1394
Table 2: Total collection documents per language
per ILR level.
numbers, normalized abbreviations, normalized
punctuation, and folded to lowercase. We identi-
fied words by splitting text on whitespace and we
identified sentences by splitting text on punctua-
tion.4.2 Features
It is necessary to define a set of features to help
the regressors distinguish between the ILR levels.
We conducted our experiments using two different
types of features: word-usage features and shallow
length features. Shallow length features are shown
to be useful in reading level prediction tasks (Feng
et al, 2010). Word-usage features, such as the
ones used here, are meant to capture some low-
level topical differences between ILR levels.Word-usage features: Word frequencies (or
weighted word frequencies) are commonly used
as features for topic classification problems, as
these features are highly correlated with topics
(e.g. words like player and touchdown are very
common in documents about topics like football,
whereas they are much less common in documents
about opera). We used TF-LOG weighted word
frequencies on bag-of-words for each document.Length features: In addition to word-usage, we
added three z-normalized length features: (1) av-
erage sentence length (in words) per document,
(2) number of words per document, and (3) aver-
age word length (in characters) per document. We
used these as a basic measure of language level
complexity. These features are easily computed
by automatic means, and they capture some of the
structural differences between the ILR levels.
Figures 2, 3, 4, and 5 show the z-normalized
average word count per sentence for Arabic, Dari,
English, and Pashto respectively. The overall data
set for each language has a normalized mean of
Figure 2: Arabic, z-normalized average word
count per sentence for ILR levels 1, 2 and 3.
Figure 3: Dari, z-normalized average word count
per sentence for ILR levels 1, 2 and 3.
Figure 4: English, z-normalized average word
count per sentence for ILR levels 1, 2 and 3.
34
MIRA SVM (linear)LEN WORDS COMBINED LEN WORDS COMBINEDAR 4.527 0.283 0.222 0.411 0.263 0.198DAR 5.538 0.430 0.330 0.473 0.409 0.301EN 5.155 0.181 0.148 0.430 0.181 0.147PS 5.371 0.410 0.360 1.871 0.393 0.391
Table 3: Performance results (MSE) for SVM and MIRA on Arabic, Dari, English and Pashto for three
different kinds of features/combinations.
Figure 5: Pashto, z-normalized average word
count per sentence for ILR levels 1, 2 and 3.
zero and unit variance, which were calculated sep-
arately for a given length feature. The x-axis
shows the deviation of documents relative to the
data set mean, in units of overall standard devia-
tion. It is clear from the separability of the levels
in these figures that sentence length could be an
important indicator of ILR level, though no fea-
ture is a perfect discriminator. This is indicated by
the significant overlap between the distributions of
document lengths at different ILR levels.4.3 Training
We split the data between training and testing us-
ing an 80/20 split of the total data for each lan-
guage. To formulate the ILR scale as continuous-
valued, we assumed that ?+? levels are 0.5 higher
than their basis (e.g. 2+ = 2.5). Though this may
not be optimal if distances between levels are non-
constant, the best systems in our experiments show
good prediction performance using this assump-
tion.
Both of the classifiers were trained to predict the
ILR value as a continuous value using regression.
We measured the performance of each method in
terms of the mean squared error on the unseen test
documents. We tested the following three con-
ditions: length-based features only (LEN), word-
usage features only (WORDS), and word and
length features combined (COMBINED). Since
each algorithm (SVM and MIRA) has a number
of parameters that can be tuned to optimize per-
formance, we report results for the best settings for
each of the algorithms. These settings were deter-
mined by sweeping parameters to optimize perfor-
mance on the training data for a range of values,
for both MIRA and SVM. For both algorithms,
we varied the number of training iterations from
500 to 3100 for each language, with stepsize of
100. We also varied the minimum word frequency
count from 2 to 26, with stepsize 1. For MIRA
only, we varied the slack parameter from 0.0005
to 0.0500, with stepsize 0.00025. For SVM (linear
kernel only), we varied the C parameter and   at a
coarse setting of 2n with values of n ranging from
-15 to 6 with stepsize 1.5 Results
We compared the performance of the online
MIRA approach with the SVM-based approach.
Table 3 shows the overall performance of MIRA
regression and SVM regression, respectively, for
the combinations of features for each language.
Mean squared error was averaged over all of the
levels in a given language. MIRA is an approx-
imation to SVM, however one of the advantages
of MIRA is that it is an online algorithm so it is
adaptable after training and training can be en-
hanced later with more data with a small number
of additional data points.
Figures 6 and 7 show the per-level performance
for each classifier with the overall best features
(COMBINED) for each language. The highest
level (Level 4) and lowest levels (Level 1) tend to
35
exhibit the worst performance across all languages
for each regression method. Poorer performance
on the outlying levels could be due to overfitting
for both SVM and MIRA on those levels. The
ILR scale includes 4 major levels at half-step in-
tervals between each one. We are not sure if us-
ing a different scale, such as grade levels ranging
from 1 to 12, would also exhibit poorer perfor-
mance on the outlying levels because the highest
ILR level corresponds to native-like fluency. This
U-shaped performance is seen across both classi-
fiers for each of the languages.6 Analysis
Our results show that SVM slightly outperformed
MIRA for all of the languages. We believe that
the reason whyMIRA performed worse than SVM
is because it was overfit during training whereas
SVM was not. This could be due to the parame-
ters that we set during our sweep in training. We
selected C and   as parameters to SVM linear-
kernel for the best performance. The   values for
English and Arabic were set at more than 1000
times smaller than the values for Pashto and Dari
(AR: =6.1035156 ? 10 5, DAR: =0.0078125,
EN: =3.0517578 ? 10 5, PS: =0.03125). This
means that the margins for Pashto and Dari were
set to be larger respective to English and Arabic.
One reason why these margins were larger is be-
cause the features that we used had more discrimi-
native power for English and Arabic. In fact, both
MIRA and SVM performed worse on Pashto and
Dari.
Since the method described here makes use of
Figure 6: MIRA performance (MSE) per ILR level
for each language.
Figure 7: SVM performance (MSE) per ILR level
for each language.
linear classifiers that weigh word-usage and length
features, it is possible to examine the weights that
a classifier learns during training to see which fea-
tures the algorithm deems most useful in discrim-
inating between ILR levels. One way to do this
is to use a multiclass classifier on our data for the
categorical levels (e.g. 1, 1+, 2, etc.) and exam-
ine the weights that were generated for each class.
MIRA is formulated to be a multiclass classifier
so we examined its weights for the features. We
chose MIRA instead of SVM, even though LIB-
SVM supports multiclass classification, because
we wanted to capture differences between levels
which we could not do with one vs. all. We exam-
ined classifier weights of greatest magnitude to see
which features were the most indicative and most
contra-indicative for that level. We report these
two types of features for Level 3 and Level 4 in
Tables 4 and 5, respectively. Level 3 documents
can have some complex topics, such as politics
and art, however it can be noted that some of the
more abstract topics like love and hate are contra-
indicative of Level 3. On the other hand, we see
that abstract topics are highly indicative Level 4
documents where topics such as philosophy, reli-gion, virtue, hypothesis, and theory are discussed.
We also note that moral is highly contra-indicative
of Level 3 but is highly indicative of Level 4.7 Discussion and Future Work
We have presented an approach to score docu-
ments based on their ILR level automatically us-
ing language-independent features. Measures of
structural complexity like the length-based fea-
36
MostIndicative + Most Contra-Indicative -
obama 1.739 said -2.259
to 1.681 your -1.480
republicans 1.478 is -1.334
? 1.398 moral -0.893
than 1.381 this -0.835
more 1.365 were -0.751
cells 1.355 area -0.751
american 1.338 love -0.730
americans 1.335 says -0.716
art 1.315 hate -0.702
it?s 1.257 against -0.682
could 1.180 people -0.669
democrats 1.143 body -0.669
as 1.139 you -0.666
a 1.072 man -0.652
but 1.041 all -0.644
america 0.982 over -0.591
Table 4: Dominant features for English at ILR
Level 3.
tures used in this work are important to achiev-
ing good ILR prediction performance. We intend
to investigate further measures that could improve
this baseline, including features from automatic
parsers or unsupervised morphology to measure
syntactic complexity. Here we have shown that
higher reading levels in English correspond more
with abstract topics. In future work, we also want
to capture some of the stylistic features of text,
such as the complexity of dialogue exchanges.
For both SVM and MIRA, the combination of
length and word-usage features had the best im-
pact on performance across languages. We found
better performance on this task overall for SVM
and we believe that MIRA was overfitting during
training. For MIRA, this is likely due to an inter-
action between a small number of features and the
stopping criterion (mean squared error = 0) that
we used in training, which tends to overfit. We in-
tend to investigate the stopping criterion in future
work. Still, we have shown that MIRA can be use-
ful in this task because it is an online algorithm,
and it allows for incremental training and active
learning.
Our current approach can be quickly adapted
for a new subset of languages because the features
that we used here were language-independent. We
plan to build a flexible architecture that enables
language-specific feature extraction to be com-
MostIndicative + Most Contra-Indicative -
of 3.298 +number+ -2.524
this 2.215 . -2.514
moral 1.880 government -1.120
philosophy 1.541 have -1.109
is 1.242 people -1.007
theory 1.138 would -0.909
in 1.131 could -0.878
absolute 1.034 after -0.875
religion 1.011 you -0.874
hyperbole 0.938 ,? -0.870
mind 0.934 were -0.827
as 0.919 was -0.811
hypothesis 0.904 years -0.795
schelling 0.883 your -0.747
thought 0.854 americans -0.746
virtue 0.835 at -0.745
alchemy 0.828 they -0.720
Table 5: Dominant features for English at ILR
Level 4.
bined with our method so that these techniques
can be easily used for new languages. We will
continuously improve this baseline using the ap-
proaches described in this paper. We found that
these two algorithms along with these types of
features performed pretty well on 4 different lan-
guages. It is surprising that these features would
correlate across languages even though there are
individual differences between each language. In
future work, we are interested to look deeper into
the nature of language-independence for this task.
With respect to content, we are interested to find
out if more word features are needed for some
languages but not others. There could be diver-
sity of vocabulary at higher ILR levels, which we
could measure with entropy. Additionally, since
the MIRA classifier that we are using is an on-
line classifier with weight vector representation
for each feature, we could examine the weights
and measure the mutual information by ILR level
above a certain threshold to find which features are
the most predictive of an ILR level, for each lan-
guage. Lastly, we have assumed that the ILR rat-
ing metric is approximately linear, and although
we have used linear classifiers in this task, we are
interested to learn if other transformations would
give us a better sense of ILR level discrimination.
37
References
Chih-Chung Chang and Chih-Jen Lin. 2001.
LIBSVM: a library for support vec-
tor machines. Software available athttp://www.csie.ntu.edu.tw/?cjlin/libsvm.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative Online Algorithms for Multiclass Prob-
lems. Journal of Machine Learning Research,
3(2003):951-991.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
Passive-Agressive Algorithms. Journal of MachineLearning Research, 7(2006):551-585.
George R. Doddington, Mark A. Przybocki, Alvin F.
Martin, and Douglas A. Reynolds. 2000. The NIST
speaker recognition evaluation - overview, method-
ology, systems, results, perspective. Speech Com-munication, 31(2-3):225-254.
Lijun Feng, Martin Jansche, Matt Huenerfauth,
Noe?mie Elhadad. 2010. A Comparison of Fea-
tures for Automatic Readability Assessment. InProceedings of the 23rd International Conference onComputational Linguistics: Posters. Association for
Computational Linguistics, 2010.
Rudolph Flesch. 1948. A new readability yardstick.Journal of Applied Psychology, 32(3):221-233.
Interagency Language Roundtable. ILR Skill Scale.http://www.govtilr.org/Skills/ILRscale4.htm, 2013. Accessed February 27,
2013.
Thorsten Joachims. 1998a. Text categorization with
support vector machines: learning with many rel-
evant features. In Proceedings of the EuropeanConference on Machine Learning, pages 137-142,
1998a.
Peter J. Kincaid, Lieutenant Robert P. Fishburne, Jr.,
Richard L. Rogers, and Brad S. Chissom. 1975.
Derivation of new readability formulas for Navy en-
listed personnel. Research Branch Report 8-75, U.S.
Naval Air Station, Memphis, 1975.
Sarah E. Petersen and Mari Ostendorf. 2009. A ma-
chine learning approach to reading level assessment.Computer Speech and Language, 23(2009):89-106.
Sarah. E. Schwarm and Mari Ostendorf. 2005. Read-
ing Level Assessment Using Support Vector Ma-
chines and Statistical Language Models. In Pro-ceedings of the 43rd Annual Meeting of the Asso-ciation for Computational Linguistics.
Kumiko Tanaka-Ishii, Satoshi Tezuka, and Hiroshi Ter-
ada. 2010. Sorting texts by readability. Computa-tional Linguistics, 36(2):203-227.
Vladimir Vapnik. 1995. The Nature of StatisticalLearning Theory. Springer, New York, 1995.
38
Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 1?9,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Meaning Unit Segmentation in English and Chinese: a New Approach toDiscourse PhenomenaJennifer Williams ?1,2, Rafael Banchs2, and Haizhou Li2
1Department of Linguistics, Georgetown University, Washington, D.C., USA
2Institute for Infocomm Research, 1 Fusionpolis Way, Singaporejaw97@georgetown.edu {rembanchs,hli}@i2r.a-star.edu.sgAbstract
We present a new approach to dialogue
processing in terms of ?meaning units?. In
our annotation task, we asked speakers of
English and Chinese to mark boundaries
where they could construct the maximal
concept using minimal words. We com-
pared English data across genres (news,
literature, and policy). We analyzed the
agreement for annotators using a state-of-
the-art segmentation similarity algorithm
and compared annotations with a random
baseline. We found that annotators are
able to identify meaning units systemati-
cally, even though they may disagree on
the quantity and position of units. Our
analysis includes an examination of phrase
structure for annotated units using con-
stituency parses.1 Introduction
When humans translate and interpret speech in
real-time, they naturally segment speech in ?min-
imal sense units? (Ole?ron & Nanpon, 1965;
Ben??tez & Bajo, 1998) in order to convey the
same information from one language to another as
though there were a 1-to-1 mapping of concepts
between both languages. Further, it is known that
people can hold up to 7+/- 2 ?chunks? of informa-
tion in memory at a time by creating and applying
meaningful organization schemes to input (Miller,
1956). However, there is no definitive linguistic
description for the kind of ?meaning units? that
human translators create (Signorelli et al, 2011;
Hamon et al, 2009; Mima et al, 1998).
The ability to chunk text according to units of
meaning is key to developing more sophisticated
machine translation (MT) systems that operate in
? Now affiliated with Massachusetts Institute of Tech-
nology Lincoln Laboratory.
real-time, as well as informing discourse process-
ing and natural language understanding (NLU)
(Kola?r?, 2008). We present an approach to dis-
course phenomena to address Keller?s (2010) call
to find a way to incorporate ?cognitive plausibil-
ity? into natural language processing (NLP) sys-
tems. As it has been observed that human trans-
lators and interpreters naturally identify a certain
kind of ?meaning unit? when translating speech
in real-time (Ole?ron & Nanpon, 1965; Ben??tez &
Bajo, 1998), we want to uncover the features of
those units in order to automatically identify them
in discourse.
This paper presents an experimental approach
to annotating meaning units using human anno-
tators from Mechanical Turk. Our goal was to
use the results of human judgments to inform
us if there are salient features of meaning units
in English and Chinese text. We predicted that
human-annotated meaning units should systemat-
ically correspond to some other linguistic features
or combinations of those features (i.e. syntax,
phrase boundaries, segments between stop words,
etc.). We are interested in the following research
questions:
? At what level of granularity do English and
Chinese speakers construct meaning units in
text?
? Do English and Chinese speakers organize
meaning units systematically such that mean-
ing unit segmentations are not random?
? How well do English and Chinese speakers
agree on meaning unit boundaries?
? Are there salient syntactic features of mean-
ing units in English and Chinese?
? Can we automatically identify a 1-to-1 map-
ping of concepts for parallel text, even if there
is paraphrasing in one or both languages?
1
While we have not built a chunker or classifier
for meaning unit detection, it is our aim that this
work will inform how to parse language system-
atically in a way that is human-understandable. It
remains to be seen that automatic tools can be de-
veloped to detect meaning units in discourse. Still,
we must be informed as to what kinds of chunks
are appropriate for humans to allow them to under-
stand information transmitted during translation
(Kola?r?, 2008). Knowledge about meaning units
could be important for real-time speech process-
ing, where it is not always obvious where an ut-
terance begins and ends, due to any combination
of natural pauses, disfluencies and fillers such as
?like, um..?. We believe this work is a step towards
creating ultra-fast human-understandable simulta-
neous translation systems that can be used for con-
versations in different languages.
This paper is organized as follows: Section 2
discusses related work, Section 3 describes the
segmentation similarity metric that we used for
measuring annotator agreement, Section 4 de-
scribes our experiment design, Section 5 shows
experiment results, Section 6 provides analysis,
and Section 7 discusses future work.2 Related Work
At the current state of the art, automatic simultane-
ous interpretation systems for speech function too
slowly to allow people to conduct normal-paced
conversations in different languages. This prob-
lem is compounded by the difficulty of identifying
meaningful endpoints of utterances before trans-
mitting a translation. For example, there is a per-
ceived lag time for speakers when trying to book
flights or order products over the phone. This lag
time diminishes conversation quality since it takes
too long for each speaker to receive a translation
at either end of the system (Paulik et al, 2009). If
we can develop a method to automatically identify
segments of meaning as they are spoken, then we
could significantly reduce the perceived lag time
in real-time speech-to-speech translation systems
and improve conversation quality (Baobao et al,
2002; Hamon et al, 2009).
The problem of absence of correspondence
arises when there is a lexical unit (single words
or groups of words) that occurs in L1 but not
in L2 (Lambert et al, 2005). It happens when
words belonging to a concept do not correspond to
phrases that can be aligned in both languages. This
problem is most seen when translating speech-to-
speech in real-time. One way to solve this prob-
lem is to identify units for translation that cor-
respond to concepts. A kind of meaning unit
had been previously proposed as information units
(IU), which would need to be richer than seman-
tic roles and also be able to adjust when a mis-
take or assumption is realized (Mima et al, 1998).
These units could be used to reduce the explosion
of unresolved structural ambiguity which happens
when ambiguity is inherited by a higher level syn-
tactic structure, similar to the use of constituent
boundaries for transfer-driven machine translation
(TDMT) (Furuse et al, 1996).
The human ability to construct concepts in-
volves both bottom-up and top-down strategies in
the brain. These two kinds of processes inter-
act and form the basis of comprehension (Kintsch,
2005). The construction-integration model (CI-2)
describes how meaning is constructed from both
long-term memory and short-term memory. One
of the challenges of modeling meaning is that it
requires a kind of world-knowledge or situationalknowledge, in addition to knowing the meanings
of individual words and knowing how words can
be combined. Meaning is therefore constructed
from long-term memory ? as can be modeled by
latent semantic analysis (LSA) ? but also from
short-term memory which people use in the mo-ment (Kintsch & Mangalath, 2011). In our work,
we are asking annotators to construct meaning
from well-formed text and annotate where units of
meaning begin and end.3 Similarity Agreement
We implemented segmentation similarity (S) from
Fournier and Inkpen (2012). Segmentation sim-
ilarity was formulated to address some gaps of
the WindowDiff (WD) metric, including unequal
penalty for errors as well as the need to add
padding to the ends of each segmentation (Pevzner
& Hearst, 2002). There are 3 types of segmenta-
tion errors for (S), listed below:
1. s1 contains a boundary that is off by n poten-
tial boundaries in s2
2. s1 contains a boundary that s2 does not, or
3. s2 contains a boundary that s1 does not
These three types of errors are understood astranspositions in the case of error type 1, and as
2
substitutions in the case of error types 2 and 3.
Note that there is no distinction between insertions
and deletions because neither of the segmentations
are considered reference or hypothesis. We show
the specification of (S) in (1):
S(si1,si2) =
t ? mass(i)  t  d(si1,si2,T )
t ? mass(i)  t
(1)
such that S scales the cardinality of the set of
boundary types t because the edit distance func-
tion d(si1,si2,T ) will return a value for potential
boundaries of [0, t ? mass(i)] normalized by the
number of potential boundaries per boundary type.
The value of mass(i) depends on task, in our
work we treat mass units as number of words, for
English, and number of characters for Chinese.
Since our annotators were marking only units of
meaning, there was only one boundary type, and
(t = 1). The distance function d(si1,si2,T ) is the
edit distance between segments calculated as the
number of boundaries involved in transposition
operations subtracted from the number of substi-
tution operations that could occur. A score of 1.0
indicates full agreement whereas a score of 0 indi-
cates no agreement.
In their analysis and comparison of this new
metric, Fournier and Inkpen (2012) demonstrated
the advantages of using (S) over using (WD)
for different kinds of segmentation cases such
as maximal/minimal segmentation, full misses,
near misses, and segmentation mass scale effects.
They found that in each of these cases (S) was
more stable than (WD) over a range of segment
sizes. That is, when considering different kinds
of misses (false-positive, false-negative, and both),
the metric (S) is less variable to internal segment
size. These are all indications that (S) is a more
reliable metric than (WD).
Further, (S) properly takes into account chance
agreement - called coder bias - which arises in
segmentation tasks when human annotators either
decide not to place a boundary at all, or are un-
sure if a boundary should be placed. Fournier and
Inkpen (2012) showed that metrics that follow (S)
specification reflect most accurately on coder bias,
when compared to mean pairwise 1   WD met-
rics. Therefore we have decided to use segmenta-
tion similarity as a metric for annotator agreement.
4 Experiment Design
This section describes how we administered our
experiment as an annotation task. We surveyed
participants using Mechanical Turk and presented
participants with either English or Chinese text.
While the ultimate goal of this research direc-
tion is to obtain meaning unit annotations for
speech, or transcribed speech, we have used well-
structured text in our experiment in order to find
out more about the potential features of meaning
units in the simplest case.4.1 Sample Text PreparationGenre: Our text data was selected from three dif-
ferent genres for English (news, literature, and
policy) and one genre for Chinese (policy). We
used 10 articles from the Universal Declaration of
Human Rights (UDHR) in parallel for English and
Chinese. The English news data (NEWS) con-
sisted of 10 paragraphs that were selected online
from www.cnn.com and reflected current events
from within the United States. The English liter-
ature data (LIT) consisted of 10 paragraphs from
the novel Tom Sawyer by Mark Twain. The En-
glish and Chinese UDHR data consisted of 12 par-
allel paragraphs from the Universal Declaration of
Human Rights. The number of words and number
of sentences by language and genre is presented
below in Table 1.Preprocessing: To prepare the text samples for
annotation, we did some preprocessing. We re-
moved periods and commas in both languages,
since these markings can give structure and mean-
ing to the text which could influence annotator de-
cisions about meaning unit boundaries. For the
English data, we did not fold to lowercase and we
acknowledge that this was a design oversight. The
Chinese text was automatically segmented into
words before the task using ICTCLAS (Zhang et
al., 2003). This was done in order to encourage
Chinese speakers to look beyond the character-
level and word-level, since word segmentation is
a well-known NLP task for the Chinese language.
The Chinese UDHR data consisted of 856 charac-
ters. We placed checkboxes between each word in
the text.4.2 Mechanical Turk Annotation
We employed annotators using Amazon Mechan-
ical Turk Human Intelligence Tasks (HITs). All
instructions for the task were presented in En-
3
Language and Genre # words # Sentences
Chinese UDHR 485 20
English NEWS 580 20
English LIT 542 27
English UDHR 586 20
Table 1: Number of words and sentences by lan-
guage and genre.
glish. Each participant was presented with a set of
numbered paragraphs with a check-box between
each word where a boundary could possibly ex-
ist. In the instructions, participants were asked
to check the boxes between words correspond-
ing to the boundaries of meaning units. They
were instructed to create units of meaning larger
than words but that are also the ?maximal concept
that you can construct that has the minimal set of
words that can be related to each individual con-
cept?1. We did not provide marked examples to
the annotators so as to avoid influencing their an-
notation decisions.
Participants were given a maximum of 40 min-
utes to complete the survey and were paid USD
$1.00 for their participation. As per Amazon
Mechanical Turk policy, each of the participants
were at least 18 years of age. The annotation
task was restricted to one task per participant, in
other words if a participant completed the English
NEWS annotation task then they could not partic-
ipate in the Chinese UDHR task, etc. We did not
test any of the annotators for language aptitude
or ability, and we did not survey language back-
ground. It is possible that for some annotators,
English and Chinese were not a native language.5 Results
We omitted survey responses for which partici-
pants marked less than 30 boundaries total, as well
as participants who completed the task in less than
5 minutes. We did this in an effort to eliminate
annotator responses that might have involved ran-
dom marking of the checkboxes, as well as those
who marked only one or two checkboxes. We de-
cided it would be implausible that less than 30
boundaries could be constructed, or that the task
1The definition of ?meaning units? we provide is very am-
biguous and can justify for different people understanding the
task differently. However, this is part of what we wanted to
measure, as giving a more precise and operational definition
would bias people to some specific segmentation criteria.
could be completed in less than 5 minutes, con-
sidering that there were several paragraphs and
sentences for each dataset. After we removed
those responses, we had solicited 47 participants
for English NEWS, 40 participants for English
LIT, 59 participants for English UDHR, and 10
participants for Chinese UDHR. The authors ac-
knowledge that the limited sample size for Chi-
nese UDHR data does not allow a direct compar-
ison across the two languages, however we have
included it in results and analysis as supplemental
findings and encourage future work on this task
across multiple languages. We are unsure as to
why there was a low number of Chinese annota-
tors in this task, except perhaps the task was not as
accessible to native Chinese speakers because the
task instructions were presented in English.5.1 Distributions by Genre
We show distributions of number of annotators
and number of units identified for each language
and genre in Figures 1 ? 4. For each of the
language/genres, we removed one annotator be-
cause the number of units that they found was
greater than 250, which we considered to be
an outlier in our data. We used the Shapiro-
Wilk Test for normality to determine which, if
any, of these distributions were normally dis-
tributed. We failed to reject the null hypothesis for
Chinese UDHR (p = 0.373) and English NEWS
(p = 0.118), and we rejected the null hypothe-
sis for English LIT (p = 1.8X10 04) and English
UDHR (p = 1.39X10 05).Dataset N Avg AvgUnits Words/Unit
Chinese UDHR 9 70.1 ?
English NEWS 46 84.9 6.8
English LIT 39 85.4 6.3
English LIT G1 26 66.9 8.1
English LIT G2 13 129.0 4.2
English UDHR 58 90.1 6.5
English UDHR G1 17 52.2 11.2
English UDHR G2 19 77.3 7.6
English UDHR G3 22 132.2 4.4
Table 2: Number of annotators (N), average num-
ber of units identified, average number of words
per unit identified, by language and genre.
Since the number of units were not normally
distributed for English LIT and English UDHR,
4
Figure 1: Distribution of total number of annota-
tions per annotator for Chinese UDHR.
Figure 2: Distribution of total number of annota-
tions per annotator for English UDHR.
Figure 3: Distribution of total number of annota-
tions per annotator for English NEWS.
we used 2-sample Kolmogorov-Smirnov (KS)
Tests to identify separate distributions for each of
these genres. We found 3 distinct groups in En-
glish UDHR (G1?G3) and 2 distinct groups in En-
glish LIT (G1 and G2). Table 2 provides more
Figure 4: Distribution of total number of annota-
tions per annotator for English LIT.
detailed information about distributions for num-
ber of annotations, as well as the average number
of units found, and average words per unit. This
information informs us as to how large or small
on average the meaning units are. Note that in Ta-
ble 2 we include information for overall English
UDHR and overall English LIT distributions for
reference. The authors found it interesting that,
from Table 2, the number of words per meaning
unit generally followed the 7 +/- 2 ?chunks? phe-
nomenon, where chunks are words.5.2 Annotator Agreement
Even though some of the annotators agreed about
the number of units, that does not imply that
they agreed on where the boundaries were placed.
We used segmentation similarity (S) as a metric
for annotator agreement. The algorithm requires
specifying a unit of measurement between bound-
aries ? in our case we used word-level units for
English data and character-level units for Chinese
data. We calculated average similarity agreement
for segment boundaries pair-wise within-group
for annotators from each of the 9 language/genre
datasets, as presented in Table 3.
While the segmentation similarity agreements
seem to indicate high annotator agreement, we
wanted to find out if that agreement was bet-
ter than what we could generate at random, so
we compared annotator agreement with random
baselines. To generate the baselines, we used
the average number of segments per paragraph in
each language/genre dataset and inserted bound-
aries at random. For each of the 9 language/genre
datasets, we generated 30 baseline samples. We
calculated the baseline segmentation similarity
5
Dataset (S) (SBL)
Chinese UDHR 0.930 0.848
English NEWS 0.891 0.796
English LIT 0.875 0.790
English LIT G1 0.929 0.824
English LIT G2 0.799 0.727
English UDHR 0.870 0.802
English UDHR G1 0.929 0.848
English UDHR G2 0.910 0.836
English UDHR G3 0.826 0.742
Table 3: Within-group segmentation similarity
agreement (S) and segmentation similarity agree-
ment for random baseline (SBL).
(SBL) in the same way using average pair-wise
agreement within-group for all of the baseline
datasets, shown in Table 3.
For English UDHR, we also calculated average
pair-wise agreement across groups, shown in Ta-
ble 4. For example, we compared English UDHR
G1 with English UDHR G2, etc. Human annota-
tors consistently outperformed the baseline across
groups for English UDHR.Dataset (S) (SBL)
English UDHR G1?G2 0.916 0.847
English UDHR G1?G3 0.853 0.782
English UDHR G2?G3 0.857 0.778
Table 4: English UDHR across-group segmenta-
tion similarity agreement (S) and random baseline
(SBL).6 Analysis
Constructing concepts in this task is systematic
as was shown from the segmentation similarity
scores. Since we know that the annotators agreed
on some things, it is important to find out what
they have agreed on. In our analysis, we exam-
ined unit boundary locations across genres in addi-
tion to phrase structure using constituency parses.
In this section, we begin to address another of
our original research questions regarding how well
speakers agree on meaning unit boundary posi-
tions across genres and which syntactic features
are the most salient for meaning units.
6.1 Unit Boundary Positions for Genres
Boundary positions are interesting because they
can potentially indicate if there are salient parts
of the texts which stand out to annotators across
genres. We have focused this analysis across gen-
res for the overall data for each of the 4 lan-
guage/genre pairs. Therefore, we have omitted the
subgroups ? English UDHR groups (G1,G2, G3)
and English LIT groups (G1, G2). Although seg-
mentation similarity is greater within-group from
Table 3, this was not enough to inform us of which
boundaries annotators fully agree on. For each of
the datasets, we counted the number of annotators
who agreed on a given boundary location and plot-
ted histograms. In these plots we show the number
of annotators of each potential boundary between
words. We show the resulting distributions in Fig-
ures 5 ? 8.
Figure 5: Annotated boundary positions Chinese
UDHR.
Figure 6: Annotated boundary positions English
UDHR.
While there were not many annotators for the
Chinese UDHR data, we can see from Figure 5
6
Figure 7: Annotated boundary positions English
NEWS.
Figure 8: Annotated boundary positions English
LIT.
that at most 4 annotators agreed on boundary po-
sitions. We can see from Figures 6 ? 8 that there
is high frequency of agreement in the text which
corresponds to paragraph boundaries for the En-
glish data, however paragraph boundaries were ar-
tificially introduced into the experiment because
each paragraph was numbered.
Since we had removed all punctuation mark-
ings, including periods and commas for both lan-
guages, it is interesting to note there was not full
agreement about sentence boundaries. While we
did not ask annotators to mark sentence bound-
aries, we hoped that these would be picked up by
the annotators when they were constructing mean-
ing units in the text. Only 3 sentence boundaries
were identified by at most 2 Chinese UDHR an-
notators. On the other hand, all of the sentence
boundaries were idenfied for English UDHR and
English NEWS, and one sentence boundary was
unmarked for English LIT. However, there were
no sentence boundaries in the English data that
were marked by all annotators - in fact the sin-
gle most heavily annotated sentence boundary was
for English NEWS, where 30% of the annota-
tors marked it. The lack for identifying sentence
boundaries could be due to an oversight by anno-
tators, or it could also be indicative of the difficulty
and ambiguity of the task.6.2 Phrase Structure
To answer our question of whether or not there are
salient syntactic features for meaning units, we did
some analysis with constituency phrase structure
and looked at the maximal projections of meaning
units. For each of the 3 English genres (UDHR,
NEWS, and LIT) we identified boundaries where
at least 50% of the annotators agreed. For the Chi-
nese UDHR data, we identified boundaries where
at least 30% of annotators agreed. We used the
Stanford PCFG Parser on the original English and
Chinese text to obtain constituency parses (Klein
& Manning, 2003), then aligned the agreeable
segment boundaries with the constituency parses.
We found the maximal projection corresponding
to each annotated unit and we calculated the fre-
quency of each of the maximal projections. The
frequencies of part-of-speech for maximal projec-
tions are shown in Tables 5 - 8. Note that the part-
of-speech tags reflected here come from the Stan-
ford PCFG Parser.Max. Projection Description Freq.S, SBAR, SINV Clause 28PP Prepositional Phrase 14VP Verb Phrase 11NP Noun Phrase 5ADJP Adjective Phrase 3ADVP Adverb Phrase 1
Table 5: Frequency of maximal projections for En-
glish UDHR, for 62 boundaries.Max. Projection Description Freq.S, SBAR, SINV Clause 30VP Verb Phrase 23NP Noun Phrase 11PP Prepositional Phrase 3ADVP Adverb Phrase 2
Table 6: Frequency of maximal projections for En-
glish NEWS, for 69 boundaries.
7
Max. Projection Description Freq.S, SBAR Clause 32VP Verb Phrase 10NP Noun Phrase 3PP Prepositional Phrase 2ADVP Adverb Phrase 2
Table 7: Frequency of maximal projections for En-
glish LIT, for 49 boundaries.Max. Projection Description Freq.NN, NR Noun 22VP Verb Phrase 8NP Noun Phrase 8CD Determiner 3ADVP Adverb Phrase 1AD Adverb 1VV Verb 1JJ Other noun mod. 1DP Determiner Phrase 1
Table 8: Frequency of maximal projections for
Chinese UDHR, for 46 boundaries.
Clauses were by far the most salient bound-
aries for annotators of English. On the other hand,
nouns, noun phrases, and verb phrases were the
most frequent for annotators of Chinese. There
is some variation across genres for English. This
analysis begins to address whether or not it is
possible to identify syntactic features of meaning
units, however it leaves open another question as
to if it is possible to automatically identify a 1-to-1
mapping of concepts across languages.7 Discussion and Future Work
We have presented an experimental framework
for examining how English and Chinese speakers
make meaning out of text by asking them to la-
bel places that they could construct concepts with
as few words as possible. Our results show that
there is not a unique ?meaning unit? segmentation
criteria among annotators. However, there seems
to be some preferential trends on how to perform
this task, which suggest that any random segmen-
tation is not acceptable. As we have simplified the
task of meaning unit identification by using well-
structured text from the Universal Declaration of
Human Rights, news, and literature, future work
should examine identifying meaning units in tran-
scribed speech.
Annotators for the English UDHR and English
LIT datasets could be characterized by their dif-
ferent granularities of annotation in terms of num-
ber of units identified. These observations are in-
sightful to our first question: what granularity do
people use to construct meaning units? For some,
meaning units consist of just a few words, whereas
for others they consist of longer phrases or possi-
bly clauses. As we did not have enough responses
for the Chinese UDHR data, we are unable to com-
ment if identification of meaning units in Chinese
fit a similar distribution as with English and we
leave in-depth cross-language analysis to future
work.
A particularly interesting finding was that hu-
man annotators share agreement even across
groups, as seen from Table 4. This means that al-
though annotators may not agree on the number of
meaning units found, they do share some agree-
ment regarding where in the text they are creating
the meaning units. These findings seem to indicate
that annotators are creating meaning units system-
atically regardless of granularity.
Our findings suggest that different people orga-
nize and process information differently. This is a
very important conclusion for discourse analysis,
machine translation and many other applications
as this suggests that there is no optimal solution
to the segmentation problems considered in these
tasks. Future research should focus on better un-
derstanding the trends we identified and the ob-
served differences among different genres. While
we did not solicit feedback from annotators in this
experiment, we believe that it will be important
to do so in future work to improve the annota-
tion task. We know that the perceived lag time in
speech-to-speech translation cannot be completely
eliminated but we are interested in systems that are
?fast? enough for humans to have quality conver-
sations in different languages.Acknowledgments
This work was partly supported by Singapore
Agency for Science, Technology and Research
(A-STAR) and the Singapore International Pre-
Graduate Award (SIPGA) and was partly sup-
ported by the National Science Foundation (NSF)
award IIS-1225629. Any opinions expressed in
this material are those of the authors and do not
necessarily reflect the views of A-STAR and NSF.
8
References
Chang Baobao, Pernilla Danielsson, and Wolfgang
Teubert. 2002. Extraction of translation units from
Chinese-English parallel corpora. In Proceedingsof the first SIGHAN workshop on Chinese languageprocessing - Volume 18 (SIGHAN ?02), 1?5.
Presentacio?n Padilla Ben??tez and Teresa Bajo. 1998.
Hacia un modelo de memoria y atencio?n en inter-
pretacio?n simulta?nea. Quaderns. Revista de tra-duccio?, 2:107?117.
Chris Fournier and Diana Inkpen. 2012. Segmenta-
tion and similarity agreement. In Proceedings ofthe 2012 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies (NAACL HLT ?12),
Montreal, Canada, 152?161.
Osamu Furuse and Hitashi Iida. 1996. Incremental
translation utilizing constituent boundary patterns.
In Proceedings of the 16th conference on Computa-tional linguistics (COLING ?96), Copenhagen, Den-
mark, 412?417.
Olivier Hamon, Christian Fgen, Djamel Mostefa, Vic-
toria Arranz1, Munstin Kolss, Alex Waibel, and
Khalid Choukri. 2009. End-to-End Evaluation in
Simultaneous Translation. In Proceedings of the12th Conference of the European Chapter of theAssociation for Computational Linguistics, (EACL?09), Athens, Greece, 345?353.
Daniel Jurafsky. 1988. Issues in relating syntax and
semantics. In Proceedings of the 12th Internationalconference on Computational Linguistics (COLING?88), Budapest, Hungary, 278?284.
Frank Keller. 2010. Cognitively plausible models of
human language processing. In Proceedings of theACL 2010 Conference Short Papers, Uppsala, Swe-
den, 60?67.
Walter Kintsch. 2005. An Overview of Top-down and
Bottom?up Effects in Comprehension: The CI Per-
spective. Discourse Processes, 39(2&3):125?128.
Walter Kintsch and Praful Mangalath. 2011. The Con-
struction of Meaning. Topics in Cognitive Science,
3:346?370.
Dan Klein and Christopher D. Manning 2003. Ac-
curate Unlexicalized Parsing. In Proceedings of the41st Meeting of the Association for ComputationalLinguistics, 423?430.
Ja?chym Kola?r?. 2008. Automatic Segmentation ofSpeech into Sentence-like Units. Ph.D. thesis, Uni-
versity of West Bohemia, Pilsen, Czech Republic.
Patrik Lambert, Adria`. De Gispert, Rafael Banchs, and
Jose? B. Marin?o. 2005. Guidelines for Word Align-
ment Evaluation and Manual Alignment. LanguageResources and Evaluation (LREC), 39:267?285.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.
2012. Fully automatic semantic MT evaluation. InProceedings of the Seventh Workshop on StatisticalMachine Translation (WMT ?12), Montreal, Canada
243?252.
George A. Miller. 1956. The Magical Number Seven,
Plus or Minus Two: Some Limits on Our Capacity of
Processing Information. The Psychological Review,
Vol 63:81?97.
Hideki Mima, Hitoshi Iida, and Osamu Furuse. 1998.
Simultaneous interpretation utilizing example-based
incremental transfer. In Proceedings of the 17th In-ternational Conference on Computational Linguis-tics (COLING ?98) Montreal, Quebec, Canada, 855?
861.
Pierre Ole?ron and Hubert Nanpon. 1965. Recherches
sur la traduction simultane?e. Journal de PsychologieNormale et Pathologique, 62(1):73?94.
Mathais Paulik and Alex Waibel. 2009. Automatic
Translation from Parallel Speech: Simultaneous In-
terpretation as MT Training Data. IEEE Workshopon Automatic Speech Recognition and Understand-ing, Merano, Italy, 496?501.
Lev Pevzner and Marti A. Hearst 2002. A critique and
improvement of an evaluation metric for text seg-
mentation. Computational Linguistics, 28(1):1936.
MIT Press, Cambridge, MA, USA.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
H. Mar- tin, and Dan Jurafsky. 2004. Shallow Se-
mantic Parsing Using Support Vector Machines. InProceedings of the 2004 Conference on Human Lan-guage Technology and the North American Chap-ter of the Association for Computational Linguistics(HLT-NAACL-04).
Baskaran Sankaran, Ajeet Grewal, and Anoop Sarkar.
2010. Incremental Decoding for Phrase-based Sta-
tistical Machine Translation. In Proceedings of theJoint 5th Workshop on Statistical Machine Transla-tion and Metrics (MATR), Uppsala, Sweden, 222?
229.
Teresa M. Signorelli, Henk J. Haarmann, and Loraine
K. Obler. 2011. Working memory in simultaneous
interpreters: Effects of task and age. InternationalJournal of Billingualism, 16(2): 192?212.
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, Qun
Liu. 2003. HHMM-based Chinese Lexical An-
alyzer ICTCLAS. In Proceedings of the SecondSIGHAN Workshop on Chinese Language Process-ing (SIGHAN ?03) - Volume 17, Sapporo, Japan,
184-187.
9
