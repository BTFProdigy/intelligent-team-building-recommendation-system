Proceedings of the SIGDIAL 2013 Conference, pages 324?328,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
In-Context Evaluation of Unsupervised Dialogue Act Models  for Tutorial Dialogue 
  Aysu Ezen-Can Department of Computer Science North Carolina State University Raleigh, North Carolina 27695 aezen@ncsu.edu 
Kristy Elizabeth Boyer Department of Computer Science North Carolina State University Raleigh, North Carolina 27695 keboyer@ncsu.edu     Abstract Unsupervised dialogue act modeling holds great promise for decreasing the develop-ment time to build dialogue systems. Work to date has utilized manual annota-tion or a synthetic task to evaluate unsu-pervised dialogue act models, but each of these evaluation approaches has substan-tial limitations. This paper presents an in-context evaluation framework for an un-supervised dialogue act model within tuto-rial dialogue. The clusters generated by the model are mapped to tutor responses by a handcrafted policy, which is applied to unseen test data and evaluated by hu-man judges. The results suggest that in-context evaluation may better reflect the performance of a model than comparing against manual dialogue act labels. 1 Introduction A central focus within the dialogue systems re-search community is developing techniques for rapidly constructing dialogue systems. One tech-nique that has proven highly promising is to take a corpus-based approach to dialogue system au-thoring, for example by bootstrapping policy learning (Henderson, Lemon, & Georgila, 2008; Williams & Young, 2003), predicting what a human agent would do (Bangalore, Di Fabbrizio, & Stent, 2008), or learning supervised dialogue act models (Stolcke et al, 2000). Traditionally, these corpus-based approaches require some amount of manual annotation prior to learning the dialogue models.  In many cases, this manual annotation is a problematic bottleneck for system development.  
For tutorial dialogue systems, which aim to support students in acquiring skills or knowledge, heavy manual annotation is often required for learning models that classify student utterances with respect to dialogue acts (Forbes-Riley & Litman, 2005; Serafin & Di Eugenio, 2004), questioning strategies (Becker, Palmer, Vuuren, & Ward, 2012), or information sharing (Mayfield, Adamson, & Ros?, 2012) For dialogue act modeling in particular, recent work has demonstrated the great promise of un-supervised approaches, which are learned with-out the use of manual labels (Crook, Granell, & Pulman, 2009; Ezen-Can & Boyer, 2013; Ritter, Cherry, & Dolan, 2010). However, because gold standard labels are not a part of model learning, how to best evaluate unsupervised models repre-sents a significant open research question (Vlachos, 2011).  Most quantitative evaluations of unsupervised dialogue act models have relied on agreement with manual dialogue act annotations, though these annotations were not used in model learn-ing (Crook et al, 2009; Rus, Moldovan, Niraula, & Graesser, 2012; Ezen-Can & Boyer, 2013). Relying on manually tagged dialogue act labels to evaluate an unsupervised model has two major drawbacks: it does not fully avoid the manual annotation bottleneck, and it imposes a hand-authored criterion onto a fully data-driven model, which may be unnecessarily limiting. Distinc-tions made by an unsupervised model may be useful within a dialogue system, even if these categories are different from the distinctions made within a hand-authored dialogue act tagset.  This paper presents a novel evaluation framework for unsupervised dialogue act classi-fication of user utterances within tutorial dia-logue. Instead of attempting to evaluate the mod-el intrinsically, we evaluate its performance on 
324
an external task: triggering an appropriate utter-ance via a simple dialogue policy. This evalua-tion, which does not require an end-to-end dia-logue system, judges the model in the simulated context of the target task. The results demon-strate that this in-context evaluation may be equally useful as comparing against gold stand-ard dialogue act labels, while substantially reduc-ing the time required for human annotation.   2 Related Work Perhaps the earliest unsupervised approach for dialogue act modeling investigated hidden Mar-kov models with a bag-of-words approach in a meeting scheduling domain (Woszczyna & Waibel, 1994), using perplexity with respect to manual labels for evaluating the number of hid-den states. Dirichlet process clustering has been investigated for dialogue act classification in the train fares and scheduling domain (Crook et al, 2009), evaluating on intra-cluster similarity and inter-cluster similarity along with error rates with respect to manual labels. Another Bayesian ap-proach utilized hidden Markov models and topic modeling to classify Twitter posts (Ritter et al, 2010). Notably, Ritter et al utilize an utterance ordering task, rather than manual labels, for quantitative evaluation. Most recently, standard k-means and EM clustering algorithms were used for dialogue act clustering on an educational cor-pus, and the model?s accuracy was again evalu-ated with respect to manual labels (Rus et al, 2012). The current paper builds on these prior findings by applying a recently developed clus-tering framework and proposing a novel in-context evaluation scheme that can be used re-gardless of the unsupervised dialogue act model-ing technique underlying it. 3 Dialogue Act Clustering We consider an unsupervised dialogue act classi-fication model on a corpus of human-human stu-dent and tutor dialogues centered on a computer programming task within a textual dialogue envi-ronment (Boyer et al, 2009). There are 1,525 student utterances and 3,332 tutor utterances in the corpus. This paper focuses on dialogue act classification for student utterances, since in a tutorial dialogue system the tutor dialogue acts are system-generated.  The corpus was manually labeled in prior work with nine dialogue acts tailored to capture phenomena of interest within tutorial dialogue: general Question, Evaluation Question (request 
specific feedback on the task), Statement, Posi-tive Feedback, Lukewarm Feedback, Negative Feedback, Grounding, Greeting, and Extra-domain (utterances that are off topic). The Kappa for agreement on these manual tags was 0.76. These tags will be used within the present work to compare the in-context performance of the unsupervised policy with a manual-tag policy, but the tags are not used to learn or tune the un-supervised model. The unsupervised dialogue act model evaluat-ed here is based on a recently developed ap-proach that adapts the query-likelihood technique from information retrieval to rank utterances similar to each target utterance (Ezen-Can & Boyer, 2013). Each utterance within the training set is queried against all other utterances within the training set using bigram features.  Vectors encode the resulting utterance simi-larity, and these vectors are provided to a k-means clustering algorithm to partition the utter-ances into dialogue acts. Our recent work (Ezen-Can & Boyer, 2013) evaluated query-likelihood dialogue act clustering against two other ap-proaches with respect to classifying manual la-bels, and the query-likelihood approach outper-formed k-means clustering using leading tokens (Rus et al, 2012) and Dirichlet process cluster-ing (Crook et al, 2009). In the current work we add to the feature vectors the first level of the parse tree as provided by the Stanford parser (Klein & Manning, 2003).  The number of clusters was selected based on sum of squared errors (SSE). As with many pa-rameterized models, model fit tends to increase with more parameters, but there are important tradeoffs in computation time and risk of overfit-ting. In experiments, k=number of clusters ranged from 2 to 24. 21 clusters were chosen, corresponding to the rightmost ?knee? within the SSE graph (see Appendix).1 4 Evaluation Framework Evaluating unsupervised dialogue act clusters presents numerous challenges. In prior evalua-tions of query-likelihood clustering, we comput-ed accuracy with respect to the manually applied dialogue act tags described earlier, demonstrating 41.64% accuracy for a model with 8 clusters, compared to 34.90% accuracy for the Rus et al                                                 1 Selecting the number of clusters is a subjective deci-sion. Nonparametric techniques, such as variations on Dirichlet process clustering, hold promise for address-ing this limitation in the future. 
325
(2012) k-means approach and 24.48% accuracy for Dirichlet process clustering (Crook et al, 2009) on our corpus. However, the goal of the current work is to substantially reduce the human tagging required to evaluate the model. We also aim to test the hypothesis that comparing against manual labels under-represents the utility of the unsupervised model. That is, a dialogue policy built on the unsupervised model could perform better than the relatively low classification accu-racy for manual tags would suggest. Our evalua-tion will explore this hypothesis. In order to achieve these goals, we first trained an unsupervised dialogue act model on 75% of the corpus using the query-likelihood approach described in Section 3. The resulting model has 21 clusters. Then, we handcrafted a dialogue pol-icy for tutor responses by qualitatively examin-ing each cluster of training data and creating one tutor response for each cluster. Some clusters and their corresponding tutor utterances are depicted in Figure 1. This policy was applied by classify-ing unseen utterances from a held-out test set (25% of the corpus) using the learned model (Figure 2). The result of this process is that for each student utterance from the test set, a tutor response is generated based on the policy. This process resulted in 373 student utterances, one for each utterance in the 25% testing set, each paired with a corresponding tutor response gen-erated by the hand-authored policy. The evaluation goal is to determine whether the responses made by this policy are reasonable, which will represent the utility of the unsuper-vised dialogue act model for its intended use within a dialogue manager. We used human judges to rate the output of the policy. Thirty student utterances and tutor responses were ran-domly selected from the available utterances generated by the test set. An example set of ut-terances and policies can be seen in the Appen-dix. These items were placed in a survey that asked the reader to rate the extent to which each tutor response makes sense given the student ut-terance. (One item was inadvertently omitted from the survey, resulting in 29 items that were evaluated by the judges and that will be analyzed here.) To avoid bias introduced by the ordering of items, they were presented in a different ran-domized order for each of the seven judges who completed the survey. (29 items from a compari-son condition using manual tags were also ran-domly interleaved into the survey, as described later in this section.) Judges used a rating scale from 1 to 4 (1=makes no sense, 2=makes a little 
sense, 3=makes a lot of sense, and 4=makes per-fect sense). Since the models only used the cur-rent student utterance, the dialogue history was also not shown to the human raters.  Across the seven judges, the average rating of the tutor responses selected by the unsupervised policy was 2.35. We also collapsed the ratings into positive (?2.5 average across seven judges) and negative (<2.5 average). With this binary categorization, 44.8% of the time tutor responses generated by the unsupervised policy were rated positively. It is important to note that no infor-mation other than dialogue act was considered for generating the tutor responses; the tutor utter-ances were relatively content-free and based only on the dialogue act categorization given by the unsupervised model.               Figure 1: Clusters from unsupervised dialogue act modeling and corresponding dialogue policy  (typographical errors originated in corpus)  For comparison, we also constructed a hand-crafted dialogue policy using the manual dia-logue act labels and applied this policy to the same utterances as were used to evaluate the un-supervised model. These pairs of student utter-ances and tutor responses were interleaved ran-domly on the same survey provided to seven human judges. The same tutor responses as in the unsupervised policy were used whenever possi-ble for this manual-tag policy. The tutor respons-es generated from the manual-tag policy received an average score of 2.22, slightly lower than the average of 2.35 for tutor responses generated by the unsupervised policy. The binary positive-negative split for these ratings reveals that 31% were rated positively (?2.5 average), compared to 44.8% for the unsupervised policy. Direct comparisons between the unsupervised policy and the manual-tag policy must be inter-preted with caution, in part because the unsuper-vised policy was more granular (based on 21 
326
clusters) than the manual-tag policy (based on 9 tags) and also because it can be difficult to en-sure that the two policies were of equal quality. On the other hand, the unsupervised policy uti-lized no manual labels and was applied to an un-seen test set, while the manual-tag policy was based on reliable tags applied to the actual utter-ances from the testing set.              
 Finally, we evaluated the extent to which the 4-category rating scheme was reliable across judges. The weighted Kappa (Cohen, 1968), used for ordinal scales because it penalizes disagree-ments less if they are closer together, was 0.30 averaged across all pairs of judges, indicating fair agreement (Landis & Koch, 2013). For the collapsed binary ratings, average pairwise ordi-nary Kappa was 0.36. 5 Discussion It was hypothesized that evaluating an unsuper-vised dialogue act model against manual labels may be an inappropriately strict metric, requiring the model to conform to the criteria used by hu-mans to handcraft the manual tagset. Indeed, the accuracy of the unsupervised dialogue act model presented here with 21 clusters was 30.4% for identifying manual labels (arrived at by assigning the majority class tag to each unsupervised clus-ter after clustering was complete). The majority class baseline (most frequent student dialogue act tag) was Evaluation Question with a relative fre-quency of 25.87%, so on accuracy for identifying manual labels, the unsupervised model improved modestly over baseline. In contrast, when this unsupervised model was used to select a tutor response within a dialogue policy, the response was judged positively 44.8% of the time by hu-man judges. Moreover, recall that the tutor re-sponses were content free and took only the dia-
logue act label into account (no information state or topic). Therefore, it is meaningful to consider what percent of the time the responses were rated as making some sense (receiving a 2, 3, or 4 rat-ing average across the human judges). By this criterion, 65.5% of tutor responses selected by the unsupervised policy were rated as sensible.  Finally, this evaluation approach demon-strates promise for alleviating the bottleneck of manual annotation for dialogue act models. Each item within the current evaluation survey re-quired approximately 15 seconds to judge, using untrained human judges, for a total of approxi-mately 1 hour of effort across all seven judges. The time required for handcrafting policies was relatively small, approximately 1 hour. In con-trast, the dialogue act annotation scheme re-quired approximately 35 seconds per utterance (amortizing substantial up-front training time for each annotator) when applied as part of previous work, for a total of approximately 50 hours per annotator.  6 Conclusion Unsupervised dialogue act modeling holds great promise for decreasing development time of dia-logue systems. We have presented an unsuper-vised dialogue act model and an evaluation framework to judge the utility of the unsuper-vised model within a dialogue management task. The results demonstrate that in-context evalua-tion of an unsupervised dialogue act model, ra-ther than accuracy against manual labels, may better reflect the usefulness of the model for dia-logue management. Furthermore, this evaluation technique may greatly reduce the time required by human judges to evaluate the model.  One of the most promising directions for fu-ture work involves devising unsupervised dia-logue act models that leverage a richer represen-tation in order to perform better. These rich fea-tures may include dialogue history, adjacency pair information, and topic modeling. Addition-ally, it is important for the community to evalu-ate unsupervised dialogue models in the full con-text of deployed systems. Acknowledgments. This material is based upon work supported by the National Science Founda-tion under Grants DRL-1007962 and CNS-1042468. Any opinions, findings, conclusions, or recommendations expressed in this report are those of the authors and do not necessarily repre-sent the views of the National Science Founda-tion. 
Figure 2: Evaluation framework structure 
327
References  Bangalore, S., Di Fabbrizio, G., & Stent, A. (2008). Learning the Structure of Task-Driven Human-Human Dialogs. IEEE Transactions on Audio, Speech and Language Processing, 16(7), 1249?1259. Becker, L., Palmer, M., Vuuren, S. Van, & Ward, W. (2012). Learning to Tutor Like a Tutor: Ranking Questions in Context. Proceedings of the Inter-national Conference on Intelligent Tutoring Systems, 368?378. Boyer, K. E., Philips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2009). Modeling Dialogue Structure with Adjacency Pair Analysis and Hidden Markov Models. Proceedings of NAACL HLT, 49?52. Cohen, J. (1968). Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit. Psychological Bulletin, 70(4), 213-220. Crook, N., Granell, R., & Pulman, S. (2009). Unsupervised Classification of Dialogue Acts Using a Dirichlet Process Mixture Model. Proceedings of SIGDIAL, 341?348. Ezen-Can, A., & Boyer, K. E. (2013). Unsupervised Classification of Student Dialogue Acts With Query-likelihood Clustering. International Conference on Educational Data Mining, 20?27. Forbes-Riley, K., & Litman, D. J. (2005). Using Bigrams to Identify Relationships Between Student Certainness States and Tutor Responses in a Spoken Dialogue Corpus. Proceedings of SIGDIAL, 87?96.  Henderson, J., Lemon, O., & Georgila, K. (2008). Hybrid Reinforcement / Supervised Learning of Dialogue Policies from Fixed Data Sets. Computational Linguistics, 34(4), 487-511. Klein, D., & Manning, C. D. (2003). Accurate Unlexicalized Parsing. Proceedings of ACL, 423?430. Landis, J. R., & Koch, G. G. (1994). The Measurement of Observer Agreement for Categorical Data Data for Categorical of Observer Agreement The Measurement. International Biometric Society, 33(1), 159?174. Mayfield, E., Adamson, D., & Ros?, C. P. (2012). Hierarchical Conversation Structure Prediction in Multi-Party Chat. Proceedings of SIGDIAL, 60?69. Ritter, A., Cherry, C., & Dolan, B. (2010). Unsupervised Modeling of Twitter Conversations. Proceedings of NAACL HLT, 172?180. Rus, V., Moldovan, C., Niraula, N., & Graesser, A. C. (2012). Automated Discovery of Speech Act 
Categories in Educational Games. Proceedings of the International Conference on Educational Data Mining, 25-32. Serafin, R., & Di Eugenio, B. (2004). FLSA??: Extending Latent Semantic Analysis with features for dialogue act classification. Proceedings of ACL, 692?699. Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., Taylor, P., et al (2000). Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech. Computational Linguistics, 26(3), 339?373. Vlachos, A. (2011). Evaluating unsupervised learning for natural language processing tasks. Proceedings of EMNLP, 35?42. Williams, J. D., & Young, S. (2003). Using Wizard-of-Oz simulations to bootstrap Reinforcement-Learning- based dialog management systems. Proceedings of SIGDIAL, 135?139. Woszczyna, M., & Waibel, A. (1994). Inferring linguistic structure in spoken language. Proceedings of ICSLP, 847-850. Appendix 
 Figure 3: Sum of squared errors graph  Table 1: Example student utterances  and tutor responses Student  Utterance Tutor Response (Unsupervised Policy) Tutor Re-sponse (Manu-al-Tag Policy) I'm trying to think, heh                                                                                                                                      Don't worry about it. I'm here to help. That part is tricky. how can I pull values out of an array or can I reference them with code like zipDigits[1]?                                                                                                                                                                       
Great question. Let's look at the task decription together to see if it can help. 
Good question. Let's analyze the code together to see if it is right. 
thanks for the reminder                                                                                                                                                                                                                                              I'm here to help! Great, seems like we agreed.  does that mean I should declare it at the top of the code?                                                                                                                                                             
Maybe you should try it out. Good question. Let's analyze the code together to see if it is right. 
328
Proceedings of the SIGDIAL 2014 Conference, pages 113?122,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
Combining Task and Dialogue Streams in
Unsupervised Dialogue Act Models
Aysu Ezen-Can and Kristy Elizabeth Boyer
Department of Computer Science
North Carolina State University
aezen,keboyer@ncsu.edu
Abstract
Unsupervised machine learning ap-
proaches hold great promise for recog-
nizing dialogue acts, but the performance
of these models tends to be much lower
than the accuracies reached by supervised
models. However, some dialogues, such
as task-oriented dialogues with parallel
task streams, hold rich information that
has not yet been leveraged within unsu-
pervised dialogue act models. This paper
investigates incorporating task features
into an unsupervised dialogue act model
trained on a corpus of human tutoring in
introductory computer science. Exper-
imental results show that incorporating
task features and dialogue history fea-
tures significantly improve unsupervised
dialogue act classification, particularly
within a hierarchical framework that gives
prominence to dialogue history. This
work constitutes a step toward building
high-performing unsupervised dialogue
act models that will be used in the next
generation of task-oriented dialogue
systems.
1 Introduction
Dialogue acts represent the underlying intent of ut-
terances (Austin, 1975; Searle, 1969), and consti-
tute a crucial level of representation for dialogue
systems (Sridhar et al., 2009). The task of auto-
matic dialogue act classification has been exten-
sively studied for decades within several domains
including train fares and timetables (Allen et al.,
1995; Core and Allen, 1997; Crook et al., 2009;
Traum, 1999), virtual personal assistants (Chen
and Di Eugenio, 2013), conversational telephone
speech (Stolcke et al., 2000), Wikipedia talk pages
(Ferschke et al., 2012) and as in the case of this
paper, tutorial dialogue (Serafin and Di Eugenio,
2004; Forbes-Riley and Litman, 2005; Boyer et
al., 2011; Dzikovska et al., 2013).
Most of the prior work on dialogue act classi-
fication has depended on manually applying dia-
logue act tags and then leveraging supervised ma-
chine learning (Di Eugenio et al., 2010; Keizer
et al., 2002; Reithinger and Klesen, 1997; Ser-
afin and Di Eugenio, 2004). This process involves
engineering a dialogue act taxonomy (or using an
existing one, though domain-specific phenomena
can be difficult to capture within multi-purpose di-
alogue act taxonomies) and manually annotating
each utterance in the corpus. Then, the tagged
utterances are provided to a supervised machine
learner. This supervised approach can achieve
strong performance, in excess of 75% accuracy
on manual tags, approaching the agreement level
that is sometimes observed between human anno-
tators (Sridhar et al., 2009; Serafin and Di Euge-
nio, 2004; Chen and Di Eugenio, 2013).
However, the supervised approach has several
major drawbacks, including the fact that hand-
crafting dialogue act tagsets and applying them
manually tend to be bottlenecks within the re-
search and design process. To overcome these
drawbacks, the field has recently seen growing
momentum surrounding unsupervised approaches,
which do not require any manual labels during
model training (Crook et al., 2009; Joty et al.,
2011; Lee et al., 2013). A variety of unsupervised
machine learning techniques have been investi-
gated for dialogue act classification, and each line
of investigation has explored which features best
support this goal. However, to date the best per-
forming unsupervised models achieve in the range
of 40% (Rus et al., 2012) to 60% (Joty et al., 2011)
training set accuracy on manual tags, substantially
lower than the mid-70% accuracy (Sridhar et al.,
2009) often achieved on testing sets with super-
vised models.
113
In order to close this performance gap between
unsupervised and supervised techniques, we sug-
gest that it is crucial to enrich the features available
to unsupervised models. In particular, when a di-
alogue is task-oriented and includes a rich source
of information within a parallel task stream, these
features may substantially boost the ability of an
unsupervised model to distinguish dialogue acts.
For example, in situated dialogue, features rep-
resenting the state of the physical world may
be highly influential for dialogue act modeling
(Grosz and Sidner, 1986).
Human tutorial dialogue, which is the domain
being considered in the current work, often ex-
hibits this structure: the task artifact is external to
the dialogue utterances themselves (in the case of
our work, this artifact is a computer program that
the student is constructing). Task features have
already been shown beneficial for supervised di-
alogue act classification in our domain (Ha et al.,
2012). We hypothesize that including these task
features within an unsupervised model will signif-
icantly improve its performance. In addition, we
hypothesize that including dialogue history as a
prominent feature within an unsupervised model
will provide significant improvement.
This paper represents the first investigation into
combining task and dialogue features within an
unsupervised dialogue act classification model.
First, we discuss representation of these task fea-
tures and dialogue structure features, and compare
these representations within both flat and hierar-
chical clustering approaches. Second, we report
on experiments that demonstrate that the inclusion
of task features significantly improves dialogue
act classification, and that a hierarchical cluster
structure which explicitly captures dialogue his-
tory performs best. Finally, we break down the
model?s performance by dialogue act and investi-
gate which features are most beneficial for distin-
guishing particular acts. These contributions con-
stitute a step toward building high-performing un-
supervised dialogue act models that can be used in
the next generation of task-oriented dialogue sys-
tems.
2 Related Work
There is a rich body of work on dialogue act clas-
sification. Supervised approaches for dialogue act
classification aimed at improving performance by
using several features such as dialogue structure
including position of the turn (Ferschke et al.,
2012), speaker of an utterance (Tavafi et al., 2013),
previous dialogue acts (Kim et al., 2010), lexical
features such as words (Stolcke et al., 2000), syn-
tactic features including part-of-speech tags (Ban-
galore et al., 2008; Marineau et al., 2000), task-
subtask structure (Boyer et al., 2010) acoustic and
prosodic cues (Sridhar et al., 2009; Jurafsky et al.,
1998), and body posture (Ha et al., 2012).
For the growing body of work in unsupervised
dialogue act classification a subset of these fea-
tures have been utilized. The words (Crook et
al., 2009), topic words (Ritter et al., 2010), func-
tion words (Ezen-Can and Boyer, 2013b), begin-
ning portions of utterances (Rus et al., 2012), part-
of-speech tags and dependency trees (Joty et al.,
2011), and state transition probabilities in Markov
models (Lee et al., 2013) are among the list of
features investigated for unsupervised modeling of
dialogue acts. However, the accuracies achieved
by the best of these models are well below the ac-
curacies achieved by supervised techniques. To
improve performance of unsupervised models for
task-oriented dialogue, utilizing a combination of
task and dialogue features is a promising direction.
3 Corpus
The task-oriented dialogue corpus used in this
work was collected in a computer-mediated hu-
man tutorial dialogue study. Students (n =
42) and tutors interacted through textual dialogue
within an online learning environment for intro-
ductory Java programming (Ha et al., 2012). The
students were novices, never having programmed
in Java previously. The tutorial dialogue inter-
face consisted of four windows, one describing the
learning task, another where students wrote pro-
gramming code, beneath that the output of either
compiling or executing the program, and finally
the textual dialogue window (Figure 1).
As students and tutors interacted through this
interface, all dialogue messages and keystroke-
level task events were logged to a database. Only
students could compose, compile, and execute the
code, so task actions represent student actions
while dialogue messages were composed by both
participants. The corpus contains six lessons for
each student-tutor pair, of which only the first les-
son was annotated with dialogue act tags (?=0.80).
This annotated set contains 5,705 utterances
(4,065 tutor and 1,640 student). The average num-
114
Figure 1: The tutorial dialogue interface with four
windows.
ber of utterances (both tutor and student) per tutor-
ing session was 116 (min = 70, max = 211). The
average number of tutor utterances per session is
96 (min=44, max=156) whereas for students it is
39 (min=18, max=69) for the annotated set. The
average number of words per utterance for stu-
dents is 4.4 and for tutors it is 5.4. This annotated
set is used in the current analysis for both training
and testing where cross-validation is applied. As
described later, a separate set containing 462 un-
annotated utterances is used as a development set
for determining the number of clusters.
The dialogue stream of this corpus was manu-
ally annotated as part of previous work on super-
vised dialogue act modeling which achieved 69%
accuracy with Conditional Random Fields (Ha et
al., 2012). A brief description of the student di-
alogue act tags, which are the focus of the mod-
els reported in this paper, is shown in Table 1.
The most frequent dialogue act (A) constitutes the
baseline chance (39.85%). In the current work, the
manually applied dialogue act labels are not uti-
lized during model training, but are only used for
evaluation purposes as our models? accuracies are
reported for manual tags on a held-out test set.
An excerpt from the corpus is shown in Table 2.
Note that the current work focuses on classifying
student dialogue act tags, since in an automated di-
alogue system the tutor moves would be generated
by the system and their dialogue acts tags would
therefore be known.
4 Features
A key issue for dialogue act classification in task-
oriented dialogue involves how to represent dia-
Student Dialogue Act Distribution
Answer (A) 39.85
Acknowledgement (ACK) 21.31
Statement (S) 21.20
Question (Q) 15.15
Request for Feedback (RF) 0.98
Clarification (C) 0.79
Other (O) 0.61
Table 1: Student dialogue act tags and their fre-
quencies.
Tutor: ready? [Q]
Student: yep [A]
Tutor moves on to next task
Student: cool [S]
Student compiles and runs the code.
Program output: ?Hello World?
Tutor: excellent [PF]
Tutor: add a space to make the output look
prettier [DIR]
Student: why doesnt it stop on the next line
in this case? [Q]
Program halts
Tutor: it did [A]
Student runs the program successfully.
Tutor: good. [PF]
Table 2: Excerpt of dialogue from the corpus and
the task action that follows utterances.
logue and task events. This section describes how
features were extracted from the corpus of human
tutorial dialogue.
We use three sets of features: lexical features,
dialogue context features, and task features. The
lexical and dialogue context features are extracted
from the textual dialogue utterances within the
corpus. The task features are extracted from the
interaction traces within the computer-mediated
learning environment and represent a keystroke-
level log of events as students worked toward solv-
ing the computer programming problems.
4.1 Lexical Features
Because one of the main goals of our work in the
longer term is to perform automatic dialogue act
classification in real time, we took as a primary
consideration the ability to quickly extract lexical
features. The features utilized in the current in-
vestigation consist only of word unigrams. In ad-
115
dition to their ease of extraction, our prior work
has shown that addition of part-of-speech tags and
and syntax features did not significantly improve
the accuracy of supervised dialogue act classifiers
in this domain (Boyer et al., 2010), and these fea-
tures can be time-consuming to extract in real time
(Ha et al., 2012).
The choice to use word unigrams rather than
higher order n-grams is further facilitated by the
fact that our clustering technique leverages the
longest common sub-sequence (LCS) metric to
measure distances between utterances. This met-
ric counts shared sub-sequences of not-necessarily
contiguous words (Hirschberg, 1975). In this way,
the LCS metric provides a flexible way for n-
grams and skip-n-grams to be treated as impor-
tant units within the clustering, while the raw fea-
tures themselves consist only of word unigrams.
(We report on a comparison between LCS and bi-
grams later in the discussion section.) Utilizing
LCS, there exists a distance (1-similarity) value
from each utterance to every other utterance.
4.2 Dialogue Context Features
Based on previous work on a similar human tuto-
rial dialogue corpus (Ha et al., 2012), we utilize
four features that provide information about the di-
alogue structure. These features are depicted in
Table 3. Note that our goal within this work is to
classify student dialogue moves, not tutor moves,
because in a dialogue system the tutor?s moves are
system-generated with associated known dialogue
acts.
Feature Description
Utterance
position
The relative position of an
utterance from the beginning of
the dialogue.
Utterance
length
The number of tokens in the
utterance, including words and
punctuation.
Previous
author
Author of the previous dialogue
message (tutor or student) at the
time message sent.
Previous
tutor
dialogue act
Dialogue act of the previous
tutor utterance.
Table 3: Dialogue context features and their de-
scriptions.
4.3 Task Features
As described previously, the corpus contains two
channels of information: the dialogue utterances,
from which the lexical and dialogue context fea-
tures were extracted, and in addition, the task
stream consisting of student problem-solving ac-
tivities such as authoring code, compiling, and ex-
ecuting the program. The programming activities
of students were logged to a database along with
all of the dialogue events during tutoring.
A set of task features was found to be impor-
tant for dialogue act classification in this domain
in prior work, including most recent programming
action, status of the most recent task activity and
task activity flag representing whether the utter-
ance was preceded by a student?s task activity (Ha
et al., 2012). We expand this set of features as
shown in Table 4.
5 Experiments
The goal of this work is to investigate the im-
pact of including task and dialogue context fea-
tures on unsupervised dialogue act models. We
hypothesize that incorporating task features will
significantly improve the performance of an un-
supervised model, and we also hypothesize that
properly incorporating dialogue context features,
which are at a different granularity than the lex-
ical features extracted from utterances, will sub-
stantially improve model accuracy.
5.1 Dialogue Act Modeling With k-medoids
Clustering
The unsupervised models investigated here use k-
medoids clustering, which is a well-known clus-
tering technique that takes actual data points as
the center of each cluster (Ng and Han, 1994),
in contrast to k-means which may have synthetic
points as centroids. In k-medoids, the centroids
are initially selected and then the algorithm iter-
ates, reassigning data points in each iteration, un-
til the clusters converge. In standard k-medoids
clustering the initial seeds are selected randomly
and then a correct distribution of data points is
identified through the iteration and convergence
process. For dialogue act classification, the in-
fluence of the initial seeds is substantial because
the frequencies across dialogue tags are typically
unbalanced. To overcome this challenge, we use
a greedy seed selection approach similar to the
one used in k-means++ (Arthur and Vassilvitskii,
116
Feature Description
prev action
Most recent action of the
student (composing a dialogue
utterance, constructing code,
compiling or executing code).
task begin
Whether the student utterance is
the first utterance since the
beginning of the subtask.
task stu
Whether the student utterance
was preceded by a task event.
task prev tut
Task activity flag indicating
whether the closest tutor
utterance in this subtask was
preceded by a task activity.
task status
The status of the most recent
coding action (begin, stop,
success, error and input sent).
time elapsed
Time elapsed between the
previous tutor message and the
current student utterance.
errors
Number of errors in the
student?s latest code.
delta errors
Difference in the number of
errors in the task between two
utterances in the same dialogue.
stu # task
Number of student dialogue
messages sent within the current
task.
stu # dial
Number of student dialogue
messages sent within the current
dialogue.
tut # task
Number of tutor dialogue
messages sent within the current
subtask.
tut # dial
Number of tutor dialogue
messages sent within the current
dialogue.
Table 4: Task features extracted from student com-
puter programming activities.
2007) which selects the first seed randomly and
then greedily chooses seeds that are farthest from
the chosen seeds. The goal of using this approach
in our application is to choose seeds from different
dialogue acts so that the final model achieves good
coverage. Our preliminary experiments demon-
strated that this greedy seed selection combined
with k-medoids outperforms other clustering ap-
proaches including those utilized in our prior work
(Ezen-Can and Boyer, 2013a).
In order to select the number of clusters k,
a subset of the corpus, constituting 25% of the
full corpus (that were not tagged) composed of
462 utterances, was separated as a development
set. First, we examined the coherence of clus-
ters at different values of k using intra-cluster dis-
tances. This technique involves identifying an ?el-
bow? where the decrease in intra-cluster distance
becomes less rapid (since adding more clusters can
continue to decrease intra-cluster distance to the
point of overfitting) (Figure 2). The graph sug-
gests an elbow at k=5. Because there may be mul-
tiple elbows in the intra-cluster distance, a sec-
ond method utilizing Bayesian Information Crite-
rion (BIC) was used which penalizes models as
the number of parameters increases. The lower the
BIC value, the better the model is, achieved at k=5
as well.
Figure 2: Intra-cluster distances with varying
number of clusters.
Unlike many other investigations into unsuper-
vised dialogue act classification, the current ap-
proach reports accuracy on held-out test data, not
on the data on which the model was trained. Even
though the model training process does not utilize
available manual tags, requiring the learned unsu-
pervised model to perform well on held-out test
data more closely mimics the broader goal of our
work which is to utilize these unsupervised mod-
els within deployed dialogue systems, where most
utterances to be classified have never been encoun-
tered by the model before.
The procedure for model training and test-
ing uses leave-one-student-out cross-validation.
Rather than other forms of leave-one-out or strat-
ified cross-validation, leave-one-student-out en-
sures that each student?s set of dialogue utterances
are treated as the testing set while the model is
trained on all other students? utterances. This
process is repeated until each student?s utterances
117
have served as a held-out test set (in our case, this
results in n=42 folds). Within each fold, the clus-
ters are learned during training and then for each
utterance in the test set, its closest cluster is com-
puted by taking the average distance of the test ut-
terance to the elements in the cluster. The majority
label of the closest cluster is assigned as the dia-
logue act tag for the test utterance. If the assigned
dialogue act tag matches the manual label of the
test utterance, the utterance is counted as correct
classification. The average accuracy is computed
as the number of correct classifications divided by
the total number of classifications.
5.2 Experimental Results
We conducted experiments with seven different
feature combinations: L, lexical features only,
T , task features only, D, dialogue context fea-
tures only, and then the combinations of these fea-
tures, T + D, T + L, D + L, and T + D + L.
We hypothesized that the addition of task features
would significantly improve the models? accuracy.
As shown in Table 5, adding task features to di-
alogue context features significantly outperforms
dialogue context features alone (T + D > D).
Similarly, adding task features to lexical features
provides significant improvement (T + L > L).
However, adding task features to the dialogue con-
text plus lexical features model does not provide
benefit, and in fact slightly (not significantly) de-
grades performance (T + D + L 6> D + L). As
reflected by the Kappa scores, the test set perfor-
mance attained by these models is hardly better
than would be expected by chance.
Features
Accuracy
(%)
Kappa
F
l
a
t
C
l
u
s
t
e
r
i
n
g
L 33 0.02
T 37.7 0.07
D 37.6 0.07
T+D 39.1* 0.07
T+L 38* 0.06
D+L 38.3 0.07
T+D+L 37.3 0.05
Table 5: Test set accuracies and Kappa for the flat
clustering model (L: Lexical features, D: Dialogue
context features, T: Task features) *indicates sta-
tistically significant compared to the similar model
without task features (p < 0.05).
5.3 Utilizing Dialogue History
The importance of dialogue history, particularly
the influence of the most recent turn on an upcom-
ing turn, is widely recognized within dialogue re-
search, notably by work on adjacency pairs (Sche-
gloff and Sacks, 1973; Forbes-Riley et al., 2007;
Midgley et al., 2009). Based on these findings, we
hypothesized that dialogue history would be sub-
stantially beneficial for unsupervised dialogue act
models as it has been observed to be in numer-
ous studies on supervised classification. However,
as seen in the previous section, adding these di-
alogue context features with equal weight to the
model using Cosine distance only improved its
performance slightly though statistically signifi-
cantly (for example, T+D > T ), while the overall
performance is still barely above random chance.
In an attempt to substantially boost the perfor-
mance of the unsupervised dialogue act classi-
fier, we experimented with a hierarchical cluster-
ing structure in which the model first branches on
the previous tutor move, and then the clustering
models are learned as described previously at the
leaves of the tree (Figure 3).
This branching approach results in some
branches with too few utterances to train a multi-
cluster model. To deal with this situation we set a
threshold of n=10 utterances. For those subgroups
with fewer than 10 utterances, we take a simple
majority vote to classify test cases, and for those
subgroups with 10 or larger utterances we train a
cluster model and use it to classify test cases. For
the entire corpus, the number of utterances in each
branch is presented in Table 6.
Tutor?s Previous Dialogue Act
Q S PF A
doclustering
...doclusteringdoclusteringdoclustering
Figure 3: Branching student utterances according
to previous tutor dialogue act.
As the results in Table 7 show, the performance
of the model with hierarchical structure is signif-
icantly better than the flat clustering model. Note
that each feature in this table leverages previous
118
Tutor Dialogue
Act
# of student
utterances
Q 818
S 464
H 125
PF 91
A 61
ACK 11
C 8
O 8
RACK 6
Table 6: The number of student utterances after
branching on the previous tutor dialogue act.
tutor dialogue act while branching. Branching
on previous tutor move boosted the model?s accu-
racy for student move dialogue act classification
by approximately 30% accuracy across all feature
sets, a difference that is statistically significant in
every case. With the hierarchical model struc-
ture, the best performance is achieved by includ-
ing all three types of features: lexical, dialogue
context and task. However, our hypothesis that
task features would significantly improve the ac-
curacy does not hold within the hierarchical clus-
tering model (T +D 6> D and T + L 6> L).
Features
Accuracy
(%)
Kappa
H
i
e
r
a
r
c
h
i
c
a
l
T 64.2
?
0.45
D 63.2
?
0.46
L 60.7
?
0.41
T+D 62.1
?
0.44
T+L 63.3*
?
0.45
D+L 63.6
?
0.46
T+D+L 65*
?
0.48
Table 7: Test set accuracies and Kappa for branch-
ing on previous tutor dialogue act (L: Lexical fea-
tures, D: Dialogue context features, T: Task fea-
tures) *indicates statistically significant compared
to the similar model without task features and ? in-
dicates hierarchical clustering performing signifi-
cantly better than flat with same features. (p <
0.05).
6 Discussion
The experimental results provide compelling ev-
idence that an inclusive approach to features for
unsupervised dialogue act modeling holds great
promise. However, we observed a stark difference
in model performance when the tutor?s previous
move was simply included as one of many features
within a flat clustering model compared to when
the previous tutor move was treated as a branch-
ing feature. In this section we take a closer look
and discuss the features that help distinguish par-
ticular dialogue acts from each other.
Using the hierarchical T +D+L model which
performed best within the experiments, we exam-
ine the confusion matrix (Figure 4). Statements
and acknowledgments prove challenging for the
model, 51.3% and 61.5% accuracy overall. More-
over, these two tags are easily confused with each
other: 29.7% of statements were misclassified
as acknowledgments, while 21.2% of acknowl-
edgments were misclassified as statements. The
worst overall classification accuracy was for ques-
tions (6%) and the best was achieved for answers
(95.3%).
Figure 4: Confusion matrix for hierarchical model
utilizing all features: T+D+L.
When we analyze the performance of different
sets of features with respect to individual dialogue
acts, some interesting results emerge. The anal-
ysis shows that task features are especially good
for classifying statements. Using only task fea-
tures, the model correctly classified 61.8% state-
ments, compared to the lower 51.3% accuracy that
the overall best model (T + D + L) achieved on
statements. When we consider the nature of the
statement dialogue act within this corpus, we note
that it is a large category that encompasses a vari-
ety of utterances, some of which have lexical fea-
tures in common with acknowledgments. In this
case, task features are particularly helpful.
For acknowledgments, a combination of task
and lexical features performed best (63.6% ac-
119
curacy) compared to the overall best performing
model which achieved a slightly lower 61.5% ac-
curacy on acknowledgments. Acknowledgments
are another example of an act that may take am-
biguous surface form; for example, in our cor-
pus an utterance ?yes? appears as both an answer
and an acknowledgment depending on its context.
Therefore, higher level features such as the ones
provided by task may be more helpful.
For questions, the highest performing feature
set is L. However, as shown in Table 8, the model
performed poorly on questions. Inspection of the
models reveals that questions are varied in terms
of structure throughout the corpus and it is hard to
distinguish them from other dialogue acts. For in-
stance there are two consequent utterances ?i need
a write statement? and ?don?t i?, both of which are
manually labeled as questions. However, in terms
of the structure, the first utterance looks very sim-
ilar to a statement and therefore the model has dif-
ficulty grouping it with questions. Due to the large
variety of question forms in the corpus, it is pos-
sible that the clustering performed poorly on this
dialogue act. In future work it will be promising to
investigate the dialogue structures which produce
questions and to weight them more in the feature
set in order to increase performance of clustering
for questions.
We performed one additional experiment to
compare the performance of the LCS metric with
bigrams. For bigrams, the average leave-one-
student-out test accuracy was 25% with flat clus-
tering compared to the lexical-only case using
LCS (L) which reached 33%.
Features S A Q ACK
L 21.5 41.3 14.2 20.4
T 61.76 95.27 7.30 40.90
D 48.16 95.27 3.00 60.30
T+D 52.69 94.68 3.43 51.64
T+L 42.78 95.13 6.01 63.58
D+L 43.63 94.98 8.58 62.09
T+D+L 51.27 95.27 6.01 61.49
Table 8: Accuracies for individual dialogue acts.
Acts with fewer than 10 utterances after branching
are omitted from the table.
7 Conclusion and Future Work
Dialogue act classification is crucial for dialogue
management, and unsupervised modeling ap-
proaches hold great promise for automatically ex-
tracting classification models from corpora. This
paper has focused on unsupervised dialogue act
classification for task-oriented dialogue, investi-
gating the impact of task features and dialogue
context features on model accuracy within both
flat and hierarchical clusterings. Experimental
results confirm that utilizing a combination of
task and dialogue features improves accuracy and
that incorporating one previous tutor move as a
high-level branching feature a provides particu-
larly marked benefit. Moreover, it was found that
task features are particularly important for iden-
tifying particular dialogue moves such as state-
ments, for which the model with task features only
outperformed the model with all features.
In addition to the task stream, future work
should consider other sources of nonverbal cues
such as posture, gesture and facial expressions to
investigate the extent to which these can be suc-
cessfully incorporated in unsupervised dialogue
act models. Second, models that are built in spe-
cialized ways to different user groups (e.g., by
gender or by incoming skill level) should be inves-
tigated. Finally, the performance of unsupervised
dialogue act classification models must ultimately
move toward evaluation within implemented dia-
logue systems (Ezen-Can and Boyer, 2013a). The
overarching goal of these investigations is to cre-
ate unsupervised dialogue act models that perform
well enough to be used within deployed dialogue
systems and enable the system to respond success-
fully. It is hoped that in the future, dialogue act
classification models for many domains can be ex-
tracted automatically from corpora of human dia-
logue in those domains without the need for any
manual annotation.
Acknowledgments
Thanks to the members of the LearnDialogue
group at North Carolina State University for their
helpful input. This work is supported in part by the
National Science Foundation through Grant DRL-
1007962 and the STARS Alliance, CNS-1042468.
Any opinions, findings, conclusions, or recom-
mendations expressed in this report are those of
the participants, and do not necessarily represent
the official views, opinions, or policy of the Na-
tional Science Foundation.
120
References
James F. Allen, Lenhart K. Schubert, George Ferguson,
Peter Heeman, Chung Hee Hwang, Tsuneaki Kato,
Marc Light, Nathaniel Martin, Bradford Miller,
Massimo Poesio, et al. 1995. The TRAINS project:
A case study in building a conversational planning
agent. Journal of Experimental & Theoretical Arti-
ficial Intelligence, 7(1):7?48.
David Arthur and Sergei Vassilvitskii. 2007. k-
means++: The advantages of careful seeding. In
Proceedings of the 18th ACM-SIAM Symposium on
Discrete Algorithms, pages 1027?1035. Society for
Industrial and Applied Mathematics.
John Langshaw Austin. 1975. How To Do Things with
Words, volume 1955. Oxford University Press.
Srinivas Bangalore, Giuseppe Di Fabbrizio, and
Amanda Stent. 2008. Learning the structure of
task-driven human?human dialogs. IEEE Transac-
tions on Audio, Speech, and Language Processing,
16(7):1249?1259.
Kristy Elizabeth Boyer, Eun Young Ha, Robert
Phillips, Michael D. Wallis, Mladen A. Vouk, and
James C. Lester. 2010. Dialogue act modeling in
a complex task-oriented domain. In Proceedings of
SIGDIAL, pages 297?305. Association for Compu-
tational Linguistics.
Kristy Elizabeth Boyer, Eun Young Ha, Robert
Phillips, and James Lester. 2011. The impact of
task-oriented feature sets on HMMs for dialogue
modeling. In Proceedings of SIGDIAL, pages 49?
58. Association for Computational Linguistics.
Lin Chen and Barbara Di Eugenio. 2013. Multimodal-
ity and dialogue act classification in the RoboHelper
project. In Proceedings of SIGDIAL, pages 183?
192.
Mark G. Core and James Allen. 1997. Coding dialogs
with the DAMSL annotation scheme. In Proceed-
ings of the AAAI Fall Symposium on Communicative
Action in Humans and Machines, pages 28?35.
Nigel Crook, Ramon Granell, and Stephen Pulman.
2009. Unsupervised classification of dialogue acts
using a Dirichlet process mixture model. In Pro-
ceedings of SIGDIAL, pages 341?348. Association
for Computational Linguistics.
Barbara Di Eugenio, Zhuli Xie, and Riccardo Serafin.
2010. Dialogue act classification, higher order di-
alogue structure, and instance-based learning. Dia-
logue & Discourse, 1(2):1?24.
Myroslava O. Dzikovska, Elaine Farrow, and Jo-
hanna D. Moore. 2013. Combining semantic inter-
pretation and statistical classification for improved
explanation processing in a tutorial dialogue system.
In Artificial Intelligence in Education, pages 279?
288.
Aysu Ezen-Can and Kristy Elizabeth Boyer. 2013a.
In-context evaluation of unsupervised dialogue act
models for tutorial dialogue. In Proceedings of SIG-
DIAL, pages 324?328.
Aysu Ezen-Can and Kristy Elizabeth Boyer. 2013b.
Unsupervised classification of student dialogue acts
with query-likelihood clustering. In International
Conference on Educational Data Mining, pages 20?
27.
Oliver Ferschke, Iryna Gurevych, and Yevgen Chebo-
tar. 2012. Behind the article: Recognizing dialog
acts in Wikipedia talk pages. In Proceedings of the
13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 777?
786.
Kate Forbes-Riley and Diane J. Litman. 2005. Us-
ing bigrams to identify relationships between stu-
dent certainness states and tutor responses in a spo-
ken dialogue corpus. In Proceedings of the SIG-
DIAL Workshop, pages 87?96.
Kate Forbes-Riley, Mihai Rotaru, Diane J. Litman, and
Joel Tetreault. 2007. Exploring affect-context de-
pendencies for adaptive system development. In
Human Language Technologies 2007: The Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 41?44.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 12(3):175?204.
Eun Young Ha, Joseph F. Grafsgaard, Christopher M.
Mitchell, Kristy Elizabeth Boyer, and James C.
Lester. 2012. Combining verbal and nonverbal
features to overcome the ?information gap? in task-
oriented dialogue. In Proceedings of SIGDIAL,
pages 247?256.
Daniel S. Hirschberg. 1975. A linear space al-
gorithm for computing maximal common subse-
quences. Communications of the ACM, 18(6):341?
343.
Shafiq Joty, Giuseppe Carenini, and Chin-Yew Lin.
2011. Unsupervised modeling of dialog acts in
asynchronous conversations. In Proceedings of the
22nd International Joint Conference on Artificial In-
telligence, pages 1807?1813.
Daniel Jurafsky, Elizabeth Shriberg, Barbara Fox, and
Traci Curl. 1998. Lexical, prosodic, and syn-
tactic cues for dialog acts. In Proceedings of the
ACL/COLING-98 Workshop on Discourse Relations
and Discourse Markers, pages 114?120.
Simon Keizer, Rieks op den Akker, and Anton Nijholt.
2002. Dialogue act recognition with Bayesian net-
works for Dutch dialogues. In Proceedings of the
SIGDIAL Workshop, pages 88?94.
121
Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2010. Classifying dialogue acts in one-on-
one live chats. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 862?871.
Donghyeon Lee, Minwoo Jeong, Kyungduk Kim, and
Seonghan Ryu. 2013. Unsupervised spoken lan-
guage understanding for a multi-domain dialog sys-
tem. IEEE Transactions On Audio, Speech, and
Language Processing, 21(11):2451?2464.
Johanna Marineau, Peter Wiemer-Hastings, Derek Har-
ter, Brent Olde, Patrick Chipman, Ashish Karnavat,
Victoria Pomeroy, Sonya Rajan, Art Graesser, Tutor-
ing Research Group, et al. 2000. Classification of
speech acts in tutorial dialog. In Proceedings of the
Workshop on Modeling Human Teaching Tactics and
Strategies at the Intelligent Tutoring Systems Con-
ference, pages 65?71.
T. Daniel Midgley, Shelly Harrison, and Cara Mac-
Nish. 2009. Empirical verification of adjacency
pairs using dialogue segmentation. In Proceedings
of SIGDIAL, pages 104?108.
Raymond Ng and Jiawei Han. 1994. Efficient and ef-
fective clustering methods for spatial data mining.
In Proceedings of the 20th International Conference
on Very Large Data Bases, pages 144?155.
Norbert Reithinger and Martin Klesen. 1997. Dia-
logue act classification using language models. In
Proceedings of EuroSpeech, pages 2235?2238.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Pro-
ceedings of the Association for Computational Lin-
guistics, pages 172?180.
Vasile Rus, Cristian Moldovan, Nobal Niraula, and
Arthur C. Graesser. 2012. Automated discovery of
speech act categories in educational games. In Inter-
national Conference on Educational Data Mining,
pages 25?32.
Emanuel A. Schegloff and Harvey Sacks. 1973. Open-
ing up closings. Semiotica, 8(4):289?327.
John R. Searle. 1969. Speech Acts: An Essay in
the Philosophy of Language. Cambridge University
Press.
Riccardo Serafin and Barbara Di Eugenio. 2004.
FLSA: Extending latent semantic analysis with fea-
tures for dialogue act classification. In Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, pages 692?699. Association
for Computational Linguistics.
Rangarajan Sridhar, Vivek Kumar, Srinivas Bangalore,
and Shrikanth Narayanan. 2009. Combining lexi-
cal, syntactic and prosodic cues for improved online
dialog act tagging. Computer Speech & Language,
23(4):407?422.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339?373.
Maryam Tavafi, Yashar Mehdad, Shafiq Joty, Giuseppe
Carenini, and Raymond Ng. 2013. Dialogue act
recognition in synchronous and asynchronous con-
versations. In Proceedings of SIGDIAL, pages 117?
121.
David R. Traum. 1999. Speech acts for dialogue
agents. In Foundations of Rational Agency, pages
169?201. Springer.
122
