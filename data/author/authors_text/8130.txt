What's yours and what's mine: Determining Intellectual 
Attribution in Scientific Text 
S imone Teufe l  t
Computer  Science Department  
Columbia University 
t eu fe l?cs ,  co lumbia ,  edu  
Marc  Moens  
HCRC Language Technology Group 
University of Ed inburgh 
Marc .  Moens?ed.  ac. uk  
Abst rac t  
We believe that identifying the structure of scien- 
tific argumentation in articles can help in tasks 
such as automatic summarization or the auto- 
mated construction of citation indexes. One par- 
ticularly important aspect of this structure is the 
question of who a given scientific statement is at- 
tributed to: other researchers, the field in general, 
or the authors themselves. 
We present he algorithm and a systematic eval- 
uation of a system which can recognize the most 
salient textual properties that contribute to the 
global argumentative structure of a text. In this 
paper we concentrate on two particular features, 
namely the occurrences ofprototypical gents and 
their actions in scientific text. 
1 In t roduct ion  
When writing an article, one does not normally 
go straight to presenting the innovative scien- 
tific claim. Insteacl, one establishes other, well- 
known scientific facts first, which are contributed 
by other researchers. Attribution of ownership of- 
ten happens explicitly, by phrases uch as "Chom- 
sky (1965) claims that". The question of intel- 
lectual attribution is important for researchers: 
not understanding the argumentative status of 
part of the text is a common problem for non- 
experts reading highly specific texts aimed at ex- 
perts (Rowley, 1982). In particular, after reading 
an article, researchers need to know who holds the 
"knowledge claim" for a certain fact that interests 
them. 
We propose that segmentation according to in- 
tellectual ownership can be done automatically, 
and that such a segmentation has advantages for 
various hallow text understanding tasks. At the 
heart of our classification scheme is the following 
trisection: 
* BACKGROUND (generally known work) 
* OWN,  new work and 
. specific OTHER work. 
The advantages of a segmentation at a rhetori- 
cal level is that rhetorics is conveniently constant 
tThis work was done while the first author was at the 
HCRC Language T chnology Group, Edinburgh. 
BACKGROUND:  
Researchers in knowledge representa- 
tion agree that one of the hard problems of 
understanding narrative is the representation 
of temporal information. Certain facts of nat- 
ural language make it hard to capture tempo- 
ral information \[...\] 
OTHER WORK:  
Recently, Researcher-4 has suggested the 
following solution to this problem \[...\]. 
WEAKNESS/CONTRAST:  
But this solution cannot be used to inter- 
pret the following Japanese examples: \[...\] 
OWN CONTRIBUT ION:  
We propose a solution which circumvents 
this p row while retaining the explanatory 
power of Researcher-4's approach. 
Figure h Fictional introduction section 
across different articles. Subject matter, on the 
contrary, is not constant, nor are writing style and 
other factors. 
We work with a corpus of scientific pa- 
pers (80 computational linguistics conference ar- 
ticles (ACL, EACL, COLING or ANLP), de- 
posited on the CMP_LG archive between 1994 
and 1996). This is a difficult test bed due to 
the large variation with respect o different fac- 
tors: subdomain (theoretical linguistics, statisti- 
cal NLP, logic programming, computational psy- 
cholinguistics), types of research (implementa- 
tion, review, evaluation, empirical vs. theoreti- 
cal research), writing style (formal vs. informal) 
and presentational styles (fixed section structure 
of type Introduction-Method-Results-Conclusion 
vs. more idiosyncratic, problem-structured presen- 
tation). 
One thing, however, is constant across all arti- 
cles: the argumentative aim of every single article 
is to show that the given work is a contribution to 
science (Swales, 1990; Myers, 1992; Hyland, 1998). 
Theories of scientific argumentation in research ar- 
ticles stress that authors follow well-predictable 
stages of argumentation, as in the fictional intro- 
duction in figure 1. 
9 
Are the scientific statements expressed 
in this sentence attributed to the 
authors, the general field, or specific other 
n work / Other Work 
Does this sentence contain material 
that describes the specific aim 
of the paper? 
Does this sentence make 
reference to the external 
structure of the paper? 
I SACKCRO D I 
D.~s it describe.a negative aspect 
of me omer worK, or a contzast 
or comparison of the own work to it? 
I CONTRAST I Does this sentence mention 
the other work as basis of 
or support for own work? 
Figure 2: Annotation Scheme for Argumentative Zones 
Our hypothesis i that a segmentation based on 
regularities of scientific argumentation and on at- 
tribution of intellectual ownership is one of the 
most stable and generalizable dimensions which 
contribute to the structure of scientific texts. In 
the next section we will describe an annotation 
scheme which we designed for capturing these ef- 
fects. Its categories are based on Swales' (1990) 
CARS model. 
1.1 The scheme 
As our corpus contains many statements talking 
about relations between own and other work, we 
decided to add two classes ("zones") for express- 
ing relations to the core set of OWN, OTHER 
and BACKGROUND, namely contrastive statements 
(CONTRAST;  comparable to Swales' (1990) move 
2A/B) and statements of intellectual ancestry 
(BAsis; Swales' move 2D). The label OTHER is 
thus reserved for neutral descriptions of other 
work. OWN segments are further subdivided to 
mark explicit aim statements (AIM; Swales' move 
3.1A/B), and explicit section previews (TEXTUAL; 
Swales' move 3.3). All other statements about the 
own work are classified as OwN. Each of the seven 
category covers one sentence. 
Our classification, which is a further develop- 
ment of the scheme in Teufel and Moens (1999), 
can be described procedurally as a decision tree 
(Figure 2), where five questions are asked about 
each sentence, concerning intellectual attribution, 
author stance and continuation vs. contrast. Fig- 
ure 3 gives typical example sentences for each zone. 
The intellectual-attribution distinction we make 
is comparable with Wiebe's (1994) distinction into 
subjective and objective statements. Subjectivity 
is a property which is related to the attribution of 
authorship as well as to author stance, but it is 
just one of the dimensions we consider. 
1.2 Use o f  Argumentat ive  Zones 
Which practical use would segmenting a paper into 
argumentative zones have? 
Firstly, rhetorical information as encoded in 
these zones should prove useful for summariza- 
tion. Sentence extracts, still the main type of 
summarization around, are notoriously context- 
insensitive. Context in the form of argumentative 
relations of segments to the overall paper could 
provide a skeleton by which to tailor sentence x- 
tracts to user expertise (as certain users or certain 
tasks do not require certain types of information). 
A system which uses such rhetorical zones to pro- 
duce task-tailored extracts for medical articles, al- 
beit on the basis of manually-segmented xts, is 
given by Wellons and Purcell (1999). 
Another hard task is sentence xtraction from 
long texts, e.g. scientific journal articles of 20 
pages of length, with a high compression. This 
task is hard because one has to make decisions 
about how the extracted sentences relate to each 
other and how they relate to the overall message 
of the text, before one can further compress them. 
Rhetorical context of the kind described above is 
very likely to make these decisions easier. 
Secondly, it should also help improve citation 
indexes, e.g. automatically derived ones like 
Lawrence et al's (1999) and Nanba and Oku- 
mura's (1999). Citation indexes help organize sci- 
entific online literature by linking cited (outgoing) 
and citing (incoming) articles with a given text. 
But these indexes are mainly "quantitative", list- 
ing other works without further qualifying whether 
a reference to another work is there to extend the 
10 
AIM "We have proposed a method of clustering words based on large corpus data." 
TEXTUAL "Section $ describes three unification-based parsers which are... " 
OWN "We also compare with the English language and draw some conclusions on the benefits 
of our approach." 
BACKGROUND "Part-of-speech tagging is the process of assigning rammatical categories to individual 
words in a corpus." 
CONTRAST "However, no method for extracting the relationships from superficial inguistic ex- 
pressions was described in their paper." 
BASIS "Our disambiauation method is based on the similaritu of context vectors, which was 
OTHER 
C :g y
originated by Wilks et al 1990." 
"Strzalkowski's Essential Arguments Approach (EAA) is a top-down approach to gen- 
eration... " 
Figure 3: Examples for Argumentative Zones 
earlier work, correct it, point out a weakness in 
it, or just provide it as general background. This 
"qualitative" information could be directly con- 
tributed by our argumentative zones. 
In this paper, we will describe the algorithm of 
an argumentative zoner. The main focus of the 
paper is the description of two features which are 
particularly useful for attribution determination: 
prototypical gents and actions. 
2 Human Annotat ion  o f  
Argumentat ive  Zones  
We have previously evaluated the scheme mpiri- 
cally by extensive experiments with three subjects, 
over a range of 48 articles (Teufel et al, 1999). 
We measured stability (the degree to which the 
same annotator will produce an annotation after 
6 weeks) and reproducibility (the degree to which 
two unrelated annotators will produce the same 
annotation), using the Kappa coefficient K (Siegel 
and Castellan, 1988; Carletta, 1996), which con- 
trols agreement P(A) for chance agreement P(E): 
K = P{A)-P(E) 
1-P(Z) 
Kappa is 0 for if agreement is only as would be 
expected by chance annotation following the same 
distribution as the observed istribution, and 1 for 
perfect agreement. Values of Kappa surpassing 
.8 are typically accepted as showing a very high 
level of agreement (Krippendorff, 1980; Landis and 
Koch, 1977). 
Our experiments show that humans can distin- 
guish own, other specific and other general work 
with high stability (K=.83, .79, .81; N=1248; k=2, 
where K stands for the Kappa coefficient, N for 
the number of items (sentences) annotated and k 
for the number of annotators) and reproducibil- 
ity (K=.78, N=4031, k=3), corresponding to 94%, 
93%, 93% (stability) and 93% (reproducibility) 
agreement. 
The full distinction into all seven categories of 
the annotation scheme is slightly less stable and 
reproducible (stability: K=.82, .81, .76; N=1220; 
k=2 (equiv. to 93%, 92%, 90% agreement); repro- 
ducibility: K=.71, N=4261, k=3 (equiv. to 87% 
agreement)), but still in the range of what is gener- 
ally accepted as reliable annotation. We conclude 
from this that humans can distinguish attribution 
and full argumentative zones, if trained. Human 
annotation is used as trMning material in our sta- 
tistical classifier. 
3 Automat ic  Argumentat ive  
Zon ing  
As our task is not defined by topic coherence 
like the related tasks of Morris and Hirst (1991), 
Hearst (1997), Kan et al (1998) and Reynar 
(1999), we predict hat keyword-based techniques 
for automatic argumentative zoning will not work 
well (cf. the results using text categorization as
described later). We decided to perform machine 
learning, based on sentential features like the ones 
used by sentence xtraction. Argumentative zones 
have properties which help us determine them on 
the surface: 
? Zones appear in typical positions in the article 
(Myers, 1992); we model this with a set of 
location features. 
? Linguistic features like tense and voice cor- 
relate with zones (Biber (1995) and Riley 
(1991) show correlation for similar zones like 
"method" and "introduction"). We model 
this with syntactic features. 
? Zones tend to follow particular other zones 
(Swales, 1990); we model this with an ngram 
model operating over sentences. 
? Beginnings of attribution zones are linguisti- 
cally marked by meta-discourse like "Other 
researchers claim that" (Swales, 1990; Hy- 
land, 1998); we model this with a specialized 
agents and actions recognizer, and by recog- 
nizing formal citations. 
? Statements without explicit attribution are 
interpreted as being of the same attribution 
as previous entences in the same segment of 
attribution; we model this with a modified 
agent feature which keeps track of previously 
recognized agents. 
11 
3.1 Recognizing Agents and Actions 
Paice (1981) introduces grammars for pattern 
matching of indicator phrases, e.g. "the 
aim/purpose of this paper/article/study" and "we 
conclude/propose". Such phrases can be useful 
indicators of overall importance. However, for 
our task, more flexible meta-diiscourse expressions 
need to be determined. The ,description of a re- 
search tradition, or the stateraent that the work 
described in the paper is the continuation ofsome 
other work, cover a wide range of syntactic and 
lexical expressions and are too hard to find for a 
mechanism like simple pattern matching. 
Agent Type Example 
US-AGENT 
THEM_AGENT 
GENERAL_AGENT 
US_PREVIOUS. AGENT 
OUR_AIM_AGENT 
REF_US_AGENT 
REF._AGENT 
THEM_PRONOUN_AGENT 
AIM_I:LEF_AGENT 
GAP_AGENT 
PROBLEM_AGENT 
SOLUTION_AGENT 
TEXTSTRUCTURE_AGENT 
we 
his approach 
traditional methods 
the approach given in 
X (99) 
the point o\] this study 
thia paper 
the paper 
they 
its goal 
none of these papers 
these drawbacks 
a way out o\] this 
dilemma 
the concluding chap- 
ter 
Figure 4: Agent Lexicon: 168 Patterns, 13 Classes 
We suggest hat the robust recognition of pro- 
totypical agents and actions is one way out of this 
dilemma. The agents we propose to recognize de- 
scribe fixed role-players in the argumentation. I  
Figure 1, prototypical agents are given in bold- 
face ("Researchers in knowledge representation, 
"Researcher-4" and "we"). We also propose pro- 
totypical actions frequently occurring in scientific 
discourse (shown underlined in Figure 1): the re- 
searchers "agree", Researcher-4 "suggested" some- 
thing, the solution "cannot be used". 
We will now describe an algorithm which rec- 
ognizes and classifies agents and actions. We 
use a manually created lexicon for patterns for 
agents, and a manually clustered verb lexicon for 
the verbs. Figure 4 lists the agent types we dis- 
tinguish. The main three types are US_aGENT, 
THEM-AGENT and GENERAL.AGENT. A fourth 
type is US.PREVIOUS_AGENT (the authors, but in 
a previous paper). 
Additional agent types include non-personal 
agents like aims, problems, solutions, absence of 
solution, or textual segments. There are four 
equivalence classes of agents with ambiguous 
reference ("this system"), namely REF_US_AGENT, 
THEM-PRONOUN_AGENT, AIM.-REF-AGENT, 
REF_AGENT. The total of 168 patterns in the 
lexicon expands to many more as we use a replace 
mechanism (@WORK_NOUN is expanded to 
"paper, article, study, chapter" etc). 
For verbs, we use a manually created the ac- 
tion lexicon summarized in Figure 6. The verb 
classes are based on semantic oncepts uch as 
similarity, contrast, competition, presentation, ar- 
gumentation and textual structure. For ex- 
ample, PRESENTATION..ACTIONS include commu- 
nication verbs like "present", "report", "state" 
(Myers, 1992; Thompson and Yiyun, 1991), RE- 
SEARCH_ACTIONS include "analyze", "conduct" 
and "observe", and ARGUMENTATION_ACTIONS 
"argue", "disagree", "object to". Domain-specific 
actions are contained in the classes indicating 
a problem ( ".fail", "degrade", "overestimate"), 
and solution-contributing actions (" "circumvent', 
solve", "mitigate"). 
The main reason for using a hand-crafted, genre-- 
specific lexicon instead of a general resource such 
as WordNet or Levin's (1993) classes (as used in 
Klavans and Kan (1998)), was to avoid polysemy 
problems without having to perform word sense 
disambiguation. Verbs in our texts often have a 
specialized meaning in the domain of scientific ar- 
gumentation, which our lexicon readily encodes. 
We did notice some ambiguity problems (e.g. "fol- 
low" can mean following another approach, or it 
can mean follow in a sense having nothing to do 
with presentation of research, e.g. following an 
arc in an algorithm). In a wider domain, however, 
ambiguity would be a much bigger problem. 
Processing of the articles includes transforma- 
tion from I~TEX into XML format, recognition 
of formal citations and author names in running 
text, tokenization, sentence separation and POS- 
tagging. The pipeline uses the TTT software pro- 
vided by the HCRC Language Technology Group 
(Grover et al, 1999). The algorithm for deter- 
mining agents in subject positions (or By-PPs in 
passive sentences) is based on a finite automaton 
which uses POS-input; cf. Figure 5. 
In the case that more than one finite verb is 
found in a sentence, the first finite verb which has 
agents and/or actions in the sentences i used as 
a value for that sentence. 
4 Eva luat ion  
We carried out two evaluations. Evaluation A 
tests whether all patterns were recognized as in- 
tended by the algorithm, and whether patterns 
were found that should not have been recognized. 
Evaluation B tests how well agent and action 
recognition helps us perform argumentative zon- 
ing automatically. 
4.1 Evaluation A: Cor rectness  
We first manually evaluated the error level of the 
POS-Tagging of finite verbs, as our algorithm cru- 
cially relies on finite verbs. In a random sample of 
100 sentences from our corpus (containing a total 
of 184 finite verbs), the tagger showed a recall of 
12 
1. Start from the first finite verb in the sentence. 
2. Check right context of the finite verb for verbal forms of interest which might make up more 
complex tenses. Remain within the assumed clause boundaries; do not cross commas or other 
finite verbs. Once the main verb of that construction (the "semantic" verb) has been found, 
a simple morphological nalysis determines its lemma; the tense and voice of the construction 
follow from the succession of auxiliary verbs encountered. 
3. Look up the lemma of semantic verb in Action Lexicon; return the associated Action Class if 
successful. Else return Action 0. 
4. Determine if one of the 32 fixed negation words contained in the lexicon (e.g. "not, don't, 
neither") is present within a fixed window of 6 to the right of the finite verb. 
5. Search for the agent either as a by-PP to the right, or as a subject-NP to the left, depending on 
the voice of the construction as determined in step 2. Remain within assumed clause boundaries. 
6. If one of the Agent Patterns matches within that area in the sentence, return the Agent Type. 
Else return Agent 0. 
7. Repeat Steps 1-6 until there are no more finite verbs left. 
Figure 5: Algorithm for Agent and Action Detection 
Action Type Example Action Type Example 
AFFECT 
ARGUMENTATION 
AWARENESS 
BETTER_SOLUTION 
CHANGE 
COMPARISON 
CONTINUATION 
CONTRAST 
FUTURE_INTEREST 
INTEREST 
we hope to improve our results 
we argue against a model of 
we are not aware of attempts 
our system outperforms . . .  
we extend <CITE /> 's  algo- 
rithm 
we tested our system against.. .  
we follow <REF/> . . .  
our approach differs from . . .  
we intend to improve . . .  
we are concerned with . . .  
NEED 
PRESENTATION 
PROBLEM 
RESEARCH 
SIMILAR 
SOLUTION 
TEXTSTRUCTURE 
USE 
COPULA 
POSSESSION 
this approach, however, lacks... 
we present here a method for. .  . 
this approach fai ls . . .  
we collected our data f rom. . .  
our approach resembles that of 
we solve this problem by. . .  
the paper is organize&.. 
we employ <REF/> 's method...  
our goal ~ to . . .  
we have three goals... 
Figure 6: Action Lexicon: 366 Verbs, 20 Classes 
95% and a precision of 93%. 
We found that for the 174 correctly determined 
finite verbs (out of the total 184), the heuristics for 
negation worked without any errors (100% accu- 
racy). The correct semantic verb was determined 
in 96% percent of all cases; errors are mostly due 
to misrecognition of clause boundaries. Action 
Type lookup was fully correct, even in the case 
of phrasal verbs and longer idiomatic expressions 
("have to" is a NEED..ACTION; "be inspired by" is 
a, CONTINUE_ACTION). There were 7 voice errors, 
2 of which were due to POS-tagging errors (past 
participle misrecognized). The remaining 5 voice 
errors correspond to a 98% accuracy. Figure 7 
gives an example for a voice error (underlined) in 
the output of the action/agent determination. 
Correctness of Agent Type determination was 
tested on a random sample of 100 sentences con- 
taining at least one agent, resulting in 111 agents. 
No agent pattern that should have been identi- 
fied was missed (100% recall). Of the 111 agents, 
105 cases were completely correct: the agent pat- 
tern covered the complete grammatical subject or 
by-PP intended (precision of 95%). There was one 
complete rror, caused by a POS-tagging error. In 
5 of the 111 agents, the pattern covered only part  
At the point where John <ACTION 
TENSE=Pi~SENT VOICE=ACTIVE 
MODAL=NOMODAL NEGATION=0 
ACT IONTYPE=0> knows </ACTION> the truth 
has been  <FINITE TENSE=PRESENT_PERFECT 
VOICE=PASSIVE  MODAL=NOMODAL NEGA-  
T ION=0 ACTIONTYPE=0> processed 
</ACTION> , a complete clause will have 
been <ACTION TENSE=FUTURE.PERFECT 
VOICE=ACTIVE MODAL=NOMODAL NEGA- 
TION=0 ACTIONTYPE=0> bu i l t  </ACTION> 
Figure 7: Sample Output of Action Detection 
of a subject NP (typically the NP in a postmodify- 
ing PP), as in the phrase "the problem with these 
approaches" which was classified as REF_AGENT. 
These cases (counted as errors) indeed constitute 
no grave errors, as they still give an indication 
which type of agents the nominal phrase is associ- 
ated with. 
13 
4.2 Evaluation B: Usefulness for 
Argumentat ive Zoning 
We evaluated the usefulness of the Agent and Ac- 
tion features by measuring if they improve the 
classification results of our stochastic classifier for 
argumentative zones. 
We use 14 features given in figure 8, some of 
which are adapted from sentence xtraction tech- 
niques (Paice, 1990; Kupiec et eL1., 1995; Teufel and 
Moens, 1999). 
. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 
11. 
12. 
13. 
14. 
Absolute location of sentence in document 
Relative location of sentence in section 
Location of a sentence in paragraph 
Presence of citations 
Location of citations 
Type of citations (self citation or not) 
Type of headline 
Presence of tf/idf key words 
Presence of title words 
Sentence length 
Presence of modal auxiliaries 
Tense of the finite verb 
Voice of the finite verb 
Presence of Formulaic Expressions 
Figure 8: Other features used 
All features except Citation Location and 
Citation Type proved helpful for classification. 
Two different statistical models were used: a Naive 
Bayesian model as in Kupiec et al's (1995) exper- 
iment, cf. Figure 9, and an ngram model over sen- 
tences, cf. Figure 10. Learning is supervised and 
training examples are provided by our previous hu- 
man annotation. Classification preceeds sentence 
by sentence. The ngram model combines evidence 
from the context (Cm-1, Cm-2) and from I senten- 
tiai features (F,~,o...Fmj-t), assuming that those 
two factors are independent ofeach other. It uses 
the same likelihood estimation as the Naive Bayes, 
but maximises a context-sensitive prior using the 
Viterbi algorithm. We received best results for 
n=2, i.e. a bigram model. 
The results of stochastic lassification (pre- 
sented in figure 11) were compiled with a 10-fold 
cross-validation our 80-paper corpus, contain- 
ing a total of 12422 sentences (classified items). 
As the first baseline, we use a standard text cat- 
egorization method for classification (where each 
sentence is considered as a document*) Baseline 1 
has an accuracy of 69%, which is low considering 
that the most frequent category (OWN) also coy- 
errs 69% of all sentences. Worse still, the classifier 
classifies almost all sentences as OWN and OTHER 
segments (the most frequent categories). Recall on 
the rare categories but important categories AIM, 
TEXTUAL, CONTRAST and BASIS is zero or very 
low. Text classification is therefore not a solution. 
*We used the Rainbow implementation of a Naive Bayes 
tf/idf method, 10-fold cross-validation. 
Baseline 2, the most frequent category (OWN), 
is a particularly bad baseline: its recall on all cate- 
gories except OWN is zero. We cannot see this bad 
performance in the percentage accuracy values, 
but only in the Kappa values (measured against 
one human annotator, i.e. k=2). As Kappa takes 
performance on rare categories into account more, 
it is a more intuitive measure for our task. 
In figure 11, NB refers to the Naive Bayes model, 
and NB+ to the Naive Bayes model augmented 
with the ngram model. We can see that the 
stochastic models obtain substantial improvement 
over the baselines, particularly with respect to pre- 
cision and recall of the rare categories, raising re- 
call considerably in all cases, while keeping preci- 
sion at the same level as Baseline 1 or improving 
it (exception: precision for BASIS drops; precision 
for AIM is insignificantly lower). 
If we look at the contribution of single features 
(reported for the Naive Bayes system in figure 12), 
we see that Agent and Action features improve 
the overall performance of the system by .02 and 
.04 Kappa points respectively (.36 to .38/.40). 
This is a good performance for single features. 
Agent is a strong feature beating both baselines. 
Taken by itself, its performance at K=.08 is still 
weaker than some other features in the pool, e.g. 
the Headline feature (K=.19), the C i tat ion fea- 
ture (K=.I8) and the Absolute Location Fea- 
ture (K=.17). (Figure 12 reports classification re- 
sults only for the stronger features, i.e. those who 
are better than Baseline 2). The Action feature, 
if considered on its own, is rather weak: it shows 
a slightly better Kappa value than Baseline 2, but 
does not even reach the level of random agreement 
(K=0). Nevertheless, if taken together with the 
other features, it still improves results. 
Building on the idea that intellectual attribu- 
tion is a segment-based phenomena, we improved 
the Agent feature by including history (feature 
SAgent). The assumption is that in unmarked sen- 
tences the agent of the previous attribution isstill 
active. Wiebe (1994) also reports segment-based 
agenthood as one of the most successful features. 
SAgent alone achieved a classification success of 
K=.21, which makes SAgent the best single fea- 
tures available in the entire feature pool. Inclusion 
of SAgent to the final model improved results to 
K=.43 (bigram model). 
Figure 12 also shows that different features are 
better at disambiguating certain categories. The 
Formulaic feature, which is not very strong on 
its own, is the most diverse, as it contributes to 
the disambiguation f six categories directly. Both 
Agent and Action features disambiguate cate-, 
gories which many of the other 12 features cannot 
disambiguate ( .g. CONTRAST), and SAgent addi- 
tionally contributes towards the determination f 
BACKGROUND zones (along with the Fo~ula ic  
and the Absolute Location feature). 
14 
P(CIFo, ..., F,~_,) ~ P(C) Nj~---?l P(FyIC) 
n- -1  
I ' I j=o P (F j )  
P(CIFo .... , F.-i ): 
P(C): 
P(FjIC): 
P(FA: 
Probability that a sentence has target category C, given its feature values F0, . . . ,  
F . - i ;  
(OveraU) probability of category C); 
Probability of feature-value pair Fj, given that the sentence is of target category C; 
Probability of feature value Fj; 
Figure 9: Naive Bayesian Classifier 
I--I F C 
P(CmlFm,o,. .,F~,~-i,C0,. . ,6~-1) ~ P(V,~lCm-l,C~-2) l-I~=?P( ~,~1 ,~) 
? " l - -1  FI~=o P(Fm,~) 
m: 
l: 
P( C,~IF~,o, . . . , F,~,~-t, Co , . . . ,  C,~-l ): 
P (C ,~IC~- , ,C~-2) :  
P(F,~j\[C,~): 
P(F~,j): 
index of sentence (ruth sentence in text) 
number of features considered 
target category associated with sentence at index m 
Probability that sentence rn has target category Cm, given its 
feature values Fro,o, . . . ,  Fmj-1 and given its context Co, ...C,~-1; 
Probability that sentence rn has target category C, given the cat- 
egories of the two previous entences; 
Probability of feature-value pair Fj occu~ing within target cate- 
gory C at position m; 
Probability of feature value Fmj; 
Figure 10: Bigram Model 
5 Discuss ion  
The result for automatic lassification is in agree- 
ment with our previous experimental results for 
human classification: humans, too, recognize the 
categories AIM and TEXTUAL most robustly (cf. 
Figure 11). AIM and TEXTUAL sentences, tating 
knowledge claims and organizing the text respec- 
tively, are conventionalized to a high degree. The 
system's results for AIM sentences, for instance, 
compares favourably to similar sentence xtraction 
experiments (cf. Kupiec et al's (1995) results of 
42%/42% recall and precision for extracting "rel- 
evant" sentences from scientific articles). BASIS 
and CONTRAST sentences have a less prototypical 
syntactic realization, and they also occur at less 
predictable places in the document. Therefore, it 
is far more difficult for both machine and human 
to recognize such sentences. 
While the system does well for AIM and TEX- 
TUAL sentences, and provides ubstantial improve- 
ment over both baselines, the difference to human 
performance is still quite large (cf. figure 11). We 
attribute most of this difference to the modest size 
of our training corpus: 80 papers are not much for 
machine learning of such high-level features. It is 
possible that a more sophisticated model, in com- 
bination with more training material, would im- 
prove results significantly. However, when we ran 
them on our data as it is now, different other sta- 
tistical models, e.g. Ripper (Cohen, 1996) and a 
Maximum Entropy model, all showed similar nu- 
merical results. 
Another factor which decreases results are in- 
consistencies in the training data: we discovered 
that 4% of the sentences with the same features 
were classified differently by the human annota- 
tion. This points to the fact that our set of fea- 
tures could be made more distinctive. In most 
of these cases, there were linguistic expressions 
present, such as subtle signs of criticism, which 
humans correctly identified, but for which the fea- 
tures are too coarse. Therefore, the addition of 
"deeper" features to the pool, which model the se- 
mantics of the meta-discourse hallowly, seemed 
a promising avenue. We consider the automatic 
and robust recognition of agents and actions, as 
presented here, to be the first incarnations of such 
features. 
6 Conc lus ions  
Argumentative zoning is the task of breaking a 
text containing a scientific argument into linear 
zones of the same argumentative status, or zones 
of the same intellectual attribution. We plan to 
use argumentative zoning as a first step for IR and 
shallow document understanding tasks like sum- 
marization. In contrast o hierarchical segmenta- 
tion (e.g. Marcu's (1997) work, which is based on 
RST (Mann and Thompson, 1987)), this type of 
segmentation aims at capturing the argumentative 
status of a piece of text in respect o the overall 
argumentative act of the paper. It does not deter- 
15 
I Method Acc. K Precision/recall per category (in %) I 
(~) AIM CONTR. TXT. OWN BACKG. BASIS OTHER 
I Human Performance 87 .71 72/56 50/55 79/79 94/92 68/75 82/34 74/83 \] 
I NB+ (best results) 71 .43 40/53 33/20 62/57 85/85 30/58 28/31 50/38 I 
I NB (best results) 7'2 .41 42/60 34/22 61/60 82/90 40/43 27/41 53/29 I 
. 
I BasoL 1: Text catog 69 13 44/9 32/42 58/14 77/90 20/5 47/12 31/16 I 
I Basel. 2: Most freq. cat. 69 -.12 0/0 0/0 0/0 69/100 0/0 0/0 0/0 I 
Figure 11: Accuracy, Kappa, Precision and Recall of Human and Automatic Processing, in comparison 
to baselines 
Features used Acc. K Precision/recallper category(in%) 
(Naive Bayes System) (%) AIM CONTR. TXT. OWN BACKG. BASIS OTHER 
Action alone 68 -.II 0/0 43/1 0/0 68/99 0/0 0/0 0/0 
Agent alone 67 .08 0/0 0/0 0/0 71/93 0/0 0/0 36/23 
Shgent alone 70 .21 0/0 17/0 0/0 74/94 53/16 0/0 46/33 
Abs. Locationalone 70 .17 0/0 0/0 0/0 74/97  40/36 0/0 28/9 
Headlinesalone 69 .19 0/0 0/0 0/0 75/95 0/0 0/0 29/25 
CitaCionalone 70 .18 0/0 0/0 0/0 73/96 0/0 0/0 43/30 
Citat2on Type alone 70 .13 0/0 0/0 0/0 72/98 0/0 0/0 43/24 
Citation Locat. alone 70 .13 0/0 0/0 0/0 72/97 0/0 0/0 43/24 
Foz~mlaicalone 70 .07 40/2 45/2 75/39 71/98 0/0 40/1 47/13 
12 other features 71 .36 37/53 32/17 54/47 81/91 39/41 22/32 45/22 
12 fea.+hction 71 .38 38/57 34/22 58/59 81/91 39/40 25/38 48/22 
12fea.+hgent 72 .40 40/57 35/18 59/51 82/91 39/43 25/34 52/29 
12fea.+SAgent 73 .40 39/57 33/19 61/51 81/91 42/43 25/33 52/29 
12 ~a.+Action+hgent 71 .43 40/53 33/20 62/57 85/85 30/58 28/31 50/38 
12 fea.+Action+Shgen~ 73 .41 41/59 34/22 62/61 82/91 41/42 27/39 51/29 
Figure 12: Accuracy, Kappa, 
individual features 
Precision and Recall of Automatic Processing (Naive Bayes system), per 
mine the rhetorical structure within zones. Sub- 
zone structure is most likely related to domain- 
specific rhetorical relations which are not directly 
relevant to the discourse-level relations we wish to 
recognize. 
We have presented a fully implemented proto- 
type for argumentative zoning. Its main inno- 
vation are two new features: prototypical agents 
and actions - -  semi-shallow representations of the 
overall scientific argumentation f the article. For 
agent and action recognition, we use syntactic 
heuristics and two extensive libraries of patterns. 
Processing is robust and very low in error. We 
evaluated the system without and with the agent 
and action features and found that the features im- 
prove results for automatic argumentative zoning 
considerably. History-aware agents are the best 
single feature in a large, extensively tested feature 
pool. 
References 
Biber, Douglas. 1995. Dimensions of Register Varia- 
tion: A Cross-linguistic Comparison. Cambridge, 
England: Cambridge University Press. 
Carletta, Jean. 1996. Assessing agreement on classi- 
fication tasks: The kappa statistic. Computational 
Linguistics 22(2): 249-.-254. 
Cohen, William W. 1996. Learning trees and rules 
with set-valued features. In Proceedings ofAAAL 
96. 
Grocer, Claire, Andrei Mikheev, and Colin Mathe- 
son. 1999. LT TTT Version 1.0: Text Tokenisa- 
tion Software. Technical report, Human Commu- 
nication Research Centre, University of Edinburgh. 
ht tp  : / /~w.  ltg. ed. ac. uk/software/ttt/.  
Hearst, Marti A. 1997. TextTiling: Segmenting text 
into multi-paragraph subtopic passages. Computa- 
tional Linguistics 23(1): 33---64. 
Hyland, Ken. 1998. Persuasion and context: The prag- 
matics of academic metadiscourse. Journal o\] Prag- 
matics 30(4): 437-455. 
Kan, Min-Yen, Judith L. Klavans, and Kathleen R. 
McKeown. 1998. Linear Segmentation and Segment 
Significance. In Proceedings o~ the Sixth Workshop 
on Very Large Corpora (COLIN G/ACL-98), 197- 
205. 
Klavans, Judith L., and Min-Yen Kan. 1998. Role 
of verbs in document analysis. In Proceedings 
of 36th Annual Meeting o\] the Association /or 
Computational Linguistics and the 17th Interna- 
tional Conference on Computational Linguistics 
(,4 CL/COLING-gS), 68O--686. 
Krippendorff, Klaus. 1980. Content Analysis: An In- 
troduction to its Methodology. Beverly Hills, CA: 
Sage Publications. 
Kupiee, Julian, Jan O. Pedersen, and Franeine Chela. 
16 
1995. A trainable document summarizer. In Pro- 
ceedings of the 18th Annual International Confer- 
ence on Research and Development in Information 
Retrieval (SIGIR-95), 68--73. 
Landis, J.R., and G.G. Koch. 1977. The Measurement 
of Observer Agreement for Categorical Data. Bio- 
metrics 33: 159-174. 
Lawrence, Steve, C. Lee Giles, and Ku_t Bollaeker. 
1999. Digital libraries and autonomous citation in- 
dexing. IEEE Computer 32(6): 67-71. 
Levin, Beth. 1993. English Verb Classes and Alterna- 
tions. Chicago, IL: University of Chicago Press. 
Mann, William C., and Sandra A. Thompson. 1987. 
Rhetorical Structure Theory: Description and Con- 
struction of text structures. In Gerard Kempen, 
ed., Natural Language Generation: New Results in 
Artificial Intelligence, Psychology, and Linguistics, 
85-95. Dordrecht, NL: Marinus Nijhoff Publishers. 
Marcu, Daniel. 1997. From Discourse Structures to 
Text Summaries. In Inderjeet Mani and Mark T. 
Maybury, eds., Proceedings of the ACL/EACL-97 
Workshop on Intelligent Scalable Text Summariza- 
tion, 82-88. 
Morris, Jane, and Graeme Hirst. 1991. Lexical cohe- 
sion computed by thesau.ral relations as an indicator 
of the structure of text. Computational Linguistics 
17: 21-48. 
Myers, Greg. 1992. In this paper we report...---speech 
acts and scientific facts. Journal of Pragmatics 
17(4): 295-313. 
:Nanba, I:Iidetsugu, and Manabu Okumura. 1999. To- 
wards multi-paper summarization using reference 
in.formation. In Proceedings of IJCAI-99, 926- 
931. http://galaga, jaist, ac. jp: 8000/'nanba/ 
study/papers .html. 
Paice, Chris D. 1981. The automatic generation of 
literary abstracts: an approach based on the iden- 
tification of self-indicating phrases. In Robert Nor- 
man Oddy, Stephen E. Robertson, Cornelis Joost 
van Pdjsbergen, and P. W. Williams, eds., Infor- 
mation Retrieval Research, 172-191. London, UK: 
Butterworth. 
Paice, Chris D. 1990. Constructing literature abstracts 
by computer: techniques and prospects. Informa- 
tion Processing and Management 26: 171-186. 
Reynar, Jeffrey C. 1999. Statistical models for topic 
segmentation. In Proceedings of the 37th Annual 
Meeting of the Association for Computational Lin- 
guistics (A CL-99), 357-364. 
Riley, Kathryn. 1991. Passive voice and rhetorical role 
in scientific writing. Journal of Technical Writing 
and Communication 21(3): 239--257. 
Rowley, Jennifer. 1982. Abstracting and Indexing. 
London, UK: Bingley. 
Siegel, Sidney, and N. John Jr. CasteUan. 1988. Non- 
parametric Statistics for the Behavioral Sciences. 
Berkeley, CA: McGraw-Hill, 2nd edn. 
Swales, John. 1990. Genre Analysis: English in Aca- 
demic and Research Settings. Chapter 7: Research 
articles in English, 110-.-176. Cambridge, UK: Cam- 
bridge University Press. 
Teufel, Simone, Jean Carletta, and Marc Moens. 1999. 
An annotation scheme for discourse-level argumen- 
tation in research articles. In Proceedings of the 8th 
Meeting of the European Chapter of the Association 
for Computational Linguistics (EA CL-99), 110-117. 
Teufel, Simone, and Marc Moens. 1999. Argumenta- 
tive classification of extracted sentences as a first 
step towards flexible abstracting. In Inderjeet Mani 
and Mark T. Maybury, eds., Advances in Auto- 
matic Text Summarization, 155-171. Cambridge, 
MA: MIT Press. 
Thompson, Geoff, and Ye Yiyun. 1991. Evaluation in 
the reporting verbs used in academic papers. Ap- 
plied Linguistics 12(4): 365-382. 
Wellons, M. E., and G. P. Purcell. 1999. Task-specific 
extracts for using the medical iterature. In Pro- 
ceedings of the American Medical Informatics Sym- 
posium, 1004-1008. 
Wiebe, Janyce. 1994. Tracking point of view in narra- 
tive. Computational Linguistics 20(2): 223-287. 
17 
  
	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1493?1502,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Towards Discipline-Independent Argumentative Zoning:
Evidence from Chemistry and Computational Linguistics
Simone Teufel
Computer Laboratory
Cambridge University
sht25@cl.cam.ac.uk
Advaith Siddharthan
Computer Laboratory
Cambridge University
as372@cl.cam.ac.uk
Colin Batchelor
Royal Society of Chemistry
Cambridge, UK
batchelorc@rsc.org
Abstract
Argumentative Zoning (AZ) is an anal-
ysis of the argumentative and rhetorical
structure of a scientific paper. It has been
shown to be reliably used by independent
human coders, and has proven useful for
various information access tasks. Annota-
tion experiments have however so far been
restricted to one discipline, computational
linguistics (CL). Here, we present a more
informative AZ scheme with 15 categories
in place of the original 7, and show that
it can be applied to the life sciences as
well as to CL. We use a domain expert
to encode basic knowledge about the sub-
ject (such as terminology and domain spe-
cific rules for individual categories) as part
of the annotation guidelines. Our results
show that non-expert human coders can
then use these guidelines to reliably an-
notate this scheme in two domains, chem-
istry and computational linguistics.
1 Introduction
Teufel et al (1999) define the task of Argumenta-
tive Zoning (AZ) as a sentence-by-sentence clas-
sification with mutually exclusive categories from
the annotation scheme given in Fig. 1. The reason-
ing behind the categories is inspired by the notion
of a knowledge claim (Myers, 1992; Luukkonen,
1992): the act of writing a paper corresponds to
an attempt of claiming ownership for a new piece
of knowledge, which is to be integrated into the
repository of scientific knowledge in the authors?
field by the process of peer review and publica-
tion. In the cause of this process, the authors
have to convince the reviewers that the knowledge
claim of the paper is valid (Swales, 1990; Hy-
land, 1998). What AZ aims to model, then, are
some of the relevant stages in this argument. We
divide the paper into zones, OTHER, OWN and
BACKGROUND. These are defined on the basis
of who owns the knowledge claim in the corre-
sponding segment. There are also two categories
which are defined by their relationship to existing
work, BASIS and CONTRAST. That means that
parts of the AZ scheme are similar to citation func-
tion classification schemes from the area of cita-
tion content analysis (Garfield, 1965; Weinstock,
1971; Spiegel-Ru?sing, 1977), and to automatic
citation function classification (Nanba and Oku-
mura, 1999; Garzone and Mercer, 2000; Teufel
et al, 2006). The remaining categories, AIM and
TEXTUAL, fulfil different rhetorical functions for
the presentation of the paper. AIM points out the
paper?s main knowledge claim, a rhetorical move
which may be repeated in the conclusion and the
introduction. TEXTUAL explains the physical lo-
cation of information, e.g., by giving a section
overview or presenting a summary of a subsec-
tion. On the basis of human-annotated training
material, AZ can be automatically classified using
supervised machine learning.
Category Description
AIM Statement of research goal.
BACKGROUND Description of generally accepted
background knowledge.
BASIS Existing KC provides basis for new
KC.
CONTRAST An existing KC is contrasted, com-
pared, or presented as weak.
OTHER Description of existing KC.
OWN Description of any other aspect of
new KC.
TEXTUAL Indication of paper?s textual
structure.
Figure 1: AZ Annotation Scheme (Teufel et al
1999).
Rhetorical information marking is useful for
1493
many novel information access tasks. For in-
stance, information retrieval can profit from
rhetorical information in the form of paradigm
shift statements (Chichester et al, 2005), as papers
containing such statements have a high impact in
an area. 75% of the ?Faculty of 1000 Biology?
papers (which are chosen by experts for their spe-
cial importance) contain paradigm shift sentences
(Agnes Sandor, personal communication).
AZ annotation allows the construction of multi-
and single document summaries which concen-
trate on differences and similarities to related
(cited) work. AZ can also be used for search in
a data base of scientific articles, in particular for
enhanced citation indexing. This has been pre-
viously explored in a task-based evaluation, were
users were asked to list positive and negative cita-
tions they would expect in a paper, given a short
extract (Teufel, 2001). In that task, AZ-based ex-
tracts outperformed other document surrogates.
Feltrim et al (2005) present a writing support
system which analyses students? drafts of sum-
maries for their PhD theses, performs an AZ anal-
ysis on them and critiques the rhetorical structure
of the students? draft on the basis of it.
The definition of the AZ categories is based
on rhetorical principles and should be decidable,
in principle, without specific domain knowledge
about what is discussed in detail in the paper. We
present here the first evidence that AZ categories
can be reliably recognised across scientific disci-
plines, using chemistry and computational linguis-
tics as our model disciplines for these experiments.
The categories just introduced are abstract and
depend on the annotators? interpretation of a
rhetorical argument. This means that there is
no guarantee that several independent annotators
would annotate similarly. It is therefore crucial
that all annotations at a high level of interpreta-
tion are backed up by human annotation with more
than one annotator. However, annotations of cita-
tion function classification typically use only the
untested annotation of a single human annotator
as gold standard, who is typically the designer of a
scheme (Spiegel-Ru?sing, 1977; Weinstock, 1971;
Nanba and Okumura, 1999; Garzone and Mercer,
2000). Teufel et al (2006) are the only exception
who test their citation function scheme using mod-
ern corpus-linguistic annotation methodology.
A study of human agreement on AZ annotation
exists (Teufel et al, 1999), but this uses articles
from only one discipline, namely computational
linguistics. In this paper, we use a similar method-
ology to Teufel et al, but with data from two disci-
plines. The preliminary conclusion from these ex-
periments is that annotation with chemistry papers
has resulted in higher agreement than annotation
with computational linguistics papers.
We extend the AZ annotation scheme to make
further distinctions, as will be discussed in sec-
tion 2. We also created an environment in which
domain knowledge that an annotator might have
about the science in a paper is systematically dis-
regarded. We will describe how this was done in
section 3, and then present the annotation experi-
ment itself in section 4.
2 Changes to the AZ Scheme
Argumentative Zoning II (AZ-II) is a new annota-
tion scheme, which is an elaboration of the orig-
inal AZ scheme. It is presented in Fig. 2. Our
annotation guidelines are 111 sides of A4 and con-
tain a decision tree, detailed description of the se-
mantics of the 15 categories, 75 rules for pairwise
distinction of the categories and copious examples
from both chemistry and computational linguis-
tics. During guideline development, 70 chemistry
papers and 20 CL papers were used, which are dis-
tinct from the ones used for annotation. It took 3
months part-time-work to prepare the guidelines
for CL, and substantially less time to adapt them
for chemistry. We have made them available at
www.cl.cam.ac.uk/research/nl/sciborg.
The differences between the original AZ and
AZ-II are as follows:
? Category AIM remained the same.
? Category BACKGROUND was renamed
CO GRO, or common ground.
? Category OTHER was split into other peo-
ple?s work (OTHR) and the authors? own pre-
vious work (PREV OWN).
? Category BASIS was split into usage (USE)
and support (SUPPORT).
? Category CONTRAST was split into neu-
tral comparison (CODI), contradiction
(ANTISUPP), and a category combining
research gaps with criticism (GAP WEAK).
? Category OWN was split into description of
method (OWN MTHD), results (OWN RES)
and conclusions (OWN CONC), and a cate-
gory which specifies recoverable errors made
by the authors (OWN FAIL).
1494
Category Description Category Description
AIM Statement of specific research goal, or
hypothesis of current paper
OWN CONC Findings, conclusions (non-measurable)
of own work
NOV ADV Novelty or advantage of own approach CODI Comparison, contrast, difference to
other solution (neutral)
CO GRO No knowledge claim is raised (or knowl-
edge claim not significant for the paper)
GAP WEAK Lack of solution in field, problem with
other solutions
OTHR Knowledge claim (significant for paper)
held by somebody else. Neutral descrip-
tion
ANTISUPP Clash with somebody else?s results or
theory; superiority of own work
PREV OWN Knowledge claim (significant) held by
authors in a previous paper. Neutral de-
scription.
SUPPORT Other work supports current work or is
supported by current work
OWN MTHD New Knowledge claim, own work:
methods
USE Other work is used in own work
OWN FAIL A solution/method/experiment in the pa-
per that did not work
FUT Statements/suggestions about future
work (own or general)
OWN RES Measurable/objective outcome of own
work
Figure 2: AZ-II Annotation Scheme.
? Category TEXTUAL was discontinued, be-
cause it is less informative than the other cat-
egories.
? Two new categories were introduced,
NOV ADV (advantages of the new knowl-
edge claim) and FUT (declaration of
limitations or future work).
Our AZ-II categories are more fine-grained than
the original AZ categories. The reasons for this are
twofold: To bring AZ closer to contemporary cita-
tion function schemes, and to incorporate distinc-
tions recently found useful by other researchers.
For instance, Chichester et al (2005) argue that
ANTISUPP is particularly important. The finer
grain in AZ-II has been accomplished purely by
splitting existing AZ categories; hence, the coarser
AZ categories are recoverable (with the exception
of the TEXTUAL category). Annotation examples
are given in the appendix.
As in AZ, citations are an important but not nec-
essarily decisive cue for a sentence to belong to
a particular zone. The guidelines mention cita-
tions as one factor in deciding whether a knowl-
edge claim holds, and citations occur in several
examples, so it is likely that the presence of ci-
tations would have influenced annotators in their
decision.
Of the changes, the distinction which is likely
to have the greatest impact on the annotation is
the split of OWN according to the stage of the au-
thors? problem solving process ? into methods, re-
sults, conclusion or local failure. In most life sci-
ences, descriptions of research as a problem solv-
ing process are a dominant phenomenon, whereby
problem-solving descriptions can be of differing
length and embeddedness. For instance, in syn-
thetic chemistry, the starting compound for the
main synthesis in the paper may first have to be
synthesised itself (if it is not commercially avail-
able, for instance). In that case, arriving at the
compound is an intermediate, smaller problem-
solving process which enables the larger problem-
solving process that represents the new KC.
The original AZ scheme didn?t mark the dis-
tinction, possibly because it is not as easily ob-
servable in CL as it is in the life sciences, and
because problem-solving stages were not part of
the main analytic interest of AZ, which focused
on how scientific argumentation is related to de-
scriptions of own and other work. Also, neither of
the traditional AZ applications (summarisation or
citation indexing) had any direct use for the subdi-
vided categories. But in the life sciences, there
are applications which would make use of such
a subdivision. For instance, in chemistry there
is a niche for search applications which guide
searchers directly to the method and/or result sec-
tions in papers. Specifically, the OWN FAIL cat-
egory is motivated by the failure?and?recovery
search. In text, OWN FAILmarks cases where the
authors helpfully mention in passing steps which
were found not to work during a long synthetic
procedure (often the ?total synthesis? of a com-
pound which is found in nature). Such cases hap-
pen frequently, and are generally followed by a
?recovery? statement which explains how the prob-
lem can be avoided. Another possible applica-
tion that calls for a subdivision is Feltrim et al?s
1495
(2005) rhetorical writing system for novice writ-
ers. It trains novices in writing rhetorically well-
formed abstracts and therefore must have a way of
distinguishing, for instance, between methods and
results.
Note that several of the applications based on
AZ and AZ-II in general rely on the rare categories
much more than they rely on the more frequent
categories. OWN FAIL is an example of a rare but
important category, and so is AIM, which is central
to summarisation applications. The comparative
and contrastive categories CODI ANTISUPP and
GAP WEAK, on the other hand, are particularly
useful to citation-based search applications.
Other AZ-like schemes for scientific discourse
created for the biomedical domain (Mizuta and
Collier, 2004) and for computer science (Feltrim
et al, 2005) also made the decision to subdivide
OWN, in similar ways to how we propose here.
The current work, however, is the first experimen-
tal proof that humans can make this distinction ?
and the others encoded in AZ-II ? reliably, and in
two quite distinct disciplines.
3 Discipline-Independent Non-Expert
Annotation
An important principle of AZ is that its categories
can be decided without domain knowledge. This
rule is anchored in the guidelines: when choosing
a category, no reasoning about the scientific facts
is allowed. The avoidance of domain-knowledge
has its motivation in a strategy for a hypotheti-
cal automatic text-understanding system for unre-
stricted texts. Given the state of the art in text pro-
cessing and knowledge representation, text under-
standing systems should in our opinion use gen-
eral, rhetorical, and logical aspects of the text,
rather than attempting to recognise or represent the
scientific knowledge contained in the text. What
the human annotation ? the gold standard ? should
then do is to simulate the best possible output that
such a system could theoretically create.
Annotators may use only general, rhetorical or
linguistic knowledge; knowledge which is shared
by all proficient speakers of a language. The
guidelines spell out what is meant by these general
principles. For instance, one can use lexical and
syntactic parallelism in a text to infer that the au-
thors were setting up a comparison between them-
selves and some other approach.
There is, however, a problem with annotator ex-
pertise and with the exact implementation of the
?no domain knowledge? principle. This problem
does not become apparent until one starts work-
ing with disciplines where at least some of the an-
notators or guideline developers are not domain
experts (chemistry, in our case). Domain experts
naturally use scientific knowledge and inference
when they make annotation decisions. It would
be unrealistic to expect them to be able to disre-
gard their domain knowledge simply because they
were instructed to do so. Additionally, when all
annotators/scheme developers are domain experts,
it is hard to even notice the cases where they ?ac-
cidentally? use domain knowledge during anno-
tation. We therefore artificially created a situa-
tion where all annotators are ?semi-informed non-
experts?, which forces them to comply with the
principle, namely by the following rules:
Justification: Annotators have to justify all an-
notation decisions by pointing to some text-based
evidence, and by giving the section heading in the
guidelines that describes the particular reason for
assigning the category. General discipline-specific
knowledge an annotator may happen to have is ex-
cluded as justification. Annotators? justifications
have to be typed into the annotation tool and are
open to challenge during the training phase. Much
of the allowable justification comes in the form
of general and linguistic principles, e.g., an ex-
plicit cue phrase, the title, or the structural simi-
larity of textual strings. For instance, annotators
are allowed to infer that process-VPs in the title
are likely to be the contribution (knowledge of the
actual concrete contribution of a paper is a require-
ment for annotation of AIM).
Discipline-specific Generics: The guidelines
contain a section with high-level facts about the
general research practices in the discipline. These
generics constitute the only scientific knowledge
which is acceptable as a justification, and are
aimed to help non-expert annotators recognise
how a paper might relate to already established
scientific knowledge, so that they will be able
to avoid common mistakes about the knowledge
claim status of a certain fact. For instance, the bet-
ter they are able to distinguish what is commonly
known from what is newly claimed by the authors,
the more consistent their annotation will be.
Annotation with expert-trained non-expert an-
notators means that a domain expert must be avail-
able initially, during the development of the anno-
1496
tation scheme and the guidelines, either as a co-
developer or as an informant. The domain expert?s
job is to describe scientific knowledge in that do-
main in a general way, in as far as it is neces-
sary for the scheme?s distinctions, and to write the
domain-specific rules for the individual categories,
including the choice of example sentences. This
means that the guidelines are split into a domain-
general and a domain-specific part.
The discipline-specific generics in chemistry
come in the form of a ?chemistry primer?, a 10-
page collection of high-level scientific domain
knowledge. It contains: a glossary of words a non-
chemist would not have heard about or would not
necessarily recognise as chemical terminology; a
list of possible types of experiments performed
in chemistry; a list of commonly used machin-
ery; a list of non-obvious negative characterisa-
tions of experiments and compounds (?sluggish?,
?inert?); and a list of possible types of knowledge
claims. For instance, in chemistry each chemi-
cal substance mentioned can have in principle a
knowledge claim associated with its discovery or
invention ? with the exception of water, rock salt,
the metals known in prehistory and a few others.
If a compound or process is however considered to
be so commonly used that it is in the ?general do-
main? (e.g., ?the Stern?Volmer equation? or ?the
Grignard reaction?), it is no longer associated with
somebody?s knowledge claim, and as a result its
usage is not to be marked with category USE.
Descriptions of individual categories can have
domain-specific subsections, as well as the gen-
eral ones. For instance, if the text states that the
authors could not replicate a published result, the
guidelines describe the cases when this is the au-
thors? fault (OWN FAIL) in contrast to the cases
where this is an indirect accusation of the previ-
ous experiment (ANTISUPP).
Another potentially unclear distinction is
between results (OWN RES) and conclusions
(OWN CONC). The difference is defined on
the basis of how much reasoning is necessary
to be able to make the statement concerned. If
all the authors did was to read a measurement
off an instrument, the label OWN RES applies.
Reasoning points to OWN CONC; it is some-
times linguistically marked (?therefore?, ?we
conclude?, ?this means that?), but in many cases,
domain knowledge may be required to decide
whether reasoning was necessary to make a
certain statement. Possible OWN RES statements,
according to the chemistry primer, include: state-
ments of simple numerical result; descriptions of
graphs; descriptions of atoms? positions in three-
dimensional space; statements of trends, unless
a reason for these results is given; comparisons
of results of more than one experiment, unless a
reason for these results is given.
The chemistry primer also lists phenomena
which in a typical experiment would be read off
chemical machinery (e.g., ?Stark effect?). This list
gives the non-expert annotator an objective crite-
rion to answer the question how likely it is that a
certain statement by the authors was arrived at by
inference. We also found that our list of phenom-
ena which can be read off machinery, which was
compiled from the first 30 papers, generalised well
to the other 40 papers considered.
The chemistry primer is not an attempt to sum-
marise all methods and experimentation types in
chemistry; this would be impossible to do, cer-
tainly in a few pages. Rather, it tries to answer
many of the high-level questions a non-expert
would have to an expert, in the framework of AZ.
This methodology allows to hire expert and
non-expert annotators and bring them in line with
each other. We believe it could be expanded rel-
atively easily into many other disciplines, using
domain experts which create similar primers for
genetics, experimental physics, cell biology, but
re-using the bulk of the guidelines.
4 Annotation Experiments
The annotators were the co-developers of the an-
notation scheme and the authors of this paper.
Whereas all three annotators have good back-
ground knowledge in CL, the largest difference be-
tween them concerns their expertise in chemistry:
Annotator A is a PhD-level chemist, Annotator B
has two years? of undergraduate training in chem-
istry and can therefore be considered a chemical
semi-expert, and Annotator C has no specialised
chemistry knowledge.
As agreement measure we choose the Kappa
coefficient ? (Fleiss, 1971; Siegel and Castellan,
1988), the agreement measure predominantly used
in natural language processing research (Carletta,
1996). ? corrects raw agreement P (A) for agree-
ment by chance P (E):
? =
P (A)?P (E)
1?P (E)
1497
No matter how many items or annotators, or
how the categories are distributed, ? = 0 when
there is no agreement other than what would be
expected by chance, and ? = 1 when agreement
is perfect. If two annotators agree less than ex-
pected by chance, ? can also be negative. Chance
agreement P (E) is defined as the level of agree-
ment which would be reached by random anno-
tation using the same distribution of categories as
the real annotators. All work done here is reported
in terms of Fleiss? ?. 1 ? is also designed to ab-
stract over the number of annotators as its formula
relies on the proportion of expected vs. observed
pairwise agreements possible in a pool. That is,
? for k annotators will be an average of the val-
ues of ? taking all possible m-tuples of annota-
tors from the annotator pool (with m < k). As a
side effect of its definition of random agreement,
? treats agreement in a rare category as more sur-
prising, and rewards such agreement more than an
agreement in a frequent category. This is a desir-
able property, because we are more interested in
the performance of the rare rhetorical categories
than we are in the performance of the more fre-
quent zone categories.
4.1 Data
For chemistry, 30 random-sampled papers from
journals published between 2004 and 2007 by the
Royal Society of Chemistry were used for anno-
tation2. The papers cover all areas of chemistry
and some areas close to chemistry, such as climate
modelling, process engineering, and a double-
blind medical trial. The data used for the exper-
iment contains a total of 3745 sentences.
For computational linguistics, 9 papers were an-
notated, with a total of 1629 sentences. The papers
were published between 1998 and 2001 at ACL,
EACL or EMNLP conferences, and were taken
from the Computation and Language archive.
Both chemistry and CL papers were automatically
sentence-split, with manual correction of errors;
acknowledgement sections were disregarded. A
1Artstein and Poesio (2008) observe that there are several
version of ? which differ in how P (E) is calculated. In par-
ticular, Fleiss? (1971) ? calculates P (E) as the average ob-
served distribution of all annotators, whereas Cohen?s (1960)
? calculates P (E) only on the basis of the other annotator(s).
2100 papers across a spread of disciplines from the Jan-
uary 2004 issues of the RSC were selected blindly (but with
an attempt to cover most areas of chemistry). 30 out of these
were random sampled for annotation; the rest were used for
annotation development.
Category Chem CL Category Chem CL
OWN MTHD 25.4 55.6 SUPPORT 1.5 0.7
OWN RES 24.0 5.6 GAP WEAK 1.1 1.0
OWN CONC 15.1 10.7 FUT 1.0 1.4
OTHR 8.3 10.0 NOV ADV 1.0 0.8
USE 7.9 2.7 CODI 0.8 1.2
CO GRO 6.7 5.7 OWN FAIL 0.8 0.1
PREV OWN 3.4 1.7 ANTISUPP 0.5 0.6
AIM 2.3 1.8
Figure 3: Frequency of AZ-II Categories (in %).
web-based annotation tool was used for guideline
definition and for annotation.
Our choice of which data sets to use was ef-
fected by the relative length of papers more than
by the journal/conference distinction. Average
article length between chemistry journal articles
(3650 words/paper) and CL conference articles
(4219 words/paper) is comparable, so conference
articles in CL seem a much better choice for com-
parative work than journal publications, which are
often very long in CL. Additionally, conferences
have a high profile in CL, and we found the con-
ference publications to be of high editorial quality.
We are nevertheless interested in the structure of
longer journal articles, and plan to investigate CL
journals in the future.
The annotations were done using a web-based
annotation tool. Every sentence is assigned a cat-
egory. No communication between the annotators
was allowed.
4.2 Results
The inter-annotator agreement for chemistry
was ? = 0.71 (N=3745,n=15,k=3). For CL,
the inter-annotator agreement was ? = 0.65
(N=1629,n=15,k=3). For comparison, the
inter-annotator agreement for the original, CL-
specific AZ with 7 categories was ? = 0.71
(N=3420,n=7,k=3). Given the subjective nature
of the task and the fact that AZ-II introduces ad-
ditional distinctions, the AZ-II agreement can be
considered acceptable for CL and relatively high
for chemistry. Additionally, chemistry annota-
tion used one non-expert annotator, who had no
chemistry-specific domain knowledge apart from
that in the chemistry primer.
The distribution of categories for the two disci-
plines is given in Fig. 3. As expected, there is a
large discrepancy in frequency between the (rare)
rhetorical categories and the (much more fre-
quent) zone categories OWN MTHD, OWN RES,
1498
OWN CONC, OTHR and CO GRO. For supervised
learning, too few examples of any category can be
a problem. There are methods which attempt to re-
duce the annotation effort by using a trained clas-
sifier to suggest possible cases to a human. How-
ever, the classifier can only find examples similar
to the ones that have already been manually clas-
sified, when the real problem is a recall-problem,
i.e., the challenge is to find more new examples in
the multitude of possible sentences. To solve this
in a fundamentally sound way, there seems to be
no other way than to annotate more texts, at the
cost of more human effort.
If we consider the differences across disci-
plines, the most striking ones concern the distri-
bution of OWN MTHD, which is more than twice
as common in CL (56% v. 25%), and OWN RES,
which is far more common in chemistry overall
(24% v 5.6%). Usage of other people?s knowl-
edge claims or materials also seems to be more
common in chemistry, or at least more explicitly
expressed (7.9% vs 2.7%). With respect to the
shorter, rarer categories, there is a marked dif-
ference in OWN FAIL (0.1% in CL, but 0.8% in
chemistry3 and SUPPORT, which is more common
in chemistry (1.5% vs 0.7%). However, this effect
is not present for ANTISUPP (contradiction of re-
sults), the ?reverse? category to SUPPORT, (0.6%
in CL vs 0.5% in chemistry).
As far as the chemistry annotation is con-
cerned, it is interesting to find out whether Annota-
tor A was influenced during annotation by domain
knowledge which Annotator C did not have, and
Annotator B had to a lower degree4. We there-
fore calculated pairwise agreement, which was
?
AC
= 0.66, ?
BC
= 0.73 and ?
AB
= 0.73 (all:
N=3745,n=15,k=2). That means that the largest
disagreements were between the non-expert (C)
and the expert (A), though the differences are
modest. This might point to the fact that Anno-
tators A and B might have used a certain amount
of domain-knowledge which the chemistry primer
in the guidelines does not yet, but should, cover.
In an attempt to determine how well cate-
gories are defined, we first consider the binary dis-
3These are not large differences in absolute terms ? 55
items identified as OWN FAIL by at least one annotator in
chemistry, vs. 7 such items in CL, the relative difference is
large and confirms that in chemistry papers, particularly de-
scriptions of synthesis procedures, OWN FAIL cases appear
relatively frequently.
4This question does not arise in the case of CL, as all an-
notators can be considered experts in this respect.
tinction between zone categories (OWN MTHD,
OWN RES, OWN CONC, OWN FAIL, OTHR,
PREV OWN and CO GRO) and rhetorical cate-
gories (the other 8). This shows an inter-annotator
agreement of ?
binary
= 0.78 (N=3745, n=2, k=3)
for chemistry and ?
binary
= 0.65 (N=1629, n=2,
k=3) for CL, indicating that annotators find it rel-
atively easy (chemistry) or at least not more dif-
ficult than the overall distinction (CL) to distin-
guish these two types of categories. We next per-
form Krippendorff?s (1980) category distinctions
(Fig. 4). Here, all categories apart from the one
diagnosed are collapsed, and what is reported is
the difference of inter-annotator agreement when
compared to the overall distinctiveness (?=0.71
for chemistry, ?=0.65 for CL). Where the differ-
ence is positive, the annotators could distinguish
the given category better than they could distin-
guish all categories, and where they are negative,
correspondingly worse.5
The results confirm that categories USE, AIM,
OWN MTHD, OWN RES and FUT are particularly
well distinguished in both disciplines. This is a
positive result, as these categories are important
for several types of searches. In these cases the
guidelines seem to fully suffice for their descrip-
tion, but then again good performance of AIM,
FUT and USE is not that surprising, as they are
signalled clearly by linguistic and non-linguistic
cues. However, there are three categories with
particularly low distinguishability in both disci-
plines: ANTISUPP, OWN FAIL and PREV OWN.
As ANTISUPP and OWN FAIL are crucial for the
envisaged downstream tasks, the problems with
their definition should be identified and fixed. We
are in the process of systematically troubleshoot-
ing the guidelines for those categories.
The table also shows that category definition
has discipline-specific problems. For instance,
we believe that the fact that distinctiveness for
OWN FAIL is so bad for CL must be due to the
fact that we only encountered very few potential
OWN FAIL cases in this domain. The definition
of the categories SUPPORT and NOV ADV also
seem to be substantially more confusing for CL
than for chemistry. However, CODI is a category
which shows average distinctiveness for CL, but
much worse distinctiveness for chemistry. We be-
lieve this is due to the fact that comparisons of
5All ? values for chemistry were measured with N=3745,
n=2, k=3; for CL with N=1629, n=2, k=3.
1499
methods and approaches are more common in CL
and are clearly expressed, whereas in chemistry
the objects that are involved in comparisons are
more varied and at a lower grade of abstraction
(e.g., compounds, properties of compounds, coef-
ficients, etc.), which obviously has a negative ef-
fect on the distinctiveness of this category.
Category Chem CL Category Chem CL
USE +0.12 +0.00 NOV ADV -0.07 -0.23
AIM +0.09 +0.08 OWN CONC -0.08 -0.13
OWN MTHD +0.05 +0.05 GAP WEAK -0.08 -0.16
OWN RES +0.02 +0.04 PREV OWN -0.11 -0.15
FUT +0.01 +0.06 OWN FAIL -0.19 -0.43
CO GRO -0.01 -0.03 ANTISUPP -0.35 -0.32
SUPPORT -0.04 -0.12 CODI -0.36 +0.00
OTHR -0.06 +0.07
Figure 4: Krippendorff?s Diagnostics for Category
Distinction (?, relative to Overall Distinctiveness).
We also provide a direct comparison of our an-
notation results with those from the original AZ
scheme. Comparisons between two similar anno-
tation schemes can be made by collapsing those
categories in each scheme which are not distin-
guished in the other scheme. Such a comparison
can of course only ever approximate the smallest
common denominator between two schemes.
The AZ-II categories were collapsed into a set
of six categories that closely resemble AZ cate-
gories, as described in section 2 (with OWN simu-
lated by the union of OWN FAIL, OWN MTHD,
OWN RES, OWN CONC, FUT, and NOV ADV).
This created a 6-category AZ annotation.
As TEXTUAL is not marked up in AZ-II, the
original AZ annotation was also collapsed, by in-
corporating TEXTUAL examples into OWN. The
two 6-pronged AZ-annotations are now more di-
rectly comparable. Inter-annotator agreement for
the collapsed AZ-II showed ? = 0.75 (N=3745,
n=6, k=3). This compares favourably to the col-
lapsed AZ?s agreement of ? = 0.71 (N=3420, n=6,
k=3); but when comparing the raw numerical re-
sults one should consider that different data from
different disciplines is used (chemistry in AZ-II,
CL in AZ).
These results should be interpreted as a pos-
itive result for the domain-independence of AZ,
and also for the feasibility of using trained non-
experts as annotators. The additional work that
went into the guidelines has produced annotation
of a high consistency, even though AZ-II provides
more distinctions (15 categories vs. 7 in AZ).
There is also the faint possibility that discourse
annotation of chemistry is intrinsically easier than
discourse annotation of CL, because it is a more
established discipline and not despite of it. For
instance, it is likely that the problem-solving cat-
egories OWN FAIL, OWN MTHD, OWN RES and
OWN CONC are easier to describe in a discipline
with an established methodology (such as chem-
istry), than they are in a younger, developing dis-
cipline such as computational linguistics.
5 Conclusion
Argumentative Zoning is an analysis of the rhetor-
ical progression of the scientific argument in a pa-
per. In this paper, we have made the following
contributions to this analysis:
? We have presented a more informative
scheme, which additionally recognises the
structure of an experiment in terms of prob-
lem solving (method ? results ? conclusions)
and makes more fine-grained distinctions in
some of the sentiment-inspired relational cat-
egories (e.g., criticism and comparisons to
other approaches).
? We introduced an annotation methodology
which attempts to systematically exclude the
use of annotators? extraneous domain knowl-
edge from the annotation.
? We have experimentally shown that human
coders can independently annotate this new
AZ scheme in two distinct disciplines. Our
results show inter-annotator agreements of
?=0.65 and ?=0.71 for computational lin-
guistics and chemistry, respectively.
Overall, the outcome of this work indicates
that the phenomena described in AZ can be de-
fined in a domain-independent way. The experi-
ment also tested how realistic the ?expert-trained
non-expert? approach to domain-knowledge free
annotation is. The fact that the agreement be-
tween three annotators (an expert, a semi-expert,
and a non-expert) is acceptable overall vindicate
our task definition as domain-knowledge free (us-
ing the tools of justification and domain-specific
generic knowledge). However, the agreements in-
volving the semi-expert are higher than the agree-
ment between expert and non-expert. This prob-
ably means that the chemistry generics were not
fully adequate to ensure that the non-expert un-
derstood enough of the chemistry to achieve the
highest-possible agreement.
1500
The automation of AZ-annotation is underway.
This requires adaptation of the high-level features
used in AZ (Teufel and Moens, 2002) to chemistry.
We are also preparing an annotation experiment
with naive annotators. Another research avenue
is the expansion of the guidelines to other disci-
plines such as bio-medicine, and to longer journal
articles, e.g., in computational linguistics.
6 Acknowledgements
This work was funded by EPSRC project Sciborg
(EP/C010035/1).
Appendix: Annotation Examples6
AIM We now describe in this paper a synthetic route for the
functionalisation of the framework of mesoporous organosil-
ica by free phosphine oxide ligands, which can act as a tem-
plate for the introduction of lanthanide ions. (b514878b)
AIM The aim of this paper is to examine the role that train-
ing plays in the tagging process . . . (9410012)
NOV ADV Moreover, the simplicity and ease of application
of the electrochemical method [...] should also be emphasised
and makes it an interesting and valuable synthetic tool.
(b513402a)
NOV ADV Other than the economic factor, an important ad-
vantage of combining morphological analysis and error detec-
tion/correction is the way the lexical tree associated with the
analysis can be used to determine correction possibilities.
(9504024)
CO GRO A wide range of organosulfur compounds are bi-
ologically active and some find commercial application as
fungicides and bactericides1?4 . (b514441h)
CO GRO It has often been stated that discourse is an inher-
ently collaborative process . . . (9504007)
OTHR In their system, antibody immobilized on a solid sub-
strate reacts with antigen, which binds with another antibody
labelled with peroxidase. (b313094k)
OTHR But in Moortgat?s mixed system all the different re-
source management modes of the different systems are left in-
tact in the combination and can be exploited in different parts
of the grammar. (9605016)
PREV OWN As a program aimed at the applications of
imines(2a,g,5) we have studied the formation of carbanions
from imines and their subsequent reactions. (b200198e)
PREV OWN Earlier work of the author (Feldweg 1993;
Feldweg 1995a) within the framework of a project on corpus
based development of lexical knowledge bases (ELWIS) has
produced LIKELY . . . (9502038)
OWN MTHD In order for it to be useful for our purposes,
the following extensions must be made: (0102021)
OWN MTHD On the other hand, a tertiary amide can be an
excellent linking functional group. (b201987f)
6Corpus examples are taken from our chemistry and CL
data sets; indicated by their respective file numbers.
OWN FAIL Initial attempts to improve the dehydration of 4
via chemical or thermal means were unsuccessful; similarly,
attempts to couple the chlorosilane (Me3Si)2 (Me2ClSi)CH
with Ag2O failed. (b510692c)
OWN FAIL When the ABL algorithms try to learn with two
completely distinct sentences, nothing can be learned.
(0104006)
OWN RES While the acid 1a readily coupled to the olefin,
the corresponding boronic ester was surprisingly inert under
the reaction conditions. (b311492a)
OWN RES All the curves have a generally upward trend but
always lie far below backoff (51% error rate). (0001012)
OWN CONC It is unlikely that every VOC emit ted by plants
serves an ecological or physiological role . . . (b507589k)
OWN CONC Unless grammar size takes on proportionately
much more significance for such longer inputs, which seems
implausible, it appears that in fact the major problems do not
lie in the area of grammar size, but in input length. (9405033)
GAP WEAK Various methods of preparation have been de-
veloped, but they often suffer from low yield and tedious
separation.[16,17,28,31] (b200888m)
GAP WEAK Here, we will produce experimental evidence
suggesting that this simple model leads to serious overesti-
mates of system error rates. . . (9407009)
CODI However, the measured values of the dielectric con-
stant (? = 310) are lower than the values reported by Ganguli
and coworkers(21) for BSTO pellets sintered at 1100 degC . . .
(b506578j)
CODI Unlike most research in pragmatics that focuses on
certain types of presuppositions or implicatures, we provide a
global framework in which one can express all these types of
pragmatic inferences. (9504017)
SUPPORT This is in line with the findings of Martin and Illas
for inorganic solids (84,85) . (b515732c)
SUPPORT Work similar to that described here has been car-
ried out by Merialdo (1994), with broadly similar conclusions.
(9410012)
USE The diamine 10 was prepared following a previously
published procedure(4d) . (b110865b)
USE We use the framework for the allocation and transfer
of control of Whittaker and Stenton (1988). (9504007)
FUT Our further efforts are directed towards the above
goal,. . . and overcoming limitations pertaining to the electron-
poor arylboronic acids. (b311492a)
FUT An important area for future research is to develop
principled methods for identifying distinct speaker strategies
pertaining to how they signal segments. (9505025)
ANTISUPP Although purification of 8b to a de of 95percent
has been reported elsewhere[31], in our hands it was always
obtained as a mixture of the two [EQN]-diastereomers.
(b310767a)
ANTISUPP This result challenges the claims of recent dis-
course theories (Grosz and Sidner 1986, Reichman 1985)
which argue for a the close relation between cue words and
discourse structure. (9504006)
1501
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder agree-
ment for computational linguistics. Computational Lin-
guistics, 34(4):555?596.
Jean Carletta. 1996. Assessing agreement on classification
tasks: The kappa statistic. Computational Linguistics,
22(2):249?254.
Christine Chichester, Frdrique Lisacek, Aaron Kaplan, and
Agnes Sandor. 2005. Discovering paradigm shift patterns
in biomedical abstracts: application to neurodegenerative
diseases. In Proceedings of First International Sympo-
sium on Semantic Mining in Biomedicine.
Jacob Cohen. 1960. A coefficient of agreement for nomi-
nal scales. Educational and Psychological Measurement,
20:37?46.
V. Feltrim, Simone Teufel, Gracas Nunes, and S. Alu-
sio. 2005. Argumentative zoning applied to critiquing
novices? scientific abstracts. In Janyce Wiebe James
G. Shanahan, Yan Qu, editor, Computing Attitude and Af-
fect in Text: Theory and Applications, pages 233?245.
Springer, Dordrecht, The Netherlands.
J. L. Fleiss. 1971. Measuring nominal scale agreement
among many raters. Psychological Bulletin, 76:378?381.
Eugene Garfield. 1965. Can citation indexing be automated?
In M. et al Stevens, editor, Statistical Association Meth-
ods for Mechanical Documentation (NBS Misc. Pub. 269).
National Bureau of Standards, Washington.
Mark Garzone and Robert E. Mercer. 2000. Towards an au-
tomated citation classifier. In Proceedings of the 13th Bi-
ennial Conference of the CSCI/SCEIO (AI-2000), pages
337?346.
Ken Hyland. 1998. Persuasion and context: The pragmat-
ics of academic metadiscourse. Journal of Pragmatics,
30(4):437?455.
Klaus Krippendorff. 1980. Content Analysis: An Introduc-
tion to its Methodology. Sage Publications, Beverly Hills,
CA.
Terttu Luukkonen. 1992. Is scientists? publishing behaviour
reward-seeking? Scientometrics, 24:297?319.
Yoko Mizuta and Nigel Collier. 2004. An annotation scheme
for rhetorical analysis of biology articles. In Proceedings
of LREC?2004.
Greg Myers. 1992. In this paper we report...?speech acts
and scientific facts. Journal of Pragmatics, 17(4):295?
313.
Hidetsugu Nanba and Manabu Okumura. 1999. Towards
multi-paper summarization using reference information.
In Proceedings of the XXth International Joint Conference
on Artificial Intelligence (IJCAI-99), pages 926?931.
Sidney Siegel and N. John Jr. Castellan. 1988. Nonparamet-
ric Statistics for the Behavioral Sciences. McGraw-Hill,
Berkeley, CA, 2nd edition.
Ina Spiegel-Ru?sing. 1977. Bibliometric and content analy-
sis. Social Studies of Science, 7:97?113.
John Swales, 1990. Genre Analysis: English in Academic
and Research Settings. Chapter 7: Research articles in
English, pages 110?176. Cambridge University Press,
Cambridge, UK.
Simone Teufel and Marc Moens. 2002. Summarising scien-
tific articles ? experiments with relevance and rhetorical
status. Computational Linguistics, 28(4):409?446.
Simone Teufel, Jean Carletta, and Marc Moens. 1999. An
annotation scheme for discourse-level argumentation in
research articles. In Proceedings of the 9th Meeting of the
European Chapter of the Association for Computational
Linguistics (EACL-99), pages 110?117, Bergen, Norway.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar. 2006.
Automatic classification of citation function. In Proceed-
ings of EMNLP-06.
Simone Teufel. 2001. Task-based evaluation of summary
quality: Describing relationships between scientific pa-
pers. In Proceedings of NAACL-01 Workshop ?Automatic
Text Summarization?, Pittsburgh, PA.
Melvin Weinstock. 1971. Citation indexes. In Encyclopedia
of Library and Information Science, volume 5, pages 16?
40. Dekker, New York, NY.
1502
c? 2002 Association for Computational Linguistics
Summarizing Scientific Articles:
Experiments with Relevance and
Rhetorical Status
Simone Teufel? Marc Moens?
Cambridge University Rhetorical Systems and University of
Edinburgh
In this article we propose a strategy for the summarization of scientific articles that concentrates
on the rhetorical status of statements in an article: Material for summaries is selected in such a
way that summaries can highlight the new contribution of the source article and situate it with
respect to earlier work.
We provide a gold standard for summaries of this kind consisting of a substantial corpus of
conference articles in computational linguistics annotated with human judgments of the rhetorical
status and relevance of each sentence in the articles. We present several experiments measuring
our judges? agreement on these annotations.
We also present an algorithm that, on the basis of the annotated training material, selects
content from unseen articles and classifies it into a fixed set of seven rhetorical categories. The
output of this extraction and classification system can be viewed as a single-document summary
in its own right; alternatively, it provides starting material for the generation of task-oriented
and user-tailored summaries designed to give users an overview of a scientific field.
1. Introduction
Summarization systems are often two-phased, consisting of a content selection step
followed by a regeneration step. In the first step, text fragments (sentences or clauses)
are assigned a score that reflects how important or contentful they are. The highest-
ranking material can then be extracted and displayed verbatim as ?extracts? (Luhn
1958; Edmundson 1969; Paice 1990; Kupiec, Pedersen, and Chen 1995). Extracts are
often useful in an information retrieval environment since they give users an idea as
to what the source document is about (Tombros and Sanderson 1998; Mani et al 1999),
but they are texts of relatively low quality. Because of this, it is generally accepted that
some kind of postprocessing should be performed to improve the final result, by
shortening, fusing, or otherwise revising the material (Grefenstette 1998; Mani, Gates,
and Bloedorn 1999; Jing and McKeown 2000; Barzilay et al 2000; Knight and Marcu
2000).
The extent to which it is possible to do postprocessing is limited, however, by
the fact that contentful material is extracted without information about the general
discourse context in which the material occurred in the source text. For instance, a
sentence describing the solution to a scientific problem might give the main contri-
? Simone Teufel, Computer Laboratory, Cambridge University, JJ Thomson Avenue, Cambridge,
CB3 OFD, England. E-mail: Simone.Teufel@cl.cam.ac.uk
? Marc Moens, Rhetorical Systems and University of Edinburgh, 2 Buccleuch Place, Edinburgh, EH8 9LS,
Scotland. E-mail: marc@cogsci.ed.ac.uk
410
Computational Linguistics Volume 28, Number 4
bution of the paper, but it might also refer to a previous approach that the authors
criticize. Depending on its rhetorical context, the same sentence should be treated very
differently in a summary. We propose in this article a method for sentence and con-
tent selection from source texts that adds context in the form of information about the
rhetorical role the extracted material plays in the source text. This added contextual
information can then be used to make the end product more informative and more
valuable than sentence extracts.
Our application domain is the summarization of scientific articles. Summariza-
tion of such texts requires a different approach from, for example, that used in the
summarization of news articles. For example, Barzilay, McKeown, and Elhadad (1999)
introduce the concept of information fusion, which is based on the identification of re-
current descriptions of the same events in news articles. This approach works well
because in the news domain, newsworthy events are frequently repeated over a short
period of time. In scientific writing, however, similar ?events? are rare: The main focus
is on new scientific ideas, whose main characteristic is their uniqueness and difference
from previous ideas.
Other approaches to the summarization of news articles make use of the typical
journalistic writing style, for example, the fact that the most newsworthy information
comes first; as a result, the first few sentences of a news article are good candidates
for a summary (Brandow, Mitze, and Rau 1995; Lin and Hovy 1997). The structure
of scientific articles does not reflect relevance this explicitly. Instead, the introduction
often starts with general statements about the importance of the topic and its history
in the field; the actual contribution of the paper itself is often given much later.
The length of scientific articles presents another problem. Let us assume that our
overall summarization strategy is first to select relevant sentences or concepts, and
then to synthesize summaries using this material. For a typical 10- to 20-sentence
news wire story, a compression to 20% or 30% of the source provides a reasonable
input set for the second step. The extracted sentences are still thematically connected,
and concepts in the sentences are not taken completely out of context. In scientific ar-
ticles, however, the compression rates have to be much higher: Shortening a 20-page
journal article to a half-page summary requires a compression to 2.5% of the original.
Here, the problematic fact that sentence selection is context insensitive does make a
qualitative difference. If only one sentence per two pages is selected, all information
about how the extracted sentences and their concepts relate to each other is lost; with-
out additional information, it is difficult to use the selected sentences as input to the
second stage.
We present an approach to summarizing scientific articles that is based on the idea
of restoring the discourse context of extracted material by adding the rhetorical status
to each sentence in a document. The innovation of our approach is that it defines
principles for content selection specifically for scientific articles and that it combines
sentence extraction with robust discourse analysis. The output of our system is a list
of extracted sentences along with their rhetorical status (e.g. sentence 11 describes the
scientific goal of the paper, and sentence 9 criticizes previous work), as illustrated in
Figure 1. (The example paper we use throughout the article is F. Pereira, N. Tishby, and
L. Lee?s ?Distributional Clustering of English Words? [ACL-1993, cmp lg/9408011]; it
was chosen because it is the paper most often cited within our collection.) Such lists
serve two purposes: in themselves, they already provide a better characterization of
scientific articles than sentence extracts do, and in the longer run, they will serve as
better input material for further processing.
An extrinsic evaluation (Teufel 2001) shows that the output of our system is al-
ready a useful document surrogate in its own right. But postprocessing could turn
411
Teufel and Moens Summarizing Scientific Articles
AIM 10 Our research addresses some of the same questions and uses similar raw data,
but we investigate how to factor word association tendencies into associations
of words to certain hidden senses classes and associations between the classes
themselves.
11 While it may be worthwhile to base such a model on preexisting sense classes
(Resnik, 1992), in the work described here we look at how to derive the classes
directly from distributional data.
162 We have demonstrated that a general divisive clustering procedure for probability
distributions can be used to group words according to their participation in
particular grammatical relations with other words.
BASIS 19 The corpus used in our first experiment was derived from newswire text auto-
matically parsed by Hindle?s parser Fidditch (Hindle, 1993).
113 The analogy with statistical mechanics suggests a deterministic annealing pro-
cedure for clustering (Rose et al, 1990), in which the number of clusters is
determined through a sequence of phase transitions by continuously increasing
the parameter EQN following an annealing schedule.
CONTRAST 9 His notion of similarity seems to agree with our intuitions in many cases, but it is
not clear how it can be used directly to construct word classes and corresponding
models of association.
14 Class construction is then combinatorially very demanding and depends on
frequency counts for joint events involving particular words, a potentially un-
reliable source of information as we noted above.
Figure 1
Extract of system output for example paper.
0 This paper?s topic is to automatically classify words according to their contexts of use.
4 The problem is that for large enough corpora the number of possible joint events is much
larger than the number of event occurrences in the corpus, so many events are seen rarely
or never, making their frequency counts unreliable estimates of their probabilities. 162
This paper?s specific goal is to group words according to their participation in particular
grammatical relations with other words, 22 more specifically to classify nouns according
to their distribution as direct objects of verbs.
Figure 2
Nonexpert summary, general purpose.
the rhetorical extracts into something even more valuable: The added rhetorical con-
text allows for the creation of a new kind of summary. Consider, for instance, the
user-oriented and task-tailored summaries shown in Figures 2 and 3. Their composi-
tion was guided by fixed building plans for different tasks and different user models,
whereby the building blocks are defined as sentences of a specific rhetorical status.
In our example, most textual material is extracted verbatim (additional material is
underlined in Figures 2 and 3; the original sentences are given in Figure 5). The first
example is a short abstract generated for a nonexpert user and for general information;
its first two sentences give background information about the problem tackled. The
second abstract is aimed at an expert; therefore, no background is given, and instead
differences between this approach and similar ones are described.
The actual construction of these summaries is a complex process involving tasks
such as sentence planning, lexical choice and syntactic realization, tasks that are outside
the scope of this article. The important point is that it is the knowledge about the
rhetorical status of the sentences that enables the tailoring of the summaries according
to users? expertise and task. The rhetorical status allows for other kinds of applications
too: Several articles can be summarized together, contrasts or complementarity among
412
Computational Linguistics Volume 28, Number 4
44 This paper?s goal is to organise a set of linguistic objects such as words according to
the contexts in which they occur, for instance grammatical constructions or n-grams.
22 More specifically: the goal is to classify nouns according to their distribution as direct
objects of verbs. 5 Unlike Hindle (1990), 9 this approach constructs word classes and
corresponding models of association directly. 14 In comparison to Brown et al (1992),
the method is combinatorially less demanding and does not depend on frequency counts
for joint events involving particular words, a potentially unreliable source of information.
Figure 3
Expert summary, contrastive links.
articles can be expressed, and summaries can be displayed together with citation links
to help users navigate several related papers.
The rest of this article is structured as follows: section 2 describes the theoretical
and empirical aspects of document structure we model in this article. These aspects
include rhetorical status and relatedness:
? Rhetorical status in terms of problem solving: What is the goal and
contribution of the paper? This type of information is often marked by
metadiscourse and by conventional patterns of presentation (cf.
section 2.1).
? Rhetorical status in terms of intellectual attribution: What information is
claimed to be new, and which statements describe other work? This type
of information can be recognized by following the ?agent structure? of
text, that is, by looking at all grammatical subjects occurring in sequence
(cf. section 2.2).
? Relatedness among articles: What articles is this work similar to, and in
what respect? This type of information can be found by examining fixed
indicator phrases like in contrast to . . . , section headers, and citations (cf.
section 2.3).
These aspects of rhetorical status are encoded in an annotation scheme that we present
in section 2.4. Annotation of relevance is covered in section 2.5.
In section 3, we report on the construction of a gold standard for rhetorical status
and relevance and on the measurement of agreement among human annotators. We
then describe in section 4 our system that simulates the human annotation. Section 5
presents an overview of the intrinsic evaluation we performed, and section 6 closes
with a summary of the contribution of this work, its limitations, and suggestions for
future work.
2. Rhetorical Status, Citations, and Relevance
It is important for our task to find the right definition of rhetorical status to describe the
content in scientific articles. The definition should both capture generalizations about
the nature of scientific texts and also provide the right kind of information to enable the
construction of better summaries for a practical application. Another requirement is
that the analysis should be applicable to research articles from different presentational
traditions and subject matters.
413
Teufel and Moens Summarizing Scientific Articles
For the development of our scheme, we used the chronologically first 80 articles
in our corpus of conference articles in computational linguistics (articles presented
at COLING, ANLP, and (E)ACL conferences or workshops). Because of the inter-
disciplinarity of the field, the papers in this collection cover a challenging range of
subject matters, such as logic programming, statistical language modeling, theoreti-
cal semantics, computational dialectology, and computational psycholinguistics. Also,
the research methodology and tradition of presentation is very different among these
fields; (computer scientists write very different papers than theoretical linguists). We
thus expect our analysis to be equally applicable in a wider range of disciplines and
subdisciplines other than those named.
2.1 Rhetorical Status
Our model relies on the following dimensions of document structure in scientific
articles.
Problem structure. Research is often described as a problem-solving activity (Jordan
1984; Trawinski 1989; Zappen 1983). Three information types can be expected to occur
in any research article: problems (research goals), solutions (methods), and results. In
many disciplines, particularly the experimental sciences, this problem-solution struc-
ture has been crystallized in a fixed presentation of the scientific material as introduc-
tion, method, result and discussion (van Dijk 1980). But many texts in computational
linguistics do not adhere to this presentation, and our analysis therefore has to be
based on the underlying logical (rhetorical) organization, using textual representation
only as an indication.
Intellectual attribution. Scientific texts should make clear what the new contribution
is, as opposed to previous work (specific other researchers? approaches) and back-
ground material (generally accepted statements). We noticed that intellectual attribu-
tion has a segmental character. Statements in a segment without any explicit attribution
are often interpreted as belonging to the most recent explicit attribution statement
(e.g., Other researchers claim that). Our rhetorical scheme assumes that readers have
no difficulty in understanding intellectual attribution, an assumption that we verified
experimentally.
Scientific argumentation. In contrast to the view of science as a disinterested ?fact
factory,? researchers like Swales (1990) have long claimed that there is a strong social
aspect to science, because the success of a researcher is correlated with her ability to
convince the field of the quality of her work and the validity of her arguments. Au-
thors construct an argument that Myers (1992) calls the ?rhetorical act of the paper?:
The statement that their work is a valid contribution to science. Swales breaks down
this ?rhetorical act? into single, nonhierarchical argumentative moves (i.e., rhetorically
coherent pieces of text, which perform the same communicative function). His Con-
structing a Research Space (CARS) model shows how patterns of these moves can be
used to describe the rhetorical structure of introduction sections of physics articles.
Importantly, Swales?s moves describe the rhetorical status of a text segment with re-
spect to the overall message of the document, and not with respect to adjacent text
segments.
Attitude toward other people?s work. We are interested in how authors include refer-
ence to other work into their argument. In the flow of the argument, each piece of
other work is mentioned for a specific reason: it is portrayed as a rival approach, as
a prior approach with a fault, or as an approach contributing parts of the authors?
own solution. In well-written papers, this relation is often expressed in an explicit
way. The next section looks at the stylistic means available to the author to express
the connection between previous approaches and their own work.
414
Computational Linguistics Volume 28, Number 4
2.2 Metadiscourse and Agentivity
Explicit metadiscourse is an integral aspect of scientific argumentation and a way of
expressing attitude toward previous work. Examples for metadiscourse are phrases
like we argue that and in contrast to common belief, we. Metadiscourse is ubiquitous in
scientific writing: Hyland (1998) found a metadiscourse phrase on average after every
15 words in running text.
A large proportion of scientific metadiscourse is conventionalized, particularly in
the experimental sciences, and particularly in the methodology or result section (e.g.,
we present original work . . . , or An ANOVA analysis revealed a marginal interaction/a main ef-
fect of . . . ). Swales (1990) lists many such fixed phrases as co-occurring with the moves
of his CARS model (pages 144, 154?158, 160?161). They are useful indicators of overall
importance (Pollock and Zamora 1975); they can also be relatively easily recognized
with information extraction techniques (e.g., regular expressions). Paice (1990) intro-
duces grammars for pattern matching of indicator phrases, e.g., the aim/purpose of this
paper/article/study and we conclude/propose.
Apart from this conventionalized metadiscourse, we noticed that our corpus con-
tains a large number of metadiscourse statements that are less formalized: statements
about aspects of the problem-solving process or the relation to other work. Figure 4,
for instance, shows that there are many ways to say that one?s research is based on
somebody else?s (?research continuation?). The sentences do not look similar on the
surface: The syntactic subject can be the authors, the originators of the method, or
even the method itself. Also, the verbs are very different (base, be related, use, follow).
Some sentences use metaphors of change and creation. The wide range of linguistic
expression we observed presents a challenge for recognition and correct classification
using standard information extraction patterns.
With respect to agents occurring in scientific metadiscourse, we make two sug-
gestions: (1) that scientific argumentation follows prototypical patterns and employs
recurrent types of agents and actions and (2) that it is possible to recognize many of
these automatically. Agents play fixed roles in the argumentation, and there are so
? We employ Suzuki?s algorithm to learn case frame patterns as dendroid distributions.
(9605013)
? Our method combines similarity-based estimates with Katz?s back-off scheme, which is widely used for language
modeling in speech recognition. (9405001)
? Thus, we base our model on the work of Clark and Wilkes-Gibbs (1986), and Heeman and Hirst (1992) . . .
(9405013)
? The starting point for this work was Scha and Polanyi?s discourse grammar (Scha and Polanyi, 1988; Pruest et
al., 1994). (9502018)
? We use the framework for the allocation and transfer of control of Whittaker and Stenton (1988).
(9504007)
? Following Laur (1993), we consider simple prepositions (like ?in?) as well as prepositional phrases (like ?in front
of?). (9503007)
? Our lexicon is based on a finite-state transducer lexicon (Karttunen et al, 1992).
(9503004)
? Instead of . . . we will adopt a simpler, monostratal representation that is more closely related to those found in
dependency grammars (e.g., Hudson (1984)). (9408014)
Figure 4
Statements expressing research continuation, with source article number.
415
Teufel and Moens Summarizing Scientific Articles
few of these roles that they can be enumerated: agents appear as rivals, as contrib-
utors of part of the solution (they), as the entire research community in the field, or
as the authors of the paper themselves (we). Note the similarity of agent roles to the
three kinds of intellectual attribution mentioned above. We also propose prototypical
actions frequently occurring in scientific discourse: the field might agree, a particular
researcher can suggest something, and a certain solution could either fail or be success-
ful. In section 4 we will describe the three features used in our implementation that
recognize metadiscourse.
Another important construct that expresses relations to other researchers? work is
formal citations, to which we will now turn.
2.3 Citations and Relatedness
Citation indexes are constructs that contain pointers between cited texts and citing
texts (Garfield 1979), traditionally in printed form. When done on-line (as in CiteSeer
[Lawrence, Giles, and Bollacker 1999], or as in Nanba and Okumura?s [1999] work),
citations are presented in context for users to browse. Browsing each citation is time-
consuming, but useful: just knowing that an article cites another is often not enough.
One needs to read the context of the citation to understand the relation between the
articles. Citations may vary in many dimensions; for example, they can be central or
perfunctory, positive or negative (i.e., critical); apart from scientific reasons, there is
also a host of social reasons for citing (?politeness, tradition, piety? [Ziman 1969]).
We concentrate on two citation contexts that are particularly important for the
information needs of researchers:
? Contexts in which an article is cited negatively or contrastively.
? Contexts in which an article is cited positively or in which the authors
state that their own work originates from the cited work.
A distinction among these contexts would enable us to build more informative citation
indexes. We suggest that such a rhetorical distinction can be made manually and
automatically for each citation; we use a large corpus of scientific papers along with
humans? judgments of this distinction to train a system to make such distinctions.
2.4 The Rhetorical Annotation Scheme
Our rhetorical annotation scheme (cf. Table 1) encodes the aspects of scientific argu-
mentation, metadiscourse, and relatedness to other work described before. The cat-
egories are assigned to full sentences, but a similar scheme could be developed for
clauses or phrases.
The annotation scheme is nonoverlapping and nonhierarchical, and each sentence
must be assigned to exactly one category. As adjacent sentences of the same status can
be considered to form zones of the same rhetorical status, we call the units rhetorical
zones. The shortest of these zones are one sentence long.
The rhetorical status of a sentence is determined on the basis of the global context
of the paper. For instance, whereas the OTHER category describes all neutral descrip-
tions of other researchers? work, the categories BASIS and CONTRAST are applicable to
sentences expressing a research continuation relationship or a contrast to other work.
Generally accepted knowledge is classified as BACKGROUND, whereas the author?s own
work is separated into the specific research goal (AIM) and all other statements about
the author?s own work (OWN).
416
Computational Linguistics Volume 28, Number 4
Table 1
Annotation scheme for rhetorical status.
AIM Specific research goal of the current paper
TEXTUAL Statements about section structure
OWN (Neutral) description of own work presented in current paper: Method-
ology, results, discussion
BACKGROUND Generally accepted scientific background
CONTRAST Statements of comparison with or contrast to other work; weaknesses of
other work
BASIS Statements of agreement with other work or continuation of other work
OTHER (Neutral) description of other researchers? work
The annotation scheme expresses important discourse and argumentation aspects
of scientific articles, but with its seven categories it is not designed to model the full
complexity of scientific texts. The category OWN, for instance, could be further sub-
divided into method (solution), results, and further work, which is not done in the
work reported here. There is a conflict between explanatory power and the simplicity
necessary for reliable human and automatic classification, and we decided to restrict
ourselves to the rhetorical distinctions that are most salient and potentially most useful
for several information access applications. The user-tailored summaries and more in-
formative citation indexes we mentioned before are just two such applications; another
one is the indexing and previewing of the internal structure of the article. To make
such indexing and previewing possible, our scheme contains the additional category
TEXTUAL, which captures previews of section structure (section 2 describes our data . . . ).
Such previews would make it possible to label sections with the author?s indication
of their contents.
Our rhetorical analysis, as noted above, is nonhierarchical, in contrast to Rhetorical
Structure Theory (RST) (Mann and Thompson 1987; Marcu 1999), and it concerns
text pieces at a lower level of granularity. Although we do agree with RST that the
structure of text is hierarchical in many cases, it is our belief that the relevance and
function of certain text pieces can be determined without analyzing the full hierarchical
structure of the text. Another difference between our analysis and that of RST is that
our analysis aims at capturing the rhetorical status of a piece of text in respect to the
overall message, and not in relation to adjacent pieces of text.
2.5 Relevance
As our immediate goal is to select important content from a text, we also need a second
set of gold standards that are defined by relevance (as opposed to rhetorical status).
Relevance is a difficult issue because it is situational to a unique occasion (Saracevic
1975; Sparck Jones 1990; Mizzaro 1997): Humans perceive relevance differently from
each other and differently in different situations. Paice and Jones (1993) report that they
abandoned an informal sentence selection experiment in which they used agriculture
articles and experts in the field as participants, as the participants were too strongly
influenced by their personal research interest.
As a result of subjectivity, a number of human sentence extraction experiments
over the years have resulted in low agreement figures. Rath, Resnick, and Savage
(1961) report that six participants agreed on only 8% of 20 sentences they were asked
to select out of short Scientific American texts and that five agreed on 32% of the
sentences. They found that after six weeks, subjects selected on average only 55%
of the sentences they themselves selected previously. Edmundson and Wyllys (1961)
417
Teufel and Moens Summarizing Scientific Articles
find similarly low human agreement for research articles. More recent experiments
reporting more positive results all used news text (Jing et al 1998; Zechner 1995).
As discussed above, the compression rates on news texts are far lower: there are
fewer sentences from which to choose, making it easier to agree on which ones to
select. Sentence selection from scientific texts also requires more background knowl-
edge, thus importing an even higher level of subjectivity into sentence selection
experiments.
Recently, researchers have been looking for more objective definitions of relevance.
Kupiec, Pedersen, and Chen (1995) define relevance by abstract similarity: A sentence
in a document is considered relevant if it shows a high level of similarity to a sentence
in the abstract. This definition of relevance has the advantage that it is fixed (i.e., the
researchers have no influence over it). It relies, however, on two assumptions: that the
writing style is such that there is a high degree of overlap between sentences in the
abstract and in the main text and that the abstract is indeed the target output that is
most adequate for the final task.
In our case, neither assumption holds. First, the experiments in Teufel and Moens
(1997) showed that in our corpus only 45% of the abstract sentences appear elsewhere
in the body of the document (either as a close variant or in identical form), whereas
Kupiec, Pedersen, and Chen report a figure of 79%. We believe that the reason for the
difference is that in our case the abstracts were produced by the document authors and
by professional abstractors in Kupiec, Pedersen, and Chen?s case. Author summaries
tend to be less systematic (Rowley 1982) and more ?deep generated,? whereas sum-
maries by professional abstractors follow an internalized building plan (Liddy 1991)
and are often created through sentence extraction (Lancaster 1998).
Second, and more importantly, the abstracts and improved citation indexes we
intend to generate are not modeled on traditional summaries, which do not pro-
vide the type of information needed for the applications we have in mind. Infor-
mation about related work plays an important role in our strategy for summarization
and citation indexing, but such information is rarely found in abstracts. We empir-
ically found that the rhetorical status of information occurring in author abstracts
is very limited and consists mostly of information about the goal of the paper and
specifics of the solution. Details of the analysis we conducted on this topic are given in
section 3.2.2.
We thus decided to augment our corpus with an independent set of human judg-
ments of relevance. We wanted to replace the vague definition of relevance often
used in sentence extraction experiments with a more operational definition based on
rhetorical status. For instance, a sentence is considered relevant only if it describes
the research goal or states a difference with a rival approach. More details of the
instructions we used to make the relevance decisions are given in section 3.
Thus, we have two parallel human annotations in our corpus: rhetorical annotation
and relevance selection. In both tasks, each sentence in the articles is classified: Each
sentence receives one rhetorical category and also the label irrelevant or relevant. This
strategy can create redundant material (e.g., when the same fact is expressed in a
sentence in the introduction, a sentence in the conclusion, and one in the middle of
the document). But this redundancy also helps mitigate one of the main problems
with sentence-based gold standards, namely, the fact that there is no one single best
extract for a document. In our annotation, all qualifying sentences in the document
are identified and classified into the same group, which makes later comparisons
with system performance fairer. Also, later steps cannot only find redundancy in the
intermediate result and remove it, but also use the redundancy as an indication of
importance.
418
Computational Linguistics Volume 28, Number 4
Aim:
10 Our research addresses some of the same questions and uses similar raw data, but we investigate
how to factor word association tendencies into associations of words to certain hidden senses
classes and associations between the classes themselves.
22 We will consider here only the problem of classifying nouns according to their distribution as
direct objects of verbs; the converse problem is formally similar.
25 The problem we study is how to use the EQN to classify the EQN.
44 In general, we are interested on how to organise a set of linguistic objects such as words according
to the contexts in which they occur, for instance grammatical constructions or n-grams.
46 Our problem can be seen as that of learning a joint distribution of pairs from a large sample of
pairs.
162 We have demonstrated that a general divisive clustering procedure for probability distributions
can be used to group words according to their participation in particular grammatical relations
with other words.
Background:
0 Methods for automatically classifying words according to their contexts of use have both scientific
and practical interest.
4 The problem is that for large enough corpora the number of possible joint events is much larger
than the number of event occurrences in the corpus, so many events are seen rarely or never,
making their frequency counts unreliable estimates of their probabilities.
Own (Details of Solution):
66 The first stage of an iteration is a maximum likelihood, or minimum distortion, estimation of
the cluster centroids given fixed membership probabilities.
140 The evaluation described below was performed on the largest data set we have worked with so far,
extracted from 44 million words of 1988 Associated Press newswire with the pattern matching
techniques mentioned earlier.
163 The resulting clusters are intuitively informative, and can be used to construct class-based word
coocurrence [sic] models with substantial predictive power.
Contrast with Other Approaches/Weaknesses of Other Approaches:
9 His notion of similarity seems to agree with our intuitions in many cases, but it is not clear how
it can be used directly to construct word classes and corresponding models of association.
14 Class construction is then combinatorially very demanding and depends on frequency counts
for joint events involving particular words, a potentially unreliable source of information as we
noted above.
41 However, this is not very satisfactory because one of the goals of our work is precisely to avoid
the problems of data sparseness by grouping words into classes.
Basis (Imported Solutions):
65 The combined entropy maximization entropy [sic] and distortion minimization is carried out
by a two-stage iterative process similar to the EM method (Dempster et al, 1977).
113 The analogy with statistical mechanics suggests a deterministic annealing procedure for clus-
tering (Rose et al, 1990), in which the number of clusters is determined through a sequence
of phase transitions by continuously increasing the parameter EQN following an annealing
schedule.
153 The data for this test was built from the training data for the previous one in the following way,
based on a suggestion by Dagan et al (1993).
Figure 5
Example of manual annotation: Relevant sentences with rhetorical status.
Figure 5 gives an example of the manual annotation. Relevant sentences of all
rhetorical categories are shown. Our system creates a list like the one in Figure 5
automatically (Figure 12 shows the actual output of the system when run on the
example paper). In the next section, we turn to the manual annotation step and the
development of the gold standard used during system training and system evaluation.
419
Teufel and Moens Summarizing Scientific Articles
3. Human Judgments: The Gold Standard
For any linguistic analysis that requires subjective interpretation and that is therefore
not objectively true or false, it is important to show that humans share some intuitions
about the analysis. This is typically done by showing that they can apply it indepen-
dently of each other and that the variation they display is bounded (i.e., not arbitrarily
high). The argument is strengthened if the judges are people other than the developers
of the analysis, preferably ?na??ve? subjects (i.e., not computational linguists). Apart
from the cognitive validation of our analysis, high agreement is essential if the anno-
tated corpus is to be used as training material for a machine learning process, like the
one we describe in section 4. Noisy and unreliably annotated training material will
very likely deteriorate the classification performance.
In inherently subjective tasks, it is also common practice to consider human perfor-
mance as an upper bound. The theoretically best performance of a system is reached
if agreement among a pool of human annotators does not decrease when the system
is added to the pool. This is so because an automatic process cannot do any better in
this situation than to be indistinguishable from human performance.
3.1 Corpus
The annotated development corpus consists of 80 conference articles in computational
linguistics (12,188 sentences; 285,934 words). It is part of a larger corpus of 260 articles
(1.1 million words) that we collected from the CMP LG archive (CMP LG 1994). The
appendix lists the 80 articles (archive numbers, titles and authors) of our development
corpus; it consists of the 80 chronologically oldest articles in the larger corpus, con-
taining articles deposited between May 1994 and May 1996 (whereas the entire corpus
stretches until 2001).
Papers were included if they were presented at one of the following conferences
(or associated workshops): the annual meeting of the Association for Computational
Linguistics (ACL), the meeting of the European Chapter of the Association for Compu-
tational Linguistics (EACL), the conference on Applied Natural Language Processing
(ANLP), the International Joint Conference on Artificial Intelligence (IJCAI), or the In-
ternational Conference on Computational Linguistics (COLING). As mentioned above,
a wide range of different subdomains of the field of computational linguistics are
covered.
We added Extensible Markup Language (XML) markup to the corpus: Titles, au-
thors, conference, date, abstract, sections, headlines, paragraphs, and sentences were
marked up. Equations, tables, images were removed and replaced by placeholders.
Bibliography lists were marked up and parsed. Citations and occurrences of au-
thor names in running text were recognized, and self-citations were recognized and
specifically marked up. (Linguistic) example sentences and example pseudocode were
manually marked up, such that clean textual material (i.e., the running text of the
article without interruptions) was isolated for automatic processing. The implemen-
tation uses the Text Tokenization Toolkit (TTT) software (Grover, Mikheev, and
Matheson 1999).
3.2 Annotation of Rhetorical Status
The annotation experiment described here (and in Teufel, Carletta, and Moens [1999]
in more detail) tests the rhetorical annotation scheme presented in section 2.4.
420
Computational Linguistics Volume 28, Number 4
3.2.1 Rationale and Experimental Design.
Annotators. Three task-trained annotators were used: Annotators A and B have
degrees in cognitive science and speech therapy. They were paid for the experiment.
Both are well-used to reading scientific articles for their studies and roughly under-
stand the contents of the articles they annotated because of the closeness of their fields
to computational linguistics. Annotator C is the first author. We did not want to de-
clare annotator C the expert annotator; we believe that in subjective tasks like the one
described here, there are no real experts.
Guidelines. Written guidelines (17 pages) describe the semantics of the categories,
ambiguous cases, and decision strategies. The guidelines also include the decision tree
reproduced in Figure 6.
Training. Annotators received a total of 20 hours of training. Training consisted
of the presentation of annotation of six example papers and the annotation of eight
training articles under real conditions (i.e., independently). In subsequent training ses-
sions, decision criteria for difficult cases encountered in the training articles were dis-
cussed. Obviously, the training articles were excluded from measurements of human
agreement.
Materials and procedure. Twenty-five articles were used for annotation. As no annota-
tion tool was available at the time, annotation was performed on paper; the categories
were later transferred to the electronic versions of the articles by hand. Skim-reading
and annotation typically took between 20 and 30 minutes per article, but there were
no time restrictions. No communication between the annotators was allowed during
annotation. Six weeks after the initial annotation, annotators were asked to reannotate
6 random articles out of the 25.
Evaluation measures. We measured two formal properties of the annotation: stability
and reproducibility (Krippendorff 1980). Stability, the extent to which one annotator
will produce the same classifications at different times, is important because an unsta-
ble annotation scheme can never be reproducible. Reproducibility, the extent to which
different annotators will produce the same classifications, is important because it mea-
sures the consistency of shared understandings (or meaning) held between annotators.
work of the authors)?
Does this sentence refer to new, current
BACKGROUND
CONTRAST
YES NO
YES NO
NOYES
YES NO
YES
BASIS
NO
NOYES
AIM
OWN
background, including phenomena
Does the sentence describe general
to be explained or linguistic example sentences?
that describes the specific aim
Does this sentence contain material
of the paper?
explicit reference to the
structure of the paper?
Does this sentence make
TEXTUAL
OTHER
or comparison of the own work to it?
of other work, or a contrast
Does it describe a negative aspect
or support for the current paper?
Does this sentence mention
other work as basis of 
work by the authors (excluding previous 
Figure 6
Decision tree for rhetorical annotation.
421
Teufel and Moens Summarizing Scientific Articles
We use the kappa coefficient K (Siegel and Castellan 1988) to measure stability and
reproducibility, following Carletta (1996). The kappa coefficient is defined as follows:
K =
P(A) ? P(E)
1 ? P(E)
where P(A) is pairwise agreement and P(E) random agreement. K varies between 1
when agreement is perfect and ?1 when there is a perfect negative correlation. K = 0
is defined as the level of agreement that would be reached by random annotation
using the same distribution of categories as the actual annotators did.
The main advantage of kappa as an annotation measure is that it factors out
random agreement by numbers of categories and by their distribution. As kappa also
abstracts over the number of annotators considered, it allows us to compare the agree-
ment numerically among a group of human annotators with the agreement between
the system and one or more annotators (section 5), which we use as one of the per-
formance measures of the system.
3.2.2 Results. The annotation experiments show that humans distinguish the seven
rhetorical categories with a stability of K = .82, .81, .76 (N = 1,220; k = 2, where K
stands for the kappa coefficient, N for the number of items (sentences) annotated, and
k for the number of annotators). This is equivalent to 93%, 92%, and 90% agreement.
Reproducibility was measured at K = .71 (N = 4,261, k = 3), which is equivalent
to 87% agreement. On Krippendorff?s (1980) scale, agreement of K = .8 or above is
considered as reliable, agreement of .67?.8 as marginally reliable, and less than .67 as
unreliable. On Landis and Koch?s (1977) more forgiving scale, agreement of .0?.2 is
considered as showing ?slight? correlation, .21?.4 as ?fair,? .41?.6 as ?moderate,? .61?
0.8 as ?substantial,? and .81 ?1.0 as ?almost perfect.? According to these guidelines,
our results can be considered reliable, substantial annotation.
Figure 7 shows that the distribution of the seven categories is very skewed, with
67% of all sentences being classified as OWN. (The distribution was calculated using
all three judgments per sentence [cf. the calculation of kappa]. The total number of
items is then k ? N, i.e., 12,783 in this case.)
Table 2 shows a confusion matrix between two annotators. The numbers repre-
sent absolute sentence numbers, and the diagonal (boldface numbers) are the counts of
sentences that were identically classified by both annotators. We used Krippendorff?s
diagnostics to determine which particular categories humans had most problems with:
For each category, agreement is measured with a new data set in which all categories
Figure 7
Distribution of rhetorical categories (entire document).
422
Computational Linguistics Volume 28, Number 4
Table 2
Confusion matrix between annotators B and C.
ANNOTATOR B
AIM CTR TXT OWN BKG BAS OTH Total
ANNOTATOR C AIM 35 2 1 19 3 2 62
CTR 86 31 16 23 156
TXT 31 7 1 39
OWN 10 62 5 2,298 25 3 84 2,487
BKG 5 13 115 20 153
BAS 2 18 1 18 14 53
OTH 1 18 2 55 10 1 412 499
Total 48 173 39 2,441 170 22 556 3,449
except for the category of interest are collapsed into one metacategory. Original agree-
ment is compared to that measured on the new (artificial) data set; high values show
that annotators can distinguish the given category well from all others. When their re-
sults are compared to the overall reproducibility of K = .71, the annotators were good
at distinguishing AIM (Krippendorff?s diagnostics; K = .79) and TEXTUAL (K = .79).
The high agreement in AIM sentences is a positive result that seems to be at odds with
previous sentence extraction experiments. We take this as an indication that some types
of rhetorical classification are easier for human minds to do than unqualified relevance
decision. We also think that the positive results are partly due to the existence of the
guidelines.
The annotators were less consistent at determining BASIS (K = .49) and CONTRAST
(K = .59). The same picture emerges if we look at precision and recall of single cate-
gories between two annotators (cf. Table 3). Precision and recall for AIM and TEXTUAL
are high at 72%/56% and 79%/79%, whereas they are lower for CONTRAST (50%/55%)
and BASIS (82%/34%).
This contrast in agreement might have to do with the location of the rhetori-
cal zones in the paper: AIM and TEXTUAL zones are usually found in fixed locations
(beginning or end of the introduction section) and are explicitly marked with metadis-
course, whereas CONTRAST sentences, and even more so BASIS sentences, are usually
interspersed within longer OWN zones. As a result, these categories are more exposed
to lapses of attention during annotation.
With respect to the longer, more neutral zones (intellectual attribution), annotators
often had problems in distinguishing OTHER work from OWN work, particularly in
cases where the authors did not express a clear distinction between new work and
previous own work (which, according to our instructions, should be annotated as OTHER).
Another persistently problematic distinction for our annotators was that between OWN
Table 3
Annotator C?s precision and recall per category if annotator B is gold standard.
AIM CTR TXT OWN BKG BAS OTH
Precision 72% 50% 79% 94% 68% 82% 74%
Recall 56% 55% 79% 92% 75% 34% 83%
423
Teufel and Moens Summarizing Scientific Articles
and BACKGROUND. This could be a sign that some authors aimed their papers at an
expert audience and thus thought it unnecessary to signal clearly which statements
are commonly agreed upon in the field, as opposed to their own new claims. If a
paper is written in such a way, it can indeed be understood only with a considerable
amount of domain knowledge, which our annotators did not have.
Because intellectual attribution (the distinction between OWN, OTHER, and BACK-
GROUND material) is an important part of our annotation scheme, we conducted a
second experiment measuring how well our annotators could distinguish just these
three roles, using the same annotators and 22 different articles. We wrote seven pages
of new guidelines describing the semantics of the three categories. Results show higher
stability compared to the full annotation scheme (K = .83, .79, .81; N = 1,248; k = 2)
and higher reproducibility (K = .78, N = 4,031, k = 3), corresponding to 94%, 93%,
and 93% agreement (stability) and 93% (reproducibility). It is most remarkable that
agreement of annotation of intellectual attribution in the abstracts is almost perfect:
K = .98 (N = 89, k = 3), corresponding to 99% agreement. This points to the fact that
authors, when writing abstracts for their papers, take care to make it clear to whom
a certain statement is attributed. This effect also holds for the annotation with the
full scheme with all seven categories: again, reproducibility in the abstract is higher
(K = .79) than in the entire document (K = .71), but the effect is much weaker.
Abstracts might be easier to annotate than the rest of a paper, but this does not
necessarily make it possible to define a gold standard solely by looking at the ab-
stracts. As foreshadowed in section 2.5, abstracts do not contain all types of rhetorical
information. AIM and OWN sentences make up 74% of the sentences in abstracts, and
only 5% of all CONTRAST sentences and 3% of all BASIS sentences occur in the abstract.
Abstracts in our corpus are also not structurally homogeneous. When we inspected
the rhetorical structure of abstracts in terms of sequences of rhetorical zones, we found
a high level of variation. Even though the sequence AIM?OWN is very common (con-
tained in 73% of all abstracts), the 80 abstracts still contain 40 different rhetorical
sequences, 28 of which are unique. This heterogeneity is in stark contrast to the sys-
tematic structures Liddy (1991) found to be produced by professional abstractors. Both
observations, the lack of certain rhetorical types in the abstracts and their rhetorical
heterogeneity, reassure us in our decision not to use human-written abstracts as a gold
standard.
3.3 Annotation of Relevance
We collected two different kinds of relevance gold standards for the documents in our
development corpus: abstract-similar document sentences and additional manually
selected sentences.
In order to establish alignment between summary and document sentences, we
used a semiautomatic method that relies on a simple surface similarity measure (long-
est common subsequence of content words, i.e., excluding words on a stop list). As in
Kupiec, Pedersen, and Chen?s experiment, final alignment was decided by a human
judge, and the criterion was semantic similarity of the two sentences. The following
sentence pair illustrates a direct match:
Summary: In understanding a reference, an agent determines his
confidence in its adequacy as a means of identifying the referent.
Document: An agent understands a reference once he is confident
in the adequacy of its (inferred) plan as a means of identifying the
referent.
424
Computational Linguistics Volume 28, Number 4
Of the 346 abstract sentences contained in the 80 documents, 156 (45%) could be
aligned this way. Because of this low agreement and because certain rhetorical types
are not present in the abstracts, we decided not to rely on abstract alignment as our
only gold standard. Instead, we used manually selected sentences as an alternative
gold standard, which is more informative, but also more subjective.
We wrote eight pages of guidelines that describe relevance criteria (e.g., our defi-
nition prescribes that neutral descriptions of other work be selected only if the other
work is an essential part of the solution presented, whereas all statements of criti-
cism are to be included). The first author annotated all documents in the development
corpus for relevance using the rhetorical zones and abstract similarity as aides in the
relevance decision, and also skim-reading the whole paper before making the decision.
This resulted in 5 to 28 sentences per paper and a total of 1,183 sentences.
Implicitly, rhetorical classification of the extracted sentences was already given as
each of these sentences already had a rhetorical status assigned to it. However, the
rhetorical scheme we used for this task is slightly different. We excluded TEXTUAL, as
this category was designed for document uses other than summarization. If a selected
sentence had the rhetorical class TEXTUAL, it was reclassified into one of the other
six categories. Figure 8 shows the resulting category distribution among these 1,183
sentences, which is far more evenly distributed than the one covering all sentences (cf.
Figure 7). CONTRAST and OWN are the two most frequent categories.
We did not verify the relevance annotation with human experiments. We accept
that the set of sentences chosen by the human annotator is only one possible gold
standard. What is more important is that humans can agree on the rhetorical status of
the relevant sentences. Liddy observed that agreement on rhetorical status was easier
for professional abstractors than sentence selection: Although they did not necessarily
agree on which individual sentences should go into an abstract, they did agree on the
rhetorical information types that make up a good abstract.
We asked our trained annotators to classify a set of 200 sentences, randomly
sampled from the 1,183 sentences selected by the first author, into the six rhetori-
cal categories. The sentences were presented in order of occurrence in the document,
but without any context in terms of surrounding sentences. We measured stability at
K = .9, .86, .83 (N = 100, k = 2) and reproducibility at K = .84 (N = 200, k = 3). These
results are reassuring: They show that the rhetorical status for important sentences can
be particularly well determined, better than rhetorical status for all sentences in the
document (for which reproducibility was K = .71; cf. section 3.2.2).
Figure 8
Distribution of rhetorical categories (relevant sentences).
425
Teufel and Moens Summarizing Scientific Articles
4. The System
We now describe an automatic system that can perform extraction and classification
of rhetorical status on unseen text (cf. also a prior version of the system reported
in Teufel and Moens [2000] and Teufel [1999]). We decided to use machine learning
to perform this extraction and classification, based on a variety of sentential features
similar to the ones reported in the sentence extraction literature. Human annotation is
used as training material such that the associations between these sentential features
and the target sentences can be learned. It is also used as gold standard for intrinsic
system evaluation.
A simpler machine learning approach using only word frequency information
and no other features, as typically used in tasks like text classification, could have
been employed (and indeed Nanba and Okumura [1999] do so for classifying citation
contexts). To test if such a simple approach would be enough, we performed a text
categorization experiment, using the Rainbow implementation of a na??ve Bayes term
frequency times inverse document frequency (TF*IDF) method (McCallum 1997) and
considering each sentence as a ?document.? The result was a classification performance
of K = .30; the classifier nearly always chooses OWN and OTHER segments. The rare but
important categories AIM, BACKGROUND, CONTRAST, and BASIS could be retrieved only
with low precision and recall. Therefore, text classification methods do not provide a
solution to our problem. This is not surprising, given that the definition of our task has
little to do with the distribution of ?content-bearing? words and phrases, much less so
than the related task of topic segmentation (Morris and Hirst 1991; Hearst 1997; Choi
2000), or Saggion and Lapalme?s (2000) approach to the summarization of scientific
articles, which relies on scientific concepts and their relations. Instead, we predict that
other indicators apart from the simple words contained in the sentence could provide
strong evidence for the modeling of rhetorical status. Also, the relatively small amount
of training material we have at our disposal requires a machine learning method that
makes optimal use of as many different kinds of features as possible. We predicted that
this would increase precision and recall on the categories in which we are interested.
The text classification experiment is still useful as it provides a nontrivial baseline for
comparison with our intrinsic system evaluation presented in section 5.
4.1 Classifiers
We use a na??ve Bayesian model as in Kupiec, Pedersen, and Chen?s (1995) experiment
(cf. Figure 9). Sentential features are collected for each sentence (Table 4 gives an
overview of the features we used). Learning is supervised: In the training phase,
associations between these features and human-provided target categories are learned.
The target categories are the seven categories in the rhetorical annotation experiment
and relevant/nonrelevant in the relevance selection experiment. In the testing phase,
the trained model provides the probability of each target category for each sentence
of unseen text, on the basis of the sentential features identified for the sentence.
4.2 Features
Some of the features in our feature pool are unique to our approach, for instance, the
metadiscourse features. Others are borrowed from the text extraction literature (Paice
1990) or related tasks and adapted to the problem of determining rhetorical status.
Absolute location of a sentence. In the news domain, sentence location is the single
most important feature for sentence selection (Brandow, Mitze, and Rau 1995); in our
domain, location information, although less dominant, can still give a useful indication.
Rhetorical zones appear in typical positions in the article, as scientific argumentation
426
Computational Linguistics Volume 28, Number 4
P(C | F0, . . . , Fn?1) ? P(C)
?n?1
j=0 P(Fj | C)
?n?1
j=0 P(Fj)
P(C | F0, . . . , Fn?1): Probability that a sentence has target category C, given its feature val-
ues F0, . . . , Fn?1;
P(C): (Overall) probability of category C;
P(Fj | C): Probability of feature-value pair Fj, given that the sentence is of target
category C;
P(Fj): Probability of feature value Fj;
Figure 9
Na??ve Bayesian classifier.
Table 4
Overview of feature pool.
Type Name Feature Description Feature Values
Absolute
location
1. Loc Position of sentence in relation to 10
segments
A-J
Explicit
structure
2. Section
Struct
Relative and absolute position of
sentence within section (e.g., first
sentence in section or somewhere in
second third)
7 values
3. Para
Struct
Relative position of sentence within a
paragraph
Initial, Medial, Final
4. Headline Type of headline of current section 15 prototypical headlines
or Non-prototypical
Sentence
length
5. Length Is the sentence longer than a certain
threshold, measured in words?
Yes or No
Content
features
6. Title Does the sentence contain words also
occurring in the title or headlines?
Yes or No
7. TF*IDF Does the sentence contain ?significant
terms? as determined by the TF*IDF
measure?
Yes or No
Verb syntax 8. Voice Voice (of first finite verb in sentence) Active or Passive or
NoVerb
9. Tense Tense (of first finite verb in sentence) 9 simple and complex
tenses or NoVerb
10. Modal Is the first finite verb modified by
modal auxiliary?
Modal or NoModal or
NoVerb
Citations 11. Cit Does the sentence contain a citation or
the name of an author contained in the
reference list? If it contains a citation,
is it a self-citation? Whereabouts in the
sentence does the citation occur?
{Citation (self), Citation
(other), Author Name,
or None} ? {Beginning,
Middle, End}
History 12. History Most probable previous category 7 Target Categories +
?BEGIN?
Meta-
discourse
13. Formulaic Type of formulaic expression occur-
ring in sentence
18 Types of Formulaic
Expressions + 9 Agent
Types or None
14. Agent Type of agent 9 Agent Types or None
15. SegAgent Type of agent 9 Agent Types or None
16. Action Type of action, with or without
negation
27 Action Types or None
427
Teufel and Moens Summarizing Scientific Articles
follows certain patterns (Swales 1990). For example, limitations of the author?s own
method can be expected to be found toward the end of the article, whereas limitations
of other researchers? work are often discussed in the introduction. We observed that the
size of rhetorical zones depends on location, with smaller rhetorical zones occurring
toward the beginning and the end of the article. We model this by assigning location
values in the following fashion: The article is divided into 20 equal parts, counting
sentences. Sentences occurring in parts 1, 2, 3, 4, 19, and 20 receive the values A, B, C,
D, I, and J, respectively. Parts 5 and 6 are pooled, and sentences occurring in them are
given the value E; the same procedure is applied to parts 15 and 16 (value G) and 17
and 18 (value H). The remaining sentences in the middle (parts 7?14) all receive the
value F (cf. Figure 10).
Section structure. Sections can have an internal structuring; for instance, sentences
toward the beginning of a section often have a summarizing function. The section
location feature divides each section into three parts and assigns seven values: first
sentence, last sentence, second or third sentence, second-last or third-last sentence, or
else either somewhere in the first, second, or last third of the section.
Paragraph structure. In many genres, paragraphs also have internal structure (Wiebe
1994), with high-level or summarizing sentences occurring more often at the periphery
of paragraphs. In this feature, sentences are distinguished into those leading or ending
a paragraph and all others.
Headlines. Prototypical headlines can be an important predictor of the rhetorical
status of sentences occurring in the given section; however, not all texts in our collec-
tion use such headlines. Whenever a prototypical headline is recognized (using a set
of regular expressions), it is classified into one of the following 15 classes: Introduction,
Implementation, Example, Conclusion, Result, Evaluation, Solution, Experiment, Discussion,
Method, Problems, Related Work, Data, Further Work, Problem Statement. If none of the
patterns match, the value Non-Prototypical is assigned.
Sentence length. Kupiec, Pedersen, and Chen (1995) report sentence length as a
useful feature for text extraction. In our implementation, sentences are divided into
long or short sentences, by comparison to a fixed threshold (12 words).
Title word contents. Sentences containing many ?content-bearing? words have been
hypothesized to be good candidates for text extraction. Baxendale (1958) extracted all
words except those on the stop list from the title and the headlines and determined
for each sentence whether or not it contained these words. We received better results
by excluding headline words and using only title words.
TF*IDF word contents. How content-bearing a word is can also be measured with
frequency counts (Salton and McGill 1983). The TF*IDF formula assigns high values
to words that occur frequently in one document, but rarely in the overall collection of
documents. We use the 18 highest-scoring TF*IDF words and classify sentences into
those that contain one or more of these words and those that do not.
Verb syntax. Linguistic features like tense and voice often correlate with rhetorical
zones; Biber (1995) and Riley (1991) show correlation of tense and voice with prototyp-
ical section structure (?method,? ?introduction?). In addition, the presence or absence
BA C D E F G H I J
126 7 81 2 3 4 9 10 11 13 14 15 16 17 18 19 205
Figure 10
Values of location feature.
428
Computational Linguistics Volume 28, Number 4
of a modal auxiliary might be relevant for detecting the phenomenon of ?hedging?
(i.e., statements in which an author distances herself from her claims or signals low
certainty: these results might indicate that . . . possibly . . . [Hyland 1998]). For each sen-
tence, we use part-of-speech-based heuristics to determine tense, voice, and presence
of modal auxiliaries. This algorithm is shared with the metadiscourse features, and
the details are described below.
Citation. There are many connections between citation behavior and relevance or
rhetorical status. First, if a sentence contains a formal citation or the name of another
author mentioned in the bibliography, it is far more likely to talk about other work than
about own work. Second, if it contains a self-citation, it is far more likely to contain
a direct statement of continuation (25%) than a criticism (3%). Third, the importance
of a citation has been related to the distinction between authorial and parenthetical
citations. Citations are called authorial if they form a syntactically integral part of the
sentence or parenthetical if they do not (Swales 1990). In most cases, authorial citations
are used as the subject of a sentence, and parenthetical ones appear toward the middle
or the end of the sentence.
We built a recognizer for formal citations. It parses the reference list at the end of
the article and determines whether a citation is a self-citation (i.e., if there is an overlap
between the names of the cited researchers and the authors of the current paper), and
it also finds occurrences of authors? names in running text, but outside of formal
citation contexts (e.g., Chomsky also claims that . . . ). The citation feature reports whether
a sentence contains an author name, a citation, or nothing. If it contains a citation,
the value records whether it is a self-citation and also records the location of the
citation in the sentence (in the beginning, the middle, or the end). This last distinction
is a heuristic for the authorial/parenthetical distinction. We also experimented with
including the number of different citations in a sentence, but this did not improve
results.
History. As there are typical patterns in the rhetorical zones (e.g., AIM sentences
tend to follow CONTRAST sentences), we wanted to include the category assigned to
the previous sentence as one of the features. In unseen text, however, the previous
target is unknown at training time (it is determined during testing). It can, however,
be calculated as a second pass process during training. In order to avoid a full Viterbi
search of all possibilities, we perform a beam search with width of three among the
candidates of the previous sentence, following Barzilay et al (2000).
Formulaic expressions. We now turn to the last three features in our feature pool, the
metadiscourse features, which are more sophisticated than the other features. The first
metadiscourse feature models formulaic expressions like the ones described by Swales,
as they are semantic indicators that we expect to be helpful for rhetorical classification.
We use a list of phrases described by regular expressions, similar to Paice?s (1990)
grammar. Our list is divided into 18 semantic classes (cf. Table 5), comprising a total
of 644 patterns. The fact that phrases are clustered is a simple way of dealing with
data sparseness. In fact, our experiments in section 5.1.2 will show the usefulness
of our (manual) semantic clusters: The clustered list performs much better than the
unclustered list (i.e., when the string itself is used as a value instead of its semantic
class).
Agent. Agents and actions are more challenging to recognize. We use a mechanism
that, dependent on the voice of a sentence, recognizes agents (subjects or prepositional
phrases headed by by) and their predicates (?actions?). Classification of agents and
actions relies on a manually created lexicon of manual classes. As in the Formulaic
feature, similar agents and actions are generalized and clustered together to avoid data
sparseness.
429
Teufel and Moens Summarizing Scientific Articles
Table 5
Formulaic expression lexicon.
Indicator Type Example Number
GAP INTRODUCTION to our knowledge 3
GENERAL FORMULAIC in traditional approaches 10
DEIXIS in this paper 11
SIMILARITY similar to 56
COMPARISON when compared to our 204
CONTRAST however 6
DETAIL this paper has also 4
METHOD a novel method for VERB?ing 33
PREVIOUS CONTEXT elsewhere, we have 25
FUTURE avenue for improvement 16
AFFECT hopefully 4
CONTINUATION following the argument in 19
IN ORDER TO in order to 1
POSITIVE ADJECTIVE appealing 68
NEGATIVE ADJECTIVE unsatisfactory 119
THEM FORMULAIC along the lines of 6
TEXTSTRUCTURE in section 3 16
NO TEXTSTRUCTURE described in the last section 43
Total of 18 classes 644
Table 6
Agent lexicon.
Agent Type Example Number Removed
US AGENT we 22
THEM AGENT his approach 21
GENERAL AGENT traditional methods 20 X
US PREVIOUS AGENT the approach in SELFCITE 7
OUR AIM AGENT the point of this study 23
REF US AGENT this method (this WORK NOUN) 6
REF AGENT the paper 11 X
THEM PRONOUN AGENT they 1 X
AIM REF AGENT its goal 8
GAP AGENT none of these papers 8
PROBLEM AGENT these drawbacks 3 X
SOLUTION AGENT a way out of this dilemma 4 X
TEXTSTRUCTURE AGENT the concluding chapter 33
Total of 13 classes 167
The lexicon for agent patterns (cf. Table 6) contains 13 types of agents and a total
of 167 patterns. These 167 patterns expand to many more strings as we use a replace
mechanism (e.g., the placeholder WORK NOUN in the sixth row of Table 6 can be
replaced by a set of 37 nouns including theory, method, prototype, algorithm).
The main three agent types we distinguish are US AGENT, THEM AGENT, and GEN-
ERAL AGENT, following the types of intellectual attribution discussed above. A fourth
type is US PREVIOUS AGENT (the authors, but in a previous paper).
Additional agent types include nonpersonal agents like aims, problems, solu-
tions, absence of solution, or textual segments. There are four equivalence classes of
430
Computational Linguistics Volume 28, Number 4
agents with ambiguous reference (?this system?): REF AGENT, REF US AGENT, THEM PRO-
NOUN AGENT, and AIM REF AGENT.
Agent classes were created based on intuition, but subsequently each class was
tested with corpus statistics to determine whether it should be removed or not. We
wanted to find and exclude classes that had a distribution very similar to the overall
distribution of the target categories, as such features are not distinctive. We measured
associations using the log-likelihood measure (Dunning 1993) for each combination of
target category and semantic class by converting each cell of the contingency into a 2?2
contingency table. We kept only classes of verbs in which at least one category showed
a high association (gscore > 5.0), as that means that in these cases the distribution was
significantly different from the overall distribution. The last column in Table 6 shows
that the classes THEM PRONOUN, GENERAL, SOLUTION, PROBLEM, and REF were removed;
removal improved the performance of the Agent feature.
SegAgent. SegAgent is a variant of the Agent feature that keeps track of previously
recognized agents; unmarked sentences receive these previous agents as a value (in
the Agent feature, they would have received the value None).
Action. We use a manually created action lexicon containing 365 verbs (cf. Table 7).
The verbs are clustered into 20 classes based on semantic concepts such as similarity,
contrast, competition, presentation, argumentation, and textual structure. For exam-
ple, PRESENTATION ACTIONs include communication verbs like present, report, and state
(Myers 1992; Thompson and Yiyun 1991), RESEARCH ACTIONS include analyze, conduct,
define and observe, and ARGUMENTATION ACTIONS include argue, disagree, and object to.
Domain-specific actions are contained in the classes indicating a problem (fail, degrade,
waste, overestimate) and solution-contributing actions (circumvent, solve, mitigate). The
Table 7
Action lexicon.
Action Type Example Number Removed
AFFECT we hope to improve our results 9 X
ARGUMENTATION we argue against a model of 19 X
AWARENESS we are not aware of attempts 5 +
BETTER SOLUTION our system outperforms . . . 9 ?
CHANGE we extend CITE?s algorithm 23
COMPARISON we tested our system against . . . 4
CONTINUATION we follow CITE . . . 13
CONTRAST our approach differs from . . . 12 ?
FUTURE INTEREST we intend to improve . . . 4 X
INTEREST we are concerned with . . . 28
NEED this approach, however, lacks . . . 8 X
PRESENTATION we present here a method for . . . 19 ?
PROBLEM this approach fails . . . 61 ?
RESEARCH we collected our data from . . . 54
SIMILAR our approach resembles that of 13
SOLUTION we solve this problem by . . . 64
TEXTSTRUCTURE the paper is organized . . . 13
USE we employ CITE?s method . . . 5
COPULA our goal is to . . . 1
POSSESSION we have three goals . . . 1
Total of 20 classes 365
431
Teufel and Moens Summarizing Scientific Articles
recognition of negation is essential; the semantics of not solving is closer to being prob-
lematic than it is to solving.
The following classes were removed by the gscore test described above, because
their distribution was too similar to the overall distribution: FUTURE INTEREST, NEED,
ARGUMENTATION, AFFECT in both negative and positive contexts (X in last column of
Table 7), and AWARENESS only in positive context (+ in last column). The following
classes had too few occurrences in negative context (< 10 occurrences in the whole verb
class) and thus the negative context of the class was also removed: BETTER SOLUTION,
CONTRAST, PRESENTATION, PROBLEM (? in last column). Again, the removal improved
the performance of the Action feature.
The algorithm for determining agents and actions relies on finite-state patterns
over part-of-speech (POS) tags. Starting from each finite verb, the algorithm collects
chains of auxiliaries belonging to the associated finite clause and thus determines the
clause?s tense and voice. Other finite verbs and commas are assumed to be clause
boundaries. Once the semantic verb is found, its stem is looked up in the action
lexicon. Negation is determined if one of 32 fixed negation words is present in a
six-word window to the right of the finite verb.
As our classifier requires one unique value for each classified item for each feature,
we had to choose one value for sentences containing more than one finite clause. We
return the following values for the action and agents feature: the first agent/action
pair, if both are nonzero, otherwise the first agent without an action, otherwise the
first action without an agent, if available.
In order to determine the level of correctness of agent and action recognition, we
had first to evaluate manually the error level of the POS tagging of finite verbs, as our
algorithm crucially relies on finite verbs. In a random sample of 100 sentences from
our development corpus that contain any finite verbs at all (they happened to contain
a total of 184 finite verbs), the tagger (which is part of the TTT software) showed a
recall of 95% and a precision of 93%.
We found that for the 174 correctly determined finite verbs, the heuristics for
negation and presence of modal auxiliaries worked without any errors (100% accu-
racy, eight negated sentences). The correct semantic verb was determined with 96%
accuracy; most errors were due to misrecognition of clause boundaries. Action Type
lookup was fully correct (100% accuracy), even in the case of phrasal verbs and longer
idiomatic expressions (have to is a NEED ACTION; be inspired by is a CONTINUE ACTION).
There were seven voice errors, two of which were due to POS-tagging errors (past par-
ticiple misrecognized). The remaining five voice errors correspond to 98% accuracy.
Correctness of Agent Type determination was tested on a random sample of 100
sentences containing at least one agent, resulting in 111 agents. No agent pattern that
should have been identified was missed (100% recall). Of the 111 agents, 105 cases were
correct (precision of 95%). Therefore, we consider the two features to be adequately
robust to serve as sentential features in our system.
Having detailed the features and classifiers of the machine learning system we
use, we will now turn to an intrinsic evaluation of its performance.
5. Intrinsic System Evaluation
Our task is to perform content selection from scientific articles, which we do by clas-
sifying sentences into seven rhetorical categories. The summaries based on this classi-
fication use some of these sentences directly, namely, sentences that express the con-
tribution of a particular article (AIM), sentences expressing contrasts with other work
(CONTRAST), and sentences stating imported solutions from other work (BASIS). Other,
432
Computational Linguistics Volume 28, Number 4
more frequent rhetorical categories, namely OTHER, OWN, and BACKGROUND, might
also be extracted into the summary.
Because the task is a mixture of extraction and classification, we report system
performance as follows:
? We first report precision and recall values for all categories, in
comparison to human performance and the text categorization baseline,
as we are primarily interested in good performance on the categories
AIM, CONTRAST, BASIS, and BACKGROUND.
? We are also interested in good overall classification performance, which
we report using kappa and macro-F as our metric. We also discuss how
well each single features does in the classification.
? We then compare the extracted sentences to our human gold standard
for relevance and report the agreement in precision and agreement per
category.
5.1 Determination of Rhetorical Status
The results of stochastic classification were compiled with a 10-fold cross-validation on
our 80-paper corpus. As we do not have much annotated material, cross-validation is
a practical way to test as it can make use of the full development corpus for training,
without ever using the same data for training and testing.
5.1.1 Overall Results. Table 8 and Figure 11 show that the stochastic model obtains
substantial improvement over the baseline in terms of precision and recall of the im-
portant categories AIM, BACKGROUND, CONTRAST, and BASIS. We use the F-measure,
defined by van Rijsbergen (1979) as 2PRP+R , as a convenient way of reporting precision (P)
and recall (R) in one value. F-measures for our categories range from .61 (TEXTUAL)
and .52 (AIM) to .45 (BACKGROUND), .38 (BASIS), and .26 (CONTRAST). The recall for
some categories is relatively low. As our gold standard is designed to contain a lot
of redundant information for the same category, this is not too worrying. Low pre-
cision in some categories (e.g., 34% for CONTRAST, in contrast to human precision of
50%), however, could potentially present a problem for later steps in the document
summarization process.
Overall, we find these results encouraging, particularly in view of the subjective
nature of the task and the high compression achieved (2% for AIM, BASIS, and TEXTUAL
sentences, 5% for CONTRAST sentences, and 6% for BACKGROUND sentences). No direct
comparison with Kupiec, Pedersen, and Chen?s results is possible as different data
sets are used and as Kupiec et al?s relevant sentences do not directly map into one
of our categories. Assuming, however, that their relevant sentences are probably most
Table 8
Performance per category: F-measure (F), precision (P) and recall (R).
AIM CONTR. TEXTUAL OWN BACKG. BASIS OTHER
F P R F P R F P R F P R F P R F P R F P R
System 52 44 65 26 34 20 61 57 66 86 84 88 45 40 50 38 37 40 44 52 39
Baseline 11 30 7 17 31 12 23 56 15 83 78 90 22 32 17 7 15 5 44 47 42
Humans 63 72 56 52 50 55 79 79 79 93 94 92 71 68 75 48 82 34 78 74 83
433
Teufel and Moens Summarizing Scientific Articles
Figure 11
Performance per category: F-measure.
Table 9
Confusion matrix: Human versus automatic annotation.
MACHINE
AIM CTR TXT OWN BKG BAS OTH Total
HUMAN AIM 127 6 13 23 19 5 10 203
CTR 21 112 4 204 87 18 126 572
TXT 14 1 145 46 6 2 6 220
OWN 100 108 84 7,231 222 71 424 8,240
BKG 14 31 1 222 370 5 101 744
BAS 17 7 7 60 8 97 39 235
OTH 6 70 10 828 215 72 773 1,974
Total 299 335 264 8,614 927 270 1,479 12,188
comparable to our AIM sentences, our precision and recall of 44% and 65% compare
favorably to theirs (42% and 42%).
Table 9 shows a confusion matrix between one annotator and the system. The
system is likely to confuse AIM and OWN sentences (e.g., 100 out of 172 sentences
incorrectly classified as AIM by the system turned out to be OWN sentences). It also
shows a tendency to confuse OTHER and OWN sentences. The system also fails to dis-
tinguish categories involving other people?s work (e.g. OTHER, BASIS, and CONTRAST).
Overall, these tendencies mirror human errors, as can be seen from a comparison with
Table 2.
Table 10 shows the results in terms of three overall measures: kappa, percentage
accuracy, and macro-F (following Lewis [1991]). Macro-F is the mean of the F-measures
of all seven categories. One reason for using macro-F and kappa is that we want to
measure success particularly on the rare categories that are needed for our final task
(i.e., AIM, BASIS, and CONTRAST). Microaveraging techniques like traditional accuracy
tend to overestimate the contribution of frequent categories in skewed distributions
like ours; this is undesirable, as OWN is the least interesting category for our purposes.
This situation has parallels in information retrieval, where precision and recall are used
because accuracy overestimates the performance on irrelevant items.
434
Computational Linguistics Volume 28, Number 4
Table 10
Overall classification results.
System/Baseline Compared with One Human Annotator 3 Humans
System Text Class. Random Random (Distr.) Most Freq.
Kappa .45 .30 ?.10 0 ?.13 .71
Accuracy .73 .72 .14 .48 .67 .87
Macro-F .50 .30 .09 .14 .11 .69
In the case of macro-F, each category is treated as one unit, independent of the
number of items contained in it. Therefore, the classification success of the individual
items in rare categories is given more importance than the classification success of
frequent-category items. When looking at the numerical values, however, one should
keep in mind that macroaveraging results are in general numerically lower (Yang and
Liu 1999). This is because there are fewer training cases for the rare categories, which
therefore perform worse with most classifiers.
In the case of kappa, classifications that incorrectly favor frequent categories are
punished because of a high random agreement. This effect can be shown most easily
when the baselines are considered. The most ambitious baseline we use is the output of
a text categorization system, as described in section 4. Other possible baselines, which
are all easier to beat, include classification by the most frequent category. This baseline
turns out to be trivial, as it does not extract any of the rare rhetorical categories in which
we are particularly interested, and therefore receives a low kappa value at K = ?.12.
Possible chance baselines include random annotation with uniform distribution (K =
?.10; accuracy of 14%) and random annotation with observed distribution. The latter
baseline is built into the definition of kappa (K = 0; accuracy of 48%).
Although our system outperforms an ambitious baseline (macro-F shows that our
system performs roughly 20% better than text classification) and also performs much
above chance, there is still a big gap in performance between humans and machine.
Macro-F shows a 20% difference between our system and human performance. If
the system is put into a pool of annotators for the 25 articles for which three-way
human judgment exists, agreement drops from K = .71 to K = .59. This is a clear
indication that the system?s annotation is still distinguishably different from human
annotation.
5.1.2 Feature Impact. The previous results were compiled using all features, which is
the optimal feature combination (as determined by an exhaustive search in the space
of feature combinations). The most distinctive single feature is Location (achieving an
agreement of K = .22 against one annotator, if this feature is used as the sole feature),
followed by SegAgent (K = .19), Citations (K = .18), Headlines (K = .17), Agent
(K = .08), and Formulaic (K = .07). In each case, the unclustered versions of Agent,
SegAgent, and Formulaic performed much worse than the clustered versions; they
did not improve final results when added into the feature pool.
Action performs slightly better at K = ?.11 than the baseline by most frequent
category, but far worse than random by observed distribution. The following features
on their own classify each sentence as OWN (and therefore achieve K = ?.12): Relative
Location, Paragraphs, TF*IDF, Title, Sentence Length, Modality, Tense, and
Voice. History performs very badly on its own at K = ?.51; it classifies almost all
sentences as BACKGROUND. It does this because the probability of the first sentence?s
435
Teufel and Moens Summarizing Scientific Articles
being a BACKGROUND sentence is almost one, and, if no other information is available,
it is very likely that another BACKGROUND sentence will follow after a BACKGROUND
sentence.
Each of these features, however, still contributes to the final result: If any of them
is taken out of the feature pool, classification performance decreases. How can this be,
given that the individual features perform worse than chance? As the classifier de-
rives the posterior probability by multiplying evidence from each feature, even slight
evidence coming from one feature can direct the decision in the right direction. A
feature that contributes little evidence on its own (too little to break the prior prob-
ability, which is strongly biased toward OWN) can thus, in combination with others,
still help in disambiguating. For the na??ve Bayesian classification method, indeed, it
is most important that the features be as independent of each other as possible. This
property cannot be assessed by looking at the feature?s isolated performance, but only
in combination with others.
It is also interesting to see that certain categories are disambiguated particularly
well by certain features (cf. Table 11). The Formulaic feature, which is by no means
the strongest feature, is nevertheless the most diverse, as it contributes to the disam-
biguation of six categories directly. This is because many different rhetorical categories
have typical cue phrases associated with them (whereas not all categories have a pre-
ferred location in the document). Not surprisingly, Location and History are the
features particularly useful for detecting BACKGROUND sentences, and SegAgent addi-
tionally contributes toward the determination of BACKGROUND zones (along with the
Formulaic and the Absolute Location features). The Agent and Action features also
prove their worth as they manage to disambiguate categories that many of the other
features alone cannot disambiguate (e.g., CONTRAST).
5.1.3 System Output: The Example Paper. In order to give the reader an impression
of how the figures reported in the previous section translate into real output, we
present in figure 12 the output of the system when run on the example paper (all
AIM, CONTRAST, and BASIS sentences). The second column shows whether the human
judge agrees with the system?s decision (a tick for correct decisions, and the human?s
preferred category for incorrect decisions). Ten out of the 15 extracted sentences have
been classified correctly.
The example also shows that the determination of rhetorical status is not always
straightforward. For example, whereas the first AIM sentence that the system proposes
(sentence 8) is clearly wrong, all other ?incorrect? AIM sentences carry important in-
Table 11
Precision and recall of rhetorical classification, individual features.
Features Precision/Recall per Category (in %)
AIM CONTR. TXT. OWN BACKG. BASIS OTHER
SegAgent alone ? 17/0 ? 74/94 53/16 ? 46/33
Agent alone ? ? ? 71/93 ? ? 36/23
Location alone ? ? ? 74/97 40/36 ? 28/9
Headlines alone ? ? ? 75/95 ? ? 29/25
Citation alone ? ? ? 73/96 ? ? 43/30
Formulaic alone 40/2 45/2 75/39 71/98 ? 40/1 47/13
Action alone ? 43/1 ? 68/99 ? ? ?
History alone ? ? ?- 70/8 16/99 ? ?
436
Computational Linguistics Volume 28, Number 4
System Human
AIM (OTH) 8 In Hindle?s proposal, words are similar if we have strong statistical
evidence that they tend to participate in the same events.?
* 10 Our research addresses some of the same questions and uses similar raw
data, but we investigate how to factor word association tendencies into
associations of words to certain hidden senses classes and associations
between the classes themselves.?
11 While it may be worthwhile to base such a model on preexisting sense
classes (Resnik, 1992), in the work described here we look at how to
derive the classes directly from distributional data.
(OWN) 12 More specifically, we model senses as probabilistic concepts or clusters
c with corresponding cluster membership probabilities EQN for each
word w.?
* 22 We will consider here only the problem of classifying nouns according
to their distribution as direct objects of verbs; the converse problem is
formally similar.
(CTR) 41 However, this is not very satisfactory because one of the goals of our
work is precisely to avoid the problems of data sparseness by grouping
words into classes.
(OWN) 150 We also evaluated asymmetric cluster models on a verb decision task
closer to possible applications to disambiguation in language analysis.?
* 162 We have demonstrated that a general divisive clustering procedure for
probability distributions can be used to group words according to their
participation in particular grammatical relations with other words.
BAS
?
19 The corpus used in our first experiment was derived from newswire text
automatically parsed by Hindle?s parser Fidditch (Hindle, 1993).?
20 More recently, we have constructed similar tables with the help of a
statistical part-of-speech tagger (Church, 1988) and of tools for regular
expression pattern matching on tagged corpora (Yarowsky, 1992).?
* 113 The analogy with statistical mechanics suggests a deterministic anneal-
ing procedure for clustering (Rose et al, 1990), in which the number
of clusters is determined through a sequence of phase transitions by
continuously increasing the parameter EQN following an annealing
schedule.
CTR
?
* 9 His notion of similarity seems to agree with our intuitions in many
cases, but it is not clear how it can be used directly to construct word
classes and corresponding models of association.?
* 14 Class construction is then combinatorially very demanding and de-
pends on frequency counts for joint events involving particular words,
a potentially unreliable source of information as we noted above.
(OWN) 21 We have not yet compared the accuracy and coverage of the two methods,
or what systematic biases they might introduce, although we took care
to filter out certain systematic errors, for instance the misparsing of the
subject of a complement clause as the direct object of a main verb for
report verbs like ?say?.?
43 This is a useful advantage of our method compared with agglomerative
clustering techniques that need to compare individual objects being
considered for grouping.
Figure 12
System output for example paper.
formation about research goals of the paper: Sentence 41 states the goal in explicit
terms, but it also contains a contrastive statement, which the annotator decided to rate
higher than the goal statement. Both sentences 12 and 150 give high-level descrip-
tions of the work that might pass as a goal statement. Similarly, in sentence 21 the
agent and action features detected that the first part of the sentence has something to
do with comparing methods, and the system then (plausibly but incorrectly) decided
437
Teufel and Moens Summarizing Scientific Articles
to classify the sentence as CONTRAST. All in all, we feel that the extracted material
conveys the rhetorical status adequately. An extrinsic evaluation additionally showed
that the end result provides considerable added value when compared to sentence
extracts (Teufel 2001).
5.2 Relevance Determination
The classifier for rhetorical status that we evaluated in the previous section is an im-
portant first step in our implementation; the next step is the determination of relevant
sentences in the text. One simple solution for relevance decision would be to use all
AIM, BASIS, and CONTRAST sentences, as these categories are rare overall. The classifier
we use has the nice property of roughly keeping the distribution of target categories,
so that we end up with a sensible number of these sentences.
The strategy of using all AIM, CONTRAST, and BASIS sentences can be evaluated
in a similar vein to the previous experiment. In terms of relevance, the asterisk in
figure 12 marks sentences that the human judge found particularly relevant in the
overall context (cf. the full set in figure 5). Six out of all 15 sentences, and 6 out of
the 10 sentences that received the correct rhetorical status, were judged relevant in the
example.
Table 12 reports the figure for the entire corpus by comparing the system?s output
of correctly classified rhetorical categories to human judgment. In all cases, the results
are far above the nontrivial baseline. On AIM, CONTRAST, and BASIS sentences, our
system achieves very high precision values of 96%, 70%, and 71%. Recall is lower at
70%, 24%, and 39%, but low recall is less of a problem in our final task. Therefore,
the main bottleneck is correct rhetorical classification. Once that is accomplished, the
selected categories show high agreement with human judgment and should therefore
represent good material for further processing steps.
If, however, one is also interested in selecting BACKGROUND sentences, as we are,
simply choosing all BACKGROUND sentences would result in low precision of 16%
(albeit with a high recall of 83%), which does not seem to be the optimal solution.
We therefore use a second classifier for finding the most relevant sentences indepen-
dently that was trained on the relevance gold standard. Our best classifier operates
at a precision of 46.5% and recall of 45.2% (using the features Location, Section
Struct, Paragraph Struct, Title, TF*IDF, Formulaic, and Citation for classifi-
cation). The second classifier (cf. rightmost columns in figure 12) raises the preci-
sion for BACKGROUND sentences from 16% to 38%, while keeping recall high at 88%.
This example shows that the right procedure for relevance determination changes
from category to category and also depends on the final task one is trying
to accomplish.
Table 12
Relevance by human selection: Precision (P) and recall (R).
AIM CONTR. BASIS BACKGROUND
Without With
Classifier Classifier
P R P R P R P R P R
System 96.2 69.8 70.1 23.8 70.5 39.4 16.0 83.3 38.4 88.2
Baseline 26.1 6.4 23.5 14.4 6.94 2.7 0.0 0.0 0.0 0.0
438
Computational Linguistics Volume 28, Number 4
6. Discussion
6.1 Contribution
We have presented a new method for content selection from scientific articles. The anal-
ysis is genre-specific; it is based on rhetorical phenomena specific to academic writing,
such as problem-solution structure, explicit intellectual attribution, and statements of
relatedness to other work. The goal of the analysis is to identify the contribution of
an article in relation to background material and to other specific current work.
Our methodology is situated between text extraction methods and fact extraction
(template-filling) methods: Although our analysis has the advantage of being more
context-sensitive than text extraction methods, it retains the robustness of this approach
toward different subdomains, presentational traditions, and writing styles.
Like fact extraction methods (e.g., Radev and McKeown 1998), our method also
uses a ?template? whose slots are being filled during analysis. The slots of our template
are defined as rhetorical categories (like ?Contrast?) rather than by domain-specific
categories (like ?Perpetrator?). This makes it possible for our approach to deal with
texts of different domains and unexpected topics.
Sparck Jones (1999) argues that it is crucial for a summarization strategy to relate
the large-scale document structure of texts to readers? tasks in the real world (i.e., to
the proposed use of the summaries). We feel that incorporating a robust analysis of
discourse structure into a document summarizer is one step along this way.
Our practical contributions are twofold. First, we present a scheme for the anno-
tation of sentences with rhetorical status, and we have shown that the annotation is
stable (K = .82, .81, .76) and reproducible (K = .71). Since these results indicate that
the annotation is reliable, we use it as our gold standard for evaluation and training.
Second, we present a machine learning system for the classification of sentences by
relevance and by rhetorical status. The contribution here is not the statistical classifier,
which is well-known and has been used in a similar task by Kupiec, Pedersen, and
Oren (1995), but instead the features we use. We have adapted 13 sentential features
in such a way that they work robustly for our task (i.e., for unrestricted, real-world
text). We also present three new features that detect scientific metadiscourse in a novel
way. The results of an intrinsic system evaluation show that the system can identify
sentences expressing the specific goal of a paper with 57% precision and 79% recall,
sentences expressing criticism or contrast with 57% precision and 42% recall, and
sentences expressing a continuation relationship to other work with 62% precision
and 43% recall. This substantially improves a baseline of text classification which uses
only a TF*IDF model over words. The agreement of correctly identified rhetorical roles
with human relevance judgments is even higher (96% precision and 70% recall for goal
statements, 70% precision and 24% recall for contrast, 71% precision and 39% recall for
continuation). We see these results as an indication that shallow discourse processing
with a well-designed set of surface-based indicators is possible.
6.2 Limitations and Future Work
The metadiscourse features, one focus of our work, currently depend on manual re-
sources. The experiments reported here explore whether metadiscourse information is
useful for the automatic determination of rhetorical status (as opposed to more shallow
features), and this is clearly the case. The next step, however, should be the automatic
creation of such resources. For the task of dialogue act disambiguation, Samuel, Car-
berry, and Vijay-Shanker (1999) suggest a method of automatically finding cue phrases
for disambiguation. It may be possible to apply this or a similar method to our data
and to compare the performance of automatically gained resources with manual ones.
439
Teufel and Moens Summarizing Scientific Articles
Further work can be done on the semantic verb clusters described in section 4.2.
Klavans and Kan (1998), who use verb clusters for document classification according to
genre, observe that verb information is rarely used in current practical natural language
applications. Most tasks such as information extraction and document classification
identify and use nominal constructs instead (e.g., noun phrases, TF*IDF words and
phrases).
The verb clusters we employ were created using our intuition of which type of
verb similarity would be useful in the genre and for the task. There are good reasons
for using such a hand-crafted, genre-specific verb lexicon instead of a general resource
such as WordNet or Levin?s (1993) classes: Many verbs used in the domain of scien-
tific argumentation have assumed a specialized meaning, which our lexicon readily
encodes. Klavans and Kan?s classes, which are based on Levin?s classes, are also man-
ually created. Resnik and Diab (2000) present yet other measures of verb similarity,
which could be used to arrive at a more data-driven definition of verb classes. We
are currently comparing our verb clusterings to Klavans and Kan?s, and to bottom-up
clusters of verb similarities generated from our annotated data.
The recognition of agents, which is already the second-best feature in the pool,
could be further improved by including named entity recognition and anaphora res-
olution. Named entity recognition would help in cases like the following,
LHIP provides a processing method which allows selected portions of the input
to be ignored or handled differently. (S-5, 9408006)
where LHIP is the name of the authors? approach and should thus be tagged as
US AGENT; to do so, however, one would need to recognize it as a named approach,
which is associated with the authors. It is very likely that such a treatment, which
would have to include information from elsewhere in the text, would improve re-
sults, particularly as named approaches are frequent in the computational linguistics
domain. Information about named approaches in themselves would also be an impor-
tant aspect to include in summaries or citation indexes.
Anaphora resolution helps in cases in which the agent is syntactically ambiguous
between own and other approaches (e.g., this system). To test whether and how much
performance would improve, we manually simulated anaphora resolution on the 632
occurrences of REF AGENT in the development corpus. (In the experiments in section 5
these occurrences had been excluded from the Agent feature by giving them the value
None; we include them now in their disambiguated state). Of the 632 REF AGENTs,
436 (69%) were classified as US AGENT, 175 (28%) as THEM AGENT, and 20 (3%) as
GENERAL AGENT. As a result of this manual disambiguation, the performance of the
Agent feature increased dramatically from K = .08 to K = .14 and that of SegAgent
from K = .19 to K = .22. This is a clear indication of the potential added value of
anaphora resolution for our task.
As far as the statistical classification is concerned, our results are still far from
perfect. Obvious ways of improving performance are the use of a more sophisticated
statistical classifier and more training material. We have experimented with a maxi-
mum entropy model, Repeated Incremental Pruning to Produce Error Reduction (RIP-
PER), and decision trees; preliminary results do not show significant improvement
over the na??ve Bayesian model. One problem is that 4% of the sentences in our cur-
rent annotated material are ambiguous: They receive the same feature representation
but are classified differently by the annotators. A possible solution is to find better
and more distinctive features; we believe that robust, higher-level features like actions
and agents are a step in the right direction. We also suspect that a big improvement
440
Computational Linguistics Volume 28, Number 4
could be achieved with smaller annotation units. Many errors come from instances in
which one half of a sentence serves one rhetorical purpose, the other another, as in
the following example:
The current paper shows how to implement this general notion, without fol-
lowing Krifka?s analysis in detail. (S-10, 9411019)
Here, the first part describes the paper?s research goal, whereas the second expresses a
contrast. Currently, one target category needs to be associated with the whole sentence
(according to a rule in the guidelines, AIM is given preference over CONTRAST). As
an undesired side effect, the CONTRAST-like textual parts (and the features associated
with this text piece, e.g., the presence of an author?s name) are wrongly associated
with the AIM target category. If we allowed for a smaller annotation unit (e.g., at the
clause level), this systematic noise in the training data could be removed.
Another improvement in classification accuracy might be achieved by performing
the classification in a cascading way. The system could first perform a classification
into OWN-like classes (OWN, AIM, and TEXTUAL pooled), OTHER-like categories (OTHER,
CONTRAST, and BASIS pooled), and BACKGROUND, similar to the way human annotation
proceeds. Subclassification among these classes would then lead to the final seven-way
classification.
Appendix: List of articles in CL development corpus
No. Title, Conference, Authors
0 9405001 Similarity-Based Estimation of Word Cooccurrence Probabilities (ACL94), I. Dagan et al
1 9405002 Temporal Relations: Reference or Discourse Coherence? (ACL94 Student), A. Kehler
2 9405004 Syntactic-Head-Driven Generation (COLING94), E. Koenig
3 9405010 Common Topics and Coherent Situations: Interpreting Ellipsis in the Context of Dis-
course Inference (ACL94), A. Kehler
4 9405013 Collaboration on Reference to Objects That Are Not Mutually Known (COLING94),
P. Edmonds
5 9405022 Grammar Specialization through Entropy Thresholds (ACL94), C. Samuelsson
6 9405023 An Integrated Heuristic Scheme for Partial Parse Evaluation (ACL94 Student), A. Lavie
7 9405028 Semantics of Complex Sentences in Japanese (COLING94), H. Nakagawa, S. Nishizawa
8 9405033 Relating Complexity to Practical Performance in Parsing with Wide-Coverage Unifica-
tion Grammars (ACL94), J. Carroll
9 9405035 Dual-Coding Theory and Connectionist Lexical Selection (ACL94 Student), Y. Wang
10 9407011 Discourse Obligations in Dialogue Processing (ACL94), D. Traum, J. Allen
11 9408003 Typed Feature Structures as Descriptions (COLING94 Reserve), P. King
12 9408004 Parsing with Principles and Probabilities (ACL94 Workshop), A. Fordham, M. Crocker
13 9408006 LHIP: Extended DCGs for Configurable Robust Parsing (COLING94), A. Bal-
lim, G. Russell
14 9408011 Distributional Clustering of English Words (ACL93), F. Pereira et al
15 9408014 Qualitative and Quantitative Models of Speech Translation (ACL94 Workshop),
H. Alshawi
16 9409004 An Experiment on Learning Appropriate Selectional Restrictions from a Parsed Corpus
(COLING94), F. Ribas
17 9410001 Improving Language Models by Clustering Training Sentences (ANLP94), D. Carter
18 9410005 A Centering Approach to Pronouns (ACL87), S. Brennan et al
19 9410006 Evaluating Discourse Processing Algorithms (ACL89), M. Walker
20 9410008 Recognizing Text Genres with Simple Metrics Using Discriminant Analysis (COLING94),
J. Karlgren, D. Cutting
21 9410009 Reserve Lexical Functions and Machine Translation (COLING94), D. Heylen et al
22 9410012 Does Baum-Welch Re-estimation Help Taggers? (ANLP94), D. Elworthy
23 9410022 Automated Tone Transcription (ACL94 SIG), S. Bird
24 9410032 Planning Argumentative Texts (COLING94), X. Huang
25 9410033 Default Handling in Incremental Generation (COLING94), K. Harbusch et al
26 9411019 Focus on ?Only? and ?Not? (COLING94), A. Ramsay
27 9411021 Free-Ordered CUG on Chemical Abstract Machine (COLING94), S. Tojo
441
Teufel and Moens Summarizing Scientific Articles
28 9411023 Abstract Generation Based on Rhetorical Structure Extraction (COLING94), K. Ono et
al.
29 9412005 Segmenting Speech without a Lexicon: The Roles of Phonotactics and Speech Source
(ACL94 SIG), T. Cartwright, M. Brent
30 9412008 Analysis of Japanese Compound Nouns Using Collocational Information (COLING94),
Y. Kobayasi et al
31 9502004 Bottom-Up Earley Deduction (COLING94), G. Erbach
32 9502005 Off-Line Optimization for Earley-Style HPSG Processing (EACL95), G. Minnen et al
33 9502006 Rapid Development of Morphological Descriptions for Full Language Processing Sys-
tems (EACL95), D. Carter
34 9502009 On Learning More Appropriate Selectional Restrictions (EACL95), F. Ribas
35 9502014 Ellipsis and Quantification: A Substitutional Approach (EACL95), R. Crouch
36 9502015 The Semantics of Resource Sharing in Lexical-Functional Grammar (EACL95), A. Kehler
et al
37 9502018 Algorithms for Analysing the Temporal Structure of Discourse (EACL95), J. Hitzeman
et al
38 9502021 A Tractable Extension of Linear Indexed Grammars (EACL95), B. Keller, D. Weir
39 9502022 Stochastic HPSG (EACL95), C. Brew
40 9502023 Splitting the Reference Time: Temporal Anaphora and Quantification in DRT (EACL95),
R. Nelken, N. Francez
41 9502024 A Robust Parser Based on Syntactic Information (EACL95), K. Lee et al
42 9502031 Cooperative Error Handling and Shallow Processing (EACL95 Student), T. Bowden
43 9502033 An Algorithm to Co-ordinate Anaphora Resolution and PPS Disambiguation Process
(EACL95 Student), S. Azzam
44 9502035 Incorporating ?Unconscious Reanalysis? into an Incremental, Monotonic Parser
(EACL95 Student), P. Sturt
45 9502037 A State-Transition Grammar for Data-Oriented Parsing (EACL95 Student), D. Tugwell
46 9502038 Implementation and Evaluation of a German HMM for POS Disambiguation (EACL95
Workshop), H. Feldweg
47 9502039 Multilingual Sentence Categorization According to Language (EACL95 Workshop),
E. Giguet
48 9503002 Computational Dialectology in Irish Gaelic (EACL95), B. Kessler
49 9503004 Creating a Tagset, Lexicon and Guesser for a French Tagger (EACL95 Workshop),
J. Chanod, P. Tapanainen
50 9503005 A Specification Language for Lexical Functional Grammars (EACL95), P. Blackburn,
C. Gardent
51 9503007 The Semantics of Motion (EACL95), P. Sablayrolles
52 9503009 Distributional Part-of-Speech Tagging (EACL95), H. Schuetze
53 9503013 Incremental Interpretation: Applications, Theory, and Relationship to Dynamic Seman-
tics (COLING95), D. Milward, R. Cooper
54 9503014 Non-constituent Coordination: Theory and Practice (COLING94), D. Milward
55 9503015 Incremental Interpretation of Categorial Grammar (EACL95), D. Milward
56 9503017 Redundancy in Collaborative Dialogue (COLING92), M. Walker
57 9503018 Discourse and Deliberation: Testing a Collaborative Strategy (COLING94), M. Walker
58 9503023 A Fast Partial Parse of Natural Language Sentences Using a Connectionist Method
(EACL95), C. Lyon, B. Dickerson
59 9503025 Occurrence Vectors from Corpora vs. Distance Vectors from Dictionaries (COLING94),
Y. Niwa, Y. Nitta
60 9504002 Tagset Design and Inflected Languages (EACL95 Workshop), D. Elworthy
61 9504006 Cues and Control in Expert-Client Dialogues (ACL88), S. Whittaker, P. Stenton
62 9504007 Mixed Initiative in Dialogue: An Investigation into Discourse Segmentation (ACL90),
M. Walker, S. Whittaker
63 9504017 A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and
Sequences of Utterances (ACL95), D. Marcu, G. Hirst
64 9504024 A Morphographemic Model for Error Correction in Nonconcatenative Strings (ACL95),
T. Bowden, G. Kiraz
65 9504026 The Intersection of Finite State Automata and Definite Clause Grammars (ACL95),
G. van Noord
66 9504027 An Efficient Generation Algorithm for Lexicalist MT (ACL95), V. Poznanski et al
67 9504030 Statistical Decision-Tree Models for Parsing (ACL95), D. Magerman
68 9504033 Corpus Statistics Meet the Noun Compound: Some Empirical Results (ACL95), M. Lauer
79 9504034 Bayesian Grammar Induction for Language Modeling (ACL95), S. Chen
70 9505001 Response Generation in Collaborative Negotiation (ACL95), J. Chu-Carroll, S. Carberry
71 9506004 Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Con-
structs (ACL95), S. Kulick
72 9511001 Countability and Number in Japanese-to-English Machine Translation (COLING94),
F. Bond et al
442
Computational Linguistics Volume 28, Number 4
73 9511006 Disambiguating Noun Groupings with Respect to WordNet Senses (ACL95 Workshop),
P. Resnik
74 9601004 Similarity between Words Computed by Spreading Activation on an English Dictionary
(EACL93), H. Kozima, T. Furugori
75 9604019 Magic for Filter Optimization in Dynamic Bottom-up Processing (ACL96), G. Minnen
76 9604022 Unsupervised Learning of Word-Category Guessing Rules (ACL96), A. Mikheev
77 9605013 Learning Dependencies between Case Frame Slots (COLING96), H. Li, N. Abe
78 9605014 Clustering Words with the MDL Principle (COLING96), H. Li, N. Abe
79 9605016 Parsing for Semidirectional Lambek Grammar is NP-Complete (ACL96), J. Doerre
Acknowledgments
The work reported in this article was
conducted while both authors were in the
HCRC Language Technology Group at the
University of Edinburgh.
The authors would like to thank Jean
Carletta for her help with the experimental
design, Chris Brew for many helpful
discussions, Claire Grover and Andrei
Mikheev for advice on the XML
implementation, and the annotators, Vasilis
Karaiskos and Anne Wilson, for their
meticulous work and criticism, which led to
several improvements in the annotation
scheme. Thanks also to Byron
Georgantopolous, who helped to collect the
first version of the corpus, and to the four
anonymous reviewers.
References
Barzilay, Regina, Michael Collins, Julia
Hirschberg, and Steve Whittaker. 2000.
The rules behind roles. In Proceedings of
AAAI-00.
Barzilay, Regina, Kathleen R. McKeown,
and Michael Elhadad. 1999. Information
fusion in the context of multi-document
summarization. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics (ACL-99),
pages 550?557.
Baxendale, Phyllis B. 1958. Man-made index
for technical literature?An experiment.
IBM Journal of Research and Development,
2(4):354?361.
Biber, Douglas. 1995. Dimensions of Register
Variation: A Cross-Linguistic Comparison.
Cambridge University Press, Cambridge,
England.
Brandow, Ronald, Karl Mitze, and Lisa
F. Rau. 1995. Automatic condensation of
electronic publications by sentence
selection. Information Processing and
Management, 31(5):675?685.
Carletta, Jean. 1996. Assessing agreement on
classification tasks. The kappa statistic.
Computational Linguistics, 22(2):249?254.
Choi, Freddy Y. Y. 2000. Advances in
domain independent linear text
segmentation. In Proceedings of the Sixth
Applied Natural Language Conference
(ANLP-00) and the First Meeting of the North
American Chapter of the Association for
Computational Linguistics (NAACL-00),
pages 26?33.
CMP LG. 1994. The computation and
language e-print archive.
http://xxx.lanl.gov/cmp-lg.
Dunning, Ted. 1993. Accurate methods for
the statistics of surprise and coincidence.
Computational Linguistics, 19(1):61?74.
Edmundson, H. P. 1969. New methods in
automatic extracting. Journal of the
Association for Computing Machinery,
16(2):264?285.
Edmundson, H. P., and Wyllys, R. E. 1961.
Automatic abstracting and
indexing?Survey and recommendations.
Communications of the ACM, 4(5):226?234.
Garfield, Eugene. 1979. Citation Indexing: Its
Theory and Application in Science, Technology
and Humanities. J. Wiley, New York.
Grefenstette, Gregory. 1998. Producing
intelligent telegraphic text reduction to
provide an audio scanning service for the
blind. In D. R. Radev and E. H. Hovy,
editors, Working Notes of the AAAI Spring
Symposium on Intelligent Text
Summarization, pages 111?117.
Grover, Claire, Andrei Mikheev, and Colin
Matheson. 1999. LT TTT version 1.0: Text
tokenisation software. Technical Report,
Human Communication Research Centre,
University of Edinburgh.
http://www.ltg.ed.ac.uk/software/ttt/.
Hearst, Marti A. 1997. Texttiling:
Segmenting text into multi-paragraph
subtopic passages. Computational
Linguistics, 23(1):33?64.
Hyland, Ken. 1998. Persuasion and context:
The pragmatics of academic
metadiscourse. Journal of Pragmatics,
30(4):437?455.
Jing, Hongyan, Regina Barzilay, Kathleen
R. McKeown, and Michael Elhadad. 1998.
Summarization evaluation methods:
Experiments and analysis. In D. R. Radev
and E. H. Hovy, editors, Working Notes of
the AAAI Spring Symposium on Intelligent
443
Teufel and Moens Summarizing Scientific Articles
Text Summarization, pages 60?68.
Jing, Hongyan and Kathleen R. McKeown.
2000. Cut and paste based summarization.
In Proceedings of the Sixth Applied Natural
Language Conference (ANLP-00) and the First
Meeting of the North American Chapter of the
Association for Computational Linguistics
(NAACL-00), pages 178?185.
Jordan, Michael P. 1984. Rhetoric of Everyday
English Texts. Allen and Unwin, London.
Klavans, Judith L. and Min-Yen Kan. 1998.
Role of verbs in document analysis. In
Proceedings of the 36th Annual Meeting of the
Association for Computational Linguistics and
the 17th International Conference on
Computational Linguistics
(ACL/COLING-98), pages 680?686.
Knight, Kevin and Daniel Marcu. 2000.
Statistics-based summarization?Step one:
Sentence compression. In Proceedings of the
17th National Conference of the American
Association for Artificial Intelligence
(AAAI-2000), pages 703?710.
Krippendorff, Klaus. 1980. Content Analysis:
An Introduction to Its Methodology. Sage
Publications, Beverly Hills, CA.
Kupiec, Julian, Jan O. Pedersen, and
Francine Chen. 1995. A trainable
document summarizer. In Proceedings of
the 18th Annual International Conference on
Research and Development in Information
Retrieval (SIGIR-95), pages 68?73.
Lancaster, Frederick Wilfrid. 1998. Indexing
and Abstracting in Theory and Practice.
Library Association, London.
Landis, J. Richard and Gary G. Koch. 1977.
The measurement of observer agreement
for categorical data. Biometrics, 33:159?174.
Lawrence, Steve, C. Lee Giles, and Kurt
Bollacker. 1999. Digital libraries and
autonomous citation indexing. IEEE
Computer, 32(6):67?71.
Levin, Beth. 1993. English Verb Classes and
Alternations. University of Chicago Press,
Chicago.
Lewis, David D. 1991. Evaluating text
categorisation. In Speech and Natural
Language: Proceedings of the ARPA Workshop
of Human Language Technology.
Liddy, Elizabeth DuRoss. 1991. The
discourse-level structure of empirical
abstracts: An exploratory study.
Information Processing and Management,
27(1):55?81.
Lin, Chin-Yew and Eduard H. Hovy. 1997.
Identifying topics by position. In
Proceedings of the Fifth Applied Natural
Language Conference (ANLP-97),
pages 283?290.
Luhn, Hans Peter. 1958. The automatic
creation of literature abstracts. IBM Journal
of Research and Development, 2(2):159?165.
Mani, Inderjeet, Therese Firmin, David
House, Gary Klein, Beth Sundheim, and
Lynette Hirschman. 1999. The TIPSTER
SUMMAC text summarization evaluation.
In Proceedings of the Ninth Meeting of the
European Chapter of the Association for
Computational Linguistics (EACL-99),
pages 77?85.
Mani, Inderjeet, Barbara Gates, and Eric
Bloedorn. 1999. Improving summaries by
revising them. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics (ACL-99),
pages 558?565.
Mann, William C. and Sandra A.
Thompson. 1987. Rhetorical structure
theory: Description and construction of
text structures. In Gerard Kempen, editor,
Natural Language Generation: New Results in
Artificial Intelligence, Psychology, and
Linguistics. Martinus Nijhoff Publishers,
Dordrecht, the Netherlands, pages 85?95.
Marcu, Daniel. 1999. Discourse trees are
good indicators of importance in text. In
I. Mani and M. T. Maybury, editors,
Advances in Automatic Text Summarization.
MIT Press, Cambridge, pages 123?136.
McCallum, Andrew. 1997. Training
algorithms for linear text classifiers. In
Proceedings of the 19th Annual International
Conference on Research and Development in
Information Retrieval (SIGIR-97).
Mizzaro, Stefano. 1997. Relevance: The
whole history. Journal of the American
Society for Information Science,
48(9):810?832.
Morris, Jane and Graeme Hirst. 1991.
Lexical cohesion computed by thesaural
relations as an indicator of the structure of
text. Computational Linguistics, 17(1):21?48.
Myers, Greg. 1992. In this paper we report
. . . ?Speech acts and scientific facts.
Journal of Pragmatics, 17(4):295?313.
Nanba, Hidetsugu and Manabu Okumura.
1999. Towards multi-paper summarization
using reference information. In
Proceedings of IJCAI-99, pages 926?931.
Paice, Chris D. 1990. Constructing literature
abstracts by computer: Techniques and
prospects. Information Processing and
Management, 26(1):171?186.
Paice, Chris D. and A. Paul Jones. 1993. The
identification of important concepts in
highly structured technical papers. In
Proceedings of the 16th Annual International
Conference on Research and Development in
Information Retrieval (SIGIR-93),
pages 69?78.
Pollock, Joseph J. and Antonio Zamora.
1975. Automatic abstracting research at
444
Computational Linguistics Volume 28, Number 4
the chemical abstracts service. Journal of
Chemical Information and Computer Sciences,
15(4):226?232.
Radev, Dragomir R. and Kathleen
R. McKeown. 1998. Generating natural
language summaries from multiple
on-line sources. Computational Linguistics,
24(3):469?500.
Rath, G. J., A. Resnick, and T. R. Savage.
1961. The formation of abstracts by the
selection of sentences. American
Documentation, 12(2):139?143.
Resnik, Philip and Mona Diab. 2000.
Measuring verb similarity. In
Twenty-Second Annual Meeting of the
Cognitive Science Society (COGSCI2000).
Riley, Kathryn. 1991. Passive voice and
rhetorical role in scientific writing. Journal
of Technical Writing and Communication,
21(3):239?257.
Rowley, Jennifer. 1982. Abstracting and
Indexing. Bingley, London.
Saggion, Horacio and Guy Lapalme. 2000.
Selective analysis for automatic
abstracting: Evaluating indicativeness and
acceptability. In Proceedings of
Content-Based Multimedia Information Access
(RIAO), pages 747?764.
Salton, Gerard and Michael J. McGill. 1983.
Introduction to Modern Information Retrieval.
McGraw-Hill, Tokyo.
Samuel, Ken, Sandra Carberry, and
K. Vijay-Shanker. 1999. Automatically
selecting useful phrases for dialogue act
tagging. In Proceedings of the Pacific
Association for Computational Linguistics
(PACLING-99).
Saracevic, Tefko. 1975. Relevance: A review
of and a framework for the thinking on
the notion in information science. Journal
of the American Society for Information
Science, 26(6):321?343.
Siegel, Sidney and N. John Castellan Jr.
1988. Nonparametric Statistics for the
Behavioral Sciences. McGraw-Hill, Berkeley,
CA, second edition.
Sparck Jones, Karen. 1990. What sort of
thing is an AI experiment? In
D. Partridge and Yorick Wilks, editors,
The Foundations of Artificial Intelligence: A
SourceBook. Cambridge University Press,
Cambridge, pages 274?281.
Sparck Jones, Karen. 1999. Automatic
summarising: Factors and directions. In
I. Mani and M. T. Maybury, editors,
Advances in Automatic Text Summarization.
MIT Press, Cambridge, pages 1?12.
Swales, John, 1990. Research articles in
English. In Genre Analysis: English in
Academic and Research Settings. Cambridge
University Press, Cambridge, chapter 7,
pages 110?176.
Teufel, Simone, 1999. Argumentative Zoning:
Information Extraction from Scientific Text.
Ph.D. thesis, School of Cognitive Science,
University of Edinburgh, Edinburgh.
Teufel, Simone. 2001. Task-based evaluation
of summary quality: Describing
relationships between scientific papers. In
Proceedings of NAACL-01 Workshop
?Automatic Text Summarization.?
Teufel, Simone, Jean Carletta, and Marc
Moens. 1999. An annotation scheme for
discourse-level argumentation in research
articles. In Proceedings of the Eighth Meeting
of the European Chapter of the Association for
Computational Linguistics (EACL-99),
pages 110?117.
Teufel, Simone and Marc Moens. 1997.
Sentence extraction as a classification
task. In Proceedings of the ACL/EACL-97
Workshop on Intelligent Scalable Text
Summarization, pages 58?65.
Teufel, Simone and Marc Moens. 2000.
What?s yours and what?s mine:
Determining intellectual attribution in
scientific text. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large
Corpora.
Thompson, Geoff and Ye Yiyun. 1991.
Evaluation in the reporting verbs used in
academic papers. Applied Linguistics,
12(4):365?382.
Tombros, Anastasios, and Mark Sanderson.
1998. Advantages of query biased
summaries. In Proceedings of the 21st
Annual International Conference on Research
and Development in Information Retrieval
(SIGIR-98). Association of Computing
Machinery.
Trawinski, Bogdan. 1989. A methodology
for writing problem-structured abstracts.
Information Processing and Management,
25(6):693?702.
van Dijk, Teun A. 1980. Macrostructures: An
Interdisciplinary Study of Global Structures in
Discourse, Interaction and Cognition.
Lawrence Erlbaum, Hillsdale, NJ.
van Rijsbergen, Cornelis Joost. 1979.
Information Retrieval. Butterworth,
London, second edition.
Wiebe Janyce. 1994. Tracking point of view
in narrative. Computational Linguistics,
20(2):223?287.
445
Teufel and Moens Summarizing Scientific Articles
Yang, Yiming and Xin Liu. 1999. A
re-examination of text categorization
methods. In Proceedings of the 22nd Annual
International Conference on Research and
Development in Information Retrieval
(SIGIR-99), pages 42?49.
Zappen, James P. 1983. A rhetoric for
research in sciences and technologies. In
Paul V. Anderson, R. John Brockman, and
Carolyn R. Miller, editors, New Essays in
Technical and Scientific Communication
Research Theory Practice. Baywood,
Farmingdale, NY, pages 123?138.
Zechner, Klaus. 1995. Automatic text
abstracting by selecting relevant passages.
Master?s thesis, Centre for Cognitive
Science, University of Edinburgh,
Edinburgh.
Ziman, John M. 1969. Information,
communication, knowledge. Nature,
224:318?324.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 391?398,
New York, June 2006. c?2006 Association for Computational Linguistics
Creating a Test Collection for Citation-based IR Experiments
Anna Ritchie
University of Cambridge
Computer Laboratory
15 J J Thompson Avenue
Cambridge, CB3 0FD, U.K.
ar283@cl.cam.ac.uk
Simone Teufel
University of Cambridge
Computer Laboratory
15 J J Thompson Avenue
Cambridge, CB3 0FD, U.K.
sht25@cl.cam.ac.uk
Stephen Robertson
Microsoft Research Ltd
Roger Needham House
7 J J Thomson Avenue
Cambridge, CB3 0FB, U.K.
ser@microsoft.com
Abstract
We present an approach to building a test
collection of research papers. The ap-
proach is based on the Cranfield 2 tests but
uses as its vehicle a current conference;
research questions and relevance judge-
ments of all cited papers are elicited from
conference authors. The resultant test col-
lection is different from TREC?s in that
it comprises scientific articles rather than
newspaper text and, thus, allows for IR
experiments that include citation informa-
tion. The test collection currently con-
sists of 170 queries with relevance judge-
ments; the document collection is the ACL
Anthology. We describe properties of
our queries and relevance judgements, and
demonstrate the use of the test collection
in an experimental setup. One potentially
problematic property of our collection is
that queries have a low number of relevant
documents; we discuss ways of alleviating
this.
1 Introduction
We present a methodology for creating a test collec-
tion of scientific papers that is based on the Cran-
field 2 methodology but uses a current conference as
the main vehicle for eliciting relevance judgements
from users, i.e., the authors.
Building a test collection is a long and expensive
process but was necessary as no ready-made test col-
lection existed on which the kinds of experiments
with citation information that we envisage could be
run. We aim to improve term-based IR on scien-
tific articles with citation information, by using in-
dex terms from the citing article to additionally de-
scribe the cited document. Exactly how to do this is
the research question that our test collection should
help to address.
This paper is structured as follows: Section 2 mo-
tivates our proposed experiments and, thereby, our
test collection. Section 3 discusses the how test col-
lections are built and, in particular, our own. Sec-
tion 4 briefly describes the practicalities of compil-
ing the document collection and the processing we
perform to prepare the documents for our experi-
ments. In Section 5, we show that our test collection
can be used with standard IR tools. Finally, Sec-
tion 6 discusses the problem of the low number of
relevant documents judged so far and two ways of
alleviating this problem.
2 Motivation
The idea of using terms external to a document,
coming from a ?citing? document, has been bor-
rowed from web-based IR. When one paper cites
another, a link is made between them and this link
structure is analogous to that of the web: ?hyper-
links ... provide semantic linkages between ob-
jects, much in the same manner that citations link
documents to other related documents? (Pitkow and
Pirolli, 1997). Link structure, particularly anchor
text, has been used to advantage in web-based IR.
While web pages are often poorly self-descriptive
(Brin and Page, 1998) anchor text is often a higher-
level description of the pointed-to page. (Davison,
391
2000) provides a good discussion of how well an-
chor text does this and provides experimental results
in support. Thus, beginning with (McBryan, 1994),
there is a trend of propagating anchor text along its
hyperlink to associate it with the linked page, as well
as the page in which it is found. Google, for ex-
ample, includes anchor text as index terms for the
linked page (Brin and Page, 1998). The TREC Web
tracks have also shown that using anchor text im-
proves retrieval effectiveness for some search tasks
(Hawking and Craswell, 2005).
This idea has already been applied to citations and
scientific articles (Bradshaw, 2003). In Bradshaw?s
experiment, scientific documents are indexed by the
text that refers to them in documents that cite them.
However, unlike in experiments with previous col-
lections, we need both the citing and the cited article
as full documents in our collection. The question of
how to identify citation ?anchor text? and its extent
is a matter for research; this requires the full text of
the citing article. Previous experiments and test col-
lections have had only limited access to the content
of the citing article: Bradshaw had access only to a
fixed window of text around the citation, as provided
by CiteSeer?s ?citation context?; in the GIRT collec-
tions (Kluck, 2003), a dozen or so content-bearing
information fields (e.g., title, abstract, methodologi-
cal descriptors) represent each document and the full
text is not available. Additionally, in Bradshaw?s ex-
periment, no access is given to the text of the cited
article itself so that the influence of a term-based IR
model cannot be studied and so that documents can
only be indexed if they have been cited at least once.
A test collection containing full text for many cit-
ing and cited documents, thus, has advantages from
a methodological point of view.
2.1 Choosing a Genre
When choosing a scientific field to study, we looked
for one that is practicable for us to compile the doc-
ument collection (freely available machine-readable
documents; as few as possible document styles),
while still ensuring good coverage of research top-
ics in an entire field. Had we chosen the medical
field or bioinformatics, the prolific number of jour-
nals would have been a problem for the practical
document preparation.
We also looked for a relatively self-contained
field. As we aim to propagate referential text to cited
papers as index terms, references from documents
in the collection to other documents within the col-
lection will be most useful. We call these internal
references. While it is impossible to find or create
a collection of documents with only internal refer-
ences, we aim for as high a proportion of internal
references as possible.
We chose the ACL (Association for Computa-
tional Linguistics) Anthology1 , a freely available
digital archive of computational linguistics research
papers. Computational linguistics is a small, ho-
mogenous research field and the Anthology contains
the most prominent publications since the beginning
of the field in 1960, consists of only 2 journals, 7
conferences and 5 less important publications, such
as discontinued conferences and a series of work-
shops, resulting in only 7000 papers2.
With the ACL Anthology, we expect a high pro-
portion of internal references within a relatively
compact document collection. We empirically mea-
sured the proportion of collection-internal refer-
ences. We found a proportion of internal refer-
ences to all references of 0.33 (the in-factor). We
wanted to compare this number to a situation in
another, larger field (genetics) but no straightfor-
ward comparison is possible, as there are very many
genetics journals and quality of journals probably
plays a larger role in a bigger field. We tried to
simulate a similar collection to the 9 main jour-
nals+conferences in the Anthology, by considering
10 journals in genetics with a range of impact fac-
tors3, resulting in an in-factor of 0.17 (dropping to
0.14 if only 5 journals are considered). Thus, our
hypothesis that the Anthology is reasonably self-
contained, at least in comparison with other possible
collections, was confirmed.
The choice of computational linguistics has the
added benefit that we are familiar with the domain;
we can interpret the subject matter better than we
would be able to in the medical domain. This should
be of use to us in our eventual experiments.
1http://www.aclweb.org/anthology/
2This is our estimate, after substracting non-papers such as
letters to the editor, tables of contents etc. The Anthology is
growing by ?500 papers per year.
3Journal impact factor is a measure of the frequency with
which its average article is cited and is a measure of the relative
importance of journals within a field (Garfield, 1972).
392
3 Building Test Collections
To turn our document collection into a test col-
lection, a parallel set of search queries and rele-
vance judgements is needed. There are a number
of alternative methods for building a test collec-
tion. For TREC, humans devise queries specifically
for a given set of documents and make relevance
judgements on pooled retrieved documents from that
set (Harman, 2005). Theirs is an extremely labour-
intensive and expensive process and an unrealistic
option in the context of our project.
The Cranfield 2 tests (Cleverdon et al, 1966) in-
troduced an alternative method for creating a test
collection, specifically for scientific texts. The
method was subject to criticism and has not been
employed much since. Nevertheless, we believe this
method to be worth revisiting for our current situa-
tion. In this section, we describe in turn the Cran-
field 2 method and our adapted method. We discuss
some of the original criticisms and their bearing on
our own work, then describe our returns thus far.
3.1 The Cranfield 2 Test Collection
The Cranfield 2 tests (Cleverdon et al, 1966) were
a comparative evaluation of indexing language de-
vices. From a base collection of 182 (high speed
aerodynamics and aircraft structures) papers, the
Cranfield test collection was built by asking the au-
thors to formulate the research question(s) behind
their work and to judge how relevant each reference
in their paper was to each of their research questions,
on a 5-point scale. Referenced documents were ob-
tained and added to the base set. Authors were also
asked to list additional relevant papers not cited in
their paper. The collection was further expanded
in a second stage, using bibliographic coupling to
search for similar papers to the referenced ones and
employing humans to search the collection for other
relevant papers. The resultant collection comprised
1400 documents and 221 queries (Cleverdon, 1997).
The principles behind the Cranfield technique are:
? Queries: Each paper has an underlying research
question or questions; these constitute valid
search queries.
? Relevant documents: A paper?s reference list is
a good starting point for finding papers relevant
to its research questions.
? Judges: The paper author is the person best
qualified to judge relevance.
3.2 Our Anthology Test Collection
We altered the Cranfield design to fit to a fixed,
existing document collection. We designed our
methodology around an upcoming conference and
approached the paper authors at around the time of
the conference, to maximize their willingness to par-
ticipate and to minimise possible changes in their
perception of relevance since they wrote the paper.
Due to the relatively high in-factor of the collection,
we expected a significant proportion of the relevance
judgements gathered in this way to be about Anthol-
ogy documents and, thus, useful as evaluation data.
Hence, the authors of accepted papers for ACL-
2005 and HLT-EMNLP-2005 were asked, by email,
for their research questions and relevance judge-
ments for their references. We defined a 4-point
relevance scale, c.f. Table 1, since we felt that the
distinctions between the Cranfield grades were not
clear enough to warrant 5. Our guidelines also in-
cluded examples of referencing situations that might
fit each category. Personalized materials for partic-
ipation were sent, including a reproduction of their
paper?s reference list in their response form. This
meant that invitations could only be sent once the
paper had been made available online.
We further deviated from the Cranfield methodol-
ogy by deciding not to ask the authors to try to list
additional references that could have been included
in their reference list. An author?s willingness to
name such references will differ more from author
to author than their naming of original references, as
referencing is part of a standardized writing process.
By asking for this data, the consistency of the data
across papers will be degraded and the status of any
additional references will be unclear. Furthermore,
feedback from an informal pilot study conducted on
ten paper authors confirmed that some authors found
this task particularly difficult.
Each co-author of the papers was invited individu-
ally to participate, rather than inviting the first author
alone. This increased the number of invitations that
needed to be prepared and sent (by a factor of around
2.5) but also increased the likelihood of getting a re-
turn for a given paper. Furthermore, data from mul-
tiple co-authors of the same paper can be used to
393
Grade Description
4 The reference is crucially relevant to the problem. Knowledge of the contents of the referred work will be fun-
damental to the reader?s understanding of your paper. Often, such relevant references are afforded a substantial
amount of text in a paper e.g., a thorough summary.
3 The reference is relevant to the problem. It may be helpful for the reader to know the contents of the referred work,
but not crucial. The reference could not have been substituted or dropped without making significant additions to
the text. A few sentences may be associated with the reference.
2 The reference is somewhat (perhaps indirectly) relevant to the problem. Following up the reference probably would
not improve the reader?s understanding of your paper. Alternative references may have been equally appropriate
(e.g., the reference was chosen as a representative example from a number of similar references or included in a
list of similar references). Or the reference could have been dropped without damaging the informativeness of your
paper. Minimal text will be associated with the reference.
1 The reference is irrelevant to this particular problem.
Table 1: Relevance Scale
measure co-author agreement on the relevance task.
This is an interesting research question, as it is not
at all clear how much even close collaborators would
agree on relevance, but we do not address this here.
We plan to expand the collection in a second
stage, in line with the Cranfield 2 design. We will
reapproach contributing authors after obtaining re-
trieval results on our collection (e.g., with a stan-
dard IR engine) and ask them to make additional rel-
evance judgements on these papers.
3.3 Criticisms of Cranfield 2
Both Cranfield 1 (Cleverdon, 1960) and 2 were sub-
ject to various criticisms; (Spa?rck Jones, 1981) gives
an excellent account of the tests and their criticisms.
The majority were criticisms of the test collection
paradigm itself and are not pertinent here. How-
ever, the source-document principle (i.e., the use of
queries created from documents in the collection) at-
tracted particular criticisms. The fundamental con-
cern was that the way in which the queries were cre-
ated led to ?an unnaturally close relation? between
the terms in the queries and those used to index
the documents in the colection (Vickery, 1967); any
such relationship might have created a bias towards
a particular indexing language, distorting the com-
parisons that were the goal of the project.
In Cranfield 1, system success was measured
by retrieval of source documents alone, criticized
for being an over-simplification and a distortion of
?real-life? searching. The evaluation procedure was
changed for Cranfield 2 so that source documents
were excluded from searches and, instead, retrieval
of other relevant documents was used to measure
success. This removed the problem that, usually,
when a user searches, there is no source document
for their query. Despite this, Vickery notes that there
were ?still verbal links between sought document
and question? in the new method: each query author
was asked to judge the relevance of the source doc-
ument?s references and ?the questions ... were for-
mulated after the cited papers had been read and has
possibly influenced the wording of his question?.
While adapting the Cranfield 2 method to our
needs, we have tried to address some of the crit-
icisms, e.g., that authors? relevance judgements
change over time. Nevertheless, we still have
source-document queries and must consider the as-
sociated criticisms. Firstly, our test collection is
not intended for comparisons of indexing languages.
Rather, we aim to compare the effect of adding ex-
tra index terms to a base indexing of the documents.
The source documents will have no influence on
the base indexing of a document above that of the
other documents. The additional index terms, com-
ing from citations to that document, will generally
be ?chosen? by someone other than the query author,
with no knowledge of the query terms4. Also, our
documents will be indexed fully automatically, fur-
ther diminishing the scope of any subconscious hu-
man influence.
Thus, we believe that the suspect relationship be-
tween queries and indexing is negligible in the con-
4The exception to this is self-citation. This (very indirectly)
allows the query author to influence the indexing but it seems
highly improbable that an author would be thinking about their
query whilst citing a previous work.
394
text of our work, as opposed to the Cranfield tests,
and that the source-document principle is sound.
3.4 Returns and Analysis
Out of around 500 invitations sent to conference au-
thors, 85 resulted in research questions with rele-
vance judgements being returned; 235 queries in to-
tal. Example queries are:
? Do standard probabilistic parsing techniques,
developed for English, fare well for French and
does lexicalistion help improve parsing results?
? Analyze the lexical differences between genders
engaging in telephone conversations.
Of the 235 queries, 18 were from authors whose
co-authors had also returned data and were dis-
carded (for retrieval purposes); we treat co-author
data on the same paper as ?the same? and keep
only the first authors?. 47 queries had no relevant
Anthology-internal references and were discarded.
Another 15 had only relevant Anthology references
not yet included in the archive5; we keep these for
the time being. This leaves 170 unique queries with
at least 1 relevant Anthology reference and an aver-
age of 3.8 relevant Anthology references each. The
average in-factor across queries is 0.42 (similar to
our previously estimated Anthology in-factor)6 .
Our average number of judged relevant docu-
ments per query is lower than for Cranfield, which
had an average of 7.2 (Spa?rck Jones et al, 2000).
However, this is the final number for the Cran-
field collection, arrived at after the second stage
of relevance judging, which we have not yet car-
ried out. Nevertheless, we must anticipate a po-
tentially low number of relevant documents per
query, particularly in comparison to, e.g., the TREC
ad hoc track (Voorhees and Harman, 1999), with
86.8 judged relevant documents per query.
4 Document Collection and Processing
The Anthology documents are distributed in PDF, a
format designed to visually render printable docu-
ments, not to preserve editable text. So the PDF col-
lection must be converted into a fully textual format.
5HLT-NAACL-2004 papers, e.g., are listed as ?in process?.
6We cannot directly compare this to Cranfield?s in-factor as
we do not have access to the documents.
IXML XML XMLPDF XMLStructure
Presentational StructureLogical
Pre?ProcesssorPTX TemplatePTX List ParserReference ProcessorCitation
XML
+ ReferenceList + Citations
OmniPage
Figure 1: Document Processing Pipeline
A pipeline of processing stages has been developed
in the framework of a wider project, illustrated in
Figure 1.
Firstly, OmniPage Pro 147, a commercial PDF
processing software package, scans the PDFs and
produces an XML encoding of character-level page
layout information. AI algorithms for heuristically
extracting character information (similar to OCR)
are necessary since many of the PDFs were created
from scanned paper-copies and others do not contain
character information in an accessible format.
The OmniPage output describes a paper as text
blocks with typesetting information such as font and
positional information. A pre-processor (Lewin et
al., 2005) filters and summarizes the OmniPage out-
put into Intermediate XML (IXML), as well as cor-
recting certain characteristic errors from that stage.
A journal-specific template converts the IXML to a
logical XML-based document structure (Teufel and
Elhadad, 2002), by exploiting low-level, presenta-
tional, journal-specific information such as font size
and positioning of text blocks.
Subsequent stages incrementally add more de-
tailed information to the logical representation. The
paper?s reference list is annotated in more detail,
marking up individual references, author names, ti-
tles and years of publication. Finally, a citation pro-
cessor identifies and marks up citations in the doc-
ument body and their constituent parts, e.g., author
names and years.
5 Preliminary Experimentation
We expect that our test collection, built for our cita-
tion experiments, will be of wider value and we in-
tend to make it publicly available. As a sanity check
on our data so far, we carried out some preliminary
experimentation, using standard IR tools: the Lemur
Toolkit8, specifically Indri (Strohman et al, 2005),
7http://www.scansoft.com/omnipage/
8http://www.lemurproject.org/
395
its integrated language-model based search engine,
and the TREC evaluation software, trec eval9.
5.1 Experimental Set-up
We indexed around 4200 Anthology documents.
This is the total number of documents that have, at
the time of writing, been processed by our pipeline
(24 years of CL journal, 25 years of ACL proceed-
ings, 14 years of assorted workshops), plus another
?90 documents for which we have relevance judge-
ments that are not currently available through the
Anthology website but should be incorporated into
the archive in the future. The indexed documents do
not yet contain annotation of the reference list or ci-
tations in text. 19 of our 170 queries have no relevant
references in the indexed documents and were not
included in these experiments. Thus, Figure 2 shows
the distribution of queries over number of relevant
Anthology references, for a total of 151 queries.
Our Indri index was built using default parameters
with no optional processing, e.g., stopping or stem-
ming, resulting in a total of 20117410 terms, 218977
unique terms and 2263 ?frequent?10 terms.
We then prepared an Indri-style query file from
the conference research questions. The Indri query
language is designed to handle highly complex
queries but, for our very basic purposes, we created
simple bag-of-words queries by stripping all punctu-
ation from the natural language questions and using
Indri?s #combine operator over all the terms. This
means Indri ranks documents in accordance with
query likelihood. Again, no stopping or stemming
was applied.
Next, the query file was run against the Anthology
index using IndriRunQuery with default parameters
and, thus, retrieving 1000 documents for each query.
Finally, for evaluation, we converted the Indri?s
ranked document lists to TREC-style top results file
and the conference relevance judgements compiled
into a TREC-style qrels file, including only judge-
ments corresponding to references within the in-
dexed documents. These files were then input to
trec eval, to calculate precision and recall metrics.
9http://trec.nist.gov/trec eval/trec eval.8.0.tar.gz
10Terms that occur in over 1000 documents.
2 3 4 5 6 7 8 9 10 11 12 13 14Threshhold (# Relevant References in Index)
0
0.05
0.1
0.15
0.2
Pre
cis
ion
 at 
5 D
ocu
me
nts
Figure 3: Effect of Thresholding on P at 5 Docs
5.2 Results and Discussion
Out of 489 relevant documents, 329 were retrieved
within 1000 (per query) documents. The mean av-
erage precision (MAP) was 0.1014 over the 151
queries. This is the precision calculated at each rele-
vant document retrieved (0.0, if that document is not
retrieved), averaged over all relevant documents for
all queries, i.e., non-interpolated. R-precision, the
precision after R (the number of relevant documents
for a query) documents are returned, was 0.0965.
The average precision at 5 documents was 0.0728.
We investigated the effect of excluding queries
with lower than a threshold number of judged rel-
evant documents. Figure 3 shows that precision at
5 documents increases as greater threshold values
are applied. Similar trends were observed with other
evaluation measures, e.g., MAP and R-precision in-
creased to 0.2018 and 0.1528, respectively, when
only queries with 13 or more relevant documents
were run, though such stringent thresholding does
result in very few queries. Nevertheless, these trends
do suggest that the present low number of relevant
documents has an adverse effect on retrieval results
and is a potential problem for our test collection.
We also investigated the effect of including only
authors? main queries, as another potential way of
objectively constructing a ?higher quality? query set.
Although, this decreased the average in-factor of rel-
evant references, it did, in fact, increase the average
absolute number of relevant references in the index.
Thus, MAP increased to 0.1165, precision at 5 doc-
uments to 0.1016 and R-precision to 0.1201.
These numbers look poor in comparison to the
performance of IR systems at TREC but, impor-
tantly, they are not intended as performance results.
Their purpose is to demonstrate that such numbers
can be produced using the data we have collected,
396
(a) (b)
0 20 40 60 80 100 120 140Query
0
10
20
30
# R
ele
van
t R
efe
ren
ces
TotalAnthology Index
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15# Relevant References in Index
0
10
20
30
# Q
uer
ies
(a) (b)
Figure 2: (a) Relevant References Per Query and (b) Distribution of Queries over Number of Relevant References
rather than to evaluate the performance of some new
retrieval system or strategy.
A second point for consideration follows directly
from the first: our experiments were carried out
on a new test collection and ?different test collec-
tions have different intrinsic difficulty? (Buckley
and Voorhees, 2004). Thus, it is meaningless to
compare statistics from this data (from a different
domain) to those from the TREC collections, where
queries and relevance judgements were collected in
a different way, and where there are very many rele-
vant documents.
Thirdly, our experiments used only the most basic
techniques and the results could undoubtedly be im-
proved by, e.g., applying a simple stop-list. Never-
theless, this notion of intrinsic difficulty means that
it may be the case that evaluations carried out on this
collection will produce characteristically low preci-
sion values.
Low numbers do not necessarily preclude our
data?s usefulness as a test collection, whose purpose
is to facilitate comparative evaluations. (Voorhees,
1998) states that ?To be viable as a laboratory tool,
a [test] collection must reliably rank different re-
trieval variants according to their true effectiveness?
and defends the Cranfield paradigm (from criticisms
based on relevance subjectivity) by demonstrating
that the relative performance of retrieval runs is sta-
ble despite differences in relevance judgements. The
underlying principle is that it is not the absolute pre-
cision values that matter but the ability to compare
these values for different retrieval techniques or sys-
tems, to investigate their relative benefits. A test col-
lection with low precision values will still allow this.
It is known that all evaluation measures are un-
stable for very small numbers of relevant documents
(Buckley and Voorhees, 2000) and there are issues
arising from incomplete relevance information in a
test collection (Buckley and Voorhees, 2004). This
makes the second stage of our test collection com-
pilation even more indispensable (asking subjects to
judge retrieved documents), as this will increase the
number of judged relevant documents, as well as
bridging the completeness gap.
There are further possibilities of how the prob-
lem could be countered. We could exclude queries
with lower than a threshold number of relevant docu-
ments (after the second stage). Given the respectable
number of queries we have, we might be able to af-
ford this luxury. We could add relevant documents
from outside the Anthology to our collection. This
is least preferable methodologically: using the An-
thology has the advantage that it has a real identity
and was created for real reasons outside our experi-
ments. Furthermore, the collection ?covers a field?,
i.e., it includes all important publications and only
those. By adding external documents to the collec-
tion, it would lose both these properties.
6 Conclusions and Future Work
We have presented an approach to building a test
collection from an existing collection of research pa-
pers and described the application of our method
to the ACL Anthology. We have collected 170
queries with relevance data, centered around the
ACL-2005 and HLT-EMNLP-2005 conferences. We
397
have sanity-checked the usability of our data by
running the queries through a retrieval system and
evaluating the results using standard software. The
collection currently has a low number of judged
relevant documents and further experimentation is
needed to determine if this poses a real problem.
We plan a second stage of collecting relevance
judgements, in line with the original Cranfield de-
sign, whereby authors who have contributed queries
will be asked to judge the relevance of documents in
retrieval rankings from standard IR models and, ide-
ally, from our eventual citation-based experiments.
Nevertheless, our test collection is likely to suffer
from incomplete relevance information. The bpref
measure (Buckley and Voorhees, 2004) gauges re-
trieval effectiveness solely on the basis of judged
documents and is more stable to differing levels
of completeness than measures such as MAP, R-
precision or precision at fixed document cutoffs.
Thus, bpref may offer a solution to the incomplete-
ness problem and we intend to investigate its poten-
tial use in our future evaluations.
When finished, we hope our test collection will
be a generally useful IR resource. In particular, we
expect the collection to be useful for experimenta-
tion with citation information, for which there is cur-
rently no existing test collection with the properties
that ours offers.
Acknowledgements Thanks to the reviewers for
their useful comments and to Karen Spa?rck Jones for
many instructive discussions.
References
Shannon Bradshaw. 2003. Reference directed indexing:Redeeming relevance for subject search in citation in-dexes. In Research and Advanced Technology for Dig-ital Libraries (ECDL), pages 499?510.
Sergey Brin and Lawrence Page. 1998. The anatomy ofa large-scale hypertextual Web search engine. Com-puter Networks and ISDN Systems, 30(1?7):107?117.
Chris Buckley and Ellen Voorhees. 2000. Evaluating
evaluation measure stability. In Research and Devel-opment in Information Retrieval (SIGIR).
Chris Buckley and Ellen Voorhees. 2004. Retrieval eval-uation with incomplete information. In Research anddevelopment in information retrieval (SIGIR).
Cyril Cleverdon, Jack Mills, and Michael Keen. 1966.Factors determining the performance of indexing
sytems, volume 1. design. Technical report, ASLIBCranfield Project.
Cyril Cleverdon. 1960. Report on the first stage of an in-
vestigation into the comparative efficiency of indexingsystems. Technical report, ASLIB Cranfield Project.
Cyril Cleverdon. 1997. The Cranfield tests on index lan-guage devices. In Readings in information retrieval,pages 47?59. Morgan Kaufmann Publishers Inc.
Brian D. Davison. 2000. Topical locality in the web.
In Research and Development in Information Retrieval(SIGIR), pages 272?279.
Eugene Garfield. 1972. Citation analysis as a tool injournal evaluation. Science, 178 (4060):471?479.
Donna Harman. 2005. The TREC test collections. InEllen Voorhees and Donna Harman, editors, TRECExperiment and Evaluation in Information Retrieval,chapter 2. MIT Press.
David Hawking and Nick Craswell. 2005. The verylarge collection and web tracks. In Ellen Voorhees and
Donna Harman, editors, TREC: Experiment and Eval-uation in Information Retrieval, chapter 9. MIT Press.
Michael Kluck. 2003. The GIRT data in the evaluationof CLIR systems - from 1997 until 2003. In CLEF,pages 376?390.
Ian Lewin, Bill Hollingsworth, and Dan Tidhar. 2005.
Retrieving hierarchical text structure from typeset sci-entific articles - a prerequisite for e-science text min-ing. In UK e-Science All Hands Meeting.
Oliver McBryan. 1994. GENVL and WWWW: Toolsfor taming the web. In World Wide Web Conference.
James Pitkow and Peter Pirolli. 1997. Life, death, and
lawfulness on the electronic frontier. In Human Fac-tors in Computing Systems.
Karen Spa?rck Jones, Steve Walker, and Stephen Robert-son. 2000. A probabilistic model of information re-trieval: development and comparative experiments -
parts 1 and 2. Information Processing and Manage-ment, 36(6):779?840.
Karen Spa?rck Jones. 1981. The Cranfield tests. InKaren Spa?rck Jones, editor, Information Retrieval Ex-periment, chapter 13, pages 256?284. Butterworths.
Trevor Strohman, Donald Metzler, Howard Turtle, and
W. Bruce Croft. 2005. Indri: a language-model basedsearch engine for complex queries. Technical report,University of Massachusetts.
Simone Teufel and Noemie Elhadad. 2002. Collection
and linguistic processing of a large-scale corpus ofmedical articles. In Language Resources and Evalu-ation Conference (LREC).
B. C. Vickery. 1967. Reviews of CLEVERDON, C. W.,MILLS, J. and KEEN, E. M. the Cranfield 2 report.Journal of Documentation, 22:247?249.
Ellen Voorhees and Donna Harman. 1999. Overview ofthe eighth Text REtrieval Conference (TREC 8). InText REtrieval Conference (TREC).
Ellen Voorhees. 1998. Variations in relevance judgmentsand the measurement of retrieval effectiveness. In Re-search and Development in Information Retrieval (SI-GIR), pages 315?323.
398
Proceedings of NAACL HLT 2007, pages 316?323,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Whose idea was this, and why does it matter?
Attributing scientific work to citations
Advaith Siddharthan & Simone Teufel
Natural Language and Information Processing Group
University of Cambridge Computer Laboratory
{as372,sht25}@cl.cam.ac.uk
Abstract
Scientific papers revolve around cita-
tions, and for many discourse level
tasks one needs to know whose work
is being talked about at any point in
the discourse. In this paper, we in-
troduce the scientific attribution task,
which links different linguistic expres-
sions to citations. We discuss the
suitability of different evaluation met-
rics and evaluate our classification ap-
proach to deciding attribution both in-
trinsically and in an extrinsic evalua-
tion where information about scientific
attribution is shown to improve per-
formance on Argumentative Zoning, a
rhetorical classification task.
1 Introduction
In the recent past, there has been a focus on
information management from scientific litera-
ture. In the genetics domain, for instance, in-
formation extraction of genes and gene?protein
interactions helps geneticists scan large amounts
of information (e.g., as explored in the TREC
Genomics track (Hersh et al, 2004)). Elsewhere,
citation indexes (Garfield, 1979) provide biblio-
metric data about the frequency with which par-
ticular papers are cited. The success of citation
indexers such as CiteSeer (Giles et al, 1998) and
Google Scholar relies on the robust detection
of formal citations in arbitrary text. In bibli-
ographic information retrieval, anchor text, i.e.,
the context of a citation can be used to charac-
terise (index) the cited paper using terms out-
side of that paper (Bradshaw, 2003); O?Connor
(1982) presents an approach for identifying the
area around citations where the text focuses on
that citation. And automatic citation classifi-
cation (Nanba and Okumura, 1999; Teufel et
al., 2006) determines the function that a cita-
tion plays in the discourse.
For such information access and retrieval pur-
poses, the relevance of a citation within a paper
is often crucial. One can estimate how impor-
tant a citation is by simply counting how often
it occurs in the paper. But as Kim and Webber
(2006) argue, this ignores many expressions in
text which refer to the cited author?s work but
which are not as easy to recognise as citations.
They address the resolution of instances of the
third person personal pronoun ?they? in astron-
omy papers: it can either refer to a citation or to
some entities that are part of research within the
paper (e.g., planets or galaxies). Several appli-
cations should profit in principle from detecting
connections between referring expressions and
citations. For instance, in citation function clas-
sification, the task is to find out if a citation is
described as flawed or as useful. Consider:
Most computational models of discourse
are based primarily on an analysis of
the intentions of the speakers [Cohen and
Perrault, 1979][Allen and Perrault,
1980][Grosz and Sidner, 1986]WEAK.
The speaker will form intentions based on
his goals and then act on these intentions, pro-
ducing utterances. The hearer will then re-
construct a model of the speaker?s intentions
upon hearing the utterance. This approach
has many strong points, but does not
provide a very satisfactory account of
the adherence to discourse conventions in di-
alogue.
The three citations above are described as flawed
(detectable by ?does not provide a very satis-
factory account?), and thus receive the label
Weak. However, in order to detect this, one
must first realise that ?this approach? refers to
316
the three cited papers. A contrasting hypoth-
esis could be that the citations are used (thus
deserving the label Use; the cue phrase ?based
on? might make us think so (as in the context
?our work is based on?). This, however, can be
ruled out if we know that ?the speaker? is not
referring to some aspect of the current paper.
2 The scientific attribution task
We define an attribution task where possible ref-
erents are members of the reference list (i.e.,
each cited paper), the Current-Paper, and
a back-off category No-Specific-Paper for
markables that are not attributable to any spe-
cific paper(s). Our markables are as follows:
all definite descriptions (e.g., ?the hearer?, and
including demonstrative noun phrases such as
?these intentions?), all ?work? nouns1, and all
pronouns (possessive, personal and demonstra-
tive); c.f., underlined strings in the above exam-
ple. Our notion of attribution link encompasses
two relations:
1. Anaphoric: The referents are entire re-
search papers, or the papers? authors
2. Subpart: The referents are some compo-
nent of an approach/argument/claim in the
research paper
There are two tasks: attributing a linguistic
expression to the right paper (including the cur-
rent paper) ? a task we call scientific attribution
? and deciding whether or not the expression is
anaphoric to the entirety of the paper, or just to
some subpart of it.
Kim and Webber (2006) solve the problem of
distinguishing between these relations for one
case. They decide whether the pronoun ?they?
anaphorically refers to the authors of a cited pa-
per, or whether it refers to some entity that is
discussed in (a subpart of) a paper (e.g., ?galax-
ies?). In this paper, we tackle the other problem
of scientific attribution.
We do not distinguish between the two types
of links stated above, but only identify which ci-
tation(s) a linguistic expression is attributable
1We use a list of around 40 research methodology re-
lated nouns from Teufel and Moens (2002), such as e.g.,
?study, account, investigation, result? etc. These are
nouns we are particularly interested in.
to. For tasks of interest to us, it is not enough
to only consider anaphoric references to entire
papers; authors often make statements compar-
ing/using/criticising aspects or subparts of cited
work. We therefore consider a far wider range
of markables than Kim and Webber?s single pro-
noun ?they?.
Our attribution task differs from the tradi-
tional anaphora resolution task in that we have
a fixed list of possible referents (the reference
list items, Current-Paper or No-Specific-
Paper) that are known upfront. Also, we do
not form co-reference chains; we attribute a re-
ferring expression directly to one or more ref-
erents. Ours is therefore a multi-label classi-
fication task, where the citations, Current-
Paper and No-Specific-Paper are the labels,
and where one or more labels are assigned to
each markable.
We evaluate intrinsically by comparing to
human-annotated attribution, and extrinsically
by showing that automatically acquired knowl-
edge about scientific attribution improves per-
formance on a discourse classification task?
Argumentative Zoning (Teufel and Moens,
2002), where sentences are labelled as one
of {Own, Other, Background, Textual,
Aim, Basis, Contrast} according to their role
in the author?s argument.
We describe our data in ?3 and methodology
in ?4, discuss evaluation metrics in ?5, and eval-
uate intrinsically in ?6 and extrinsically in ?7.
3 Data
We used data from the CmpLg (Computation
and Language archive; 320 conference articles
in computational linguistics). The articles are
in XML format.
We produced an annotated corpus (10 arti-
cles, 4290 data points, i.e., markables) based on
written guidelines. The task was found to be
quite intuitive by our annotators, and this was
reflected in high agreement - Krippendorff?s al-
pha2 of more than 0.8 (2 annotators, 3 papers,
1429 data points) on the attribution task. The
distribution of classes was, as expected, quite
skewed: 69% of markables are attributable to
2see description in ?5.2
317
Current-Paper, 7% to no specific paper and
24% to specific references (on average, 1.7 per
reference). Details about the annotation pro-
cess and human agreement figures can be found
in Siddharthan and Teufel (2007).
4 Machine Learning Approach
We frame the attribution problem as a classi-
fication task: Given a markable (the definite
description/pronoun/work noun under consid-
eration), a binary yes/no decision is made for
each cited paper, and a binary yes/no decision
is made for whether the markable is attributable
to the current paper. The list of labels for the
markable is compiled by including all the cita-
tions for which the machine learner returns yes,
and Current-Paper if the learner returns yes.
If the list is empty (learner returns no for every-
thing), the label is No-Specific-Paper.
Since the model for whether a markable is at-
tributable to the current work is likely to be
different from the model for whether it is at-
tributable to a citation, we trained separate
models for the two problems.
4.1 Deciding attribution to a citation
For each data point to be classified (called NP
below), we create a machine learning instance
for each reference list item by automatically
computing the following features from POS-
tagged text:
1. Properties of data point (NP) and the closest Cita-
tion instance (CIT) of the reference list item:
(a) Type of NP (Definite Description/Work
Noun/Pronoun)
(b) CIT is a self Citation or not
(c) CIT is syntactic (in running text) or paren-
thetical
(d) Is CIT Hobbs? prediction (searching left?right
starting from current sentence and then con-
sidering previous sentences, is CIT the first
citation or reference to current work found)?
2. Distance measures:
(a) Dist. between NP and CIT measured in words
(b) Dist. between NP and CIT measured in sen-
tences
(c) Dist. between NP and CIT measured in para-
graphs
(d) Is CIT after NP in the discourse (cataphor)?
(e) Distance between CIT and the closest first
person pronoun or ?this paper? in words
3. Contextual:
(a) Rank of CIT (how many other reference list
items are closer)
(b) Number of times CIT is cited in the paragraph
(c) Number of times CIT is cited in the whole
paper
(d) Current Section heading (this feature has 5
values: Introduction, Methods, Results, Con-
clusions, Unrecognised)
4. Agreement:
(a) Agreement Number (He/She & single author
non-self citation)
(b) Agreement Person (First & Current/Self Ci-
tation, Third and Not-Current)
We have a chicken and egg problem with cal-
culating the distance of a reference to current
work in 2(e). Unlike citations, these are not un-
ambiguously marked in the text. We calculate
distance from the closest first person pronoun
(even though these could possibly refer to a self
citation, rather than current work) or the phrase
?this paper?, which can again refer to other cita-
tions but predominantly refers to current work.
4.2 Deciding attribution to current work
We use the same features for the second clas-
sifier that makes the decision on whether the
data point refers to Current-Paper, with the
following changes: Features 1(b,c) are removed
as they are meaningless; 1(d) checks Hobbs?
prediction for a first person pronoun/?this pa-
per?, rather than CIT; in 2(a?d), the distance is
measured between the closest first person pro-
noun/?this paper? and the markable, rather
than a citation and the markable; similarly, in
3(b,c) we count instances of first person pro-
noun/?this paper?; for 2(e), we now calculate
the distance of the closest citation instance. In
short, the same features are used, but current
work and citations are swapped.
5 Evaluation Metrics
We consider two evaluation metrics. The first
is the scoring system used for the co-reference
task in the Message Understanding Conferences
MUC-6 and MUC-7. The second is Krippen-
dorff?s ?. We briefly discuss both below.
5.1 The MUC-6/MUC-7 Metric
The MUC-6/MUC-7 Co-reference evaluation
metric (Vilain et al, 1995) works by compar-
ing co-reference classes across two annotated
318
files. Calling one annotation the ?model? and
the other the ?system?, for each co-reference
class S in the model, c(S) is the minimal num-
ber of co-reference links needed to generate the
class (this is one less than the cardinality of the
class; c(S) = |S| ? 1). m(S) is the number of
?missing? links in the system annotation rela-
tive to the co-reference class as marked up in
the model. In other words, this is the minimum
number of co-reference links that need to be
added to the system annotation to fully gener-
ate the co-reference class S in the model. Recall
error is then RE(S) = m(S)/c(S) and Recall is
R(S) = 1 ? RE = c(S)?m(S)c(S) . Recall for the en-
tire file (or set of files) is calculated by summing
over all co-reference classes in the model:
R =
?
i c(Si) ? m(Si)
?
i c(Si)
Precision (P ) is calculated by swapping the
model and system and the f-measure (F =
2R ? P/(R + P )) is symmetric with respect to
both annotations.
5.2 Krippendorff?s Alpha
We follow Passonneau (2004) and Poesio and
Artstein (2005) in using Krippendorff (1980)?s
? metric to compute agreement between anno-
tations. The advantage of ? over the more com-
monly used ? metric is that ? allows for par-
tial agreement when annotators assign multiple
labels to the same markable; in this case calcu-
lating agreement on a markable requires a more
graded agreement calculation than the ?1 if sets
are identical and 0 otherwise? provided for by
?. Krippendorff?s ? measures disagreement, and
allows for the use of distance metrics to calculate
partial disagreement. Following Passonneau, we
present results using four distance metrics:
1. (N)ominal: Two sets have distance N = 0
if they are identical and N = 1 if they are
not. ? calculated using the nominal dis-
tance metric is equivalent to ?.
2. (J)accard: Two sets A and B have dis-
tance J = 1 ? |A ? B|/|A ? B|. In other
words, the distance between two sets is
larger, the smaller their intersection and the
larger their union.
3. (D)ice: Two sets A and B have distance
D = 1 ? 2 ? |A ? B|/(|A| + |B|). In prac-
tice, the Dice distance metric behaves simi-
larly to the Jaccard metric, but tends to be
smaller, resulting in slightly higher ?.
4. (M)ASI: This is the Jaccard distance J
weighted by a monotonicity distance m
where, m = 0 if two sets are identical;
m = 0.33 if one is a subset of the other;
m = 0.67 if the intersection and the two
set differences are all non-null; m = 1 if the
two sets are disjoint. Formally, the MASI
metric is M = m ? J .
As an example, consider two sets {a, b, c} and
{b, c, d}. The distances between these sets are
N = 1, J = 1?2/4 = 0.5, D = 1?2?2/(3+3) =
0.33 and M = 0.67 ? 0.5 = 0.33.
Krippendorff?s ? is defined as ? = 1?Do/De,
where Do is the observed disagreement and De
is the disagreement that is expected by chance:
Do =
1
c(c ? 1)
?
j
?
k
?
k?
njknjk?dkk?
De =
1
c(c ? 1)
?
k
?
k?
nknk?dkk?
In the above formulae, c is the number of
coders, njk is the number of times item j is
classed as category k, nk is the number of times
any item is classed as category k and dkk? is the
distance between categories k and k ?.
Like ?, Krippendorff?s ? is 1 when there is
perfect agreement, 0 when the observed agree-
ment is only what was expected by chance, neg-
ative when observed agreement is less than ex-
pected by chance and positive when observed
agreement is greater than expected by chance.
6 Intrinsic Evaluation Results
We ran a machine learning experiment us-
ing 10-fold cross-validation and the memory-
based learner IBk3 (with k=6), using the Weka
toolkit (Witten and Frank, 2000). The perfor-
mance is shown in Tables 1 and 2. To position
these results we compare them with three base-
line lower bounds and the human performance
upper bound in Table 3. We use three baselines:
3Memory based learning gave better results on this
task than other learners (NB, HNB, IBk, J48, cf. ? 7.3.
319
Paper Items ?-N ?-J ?-D ?-M %A?
0003055 446 .601 .606 .607 .610 85%
0005006 446 .670 .704 .711 .715 81%
0005015 462 .679 .696 .701 .706 81%
0005025 277 .707 .707 .707 .707 86%
0006011 393 .766 .771 .772 .775 88%
0006038 578 .551 .568 .573 .578 79%
0007035 393 .570 .590 .600 .609 90%
0008026 449 .700 .700 .700 .700 87%
0001001 420 .564 .565 .569 .571 88%
0001020 429 .730 .778 .790 .801 88%
AVG. 429 .654 .669 .673 .677 85%
?% Agreement, the conservative estimate measured
using the Nominal metric
Table 1: Agreement with Human Gold Standard
? BASEM (Major Class): All data points are
labelled CURRENT-WORK
? BASEP (Previous): Data points are tagged
with the most recent label
? BASEH (Hobbs? Prediction): Data points
are tagged with the label found by Hobbs?
(1986) search (Search left to right in each
sentence, starting from current sentence,
then considering previous sentences)
As Table 3 shows, our machine learning ap-
proach performs much better than the base-
lines on all the agreement metrics, and is indeed
closer to human performance than to any of the
baselines. The MUC evaluation appears to pro-
duce highly inflated results on our task ? when
there is a small set of co-reference classes and
one of these classes contains 70% of data points,
it takes only a small number of missing links to
correct annotations. This results in unreason-
ably high values, particularly for the majority
class baseline of labelling every data point as
Current-Paper. We believe that the ? met-
rics provide a much more realistic estimate of
the difficulty of the task and the relative perfor-
mances of different approaches.
Table 4 shows the performance of the ma-
chine learner for each of the three types of lin-
guistic expressions considered. Pronouns are
the easiest to resolve, with on average 90% re-
solved correctly (an agreement with the human
gold standard of ? = .71). This drops to 85%
(? = .68) for definite descriptions and demon-
stratives, and further to 78% (? = .63) for re-
Paper No. Classes Recall Precision F
0003055 14 .934 .886 .910
0005006 17 .875 .870 .872
0005015 19 .897 .876 .886
0005025 16 .903 .874 .888
0006011 14 .942 .909 .925
0006038 25 .905 .893 .899
0007035 18 .957 .926 .941
0008026 9 .966 .962 .964
0001001 14 .949 .908 .928
0001020 18 .924 .926 .925
TOTAL 164 .924 .903 .913
Table 2: Evaluation using MUC-6/7 software
Algo ?-N ?-J ?-D ?-M %Agr?muc-f
BaseM .002 .001 .001 .001 69% .934
BaseP -.101 -.083 -.081 -.077 19% .894
BaseH .387 .397 .399 .407 72% .910
IBk .654 .669 .673 .677 85% .913
Hum?? .806 .808 .808 .809 91% .965
?% Agreement, the conservative estimate measured
using the Nominal metric
??Agreement between two human annotators over a
subset of the corpus (3 files, 1429 data points)
Table 3: Comparison with Baselines and Human
Performance (Averaged results)
maining work nouns (i.e., those not already in a
definite noun phrase).
While all the features contributed to the re-
ported results, the most important features (in
terms of information gain) for deciding attribu-
tion to a citation were the paragraph level cita-
tion count 3(b), the distance features 2(a,b,c,d),
the rank 3(a) and the Hobbs? prediction 1(d).
The most important features for deciding attri-
bution to the current paper were the distance
features 2(a,c,e), the rank 3(a) and the Hobbs?
prediction 1(d).
7 Extrinsic Evaluation
To demonstrate the use of automatic scientific
attribution classification, we studied its util-
ity for one well known discourse annotation
task: Argumentative Zoning (Teufel and Moens,
2002). Argumentative Zoning (AZ) is the task of
applying one of seven discourse level tags (Fig-
ure 1) to each sentence in a scientific paper.
These categories model several aspects of sci-
entific papers: from the distinction of segments
by who an idea is attributed to (Own ? Other ?
Background), to the judgement of how the au-
320
Paper Pronouns Definites Work Nouns
?M %N ?M %N ?M %N
0003055 .746 94% .556 83% .735 87%
0005006 .846 91% .703 85% .700 78%
0005015 .662 83% .692 79% .787 86%
0005025 .804 89% .717 87% .514 78%
0006011 .824 91% .807 91% .615 76%
0006038 .603 90% .609 81% .430 66%
0007035 .577 94% .507 91% .770 87%
0008026 .678 88% .726 87% .551 78%
0011001 .562 97% .633 87% .377 81%
0011020 .792 90% .798 92% .808 89%
AVG. .709 90% .675 85% .629 78%
Table 4: Results for different markable types
Category Description
Background Generally accepted background knowl-
edge
Other Specific other work
Own Own work: method, results, future
work
Aim Specific research goal
Textual Textual section structure
Contrast Contrast, comparison, weakness of
other solution
Basis Other work provides basis for own work
Figure 1: AZ Annotation scheme
thors relate to other work (Contrast ? Basis)
to the rhetorical status of high-level discourse
goals (statement of Aim; overview of section
structure (Textual)). Some of these categories
(Background, Other and Own) occur in zones
that span many sentences. Other categories typ-
ically occur in short zones, often just a single
sentence (Textual, Aim, Contrast, Basis).
In all work to date, classification of sentences
into one of the AZ categories has been performed
on the basis of features extracted from within
the sentence, and a few contextual features such
as section heading and location in document.
Scientific attribution links previously unresolved
noun phrases or pronouns in the sentence to cita-
tions. As this provides the machine learner with
more information, AZ results should improve.
7.1 AZ Data
The evaluation corpus used is the one from
Teufel and Moens (2002). It consists of 80 con-
ference papers in computational linguistics, con-
taining around 12000 sentences. Each of these
is manually tagged as one of {OWN, OTH, BKG,
BAS, AIM, CTR, TXT}. The reliability observed
is reasonable (Kappa=0.71)).
7.2 Features
Following Teufel and Moens (2002), we used su-
pervised ML using features extracted by shallow
processing (POS tagging and pattern matching):
? Lexical (cue phrase) features consist
of three features: the first models occur-
rence of about 1700 manually identified sci-
entific cue phrases (such as ?in this paper?).
The cue phrases are classified into semantic
groups. The second models the main verb
of the sentence, by lookup in a verb lexicon
organised by 13 main clusters of verb types
(e.g. ?change verbs?), and the third models
the likely subject of the sentence, by clas-
sifying them either as the authors, or other
researchers, or none of the above, using an
extensive lexicon of regular expressions.
? Content word features model occurrence
and density of content words in the sen-
tences, where content words are either de-
fined as non-stoplist words in the subsection
heading preceding the sentence, or as words
with a high TF*IDF score.
? Linguistic features include (complex)
tense, voice, and presence of an auxiliary.
? Citation features detect properties of for-
mal citations in text, such as the occurrence
of authors? names in text, the position of a
citation in text, and whether the citation
is a self citation (i.e., includes any of the
authors of the paper itself).
? Location features: Rhetorical roles are
expected at certain places in the document,
for instance, background sentences are more
likely to occur at the beginning of the text,
and goal statements often occur after about
a fifth of the paper. We model this by split-
ting the text into ten segments and assign-
ing each sentence to the segment it is lo-
cated in. We also use the section heading
as a contextual feature.
Some categories tend to occur in blocks (e.g.,
Own, Other, Background), and the context
in terms of the label of the previous sentence
has good predictive value. We model this (the
321
Learner kappa Macro-F
No Attrib With Attrib No With
NB .45 .46 .53 .53
HNB .42 .45 .51 .53
IBk .34 .36 .39 .39
J48 .38 .41 .41 .48
Stacking .45 .48 .51 .53
Table 5: Improvement on AZ from using auto-
matic scientific attribution classification.
so-called History feature) by running the clas-
sifier twice, and including the prediction for the
previous sentence as a feature the second time.
Due to practical considerations, we obtained
our linguistic features using the RASP part of
speech tagger (Briscoe and Carroll, 1995), when
in previous work we used the LT TTT (Grover
et al, 2000). We would not expect this to in-
fluence results much, however. Another differ-
ence is that we use around 1700 additional cue
phrases acquired from previous work on another
discourse task4 (Teufel et al, 2006).
In addition to these features, we use four
features obtained from the scientific attribution
task described in this paper:
Scientific Attribution Features:
? Whether there is any reference to current
work in the sentence
? Whether there is any reference to any spe-
cific citation in the sentence
? Whether there is any reference in the sen-
tence to work that is in neither the current
paper nor any specific citation
? Which of these, if any, is in subject position
Our aim is to explore whether these features
obtained from the scientific attribution task in-
fluence machine learning performance on AZ.
7.3 AZ results
We ran five different machine learners with and
without the four scientific attribution features
(c.f., ?7.2). Note that our labelled data for the
attribution task does not overlap with the 80 pa-
pers in the AZ corpus, and all attribution pre-
dictions used in features for this AZ experiment
4These cues are acquired manually from files that are
not part of the AZ evaluation corpus.
Without Attribution Features
Aim Ctr Txt Own Bkg Bas Oth
P .44 .42 .52 .84 .46 .34 .47
R .61 .30 .68 .88 .45 .37 .37
F .52 .35 .59 .86 .46 .35 .42
Correctly Classified Instances 73.0%
Kappa statistic 0.45
Macro-F 0.51
With Attribution Features
Aim Ctr Txt Own Bkg Bas Oth
P .57 .42 .57 .84 .44 .40 .55
R .61 .27 .66 .90 .47 .43 .42
F .59 .33 .61 .87 .46 .41 .47
Correctly Classified Instances 74.7%
Kappa statistic 0.48
Macro-F 0.53
Table 6: Best AZ results using Stacked classifier:
with and without Attribution Features.
are obtained entirely from unseen (and indeed
unlabelled) data based on the model learnt on
10 papers (c.f., ?6). The learners we used (with
default Weka settings) are:
? NB: Naive Bayes learner
? HNB: Hidden Naive Bayes learner
? IBk: Memory based learner
? J48: Decision tree based learner
? STACKING: combining NB and J48 classi-
fiers with the stacking method
As mentioned under History feature above, we
run each learner twice, the second time includ-
ing the machine learning prediction for the pre-
vious sentence (as we found in Teufel and Moens
(2002) for NB, we noticed a slight improvement
in performance when using the history feature
(between .005 and .01 on both ? and Macro-
F for all learners)). We found an improvement
from including the four reference features with
all the learners, as shown in Table 5.
For a more detailed view of where the im-
provement comes from, refer to Table 6, which
shows precision, recall and f-measure per cate-
gory for our best learner. The biggest improve-
ments from using attribution features are for the
categories Other, Aim and Bas. The improve-
ment in Other was to be be expected, as this
zone is directly related to the attribution classi-
fication. The large improvements in Aim and
322
Aim Ctr Txt Own Bkg Bas Oth
P .44 .34 .57 .84 .40 .37 .52
R .65 .20 .66 .88 .50 .40 .39
F .52 .26 .61 .86 .44 .38 .44
Correctly Classified Instances 72.5%
Kappa statistic 0.45
Macro-F 0.50
Table 7: Teufel and Moens (2002)?s best AZ re-
sults (Naive Bayes Classifier).
Bas is good news, as these are amongst our
most informative rhetorical categories for down-
stream tasks. Our best results of Kappa=0.48
and Macro-F=0.53 are better than the best pre-
viously published results on task (Kappa=0.45
and Macro-F=0.50 in Teufel and Moens (2002)).
Our results improve on the results of Teufel and
Moens (2002) (reproduced in Table 7) ? both
overall and for each individual category.
8 Conclusions
We have described a new reference task - decid-
ing scientific attribution, and demonstrated high
human agreement (? > 0.8) on this task. Our
machine learning solution using shallow features
achieves an agreement of ?M = 0.68 with the
human gold standard, increasing to ?M = 0.71
if only pronouns need to be resolved. We have
also demonstrated that information about scien-
tific attribution improves results for a discourse
classification task (Argumentative Zoning).
We believe that similar improvements can be
achieved on other discourse annotation tasks in
the scientific literature domain. In particular,
we plan to investigate the use of scientific at-
tribution information for the citation function
classification task.
Acknowledgements
This work was funded by the EPSRC project
SciBorg (EP/C010035/1, Extracting the Science
from Scientific Publications).
References
S. Bradshaw. 2003. Reference directed indexing: Re-
deeming relevance for subject search in citation
indexes. In Proc. of ECDL.
T. Briscoe and J. Carroll. 1995. Developing and
evaluating a probabilistic LR parser of part-of-
speech and punctuation labels. In Proc. of IWPT-
95, Prague / Karlovy Vary, Czech Republic.
E. Garfield. 1979. Citation Indexing: Its Theory and
Application in Science, Technology and Humani-
ties. J. Wiley, New York, NY.
C. L. Giles, K. Bollacker, and S. Lawrence. 1998.
Citeseer: An automatic citation indexing system.
In Proc. of the Third ACM Conference on Digital
Libraries.
C. Grover, C. Matheson, A. Mikheev, and M. Moens.
2000. LT TTT - A flexible tokenisation tool. In
Proc. of LREC-00, Athens, Greece.
W. Hersh, R. Bhuptiraju, L. Ross, P. Johnson, A.
Cohen, and D. Kraemer. 2004. Trec 2004 ge-
nomics track overview. In Proc. of TREC.
J. Hobbs. 1986. Resolving Pronoun References. In
Readings in Natural Language, Grosz, B., Sparck-
Jones, K. and Webber, B. (eds.) Morgan Kauf-
man.
Y. Kim and B. Webber. 2006. Automatic refer-
ence resolution in astronomy articles. In Proc. of
20th International CODATA Conference, Beijing,
China.
K. Krippendorff. 1980. Content Analysis: An in-
troduction to its methodology. Sage Publications,
Beverly Hills.
H. Nanba and M. Okumura. 1999. Towards multi-
paper summarization using reference information.
In Proc. of IJCAI-99.
J. O?Connor. 1982. Citing statements: Computer
recognition and use to improve retrieval. Informa-
tion Processing and Management, 18(3):125?131.
R. Passonneau. 2004. Computing reliability for
coreference annotation. In Proc. of LREC-04, Lis-
bon, Portugal.
M. Poesio and R. Artstein. 2005. Annotating
(anaphoric) ambiguity. In Proc. of the Corpus
Linguistics Conference, Birmingham, UK.
A. Siddharthan and S. Teufel. 2007. Whose
idea was this? Deciding attribution in scien-
tific literature. In Proc. of the 6th Discourse
Anaphora and Anaphor Resolution Colloquium
(DAARC?07), Lagos, Portugal.
S. Teufel and M. Moens. 2002. Summarising sci-
entific articles ? experiments with relevance an
d rhetorical status. Computational Linguistics,
28(4):409?446.
S. Teufel, A. Siddharthan, and D. Tidhar. 2006. Au-
tomatic classification of citation function. In Proc.
of EMNLP-06, Sydney, Australia.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic corefer-
ence scoring scheme. In Proc. of the 6th Message
Understanding Conference, San Francisco.
I. Witten and E. Frank. 2000. Data Mining: Prac-
tical Machine Learning Tools and Techniques with
Java Implementations. Morgan Kaufmann.
323
Evaluation challenges in large-scale document summarization
Dragomir R. Radev
U. of Michigan
radev@umich.edu
Wai Lam
Chinese U. of Hong Kong
wlam@se.cuhk.edu.hk
Arda C?elebi
USC/ISI
ardax@isi.edu
Simone Teufel
U. of Cambridge
simone.teufel@cl.cam.ac.uk
John Blitzer
U. of Pennsylvania
blitzer@seas.upenn.edu
Danyu Liu
U. of Alabama
liudy@cis.uab.edu
Horacio Saggion
U. of Sheffield
h.saggion@dcs.shef.ac.uk
Hong Qi
U. of Michigan
hqi@umich.edu
Elliott Drabek
Johns Hopkins U.
edrabek@cs.jhu.edu
Abstract
We present a large-scale meta evaluation
of eight evaluation measures for both
single-document and multi-document
summarizers. To this end we built a
corpus consisting of (a) 100 Million auto-
matic summaries using six summarizers
and baselines at ten summary lengths in
both English and Chinese, (b) more than
10,000 manual abstracts and extracts, and
(c) 200 Million automatic document and
summary retrievals using 20 queries. We
present both qualitative and quantitative
results showing the strengths and draw-
backs of all evaluation methods and how
they rank the different summarizers.
1 Introduction
Automatic document summarization is a field that
has seen increasing attention from the NLP commu-
nity in recent years. In part, this is because sum-
marization incorporates many important aspects of
both natural language understanding and natural lan-
guage generation. In part it is because effective auto-
matic summarization would be useful in a variety of
areas. Unfortunately, evaluating automatic summa-
rization in a standard and inexpensive way is a diffi-
cult task (Mani et al, 2001). Traditional large-scale
evaluations are either too simplistic (using measures
like precision, recall, and percent agreement which
(1) don?t take chance agreement into account and (2)
don?t account for the fact that human judges don?t
agree which sentences should be in a summary) or
too expensive (an approach using manual judge-
ments can scale up to a few hundred summaries but
not to tens or hundreds of thousands).
In this paper, we present a comparison of six
summarizers as well as a meta-evaluation including
eight measures: Precision/Recall, Percent Agree-
ment, Kappa, Relative Utility, Relevance Correla-
tion, and three types of Content-Based measures
(cosine, longest common subsequence, and word
overlap). We found that while all measures tend
to rank summarizers in different orders, measures
like Kappa, Relative Utility, Relevance Correlation
and Content-Based each offer significant advantages
over the more simplistic methods.
2 Data, Annotation, and Experimental
Design
We performed our experiments on the Hong Kong
News corpus provided by the Hong Kong SAR of
the People?s Republic of China (LDC catalog num-
ber LDC2000T46). It contains 18,146 pairs of par-
allel documents in English and Chinese. The texts
are not typical news articles. The Hong Kong News-
paper mainly publishes announcements of the local
administration and descriptions of municipal events,
such as an anniversary of the fire department, or sea-
sonal festivals. We tokenized the corpus to iden-
tify headlines and sentence boundaries. For the En-
glish text, we used a lemmatizer for nouns and verbs.
We also segmented the Chinese documents using the
tool provided at http://www.mandarintools.com.
Several steps of the meta evaluation that we per-
formed involved human annotator support. First, we
Cluster 2 Meetings with foreign leaders
Cluster 46 Improving Employment Opportunities
Cluster 54 Illegal immigrants
Cluster 60 Customs staff doing good job.
Cluster 61 Permits for charitable fund raising
Cluster 62 Y2K readiness
Cluster 112 Autumn and sports carnivals
Cluster 125 Narcotics Rehabilitation
Cluster 199 Intellectual Property Rights
Cluster 241 Fire safety, building management concerns
Cluster 323 Battle against disc piracy
Cluster 398 Flu results in Health Controls
Cluster 447 Housing (Amendment) Bill Brings Assorted Improvements
Cluster 551 Natural disaster victims aided
Cluster 827 Health education for youngsters
Cluster 885 Customs combats contraband/dutiable cigarette operations
Cluster 883 Public health concerns cause food-business closings
Cluster 1014 Traffic Safety Enforcement
Cluster 1018 Flower shows
Cluster 1197 Museums: exhibits/hours
Figure 1: Twenty queries created by the LDC for
this experiment.
asked LDC to build a set of queries (Figure 1). Each
of these queries produced a cluster of relevant doc-
uments. Twenty of these clusters were used in the
experiments in this paper.
Additionally, we needed manual summaries or ex-
tracts for reference. The LDC annotators produced
summaries for each document in all clusters. In or-
der to produce human extracts, our judges also la-
beled sentences with ?relevance judgements?, which
indicate the relevance of sentence to the topic of the
document. The relevance judgements for sentences
range from 0 (irrelevant) to 10 (essential). As in
(Radev et al, 2000), in order to create an extract of
a certain length, we simply extract the top scoring
sentences that add up to that length.
For each target summary length, we produce an
extract using a summarizer or baseline. Then we
compare the output of the summarizer or baseline
with the extract produced from the human relevance
judgements. Both the summarizers and the evalua-
tion measures are described in greater detail in the
next two sections.
2.1 Summarizers and baselines
This section briefly describes the summarizers we
used in the evaluation. All summarizers take as input
a target length (n%) and a document (or cluster) split
into sentences. Their output is an n% extract of the
document (or cluster).
? MEAD (Radev et al, 2000): MEAD is
a centroid-based extractive summarizer that
scores sentences based on sentence-level and
inter-sentence features which indicate the qual-
ity of the sentence as a summary sentence. It
then chooses the top-ranked sentences for in-
clusion in the output summary. MEAD runs on
both English documents and on BIG5-encoded
Chinese. We tested the summarizer in both lan-
guages.
? WEBS (Websumm (Mani and Bloedorn,
2000)): can be used to produce generic and
query-based summaries. Websumm uses a
graph-connectivity model and operates under
the assumption that nodes which are connected
to many other nodes are likely to carry salient
information.
? SUMM (Summarist (Hovy and Lin, 1999)):
an extractive summarizer based on topic signa-
tures.
? ALGN (alignment-based): We ran a sentence
alignment algorithm (Gale and Church, 1993)
for each pair of English and Chinese stories.
We used it to automatically generate Chinese
?manual? extracts from the English manual ex-
tracts we received from LDC.
? LEAD (lead-based): n% sentences are chosen
from the beginning of the text.
? RAND (random): n% sentences are chosen at
random.
The six summarizers were run at ten different tar-
get lengths to produce more than 100 million sum-
maries (Figure 2). For the purpose of this paper, we
only focus on a small portion of the possible experi-
ments that our corpus can facilitate.
3 Summary Evaluation Techniques
We used three general types of evaluation measures:
co-selection, content-based similarity, and relevance
correlation. Co-selection measures include preci-
sion and recall of co-selected sentences, relative util-
ity (Radev et al, 2000), and Kappa (Siegel and
Castellan, 1988; Carletta, 1996). Co-selection meth-
ods have some restrictions: they only work for ex-
tractive summarizers. Two manual summaries of the
same input do not in general share many identical
sentences. We address this weakness of co-selection
Lengths #dj
05W 05S 10W 10S 20W 20S 30W 30S 40W 40S FD
E-FD - - - - - - - - - - x 40
E-LD X X X X x x X X X X - 440
E-RA X X X X x x X X X X - 440
E-MO x x X x x x X x X x - 540
E-M2 - - - - - X - - - - - 20
E-M3 - - - - - X - - - - - 8
E-S2 - - - - - X - - - - - 8
E-WS - X - X x x - X - X - 160
E-WQ - - - - - X - - - - - 10
E-LC - - - - - - x - - - - 40
E-CY - X - X - x - X - X - 120
E-AL X X X X X X X X X X - 200
E-AR X X X X X X X X X X - 200
E-AM X X X X X X X X X X - 200
C-FD - - - - - - - - - - x 40
C-LD X X X X x x X X X X - 240
C-RA X X X X x x X X X X - 240
C-MO X x X x x x X x X x - 320
C-M2 - - - - - X - - - - - 20
C-CY - X - X - x - X - X - 120
C-AL X X X X X X X X X X - 180
C-AR X X X X X X X X X X - 200
C-AM - X X X X X X X X - 120
X-FD - - - - - - - - - - x 40
X-LD X X X X x x X X X X - 240
X-RA X X X X x x X X X X - 240
X-MO X x X x x x X x X x - 320
X-M2 - - - - - X - - - - - 20
X-CY - X - X - x - X - X - 120
X-AL X X X X X X X X X X - 140
X-AR X X X X X X X X X X - 160
X-AM - X X X X X X X - X - 120
Figure 2: All runs performed (X = 20 clusters, x = 10 clusters). Language: E = English, C = Chinese,
X = cross-lingual; Summarizer: LD=LEAD, RA=RAND, WS=WEBS, WQ=WEBS-query based, etc.; S =
sentence-based, W = word-based; #dj = number of ?docjudges? (ranked lists of documents and summaries).
Target lengths above 50% are not shown in this table for lack of space. Each run is available using two
different retrieval schemes. We report results using the cross-lingual retrievals in a separate paper.
measures with several content-based similarity mea-
sures. The similarity measures we use are word
overlap, longest common subsequence, and cosine.
One advantage of similarity measures is that they
can compare manual and automatic extracts with
manual abstracts. To our knowledge, no system-
atic experiments about agreement on the task of
summary writing have been performed before. We
use similarity measures to measure interjudge agree-
ment among three judges per topic. We also ap-
ply the measures between human extracts and sum-
maries, which answers the question if human ex-
tracts are more similar to automatic extracts or to
human summaries.
The third group of evaluation measures includes
relevance correlation. It shows the relative perfor-
mance of a summary: how much the performance
of document retrieval decreases when indexing sum-
maries rather than full texts.
Task-based evaluations (e.g., SUMMAC (Mani
et al, 2001), DUC (Harman and Marcu, 2001), or
(Tombros et al, 1998) measure human performance
using the summaries for a certain task (after the
summaries are created). Although they can be a
very effective way of measuring summary quality,
task-based evaluations are prohibitively expensive at
large scales. In this project, we didn?t perform any
task-based evaluations as they would not be appro-
priate at the scale of millions of summaries.
3.1 Evaluation by sentence co-selection
For each document and target length we produce
three extracts from the three different judges, which
we label throughout as J1, J2, and J3.
We used the rates 5%, 10%, 20%, 30%, 40% for
most experiments. For some experiments, we also
consider summaries of 50%, 60%, 70%, 80% and
90% of the original length of the documents. Figure
3 shows some abbreviations for co-selection that we
will use throughout this section.
3.1.1 Precision and Recall
Precision and recall are defined as:
PJ2 (J1) =
A
A+ C
,RJ2 (J1) =
A
A+ B
J2
Sentence in
Extract
Sentence not
in Extract
Sentence in
Extract
A B A+ B
J1 Sentence not
in Extract
C D C +D
A+ C B +D N = A +
B+C+D
Figure 3: Contingency table comparing sentences
extracted by the system and the judges.
In our case, each set of documents which is com-
pared has the same number of sentences and also
the same number of sentences are extracted; thus
P = R.
The average precision Pavg(SY STEM) and re-
call Ravg(SY STEM) are calculated by summing
over individual judges and normalizing. The aver-
age interjudge precision and recall is computed by
averaging over all judge pairs.
However, precision and recall do not take chance
agreement into account. The amount of agreement
one would expect two judges to reach by chance de-
pends on the number and relative proportions of the
categories used by the coders. The next section on
Kappa shows that chance agreement is very high in
extractive summarization.
3.1.2 Kappa
Kappa (Siegel and Castellan, 1988) is an evalua-
tion measure which is increasingly used in NLP an-
notation work (Krippendorff, 1980; Carletta, 1996).
Kappa has the following advantages over P and R:
? It factors out random agreement. Random
agreement is defined as the level of agreement
which would be reached by random annotation
using the same distribution of categories as the
real annotators.
? It allows for comparisons between arbitrary
numbers of annotators and items.
? It treats less frequent categories as more im-
portant (in our case: selected sentences), simi-
larly to precision and recall but it also consid-
ers (with a smaller weight) more frequent cate-
gories as well.
The Kappa coefficient controls agreement P (A)
by taking into account agreement by chance P (E) :
K =
P (A)? P (E)
1? P (E)
No matter how many items or annotators, or how
the categories are distributed, K = 0 when there is
no agreement other than what would be expected by
chance, and K = 1 when agreement is perfect. If
two annotators agree less than expected by chance,
Kappa can also be negative.
We report Kappa between three annotators in the
case of human agreement, and between three hu-
mans and a system (i.e. four judges) in the next sec-
tion.
3.1.3 Relative Utility
Relative Utility (RU) (Radev et al, 2000) is tested
on a large corpus for the first time in this project.
RU takes into account chance agreement as a lower
bound and interjudge agreement as an upper bound
of performance. RU allows judges and summarizers
to pick different sentences with similar content in
their summaries without penalizing them for doing
so. Each judge is asked to indicate the importance
of each sentence in a cluster on a scale from 0 to
10. Judges also specify which sentences subsume or
paraphrase each other. In relative utility, the score
of an automatic summary increases with the impor-
tance of the sentences that it includes but goes down
with the inclusion of redundant sentences.
3.2 Content-based Similarity measures
Content-based similarity measures compute the sim-
ilarity between two summaries at a more fine-
grained level than just sentences. For each automatic
extract S and similarity measure M we compute the
following number:
sim(M,S, {J1, J2, J3}) =
M(S, J1) +M(S, J2) +M(S, J3)
3
We used several content-based similarity mea-
sures that take into account different properties of
the text:
Cosine similarity is computed using the follow-
ing formula (Salton, 1988):
cos(X,Y ) =
?
xi ? yi
??
(xi)2 ?
??
(yi)2
where X and Y are text representations based on
the vector space model.
Longest Common Subsequence is computed as
follows:
lcs(X,Y ) = (length(X) + length(Y )? d(X,Y ))/2
where X and Y are representations based on
sequences and where lcs(X,Y ) is the length of
the longest common subsequence between X and
Y , length(X) is the length of the string X , and
d(X,Y ) is the minimum number of deletion and in-
sertions needed to transform X into Y (Crochemore
and Rytter, 1994).
3.3 Relevance Correlation
Relevance correlation (RC) is a new measure for as-
sessing the relative decrease in retrieval performance
when indexing summaries instead of full documents.
The idea behind it is similar to (Sparck-Jones and
Sakai, 2001). In that experiment, Sparck-Jones and
Sakai determine that short summaries are good sub-
stitutes for full documents at the high precision end.
With RC we attempt to rank all documents given a
query.
Suppose that given a queryQ and a corpus of doc-
uments Di, a search engine ranks all documents in
Di according to their relevance to the query Q. If
instead of the corpus Di, the respective summaries
of all documents are substituted for the full docu-
ments and the resulting corpus of summaries Si is
ranked by the same retrieval engine for relevance to
the query, a different ranking will be obtained. If
the summaries are good surrogates for the full docu-
ments, then it can be expected that rankings will be
similar.
There exist several methods for measuring the
similarity of rankings. One such method is Kendall?s
tau and another is Spearman?s rank correlation. Both
methods are quite appropriate for the task that we
want to perform; however, since search engines pro-
duce relevance scores in addition to rankings, we
can use a stronger similarity test, linear correlation
between retrieval scores. When two identical rank-
ings are compared, their correlation is 1. Two com-
pletely independent rankings result in a score of 0
while two rankings that are reverse versions of one
another have a score of -1. Although rank correla-
tion seems to be another valid measure, given the
large number of irrelevant documents per query re-
sulting in a large number of tied ranks, we opted for
linear correlation. Interestingly enough, linear cor-
relation and rank correlation agreed with each other.
Relevance correlation r is defined as the linear
correlation of the relevance scores (x and y) as-
signed by two different IR algorithms on the same
set of documents or by the same IR algorithm on
different data sets:
r =
?
i
(xi ? x)(yi ? y)
??
i
(xi ? x)2
??
i
(yi ? y)2
Here x and y are the means of the relevance scores
for the document sequence.
We preprocess the documents and use Smart to
index and retrieve them. After the retrieval process,
each summary is associated with a score indicating
the relevance of the summary to the query. The
relevance score is actually calculated as the inner
product of the summary vector and the query vec-
tor. Based on the relevance score, we can produce a
full ranking of all the summaries in the corpus.
In contrast to (Brandow et al, 1995) who run 12
Boolean queries on a corpus of 21,000 documents
and compare three types of documents (full docu-
ments, lead extracts, and ANES extracts), we mea-
sure retrieval performance under more than 300 con-
ditions (by language, summary length, retrieval pol-
icy for 8 summarizers or baselines).
4 Results
This section reports results for the summarizers and
baselines described above. We relied directly on the
relevance judgements to create ?manual extracts? to
use as gold standards for evaluating the English sys-
tems. To evaluate Chinese, we made use of a ta-
ble of automatically produced alignments. While
the accuracy of the alignments is quite high, we
have not thoroughly measured the errors produced
when mapping target English summaries into Chi-
nese. This will be done in future work.
4.1 Co-selection results
Co-selection agreement (Section 3.1) is reported in
Figures 4, and 5). The tables assume human perfor-
mance is the upper bound, the next rows compare
the different summarizers.
Figure 4 shows results for precision and recall.
We observe the effect of a dependence of the nu-
merical results on the length of the summary, which
is a well-known fact from information retrieval eval-
uations.
Websumm has an advantage over MEAD for
longer summaries but not for 20% or less. Lead
summaries perform better than all the automatic
summarizers, and better than the human judges.
This result usually occurs when the judges choose
different, but early sentences. Human judgements
overtake the lead baseline for summaries of length
50% or more.
5% 10% 20% 30% 40%
Humans .187 .246 .379 .467 .579
MEAD .160 .231 .351 .420 .519
WEBS .310 .305 .358 .439 .543
LEAD .354 .387 .447 .483 .583
RAND .094 .113 .224 .357 .432
Figure 4: Results in precision=recall (averaged over
20 clusters).
Figure 5 shows results using Kappa. Random
agreement is 0 by definition between a random pro-
cess and a non-random process.
While the results are overall rather low, the num-
bers still show the following trends:
? MEAD outperforms Websumm for all but the
5% target length.
? Lead summaries perform best below 20%,
whereas human agreement is higher after that.
? There is a rather large difference between the
two summarizers and the humans (except for
the 5% case for Websumm). This numerical
difference is relatively higher than for any other
co-selection measure treated here.
? Random is overall the worst performer.
? Agreement improves with summary length.
Figures 6 and 7 summarize the results obtained
through Relative Utility. As the figures indicate,
random performance is quite high although all non-
random methods outperform it significantly. Fur-
ther, and in contrast with other co-selection evalua-
tion criteria, in both the single- and multi-document
5% 10% 20% 30% 40%
Humans .127 .157 .194 .225 .274
MEAD .109 .136 .168 .192 .230
WEBS .138 .128 .146 .159 .192
LEAD .180 .198 .213 .220 .261
RAND .064 .081 .097 .116 .137
Figure 5: Results in kappa, averaged over 20 clus-
ters.
case MEAD outperforms LEAD for shorter sum-
maries (5-30%). The lower bound (R) represents the
average performance of all extracts at the given sum-
mary length while the upper bound (J) is the inter-
judge agreement among the three judges.
5% 10% 20% 30% 40%
R 0.66 0.68 0.71 0.74 0.76
RAND 0.67 0.67 0.71 0.75 0.77
WEBS 0.72 0.73 0.76 0.79 0.82
LEAD 0.72 0.73 0.77 0.80 0.83
MEAD 0.78 0.79 0.79 0.81 0.83
J 0.80 0.81 0.83 0.85 0.87
Figure 6: RU per summarizer and summary length
(Single-document).
5% 10% 20% 30% 40%
R 0.64 0.66 0.69 0.72 0.74
RAND 0.63 0.65 0.71 0.72 0.74
LEAD 0.71 0.71 0.76 0.79 0.82
MEAD 0.73 0.75 0.78 0.79 0.81
J 0.76 0.78 0.81 0.83 0.85
Figure 7: RU per summarizer and summary length
(Multi-document).
4.2 Content-based results
The results obtained for a subset of target lengths
using content-based evaluation can be seen in Fig-
ures 8 and 9. In all our experiments with tf ? idf -
weighted cosine, the lead-based summarizer ob-
tained results close to the judges in most of the target
lengths while MEAD is ranked in second position.
In all our experiments using longest common sub-
sequence, no system obtained better results in the
majority of the cases.
10% 20% 30% 40%
LEAD 0.55 0.65 0.70 0.79
MEAD 0.46 0.61 0.70 0.78
RAND 0.31 0.47 0.60 0.69
WEBS 0.52 0.60 0.68 0.77
Figure 8: Cosine (tf?idf ). Average over 10 clusters.
10% 20% 30% 40%
LEAD 0.47 0.55 0.60 0.70
MEAD 0.37 0.52 0.61 0.70
RAND 0.25 0.38 0.50 0.58
WEBS 0.39 0.45 0.53 0.64
Figure 9: Longest Common Subsequence. Average
over 10 clusters.
The numbers obtained in the evaluation of Chi-
nese summaries for cosine and longest common sub-
sequence can be seen in Figures 10 and 11. Both
measures identify MEAD as the summarizer that
produced results closer to the ideal summaries (these
results also were observed across measures and text
representations).
10% 20% 30% 40%
SUMM 0.44 0.65 0.71 0.78
LEAD 0.54 0.63 0.68 0.77
MEAD 0.49 0.65 0.74 0.82
RAND 0.31 0.50 0.65 0.71
Figure 10: Chinese Summaries. Cosine (tf ? idf ).
Average over 10 clusters. Vector space of Words as
Text Representation.
10% 20% 30% 40%
SUMM 0.32 0.53 0.57 0.65
LEAD 0.42 0.49 0.54 0.64
MEAD 0.35 0.50 0.60 0.70
RAND 0.21 0.35 0.49 0.54
Figure 11: Chinese Summaries. Longest Common
Subsequence. Average over 10 clusters. Chinese
Words as Text Representation.
We have based this evaluation on target sum-
maries produced by LDC assessors, although other
alternatives exist. Content-based similarity mea-
sures do not require the target summary to be a sub-
set of sentences from the source document, thus,
content evaluation based on similarity measures
can be done using summaries published with the
source documents which are in many cases available
(Teufel and Moens, 1997; Saggion, 2000).
4.3 Relevance Correlation results
We present several results using Relevance Correla-
tion. Figures 12 and 13 show how RC changes de-
pending on the summarizer and the language used.
RC is as high as 1.0 when full documents (FD) are
compared to themselves. One can notice that even
random extracts get a relatively high RC score. It is
also worth observing that Chinese summaries score
lower than their corresponding English summaries.
Figure 14 shows the effects of summary length and
summarizers on RC. As one might expect, longer
summaries carry more of the content of the full doc-
ument than shorter ones. At the same time, the rel-
ative performance of the different summarizers re-
mains the same across compression rates.
C112 C125 C241 C323 C551 AVG10
FD 1.00 1.00 1.00 1.00 1.00 1.000
MEAD 0.91 0.92 0.93 0.92 0.90 0.903
WEBS 0.88 0.82 0.89 0.91 0.88 0.843
LEAD 0.80 0.80 0.84 0.85 0.81 0.802
RAND 0.80 0.78 0.87 0.85 0.79 0.800
SUMM 0.77 0.79 0.85 0.88 0.81 0.775
Figure 12: RC per summarizer (English 20%).
C112 C125 C241 C323 C551 AVG10
FD 1.00 1.00 1.00 1.00 1.00 1.000
MEAD 0.78 0.87 0.93 0.66 0.91 0.850
SUMM 0.76 0.75 0.85 0.84 0.75 0.755
RAND 0.71 0.75 0.85 0.60 0.74 0.744
ALGN 0.74 0.72 0.83 0.95 0.72 0.738
LEAD 0.72 0.71 0.83 0.58 0.75 0.733
Figure 13: RC per summarizer (Chinese, 20%).
5% 10% 20% 30% 40%
FD 1.000 1.000 1.000 1.000 1.000
MEAD 0.724 0.834 0.916 0.946 0.962
WEBS 0.730 0.804 0.876 0.912 0.936
LEAD 0.660 0.730 0.820 0.880 0.906
SUMM 0.622 0.710 0.820 0.848 0.862
RAND 0.554 0.708 0.818 0.884 0.922
Figure 14: RC per summary length and summarizer.
5 Conclusion
This paper describes several contributions to text
summarization:
First, we observed that different measures rank
summaries differently, although most of them
showed that ?intelligent? summarizers outperform
lead-based summaries which is encouraging given
that previous results had cast doubt on the ability of
summarizers to do better than simple baselines.
Second, we found that measures like Kappa, Rel-
ative Utility, Relevance Correlation and Content-
Based, each offer significant advantages over more
simplistic methods like Precision, Recall, and Per-
cent Agreement with respect to scalability, applica-
bility to multidocument summaries, and ability to
include human and chance agreement. Figure 15
Property Prec, recall Kappa Normalized RU Word overlap, cosine, LCS Relevance Correlation
Intrinsic (I)/extrinsic (E) I I I I E
Agreement between human extracts X X X X X
Agreement human extracts and automatic extracts X X X X X
Agreement human abstracts and human extracts X
Non-binary decisions X X
Takes random agreement into account by design X X
Full documents vs. extracts X X
Systems with different sentence segmentation X X
Multidocument extracts X X X X
Full corpus coverage X X
Figure 15: Properties of evaluation measures used in this project.
presents a short comparison of all these evaluation
measures.
Third, we performed extensive experiments using
a new evaluation measure, Relevance Correlation,
which measures how well a summary can be used
to replace a document for retrieval purposes.
Finally, we have packaged the code used for this
project into a summarization evaluation toolkit and
produced what we believe is the largest and most
complete annotated corpus for further research in
text summarization. The corpus and related software
is slated for release by the LDC in mid 2003.
References
Ron Brandow, Karl Mitze, and Lisa F. Rau. 1995. Auto-
matic Condensation of Electronic Publications by Sen-
tence Selection. Information Processing and Manage-
ment, 31(5):675?685.
Jean Carletta. 1996. Assessing Agreement on Classifica-
tion Tasks: The Kappa Statistic. CL, 22(2):249?254.
Maxime Crochemore and Wojciech Rytter. 1994. Text
Algorithms. Oxford University Press.
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics, 19(1):75?102.
Donna Harman and Daniel Marcu, editors. 2001. Pro-
ceedings of the 1st Document Understanding Confer-
ence. New Orleans, LA, September.
Eduard Hovy and Chin Yew Lin. 1999. Automated Text
Summarization in SUMMARIST. In Inderjeet Mani
and Mark T. Maybury, editors, Advances in Automatic
Text Summarization, pages 81?94. The MIT Press.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to its Methodology. Sage Publications, Bev-
erly Hills, CA.
Inderjeet Mani and Eric Bloedorn. 2000. Summariz-
ing Similarities and Differences Among Related Doc-
uments. Information Retrieval, 1(1).
Inderjeet Mani, The?re`se Firmin, David House, Gary
Klein, Beth Sundheim, and Lynette Hirschman. 2001.
The TIPSTER SUMMAC Text Summarization Evalu-
ation. In Natural Language Engineering.
Dragomir R. Radev, Hongyan Jing, and Malgorzata
Budzikowska. 2000. Centroid-Based Summarization
of Multiple Documents: Sentence Extraction, Utility-
Based Evaluation, and User Studies. In Proceedings
of the Workshop on Automatic Summarization at the
6th Applied Natural Language Processing Conference
and the 1st Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Seattle, WA, April.
Horacio Saggion. 2000. Ge?ne?ration automatique
de re?sume?s par analyse se?lective. Ph.D. the-
sis, De?partement d?informatique et de recherche
ope?rationnelle. Faculte? des arts et des sciences. Uni-
versite? de Montre?al, August.
Gerard Salton. 1988. Automatic Text Processing.
Addison-Wesley Publishing Company.
Sidney Siegel and N. John Jr. Castellan. 1988. Non-
parametric Statistics for the Behavioral Sciences.
McGraw-Hill, Berkeley, CA, 2nd edition.
Karen Sparck-Jones and Tetsuya Sakai. 2001. Generic
Summaries for Indexing in IR. In Proceedings of the
24th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 190?198, New Orleans, LA, September.
Simone Teufel and Marc Moens. 1997. Sentence Ex-
traction as a Classification Task. In Proceedings of the
Workshop on Intelligent Scalable Text Summarization
at the 35th Meeting of the Association for Computa-
tional Linguistics, and the 8th Conference of the Eu-
ropean Chapter of the Assocation for Computational
Linguistics, Madrid, Spain.
Anastasios Tombros, Mark Sanderson, and Phil Gray.
1998. Advantages of Query Biased Summaries in In-
formation Retrieval. In Eduard Hovy and Dragomir R.
Radev, editors, Proceedings of the AAAI Symposium
on Intelligent Text Summarization, pages 34?43, Stan-
ford, California, USA, March 23?25,. The AAAI
Press.
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 921?928,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Bootstrapping Approach to Unsupervised Detection of Cue Phrase
Variants
Rashid M. Abdalla and Simone Teufel
Computer Laboratory, University of Cambridge
15 JJ Thomson Avenue, Cambridge CB3 OFD, UK
rma33@cam.ac.uk, sht25@cam.ac.uk
Abstract
We investigate the unsupervised detection
of semi-fixed cue phrases such as ?This
paper proposes a novel approach. . . 1?
from unseen text, on the basis of only a
handful of seed cue phrases with the de-
sired semantics. The problem, in contrast
to bootstrapping approaches for Question
Answering and Information Extraction, is
that it is hard to find a constraining context
for occurrences of semi-fixed cue phrases.
Our method uses components of the cue
phrase itself, rather than external con-
text, to bootstrap. It successfully excludes
phrases which are different from the tar-
get semantics, but which look superficially
similar. The method achieves 88% ac-
curacy, outperforming standard bootstrap-
ping approaches.
1 Introduction
Cue phrases such as ?This paper proposes a novel
approach to. . . ?, ?no method for . . . exists? or even
?you will hear from my lawyer? are semi-fixed in
that they constitute a formulaic pattern with a clear
semantics, but with syntactic and lexical variations
which are hard to predict and thus hard to detect
in unseen text (e.g. ?a new algorithm for . . . is
suggested in the current paper? or ?I envisage le-
gal action?). In scientific discourse, such meta-
discourse (Myers, 1992; Hyland, 1998) abounds
and plays an important role in marking the dis-
course structure of the texts.
Finding these variants can be useful for many
text understanding tasks because semi-fixed cue
phrases act as linguistic markers indicating the im-
portance and/or the rhetorical role of some ad-
jacent text. For the summarisation of scientific
1In contrast to standard work in discourse linguistics,which mostly considers sentence connectives and adverbialsas cue phrases, our definition includes longer phrases, some-times even entire sentences.
papers, cue phrases such as ?Our paper deals
with. . . ? are commonly used as indicators of
extraction-worthiness of sentences (Kupiec et al,
1995). Re-generative (rather than extractive) sum-
marisation methods may want to go further than
that and directly use the knowledge that a certain
sentence contains the particular research aim of a
paper, or a claimed gap in the literature. Similarly,
in the task of automatic routing of customer emails
and automatic answering of some of these, the de-
tection of threats of legal action could be useful.
However, systems that use cue phrases usually
rely on manually compiled lists, the acquisition
of which is time-consuming and error-prone and
results in cue phrases which are genre-specific.
Methods for finding cue phrases automatically in-
clude Hovy and Lin (1998) (using the ratio of
word frequency counts in summaries and their cor-
responding texts), Teufel (1998) (using the most
frequent n-grams), and Paice (1981) (using a pat-
tern matching grammar and a lexicon of manu-
ally collected equivalence classes). The main is-
sue with string-based pattern matching techniques
is that they cannot capture syntactic generalisa-
tions such as active/passive constructions, differ-
ent tenses and modification by adverbial, adjecti-
val or prepositional phrases, appositions and other
parenthetical material.
For instance, we may be looking for sentences
expressing the goal or main contribution of a pa-
per; Fig. 1 shows candidates of such sentences.
Cases a)?e), which do indeed describe the authors?
goal, display a wide range of syntactic variation.
a) In this paper, we introduce a method for similarity-based estimation of . . .b) We introduce and justify a method. . .c) A method (described in section 1) is introducedd) The method introduced here is a variation. . .e) We wanted to introduce a method. . .
f) We do not introduce a method. . .g) We introduce and adopt the method given in [1]. . .h) Previously we introduced a similar method. . .i) They introduce a similar method. . .
Figure 1: Goal statements and syntactic variation ? cor-rect matches (a-e) and incorrect matches (f-i)
921
Cases f)?i) in contrast are false matches: they do
not express the authors? goals, although they are
superficially similar to the correct contexts. While
string-based approaches (Paice, 1981; Teufel,
1998) are too restrictive to cover the wide varia-
tion within the correct contexts, bag-of-words ap-
proaches such as Agichtein and Gravano?s (2000)
are too permissive and would miss many of the
distinctions between correct and incorrect con-
texts.
Lisacek et al (2005) address the task of iden-
tifying ?paradigm shift? sentences in the biomed-
ical literature, i.e. statements of thwarted expec-
tation. This task is somewhat similar to ours in
its definition by rhetorical context. Their method
goes beyond string-based matching: In order for a
sentence to qualify, the right set of concepts must
be present in a sentence, with any syntactic re-
lationship holding between them. Each concept
set is encoded as a fixed, manually compiled lists
of strings. Their method covers only one particu-
lar context (the paradigm shift one), whereas we
are looking for a method where many types of cue
phrases can be acquired. Whereas it relies on man-
ually assembled lists, we advocate data-driven ac-
quisition of new contexts. This is generally pre-
ferrable to manual definition, as language use is
changing, inventive and hard to predict and as
many of the relevant concepts in a domain may be
infrequent (cf. the formulation ?be cursed?, which
was used in our corpus as a way of describing a
method?s problems). It also allows the acquisition
of cue phrases in new domains, where the exact
prevalent meta-discourse might not be known.
Riloff?s (1993) method for learning information
extraction (IE) patterns uses a syntactic parse and
correspondences between the text and filled MUC-
style templates to learn context in terms of lexico-
semantic patterns. However, it too requires sub-
stantial hand-crafted knowledge: 1500 filled tem-
plates as training material, and a lexicon of se-
mantic features for roughly 3000 nouns for con-
straint checking. Unsupervised methods for simi-
lar tasks include Agichtein and Gravano?s (2000)
work, which shows that clusters of vector-space-
based patterns can be successfully employed to
detect specific IE relationships (companies and
their headquarters), and Ravichandran and Hovy?s
(2002) algorithm for finding patterns for a Ques-
tion Answering (QA) task. Based on training ma-
terial in the shape of pairs of question and answer
terms ? e.g., (e.g. {Mozart, 1756}), they learn the
a) In this paper, we introduce a method for similarity-based estimation of . . .b) Here, we present a similarity-based approach for esti-mation of. . .c) In this paper, we propose an algorithm which is . . .d) We will here dene a technique for similarity-based. . .
Figure 2: Context around cue phrases (lexical variants)
semantics holding between these terms (?birth
year?) via frequent string patterns occurring in the
context, such as ?A was born in B?, by consider-
ing n-grams of all repeated substrings. What is
common to these three works is that bootstrapping
relies on constraints between the context external
to the extracted material and the extracted mate-
rial itself, and that the target extraction material is
defined by real-world relations.
Our task differs in that the cue phrases we ex-
tract are based on general rhetorical relations hold-
ing in all scientific discourse. Our approach for
finding semantically similar variants in an unsu-
pervised fashion relies on bootstrapping of seeds
from within the cue phrase. The assumption is that
every semi-fixed cue phrase contains at least two
main concepts whose syntax and semantics mutu-
ally constrain each other (e.g. verb and direct ob-
ject in phrases such as ?(we) present an approach
for?). The expanded cue phrases are recognised
in various syntactic contexts using a parser2. Gen-
eral semantic constraints valid for groups of se-
mantically similar cue phrases are then applied to
model, e.g., the fact that it must be the authors who
present the method, not somebody else.
We demonstrate that such an approach is more
appropriate for our task than IE/QA bootstrapping
mechanisms based on cue phrase-external con-
text. Part of the reason for why normal boot-
strapping does not work for our phrases is the dif-
ficulty of finding negatives contexts, essential in
bootstrapping to evaluate the quality of the pat-
terns automatically. IE and QA approaches, due
to uniqueness assumptions of the real-world rela-
tions that these methods search for, have an auto-
matic definition of negative contexts by hard con-
straints (i.e., all contexts involving Mozart and any
other year are by definition of the wrong seman-
tics; so are all contexts involving Microsoft and
a city other than Redmond). As our task is not
grounded in real-world relations but in rhetorical
ones, constraints found in the context tend to be
2Thus, our task shows some parallels to work in para-phrasing (Barzilay and Lee, 2002) and syntactic variant gen-eration (Jacquemin et al, 1997), but the methods are verydifferent.
922
soft rather than hard (cf. Fig 2): while it it possible
that strings such as ?we? and ?in this paper? occur
more often in the context of a given cue phrase,
they also occur in many other places in the paper
where the cue phrase is not present. Thus, it is
hard to define clear negative contexts for our task.
The novelty of our work is thus the new pattern
extraction task (finding variants of semi-fixed cue
phrases), a task for which it is hard to directly use
the context the patterns appear in, and an iterative
unsupervised bootstrapping algorithm for lexical
variants, using phrase-internal seeds and ranking
similar candidates based on relation strength be-
tween the seeds.
While our method is applicable to general cue
phrases, we demonstrate it here with transitive
verb?direct object pairs, namely a) cue phrases in-
troducing a new methodology (and thus the main
research goal of the scientific article; e.g. ?In
this paper, we propose a novel algorithm. . . ?) ?
we call those goal-type cue phrases; and b) cue
phrases indicating continuation of previous other
research (e.g. ?Therefore, we adopt the approach
presented in [1]. . . ?) ? continuation-type cue
phrases.
2 Lexical Bootstrapping Algorithm
The task of this module is to find lexical vari-
ants of the components of the seed cue phrases.
Given the seed phrases ?we introduce a method?
and ?we propose a model?, the algorithm starts
by finding all direct objects of ?introduce? in a
given corpus and, using an appropriate similar-
ity measure, ranks them according to their dis-
tributional similarity to the nouns ?method? and
?model?. Subsequently, the noun ?method? is used
to find transitive verbs and rank them according to
their similarity to ?introduce? and ?propose?. In
both cases, the ranking step retains variants that
preserve the semantics of the cue phrase (e.g. ?de-
velop? and ?approach?) and filters irrelevant terms
that change the phrase semantics (e.g. ?need? and
?example?).
Stopping at this point would limit us to those
terms that co-occur with the seed words in the
training corpus. Therefore additional iterations us-
ing automatically generated verbs and nouns are
applied in order to recover more and more vari-
ants. The full algorithm is given in Fig. 3.
The algorithm requires corpus data for the steps
Hypothesize (producing a list of potential candi-
dates) and Rank (testing them for similarity). We
Input: Tuples {A1, A2, . . . , Am} and {B1, B2, . . . , Bn}.Initialisation: Set the concept-A reference set to
{A1, A2, . . . , Am} and the concept-B reference set to
{B1, B2, . . . , Bn}. Set the concept-A active element to A1and the concept-B active element to B1.Recursion:1. Concept B retrieval:(i) Hypothesize: Find terms in the corpus whichare in the desired relationship with the concept-Aactive element (e.g. direct objects of a verb activeelement). This results in the concept-B candidateset.(ii) Rank: Rank the concept-B candidate set usinga suitable ranking methodology that may make useof the concept-B reference set. In this process, eachmember of the candidate set is assigned a score.(iii) Accumulate: Add the top s items of theconcept-B candidate set to the concept-B accumu-lator list (based on empirical results, s is the rank ofthe candidate set during the initial iteration and 50for the remaining iterations). If an item is alreadyon the accumulator list, add its ranking score to theexisting item?s score.2. Concept A retrieval: as above, with concepts A andB swapped.3. Updating active elements:(i) Set the concept-B active element to the highestranked instance in the concept-B accumulator listwhich has not been used as an active element be-fore.(ii) Set the concept-A active element to the highestranked instance in the concept-A accumulator listwhich has not been used as an active element be-fore.Repeat steps 1-3 for k iterationsOutput: top M words of concept-A (verb) accumulator listand top N words of concept-B (noun) accumulator listReference set: a set of seed words which define the col-lective semantics of the concept we are looking for in thisiterationActive element: the instance of the concept used in the cur-rent iteration for retrieving instances of the other concept.If we are finding lexical variants of Concept A by exploit-ing relationships between Concepts A and B, then the activeelement is from Concept B.Candidate set: the set of candidate terms for one concept(eg. Concept A) obtained using an active element from theother concept (eg. Concept B). The more semantically simi-lar a term in the candidate set is to the members of the refer-ence set, the higher its ranking should be. This set containsverbs if the active element is a noun and vice versa.Accumulator list: a sorted list that accumulates the rankedmembers of the candidate set.
Figure 3: Lexical variant bootstrapping algorithm
estimate frequencies for the Rank step from the
written portion of the British National Corpus
(BNC, Burnard (1995)), 90 Million words. For
the Hypothesize step, we experiment with two
data sets: First, the scientific subsection of the
BNC (24 Million words), which we parse using
RASP (Briscoe and Carroll, 2002); we then ex-
amine the grammatical relations (GRs) for transi-
tive verb constructions, both in active and passive
voice. This method guarantees that we find al-
most all transitive verb constructions cleanly; Car-
roll et al (1999) report an accuracy of .85 for
923
DOs, Active: "AGENT STRING AUX active-verb-element DETERMINER * POSTMOD"DOs, Passive: "DETERMINER * AUX active-verb-element element"TVs, Active: "AGENT STRING AUX * DETERMINER active-noun- element POSTMOD"TVs, Passive:"DET active-noun-element AUX * POSTMOD"
Figure 4: Query patterns for retrieving direct objects (DOs) and transitive verbs (TVs) in the Hypothesize step.
newspaper articles for this relation. Second, in
order to obtain larger coverage and more current
data we also experiment with Google Scholar3, an
automatic web-based indexer of scientific litera-
ture (mainly peer-reviewed papers, technical re-
ports, books, pre-prints and abstracts). Google
Scholar snippets are often incomplete fragments
which cannot be parsed. For practical reasons, we
decided against processing the entire documents,
and obtain an approximation to direct objects and
transitive verbs with regular expressions over the
result snippets in both active and passive voice
(cf. Fig. 4), designed to be high-precision4 . The
amount of data available from BNC and Google
Scholar is not directly comparable: harvesting
Google Scholar snippets for both active and pas-
sive constructions gives around 2000 sentences per
seed (Google Scholar returns up to 1000 results
per query), while the number of BNC sentences
containing seed words in active and passive form
varies from 11 (?formalism?) to 5467 (?develop?)
with an average of 1361 sentences for the experi-
mental seed pairs.
Ranking
Having obtained our candidate sets (either from
the scientific subsection of the BNC or from
Google Scholar), the members are ranked using
BNC frequencies. We investigate two ranking
methodologies: frequency-based and context-
based. Frequency-based ranking simply ranks
each member of the candidate set by how many
times it is retrieved together with the current active
element. Context-based ranking uses a similarity
measure for computing the scores, giving a higher
score to those words that share sufficiently similar
contexts with the members of the reference set.
We consider similarity measures in a vector space
defined either by a fixed window, by the sentence
window, or by syntactic relationships. The score
assigned to each word in the candidate set is the
sum of its semantic similarity values computed
with respect to each member in the reference set.
3http://scholar.google.com
4The capitalised words in these patterns are replaced byactual words (e.g. AGENT STRING: We/I, DETERMINER:a/ an/our), and the extracted words (indicated by ?*?) are lem-matised.
Syntactic contexts, as opposed to window-based
contexts, constrain the context of a word to only
those words that are grammatically related to
it. We use verb-object relations in both active
and passive voice constructions as did Pereira et
al. (1993) and Lee (1999), among others. We
use the cosine similarity measure for window-
based contexts and the following commonly
used similarity measures for the syntactic vector
space: Hindle?s (1990) measure, the weighted Lin
measure (Wu and Zhou, 2003), the ?-Skew diver-
gence measure (Lee, 1999), the Jensen-Shannon
(JS) divergence measure (Lin, 1991), Jaccard?s
coefficient (van Rijsbergen, 1979) and the Con-
fusion probability (Essen and Steinbiss, 1992).
The Jensen-Shannon measure JS (x1, x2) =
?
y?Y
?
x?{x1,x2}
(
P (y|x) log
(
P(y|x)
1
2 (P(y|x1)+P(y|x2))
))
subsequently performed best for our task. We
compare the different ranking methodologies
and data sets with respect to a manually-defined
gold standard list of 20 goal-type verbs and 20
nouns. This list was manually assembled from
Teufel (1999); WordNet synonyms and other
plausible verbs and nouns found via Web searches
on scientific articles were added. We ensured by
searches on the ACL anthology that there is good
evidence that the gold-standard words indeed
occur in the right contexts, i.e. in goal statement
sentences. As we want to find similarity metrics
and data sources which result in accumulator lists
with many of these gold members at high ranks,
we need a measure that rewards exactly those
lists. We use non-interpolated Mean Average
Precision (MAP), a standard measure for eval-
uating ranked information retrieval runs, which
combines precision and recall and ranges from 0
to 15.
We use 8 pairs of 2-tuples as input (e.g. [in-
troduce, study] & [approach, method]), randomly
selected from the gold standard list. MAP was cal-
5MAP = 1N
?N
j=1 APj =
1
N
?N
j=1
1
M
?M
i=1 P (gi)where P (gi) = nijrij if gi is retrieved and 0 otherwise, N isthe number of seed combinations, M is the size of the goldenlist, gi is the ith member of the golden list and rij is its rankin the retrieved list of combination j while nij is the numberof golden members found up to and including rank rij .
924
Ranking scheme BNC Google Scholar
Frequency-based 0.123 0.446Sentence-window 0.200 0.344Fixedsize-window 0.184 0.342Hindle 0.293 0.416Weighted Lin 0.358 0.509
?-Skew 0.361 0.486Jensen-Shannon 0.404 0.550Jaccard?s coef. 0.301 0.436Confusion prob. 0.171 0.293
Figure 5: MAPs after the first iteration
culated over the verbs and nouns retrieved using
our algorithm and averaged. Fig. 5 summarises the
MAP scores for the first iteration, where Google
Scholar significantly outperformed the BNC. The
best result for this iteration (MAP=.550) was
achieved by combining Google Scholar and the
Jensen-Shannon measure. The algorithm stops to
iterate when no more improvement can be ob-
tained, in this case after 4 iterations, resulting in
a final MAP of .619.
Although ?-Skew outperforms the simpler mea-
sures in ranking nouns, its performance on verbs
is worse than the performance of Weighted Lin.
While Lee (1999) argues that ?-Skew?s asymme-
try can be advantageous for nouns, this probably
does not hold for verbs: verb hierarchies have
much shallower structure than noun hierarchies
with most verbs concentrated on one level (Miller
et al, 1990). This would explain why JS, which
is symmetric compared to the ?-Skew metric, per-
formed better in our experiments.
In the evaluation presented here we therefore
use Google Scholar data and the JS measure. An
additional improvement (MAP=.630) is achieved
when we incorporate a filter based on the follow-
ing hypothesis: goal-type verbs should be more
likely to have their direct objects preceded by in-
definite articles rather than definite articles or pos-
sessive determiners (because a new method is in-
troduced) whereas continuation-type verbs should
prefer definite articles with their direct objects (as
an existing method is involved).
3 Syntactic variants and semantic filters
The syntactic variant extractor takes as its input
the raw text and the lists of verbs and nouns gen-
erated by the lexical bootstrapper. After RASP-
parsing the input text, all instances of the input
verbs are located and, based on the grammatical
relations output by RASP6, a set of relevant en-
6The grammatical relations used are nsubj, dobj, iobj,aux, argmod, detmod, ncmod and mod.
The agent of the verb (e.g., ?We adopt. . .. . . adopted by the author?), the agent?s determiner andrelated adjectives.
The direct object of the verb, the object?s determinerand adjectives, in addition to any post-modifiers (e.g.,?. . . apply a method proposed by [1] . . . ? , ?. . . followan approach of [1] . . . ?
Auxiliaries of the verb (e.g., ?In a similar manner, wemay propose a . . . ?)
Adverbial modification of the verb (e.g., ?We have pre-viously presented a . . . .?)
Prepositional phrases related to the verb (e.g., ?In thispaper we present. . . ?, ?. . . adopted from their work?)
Figure 6: Grammatical relations considered
tities and modifiers for each verb is constructed,
grouped into five categories (cf. Fig. 6).
Next, semantic filters are applied to each of the
potential candidates (represented by the extracted
entities and modifiers), and a fitness score is cal-
culated. These constraints encode semantic princi-
ples that will apply to all cue phrases of that rhetor-
ical category. Examples for constraints are: if
work is referred to as being done in previous own
work, it is probably not a goal statement; the work
in a goal statement must be presented here or in the
current paper (the concept of ?here-ness?); and the
agents of a goal statement have to be the authors,
not other people. While these filters are manually
defined, they are modular, encode general princi-
ples, and can be combined to express a wide range
of rhetorical contexts. We verified that around 20
semantic constraints are enough to cover a large
sets of different cue phrases (the 1700 cue phrases
from Teufel (1999)), though not all of these are
implemented yet.
A nice side-effect of our approach is the simple
characterisation of a cue phrase (by a syntactic re-
lationship, some seed words for each concept, and
some general, reusable semantic constraints). This
characterisation is more informative and specific
than string-based approaches, yet it has the poten-
tial for generalisation (useful if the cue phrases are
ever manually assessed and put into a lexicon).
Fig. 7 shows successful extraction examples
from our corpus7 , illustrating the difficulty of
the task: the system correctly identified sen-
tences with syntactically complex goal-type and
continuation-type cue phrases, and correctly re-
jected deceptive variants8 .
7Numbers after examples give CmpLg archive numbers,followed by sentence numbers according to our preprocess-ing.
8The seeds in this example were [analyse, present] & [ar-chitecture, method] (for goal) and [improve, adopt] & [model,method] (for continuation).
925
Correctly found:Goal-type:What we aim in this paper is to propose a paradigmthat enables partial/local generation through de-compositions and reorganizations of tentative localstructures. (9411021, S-5)Continuation-type:In this paper we have discussed how the lexico-graphical concept of lexical functions, introducedby Melcuk to describe collocations, can be used asan interlingual device in the machine translation ofsuch structures. (9410009, S-126)Correctly rejected:Goal-type:Perhaps the method proposed by Pereira et al(1993) is the most relevant in our context.(9605014, S-76)Continuation-type:Neither Kamp nor Kehler extend their copying/ sub-stitution mechanism to anything besides pronouns,as we have done. (9502014, S-174)
Figure 7: Sentences correctly processed by our system
4 Gold standard evaluation
We evaluated the quality of the extracted phrases
in two ways: by comparing our system output to
gold standard annotation, and by human judge-
ment of the quality of the returned sentences. In
both cases bootstrapping was done using the seed
tuples [analyse, present] & [architecture, method].
For the gold standard-evaluation, we ran our sys-
tem on a test set of 121 scientific articles drawn
from the CmpLg corpus (Teufel, 1999) ? en-
tirely different texts from the ones the system was
trained on. Documents were manually annotated
by the second author for (possibly more than one)
goal-type sentence; annotation of that type has
been previously shown to be reliable at K=.71
(Teufel, 1999). Our evaluation recorded how often
the system?s highest-ranked candidate was indeed
a goal-type sentence; as this is a precision-critical
task, we do not measure recall here.
We compared our system against our reimple-
mentation of Ravichandran and Hovy?s (2002)
paraphrase learning. The seed words were of the
form {goal-verb, goal-noun}, and we submitted
each of the 4 combinations of the seed pair to
Google Scholar. From the top 1000 documents for
each query, we harvested 3965 sentences contain-
ing both the goal-verb and the goal-noun. By con-
sidering all possible substrings, an extensive list of
candidate patterns was assembled. Patterns with
single occurrences were discarded, leaving a list
of 5580 patterns (examples in Fig. 8). In order
to rank the patterns by precision, the goal-verbs
were submitted as queries and the top 1000 doc-
uments were downloaded for each. From these,
we <verb> a <noun> forof a new <noun> to <verb> theIn this section , we <verb> the <noun> ofthe <noun> <verb> in this paperis to <verb> the <noun> after
Figure 8: Examples of patterns extracted usingRavichandran and Hovy?s (2002) method
Method Correctsentences
Our system with bootstrapping 88 (73%)Ravichandran and Hovy (2002) 58 (48%)Our system, no bootstrapping, WordNet 50 (41%)Our system, no bootstrapping, seeds only 37 (30%)
Figure 9: Gold standard evaluation: results
the precision of each pattern was calculated by di-
viding the number of strings matching the pattern
instantiated with both the goal-verb and all Word-
Net synonyms of the goal-noun, by the number
of strings matching the patterns instantiated with
the goal-verb only. An important point here is
that while the tight semantic coupling between the
question and answer terms in the original method
accurately identifies all the positive and negative
examples, we can only approximate this by using a
sensible synonym set for the seed goal-nouns. For
each document in the test set, the sentence contain-
ing the pattern with the highest precision (if any)
was extracted as the goal sentence.
We also compared our system to two baselines.
We replaced the lists obtained from the lexical
bootstrapping module with a) just the seed pair
and b) the seed pair and all the WordNet synonyms
of the components of the seed pair9.
The results of these experiments are given
in Fig. 9. All differences are statistically
significant with the ?2 test at p=.01 (except
those between Ravichandran/Hovy and our non-
bootstrapping/WordNet system). Our bootstrap-
ping system outperforms the Ravichandran and
Hovy algorithm by 34%. This is not surprising,
because this algorithm was not designed to per-
form well in tasks where there is no clear negative
context. The results also show that bootstrapping
outperforms a general thesaurus such as WordNet.
Out of the 33 articles where our system?s
favourite was not an annotated goal-type sentence,
only 15 are due to bootstrapping errors (i.e., to
an incorrect ranking of the lexical variants), corre-
9Bootstrapping should in principle do better than a the-saurus, as some of our correctly identified variants are nottrue synonyms (e.g., theory vs. method), and as noise throughovergeneration of unrelated senses might occur unless auto-matic word sense diambiguation is performed.
926
System chose: but should have chosen:
derive set compare model
illustrate algorithm present formalisation
discuss measures present variationsdescribe modificationspropose measures
accommodate material describe approach
examine material present study
Figure 10: Wrong bootstrapping decisions
Ceiling System Baseline
Exp. A 3.91 3.08 1.58Exp.B 4.33 3.67 2.50
Figure 11: Extrinsic evaluation: judges? scores
sponding to a 88% accuracy of the bootstrapping
module. Examples from those 15 error cases are
given in Fig. 10. The other errors were due to the
cue phrase not being a transitive verb?direct ob-
ject pattern (e.g. we show that, our goal is and
we focus on), so the system could not have found
anything (11 cases, or an 80% accuracy), ungram-
matical English or syntactic construction too com-
plex, resulting in a lack of RASP detection of the
crucial grammatical relation (2) and failure of the
semantic filter to catch non-goal contexts (5).
5 Human evaluation
We next perform two human experiments to in-
directly evaluate the quality of the automatically
generated cue phrase variants. Given an abstract of
an article and a sentence extracted from the article,
judges are asked to assign a score ranging from 1
(low) to 5 (high) depending on how well the sen-
tence expresses the goal of that article (Exp. A),
or the continuation of previous work (Exp. B).
Each experiment involves 24 articles drawn ran-
domly from a subset of 80 articles in the CmpLg
corpus that contain manual annotation for goal-
type and continuation-type sentences. The experi-
ments use three external judges (graduate students
in computational linguistics), and a Latin Square
experimental design with three conditions: Base-
line (see below), System-generated and Ceiling
(extracted from the gold standard annotation used
in Teufel (1999)). Judges were not told how the
sentences were generated, and no judge saw an
item in more than one condition.
The baseline for Experiment A was a random
selection of sentences with the highest TF*IDF
scores, because goal-type sentences typically con-
tain many content-words. The baseline for ex-
periment B (continuation-type) were randomly se-
lected sentences containing citations, because they
often co-occur with statements of continuation. In
both cases, the length of the baseline sentence was
controlled for by the average lengths of the gold
standard and the system-extracted sentences in the
document.
Fig. 11 shows that judges gave an average score
of 3.08 to system-extracted sentences in Exp. A,
compared with a baseline of 1.58 and a ceiling of
3.9110; in Exp. B, the system scored 3.67, with
a higher baseline of 2.50 and a ceiling of 4.33.
According to the Wilcoxon signed-ranks test at
? = .01, the system is indistinguishable from
the gold standard, but significantly different from
the baseline, in both experiments. Although this
study is on a small scale, it indicates that humans
judged sentences obtained with our method as al-
most equally characteristic of their rhetorical func-
tion as human-chosen sentences, and much better
than non-trivial baselines.
6 Conclusion
In this paper we have investigated the automatic
acquisition of semi-fixed cue phrases as a boot-
strapping task which requires very little manual
input for each cue phrase and yet generalises to
a wide range of syntactic and lexical variants in
running text. Our system takes a few seeds of the
type of cue phrase as input, and bootstraps lex-
ical variants from a large corpus. It filters out
many semantically invalid contexts, and finds cue
phrases in various syntactic variants. The system
achieved 80% precision of goal-type phrases of
the targeted syntactic shape (88% if only the boot-
strapping module is evaluated), and good quality
ratings from human judges. We found Google
Scholar to perform better than BNC as source for
finding hypotheses for lexical variants, which may
be due to the larger amount of data available to
Google Scholar. This seems to outweigh the dis-
advantage of only being able to use POS patterns
with Google Scholar, as opposed to robust parsing
with the BNC.
In the experiments reported, we bootstrap only
from one type of cue phrase (transitive verbs and
direct objects). This type covers a large propor-
tion of the cue phrases needed practically, but our
algorithm should in principle work for any kind of
semi-fixed cue phrase, as long as they have two
core concepts and a syntactic and semantic
10This score seems somewhat low, considering that thesewere the best sentences available as goal descriptions, accord-ing to the gold standard.
927
CUE PHRASE: ?(previous) methods fail? (Subj?Verb)
VARIANTS SEED 1: methodology, approach,technique. . .VARIANTS SEED 2: be cursed, be incapable of, berestricted to, be troubled, degrade, fall prey to, . . .
CUE PHRASE: ?advantage over previous methods?(NP?PP postmod + adj?noun premod.)
VARIANTS SEED 1: benefit, breakthrough, edge,improvement, innovation, success, triumph. . .VARIANTS SEED 2: available, better-known,cited, classic, common, conventional, current, cus-tomary, established, existing, extant,. . .
Figure 12: Cues with other syntactic relationships
relation between them. Examples for such other
types of phrases are given in Fig. 12; the second
cue phrase involves a complex syntactic relation-
ship between the two seeds (or possibly it could
be considered as a cue phrase with three seeds).
We will next investigate if the positive results pre-
sented here can be maintained for other syntactic
contexts and for cue phrases with more than two
seeds.
The syntactic variant extractor could be en-
hanced in various ways, eg. by resolving anaphora
in cue phrases. A more sophisticated model of
syntactically weighted vector space (Pado and La-
pata, 2003) may help improve the lexical acquisi-
tion phase. Another line for future work is boot-
strapping meaning across cue phrases within the
same rhetorical class, e.g. to learn that we propose
a method for X and we aim to do X are equivalent.
As some papers will contain both variants of the
cue phrase, with very similar material (X) in the
vicinity, they could be used as starting point for
experiments to validate cue phrase equivalence.
7 Acknowledgements
This work was funded by the EPSRC projects CIT-
RAZ (GR/S27832/01, ?Rhetorical Citation Maps
and Domain-independent Argumentative Zon-
ing?) and SCIBORG (EP/C010035/1, ?Extracting
the Science from Scientific Publications?).
References
Eugene Agichtein and Luis Gravano. 2000. Snowball: Ex-
tracting relations from large plain-text collections. In Pro-ceedings of the 5th ACM International Conference on Dig-ital Libraries.
Regina Barzilay and Lillian Lee. 2002. Bootstrapping lex-
ical choice via multiple-sequence alignment. In Proc. ofEMNLP.
Ted Briscoe and John Carroll. 2002. Robust accurate statis-
tical annotation of general text. In Proc. of LREC.
Lou Burnard, 1995. Users Reference Guide, British NationalCorpus Version 1.0. Oxford University, UK.
John Carroll, Guido Minnen, and Ted Briscoe. 1999. Cor-
pus annotation for parser evaluation. In Proceedingsof Linguistically Interpreted Corpora (LINC-99), EACL-workshop.
Ute Essen and Volker Steinbiss. 1992. Co-occurrence
smoothing for stochastic language modelling. In Proc. ofICASSP.
Donald Hindle. 1990. Noun classification from predicate-
argument structures. In Proc. of the ACL.
Edvard Hovy and Chin-Yew Lin. 1998. Automated text sum-
marization and the Summarist system. In Proc. of the TIP-STER Text Program.
Ken Hyland. 1998. Persuasion and context: The pragmat-
ics of academic metadiscourse. Journal of Pragmatics,
30(4):437?455.
Christian Jacquemin, Judith Klavans, and Evelyn Tzouker-
mann. 1997. Expansion of multi-word terms for indexing
and retrieval using morphology and syntax. In Proc. of theACL.
Julian Kupiec, Jan O. Pedersen, and Francine Chen. 1995. A
trainable document summarizer. In Proc. of SIGIR-95.
Lillian Lee. 1999. Measures of distributional similarity. InProc. of the ACL.
Jianhua Lin. 1991. Divergence measures based on the Shan-
non entropy. IEEE transactions on Information Theory,
37(1):145?151.
Frederique Lisacek, Christine Chichester, Aaron Kaplan, and
Sandor Agnes. 2005. Discovering paradigm shift patterns
in biomedical abstracts: Application to neurodegenerative
diseases. In Proc. of the SMBM.
George Miller, Richard Beckwith, Christiane Fellbaum,
Derek Gross, and Katherine Miller. 1990. Five papers
on WordNet. Technical report, Cognitive Science Labora-
tory, Princeton University.
Greg Myers. 1992. In this paper we report...?speech acts
and scientific facts. Journal of Pragmatics, 17(4):295?
313.
Sebastian Pado and Mirella Lapata. 2003. Constructing se-
mantic space models from parsed corpora. In Proc. ofACL.
Chris D. Paice. 1981. The automatic generation of lit-
erary abstracts: an approach based on the identifica-
tion of self-indicating phrases. In Robert Norman Oddy,
Stephen E. Robertson, Cornelis Joost van Rijsbergen, and
P. W. Williams, editors, Information Retrieval Research,
Butterworth, London, UK.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993. Dis-
tributional clustering of English words. In Proc. of theACL.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system. InProc. of the ACL.
Ellen Riloff. 1993. Automatically constructing a dictionary
for information extraction tasks. In Proc. of AAAI-93.
Simone Teufel. 1998. Meta-discourse markers and problem-
structuring in scientific articles. In Proceedings of theACL-98 Workshop on Discourse Structure and DiscourseMarkers.
Simone Teufel. 1999. Argumentative Zoning: InformationExtraction from Scientific Text. Ph.D. thesis, School of
Cognitive Science, University of Edinburgh, UK.
Cornelis Joost van Rijsbergen. 1979. Information Retrieval.
Butterworth, London, UK, 2nd edition.
Hua Wu and Ming Zhou. 2003. Synonymous collocation
extraction using translation information. In Proc. of theACL.
928
Examining the consensus between human summaries: initial
experiments with factoid analysis
Hans van Halteren
Department of Language and Speech
University of Nijmegen, The Netherlands
Simone Teufel
Computer Laboratory
Cambridge University, UK
Abstract
We present a new approach to summary evaluation
which combines two novel aspects, namely (a) con-
tent comparison between gold standard summary
and system summary via factoids, a pseudo-semantic
representation based on atomic information units
which can be robustly marked in text, and (b) use
of a gold standard consensus summary, in our case
based on 50 individual summaries of one text. Even
though future work on more than one source text is
imperative, our experiments indicate that (1) rank-
ing with regard to a single gold standard summary is
insufficient as rankings based on any two randomly
chosen summaries are very dissimilar (correlations
average ? = 0.20), (2) a stable consensus summary
can only be expected if a larger number of sum-
maries are collected (in the range of at least 30-40
summaries), and (3) similarity measurement using
unigrams shows a similarly low ranking correlation
when compared with factoid-based ranking.
1 Introduction
It is an understatement to say that measuring the
quality of summaries is hard. In fact, there is unan-
imous consensus in the summarisation community
that evaluation of summaries is a monstrously diffi-
cult task. In the past years, there has been quite a
lot of summarisation work that has effectively aimed
at finding viable evaluation strategies (Spa?rck Jones,
1999; Jing et al, 1998; Donaway et al, 2000). Large-
scale conferences like SUMMAC (Mani et al, 1999)
and DUC (2002) have unfortunately shown weak re-
sults in that current evaluation measures could not
distinguish between automatic summaries ? though
they are effective enough to distinguish them from
human-written summaries.
In principle, the best way to evaluate a summary
is to try to perform the task for which the sum-
mary was meant in the first place, and measure the
quality of the summary on the basis of degree of
success in executing the task. However, such extrin-
sic evaluations are so time-consuming to set up that
they cannot be used for the day-to-day evaluation
needed during system development. So in practice,
a method for intrinsic evaluation is needed, where
the properties of the summary itself are examined,
independent of its application.
We think one of the reasons for the difficulty of an
intrinsic evaluation is that summarisation has to call
upon at least two hard subtasks: selection of infor-
mation and production of new text. Both tasks are
known from various NLP fields (e.g. information re-
trieval and information extraction for selection; gen-
eration and machine translation (MT) for produc-
tion) to be not only hard to execute, but also hard to
evaluate. This is caused for a large part by the fact
that in both cases there is no single ?best? result, but
rather various ?good? results. It is hence no won-
der that the evaluation of summarisation, combining
these two, is even harder. The general approach for
intrinsic evaluations, then (Mani, 2001), is to sepa-
rate the evaluation of the form of the text (quality)
and its information content (informativeness).
In this paper, we will focus on the latter, the in-
trinsic evaluation of informativeness, and we will ad-
dress two aspects: the (in)sufficiency of the single
human summary to measure against, and the infor-
mation unit on which similarity measures are based.
1.1 Gold standards
In various NLP fields, such as POS tagging, systems
are tested by way of comparison against a ?gold stan-
dard?, a manually produced result which is supposed
to be the ?correct?, ?true? or ?best? result. This
presupposes, however, that there is a single ?best?
result. In summarisation there appears to be no ?one
truth?, as is evidenced by a low agreement between
humans in producing gold standard summaries by
sentence selection (Rath et al, 1961; Jing et al,
1998; Zechner, 1996), and low overlap measures be-
tween humans when gold standards summaries are
created by reformulation in the summarisers? own
words (e.g. the average overlap for the 542 single
document summary pairs in DUC-02 was only about
47%).
But even though the non-existence of any one gold
standard is generally acknowledged in the summari-
sation community, actual practice nevertheless ig-
nores this. Comparisons against a single gold stan-
dard are widely used, due to the expense of compil-
ing summary gold standards and the lack of compos-
ite measures for comparison to more than one gold
standard.
In a related field, information retrieval (IR), the
problem of subjectivity of relevance judgements is
circumvented by extensive sampling: many differ-
ent queries are collected to level out the difference
humans have in suggesting queries and in select-
ing relevant documents. While relevance judgements
between humans remain different, Voorhees (2000)
shows that the relative rankings of systems are nev-
ertheless stable across annotators, which means that
meaningful IR measures have been found despite the
inherent subjectivity of relevance judgements.
Similarly, in MT, the recent Bleu measure also
uses the idea that one gold standard is not enough.
In an experiment, Papineni et al (2001) based an
evaluation on a collection of four reference trans-
lations of 40 general news stories and showed the
evaluation to be comparable to human judgement.
Lin and Hovy (2002) examine the use of a multi-
ple gold standard for summarisation evaluation, and
conclude ?we need more than one model summary
although we cannot estimate how many model sum-
maries are required to achieve reliable automated
summary evaluation?. We explore the differences
and similarities between various human summaries
in order to create a basis for such an estimate, and as
a side-effect, also re-examine the degree of difference
between the use of a single summary gold standard
and the use of a compound gold standard.
1.2 Similarity measures
The second aspect we examine is the similarity
measure to be used for gold standard comparison.
In principle, the comparison can be done via co-
selection of extracted sentences (Rath et al, 1961;
Jing et al, 1998; Zechner, 1996), by string-based sur-
face measures (Lin and Hovy, 2002; Saggion et al,
2002), or by subjective judgements of the amount
of information overlap (DUC, 2002). The rationale
for using information overlap judgement as the main
evaluation metric for DUC is the wish to measure
the meaning of sentences rather than use surface-
based similarity such as co-selection (which does not
even take identical information expressed in different
sentences into account) and string-based measures.
In the DUC competitions, assessors judge the infor-
mational overlap between ?model units? ( elemen-
tary discourse units (EDUs), i.e. clause-like units,
taken from the gold standard summary) and ?peer
units? (sentences taken from the participating sum-
maries) on the basis of the question: ?How much
of the information in a model unit is contained in a
peer unit: all of it, most, some, any, or none.? This
overlap judgement is done for each system-produced
summary, and weighted recall measures report how
much gold standard information is present in the
summaries.
However, Lin and Hovy (2002) report low agree-
ment for two tasks: producing the human summaries
(around 40%), and assigning information overlap be-
tween them. In those cases where annotators had
to judge a pair consisting of a gold standard sen-
tence and a system sentence more than once (be-
cause different systems returned the same sentence),
they agreed with their own prior judgement in only
82% of the cases. This relatively low intra-annotator
agreement points to the fact that the overlap judge-
ment remains a subjective task where judges will
disagree. Lin and Hovy show the instability of the
evaluation, expressed in system rankings.
We propose a gold standard comparison based
on factoids, a pseudo-semantic representation of the
text, which measures information rather than string
similarity, like DUC, but which is more objective
than DUC-style information overlap judgement.
2 Data and factoid annotation
Our goal is to compare the information content of
different summaries of the same text. In this ini-
tial investigation we decided to focus on a single
text. The text used for the experiment is a BBC
report on the killing of the Dutch politician Pim
Fortuyn. It is about 600 words long, and contains
a mix of factual information and personal reactions.
Our guidelines asked the human subjects to write
generic summaries of roughly 100 words. We asked
them to formulate the summary in their own words,
so that we can also see which different textual forms
are produced for the same information.
Knowledge about the variability of expression is
important both for evaluation and system building,
and particularly so in in multi-document summarisa-
tion, where redundant information is likely to occur
in different textual forms.
We used two types of human summarisers. The
largest group consisted of Dutch students of English
and of Business Communications (with English as
a chosen second language). Of the 60 summaries
we received, we had to remove 20. Summaries were
removed if it was obvious from the summary that
the student had insufficient skill in English or if the
word count was too high (above 130 words). A sec-
ond group consisted of 10 researchers, who are either
native or near-native English speakers. With this
group there were no problems with language, for-
mat or length, and we could use all 10 summaries.
Our total number of summaries was thus 50.
2.1 The factoid as atomic information units
We use atomic semantic units called factoids to rep-
resent the meaning of a sentence. For instance, we
represent the sentence
The police have arrested a white Dutch man.
by the union of the following factoids:
FP20 A suspect was arrested
FP21 The police did the arresting
FP24 The suspect is white
FP25 The suspect is Dutch
FP26 The suspect is male
Note that in this case, factoids correspond to ex-
pressions in a FOPL-style semantics, which are com-
positionally interpreted. However, we define atom-
icity as a concept which depends on the set of sum-
maries we work with. If a certain set of potential
factoids always occurs together, this set of factoids
is treated as one factoid, because differentiation of
this set would not help us in distinguishing the sum-
maries. If we had found, e.g., that there is no sum-
mary that mentions only one of FP25 and FP26,
those factoids would be combined into one new fac-
toid ?FP27 The suspect is a Dutch man?.
Our definition of atomicity means that the
?amount? of information associated with one factoid
can vary from a single word to an entire sentence.
An example for a large chunk of information that
occurred atomically in our texts was the fact that
the victim wanted to become PM (FV71), a factoid
which covers an entire sentence. On the other hand,
a single word may contain several factoids. The word
?gunman? leads to two factoids: ?FP24 The perpe-
trator is male? and ?FA20 A gun was used in the
attack?.
The advantage of our functional, summary-set-
dependent definition of atomicity is that the defi-
nition of what counts as a factoid is more objec-
tive than if factoids had to be invented by intuition,
which is hard. One possible disadvantage of our def-
inition of atomicity (which is dependent on a given
set of summaries) is that the set of factoids used
may have to be adjusted if further summaries are
added to the collection. In practice, for a fixed set
of summaries for experiments, this is less of an issue.
We decompose meanings into separate (composi-
tionally interpreted) factoids, if there are mentions
in our texts which imply information overlap. If
one summary contains ?was murdered? and another
?was shot dead?, we can identify the factoids
FA10 There was an attack
FA40 The victim died
FA20 A gun was used
The first summary contains only the first two fac-
toids, whereas the second contains all three. That
way, the semantic similarity between related words
can be expressed.
2.2 Compositionality, generalisation and
factuality
The guidelines for manual annotation of summaries
with factoids stated that only factoids which are
explicitly expressed in the text should be marked.
When we identified factoids in our actual summary
collection, most factoids turned out to be indepen-
dent of each other, i.e. the union of the factoids can
be compositionally interpreted. However, there are
relations between factoids which are not as straight-
forward. For instance, in the case of ?FA21 Mul-
tiple shots were fired? and ?FA22 Six shots were
fired?, FA22 implies FA21; any attempt to express
the relationship between the factoids in a composi-
tional way would result in awkward factoids. We
accept that there are factoids which are most natu-
rally expressed as generalisations of other factoids,
and record for each factoid a list of factoids that are
more general than it is, so that we can include these
related factoids as well. In one view of our data, if a
summary states FA22, FA21 is automatically added.
In addition to generality, there are two further
complicated phenomena we had to deal with. The
first one is real inference, rather than generalisation,
as in the following cases:
FL52 The scene of the murder had tight security
checks
FL51 The scene of the murder was difficult to
get into
FL50 It is unclear how the perpetrator got to
the victim
FL52 implies (in the sense of real inference) FL51,
which in turn implies FL50. We again record infer-
ence relations and automatically compute the transi-
tive closure of all inferences, but we do not currently
formally distinguish them from the simpler general-
isation relations.
The second phenomenon is the description of peo-
ple?s opinions. In our source document, quotations
of the reactions of several politicians were given. In
the summaries, our subjects often generalised these
reactions and produced statements such as
Dutch as well as international politicians have expressed
their grief and disbelief.
As more than one entity can be reported as saying
the same thing, straightforward factoid union is not
powerful enough to accurately represent the attri-
bution of opinions, as our notation does not contain
variables for discourse referents and quoted state-
ments. We therefore revert to a separate set of fac-
toids, which are multiplied-out factoids that com-
bine the statement (what is being said) together with
a description of who said it. Elements of the descrip-
tion can be interpreted in a compositional manner.
For instance, the above sentence is expressed in
our notation as
OG10 Grief was expressed
OG60 Dutch persons or organizations expressed
grief
OG62 International persons or organizations
expressed grief
OG40 Politicians expressed grief
OS10 Disbelief was expressed
OS60 Dutch persons or organizations expressed
disbelief
OS62 International persons or organizations
expressed disbelief
OS40 Politicians expressed disbelief
Another problem with attribution of opinions is
that there is not always a clear distinction between
fact and opinion. For instance, the following sen-
tence is presented as opinion in the original ?Geral-
dine Coughlan in the Hague says it would have been
difficult to gain access to the media park.? Never-
theless, our summarisers often decided to represent
such opinions as facts, ie. as ?The media park was
difficult to gain entry to.? ? in fact, in our data,
every summary containing this factoid presents it
as fact. For now, we have taken the pragmatic ap-
proach that the classification of factoids into factual
and opinion factoids is determined by the actual rep-
resentation of the information in the summaries (cf.
FL51 above, where the first letter ?F? stands for
factual, the first letter ?O? for opinion).
The factoid approach can capture much finer
shades of meaning differentiations than DUC-style
information overlap does ? in an example from Lin
and Hovy (2002), an assessor judged some content
overlap between ?Thousands of people are feared
dead? and ?3,000 and perhaps ... 5,000 people have
been killed.? In our factoid representation, a dis-
tinction between ?killed? and ?feared dead? would
be made, and different numbers of people mentioned
would have been differentiated.
2.3 Factoid annotation
The authors have independently marked the pres-
ence of factoids in all summaries in the collection.
Factoid annotation of a 100 word summary takes
roughly half an hour. Even with only short guide-
lines, the agreement on which factoids are present
in a summary appears to be high. The recall of an
individual annotator with regard to the consensus
annotation is about 96%, and precision about 97%.
This means that we can work with the current fac-
toid presence table with reasonable confidence.
Whereas single summaries contain between 32 and
55 factoids, the collection as a whole contains 256
different factoids. Figure 1 shows the growth of the
number of factoids with the size of the collection (1
to 40 summaries). We assume that the curve is Zip-
fian. This observation implies that larger numbers
of summaries are necessary if we are looking for a
definitive factoid list of a document.
Figure 1: Average number of factoids in collections
of size 1?40
The maximum number of possible factoids is not
bounded by the number of factoids occurring in the
document itself. As we explained above, factoids
come into existence because they are observed in the
collection of summaries, and summaries sometimes
contain factoids which are not actually present in the
document. Examples of such factoids are ?FP31 The
suspect has made no statement?, which is true but
not stated in the source text, and ?FP23 The suspect
was arrested on the scene?, which is not even true.
The reasons for such ?creative? factoids vary from
the expression of the summarisers? personal knowl-
edge or opinion to misinterpretation of the source
text. In total we find 87 such factoids, 51 factual
ones and 36 incorrect generalisations of attribution.
Of the remaining 169 ?correct? factoids, most
(125) are factual. Within these factoids, we find
74 generalisation links. The rest of the factoids con-
cern opinions and their attribution. There are 18
descriptions of opinion, with 11 generalisation links,
and 26 descriptions of attribution, with 16 general-
isation links. For all types, we see that most facts
are being represented at differing levels of generali-
sation. Some of the generalisation links are part of
3- or 4-link hierarchies, e.g. ?FV40 Victim outspo-
ken about/campaigning on immigration issues? (26
mentions) to ?FV41 Victim was anti- immigration?
(23) to ?FV42 Victim wanted to close borders to im-
migration? (9), or ?FV50 Victim outspoken about
race/religion issues? (17 mentions) to ?FV51 Vic-
tim outspoken about Islam/Muslims? (16) to ?FV52
Victim made negative remarks about Islam? (14) to
?FV53 Victim called Islam a backward religion? (9).
It is not surprising that more specific factoids are
less frequent than their generalisations, but we ex-
pect interesting correlations between a factoid?s im-
portance and the degree and shape of the decline
of its generalisation hierarchy, especially where fac-
toids about the attribution of opinion are concerned.
This is an issue for further research.
3 Human summaries as benchmark
for evaluation
If we plan to use human summaries as a refer-
ence point for the evaluation of machine-made sum-
maries, we are assuming that there is some consensus
between the human summarisers as to which infor-
mation is important enough to include in a summary.
Whether such consensus actually exists is uncertain.
In very broad terms, we can distinguish four possible
scenarios:
1. There is a good consensus between all human
summarisers. A large percentage of the factoids
present in the summaries is in fact present in a
large percentage of the summaries. We can de-
termine whether this is so by measuring factoid
overlap.
2. There is no such overall consensus between all
summarisers, but there are subsets of summaris-
ers between whom consensus exists. Each of
these subsets has summarised from a particular
point of view, even though a generic summary
was requested, and the point of view has led
to group consensus. We can determine whether
this is so by doing a cluster analysis on the fac-
toid presence vectors. We should find clusters
if and only if group consensus exists.
3. There is no such thing as overall consensus, but
there is a difference in perceived importance be-
tween the various factoids. We can determine
whether this is the case by examining how often
each factoid is used in the summaries. Factoids
that are more important ought to be included
more often. In that case, it is still possible to
create a consensus-like reference summary for
any desired summary size.
4. There is no difference in perceived importance
of the various factoids at all. Inclusion of fac-
toids in summaries appears to be random.
3.1 Factoid frequency and consensus
We will start by examining whether an importance
hierarchy exists, as this can help us decide between
scenario 1, 3 or 4. If still necessary, we can check for
group consensus later.
If we count how often each factoid is used, it
quickly becomes clear that we do not have to worry
about worst-case scenario 4. There are clear differ-
ences in the frequency of use of the factoids. On
the other hand, scenario 1 does not appear to be
very likely either. There is full consensus on the in-
clusion of only a meager 3 factoids, which can be
summarised in 3 words:
Fortuyn was murdered.
If we accept some disagreement, and take the fac-
toids which occur in at least 90% of the summaries,
this increases the consensus summary to 5 factoids
and 6 words:
Fortuyn, a politician, was shot dead.
Setting our aims ever lower, 75% of the summaries
include 6 further factoids and the summary goes up
to 20 words:
Pim Fortuyn, a Dutch right-wing politician, was shot
dead before the election. A suspect was arrested. Fortuyn
had received threats.
A 50% threshold yields 8 more factoids and the
47-word summary:
Pim Fortuyn, a Dutch right-wing politician, was shot
dead at a radio station in Hilversum. Fortuyn was cam-
paigning on immigration issues and was expected to do
well in the election. He had received threats. There were
shocked reactions. Political campaigning was halted. The
police arrested a man.
If we want to arrive at a 100-word summary (ac-
tually 104), we need to include 26 more factoids, and
we need to allow all factoids which occur in at least
30% of the summaries:
Pim Fortuyn was shot six times and died shortly after-
wards. He was attacked when leaving a radio station in
the (well-secured) media park in Hilversum. The Dutch
far-right politician was campaigning on an anti- immi-
gration ticket and was outspoken about Islam. He was
expected to do well in the upcoming election, getting at
least 15% of the votes. Fortuyn had received threats. He
expected an attack and used bodyguards. Dutch and in-
ternational politicians were shocked and condemned the
attack. The Dutch government called a halt to political
campaigning. The gunman was chased. The police later
arrested a white Dutch man. The motive is unknown.
We conclude that the extreme scenarios, full con-
sensus and full absence of consensus, can be rejected
for this text. This leaves the question whether the
partial consensus takes the form of clusters of con-
senting summarisers.
3.2 Summariser clusters
In order to determine whether the summarisers can
be assigned to groups within which a large amount of
consensus can be found, we turn to statistical tech-
niques. We first form 256-dimensional binary vectors
recording the presence of each of the factoids in each
?2 ?1 0 1 2
?
2
?
1
0
1
2
R001
R002
R003
R004
R005
R006
R007
R008
R009
R010
S001
S002S003
S004
S005
S007
S009
S010
S011S012
S013
S014
S015
S016
S017
S018
S019S023S027
S028S030
S031
S032
S033
S034
S036
S038
S041
S042
S045
S046
S048
S049
S051
S053
S054
S055
S056
S057 S061
Cons
Figure 2: Classical multi-dimensional scaling of dis-
tances between factoid vectors into two dimensions
summariser?s summary. We also added a vector for
the 104-word consensus summary above (?Cons?).
We then calculate the distances between the vari-
ous vectors and use these as input for classical multi-
dimensional scaling. The result of scaling into two
dimensions is shown in Figure 2.
Only a few small clusters appear to emerge. Al-
though we certainly cannot conclude that there are
no clusters, we would have expected more clearly de-
limited groups of summarisers, i.e. different points
of view, if scenario 2 described the actual situation.
For now we will assume that, for this document, sce-
nario 3 is the most likely.
3.3 The consensus summary as an
evaluation tool
Two of the main demands on a gold standard generic
summary for evaluation are: a) that it contains the
information deemed most important in the docu-
ment and b) that two gold standard summaries con-
structed along the same lines lead to the same, or
at least very similar, ranking of a set of summaries
which are evaluated.
If we decide to use a single human summary as
a gold standard, we in fact assume that this hu-
man?s choice of important material is acceptable for
all other summary users, which it the wrong assump-
tion, as the lack of consensus between the various
human summaries shows. We propose that the use
of a reference summary which is based on the factoid
importance hierarchy described above, as it uses a
less subjective indication of the relative importance
of the information units in the text across a popu-
lation of summary writers. The reference summary
would then take the form of a consensus summary,
in our case the 100-word compound summary on the
basis of factoids over the 30% threshold.
The construction of the consensus summary would
indicate that demand a) will be catered for, but we
still have to check demand b). We can do this by
computing rankings based on the F-measure for in-
cluded factoids, and measuring the correlation coef-
ficient ? between them.
As we do not have a large number of automatic
summaries of our text available, we use our 50 hu-
man summaries as data, pretending that they are
summaries we wish to rank (evaluate).
If we compare the rankings on the basis of sin-
gle human summaries as gold standard, it turns out
that the ranking correlation ? between two ?gold?
standards is indeed very low at an average of 0.20
(variation between -0.51 and 0.85). For the consen-
sus summary, we can compare rankings for various
numbers of base summaries. After all, the consensus
summary should improve with the number of con-
tributing base summaries and ought to approach an
ideal consensus summary, which would be demon-
strated by a stabilizing derived ranking.
We investigate if this assumption is correct by cre-
ating pairs of samples of N=5 to 200 base summaries,
drawn (in a way similar to bootstrapping) from our
original sample of 50. For each pair of samples, we
automatically create a pair of consensus summaries
and then determine how well these two agree in their
ranking. Figure 3 shows how ? increases with N
(based on 1000 trials per N). At N=5 and 10, ? has
a still clearly unacceptable average 0.40 or 0.53. The
average reaches 0.80 at 45, 0.90 at 95 and 0.95 at a
staggering 180 base summaries.
We must note, however, that we have to be care-
ful with these measurements, since 40 of our 50
starting summaries were made by less experienced
non-natives. In fact, if we bootstrap pairs of N=10
base summary samples (100 trials) on just the 10
higher-quality summaries (created by natives and
near-natives), we get an average ? of 0.74. The
same experiment on 10 different summaries from
the other 40 (100 trials for choosing the 10, and for
each 100 trials to estimate average ?) yields average
??s ranging from 0.55 to 0.63. So clearly the differ-
ence in experience has its effect. Even so, even the
?better? summaries lead to a ranking correlation of
?=0.74 at N=10, which still is much lower than we
would like to see. We estimate that with this type of
summaries an acceptably stable ranking (? around
0.90) would be reached somewhere between 30 and
40 summaries.
3.4 Using unigrams instead of factoids
Apart from the need for human summaries, the
factoid-based comparisons have another problem,
5 15 25 35 45 55 65 75 85 95 110 125 140 155 170 185 200
?0
.2
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Figure 3: Correlation coefficient ? between rankings
for 50 summaries on the basis of two consensus sum-
maries, each based on a size N base summary collec-
tion, for N between 5 and 200
viz. the need for human interpretation when map-
ping summaries to factoid lists. The question is
whether simpler measures might not be equally in-
formative. We investigate this using unigram over-
lap, following Papineni et al (2001) in their sug-
gestion that unigrams best represent contents, while
longer n-grams best represent fluency.
Again, we reuse our 50 summaries as summaries to
be evaluated. For each of these summaries, we cal-
culate the F-measure for the included factoids with
regard to the consensus summary shown above. In
a similar fashion, we build a consensus unigram list,
containing the 103 unigrams that occur in at least
11 summaries, and calculate the F-measure for un-
igrams. The two measures are plotted against each
other in Figure 4.
Some correlation is present (r = 0.48 and Spear-
man?s ranking correlation ? = 0.45), but there are
clearly profound differences. If we look at the rank-
ings produced from these two F-measures, S054, on
position 16 on the basis of factoids, drops to posi-
tion 37 on the basis of unigrams. S046, on the other
hand, climbs from 42nd to 4th place when consid-
ered by unigrams instead of factoids. Apart from
these extreme cases, these are also clear differences
in the top-5 for the two measurements: S030, S028,
R001, S003 and S023 are the top-5 when measuring
with factoids, whereas S032, R002, S030, S046 and
S028 are the top-5 when measuring with unigrams.
It would seem that unigrams, though they are much
cheaper, are not a viable substitute for factoids.
0.40 0.45 0.50 0.55 0.60 0.65 0.70
0.
40
0.
45
0.
50
0.
55
0.
60
F(factoids)
F(
un
ig
ra
m
s)
R001
R002
R003
R004
R005
R006
R007
R008
R009
R010 S001
S002
S003
S004
S005
S007 S009
S010
S011
S012
S013
S014
S015
S016
S017
S018
S019
S023
S027
S028
S030
S031
S032
S033
S034
S036
S038
S041 S042
S045
S046
S048
S049
S051
S053
S054
S055
S056
S057
S061
Figure 4: F-measures of summarisers with regard to
consensus data: factoid-based versus unigram-based
4 Discussion and future work
From our experiences so far, it seems that both our
innovations, viz. using multiple summaries and mea-
suring with factoids, appear to be worth pursuing
further. We summarise the results for our test text
in the following:
? We observe a very wide selection of factoids in
the summaries, only few of which are included
by all summarisers.
? The number of factoids found if new summaries
are considered does not tail off.
? There is a clear importance hierarchy of fac-
toids which allows us to compile a consensus
summary.
? If single summaries are used as gold standard,
the correlation between rankings based on two
such gold standard summaries is low.
? We could not find any large clusters of highly
correlated summarisers in our data.
? Stability with respect to the consensus sum-
mary can only be expected if a larger number
of summaries are collected (in the range of at
least 30-40 summaries).
? A unigram-based measurement shows only low
correlation with the factoid-based measure-
ment.
The information that is gained through multi-
ple summaries with factoid-similarity is insufficiently
approximated with the currently used substitutes,
as the observations above show. However, what we
have described here must clearly be seen as an initial
experiment, and there is yet much to be done.
First of all, the notation of the factoid (currently
flat atoms) needs to be made more expressive, e.g.
by the addition of variables for discourse referents
and events, which will make factoids more similar
to FOPL expressions, and/or by the use of a typing
mechanism to indicate the various forms of infer-
ence/implication.
We also need to identify a good weighting scheme
to be used in measuring similarity of factoid vec-
tors. The weighting should correct for the variation
between factoids in information content, for their
different position along an inference chain, and pos-
sibly for their position in the summary. It should
also be able to express some notion of importance
of the factoids, e.g. as measured by the number of
summaries containing the factoid.
Something else to investigate is the presence and
distribution of factoids, types of factoids and rela-
tions between factoids in summaries and summary
collections. We have the strong feeling that some
of our observations were tightly linked to the type
of text we used. We would like to build a balanced
corpus of texts, of various subject areas and lengths,
and their summaries, at several different lengths and
possibly even multi-document, so that we can study
this factor. An open question is how many sum-
maries we should try to get for each of the texts in
the corpus. It is unlikely we will be able to collect
50 summaries for each new text. Furthermore, the
texts of the corpus should also be summarised by as
many machine summarisers as possible, so that we
can test ranking these on the basis of factoids, in a
realistic framework.
A final line of investigation is searching for ways to
reduce the cost of factoid analysis. The first reason
why this analysis is currently expensive is the need
for large summary bases for consensus summaries.
There is yet hope that this can be circumvented by
using larger numbers of texts, as is the case in IR
and in MT, where discrepancies prove to average out
when large enough datasets are used. Papineni et al,
e.g., were able to show that the ranking with their
Bleu measure of the five evaluated translators (two
human and three machine) remained stable if only
a single reference translation was used, suggesting
that ?we may use a big corpus with a single reference
translation, provided that the translations are not all
from the same translator?. Possibly a similar aver-
aging effect will occur in the evaluation of summari-
sation so that smaller summary bases can be used.
The second reason is the need for human annotation
of factoids. Although simple unigram-based meth-
ods prove insufficient, we will hopefully be able to
come a long way in automating factoid identification
on the basis of existing NLP techniques, combined
with information gained about factoids in research
as described in the previous paragraph. All in all,
the use of consensus summaries and factoid analy-
sis, even though expensive to set up for the moment,
provides a promising alternative which could well
bring us closer to a solution to several problems in
summarisation evaluation.
References
Donaway, Robert L., Kevin W. Drummey, and Laura A.
Mather. 2000. A comparison of rankings produced by
summarization evaluation measures. In Proceedings
of the ANLP/NAACL 2000 Workshop on Automatic
Summarization.
DUC. 2002. Document Understanding Conference
(DUC). Electronic proceedings, http://www-nlpir.
nist.gov/projects/duc/pubs.html.
Jing, Hongyan, Regina Barzilay, Kathleen R. McKe-
own, and Michael Elhadad. 1998. Summarization
Evaluation Methods: Experiments and Analysis. In
Dragomir R. Radev and Eduard H. Hovy, eds., Work-
ing Notes of the AAAI Spring Symposium on Intelli-
gent Text Summarization, 60?68.
Lin, Chin-Yew, and Eduard Hovy. 2002. Manual and
automatic evaluation of summaries. In DUC 2002.
Mani, Inderjeet. 2001. Automatic Summarization. John
Benjamins.
Mani, Inderjeet, Therese Firmin, David House, Gary
Klein, Beth Sundheim, and Lynette Hirschman. 1999.
The TIPSTER Summac Text Summarization Evalu-
ation. In Proceedings of EACL-99 , 77?85.
Papineni, K, S. Roukos, T Ward, and W-J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL-02 , 311?318.
Rath, G.J, A. Resnick, and T. R. Savage. 1961. The
Formation of Abstracts by the Selection of Sentences.
American Documentation 12(2): 139?143.
Saggion, Horacio, Dragomir Radev, Simone Teufel, Wai
Lam, and Stephanie M. Strassel. 2002. Developing
Infrastructure for the Evaluation of Single and Multi-
document Summarization Systems in a Cross-lingual
Environment. In Proceedings of LREC 2002 , 747?754.
Spa?rck Jones, Karen. 1999. Automatic Summaris-
ing: Factors and Directions. In Inderjeet Mani and
Mark T. Maybury, eds., Advances in Automatic Text
Summarization, 1?12. Cambridge, MA: MIT Press.
Voorhees, Ellen. 2000. Variations in relevance judge-
ments and the measurement of retrieval effectiveness.
Information Processing and Management 36: 697?
716.
Zechner, Klaus. 1996. Fast Generation of Abstracts from
General Domain Text Corpora by Extracting Relevant
Sentences. In Proceedings of COLING-96 , 986?989.
Evaluating information content by factoid analysis: human
annotation and stability
Simone Teufel
Computer Laboratory
Cambridge University, UK
Hans van Halteren
Language and Speech
University of Nijmegen, The Netherlands
Abstract
We present a new approach to intrinsic sum-
mary evaluation, based on initial experiments
in van Halteren and Teufel (2003), which com-
bines two novel aspects: comparison of infor-
mation content (rather than string similarity)
in gold standard and system summary, mea-
sured in shared atomic information units which
we call factoids, and comparison to more than
one gold standard summary (in our data: 20
and 50 summaries respectively). In this paper,
we show that factoid annotation is highly re-
producible, introduce a weighted factoid score,
estimate how many summaries are required for
stable system rankings, and show that the fac-
toid scores cannot be sufficiently approximated
by unigrams and the DUC information overlap
measure.
1 Introduction
Many researchers in summarisation believe that
the best way to evaluate a summary is extrin-
sic evaluation (Spa?rck Jones, 1999): to measure
the quality of the summary on the basis of de-
gree of success in executing a specific task with
that summary. The summary evaluation per-
formed in SUMMAC (Mani et al, 1999) fol-
lowed that strategy. However, extrinsic eval-
uations are time-consuming to set up and can
thus not be used for the day-to-day evaluation
needed during system development. So in prac-
tice, a method for intrinsic evaluation is needed,
where the properties of the summary itself are
examined, independently of its application.
Intrinsic evaluation of summary quality is un-
deniably hard, as there are two subtasks of sum-
marisation which need to be evaluated, infor-
mation selection and text production ? in fact
these two subtasks are often separated in evalu-
ation (Mani, 2001). If we restrict our attention
to information selection, systems are tested by
way of comparison against a ?gold standard?, a
manually produced result which is supposed to
be the ?correct?, ?true? or ?best? result.
In summarisation there appears to be no ?one
truth?, but rather various ?good? results. Hu-
man subjectivity in what counts as the most im-
portant information is high. This is evidenced
by low agreement on sentence selection tasks
(Rath et al, 1961; Jing et al, 1998), and low
word overlap measures in the task of creating
summaries by reformulation in the summaris-
ers? own words (e.g. word overlap of the 542
single document summary pairs in DUC-02 av-
eraged only 47%).
But even though the non-existence of any one
gold standard is generally acknowledged in the
summarisation community, actual practice nev-
ertheless ignores this, mostly due to the expense
of compiling summary gold standards and the
lack of composite measures for comparison to
more than one gold standard.
Other fields such as information retrieval (IR)
also have to deal with human variability con-
cerning the question of what ?relevant to a
query? means. This problem is circumvented
by extensive sampling: many different queries
are collected to level out the differences in
query formulation and relevance judgements.
Voorhees (2000) shows that the relative rank-
ings of IR systems are stable across annota-
tors even though relevance judgements differ
significantly between humans. Similarly, in MT,
the recent BLEU metric (Papineni et al, 2001)
also uses the idea that one gold standard is
not enough. Their ngram-based metric derived
from four reference translations of 40 general
news stories shows high correlation with human
judgement.
Lin and Hovy (2002) examine the use of
ngram-based multiple gold standards for sum-
marisation evaluation, and conclude ?we need
more than one model summary although we
cannot estimate how many model summaries
are required to achieve reliable automated sum-
mary evaluation?. In this paper, we explore the
differences and similarities between various hu-
man summaries in order to create a basis for
such an estimate and examine the degree of dif-
ference between the use of a single summary
gold standard and the use of a consensus gold
standard for two sample texts, based on 20 and
50 summaries respectively.
The second aspect we examine is the similar-
ity measure which compares system and gold
standard summaries. In principle, the com-
parison can be done via co-selection of ex-
tracted sentences (Rath et al, 1961; Jing et al,
1998), by string-based surface measures (Lin
and Hovy, 2002), or by subjective judgements
of the amount of information overlap (DUC,
2002). String-based metrics are superior to sen-
tence co-selection, as co-selection cannot take
similar or even identical information into ac-
count if it does not occur in the sentences which
were chosen. The choice of information overlap
judgements as the main metric in DUC reflects
the intuition that human judgements of shared
?meaning? of two texts should in principle be
superior to surface-based similarity.
DUC assessors judge the informational over-
lap between ?model units? (elementary dis-
course units (EDUs), i.e. clause-like units,
taken from the gold standard summary) and
?peer units? (sentences taken from the partici-
pating summaries) on the basis of the question:
?How much of the information in a model unit is
contained in a peer unit: 100%, 80%, 60%, 40%,
20%, 0%?? Weighted recall measures report
how much gold standard information is present
in the summaries.
However, information overlap judgement is
not something humans seem to be good at, ei-
ther. Lin and Hovy (2002) show the instabil-
ity of the evaluation, expressed in system rank-
ings. They also examined those cases where
annotators incidentially had to judge a given
model?peer pair more than once (because differ-
ent systems returned the same ?peer? sentence).
In those cases, assessors agreed with their own
prior judgement in only 82% of the cases.
We propose a novel gold standard comparison
based on factoids. Identifying factoids in text is
a more objective task than judging information
overlap a` la DUC. Our annotation experiments
show high human agreement on the factoid an-
notation task. We believe this is due to the way
how factoids are defined, and due to our pre-
cise guidelines. The factoid measure also allows
quantification of the specific elements of infor-
mation overlap, rather than just giving a quan-
titative judgement expressed in percentages.
In an example from Lin and Hovy (2002), a
DUC assessor judged some content overlap be-
tween ?Thousands of people are feared dead?
and ?3,000 and perhaps ... 5,000 people have
been killed.? In our factoid representation, a
distinction between ?killed? and ?feared dead?
would be made, and different numbers of peo-
ple mentioned would have been differentiated.
Thus, the factoid approach can capture much
finer shades of meaning differentiation than
DUC-style information overlap does. Futher-
more, it can provide feedback to system builders
on the exact information their systems fail to
include or include superfluously.
We describe factoid analysis in section 2, a
method for comparison of the information con-
tent of different summaries of the same texts,
and describe our method for measuring agree-
ment and present results in section 3. We then
investigate the distribution of factoids across
the summaries in our data sets in section 4,
and define a weighted factoid score in section
5. In that section, we also perform stability ex-
periments, to test whether rankings of system
summaries remain stable if fewer than all sum-
maries which we have available are used, and
compare weighted factoid scores to other sum-
mary evaluation metrics.
2 Data and factoid annotation
We use two texts: a 600-word BBC report on
the killing of the Dutch politician Pim Fortuyn
(as used in van Halteren and Teufel (2003)),
which contains a mix of factual information and
personal reactions, and a 573-word article on
the Iraqi invasion of Kuwait (used in DUC-2002,
LA080290-0233).
For these two texts, we collected human writ-
ten generic summaries of roughly 100 words.
Our guidelines asked the human subjects to for-
mulate the summary in their own words, in or-
der to elicit different linguistic expressions for
the same information. Knowledge about the
variability of expression is important both for
evaluation and system building.
The Fortuyn text was summarised by 40
Dutch students1, and 10 NLP researchers (na-
tive or near-native English speakers), resulting
in a total of 50 summaries. For the Kuwait text,
1Another 20 summaries of the same source were re-
moved due to insufficient English or excessive length.
we used the 6 DUC-provided summaries, 17
ELSNET-02 student participants (7 summaries
removed), and summaries by 4 additional re-
searchers, resulting in a total of 20 summaries.
We use atomic semantic units called factoids
to represent the meaning of a sentence. For
instance, we represent the sentence ?The police
have arrested a white Dutch man? by the union of
the following factoids:
FP20 A suspect was arrested
FP21 The police did the arresting
FP24 The suspect is white
FP25 The suspect is Dutch
FP26 The suspect is male
Factoids are defined empirically based on the
data in the set of summaries we work with. Fac-
toid definition starts with the comparison of the
information contained in two summaries, and
gets refined (factoids get added or split) as in-
crementally other summaries are considered. If
two pieces of information occur together in all
summaries ? and within the same sentence ?
they are treated as one factoid, because dif-
ferentiation into more than one factoid would
not help us in distinguishing the summaries. In
our data, there must have been at least one
summary that contained either only FP25 or
only FP26 ? otherwise those factoids would have
been combined into a single factoid ?FP27 The
suspect is a Dutch man?. Factoids are labelled
with descriptions in natural language; initially,
these are close in wording to the factoid?s oc-
currence in the first summaries, though the an-
notator tries to identify and treat equally para-
phrases of the factoid information when they
occur in other summaries.
Our definition of atomicity implies that the
?amount? of information associated with one
factoid can vary from a single word to an en-
tire sentence. An example for a large chunk
of information that occurred atomically in our
texts was the fact that Fortuyn wanted to be-
come PM (FV71), a factoid which covers an en-
tire sentence. On the other hand, a single word
may break down into more than one factoids.
If (together with various statements in other
summaries) one summary contains ?was killed?
and another ?was shot dead?, we identify the
factoids
FA10 There was an attack
FA40 The victim died
FA20 A gun was used
The first summary contains only the first two
factoids, whereas the second contains all three.
That way, the semantic similarity between re-
lated sentences can be expressed.
When we identified factoids in our summary
collections, most factoids turned out to be in-
dependent of each other. But when dealing
with naturally occuring documents many dif-
ficult cases appear, e.g. ambiguous expressions,
slight differences in numbers and meaning, and
inference.
Another difficult phenomenon is attribution.
In both source texts, quotations of the reactions
of several politicians and officials are given, and
the subjects often generalised these reactions
and produced statements such as ?Dutch as well
as international politicians have expressed their grief
and disbelief.? Due to coordination of speak-
ers (in the subject) and coordination of reac-
tions (in the direct object), it is hard to ac-
curately represent the attribution of opinions.
We therefore introduce combinatorical factoids,
such as ? OG40 Politicians expressed grief? and
?OS62 International persons/organizations ex-
pressed disbelief? which can be combined with
similar factoids to express the above sentence.
We wrote guidelines (10 pages long) which
describe how to derive factoids from texts. The
guidelines cover questions such as: how to cre-
ate generalising factoids when numerical val-
ues vary (summaries might talk about ?200?,
?about 200? or ?almost 200 Kuwaitis were
killed?), how to create factoids dealing with at-
tribution of opinion, and how to deal with coor-
dination of NPs in subject position, cataphors
and other syntactic constructions. We believe
that written guidelines should contain all the
rules by which this process is done; this is the
only way that other annotators, who do not
have access to all the discussions the original an-
notators had, can replicate the annotation with
a high agreement. We therefore consider the
guidelines as one of the most valuable outcomes
of this exercise, and we will make them and our
annotated material generally available.
The advantage of our empirical, summary-
set-dependent definition of factoid atomicity is
that the annotation is more objective than if
factoids had to be invented by intuition of se-
mantic constructions from scratch. One possi-
ble disadvantage of our definition of atomicity
is that the set of factoids used may have to be
adjusted if new summaries are judged, as a re-
quired factoid might be missing, or an existing
one might require splitting. Using a large num-
ber of gold-standard summaries for the defini-
tion of factoids decreases the likelihood of this
happening.
3 Agreement
In our previous work, a ?definitive? list of fac-
toids was given (created by one author), and
we were interested in whether annotators could
consistently mark the text with the factoids con-
tained in this list. In the new annotation cycle
reported on here, we study the process of factoid
lists creation, which is more time-consuming.
We will discuss agreement in factoid annotation
first, as it is a more straightforward concept,
even though procedurally, factoids are first de-
fined (cf. section 3.2) and then annotated (cf.
section 3.1).
3.1 Agreement of factoid annotation
Assuming that we already have the right list of
factoids available, factoid annotation of a 100
word summary takes roughly 10 minutes, and
measuring agreement on the decision of assign-
ing factoids to sentences is relatively straight-
forward. We calculate agreement in terms of
Kappa, where the set of items to be classified are
all factoid?summary combinations (e.g. in the
case of Phase 1, N=153 factoids times 20 sen-
tences = 2920), and where there are two cate-
gories, either ?factoid is present in summary (1)?
or ?factoid is not present in summary (0)?. P(E),
probability of error, is calculated on the basis
of the distribution of the categories, whereas
P(A), probability of agreement, is calculated
as the average of observed to possible pairwise
agreements per item. Kappa is calculated as
k = P (A)?P (E)1?P (E) ; results for our two texts are
given in Figure 1.
We measure agreement at two stages in
the process: entirely independent annotation
(Phase 1), and corrected annotation (Phase 2).
In Phase 2, annotators see an automatically
generated list of discrepancies with the other
annotator, so that slips of attention can be cor-
rected. Crucially, Phase 2 was conducted with-
out any discussion. After Phase 2 measurement,
discussion on the open points took place and a
consensus was reached (which is used for the
experiments in the rest of the paper).
Figure 1 includes results for the Fortuyn text
as we have factoid?summary annotations by
both annotators for both texts. The Kappa fig-
ures indicate high agreement, even in Phase 1
(K=.87 and K=.86); in Phase 2, Kappas are as
high as .89 and .95. Note that there is a dif-
ference between the annotation of the Fortuyn
and the Kuwait text: in the Fortuyn case, there
was no discussion or disclosure of any kind in
Phase 1; one author created the factoids, and
both used this list to annotate. The agreement
of K=.86 was thus measured on entirely inde-
pendent annotations, with no prior communica-
tion whatsoever. In the case of the Kuwait text,
the prior step of finding a consensus factoid list
had already taken place, including some discus-
sion.
Fortuyn text
K N k n P(A) P(E)
Phase 1 .86 14178 2 2 .970 .787
Phase 2 .95 14178 2 2 .989 .779
Kuwait text
K N k n P(A) P(E)
Phase 1 .87 3060 2 2 .956 .670
Phase 2 .89 2940 2 2 .962 .663
Figure 1: Agreement of factoid annotation.
3.2 Agreement of factoid definition.
We realised during our previous work, where
only one author created the factoids, that the
task of defining factoids is a complicated pro-
cess and that we should measure agreement on
this task too (using the Kuwait text). Thus,
we do not have this information for the Fortuyn
text.
But how should the measurement of agree-
ment on factoid creation proceed? It is diffi-
cult to find a fair measure of agreement over set
operations like factoid splitting, particularly as
the sets can contain a different set of summaries
marked for each factoid. For instance, consider
the following two sentences: (1) M01-004 Sad-
dam Hussein said ... that they will leave the
country when the situation stabilizes. and (2)
M06-004 Iraq claims it ... would withdraw soon.
One annotator created a factoid ?(P30) Sad-
dam H/Iraq will leave the country soon/when
situation stabilises? whereas the other anno-
tator split this into two factoids (F9.21 and
F9.22). Note that the annotators use their own,
independently chosen factoid names.
Our procedure for annotation measurement is
as follows. We create a list of identity and sub-
sumption relations between factoids by the two
annotators. In the example above, P30 would
be listed as subsuming F9.21 and F9.22. It is
time-consuming but necessary to create such
a list, as we want to measure agreement only
amongst those factoids which are semantically
related. We use a program which maximises
shared factoids between two summary sentences
A1 A2 A1 A2
P30 ? F9.21 ? a 1 1 P30 ? F9.22 ? a 1 0
P30 ? F9.21 ? b 0 0 P30 ? F9.22 ? b 0 0
P30 ? F9.21 ? c 1 0 P30 ? F9.22 ? c 1 1
P30 ? F9.21 ? d 0 0 P30 ? F9.22 ? d 0 0
P30 ? F9.21 ? e 1 0 P30 ? F9.22 ? e 1 1
Figure 2: Items for kappa calculation.
to suggest such identities and subsumption re-
lations.
We then calculate Kappa at Phases 1 and 2.
It is not trivial to define what an ?item? in
the Kappa calculation should be. Possibly
the use of Krippendorff?s alpha will provide
a better approach (cf. Nenkova and Passon-
neau (2004)), but for now we measure using
the better-known kappa, in the following way:
For each equivalence between factoids A and
C, create items { A ? C ? s | s ? S } (where
S is the set of all summaries). For each fac-
toid A subsumed by a set B of factoids, create
items { A ? b ? s | b ? B, s ? S}. For exam-
ple, given 5 summaries a, b, c, d, e, Annotator
A1 assigns P30 to summaries a, c and e. An-
notator A2 (who has split P30 into F9.21 and
F9.22), assigns a to F9.21 and c and e to F9.22.
This creates the 10 items for Kappa calculation
given in Figure 2.
Results for our data set are given in Figure 3.
For Phase 1 of factoid definition, K=.7 indicates
relatively good agreement (but lower than for
the task of factoid annotation). Many of the
disagreements can be reduced to slips of atten-
tion, as the increased Kappa of .81 for Phase 2
shows.
Overall, we can observe that this high agree-
ment for both tasks points to the fact that the
task can be robustly performed in naturally oc-
curring text, without any copy-editing. Still,
from our observations, it seems that the task
of factoid annotation is easier than the task of
factoid definition.
Kuwait text
K N k n P(A) P(E)
Phase 1 .70 3560 2 2 .91 .69
Phase 2 .81 3240 2 2 .94 .67
Figure 3: Agreement of factoid definition.
One of us then used the Kuwait consensus
agreement to annotate the 16 machine sum-
maries for that text which were created by dif-
ferent participants in DUC-2002, an annotation
which could be done rather quickly. However, a
small number of missing factoids were detected,
for instance the (incorrect) factoid that Saudi
Arabia was invaded, that the invasion happened
on a Monday night, and that Kuwait City is
Kuwait?s only sizable town. Overall, the set of
factoids available was considered adequate for
the annotation of these new texts.
0 10 20 30 40 50
0
50
10
0
15
0
20
0
25
0
Number of summaries
Av
er
ag
e 
nu
m
be
r o
f f
ac
to
id
s
Figure 4: Average size of factoid inventory as a
function of number of underlying summaries.
4 Growth of the factoid inventory
The more summaries we include in the analy-
sis, the more factoids we identify. This growth
of the factoid set stems from two factors. Dif-
ferent summarisers select different information
and hence completely new factoids are intro-
duced to account for information not yet seen
in previous summaries. This factor also implies
that the factoid inventory can never be complete
as summarisers sometimes include information
which is not actually in the original text. The
second factor comes from splitting: when a new
summary is examined, it often becomes neces-
sary to split a single factoid into multiple fac-
toids because only a certain part of it is included
in the new summary. After the very first sum-
mary, each factoid is a full sentence, and these
are gradually subdivided.
In order to determine how many factoids exist
in a given set of N summaries, we simulate ear-
lier stages of the factoid set by automatically re?
merging those factoids which never occur apart
within the given set of summaries.
Figure 4 shows the average number of factoids
over 100 drawings of N different summaries from
the whole set, which grows from 1.0 to about 4.5
for the Kuwait text (long curve) and about 4.1
for the Fortuyn text (short curve). The Kuwait
curve shows a steeper incline, possibly due to
the fact that the sentences in the Kuwait text
are longer. Given the overall growth for the
total number of factoids and the number of fac-
toids per sentence, it would seem that the split-
ting factors and the new information factor are
equally productive.
Neither curve in Figure 4 shows signs that it
might be approaching an assymptote. This con-
firms our earlier conclusion (van Halteren and
Teufel, 2003) that many more summaries than
10 or 20 are needed for a full factoid inventory.2
5 Weighted factoid scores and
stability
The main reason to do factoid analysis is to
measure the quality of summaries, including
machine summaries. In our previous work, we
do this with a consensus summary. We are now
investigating different weighting factors for the
importance of factoids. Previously, the weight-
ing factors we suggested were information con-
tent, position in the summaries and frequency.
We investigated the latter two.
Each factoid we find in a summary to be eval-
uated contributes to the score of the summary,
by an amount which reflects the perceived value
of the factoid, what we will call the ?weighted
factoid score (WFS)?. The main component in
this value is frequency, i.e., the number of model
summaries in which the factoid is observed.
When frequency weighting is used by itself,
each factoid occurrence is worth one.3 We could
also assume that more important factoids are
placed earlier in a summary, and that the fre-
quency weight is adjusted on the basis of po-
sition. Experimentation is not complete, but
the adjustments appear to influence the rank-
ing only slightly. The results we present here
are those using pure frequency weights.
We noted in our earlier paper that a good
quality measure should demonstrate at least the
following properties: a) it should reward inclu-
sion in a summary of the information deemed
2It should be noted that the estimation in Figure 4
improves upon the original estimation in that paper, as
the determination of number of factoids for that figure
did not consider the splitting factor, but just counted
the number of factoids as taken from the inventory at its
highest granularity.
3This is similar to the relative utility measure intro-
duced by Radev and Tam (2003), which however oper-
ates on sentences rather than factoids. It also corre-
sponds to the pyramid measure proposed by Nenkova
and Passonneau (2004), which also considers an estima-
tion of the maximum value reachable. Here, we use no
such maximum estimation as our comparisons will all be
relative.
1 3 5 7 9 12 15 18 21 24 27 30 33 36 39 42 45 48
?0
.2
0.0
0.2
0.4
0.6
0.8
1.0
Number of summaries (N) that score is based on
Ra
nk
ing
 co
rre
lat
ion
 be
tw
ee
n t
wo
 sa
mp
lin
gs
 (a
llo
win
g r
ep
ea
ts)
 of
 N
 su
mm
ari
es
Figure 5: Correlation (Spearman?s ?) between
summary rankings on the basis of two different
sets of N summaries, for N between 1 and 50.
most important in the document and b) mea-
sures based on two factoid analyses constructed
along the same lines should lead to the same,
or at least very similar, ranking of a set of sum-
maries which are evaluated. Since our measure
rewards inclusion of factoids which are men-
tioned often and early, demand a) ought to be
satisfied by construction.
For demand b), some experimentation is in
order. For various numbers of summaries N,
we take two samples of N summaries from the
whole set (allowing repeats so that we can use N
larger than the number of available summaries;
a statistical method called ?bootstrap?). For
each sample in a pair, we use the weighted fac-
toid score with regard to that sample of N sum-
maries to rank the summaries, and then deter-
mine the ranking correlation (Spearman?s ?) be-
tween the two rankings. The summaries that we
rank here are the 20 human summaries of the
Kuwait text, plus 16 machine summaries sub-
mitted for DUC-2002.
Figure 5 shows how the ranking correlation
increases with N for the Kuwait text. Its mean
value surpasses 0.8 at N=11 and 0.9 at N=19.
At N=50, it is 0.98. What this means for the
scores of individual summaries is shown in Fig-
ure 6, which contains a box plot for the scores
for each summary as observed in the 200 draw-
ings for N=50. The high ranking correlation
and the reasonable stability of the scores shows
that our measure fulfills demand b), at least at
a high enough N. What could be worrying is
the fact that the machine summaries (right of
the dotted line) do not seem to be performing
significantly worse than the human ones (left
Submitted summaries (Human | Machine)
Sc
ore
 ba
se
d o
n 5
0 s
um
ma
rie
s, 
wit
h f
req
ue
nc
y w
eig
hti
ng
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Figure 6: Variation in summary scores in eval-
uations based on 200 different sets of 50 model
summaries.
Submitted summaries (Human | Machine)
Sc
ore
 ba
se
d o
n 1
0 s
um
ma
rie
s, 
wit
h f
req
ue
nc
y w
eig
hti
ng
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Figure 7: Variation in summary scores in eval-
uations based on 200 different sets of 10 model
summaries.
of the line). However, an examination of the
better scoring machine summaries show that in
this particular case, their information content
is indeed good. The very low human scores ap-
pear to be cases of especially short summaries
(including one DUC summariser) and/or sum-
maries with a deviating angle on the story.
It has been suggested in DUC circles that a
lower N should suffice. That even a value as
high as 10 is insufficient is already indicated by
the ranking correlation of only 0.76. It becomes
even clearer with Figure 7, which mirrors Figure
6 but uses N=10. The scores for the summaries
vary wildly, which means that ranking is almost
random.
Of course, the suggestion might be made that
the system ranking will most likely also be sta-
bilised by scoring summaries for more texts,
even with such a low (or even lower) N per text.
However, in that case, the measure only yields
information at the macro level: it merely gives
an ordering between systems. A factoid-based
measure with a high N also yields feedback on a
micro level: it can show system builders which
vital information they are missing and which
superfluous information they are including. We
expect this feedback only to be reliable at the
same order of N at which single-text-based scor-
ing starts to stabilise, i.e. around 20 to 30.
As the average ranking correlation between
two weighted factoid score rankings based on
20 summaries is 0.91, we could assume that
the ranking based on our full set of 20 differ-
ent summaries should be an accurate ranking.
If we compare it to the DUC information over-
lap rankings for this text, we find that the indi-
vidual rankings for D086, D108 and D110 have
correlations with our ranking of 0.50, 0.64 and
0.79. When we average over the three, this goes
up to 0.83.
In van Halteren and Teufel (2003), we com-
pared a consensus summary based on the top-
scoring factoids with unigram scores. For the 50
Fortuyn summaries, we calculate the F-measure
for the included factoids with regard to the con-
sensus summary. In a similar fashion, we build
a consensus unigram list, containing the 103
unigrams that occur in at least 11 summaries,
and calculate the F-measure for unigrams. The
correlation between those two scores was low
(Spearman?s ? = 0.45). We concluded from
this experiment that unigrams, though much
cheaper, are not a viable substitute for factoids.
6 Discussion and future work
We have presented a new information-based
summarization metric called weighted factoid
score, which uses multiple summaries as gold
standard and which measures information over-
lap, not string overlap. It can be reliably and
objectively annotated in arbitrary text, which is
shown by our high values for human agreement.
We summarise our results as follows: Factoids
can be defined with high agreement by indepen-
dently operating annotators in naturally occur-
ring text (K=.70) and independently annotated
with even higher agreement (K=.86 and .87).
Therefore, we consider the definition of factoids
intuitive and reproducible.
The number of factoids found if new sum-
maries are considered does not tail off, but
weighting of factoids by frequency and/or lo-
cation in the summary allows for a stable sum-
mary metric. We expect this can be improved
further by including an information content
weighting factor.
If single summaries are used as gold standard
(as many other summarization evaluations do),
the correlation between rankings based on two
such gold standard summaries can be expected
to be low; in our two experiments, the correla-
tions were ?=0.20 and 0.48 on average. Accord-
ing to our estimations, stability with respect
to the factoid scores can only be expected if
a larger number of summaries are collected (in
the range of 20?30 summaries).
System rankings based on the factoid score
shows only low correlation with rankings based
on a) DUC-based information overlap, and
b) unigrams, a measurement based on shared
words between gold standard summaries and
system summary. As far as b) is concerned,
this is expected, as factoid comparison abstracts
over wording and captures linguistic variation
of the same meaning. However, the ROUGE
measure currently in development is considering
various n-grams and Wordnet-based paraphras-
ing options (Lin, personal communication). We
expect that this measure has the potential for
better ranking correlation with factoid ranking,
and we are currently investigating this.
We also plan to expand our data sets to more
texts, in order to investigate the presence and
distribution of factoids, types of factoids and re-
lations between factoids in summaries and sum-
mary collections. Currently, we have two large
factoid-annotated data sets with 20 and 50 sum-
maries, and a workable procedure to annotate
factoids, including guidelines which were used
to achieve good agreement. We now plan to
elicit the help of new annotators to increase our
data pool.
Another pressing line of investigation is re-
ducing the cost of factoid analysis. The first rea-
son why this analysis is currently expensive is
the need for large summary bases for consensus
summaries. Possibly this can be circumvented
by using larger numbers of different texts, as is
the case in IR and in MT, where discrepancies
prove to average out when large enough datasets
are used. The second reason is the need for
human annotation of factoids. Although sim-
ple word-based methods prove insufficient, we
expect that existing and emerging NLP tech-
niques based on deeper processing might help
with automatic factoid identification.
All in all, the use of factoid analysis and
weighted factoid score, even though initially ex-
pensive to set up, provides a promising alterna-
tive which could well bring us closer to a solu-
tion to several problems in summarisation eval-
uation.
References
DUC. 2002. Document Understanding Con-
ference (DUC). Electronic proceedings,
http://www-nlpir.nist.gov/projects/duc/
pubs.html.
Jing, H., R. Barzilay, K. R. McKeown, and M. El-
hadad. 1998. Summarization Evaluation Meth-
ods: Experiments and Analysis. In Working Notes
of the AAAI Spring Symposium on Intelligent
Text Summarization, 60?68.
Lin, C., and E. Hovy. 2002. Manual and automatic
evaluation of summaries. In DUC 2002.
Mani, I. 2001. Automatic Summarization. John Ben-
jamins.
Mani, I., T. Firmin, D. House, G. Klein, B. Sund-
heim, and L. Hirschman. 1999. The TIPSTER
Summac Text Summarization Evaluation. In Pro-
ceedings of EACL-99 , 77?85.
Nenkova, A., and R. Passonneau. 2004. Evaluating
Content Selection in Summarization: the Pyra-
mid Method. In Proceedings of NAACL/HLT-
2003 .
Papineni, K, S. Roukos, T Ward, and W-J. Zhu.
2001. Bleu: a method for automatic evaluation of
machine translation. In Proceedings of ACL-02 ,
311?318.
Radev, D., and D. Tam. 2003. Summarization eval-
uation using relative utility. In Proceedings of the
Twelfth International Conference on Information
and Knowledge Management , 508?511.
Rath, G.J, A. Resnick, and T. R. Savage. 1961. The
Formation of Abstracts by the Selection of Sen-
tences. American Documentation 12(2): 139?143.
Spa?rck Jones, K. 1999. Automatic Summarising:
Factors and Directions. In I. Mani and M. May-
bury, eds., Advances in Automatic Text Summa-
rization, 1?12. Cambridge, MA: MIT Press.
van Halteren, H., and S. Teufel. 2003. Examining
the consensus between human summaries: initial
experiments with factoid analysis. In Proceedings
of the HLT workshop on Automatic Summariza-
tion.
Voorhees, E. 2000. Variations in relevance judge-
ments and the measurement of retrieval effective-
ness. Information Processing and Management
36: 697?716.
Proceedings of the Workshop on How Can Computational Linguistics Improve Information Retrieval?, pages 25?32,
Sydney, July 2006. c?2006 Association for Computational Linguistics
How to Find Better Index Terms Through Citations
Anna Ritchie
University of Cambridge
Computer Laboratory
15 J J Thompson Avenue
Cambridge, CB3 0FD, U.K.
ar283@cl.cam.ac.uk
Simone Teufel
University of Cambridge
Computer Laboratory
15 J J Thompson Avenue
Cambridge, CB3 0FD, U.K.
sht25@cl.cam.ac.uk
Stephen Robertson
Microsoft Research Ltd
Roger Needham House
7 J J Thomson Avenue
Cambridge, CB3 0FB, U.K.
ser@microsoft.com
Abstract
We consider the question of how informa-
tion from the textual context of citations
in scientific papers could improve index-
ing of the cited papers. We first present ex-
amples which show that the context should
in principle provide better and new index
terms. We then discuss linguistic phenom-
ena around citations and which type of
processing would improve the automatic
determination of the right context. We
present a case study, studying the effect
of combining the existing index terms of
a paper with additional terms from papers
citing that paper in our corpus. Finally, we
discuss the need for experimentation for
the practical validation of our claim.
1 Introduction
Information Retrieval (IR) is an established field
and, today, the ?conventional? IR task is embodied
by web searching. IR is mostly term-based, re-
lying on the words within documents to describe
them and, thence, try to determine which docu-
ments are relevant to a given user query. There are
theoretically motivated and experimentally vali-
dated techniques that have become standard in the
field. An example is the Okapi model; a prob-
abilistic function for term weighting and docu-
ment ranking (Spa?rck Jones, Walker & Robertson
2000). IR techniques using such statistical mod-
els almost always outperform more linguistically
based ones. So, as statistical models are developed
and refined, it begs the question ?Can Computa-
tional Linguistics improve Information Retrieval??
Our particular research involves IR on scien-
tific papers. There are definite parallels between
the web and scientific literature, such as hyper-
links between webpages alongside citation links
between papers. However, there are also funda-
mental differences, like the greater variability of
webpages and the independent quality control of
academic texts through the peer review process.
The analogy between hyperlinks and citations it-
self is not perfect: whereas the number of hyper-
links varies greatly from webpage to webpage, the
number of citations in papers is more constrained,
due to the combination of strict page limits, the
need to cite to show awareness of other work and
the need to conserve space by including only the
most relevant citations. Thus, while some aspects
of web-based techniques will carry across to the
current research domain, others will probably not.
We are interested in investigating which lessons
learned from web IR can successfully be applied
to this slightly different domain.
2 Index Terms Through Link Structure
We aim to improve automatic indexing of scien-
tific papers by finding additional index terms out-
side of the documents themselves. In particular,
we believe that good index terms can be found by
following the link structure between documents.
2.1 Hyperlinks
There is a wealth of literature on exploiting link
structure between web documents for IR, includ-
ing the ?sharing? of index terms between hyper-
linked pages. Bharat & Mihaila (2001), for in-
stance, propagate title and header terms to the
pointed-to page, while Marchiori (1997) recur-
sively augments the textual content of a page with
all the text of the pages it points to.
Research has particularly concentrated on an-
chor text as a good place to find index terms, i.e.,
25
the text enclosed in the ?a? tags of the HTML
document. It is a well-documented problem that
webpages are often poorly self-descriptive (e.g.,
Brin & Page 1998, Kleinberg 1999). For in-
stance, www.google.com does not contain the
phrase search engine. Anchor text, on the other
hand, is often a higher-level description of the
pointed-to page. Davison (2000) provides a good
discussion of just how well anchor text does this
and provides experimental results to back this
claim. Thus, beginning with McBryan (1994),
there is a trend of propagating anchor text along
its hyperlink to associate it with the linked page,
as well as that in which it is found. Google, for
example, includes anchor text as index terms for
the linked page (Brin & Page 1998).
Extending beyond anchor text, Chakrabarti
et al (1998) look for topic terms in a window
of text around hyperlinks and weight that link ac-
cordingly, in the framework of a link structure al-
gorithm, HITS (Kleinberg 1999).
2.2 Citations
The anchor text phenomenon is also observed with
citations: they are introduced purposefully along-
side some descriptive reference to the cited doc-
ument. Thus, this text should contain good in-
dex terms for the cited document. In the fol-
lowing sections, we motivate the use of reference
terms as index terms for cited documents, firstly,
with some citation examples and, secondly, by dis-
cussing previous work.
Examples: Reference Terms as Index Terms
Figure 1 shows some citations that exemplify
why reference terms should be good index terms
for the cited document. (1) is an example of a ci-
tation with intuitively good index terms (those un-
derlined) for the cited paper around it; a searcher
looking for papers about a learning system, partic-
ularly one that uses theory renement and/or one
that learns non-recursive NP and VP structures
might be interested in the paper, as might those
searching for information about ALLiS.
The fact that an author has chosen those partic-
ular terms in referring to the paper means that they
reflect what that author feels is important about the
paper. It is reasonable, then, that other researchers
interested in the same things would find the cited
paper useful and could plausibly use such terms
as query terms. It is true that the cited paper may
well contain these terms, and they may even be
important, prominent terms, but this is not neces-
sarily the case. There are numerous situations in
which the terms in the document are not the best
indicators of what is important in it. Firstly, what
is important in a paper in terms of what it is known
and cited for is not always the same as what is
important in it in terms of subject matter or fo-
cus. Secondly, what are considered to be the im-
portant contributions of a paper may change over
time. Thirdly, the terminology used to describe the
important contributions may be different from that
used in the paper or may change over time.
(2) exemplifies this special case, where a paper
is referred to using terms that are not in the paper
itself: the cited paper is the standard reference for
the HITS algorithm yet the name HITS was only
attributed to the algorithm after the paper was writ-
ten and it doesn?t contain the term at all1.
The last two examples show how citing au-
thors can provide higher level descriptions of the
cited paper, e.g., good overview and comparison.
These meta-descriptors are less likely to appear
in the papers themselves as prominent terms yet,
again, could plausibly be used as query terms for
a searcher.
Reference Directed Indexing
These examples (and many more) suggest that
text used in reference to papers can provide use-
ful index terms, just as anchor text does for web-
pages. Bradshaw & Hammond (2002) even go so
far as to argue that reference is more valuable as
a source of index terms than the document?s own
content. Bradshaw?s theory is that, when citing,
authors describe a document in terms similar to a
searcher?s query for the information it contains.
However, there is no anchor text, per se, in pa-
pers, i.e., there are no HTML tags to delimit the
text associated with a citation, unlike in webpages.
The question is raised, therefore, of what is the
anchor text equivalent for formal citations. Brad-
shaw (2003) extracts NPs from a fixed window of
around one hundred words around the citation and
uses these as the basis of his Reference-Directed
Indexing (RDI).
Bradshaw evaluates RDI by, first, indexing doc-
uments provided by Citeseer (Lawrence, Bol-
lacker & Giles 1999). A set of 32 queries was cre-
ated by randomly selecting keyword phrases from
1There is a poetic irony in this: Kleinberg?s paper notes
the analagous problem of poorly self-descriptive webpages.
26
(1) ALLiS (Architecture for Learning Linguistic Structures) is a learning system which uses
theory renement in order to learn non-recursive NP and VP structures (Dejean, 2000).
(2) Such estimation is simplied from HITS algorithm (Kleinberg, 1998).
(3) As two examples, (Rabiner, 1989) and (Charniak et al, 1993) give
good overviews of the techniques and equations used for Markov models and part-of-speech tagging,
but they are not very explicit in the details that are needed for their application.
(4) For a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999).
Figure 1: Citations Motivating Reference Index Terms
24 documents in the collection with an author-
written keywords section. Document relevance
was determined by judging whether it addressed
the same topic as the topic in the query source
paper that is identified by the query keywords.
Thus, the performance of RDI was compared to
that of a standard vector-space model implementa-
tion (TF*IDF term weighting and cosine similarity
retrieval), with RDI achieving better precision at
top 10 documents (0.484 compared to 0.318, sta-
tistically significant at 99.5% confidence).
Citing Statements
In a considerably earlier study, closer to our
own project, O?Connor (1982) motivated the use
of words from citing statements as additional
terms to augment an existing document represen-
tation. Though O?Connor did not have machine-
readable documents, procedures for ?automatic?
recognition of citing statements were developed
and manually carried out on a collection of chem-
istry journal articles.
Proceeding from the sentence in which a ci-
tation is found, a set of hand-crafted, mostly
sentence-based rules were applied to select the
parts of the citing paper that conveyed informa-
tion about the cited paper. For instance, the citing
sentence, S, was always selected. If S contained a
connector (a keyword, e.g., this, similarly, former)
in its first twelve words, its predecessor, S?1, wasalso selected etc. The majority of rules selected
sentences from the text; others selected titles and
words from tables, figures and captions.
The selected statements (minus stop words)
were added to an existing representation for the
cited documents, comprising human index terms
and title and abstract terms, and a small-scale re-
trieval experiment was performed. A 20% in-
crease in recall was found using the citing state-
ments in addition to the existing index terms,
though in a follow-up study on biomedical papers,
the increase was only 4%2 (O?Connor 1983).
O?Connor concludes that citing statements can
aid retrieval but notes the inherent difficulty in
identifying them. Some of the selection rules were
only semi-automatic (e.g., required human identi-
fication of an article as a review) and most relied
on knowledge of sentence boundaries, which is a
non-trivial problem in itself. In all sentence-based
cases, sentences were either selected in their en-
tirety or not at all and O?Connor notes this as a
source of falsely assigned terms.
3 Complex Citation Contexts
There is evidence, therefore, that good index terms
for scholarly documents can be found in the doc-
uments that cite them. Identifying which terms
around a citation really refer to it, however, is non-
trivial. In this section, we discuss some exam-
ples of citations where this is the case and propose
potential ways in which computational linguistics
techniques may be useful in more accurately lo-
cating those reference terms. We take as our theo-
retical baseline all terms in a fixed window around
a citation.
3.1 Examples: Finding Reference Terms
The first two examples in Figure 2 illustrate how
the amount of text that refers to a citation can vary.
Sometimes, only two or three terms will refer to a
citation, as is often the case in enumerations such
as (5). On the other hand, (6) shows a citation
where much of the following section refers to the
cited work. When a paper is heavily based on pre-
vious work, for example, extensive text may be af-
forded to describing that work in detail. Thus, this
context could contribute dozens of legitimate in-
dex terms. A fixed size window around a citation
2O?Connor attributes this to a lower average number of
citing papers in the biomedical domain.
27
(5) Similar advances have been made in machine translation (Frederking and Nirenburg, 1994),
speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al, 1998).
(6) Brown et al (1993) proposed a series of statistical models of the translation process.
IBM translation models try to model the translation probability ... which describes the relationship
between a source language sentence ... and a target language sentence ... . In
statistical alignment models ... a ?hidden? alignment ... is introduced, which describes a mapping
from a target position ... to a source position ... . The relationship between the translation model
and the alignment model is given by: ...
(7) The results of disambiguation strategies reported for pseudo-words and the like are consistently
above 95% overall accuracy, far higher than those reported for
disambiguating three or more senses of polysemous words (Wilks et al 1993; Leacock, Towell,
and Voorhees 1993).
(8) This paper concentrates on the use of zero, pronominal, and nominal anaphora in Chinese
generated text. We are not concerned with lexical anaphora (Tutin and Kittredge 1992) where the
anaphor and its antecedent share meaning components, while the anaphor belongs to an open
lexical class.
(9) Previous work on the generation of referring expressions focused on
producing minimal distinguishing descriptions (Dale and Haddock 1991; Dale 1992; Reiter and
Dale 1992) or descriptions customized for different levels of hearers (Reiter 1990). Since we are
not concerned with the generation of descriptions for different levels of users, we look only at the
former group of work, which aims at generating descriptions for a subsequent reference to
distinguish it from the set of entities with which it might be confused.
(10) Ferro et al (1999) and Buchholz et al (1999) both describe learning systems to nd GRs. The
former (TR) uses transformation-based error-driven learning (Brill and Resnik, 1994) and the
latter (MB) uses memory-based learning (Daelemans et al, 1999).
Figure 2: Citations Motivating Computational Linguistics
would not capture all the terms referring to it and
only those.
In list examples such as (5), where multiple ci-
tations are in close proximity, almost any window
size would result in overlapping windows and in
terms being attributed to the wrong citation(s), as
well as the right one. In such examples, the pres-
ence of other citations indicates a change in refer-
ence term ?ownership?. The same is often true of
sentence boundaries, as they often signal a change
in topic. Citations frequently occur at the start of
sentences, as in (6), where a different approach is
introduced. Similarly, a citation at the end of a
sentence, as in (7), often indicates the completion
of the current topic. In both cases, the sentence
boundary (c.f. topic change) is also the boundary
of the reference text. The same arguments increas-
ingly apply to paragraph and section boundaries.
(8) is another example where the reference text
does not extend beyond the citation sentence,
though the citation is not at a sentence boundary.
Instead, the topic contrast is indicated by a linguic-
tic cue, i.e., the negation in We are not. This il-
lustrates another phenomenon of citations: in con-
trasting their work with others?, researchers often
explicitly state what their paper is not about. Intu-
itively, not only are these terms better descriptors
of the cited rather than citing paper, they might
even raise the question of whether one should go
as far as excluding selected terms during index-
ing of the citing paper. We are not advocating this
here, though, and note that, in practice, such terms
would not have much impact on the document: we
would expect them to have low term frequencies
in comparison to the important terms in that doc-
ument and in comparison to their frequencies in
other documents where they are important.
(9) is another example of this negation effect
(We are not concerned with...). Along with (10),
it also shows how complex the mapping between
reference terms and citations can be. Firstly, ref-
erence terms may belong to more than one cita-
28
tion. For instance, in (10), describe learning sys-
tems to nd GRs refers to both Ferro et al (1999)
and Buchholz et al (1999). Here, the presence of
a second citation does not end the domain of the
first?s reference text, indicated by the use of both
and the conjunction between the citations. Simi-
larly, transformation-based error-driven learning
also refers to two citations but, in this case, they
are on opposite sides of the reference text, i.e.,
Ferro et al (1999) and (Brill and Resnik, 1994).
Moreover, there is an intervening citation that it
does not refer to, i.e., Buchholz et al (1999). The
same is true of memory-based learning.
4 Case Study
In this section, we study the effect of adding ci-
tation index terms to one document: The Mathe-
matics of Statistical Machine Translation: Param-
eter Estimation from the Computational Linguis-
tics journal3 . Our experimental setting is a corpus
of ?9000 papers in the ACL Anthology4 , a digital
archive of computational linguistics research pa-
pers. We found 24 citations to the paper in 10 other
Anthology papers (that we knew to have citations
to this paper through an unrelated study). As a
simulation of ideal processing, we then manually
extracted the terms from those around those cita-
tions that specifically referred to the paper, hence-
forth ideal reference terms. Next, we extracted
all terms from a fixed window of ?50 terms on
either side (equivalent to Bradshaw (2003)?s win-
dow size), henceforth xed reference terms. Fi-
nally, we calculated various term statistics, includ-
ing IDF values across the corpus. All terms were
decapitalized. We now attempt to draw a ?term
profile? of the document, both before and after
those reference terms are added to the document,
and discuss the implications for IR.
4.1 Index Term Analysis
Table 1 gives the top twenty ideal reference terms
ranked by their TF*IDF values in the original doc-
ument. Note that we observe the effects on the
relative rankings of the ideal reference terms only,
since it is these hand-picked terms that we con-
sider to be important descriptors for the document
and whose statistics will be most affected by the
inclusion of reference terms. To give an indication
of their importance relative to other terms in the
3http://www.aclweb.org/anthology/J93-2003.pdf
4http://www.aclweb.org/anthology/
Rank
Ideal Doc TF*IDF Term
1 1 351.73 french
2 2 246.52 alignments
3 3 238.39 fertility
4 4 212.20 alignment
5 5 203.28 cept
6 8 158.45 probabilities
7 9 150.74 translation
8 12 106.11 model
9 17 79.47 probability
10 18 78.37 models
11 19 78.02 english
12 21 76.23 parameters
13 24 71.77 connected
14 28 62.48 words
15 32 57.57 em
13 35 54.88 iterations
14 45 45.00 statistical
15 54 38.25 training
16 69 32.93 word
17 74 31.31 pairs
18 81 29.29 machine
19 83 28.53 empty
20 130 19.72 series
Table 1: Ideal Reference Term Ranking by
TF*IDF
document, however, the second column in Table 1
gives the absolute rankings of these terms in the
original document. These numbers confirm that
our ideal reference terms are, in fact, relatively im-
portant in the document; indeed, the top five terms
in the document are all ideal reference terms. Fur-
ther down the ranking, the ideal reference terms
become more ?diluted? with terms not picked from
our 24 citations. An inspection revealed that many
of these terms were French words from example
translations, since the paper deals with machine
translation between English and French. Thus,
they were bad index terms, for our purposes.
Hence, we observed the effect of adding, first,
the ideal reference terms then, separately, the fixed
reference terms to the document, summarized in
Tables 2 to 5. Tables 2 and 3 show the terms with
the largest differences in positions as a result of
adding the ideal and fixed reference terms respec-
tively.
For instance, ibm?s TF*IDF value more than
doubled. The term ibm appears only six times in
the document (and not even from the main text
but from authors? institutions and one bibliogra-
phy item) yet one of its major contributions is
the machine translation models it introduced, now
standardly referred to as ?the IBM models?. Con-
29
TF*IDF Ideal
Term ? Doc+ideal Rank ?
ibm 24.24 37.46 28? 20
generative 4.44 11.10 38? 33
source 5.35 6.42 65? 44
decoders 6.41 6.41 ? 45
corruption 6.02 6.02 ? 46
expectation 2.97 5.94 51? 47
relationship 2.96 5.92 52? 48
story 2.94 5.88 53? 49
noisy-channel 5.75 5.75 ?52
extract 1.51 7.54 41? 38
Table 2: Term Ranking Changes (Ideal)
TF*IDF Ideal
Term ? Doc+fixed Rank ?
ibm 48.48 61.70 28? 18
target 19.64 19.64 ? 26
source 14.99 16.06 65? 32
phrase-based 14.77 14.77 ? 36
trained 14.64 19.52 43? 27
approaches 11.03 11.03 ? 41
parallel 9.72 17.81 34? 29
generative 8.88 15.54 38? 33
train 8.21 8.21 ? 45
channel 6.94 6.94 ? 55
expectation 5.93 8.90 51? 44
learn 5.93 7.77 60? 47
Table 3: Term Ranking Changes (Fixed)
sequently, ?IBM? was contained in many citation
contexts in citing papers, leading to an ideal ref-
erence term frequency of 11 for ibm. As a result,
ibm is boosted eight places to rank 20. This exem-
plifies how reference terms can better describe a
document, in terms of what searchers might plau-
sibly look for (c.f. Example 2).
There were twenty terms that do not occur
in the document itself but are nevertheless used
by citing authors to describe it, shown in Ta-
bles 4 and 5. Many of these have high IDF val-
ues, indicating their distinctiveness in the corpus,
e.g., decoders (6.41), corruption (6.02) and noisy-
channel (5.75). This, combined with the fact that
citing authors use these terms in describing the
paper, means that these terms are intuitively high
quality descriptors of the paper. Without the refer-
ence index terms, however, the paper would score
zero for these terms as query terms.
Many more fixed reference terms were found
per citation than ideal ones. This can introduce
noise. In general, the TF*IDF values of ideal ref-
erence terms can only be further boosted by in-
cluding more terms and a comparison of Tables 2
Term TF*IDF
decoders 6.41
corruption 6.02
noisy-channel 5.75
attainable 5.45
target 5.24
source-language 4.99
phrase-based 4.92
target-language 4.82
application-specific 4.40
train 4.10
intermediate 4.01
channel 3.47
approaches 3.01
combinations 1.70
style 2.12
add 1.32
major 1.16
due 0.83
considered 0.81
developed 0.78
Table 4: New Non-zero TF*IDF Terms (Ideal)
with 3 (or 4 with 5) shows that this is sometimes
the case, e.g, ibm occurred a further eleven times
in the fixed reference terms, doubling its increase
in TF*IDF. However, instances of those terms that
only occurred in the fixed reference terms did not,
in fact, refer to the citation of the paper, by defi-
nition of the ideal reference terms. For instance,
one such extra occurrence of ibm is from a sen-
tence following the citation that describes the ex-
act model used in the current work:
(11) According to the IBM models (Brown et al,
1993), the statistical word alignment model
can be generally represented as in Equation
(1) ... In this paper, we use a simplied IBM
model 4 (Al-Onaizan et al, 1999), which ...
Here, the second occurrence refers to (Al-
Onaizan et al, 1999) but, by its proximity to the
citation to our example paper (Brown et al, 1993),
is picked up by the fixed window. Since the term
was arguably not directly intended to describe our
paper, then, a different term might equally have
been used; one that was inappropriate as an in-
dex term. Table 6 lists the fixed reference terms
that were not also in the ideal reference terms; al-
most 400 in total. The vast majority of these occur
very infrequently which suggests that they should
not greatly affect the term profile of the document.
However, the argument for adding good, high IDF
reference terms that are not in the document itself
30
Term TF*IDF
target 19.64
phrase-based 14.77
approaches 11.03
train 8.21
channel 6.94
decoders 6.41
corruption 6.02
noisy-channel 5.75
attainable 5.45
source-language 4.99
target-language 4.82
application-specific 4.40
intermediate 4.01
combinations 3.40
style 2.12
considered 1.62
major 1.16
due 0.83
developed 0.78
Table 5: New Non-zero TF*IDF Terms (Fixed)
conversely applies to adding bad ones: an ?incor-
rect? reference term added to the document will
have its TF*IDF pushed off the zero mark, giving
it the potential to score against inappropriate query
terms. If such a term is distinctive (i.e., has a high
IDF), the effect may be significant. The term giza,
for example, has an IDF of 6.34 and is the name
of a particular tool that is not mentioned in our
example paper. However, since the tool is used
to train IBM models, the two papers in the exam-
ple above are often cited by the same papers and
in close proximity. This increases the chances of
such terms being picked up as reference terms for
the wrong citation by a fixed window, heightening
the adverse effect on its term profile.
5 Discussion and Conclusions
It is not too hard to find examples of citations that
show a fixed window size is suboptimal for finding
terms used in reference to cited papers. In extract-
ing the ideal reference terms from only 24 cita-
tions for our case study, we saw just how difficult
it is to decide which terms refer to which citations.
We, the authors, came across examples where it
was ambiguous how many citations certain terms
referred to, ones where knowledge of the cited pa-
pers was required to interpret the scope of the cita-
tion and ones where we simply did not agree. This
is a highly complex indexing task; one which hu-
mans have difficulty with, one for which we expect
low human agreement and, therefore, the type that
computational linguistics struggles to achieve high
performance on. We agree with O?Connor (1982)
that it is hard. We make no claims that computa-
tional linguistics will provide a full solution.
Nevertheless, our examples suggest that even
simple computational linguistics techniques
should help to more accurately locate reference
terms. While it may be impossible to automati-
cally pick out each specific piece of text that does
refer to a given citation, there is much scope for
improvement over a fixed window. The examples
in Section 2 suggest that altering the size of the
window that is applied would be a good first step.
Some form of text segmentation, whether it be
full-blown discourse analysis or simple sentence
boundary detection, may be useful in determining
where the extent of the reference text is.
While the case study presented here highlights
several interesting effects of using terms from
around citations as additional index terms for the
cited paper, it cannot answer questions about how
successful a practical method based on these ob-
servations would be, over a using simple fixed
window, for example. In order for any real im-
provement in IR, the term profile of a document
would have to be significantly altered by the refer-
ence terms. Enough terms, in particular repeated
terms, would have to be successfully found via ci-
tations for such a quantitative improvement. It is
not clear that computational linguistic techniques
will improve over the statistical effects of redun-
dant data.
We are thus in the last stages of setting up a
larger experiment that will shed more light on this
question. The experimental setup requires data
where there are a significant number of citations
to a number of test documents and a significant
number of reference set terms. We have recently
presented a test collection of scientific research pa-
pers (Ritchie, Teufel & Robertson 2006), which
we intend to use for this experiment.
References
Bharat, K. & Mihaila, G. A. (2001), When experts agree:using non-affiliated experts to rank popular topics, in?Tenth International World Wide Web Conference?,
pp. 597?602.
Bradshaw, S. (2003), Reference directed indexing: Re-
deeming relevance for subject search in citation in-dexes., in ?ECDL?, pp. 499?510.
Bradshaw, S. & Hammond, K. (2002), Automatically in-
dexing documents: Content vs. reference, in ?Intel-ligent User Interfaces?.
31
TF # Terms Terms
13 1 asr
8 4 caption, closed, section, methods
7 2 method, sentences
6 4 describes, example, languages, system
5 6 corpus, dictionary, heuristic, large, paper, results
4 17 account, aligned, confidence, dependency, details, during, equation, generally, given, manual, measures,
order, probabilistic, proposed, shown, simplified, systems, word-aligned
3 29 according, algorithm, applications, build, case, choosing, chunk, current, described, employed, equiv-
alence, experiments, introduced, introduction, length, links, number, obtain, obtained, performance,
performing, problem, produced, related, show, sum, true, types, work
2 64 adaptation, akin, approximate, bitext, calculated, called, categories, certain, chunks, common, consider,
consists, domain-specific, error, estimation, experimental, extracted, families, feature, features, found,
functions, generated, generic, giza, good, high, improve, information, input, iraq, knowledge, large-
scale, lexicon, linked, log-linear, maximum, measure, notion, omitted, original, output, parameter, pick,
position, practice, presents, quality, rate, represented, researchers, rock, role, sinhalese, takes, tamil,
text-to-text, toolkit, transcripts, transcriptions, translations, version, word-based, word-to-word
1 252 access, accuracy, achieve, achieving, actual, addition, address, adopted, advance, advantages, align-
ing, amalgam, annotated, applied, apply, applying, approximated, association, asymmetric, augmented,
availability, available, average, back-off, base, baum-welch, begin, bitexts, bunetsu, candidate, can-
didates, cat, central, chinese, choose, chunk-based, class, closely, collecting, combination, compare,
compared, compares, computed, concludes, consequently, contributed, convention, corpora, correspon-
dence, corrupts, cost, counts, coverage, crucial, currently, decades, decoding, defines, denote, dependent,
depending, determine, dictionaries, direct, directions, disadvantages, distinction, dominated, dynamic,
efforts, english-chinese, english-spanish, enumerate, eojeol, eq, equations, errors, evaluation, excellent,
expansion, explicitly, extracts, failed, fairly, final, finally, fit, flat-start, followed, form, formalisms, for-
mulation, generation, gis, give, grouped, hallucination, halogen, handle, heuristic-based, hidden, highly,
hill-climbing, hmm-based, hypothesis, ideal, identified, identify, identity, immediate, implemented, im-
proved, improves, incorporate, increase, influence, initial, initialize, inspired, interchanging, introduces,
investigations, involve, kate, kind, learning, learns, letter, letters, lexical, likelihood, link, list, longer,
lowercase, main, make, makes, mapping, maximal, maximizes, means, modeling, modified, names,
needed, nitrogen, nodes, occupy, omitting, optimal, outperform, overcome, parse, parser, part, part-of-
speech, path, performed, play, plays, popular, pos, positions, power, precision, probable, produce, pro-
gramming, promising, real-valued, reason, recall, recent, recently, recognition, recursion, recursively, re-
duction, reductions, refine, relative, relying, renormalization, representation, require, requires, research,
restricting, reveal, sample, sampling, satisfactory, segments, semantic, sequences, setting, shortcom-
ings, showed, significant, significantly, similarity, similarly, simple, simplicity, situation, space, speech,
spelling, state-of-the-art, step, strategies, string, strong, studies, summaries, summarization, supervised,
syntactic, tags, task-specific, technique, techniques, technologies, terms, testing, threshold, translation-
related, transliteration, tree, trees, trellis, type, underlying, unrealistic, unsupervised, uppercase, value,
viterbi, wanted, ways, well-formedness, well-founded, widely, widespread, works, written, wtop, yas-
met, years, yields
Table 6: Term Frequencies of ?Noisy? Reference Index Terms
Brin, S. & Page, L. (1998), ?The anatomy of a large-scale hypertextual Web search engine?, ComputerNetworks and ISDN Systems 30(1?7), 107?117.
Chakrabarti, S., Dom, B., Gibson, D., Kleinberg, J.,Raghavan, P. & Rajagopalan, S. (1998), Automaticresource list compilation by analyzing hyperlink
structure and associated text, in ?Seventh Interna-tional World Wide Web Conference?.
Davison, B. D. (2000), Topical locality in the web,in ?Research and Development in Information Re-
trieval (SIGIR)?, pp. 272?279.
Kleinberg, J. M. (1999), ?Authoritative sources in ahyperlinked environment?, Journal of the ACM46(5), 604?632.
Lawrence, S., Bollacker, K. & Giles, C. L. (1999), In-
dexing and retrieval of scientific literature, in ?Con-ference on Information and Knowledge Manage-ment (CIKM)?, pp. 139?146.
Marchiori, M. (1997), ?The quest for correct informa-
tion on the Web: Hyper search engines?, ComputerNetworks and ISDN Systems 29(8?13), 1225?1236.
McBryan, O. (1994), GENVL and WWWW: Tools for
taming the web, in ?First International World WideWeb Conference?.
O?Connor, J. (1982), ?Citing statements: Computerrecognition and use to improve retrieval?, Informa-tion Processing and Management 18(3), 125?131.
O?Connor, J. (1983), ?Biomedical citing statements:Computer recognition and use to aid full-text re-
trieval?, Information Processing and Management19, 361?368.
Ritchie, A., Teufel, S. & Robertson, S. (2006), Creatinga test collection for citation-based IR experiments,in ?HLT-NAACL?.
Spa?rck Jones, K., Walker, S. & Robertson, S. E. (2000),?A probabilistic model of information retrieval: de-
velopment and comparative experiments - parts 1& 2.?, Information Processing and Management36(6), 779?840.
32
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 80?87,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An annotation scheme for citation function
Simone Teufel Advaith Siddharthan Dan Tidhar
Natural Language and Information Processing Group
Computer Laboratory
Cambridge University, CB3 0FD, UK
{Simone.Teufel,Advaith.Siddharthan,Dan.Tidhar}@cl.cam.ac.uk
Abstract
We study the interplay of the discourse struc-
ture of a scientific argument with formal ci-
tations. One subproblem of this is to clas-
sify academic citations in scientific articles ac-
cording to their rhetorical function, e.g., as a
rival approach, as a part of the solution, or
as a flawed approach that justifies the cur-
rent research. Here, we introduce our anno-
tation scheme with 12 categories, and present
an agreement study.
1 Scientific writing, discourse structure
and citations
In recent years, there has been increasing interest in
applying natural language processing technologies to
scientific literature. The overwhelmingly large num-
ber of papers published in fields like biology, genetics
and chemistry each year means that researchers need
tools for information access (extraction, retrieval, sum-
marization, question answering etc). There is also in-
creased interest in automatic citation indexing, e.g.,
the highly successful search tools Google Scholar and
CiteSeer (Giles et al, 1998).1 This general interest in
improving access to scientific articles fits well with re-
search on discourse structure, as knowledge about the
overall structure and goal of papers can guide better in-
formation access.
Shum (1998) argues that experienced researchers are
often interested in relations between articles. They
need to know if a certain article criticises another and
what the criticism is, or if the current work is based
on that prior work. This type of information is hard
to come by with current search technology. Neither
the author?s abstract, nor raw citation counts help users
in assessing the relation between articles. And even
though CiteSeer shows a text snippet around the phys-
ical location for searchers to peruse, there is no guar-
antee that the text snippet provides enough information
for the searcher to infer the relation. In fact, studies
from our annotated corpus (Teufel, 1999), show that
69% of the 600 sentences stating contrast with other
work and 21% of the 246 sentences stating research
continuation with other work do not contain the cor-
responding citation; the citation is found in preceding
1CiteSeer automatically citation-indexes all scientific ar-
ticles reached by a web-crawler, making them available to
searchers via authors or keywords in the title.
Nitta and Niwa 94
Resnik 95
Brown et al 90a
Rose et al 90
Church and Gale 91
Dagan et al 94
Li and Abe 96
Hindle 93
Hindle 90
Dagan et al93
Pereira et al 93
Following Pereira et al we measure
word similarity by the relative entropy
or Kulbach?Leibler (KL) distance, bet?
ween the corresponding conditional
distributions.
His notion of similarityseems to agree with ourintuitions in many cases,but it is not clear how itcan  be used directly toconstruct word classesand corresponding models of association.
Figure 1: A rhetorical citation map
sentences (i.e., the sentence expressing the contrast or
continuation would be outside the CiteSeer snippet).
We present here an approach which uses the classifica-
tion of citations to help provide relational information
across papers.
Citations play a central role in the process of writing
a paper. Swales (1990) argues that scientific writing
follows a general rhetorical argumentation structure:
researchers must justify that their paper makes a con-
tribution to the knowledge in their discipline. Several
argumentation steps are required to make this justifica-
tion work, e.g., the statement of their specific goal in
the paper (Myers, 1992). Importantly, the authors also
must relate their current work to previous research, and
acknowledge previous knowledge claims; this is done
with a formal citation, and with language connecting
the citation to the argument, e.g., statements of usage of
other people?s approaches (often near textual segments
in the paper where these approaches are described), and
statements of contrast with them (particularly in the
discussion or related work sections). We argue that the
automatic recognition of citation function is interest-
ing for two reasons: a) it serves to build better citation
indexers and b) in the long run, it will help constrain
interpretations of the overall argumentative structure of
a scientific paper.
Being able to interpret the rhetorical status of a ci-
tation at a glance would add considerable value to ci-
tation indexes, as shown in Fig. 1. Here differences
and similarities are shown between the example paper
(Pereira et al, 1993) and the papers it cites, as well as
80
the papers that cite it. Contrastive links are shown in
grey ? links to rival papers and papers the current pa-
per contrasts itself to. Continuative links are shown in
black ? links to papers that are taken as starting point of
the current research, or as part of the methodology of
the current paper. The most important textual sentence
about each citation could be extracted and displayed.
For instance, we see which aspect of Hindle (1990) the
Pereira et al paper criticises, and in which way Pereira
et al?s work was used by Dagan et al (1994).
We present an annotation scheme for citations, based
on empirical work in content citation analysis, which
fits into this general framework of scientific argument
structure. It consists of 12 categories, which allow us
to mark the relationships of the current paper with the
cited work. Each citation is labelled with exactly one
category. The following top-level four-way distinction
applies:
? Weakness: Authors point out a weakness in cited
work
? Contrast: Authors make contrast/comparison with
cited work (4 categories)
? Positive: Authors agree with/make use of/show
compatibility or similarity with cited work (6 cat-
egories), and
? Neutral: Function of citation is either neutral, or
weakly signalled, or different from the three func-
tions stated above.
We first turn to the point of how to classify citation
function in a robust way. Later in this paper, we will
report results for a human annotation experiment with
three annotators.
2 Annotation schemes for citations
In the field of library sciences (more specifically, the
field of Content Citation Analysis), the use of informa-
tion from citations above and beyond simple citation
counting has received considerable attention. Biblio-
metric measures assesses the quality of a researcher?s
output, in a purely quantitative manner, by counting
how many papers cite a given paper (White, 2004;
Luukkonen, 1992) or by more sophisticated measures
like the h-index (Hirsch, 2005). But not all citations
are alike. Researchers in content citation analysis have
long stated that the classification of motivations is a
central element in understanding the relevance of the
paper in the field. Bonzi (1982), for example, points out
that negational citations, while pointing to the fact that
a given work has been noticed in a field, do not mean
that that work is received well, and Ziman (1968) states
that many citations are done out of ?politeness? (to-
wards powerful rival approaches), ?policy? (by name-
dropping and argument by authority) or ?piety? (to-
wards one?s friends, collaborators and superiors). Re-
searchers also often follow the custom of citing some
1. Cited source is mentioned in the introduction or
discussion as part of the history and state of the
art of the research question under investigation.
2. Cited source is the specific point of departure for
the research question investigated.
3. Cited source contains the concepts, definitions,
interpretations used (and pertaining to the disci-
pline of the citing article).
4. Cited source contains the data (pertaining to the
discipline of the citing article) which are used
sporadically in the article.
5. Cited source contains the data (pertaining to the
discipline of the citing particle) which are used
for comparative purposes, in tables and statistics.
6. Cited source contains data and material (from
other disciplines than citing article) which is
used sporadically in the citing text, in tables or
statistics.
7. Cited source contains the method used.
8. Cited source substantiated a statement or assump-
tion, or points to further information.
9. Cited source is positively evaluated.
10. Cited source is negatively evaluated.
11. Results of citing article prove, verify, substantiate
the data or interpretation of cited source.
12. Results of citing article disprove, put into ques-
tion the data as interpretation of cited source.
13. Results of citing article furnish a new interpreta-
tion/explanation to the data of the cited source.
Figure 2: Spiegel-Ru?sing?s (1977) Categories for Cita-
tion Motivations
particular early, basic paper, which gives the founda-
tion of their current subject (?paying homage to pio-
neers?). Many classification schemes for citation func-
tions have been developed (Weinstock, 1971; Swales,
1990; Oppenheim and Renn, 1978; Frost, 1979; Chu-
bin and Moitra, 1975), inter alia. Based on such an-
notation schemes and hand-analyzed data, different in-
fluences on citation behaviour can be determined, but
annotation in this field is usually done manually on
small samples of text by the author, and not confirmed
by reliability studies. As one of the earliest such stud-
ies, Moravcsik and Murugesan (1975) divide citations
in running text into four dimensions: conceptual or
operational use (i.e., use of theory vs. use of techni-
cal method); evolutionary or juxtapositional (i.e., own
work is based on the cited work vs. own work is an al-
ternative to it); organic or perfunctory (i.e., work is cru-
cially needed for understanding of citing article or just
a general acknowledgement); and finally confirmative
vs. negational (i.e., is the correctness of the findings
disputed?). They found, for example, that 40% of the
citations were perfunctory, which casts further doubt
on the citation-counting approach.
Other content citation analysis research which is rel-
81
evant to our work concentrates on relating textual spans
to authors? descriptions of other work. For example, in
O?Connor?s (1982) experiment, citing statements (one
or more sentences referring to other researchers? work)
were identified manually. The main problem encoun-
tered in that work is the fact that many instances of cita-
tion context are linguistically unmarked. Our data con-
firms this: articles often contain large segments, par-
ticularly in the central parts, which describe other peo-
ple?s research in a fairly neutral way. We would thus
expect many citations to be neutral (i.e., not to carry
any function relating to the argumentation per se).
Many of the distinctions typically made in content
citation analysis are immaterial to the task considered
here as they are too sociologically orientated, and can
thus be difficult to operationalise without deep knowl-
edge of the field and its participants (Swales, 1986). In
particular, citations for general reference (background
material, homage to pioneers) are not part of our an-
alytic interest here, and so are citations ?in passing?,
which are only marginally related to the argumentation
of the overall paper (Ziman, 1968).
Spiegel-Ru?sing?s (1977) scheme (Fig. 2) is an exam-
ple of a scheme which is easier to operationalise than
most. In her scheme, more than one category can apply
to a citation; for instance positive and negative evalu-
ation (category 9 and 10) can be cross-classified with
other categories. Out of 2309 citations examined, 80%
substantiated statements (category 8), 6% discussed
history or state of the art of the research area (cate-
gory 1) and 5% cited comparative data (category 5).
Category Description
Weak Weakness of cited approach
CoCoGM Contrast/Comparison in Goals or Meth-
ods (neutral)
CoCoR0 Contrast/Comparison in Results (neutral)
CoCo- Unfavourable Contrast/Comparison (cur-
rent work is better than cited work)
CoCoXY Contrast between 2 cited methods
PBas author uses cited work as starting point
PUse author uses tools/algorithms/data
PModi author adapts or modifies
tools/algorithms/data
PMot this citation is positive about approach or
problem addressed (used to motivate work
in current paper)
PSim author?s work and cited work are similar
PSup author?s work and cited work are compat-
ible/provide support for each other
Neut Neutral description of cited work, or not
enough textual evidence for above cate-
gories or unlisted citation function
Figure 3: Our annotation scheme for citation function
Our scheme (given in Fig. 3) is an adaptation of the
scheme in Fig. 2, which we arrived at after an analysis
of a corpus of scientific articles in computational lin-
guistics. We tried to redefine the categories such that
they should be reasonably reliably annotatable; at the
same time, they should be informative for the appli-
cation we have in mind. A third criterion is that they
should have some (theoretical) relation to the particu-
lar discourse structure we work with (Teufel, 1999).
Our categories are as follows: One category (Weak)
is reserved for weakness of previous research, if it is ad-
dressed by the authors (cf. Spiegel-Ru?sing?s categories
10, 12, possibly 13). The next three categories describe
comparisons or contrasts between own and other work
(cf. Spiegel-Ru?sing?s category 5). The difference be-
tween them concerns whether the comparison is be-
tween methods/goals (CoCoGM) or results (CoCoR0).
These two categories are for comparisons without ex-
plicit value judgements. We use a different category
(CoCo-) when the authors claim their approach is bet-
ter than the cited work.
Our interest in differences and similarities between
approaches stems from one possible application we
have in mind (the rhetorical citation search tool). We
do not only consider differences stated between the cur-
rent work and other work, but we also mark citations if
they are explicitly compared and contrasted with other
work (not the current paper). This is expressed in cat-
egory CoCoXY. It is a category not typically consid-
ered in the literature, but it is related to the other con-
trastive categories, and useful to us because we think
it can be exploited for search of differences and rival
approaches.
The next set of categories we propose concerns pos-
itive sentiment expressed towards a citation, or a state-
ment that the other work is actively used in the cur-
rent work (which is the ultimate praise). Like Spiegel-
Ru?sing, we are interested in use of data and methods
(her categories 4, 5, 6, 7), but we cluster different us-
ages together and instead differentiate unchanged use
(PUse) from use with adaptations (PModi). Work
which is stated as the explicit starting point or intellec-
tual ancestry is marked with our category PBas (her
category 2). If a claim in the literature is used to
strengthen the authors? argument, this is expressed in
her category 8, and vice versa, category 11. We col-
lapse these two in our category PSup. We use two
categories she does not have definitions for, namely
similarity of (aspect of) approach to other approach
(PSim), and motivation of approach used or problem
addressed (PMot). We found evidence for prototypi-
cal use of these citation functions in our texts. How-
ever, we found little evidence for her categories 12 or
13 (disproval or new interpretation of claims in cited
literature), and we decided against a ?state-of-the-art?
category (her category 1), which would have been in
conflict with our PMot definition in many cases.
Our fourteenth category, Neut, bundles truly neutral
descriptions of other researchers? approaches with all
those cases where the textual evidence for a citation
function was not enough to warrant annotation of that
category, and all other functions for which our scheme
did not provide a specific category. As stated above, we
do in fact expect many of our citations to be neutral.
82
Citation function is hard to annotate because it in
principle requires interpretation of author intentions
(what could the author?s intention have been in choos-
ing a certain citation?). Typical results of earlier cita-
tion function studies are that the sociological aspect of
citing is not to be underestimated. One of our most fun-
damental ideas for annotation is to only mark explicitly
signalled citation functions. Our guidelines explicitly
state that a general linguistic phrase such as ?better?
or ?used by us? must be present, in order to increase
objectivity in finding citation function. Annotators are
encouraged to point to textual evidence they have for
assigning a particular function (and are asked to type
the source of this evidence into the annotation tool for
each citation). Categories are defined in terms of cer-
tain objective types of statements (e.g., there are 7 cases
for PMot). Annotators can use general text interpreta-
tion principles when assigning the categories, but are
not allowed to use in-depth knowledge of the field or
of the authors.
There are other problematic aspects of the annota-
tion. Some concern the fact that authors do not al-
ways state their purpose clearly. For instance, several
earlier studies found that negational citations are rare
(Moravcsik and Murugesan, 1975; Spiegel-Ru?sing,
1977); MacRoberts and MacRoberts (1984) argue that
the reason for this is that they are potentially politically
dangerous, and that the authors go through lengths to
diffuse the impact of negative references, hiding a neg-
ative point behind insincere praise, or diffusing the
thrust of criticism with perfunctory remarks. In our
data we found ample evidence of this effect, illustrated
by the following example:
Hidden Markov Models (HMMs) (Huang
et al 1990) offer a powerful statistical ap-
proach to this problem, though it is unclear
how they could be used to recognise the units
of interest to phonologists. (9410022, S-24)2
It is also sometimes extremely hard to distinguish
usage of a method from statements of similarity be-
tween a method and the own method. This happens
in cases where authors do not want to admit they are
using somebody else?s method:
The same test was used in Abney and Light
(1999). (0008020, S-151)
Unication of indices proceeds in the same
manner as unication of all other typed
feature structures (Carpenter 1992).
(0008023, S-87)
In this case, our annotators had to choose between
categories PSim and PUse.
It can also be hard to distinguish between continu-
ation of somebody?s research (i.e., taking somebody?s
2In all corpus examples, numbers in brackets correspond
to the official Cmp lg archive number, ?S-? numbers to sen-
tence numbers according to our preprocessing.
research as starting point, as intellectual ancestry, i.e.
PBas) and simply using it (PUse). In principle, one
would hope that annotation of all usage/positive cate-
gories (starting with P), if clustered together, should re-
sult in higher agreement (as they are similar, and as the
resulting scheme has fewer distinctions). We would ex-
pect this to be the case in general, but as always, cases
exist where a conflict between a contrast (CoCo) and a
change to a method (PModi) occur:
In contrast to McCarthy, Kay and Kiraz,
we combine the three components into a sin-
gle projection. (0006044, S-182)
The markable units in our scheme are a) all full cita-
tions (as recognized by our automatic citation proces-
sor on our corpus), and b) all names of authors of cited
papers anywhere in running text outside of a formal
citation context (i.e., without date). Our citation pro-
cessor recognizes these latter names after parsing the
citation list an marks them up. This is unusual in com-
parison to other citation indexers, but we believe these
names function as important referents comparable in
importance to formal citations. In principle, one could
go even further as there are many other linguistic ex-
pressions by which the authors could refer to other peo-
ple?s work: pronouns, abbreviations such as ?Mueller
and Sag (1990), henceforth M & S?, and names of ap-
proaches or theories which are associated with partic-
ular authors. If we could mark all of these up auto-
matically (which is not technically possible), annota-
tion would become less difficult to decide, but techni-
cal difficulty prevent us from recognizing these other
cases automatically. As a result, in these contexts it is
impossible to annotate citation function directly on the
referent, which sometimes causes problems. Because
this means that annotators have to consider non-local
context, one markable may have different competing
contexts with different potential citation functions, and
problems about which context is ?stronger? may oc-
cur. We have rules that context is to be constrained to
the paragraph boundary, but for some categories paper-
wide information is required (e.g., for PMot, we need
to know that a praised approach is used by the authors,
information which may not be local in the paragraph).
Appendix A gives unambiguous example cases
where the citation function can be decided on the ba-
sis of the sentence alone, but Fig. 4 shows a more typ-
ical example where more context is required to inter-
pret the function. The evaluation of the citation Hin-
dle (1990) is contrastive; the evaluative statement is
found 4 sentences after the sentence containing the ci-
tation3. It consists of a positive statement (agreement
with authors? view), followed by a weakness, under-
lined, which is the chosen category. This is marked on
the nearest markable (Hindle, 3 sentences after the ci-
tation).
3In Fig. 4, markables are shown in boxes, evaluative state-
ments underlined, and referents in bold face.
83
S-5 Hindle (1990)/Neut proposed dealing with the
sparseness problem by estimating the likelihood of un-seen events from that of ?similar? events that have beenseen.S-6 For instance, one may estimate the likelihood of aparticular direct object for a verb from the likelihoodsof that direct object for similar verbs.S-7 This requires a reasonable definition of verb simi-larity and a similarity estimation method.
S-8 In Hindle/Weak ?s proposal, words are similarif we have strong statistical evidence that they tend toparticipate in the same events.S-9 His notion of similarity seems to agree with our in-tuitions in many cases, but it is not clear how it can beused directly to construct word classes and correspond-
ing models of association. (9408011)
Figure 4: Annotation example: influence of context
A naive view on this annotation scheme could con-
sider the first two sets of categories in our scheme as
?negative? and the third set of categories ?positive?.
There is indeed a sentiment aspect to the interpretation
of citations, due to the fact that authors need to make
a point in their paper and thus have a stance towards
their citations. But this is not the whole story: many
of our ?positive? categories are more concerned with
different ways in which the cited work is useful to the
current work (which aspect of it is used, e.g., just a
definition or the entire solution?), and many of the con-
trastive statements have no negative connotation at all
and simply state a (value-free) difference between ap-
proaches. However, if one looks at the distribution of
positive and negative adjectives around citations, one
notices a (non-trivial) connection between our task and
sentiment classification.
There are written guidelines of 25 pages, which in-
struct the annotators to only assign one category per
citation, and to skim-read the paper before annotation.
The guidelines provide a decision tree and give deci-
sion aids in systematically ambiguous cases, but sub-
jective judgement of the annotators is nevertheless nec-
essary to assign a single tag in an unseen context. We
implemented an annotation tool based on XML/XSLT
technology, which allows us to use any web browser to
interactively assign one of the 12 tags (presented as a
pull-down list) to each citation.
3 Data
The data we used came from the CmpLg (Computation
and Language archive; 320 conference articles in com-
putational linguistics). The articles are in XML format.
Headlines, titles, authors and reference list items are
automatically marked up with the corresponding tags.
Reference lists are parsed, and cited authors? names
are identified. Our citation parser then applies regu-
lar patterns and finds citations and other occurrences of
the names of cited authors (without a date) in running
text and marks them up. Self-citations are detected by
overlap of citing and cited authors. The citation pro-
cessor developped in our group (Ritchie et al, 2006)
achieves high accuracy for this task (96% of citations
recognized, provided the reference list was error-free).
On average, our papers contain 26.8 citation instances
in running text4.
4 Human Annotation: results
In order to machine learn citation function, we are
in the process of creating a corpus of scientific arti-
cles with human annotated citations, according to the
scheme discussed before. Here we report preliminary
results with that scheme, with three annotators who are
developers of the scheme.
In our experiment, the annotators independently an-
notated 26 conference articles with this scheme, on the
basis of guidelines which were frozen once annotation
started5. The data used for the experiment contained a
total of 120,000 running words and 548 citations.
The relative frequency of each category observed in
the annotation is listed in Fig. 5. As expected, the dis-
tribution is very skewed, with more than 60% of the
citations of category Neut.6 What is interesting is the
relatively high frequency of usage categories (PUse,
PModi, PBas) with a total of 18.9%. There is
a relatively low frequency of clearly negative cita-
tions (Weak, CoCoR-, total of 4.1%), whereas the
neutral?contrastive categories (CoCoGM, CoCoR0,
CoCoXY) are slightly more frequent at 7.6%. This
is in concordance with earlier annotation experiments
(Moravcsik and Murugesan, 1975; Spiegel-Ru?sing,
1977).
We reached an inter-annotator agreement of K=.72
(n=12;N=548;k=3)7. This is comparable to aggreement
on other discourse annotation tasks such as dialogue
act parsing and Argumentative Zoning (Teufel et al,
1999). We consider the agreement quite good, consid-
ering the number of categories and the difficulties (e.g.,
non-local dependencies) of the task.
The annotators are obviously still disagreeing on
some categories. We were wondering to what de-
gree the fine granularity of the scheme is a prob-
lem. When we collapsed the obvious similar cat-
egories (all P categories into one category, and
all CoCo categories into another) to give four top
level categories (Weak, Positive, Contrast,
Neutral), this only raised kappa to 0.76. This
4As opposed to reference list items, which are fewer.
5The development of the scheme was done with 40+ dif-
ferent articles.
6Spiegel-Ru?sing found that out of 2309 citations she ex-
amined, 80% substantiated statements.
7Following Carletta (1996), we measure agreement in
Kappa, which follows the formula K = P (A)?P (E)1?P (E) whereP(A) is observed, and P(E) expected agreement. Kappa
ranges between -1 and 1. K=0 means agreement is only as
expected by chance. Generally, Kappas of 0.8 are considered
stable, and Kappas of .69 as marginally stable, according to
the strictest scheme applied in the field.
84
Neut PUse CoCoGM PSim Weak CoCoXY PMot PModi PBas PSup CoCo- CoCoR0
62.7% 15.8% 3.9% 3.8% 3.1% 2.9% 2.2% 1.6% 1.5% 1.1% 1.0% 0.8%
Figure 5: Distribution of the categories
Weak CoCo- CoCoGM CoCoR0 CoCoXY PUse PBas PModi PMot PSim PSup Neut
Weak 5 3
CoCo- 1 3
CoCoGM 23 3
CoCoR0 4
CoCoXY 1
PUse 86 6 2 1 12
PBas 3 2
PModi 3
PMot 13 4
PSim 3 20 5
PSup 1 2 1
Neut 6 10 6 4 17 1 6 4 287
Figure 6: Confusion matrix between two annotators
points to the fact that most of our annotators disagreed
about whether to assign a more informative category
or Neut, the neutral fall-back category. Unfortunately,
Kappa is only partially sensitive to such specialised dis-
agreements. While it will reward agreement with in-
frequent categories more than agreement with frequent
categories, it nevertheless does not allow us to weight
disagreements we care less about (Neut vs more in-
formative category) less than disagreements we do care
a lot about (informative categories which are mutually
exclusive, such as Weak and PSim).
Fig. 6 shows a confusion matrix between the two an-
notators who agreed most with each other. This again
points to the fact that a large proportion of the confu-
sion involves an informative category and Neut. The
issue with Neut and Weak is a point at hand: au-
thors seem to often (deliberately or not) mask their in-
tended citation function with seemingly neutral state-
ments. Many statements of weakness of other ap-
proaches were stated in such caged terms that our anno-
tators disagreed about whether the signals given were
?explicit? enough.
While our focus is not sentiment analysis, it is pos-
sible to conflate our 12 categories into three: positive,
weakness and neutral by the following mapping:
Old Categories New Category
Weak, CoCo- Negative
PMot, PUse, PBas, PModi, PSim, PSup Positive
CoCoGM, CoCoR0, CoCoXY, Neut Neutral
Thus negative contrasts and weaknesses are grouped
into Negative, while neutral contrasts are grouped
into Neutral. All the positive classes are conflated
into Positive. This resulted in kappa=0.75 for three
annotators.
Fig. 7 shows the confusion matrix between two an-
notators for this sentiment classification. Fig. 7 is par-
ticularly instructive, because it shows that annotators
Weakness Positive Neutral
Weakness 9 1 12
Positive 140 13
Neutral 4 30 339
Figure 7: Confusion matrix between two annotators;
categories collapsed to reflect sentiment
have only one case of confusion between positive and
negative references to cited work. The vast majority of
disagreements reflects genuine ambiguity as to whether
the authors were trying to stay neutral or express a sen-
timent.
Distinction Kappa
PMot v. all others .790
CoCoGM v. all others .765
PUse v. all others .761
CoCoR0 v. all others .746
Neut v. all others .742
PSim v. all others .649
PModi v. all others .553
CoCoXY v. all others .553
Weak v. all others .522
CoCo- v. all others .462
PBas v. all others .414
PSup v. all others .268
Figure 8: Distinctiveness of categories
In an attempt to determine how well each cate-
gory was defined, we created artificial splits of the
data into binary distinctions: each category versus a
super-category consisting of all the other collapsed cat-
egories. The kappas measured on these datasets are
given in Fig. 8. The higher they are, the better the anno-
tators could distinguish the given category from all the
other categories. We can see that out of the informa-
85
tive categories, four are defined at least as well as the
overall distinction (i.e. above the line in Fig. 8: PMot,
PUse, CoCoGM and CoCoR0. This is encouraging,
as the application of citation maps is almost entirely
centered around usage and contrast. However, the se-
mantics of some categories are less well-understood by
our annotators: in particular PSup (where the difficulty
lies in what an annotator understands as ?mutual sup-
port? of two theories), and (unfortunately) PBas. The
problem with PBas is that its distinction from PUse is
based on subjective judgement of whether the authors
use a part of somebody?s previous work, or base them-
selves entirely on this previous work (i.e., see them-
selves as following in the same intellectual framework).
Another problem concerns the low distinctivity for the
clearly negative categories CoCo- and Weak. This is
in line with MacRoberts and MacRoberts? hypothesis
that criticism is often hedged and not clearly lexically
signalled, which makes it more difficult to reliably an-
notate such citations.
5 Conclusion
We have described a new task: human annotation of
citation function, a phenomenon which we believe to
be closely related to the overall discourse structure of
scientific articles. Our annotation scheme concentrates
on contrast, weaknesses of other work, similarities be-
tween work and usage of other work. One of its prin-
ciples is the fact that relations are only to be marked if
they are explicitly signalled. Here, we report positive
results in terms of interannotator agreement.
Future work on the annotation scheme will concen-
trate on improving guidelines for currently suboptimal
categories, and on measuring intra-annotator agree-
ment and inter-annotator agreement with naive annota-
tors. We are also currently investigating how well our
scheme will work on text from a different discipline,
namely chemistry. Work on applying machine learning
techniques for automatic citation classification is cur-
rently underway (Teufel et al, 2006); the agreement
of one annotator and the system is currently K=.57,
leaving plenty of room for improvement in comparison
with the human annotation results presented here.
6 Acknowledgements
This work was funded by the EPSRC projects
CITRAZ (GR/S27832/01, ?Rhetorical Citation Maps
and Domain-independent Argumentative Zoning?) and
SCIBORG (EP/C010035/1, ?Extracting the Science
from Scientific Publications?).
References
Susan Bonzi. 1982. Characteristics of a literature as predic-
tors of relatedness between cited and citing works. JASIS,
33(4):208?216.
Jean Carletta. 1996. Assessing agreement on classification
tasks: The kappa statistic. Computational Linguistics,
22(2):249?254.
Daryl E. Chubin and S. D. Moitra. 1975. Content analysis
of references: Adjunct or alternative to citation counting?Social Studies of Science, 5(4):423?441.
Carolyn O. Frost. 1979. The use of citations in literary re-
search: A preliminary classification of citation functions.Library Quarterly, 49:405.
C. Lee Giles, Kurt D. Bollacker, and Steve Lawrence. 1998.
Citeseer: An automatic citation indexing system. In Pro-ceedings of the Third ACM Conference on Digital Li-braries, pages 89?98.
Jorge E. Hirsch. 2005. An index to quantify an individ-
ual?s scientific research output. Proceedings of the Na-tional Academy of Sciences of the United Stated of Amer-ica (PNAS), 102(46).
Terttu Luukkonen. 1992. Is scientists? publishing behaviour
reward-seeking? Scientometrics, 24:297?319.
Michael H. MacRoberts and Barbara R. MacRoberts. 1984.
The negational reference: Or the art of dissembling. So-cial Studies of Science, 14:91?94.
Michael J. Moravcsik and Poovanalingan Murugesan. 1975.
Some results on the function and quality of citations. So-cial Studies of Science, 5:88?91.
Greg Myers. 1992. In this paper we report...?speech acts
and scientific facts. Journal of Pragmatics, 17(4):295?
313.
John O?Connor. 1982. Citing statements: Computer recogni-
tion and use to improve retrieval. Information Processingand Management, 18(3):125?131.
Charles Oppenheim and Susan P. Renn. 1978. Highly cited
old papers and the reasons why they continue to be cited.JASIS, 29:226?230.
Anna Ritchie, Simone Teufel, and Steven Robertson. 2006.
Creating a test collection for citation-based IR experi-
ments. In Proceedings of HLT-06.
Simon Buckingham Shum. 1998. Evolving the web for sci-
entific knowledge: First steps towards an ?HCI knowledge
web?. Interfaces, British HCI Group Magazine, 39:16?21.
Ina Spiegel-Ru?sing. 1977. Bibliometric and content analy-
sis. Social Studies of Science, 7:97?113.
John Swales. 1986. Citation analysis and discourse analysis.Applied Linguistics, 7(1):39?56.
John Swales, 1990. Genre Analysis: English in Academicand Research Settings. Chapter 7: Research articles in En-glish, pages 110?176. Cambridge University Press, Cam-
bridge, UK.
Simone Teufel, Jean Carletta, and Marc Moens. 1999. An
annotation scheme for discourse-level argumentation in re-
search articles. In Proceedings of the Ninth Meeting of theEuropean Chapter of the Association for ComputationalLinguistics (EACL-99), pages 110?117.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar. 2006.
Automatic classification of citation function. In Proceed-ings of EMNLP-06.
Simone Teufel. 1999. Argumentative Zoning: InformationExtraction from Scientific Text. Ph.D. thesis, School of
Cognitive Science, University of Edinburgh, UK.
Melvin Weinstock. 1971. Citation indexes. In Encyclopediaof Library and Information Science, volume 5, pages 16?
40. Dekker, New York, NY.
Howard D. White. 2004. Citation analysis and discourse
analysis revisited. Applied Linguistics, 25(1):89?116.
John M. Ziman. 1968. Public Knowledge: An Essay Con-cerning the Social Dimensions of Science. Cambridge
University Press, Cambridge, UK.
86
A Annotation examples
Weak However, Koskenniemi himself understood that his initial implementation had signif-
icant limitations in handling non-concatenative morphotactic processes.
(0006044, S-4)
CoCoGM The goals of the two papers are slightly different: Moore ?s approach is designed to
reduce the total grammar size (i.e., the sum of the lengths of the productions), while
our approach minimizes the number of productions.
(0008021, S-22)
CoCoR0 This is similar to results in the literature (Ramshaw and Marcus 1995).
(0008022, S-147)
CoCo- For the Penn Treebank, Ratnaparkhi (1996) reports an accuracy of 96.6% using the
Maximum Entropy approach, our much simpler and therefore faster HMM approach
delivers 96.7%. (0003055, S-156)
CoCoXY Unlike previous approaches (Ellison 1994, Walther 1996), Karttunen ?s approach
is encoded entirely in the nite state calculus, with no extra-logical procedures for
counting constraint violations. (0006038, S-5)
PBas Our starting point is the work described in Ferro et al (1999) , which used a fairly
small training set. (0008004, S-11)
PUse In our application, we tried out the Learning Vector Quantization (LVQ) (Kohonen et
al. 1996). (0003060, S-105)
PModi In our experiments, we have used a conjugate-gradient optimization program adapted
from the one presented in Press et al (0008028, S-72)
PMot It has also been shown that the combined accuracy of an ensemble of multiple clas-
siers is often signicantly greater than that of any of the individual classiers that
make up the ensemble (e.g., Dietterich (1997)). (0005006, S-9)
PSim Our system is closely related to those proposed in Resnik (1997) and Abney and
Light (1999). (0008020, S-24)
PSup In all experiments the SVM Light system outperformed other learning algorithms,
which conrms Yang and Liu ?s (1999) results for SVMs fed with Reuters data.
(0003060, S-141)
Neut The cosine metric and Jaccard?s coefcient are commonly used in information re-
trieval as measures of association (Salton and McGill 1983). (0001012, S-29)
87
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 103?110,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatic classication of citation function
Simone Teufel Advaith Siddharthan Dan Tidhar
Natural Language and Information Processing Group
Computer Laboratory
Cambridge University, CB3 0FD, UK
{Simone.Teufel,Advaith.Siddharthan,Dan.Tidhar}@cl.cam.ac.uk
Abstract
Citation function is defined as the author?s
reason for citing a given paper (e.g. ac-
knowledgement of the use of the cited
method). The automatic recognition of the
rhetorical function of citations in scientific
text has many applications, from improve-
ment of impact factor calculations to text
summarisation and more informative ci-
tation indexers. We show that our anno-
tation scheme for citation function is re-
liable, and present a supervised machine
learning framework to automatically clas-
sify citation function, using both shallow
and linguistically-inspired features. We
find, amongst other things, a strong re-
lationship between citation function and
sentiment classification.
1 Introduction
Why do researchers cite a particular paper? This
is a question that has interested researchers in
discourse analysis, sociology of science, and in-
formation sciences (library sciences) for decades
(Garfield, 1979; Small, 1982; White, 2004). Many
annotation schemes for citation motivation have
been created over the years, and the question has
been studied in detail, even to the level of in-depth
interviews with writers about each individual cita-
tion (Hodges, 1972).
Part of this sustained interest in citations can
be explained by the fact that bibliometric met-
rics are commonly used to measure the impact of
a researcher?s work by how often they are cited
(Borgman, 1990; Luukkonen, 1992). However, re-
searchers from the field of discourse studies have
long criticised purely quantitative citation analy-
sis, pointing out that many citations are done out
of ?politeness, policy or piety? (Ziman, 1968),
and that criticising citations or citations in pass-
ing should not ?count? as much as central cita-
tions in a paper, or as those citations where a re-
searcher?s work is used as the starting point of
somebody else?s work (Bonzi, 1982). A plethora
of manual annotation schemes for citation motiva-
tion have been invented over the years (Garfield,
1979; Hodges, 1972; Chubin and Moitra, 1975).
Other schemes concentrate on citation function
(Spiegel-Ru?sing, 1977; O?Connor, 1982; Wein-
stock, 1971; Swales, 1990; Small, 1982)). One
of the best-known of these studies (Moravcsik
and Murugesan, 1975) divides citations in running
text into four dimensions: conceptual or opera-
tional use (i.e., use of theory vs. use of technical
method); evolutionary or juxtapositional (i.e., own
work is based on the cited work vs. own work is an
alternative to it); organic or perfunctory (i.e., work
is crucially needed for understanding of citing ar-
ticle or just a general acknowledgement); and fi-
nally confirmative vs. negational (i.e., is the cor-
rectness of the findings disputed?). They found,
for example, that 40% of the citations were per-
functory, which casts further doubt on the citation-
counting approach.
Based on such annotation schemes and hand-
analyzed data, different influences on citation be-
haviour can be determined. Nevertheless, re-
searchers in the field of citation content analysis
do not normally cross-validate their schemes with
independent annotation studies with other human
annotators, and usually only annotate a small num-
ber of citations (in the range of hundreds or thou-
sands). Also, automated application of the annota-
tion is not something that is generally considered
in the field, though White (2004) sees the future of
discourse-analytic citation analysis in automation.
Apart from raw material for bibliometric stud-
ies, citations can also be used for search purposes
in document retrieval applications. In the library
world, printed or electronic citation indexes such
as ISI (Garfield, 1979) serve as an orthogonal
103
Nitta and Niwa 94
Resnik 95
Brown et al 90a
Rose et al 90
Church and Gale 91
Dagan et al 94
Li and Abe 96
Hindle 93
Hindle 90
Dagan et al93
Pereira et al 93
Following Pereira et al we measure
word similarity by the relative entropy
or Kulbach?Leibler (KL) distance, bet?
ween the corresponding conditional
distributions.
His notion of similarityseems to agree with ourintuitions in many cases,but it is not clear how itcan  be used directly toconstruct word classesand corresponding models of association.
Figure 1: A rhetorical citation map
search tool to find relevant papers, starting from a
source paper of interest. With the increased avail-
ability of documents in electronic form in recent
years, citation-based search and automatic citation
indexing have become highly popular, cf. the suc-
cessful search tools Google Scholar and CiteSeer
(Giles et al, 1998).1
But not all search needs are fulfilled by current
citation indexers. Experienced researchers are of-
ten interested in relations between articles (Shum,
1998). They want to know if a certain article crit-
icises another and what the criticism is, or if the
current work is based on that prior work. This
type of information is hard to come by with current
search technology. Neither the author?s abstract,
nor raw citation counts help users in assessing the
relation between articles.
Fig. 1 shows a hypothetical search tool which
displays differences and similarities between a tar-
get paper (here: Pereira et al, 1993) and the pa-
pers that it cites and that cite it. Contrastive links
are shown in grey ? links to rival papers and pa-
pers the current paper contrasts itself to. Continu-
ative links are shown in black ? links to papers that
use the methodology of the current paper. Fig. 1
also displays the most characteristic textual sen-
tence about each citation. For instance, we can see
which aspect of Hindle (1990) our example paper
criticises, and in which way the example paper?s
work was used by Dagan et al (1994).
Note that not even the CiteSeer text snippet
1These tools automatically citation-index all scientific ar-
ticles reached by a web-crawler, making them available to
searchers via authors or keywords in the title, and displaying
the citation in context of a text snippet.
can fulfil the relation search need: it is always
centered around the physical location of the ci-
tations, but the context is often not informative
enough for the searcher to infer the relation. In
fact, studies from our annotated corpus (Teufel,
1999) show that 69% of the 600 sentences stat-
ing contrast with other work and 21% of the
246 sentences stating research continuation with
other work do not contain the corresponding cita-
tion; the citation is found in preceding sentences
(which means that the sentence expressing the
contrast or continuation is outside the CiteSeer
snippet). A more sophisticated, discourse-aware
citation indexer which finds these sentences and
associates them with the citation would add con-
siderable value to the researcher?s bibliographic
search (Ritchie et al, 2006b).
Our annotation scheme for citations is based
on empirical work in content citation analysis. It
is designed for information retrieval applications
such as improved citation indexing and better bib-
liometric measures (Teufel et al, 2006). Its 12 cat-
egories mark relationships with other works. Each
citation is labelled with exactly one category. The
following top-level four-way distinction applies:
? Explicit statement of weakness
? Contrast or comparison with other work (4
categories)
? Agreement/usage/compatibility with other
work (6 categories), and
? A neutral category.
In this paper, we show that the scheme can be
reliably annotated by independent coders. We also
report results of a supervised machine learning ex-
periment which replicates the human annotation.
2 An annotation scheme for citations
Our scheme (given in Fig. 2) is adapted from that
of Spiegel-Ru?sing (1977) after an analysis of a
corpus of scientific articles in computational lin-
guistics. We avoid sociologically orientated dis-
tinctions (?paying homage to pioneers?), as they
can be difficult to operationalise without deep
knowledge of the field and its participants (Swales,
1986). Our redefinition of the categories aims at
reliably annotation; at the same time, the cate-
gories should be informative enough for the docu-
ment management application sketched in the in-
troduction.
104
Category Description
Weak Weakness of cited approach
CoCoGM Contrast/Comparison in Goals or Meth-
ods(neutral)
CoCo- Author?s work is stated to be superior to
cited work
CoCoR0 Contrast/Comparison in Results (neutral)
CoCoXY Contrast between 2 cited methods
PBas Author uses cited work as basis or starting
point
PUse Author uses
tools/algorithms/data/definitions
PModi Author adapts or modifies
tools/algorithms/data
PMot This citation is positive about approach
used or problem addressed (used to mo-
tivate work in current paper)
PSim Author?s work and cited work are similar
PSup Author?s work and cited work are compat-
ible/provide support for each other
Neut Neutral description of cited work, or not
enough textual evidence for above cate-
gories, or unlisted citation function
Figure 2: Annotation scheme for citation function.
Our categories are as follows: One category
(Weak) is reserved for weakness of previous re-
search, if it is addressed by the authors. The next
four categories describe comparisons or contrasts
between own and other work. The difference be-
tween them concerns whether the contrast is be-
tween methods employed or goals (CoCoGM), or
results, and in the case of results, a difference is
made between the cited results being worse than
the current work (CoCo-), or comparable or bet-
ter results (CoCoR0). As well as considering dif-
ferences between the current work and other work,
we also mark citations if they are explicitly com-
pared and contrasted with other work (i.e. not
the work in the current paper). This is expressed
in category CoCoXY. While this is not typically
annotated in the literature, we expect a potential
practical benefit of this category for our applica-
tion, particularly in searches for differences and
rival approaches.
The next set of categories we propose concerns
positive sentiment expressed towards a citation, or
a statement that the other work is actively used
in the current work (which we consider the ulti-
mate praise). We mark statements of use of data
and methods of the cited work, differentiating un-
changed use (PUse) from use with adaptations
(PModi). Work which is stated as the explicit
starting point or intellectual ancestry is marked
with our category PBas. If a claim in the liter-
ature is used to strengthen the authors? argument,
or vice versa, we assign the category PSup. We
also mark similarity of (an aspect of) the approach
to the cited work (PSim), and motivation of ap-
proach used or problem addressed (PMot).
Our twelfth category, Neut, bundles truly neu-
tral descriptions of cited work with those cases
where the textual evidence for a citation function
was not enough to warrant annotation of that cate-
gory, and all other functions for which our scheme
did not provide a specific category.
Citation function is hard to annotate because it
in principle requires interpretation of author inten-
tions (what could the author?s intention have been
in choosing a certain citation?). One of our most
fundamental principles is thus to only mark explic-
itly signalled citation functions. Our guidelines
explicitly state that a general linguistic phrase such
as ?better? or ?used by us? must be present; this
increases the objectivity of defining citation func-
tion. Annotators must be able to point to textual
evidence for assigning a particular function (and
are asked to type the source of this evidence into
the annotation tool for each citation). Categories
are defined in terms of certain objective types of
statements (e.g., there are 7 cases for PMot, e.g.
?Citation claims that or gives reasons for why
problem Y is hard?). Annotators can use general
text interpretation principles when assigning the
categories (such as anaphora resolution and par-
allel constructions), but are not allowed to use in-
depth knowledge of the field or of the authors.
Guidelines (25 pages, ? 150 rules) describe the
categories with examples, provide a decision tree
and give decision aids in systematically ambigu-
ous cases. Nevertheless, subjective judgement of
the annotators is still necessary to assign a single
tag in an unseen context, because of the many dif-
ficult cases for annotation. Some of these concern
the fact that authors do not always state their pur-
pose clearly. For instance, several earlier studies
found that negational citations are rare (Moravc-
sik and Murugesan, 1975; Spiegel-Ru?sing, 1977);
MacRoberts and MacRoberts (1984) argue that the
reason for this is that they are potentially politi-
cally dangerous. In our data we found ample evi-
dence of the ?meekness? effect. Other difficulties
concern the distinction of the usage of a method
from statements of similarity between a method
and the own method (i.e., the choice between cat-
egories PSim and PUse). This happens in cases
where authors do not want to admit (or stress)
105
that they are using somebody else?s method. An-
other difficult distinction concerns the judgement
of whether the authors continue somebody?s re-
search (i.e., consider their research as intellectual
ancestry, i.e. PBas), or whether they simply use
the work (PUse).
The unit of annotation is a) the full citation (as
recognised by our automatic citation processor on
our corpus), and b) names of authors of cited pa-
pers anywhere in running text outside of a for-
mal citation context (i.e., without date). These
latter are marked up, slightly unusually in com-
parison to other citation indexers, because we be-
lieve they function as important referents compa-
rable in importance to formal citations.2 In prin-
ciple, there are many other linguistic expressions
by which the authors could refer to other people?s
work: pronouns, abbreviations such as ?Mueller
and Sag (1990), henceforth M & S?, and names of
approaches or theories which are associated with
particular authors. The fact that in these contexts
citation function cannot be annotated (because it
is not technically feasible to recognise them well
enough) sometimes causes problems with context
dependencies.
While there are unambiguous example cases
where the citation function can be decided on the
basis of the sentence alone, this is not always the
case. Most approaches are not criticised in the
same sentence where they are also cited: it is more
likely that there are several descriptive sentences
about a cited approach between its formal cita-
tion and the evaluative statement, which is often at
the end of the textual segment about this citation.
Nevertheless, the annotator must mark the func-
tion on the nearest appropriate annotation unit (ci-
tation or author name). Our rules decree that con-
text is in most cases constrained to the paragraph
boundary. In rare cases, paper-wide information
is required (e.g., for PMot, we need to know that
a praised approach is used by the authors, infor-
mation which may not be local in the paragraph).
Annotators are thus asked to skim-read the paper
before annotation.
One possible view on this annotation scheme
could consider the first two sets of categories as
?negative? and the third set of categories ?posi-
tive?, in the sense of Pang et al (2002) and Turney
(2002). Authors need to make a point (namely,
2Our citation processor can recognise these after parsing
the citation list.
that they have contributed something which is bet-
ter or at least new (Myers, 1992)), and they thus
have a stance towards their citations. But although
there is a sentiment aspect to the interpretation of
citations, this is not the whole story. Many of our
?positive? categories are more concerned with dif-
ferent ways in which the cited work is useful to the
current work (which aspect of it is used, e.g., just a
definition or the entire solution?), and many of the
contrastive statements have no negative connota-
tion at all and simply state a (value-free) differ-
ence between approaches. However, if one looks
at the distribution of positive and negative adjec-
tives around citations, it is clear that there is a non-
trivial connection between our task and sentiment
classification.
The data we use comes from our corpus of
360 conference articles in computational linguis-
tics, drawn from the Computation and Language
E-Print Archive (http://xxx.lanl.gov/cmp-lg). The
articles are transformed into XML format; head-
lines, titles, authors and reference list items are au-
tomatically marked up. Reference lists are parsed
using regular patterns, and cited authors? names
are identified. Our citation parser then finds cita-
tions and author names in running text and marks
them up. Ritchie et al (2006a) report high ac-
curacy for this task (94% of citations recognised,
provided the reference list was error-free). On av-
erage, our papers contain 26.8 citation instances in
running text3. For human annotation, we use our
own annotation tool based on XML/XSLT tech-
nology, which allows us to use a web browser to
interactively assign one of the 12 tags (presented
as a pull-down list) to each citation.
We measure inter-annotator agreement between
three annotators (the three authors), who indepen-
dently annotated 26 articles with the scheme (con-
taining a total of 120,000 running words and 548
citations), using the written guidelines. The guide-
lines were developed on a different set of articles
from the ones used for annotation.
Inter-annotator agreement was Kappa=.72
(n=12;N=548;k=3)4 . This is quite high, consider-
ing the number of categories and the difficulties
3As opposed to reference list items, which are fewer.
4Following Carletta (1996), we measure agreement in
Kappa, which follows the formula K = P (A)?P (E)1?P (E) whereP(A) is observed, and P(E) expected agreement. Kappa
ranges between -1 and 1. K=0 means agreement is only as
expected by chance. Generally, Kappas of 0.8 are considered
stable, and Kappas of .69 as marginally stable, according to
the strictest scheme applied in the field.
106
(e.g., non-local dependencies) of the task. The
relative frequency of each category observed in
the annotation is listed in Fig. 3. As expected,
the distribution is very skewed, with more than
60% of the citations of category Neut.5 What
is interesting is the relatively high frequency
of usage categories (PUse, PModi, PBas)
with a total of 18.9%. There is a relatively low
frequency of clearly negative citations (Weak,
CoCo-, total of 4.1%), whereas the neutral?
contrastive categories (CoCoR0, CoCoXY,
CoCoGM) are slightly more frequent at 7.6%.
This is in concordance with earlier annotation
experiments (Moravcsik and Murugesan, 1975;
Spiegel-Ru?sing, 1977).
3 Features for automatic recognition of
citation function
This section summarises the features we use for
machine learning citation function. Some of these
features were previously found useful for a dif-
ferent application, namely Argumentative Zoning
(Teufel, 1999; Teufel and Moens, 2002), some are
specific to citation classification.
3.1 Cue phrases
Myers (1992) calls meta-discourse the set of ex-
pressions that talk about the act of presenting re-
search in a paper, rather than the research itself
(which is called object-level discourse). For in-
stance, Swales (1990) names phrases such as ?to
our knowledge, no. . . ? or ?As far as we aware? as
meta-discourse associated with a gap in the cur-
rent literature. Strings such as these have been
used in extractive summarisation successfully ever
since Paice?s (1981) work.
We model meta-discourse (cue phrases) and
treat it differently from object-level discourse.
There are two different mechanisms: A finite
grammar over strings with a placeholder mecha-
nism for POS and for sets of similar words which
can be substituted into a string-based cue phrase
(Teufel, 1999). The grammar corresponds to 1762
cue phrases. It was developed on 80 papers which
are different to the papers used for our experiments
here.
The other mechanism is a POS-based recog-
niser of agents and a recogniser for specific actions
these agents perform. Two main agent types (the
5Spiegel-Ru?sing found that out of 2309 citations she ex-
amined, 80% substantiated statements.
authors of the paper, and everybody else) are mod-
elled by 185 patterns. For instance, in a paragraph
describing related work, we expect to find refer-
ences to other people in subject position more of-
ten than in the section detailing the authors? own
methods, whereas in the background section, we
often find general subjects such as ?researchers in
computational linguistics? or ?in the literature?.
For each sentence to be classified, its grammatical
subject is determined by POS patterns and, if pos-
sible, classified as one of these agent types. We
also use the observation that in sentences without
meta-discourse, one can assume that agenthood
has not changed.
20 different action types model the main verbs
involved in meta-discourse. For instance, there is
a set of verbs that is often used when the over-
all scientific goal of a paper is defined. These
are the verbs of presentation, such as ?propose,
present, report? and ?suggest?; in the corpus we
found other verbs in this function, but with a lower
frequency, namely ?describe, discuss, give, intro-
duce, put forward, show, sketch, state? and ?talk
about?. There are also specialised verb clusters
which co-occur with PBas sentences, e.g., the
cluster of continuation of ideas (eg. ?adopt, agree
with, base, be based on, be derived from, be orig-
inated in, be inspired by, borrow, build on,. . . ?).
On the other hand, the semantics of verbs in Weak
sentences is often concerned with failing (of other
researchers? approaches), and often contain verbs
such as ?abound, aggravate, arise, be cursed, be
incapable of, be forced to, be limited to, . . . ?.
We use 20 manually acquired verb clusters.
Negation is recognised, but too rare to define its
own clusters: out of the 20 ? 2 = 40 theoretically
possible verb clusters, only 27 were observed in
our development corpus. We have recently auto-
mated the process of verb?object pair acquisition
from corpora for two types of cue phrases (Abdalla
and Teufel, 2006) and are planning on expanding
this work to other cue phrases.
3.2 Cues Identified by annotators
During the annotator training phase, the anno-
tators were encouraged to type in the meta-
description cue phrases that justify their choice of
category. We went through this list by hand and
extracted 892 cue phrases (around 75 per cate-
gory). The files these cues came from were not
part of the test corpus. We included 12 features
107
Neut PUse CoCoGM PSim Weak PMot CoCoR0 PBas CoCoXY CoCo- PModi PSup
62.7% 15.8% 3.9% 3.8% 3.1% 2.2% 0.8% 1.5% 2.9% 1.0% 1.6% 1.1%
Figure 3: Distribution of citation categories
Weak CoCoGM CoCoR0 CoCo- CoCoXY PBas PUse PModi PMot PSim PSup Neut
P .78 .81 .77 .56 .72 .76 .66 .60 .75 .68 .83 .80
R .49 .52 .46 .19 .54 .46 .61 .27 .64 .38 .32 .92
F .60 .64 .57 .28 .62 .58 .63 .37 .69 .48 .47 .86
Percentage Accuracy 0.77
Kappa (n=12; N=2829; k=2) 0.57
Macro-F 0.57
Figure 4: Summary of Citation Analysis results (10-fold cross-validation; IBk algorithm; k=3).
that recorded the presence of cues that our annota-
tors associated with a particular class.
3.3 Other features
There are other features which we use for this
task. We know from Teufel and Moens (2002) that
verb tense and voice should be useful for recogniz-
ing statements of previous work, future work and
work performed in the paper. We also recognise
modality (whether or not a main verb is modified
by an auxiliary, and which auxiliary it is).
The overall location of a sentence containing
a reference should be relevant. We observe that
more PMot categories appear towards the begin-
ning of the paper, as do Weak citations, whereas
comparative results (CoCoR0, CoCoR-) appear
towards the end of articles. More fine-grained lo-
cation features, such as the location within the
paragraph and the section, have also been imple-
mented.
The fact that a citation points to own previous
work can be recognised, as we know who the pa-
per authors are. As we have access to the infor-
mation in the reference list, we also know the last
names of all cited authors (even in the case where
an et al statement in running text obscures the
later-occurring authors). With self-citations, one
might assume that the probability of re-use of ma-
terial from previous own work should be higher,
and the tendency to criticise lower.
4 Results
Our evaluation corpus for citation analysis con-
sists of 116 articles (randomly drawn from the part
of our corpus which was not used for guideline
development or cue phrase acquisition). The 116
articles contain 2829 citation instances. Each
citation instance was manually tagged as one
Weakness Positive Contrast Neutral
P .80 .75 .77 .81
R .49 .65 .52 .90
F .61 .70 .62 .86
Percentage Accuracy 0.79
Kappa (n=12; N=2829; k=2) 0.59
Macro-F 0.68
Figure 5: Summary of results (10-fold cross-
validation; IBk algorithm; k=3): Top level classes.
Weakness Positive Neutral
P .77 .75 .85
R .42 .65 .92
F .54 .70 .89
Percentage Accuracy 0.83
Kappa (n=12; N=2829; k=2) 0.58
Macro-F 0.71
Figure 6: Summary of results (10-fold cross-
validation; IBk algorithm; k=3): Sentiment Anal-
ysis.
of {Weak, CoCoGM, CoCo-, CoCoR0, CoCoXY,
PBas, PUse, PModi, PMot, PSim, PSup, Neut}.
The papers are then further processed (e.g. to-
kenised and POS-tagged). All other features are
automatically determined (e.g. self-citations are
detected by overlap of citing and cited authors);
then, machine learning is applied to the feature
vectors.
The 10-fold cross-validation results for citation
classification are given in Figure 4, comparing the
system to one of the annotators. Results are given
in three overall measures: Kappa, percentage ac-
curacy, and Macro-F (following Lewis (1991)).
Macro-F is the mean of the F-measures of all
twelve categories. We use Macro-F and Kappa be-
cause we want to measure success particularly on
the rare categories, and because Micro-averaging
techniques like percentage accuracy tend to over-
estimate the contribution of frequent categories in
108
heavily skewed distributions like ours6.
In the case of Macro-F, each category is treated
as one unit, independent of the number of items
contained in it. Therefore, the classification suc-
cess of the individual items in rare categories
is given more importance than classification suc-
cess of frequent category items. However, one
should keep in mind that numerical values in
macro-averaging are generally lower (Yang and
Liu, 1999), due to fewer training cases for the rare
categories. Kappa has the additional advantage
over Macro-F that it filters out random agreement
(random use, but following the observed distribu-
tion of categories).
For our task, memory-based learning outper-
formed other models. The reported results use the
IBk algorithm with k = 3 (we used the Weka ma-
chine learning toolkit (Witten and Frank, 2005)
for our experiments). Fig. 7 provides a few ex-
amples from one file in the corpus, along with the
gold standard citation class, the machine predic-
tion, and a comment.
Kappa is even higher for the top level distinc-
tion. We collapsed the obvious similar categories
(all P categories into one category, and all CoCo
categories into another) to give four top level
categories (Weak, Positive, Contrast,
Neutral; results in Fig. 5). Precision for all the
categories is above 0.75, and K=0.59. For con-
trast, the human agreement for this situation was
K=0.76 (n=3,N=548,k=3).
In a different experiment, we grouped the cate-
gories as follows, in an attempt to perform senti-
ment analysis over the classifications:
Old Categories New Category
Weak, CoCo- Negative
PMot, PUse, PBas, PModi, PSim, PSup Positive
CoCoGM, CoCoR0, CoCoXY, Neut Neutral
Thus negative contrasts and weaknesses are
grouped into Negative, while neutral contrasts
are grouped into Neutral. All positive classes
are conflated into Positive.
Results show that this grouping raises results
to a smaller degree than the top-level distinction
did (to K=.58). For contrast, the human agree-
ment for these collapsed categories was K=.75
(n=3,N=548,k=3).
6This situation has parallels in information retrieval,
where precision and recall are used because accuracy over-
estimates the performance on irrelevant items.
5 Conclusion
We have presented a new task: annotation of ci-
tation function in scientific text, a phenomenon
which we believe to be closely related to the over-
all discourse structure of scientific articles. Our
annotation scheme concentrates on weaknesses of
other work, and on similarities and contrast be-
tween work and usage of other work. In this
paper, we present machine learning experiments
for replicating the human annotation (which is re-
liable at K=.72). The automatic result reached
K=.57 (acc=.77) for the full annotation scheme;
rising to Kappa=.58 (acc=.83) for a three-way
classification (Weak, Positive, Neutral).
We are currently performing an experiment to
see if citation processing can increase perfor-
mance in a large-scale, real-world information
retrieval task, by creating a test collection of
researchers? queries and relevant documents for
these (Ritchie et al, 2006a).
6 Acknowledgements
This work was funded by the EPSRC projects CIT-
RAZ (GR/S27832/01, ?Rhetorical Citation Maps
and Domain-independent Argumentative Zon-
ing?) and SCIBORG (EP/C010035/1, ?Extracting
the Science from Scientific Publications?).
References
Rashid M. Abdalla and Simone Teufel. 2006. A bootstrap-
ping approach to unsupervised detection of cue phrase
variants. In Proc. of ACL/COLING-06.
Susan Bonzi. 1982. Characteristics of a literature as predic-
tors of relatedness between cited and citing works. JASIS,
33(4):208?216.
Christine L. Borgman, editor. 1990. Scholarly Communica-tion and Bibliometrics. Sage Publications, CA.
Jean Carletta. 1996. Assessing agreement on classification
tasks: The kappa statistic. Computational Linguistics,
22(2):249?254.
Daryl E. Chubin and S. D. Moitra. 1975. Content analysis
of references: Adjunct or alternative to citation counting?Social Studies of Science, 5(4):423?441.
Eugene Garfield. 1979. Citation Indexing: Its Theory andApplication in Science, Technology and Humanities. J.
Wiley, New York, NY.
C. Lee Giles, Kurt D. Bollacker, and Steve Lawrence. 1998.
Citeseer: An automatic citation indexing system. In Proc.of the Third ACM Conference on Digital Libraries, pages
89?98.
T.L. Hodges. 1972. Citation Indexing: Its Potential for Bibli-ographical Control. Ph.D. thesis, University of California
at Berkeley.
David D. Lewis. 1991. Evaluating text categorisation. InSpeech and Natural Language: Proceedings of the ARPAWorkshop of Human Language Technology.
109
Context Human Machine Comment
We have compared four complete and three partial data rep-
resentation formats for the baseNP recognition task pre-
sented in Ramshaw and Marcus (1995).
PUse PUse Cues can be weak: ?for... task...presented in?
In the version of the algorithm that we have used, IB1-IG,
the distances between feature representations are computed
as the weighted sum of distances between individual features(Bosch 1998).
Neut PUse Human decided citation was for
detail in used package, not di-
rectly used by paper.
We have used the baseNP data presented in Ramshaw andMarcus (1995). PUse PUse Straightforward case
We will follow Argamon et al (1998) and use
a combination of the precision and recall rates:
F=(2*precision*recall)/(precision+recall).
PSim PUse Human decided F-measure was
not attributable to citation. Hence
similarity rather than usage.
This algorithm standardly uses the single training item clos-
est to the test i.e. However Daelemans et al (1999) report
that for baseNP recognition better results can be obtained by
making the algorithm consider the classification values of the
three closest training items.
Neut PUse Shallow processing by Machine
means that it is mislead by the
strong cue in preceding sentence.
They are better than the results for section 15 because more
training data was used in these experiments. Again the
best result was obtained with IOB1 (F=92.37) which is an
improvement of the best reported F-rate for this data setRamshaw and Marcus 1995 (F=92.03).
CoCo- PUse Machine attached citation to the
data set being used. Human at-
tached citation to the result being
compared.
Figure 7: Examples of classifications by the machine learner.
Terttu Luukkonen. 1992. Is scientists? publishing behaviour
reward-seeking? Scientometrics, 24:297?319.
Michael H. MacRoberts and Barbara R. MacRoberts. 1984.
The negational reference: Or the art of dissembling. So-cial Studies of Science, 14:91?94.
Michael J. Moravcsik and Poovanalingan Murugesan. 1975.
Some results on the function and quality of citations. So-cial Studies of Science, 5:88?91.
Greg Myers. 1992. In this paper we report...?speech acts
and scientific facts. Journal of Pragmatics, 17(4).
John O?Connor. 1982. Citing statements: Computer recogni-
tion and use to improve retrieval. Information Processingand Management, 18(3):125?131.
Chris D. Paice. 1981. The automatic generation of literary
abstracts: an approach based on the identification of self-
indicating phrases. In R. Oddy, S. Robertson, C. van Rijs-
bergen, and P. W. Williams, editors, Information RetrievalResearch. Butterworth, London, UK.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002.
Thumbs up? Sentiment classification using machine
learning techniques. In Proc. of EMNLP-02.
Anna Ritchie, Simone Teufel, and Stephen Robertson.
2006a. Creating a test collection for citation-based IR ex-
periments. In Proc. of HLT/NAACL 2006, New York, US.
Anna Ritchie, Simone Teufel, and Stephen Robertson.
2006b. How to find better index terms through citations.
In Proc. of ACL/COLING workshop ?Can ComputationalLinguistics improve IR?.
Simon Buckingham Shum. 1998. Evolving the web for sci-
entific knowledge: First steps towards an ?HCI knowledge
web?. Interfaces, British HCI Group Magazine, 39.
Henry Small. 1982. Citation context analysis. In P. Dervin
and M. J. Voigt, editors, Progress in Communication Sci-ences 3, pages 287?310. Ablex, Norwood, N.J.
Ina Spiegel-Ru?sing. 1977. Bibliometric and content analy-
sis. Social Studies of Science, 7:97?113.
John Swales. 1986. Citation analysis and discourse analysis.Applied Linguistics, 7(1):39?56.
John Swales, 1990. Genre Analysis: English in Academicand Research Settings. Chapter 7: Research articles inEnglish, pages 110?176. Cambridge University Press,
Cambridge, UK.
Simone Teufel and Marc Moens. 2002. Summarising scien-
tific articles ? experiments with relevance and rhetorical
status. Computational Linguistics, 28(4):409?446.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar. 2006.
An annotation scheme for citation function. In Proc. ofSIGDial-06.
Simone Teufel. 1999. Argumentative Zoning: InformationExtraction from Scientific Text. Ph.D. thesis, School of
Cognitive Science, University of Edinburgh, UK.
Peter D. Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classification
of reviews. In Proc. of ACL-02.
Melvin Weinstock. 1971. Citation indexes. In Encyclope-dia of Library and Information Science, volume 5. Dekker,
New York, NY.
Howard D. White. 2004. Citation analysis and discourse
analysis revisited. Applied Linguistics, 25(1):89?116.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Practi-cal machine learning tools and techniques. Morgan Kauf-
mann, San Francisco.
Yiming Yang and Xin Liu. 1999. A re-examination of text
categorization methods. In Proc. of SIGIR-99.
John M. Ziman. 1968. Public Knowledge: An Essay Con-cerning the Social Dimensions of Science. Cambridge
University Press, Cambridge, UK.
110
BioNLP 2007: Biological, translational, and clinical language processing, pages 57?64,
Prague, June 2007. c?2007 Association for Computational Linguistics
Annotation of Chemical Named Entities
Peter Corbett
Cambridge University
Chemical Laboratory
Lensfield Road
Cambridge
UK CB2 1EW
ptc24@cam.ac.uk
Colin Batchelor
Royal Society of Chemistry
Thomas Graham House
Milton Road
Cambridge
UK CB4 0WF
batchelorc@rsc.org
Simone Teufel
Natural Language and
Information Processing Group
Computer Laboratory
University of Cambridge
UK CB3 0FD
sht25@cam.ac.uk
Abstract
We describe the annotation of chemical
named entities in scientific text. A set of an-
notation guidelines defines 5 types of named
entities, and provides instructions for the
resolution of special cases. A corpus of full-
text chemistry papers was annotated, with an
inter-annotator agreement
 
score of 93%.
An investigation of named entity recogni-
tion using LingPipe suggests that
 
scores
of 63% are possible without customisation,
and scores of 74% are possible with the ad-
dition of custom tokenisation and the use of
dictionaries.
1 Introduction
Recent efforts in applying natural language pro-
cessing to natural science texts have focused on
the recognition of genes and proteins in biomedi-
cal text. These large biomolecules are?mostly?
conveniently described as sequences of subunits,
strings written in alphabets of 4 or 20 letters. Ad-
vances in sequencing techniques have lead to a boom
in genomics and proteomics, with a concomitant
need for natural language processing techniques to
analyse the texts in which they are discussed.
However, proteins and nucleic acids provide only
a part of the biochemical picture. Smaller chemical
species, which are better described atom-by-atom,
play their roles too, both in terms of their inter-
actions with large biomolecules like proteins, and
in the more general biomedical context. A num-
ber of resources exist to provide chemical infor-
mation to the biological community. For example,
the National Center For Biotechnology Information
(NCBI) has added the chemical database PubChem1
to its collections of bioinformatics data, and the on-
tology ChEBI (Chemical Entities of Biological In-
terest) (de Matos et al, 2006) has been added to the
Open Biological Ontologies (OBO) family.
Small-molecule chemistry also plays a role in
biomedical natural language processing. PubMed
has included abstracts from medicinal chemistry
journals for a long time, and is increasingly carry-
ing other chemistry journals too. Both the GENIA
corpus (Kim et al, 2003) and the BioIE cytochrome
P450 corpus (Kulick et al, 2004) come with named
entity annotations that include a proportion of chem-
icals, and at least a few abstracts that are recognis-
able as chemistry abstracts.
Chemical named entity recognition enables a
number of applications. Linking chemical names to
chemical structures, by a mixture of database lookup
and the parsing of systematic nomenclature, allows
the creation of semantically enhanced articles, with
benefits for readers. An example of this is shown in
the Project Prospect2 annotations by the Royal So-
ciety of Chemistry (RSC). Linking chemical NER
to chemical information retrieval techniques allows
corpora to be searched for chemicals with similar
structures to a query molecule, or chemicals that
contain a particular structural motif (Corbett and
Murray-Rust, 2006). With information extraction
techniques, chemicals could be linked to their prop-
erties, applications and reactions, and with tradi-
tional gene/protein NLP techniques, it could be pos-
1http://pubchem.ncbi.nlm.nih.gov/
2http://www.projectprospect.org/
57
sible to discover new links between chemical data
and bioinformatics data.
A few chemical named entity recognition (Cor-
bett and Murray-Rust, 2006; Townsend et al, 2005;
Vasserman, 2004; Kemp and Lynch, 1998; Sun et
al., 2007) or classification (Wilbur et al, 1999) sys-
tems have been published. A plugin for the GATE
system3 will also recognise a limited range of chem-
ical entities. Other named entity recognition or
classification systems (Narayanaswamy et al, 2003;
Torii et al, 2004; Torii and Vijay-Shanker, 2002;
Spasic and Ananiadou, 2004) sometimes include
chemicals as well as genes, proteins and other bio-
logical entities. However, due to differences in cor-
pora and the scope of the task, it is difficult to com-
pare them. There has been no chemical equivalent
of the JNLPBA (Kim et al, 2004) or BioCreAtIvE
(Yeh et al, 2005) evaluations. Therefore, a corpus
and a task definition are required.
To find an upper bound on the levels of perfor-
mance that are available for the task, it is necessary
to study the inter-annotator agreement for the man-
ual annotation of the texts. In particular, it is useful
to see to what extent the guidelines can be applied by
those not involved in their development. Producing
guidelines that enable a highly consistent annotation
may raise the quality of the results of any machine-
learning techniques that use training data applied to
the guidelines, and producing guidelines that cover
a broad range of subdomains is also important (Din-
gare et al, 2005).
2 Annotation Guidelines
We have prepared a set of guidelines for the an-
notation of the names of chemical compounds and
related entities in scientific papers. These guide-
lines grew out of work on PubMed abstracts, and
have since been developed with reference to organic
chemistry journals, and later a range of journals en-
compassing the whole of chemistry.
Our annotation guidelines focus on the chemicals
themselves; we believe that these represent the ma-
jor source of rare words in chemistry papers, and
are of the greatest interest to end-users. Further-
more, many chemical names are formed systemat-
ically or semi-systematically, and can be interpreted
3http://www.gate.ac.uk/
without resorting to dictionaries and databases. As
well as chemical names themselves, we also con-
sider other words or phrases that are formed from
chemical names.
The various types are summarised in Table 1.
Type Description Example
CM chemical compound citric acid
RN chemical reaction 1,3-dimethylation
CJ chemical adjective pyrazolic
ASE enzyme methylase
CPR chemical prefix 1,3-
Table 1: Named entity types
The logic behind the classes is best explained with
an example drawn from the corpus described in the
next section:
In addition, we have found in previous
studies that the Zn  

?Tris system is also
capable of efficiently hydrolyzing other

-
lactams, such as clavulanic acid, which
is a typical mechanism-based inhibitor of
active-site serine

-lactamases (clavulanic
acid is also a fairly good substrate of the
zinc-

-lactamase from B. fragilis).
Here, ?clavulanic acid? is a specific chemical com-
pound (a CM), referred to by a trivial (unsystem-
atic) name, and ?  -lactams? is a class of chemi-
cal compounds (also a CM), defined by a particu-
lar structural motif. ?Zn  

?Tris? is another CM (a
complex rather than a molecule), and despite be-
ing named in an ad hoc manner, the name is com-
positional and it is reasonably clear to a trained
chemist what it is. ?Serine? (another CM) can be
used to refer to an amino acid as a whole compound,
but in this case refers to it as a part of a larger
biomolecule. The word ?hydrolyzing? (an RN) de-
notes a reaction involving the chemical ?water?. ?

-
lactamases? (an ASE) denotes a class of enzymes
that process

-lactams, and ?zinc-

-lactamase? (an-
other ASE) denotes a  -lactamase that uses zinc.
By our guidelines, the terms ?mechanism-based in-
hibitor? or ?substrate? are not annotated, as they de-
note a chemical role, rather than giving information
about the structure or composition of the chemicals.
58
The full guidelines occupy 31 pages (including a
quick reference section), and contain 93 rules. Al-
most all of these have examples, and many have sev-
eral examples.
A few distinctions need to be explained here. The
classes RN, CJ and ASE do not include all reactions,
adjectives or enzymes, but only those that entail
specific chemicals or classes of chemicals?usually
by being formed by the modification of a chemical
name?for example, ?

-lactamases? in the example
above is formed from the name of a class of chem-
icals. Words derived from Greek and Latin words
for ?water?, such as ?aqueous? and ?hydrolysis?, are
included when making these annotations.
The class CPR consists of prefixes, more often
found in systematic chemical names, giving details
of the geometry of molecules, that are attached to
normal English words. For example, the chemi-
cal 1,2-diiodopentane is a 1,2-disubstituted pentane,
and the ?1,2-? forms the CPR in ?1,2-disubstituted?.
Although these contructions sometimes occur as in-
fixes within chemical names, we have only seen
these used as prefixes outside of them. We believe
that identifying these prefixes will be useful in the
adaptation of lexicalised parsers to chemical text.
The annotation task includes a small amount of
word sense disambiguation. Although most chemi-
cal names do not have non-chemical homonyms, a
few do. Chemical elements, and element symbols,
give particular problems. Examples of this include
?lead?, ?In? (indium), ?As? (arsenic), ?Be? (beryl-
lium), ?No? (nobelium) and ?K? (potassium?this is
confusable with Kelvin). These are only annotated
when they occur in their chemical sense.
2.1 Related Work
We know of two publicly available corpora that also
include chemicals in their named-entity markup. In
both of these, there are significant differences to
many aspects of the annotation. In general, our
guidelines tend to give more importance to concepts
regarding chemical structure, and less importance to
biological role, than the other corpora do.
The GENIA corpus (Kim et al, 2003) in-
cludes several different classes for chemi-
cals. Our class CM roughly corresponds to
the union of GENIA?s atom, inorganic,
other organic compound, nucleotide
and amino acid monomer classes, and also
parts of lipid and carbohydrate (we ex-
clude macromolecules such as lipoproteins and
lipopolysaccharides). Occasionally terms that
match our class RN are included as other name.
Our CM class also includes chemical names
that occur within enzyme or other protein names
(e.g. ?inosine-5   -monophosphate? in ?inosine-5   -
monophosphate dehydrogenase?) whereas the
GENIA corpus (which allows nesting) typically
does not. The GENIA corpus also sometimes
includes qualifiers in terms, giving ?intracellular
calcium? where we would only annotate ?calcium?,
and also includes some role/application terms such
as ?antioxidant? and ?reactive intermediate?.
The BioIE P450 corpus (Kulick et al, 2004), by
contrast, includes chemicals, proteins and other sub-
stances such as foodstuffs in a single category called
?substance?. Again, role terms such as ?inhibitor? are
included, and may be merged with chemical names
to make entities such as ?fentanyl metabolites? (we
would only mark up ?fentanyl?). Fragments of
chemicals such as ?methyl group? are not marked up;
in our annotations, the ?methyl? is marked up.
The BioIE corpus was produced with extensive
guidelines; in the GENIA corpus, much more was
left to the judgement of the annotators. These lead
to inconsistencies, such as whether to annotate ?an-
tioxidant? (our guidelines treat this as a biological
role, and do not mark it up). We are unaware of an
inter-annotator agreement study for either corpus.
Both of these corpora include other classes of
named entities, and additional information such as
sentence boundaries.
3 Inter-annotator Agreement
3.1 Related Work
We are unaware of any studies of inter-annotator
agreement with regards to chemicals. However, a
few studies of gene/protein inter-annotator agree-
ment do exist. Demetriou and Gaizauskas (2003)
report an
 
score of 89% between two domain ex-
perts for a task involving various aspects of protein
science. Morgan et al (2004) report an   score of
87% between a domain expert and a systems devel-
oper for D. melanogaster gene names. Vlachos and
Gasperin (2006) produced a revised version of the
59
guidelines for the task, and were able to achieve an
 
score of 91%, and a kappa of 0.905, between a
computational linguist and a domain expert.
3.2 Subjects
Three subjects took part in the study. Subject A
was a chemist and the main author of the guidelines.
Subject B was another chemist, highly involved in
the development of the guidelines. Subject C was a
PhD student with a chemistry degree. His involve-
ment in the development of guidelines was limited to
proof-reading an early version of the guidelines. C
was trained by A, by being given half an hour?s train-
ing, a test paper to annotate (which satisfied A that C
understood the general principles of the guidelines),
and a short debriefing session before being given the
papers to annotate.
3.3 Materials
The study was performed on 14 papers (full pa-
pers and communications only, not review articles
or other secondary publications) published by the
Royal Society of Chemistry. These were taken from
the journal issues from January 2004 (excluding a
themed issue of one of the journals). One paper was
randomly selected to represent each of the 14 jour-
nals that carried suitable papers. These 14 papers
represent a diverse sample of topics, covering areas
of organic, inorganic, physical, analytical and com-
putational chemistry, and also areas where chemistry
overlaps with biology, environmental science, mate-
rials and mineral science, and education.
From these papers, we collected the title, section
headings, abstract and paragraphs, and discarded the
rest. To maximise the value of annotator effort, we
also automatically discarded the experimental sec-
tions, by looking for headers such as ?Experimen-
tal?. This policy can be justified thus: In chemistry
papers, a section titled ?Results and Discussion? car-
ries enough information about the experiments per-
formed to follow the argument of the paper, whereas
the experimental section carries precise details of the
protocols that are usually only of interest to people
intending to replicate or adapt the experiments per-
formed. It is increasingly common for chemistry pa-
pers not to contain an experimental section in the
paper proper, but to include one in the supporting
online information. Furthermore, experimental sec-
tions are often quite long and tedious to annotate,
and previous studies have shown that named-entity
recognition is easier on experimental sections too
(Townsend et al, 2005).
A few experimental sections (or parts thereof)
were not automatically detected, and instead were
removed by hand.
3.4 Procedure
The papers were hand-annotated using our in-house
annotation software. This software displays the text
so as to preserve aspects of the style of the text such
as subscripts and superscripts, and allows the anno-
tators to freely select spans of text with character-
level precision?the text was not tokenised prior to
annotation. Spans were not allowed to overlap or to
nest. Each selected span was assigned to exactly one
of the five available classes.
During annotation the subjects were allowed to
refer to the guidelines (explained in the previous sec-
tion), to reference sources such as PubChem and
Wikipedia, and to use their domain knowledge as
chemists. They were not allowed to confer with
anyone over the annotation, nor to refer to texts an-
notated during development of the guidelines. The
training of subject C by A was completed prior to A
annotating the papers involved in the exercise.
3.5 Evaluation Methodology
Inter-annotator agreement was measured pairwise,
using the
 
score. To calculate this, all of the ex-
act matches were found and counted, and all of the
entities annotated by one annotator but not the other
(and vice versa) were counted. For an exact match,
the left boundary, right boundary and type of the an-
notation had to match entirely. Thus, if one anno-
tator had annotated ?hexane?ethyl acetate? as a sin-
gle entity, and the other had annotated it as ?hexane?
and ?ethyl acetate?, then that would count as three
cases of disagreement and no cases of agreement.
We use the
 
score as it is a standard measure in the
domain?however, as a measure it has weaknesses
which will be discussed in the next subsection.
Given the character-level nature of the annotation
task, and that the papers were not tokenised, the task
cannot sensibly be cast as a classification problem,
and so we have not calculated any kappa scores.
60
Overall results were calculated using two meth-
ods. The first method was to calculate the total lev-
els of agreement and disagreement across the whole
corpus, and to calculate a total   score based on that.
The second method was to calculate   scores for in-
dividual papers (removing a single paper that con-
tained two named entities?neither of which were
spotted by subject B?as an outlier), and to calculate
an unweighted mean, standard deviation and 95%
confidence intervals based on those scores.
3.6 Results and Discussion
Subjects   (corpus)   (average) std. dev.
A?B 92.8% 92.9%   3.4% 6.2%
A?C 90.0% 91.4%   3.1% 5.7%
B?C 86.1% 87.6%   3.1% 5.7%
Table 2: Inter-annotator agreement results.   values
are 95% confidence intervals.
The results of the analysis are shown in Table 2.
The whole-corpus
 
scores suggest that high levels
of agreement (93%) are possible. This is equivalent
to or better than quoted values for biomedical inter-
annotator agreement. However, the poorer agree-
ments involving C would suggest that some of this is
due to some extra information being communicated
during the development of the guidelines.
A closer analysis shows that this is not the case. A
single paper, containing a large number of entities, is
notable as a major source of disagreement between
A and C, and B and C, but not A and B. Looking
at the annotations themselves, the paper contained
many repetitions of the difficult entity ?Zn  

?Tris?,
and also of similar entities. If the offending paper is
removed from consideration, the agreement between
A and C exceeds the agreement between A and B.
This analysis is confirmed using the per-paper  
scores. Two-tailed, pairwise t-tests (excluding the
outlier paper) showed that the difference in mean  
scores between the A?B and A?C agreements was
not statistically significant at the 0.05 significance
level; however, the differences between B?C and A?
B, and between B?C and A?C were.
A breakdown of the inter-annotator agreements
by type is shown in Table 3. CM and RN, at least,
seem to be reliably annotated. The other classes are
less easy to assess, due to their rarity, both in terms
Type
 
Number
CM 93% 2751
RN 94% 79
CJ 56% 20
ASE 96% 25
CPR 77% 10
Table 3: Inter-annotator agreement, by type.  
scores are corpus totals, between Subjects A and C.
The number is the number of entities of that class
found by Subject A.
of their total occurrence in the corpus and the num-
ber of papers that contain them.
We speculate that the poorer B?C agreement may
be due to differing error rates in the annotation. In
many cases, it was clear from the corpus that errors
were made due to failing to spot relevant entities, or
by failing to look up difficult cases in the guidelines.
Although it is not possible to make a formal analy-
sis of this, we suspect that A made fewer errors, due
to a greater familiarity with the task and the guide-
lines. This is supported by the results, as more er-
rors would be involved in the B?C comparison than
in comparisons involving A, leading to higher levels
of disagreement.
We have also examined the types of disagree-
ments made. There were very few cases where two
annotators had annotated an entity with the same
start and end point, but a different type; there were
2 cases of this between A and C, and 3 cases in each
of the other two comparisons. All of these were con-
fusions between CM and CJ.
In the A?B comparison, there were 415 entities
that were annotated by either A or B that did not
have a corresponding exact match. 183 (44%) of
those were simple cases where the two annotators
did not agree as to whether the entity should be
marked up or not (i.e. the other annotator had not
placed any entity wholly or partially within that
span). For example, some annotators failed to spot
instances of ?water?, or disagreed over whether ?fat?
(as a synonym for ?lipid?) was to be marked up.
The remainder of those disagreements are due
to disagreements of class, of where the boundaries
should be, of how many entities there should be in
a given span, and combinations of the above. In all
61
of these cases, the fact that the annotators produce at
least one entity each for a given case means that dis-
agreements of this type are penalised harshly, and
therefore are given disproportionate weight. How-
ever, it is also likely that disagreements over whether
to mark an entity up are more likely to represent a
simple mistake than a disagreement over how to in-
terpret the guidelines; it is easy to miss an entity that
should be marked up when scanning the text.
A particularly interesting class of disagreement
concerns whether a span of text should be anno-
tated as one entity or two. For example, ?Zn  

?Tris?
could be marked up as a single entity, or as ?Zn  

?
and ?Tris?. We looked for cases where one annota-
tor had a single entity, the left edge of which cor-
responded to the left edge of an entity annotated by
the other annotator, and the right edge corresponded
to the right edge of a different entity. We found 43
cases of this. As in each of these cases, at least three
entities are involved, this pattern accounts for at least
30% of the inter-annotator disagreement. Only 17 of
these cases contained whitespace?in the rest of the
cases, hyphens, dashes or slashes were involved.
4 Analysis of the Corpus
To generate a larger corpus, a further two batches of
papers were selected and preprocessed in the manner
described for the inter-annotator agreement study
and annotated by Subject A. These were combined
with the annotations made by Subject A during the
agreement study, to produce a corpus of 42 papers.
Type Entities Papers
CM 6865 94.1% 42 100%
RN 288 4.0% 23 55%
CJ 60 0.8% 20 48%
ASE 31 0.4% 5 12%
CPR 53 0.7% 9 21%
Table 4: Occurrence of entities in the corpus, and
numbers of papers containing at least one entity of a
type.
From Table 4 it is clear that CM is by far the most
common type of named entity in the corpus. Obser-
vation of the corpus shows that RN is common in
certain genres of paper (for example organic synthe-
sis papers), and generally absent from other genres.
ASE, too, is a specialised category, and did not occur
much in this corpus.
A closer examination of CM showed more than
90% of these to contain no whitespace. However,
this is not to say that there are not significant num-
bers of multi-token entities. The difficulty of to-
kenising the corpus is illustrated by the fact that
1114 CM entities contained hyphens or dashes, and
388 CM entities were adjacent to hyphens or dashes
in the corpus. This means that any named entity
recogniser will have to have a specialised tokeniser,
or be good at handling multi-token entities.
Tokenising the CM entities on whitespace and
normalising their case revealed 1579 distinct
words?of these, 1364 only occurred in one paper.
There were 4301 occurrences of these words (out of
a total of 7626). Whereas the difficulties found in
gene/protein NER with complex multiword entities
and polysemous words are less likely to be a prob-
lem here, the problems with tokenisation and large
numbers of unknown words remain just as pressing.
As with biomedical text (Yeh et al, 2005), cases
of conjunctive and disjunctive nomenclature, such
as ?benzoic and thiophenic acids? and ?bromo- or
chlorobenzene? exist in the corpus. However, these
only accounted for 27 CM entities.
5 Named-Entity Recognition
To establish some baseline measures of perfor-
mance, we applied the named-entity modules from
the toolkit LingPipe,4 which has been success-
fully applied to NER of D. melanogaster genes
(e.g. by Vlachos and Gasperin (2006)). Ling-
Pipe uses a first-order HMM, using an enriched
tagset that marks not only the positions of the
named entities, but the tokens in front of and
behind them. Two different strategies are em-
ployed for handling unknown tokens. The
first (the TokenShapeChunker) replaces un-
known or rare tokens with a morphologically-
based classification. The second, newer module
(the CharLmHmmChunker) estimates the prob-
ability of an observed word given a tag us-
ing language models based on character-level   -
grams. The LingPipe developers suggest that the
TokenShapeChunker typically outperforms the
4http://www.alias-i.com/lingpipe/
62
CharLmHmmChunker. However, the more so-
phisticated handling of unknown words by the
CharLmHmmChunker suggests that it might be a
good fit to the domain.
As well as examining the performance of Ling-
Pipe out of the box, we were also able to make some
customisations. We have a custom tokeniser, con-
taining several adaptations to chemical text. For ex-
ample, our tokeniser will only remove brackets from
the front and back of tokens, and only if that would
not cause the brackets within the token to become
unbalanced. For example, no brackets would be re-
moved from ?(R)-acetoin?. Likewise, it will only
tokenise on a hyphen if the hyphen is surrounded
by two lower-case letters on either side (and if the
letters to the left are not common prehyphen com-
ponents of chemical names), or if the string to the
right has been seen in the training data to be hy-
phenated with a chemical name (e.g. ?derived? in
?benzene-derived?). By contrast, the default Ling-
Pipe tokeniser is much more aggressive, and will to-
kenise on hyphens and brackets wherever they occur.
The CharLmHmmChunker?s language models
can also be fed dictionaries as additional training
data?we have experimented with using a list of
chemical names derived from ChEBI (de Matos et
al., 2006), and a list of chemical elements. We have
also made an extension to LingPipe?s token classi-
fier, which adds classification based on chemically-
relevant suffixes (e.g. -yl, -ate, -ic, -ase, -lysis), and
membership in the aforementioned chemical lists, or
in a standard English dictionary.
We analysed the performance of the different
LingPipe configurations by 3-fold cross-validation,
using the 42-paper corpus described in the previous
section. In each fold, 28 whole papers were used as
training data, holding out the other 14 as test data.
The results are shown in Table 5.
From Table 5, we can see that the character   -
gram language models offer clear advantages over
the older techniques, especially when coupled to a
custom tokeniser (which gives a boost to   of over
7%), and trained with additional chemical names.
The usefulness of character-based   -grams has also
been demonstrated elsewhere (Wilbur et al, 1999;
Vasserman, 2004; Townsend et al, 2005). Their use
here in an HMM is particularly apt, as it allows the
token-internal features in the language model to be
Configuration
    
TokenShape 67.0% 52.9% 59.1%
+  71.2% 62.3% 66.5%
+  67.4% 52.5% 59.0%
+  +  73.3% 62.5% 67.4%
CharLm 62.7% 63.4% 63.1%
+  59.8% 68.8% 64.0%
+  71.1% 70.0% 70.5%
+  +  75.3% 73.5% 74.4%
Table 5: LingPipe performance using different con-
figurations.  = custom token classifier,  = chemical
name lists,  = custom tokeniser
combined with the token context.
The impact of custom tokenisation upon
the older TokenShapeChunker is less dra-
matic. It is possible that tokens that contain
hyphens, brackets and other special characters are
more likely to be unknown or rare tokens?the
TokenShapeChunker has previously been
reported to make most of its mistakes on these
(Vlachos and Gasperin, 2006), so tokenising them
is likely to make less of an impact. It is also
possible that chemical names are more distinctive
as a string of subtokens rather than as one large
token?this may offset the loss in accuracy from
getting the start and end positions wrong. The
CharLmHmmChunker already has a mecha-
nism for spotting distinctive substrings such as
?N,N?-? and ?-3-?, and so the case for having long,
well-formed tokens becomes much less equivocal.
It is also notable that improvements in tokenisa-
tion are synergistic with other improvements?the
advantage of using the CharLmHmmChunker is
much more apparent when the custom tokeniser is
used, as is the advantage of using word lists as addi-
tional training data. It is notable that for the unmod-
ified TokenShapeChunker, using the custom to-
keniser actually harms performance.
6 Conclusion
We have produced annotation guidelines that enable
the annotation of chemicals and related entities in
scientific texts in a highly consistent manner. We
have annotated a corpus using these guidelines, an
analysis of which, and the results of using an off-
63
the-shelf NER toolkit, show that finding good ap-
proaches to tokenisation and the handling of un-
known words is critical in the recognition of these
entities. The corpus and guidelines are available by
contacting the first author.
7 Acknowledgements
We thank Ann Copestake and Peter Murray-Rust
for supervision, Andreas Vlachos and Advaith Sid-
dharthan for valuable discussions, and David Jessop
for annotation. We thank the RSC for providing the
papers, and the UK eScience Programme and EP-
SRC (EP/C010035/1) for funding.
References
Peter T. Corbett and Peter Murray-Rust. 2006. High-
Throughput Identification of Chemistry in Life Sci-
ence Texts. CompLife, LNBI 4216:107?118.
P. de Matos, M. Ennis, M. Darsow, M. Guedj, K. Degt-
yarenko and R. Apweiler. 2006. ChEBI ? Chemi-
cal Entities of Biological Interest. Nucleic Acids Res,
Database Summary Paper 646.
George Demetriou and Rob Gaizauskas. 2003. Cor-
pus resources for development and evaluation of a bi-
ological text mining system. Proceedings of the Third
Meeting of the Special Interest Group on Text Mining,
Brisbane, Australia, July.
Shipra Dingare, Malvina Nissim, Jenny Finkel, Christo-
pher Manning and Claire Grover. 2005. A system for
identifying named entities in biomedical text: how re-
sults from two evaluations reflect on both the system
and the evaluations. Comparative and Functional Ge-
nomics, 6(1-2),77-85.
Nick Kemp and Michael Lynch. 1998. Extraction of In-
formation from the Text of Chemical Patents. 1. Iden-
tification of Specific Chemical Names. J. Chem. Inf.
Comput. Sci., 38:544-551.
J.-D. Kim, T. Ohta, Y. Tateisi and J. Tsujii. 2003. GE-
NIA corpus?a semantically annotated corpus for bio-
textmining. Bioinformatics, 19(Suppl 1):i180-i182.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi and Nigel Collier. 2004. Introduction
to the Bio-Entity Recognition Task at JNLPBA. Pro-
ceedings of the International Joint Workshop on Nat-
ural Language Processing in Biomedicine and its Ap-
plications, 70-75.
Seth Kulick, Ann Bies, Mark Liberman, Mark Mandel,
Ryan McDonald, Martha Palmer, Andrew Schein and
Lyle Ungar. 2004. Integrated Annotation for Biomed-
ical Information Extraction HLT/NAACL BioLINK
workshop, 61-68.
Alexander A. Morgan, Lynette Hirschman, Marc
Colosimo, Alexander S. Yeh and Jeff B. Colombe.
2004. Gene name identification and normalization us-
ing a model organism database. Journal of Biomedical
Informatics, 37(6):396-410.
Meenakshi Narayanaswamy, K. E. Ravikumar and K.
Vijay-Shanker. 2003. A Biological Named Entity
Recogniser. Pac. Symp. Biocomput., 427-438.
Irena Spasic and Sophia Ananiadou. 2004. Using
Automatically Learnt Verb Selectional Preferences
for Classification of Biomedical Terms. Journal of
Biomedical Informatics, 37(6):483-497.
Bingjun Sun, Qingzhao Tan, Prasenjit Mitra and C. Lee
Giles. 2007. Extraction and Search of Chemical For-
mulae in Text Documents on the Web. The 16th In-
ternational World Wide Web Conference (WWW?07),
251-259.
Manabu Torii and K. Vijay-Shanker. 2002. Using Unla-
beled MEDLINE Abstracts for Biological Named En-
tity Classification. Genome Informatics, 13:567-568.
Manabu Torii, Sachin Kamboj and K. Vijay-Shanker.
2004. Using name-internal and contextual features to
classify biological terms. Journal of Biomedical Infor-
matics, 37:498-511.
Joe A. Townsend, Ann A. Copestake, Peter Murray-Rust,
Simone H. Teufel and Christopher A. Waudby. 2005.
Language Technology for Processing Chemistry Pub-
lications. Proceedings of the fourth UK e-Science All
Hands Meeting, 247-253.
Alexander Vasserman. 2004. Identifying Chemical
Names in Biomedical Text: An Investigation of the
Substring Co-occurence Based Approaches. Pro-
ceedings of the Student Research Workshop at HLT-
NAACL. 7-12.
Andreas Vlachos and Caroline Gasperin. 2006. Boot-
strapping and Evaluating Named Entity Recognition
in the Biomedical Domain. Proceedings of BioNLP in
HLT-NAACL. 138-145.
W. John Wilbur, George F. Hazard, Jr., Guy Divita,
James G. Mork, Alan R. Aronson and Allen C.
Browne. 1999. Analysis of Biomedical Text for
Chemical Names: A Comparison of Three Methods.
Proc. AMIA Symp. 176-180.
Alexander Yeh, Alexander Morgan, Marc Colosimo and
Lynette Hirschman. 2005. BioCreAtIvE Task IA:
gene mention finding evaluation. BMC Bioinformat-
ics 6(Suppl I):S2.
64
Proceedings of the Linguistic Annotation Workshop, pages 191?196,
Prague, June 2007. c?2007 Association for Computational Linguistics
Panel Session: Discourse Annotation
Manfred Stede
Dept. of Linguistics
University of Potsdam
stede@ling.uni-potsdam.de
Janyce Wiebe
Dept. of Computer Science
University of Pittsburgh
wiebe@cs.pitt.edu
Eva Hajic?ova?
Faculty of Math. and Physics
Charles University
hajicova@ufal.ms.mff.cuni.cz
Brian Reese
Dept. of Linguistics
Univ. of Texas at Austin
bjreese@mail.utexas.edu
Simone Teufel
Computer Laboratory
Univ. of Cambridge
sht25@cl.cam.uk
Bonnie Webber
School of Informatics
Univ. of Edinburgh
bonnie@inf.ed.ac.uk
Theresa Wilson
Dept. of Comp. Science
Univ. of Pittsburgh
twilson@cs.pitt.edu
1 Introduction
The classical ?success story? of corpus annotation
are the various syntax treebanks that provide struc-
tural analyses of sentences and have enabled re-
searchers to develop a range of new and highly suc-
cessful data-oriented approaches to sentence pars-
ing. In recent years, however, a number of corpora
have been constructed that provide annotations on
the discourse level, i.e. information that reaches be-
yond the sentence boundaries. Phenomena that have
been annotated include coreference links, the scope
of connectives, and coherence relations. Many of
these are phenomena on whose handling there is
not a general agreement in the research community,
and therefore the question of ?recycling? corpora by
other people and for other purposes is often diffi-
cult. (To some extent, this is due to the fact that dis-
course annotation deals ?only? with surface reflec-
tions of underlying, abstract objects.) At the same
time, the efforts needed for building high-quality
discourse corpora are considerable, and thus one
should be careful in deciding how to invest those ef-
forts. One aspect of providing added-value with an-
notation projects is that of shared corpora: If a vari-
ety of annotation efforts is executed on the same pri-
mary data, the series of annotation levels can yield
insights that the creators of the individual levels had
not explicitly planned for. A clear case is the rela-
tionship between coherence relations and connective
use: When both levels are marked individually and
with independent annotation guidelines, then after-
wards the correlations between coherence relations,
cue usage (and possibly other factors, if annotated)
can be studied systematically. This conception of
multi-level annotation presupposes, of course, that
the technical problems of setting annotation levels
in correspondence to one another be resolved.
The panel on discourse annotation is organized
by Manfred Stede and Janyce Wiebe. It aims at
surveying the scene of discourse corpora, exploring
chances for synergy, and identifying desiderata for
future corpus creation projects. In preparation for
the panel, the participants have provided the follow-
ing short descriptions of the various copora in whose
construction they have been involved.
2 Prague Dependency Treebank
(Eva Hajic?ova?, Prague)
One of the maxims of the work on the Prague De-
pendency Treebank is that one should not overlook,
disregard and thus lose what the sentence structure
offers when one attempts to analyze the structure of
discourse, thus moving from ?the trees? to ?the for-
est?. Therefore, we emphasize that discourse anno-
tation should make use of every possible detail the
annotation of the component parts of the discourse,
namely the sentences, puts at our disposal. This
is, of course, not only true for the surface shape of
the sentence (i.e., the surface means of expression),
but (and most importantly) for the underlying repre-
sentation of sentences. The panel contribution will
introduce the (multilayered) annotation scenario of
the Prague Dependency Treebank and illustrate the
point using some of the particular features of the un-
derlying structure of sentences that can be made use
of in planning the scenario of discourse ?treebanks?.
191
3 SDRT in Newspaper Text
(Brian Reese, Austin)
We are currently working under the auspices of
an NSF grant to build and train a discourse parser
and codependent anaphora resolution program to
test discourse theories empirically. The training re-
quires the construction of a corpus annotated with
discourse structure and coreference information. So
far, we have annotated the MUC61 corpus for dis-
course structure and are in the process of annotating
the ACE22 corpus; both corpora are already anno-
tated for coreference. One of the goals of the project
is to investigate whether using the right frontier con-
straint improves the system?s performance in resolv-
ing anaphors. Here we detail some experiences we
have had with the discourse annotation process.
An implementation of the extant SDRT (Asher and
Lascarides, 2003) glue logic for building discourse
structures is insufficient to deal with open domain
text, and we cannot envision an extended version
at the present time able to deal with the problem.
Thus, we have opted for a machine learning based
approach to discourse parsing based on superficial
features, like BNL. To build an implementation to
test these ideas, we have had to devise a corpus of
texts annotated for discourse structure in SDRT.
Each of the 60 texts in the MUC6 corpus, and now
18 of the news stories in ACE2, were annotated by
two people familiar with SDRT. The annotators then
conferred and agreed upon a gold standard. Our
annotation effort took the hierarchical structure of
SDRT seriously and built graphs in which the nodes
are discourse units and the arcs represent discourse
relations between the units. The units could either be
simple (elementary discourse units: EDUs) or they
could be complex. We assumed that in principle the
units were recursively generated and could have an
arbitrary though finite degree of complexity.
4 Potsdam Commentary Corpus
(Manfred Stede, Potsdam)
Construction of the Potsdam Commentary Corpus
(PCC) began in 2003 and is still ongoing. It is a
1The Message Understanding Conference, www-nlpir.
nist.gov/related projects/muc/.
2The Automated Content Extraction program,
www.nist.gov/speech/tests/ace/.
genre-specific corpus of German newspaper com-
mentaries, taken from the daily papers Ma?rkische
Allgemeine Zeitung and Tagesspiegel. One central
aim is to provide a tool for studying mechanisms
of argumentation and how they are reflected on the
linguistic surface. The corpus on the one hand is a
collection of ?raw? data, which is used for genre-
oriented statistical explorations. On the other hand,
we have identified two sub-corpora that are subject
to a rich multi-level annotation (MLA).
The PCC176 (Stede, 2004) is a sub-corpus that
is available upon request for research purposes. It
consists of 176 relatively short commentaries (12-
15 sentences), with 33.000 tokens in total. The
sentences have been PoS-tagged automatically (and
manually checked); sentence syntax was anno-
tated semi-automatically using the TIGER scheme
(Brants et al, 2002) and Annotate3 tool. In addition,
we annotated coreference (PoCos (Krasavina and
Chiarcos, 2007)) and rhetorical structure according
to RST (Mann and Thompson, 1988). Our anno-
tation software architecture consists of a variety of
standard, external tools that can be used effectively
for the different annotation types. Their XML output
is then automatically converted to a generic format
(PAULA, (Dipper, 2005)), which is read into the lin-
guistic database ANNIS (Dipper et al, 2004), where
the annotations are aligned, so that the data can be
viewed and queried across annotation levels.
The PCC10 is a sub-corpus of 10 commentaries
that serves as ?testbed? for further developing the
annotation levels. On the one hand, we are apply-
ing recent guidelines on annotation of information
structure (Go?tze et al, 2007). On the other hand,
based on experiences with the RST annotation, we
are replacing the rhetorical trees with a set of dis-
tinct, simpler annotation layers: thematic structure,
conjunctive relations (Martin, 1992), and argumen-
tation structure (Freeman, 1991); these are comple-
mented by the other levels mentioned above for the
PCC176. The primary motivation for this step is the
high degree of arbitrariness that annotators reported
when producing the RST trees (see (Stede, 2007)).
By separating the thematic from the intentional in-
formation, and accounting for the surface-oriented
3www.coli.uni-saarland.de/projects/
sfb378/negra-corpus/annotate.html
192
conjunctive relations (which are similar to what is
annotated in the PDTB, see Section 6), we hope to
? make annotation easier: handling several ?sim-
ple? levels individually should be more effec-
tive than a single, very complex annotation
step;
? end up with less ambiguity in the annotations,
since the reasons for specific decisions can be
made explicit (by annotations on ?simpler? lev-
els);
? be more explicit than a single tree can be: if a
discourse fulfills, for example, a function both
for thematic development and for the writer?s
intention, they can both be accounted for;
? provide the central information that a ?tradi-
tional? rhetorical tree conveys, without loosing
essential information.
5 AZ Corpus
(Simone Teufel, Cambridge)
The Argumentative Zoning (AZ) annotation scheme
(Teufel, 2000; Teufel and Moens, 2002) is con-
cerned with marking argumentation steps in scien-
tific articles. One example for an argumentation step
is the description of the research goal, another an
overt comparison of the authors? work with rival ap-
proaches. In our scheme, these argumentation steps
have to be associated with text spans (sentences or
sequences of sentences). AZ?Annotation is the la-
belling of each sentence in the text with one of these
labels (7 in the original scheme in (Teufel, 2000)).
The AZ labels are seen as relations holding between
the meanings of these spans, and the rhetorical act
of the entire paper. (Teufel et al, 1999) reports on
interannotator agreement studies with this scheme.
There is a strong interrelationship between the ar-
gumentation in a paper, and the citations writers use
to support their argument. Therefore, a part of the
computational linguistics corpus has a second layer
of annotation, called CFC (Teufel et al, 2006) or
Citation Function Classification. CFC? annotation
records for each citation which rhetorical function it
plays in the argument. This is following the spirit of
research in citation content analysis (e.g., (Moravc-
sik and Murugesan, 1975)). An example for a ci-
tation function would be ?motivate that the method
used is sound?. The annotation scheme contains
12 functions, clustered into ?superiority?, ?neutral
comparison/contrast?, ?praise or usage? and ?neu-
tral?.
One type of research we hope to do in the future
is to study the relationship between these rhetori-
cal phonemena with more traditional discourse phe-
nomena, e.g. anaphoric expressions.
The CmpLg/ACL Anthology corpora consist of
320/9000 papers in computational linguistics. They
are partially annotated with AZ and CFC markup. A
subcorpus of 80 parallelly annotated papers (AZ and
CFF) can be obtained from us for research (12000
sentences, 1756 citations). We are currently port-
ing both schemes to chemistry in the framework
of the EPSRC-sponsored project SciBorg. In the
course of this work a larger, more general AZ an-
notation scheme was developed. The SciBorg effort
will result in an AZ/CFC?annotated chemistry cor-
pus available to the community in 2009.
In terms of challenges, the most time-consuming
aspects of creating this annotated corpus were for-
mat conversions on the corpora, and cyclic adapta-
tions of scheme and guidelines. Another problem is
the simplification of annotating only full sentences;
sometimes, annotators would rather mark a clause
or sometimes even just an NP. However, we found
these cases to be relatively rare.
6 Penn Discourse Treebank
(Bonnie Webber, Edinburgh)
The Penn Discourse TreeBank (Miltsakaki et al,
2004; Prasad et al, 2004; Webber, 2005) anno-
tates discourse relations over the Wall Street Jour-
nal corpus (Marcus et al, 1993), in terms of dis-
course connectives and their arguments. Following
the approach towards discourse structure in (Webber
et al, 2003), the PDTB takes a lexicalized approach,
treating discourse connectives as the anchors of the
relations and thus as discourse-level predicates tak-
ing two Abstract Objects as their arguments. An-
notated are the text spans that give rise to these ar-
guments. There are primarily two types of connec-
tives in the PDTB: explicit and implicit, the latter
being inserted between adjacent paragraph-internal
sentence pairs not related by an explicit connective.
193
Also annotated in the PDTB is the attribution of
each discourse relation and of its arguments (Dinesh
et al, 2005; Prasad et al, 2007). (Attribution itself
is not considered a discourse relation.) A prelimi-
nary version of the PDTB was released in April 2006
(PDTB-Group, 2006), and is available for download
at http://www.seas.upenn.edu/?pdtb. This release only has
implicit connectives annotated in three sections of
the corpus. The annotation of all implicit connec-
tives, along with a hierarchical semantic classifica-
tion of all connectives (Miltsakaki et al, 2005), will
appear in the final release of the PDTB in August
2007.
Here I want to mention three of the challenges we
have faced in developing the PDTB:
(I) Words and phrases that can function as con-
nectives can also serve other roles. (Eg, when can be
a relative pronoun, as well as a subordinating con-
junction.) It has been difficult to identify all and
only those cases where a token functions as a dis-
course connective, and in many cases, the syntactic
analysis in the Penn TreeBank (Marcus et al, 1993)
provides no help. For example, is as though always a
subordinating conjunction (and hence a connective)
or do some tokens simply head a manner adverbial
(eg, seems as though . . . versus seems more rushed
as though . . . )? Is also sometimes a discourse con-
nective relating two abstract objects and other times,
an adverb that presupposes that a particular property
holds of some other entity? If so, when one and
when the other? In the PDTB, annotation has erred
on the side of false positives.
(II) In annotating implicit connectives, we discov-
ered systematic non-lexical indicators of discourse
relations. In English, these include cases of marked
syntax (eg, Had I known the Queen would be here,
I would have dressed better.) and cases of sentence-
initial PPs and adjuncts with anaphoric or deictic
NPs such as at the other end of the spectrum, adding
to that speculation. These cases labelled ALTLEX,
for ?alternative lexicalisation? have not been anno-
tated as connectives in the PDTB because they are
fully productive (ie, not members of a more eas-
ily annotated closed set of tokens). They comprise
about 1% of the cases the annotators have consid-
ered. Future discourse annotation will benefit from
further specifying the types of these cases.
(III) The way in which spans are annotated as ar-
guments to connectives also raises a challenge. First,
because the PDTB annotates both structural and
anaphoric connectives (Webber et al, 2003), a span
can serve as argument to >1 connective. Secondly,
unlike in the RST corpus (Carlson et al, 2003) or the
Discourse GraphBank (Wolf and Gibson, 2005), dis-
course segments are not separately annotated, with
annotators then identifying what discourse relations
hold between them. Instead, in annotating argu-
ments, PDTB annotators have selected the minimal
clausal text span needed to interpret the relation.
This could comprise an embedded, subordinate or
coordinate clause, an entire sentence, or a (possi-
bly disjoint) sequence of sentences. As a result,
there are fairly complex patterns of spans within and
across sentences that serve as arguments to differ-
ent connectives, and there are parts of sentences that
don?t appear within the span of any connective, ex-
plicit or implicit. The result is that the PDTB pro-
vides only a partial but complexly-patterned cover
of the corpus. Understanding what?s going on and
what it implies for discourse structure (and possibly
syntactic structure as well) is a challenge we?re cur-
rently trying to address (Lee et al, 2006).
7 MPQA Opinion Corpus
(Theresa Wilson, Pittsburgh)
Our opinion annotation scheme (Wiebe et al, 2005)
is centered on the notion of private state, a gen-
eral term that covers opinions, beliefs, thoughts, sen-
timents, emotions, intentions and evaluations. As
Quirk et al (1985) define it, a private state is a state
that is not open to objective observation or verifica-
tion. We can further view private states in terms of
their functional components ? as states of experi-
encers holding attitudes, optionally toward targets.
For example, for the private state expressed in the
sentence John hates Mary, the experiencer is John,
the attitude is hate, and the target is Mary.
We create private state frames for three main types
of private state expressions (subjective expressions)
in text:
? explicit mentions of private states, such as
?fears? in ?The U.S. fears a spill-over?
? speech events expressing private states, such as
?said? in ?The report is full of absurdities,?
194
Xirao-Nima said.
? expressive subjective elements, such as ?full of
absurdities? in the sentence just above.
Frames include the source (experiencer) of the
private state, the target, and various properties such
as polarity (positive, negative, or neutral) and inten-
sity (high, medium, or low). Sources are nested. For
example, for the sentence ?China criticized the U.S.
report?s criticism of China?s human rights record?,
the source is ?writer, China, U.S. report?, reflecting
the facts that the writer wrote the sentence and the
U.S. report?s criticism is the target of China?s criti-
cism. It is common for multiple frames to be created
for a single clause, reflecting various levels of nest-
ing and the type of subjective expression.
The annotation scheme has been applied to a
corpus, called the ?Multi-Perspective Question An-
swering (MPQA) Corpus,? reflecting its origins in
the 2002 NRRC Workshop on Multi-Perspective
Question Answering (MPQA) (Wiebe et al, 2003)
sponsored by ARDA AQUAINT (it is also called
?OpinionBank?). It contains 535 documents and a
total of 11,114 sentences. The articles in the cor-
pus are from 187 different foreign and U.S. news
sources, dating from June 2001 to May 2002. Please
see (Wiebe et al, 2005) and Theresa Wilson?s forth-
coming PhD dissertation for further information, in-
cluding the results of inter-coder agreement studies.
References
Nicholas Asher and Alex Lascarides. 2003. Logics
of Conversation. Cambridge University Press, Cam-
bridge.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2003. Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In J. van
Kuppevelt & R. Smith, editor, Current Directions in
Discourse and Dialogue. Kluwer, New York.
Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Rashmi
Prasad, Aravind Joshi, and Bonnie Webber. 2005. At-
tribution and the (non-)alignment of syntactic and dis-
course arguments of connectives. In ACL Workshop
on Frontiers in Corpus Annotation, Ann Arbor MI.
Stefanie Dipper, Michael Go?tze, Manfred Stede, and Till-
mann Wegst. 2004. Annis: A linguistic database for
exploring information structure. In Interdisciplinary
Studies on Information Structure, ISIS Working papers
of the SFB 632 (1), pages 245?279.
Stefanie Dipper. 2005. XML-based stand-off represen-
tation and exploitation of multi-level linguistic annota-
tion. In Rainer Eckstein and Robert Tolksdorf, editors,
Proceedings of Berliner XML Tage, pages 39?50.
James B. Freeman. 1991. Dialectics and the
Macrostructure of Argument. Foris, Berlin.
Michael Go?tze, Cornelia Endriss, Stefan Hinterwimmer,
Ines Fiedler, Svetlana Petrova, Anne Schwarz, Stavros
Skopeteas, Ruben Stoel, and Thomas Weskott. 2007.
Information structure. In Information structure in
cross-linguistic corpora: annotation guidelines for
morphology, syntax, semantics, and information struc-
ture, volume 7 of ISIS Working papers of the SFB 632,
pages 145?187.
Olga Krasavina and Christian Chiarcos. 2007. Potsdam
Coreference Scheme. In this volume.
Alan Lee, Rashmi Prasad, Aravind Joshi, Nikhil Dinesh,
and Bonnie Webber. 2006. Complexity of dependen-
cies in discourse. In Proc. 5th Workshop on Treebanks
and Linguistic Theory (TLT?06), Prague.
William Mann and Sandra Thompson. 1988. Rhetorical
structure theory: Towards a functional theory of text
organization. TEXT, 8:243?281.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large scale anno-
tated corpus of English: The Penn TreeBank. Compu-
tational Linguistics, 19:313?330.
James R. Martin. 1992. English text: system and struc-
ture. John Benjamins, Philadelphia/Amsterdam.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. Annotating discourse connec-
tives and their arguments. In NAACL/HLT Workshop
on Frontiers in Corpus Annotation, Boston.
Eleni Miltsakaki, Nikhil Dinesh, Rashmi Prasad, Ar-
avind Joshi, and Bonnie Webber. 2005. Experiments
on sense annotation and sense disambiguation of dis-
course connectives. In 4t Workshop on Treebanks and
Linguistic Theory (TLT?05), Barcelona, Spain.
Michael J. Moravcsik and Poovanalingan Murugesan.
1975. Some results on the function and quality of ci-
tations. Soc. Stud. Sci., 5:88?91.
The PDTB-Group. 2006. The Penn Discourse TreeBank
1.0 annotation manual. Technical Report IRCS 06-01,
University of Pennsylvania.
195
Rashmi Prasad, Eleni Miltsakaki, Aravind Joshi, and
Bonnie Webber. 2004. Annotation and data mining
of the Penn Discourse TreeBank. In ACL Workshop
on Discourse Annotation, Barcelona, Spain, July.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Aravind Joshi,
and Bonnie Webber. 2007. Attribution and its annota-
tion in the Penn Discourse TreeBank. TAL (Traitement
Automatique des Langues.
Randolph Quirk, Sidney Greenbaum, Geoffry Leech, and
Jan Svartvik. 1985. A Comprehensive Grammar of the
English Language. Longman, New York.
Manfred Stede. 2004. The Potsdam commentary corpus.
In Proceedings of the ACL Workshop on Discourse An-
notation, pages 96?102, Barcelona.
Manfred Stede. 2007. RST revisited: disentangling nu-
clearity. In Cathrine Fabricius-Hansen and Wiebke
Ramm, editors, ?Subordination? versus ?coordination?
in sentence and text ? from a cross-linguistic perspec-
tive. John Benjamins, Amsterdam. (to appear).
Simone Teufel and Marc Moens. 2002. Summaris-
ing scientific articles ? experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409?446.
Simone Teufel, Jean Carletta, and Marc Moens. 1999.
An annotation scheme for discourse-level argumenta-
tion in research articles. In Proceedings of the 9th Eu-
ropean Conference of the ACL (EACL-99), pages 110?
117, Bergen, Norway.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. An annotation scheme for citation function. In
Proceedings of SIGDIAL-06, Sydney, Australia.
Simone Teufel. 2000. Argumentative Zoning: Infor-
mation Extraction from Scientific Text. Ph.D. thesis,
School of Cognitive Science, University of Edinburgh,
Edinburgh, UK.
Bonnie Webber, Matthew Stone, Aravind Joshi, and Al-
istair Knott. 2003. Anaphora and discourse structure.
Computational Linguistics, 29:545?587.
Bonnie Webber. 2005. A short introduction to the Penn
Discourse TreeBank. In Copenhagen Working Papers
in Language and Speech Processing.
Janyce Wiebe, Eric Breck, Chris Buckley, Claire Cardie,
Paul Davis, Bruce Fraser, Diane Litman, David Pierce,
Ellen Riloff, Theresa Wilson, David Day, and Mark
Maybury. 2003. Recognizing and organizing opinions
expressed in the world press. In Working Notes of the
AAAI Spring Symposium in New Directions in Ques-
tion Answering, pages 12?19, Palo Alto, California.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions
in language. Language Resources and Evaluation,
39(2/3):164?210.
Florian Wolf and Edward Gibson. 2005. Representing
discourse coherence: A corpus-based study. Compu-
tational Linguistics, 31:249?287.
196
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2?13, Dublin, Ireland, August 23-29 2014.
Unsupervised learning of rhetorical structure with un-topic models
Diarmuid
?
O S
?
eaghdha
Computer Laboratory
University of Cambridge
Cambridge, UK
do242@cam.ac.uk
Simone Teufel
Computer Laboratory
University of Cambridge
Cambridge, UK
sht25@cam.ac.uk
Abstract
In this paper we investigate whether unsupervised models can be used to induce conventional
aspects of rhetorical language in scientific writing. We rely on the intuition that the rhetorical
language used in a document is general in nature and independent of the document?s topic.
We describe a Bayesian latent-variable model that implements this intuition. In two empirical
evaluations based on the task of argumentative zoning (AZ), we demonstrate that our generality
hypothesis is crucial for distinguishing between rhetorical and topical language and that features
provided by our unsupervised model trained on a large corpus can improve the performance of a
supervised AZ classifier.
1 Introduction
Scientific writing has many conventions. Some exist at the level of sentence construction, such as
a preference for the passive voice or for deverbal nominalisations. Others relate to the high-level
organisation of a paper: a typical paper at an NLP conference may be divided into sections covering the
introduction, related work, methods, experimental results and conclusion. There are also intermediate
levels of convention that use lexical and phrasal items to signal the role played by each part of the text in
the argument the authors wish to construct. The theory of argumentative zoning (AZ) describes how a
scientific article can be analysed in terms of text blocks (or zones) that share a rhetorical function (Teufel,
2010). For example: part of the article may consist of background information, another part may describe
the aim of the research, other parts may report the authors? own work or compare that work to alternative
approaches in the literature. Supervised computational systems can be trained to mark up the AZ structure
of a text automatically (see Section 2); the output of such systems has been shown to aid summarisation
and human browsing of the scientific literature (Teufel and Moens, 2002; Guo et al., 2011a; Contractor et
al., 2012). However, supervised systems require manually annotated training data that must be created
anew for each discipline (and language) before they can be deployed, while large quantities of unannotated
text are often available. For this reason, there is considerable value in developing unsupervised systems
that induce aspects of rhetorical structure from unannotated text.
In this paper we advance a hypothesis about the generality of rhetorical language. We propose that the
words and linguistic constructs used to express rhetorical function in a scientific paper are independent
of the paper?s topic. Naturally there will be some variation across research areas and there may be large
differences across disciplines, but within a discipline we do not expect that the specific subject of a paper
plays a significant role in how the authors construct their argument. For example, the following template
could be used to generate an abstract for very many papers in NLP and other fields:
The problem of has received a lot of attention because of its relevance to
. CITATION proposed an approach based on the method of .
In this paper we present a method for that has the following advantages over prior work:
. We demonstrate the empirical effectiveness of our method by reporting
experiments on data, where it outperforms the approach of CITATION by %.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
2
This leads us to the idea of two-stage ?recipes? for scientific papers, whereby the authors start with a
framework of boilerplate text that matches the rhetorical argument they wish to make. The authors can
then fill in the gaps with the substance of their research contribution.
The two-stage model is of course an idealisation of how scientists construct their papers, but it is
useful as an inspiration for a computational model that implements the generality hypothesis. We propose
BOILERPLATE-LDA, a generative model that assigns responsibility for generating each word in an
abstract to a document-specific topic model or to a rhetorical language model that is not specific to
the document. Essentially, we induce argumentative structure from the parts of the text that are not
well-explained by the topic model. Hence we describe BOILERPLATE-LDA as an ?un-topic model?. We
evaluate our model in two settings: a clustering evaluation that treats BOILERPLATE-LDA as performing
unsupervised argumentative zoning, and a downstream evaluation where the induced structure is not taken
as explicitly modelling argumentative zones but is used to provide informative features for a supervised
AZ classifier. In both cases, we show that BOILERPLATE-LDA performs well on a very challenging task.
2 Related work
There has been great interest in unsupervised learning among NLP researchers due to the availability
of large amounts of unprocessed text through the Web, newswire providers, scientific repositories and
other sources in contrast to the onerous requirements of creating task-specific manually annotated data
for training supervised analysers. Particularly relevant to our work is the field of topic modelling, where
Bayesian latent-variable models are used to induce meaningful generalisations from observations of
co-occurrences. Blei et al. (2003) introduced Latent Dirichlet Allocation (LDA) as a model of thematic
structure in documents, but subsequent work has adapted the general framework to many different purposes
in modelling text as well as other kinds of data. This includes research on modelling aspects of document
structure such as topic segmentation, implementing the intuitions that neighbouring blocks of text are
coherent in the sense of lexical similarity (Purver et al., 2006; Gruber et al., 2007; Eisenstein and Barzilay,
2008; Du et al., 2013). The model most similar to ours (that we are aware of) is the model of Ritter et
al. (2010), which captures dialogue acts and transitions between them in Twitter conversations.
Despite the general popularity of unsupervised approaches, rhetorical analysis has generally been
treated as a problem for supervised machine learning. Classification-based approaches to argumentative
zoning typically use a sequence classifier such as a maximum-entropy Markov model or conditional
random field (Teufel and Moens, 2002; Siddharthan and Teufel, 2007; Hirohata et al., 2008; Guo et al.,
2010). Guo et al. (2011b) take a semi-supervised approach based on active learning and self-training.
Two unsupervised approaches in the literature are Varga et al. (2012) and Reichart and Korhonen
(2012). Varga et al. use a topic model variant called ZONE-LDA that assigns each sentence a latent
variable index or ?topic? and assumes that the words in the sentence are generated from a distribution
particular to the topic; in this situation each topic is assumed to correspond to a distinct argumentative
zone. Such a model will have the effect of clustering sentences that share lexical items. Varga et al. also
propose a model they call ZONE-LDA-B, in which some common words are assigned to a ?background?
distribution that is independent of the sentence category; this model performs worse than ZONE-LDA in
their evaluation. Reichart and Korhonen take an approach based on Markov random fields. They construct
a graphical model in which sentence vertices are connected by potentials weighted according to adjacency
and sentence similarity, as well as hand-defined rules about passivisation and sentence location.
The papers cited in the two preceding paragraphs have focused on rhetorical analysis in scientific
writing, yet there are many other textual genres where argumentation is conventionalised. For example,
Burstein et al. (2003) identify building blocks analogous to AZ zones in the writing of English language
learners and demonstrate that a supervised classification approach can be used to mark up their essays.
Also in the educational domain, Madnani et al. (2012) train a supervised classifier to detect the ?shell?
language that learners use to organise the high-level structure of their compositions; this is quite close
to our idea of ?templates? or ?recipes? for scientific papers. Sauper and Barzilay (2009) and Chen et
al. (2009) both present models that learn structural conventions in Wikipedia articles without relying on
human annotation. Sauper and Barzilay?s model induces the typical section structure of Wikipedia articles
3
about a specific entity type (e.g., Actors or Diseases) and retrieves web snippets relevant to each section
for a target entity, before performing multidocument summarisation to produce a new entry for posting
to Wikipedia. Chen et al. take a Bayesian segmentation approach to implicitly learn the topical section
structure of articles and use a generalised Mallows model, a distribution over permutations, to identify a
canonical ordering for sections.
1
Other forms of general rhetorical analysis include Rhetorical Structure
Theory (Mann and Thompson, 1988; Marcu, 2000), which captures local discourse relations between
segments of text; RST provides a layer of analysis that is separate and complementary to more global
schemes such as argumentative zoning.
3 Intuitions
The performance of unsupervised learning depends on how intuitions about the task are incorporated in
the statistical model. Our approach relies on three main intuitions:
Sentence similarity: All else being equal, we expect that lexically similar sentences will have similar
purposes. At the same time, lexical similarity alone is not sufficient to capture shared argumentative
function: all sentences in a paper about parsing will be similar to each other, while the introductory
sentences of a parsing paper and a machine translation paper may share few similar lexical items.
Adjacency: The theory of argumentative zones suggests that sentences with the same rhetorical function
will often be grouped together into blocks. Additionally, we expect that authors will follow general
conventions about the order of zones, e.g., starting with background and goal statements and
progressing to results and conclusions.
Generality: We expect that the language used to convey rhetorical function is independent of the topical
content of the paper.
Sentence similarity can be captured using standard lexical similarity measures or through the clustering
effects of a topic model. The adjacency assumption can be implemented using a linear-chain sequence
model such as a Hidden Markov Model. The ZONE-LDA approach of Varga et al. (2012) relies on
sentence similarity alone. Reichart and Korhonen?s (2012) model combines sentence similarity and
adjacency. To the best of our knowledge, the generality hypothesis has not previously been investigated.
The model we describe in Section 4 incorporates all three intuitions in its structure.
4 Models
The model we propose assumes that each word in a sentence is generated either from an LDA-style topic
model or from a distribution associated with the rhetorical category assigned to the sentence. The former
captures the subject matter of the document; the latter captures conventional language that is independent
of the document?s subject matter. The sentence categories are generated from a first-order Markov model.
The assignment of responsibility for a word is implemented through a so-called ?switching variable?, a
binary-valued latent variable. This is a commonly used mechanism for interpolating language models
(Griffiths et al., 2004; Reisinger and Mooney, 2010; Ahmed and Xing, 2010); in many cases, the goal is to
assign common words to a ?background? distribution that is not considered an object of interest from a
topic modelling perspective. In our case it is this non-topical part of the text that is the object of interest.
The dependencies between variables in our full BOILERPLATE-LDA model are shown by the plate
diagram in Figure 1. The corresponding ?generative story? is as follows:
1
It would be interesting to swap in Chen et al.?s generalised Mallows model for the HMM-style ordering model in
BOILERPLATE-LDA. The former has the advantage of capturing non-local ordering effects, while the latter has the advantage of
not assuming a single canonical ordering.
4
for topic t ? {1 . . . |T |} do
(Draw a distribution over words)
?
t
? Dirichlet(?)
end for
for zone z ? {1 . . . |Z|} do
(Draw a distribution over words)
?
z
? Dirichlet(?)
(Draw a transition distribution)
?
z
? Dirichlet(?)
end for
(Draw the switch distribution)
? ? Beta(?
0
, ?
1
)
for doc d ? {1 . . . |D|} do
(Draw a distribution over topics)
?
d
? Dirichlet(?)
for sentence s ? Sentences(d) do
z
s
?Multinomial(?
z
s?1
)
for word i ?Words(s) do
(Draw a switch indicator)
b
i
= Beta(?)
if b
i
= 0 then
(Draw a word from the zone-word distribution)
w
i
?Multinomial(?
z
s
)
else
(Draw a topic)
t
i
?Multinomial(?
d
)
(Draw a word from the topic-word distribution)
w
i
?Multinomial(?
t
i
)
end if
end for
end for
end for
We train the model using Gibbs sampling. Due to Dirichlet-multinomial and beta-Bernoulli conjugacy
it is relatively straightforward to integrate out the multinomial and Bernoulli distribution parameters
?, ?, ? and ? and derive update rules for a collapsed Gibbs sampler. Each iteration of the sampler visits
each sentence in the corpus in turn, first sampling the sentence label assignment z
s
and then sampling for
each word in the sentence the switch indicator b
i
and (if b
i
= 1) the topic assignment t
i
. The sentence
label update is performed using what Gao and Johnson (2008) call a pointwise collapsed Gibbs sampler.
Omitting hyperparameters for clarity, the sampling probabilities can be written as
P (z
i
= z|z
?i
,w, b) ?
f
z
i?1
?z
+ ?
z
f
z
i?1
+
?
z
?
?
z
?
f
z?z
i+1
+ I(z = z
i+1
) + ?
z
i+1
f
?i
z
+ I(z = z
i+1
) +
?
z
?
?
z
?
?
v?V
?(f
?i
zv,b=0
+ f
s
i
v,b=0
+ ?)
?(f
?i
z
+ f
s
i
+ ?|V |)
(1)
where f
z?>z
?
is the transition frequency from zone z to zone z
?
, f
z
is the number of sentences assigned
zone z; I(z = z
i+1
) has value 1 if the two zone assignments are equal and 0 otherwise; V is the vocabulary
of word types; f
zv,b=0
is the number of words of type z that appear in sentences assigned zone z and
whose corresponding switch variable has value 0; f
s
i
v,b=0
is the number of words of type v that appear in
sentence s
i
and whose corresponding switch variable has value 0; the superscript
?i
indicates that the
frequency is calculated over all sentences except s
i
. We introduce observed start and end state variables
z
s
and z
e
to handle the boundaries at the beginning and end of each document.
5
zs
z
1
z
2
z
n
z
e
w w
w
t t
t
Words(s
1
) Words(s
2
)
Words(s
n
)
b b
b
?
?
D
?
?
0
?
1
??
Z
?
?
Z
?
?
T
Figure 1: Plate diagram for BOILERPLATE-LDA
The topic and switch variables for each word are sampled in a blocked fashion; the sampling probabilities
are similar to the standard LDA updates:
P (b
j
= 0, t
j
= ?|z
?j
,b
?j
, t,w) ? (f
?j
b=0
+ ?
0
)
f
?j
z
i
w
j
,b=0
+ ?
f
z
i
,b=0
+ |V |?
P (b
j
= 1, t
j
= t|z
?j
,b
?j
, t,w) ? (f
?j
b=1
+ ?
1
)
f
?j
tw
j
+ ?
z
f
?j
w
j
,b=1
+
?
z
?
?
z
?
f
?j
zw
j
+ ?
f
?j
z
+ |V |?
P (b
i
= 0, t
i
6= ?|z
?j
,b
?j
, t,w) = 0
P (b
i
= 1, t
i
= ?|z
?j
,b
?j
, t,w) = 0 (2)
where we use j to index words and i to index sentences; f
tw
j
is the number of words of type w
j
that are
assigned topic t; the superscript
?j
indicates that the frequency is calculated over all words except j.
5 Experiments
5.1 Data
For evaluation, we use a collection of abstracts compiled by Guo et al. (2010). These abstracts had
originally been collected in the context of semi-automated cancer risk assessment by searching PubMed
for abstracts mentioning one or more of a list of chemicals known to have carcinogenic properties
(Korhonen et al., 2009). Guo et al. annotated abstracts for five of these chemicals using an AZ scheme
with seven categories: Background, Objective, Method, Result, Conclusion, Related work and Future
work.
2
In order to test whether our models can also perform over a large, heterogeneous dataset, we also
used a collection of 129,595 abstracts taken from a collection of open-access journal articles. Preprocessing
involved sentence splitting, tokenisation and part-of-speech tagging using the Stanford CoreNLP toolkit
3
and the removal of all tokens containing non-alphanumeric characters, all tokens of character length one
2
The annotated dataset has been made available at http://www.cl.cam.ac.uk/
?
yg244/abstract_az.html.
3
http://nlp.stanford.edu/software/corenlp.shtml
6
and a small set of stop words.
4
This left a training corpus of 16,841,280 tokens.
5.2 Clustering Evaluation
5.2.1 Evaluation
Our first quantitative evaluation investigates whether the zones induced by BOILERPLATE-LDA corre-
spond to the argumentative zones identified by human theorists. We treat this as a clustering task with
the gold standard provided by Guo et al.?s (2010) dataset. The clustering evaluation measures we use
are the Adjusted Rand Index (Hubert and Arabie, 1985) and Adjusted Mutual Information (Vinh et al.,
2010); both measures are normalised to have a maximum value of 1 and are adjusted for chance so that
the expected score given to a random clustering is 0. This second property makes them conservative in
comparison to other evaluation measures. We report results with the number of zones |Z| ? {10, 20, 50}
and number of topics |T | ? {10, 20, 50, 100}; for each combination of settings we report the average
evaluation score attained by three independent runs of the learning algorithm.
5.2.2 Models
For our evaluation, we test the following models:
BOILERPLATE-LDA: Our full model, as described in Section 4.
BOILERPLATE-LDA-MULT: A simplified model where the Markov dependencies between zone as-
signments are replaced by a flat multinomial; the probability of a zone is independent of the adjacent
sentences.
BOILERPLATE-LDA-NOTOPICS: A simplified model where all words in a sentence are generated
from the zone distribution ?
z
s
; this is almost identical to Varga et al.?s (2012) ZONE-LDA model.
K-MEANS: A standard k-means clustering model run until convergence. The features for each sentence
consist of tf-idf-transformed lexical frequencies, part-of-speech tags and a location feature computed
by dividing the abstract into 5 bins.
The BOILERPLATE-LDA models are all trained for 1000 iterations of Gibbs sampling. The Dirichlet
hyperparameters are re-estimated every 10 iterations; the topic hyperparameters ? are optimised using
a fixed-point iteration to maximise the log-evidence (Minka, 2003; Wallach, 2008), while the other
hyperparameters are sampled using Hamiltonian Monte Carlo (Neal, 2010). K-MEANS was run until
convergence.
5.2.3 Results
Figure 2 gives an illustration of the zone representation induced at the end of one run of BOILERPLATE-
LDA with the settings |Z| = 10, |T | = 100. Firstly, we list the most probable words for each zone
(2a). While the model may not find a perfect match for the gold-standard inventory of argumentative
zones, we can see that some induced zones describe standard methodology (8,9), others describe results
and implications (1,3,7) and others describe motivations (2,5,6). Inspection of the transition matrix (2b)
confirms our expectation that self-transitions have the highest probability; we also observed that the
zones most frequently transitioned to from the start state are the motivational zones and the zones most
frequently transitioned from to the end state are the results/implications zones. The example abstracts in
Figure 3 illustrate how BOILERPLATE-LDA can be used to mark up the text of an abstract as ?boilerplate?
or ?non-boilerplate? based on the values of the switch variables b
i
.
4
The part-of-speech tags are not used by BOILERPLATE-LDA but they are used as features for other models.
7
1 results, suggest, our, data, study, role, findings, we, between, indicate, important, studies
2 study, we, using, used, investigated, determine, present, between, investigate, analysis, aim
3 increased, significantly, levels, showed, found, observed, significant, after, compared, higher
4 two, sequence, we, found, region, sequences, we, three, identified, between, different, analysis
5 use, more, studies, study, used, however, important, health, most, treatment, clinical, potential
6 role, important, known, studies, however, shown, including, involved, mechanisms, cell
7 case, we, patient, report, rare, most, common, reported, presented, disease, associated, cause
8 CI, significantly, respectively, significant, between, group, mean, higher, compared, more, found
9 study, years, using, two, patients, included, total, group, three, data, after, used, collected, age
10 we, data, analysis, used, using, new, approach, based, method, information, developed, more
(a) Most probable words for each zone
From
To
Start 1 2 3 4 5 6 7 8 9 10 End
Start 0.00 0.00 0.10 0.01 0.08 0.24 0.36 0.10 0.00 0.03 0.08 0.00
1 0.00 0.37 0.01 0.04 0.01 0.03 0.01 0.00 0.00 0.00 0.01 0.50
2 0.00 0.02 0.26 0.25 0.07 0.01 0.02 0.00 0.05 0.29 0.01 0.00
3 0.00 0.27 0.02 0.59 0.02 0.02 0.02 0.00 0.02 0.00 0.01 0.04
4 0.00 0.12 0.02 0.09 0.62 0.00 0.03 0.00 0.01 0.00 0.05 0.05
5 0.00 0.01 0.10 0.00 0.00 0.55 0.03 0.02 0.00 0.06 0.04 0.18
6 0.00 0.06 0.21 0.05 0.05 0.05 0.50 0.01 0.00 0.01 0.05 0.02
7 0.00 0.02 0.02 0.01 0.01 0.15 0.04 0.63 0.01 0.02 0.00 0.08
8 0.00 0.09 0.01 0.14 0.01 0.07 0.00 0.02 0.61 0.04 0.01 0.01
9 0.00 0.01 0.11 0.05 0.01 0.05 0.00 0.02 0.20 0.54 0.01 0.00
10 0.00 0.05 0.02 0.01 0.07 0.02 0.01 0.00 0.01 0.01 0.68 0.12
End 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
(b) Zone transition probabilities between adjacent sentences
Figure 2: Zones induced by one run of BOILERPLATE-LDA (|Z| = 10, |T | = 100)
8
VASP: A Volumetric Analysis of Surface Properties Yields Insights into Protein-
Ligand Binding Specificity
Many algorithms that compare protein structures can reveal similarities that suggest
related biological functions, even at great evolutionary distances. Proteins with re-
lated function often exhibit differences in binding specificity, but few algorithms
identify structural variations that effect specificity. To address this problem, we de-
scribe the Volumetric Analysis of Surface Properties (VASP), a novel volumetric
analysis tool for the comparison of binding sites in aligned protein structures. VASP
uses solid volumes to represent protein shape and the shape of surface cavities,
clefts and tunnels that are defined with other methods. Our approach, inspired by
techniques from constructive solid geometry, enables the isolation of volumetri-
cally conserved and variable regions within three dimensionally superposed volumes.
We applied VASP to compute a comparative volumetric analysis of the ligand binding
sites formed by members of the steroidogenic acute regulatory protein (StAR)-related
lipid transfer (START) domains and the serine proteases. Within both families, VASP
isolated individual amino acids that create structural differences between ligand bind-
ing cavities that are known to influence differences in binding specificity. Also, VASP
isolated cavity subregions that differ between ligand binding cavities which are essen-
tial for differences in binding specificity. As such, VASP should prove a valuable tool
in the studyof protein-ligand binding specificity.
A new usage of functionalized oligodeoxynucleotide probe for site-specific
modification of a guanine base within RNA
Site-specific modification of RNA is of great significance to investigate RNA structure,
function and dynamics. Recently, we reported a new method for sequence- and
cytosine-selective chemical modification of RNA based on the functional group trans-
fer reaction of the 1-phenyl-2-methylydene-1,3-diketone unit of the 6-thioguanosine
base incorporated in the oligodeoxynucleotide probe. In this study, we describe that
the functionality transfer rate is greatly enhanced and the selectivity is shifted to the
guanine base when the reaction is performed under alkaline conditions.
Detailed investigation indicated that the 2-amino group of the enolate form of rG is
the reactant of the functionality transfer reaction. As a potential application of this
efficient functionality transfer reaction, a pyrene group as a relatively large fluorescent
group was successfully transferred to the target guanine base of RNA with a high
guanine and site selectivity. This functionality transfer reaction with high efficiency and
high site-selectivity would provide a new opportunity as a unique tool for the study of
RNA.
Figure 3: Examples of abstracts marked up for boilerplate (underlined) and non-boilerplate (faded text) by
BOILERPLATE-LDA
9
Model |T | |Z| = 10 |Z| = 20 |Z| = 50
ARI NMI ARI NMI ARI NMI
BOILERPLATE-LDA 10 0.19 0.15 0.09 0.09 0.04 0.07
20 0.20 0.16 0.03 0.10 0.03 0.08
50 0.26 0.21 0.18 0.16 0.05 0.10
100 0.32 0.28 0.20 0.20 0.07 0.14
BOILERPLATE-LDA-MULT 10 0.13 0.11 0.08 0.08 0.04 0.06
20 0.10 0.13 0.04 0.09 0.03 0.07
50 0.21 0.16 0.13 0.14 0.06 0.10
100 0.18 0.16 0.14 0.14 0.07 0.11
BOILERPLATE-LDA-NOTOPICS 0 0.00 0.02 0.04 0.05 0.06 0.05
K-MEANS 0 0.05 0.05 0.03 0.06 0.03 0.04
Table 1: Results of the clustering evaluation. |Z| is the number of zones; |T | is the number of topics.
The results of the clustering evaluation are presented in Table 1. Clearly, this is a challenging task; the
BOILERPLATE-LDA-NOTOPICS and K-MEANS models, which do not filter out topic-specific vocabulary,
perform little better than chance in terms of identifying argumentative zones (recall that for the ARI
and AMI measures, zero means ?not greater than expected by chance? rather than ?no correlation at
all?). BOILERPLATE-LDA-MULT performs better than those models though not as well as the full
BOILERPLATE-LDA model, indicating that sequential structure is important for inducing rhetorical
regularities. In general, the best results are attained with low settings of |Z| and high settings of |T |; this
seems to create the ?bottleneck? effect needed to focus the model on purely rhetorical information. The
highest scores (ARI = 0.32, AMI = 0.28) are attained by BOILERPLATE-LDA with the settings |Z| = 10,
|T | = 100.
5.3 Supervised Evaluation
5.3.1 Evaluation
A second evaluation of BOILERPLATE-LDA?s usefulness is to test whether it can yield features that
improve the performance of a supervised argumentative zoning system. It is possible for an unsupervised
model to induce structure that does not map exactly onto a pre-existing set of labels but still captures
valuable information about the underlying phenomenon that can be of use to a supervised classifier when
combined with other information sources. To this end, we train and evaluate supervised models on the
same dataset of Guo et al. (2010) that we used for the clustering evaluation. We perform 10-fold cross-
validation and report Accuracy (proportion of sentences labelled correctly) as well as macro-averaged
Precision, Recall and F-Score. To measure statistical significance we use two-tailed paired t-tests,
following Dietterich (1998).
5
5.3.2 Models
We use two supervised sequence classification algorithms for training models:
LR: A logistic regression classifier with a ?history? feature encoding the previous sentence?s label, trained
with L
1
regularisation, using the implementation in LibLinear.
6
CRF: A first-order conditional random field classifier, trained with L
1
regularisation, using the imple-
mentation in Mallet.
7
In both cases, the predicted labelling for a test document is given by the most probable (Viterbi) sequence
according to the trained model. We use the following feature sets:
5
In order to address concerns about the suitability of the t-tests under non-normality, we replicated the tests using Wilcoxon?s
signed-ranks test as recommended by Dem?sar (2006); the results were identical.
6
http://www.csie.ntu.edu.tw/
?
cjlin/liblinear/
7
http://mallet.cs.umass.edu/
10
LR CRF
Model Acc P R F Acc P R F
BASELINE 0.83 0.71 0.70 0.70 0.85 0.75 0.64 0.67
+BOILERPLATE-LDA 0.84 0.72 0.71 0.71 0.86 0.74 0.65 0.68
+LDA-BAG (50) 0.83 0.69 0.68 0.68 0.84 0.73 0.62 0.64
+LDA-BAG (100) 0.83 0.69 0.69 0.69 0.84 0.72 0.64 0.66
+LDA-MAX (50) 0.83 0.71 0.69 0.69 0.85 0.72 0.64 0.66
+LDA-MAX (100) 0.84 0.71 0.69 0.70 0.85 0.74 0.63 0.66
Table 2: Results of the supervised evaluation
BASELINE: Our baseline set of features is a standard set for supervised argumentative zoning: all
unigrams and bigrams in the sentence, all part-of-speech tags in the sentence and a location feature
computed by dividing the abstract into 5 bins.
+BOILERPLATE-LDA: The baseline model with additional features corresponding to the zone index
assigned by BOILERPLATE-LDA to the sentence. We set |Z| = 10, |T | = 100 since that setting
performed best in the clustering evaluation. As before, we use the output of three independently
learned sampling chains, giving each sentence three zone features; the classifier should learn which
chains are better than others during training.
+LDA-BAG: The baseline model with additional features derived from standard Latent Dirichlet Alloca-
tion models trained on the same corpus as BOILERPLATE-LDA. As LDA assigns a topic to each
word in a sentence, we add all topics assigned to all words in the sentence as additional features. As
above, we use the output of three sampling chains. We report results for models with 50 topics and
100 topics.
+LDA-MAX: The baseline model with additional features derived from LDA models. Here each model
assigns each sentence the single topic assigned to the greatest number of words in the sentence (ties
are broken randomly).
5.3.3 Results
Results for the supervised evaluation are presented in Table 2. +BOILERPLATE-LDA is the only aug-
mented feature set that consistently gives an improvement over the baseline features. The improvements
in accuracy are statistically significant (p < 0.01). In every case but one (which is not statistically
significant), the LDA models fail to improve on the baseline in either accuracy or F-Score, showing that
the latent structure induced by BOILERPLATE-LDA captures aspects of rhetorical language that are not
captured by topical word clustering.
6 Conclusion
We consider the work presented in this paper to be a first step towards the ambitious goal of inducing
latent descriptions of the templates used by scientists and writers in other fields. We have shown how our
hypothesis about the generality of rhetorical language allows the construction of models that can separate
out topical and rhetorical language use. One focus for future work will be to enrich the model structure; an
approach based on adaptor grammars (Johnson et al., 2006) could be used to break the reductive unigram
assumption in BOILERPLATE-LDA and identify multiword collocations that carry rhetorical information.
Another focus will be to broaden our understanding of how unsupervised rhetorical models trained on
large corpora can improve the robustness of supervised systems. For example, we have observed that
lexicalised AZ classifiers trained on texts from one scientific domain will often perform poorly on texts
from another domain; unsupervised models have the potential to induce relevant lexical commonalities
across domains.
11
Acknowledgements
This research is supported by the Intelligence Advanced Research Projects Activity (IARPA) via De-
partment of Interior National Business Center (DoI/NBC) contract number D11PC20153. The U.S.
Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding
any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of
the authors and should not be interpreted as necessarily representing the official policies or endorsements,
either expressed or implied, of IARPA, DoI/NBC, or the U.S. Government.
References
Amr Ahmed and Eric P. Xing. 2010. Staying informed: Supervised and semi-supervised multi-view topical
analysis of ideological perspective. In Proceedings of EMNLP-10, Cambridge, MA.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:993?1022.
Jill Burstein, Daniel Marcu, and Kevin Knight. 2003. Finding the WRITE stuff: Automatic identification of
discourse structure in student essays. IEEE Intelligent Systems, 18(1):32?39.
Harr Chen, S.R.K. Branavan, Regina Barzilay, and David R. Karger. 2009. Global models of document structure
using latent permutations. In Proceedings of NAACL-09, Boulder, CO.
Danish Contractor, Yufan Guo, and Anna Korhonen. 2012. Using argumentative zones for extractive summariza-
tion of scientific articles. In Proceedings of COLING-12, Mumbai, India.
Janez Dem?sar. 2006. Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning
Research, 7:1?30.
Thomas G. Dietterich. 1998. Approximate statistical tests for comparing supervised classification learning algo-
rithms. Neural Computation, 10(7):1895?1923.
Lan Du, Wray Buntine, and Mark Johnson. 2013. Topic segmentation with a structured topic model. In Proceed-
ings of NAACL-13, Atlanta, GA.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian unsupervised topic segmentation. In Proceedings of
EMNLP-08, Honolulu, HI.
Jianfeng Gao and Mark Johnson. 2008. A comparison of Bayesian estimators for unsupervised Hidden Markov
Model POS taggers. In Proceedings of EMNLP-08, Honolulu, HI.
Thomas L. Griffiths, Mark Steyvers, David M. Blei, and Joshua B. Tenenbaum. 2004. Integrating topics and
syntax. In Proceedings of NIPS-04, Vancouver, BC.
Amit Gruber, Michal Rosen-Zvi, and Yair Weiss. 2007. Hidden topic Markov models. In Proceedings of AISTATS-
07, San Juan, Puerto Rico.
Yufan Guo, Anna Korhonen, Maria Liakata, Ilona Silins, Lin Sun, and Ulla Stenius. 2010. Identifying the informa-
tion structure of scientific abstracts: An investigation of three different schemes. In Proceedings of BioNLP-10,
Uppsala, Sweden.
Yufan Guo, Anna Korhonen, Maria Liakata, Ilona Silins, Johan H?ogberg, and Ulla Stenius. 2011a. A comparison
and user-based evaluation of models of textual information structure in the context of cancer risk assessment.
BMC Bioinformatics, 12:69.
Yufan Guo, Anna Korhonen, and Thierry Poibeau. 2011b. A weakly-supervised approach to argumentative zoning
of scientific documents. In Proceedings of EMNLP-11, Edinburgh, UK.
Kenji Hirohata, Naoaki Okazaki, Sophia Ananiadou, and Mitsuru Ishizuka. 2008. Identifying sections in scientific
abstracts using conditional random fields. In Proceedings of IJCNLP-08, Hyderabad, India.
Lawrence Hubert and Phipps Arabie. 1985. Comparing partitions. Journal of Classification, 2(1):193?218.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2006. Adaptor grammars: A framework for specifying
compositional nonparametric Bayesian models. In Proceedings of NIPS-06, Vancouver, BC.
12
Anna Korhonen, Ilona Silins, Lin Sun, and Ulla Stenius. 2009. The first step in the development of text min-
ing technology for cancer risk assessment: identifying and organizing scientific evidence in risk assessment
literature. BMC Bioinformatics, 10:303.
Nitin Madnani, Michael Heilman, Joel Tetreault, and Martin Chodorow. 2012. Identifying high-level organiza-
tional elements in argumentative discourse. In Proceedings of NAACL-12, Montreal, QC.
William C. Mann and Sandra A. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Daniel Marcu. 2000. The rhetorical parsing of unrestricted texts: A surface-based approach. Computational
Linguistics, 26(3):395?448.
Thomas P. Minka. 2003. Estimating a Dirichlet distribution. Available at http://research.microsoft.
com/en-us/um/people/minka/papers/dirichlet/.
Radford M. Neal. 2010. MCMC using Hamiltonian dynamics. In Steve Brooks, Andrew Gelman, Galin L. Jones,
and Xiao-Li Meng, editors, Handbook of Markov Chain Monte Carlo. Chapman and Hall/CRC Press, Boca
Raton, FL.
Matthew Purver, Konrad K?ording, Tom Griffiths, and Josh Tenenbaum. 2006. Unsupervised topic modelling for
multi-party spoken discourse. In Proceedings of COLING-ACL-06, Sydney, Australia.
Roi Reichart and Anna Korhonen. 2012. Document and corpus level inference for unsupervised and transductive
learning of information structure of scientific documents. In Proceedings of COLING-12, Mumbai, India.
Joseph Reisinger and Raymond Mooney. 2010. A mixture model with sharing for lexical semantics. In Proceed-
ings of EMNLP-10, Cambridge, MA.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsupervised modeling of Twitter conversations. In Proceedings
of NAACL-HLT-10, Los Angeles, CA.
Christina Sauper and Regina Barzilay. 2009. Automatically generating Wikipedia articles: A structure-aware
approach. In Proceedings of ACL-IJCNLP-09, Singapore.
Advaith Siddharthan and Simone Teufel. 2007. Whose idea was this, and why does it matter? Attributing scientific
work to citations. In Proceedings of NAACL-07, Rochester, NY.
Simone Teufel and Marc Moens. 2002. Summarizing scientific articles: Experiments with relevance and rhetorical
status. Computational Linguistics, 28(4):409?445.
Simone Teufel. 2010. The Structure of Scientific Articles: Applications to Citation Indexing and Summarization.
CSLI Publications, Stanford, CA.
Andrea Varga, Daniel Preot?iuc-Pietro, and Fabio Ciravegna. 2012. Unsupervised document zone identification
using probabilistic graphical models. In Proceedings of LREC-12, Istanbul, Turkey.
Nguyen Xuan Vinh, Julien Epps, and James Bailey. 2010. Information theoretic measures for clusterings com-
parison: Variants, properties, normalization and correction for chance. Journal of Machine Learning Research,
11:2837?2854.
Hanna M. Wallach. 2008. Structured topic models for language. Ph.D. thesis, University of Cambridge.
13
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 501?510,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Topical PageRank: A Model of Scientific Expertise for Bibliographic Search
James Jardine Simone Teufel
Natural Language and Information Processing Group
Computer Laboratory
Cambridge University, CB3 0FD, UK
{jgj29,sht25}@cam.ac.uk
Abstract
We model scientific expertise as a mixture
of topics and authority. Authority is calcu-
lated based on the network properties of each
topic network. ThemedPageRank, our combi-
nation of LDA-derived topics with PageRank
differs from previous models in that topics in-
fluence both the bias and transition probabili-
ties of PageRank. It also incorporates the age
of documents. Our model is general in that
it can be applied to all tasks which require an
estimate of document?document, document?
query, document?topic and topic?query sim-
ilarities. We present two evaluations, one
on the task of restoring the reference lists of
10,000 articles, the other on the task of au-
tomatically creating reading lists that mimic
reading lists created by experts. In both eval-
uations, our system beats state-of-the-art, as
well as Google Scholar and Google Search in-
dexed againt the corpus. Our experiments also
allow us to quantify the beneficial effect of our
two proposed modifications to PageRank.
1 Introduction
For search, the presence of links in a document
collection adds valuable information over that con-
tained in the text of the documents alone. Each act
of linking can be interpreted as a latent judgement of
authority or trust which is bestowed onto the linked
documents (Kleinberg, 1998). This makes author-
ity an objective measure of how important that pa-
per is to a community who confer that authority.
The citation count is the simplest of these, which
has been used successfully for decades for biblio-
metrics (Garfield, 1972) and for mapping out scien-
tific fields via bibliometric coupling (Kessler, 1963)
and co-citations (Small, 1978). More recently, cita-
tion counts have been shown to improve effective-
ness of ad-hoc retrieval (Meij and De Rijke, 2007;
Fujii, 2007).
In science, the peer review process ensures that
the right to cite is hard-earned, but on the web, hy-
perlinking is infinitely cheap. This means that that
the authority of webpages cannot simply be approx-
imated as the number of incoming links. Algorith-
mically more complex authority such as the random-
surfer model PageRank (Brin and Page, 1998) or the
authorities/hub based algorithm HITS (Kleinberg,
1998)) have spectacularly improved search results in
comparison to standard IR models relying on simi-
larity calculations based on the words in the text and
other text-internal informatioh.
Much recent work in bibliographic search has
been driven by the intuition that what works for the
web should also work for science, even though ci-
tations are more comparable to each other in weight
than hyperlinks. Case studies comparing PageRank-
based authority measures against citation counts
alone report some cases where PageRank is supe-
rior (Chen et al., 2007; Ma et al., 2008), but exper-
imental proof of standard PageRank outperforming
citation counts in a large-scale bibliographic search
experiment is still outstanding. In at least one such
experiment, PageRank performed worse than cita-
tion count (Bethard and Jurafsky, 2010).
Straightforward PageRank calculations, when ap-
plied to the scientific literature, are hampered by two
factors: on the one hand, the progression of time im-
poses a directional structure on the citation network.
Therefore, PageRank values of older papers are sys-
tematically inflated as PageRank can only ever flow
from newer to older papers (Walker et al., 2007).
501
Secondly, and more interestingly, researchers earn
their expertise in particular, well-defined scientific
fields. We propose that this requires a more fine-
grained notion of specific ? not global ? expertise.
Our solution is to use LDA-derived topics (Blei
et al., 2003) as approximations for scientific fields,
and to model the importance of a paper as a mixture
of its relative expertise in each of the topics it cov-
ers. The second aspect of our solution, somewhat
more mundane but still necessary to adapt PageR-
ank successfully to model scientific expertise, is to
age-taper the resultant estimation.
In this paper, we present ThemedPageRank
(TPR), our model of topic-specific scientific exper-
tise, which incorporates the two modifications, and
provide evidence that both are necessary for the ad-
equate application of PageRank-style authority cal-
culations to the scientific literature. In two evalua-
tions, our model beats standard PageRank and cita-
tion counts by a large margin. Previous models exist
which combine the idea of personalising PageRank
by topics, but our manipulation of both PageRank?s
bias and transition probabilities differs from these.
Our experiments also support the claim of our sys-
tem?s superiority over these models.
We use two tasks to evaluate the system?s per-
formance. The first is the reintroduction of an ar-
ticle?s reference items that have been artificially re-
moved. The assumption here is that a good model
of document?document similarity should be able to
guess which articles any given paper would have
cited. The second task is the automatic creation of
reading lists, of the kind that an expert might pre-
pare for their students. We asked experts to create a
gold standard of such reading lists, and compare our
system against the current de facto state-of-the-art in
such tasks, Google Scholar, and again find that our
system beats it comfortably.
This article is structured as follows: the next sec-
tion describes our model, which section 3 contrasts
to related work. The evaluations are described in
sections 4 and 5. Section 6 concludes.
2 Authority Model
Our model first determines an LDA space (Blei et
al., 2003) representing the entire document collec-
tion, which results in a set of topics describing the
entirety of the field. It then calculates an author-
Figure 1: A High-level view of LDA.
ity model for each topic based on a modification
of Personalised PageRank (Page et al., 1998). De-
pending on the search need, the input (one or more
keyword(s) or paper(s)) is converted into a topic dis-
tribution, which we then use to linearly combine the
multiple topic-specific expertise scores of our model
into a unique authority score representing the fit be-
tween search need and document.
Latent Dirichlet Allocation (LDA) (Blei et al.,
2003) is a Bayesian generative probabilistic model
for collections of discrete data, which has become
popular for the modelling of scientific text corpora
(Wei and Croft, 2006; He et al., 2009; Blei and
Lafferty, 2006). In LDA, a document in the cor-
pus is modelled and explicitly represented as a fi-
nite mixture over an underlying set of topics, while
each topic is modelled as an infinite mixture over
the underlying set of words in the corpus. We use
LDA predominantly to produce the latent topics that
form a foundation for the relationships between pa-
pers and technical terms in a corpus.
Technical terms act as the terms in our model
(rather than words), because technical terms are im-
portant artefacts for formulating knowledge from
scientific texts (Ananiadou, 1994; Justeson and
Katz, 1995), because descriptions of topics are bet-
ter understandable using technical terms rather than
words (Wallach, 2006; Wang et al., 2007); and to
make our model more scalable to large corpora. The
method we use to find technical terms is light-weight
and requires little infrastructure, but does not repre-
sent state-of-the-art in terminology detection (Lopez
and Romary, 2010; Wang et al., 2007). We collect
all n-grams of words which appear in 2 or more titles
of all documents in the corpus, filter out all unigrams
appearing in the Scrabble TWL98 word list, then all
n-grams starting or ending in stop words. To de-
502
cide whether a subsumed term should be removed
if the subsuming term exists (?statistical machine
translation? subsumes both ?statistical machine? and
?machine translation?), we remove those n-grams
whose frequency is lower than 25% of their subsum-
ing terms. Finally, only the most frequent 25% of the
remaining unigrams and bigrams are retained.
We then build a D ? V matrix ?, which con-
tains the counts of V technical-terms (the columns)
in each of the D documents (the rows) in Fig. 1. Our
own implementation of LDA (with LDA parameters
? = ? = 0.01) is used to collapse matrix ? into two
denser, smaller matrices ? (containing the distribu-
tion of documents over topics), and ? (containing
the distribution of topics over technical-terms).
To model topic-specific expertise in science, we
modify the original PageRank calculation of Page at
al. (1998) by adding a topic dimension to the score
of both the bias and transition probabilities:
TPR(t, d, k + 1) = ?B(t, d)
+(1? ?)
?
d
?
?l
i
(d)
T (t, d, d
?
)TPR(t, d
?
, k)
where TPR(t, d, k) is the topic-specific PageR-
ank of topic t for paper d at iteration k; B(t, d) is
the probability that paper d is chosen at random from
the corpus, given topic t, and T (t, d, d?) is the tran-
sition probability of reaching page d from page d?,
given topic t. In our formula, the transition proba-
bility T (t, d, d?) takes into account the probabilities
of topic t not only in documents d and d?, but also in
the other documents d?? referenced by document d?:
B(t, d) =
P (t|d)
?
d
?
?D
P (t|d
?
)
T
?
(t, d, d
?
) =
?
P (t|d
?
)
?
d
?
?D
P (t|d
?
)
P (t|d)
?
d
??
?l
o
(d
?
)
P (t|d
??
)
T (t, d, d
?
) =
T
?
(t, d, d
?
)
?
d
?
?l
i
(d)
T
?
(t, d, d
?
)
Here d is a document whose TPR is being calcu-
lated, d? is a document that refers to document d and
whose TPR score is being distributed during this it-
eration of the algorithm, and d?? is a document that
is referred to by document d?. The first term in the
transition function ensures that TPR scores are prop-
agated only from citing documents that are highly
relevant to topic t. The second term ensures that a
larger proportion of a documents TPR score is prop-
agated to cited documents that are highly relevant to
topic t. The value P (t|d) can be read directly from
matrix ? in Fig. 1.
In a final step, we age-taper TPR by dividing
TPR values by the age of the citation concerned in
years. Experimentally, this achieved the best model
in comparison to more complex dampening methods
(e.g., exponential).
3 Related Work
Others before us have observed that time effects bias
PageRank if applied unmodified to the scientific lit-
erature (Walker et al., 2007). Walker et al.?s Cit-
eRank algorithm modifies the bias probabilities of
PageRank exponentially with age, favouring more
recent publications.
We are also not the first to have combined a notion
of topic-specification with Personalised PageRank.
The idea goes back to the original PageRank paper
by Page et al. (1998), who discuss the personaliza-
tion of PageRank by introducing a bias towards only
a set of trusted web sites W . Page et al. alter only
the bias probability B, while leaving the transition
probabilities T unchanged from global PageRank:
B(t, d) =
{
1
|W |
if d ?W
0 if d /?W
T (t, d, d
?
) =
1
|l
o
(d
?
)|
Richardson and Domingos (2002) first used
PageRank personalisation for specialisation at
search time. For query q with corresponding topic
t = q, they use the relevance of document d to query
q as a bias. Haveliwalla (2003) calculates a Person-
alised PageRank for each of a set of 16 manually
created topics t comprised of several documents by
altering only the Bias termB, using Page et al.?s for-
mula above. This solution avoids the computational
scalability problem with Richardson and Domingos?
approach, but is limited in applicability by requiring
predefined topics. Several researchers followed Brin
and Page and Haveliwala in altering only the bias
503
probabilities, including Wu et al. (2006) and Gori
and Pucci (2006).
In contrast, Narayan et al. (2003) and Pal and
Narayan (2005) propose a model of personalisation
that alters the transition probabilities instead of the
bias probabilities. Under their model, the transition
probability T (t, d) is proportional to the number of
words in document d that are strongly present in the
documents contained in topic t. Nie et al. (2006)
produce a more computationally scalable version of
the ideas presented in Pal and Narayan (2005) by as-
sociating a context vector with each document, with
a fixed set of topics (12 in their case), for which they
learn these context vectors using a naive Bayes clas-
sifier. They then provide the possibility to alter both
the bias and transition probabilities of each webpage
as follows:
B(t, d) =
1
D
C
t
(d)
T (t, d, d
?
) = ?
1
|l
o
(d
?
)|
+ (1? ?)
?
t
?
6=t
C
t
?
(d
?
)
l
o
(d
?
)
where C
t
(d) is the context vector score for topic
t associated with document d; the first term in
T (t, d, d
?
) corresponds to the probability of arriving
at page d from other pages in the same topic con-
text; the second term is the probability of arriving at
page d from other pages in a different context; and
? is a factor that weights the influence of same-topic
jumps over other-topic jumps. Their results suggest
that ? should be close to 1, indicating that distribut-
ing PageRank within topics generates better Person-
alised PageRank scores.
Other than the fact that they treat bias and transi-
tion probabilities differently to how we treat them,
all personalisation methods discussed up to now
have the disadvantage that they rely on a fixed list
of manually selected topics, whereas our method of-
fers adaptive specialisation to corpus or domain.
The previous work closest to ours is Yang et al.
(2009), who were the first to use LDA to automat-
ically discover abstract topic distributions in a cor-
pus of scientific articles, and to combine them with
Pagerank by ? in principle ? altering both the bias
and transition probabilities according to the follow-
ing model:
B(t, d) =
1
D
P (t|d)
T (t, d, d
?
) = ?T
s t
(t, d, d
?
) + (1? ?)T
o t
(t, d, d
?
)
T
s t
(t, d, d
?
) = P (d|d
?
, t)
?
=
1
|l
o
(d
?
)|
where T is the number of LDA topics, P (t|d) is a
probability of topic t given document d, which can
be read directly from the generated LDA probabili-
ties, T
s t
is the probability of arriving at page d from
other pages in the same topic context, whereas T
o t
treats the case of arriving at a different topic. Like
Nie et al., they achieve best results with ? = 1, so
they ultimately only use bias probabilities, like the
models discussed above. Crucially, their decision
that P (d|d?, t) does not to involve any of the LDA
topic distributions is surprising. Under their model,
as in ours, when the reader randomly jumps to a new
paper, they will tend to favour papers that are closely
associated with the topic. However, when they fol-
low a citation in Yang et. al?s model, one is picked
with equal probability. In contrast, our model imple-
ments the obvious intuition that if one follows cita-
tions, one should also favour those that are closely
associated with the topic.
Let us now turn to the task of reference list rein-
troduction (RLR), i.e., the prediction of which pa-
pers a target papers originally cited, given only some
information about the paper which stands in as a
search need ? either its abstract, author names and
other bibliometric information, and/or the full text of
a paper (with citation information redacted). Evalu-
ation of a search model by RLR is cheap because of
the readily available gold standard, and it thus allows
for experiments with large data sets.
State-of-the-art solutions to RLR combine lexical
similarity (often via topic models), measures of au-
thority over a citation graph, and information about
social constructs and historic patterns of citation be-
haviour. Strohman et al. (2007) perform RLR with
the paper text as a query to their recommendation
system, using text similarity, citation counts, cita-
tion coupling, author information, and the citation
graph. Their model achieves a mean-average pre-
cision of 0.102 against a corpus from the Rexa10
database. Bethard and Jurafsky (2010) improve on
Strohman et al. by the use of a SVM with 19 fea-
tures from 6 broad categories: similar terms; cited
by others; recency; cited using similar terms; simi-
lar topics; and social habits. They achieve a MAP of
504
0.279 against the ACL Anthology Reference Corpus
(Bird et al., 2008), with the following features per-
forming best: publication age, citation counts, the
terms in citation sentences, and the LDA topics of
the citing documents. They also use (unchanged)
PageRank authority counts as one of the features,
but find that it provides little discriminative power
to the SVM. A drawback of their method is the large
amount of information that has to be provided to
create their SVM features, and the expensive train-
ing routine, which is based on pairwise paper?paper
comparisons in the corpus.
Variations of the RLR tasks exist, which addi-
tionally determine the position in the text of a pa-
per where each recommended citation should occur
(Tang and Zhang, 2009; He et al., 2011; Lu, 2011), a
task which is typically solved by comparing a mov-
ing window in the query paper against millions of
previously located citation contexts with. The draw-
back of this technique in contrast to ours is the fact
that new papers, which have not collected sufficient
contexts in the literature, are severely disadvantaged
and will never be recommended.
We first create topics and then apply PageRank
to find expertise within topical networks. It is how-
ever also possible to simultaneously model citations
and terms (Cohn and Hofmann, 2001; Mann et al.,
2006). Such models are not normally directly com-
parable to ours; for instance Bharat and Henzinger?s
(1998) model, a modified version of HITS (Klein-
berg, 1998), is query-specific.
There are numerous extensions to LDA that incor-
porate external information in addition to the lex-
ical information inside the documents in a corpus,
via author-topic models and models of publication
venues (Steyvers and Griffiths, 2007; Rosen-Zvi et
al., 2010; Tang et al., 2008). Erosheva et al. (2004)
model a corpus using a multinomial distribution si-
multaneously over the citations and terms in each
document. Topics (which they call aspects) are as-
sociated with a list of the most likely words (inter-
pretable as topics) and citations (interpretable as au-
thorities) in that aspect. Extensions of the model ex-
ist (Nallapati and Cohen, 2008; Gruber et al., 2007;
Chang and Blei, 2010; Kataria et al., 2010; Dietz et
al., 2007).
We avoid the tight coupling of topic discovery and
citation modeling that the above-mentioned works
follow for several reasons. Firstly, such models only
work for papers and citations that were present dur-
ing the learning stage, and there is no mechanism
for predicting influential citations for topics in gen-
eral, or for combinations of topics. The tight cou-
pling might also result in overlooking some author-
ities, namely those that are authoritative across sev-
eral topics, which will be penalised via low joint
distribution probabilities in combined methods be-
cause of the division of the probabilities across sev-
eral topics. Secondly, and more disturbingly, such
models will not locate topics that lack an authority
because the authority component of the joint distri-
bution will be near-zero. This rules out niches in
a corpus where papers are equally relevant to each
other, or where the niches are so young that they do
not yet have an established citation network. There
is also a scalability issue with joint models of top-
ics and citations. The evaluation data used in cou-
pled models is generally small, with the number of
papers ranging under around 2,000, the number of
citations ranging under 10,000, and the number of
topics in their models ranging from eight to twenty.
But LDA has been shown to scale to corpora of mil-
lions of terms (Newman et al., 2006), and PageRank
to billions (Page et al., 1998) of documents. Our
model, which advocates a pipelined approach, ben-
efits from the fact that separate topic modelling is
computationally tractable using LDA, and the fact
that citation graph modelling is cheap using Person-
alised PageRank.
4 Evaluation 1: RLR
We evaluate our authority-based search model us-
ing the 2010 ACL Anthology Network (Radev et al.,
2009). We removed from it corrupted documents,
i.e., those of less than 100 characters or contain-
ing only control characters. The ACL Anthology
Network provides external meta-data about the ar-
ticles, which was manually curated. We do not use
this meta-data because we wanted to build as system
that can be applied to any large collection of arti-
cles, where external meta-data would not normally
exist. We therefore build an approximate citation
graph from the paper text itself, as a one-off task
when constructing the LDA space. We extract titles,
dates and full-text from every article and perform a
search of each articles title in the full-text of all other
505
Model MAP
800 test papers, as in B&J (2010)
B&J; best model 0.287
TPR-NoDB 0.264
TPR-NoAge 0.267
TPR 0.302
10,000 test papers
A: NFIDF Cosine 0.062
B: NFIDF + citation count 0.092
C: NFIDF + global PageRank 0.099
D: NFIDF LDA (KL divergence) 0.115
E: TPR-NoDB 0.233
F: TPR-NoAge 0.242
G: TPR 0.268
Figure 2: RLR results
articles (i.e., under the assumption that the reference
list is the (only) place where we will find such titles).
Our system generates the RLR output (the recom-
mended articles) for an article d by extracting tech-
nical terms as described in section 2, examining the
topic distribution for that article ?
d,t
(i.e. a ?
i
in
Fig. 1). We use the topic distribution of article d in
place to generate the unique age-adjusted TPR tai-
lored to the article, TPR(d, d?). The 100 articles
d
? with the highest ThemedPageRanks are recom-
mend as citations for article d. Results are reported
as mean average precision (MAP) of these 100 doc-
uments against the actual citations in the article.
We first compare our model to the state-of-the-
art (Bethard and Jurafsky, 2010). We emulate their
experimental setup by including only the pre-2004
articles in the corpus and testing only on the roughly
800 2005/6 articles with more than 5 intra-corpus
citations in their reference list, for which we have
per-paper average precision scores. The top part of
Fig. 2 shows that our model (MAP=0.302) outper-
forms their best model (MAP=0.287; difference at
5% confidence with Wilcoxon Ranked Squares test),
despite our model being a general, light-weight IR
system, which relies on LDA and PageRank alone,
and theirs is a specialised state-of-the art system,
which relies on heavy-weight machine learning and
on additional sociological features.
The lower part of Fig. 2 compares the influence
of citation count, global PageRank, topic similar-
ity, and combinations of topic similarity with ci-
tation counts or global PageRank, and our model
(TPR). For these tests, we use the entire corpus of
10,000 papers with more than 5 citations. Over the
baseline (A), n-gram-frequency-inverse-document-
frequency (NFIDF), both citation counts (B) and
global PageRank (C) make a small improvement.
Global LDA similarity scores (D) fare little better.
As the performance of the full model (G;
MAP=0.268) shows, the inclusion of topic models
lead to a large improvement over any of the above.
This is, as far as we are aware, the first time that a
large-scale evaluation that finds significant improve-
ments of a PageRank implementation over citation
counts in scientific search.
We next consider our two modifications, age-
adjusting (E) and double-biasing (F), in isolation.
We use two versions of our system where we
switched off age-tapering and double-biasing (ie.,
we only work with a change in the bias probabili-
ties, as do Nie etal. (2006), Havaliwala (2003) (al-
though their models do not include automatically
generated topics) and Yang et al. (2009)). Our
model comfortably outperforms TPR-NoDB in both
the 800 and 10,000 paper experiment. Similarly,
the effect of age-tapering alone can be seen from
the performance of TPR-NoAge (our model with-
out age-adjusting), in the difference between 0.267
and 0.302 and that between 0.242 and 0.268 (signif-
icant at 99%). This confirms our claim that a topic-
specific age-tapered PageRank is superior to global
PageRank in scientific citation networks.
5 Evaluation 2: Reading Lists
The aim of the second experiment is to test our
model against a much cleaner, albeit smaller gold
standard: on the task of reconstructing the mate-
rial of expert-created reading lists. We compare our
system?s performance to three standard, commonly
used search engines: Lucene TFIDF, the Google-
indexed ACL Anthology, and Google Scholar. We
chose Google-index and Google Scholar because
they represent commonly used state-of-the-art com-
mercial search engines, and the Google-index is
what is currently offered as the standard ACL An-
thology search tool. In contrast, Lucene TFIDF
was chosen to represent an easy-to-interpret, repro-
ducible, out-of-the-box baseline implementing the
simplest kind of lexical similarity search without
any notion of authority. Of the three search engines,
506
we would predict Google Scholar to be the tough-
est competitor to TPR, because it uses citation in-
formation directly and it is reasonable to expect that
the Google Scholar algorithm employs some domain
adaptation to the scientific domain.
We created gold standard expert-written reading
lists using the following protocol. Eight experts
were recruited from the computational linguistics
groups of two universities (3 from one, 5 from the
other). All experts had a PhD in computational lin-
guistics and several years of research experience.
They were asked to choose a subject for an (imag-
inary or existing) reading list for an MPhil student,
concerning an area in which they know the litera-
ture well. We purposefully did not give them guid-
ance as to the size of the reading list as we wanted
to observe how experts create reading lists. During
the interview, the experimenter documented the final
list chosen by the expert and made sure all papers
chosen were present in the 2010 version of the ACL
Anthology Network.
This procedure resulted in reading lists of the fol-
lowing topics and sizes: statistical parsing (22 pa-
pers); parser evaluation (4); distributional semantics
(14); domain adaptation for parsing (11); informa-
tion extraction (9); lexical semantics (14); statistical
machine translation models (5); and concept-to-text
generation (16).
In our retrieval model, which topic distribution is
chosen for a query depends on whether the query is
an exact match to one of the technical terms found
by our model. If it is, then the topic distribution
of the technical term is used directly as the query
topic distribution ?q, t (i.e. a transposed renormal-
ized ? in Fig. 1). If not, we perform a keyword-
based search (using Lucene TFIDF), and use the av-
erage topic distribution of the top 20 documents re-
turned as the query topic distribution (i.e. several ?i
in Fig. 1). The query topic distribution is then used
to linearly combine the topic-specific TPRs into a
unique TPR tailored to the query. The 20 documents
with the highest TPR are recommended.
The three baselines are used as follows in the
experiment: The experiment is performed by issu-
ing the topic of the reading list (exactly as given
to us by the experts) as a key-word based query to
each system and recording the top 20 resulting pa-
pers answers. For Lucene TFIDF, we downloaded
Lucene.NET v2.9.2 and indexed our 2010 snapshot
of the ACL Anthology using standard Lucene pa-
rameters for the TFIDF model. For the Google-
indexed ACL Anthology (AAN), we use the in-
terface provided on the ACL Anthology website.
In order to provide an identical search ground, we
automatically exclude from the return lists papers
added after the creation of the AAN snapshot. For
Google Scholar (GS), we use the interface provided
at scholar.google.com, and parse returns to ex-
clude non-AAN material semi-automatically. In
the case of Google Scholar, we restrict the search
ground to the ACL Anthology by filtering the top
200 return sets (which may lead to fewer than 20
papers returned).
We report FCSC, RCSC and F-score for each al-
gorithm. FCSC and RCSC are new metrics which
address the problem that F-score, being binary, does
not support the notion of a ?close hit?, combined
with the fact that we require a fine-grained compari-
son of the quality of different systems retrieved lists
despite the small size of our gold standard. Cita-
tion Substitution Coefficient (FCSC), a new metric
for RLR, gives higher scores to papers closely re-
lated to the target papers by citation distance. The
FCSC of each expert paper is the inverse of the num-
ber of nodes in the minimal citation graph connect-
ing each expert paper to any system-retrieved pa-
per (thus ranging between 0 and 1; non-connected
expert papers receive a zero score). We also in-
troduce Reverse Citation Substitution Coefficient
(RCSC), which measures the inverse of the num-
ber of nodes in the minimal citation graph connect-
ing each system-retrieved paper to any expert pa-
per. RCSC makes sure that systems cannot simply
increase their FCSC values by returning many ir-
relevant papers. RCSC thus corresponds to preci-
sion, while FCSC corresponds to recall. The sys-
tem RCSC and FCSC scores we report are the av-
erage scores of all the system-retrieved and expert
papers, respectively. Reporting both scores gives a
good overall picture of system performance, partic-
ularly when read together with the F-score.
Fig. 3 shows that our model comfortably beats the
competitor systems according to all metrics. In par-
ticular, our model > GS/AAN > Lucene TFIDF1.
1For FCSC, the differences are statistically significant at
507
FCSC RCSC F-score
AAN/Google 0.527 0.317 0.117
GS 0.519 0.364 0.112
Lucene TFIDF 0.412 0.330 0.040
TPR 0.563 0.456 0.128
Figure 3: Reading List Creation: Results.
Concerning simpler methods of estimating author-
ity, Fig. 4 shows that a multiplication of TFIDF
by citation count (as Fujii (2007) does) results in a
FCSC/RCSC of 0.419/0.359 (reported as TF-CC),
and age-tapering of citation-count by dividing the
citation count by the age of the paper in years
(reported as TF-CC-A) results in FCSC/RCSC of
0.491/0.442. We again compare different versions
of PageRank. Global PageRank can be built into
the system by simple multiplication of PR scores
as above, with and without age-tapering (reported
as TF-PR and TF-PR-A, respectively). We observe
a similar effect to the one reported by Bethard and
Jurafsky and seen in experiment 1, namely that
global PageRank only performs similar to citation
counts (0.450/0.360 vs 0.419/0.359). With respect
to double-biasing and age-tapering we see the same
effect as in experiment 22. In fact, we can see from
these results that global PageRank barely improves
over standard TFIDF, while age-tapering even with-
out topics already brings quite some improvement.
Overall, these results confirms our claim of the su-
periority of a topic-specific PageRank over global
PageRank in scientific citation networks.
6 Conclusions
We present here the first experiments that pinpoint
which modifications to PageRank are necessary to
99% confidence via a two-tailed Wilcoxon Signed Ranks test,
except that between GS and AAN (for which the confidence in-
terval is only 96%) and that between Lucene and AAN, where
it is 98%. Non-parametric paired tests such as the Wilcoxon
Signed Ranks test can be used on FCSC, but not on RCSC,
as there are different sets of underlying system-retrieved pa-
pers in each case. For RCSC, differences between our model
and all others at 99% confidence interval, between GS and
AAN/Lucene TFIDF at the 95% interval. F-score is reported
for completeness.
2Wilcoxon Signed Rank test found all differences significant
at the 99% level, except that between TF-PR and Lucene TFIDF
(significant only at the 90% level), and the following equiva-
lences: Lucene TFIDF = TF-CC; TF-PR = TF-CC; TF-CC-A =
TF-PR-A; TF-CC-A = TF-PR.
FCSC RCSC
TF-CC 0.419 0.359
TF-CC-A 0.491 0.442
TF-PR 0.450 0.360
TF-PR-A 0.512 0.407
TPR-NoDB 0.541 0.440
TPR-NoAge 0.526 0.436
Figure 4: Citation counts and PageRank variants.
adequately cater for the highly specialised situation
we encounter in science. The modification we sug-
gest are to use LDA-derived topics (Blei et al., 2003)
as approximations for scientific fields, to calculate
authority in a topic-specific way, and to age-taper
the authority scores. We present formulae where
topics personalise both the bias and the transition
probabilities. This results in a general IR model
for science incorporating a robust notion of author-
ity. Our implementation requires only minimal re-
sources and relies only on LDA and PageRank cal-
culation, which means that it is efficient during train-
ing, retraining and at search time.
We perform two evaluations. In both, our
model significantly outperforms not only state-
of-the-art, but also standard PageRank, non-age-
tapered (but topical) PageRank, and non-topical (but
age-tapered) PageRank. Our model achieves its
competitive performance by using only the raw text
and citation links. It requires no external informa-
tion, neither explicit sociological information such
as past collaborations between authors, nor the ex-
pertise and cooperation of like-minded readers, as
collaborative models do. While successful applica-
tions of collaborative filtering to bibliometric search
are rife (Goldberg et al., 2001; Agarwal et al., 2005;
McNee et al., 2006; Torres et al., 2004), including
to reading list generation (Ekstrand et al., 2010), we
wanted an entirely independent authority-based IR
model similarity. CF also suffers from a cold-start
phenomenon, where recommendations are generally
poor where data is sparse, and has to wait for papers
to be rated by a large number of authors (rather than
cited) before it can rank them.
Should the reader wish to evaluate the perfor-
mance of TPR on their own PDF papers, it has been
incorporated into the Qiqqa reference management
software 3.
3Available at http://www.qiqqa.com
508
References
N. Agarwal, E. Haque, H. Liu, and L. Parsons. 2005. Re-
search paper recommender systems: A subspace clus-
tering approach. Advances in Web-Age Information
Management.
S. Ananiadou. 1994. A methodology for automatic term
recognition. In Proceedings of COLING.
S.K. Pal B. Narayan, C. Murthy. 2003. Topic continu-
ity for web document categorization and ranking. In
IEEE/WIC International Conference on Web Intelli-
gence.
B.D. Davison B. Wu, V. Goel. 2006. Topical trustrank:
Using topicality to combat web spam. In Proceedings
of the 15th international conference on World Wide
Web.
S. Bethard and D. Jurafsky. 2010. Who should i cite:
learning literature search models from citation behav-
ior. In Proceedings of the 19th ACM International
Conference on Information and Knowledge Manage-
ment.
K. Bharat and M.R. Henzinger. 1998. Improved algo-
rithms for topic distillation in a hyperlinked environ-
ment. In Proceedings of SIGIR.
S. Bird, R. Dale, B.J. Dorr, B. Gibson, M.T. Joseph, M.Y.
Kan, D. Lee, B. Powley, D.R. Radev, and Y.F. Tan.
2008. The ACL anthology reference corpus: A ref-
erence dataset for bibliographic research in computa-
tional linguistics. In Proc. of LREC08.
D.M. Blei and J.D. Lafferty. 2006. Correlated Topic
Models. In Advances in Neural Information Process-
ing Systems 18: Proceedings of the 2005 Conference,
page 147. Citeseer.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet allocation. The Journal of Machine Learning
Research, 3:993?1022.
J. Boyd-Graber, D. Blei, and X. Zhu. 2007. A topic
model for word sense disambiguation. In Proceedings
of EMNLP-CoNLL, pages 1024?1033.
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual web search engine. In Proceedings of the
7th International World Wide Web Conference.
J. Chang and D.M. Blei. 2010. Hierarchical relational
models for document networks. The Annals of Applied
Statistics, 4(1):124?150.
P. Chen, H. Xie, S. Maslov, and S. Redner. 2007. Find-
ing scientific gems with google?s pagerank algorithm.
Journal of Infometrics, 1(1):8?15.
D. Cohn and T. Hofmann. 2001. The missing link-a
probabilistic model of document content and hypertext
connectivity. Advances in neural information process-
ing systems, pages 430?436.
L. Dietz, S. Bickel, and T. Scheffer. 2007. Unsuper-
vised prediction of citation influences. In Proceedings
of the 24th international conference on Machine learn-
ing, page 240. ACM.
M.D. Ekstrand, P. Kannan, J.A. Stemper, J.T. Butler,
J.A. Konstan, and J.T. Riedl. 2010. Automatically
building research reading lists. In Proceedings of
the fourth ACM conference on Recommender systems,
pages 159?166. ACM.
E. Erosheva, S. Fienberg, and J. Lafferty. 2004. Mixed-
membership models of scientific publications. Pro-
ceedings of the National Academy of Sciences of the
United States of America, 101(Suppl 1):5220.
A. Fujii. 2007. Enhancing patent retrieval by citation
analysis. In Proceedings of SIGIR.
E. Garfield. 1972. Citation analysis as a tool in jour-
nal evaluation. American Association for the Advance-
ment of Science.
K. Goldberg, T. Roeder, D. Gupta, and C. Perkins. 2001.
Eigentaste: A constant time collaborative filtering al-
gorithm. Information Retrieval, 4(2):133?151.
M. Gori and A. Pucci. 2006. Research paper rec-
ommender systems: A random-walk based approach.
IEEE Computer Society.
A. Gruber, M. Rosen-Zvi, and Y. Weiss. 2007. Hidden
topic markov models. In Proceedings of AISTATS.
T.H. Haveliwala. 2003. Topic-sensitive pagerank: A
context-sensitive ranking algorithm for web search.
IEEE transactions on knowledge and data engineer-
ing, pages 784?796.
Q. He, B. Chen, J. Pei, B. Qiu, P. Mitra, and L. Giles.
2009. Detecting topic evolution in scientific literature:
how can citations help? In Proceeding of the 18th
ACM conference on Information and knowledge man-
agement.
Q. He, D. Kifer, J. Pei, P. Mitra, and C.L. Giles. 2011.
Citation recommendation without author supervision.
In Proceedings of the fourth ACM international con-
ference on Web search and data mining.
J.S. Justeson and S.M. Katz. 1995. Technical terminol-
ogy: some linguistic properties and an algorithm for
identification in text. Natural Language Engineering,
1(01):9?27.
S. Kataria, P. Mitra, and S. Bhatia. 2010. Utilizing Con-
text in Generative Bayesian Models for Linked Cor-
pus. In Proceedings of AAAI.
M.M. Kessler. 1963. Bibliographic coupling be-
tween scientific papers. American Documentation,
14(1):10?25.
J. Kleinberg. 1998. Authoritative sources in a hy-
perlinked environment. In Proceedings of the 9th
ACM-SIAM Symposium on Discrete Algorithms. Also
available from http://www.cs.cornell.edu/
home/kleinber/.
509
P. Lopez and L. Romary. 2010. HUMB: Automatic Key
Term Extraction from Scientific Articles in GROBID.
In SemEval 2010 Workshop.
Y. et al. Lu. 2011. Recommending citations with transla-
tion model. In Proceedings of the 20th ACM interna-
tional conference on Information and knowledge man-
agement.
N. Ma, J. Guan, and Y. Zhao. 2008. Bringing pagerank
to the citation analysis. Information Processing and
Management, 44(2):800?810.
G.S. Mann, D. Mimno, and A. McCallum. 2006. Biblio-
metric impact measures leveraging topic analysis. In
Proceedings of the 6th ACM/IEEE-CS joint conference
on Digital libraries.
S.M. McNee, J. Riedl, and J.A. Konstan. 2006. Mak-
ing recommendations better: an analytic model for
human-recommender interaction. In CHI?06 extended
abstracts on Human factors in computing systems.
E. Meij and M. De Rijke. 2007. Using prior information
derived from citations in literature search. In Large
Scale Semantic Access to Content (Text, Image, Video,
and Sound).
R. Nallapati and W. Cohen. 2008. Link-plsa-lda: A new
unsupervised model for topics and influence of blogs.
In International Conference for Weblogs and Social
Media.
D. Newman, P. Smyth, and M. Steyvers. 2006. Scalable
Parallel Topic Models. Journal of Intelligence Com-
munity Research and Development.
L. Nie, B.D. Davison, and X. Qi. 2006. Topical link
analysis for web search. In Proceedings of SIGIR.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The pagerank citation ranking: Bringing order to the
web. Stanford Digital Library Technologies Project.
D.R. Radev, P. Muthukrishnan, and V. Qazvinian. 2009.
The ACL Anthology Network Corpus. In Proceed-
ings, ACL Workshop on NLP and IR for Digital Li-
braries, Singapore.
M. Richardson and P Domingos. 2002. The intelligent
surfer: Probabilistic combination of link and content
information in pagerank. Advances in neural informa-
tion processing systems, 14:1441?1448.
M. Rosen-Zvi, C. Chemudugunta, T. Griffiths, P. Smyth,
and M. Steyvers. 2010. Learning author-topic models
from text corpora. ACM Transactions on Information
Systems (TOIS), 28(1):1?38.
B. Narayan S.K. Pal. 2005. A web surfer model incorpo-
rating topic continuity. IEEE Transactions on Knowl-
edge and Data Engineering, 17:726729.
H.G. Small. 1978. Cited documents as concept symbols.
Social Studies of Science, 8:327?340.
M. Steyvers and T. Griffiths. 2007. Probabilistic topic
models. In T. Landauer, D. S. McNamara, S. Dennis,
and W. Kintsch, editors, Handbook of latent semantic
analysis, page 427. Erlbaum, Hillsdale, NJ.
T. Strohman, W.B. Croft, and D. Jensen. 2007. Recom-
mending citations for academic papers. In Proceed-
ings of SIGIR.
J. Tang and J. Zhang. 2009. A discriminative approach
to Topic-Based citation recommendation. Advances in
Knowledge Discovery and Data Mining.
J. Tang, R. Jin, and J. Zhang. 2008. A topic model-
ing approach and its integration into the random walk
framework for academic search. In Eighth IEEE Inter-
national Conference on Data Mining.
R. Torres, S.M. McNee, M. Abel, J.A. Konstan, and
J. Riedl. 2004. Enhancing digital libraries with Tech-
Lens+. In Proceedings of the 4th ACM/IEEE-CS joint
conference on Digital libraries.
D. Walker, H. Xie, K.K. Yan, and S. Maslov. 2007.
Ranking scientific publications using a model of net-
work traffic. Journal of Statistical Mechanics: Theory
and Experiment, 2007:P06010.
H.M. Wallach. 2006. Topic modeling: beyond bag-of-
words (powerpoint). In Proceedings of the 23rd inter-
national conference on Machine learning.
X. Wang, A. McCallum, and X. Wei. 2007. Topical n-
grams: Phrase and topic discovery, with an applica-
tion to information retrieval. In Proceedings of the 7th
IEEE international conference on data mining.
X. Wei and W.B. Croft. 2006. LDA-based document
models for ad-hoc retrieval. In Proceedings of SIGIR.
W. Wong, W. Liu, and M. Bennamoun. 2009. A proba-
bilistic framework for automatic term recognition. In-
telligent Data Analysis, 13(4):499?539.
Z. Yang, J. Tang, J. Zhang, J. Li, and B. Gao. 2009.
Topic-level random walk through probabilistic model.
Advances in Data and Web Management.
D. Zhou, S. Zhu, K. Yu, X. Song, B.L. Tseng, H. Zha, and
C.L. Giles. 2008. Learning multiple graphs for doc-
ument recommendations. In Proceeding of the 17th
international conference on World Wide Web.
510
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 732?741,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
A Summariser based on Human Memory Limitations and
Lexical Competition
Yimai Fang
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue, CB3 0FD, UK
Yimai.Fang@cl.cam.ac.uk
Simone Teufel
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue, CB3 0FD, UK
Simone.Teufel@cl.cam.ac.uk
Abstract
Kintsch and van Dijk proposed a model
of human comprehension and summarisa-
tion which is based on the idea of pro-
cessing propositions on a sentence-by-
sentence basis, detecting argument over-
lap, and creating a summary on the basis
of the best connected propositions. We
present an implementation of that model,
which gets around the problem of identi-
fying concepts in text by applying coref-
erence resolution, named entity detection,
and semantic similarity detection, imple-
mented as a two-step competition. We
evaluate the resulting summariser against
two commonly used extractive summaris-
ers using ROUGE, with encouraging re-
sults.
1 Introduction
Kintsch and van Dijk (1978) (henceforth KvD)
present a model of human comprehension and
memory retention which is based on research in ar-
tificial intelligence, experimental psychology and
discourse linguistics. It models the processing of
incoming text or speech by human memory lim-
itations, and makes verifiable predictions about
which propositions in a text will be recalled by
subjects later. It has been very influential, particu-
larly in the 1980 and 1990s in educational (Palin-
scar and Brown, 1984; King, 1992) and cognitive
(Paivio, 1990) psychology, and is still today used
as a theoretical model of reading and comprehen-
sion (Baddeley, 2007; Zwaan, 2003; DeLong et
al., 2005; Smith, 2004). It has also been used for
improving education, particularly for the produc-
tion of better instructional text (Britton and Gul-
goz, 1991; Pressley, 2006), and for teaching hu-
mans how to read for deep comprehension (Coiro
and Dobler, 2007; Duke and Pearson, 2002; Koda,
2005; Driscoll, 2005) and to summarise (Hidi,
1986; Brown et al., 1983).
In the summarisation community, the model has
been commended for its elegant and explana-
tory ?deep? treatment of the summarisation pro-
cess (Lehnert, 1981; Sp?arck Jones, 1993; Endres-
Niggemeyer, 1998), but has not lead to any prac-
tical prototypes, mainly due the impossibility of
implementing the knowledge- and inference-based
aspects the model relies on.
We present here an implementation of the model,
which attempts to circumvent some of these prob-
lems by the application of distributional seman-
tics, and by modelling the construction of the co-
herence tree as a double competition (firstly of
concept partners for word forms, secondly of at-
tachment sites for propositions).
In the KvD model, a text (e.g. Figure 1) is con-
verted into propositions (see Table 1) which have
one functor and one or more arguments. The func-
tor can be taken either from a fixed list of gram-
matical relations (e.g. IS A; AT; BETWEEN; OR)
or an open class-set of so-called concepts, (e.g.
BLOODY; TEACH). Arguments can be concepts
or proposition numbers. Proposition numbers ex-
press embedded semantic structures (e.g. #9 in
Table 1). Kintsch et al. (1979) assumed that this
tranformation is performed manually; they were
able to train humans to do so consistently.
A series of violent, bloody encounters between police
and Black Panther members punctuated the early sum-
mer days of 1969. Soon after, a group of black students
I teach at California State College, Los Angeles, who
were members of the Panther Party, began to complain
of continuous harassment by law enforcement officers.
Figure 1: First two sentences from the example
paragraph Bumperstickers by KvD (1978).
732
No. Proposition
Cycle 1
1 SERIES (ENCOUNTER)
2 VIOLENT (ENCOUNTER)
3 BLOODY (ENCOUNTER)
4 BETWEEN (ENCOUNTER, POLICE, BLACK PAN-
THER)
5 TIME: IN (ENCOUNTER, SUMMER)
6 EARLY (SUMMER)
7 TIME: IN (SUMMER, 1969)
Cycle 2
8 SOON (#9)
9 AFTER (#4, #16)
10 GROUP (STUDENT)
11 BLACK (STUDENT)
12 TEACH (SPEAKER, STUDENT)
13 LOCATION: AT (#12, CAL STATE COLLEGE)
14 LOCATION: AT (CAL STATE COLLEGE, LOS
ANGELES)
15 IS A (STUDENT, BLACK PANTHER)
16 BEGIN (#17)
17 COMPLAIN (STUDENT, #19)
18 CONTINUOUS (#19)
19 HARASS (POLICE, STUDENT)
Table 1: Propositions for Figure 1.
The KvD algorithm is manually simulated in their
work, but is described in a mechanistic manner
that should in principle lend itself to implemen-
tation, once propositions are created. Propositions
form a tree where a proposition is attached to an-
other proposition with which they share at least
one argument; attachment higher in the tree is pre-
ferred. The tree is built incrementally; blocks of
propositions, each of which roughly correspond-
ing to one sentence, are processed in cycles. Af-
ter each cycle, a process of ?forgetting? is sim-
ulated by copying only the most salient proposi-
tions to the short-term memory (STM). This se-
lection is performed by the so-called leading edge
strategy (LES), which prefers propositions that
are attached more recently and those attached at
higher positions. This algorithms mirrors van
Dijk?s (1977) model of textual coherence.
When choosing an attachment site for proposition,
arguments which are currently in STM are pre-
ferred. A resource-consuming search in long-term
memory (LTM) is only triggered if a proposition
cannot be attached in STM; in that case a bridging
proposition is reintroduced into the tree.
The KvD model can be used to explain human re-
call of stories, and can also to create a summary of
a text. The most natural way for a human to sum-
marise from scratch is to replace propositions with
so-called macropropositions, and the KvD model
prefers this style of summary creation. An exam-
ple for macroproposition is a statement that gen-
eralises over other propositions. This results in a
more abstract version of the text. However if for
any reason it is not possible to create macropropo-
sitions (for instance due to lack of deep knowledge
representation), a summary can also be created in
a simpler way based only on the propositions con-
tained in the text. In that case, the selection cri-
terion is the number of cycles a proposition has
remained in STM.
There are three main stumbling blocks in the way
of an implementation of the KvD model:
1. The automatic creation of propositions from
text, and of summary text from summary
propositions;
2. The automatic creation of concepts from
words (including coreference resolution);
3. The creation of macropropositions, which
would require sophisticated knowledge rep-
resentation and reasoning.
We present a fully automatic version of the KvD
model based on the following assumptions:
1. Current parser technology allows us to recon-
struct the compositional semantics of the text
well enough to make the KvD model opera-
tional, both in terms of creating propositions
from text, and in terms of creating reasonably
understandable output text from propositions
(even if not fully grammatical).
2. We model the lexical variation of how a con-
cept is expressed in a text probabilistically
by semantic similarity and coreference reso-
lution. This creates a competition between
plausible expressions for argument overlap.
3. Our core algorithm is modelled as two com-
petitions: (a) the competition between con-
cept matches as mentioned in the point
above; and (b) the competition between pos-
sible positions in a tree where a proposition
could attach.
4. We also observed that KvD?s method of
choosing the tree root in the first processing
cycle, and to never change it afterwards un-
less texts are truly incoherent (resorting to
multiple trees), is too limiting, in particu-
lar in combination with their LES. Texts can
have topic changes and still be perfectly co-
733
herent, particularly if they are longer and less
linearly structured than the examples used
by KvD. We therefore experiment with more
flexible root choice strategies.
We have nothing to say on the third and biggest
obstacle, the creation of macropropositions. Nev-
ertheless, the experiments presented here test
whether our hypotheses 1 ? 4 are strong enough
to provide our summariser with useful informa-
tion concerning the discourse structure of the texts.
We test this by comparing its performance to that
of two current state-of-the-art summarisers, which
instead rely on the sole use of lexical informa-
tion. A psychologically-motivated summariser
such as ours should be evaluated by compari-
son to abstractive, i.e., reformulated human sum-
maries, rather than by comparison to extractive
summaries. We do so using ROUGE, an evalu-
ation framework that supports such comparisons
(Lin and Hovy, 2003).
The structure of the paper is as follows. In the
next section, we will detail our implementation of
the KvD model, with particular emphasis on the
creation of propositions, probabilistic concepts,
proposition attachment, and root choice. In Sec-
tion 4, we will present experiments comparing our
summariser against two research extractive sum-
marisers, MEAD and LexRank. We also test how
our inventions including similarity-based concept
matching and root choice strategy contribute to
performance. We compare to related work in Sec-
tion 3, and draw our conclusions in Section 5.
2 Our implementation of KvD
Figure 2 shows the structure of our summariser.
The Proposition Creation module transforms sur-
face text to propositions with the aid of a grammat-
ical parser. Recall that in the original KvD model
(shown as ?Human (KvD)?), propositions are gen-
erated manually. Apart from such, our implemen-
tation follows the KvD algorithm as closely as
possible. The core of this algorithm is the Mem-
ory Retention Cycle in the centre of the figure.
A cycle begins with the detection of coherence be-
tween the new propositions and the current STM
content. This results in a hierarchy of all so-far
processed propositions called the Coherence Tree.
Propositions are attached to the tree by a variety of
strategies, as explained in Subsection 2.2.
Input: Full text	

Parser	

Proposition Builder	

Dependencies	
 Human (KvD)	

Propositions	

Coherence Detector	
 Coherence Tree	

Selector	
IPs in STM	

Summary propositions	

Extractor	
 Human (KvD)	

Output: Summary text	

Each sentence	

Most frequent ones	

Pro
pos
itio
n C
reat
ion	

Me
mor
y R
eten
tion
 Cy
cle	

Pro
pos
itio
n to
 tex
t	

LTM	

Figure 2: Framework of the summariser.
At the end of each cycle, important propositions
(IPs) are selected by the Selector, stored in STM,
and thus retained for the next cycle, where they are
available for new incoming propositions to attach
to. The selector is a full implementation of KvD?s
LES, which also updates the recency of proposi-
tions reinstantiated from the LTM.
1
Less impor-
tant propositions leave the cycle and go into the
LTM, which is conceptually a secondary reposi-
tory of propositions to provide the ?missing links?
when no coherence between the STM and the in-
coming propositions can be established.
After the text is consumed, a propositional repre-
sentation of the summary is created by recalling
the propositions that were retained in STM most
frequently. The summary text is then either cre-
ated manually (in the KvD model), or in our im-
plementation, as a prototype, automatically by ex-
tracting words from the parser?s dependencies.
2.1 Proposition builder
We aim to create propositions of comparable se-
mantic weight to each other. This is a consequence
of our decision to recast KvD as a competition
model (as will become clear in subsection 2.2),
because by defining propositions as blocks of ar-
guments they should contain a similar number of
1
KvD implied this in the last cycle of the Bumperstick-
ers paragraph, by placing the two reinstantiated propositions
below #37, though they are older than #37.
734
meaningful arguments to ensure similar potential
for overlap.
To achieve suitable granularity of propositions,
we aggregate information spread out over several
grammatical dependencies, and exclude semanti-
cally empty words from participating in argument
overlap. We use Stanford Parser (Klein and Man-
ning, 2003), and aggregate subjects and comple-
ments of a predicate into a single proposition. Ac-
tive and passive voices are unified; clauses are
treated as embedded propositions; controlling sub-
jects of open clausal complements are recovered.
Some predicates are not verbs, but nominalised
verbs or coordination. For instance, KvD model
the phrase ? encounters between police and Black
Panther Party members ? as BETWEEN (EN-
COUNTER, POLICE, BLACK PANTHER). Produc-
ing such a proposition instead of two separate
ones BETWEEN (ENCOUNTER, POLICE) and BE-
TWEEN (ENCOUNTER, BLACK PANTHER) is ad-
vantageous, because this single proposition pro-
vides a strong connection between POLICE and
BLACK PANTHER which cannot be derived from
other dependencies.
However we lack a subcategorisation lexicon that
provides information about how many arguments
a preposition like ?between? takes. Therefore we
scan conjoined prepositional phrases, aggregate
the objects, and attach them to the governors of the
prepositional phrases. In this example, the result-
ing preposition is ENCOUNTER (POLICE, MEM-
BER). The word ?between? is excluded because it
is semantically empty and may interfere with over-
lap detection.
We take care to detect and exclude semantically
empty material. For instance, the empty semantic
heads in noun phrases such as ?a series of? and ?a
group of? are detected using a list of of 21 words
we collected, and treated by redirecting the depen-
dencies involving the empty heads to the corre-
sponding content heads. In this treatment, the rela-
tion between an empty head and its content head is
not entirely erased, but encoded as a general mod-
ifier relation.
2.2 Probabilistic concept matching
The notion of argument overlap in KvD?s model
is sophisticated in that it ?knows? which surface
expressions (pronouns, synonyms, etc) in text re-
fer to the same concept. Concept mapping is the
task of forming equivalence classes of surface ex-
pressions; each concept then corresponds to one
such equivalence class. The KvD model, because
it simulates concept mapping and proposition at-
tachment in parallel, conceals some of the choices
that a fully automatic model has to make.
Given current technology, concept mapping can
only be performed probabilistically. We use the
Stanford coreference resolution, named-entity de-
tection (to extend coreference detection to non-
same-head references, e.g. mapping ?the tech
giant? to ?Apple Inc.?
2
); and to find synonymy
or at least semantic relatedness, we use a well-
known measure of semantic similarity, namely
Lin?s Dependency-Based Thesaurus (Lin, 1998).
We are not committed to this particular measure,
but it empirically performed best out of the 11 we
tried; especially it outperformed WordNet path-
based measures. Note however that only the 200
most similar words for each word are provided by
this tool. The similarity measure is normalised by
relative ranking to provide the probability that an
expression refers to the same concept as another
expression. We use WordNet (Miller, 1995) for
derivationally related forms (to solve e.g. nomi-
nalisation). This establishes the first competition,
the one between concept matches.
police
Black Pan-
ther members
law enforce-
ment officers
members of the
Panther Party
1 1
1
1
Figure 3: KvD?s concept matching.
police
Black Pan-
ther members
law enforce-
ment officers
members of the
Panther Party
0.99 0.67
0
0.33
1 1
0.01
0
Figure 4: Probabilistic concept matching.
Modelling concepts probabilistically has its impli-
cation for the next task: finding the best attach-
ment site for a proposition. Let us explain this with
an example. Notice that in the example text in Fig-
ure 1, ?police? (from #4, in the first sentence) and
2
A WordNet synset is defined for each named-entity type;
here ?giant? is connected to its hypernym ?organization? via
?enterprise?.
735
?law enforcement officer? (from #19, in the sec-
ond sentence) refer to the same concept POLICE.
Figure 3 illustrates how this is handled in KvD?s
model, where intelligent concept matching estab-
lishes with 100% certainty that the two strings re-
fer to the same concept. Certainty about the ar-
gument overlap then enables them to later attach
#19 to #4. In their model it is important whether
a matching proposition is found in STM or LTM:
If the only proposition that mentions ?police? (#4)
is no longer in STM when the proposition contain-
ing ?law enforcement officer? (#19) is processed,
and for any reason the other arguments in #19 (i.e.
STUDENT) cannot find overlaps either, KvD find
no concept match in STM and know therefore,
again with full certainty, that an LTM search must
be triggered
3
, which in this case leads to the suc-
cessful recall of #19 for #4 to attach to.
Figure 4 illustrates the corresponding situation in
our model, where #4 with ?police? is in LTM, the
probability of a concept match between ?law en-
forcement officer? and ?police? is 66.7%, whereas
that of a match with ?members?, which is in STM,
is 33.3%. The probabilistic concept matching can-
not provide enough certainty to single out #4 be-
cause of full argument overlap. The probabilities
of concept match have to act as a much weaker
filter in our model, and all previous propositions
have to be considered as potential landing sites
for #19. In particular, we do not know whether
a concept match within STM is ?good enough?,
or whether a LTM search is needed. There is, in
this case, a competition between a weak match in
STM (the direct vicinity) and a strong match in
LTM (further away), which will hopefully result
in a successful match between ?police? and ?law
enforcement officer?. In other words, we always
have to search for matches in both repositories.
After obtaining the graph of interrelated expres-
sions, the competition between landing sites for
each proposition takes place, whereby higher po-
sitions are preferred. This double competition is a
core aspect of our model.
2.3 Choice of root
The KvD model almost always maintains the root
determined in the first cycle (either by overlap
3
KvD only mentioned retrieving embedded propositions
as LTM search rarely happens, but the goal is the same as
here: to establish overlap.
with title concepts or by coverage of the main
clause of the first sentence). The model intro-
duces multiple roots if a text is totally incoher-
ent, namely when propositions cannot be attached
anywhere and therefore a forest of disjoint trees
has to be developed. This strategy does not gen-
eralise well to longer texts with topic changes,
for example newspaper texts with anecdotal leads.
Although these texts are perfectly coherent, KvD
cannot treat them appropriately.
4
Our more flexible rooting strategy is run once
in each cycle, assessing whether any of the cur-
rent root?s children in the working memory would
make a better root. In case of a root change, the
edge between the old and the new root is reversed,
and the old root becomes a child of the new root.
Then we perform the same strategy on the new tree
until no root change is needed.
We denote the current root as i, and a new root
candidate (a child of i) j. J is the set of descen-
dants of j (inclusive of j), and I the set of all nodes
V excluding J , i.e. I = V \ J . Then nodes in J
will be promoted after the root change, while those
in I will go one level deeper. Since edge weights,
i.e. attachment strengths, are asymmetric, we de-
note the weight for j being a child of i as w
i,j
, and
w
j,i
for the reversed attachment. Each node v also
carries a weight x
v
= m
v
? a
d
v
, where m
v
is a
memory status factor (e.g. m
v
= 1 if v is in STM,
0.5 if otherwise), 0 < a ? 1 is an attenuation fac-
tor, and d
v
is depth of v in the tree. To decide, we
evaluate
s = w
j,i
?
v?J
x
v
? w
i,j
?
v?I
x
v
(1)
If s > 0, the root change is permitted.
5
This evalu-
ation makes root change easier if the edge in ques-
tion favours i being a child of j, or there are more
important nodes that can benefit from the change,
and vice versa.
An example of such a root change taken from the
Bumperstickers is given in Figure 5 (refer to Ta-
ble 1 for proposition contents). As the central
topic of the text changes from the encounters to
4
In our scenario the situation can barely ever arise where
absolutely no proposition attachment is possible, as the prob-
abilistic concept mapping is usually able suggest some con-
cept match, albeit with small probability.
5
In case when multiple candidates are permitted, the one
with the highest s is chosen.
736
that the identity of Panther Party members are ac-
tually the author?s students, the summariser recog-
nises this change after reading one more sentence,
by flipping the edge connecting #3 and #14.
3 2 4 14 16 17 18 
3 2 4 14 16 17 18 
Figure 5: Tree before and after a root change.
3 Related Work
One of the dilemmas in summarisation research is
how ?deep?, i.e. semantics-oriented, a summariser
should be. Shallow analysis of lexical similarity
between sentences and/or the keywords contained
in sentences has lead to summarisers that are ro-
bust and perform very well for most texts (Radev
et al., 2004; Dorr and Zajic, 2003; Carbonell and
Goldstein, 1998). The methods applied include a
random-surfer model (Mihalcea and Tarau, 2004;
Radev, 2004), a model of attraction and repul-
sion of similar summary sentences (Carbonell and
Goldstein, 1998). There are statistical models of
sentence shortening (Knight and Marcu, 2002).
While much work in summarisation has concen-
trated on multi-document summarisation, where
the main challenge is the detection of redundant
information, the summariser presented here is a
single-document summariser.
However, researchers have been attracted by
deeper, more symbolic and thus more explana-
tory summarisation models that use semantic rep-
resentations of some form (Radev and McKe-
own, 1998) and often rely on explicit discourse
modelling (Lehnert, 1981; Kintsch and van Dijk,
1978; Cohen, 1984). The problem with template-
based summarisers is that they tend to be domain-
dependent; the problem with discourse structure-
based summarisers is in general that they require
knowledge modelling and reasoning far beyond
the capability of today?s state of the art in arti-
ficial intelligence. Rhetorical Structure Theory
(Mann and Thompson, 1987) provides a domain-
independent framework that takes local discourse
structure into account, which has lead to a suc-
cessful prototype summariser (Marcu, 2000). This
summarisation strategy does not however look at
the lexical content of the propositions or clause-
like units it connects, only at the way how the con-
nection is performed.
The summariser presented here is a hybrid: its
core algorithm is symbolic, but its limited powers
of generalisation come from a semantic similarity
metric that is defined via distributionally derived
probabilities. Because its core processing is sym-
bolic and based on a simple semantic representa-
tion, it is possible to derive an explanation based
on the coherence tree and the propositions selected
from it. There are some similarities to the idea of
summarisation via lexical chains (Barzilay and El-
hadad, 1997), as both methods trace concepts (as
representatives of topics) across a document. The
KvD model arguably uses more informative mean-
ing units, as it is based on the combination of con-
cepts within propositions, rather than on concept
repetition alone.
A different, related stream of research looked
at the automatic detection of coherence in text.
Graesser et al (2004) present a coherence checker
based on over 200 coherence metrics, including
argument overlap as in KvD. Barzilay and Lap-
ata (2008) use a profiling of texts akin to Centering
theory to rank texts according to their coherence.
It would be interesting to combine their notion of
entity-based coherence with KvD?s notion of ar-
gument overlap.
4 Experiments
We now perform two experiments. The first tests
the contribution of our concept matcher and root
change strategy on a small document set we have
collected, and compares against two research sum-
marisers. In the second experiment, we test the
performance of our summariser on a much larger
and standard dataset.
We will use the intrinsic evaluation strategy of
comparison to a gold standard. Human judge-
ments would be the most credible, but as a cheap
alternative, we use ROUGE-L (Lin, 2004), which
has been shown to correlate well to human judge-
ments. For each sentence, ROUGE-L treats it as a
sequence of words, and finds the longest common
subsequences (LCSs) with any sentence in a gold
standard summary. The score is defined as the F-
measure of the precision and recall of the LCSs.
737
The next question is how the gold standard sum-
maries used in ROUGE are defined. Because our
summariser is deep and has a fine granularity, it
should be compared against human-written sum-
maries on a variety of texts.
For the first experiment, we have collected from
volunteers 8 human abstractive summaries for
each of the 4 short scientific articles or stories
we found in Kintsch and Vipond (1979) (average
length: 120 words), and 4 for each of 2 longer po-
litical news texts (average length: 523 words). The
volunteers were instructed to condense the text to
1/3 of its length for the short texts, and to 100
words for the longer ones. They were also in-
structed not to paraphrase, but to use the words in
the text as much as possible. This was because no
summariser in this experiment has a paraphrasing
ability. Nevertheless, not all subjects followed this
instruction strictly.
For the second experiment, we use the DUC 2002
dataset (Over and Liggett, 2002). There are 827
texts from news media, of a variety of topics and
lengths, among which our script is able to extract
titles and contents of 822 documents. We use the
provided single document abstractive summaries,
which are of 100 words in length each, as gold
standard summaries. A few of the documents are
selected in multiple clusters and therefore have
multiple summaries; all of them are used in evalu-
ation.
We compare our summariser against a baseline
constructed with the first n words from the origi-
nal text, where n is the summary length as defined
above, and two summarisers: MEAD (Radev et
al., 2004) is a research summariser which uses a
centroid-based paradigm and is known to perform
generally well over a range of texts. LexRank
(Radev, 2004) uses lexically derived similarities in
its similarity graph of sentences, sharing the same
idea of sentence similarity with MEAD. Note that
both summarisers are extractive.
We illustrate what our summaries look like in Ta-
ble 2, where we asked the summariser to give us
summaries as close to 20 and 50 word summaries
as possible, with Table 3 showing the underlying
propositions. In contrast, MEAD can only extract
sentences as-is (thus not as flexible in length), and
does not have meaning blocks like our proposi-
tions.
Encounters between police and Black Panther members.
Students to complain of harassment. Automobiles Panther
Party signs glued to bumpers.
Bloody encounters between police and Black Panther
members punctuated the summer days of 1969. Students
to complain of continuous harassment by law enforcement
officers. They receiving many traffic citations. Automo-
biles with Panther Party signs glued to their bumpers. I to
determine whether we were hearing the voice of paranoia
or reality.
Table 2: Summaries produced by our summariser.
3 encounters (between: police; between: Black Pan-
ther members)
16 to complain (students; of: harassment)
34 with: Panther Party signs (automobiles)
35 glued (#34; to: bumpers)
Table 3: Summary propositions for the first sum-
mary above.
We create summaries for all three summarisers
following this procedure: We provide sentence-
split texts and their headlines (not needed by
LexRank), and run the summarisers in such a way
as to produce a summary of the same length as
stipulated for the standard summaries. Our sum-
mariser controls word count precisely; we require
MEAD to produce summaries close to the length
(allowing variations), and for LexRank we allow
it to go beyond the limit by less than one sentence
and then discard the exceeding part in the sentence
with the lowest salience.
The results of Experiment 1 are summarised in Ta-
ble 4. As is well-known from similar experiments,
it is hard beating the first n baseline due to the fact
that journalistic style (in the long texts) already
puts a summary of each text first. It is slightly
surprising that this effect also holds for the short
texts (literary style). It is of note that our KvD
summariser beats both MEAD and LexRank on
this dataset, which is shelved away during devel-
opment, with statistical significance on the long
texts: the 95%-confidence interval of ours is 0.403
? 0.432, and that of MEAD is 0.370 ? 0.411.
Long Texts Short Texts
Ours 0.418 0.333
Ours ? without similarity 0.396 0.271
Ours ? without word info 0.319 0.185
Ours ? without root change 0.388 0.348
MEAD 0.391 0.343
LexRank 0.378 0.326
First n words 0.460 0.368
Table 4: ROUGE-L F-measures for Experiment 1.
738
Precision Recall F-measure
Our summariser 0.361 0.332 0.344
MEAD 0.366 0.355 0.358
First n words 0.403 0.395 0.399
Table 5: ROUGE-L scores for Experiment 2.
We test whether concept matching is beneficial
by switching off similarity derived from distribu-
tional semantics, or switching off all ?word infor-
mation? which includes distributional semantics,
lemmatisation, and coreference detection, i.e. to
consider matching only for the same word. Per-
formance deteriorates when concept matching is
switched off, substantially if all word information
is off. This confirms our hypothesis that one of
the cornerstones of KvD, concept matching, can
be at least partially simulated using today?s distri-
butional semantics methods. As for root change,
turning it off seems to hurt performance on the
longer texts, but not so on the shorter ones, which
matches our speculation that root change is useful
for longer texts, which have some focus shifts.
The result of Experiment 2 is shown in Table 5.
This experiment on a large dataset demonstrates
that our summariser performs in the ballpark of
typical results of extractive summarisers, although
it is still statistically a little worse than the state-of-
the-art MEAD (whose F-measure 95%-confidence
interval is 0.349 ? 0.367). Our summariser is
good at precision because many summaries pro-
duced have not used up the 100-word limit, mak-
ing the average summary length smaller than that
of MEAD?s. This indicates that our summariser
might be good at very short summaries, or we
could improve the memory selection to allow for
a more diversified important proposition set. Con-
sidering this, and the fact that we have many pa-
rameters not tuned for the task, and we have not
utilised the structural / positional features (whose
importance is shown in the first-n baseline), the
result is still encouraging.
5 Conclusions
We present here a first prototype of the feasibility
of basing a summarisation algorithm on Kintsch
and van Dijk?s (1978) model. Our implemen-
tation successfully creates flexible-length sum-
maries, highly compressed if desired, and provides
some explanation for why certain meaning units
appear in the summary. We have avoided some of
the hardest aspects of KvD?s model, which have to
do with the generation of macropropositions and
with keeping closer track of larger discourse struc-
tures, but we show that some core aspects of the
model can be approximated with today?s parsing
and lexical semantics technology. Although the
output summaries are not yet in all cases grammat-
ical, we show that our system performs compara-
bly with extractive state-of-the-art summarisers.
During the implementation, we had to solve sev-
eral practical problems that the KvD did not give
enough procedural detail about, or skipped over
in their manual simulation. For instance, we have
turned the distinction between LTM and STM to
two parallel salience levels from KvD?s two dis-
joint stages, formalised the tree building process
and improved KvD?s root choice strategy.
The KvD model does not keep track of unique
events, but would profit from doing so, for in-
stance in texts where more than one event of the
same type is referred to. It has no explicit model
of time, but would profit from one. It does not
even use information about which entities in a text
form the same concept or individual, for selecting
all information about that concept into the sum-
mary. There are also many interesting ways how
the memory cycle could be modified by giving
more weight to particular events, concepts and in-
dividuals.
On the implementational side, much remains to
be tried. Anything that improves the proposition
builder should bear direct fruit in the quality of
the summaries. The limitations of our proposi-
tion builder come from the limitations of parsing
technology as well as the fact that semantics is not
entirely determined by syntax. For instance, we
noticed some problems caused by incorrect prepo-
sitional phrase attachment. A better coreference
system would also improve this summariser im-
mensely, reducing much uncertainty in the con-
cept matching. The deep nature of the summariser
also enables natural language generation to im-
prove the readability of our textual summary.
Acknowledgement
Joint scholarship from the Cambridge Common-
wealth, European & International Trust and the
China Scholarship Council is gratefully acknowl-
edged.
739
References
A Baddeley. 2007. Working memory, thought, and ac-
tion. Oxford University Press.
Regina Barzilay and Michael Elhadad. 1997. Us-
ing lexical chains for text summarization. In Inderjeet
Mani and Mark T. Maybury, editors, Proceedings of the
ACL/EACL-97 Workshop on Intelligent Scalable Text
Summarization.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
BK Britton and S Gulgoz. 1991. Using kintsch?s
computational model to improve instructional text: Ef-
fects of repairing inference calls on recall and cognitive
structures. Journal of Educational Psychology.
Ann L. Brown, Jeanne D. Day, and Jones R. S. 1983.
The development of plans for summarizing text. Child
development. was in press in 1983.
Jaime Carbonell and Jade Goldstein. 1998. The use
of MMR, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings of
the 21th (SIGIR-98), pages 335?336, Melbourne, Aus-
tralia.
Robin Cohen. 1984. A computational theory of the
function of clue words in argument understanding. In
Proceedings of the 10th (COLING-84), pages 251?255.
J Coiro and E Dobler. 2007. Exploring the online
reading comprehension strategies used by sixthgrade
skilled readers to search for and locate information on
the internet. Reading research quarterly.
KA DeLong, TP Urbach, and M Kutas. 2005. Prob-
abilistic word pre-activation during language compre-
hension inferred from electrical brain activity. Nature
neuroscience.
Bonnie Dorr and David Zajic. 2003. Hedge trimmer:
A parse-and-trim approach to headline generation. In
in Proceedings of Workshop on Automatic Summariza-
tion, pages 1?8.
MP Driscoll. 2005. Psychology of learning for instruc-
tion. Allyn and Bacon.
NK Duke and PD Pearson. 2002. Effective practices
for developing reading comprehension. In Alan E.
Farstrup and S. Jay Samuels, editors, What research
has to say about reading instruction.
Brigitte Endres-Niggemeyer. 1998. Summarizing In-
formation. Springer-Verlag, New York, NY.
Arthur C. Graesser, Danielle S. McNamara, Max M.
Louwerse, and Zhiqiang Cai. 2004. Coh-metrix:
Analysis of text on cohesion and language. Behav-
ior Research Methods, Instruments, & Computers,
36(2):193?202.
V Anderson Hidi. 1986. Producing written sum-
maries: Task demands, cognitive operations, and impli-
cations for instruction. Review of educational research.
A King. 1992. Comparison of self-questioning, sum-
marizing, and notetaking-review as strategies for learn-
ing from lectures. American Educational Research
Journal.
Walter Kintsch and Teun A. van Dijk. 1978. Toward a
model of text comprehension and production. Psycho-
logical review, 85(5):363?394.
Walter Kintsch and Douglas Vipond. 1979. Read-
ing comprehension and readability in educational prac-
tice and psychological theory. In Lars-G?oran Nilsson,
editor, Perspectives on Memory Research: Essays in
Honor of Uppsala?s 500th Anniversary, pages 329?
365. Erlbaum Associates.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 423?430. Association for
Computational Linguistics.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelligence,
139(1).
Keiko Koda. 2005. Insights into second language
reading: A cross-linguistic approach. Cambridge Uni-
veristy Press.
Wendy G Lehnert. 1981. Plot units and narrative sum-
marization. Cognitive Science, 5(4):293?331.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic
evaluation of summaries using n-gram co-occurrence
statistics. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Compu-
tational Linguistics on Human Language Technology-
Volume 1, pages 71?78. Association for Computational
Linguistics.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th interna-
tional conference on Computational linguistics-Volume
2, pages 768?774. Association for Computational Lin-
guistics.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Workshop,
pages 74?81.
William C. Mann and Sandra A. Thompson. 1987.
Rhetorical Structure Theory: Description and construc-
tion of text structures. In Gerard Kempen, editor, Natu-
ral Language Generation: New Results in Artificial In-
telligence, Psychology, and Linguistics, pages 85?95.
Marinus Nijhoff Publishers, Dordrecht, NL.
Daniel Marcu. 2000. The Theory and Practice of Dis-
course Parsing and Summarization. MIT Press.
R Mihalcea and P Tarau. 2004. Textrank: Bringing
order into texts. In Proceedings of the EMLNP.
George A Miller. 1995. Wordnet: a lexical database
740
for english. Communications of the ACM, 38(11):39?
41.
Paul Over and W Liggett. 2002. Introduction to
duc: An intrinsic evaluation of generic news text sum-
marization systems. In Proc. DUC. http://www-
nlpir.nist.gov/projects/duc/guidelines/2002.html.
A Paivio. 1990. Mental representations. Oxford Sci-
ence Publications.
Aannemarie Sullivan Palinscar and Ann L. Brown.
1984. Reciprocal teaching of comprehension-fostering
and comprehension-monitoring activities. Cognition
and Instruction, 1:117?175.
Michael Pressley. 2006. Reading instruction that
works: The case for balanced teaching. Guildford
Press.
Dragomir R. Radev and Kathleen R. McKeown. 1998.
Generating natural language summaries from multiple
on-line sources. 24(3):469?500.
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda Celebi, Stanko Dim-
itrov, Elliott Drabek, Ali Hakim, Wai Lam, Danyu Liu,
Jahna Otterbacher, Hong Qi, Horacio Saggion, Simone
Teufel, Michael Topper, Adam Winkel, and Zhu Zhang.
2004. Mead ? a platform for multidocument multilin-
gual text summarization. In Proceedings of LREC-04.
Dragomir R. Radev. 2004. Lexrank: Graph-based lex-
ical centrality as salience in text summarization. Jour-
nal of Artificial Intelligence Research.
F Smith. 2004. Understanding reading: A psy-
cholinguistic analysis of reading and learning to read.
Lawrence Erlbaum.
Karen Sp?arck Jones. 1993. What might be in a sum-
mary? Technical report, Computer Laboratory, Uni-
versity of Cambridge.
Teun A. van Dijk. 1977. Text and Context: Explo-
rations in the Semantics and Pragmatics of Discourse.
Longman, London, UK.
RA Zwaan. 2003. The immersed experiencer: Toward
an embodied theory of language comprehension. Psy-
chology of learning and motivation.
741
Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 45?55,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Resolving Coreferent and Associative Noun Phrases in Scientific Text
Ina R
?
osiger
Institute for Natural Language Processing
University of Stuttgart, Germany
Pfaffenwaldring 5b, 70569 Stuttgart
roesigia@ims.uni-stuttgart.de
Simone Teufel
Computer Laboratory
University of Cambridge, UK
15 JJ Thomson Avenue, Cambridge CB3 0FD
sht25@cl.cam.ac.uk
Abstract
We present a study of information sta-
tus in scientific text as well as ongoing
work on the resolution of coreferent and
associative anaphora in two different sci-
entific disciplines, namely computational
linguistics and genetics. We present an an-
notated corpus of over 8000 definite de-
scriptions in scientific articles. To adapt a
state-of-the-art coreference resolver to the
new domain, we develop features aimed at
modelling technical terminology and inte-
grate these into the coreference resolver.
Our results indicate that this integration,
combined with domain-dependent train-
ing data, can outperform the performance
of an out-of-the-box coreference resolver.
For the (much harder) task of resolving as-
sociative anaphora, our preliminary results
show the need for and the effect of seman-
tic features.
1 Introduction
Resolving anaphoric relations automatically re-
quires annotated data for training and testing.
Anaphora and coreference resolution systems
have been tested and evaluated on different genres,
mainly news articles and dialogue. However, for
scientific text, annotated data are scarce and coref-
erence resolution systems are lacking (Sch?afer et
al., 2012). We present a study of anaphora in sci-
entific literature and show the difficulties that arise
when resolving coreferent and associative entities
in two different scientific disciplines, namely com-
putational linguistics and genetics.
Coreference resolution in scientific articles is con-
sidered difficult due to the high proportion of def-
inite descriptions (Watson et al., 2003), which
typically require domain knowledge to be re-
solved. The more complex nature of the texts is
also reflected in the heavy use of abstract entities
such as results or variables, while easy-to-resolve
named entities are less frequently used. We test
an existing, state-of-the-art coreference resolution
tool on scientific text, a domain on which it has
not been trained, and adapt it to this new do-
main. We also address the resolution of asso-
ciative anaphora (Clark, 1975; Prince, 1981), a
related phenomenon, which is also called bridg-
ing anaphora. The interpretation of an associative
anaphor is based on the associated antecedent, but
the two are not coreferent. Examples 1 and 2 show
two science-specific cases of associative anaphora
from our data.
(1) Xe-Ar was found to be in a layered structure
with Ar on the surface
1
.
(2) We base our experiments on the Penn tree-
bank. The corpus size is ...
The resolution of associative links is important be-
cause it can help in tasks which use the concept
of textual coherence, e.g. Barzilay and Lapata
(2008)?s entity grid or Hearst (1994)?s text seg-
mentation. They might also be of use in higher-
level text understanding tasks such as textual en-
tailment (Mirkin et al., 2010) or summarisation
based on argument overlap (Kintsch and van Dijk,
1978; Fang and Teufel, 2014).
Gasperin (2009) showed that biological texts dif-
fer considerably from other text genres, such as
news text or dialogue. In this respect, our results
confirm that the proportion between non-referring
and referring entities in scientific text differs from
that reported for other genres. The same holds for
the type and relative number of linguistic expres-
sions used for reference. To address this issue, we
decided to investigate information status (Nissim
et al., 2004) of noun phrases. Information status
tells us whether a noun phrase refers to an already
1
Anaphors are typed in bold face, their antecedents shown
in italics.
45
known entity, or whether it can be treated as non-
referring. Since no corpus of full-text scientific ar-
ticles annotated with both information status and
anaphoric relations was available, we had to cre-
ate and annotate our own corpus. The main con-
tributions of this work are (i) a new information
status-based annotation scheme and an annotated
corpus of scientific articles, (ii) a study of infor-
mation status in scientific text that compares the
distribution of the different categories in scientific
text with the distribution in news text, as well as
between the two scientific disciplines, (iii) exper-
iments on the resolution of coreferent anaphora:
we devise domain adaptation for science and show
how this improves an out-of-the-box coreference
resolver, and (iv) experiments on the resolution of
associative anaphora with a coreference resolver
that is adapted to this new notion of ?reference?
by including semantic features. To the best of our
knowledge, this is the first work on anaphora res-
olution in multi-discipline, full-text scientific pa-
pers that also deals with associative anaphora.
2 Related Work
Noun phrase coreference resolution is the task of
determining which noun phrases (NPs) in a text or
dialogue refer to the same real-world entities (Ng,
2010). Resolving anaphora in scientific text has
only recently gained interest in the research com-
munity and focuses mostly on the biomedical do-
main (Gasperin, 2009; Batista-Navarro and Ana-
niadou, 2011; Cohen et al., 2010). Some work has
been done for other disciplines, such as compu-
tational linguistics. Sch?afer et al. (2012) present
a large corpus of 266 full-text computational lin-
guistics papers from the ACL Anthology, anno-
tated with coreference links. The CoNLL shared
task 2012 on modelling multilingual unrestricted
coreference in OntoNotes (Pradhan et al., 2012)
produced several state-of-the-art coreference sys-
tems (Fernandes et al., 2012; Bj?orkelund and
Farkas, 2012; Chen and Ng, 2012) trained on news
text and dialogue, as provided in the OntoNotes
corpus (Hovy et al., 2006). Other state-of-the-art
systems, such as Raghunathan et al. (2010) and
Berkeley?s Coreference Resolution System (Dur-
rett and Klein, 2013), also treat coreference as
a task on news text and dialogue. We base our
experiments on the IMS coreference resolver by
Bj?orkelund and Farkas (2012), one of the best pub-
licly available English coreference systems. The
resolver uses the decision of a cluster-based de-
coding algorithm, i.e. one that decides whether
two mentions are placed in the same or in different
clusters, or whether they should be considered sin-
gletons. Their novel idea is that the decision of this
algorithm is encoded as a feature and fed to a pair-
wise classifier, which makes decisions about pairs
of mentions rather than clusters. This stacked ap-
proach overcomes problems of previous systems
that are based on the isolated pairwise decision.
The features used are mostly taken from previous
work on coreference resolution and encode a va-
riety of information, i.e, surface forms and their
POS tags, subcategorisation frames and paths in
the syntax tree as well as the semantic distance be-
tween the surface forms (e.g. edit distance).
However, none of this work is concerned with
associative anaphora. Hou et al. (2013) present
a corpus of news text annotated with associative
links that are not limited with respect to semantic
relations between anaphor and antecedent. Their
experiments focus on antecedent selection only,
assuming that the recognition of associative enti-
ties has already been performed. Information sta-
tus has been investigated extensively in different
genres such as news text, e.g. in Markert et al.
(2012). Poesio and Vieira (1998) performed an in-
formation status-based corpus study on news text,
defining the following categories: coreferential,
bridging, larger situation, unfamiliar and doubt.
To the best of our knowledge, there is currently
no study on information status in scientific text.
In this paper, we propose a classification scheme
for scientific text that is derived from Riester et
al. (2010) and Poesio and Vieira (1998). We in-
vestigate the differences between news text and
scientific text by analysing the distribution of in-
formation status categories. We hypothesise that
the proportion of associative anaphora in scientific
text is higher than in news text, making it neces-
sary to resolve them in some form. Our exper-
iments on the resolution of coreferent anaphora
concern the domain-adaptation of a coreference
resolver to this new domain and examine the effect
of domain-dependent training data and features
aimed at capturing technical terminology. We also
present an unusual setup where we assume that an
existing coreference resolver can also be used to
identify associative links. We integrate semantic
features in the hope of detecting cases where do-
main knowledge is required to establish the rela-
tion between the anaphor and the antecedent.
46
Category Example
COREFERENCE LINKS GIVEN (SPECIFIC) We present the following experiment. It deals with ...
GIVEN (GENERIC) We use the Jaccard similarity coefficient in our experiments.
The Jaccard similarity coefficient is useful for ...
ASSOCIATIVE LINKS ASSOCIATIVE Xe-Ar was found to be in a layered structure with Ar
on the surface .
ASSOCIATIVE
The structure of the protein ...
(SELF-CONTAINING)
DESCRIPTION The fact that the accuracy improves ...
Categories UNUSED Noam Chomsky introduced the notion of ...
without links DEICTIC This experiment deals with ...
PREDICATIVE Pepsin, the enzyme, ...
IDIOM On the one hand ... on the other hand ...
DOUBT
Table 1: Categories in our classification scheme
3 Corpus Creation
We manually annotated a small scientific corpus
to provide a training and test corpus for our exper-
iments, using the annotation tool Slate (Kaplan et
al., 2012).
3.1 Annotation Scheme
Two types of reference are annotated, namely
COREFERENCE and ASSOCIATIVE LINKS.
COREFERENCE LINKS are annotated for all types
of nominal phrases; such links are annotated
between enitites that refer to the same referent
in the real world. ASSOCIATIVE LINKS and
information status categories are only annotated
for definite noun phrases. In our scheme, ASSO-
CIATIVE LINKS are only annotated when there is
a clear relation between the two entities. As we
do not pre-define possible associative relations,
this definition is vague, but it is necessary to keep
the task as general as possible. Additionally,
we distinguish the following nine categories,
as shown in Table 1
2
: The category GIVEN
comprises coreferent entities that refer back to an
already introduced entity. If a coreference link
is detected, the referring expression is marked
as GIVEN and the link with its referent NP is
annotated. The obligatory attribute GENERIC tells
us whether the given entity has a generic or a
specific reading. ASSOCIATIVE refers to entities
that are not coreferent but whose interpretation
is based on a previously introduced entity. A
typical relation between the two noun phrases is
meronymy, but as mentioned above we do not
pre-define a set of allowed semantic relations.
2
The entity being classified is typed in bold face, referring
expressions are marked by a box and the referent is shown in
italics.
The category ASSOCIATIVE (SELF-CONTAINING)
comprises cases where we identify an associative
relation between the head noun phrase and the
modifier. ASSOCIATIVE SELF-CONTAINING
entities are annotated without a link between the
two parts. In scientific text, an entity is considered
DEICTIC if it points to an object that is connected
to the current text. Therefore, we include all
entities that refer to the current paper (or aspects
thereof) in this category. Entities that have not
been mentioned before and are not related to any
other entity in the text, but can be interpreted
because they are part of the common knowledge
of the writer and the reader are covered by the
category UNUSED. DESCRIPTION is annotated
for entities that are self-explanatory and typically
occur in particular syntactic patterns such as
NP complements or relative clauses. Idiomatic
expressions or metaphoric use are covered in
the category IDIOM. Predicative expressions,
including appositions, are annotated as PRED-
ICATIVE. Finally, the category DOUBT is used
when the text or the antecedent is unclear. Note
that NEW, a category that has been part of most
previous classification schemes of information
status, is not present as this information status is
typically observed in indefinite noun phrases. As
we deal exclusively with definite noun phrases
3
,
we do not include this category in our scheme.
In contrast to Poesio and Vieira?s scheme, ours
contains the additional categories PREDICATIVE,
ASSOCIATIVE SELF-CONTAINING, DEICTIC and
IDIOM.
3
With the exception of coreferring anaphoric expressions,
as previously discussed.
47
GEN CL
Sentences 1834 1637
Words 43691 38794
Def. descriptions 3800 4247
Table 2: Properties of the annotated two subcor-
pora, genetics (GEN) and computational linguis-
tics (CL)
GEN CL
Coreference links 1976 2043
Associative links 328 324
Given 1977 2064
Associative 315 280
Associative (sc) 290 272
Description 810 1215
Unused 286 286
Deictic 28 54
Predicative 9 19
Idiom 9 34
Doubt 39 22
Table 3: Distribution of information status cate-
gories and links in the two disciplines, in absolute
numbers
3.2 Resulting Corpus
Our annotated corpus contains 16 full-text sci-
entific papers, 8 papers for each of the two
disciplines. The computational linguistics (CL)
papers cover various topics ranging from dialogue
systems to machine translation; the genetics
(GEN) papers deal mostly with the topic of short
interfering RNAs, but focus on different aspects of
it. In total, the annotated computational linguistics
papers contain 1637 sentences, 38,794 words and
4247 annotated definite descriptions while the
annotated genetics papers contain 1834 sentences,
43,691 words and 3800 definite descriptions; the
two domain subcorpora are thus fairly comparable
in size. See Table 2 for corpus statistics and
Table 3 for the distribution of categories and links.
It is well-known that there are large differences
in reference phenomena between scientific text
and other domains (Gasperin, 2009). In scientific
text, it is assumed that the reader has a relatively
high level of background. We would expect this
general property of scientific text to have an im-
pact on the distribution of categories with respect
to information status.
Table 4 compares the two scientific disciplines in
our study with each other. We note that the propor-
tion of entities classified as DESCRIPTION in the
CL papers is considerably higher than in the GEN
papers. The proportions of the other categories are
similar, though the proportion of GIVEN, ASSO-
CIATIVE and UNUSED entities is slightly higher in
the GEN articles.
Table 4 also compares the distribution of cat-
egories in news text (Poesio and Vieira, 1998;
P&V) with that of ours (as far as they are
alignable, using our names for categories). Note
that on a conceptual level, these categories are
equivalent, but there are some differences with re-
spect to the annotation guidelines.
The most apparent difference is the proportion of
UNUSED entities (6-7 % in science, 23 % in news
text) which might be due to the prevalence of
named entities in news text. Compared to the dis-
tribution of categories in news text, the proportion
of GIVEN entities is about 4-8 % higher in scien-
tific text. The proportion of ASSOCIATIVE enti-
ties
4
is twice as high in the scientific domain com-
pared to news text. UNUSED entities have a dis-
tinctly lower proportion, with about 7%. As our
guidelines limit deictic references to only those
that refer to (parts of) the current paper, we get
a slightly lower proportion than the 2 % in news
text, reported by Poesio and Vieira (1998) in an
earlier experiment, where no such limitation was
present.
Category GEN CL P&V
Given 52.03 48.60 44.00
Associative 8.29 6.59 8.50
Associative (sc) 7.63 6.40 ?
Description 21.31 28.61 21.30
Unused 7.53 6.73 23.50
Deictic 0.74 1.27 ?
Predicative 0.24 0.45 ?
Idiom 0.24 0.80 (2.00)
Doubt 1.03 0.52 2.60
Table 4: Distribution of information status cate-
gories in different domains, in percent
It has been shown in similar annotation exper-
iments on information status, with similarly fine-
grained schemes (Markert et al., 2012; Riester et
al., 2010), that it is possible to achieve annotation
with marginally to highly reliable inter-annotator
agreement. In our experiments, only one per-
son (the first author) performed the annotation,
so that we cannot compute any agreement mea-
surements. We are currently performing an inter-
annotator study with two additional annotators so
that we can better judge human agreement and use
the annotations as a reliable gold standard.
4
The union of categories ASSOCIATIVE and ASSOCIA-
TIVE SELF-CONTAINING.
48
4 Adapting a Coreference Resolver to
the Scientific Domain
To show the difficulties that a coreference resolver
faces in the scientific domain, we ran, out-of-the-
box, a coreference system (Bj?orkelund and Farkas,
2012), that has not been trained on scientific text,
on our corpus and perform an error analysis. In
particular, we are curious about which of the sys-
tem?s errors are domain-dependent. This analysis
motivates a set of terminological features that are
incorporated and tested in Section 6.
4.1 Error Analysis
Domain-dependent errors. The lack of seman-
tic, domain-dependent knowledge results in the
system?s failure to identify coreferent expressions,
e.g. those expressed as synonyms. This type of
error can be prevented by implementing domain-
dependent knowledge. In Example 3, we would
like to generate a link between treebank and cor-
pus as these terms are used as synonyms. The
same is true for protein-forming molecules and
amino acids in Example 4.
(3) Experiments were performed with the clean
part of the treebank. The corpus consists of
1 million words.
(4) Amino acids are organic compounds made
from amine (-NH2) and carboxylic acid (-
COOH) functional groups. The protein-
forming molecules ...
Another common error is that the coreference
resolver links all occurrences of demonstrative
science-specific expressions such as this paper
or this approach to each other, even if they are
several paragraphs apart. In most cases, these
demonstrative expressions do not corefer, but
refer to an approach or a paper recently described
or cited. This type of error is particularly frequent
in the computational linguistics domain and
might be reduced by a feature that captures this
peculiarity. A special case occurs when authors
re-use clauses of the abstract in the introduction.
The coreference resolver then interprets rather
large spans as coreferent which are not annotated
in the gold standard. Yet a different kind of error
is based on the fact that the coreference resolver
has been trained on OntoNotes, i.e. mostly on
non-written text. Thus, the classifier has not seen
certain phenomena and, for example, links all
occurrences of e.g. into one equivalence class as
it is interpreted as a named entity.
General errors. Some errors are general er-
rors of coreference resolvers in the sense that they
have very little to do with domain dependence,
such as choosing the wrong antecedent or link-
ing non-referential occurrences of it (see Exam-
ples 5 and 6).
(5) This approach allows the processes of build-
ing referring expressions and identifying
their referents.
(6) The issue of how to design sirnas that pro-
duce high efficacy is the focus of a lot of cur-
rent research. Since it was discovered that ...
4.2 Terminological Features
This section deals with the design of possible
terminological features for our experiments that
are aimed at capturing some form of domain
knowledge. We create these using the information
in 1000 computational linguistics and 1000
genetics papers that are not part of our scientific
corpus.
Non-coreferring bias list. Our first feature
concentrates on nouns which have a low proba-
bility to be coreferring (i.e. category GIVEN) if
they appear as the head of noun phrase. We as-
sume that the normal case of coreference between
definite noun phrases is that of a concept intro-
duced as an indefinite NP and later referred to as
a definite NP, and compile a list of lexemes that
do not follow this pattern. NPs with those lexemes
should be more likely to be of category UNUSED or
DESCRIPTION. We find the lexemes by recording
head nouns of definite NPs which are not observed
in a prior indefinite NP in the same document (lo-
cal list) or the entire document collection (global
list). We create two lists of such head words for
every discipline. The lexemes are arranged in de-
creasing order of their frequency so that we can
use both their presence or non-presence on the list
and their rank on the list as potential features.
As can be seen in Table 5, the presence, the be-
ginning and the literature are definite descriptions
that are always used without having been intro-
duced to the discourse. These terms are either part
of domain knowledge (the hearer, the reader) or
part of the general scientific terminology (the lit-
erature). In the local list we see expressions that
can be used without having been introduced, but
49
may in some contexts occur in the indefinite form
as well, e.g. the word or the sentence.
CL GEN
(a) global (b) local (a) global (b) local
presence number manuscript data
beginning word respect region
literature sentence prediction gene
hearer training monograph case
reader user notion species
Table 5: Top five terms of local and global non-
coreferring bias lists
Collocation list. One of our hypotheses is that
the NPs occurring in verb-object collocations are
typically not part of any coreference chain. To test
this, we use our collection of 2000 scientific pa-
pers to extract domain-specific verb-object collo-
cations. We assume that for some collocations,
this tendency is stronger (make use, take place)
than for others that could potentially be corefer-
ring (see figure, apply rule). The collocations have
been identified with a term extraction tool (Gojun
et al., 2012). Every collocation that occurs at least
twice in the data is present on the list. Table 6
shows the most frequent terms.
make + use take + place
give + rise silence + activity
derive + form refashion + plan
parse + sentence predict + sirna
sort + feature match + predicate
see + figure use + information
silence + efficiency follow + transfection
embed + sentence apply + rule
focus + algorithm stack + symbol
Table 6: Most frequent occurring collocation can-
didates in scientific text (unsorted)
Argumentation nouns, work nouns and
idioms. As mentioned in Section 4.1, the
baseline classifier often links demonstrative,
science-specific entities, even if they are several
paragraphs apart. To prevent this, we combine a
distance measure with a set of 182 argumentation
and work nouns taken from Teufel (2010), such as
achievement, claim or experiment. We also create
a small list of idioms as they are never part of a
coreference chain.
5 Adapting a Coreference Resolver for
Associative Links in Science
We now turn to the much harder task of resolving
associative anaphora.
5.1 Types of Associative Anaphora
To illustrate the different types of associative
anaphora, we here show a few examples, mostly
taken from the genetics papers. The anaphors
are shown in bold face, the antecedents in italics.
Many associative anaphors include noun phrases
with the same head. In most of these cases, the
anaphor contains a different modifier than the an-
tecedent, such as
(8) the negative strain ... the positive strain;
(9) three categories ... the first category;
(10) siRNAs ... the most effective siRNAs.
We assume that these associative relations can
be identified with a coreference resolver without
adding additional features. Other cases are much
harder to identify automatically, such as those
where semantic knowledge is required to interpret
the relation between the entities:
(11) the classifier ... the training data;
(12) this database ... the large dataset.
In other cases, the nominal phrase in the an-
tecedent tends to be derivationally related to the
head word in the anaphor, as in
(13) the spotty distribution ...the spots;
(14) competitor ... the competitive effect.
There are also a number of special cases, such as
(15) the one interference process ... the other in-
terference process.
We hypothesise that the integration of semantic
features discussed in the previous section enables
the resolver to cover more than just those cases
that are based on the similarity of word forms.
5.2 Semantic Features
It is apparent that the recognition and correct
resolution of associative anaphora requires se-
mantic knowledge. Therefore, we adapt the
coreference resolver by extending the WordNet
feature, one of the features implemented in the
IMS resolver, to capture more than just synonyms.
50
We use the following WordNet relations: Hyper-
nymy (macromolecule ? protein), hyponymy
(nucleoprotein ? protein), meronymy (surface
? structure), substance meronymy (amino
acid ? protein), topic member (acute, chronic?
medicine) and topic ( periodic table? chemistry).
WordNet?s coverage in the scientific domain is
surprisingly good: 75,91 % of all common nouns
in the GEN papers and 88,12 % in the CL papers
are listed in WordNet. Terms that are not cov-
ered are, for example, abbreviations of different
types of ribonucleic acid in genetics or specialist
terms like tagging, subdialogue or SVM in com-
putational linguistics.
6 Experiments
We now compare the performance of an out-
of-the-box coreference system with the resolver
trained on our annotated scientific corpus (Sec-
tion 6.2). We also show the effect of adding ad-
ditional features aimed at capturing technical ter-
minology. In the experiments on the resolution of
associative anaphora (Section 6.3), we test the hy-
pothesis that the coreference resolver is able to ad-
just to the new notion of reference and show the
effect of semantic features.
6.1 Experimental Setup
We perform our experiments using the IMS coref-
erence resolver as a state-of-the-art coreference
resolution system (Bj?orkelund and Farkas, 2012)
5
.
The algorithm and the features included have not
been changed except where otherwise stated. We
use the OntoNotes datasets from the CoNLL 2011
shared task
6
(Pradhan et al., 2012; Hovy et al.,
2006), only for training the out-of-the-box sys-
tem. We also use WordNet version 3.0 as pro-
vided in the 2012 shared task
7
as well as JAWS,
the Java API for WordNet searching
8
. Perfor-
mance is reported on our annotated corpus, us-
ing 8-fold cross-validation and the official CoNLL
scorer (version 5).
5
See: www.ims.uni-stuttgart.de/forschung/
ressourcen/werkzeuge/IMSCoref.html
We follow their strategy to use the AMP decoder as the
first decoder and the PCF decoder, a pairwise decoder, as a
second. The probability threshold is set to 0.5 for the first
and 0.65 for the second decoder.
6
http://conll.cemantix.org/2011/data.html
7
http://conll.cemantix.org/2012/data.html
8
http://lyle.smu.edu/
?
tspell/jaws/
6.2 Resolving Coreferent References
IMS coreference resolver unchanged. To be
able to judge the performance of an existing coref-
erence resolver on scientific text, we first re-
port performance without making any changes to
the resolver whatsoever, using different training
data. The BASELINE version is trained on the
OntoNotes dataset from the CoNLL 2011 shared
task. In the SCIENTIFIC version, we only use our
annotated scientific papers. MIXED contains the
entire OntoNotes dataset as well as the scientific
papers, leading to a larger training corpus which
compensates for the rather small size of the scien-
tific corpus
9
. Table 7 shows the average CoNLL
scores
10
of the two subdomains genetics and com-
putational linguistics.
Training Set GEN CL GEN+CL
Baseline 35.30 40.30 37.80
Scientific 44.94 42.41 43.68
Mixed 47.92 47.44 47.68
Table 7: Resolving coreferent references:
CoNLL metric scores for different training sets
The BASELINE achieves relatively low results
in comparison to the score of 61.24 that was
reported in the shared task (Bj?orkelund and
Farkas, 2012). Even though our scientific corpus
is only 7% the size of the OntoNotes dataset, it
inceases performance of the BASELINE system
by 15,6%. The SCIENTIFIC version outperforms
the BASELINE version for all of the GEN papers
and for 6 out of 8 CL papers. MIXED, the version
that combines the scientific corpus with the entire
OntoNotes dataset, proves to work best (47.92 for
GEN and 47.44 for CL). In THE BASELINE ver-
sion, the performance on the CL papers is better
than on the GEN papers. Interestingly, this is not
true for the SCIENTIFIC version, where the per-
formance on the GEN papers is better. However,
as the main result here, we can see that training
on scientific text was successful. The increase in
score in both the SCIENTIFIC and the MIXED ver-
sion over BASELINE is statistically significant
11
9
We also experimented with a balanced version, which
contains an equal amount of sentences from the OntoNotes
corpus and our scientific corpus. The results are not reported
here as this version performed worse.
10
The CoNLL score is the arithmetic mean of MUC, B
3
and CEAFE.
11
We compute significance using the Wilcoxon signed rank
test (Siegel and Castellan, 1988) at the 0.01 level unless oth-
erwise stated.
51
(+9.64 and +12.62 in the GEN papers, +2.11 and
+7.14 in the CL papers, absolute in CoNLL score).
IMS coreference resolver, adapted to the do-
main. We show the results from the expansion of
the feature set in Table 8. Each of the single fea-
tures is added to the version in the line above the
current version. Compared to the MIXED version,
adding the features to the resolver results in an
increase in performance for both of the scientific
disciplines. However, when adding the colloca-
tion feature to the version including the bias lists,
the argumentation nouns as well as idioms, perfor-
mance drops slightly. This might indicate the need
for a revised collocation list where those nouns are
filtered out that could potentially be coreferring,
e.g. see figure. For the best version of the CL
papers, the increase in CoNLL score, compared
with the MIXED version, is +1.08; for the GEN pa-
pers it is slightly less, namely +0.22. This increase
in score is promising, but the data is too small to
show significance.
GEN CL GEN+CL
Mixed 47.92 47.44 47.68
+ Bias Lists 48.04 47.79 47.94
+ Arg. Nouns and Idioms 48.14 48.52 48.33
+ Collocations 48.03 48.12 48.08
Table 8: Resolving coreferent references:
CoNLL scores for the extended feature sets
However, compared with the BASELINE ver-
sion, the final version (marked bold) performs sig-
nificantly better and outperforms the out-of-the-
box run by 36.47 % absolute on the CoNLL met-
ric for the GEN papers and by 20.40 % for the CL
papers. The results also show that, in our experi-
ments, the effect of using domain-specific training
material is larger than the effect of adding termi-
nological features.
6.3 Resolving Associative References
IMS coreference resolver unchanged. As
associative references are not annotated in the
OntoNotes dataset, the only possible baseline we
can use is the system trained on the scientific cor-
pus. Average CoNLL scores were 33.52 for GEN
and 32.86 for CL (33.14 overall). As expected,
the performance on associative anaphora is worse
than on coreferent anaphora. We have not made
any changes to the resolver, so it is interesting to
see that the resolver is indeed able to adjust to
the new notion of reference and manages to link
associative references.
We found that the resolver generally identifies
very few associative references and so the most
common error of the system is that it fails to
recognise associative relations, particularly if
the computed edit distance, one of the standard
features in the coreference resolver, is very high.
The easiest associative relations to detect are
those which have similar surface forms. For
example, the coreference resolver correctly links
RNAI and RNAI genes, the sense strand and
the anti-sense strand or siRNAs and efficacious
siRNAs. However, for most of the associative
references, the lack of easily identifiable surface
markers makes the task difficult. Ironically, the
system also falsely classifies many coreference
links as associative, although it has this time of
course been trained only on associative references.
This is not surprising, given that the tasks are
so similar that we are able to use a coreference
resolver for the associative task in the first place.
IMS coreference resolver using semantic fea-
tures. Table 9 gives the results of the extended
feature set that includes the semantic features de-
scribed in Section 5.2. Each of the respective se-
mantic features shown in the table is added to the
version in the line above the current version.
It can be seen that the different WordNet re-
lations have different effects on the two scien-
tific disciplines. For the genetics papers, the in-
clusion of synonyms, hyponyms and hypernyms
results in the highest increase in performance
(+2.02). For the computational linguistics pa-
pers, the inclusion of synonyms, hyponyms, top-
ics and meronyms obtains the best performance
(+1.19). As the effect of the features is discipline-
dependent, we create two separate final feature
sets for the two disciplines. The GEN version con-
tains synonyms, hyponyms and hypernyms while
the CL version contains synonyms, hyponyms,
topics and meronyms. The highest increase in per-
formance for the CL feature set (and the one re-
sulting in the final system) was achieved by drop-
ping topic members and hypernyms. In the final
CL system, the increase in performance compared
to the baseline version is +1.35. Both final ver-
sions significantly outperform the baseline.
When comparing the output of the extended
system to the baseline system, it can be seen that
52
GEN CL GEN+CL
Baseline 33.52 32.86 33.19
+ Synonyms 33.95 32.87 33.41
+ Hyponyms 34.04 32.94 33.49
+ Hypernyms 35.54 31.35 33.45
+ Topic members 34.61 30.61 32.61
+ Topics 34.09 32.88 33.49
+ Meronyms 33.70 34.05 33.88
+ Substance meronyms 33.57 32.40 32.99
Final version 35.54 34.21 34.88
(domain-dependent)
Table 9: Resolving associative references:
CoNLL metric scores for the extended feature sets
the resolver now links many more mentions (5.7
times more in the GEN papers, 3.8 times more
in the CL papers). The reason why this does not
lead to an even larger increase in performance lies
in the large number of false positives. However,
when looking at the data it becomes apparent that
the newly created links are mostly links that poten-
tially could have been annotated during the anno-
tation, but are not part of the gold standard because
the associative antecedent is not absolutely neces-
sary in order to interpret the anaphor or because
the entity has been linked to a different entity
where the associative relation is stronger. The ex-
istence of more-or-less acceptable alternative as-
sociative links casts some doubt on using a gold
standard as the sole evaluation criterion. An alter-
native would be to ask humans for a rating of the
sensibility of the links determined by the system.
7 Conclusion and Future Work
We have presented a study of information status
in two scientific disciplines as well as preliminary
experiments on the resolution of both coreferent
and associative anaphora in these disciplines. Our
results show a marked difference in the distribu-
tions of information status categories between sci-
entific and news text. Our corpus of 16 full-text
scholarly papers annotated with information sta-
tus and anaphoric links, which we plan to release
soon, contains over 8000 annotated definite noun
phrases. We demonstrate that the integration of
domain-dependent terminological features, com-
bined with domain-dependent training data, out-
performs the unadjusted IMS system (Bj?orkelund
and Farkas, 2012) by 36.47 % absolute on the
CoNLL metric for the genetics papers and by
20.40 % absolute for the computational linguistics
papers. The effect of domain-dependent training
material was stronger than the integration of ter-
minological features. As far as the resolution of
associative anaphora is concerned, we have shown
that it is generally possible to adapt a corefer-
ence resolver to this task, and we have achieved
an improvement in performance using novel se-
mantic features. We are currently performing
an inter-annotator study with two additional an-
notators, which will also lead to a better under-
standing of the relative difficulty of the categories.
Furthermore, we plan to convert the coreference-
annotated ACL papers by Sch?afer et al. (2012) into
CoNLL format and use them for training the coref-
erence resolver. As we have annotated our corpus
with information status, it might also be interest-
ing to train a classifier on the information status
categories and use its predictions to improve the
performance on anaphora resolution tasks. To do
so, we will create a separate corpus for testing,
annotated solely with coreference and associative
links.
Acknowledgements
We would like to thank Jonas Kuhn, Ulrich Heid
and Arndt Riester for their valuable comments
as well as the anonymous reviewers for their in-
sightful remarks. We gratefully acknowledge fi-
nancial support from the Deutsche Forschungsge-
meinschaft (DFG) in the framework of project B3
and D7 of the SFB 732. This research is par-
tially supported by the Intelligence Advanced Re-
search Projects Activity (IARPA) via Department
of Interior National Business Center (DoI/NBC)
contract number D11PC20153. The U.S. Gov-
ernment is authorized to reproduce and distribute
reprints for Governmental purposes notwithstand-
ing any copyright annotation thereon. Disclaimer:
The views and conclusions contained herein are
those of the authors and should not be interpreted
as necessarily representing the official policies
or endorsements, either expressed or implied, of
IARPA, DoI/NBC, or the U.S. Government.
References
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1?34.
Riza T. Batista-Navarro and Sophia Ananiadou. 2011.
Building a coreference-annotated corpus from the
domain of biochemistry. In Proceedings of BioNLP
2011 Workshop, pages 83?91. Association for Com-
putational Linguistics.
53
Anders Bj?orkelund and Rich?ard Farkas. 2012. Data-
driven multilingual coreference resolution using re-
solver stacking. In Joint Conference on EMNLP and
CoNLL-Shared Task, pages 49?55. Association for
Computational Linguistics.
Chen Chen and Vincent Ng. 2012. Combining the
best of two worlds: A hybrid approach to multilin-
gual coreference resolution. In Joint Conference on
EMNLP and CoNLL - Shared Task, pages 56?63,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
Herbert H. Clark. 1975. Bridging. In Proceedings
of the 1975 workshop on Theoretical issues in nat-
ural language processing, pages 169?174. Associa-
tion for Computational Linguistics.
Kevin B. Cohen, Arrick Lanfranchi, William Cor-
vey, William A. Baumgartner Jr, Christophe Roeder,
Philip V. Ogren, Martha Palmer, and Lawrence
Hunter. 2010. Annotation of all coreference in
biomedical text: Guideline selection and adaptation.
In Proceedings of BioTxtM 2010: 2nd workshop
on building and evaluating resources for biomedical
text mining, pages 37?41.
Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, Seattle, Washington, Oc-
tober. Association for Computational Linguistics.
Yimai Fang and Simone Teufel. 2014. A summariser
based on human memory limitations and lexical
competition. In Proceedings of the EACL. Associ-
ation for Computational Linguistics. (to appear).
Eraldo Fernandes, C??cero dos Santos, and Ruy Milidi?u.
2012. Latent structure perceptron with feature in-
duction for unrestricted coreference resolution. In
Joint Conference on EMNLP and CoNLL - Shared
Task, pages 41?48, Jeju Island, Korea, July. Associ-
ation for Computational Linguistics.
Caroline V. Gasperin. 2009. Statistical anaphora
resolution in biomedical texts. Technical Re-
port UCAM-CL-TR-764, University of Cambridge,
Computer Laboratory, December.
Anita Gojun, Ulrich Heid, Bernd Weissbach, Carola
Loth, and Insa Mingers. 2012. Adapting and evalu-
ating a generic term extraction tool. In Proceedings
of the 8th international conference on Language Re-
sources and Evaluation (LREC), pages 651?656.
Marti A. Hearst. 1994. Multi-paragraph segmentation
of expository text. In Proceedings of the 32nd An-
nual Meeting of the Association for Computational
Linguistics, pages 9?16.
Yufang Hou, Katja Markert, and Michael Strube. 2013.
Global inference for bridging anaphora resolution.
In Proceedings of NAACL-HLT, pages 907?917.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: the 90% solution. In Proceedings of
the human language technology conference of the
NAACL, Companion Volume: Short Papers, pages
57?60. Association for Computational Linguistics.
Dain Kaplan, Ryu Iida, Kikuko Nishina, and Takenobu
Tokunaga. 2012. Slate ? a tool for creating and
maintaining annotated corpora. Journal for Lan-
guage Technology and Computational Linguistics,
pages 89?101.
Walter Kintsch and Teun A. van Dijk. 1978. Toward a
model of text comprehension and production. Psy-
chological Review, 85(5):363?394.
Katja Markert, Yufang Hou, and Michael Strube. 2012.
Collective classification for fine-grained information
status. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics:
Long Papers-Volume 1, pages 795?804. Association
for Computational Linguistics.
Shachar Mirkin, Ido Dagan, and Sebastian Pad?o. 2010.
Assessing the role of discourse references in entail-
ment inference. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, ACL 2010, pages 1209?1219. Association
for Computational Linguistics.
Vincent Ng. 2010. Supervised noun phrase coref-
erence research: The first fifteen years. In Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1396?
1411. Association for Computational Linguistics.
Malvina Nissim, Shipra Dingare, Jean Carletta, and
Mark Steedman. 2004. An annotation scheme for
information status in dialogue. LREC 2004.
Massimo Poesio and Renata Vieira. 1998. A corpus-
based investigation of definite description use. Com-
putational linguistics, 24(2):183?216.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Proceedings
of the Joint Conference on EMNLP and CoNLL:
Shared Task, pages 1?40.
Ellen F. Prince. 1981. Toward a taxonomy of given-
new information. In Radical Pragmatics, pages
223?55. Academic Press.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of EMNLP 2010.
Arndt Riester, David Lorenz, and Nina Seemann.
2010. A recursive annotation scheme for referential
information status. In Proceedings of the Seventh In-
ternational Conference on Language Resources and
Evaluation (LREC), pages 717?722.
54
Ulrich Sch?afer, Christian Spurk, and J?org Steffen.
2012. A fully coreference-annotated corpus of
scholarly papers from the acl anthology. In Proceed-
ings of the 24th International Conference on Com-
putational Linguistics. International Conference on
Computational Linguistics (COLING-2012), De-
cember 10-14, Mumbai, India, pages 1059?1070.
Sidney Siegel and N. John Jr. Castellan. 1988. Non-
parametric Statistics for the Behavioral Sciences.
McGraw-Hill, Berkeley, CA, 2nd edition.
Simone Teufel. 2010. The Structure of Scientific Arti-
cles: Applications to Citation Indexing and Summa-
rization. CSLI Publications.
Rebecca Watson, Judita Preiss, and Ted Briscoe.
2003. The contribution of domain-independent
robust pronominal anaphora resolution to open-
domain question-answering. In Proceedings of the
Symposium on Reference Resolution and its Appli-
cations to Question Answering and Summarization.
Venice, Italy June, pages 23?25.
55
Statistical Metaphor Processing
Ekaterina Shutova?
University of Cambridge
Simone Teufel?
University of Cambridge
Anna Korhonen?
University of Cambridge
Metaphor is highly frequent in language, which makes its computational processing indis-
pensable for real-world NLP applications addressing semantic tasks. Previous approaches to
metaphor modeling rely on task-specific hand-coded knowledge and operate on a limited domain
or a subset of phenomena. We present the first integrated open-domain statistical model of
metaphor processing in unrestricted text. Our method first identifies metaphorical expressions
in running text and then paraphrases them with their literal paraphrases. Such a text-to-text
model of metaphor interpretation is compatible with other NLP applications that can benefit
from metaphor resolution. Our approach is minimally supervised, relies on the state-of-the-art
parsing and lexical acquisition technologies (distributional clustering and selectional preference
induction), and operates with a high accuracy.
1. Introduction
Our production and comprehension of language is a multi-layered computational
process. Humans carry out high-level semantic tasks effortlessly by subconsciously
using a vast inventory of complex linguistic devices, while simultaneously integrating
their background knowledge, to reason about reality. An ideal computational model
of language understanding would also be capable of performing such high-level se-
mantic tasks. With the rapid advances in statistical natural language processing (NLP)
and computational lexical semantics, increasingly complex semantic tasks can now
be addressed. Tasks that have received much attention so far include, for example,
word sense disambiguation (WSD), supervised and unsupervised lexical classification,
selectional preference induction, and semantic role labeling. In this article, we take a
step further and show that state-of-the-art statistical NLP and computational lexical
semantic techniques can be used to successfully model complexmeaning transfers, such
as metaphor.
? Computer Laboratory, William Gates Building, 15 JJ Thomson Avenue, Cambridge CB3 0FD, UK.
E-mail: {Ekaterina.Shutova, Simone.Teufel, Anna.Korhonen}@cl.cam.ac.uk.
Submission received: 28 July 2011; revised submission received: 21 April 2012; accepted for publication:
31 May 2012.
doi:10.1162/COLI a 00124
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 2
Metaphors arise when one concept is viewed in terms of the properties of another.
Humans often use metaphor to describe abstract concepts through reference to more
concrete or physical experiences. Some examples of metaphor include the following.
(1) How can I kill a process? (Martin 1988)
(2) Hillary brushed aside the accusations.
(3) I investedmyself fully in this research.
(4) And then my heart with pleasure fills,
And danceswith the daffodils.
(?I wandered lonely as a cloud,? William Wordsworth, 1804)
Metaphorical expressions may take a great variety of forms, ranging from conventional
metaphors, which we produce and comprehend every day, for example, those in Exam-
ples (1)?(3), to poetic and novel ones, such as Example (4). In metaphorical expressions,
seemingly unrelated features of one concept are attributed to another concept. In Ex-
ample (1), a computational process is viewed as something alive and, therefore, its forced
termination is associated with the act of killing. In Example (2) Hillary is not literally
cleaning the space by sweeping accusations. Instead, the accusations lose their validity
in that situation, in other words Hillary rejects them. The verbs brush aside and reject both
entail the resulting disappearance of their object, which is the shared salient property
that makes it possible for this analogy to be lexically expressed as a metaphor.
Characteristic of all areas of human activity (from poetic to ordinary to scientific)
and thus of all types of discourse, metaphor becomes an important problem for NLP.
As Shutova and Teufel (2010) have shown in an empirical study, the use of conventional
metaphor is ubiquitous in natural language text (according to their data, on average
every third sentence in general-domain text contains a metaphorical expression). This
makes metaphor processing essential for automatic text understanding. For example,
an NLP application which is unaware that a ?leaked report? is a ?disclosed report?
and not, for example, a ?wet report,? would fail further semantic processing of the
piece of discourse in which this phrase appears. A system capable of recognizing and
interpreting metaphorical expressions in unrestricted text would become an invaluable
component of any real-world NLP application that needs to access semantics (e.g., infor-
mation retrieval [IR], machine translation [MT], question answering [QA], information
extraction [IE], and opinion mining).
So far, these applications have not used any metaphor processing techniques and
thus often fail to interpret metaphorical data correctly. Consider an example from MT.
Figure 1 shows metaphor translation from English into Russian by a state-of-the-art
statistical MT system (Google Translate1). For both sentences the MT system produces
literal translations of metaphorical terms in English, rather than their literal interpreta-
tions. This results in otherwise grammatical sentences being semantically infelicitous,
poorly formed, and barely understandable to a native speaker of Russian. The meaning
of stir in Figure 1 (1) and spill in Figure 1 (2) would normally be realized in Russian only
via their literal interpretation in the given context (provoke and tell), as shown under
CORRECT TRANSLATION in Figure 1. A metaphor processing component could help to
1 http://translate.google.com/.
302
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Figure 1
Examples of metaphor translation.
avoid such errors. We conducted a pilot study of the importance of metaphor for MT,
by running an English-to-Russian MT system (Google Translate) on the sentences from
the data set of Shutova (2010) containing single-word verb metaphors. We found that
27 out of 62 sentences (44%) were translated incorrectly due to metaphoricity. Due to
the high frequency of metaphor in text according to corpus studies, such a high level of
error becomes important for MT.
Examples where metaphor understanding is crucial can also be found in opinion
mining, that is, detection of the speaker?s attitude to what is said and to the topic.
Consider the following sentences.
(5) a. Government loosened its strangle-hold on business. (Narayanan 1999)
b. Government deregulated business. (Narayanan 1999)
Both sentences describe the same fact. The use of the metaphor loosened strangle-hold in
Example (5a) suggests that the speaker opposes government control of economy, how-
ever, whereas Example (5b) does not imply this. One can infer the speaker?s negative
attitude via the presence of a negativeword strangle-hold. Ametaphor processing system
would establish the correct meaning of Example (5a) and thus discover the actual fact
towards which the speaker has a negative attitude.
Because metaphor understanding requires resolving non-literal meanings via ana-
logical comparisons, the development of a complete and computationally practical
account of this phenomenon is a challenging and complex task. Despite the impor-
tance of metaphor for NLP systems dealing with semantic interpretation, its automatic
processing has received little attention in contemporary NLP, and is far from being a
solved problem. The majority of computational approaches to metaphor still exploit
ideas articulated two or three decades ago (Wilks 1978; Lakoff and Johnson 1980). They
often rely on task-specific hand-coded knowledge (Martin 1990; Fass 1991; Narayanan
1997, 1999; Barnden and Lee 2002; Feldman and Narayanan 2004; Agerri et al 2007)
and reduce the task to reasoning about a limited domain or a subset of phenomena
(Gedigian et al 2006; Krishnakumaran and Zhu 2007). So far there has been no robust
statistical system operating on unrestricted text. State-of-the-art accurate parsing (Klein
andManning 2003; Briscoe, Carroll, andWatson 2006; Clark and Curran 2007), however,
as well as recent work on computational lexical semantics (Schulte im Walde 2006;
303
Computational Linguistics Volume 39, Number 2
Mitchell and Lapata 2008; Davidov, Reichart, and Rappoport 2009; Erk and McCarthy
2009; Sun and Korhonen 2009; Abend and Rappoport 2010; O? Se?aghdha 2010) open up
many avenues for the creation of such a system. This is the niche the presented work is
intending to fill.
1.1 What Is Metaphor?
Metaphor has traditionally been viewed as an artistic device that lends vividness and
distinction to an author?s style. This view was first challenged by Lakoff and Johnson
(1980), who claimed that it is a productive phenomenon that operates at the level of
mental processes. According to Lakoff and Johnson, metaphor is thus not merely a
property of language (i.e., a linguistic phenomenon), but rather a property of thought
(i.e., a cognitive phenomenon). This view was subsequently adopted and extended
by a multitude of approaches (Grady 1997; Narayanan 1997; Fauconnier and Turner
2002; Feldman 2006; Pinker 2007) and the term conceptual metaphor was coined to
describe it.
The view postulates that metaphor is not limited to similarity-based meaning ex-
tensions of individual words, but rather involves reconceptualization of a whole area
of experience in terms of another. Thus metaphor always involves two concepts or
conceptual domains: the target (also called the topic or tenor in the linguistics literature)
and the source (also called the vehicle). Consider Examples (6) and (7).
(6) He shot down all of my arguments. (Lakoff and Johnson 1980)
(7) He attacked every weak point in my argument. (Lakoff and Johnson 1980)
According to Lakoff and Johnson, a mapping of the concept of argument to that of war is
used in both Examples (6) and (7). The argument, which is the target concept, is viewed
in terms of a battle (or awar), the source concept. The existence of such a link allows us to
talk about arguments using war terminology, thus giving rise to a number of metaphors.
Conceptual metaphor, or source?target domain mapping, is thus a generalization over
a set of individual metaphorical expressions that covers multiple cases in which ways
of reasoning about the source domain systematically correspond to ways of reasoning
about the target.
Conceptual metaphor manifests itself in natural language in the form of linguistic
metaphor (or metaphorical expressions) in a variety of ways. The most common types
of linguistic metaphor are lexical metaphor (i.e., metaphor at the level of a single
word sense, as in the Examples (1)?(4)), multi-word metaphorical expressions (e.g.,
?whether we go on pilgrimagewith Raleigh or put out to seawith Tennyson?), or extended
metaphor, that spans over longer discourse fragments.
Lexical metaphor is by far the most frequent type. In the presence of a certain
conceptual metaphor individual words can be used in entirely novel contexts, which
results in the formation of new meanings. Consider the following example.
(8) How can we build a ?Knowledge economy? if research is handcuffed? (Barque
and Chaumartin 2009)
In this sentence the physical verb handcuff is used with an abstract object research
and its meaning adapts accordingly. Metaphor is a productive phenomenon (i.e., its
304
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
novel examples continue to emerge in language). A large number of metaphorical
expressions, however, become conventionalized (e.g., ?I cannot grasp his way of think-
ing?). Although metaphorical in nature, their meanings are deeply entrenched in every-
day use, and are thus cognitively treated as literal terms. Both novel and conventional
metaphors are important for text processing, hence our work is concerned with both
types. Fixed non-compositional idiomatic expressions (e.g., kick the bucket, rock the boat,
put a damper on), however, are left aside, because the mechanisms of their formation are
no longer productive in modern language and, as such, they are of little interest for the
design of a generalizable computational model of metaphor.
Extended metaphor refers to the use of metaphor at the discourse level. A famous
example of extended metaphor can be found in William Shakespeare?s play As You Like
It, where he first compares the world to a stage and then in the following discourse
describes its inhabitants as players. Extended metaphor often appears in literature
in the form of an allegory or a parable, whereby a whole story from one domain is
metaphorically transferred onto another in order to highlight certain attributes of the
subject or teach a moral lesson.
1.2 Computational Modeling of Metaphor
In this article we focus on lexical metaphor and the computational modeling thereof.
From an NLP viewpoint, not all metaphorical expressions are equally important. A
metaphorical expression is interesting for computational modeling if its metaphorical
sense is significantly distinct from its original literal sense and cannot be interpreted
directly (e.g., by existing word sense disambiguation techniques using a predefined
sense inventory). The identification of highly conventionalized metaphors (e.g., the
verb impress, whose meaning originally stems from printing) are not of interest for NLP
tasks, because their metaphorical senses have long been dominant in language and their
original literal senses may no longer be used. A number of conventionalizedmetaphors,
however, require explicit interpretation in order to be understood by computer (e.g.,
?cast doubt,? ?polish the thesis,? ?catch a disease?), as do all novel metaphors. Thus
we are concerned with both novel and conventional metaphors, but only consider the
cases whereby the literal and metaphorical senses of the word are in clear opposition in
common use in contemporary language.
Automatic processing of metaphor can be divided into two subtasks: metaphor
identification, or recognition (distinguishing between literal and metaphorical language
in text); and metaphor interpretation (identifying the intended literal meaning of a
metaphorical expression). An ideal metaphor processing system should address both
of these tasks and provide useful information to support semantic interpretation in
real-world NLP applications. In order to be directly applicable to other NLP systems,
it should satisfy the following criteria:
 Provide a representation of metaphor interpretation that can be easily
integrated with other NLP systems: This criterion places constraints
on how the metaphor processing task should be defined. The most
universally applicable metaphor interpretation would be in the text-to-text
form. This means that a metaphor processing system would take raw text
as input and provide a more literal text as output, in which metaphors
are interpreted.
305
Computational Linguistics Volume 39, Number 2
 Operate on unrestricted running text: In order to be useful for real-world
NLP the system needs to be capable of processing real-world data. Rather
than only dealing with individual carefully selected clear-cut examples,
the system should be fully implemented and tested on free naturally
occurring text.
 Be open-domain: The system needs to cover all domains, genres, and
topics. Thus it should not rely on any domain-specific information or focus
on individual types of instances (e.g., a hand-chosen limited set of
source-target domain mappings).
 Be unsupervised or minimally supervised: To be easily adaptable to new
domains, the system needs to be unsupervised or minimally supervised.
This means it should not use any task-specific (i.e., metaphor-specific)
hand-coded knowledge. The only acceptable exception might be a
multi-purpose general-domain lexicon that is already in existence and
does not need to be created in a costly manner, although it would be an
advantage if no such resource is required.
 Cover all syntactic constructions: To be robust, the system needs to be
able to deal with metaphors represented by all word classes and syntactic
constructions.
In this article, we address both the metaphor identification and interpretation
tasks, resulting in the first integrated domain-independent corpus-based computational
model of metaphor. The method is designed with the listed criteria in mind. It takes
unrestricted text as input and produces textual output. Metaphor identification and
interpretation modules, based on the algorithms of Shutova, Sun, and Korhonen (2010)
and Shutova (2010), are first evaluated independently, and then combined and evalu-
ated together as an integrated system. All components of the method are in principle
applicable to all part-of-speech classes and syntactic constructions. In the current ex-
periments, however, we tested the system only on single-word metaphors expressed by
a verb. Verbs are frequent in language and central to conceptual metaphor. Cameron
(2003) conducted a corpus study of the use of metaphor in educational discourse for all
parts of speech. She found that verbs account for around 50% of the data, the rest being
shared by nouns, adjectives, adverbs, copula constructions, and multi-word metaphors.
This suggests that verb metaphors provide a reliable testbed for both linguistic and
computational experiments. Restricting the scope to verbs is a methodological step
aimed at testing the main principles of the proposed approach in a well-defined setting.
We would, however, expect the presented methods to scale to other parts of speech
and to a wide range of syntactic constructions, because they rely on techniques from
computational lexical semantics that have been shown to be effective in modeling not
only verb meanings, but also those of nouns and adjectives.
As opposed to previous approaches that modeled metaphorical reasoning starting
from a hand-crafted description and applying it to explain the data, we aim to design
a statistical model that captures regular patterns of metaphoricity in a large corpus and
thus generalizes to unseen examples. Compared to labor-intensive manual efforts, this
approach is more robust and, being nearly unsupervised, cost-effective. In contrast to
previous statistical approaches, which addressed metaphors of a specific topic or did
not consider linguistic metaphor at all (e.g., Mason 2004), the proposed method covers
all metaphors in principle, can be applied to unrestricted text, and can be adapted to
different domains and genres.
306
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Our first experiment is concerned with the identification of metaphorical expres-
sions in unrestricted text. Starting from a small set of metaphorical expressions, the
system learns the analogies involved in their production in aminimally supervised way.
It generalizes over the exemplified analogies by means of verb and noun clustering
(i.e., the identification of groups of similar concepts). This generalization allows it to
recognize previously unseen metaphorical expressions in text. Consider the following
examples:
(9) All of this stirred an uncontrollable excitement in her.
(10) Time and time again he would stare at the ground, hand on hip, and then
swallow his anger and play tennis.
Having once seen the metaphor ?stir excitement? in Example (9) the metaphor identifi-
cation system successfully concludes that ?swallow anger? in Example (10) is also used
metaphorically.
The identified metaphors then need to be interpreted. Ideally, a metaphor interpre-
tation task should be aimed at producing a representation of metaphor understanding
that can be directly embedded into other NLP applications that could benefit frommeta-
phor resolution. We define metaphor interpretation as a paraphrasing task and build
a system that discovers literal meanings of metaphorical expressions in text and pro-
duces their literal paraphrases. For example, for metaphors in Examples (11a) and (12a)
the system produces the paraphrases in Examples (11b) and (12b), respectively.
(11) a. All of this stirred an uncontrollable excitement in her.
b. All of this provoked an uncontrollable excitement in her.
(12) a. a carelessly leaked report
b. a carelessly disclosed report
The paraphrases for metaphorical expressions are acquired in a data-driven manner
from a large corpus. Literal paraphrases are then identified using a selectional prefer-
ence model.
This article first surveys the relevant theoretical and computational work on meta-
phor, then describes the design of the identification and paraphrasingmodules and their
independent evaluation, and concludes with the evaluation of the integrated text-to-text
metaphor processing system. The evaluations were carried out with the aid of human
subjects. In the case of identification, the subjects were asked to judge whether a system-
annotated phrase is a metaphor. In case of paraphrasing, they had to decide whether
the system-produced paraphrase for the metaphorical expression is correct and literal
in the given context. In addition, we created a metaphor paraphrasing gold standard
by asking human subjects (not previously exposed to system output) to produce their
own literal paraphrases for metaphorical verbs. The system paraphrasing was then also
evaluated against this gold standard.
2. Theoretical and Computational Background
2.1 Metaphor and Polysemy
Theorists of metaphor distinguish between two kinds of metaphorical language: novel
(or poetic) metaphors (i.e., those that are imaginative), and conventionalized metaphors
307
Computational Linguistics Volume 39, Number 2
(i.e., those that are used as a part of an ordinary discourse). According to Nunberg
(1987), all metaphors emerge as novel, but over time they become part of general usage
and their rhetorical effect vanishes, resulting in conventionalized metaphors. Following
Orwell (1946), Nunberg calls such metaphors ?dead? and claims that they are not
psychologically distinct from literally used terms. The scheme described by Nunberg
demonstrates how metaphorical associations capture patterns governing polysemy,
namely, the capacity of aword to havemultiplemeanings. Over time some of the aspects
of the target domain are added to the meaning of a term in the source domain, resulting
in a (metaphorical) sense extension of this term. Copestake and Briscoe (1995) discuss
sense extension mainly based on metonymic examples and model the phenomenon
using lexical rules encoding metonymic patterns. They also suggest that similar mecha-
nisms can be used to account for metaphorical processes. According to Copestake and
Briscoe, the conceptual mappings encoded in the sense extension rules would define
the limits to the possible shifts in meaning.
General-domain lexical resources often include information about metaphorical
word senses, although unsystematically and without any accompanying semantic an-
notation. For example, WordNet2 (Fellbaum 1998) contains the comprehension sense
of grasp, defined as ?get the meaning of something,? and the reading sense of skim,
defined as ?read superficially.? A great deal of metaphorical senses are absent from
the current version of WordNet, however. A number of researchers have advocated the
necessity of systematic inclusion and mark-up of metaphorical senses in such general-
domain lexical resources (Alonge and Castelli 2003; Lo?nneker and Eilts 2004) and claim
that this would be beneficial for the computational modeling of metaphor. Metaphor
processing systems could then either use this knowledge or be evaluated against it.
Lo?nneker (2004) mapped the senses from EuroWordNet3 to the Hamburg Metaphor
Database (Lo?nneker 2004; Reining and Lo?nneker-Rodman 2007) containing examples
of metaphorical expressions in German and French. Currently no explicit information
about metaphor is integrated into WordNet for English, however.
Although consistent inclusion in WordNet is in principle possible for conventional
metaphorical senses, it is not viable for novel contextual sense alternations. Because
metaphor is a productive phenomenon, all possible cases of contextual meaning alter-
nations it results in cannot be described via simple sense enumeration (Pustejovsky
1995). Computational metaphor processing therefore cannot be approached using the
standard word sense disambiguation paradigm, whereby the contextual use of a word
is classified according to an existing sense inventory. The metaphor interpretation task
is inherently more complex and requires generation of new and often uncommon
meanings of the metaphorical term based on the context.
2.2 Theoretical Views on Metaphor
The following views on metaphor are prominent in linguistics and philosophy: the
comparison view (e.g., the Structure-Mapping Theory of Gentner [1983]), the interaction
view (Black 1962; Hesse 1966), the selectional restrictions violation view (Wilks 1975,
1978), and conceptual metaphor theory (CMT) (Lakoff and Johnson 1980). All of these
2 http://wordnet.princeton.edu/.
3 EuroWordNet is a multilingual database containing WordNets for several European languages (Dutch,
Italian, Spanish, German, French, Czech, and Estonian). The WordNets are structured in the same way
as the Princeton WordNet for English. URL: http://www.illc.uva.nl/EuroWordNet/.
308
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
approaches share the idea of an interconceptual mapping that underlies the production
of metaphorical expressions. Gentner?s Structure-Mapping Theory postulates that the
ground for metaphor lies in similar properties and relations shared by the two con-
cepts (the target and the source). Tourangeau and Sternberg (1982), however, criticize
this view by noting that ?everything has some feature or category that it shares with
everything else, but we cannot combine just any two things in metaphor? (Tourangeau
and Sternberg 1982, page 226). The interaction view focuses on the surprise and novelty
that metaphor introduces. Its proponents claim that the source concept (or domain) rep-
resents a template for seeing the target concept in an entirely new way. The conceptual
metaphor theory of Lakoff and Johnson (1980) takes this idea much further by stating
that metaphor operates at the level of thought rather than at the level of language, and
that it is based on a set of cognitive mappings between source and target domains. Thus
Lakoff and Johnson put the emphasis on the structural aspect of metaphor, rather than
its decorative function in language that dominated the preceding theories. The selec-
tional restrictions violation view of Wilks (1978) concerns manifestation of metaphor in
language. Wilks suggests that metaphor represents a violation of combinatory norms
in the linguistic context and that metaphorical expressions can be detected via such
violation.
2.2.1 Conceptual Metaphor Theory. Examples (6) and (7) provided a good illustration of
CMT. Lakoff and Johnson explain them via the conceptual metaphor ARGUMENT IS
WAR, which is systematically reflected in language in a variety of expressions.
(13) Your claims are indefensible. (Lakoff and Johnson 1980)
(14) I demolished his argument. (Lakoff and Johnson 1980)
(15) I?ve never won an argument with him. (Lakoff and Johnson 1980)
(16) You disagree? Okay, shoot! (Lakoff and Johnson 1980)
According to CMT, we conceptualize and structure arguments in terms of battle, which
systematically influences the way we talk about arguments within our culture. In
other words, the conceptual structure behind battle (i.e., that one can shoot, demolish,
devise a strategy, win, and so on), is metaphorically transferred onto the domain of
argument.
Manifestations of conceptual metaphor are ubiquitous in language and communi-
cation. Here are a few other examples of common metaphorical mappings.
 TIME IS MONEY (e.g., ?That flat tire costme an hour?)
 IDEAS ARE PHYSICAL OBJECTS (e.g., ?I cannot grasp his way of
thinking?)
 LINGUISTIC EXPRESSIONS ARE CONTAINERS (e.g., ?I would not
be able to put all my feelings intowords?)
 EMOTIONS ARE VEHICLES (e.g., ?[...] she was transportedwith
pleasure?)
 FEELINGS ARE LIQUIDS (e.g., ?[...] all of this stirred an unfathomable
excitement in her?)
309
Computational Linguistics Volume 39, Number 2
 LIFE IS A JOURNEY (e.g., ?He arrived at the end of his life with very little
emotional baggage?)
CMT produced a significant resonance in the fields of philosophy, linguistics, cogni-
tive science, and artificial intelligence, including NLP. It inspired novel research (Martin
1990, 1994; Narayanan 1997, 1999; Barnden and Lee 2002; Feldman andNarayanan 2004;
Mason 2004; Martin 2006; Agerri et al 2007), but was also criticized for the lack of
consistency and empirical verification (Murphy 1996; Shalizi 2003; Pinker 2007). The
sole evidence with which Lakoff and Johnson (1980) supported their theory was a set of
carefully selected examples. Such examples, albeit clearly illustrating the main tenets of
the theory, are not representative. They cannot possibly capture the whole spectrum
of metaphorical expressions, and thus do not provide evidence that the theory can
adequately explain the majority of metaphors in real-world texts. Aiming to verify the
latter, Shutova and Teufel (2010) conducted a corpus-based analysis of conceptual meta-
phor in the data from the British National Corpus (BNC) (Burnard 2007). In their study
three independent participants annotated both linguistic metaphors and the underlying
source?target domain mappings. Their results show that although the annotators reach
some overall agreement on the annotation of interconceptual mappings, they experi-
enced a number of difficulties, one of which was the problem of finding the right level
of abstraction for the source and target domain categories. The difficulties in category
assignment for conceptual metaphor suggest that it is hard to consistently assign explicit
labels to source and target domains, even though the interconceptual associations exist
in some sense and are intuitive to humans.
2.2.2 Selectional Restrictions Violation View. Lakoff and Johnson do not discuss howmeta-
phors can be recognized in linguistic data. To date, the most influential account of this
issue is that ofWilks (1975, 1978). According toWilks, metaphors represent a violation of
selectional restrictions (or preferences) in a given context. Selectional restrictions are the
semantic constraints that a predicate places onto its arguments. Consider the following
example.
(17) a. My aunt always drinks her tea on the terrace.
b. My car drinks gasoline. (Wilks 1978)
The verb drink normally requires a grammatical subject of type ANIMATE and a gram-
matical object of type LIQUID, as in Example (17a). Therefore, drink taking a car as a
subject in (17b) is an anomaly, which, according to Wilks, indicates a metaphorical use
of drink.
Although Wilks?s idea inspired a number of computational experiments on meta-
phor recognition (Fass and Wilks 1983; Fass 1991; Krishnakumaran and Zhu 2007), it
is important to note that in practice this approach has a number of limitations. Firstly,
there are other kinds of non-literalness or anomaly in language that cause a violation of
semantic norm, such as metonymies. Thus the method would overgenerate. Secondly,
there are kinds of metaphor that do not represent a violation of selectional restrictions
(i.e., the approach may also undergenerate). This would happen, for example, when
highly conventionalized metaphorical word senses are more frequent than the original
literal senses. Due to their frequency, selectional preference distributions of such words
in real-world data would be skewed towards the metaphorical senses (e.g., capturemay
select for ideas rather than captives according to the data). As a result, no selectional
preferences violation can be detected in the use of such verbs. Another case where the
310
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
method does not apply is copula constructions, such as ?All the world?s a stage.? And
finally, the method does not take into account the fact that interpretation (of metaphor
as well as other linguistic phenomena) is always context-dependent. For example,
the phrase ?All men are animals? uttered by a biology professor or a feminist would
have entirely different interpretations, the latter clearly metaphorical, but without any
violation of selectional restrictions.
2.3 Computational Approaches to Metaphor
2.3.1 Automatic Metaphor Recognition. One of the first attempts to automatically identify
and interpret metaphorical expressions in text is the approach of Fass (1991). It origi-
nates in the idea of Wilks (1978) and utilizes hand-coded knowledge. Fass developed a
system called met*, which is capable of discriminating between literalness, metonymy,
metaphor, and anomaly. It does this in three stages. First, literalness is distinguished
from non-literalness using selectional preference violation as an indicator. In the case
that non-literalness is detected, the respective phrase is tested for being metonymic
using hand-coded patterns (such as CONTAINER-FOR-CONTENT). If the system fails
to recognize metonymy, it proceeds to search the knowledge base for a relevant analogy
in order to discriminate metaphorical relations from anomalous ones. For example, the
sentence in Example (17b) would be represented in this framework as (car,drink,gasoline),
which does not satisfy the preference (animal,drink,liquid), as car is not a hyponym of
animal. met* then searches its knowledge base for a triple containing a hypernym of
both the actual argument and the desired argument and finds (thing,use,energy source),
which represents the metaphorical interpretation.
Goatly (1997) identifies a set of linguistic cues, namely, lexical patterns indicating
the presence of a metaphorical expression in running text, such as metaphorically speak-
ing, utterly, completely, so to speak, and literally. This approach, however, is likely to find
only a small proportion ofmetaphorical expressions, as the vast majority of them appear
without any signaling context. We conducted a corpus study in order to investigate the
effectiveness of linguistic cues asmetaphor indicators. For each cue suggested by Goatly
(1997), we randomly sampled 50 sentences from the BNC containing it and manually
annotated them for metaphoricity. The results are presented in Table 1. The average
precision (i.e., the proportion of identified expressions that were metaphorical) of the
linguistic cue method according to these data is 0.40, which suggests that the set of
metaphors that this method generates contains a great deal of noise. Thus the cues are
unlikely to be sufficient for metaphor extraction on their own, but together with some
additional filters, they could contribute to a more complex system.
The work of Peters and Peters (2000) concentrates on detecting figurative language
in lexical resources. They mine WordNet (Fellbaum 1998) for examples of systematic
Table 1
Corpus statistics for linguistic cues.
Cue BNC frequency Sample size Metaphors Precision
?metaphorically speaking? 7 7 5 0.71
?literally? 1,936 50 13 0.26
?figurative? 125 50 9 0.18
?utterly? 1,251 50 16 0.32
?completely? 8,339 50 13 0.26
?so to speak? 353 49 35 0.71
311
Computational Linguistics Volume 39, Number 2
polysemy, which allows them to capture metonymic and metaphorical relations. Their
system searches for nodes that are relatively high in the WordNet hierarchy (i.e., are
relatively general) and that share a set of commonword forms among their descendants.
Peters and Peters found that such nodes often happen to be in a metonymic (e.g.,
publisher ? publication) or a metaphorical (e.g., theory ? supporting structure) relation.
The CorMet system (Mason 2004) is the first attempt at discovering source?
target domain mappings automatically. It does this by finding systematic variations in
domain-specific selectional preferences, which are inferred from texts on the Web. For
example, Mason collects texts from the LAB domain and the FINANCE domain, in both
of which pourwould be a characteristic verb. In the LAB domain pour has a strong selec-
tional preference for objects of type liquid, whereas in the FINANCE domain it selects for
money. From this Mason?s system infers the domain mapping FINANCE ? LAB and the
concept mappingMONEY IS LIQUID. He compares the output of his system against the
Master Metaphor List (MML; Lakoff, Espenson, and Schwartz 1991) and reports a per-
formance of 77% in terms of accuracy (i.e., proportion of correctly induced mappings).
Birke and Sarkar (2006) present a sentence clustering approach for non-literal lan-
guage recognition, implemented in the TroFi system (Trope Finder). The idea behind
their system originates from a similarity-based word sense disambiguation method
developed by Karov and Edelman (1998). The latter uses a set of seed sentences anno-
tated with respect to word sense. The system computes similarity between the sentence
containing the word to be disambiguated and all of the seed sentences and selects the
sense corresponding to the annotation in the most similar seed sentences. Birke and
Sarkar adapt this algorithm to perform a two-way classification (literal vs. non-literal),
not aiming to distinguish between specific kinds of tropes. An example for the verb
pour in their database is shown in Figure 2. They attain a performance of 0.54 in terms
of F-measure (van Rijsbergen 1979).
Themethod of Gedigian et al (2006) discriminates between literal andmetaphorical
use. The authors trained a maximum entropy classifier for this purpose. They col-
lected their data using FrameNet (Fillmore, Johnson, and Petruck 2003) and PropBank
(Kingsbury and Palmer 2002) annotations. FrameNet is a lexical resource for English
containing information on words? semantic and syntactic combinatory possibilities, or
valencies, in each of their senses. PropBank is a corpus annotated with verbal propo-
sitions and their arguments. Gedigian et al (2006) extracted the lexical items whose
frames are related to MOTION and CURE from FrameNet, then searched the PropBank
Wall Street Journal corpus (Kingsbury and Palmer 2002) for sentences containing such
lexical items and annotated them with respect to metaphoricity. For example, the verb
run in the sentence ?Texas Air has run into difficulty? was annotated as metaphorical,
and in ?I was doing the laundry and nearly broke my neck running upstairs to see?
as literal. Gedigian et al used PropBank annotation (arguments and their semantic
pour
*nonliteral cluster*
wsj04:7878 N As manufacturers get bigger, they are likely to pour more money into the battle for shelf
space, raising the ante for new players.
wsj25:3283 N Salsa and rap music pour out of the windows.
wsj06:300 U Investors hungering for safety and high yields are pouring record sums into single-
premium, interest-earning annuities.
*literal cluster*
wsj59:3286 L Custom demands that cognac be poured from a freshly opened bottle.
Figure 2
An example of the data of Birke and Sarkar (2006).
312
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
types) as features to train the classifier, and report an accuracy of 95.12%. This result is,
however, only 2.22 percentage points higher than the performance of the naive baseline
assigning majority class to all instances (92.90%). Such high performance of their system
can be explained by the fact that 92.90% of the verbs ofMOTION and CURE in their data
are used metaphorically, thus making the data set unbalanced with respect to target
categories and making the task easier.
Both Birke and Sarkar (2006) and Gedigian et al (2006) focus only on metaphors
expressed by a verb. The approach of Krishnakumaran and Zhu (2007) additionally
covers metaphors expressed by nouns and adjectives. Krishnakumaran and Zhu use
hyponymy relation in WordNet and word bigram counts to predict metaphors at a
sentence level. Given a metaphor in copula constructions, or an IS-A metaphor (e.g., the
famous quote by William Shakespeare ?All the world?s a stage?) they verify if the two
nouns involved are in hyponymy relation inWordNet, otherwise this sentence is tagged
as containing a metaphor. They also treat expressions containing a verb or an adjective
used metaphorically (e.g., ?He planted good ideas in their minds? or ?He has a fertile
imagination?). For those cases, they calculate bigram probabilities of verb?noun and
adjective?noun pairs (including the hyponyms/hypernyms of the noun in question). If
the combination is not observed in the data with sufficient frequency, the system tags
the sentence as metaphorical. This idea is a modification of the selectional preference
view of Wilks, although applied at the bigram level. Alternatively, one could extract
verb?object relations from parsed text. Compared to the latter, Krishnakumaran and
Zhu (2007) lose a great deal of information. The authors evaluated their system on a
set of example sentences compiled from the Master Metaphor List, whereby highly con-
ventionalized metaphors are taken to be negative examples. Thus they do not deal with
literal examples as such. Essentially, the distinction Krishnakumaran and Zhu are mak-
ing is between the senses included inWordNet, even if they are conventional metaphors
(e.g., ?capture an idea?), and those not included in WordNet (e.g., ?planted good ideas?).
2.3.2 Automatic Metaphor Interpretation. One of the first computational accounts of meta-
phor interpretation is that of Martin (1990). In his metaphor interpretation, denotation
and acquisition system (MIDAS), Martin models the hierarchical organization of con-
ventional metaphors. The main assumption underlying this approach is that more spe-
cific conventional metaphors (e.g., COMPUTATIONAL PROCESS viewed as a LIVING
BEING in ?How can I kill a process??) descend from more general ones (e.g., PROCESS
[general, as a sequence of events] is a LIVINGBEING). Given an example of ametaphor-
ical expression, MIDAS searches its database for a corresponding conceptual metaphor
that would explain the anomaly. If it does not find any, it abstracts from the example to
more general concepts and repeats the search. If a suitable general metaphor is found,
it creates a new mapping for its descendant, a more specific metaphor, based on this
example. This is also how novel conceptual metaphors are acquired by the system.
The metaphors are then organized into a resource called MetaBank (Martin 1994). The
knowledge is represented in MetaBank in the form of metaphor maps (Martin 1988)
containing detailed information about source-target concept mappings and empirically
derived examples. MIDAS has been integrated with Unix Consultant, a system that
answers users? questions about Unix. The system first tries to find a literal answer to the
question. If it is not able to, it calls MIDAS, which detects metaphorical expressions via
selectional preference violation and searches its database for a metaphor explaining the
anomaly in the question.
Another cohort of approaches aims to perform inference about entities and events
in the source and target domains for the purpose of metaphor interpretation. These
313
Computational Linguistics Volume 39, Number 2
include the KARMA system (Narayanan 1997, 1999; Feldman and Narayanan 2004)
and the ATT-Meta project (Barnden and Lee 2002; Agerri et al 2007). Within both
systems the authors developed a metaphor-based reasoning framework in accordance
with CMT. The reasoning process relies on manually coded knowledge about the world
and operates mainly in the source domain. The results are then projected onto the target
domain using the conceptual mapping representation. The ATT-Meta project concerns
metaphorical and metonymic description of mental states; and reasoning about mental
states is performed using first order logic. Their system, however, does not take natural
language sentences as input, but hand-coded logical expressions that are representa-
tions of small discourse fragments. KARMA in turn deals with a broad range of abstract
actions and events and takes parsed text as input.
Veale and Hao (2008) derive a ?fluid knowledge representation for metaphor inter-
pretation and generation? called Talking Points. Talking Points is a set of characteristics
of concepts belonging to source and target domains and related facts about the world
which are acquired automatically from WordNet and from the Web. Talking Points are
then organized in Slipnet, a framework that allows for a number of insertions, deletions,
and substitutions in definitions of such characteristics in order to establish a connection
between the target and the source concepts. This work builds on the idea of slippage in
knowledge representation for understanding analogies in abstract domains (Hofstadter
and Mitchell 1994; Hofstadter 1995). The following is an example demonstrating how
slippage operates to explain the metaphorMake-up is a Western burqa.
Make-up =>
? typically worn by women
? expected to be worn by women
?must be worn by women
?must be worn by Muslim women
Burqa <=
By doing insertions and substitutions, the system arrives from the definition ?typi-
cally worn by women? to that of ?must be worn byMuslimwomen.? Thus it establishes
a link between the concepts of make-up and burqa. Veale and Hao, however, did not
evaluate to what extent their system is able to interpret metaphorical expressions in
real-world text.
The next sections of the paper are devoted to our own experiments on metaphor
identification and interpretation.
3. Metaphor Identification Method and Experiments
The first task for metaphor processing within NLP is its identification in text. As dis-
cussed earlier, previous approaches to this problem either utilize hand-coded knowl-
edge (Fass 1991; Krishnakumaran and Zhu 2007) or reduce the task to searching for
metaphors of a specific domain defined a priori (e.g., MOTIONmetaphors) in a specific
type of discourse (e.g., the Wall Street Journal [Gedigian et al 2006]). In contrast, the
search space in our experiments is the entire BNC and the domain of the expressions
identified is unrestricted. In addition, the developed technique does not rely on any
hand-crafted lexical or world knowledge, but rather captures metaphoricity by means
of verb and noun clustering in a data-driven manner.
Themotivation behind the use of clusteringmethods for themetaphor identification
task lies in CMT. The patterns of conceptual metaphor (e.g., FEELINGS ARE LIQUIDS)
314
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
always operate on semantic classes, that is, groups of related concepts, defined by
Lakoff and Johnson as conceptual domains (FEELINGS include love, anger, hatred, etc.;
LIQUIDS include water, tea, petrol, beer, etc.). Thus modeling metaphorical mechanisms
in accordance with CMT would involve capturing such semantic classes automatically.
Previous research on corpus-based lexical semantics has shown that it is possible to
automatically induce semantic word classes from corpus data via clustering of contex-
tual cues (Pereira, Tishby, and Lee 1993; Lin 1998; Schulte im Walde 2006). The current
consensus is that the lexical items showing similar behavior in a large body of text most
likely have related meanings.
The second reason for the use of unsupervised and weakly supervised methods
is suggested by the results of corpus-based studies of conceptual metaphor. The anal-
ysis of conceptual mappings in unrestricted text, conducted by Shutova and Teufel
(2010), although confirming some aspects of CMT, uncovered a number of fundamental
difficulties. One of these is the choice of the level of abstraction and granularity of
categories (i.e., labels for source and target domains). This suggests that it is hard
to define a comprehensive inventory of labels for source and target domains. Thus a
computational model of metaphorical associations should not rely on explicit domain
labels. Unsupervised methods allow us to recover patterns in data without assigning
any explicit labels to concepts, and thus to model interconceptual mappings implicitly.
The method behind our metaphor identification system relies on distributional
clustering. Noun clustering, specifically, is central to the approach. It is traditionally
assumed that noun clusters produced using distributional clustering contain concepts
that are similar to each other. This is true only in part, however. There exist two types
of concepts: concrete, those concepts denoting physical entities or physical experiences
(e.g., chair, apple, house, rain) and abstract, those concepts that do not physically exist
at any particular time or place, but rather exist as a type of thing or as an idea (e.g.,
justice, love, democracy). It is the abstract concepts that tend to be described metaphori-
cally, rather than concrete concepts. Humans use metaphor attempting to gain a better
understanding of an abstract concept by comparing it to their physical experiences. As
a result, abstract concepts expose different distributional behavior in a corpus. This
in turn affects the application of clustering techniques and the obtained clusters for
concrete and abstract concepts would be structured differently. Consider the example in
Figure 3. The figure shows a cluster containing concrete concepts (on the right) that are
various kinds of mechanisms; a cluster containing verbs co-occurring with mechanisms
in the corpus (at the bottom); and a cluster containing abstract concepts (on the left)
that tend to co-occur with these verbs. Such abstract concepts, albeit having quite
distinct meanings (e.g., marriage and democracy), are observed in similar lexico-syntactic
environments. This is due to the fact that they are systematically used metaphorically
with the verbs from the domain of MECHANISM. Hence, they are automatically
assigned to the same cluster. The following examples illustrate this phenomenon in
textual data.
(18) Our relationship is not really working.
(19) Diana and Charles did not succeed in mending their marriage.
(20) The wheels of Stalin?s regime were well oiled and already turning.
Such a structure of the abstract clusters can be explained by the fact that relationships,
marriages, collaborations, and political systems are all cognitively mapped to the same
315
Computational Linguistics Volume 39, Number 2
Figure 3
Cluster of target concepts associated with MECHANISM.
source domain of MECHANISM. In contrast to concrete concepts, such as tea, water,
coffee, beer, drink, liquid, that are clustered together when they have similar meanings,
abstract concepts tend to be clustered together if they are associated with the same
source domain.We define this phenomenon as clustering by association and it becomes
central to the system design. The expectation is that clustering by association would
allow the harvesting of new target domains that are associated with the same source
domain, and thus identify new metaphors.
The metaphor identification system starts from a small set of seed metaphorical
expressions, that is, annotatedmetaphors (such as those in Examples (18) or (19)), which
serve as training data. Note that seed annotation only concerns linguistic metaphors;
metaphorical mappings are not annotated. The system then (1) creates source domains
describing these examples by means of verb clustering (such as the verb cluster in
Figure 3); (2) identifies new target domains associated with the same source domain by
means of noun clustering (see, e.g., ABSTRACT cluster in Figure 3), and (3) establishes a
link between the source and the target clusters based on the seed examples.
Thus the system captures metaphorical associations implicitly. It generalizes over
the associated domains by means of verb and noun clustering. The obtained clusters
then represent source and target concepts between which metaphorical associations
hold. The knowledge of such associations is then used to identify new metaphorical
expressions in a large corpus.
In addition to this, we build a selectional preference?based metaphor filter. This
idea stems from the view of Wilks (1978), but is, however, a modification of it. The
filter assumes that the verbs exhibiting weak selectional preferences, namely, verbs co-
occurring with any argument class in linguistic data (remember, influence, etc.) generally
have no or only weak potential for being a metaphor. It has been previously shown
that it is possible to quantify verb selectional preferences on the basis of corpus data,
using, for example, a measure defined by Resnik (1993). Once the candidate metaphors
are identified in the corpus using clustering methods, those displaying weak selectional
preferences can be filtered out.
Figures 4 and 5 depict the metaphor identification pipeline: first, the identifica-
tion of metaphorical associations and then that of metaphorical expressions in text. In
316
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Figure 4
Learning metaphorical associations by means of verb and noun clustering and using the seed set.
summary, the system (1) starts from a seed set of metaphorical expressions exemplifying
a range of source?target domain mappings; (2) performs noun clustering in order to
harvest various target concepts associated with the same source domain; (3) creates a
source domain verb lexicon by means of verb clustering; (4) searches the corpus for
metaphorical expressions describing the target domain concepts using the verbs from
the source domain lexicon; and (5) filters out the candidates exposing weak selectional
preference strength as non-metaphorical.
Figure 5
Identification of new metaphorical expressions in text.
317
Computational Linguistics Volume 39, Number 2
3.1 Experimental Data
The identification system takes a list of seed phrases as input. Seed phrases contain
manually annotated linguistic metaphors. The system generalizes from these linguistic
metaphors to the respective conceptual metaphors by means of clustering. This gen-
eralization is then used to harvest a large number of new metaphorical expressions in
unseen text. Thus the data needed for the identification experiment consist of a seed set,
data sets of verbs and nouns that are subsequently clustered, and an evaluation corpus.
3.1.1 Metaphor Corpus and Seed Phrases. The data to test the identification module were
extracted from the metaphor corpus created by Shutova and Teufel (2010). Their corpus
is a subset of the BNC (Burnard 2007) and, as such, it provides a suitable platform
for testing the metaphor processing system on real-world general-domain expressions
in contemporary English. Our data set consists of verb?subject and verb?direct object
metaphorical expressions. In order to avoid extra noise, we enforced some additional
selection criteria. All phrases were included unless they fell in one of the following
categories:
 Phrases where the subject or object referent is unknown (e.g., containing
pronouns such as in ?in which they [changes] operated?) or represented
by a named entity (e.g., ?Then Hillary leapt into the conversation?).
These cases were excluded from the data set because their processing
would involve the use of additional modules for coreference resolution
and named entity recognition, which in turn may introduce additional
errors into the system.
 Phrases whose metaphorical meaning is realized solely in passive
constructions (e.g., ?sociologists have been inclined to [..]?). These cases
were excluded because for many such examples it was hard for humans
to produce a literal paraphrase realized in the form of the same syntactic
construction. Thus their paraphrasing was deemed to be an unfairly
hard task for the system.
 Multiword metaphors (e.g., ?whether we go on pilgrimage with Raleigh or
put out to seawith Tennyson?). The current system is designed to identify
and paraphrase single-word, lexical metaphors. In the future the system
needs to be modified to process multiword metaphorical expressions;
this is, however, outside the scope of the current experiments.
The resulting data set consists of 62 phrases that are different single-word metaphors
representing verb?subject and verb?direct object relations, where a verb is used meta-
phorically. The phrases include, for instance, ?stir excitement,? ?reflect enthusiasm,?
?grasp theory,? ?cast doubt,? ?suppressmemory,? ?throw remark? (verb?direct object con-
structions); and ?campaign surged,? ?factor shaped [...],? ?tension mounted,? ?ideology
embraces,? ?example illustrates? (subject?verb constructions). This data set was used as
a seed set in the identification experiments. The phrases in the data set were manually
annotated for grammatical relations.
3.1.2 Verb and Noun Data Sets. The noun data set used for clustering consists of the
2,000 most frequent nouns in the BNC. The 2,000 most frequent nouns cover most
common target categories and their linguistic realizations. BNC represents a suitable
318
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
source for such nouns because the corpus is balanced with respect to genre, style,
and theme.
The verb data set is a subset of VerbNet (Kipper et al 2006). VerbNet is the largest
resource for general-domain verbs organized into semantic classes as proposed by Levin
(1993). The data set includes all the verbs in VerbNet with the exception of highly
infrequent ones. The frequency of the verbs was estimated from the data collected by
Korhonen, Krymolowski, and Briscoe (2006) for the construction of the VALEX lexicon,
which to date is one of the largest automatically created verb resources. The verbs from
VerbNet that appear less than 150 times in this data were excluded. The resulting data
set consists of 1,610 general-domain verbs.
3.1.3 Evaluation Corpus. The evaluation data for metaphor identification was the BNC
parsed by the RASP parser (Briscoe, Carroll, and Watson 2006). We used the gram-
matical relation (GR) output of RASP for the BNC created by Andersen et al (2008).
The system searched the corpus for the source and target domain vocabulary within a
particular grammatical relation (verb?direct object or verb?subject).
3.2 Method
The main components of the method include (1) distributional clustering of verbs
and nouns, (2) search through the parsed corpus, and (3) selectional preference-based
filtering.
3.2.1 Verb and Noun Clustering Method. The metaphor identification system relies on
the clustering method of Sun and Korhonen (2009). They use a rich set of syntactic
and semantic features (GRs, verb subcategorization frames [SCFs], and selectional
preferences) and spectral clustering, a method particularly suitable for the resulting
high dimensional feature space. This algorithm has proved to be effective in previous
verb clustering experiments (Brew and Schulte im Walde 2002) and in other NLP tasks
involving high dimensional data (Chen et al 2006).
Spectral clustering partitions objects relying on their similarity matrix. Given a set
of data points, the similarity matrix records similarities between all pairs of points. The
system of Sun and Korhonen (2009) constructs similarity matrices using the Jensen-
Shannon divergence as a measure. Jensen-Shannon divergence between two feature
vectors wi and wj is defined as follows:
JSD(wi,wj) =
1
2
D(wi||m)+ 12D(wj||m) (1)
where D is the Kullback-Leibler divergence, and m is the average of the wi and wj.
Spectral clustering can be viewed in abstract terms as the partitioning of a graph
G over a set of words W. The weights on the edges of G are the similarities Sij. The
similarity matrix S thus represents the adjacency matrix for G. The clustering problem
is then defined as identifying the optimal partition, or cut, of the graph into clusters,
such that the intra-cluster weights are high and the inter-cluster weights are low. The
system of Sun and Korhonen (2009) uses the MNCut algorithm of Meila and Shi (2001)
for this purpose.
Sun and Korhonen (2009) evaluated their clustering approach on 204 verbs from
17 Levin classes and obtained an F-measure of 80.4, which is the state-of-the-art
319
Computational Linguistics Volume 39, Number 2
performance level. The metaphor identification system uses the method of Sun and
Korhonen to cluster both verbs and nouns (separately), however, significantly extending
its coverage to unrestricted general-domain data and applying the method to a con-
siderably larger data set of 1,610 verbs.
3.2.2 Feature Extraction and Clustering Experiments. For verb clustering, the best perform-
ing features from Sun and Korhonen (2009) were adopted. These include automatically
acquired verb SCFs parameterized by their selectional preferences. These features were
obtained using the SCF acquisition system of Preiss, Briscoe, and Korhonen (2007). The
system tags and parses corpus data using the RASP parser (Briscoe, Carroll, andWatson
2006) and extracts SCFs from the produced grammatical relations using a rule-based
classifier which identifies 168 SCF types for English verbs. It produces a lexical entry
for each verb and SCF combination occurring in corpus data. The selectional preference
classes were obtained by clustering nominal arguments appearing in the subject and
object slots of verbs in the resulting lexicon.
Following previous works on semantic noun classification (Pantel and Lin 2002;
Bergsma, Lin, and Goebel 2008), grammatical relations were used as features for noun
clustering. More specifically, the frequencies of nouns and verb lemmas appearing in
the subject, direct object, and indirect object relations in the RASP-parsed BNC were
included in the feature vectors. For example, the feature vector for bananawould contain
the following entries: {eat-dobj n1, fry-dobj n2, sell-dobj n3,..., eat with-iobj ni,
look at-iobj ni+1,..., rot-subj nk, grow-subj nk+1,...}.
We experimented with different clustering granularities, subjectively examined the
obtained clusters, and determined that the number of clusters set to 200 is the most
suitable setting for both nouns and verbs in our task. This was done by means of qual-
itative analysis of the clusters as representations of source and target domains?that is,
by judging how complete and homogeneous the verb clusters were as lists of potential
source domain vocabulary and howmany new target domains associated with the same
source domain were found correctly in the noun clusters. This analysis was performed
on a randomly selected set of 10 clusters taken from different granularity settings and
none of the seed expressions were used for it. Examples of such clusters are shown in
Figures 6 (nouns) and 7 (verbs), respectively. The noun clusters represent target concepts
associated with the same source concept (some suggested source concepts are given in
Figure 6, although the system only captures those implicitly). The verb clusters contain
lists of source domain vocabulary.
3.2.3 Corpus Search.Once the clusters have been obtained, the system proceeds to search
the corpus for source and target domain terms within verb?object (both direct and
indirect) and verb?subject relations. For each seed expression, a cluster is retrieved for
the verb to form the source concept, and a cluster is retrieved for the noun to form a list
of target concepts. The retrieved verb and noun clusters are then linked, and such links
represent metaphorical associations. The system then classifies grammatical relations in
the corpus as metaphorical if the lexical items in the grammatical relation appear in the
linked source (verb) and target (noun) clusters. This search is performed on the BNC
parsed by RASP. Consider the following example sentence extracted from the BNC (the
BNC text ID is given in brackets, followed by the hypothetical conceptual metaphor):
(21) Few would deny that in the nineteenth century change was greatly accelerated.
(ACA) ? CHANGE IS MOTION
320
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Source: MECHANISM
Target Cluster: consensus relation tradition partnership resistance foundation alliance friendship con-
tact reserve unity link peace bond myth identity hierarchy relationship connection balance marriage
democracy defense faith empire distinction coalition regime division
Source: PHYSICAL OBJECT; LIVING BEING; STRUCTURE
Target Cluster: view conception theory concept ideal belief doctrine logic hypothesis interpretation
proposition thesis assumption idea argument ideology conclusion principle notion philosophy
Source: STORY; JOURNEY
Target Cluster: politics practice trading reading occupation profession sport pursuit affair career think-
ing life
Source: LIQUID
Target Cluster: disappointment rage concern desire hostility excitement anxiety passion doubt panic
delight anger fear curiosity shock terror surprise pride happiness pain enthusiasm alarm hope memory
love satisfaction sympathy spirit frustration impulse instinct warmth beauty ambition thought guilt
emotion sensation horror feeling laughter suspicion pleasure
Source: LIVING BEING; END
Target Cluster: defeat fall death tragedy loss collapse decline disaster destruction fate
Figure 6
Clustered nouns (the associated source domain labels are suggested by the authors for clarity;
the system does not assign any labels, but models source and target domains implicitly).
Source Cluster: sparkle glow widen flash flare gleam darken narrow flicker shine blaze bulge
Source Cluster: gulp drain stir empty pour sip spill swallow drink pollute seep flow drip purify ooze
pump bubble splash ripple simmer boil tread
Source Cluster: polish clean scrape scrub soak
Source Cluster: kick hurl push fling throw pull drag haul
Source Cluster: rise fall shrink drop double fluctuate dwindle decline plunge decrease soar tumble
surge spiral boom
Source Cluster: initiate inhibit aid halt trace track speed obstruct impede accelerate slow stimulate
hinder block
Source Cluster: work escape fight head ride fly arrive travel come run go slip move
Figure 7
Clustered verbs.
The relevant GRs identified by the parser are presented in Figure 8. The relation between
the verb accelerate and its semantic object change in Example (21) is expressed in the
passive voice and is, therefore, tagged by RASP as an ncsubj GR. Because this GR con-
tains terminology from associated source (MOTION) and target (CHANGE) domains,
it is marked as metaphorical and so is the term accelerate, which belongs to the source
domain of MOTION.
3.2.4 Selectional Preference Strength Filter. In the previous step a set of candidate verb
metaphors and the associated grammatical relations were extracted from the BNC.
These now need to be filtered based on selectional preference strength. To do this, we
Figure 8
Grammatical relations output for metaphorical expressions.
321
Computational Linguistics Volume 39, Number 2
automatically acquire selectional preference distributions for verb?subject and verb?
direct object relations from the RASP-parsed BNC. The noun clusters obtained using
Sun and Korhonen?s method as described earlier form the selectional preference classes.
To quantify selectional preferences, we adopt the selectional preference strength (SPS)
measure of Resnik (1993). Resnik models selectional preferences of a verb in proba-
bilistic terms as the difference between the posterior distribution of noun classes in a
particular relation with the verb and their prior distribution in that syntactic position
irrespective of the identity of the verb. He quantifies this difference using the Kullback-
Leibler divergence and defines selectional preference strength as follows:
SR(v) = D(P(c|v)||P(c)) =
?
c
P(c|v) log
P(c|v)
P(c) (2)
where P(c) is the prior probability of the noun class, P(c|v) is the posterior probability
of the noun class given the verb, and R is the grammatical relation in question. In order
to quantify how well a particular argument class fits the verb, Resnik defines another
measure called selectional association:
AR(v, c) =
1
SR(v)
P(c|v) log
P(c|v)
P(c)
(3)
which stands for the contribution of a particular argument class to the overall selectional
preference strength of a verb.
The probabilities P(c|v) and P(c) were estimated from the corpus data as follows:
P(c|v) =
f (v, c)
?
k f (v, ck)
(4)
P(c) =
f (c)
?
k f (ck)
(5)
where f (v, c) is the number of times the predicate v co-occurs with the argument class c
in the relation R, and f (c) is the number of times the argument class occurs in the relation
R regardless of the identity of the predicate.
Thus for each verb, its SPS can be calculated for specific grammatical relations.
This measure was used to filter out the verbs with weak selectional preferences. The
expectation is that such verbs are unlikely to be used metaphorically. The optimal
selectional preference strength threshold was set experimentally for both verb?subject
and verb?object relations on a small held-out data set (via qualitative analysis of the
data). It approximates to 1.32. The system excludes expressions containing the verbs
with preference strength below this threshold from the set of candidate metaphors.
Examples of verbs with weak and strong direct object SPs are shown in Tables 2 and
3, respectively. Given the SPS threshold of 1.32, the filter discards 31% of candidate
expressions initially identified in the corpus.
3.3 Evaluation
In order to show that the described metaphor identification method generalizes well
over the seed set and that it operates beyond synonymy, its output was compared to
322
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Table 2
Verbs with weak direct object SPs.
SPS Verb
1.3175 undo
1.3160 bud
1.3143 deplore
1.3138 seal
1.3131 slide
1.3126 omit
1.3118 reject
1.3097 augment
1.3094 frustrate
1.3087 restrict
1.3082 employ
1.3081 highlight
1.3081 correspond
1.3056 dab
1.3053 assist
1.3043 neglect
...
Table 3
Verbs with strong direct object SPs.
SPS Verb SPS Verb
...
3.0810 aggravate 2.9434 coop
3.0692 dispose 2.9326 hobble
3.0536 rim 2.9285 paper
3.0504 deteriorate 2.9212 sip
3.0372 mourn ...
3.0365 tread 1.7889 schedule
3.0348 cadge 1.7867 cheat
3.0254 intersperse 1.7860 update
3.0225 activate 1.7840 belt
3.0085 predominate 1.7835 roar
3.0033 lope 1.7824 intensify
2.9957 bone 1.7811 read
2.9955 pummel 1.7805 unnerve
2.9868 disapprove 1.7776 arrive
2.9838 hoover 1.7775 publish
2.9824 beam 1.7775 reason
2.9807 amble 1.7774 bond
2.9760 diversify 1.7770 issue
2.9759 mantle 1.7760 verify
2.9730 pulverize 1.7734 vomit
2.9604 skim 1.7728 impose
2.9539 slam 1.7726 phone
2.9523 archive 1.7723 purify
2.9504 grease ...
323
Computational Linguistics Volume 39, Number 2
that of a baseline using WordNet. In the baseline system, WordNet synsets represent
source and target domains. The quality of metaphor identification for both the system
and the baseline was evaluated in terms of precision with the aid of human judges.
To compare the coverage of the system to that of the baseline in quantitative terms we
assessed how broadly they expand on the seed set. To do this, we estimated the number
of word senses captured by the two systems and the proportion of identified metaphors
that are not synonymous with any of those seen in the seed set, according to WordNet.
This type of evaluation assesses how well clustering methods are suited to identify new
metaphors not directly related to those in the seed set.
3.3.1 Comparison with WordNet Baseline. The baseline system was implemented using
synonymy information from WordNet to expand on the seed set. Source and target
domain vocabularies were thus represented as sets of synonyms of verbs and nouns in
seed expressions. The baseline system then searched the corpus for phrases composed
of lexical items belonging to those vocabularies. For example, given a seed expression
?stir excitement,? the baseline finds phrases such as ?arouse fervour, stimulate agitation,
stir turmoil,? and so forth. It is not able to generalize over the concepts to broad
semantic classes, however?for example, it does not find other FEELINGS such as
rage, fear, anger, pleasure. This, however, is necessary to fully characterize the target
domain. Similarly, in the source domain, the system only has access to direct synonyms
of stir, rather than to other verbs characteristic of the domain of LIQUIDS (pour, flow,
boil, etc.).
To compare the coverage achieved by the system using clustering to that of the
baseline in quantitative terms, we estimated the number of WordNet synsets, that
is, different word senses, in the metaphorical expressions captured by the two sys-
tems. We found that the baseline system covers only 13% of the data identified using
clustering. This is due to the fact that it does not reach beyond the concepts present
in the seed set. In contrast, most metaphors tagged by the clustering method (87%)
are non-synonymous to those in the seed set and some of them are novel. Together,
these metaphors represent a considerably wider range of meanings. Given the seed
metaphors ?stir excitement, throw remark, cast doubt,? the system identifies previously
unseen expressions ?swallow anger, hurl comment, spark enthusiasm,? and so on, as
metaphorical. Tables 4 and 5 show examples of how the system and the baseline expand
on the seed set, respectively. Full sentences containing metaphors annotated by the
system are shown in Figure 9. Twenty-one percent of the expressions identified by the
system do not have their correspondingmetaphorical senses included inWordNet, such
as ?spark enthusiasm?; the remaining 79% are, however, more common conventional
metaphors. Starting with a seed set of only 62 examples, the system expands signif-
icantly on the seed set and identifies a total of 4,456 metaphorical expressions in the
BNC. This suggests that the method has the potential to attain a broad coverage of the
corpus given a large and representative seed set.
3.3.2 Evaluation Against Human Judgments. In order to assess the quality of metaphor
identification by both systems, their output was assessed by human judgments. For
this purpose, we randomly sampled sentences containing metaphorical expressions as
annotated by the system and by the baseline and asked human annotators to decide
whether these were metaphorical or not.
Participants Five volunteers participated in the experiment. They were all native
speakers of English and had no formal training in linguistics.
324
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Table 4
Examples of seed set expansion by the system.
Seed phrase Harvested metaphors BNC frequency
reflect concern (V-O): reflect concern 78
reflect interest 74
reflect commitment 26
reflect preference 22
reflect wish 17
reflect determination 12
reflect intention 8
reflect willingness 4
reflect sympathy 3
reflect loyalty 2
disclose interest 10
disclose intention 3
disclose concern 2
disclose sympathy 1
disclose commitment 1
disguise interest 6
disguise intention 3
disguise determination 2
obscure interest 1
obscure determination 1
cast doubt (V-O): cast doubt 197
cast fear 3
cast suspicion 2
catch feeling 3
catch suspicion 2
catch enthusiasm 1
catch emotion 1
spark fear 10
spark enthusiasm 3
spark passion 1
spark feeling 1
campaign surged (S-V): campaign surged 1
charity boomed 1
effort decreased 1
expedition doubled 1
effort doubled 1
campaign shrank 1
campaign soared 1
drive spiraled 1
Materials The subjects were presented with a set of 78 randomly sampled sentences
annotated by the two systems. Fifty percent of the data set were the sentences annotated
by the identification system and the remaining 50%were annotated by the baseline; and
the sentences were randomized. The annotation was done electronically in Microsoft
Word. An example of annotated sentences is given in Figure 10.
Task and guidelines The subjects were asked to mark which of the expressions were
metaphorical in their judgment. The participants were encouraged to rely on their
own intuition of what a metaphor is in the annotation process. Additional guidance,
325
Computational Linguistics Volume 39, Number 2
Table 5
Examples of seed set expansion by the baseline.
Seed phrase Harvested metaphors BNC frequency
reflect concern (V-O): reflect concern 78
ponder business 1
ponder headache 1
reflect business 4
reflect care 2
reflect fear 19
reflect worry 3
cast doubt (V-O): cast doubt 197
cast question 11
couch question 1
drop question 2
frame question 21
purge doubt 2
put doubt 12
put question 151
range question 1
roll question 1
shed doubt 2
stray question 1
throw doubt 35
throw question 17
throw uncertainty 1
campaign surged (S-V): campaign surged 1
campaign soared 1
however, in the form of the following definition of metaphor (Pragglejaz Group 2007)
was also provided:
1. For each verb establish its meaning in context and try to imagine a more
basic meaning of this verb in other contexts. Basic meanings normally are:
(1) more concrete; (2) related to bodily action; (3) more precise (as opposed
to vague); (4) historically older.
2. If you can establish a basic meaning that is distinct from the meaning of
the verb in this context, the verb is likely to be used metaphorically.
CKM 391 Time and time again he would stare at the ground, hand on hip, if he thought he had received
a bad call, and then swallow his anger and play tennis.
AD9 3205 He tried to disguise the anxiety he felt when he found the comms system down, but Tammuz
was nearly hysterical by this stage.
AMA 349Wewill halt the reduction in NHS services for long-term care and community health services
which support elderly and disabled patients at home.
ADK 634 Catch their interest and spark their enthusiasm so that they begin to see the product?s
potential.
K2W 1771 The committee heard today that gangs regularly hurled abusive comments at local people,
making an unacceptable level of noise and leaving litter behind them.
Figure 9
Sentences tagged by the system (metaphors in bold).
326
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Figure 10
Evaluation of metaphor identification.
Interannotator agreement Reliability was measured at ? = 0.63 (n = 2,N = 78, k = 5).
The data suggest that the main source of disagreement between the annotators was the
presence of conventional metaphors (e.g., verbs such as adopt, convey, decline).
Results The system performance was then evaluated against the elicited judgments in
terms of precision. The system output was compared to the gold standard constructed
by merging the judgments, whereby the expressions tagged as metaphorical by at least
three annotators were considered to be correct. This resulted in P = 0.79, with the
baseline attaining P = 0.44. In addition, the system tagging was compared to that of
each annotator pairwise, yielding an average P = 0.74 for the system and P = 0.41 for
the baseline.
In order to compare system performance to the human ceiling, pairwise agreement
was additionally calculated in terms of precision between the majority gold standard
and each judge. This corresponds to an average of P = 0.94.
To show that the system performance is significantly different from that of the base-
line, we annotated additional 150 instances identified by both systems for correctness
and conducted a one-tailed t-test for independent samples. The difference is statistically
significant with t = 4.11 (df = 148, p < 0.0005).
3.4 Discussion
We have shown that the method leads to a considerable expansion on the seed set and
operates with high precision?namely, it produces high quality annotations, and iden-
tifies fully novel metaphorical expressions relying only on the knowledge of source?
target domain mappings that it learns automatically. By comparing its coverage to that
of a WordNet baseline, we showed that the method reaches beyond synonymy and
generalizes well over the source and target domains.
The observed discrepancy in precision between the clustering approach and the
baseline can be explained by the fact that a large number of metaphorical senses are
included in WordNet. This means that in WordNet synsets source domain verbs appear
together with more abstract terms. For instance, the metaphorical sense of shape in
the phrase ?shape opinion? is part of the synset ?(determine, shape, mold, influence,
regulate).? This results in the low precision of the baseline system, because it tags literal
expressions (e.g., influence opinion) as metaphorical, assuming that all verbs from the
synset belong to the source domain.
327
Computational Linguistics Volume 39, Number 2
To perform a more comprehensive error analysis, we examined a larger subset of
the metaphorical expressions identified by the system (200 sentences, equally covering
verb?subject and verb?object constructions). System precision against the additional
judgments by one of the authors was measured at 76% (48 instances were tagged
incorrectly according to the judgments). The classification of system errors by type is
presented in Table 6. Precision errors in the output of the system were also concentrated
around the problem of conventionality of some metaphorical verbs, such as those in
?hold views, adopt traditions, tackle a problem.? This conventionality is reflected in the
data in that such verbs are frequently used in their ?metaphorical? contexts. As a result,
they are clustered together with literally used terms. For instance, the verb tackle is
found in a cluster with solve, resolve, handle, confront, face, and so forth. This results in
the system tagging ?resolve a problem? as metaphorical if it has previously seen ?tackle
a problem.?
A number of system errors affecting its precision are also due to cases of general
polysemy and homonymy of both verbs and nouns. For example, the noun passage
can mean both ?the act of passing from one state or place to the next? and ?a section
of text; particularly a section of medium length,? as defined in WordNet. Sun and
Korhonen?s (2009) method performs hard clustering, that is, it does not distinguish
between different word senses. Hence the noun passage occurred in only one cluster,
containing concepts like thought, word, sentence, expression, reference, address, description,
and so on. This cluster models the ?textual? meaning of passage. As a result of sense
ambiguity within the cluster, given the seed phrase ?she blocked the thought,? the system
tags such expressions as ?block passage,? ?impede passage,? ?obstruct passage,? and
?speed passage? as metaphorical.
The errors that may cause low recall of the system are of a different nature. Whereas
noun clustering considerably expands the seed set by identifying new associated tar-
get concepts (e.g., given the seed metaphor ?sell soul? it identifies ?sell skin? and
?launch pulse? as metaphorical), the verb clusters sometimes miss a certain proportion
of source domain vocabulary. For instance, given the seed metaphor ?example illus-
trates,? the system identifies the following expressions: ?history illustrates,? ?episode
illustrates,? ?tale illustrates,? ?combination illustrates,? ?event illustrates,? and so forth. It
does not, however, capture obvious verb-based expansions, such as ?episode portrays,?
present in the BNC. This is one of the problems that could lead to a lower recall
of the system.
Nevertheless, in many cases the system benefits not only from dissimilar concepts
within the noun clusters used to detect new target domains, but also from dissim-
ilar concepts in the verb clusters. Verb clusters produced automatically relying on
Table 6
Common system errors by type.
Source of error Subject?Verb Verb?Object Totals
Metaphor conventionality 7 14 21
General polysemy 9 6 15
Verb clustering 4 5 9
Noun clustering 2 1 3
SP filter 0 0 0
Totals 22 26 48
328
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
contextual features may contain lexical items with distinct, or even opposite meanings
(e.g., throw and catch, take off and land). They tend to belong to the same semantic
domain, however (e.g., verbs of dealing with LIQUIDS, verbs describing a FIGHT) It is
the diversity of verb meanings within the domain cluster that allows the generalization
from a limited number of seed expressions to a broader spectrum of previously unseen
and novel metaphors, non-synonymous with those in the seed set.
The fact that the approach is seed-dependent is one of its possible limitations,
affecting the coverage of the system. Wide coverage is essential for the practical use
of the system. At this stage, however, it was impossible for us to reliably measure the
recall of the system, because there is no large corpus annotated for metaphor available.
In addition, because the current system was only tested with very few seeds (again,
due to the lack of metaphor-annotated data), we expect the current overall recall of the
system to be relatively low. In order to obtain a full coverage of the corpus, a large and
representative seed set is necessary. Although it is hard to capture the whole variety of
metaphorical language in a limited set of examples, it is possible to compile a seed set
representative of all common source?target domain mappings. The learning capabilities
of the system can then be used to expand on those to the whole range of conventional
and novel metaphorical mappings and expressions. In addition, because the precision
of the system was measured on the data set produced by expanding individual seed
expressions, we would expect the expansion of other, new seed expressions to yield a
comparable quality of annotations. Incorporating new seed expressions is thus likely to
result in increasing recall without a significant loss in precision.
The current system harvests a large and relatively clean set of metaphorical
expressions from the corpus. These annotations could provide a new platform for the
development and testing of other metaphor systems.
4. Metaphor Interpretation Method and Experiments
As is the case in metaphor identification, the majority of existing approaches to meta-
phor interpretation also rely on task-specific hand-coded knowledge (Martin 1990; Fass
1991; Narayanan 1997, 1999; Barnden and Lee 2002; Feldman and Narayanan 2004;
Agerri et al 2007) and produce interpretations in a non-textual format (Veale and Hao
2008). The ultimate objective of automatic metaphor processing, however, is a type
of interpretation that can be directly embedded into other systems to enhance their
performance. We thus define metaphor interpretation as a paraphrasing task and build
a system that automatically derives literal paraphrases for metaphorical expressions in
unrestricted text. Our method is also distinguished from previous work in that it does
not rely on any hand-crafted knowledge aboutmetaphor, but in contrast is corpus-based
and uses automatically induced selectional preferences.
The metaphor paraphrasing task can be divided into two subtasks: (1) generating
paraphrases, that is, other ways of expressing the same meaning in a given context,
and (2) discriminating between literal and metaphorical paraphrases. Consequently,
the proposed approach is theoretically grounded in two ideas underlying each of these
subtasks:
 The meaning of a word in context emerges through interaction with the
meaning of the words surrounding it. This assumption is widely accepted
in lexical semantics theory (Pustejovsky 1995; Hanks and Pustejovsky
2005) and has been exploited for lexical acquisition (Schulte im Walde
2006; Sun and Korhonen 2009). It suggests that the context itself imposes
329
Computational Linguistics Volume 39, Number 2
certain semantic restrictions on the words which can occur within it.
Given a large amount of linguistic data, it is possible to model these
semantic restrictions in probabilistic terms (Lapata 2001). This can be
done by deriving a ranking scheme for possible paraphrases that fit or
do not fit in a specific context based on word co-occurrence evidence.
This is how initial paraphrases are generated within the metaphor
paraphrasing module.
 Literalness can be detected via strong selectional preference. This idea
is a mirror-image of the selectional preference violation view of Wilks
(1978), who suggested that a violation of selectional preferences indicates
a metaphor. The key information that selectional preferences provide is
whether there is an association between the predicate and its potential
argument and how strong that association is. A literal paraphrase
would normally come from the target domain (e.g., ?understand the
explanation?) and be strongly associated with the target concept, whereas
a metaphorical paraphrase would belong to the source domain (e.g.,
?grasp the explanation?) and be associated with the concepts from this
source domain more strongly than with the target concept. Hence we
use a selectional preference model to measure the semantic fit of the
generated paraphrases into the given context as opposed to all other
contexts. The highest semantic fit then indicates the most literal
paraphrase.
Thus the context-based probabilistic model is used for paraphrase generation and
the selectional preference model for literalness detection. The key difference between
the two models is that the former favors the paraphrases that co-occur with the words
in the context more frequently than other paraphrases do, and the latter favors the
paraphrases that co-occur with the words from the context more frequently than with
any other lexical items in the corpus. This is the main intuition behind our approach.
The system thus incorporates the following components:
 a context-based probabilistic model that acquires paraphrases for
metaphorical expressions from a large corpus;
 a WordNet similarity component that filters out the irrelevant
paraphrases based on their similarity to the metaphorical term (similarity
is defined as sharing a common hypernym within three levels in the
WordNet hierarchy);
 a selectional preference model that discriminates literal paraphrases from
the metaphorical ones. It re-ranks the paraphrases, de-emphasizing the
metaphorical ones and emphasizing the literal ones.
In addition, the system disambiguates the sense of the paraphrases using the
WordNet inventory of senses. The context-based model together with the WordNet
filter constitute a metaphor paraphrasing baseline. By comparing the final system to
this baseline, we demonstrate that simple context-based substitution, even supplied by
extensive knowledge contained in lexical resources, is not sufficient for metaphor inter-
pretation and that a selectional preference model is needed to establish the literalness of
the paraphrases.
330
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
This section first provides an overview of paraphrasing and lexical substitution
and relates these tasks to the problem of metaphor interpretation. It then describes
the experimental data used to develop and test the paraphrasing system and the
method itself, and finally, concludes with the system evaluation and the presentation
of results.
4.1 Paraphrasing and Lexical Substitution
Paraphrasing can be viewed as a text-to-text generation problem, whereby a new piece
of text is produced conveying the same meaning as the original text. Paraphrasing can
be carried out at multiple levels (sentence-, phrase-, and word-levels), and may involve
both syntactic and lexical transformations. Paraphrasing by replacing individual words
in a sentence is known as lexical substitution (McCarthy 2002). Because, in this article,
we address the phenomenon of metaphor at a single-word level, our task is close in
nature to lexical substitution. The task of lexical substitution originates from word
sense disambiguation (WSD). The key difference between the two is that whereas WSD
makes use of a predefined sense-inventory to characterize the meaning of a word in
context, lexical substitution is aimed at automatic induction of meanings. Thus the goal
of lexical substitution is to generate the set of semantically valid substitutes for the
word. Consider the following sentences from Preiss, Coonce, and Baker (2009).
(22) His parents felt that he was a bright boy.
(23) Our sun is a bright star.
Bright in Example (22) can be replaced by the word intelligent. The same replacement
in the context of Example (23) will not produce an appropriate sentence. A lexical
substitution system needs to (1) find a set of candidate synonyms for the word and
(2) select the candidate that matches the context of the word best.
Both sentence- or phrase-level paraphrasing and lexical substitution find a wide
range of applications in NLP. These include summarization (Knight and Marcu 2000;
Zhou et al 2006), information extraction (Shinyama and Sekine 2003), machine trans-
lation (Kurohashi 2001; Callison-Burch, Koehn, and Osborne 2006), text simplification
(Carroll et al 1999), question answering (McKeown 1979; Lin and Pantel 2001) and
textual entailment (Sekine et al 2007). Consequently, there has been a plethora of
NLP approaches to paraphrasing (McKeown 1979; Meteer and Shaked 1988; Dras 1999;
Barzilay and McKeown 2001; Lin and Pantel 2001; Barzilay and Lee 2003; Bolshakov
and Gelbukh 2004; Quirk, Brockett, and Dolan 2004; Kauchak and Barzilay 2006; Zhao
et al 2009; Kok and Brockett 2010) and lexical substitution (McCarthy and Navigli 2007,
2009; Erk and Pado? 2009; Preiss, Coonce, and Baker 2009; Toral 2009; McCarthy, Keller,
and Navigli 2010).
Among paraphrasing methods one can distinguish (1) rule-based approaches,
which rely on a set of hand-crafted (McKeown 1979; Zong, Zhang, and Yamamoto 2001)
or automatically learned (Lin and Pantel 2001; Barzilay and Lee 2003; Zhao et al 2008)
paraphrasing patterns; (2) thesaurus-based approaches, which generate paraphrases
by substituting words in the sentence by their synonyms (Bolshakov and Gelbukh
2004; Kauchak and Barzilay 2006); (3) natural language generation?based approaches
(Kozlowski, McCoy, and Vijay-Shanker 2003; Power and Scott 2005), which transform
a sentence into its semantic representation and generate a new sentence from it; and
(4) SMT-based methods (Quirk, Brockett, and Dolan 2004), operating as monolingual
331
Computational Linguistics Volume 39, Number 2
MT. A number of approaches to lexical substitution rely on manually constructed
thesauri to find sets of candidate synonyms (McCarthy and Navigli 2007), whereas
others address the task in a fully unsupervised fashion. In order to derive and rank
candidate substitutes, the latter systems make use of distributional similarity measures
(Pucci et al 2009; McCarthy, Keller, and Navigli 2010), vector space models of word
meaning (De Cao and Basili 2009; Erk and Pado? 2009) or statistical learning techniques,
such as hidden Markov models and n-grams (Preiss, Coonce, and Baker 2009).
The metaphor interpretation task is different from the WSD task, because it is
impossible to predefine a set of senses of metaphorical words, in particular for novel
metaphors. Instead, the correct substitute for the metaphorical term needs to be gen-
erated in a data-driven manner, as for lexical substitution. The metaphor paraphrasing
task, however, also differs from lexical substitution in the following two ways. Firstly,
a suitable substitute needs to be used literally in the target context, or at least more
conventionally than the original word. Secondly, by definition, the substitution is not
required to be a synonym of the metaphorical word. Moreover, for our task this is not
even desired, because there is the danger that synonymous paraphrasing may result
in another metaphorical expression, rather than the literal interpretation of the original
one. Metaphor paraphrasing therefore presents an additional challenge in comparison
to lexical substitution, namely, that of discriminating between literal and metaphorical
substitutes. This second, harder, and not previously addressed task is the main focus
of the work presented in this section. The remainder of the section is devoted to the
description of the metaphor paraphrasing experiment.
4.2 Experimental Data
The paraphrasing system is first tested individually on a set of metaphorical expres-
sions extracted from a manually annotated metaphor corpus of Shutova and Teufel
(2010). This is the same data set as the one used for seeding the identification module
(see Section 3.1.1 for description). Because the paraphrasing evaluation described in
this section is conducted independently from the identification experiment, and no
part of the paraphrasing system relies on the output of the identification system and
vice versa, the use of the same data set does not give any unfair advantage to the
systems. In the later experiment (Section 5) when the identification and paraphrasing
system are evaluated jointly, again the same seed set will be used for identification;
paraphrasing, however, will be performed on the output of the identification system
(i.e., the new identified metaphors) and both the identified metaphors and their para-
phrases will be evaluated by human judges not used in the previous and the current
experiments.
4.3 Method
The system takes phrases containing annotated single-word metaphors as input; where
a verb is used metaphorically, its context is used literally. It generates a list of possible
paraphrases of the verb that can occur in the same context and ranks them according
to their likelihood, as derived from the corpus. It then identifies shared features of the
paraphrases and themetaphorical verb using theWordNet hierarchy and removes unre-
lated concepts. It then identifies the literal paraphrases among the remaining candidates
based on the verb?s automatically induced selectional preferences and the properties of
the context.
332
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
4.3.1 Context-based Paraphrase Ranking Model. Terms replacing the metaphorical verb v
will be called its interpretations i. We model the likelihood L of a particular paraphrase
as a joint probability of the following events: the interpretation i co-occurring with the
other lexical items from its context w1, ...,wN in syntactic relations r1, ..., rN, respectively.
Li = P(i, (w1, r1), (w2, r2), ..., (wN, rN )) (6)
where w1, ...,wN and r1, ..., rN represent the fixed context of the term used metaphori-
cally in the sentence. In the system output, the context w1, ...,wN will be preserved, and
the verb v will be replaced by the interpretation i.
We assume statistical independence between the relations of the terms in a phrase.
For instance, for a verb that stands in a relation with both a subject and an object, the
verb?subject and verb?direct object relations are considered to be independent events
within the model. The likelihood of an interpretation is then calculated as follows:
P(i, (w1, r1), (w2, r2), ..., (wN, rN )) = P(i) ? P((w1, r1)|i) ? ... ? P((wN, rN )|i) (7)
The probabilities can be calculated using maximum likelihood estimation
P(i) =
f (i)
?
k f (ik)
(8)
P(wn, rn|i) =
f (wn, rn, i)
f (i)
(9)
where f (i) is the frequency of the interpretation irrespective of its arguments,
?
k f (ik) is
the number of times its part of speech class is attested in the corpus, and f (wn, rn, i) is
the number of times the interpretation co-occurs with context word wn in relation rn. By
performing appropriate substitutions into Equation (7) one obtains
P(i, (w1, r1), (w2, r2), ..., (wN, rN )) =
f (i)
?
k f (ik)
?
f (w1, r1, i)
f (i)
? ... ?
f (wN, rN, i)
f (i)
=
?N
n=1 f (wn, rn, i)
( f (i))N?1 ?
?
k f (ik)
(10)
This model is then used to rank the possible replacements of the term used meta-
phorically in the fixed context according to the data. The parameters of the model were
estimated from the RASP-parsed BNC using the grammatical relations output created
by Andersen et al (2008).
4.3.2 WordNet Filter. The context-based model described in Section 4.3.1 overgenerates
and hence there is a need to further narrow down the results. It is acknowledged in the
linguistics community that metaphor is, to a great extent, based on similarity between
the concepts involved (Gentner et al 2001). We exploit this fact to refine paraphrasing.
After obtaining the initial list of possible substitutes for the metaphorical term, the
system filters out the terms whose meanings do not share any common properties
with that of the metaphorical term. Consider the computer science metaphor ?kill a
process,? which stands for ?terminate a process.? The basic sense of kill implies an end
333
Computational Linguistics Volume 39, Number 2
Table 7
The list of paraphrases with the initial ranking (correct paraphrases are underlined).
Log-likelihood Replacement
Verb?DirectObject
hold back truth:
?13.09 contain
?14.15 conceal
?14.62 suppress
?15.13 hold
?16.23 keep
?16.24 defend
stir excitement:
?14.28 create
?14.84 provoke
?15.53 make
?15.53 elicit
?15.53 arouse
?16.23 stimulate
?16.23 raise
?16.23 excite
?16.23 conjure
leak report:
?11.78 reveal
?12.59 issue
?13.18 disclose
?13.28 emerge
?14.84 expose
?16.23 discover
Subject?Verb
campaign surge:
?13.01 run
?15.53 improve
?16.23 soar
?16.23 lift
or termination of life. Thus termination is the shared element of the metaphorical verb
and its literal interpretation.
Such an overlap of properties can be identified using the hyponymy relations in the
WordNet taxonomy. Within the initial list of paraphrases, the system selects the terms
that are hypernyms of the metaphorical term, or share a common hypernym with it. To
maximize the accuracy, we restrict the hypernym search to a depth of three levels in the
taxomomy. Table 7 shows the filtered lists of paraphrases for some of the test phrases,
together with their log-likelihood. Selecting the highest ranked paraphrase from this list
as a literal interpretation will serve as a baseline.
4.3.3 Re-ranking Based on Selectional Preferences. The lists which were generated contain
some irrelevant paraphrases (e.g., ?contain the truth? for ?hold back the truth?) and
some paraphrases where the substitute itself is metaphorically used (e.g., ?suppress the
334
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
truth?). As the task is to identify the literal interpretation, however, the system should
remove these.
One way of dealing with both problems simultaneously is to use selectional prefer-
ences of the verbs. Verbs used metaphorically are likely to demonstrate semantic pref-
erence for the source domain, e.g., suppress would select for MOVEMENTS (political)
rather than IDEAS, or TRUTH (the target domain), whereas the ones used literally for
the target domain (e.g., conceal) would select for TRUTH. Selecting the verbs whose
preferences the noun in the metaphorical expression matches best should allow filtering
out non-literalness, as well as unrelated terms.
We automatically acquired selectional preference distributions of the verbs in the
paraphrase lists (for verb?subject and verb?direct object relations) from the RASP-
parsed BNC. As in the identification experiment, we derived selectional preference
classes by clustering the 2,000 most frequent nouns in the BNC into 200 clusters us-
ing Sun and Korhonen?s (2009) algorithm. In order to quantify how well a particular
argument class fits the verb, we adopted the selectional association measure proposed
by Resnik (1993), identical to the one we used within the selectional preference-based
filter for metaphor identification, as described in Section 3.2.4. To remind the reader,
selectional association is defined as follows:
AR(v, c) =
1
SR(v)
P(c|v) log
P(c|v)
P(c)
(11)
where P(c) is the prior probability of the noun class, P(c|v) is the posterior probability
of the noun class given the verb, and SR is the overall selectional preference strength of
the verb in the grammatical relation R.
We use selectional association as a measure of semantic fitness (i.e., literalness) of
the paraphrases. The paraphrases are re-ranked based on their selectional association
with the noun in the context. Those paraphrases that are not well suited or used meta-
phorically are dispreferred within this ranking. The new ranking is shown in Table 8.
The expectation is that the paraphrase in the first rank (i.e., the verb with which the
noun in the context has the highest association) represents a literal interpretation.
4.4 Evaluation and Discussion
As in the case of identification, the paraphrasing system was tested on verb?subject and
verb?direct object metaphorical expressions. These were extracted from the manually
annotated metaphor corpus of Shutova and Teufel (2010), as described in Section 3.1.1.
We compared the output of the final selectional-preference based system to that of the
WordNet filter acting as a baseline. We evaluated the quality of paraphrasing with the
help of human judges in two different experimental settings. The first setting involved
direct judgments of system output by humans. In the second setting, the subjects did
not have access to system output and had to provide their own literal paraphrases for
the metaphorical expressions in the data set. The system was then evaluated against
human judgments in Setting 1 and a paraphrasing gold standard created by merging
annotations in Setting 2.
4.4.1 Setting 1: Direct Judgment of System Output. The subjects were presented with a
set of sentences containing metaphorical expressions and the top-ranked paraphrases
produced by the system and by the baseline, randomized. They were asked to mark as
335
Computational Linguistics Volume 39, Number 2
Table 8
Paraphrases re-ranked by SP model (correct paraphrases are underlined).
Association Replacement
Verb?DirectObject
hold back truth:
0.1161 conceal
0.0214 keep
0.0070 suppress
0.0022 contain
0.0018 defend
0.0006 hold
stir excitement:
0.0696 provoke
0.0245 elicit
0.0194 arouse
0.0061 conjure
0.0028 create
0.0001 stimulate
? 0 raise
? 0 make
? 0 excite
leak report:
0.1492 disclose
0.1463 discover
0.0674 reveal
0.0597 issue
? 0 emerge
? 0 expose
Subject?Verb
campaign surge:
0.0086 improve
0.0009 run
? 0 soar
? 0 lift
correct the paraphrases that have the same meaning as the term used metaphorically if
they are used literally in the given context.
Subjects Seven volunteers participated in the experiment. They were all native
speakers of English (one bilingual) and had little or no linguistics expertise.
Interannotator agreement The reliability was measured at ? = 0.62 (n = 2,
N = 95, k = 7).
System evaluation against judgments We then evaluated the system performance
against the subjects? judgments in terms of Precision at Rank 1, P(1). Precision at Rank
(1) measures the proportion of correct literal interpretations among the paraphrases
in rank 1. The results are shown in Table 9. The system identifies literal paraphrases
with a P(1) = 0.81 and the baseline with a P(1) = 0.55. We then conducted a one-tailed
Sign test (Siegel and Castellan 1988) that showed that this difference in performance is
statistically significant (N = 15, x = 1, p < 0.001).
336
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Table 9
System and baseline P(1) and MAP.
Relation System P(1) Baseline P(1) System MAP Baseline MAP
Verb?DirectObject 0.79 0.52 0.60 0.54
Verb?Subject 0.83 0.57 0.66 0.57
Average 0.81 0.55 0.62 0.56
4.4.2 Setting 2: Creation of a Paraphrasing Gold Standard. The subjects were presented with
a set of sentences containing metaphorical expressions and asked to write down all suit-
able literal paraphrases for the highlighted metaphorical verbs that they could think of.
Subjects Five volunteer subjects who were different from the ones used in the pre-
vious setting participated in this experiment. They were all native speakers of English
and some of them had a linguistics background (postgraduate-level degree in English).
Gold Standard The elicited paraphrases combined together can be interpreted
as a gold standard. For instance, the gold standard for the phrase ?brushed aside the
accusations? consists of the verbs rejected, ignored, disregarded, dismissed, overlooked, and
discarded.
System evaluation by gold standard comparison The system output was com-
pared against the gold standard using mean average precision (MAP) as a measure.
MAP is defined as follows:
MAP = 1
M
M
?
j=1
1
Nj
Nj
?
i=1
Pji (12)
where M is the number of metaphorical expressions, Nj is the number of correct para-
phrases for the metaphorical expression j, Pji is the precision at each correct paraphrase
(the number of correct paraphrases among the top i ranks). First, average precision
is estimated for individual metaphorical expressions, and then the mean is computed
across the data set. This measure allows one to assess ranking quality beyond rank 1,
as well as the recall of the system. As compared with the gold standard, MAP of the
paraphrasing system is 0.62 and that of the baseline is 0.56, as shown in Table 9.
4.4.3 Discussion. Given that the metaphor paraphrasing task is open-ended, any gold
standard elicited on the basis of it cannot be exhaustive. Some of the correct paraphrases
may not occur to subjects during the experiment. As an example, for the phrase ?stir
excitement? most subjects suggested only one paraphrase ?create excitement,? which is
found in rank 3, suggesting an average precision of 0.33 for this phrase. The top ranks of
the system output are occupied by provoke and stimulate, however, which are intuitively
correct, more precise paraphrases, despite none of the subjects having thought of them.
Such examples contribute to the fact that the system?s MAP is significantly lower than
its precision at rank 1, because a number of correct paraphrases proposed by the system
are not included in the gold standard.
The selectional preference-based re-ranking yields a considerable improvement in
precision at rank 1 (26%) over the baseline. This component is also responsible for some
errors of the system, however. One of the potential limitations of selectional preference-
based approaches to metaphor paraphrasing is the presence of verbs exhibiting weak
337
Computational Linguistics Volume 39, Number 2
selectional preferences. This means that these verbs are not strongly associated with
any of their argument classes. As noted in Section 3, such verbs tend to be used
literally, and are therefore suitable paraphrases. Our selectional preference model de-
emphasizes them, however, and, as a result, they are not selected as literal paraphrases
despite matching the context. This type of error is exemplified by the phrase ?mend
marriage.? For this phrase, the system ranking overruns the correct top suggestion
of the baseline, ?improve marriage,? and outputs ?repair marriage? as the most likely
literal interpretation, although it is in fact a metaphorical use. This is likely to be due to
the fact that improve exposes a moderate selectional preference strength.
Table 10 provides frequencies of the common errors of the system by type. The
most common type of error is triggered by the conventionality of certain metaphorical
verbs. Because they frequently co-occur with the target noun class in the corpus, they
receive a high association score with that noun class. This results in a high ranking
of conventional metaphorical paraphrases. Examples of top-ranked metaphorical para-
phrases include ?confront a question? for ?tackle a question,? ?repairmarriage? for ?mend
marriage,? ?example pictures? for ?example illustrates.?
These errors concern non-literalness of the produced paraphrases. A less frequently
occurring error was paraphrasing with a verb that has a different meaning. One such
example was the metaphorical expression ?tensionmounted,? for which the system pro-
duced a paraphrase ?tension lifted,? which has the opposite meaning. This error is likely
to have been triggered by the WordNet filter, whereby one of the senses of lift would
have a common hypernym with the metaphorical verb mount. This results in lift not
being discarded by the filter, and subsequently ranked top due to the conventionality of
the expression ?tension lifted.?
Another important issue that the paraphrase analysis brought to the foreground
is the influence of wider context on metaphorical interpretation. The current system
processes only the information contained within the GR of interest, discarding the
rest of the context. For some cases, however, this is not sufficient and the analysis
of a wider context is necessary. For instance, given the phrase ?scientists focus? the
system produces a paraphrase ?scientists think,? rather than the more likely paraphrase
?scientists study.? Such ambiguity of focus could potentially be resolved by taking its
wider context into account. The context-based paraphrase ranking model described in
Section 4.3.1 allows for the incorporation of multiple relations of the metaphorical verb
in the sentence.
Although the paraphrasing system uses hand-coded lexical knowledge from
WordNet, it is important to note that metaphor paraphrasing is not restricted to
metaphorical senses included in WordNet. Even if a metaphorical sense is absent from
WordNet, the system can still identify its correct literal paraphrase relying on the
Table 10
Common system errors by type.
Source of error Subject?Verb Verb?Object Totals
Metaphor conventionality 0 5 5
General polysemy/WordNet filter 1 1 2
SP re-ranking 0 1 1
Lack of context 1 1 2
Totals 2 8 10
338
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
hyponymy relation and similarity between concepts, as described in Section 4.3.2. For
example, the metaphorical sense of handcuff in ?research is handcuffed? is not included
in Wordnet, although the system correctly identifies its paraphrase confine (?research is
confined?).
5. Evaluation of Integrated System
Up to now, the identification and the paraphrasing systemswere evaluated individually
as modules. To determine to which extent the presented systems are applicable within
NLP, we then ran the two systems together in a pipeline and evaluated the accuracy of
the resulting text-to-text metaphor processing. First, the metaphor identification system
was applied to naturally occurring text taken from the BNC and then the metaphorical
expressions identified in those texts were paraphrased by the paraphrasing system.
Some of the expressions identified and paraphrased by the integrated system are shown
in Figure 11. The system output was compared against human judgments in two
phases. In phase 1, a small sample of sentences containing metaphors identified and
paraphrased by the system was judged by multiple judges. In phase 2, a larger sample
of phrases was judged by only one judge (one of the authors of this article). Agreement
of the judgments of the latter with the other judges was measured on the data from
phase 1.
Because our goal was to evaluate both the accuracy of the integrated system and
its usability by other NLP tasks, we assessed its performance in a two-fold fashion.
Instances where metaphors were both correctly identified and paraphrased by the
system were considered strictly correct, as they show that the system fully achieved
CKM 391 Time and time again he would stare at the ground, hand on hip, if he thought he had received
a bad call, and then swallow his anger and play tennis.
CKM 391 Time and time again he would stare at the ground, hand on hip, if he thought he had received
a bad call, and then suppress his anger and play tennis.
AD9 3205 He tried to disguise the anxiety he felt when he found the comms system down, but Tammuz
was nearly hysterical by this stage.
AD9 3205 He tried to hide the anxiety he felt when he found the comms system down, but Tammuz
was nearly hysterical by this stage.
AMA 349Wewill halt the reduction in NHS services for long-term care and community health services
which support elderly and disabled patients at home.
AMA 349 We will prevent the reduction in NHS services for long-term care and community health
services which support elderly and disabled patients at home.
J7F 77 An economist would frame this question in terms of a cost-benefit analysis: the maximization of
returns for the minimum amount of effort injected.
J7F 77 An economist would phrase this question in terms of a cost-benefit analysis: the maximization
of returns for the minimum amount of effort injected.
EEC 1362 In it, Younger stressed the need for additional alternatives to custodial sentences, which had
been implicit in the decision to ask the Council to undertake the enquiry.
EEC 1362 In it, Younger stressed the need for additional alternatives to custodial sentences, which had
been implicit in the decision to ask the Council to initiate the enquiry.
A1F 24 Moreover, Mr Kinnock brushed aside the suggestion that he needed a big idea or unique selling
point to challenge the appeal of Thatcherism.
A1F 24 Moreover, Mr Kinnock dismissed the suggestion that he needed a big idea or unique selling
point to challenge the appeal of Thatcherism.
Figure 11
Metaphors identified (first sentences) and paraphrased (second sentences) by the system.
339
Computational Linguistics Volume 39, Number 2
its goals. Instances where the paraphrasing retained the meaning and resulted in a
literal paraphrase (including the cases where the identification module tagged a literal
expression as a metaphor) were considered correct lenient. The intuition behind this
evaluation setting is that correct paraphrasing of literal expressions by other literal
expressions, albeit not demonstrating the positive contribution of metaphor processing,
does not lead to any errors in system output and thus does not hamper the overall
usability of the integrated system.
5.1 Phase 1: Small Sample, Multiple Judges
Three volunteer subjects participated in the experiment. They were all native speakers
of English and had no formal training in linguistics.
Materials and task Subjects were presented with a set of sentences containing
metaphorical expressions identified by the system and their paraphrases, as shown
in Figure 12. There were 35 such sentences in the sample. They were asked to do the
following:
1. Compare the sentences, decide whether the highlighted expressions have
the same meaning, and record this in the box provided;
2. Decide whether the verbs in both sentences are used metaphorically or
literally and tick the respective boxes.
For the second task, the same definition of metaphor as in the identification evaluation
(cf. Section 3.3.2) was provided for guidance.
Interannotator agreement The reliability of annotations was evaluated indepen-
dently for judgments on similarity of paraphrases and their literalness. The inter-
annotator agreement on the task of distinguishing metaphoricity from literalness was
measured at ? = 0.53 (n = 2,N = 70, k = 3). On the paraphrase (i.e., meaning retention)
task, reliability was measured at ? = 0.63 (n = 2,N = 35, k = 3).
System performance We then evaluated the integrated system performance
against the subjects? judgments in terms of accuracy (both strictly correct and correct
Figure 12
Evaluation of metaphor identification and paraphrasing.
340
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Table 11
Integrated system performance.
Tagging case Acceptability Percentage
Correct paraphrase: metaphorical? literal ? 53.8
Correct paraphrase: literal? literal ? 13.5
Correct paraphrase: literal?metaphorical ? 0.5
Correct paraphrase: metaphorical?metaphorical ? 10.7
Incorrect paraphrase ? 21.5
lenient). Strictly correct accuracy in this task measures the proportion of metaphors
both identified and paraphrased correctly in the given set of sentences. Correct lenient
accuracy, which demonstrates applicability of the system, is represented by the overall
proportion of paraphrases that retained their meaning and resulted in a literal para-
phrase (i.e., including literal paraphrasing of literal expressions in original sentences).
Human judgments were merged into a majority gold standard, which consists of those
instances that were considered correct (i.e., identified metaphor correctly paraphrased
by the system) by at least two judges. Compared to this majority gold standard, the
integrated system operates with a strictly correct accuracy of 0.66 and correct lenient
accuracy of 0.71. The average human agreement with the majority gold standard in
terms of accuracy is 0.80 on the literalness judgments and 0.89 on the meaning retention
judgments.
5.2 Phase 2: Larger Sample, One Judge
The systemwas also evaluated on a larger sample of automatically annotatedmetaphor-
ical expressions (600 sentences) using one person?s judgments produced following
the procedure from phase 1. We measured how far these judgments agree with the
judges used in phase 1. The agreement on meaning retention was measured at ? = 0.59
(n = 2,N = 35, k = 4) and that on the literalness of paraphrases at ? = 0.54 (n = 2,N =
70, k = 4).
On this larger data set, the system achieved an accuracy of 0.54 (strictly correct) and
0.67 (correct lenient). The proportions of different tagging cases are shown in Table 11.
The table also shows the acceptability of tagging cases. Acceptability indicates whether
or not this type of system paraphrasing would cause an error when hypothetically
integrated with an external NLP application. Cases where the system produces correct
literal paraphrases for metaphorical expressions identified in the text would benefit
another NLP application, whereas cases where literal expressions are correctly para-
phrased by other literal expressions are considered neutral. Both such cases are deemed
acceptable, because they increase or preserve literalness of the text. All other tagging
cases introduce errors, thus they are marked as unacceptable. Examples of different
tagging cases are shown in Table 12.
The accuracy of metaphor-to-literal paraphrasing (0.54) indicates the level of in-
formative contribution of the system, and the overall accuracy of correct paraphrasing
resulting in a literal expression (0.67) represents the level of its acceptability within NLP.
5.3 Discussion and Error Analysis
The results of integrated system evaluation suggest that the system is capable of pro-
viding useful information about metaphor for an external text processing application
341
Computational Linguistics Volume 39, Number 2
Table 12
Examples of different tagging cases.
Tagging case Examples
Correct paraphrase: met? lit throw an idea? express an idea
Correct paraphrase: lit? lit adopt a recommendation? accept a recommendation
Correct paraphrase: lit?met arouse memory? awakenmemory
Correct paraphrase: met?met work killed him?work exhausted him
with a reasonable accuracy (0.67). It may, however, also introduce errors in the text by
incorrect paraphrasing, as well as by producing metaphorical paraphrases. If the latter
errors are rare (0.5%), the errors of the former type are sufficiently frequent (21.5%) to
make the metaphor system less desirable for use in NLP. It is therefore important to
address such errors.
Table 13 shows the contribution of the individual system components to the overall
error. The identification system tags 28% of all instances incorrectly (170). This yields a
component performance of 72%. This result is slightly lower than that obtained in its
individual evaluation in a setting with multiple judges (79%). This can be explained
by the fact that the integrated system was evaluated by one judge only, rather than
using a majority gold standard. When compared with the judgments of each annotator
pairwise the system precision was measured at 74% (cf. Section 3.3.2). Some of the
literal instances tagged as ametaphor by the identification component are then correctly
paraphrased with a literal expression by the paraphrasing component. Such cases do
not change the meaning of the text, and hence are considered acceptable. The resulting
contribution of the identification component to the overall error of the integrated system
is thus 15%.
As Table 13 shows, the paraphrasing component failed in 32% of all cases
(196 instances out of 600 were paraphrased incorrectly). As mentioned previously, this
error can be further split into paraphrasing without meaning retention (21.5%) and
metaphorical paraphrasing (11%). Both of these error types are unacceptable and lead
to lower performance of the integrated system. This error rate is also higher than that
of the paraphrasing system when evaluated individually on a manually created data
set (19%). The reasons for incorrect paraphrasing by the integrated system are manifold
Table 13
System errors by component. Three categories are cases where the identification model
incorrectly tagged a literal expression as metaphoric (false negatives from this module
were not measured). The remaining two categories are for paraphrase errors on correctly
identified metaphors.
Type of error Identification Paraphrasing
Correct paraphrase: lit? lit 81 0
Correct paraphrase: lit?met 3 3
Correct paraphrase: met?met ? 64
Incorrect paraphrase for literal 86 86
Incorrect paraphrase for metaphor ? 43
Totals 170 196
342
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
and concern both themetaphor identification and paraphrasing components. One of the
central problems stems from the initial tagging of literal expressions as metaphorical by
the identification system. The paraphrasing system is not designed with literal-to-literal
paraphrasing in mind. When it receives literal expressions which have been incorrectly
identified as input, it searches for a more literal paraphrase for them. Not all literally
used words have suitable substitutes in the given context, however. For instance,
the literal expression ?approve conclusion? is incorrectly paraphrased as ?evaluate
conclusion.?
Similar errors occur when metaphorical expressions do not have any single-word
literal paraphrases, for example, ?country functions according to...?. This is, however,
a more fundamental problem for metaphor paraphrasing as a task. In such cases, the
system, nonetheless, attempts to produce a substitute with approximately the same
meaning, which often leads to either metaphorical or incorrect paraphrasing. For in-
stance, ?country functions? is paraphrased by ?country runs,? with suggestions with
lower rank being ?country works? and ?country operates.?
Some errors that occur at the paraphrasing level are also due to the general
word sense ambiguity of certain verbs or nouns. Consider the following paraphras-
ing example, where Example (24a) shows an automatically identified metaphor and
Example (24b) its system-derived paraphrase:
(24) a. B71 852 Craig Packer and Anne Pusey of the University of Chicago have
continued to follow the life and loves of these Tanzanian lions.
b. B71 852 Craig Packer and Anne Pusey of the University of Chicago have
continued to succeed the life and loves of these Tanzanian lions.
This error results from the fact that the verb succeed has a high selectional preference for
life in one of its senses (?attain success or reach a desired goal?) and is similar to follow
in WordNet in another of its senses (?be the successor [of]?). The system merges these
two senses in one, resulting in an incorrect paraphrase.
One automatically identified example exhibited interaction of metaphor with
metonymy at the interpretation level. In the phrase ?break word,? the verb break is used
metaphorically (although conventionally) and the noun word is a metonym standing
for promise. This affected paraphrasing in that the system searched for verbs denoting
actions that could be done with words, rather than promises, and suggested the para-
phrase ?interrupt word(s).? This paraphrase is interpretable in the context of a person
giving a speech, but not in the context of a person giving a promise. This was the only
case of metonymy in the analyzed data, however.
Another issue that the evaluation on a larger data set revealed is the limitations of
the WordNet filter used in the paraphrasing system. Despite being a wide-coverage
general-domain database, WordNet does not include information about all possible
relations that exist between particular word senses. This means that some of the correct
paraphrases suggested by the context-based model get discarded by the WordNet filter
due to missing information in WordNet. For instance, the system produces no para-
phrase for the metaphors ?hurl comment,? ?spark enthusiasm,? and ?magnify thought?
that it correctly identified. This problemmotivates the exploration of possibleWordNet-
free solutions for similarity detection in the metaphor paraphrasing task. The system
could either rely entirely on such a solution, or back off to it in cases when theWordNet-
based system fails.
Table 14 provides a summary of system errors by type. The most common errors
are caused by metaphor conventionality resulting in metaphorical paraphrasing (e.g.,
343
Computational Linguistics Volume 39, Number 2
Table 14
Errors of the paraphrasing component by type.
Source of error Met?Met Lit?Met Incorr. for Lit Incorr. for Met Total
No literal paraphrase exists 11 0 5 2 18
Metaphor conventionality 53 3 0 0 56
General polysemy 0 0 13 10 23
WordNet filter 0 0 21 21 42
SP re-ranking 0 0 41 7 48
Lack of context 0 0 6 2 8
Interaction with metonymy 0 0 0 1 1
Totals 64 3 86 43 196
?swallow anger? suppress anger,? ?work killed him? work exhausted him?), followed
by the WordNet filter? and general polysemy?related errors (e.g. ?follow lives ?
succeed lives?), resulting in incorrect paraphrasing or the system not producing any
paraphrase at all. Metaphor paraphrasing by another conventional metaphor instead
of a literal expression is undesirable, although it may still be useful if the paraphrases
are more lexicalized than the original expression. The word sense ambiguity? and
WordNet-based errors are more problematic, however, and need to be addressed in the
future. SP re-ranking is responsible for the majority of incorrect paraphrasing of literal
expressions. This may be due to the fact that the model is ignorant of the meaning
retention aspect, but rather favors the paraphrases that are used literally (albeit
incorrectly) in the given context. This shows that when building an integrated system,
it is necessary to adapt the metaphor paraphrasing module to be able to also handle
literal expressions, because the identification module is likely to produce at least some
of them.
5.4 Comparison to the CorMet System
It is hard to directly compare the performance of the presented system to the other
recent approaches to metaphor, because all of these approaches assume different task
definitions, and hence use data sets and evaluation techniques of their own. Among the
data-driven methods, however, the closest in nature to ours is Mason?s (2004) CorMet
system. Mason?s system does not perform metaphor interpretation or identification of
metaphorical expressions in text, but rather focuses on the detection of metaphorical
links between distant domains. Our system also involves such detection. Whereas
Mason relies on domain-specific selectional preferences for this purpose, however, our
system uses information about verb subcategorization, as well as general selectional
preferences, to perform distributional clustering of verbs and nouns and then link the
clusters based on metaphorical seeds. Another fundamental difference is that whereas
CorMet assigns explicit domain labels, our system models source and target domains
implicitly. In the evaluation of the CorMet system, the acquired metaphorical mappings
are compared to those in the manually created Master Metaphor List demonstrating the
accuracy of 77%. In our system, on the contrary, metaphor acquisition is evaluated via
extraction of naturally occurring metaphorical expressions, achieving a performance of
79% in terms of precision. In order to compare the new mapping acquisition ability
344
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
of our system to that of CorMet, however, we performed an additional analysis of the
mappings hypothesized by our noun clusters in relation to those in the MML. It was not
possible to compare the new mappings discovered by our system to the MML directly
as was done in Mason?s experiments, because in our approach source domains are
represented by clusters of their characteristic verbs. The analysis of the noun clusters
with respect to expansion of the the seed mappings taken from the MML, however,
allowed us to evaluate the mapping acquisition by our system in terms of both precision
and recall. The goal was to confirm our hypothesis that abstract concepts get clustered
together if they are associated with the same source domain and to evaluate the quality
of the newly acquired mappings.
To do this, we randomly selected 10 target domain categories described in the
MML and manually extracted all corresponding mappings (42 mappings in total).
The categories included SOCIETY, IDEA, LIFE, OPPORTUNITY, CHANGE, LOVE,
DIFFICULTY, CREATION, RELATIONSHIP, and COMPETITION. For the concept of
OPPORTUNITY, for example, three mappings were present in the MML: OPPORTU-
NITIES ARE PHYSICAL OBJECTS, OPPORTUNITIES ARE MOVING ENTITIES, and
OPPORTUNITIES ARE OPEN PATHS, whereas for the concept of COMPETITION the
list describes only two mappings: COMPETITION IS A RACE, COMPETITION IS A
WAR.
We then extracted the system-produced clusters containing the selected 10 target
concepts. Examples of the mappings and the corresponding clusters are shown in
Figure 13. Our goal was to verify whether other concepts in the cluster containing the
target concept are associated with the source domains given in the mappings. Each
member of these clusters was analyzed for possible association with the respective
source domains. For each concept in a cluster, we verified that it is associated with
the respective source domain by finding a corresponding metaphorical expression and
annotating the concepts accordingly. The degree of association of the members of the
clusters with a given source domain was evaluated in terms of precision on the set
of hypothesized mappings. The precision of the cluster?s association with the source
Conceptual mapping: RELATIONSHIP IS A MECHANISM (VEHICLE)
Cluster: consensus relation tradition partnership resistance foundation alliance friendship con-
tact reserve unity link peace bond myth identity hierarchy relationship connection balance
marriage democracy defense faith empire distinction coalition regime division
Conceptual mapping: LIFE IS A JOURNEY
Cluster: politics practice trading reading occupation profession sport pursuit affair career
thinking life
Conceptual mapping: SOCIETY is a (HUMAN) BODY
Cluster: class population nation state country family generation trade profession household
kingdom business industry economy market enterprise world community institution society
sector
Conceptual mapping: DIFFICULTY is DIFFICULTY IN MOVING (OBSTACLE); PHYSICAL
HARDNESS
Cluster: threat crisis risk problem poverty obstacle dilemma challenge prospect danger dis-
crimination barrier difficulty shortage
Conceptual mapping: OPPORTUNITY is a PHYSICAL OBJECT; MOVING ENTITY; OPEN
PATH
Cluster: incentive attraction scope remedy chance choice solution option perspective range
possibility contrast opportunity selection alternative focus
Figure 13
Noun clusters.
345
Computational Linguistics Volume 39, Number 2
concept was calculated as a proportion of the associated concepts in it. Based on these
results we computed the average precision (AP) as follows:
AP = 1
M
M
?
j=1
#associated concepts in cluster cj
|cj|
(13)
whereM is the number of hypothesizedmappings and cj is the cluster of target concepts
corresponding to mapping j.
The annotation was carried out by one of the authors and its average precision
is 0.82. This confirms the hypothesis of clustering by association and shows that our
method favorably compares to Mason?s system. This is only an approximate compari-
son, however. Direct comparison of metaphor acquisition by the two systems was not
possible, as they produce the output in different formats and, as mentioned earlier, our
system models conceptual mappings implicitly, both within the noun clusters, as well
as by linking them to the verb clusters.
We then additionally evaluated the recall of mapping acquisition by our system
against the MML. For each selected MML mapping, we manually extracted all alter-
native target concepts associated with the source domain in the mapping from the
MML. For example, in case of LIFE IS A JOURNEY we identified all target concepts
associated with JOURNEY according to the MML and extracted them. These included
LIFE, CAREER, LOVE, and CHANGE. We then verified whether the relevant system-
produced noun clusters contained these concepts. The recall was then calculated as a
proportion of the concepts in this list within one cluster. For example, the concepts LIFE
and CAREER are found in the same cluster, but not LOVE and CHANGE. The overall
recall of mapping acquisition was measured at 0.50.
These results show that the system is able to discover a large number of metaphor-
ical connections in the data with high precision. Although the evaluation against the
Master Metaphor List is subjective, it suggests that the use of statistical data-driven
methods in general, and distributional clustering in particular, is a promising direction
for computational modeling of metaphor.
6. Conclusion and Future Directions
The 1980s and 1990s provided us with a wealth of ideas on the structure and mecha-
nisms of metaphor. The computational approaches formulated back then are still highly
influential, although their use of task-specific hand-coded knowledge is becoming in-
creasingly less popular. The last decade witnessed a significant technological leap in
natural language computation, whereby manually crafted rules gradually gave way
to more robust corpus-based statistical methods. This is also the case for metaphor
research. In this article, we presented the first integrated statistical system for metaphor
processing in unrestricted text. Our method is distinguished from previous work in that
it does not rely on anymetaphor-specific hand-coded knowledge (besides the seed set in
the identification experiments), operates on open-domain text, and produces interpreta-
tions in textual format. The system, consisting of independent metaphor identification
and paraphrasing modules, operates with a high precision (0.79 for identification, 0.81
for paraphrasing, and 0.67 as an integrated system). Although the system has been
tested only on verb?subject and verb?object metaphors at this stage, the described iden-
tification and paraphrasing methods should be similarly applicable to a wider range
of syntactic constructions. This expectation rests on the fact that both distributional
346
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
clustering and selectional preference induction techniques have been shown to model
the meanings of a range of word classes (Hatzivassiloglou and McKeown 1993; Boleda
Torrent and Alonso i Alemany 2003; Brockmann and Lapata 2003; Zapirain, Agirre,
and Ma`rquez 2009). Extending the system to deal with metaphors represented by other
word classes and constructions as well as multi-word metaphors is part of future work.
Such an extension of the identification system would require the creation of a
seed set exemplifying more syntactic constructions and the corpus search over fur-
ther grammatical relations (e.g., verb?prepositional phrase [PP] complement relations:
?Hillary leapt in the conversation;? adjectival modifier?noun relations ?slippery mind,
deep unease, heavy loss;? noun?PP complement relations: ?a fraction of self-control, a
foot of a mountain;? verb?VP complement relations: ?aching to begin the day;? and
copula constructions: ?Death is the sorry end of the human story, not a mysterious
prelude to a new one?). Besides noun and verb clustering, it would also be necessary
to perform clustering of adjectives and adverbs. Clusters of verbs, adjectives, adverbs,
and concrete nouns would then represent source domains within the model. The data
study of Shutova and Teufel (2010) suggested that it is sometimes difficult to choose the
optimal level of abstraction of domain categories that would generalize well over the
data. Although the system does not explicitly assign any domain labels, its domain
representation is still restricted by the fixed level of generality of source concepts,
defined by the chosen cluster granularity. To relax this constraint, one could attempt
to automatically optimize cluster granularity to fit the data more accurately and to
ensure that the generated clusters explain themetaphorical expressions in the data more
comprehensively. A hierarchical clustering algorithm, such as that of Yu, Yu, and Tresp
(2006) or Sun andKorhonen (2011), could be used for this purpose. Besides this, it would
be desirable to be able to generalize metaphorical associations learned from one type
of syntactic construction across all syntactic constructions, without providing explicit
seed examples for the latter. For instance, given the seed phrase ?stir excitement,?
representing the conceptual mapping FEELINGS ARE LIQUIDS, the system should be
able to discover not only that phrases such as ?swallow anger? are metaphorical, but
that phrases such as ?ocean of happiness? are as well.
The extension of the paraphrasing system to other syntactic constructions would
involve the extraction of further grammatical relations from the corpus, such as those
listed herein, and their incorporation into the context-based paraphrase selectionmodel.
Extending both the identification system and the paraphrasing system would require
the application of the selectional preference model to other word classes. Although
Resnik?s selectional association measure has been used to model selectional preferences
of verbs for their nominal arguments, it is in principle a generalizable measure of word
association. Information-theoretic word association measures (e.g., mutual information
[Church and Hanks 1990]) have been continuously successfully applied to a range of
syntactic constructions in a number of NLP tasks (Hoang, Kim, and Kan 2009; Baldwin
and Kim 2010). This suggests that applying a distributional association measure, such
as the one proposed by Resnik, to other part-of-speech classes should still result in a
realistic model of semantic fitness, which in our terms corresponds to a measure of
?literalness? of the paraphrases.
In addition, the selectional preference model can be improved by using an SP
acquisition algorithm that can handle word sense ambiguity (e.g., Rooth et al 1999;
O? Se?aghdha 2010; Reisinger and Mooney 2010). The current approach relies on SP
classes produced by hard clustering and fails to accurately model word senses of gener-
ally polysemous words. This resulted in a number of errors in metaphor paraphrasing
and it therefore needs to be addressed in the future.
347
Computational Linguistics Volume 39, Number 2
The current version of the metaphor paraphrasing system still relies on some hand-
coded knowledge in the form of WordNet. WordNet has been criticized for a lack of
consistency, high granularity of senses, and negligence with respect to some important
semantic relations (Lenat, Miller, and Yokoi 1995). In addition, WordNet is a general-
domain resource, which is less suitable if one wanted to apply the system to domain-
specific data. For all of these reasons it would be preferable to develop a WordNet-free
fully automated approach to metaphor resolution. Vector space models of word mean-
ing (Erk 2009; Rudolph and Giesbrecht 2010; Van de Cruys, Poibeau, and Korhonen
2011) might provide a solution, as they have proved efficient in general paraphrasing
and lexical substitution settings (Erk and Pado? 2009). The feature similarity component
of the paraphrasing system that is currently based on WordNet could be replaced by
such a model.
Another crucial problem that needs to be addressed is the coverage of the iden-
tification system. To enable high usability of the system it is necessary to perform
high-recall processing. One way to improve the coverage is the creation of a larger,
more diverse seed set. Although it is hardly possible to describe the whole variety of
metaphorical language, it is possible to compile a set representative of (1) all most com-
mon source?target domain mappings and (2) all types of syntactic constructions that
exhibit metaphoricity. The existing metaphor resources, primarily the Master Metaphor
List (Lakoff, Espenson, and Schwartz 1991), and examples from the linguistic literature
about metaphor, could be a sensible starting point on a route to such a data set. Having
a diverse seed set should enable the identification system to attain a broad coverage of
the corpus.
The proposed text-to-text representation of metaphor processing is directly trans-
ferable to other NLP tasks and applications that could benefit from the inclusion of
a metaphor processing component. Overall, our results suggest that the system can
provide useful and accurate information about metaphor to other NLP tasks relying
on lexical semantics. In order to prove its usefulness for external applications, however,
an extrinsic task-based evaluation is outstanding. In the future, we intend to integrate
metaphor processing with NLP applications, exemplified by MT and opinion mining,
in order to demonstrate the contribution of this pervasive yet rarely addressed phe-
nomenon to natural language semantics.
Acknowledgments
We would like to thank the volunteer
annotators for their help in the evaluations,
as well as the Cambridge Overseas Trust
(UK), EU FP-7 PANACEA project, and the
Royal Society (UK), who funded our work.
References
Abend, Omri and Ari Rappoport. 2010.
Fully unsupervised core-adjunct argument
classification. In Proceedings of the 48th
Annual Meeting of the Association for
Computational Linguistics, pages 226?236,
Uppsala.
Agerri, Rodrigo, John Barnden, Mark Lee,
and Alan Wallington. 2007. Metaphor,
inference and domain-independent
mappings. In Proceedings of RANLP-2007,
pages 17?23, Borovets.
Alonge, Antonietta and Margherita Castelli.
2003. Encoding information on metaphoric
expressions in WordNet-like resources.
In Proceedings of the ACL 2003 Workshop
on Lexicon and Figurative Language,
pages 10?17, Sapporo.
Andersen, Oistein, Julien Nioche, Ted
Briscoe, and John Carroll. 2008. The
BNC parsed with RASP4UIMA. In
Proceedings of LREC 2008, pages 865?869,
Marrakech.
Baldwin, Timothy and Su Nam Kim. 2010.
Multiword expressions. In N. Indurkhya
and F. J. Damerau, editors, Handbook of
Natural Language Processing, Second Edition.
CRC Press, Taylor and Francis Group,
Boca Raton, FL, pages 267?292.
348
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Barnden, John and Mark Lee. 2002. An
artificial intelligence approach to
metaphor understanding. Theoria et
Historia Scientiarum, 6(1):399?412.
Barque, Lucie and Franc?ois-Re?gis
Chaumartin. 2009. LDV Forum, 24(2):5?18.
Barzilay, Regina and Lillian Lee. 2003.
Learning to paraphrase: an unsupervised
approach using multiple-sequence
alignment. In Proceedings of the 2003
Conference of the North American Chapter of
the Association for Computational Linguistics
on Human Language Technology - Volume 1,
NAACL ?03, pages 16?23, Edmonton.
Barzilay, Regina and Kathryn McKeown.
2001. Extracting paraphrases from a
parallel corpus. In Proceedings of the
39th Annual Meeting on Association for
Computational Linguistics, ACL ?01,
pages 50?57, Toulouse.
Bergsma, Shane, Dekang Lin, and Randy
Goebel. 2008. Discriminative learning of
selectional preference from unlabeled text.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing,
EMNLP ?08, pages 59?68, Honolulu, HI.
Birke, Julia and Anoop Sarkar. 2006.
A clustering approach for the nearly
unsupervised recognition of nonliteral
language. In Proceedings of EACL-06,
pages 329?336, Trento.
Black, Max. 1962.Models and Metaphors.
Cornell University Press, Ithaca, NY.
Boleda Torrent, Gemma and Laura Alonso i
Alemany. 2003. Clustering adjectives for
class acquisition. In Proceedings of the Tenth
Conference of the European Chapter of the
Association for Computational Linguistics -
Volume 2, EACL ?03, pages 9?16, Budapest.
Bolshakov, Igor and Alexander Gelbukh.
2004. Synonymous paraphrasing using
Wordnet and Internet. In Proceedings of the
9th International Conference on Applications
of Natural Language to Information Systems,
NLDB 2004, pages 312?323, Alicante.
Brew, Chris and Sabine Schulte im Walde.
2002. Spectral clustering for German verbs.
In Proceedings of EMNLP, pages 117?124,
Philadelphia, PA.
Briscoe, Ted, John Carroll, and Rebecca
Watson. 2006. The second release of
the RASP system. In Proceedings of the
COLING/ACL on Interactive Presentation
Sessions, pages 77?80, Sydney.
Brockmann, Carsten and Mirella Lapata.
2003. Evaluating and combining
approaches to selectional preference
acquisition. In Proceedings of the Tenth
Conference of the European Chapter of the
Association for Computational Linguistics -
Volume 1, EACL ?03, pages 27?34,
Budapest.
Burnard, Lou. 2007. Reference Guide for the
British National Corpus (XML Edition).
Available at http://www.natcorp.
ox.ac.uk/docs/URG.
Callison-Burch, Chris, Philipp Koehn, and
Miles Osborne. 2006. Improved statistical
machine translation using paraphrases. In
Proceedings of NAACL, HLT-NAACL ?06,
pages 17?24, New York, NY.
Cameron, Lynne. 2003.Metaphor in
Educational Discourse. Continuum, London.
Carroll, John, Guido Minnen, Darren Pearce,
Yvonne Canning, Siobhan Devlin, and
John Tait. 1999. Simplifying text for
language-impaired readers. In Proceedings
of the 9th Conference of the European Chapter
of the Association for Computational
Linguistics (EACL), pages 269?270, Bergen.
Chen, Jinxiu, Donghong Ji, Chew Lim Tan,
and Zhengyu Niu. 2006. Unsupervised
relation disambiguation using spectral
clustering. In Proceedings of the
COLING/ACL, pages 89?96, Sydney.
Church, Kenneth and Patrick Hanks.
1990. Word association norms, mutual
information, and lexicography.
Computational Linguistics, 16(1):22?29.
Clark, Stephen and James Curran. 2007.
Wide-coverage efficient statistical
parsing with CCG and log-linear models.
Computational Linguistics, 33(4):493?552.
Copestake, Ann and Ted Briscoe. 1995.
Semi-productive polysemy and sense
extension. Journal of Semantics, 12:15?67.
Davidov, Dmitry, Roi Reichart, and Ari
Rappoport. 2009. Superior and efficient
fully unsupervised pattern-based concept
acquisition using an unsupervised parser.
In Proceedings of the Thirteenth Conference on
Computational Natural Language Learning,
CoNLL ?09, pages 48?56, Boulder, CO.
De Cao, Diego and Roberto Basili.
2009. Combining distributional and
paradigmatic information in a lexical
substitution task. In Proceedings of
EVALITA Workshop, 11th Congress of
Italian Association for Artificial Intelligence,
Reggie Emilia.
Dras, Mark. 1999. Tree Adjoining Grammar
and the Reluctant Paraphrasing of Text. Ph.D.
thesis, Macquarie University, Australia.
Erk, Katrin. 2009. Representing words as
regions in vector space. In Proceedings of
the Thirteenth Conference on Computational
Natural Language Learning, pages 57?65,
Boulder, CO.
349
Computational Linguistics Volume 39, Number 2
Erk, Katrin and Diana McCarthy. 2009.
Graded word sense assignment. In
Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing, pages 440?449, Edinburgh.
Erk, Katrin and Sebastian Pado?. 2009.
Paraphrase assessment in structured
vector space: exploring parameters and
datasets. In Proceedings of the Workshop on
Geometrical Models of Natural Language
Semantics, pages 57?65, Athens.
Fass, Dan. 1991. met*: A method for
discriminating metonymy and metaphor
by computer. Computational Linguistics,
17(1):49?90.
Fass, Dan and Yorick Wilks. 1983. Preference
semantics, ill-formedness, and metaphor.
Computational Linguistics, 9(3-4):178?187.
Fauconnier, Gilles and Mark Turner. 2002.
The Way We Think: Conceptual Blending and
the Mind?s Hidden Complexities. Basic Books,
New York, NY.
Feldman, Jerome. 2006. From Molecule to
Metaphor: A Neural Theory of Language.
The MIT Press, Cambridge, MA.
Feldman, Jerome and Srini Narayanan.
2004. Embodied meaning in a neural
theory of language. Brain and Language,
89(2):385?392.
Fellbaum, Christiane, editor. 1998.
WordNet: An Electronic Lexical Database
(ISBN: 0-262-06197-X). MIT Press,
Cambridge, MA.
Fillmore, Charles, Christopher Johnson,
and Miriam Petruck. 2003. Background
to FrameNet. International Journal of
Lexicography, 16(3):235?250.
Gedigian, Matt, John Bryant, Srini
Narayanan, and Branimir Ciric. 2006.
Catching metaphors. In Proceedings
of the 3rd Workshop on Scalable Natural
Language Understanding, pages 41?48,
New York, NY.
Gentner, Dedre. 1983. Structure mapping:
A theoretical framework for analogy.
Cognitive Science, 7:155?170.
Gentner, Dedre, Brian Bowdle, Phillip Wolff,
and Consuelo Boronat. 2001. Metaphor is
like analogy. In D. Gentner, K. J. Holyoak,
and B. N. Kokinov, editors, The Analogical
Mind: Perspectives from Cognitive Science.
MIT Press, Cambridge, MA,
pages 199?253.
Goatly, Andrew. 1997. The Language of
Metaphors. Routledge, London.
Grady, Joe. 1997. Foundations of Meaning:
Primary Metaphors and Primary Scenes.
Ph.D. thesis, University of California
at Berkeley.
Hanks, Patrick and James Pustejovsky. 2005.
A pattern dictionary for natural language
processing. Revue Franc?aise de linguistique
applique?e, 10(2):63?82.
Hatzivassiloglou, Vasileios and Kathleen R.
McKeown. 1993. Towards the automatic
identification of adjectival scales:
Clustering adjectives according to
meaning. In Proceedings of the 31st Annual
Meeting of the Association for Computational
Linguistics, ACL ?93, pages 172?182,
Columbus, OH.
Hesse, Mary. 1966.Models and Analogies in
Science. Notre Dame University Press,
Notre Dame, IN.
Hoang, Hung Huu, Su Nam Kim, and
Min-Yen Kan. 2009. A re-examination of
lexical association measures. In Proceedings
of the Workshop on Multiword Expressions,
pages 31?39, Singapore.
Hofstadter, Douglas. 1995. Fluid Concepts and
Creative Analogies: Computer Models of the
Fundamental Mechanisms of Thought.
HarperCollins Publishers, London.
Hofstadter, Douglas and Melanie Mitchell.
1994. The Copycat Project: A model of
mental fluidity and analogy-making. In
K. J. Holyoak and J. A. Barnden, editors,
Advances in Connectionist and Neural
Computation Theory. Ablex, New York, NY.
Karov, Yael and Shimon Edelman.
1998. Similarity-based word sense
disambiguation. Computational
Linguistics, 24(1):41?59.
Kauchak, David and Regina Barzilay. 2006.
Paraphrasing for automatic evaluation. In
Proceedings of the Main Conference on Human
Language Technology, Conference of the North
American Chapter of the Association of
Computational Linguistics, HLT-NAACL
?06, pages 455?462, New York, NY.
Kingsbury, Paul and Martha Palmer. 2002.
From TreeBank to PropBank. In Proceedings
of LREC-2002, pages 1989?1993,
Gran Canaria, Canary Islands.
Kipper, Karin, Anna Korhonen, Neville
Ryant, and Martha Palmer. 2006.
Extensive classifications of English verbs.
In Proceedings of the 12th EURALEX
International Congress, pages 1?15, Torino.
Klein, Dan and Christopher Manning.
2003. Accurate unlexicalized parsing.
In Proceedings of the 41st Annual Meeting
of the Association for Computational
Linguistics, pages 423?430, Sapporo.
Knight, Kevin and Daniel Marcu. 2000.
Statistics-based summarization?step one:
Sentence compression. In Proceedings of the
Seventeenth National Conference on Artificial
350
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
Intelligence and Twelfth Conference on
Innovative Applications of Artificial
Intelligence, pages 703?710, Austin, TX.
Kok, Stanley and Chris Brockett. 2010.
Hitting the right paraphrases in good time.
In Human Language Technologies: The 2010
Annual Conference of the North American
Chapter of the Association for Computational
Linguistics, HLT ?10, pages 145?153,
Los Angeles, CA.
Korhonen, Anna, Yuval Krymolowski,
and Ted Briscoe. 2006. A large
subcategorization lexicon for natural
language processing applications.
In Proceedings of LREC 2006,
pages 1015?1020, Genoa.
Kozlowski, Raymond, Kathleen F.
McCoy, and K. Vijay-Shanker. 2003.
Generation of single-sentence
paraphrases from predicate/argument
structure using lexico-grammatical
resources. In Proceedings of the Second
International Workshop on Paraphrasing -
Volume 16, PARAPHRASE ?03,
pages 1?8, Sapporo.
Krishnakumaran, Saisuresh and Xiaojin Zhu.
2007. Hunting elusive metaphors using
lexical resources. In Proceedings of the
Workshop on Computational Approaches to
Figurative Language, pages 13?20,
Rochester, NY.
Kurohashi, Sadao. 2001. SENSEVAL-2
Japanese translation task. In Proceedings of
the SENSEVAL-2 Workshop, pages 37?44,
Toulouse.
Lakoff, George, Jane Espenson, and Alan
Schwartz. 1991. The master metaphor list.
Technical report, University of California
at Berkeley.
Lakoff, George and Mark Johnson. 1980.
Metaphors We Live By. University of
Chicago Press, Chicago, IL.
Lapata, Mirella. 2001. The Acquisition
and Modeling of Lexical Knowledge: A
Corpus-Based Investigation of Systematic
Polysemy. Ph.D. thesis, University of
Edinburgh.
Lenat, Doug, George Miller, and Toshio
Yokoi. 1995. CYC, WordNet, and EDR:
Critiques and responses. Commun.
ACM, 38(11):45?48.
Levin, Beth. 1993. English Verb Classes and
Alternations. University of Chicago Press,
Chicago, IL.
Lin, Dekang. 1998. Automatic retrieval and
clustering of similar words. In Proceedings
of the 17th International Conference on
Computational Linguistics, pages 768?774,
Montreal.
Lin, Dekang and Patrick Pantel. 2001.
Discovery of inference rules for question
answering. Natural Language Engineering,
7:343?360.
Lo?nneker, Birte. 2004. Lexical databases as
resources for linguistic creativity: Focus on
metaphor. In Proceedings of the LREC 2004
Workshop on Language Resources for
Linguistic Creativity, pages 9?16, Lisbon.
Lo?nneker, Birte and Carina Eilts. 2004. A
current resource and future perspectives
for enriching Wordnets with metaphor
information. In Proceedings of the Second
International WordNet Conference?GWC
2004, pages 157?162, Brno.
Martin, James. 1988. Representing
regularities in the metaphoric lexicon.
In Proceedings of the 12th Conference on
Computational Linguistics, pages 396?401,
Budapest.
Martin, James. 1990. A Computational Model of
Metaphor Interpretation. Academic Press
Professional, Inc., San Diego, CA.
Martin, James. 1994. Metabank: A
knowledge-base of metaphoric language
conventions. Computational Intelligence,
10:134?149.
Martin, James. 2006. A corpus-based
analysis of context effects on metaphor
comprehension. In A. Stefanowitsch and
S. T. Gries, editors, Corpus-Based Approaches
to Metaphor and Metonymy. Mouton de
Gruyter, Berlin, pages 214?236.
Mason, Zachary. 2004. Cormet: A
computational, corpus-based conventional
metaphor extraction system. Computational
Linguistics, 30(1):23?44.
McCarthy, Diana. 2002. Lexical
substitution as a task for WSD evaluation.
In Proceedings of the ACL-02 Workshop on
Word Sense Disambiguation: Recent Successes
and Future Directions - Volume 8, WSD ?02,
pages 109?115, Philadelphia, PA.
McCarthy, Diana, Bill Keller, and
Roberto Navigli. 2010. Getting synonym
candidates from raw data in the English
lexical substitution task. In Proceedings of
the 14th EURALEX International Congress,
Leeuwarden.
McCarthy, Diana and Roberto Navigli.
2007. Semeval-2007 task 10: English
lexical substitution task. In Proceedings
of the 4th Workshop on Semantic
Evaluations (SemEval-2007), pages 48?53,
Prague.
McCarthy, Diana and Roberto Navigli. 2009.
The English lexical substitution task.
Language Resources and Evaluation,
43(2):139?159.
351
Computational Linguistics Volume 39, Number 2
McKeown, Kathleen. 1979. Paraphrasing
using given and new information in a
question-answer system. In Proceedings
of the 17th Annual Meeting of the Association
for Computational Linguistics, ACL ?79,
pages 67?72, La Jolla, CA.
Meila, Marina and Jianbo Shi. 2001.
A random walks view of spectral
segmentation. In AISTATS, Key West, FL.
Meteer, Marie and Varda Shaked. 1988.
Strategies for effective paraphrasing.
In Proceedings of the 12th Conference on
Computational Linguistics - Volume 2,
COLING ?88, pages 431?436, Budapest.
Mitchell, Jeff and Mirella Lapata. 2008.
Vector-based models of semantic
composition. In Proceedings of ACL,
pages 236?244, Columbus, OH.
Murphy, Gregory. 1996. On metaphoric
representation. Cognition, 60:173?204.
Narayanan, Srini. 1997. Knowledge-based
Action Representations for Metaphor and
Aspect (KARMA). Ph.D. thesis, University
of California at Berkeley.
Narayanan, Srini. 1999. Moving right along:
A computational model of metaphoric
reasoning about events. In Proceedings of
AAAI 99, pages 121?128, Orlando, FL.
Nunberg, Geoffrey. 1987. Poetic and prosaic
metaphors. In Proceedings of the 1987
Workshop on Theoretical Issues in Natural
Language Processing, pages 198?201,
Stroudsburg, PA.
O? Se?aghdha, Diarmuid. 2010. Latent
variable models of selectional preference.
In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics,
pages 435?444, Uppsala.
Orwell, George. 1946. Politics and the
English Language. Horizon, 13(76):252?265.
Pantel, Patrick and Dekang Lin. 2002.
Discovering word senses from text. In
Proceedings of the Eighth ACM SIGKDD
International Conference on Knowledge
Discovery and Data Mining, pages 613?619,
Edmonton.
Pereira, Fernando, Naftali Tishby, and
Lillian Lee. 1993. Distributional clustering
of English words. In Proceedings of ACL-93,
pages 183?190, Morristown, NJ.
Peters, Wim and Ivonne Peters. 2000.
Lexicalised systematic polysemy in
Wordnet. In Proceedings of LREC 2000,
Athens.
Pinker, Stephen. 2007. The Stuff of Thought:
Language as a Window into Human Nature.
Viking Adult, New York, NY.
Power, Richard and Donia Scott. 2005.
Automatic generation of large-scale
paraphrases. In Proceedings of IWP,
pages 73?79.
Pragglejaz Group. 2007. MIP: A method
for identifying metaphorically used
words in discourse.Metaphor and Symbol,
22:1?39.
Preiss, Judita, Ted Briscoe, and Anna
Korhonen. 2007. A system for large-scale
acquisition of verbal, nominal and
adjectival subcategorization frames from
corpora. In Proceedings of ACL-2007,
volume 45, page 912, Prague.
Preiss, Judita, Andrew Coonce, and
Brittany Baker. 2009. HMMs, GRs, and
n-grams as lexical substitution techniques:
are they portable to other languages?
In Proceedings of the Workshop on Natural
Language Processing Methods and Corpora
in Translation, Lexicography, and Language
Learning, MCTLLL ?09, pages 21?27,
Borovets.
Pucci, Dario, Marco Baroni, Franco Cutugno,
and Alessandro Lenci. 2009. Unsupervised
lexical substitution with a word space
model. In Proceedings of the EVALITA
Workshop, 11th Congress of Italian
Association for Artificial Intelligence,
Reggie Emilia.
Pustejovsky, James. 1995. The Generative
Lexicon.MIT Press, Cambridge, MA.
Quirk, Chris, Chris Brockett, and William
Dolan. 2004. Monolingual machine
translation for paraphrase generation.
In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language
Processing, pages 142?149, Barcelona.
Reining, Astrid and Birte Lo?nneker-Rodman.
2007. Corpus-driven metaphor harvesting.
In Proceedings of the HLT/NAACL-07
Workshop on Computational Approaches
to Figurative Language, pages 5?12,
Rochester, NY.
Reisinger, Joseph and Raymond Mooney.
2010. A mixture model with sharing for
lexical semantics. In Proceedings of the
2010 Conference on Empirical Methods in
Natural Language Processing, EMNLP ?10,
pages 1173?1182, Cambridge, MA.
Resnik, Philip. 1993. Selection and
Information: A Class-based Approach to
Lexical Relationships. Ph.D. thesis,
University of Pennsylvania.
Rooth, Mats, Stefan Riezler, Detlef Prescher,
Glenn Carroll, and Franz Beil. 1999.
Inducing a semantically annotated lexicon
via EM-based clustering. In Proceedings
of ACL 99, pages 104?111, Maryland.
Rudolph, Sebastian and Eugenie Giesbrecht.
2010. Compositional matrix-space
352
Shutova, Teufel, and Korhonen Statistical Metaphor Processing
models of language. In Proceedings of
the 48th Annual Meeting of the Association for
Computational Linguistics, pages 907?916,
Uppsala.
Schulte im Walde, Sabine. 2006. Experiments
on the automatic induction of German
semantic verb classes. Computational
Linguistics, 32(2):159?194.
Sekine, Satoshi, Kentaro Inui, Ido Dagan,
Bill Dolan, Danilo Giampiccolo, and
Bernardo Magnini, editors. 2007.
Proceedings of the ACL-PASCAL Workshop
on Textual Entailment and Paraphrasing.
Prague.
Shalizi, Cosma. 2003. Analogy and metaphor.
Available at http://masi.cscs.lsa.
umich.edu/?crshalizi/notabene.
Shinyama, Yusuke and Satoshi Sekine. 2003.
Paraphrase acquisition for information
extraction. In Proceedings of the Second
International Workshop on Paraphrasing -
Volume 16, PARAPHRASE ?03,
pages 65?71, Sapporo.
Shutova, Ekaterina. 2010. Automatic
metaphor interpretation as a paraphrasing
task. In Proceedings of NAACL 2010,
pages 1029?1037, Los Angeles, CA.
Shutova, Ekaterina, Lin Sun, and Anna
Korhonen. 2010. Metaphor identification
using verb and noun clustering.
In Proceedings of COLING 2010,
pages 1,002?1,010, Beijing.
Shutova, Ekaterina and Simone Teufel.
2010. Metaphor corpus annotated for
source?target domain mappings.
In Proceedings of LREC 2010,
pages 3,255?3,261, Malta.
Siegel, Sidney and N. John Castellan. 1988.
Nonparametric Statistics for the Behavioral
Sciences. McGraw-Hill Book Company,
New York, NY.
Sun, Lin and Anna Korhonen. 2009.
Improving verb clustering with
automatically acquired selectional
preferences. In Proceedings of
EMNLP 2009, pages 638?647, Singapore.
Sun, Lin and Anna Korhonen. 2011.
Hierarchical verb clustering using graph
factorization. In Proceedings of EMNLP,
pages 1,023?1,033, Edinburgh.
Toral, Antonio. 2009. The lexical substitution
task at EVALITA 2009. In Proceedings of
EVALITA Workshop, 11th Congress of Italian
Association for Artificial Intelligence,
Regio Emilia.
Tourangeau, Roger and Robert Sternberg.
1982. Understanding and appreciating
metaphors. Cognition, 11:203?244.
Van de Cruys, Tim, Thierry Poibeau, and
Anna Korhonen. 2011. Latent vector
weighting for word meaning in
context. In Proceedings of EMNLP,
pages 1,012?1,022, Edinburgh.
van Rijsbergen, Keith. 1979. Information
Retrieval, 2nd edition. Butterworths,
London.
Veale, Tony and Yanfen Hao. 2008.
A fluid knowledge representation for
understanding and generating creative
metaphors. In Proceedings of COLING 2008,
pages 945?952, Manchester.
Wilks, Yorick. 1975. A preferential
pattern-seeking semantics for natural
language inference. Artificial Intelligence,
6:53?74.
Wilks, Yorick. 1978. Making preferences more
active. Artificial Intelligence, 11(3):197?223.
Yu, K., S. Yu, and V. Tresp. 2006. Soft
clustering on graphs. NIPS,
pages 1553?1561, Vancouver.
Zapirain, Ben?at, Eneko Agirre, and Llu??s
Ma`rquez. 2009. Generalizing over lexical
features: selectional preferences for
semantic role classification. In Proceedings
of the ACL-IJCNLP 2009 Conference Short
Papers, pages 73?76, Singapore.
Zhao, S., H. Wang, T. Liu, and S. Li. 2008.
Pivot approach for extracting paraphrase
patterns from bilingual corpora.
In Proceedings of ACL-08:HLT,
pages 780?788, Columbus, OH.
Zhao, Shiqi, Xiang Lan, Ting Liu, and
Sheng Li. 2009. Application-driven
statistical paraphrase generation. In
Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the
4th International Joint Conference on
Natural Language Processing of the AFNLP:
Volume 2, ACL ?09, pages 834?842, Suntec.
Zhou, Liang, Chin-Yew Lin, Dragos Stefan
Munteanu, and Eduard H. Hovy. 2006.
PARAEVAL: Using paraphrases to
evaluate summaries automatically.
In Proceedings of HLT-NAACL,
pages 447?454, New York, NY.
Zong, Chengqing, Yujie Zhang, and
Kazuhide Yamamoto. 2001. Approach to
spoken Chinese paraphrasing based on
feature extraction. In Proceedings of NLPRS,
pages 551?556, Tokyo.
353

2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 597?601,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Context-Enhanced Citation Sentiment Detection
Awais Athar
University of Cambridge
Computer Laboratory
15 JJ Thomson Avenue
Cambridge, CB3 0FD, U.K.
awais.athar@cl.cam.ac.uk
Simone Teufel
University of Cambridge
Computer Laboratory
15 JJ Thomson Avenue
Cambridge, CB3 0FD, U.K.
simone.teufel@cl.cam.ac.uk
Abstract
Sentiment analysis of citations in scientific pa-
pers and articles is a new and interesting prob-
lem which can open up many exciting new ap-
plications in bibliographic search and biblio-
metrics. Current work on citation sentiment
detection focuses on only the citation sen-
tence. In this paper, we address the problem
of context-enhanced citation sentiment detec-
tion. We present a new citation sentiment cor-
pus which has been annotated to take the dom-
inant sentiment in the entire citation context
into account. We believe that this gold stan-
dard is closer to the truth than annotation that
looks only at the citation sentence itself. We
then explore the effect of context windows of
different lengths on the performance of a state-
of-the-art citation sentiment detection system
when using this context-enhanced gold stan-
dard definition.
1 Introduction
Sentiment analysis of citations in scientific papers
and articles is a new and interesting problem. It can
open up many exciting new applications in biblio-
graphic search and in bibliometrics, i.e., the auto-
matic evaluation of the influence and impact of in-
dividuals and journals via citations. Automatic de-
tection of citation sentiment can also be used as a
first step to scientific summarisation (Abu-Jbara and
Radev, 2011). Alternatively, it can help researchers
during search, e.g., by identifying problems with a
particular approach, or by helping to recognise un-
addressed issues and possible gaps in the current re-
search.
However, there is a problem with the expression
of sentiment in scientific text. Conventionally, the
writing style in scientific writing is meant to be ob-
jective. Any personal bias by authors has to be
hedged (Hyland, 1995). Negative sentiment is po-
litically particularly dangerous (Ziman, 1968), and
some authors have documented the strategy of pref-
acing the intended criticism by slightly disingenuous
praise (MacRoberts and MacRoberts, 1984). This
makes the problem of identifying such opinions par-
ticularly challenging. This non-local expression of
sentiment has been observed in other genres as well
(Wilson et al, 2009; Polanyi and Zaenen, 2006).
Figure 1: Example of anaphora in citations
A typical case is illustrated in Figure 1. While the
first sentence praises some aspects of the cited pa-
per, the remaining sentences list its shortcomings. It
is clear that criticism is the intended sentiment, but
597
if we define our gold standard only by looking at
the citation sentence, we lose a significant amount
of sentiment hidden in the text. Given that most ci-
tations are neutral (Spiegel-Rosing, 1977; Teufel et
al., 2006), this makes it ever more important to re-
cover what explicit sentiment there is from the con-
text of the citation.
However, the dominant assumption in current ci-
tation identification methods (Ritchie et al, 2008;
Radev et al, 2009) is that the sentiment present in
the citation sentence represents the true sentiment
of the author towards the cited paper. This is due
to the difficulty of determining the relevant context,
whereas it is substantially easier to identify the cita-
tion sentence. In our example above, however, such
an approach would lead to the wrong prediction of
praise or neutral sentiment.
In this paper, we address the problem of context-
enhanced citation sentiment detection. We present
a new citation sentiment corpus where each citation
has been annotated according to the dominant sen-
timent in the corresponding citation context. We
claim that this corpus is closer to the truth than an-
notation that considers only the citation sentence it-
self. We show that it increases citation sentiment
coverage, particularly for negative sentiment. Using
this gold standard, we explore the effect of assum-
ing context windows of different but fixed lengths
on the performance of a state-of-the-art citation sen-
timent detection system where the sentiment of ci-
tation is considered in the entire context of the ci-
tation and more than one single sentiment can be
assigned. Previous approaches neither detect cita-
tion sentiment and context simultaneously nor use
as large a corpus as we do.
2 Corpus Construction
We chose the dataset used by Athar (2011) compris-
ing 310 papers taken from the ACL Anthology (Bird
et al, 2008). The citation summary data from the
ACL Anthology Network1 (Radev et al, 2009) was
used. This dataset is rather large (8736 citations) and
since manual annotation of context for each citation
is a time consuming task, a subset of 20 papers were
selected corresponding to approximately 20% of the
original dataset.
1http://www.aclweb.org
We selected a four-class scheme for annotation.
Every sentence that is in a window of 4 sentences
of the citation and does not contain any direct or in-
direct mention of the citation was labelled as being
excluded (x). The window length was motivated by
recent research (Qazvinian and Radev, 2010) which
shows the best score for a four-sentence boundary
when detecting non-explicit citation. The rest of the
sentences were marked either positive (p), negative
(n) or objective/neutral (o).
A total of 1,741 citations were annotated. Al-
though this annotation was performed by the first
author only, we know from previous work that simi-
lar styles of annotation can achieve acceptable inter-
annotator agreement (Teufel et al, 2006). An exam-
ple annotation for Smadja (1993) is given in Figure
2, where the first column shows the line number and
the second one shows the class label.
Figure 2: Example annotation of a citation context.
To compare our work with Athar (2011), we also
applied a three-class annotation scheme. In this
method of annotation, we merge the citation context
into a single sentence. Since the context introduces
more than one sentiment per citation, we marked the
citation sentiment with the last sentiment mentioned
in the context window as this is pragmatically most
likely to be the real intention (MacRoberts and Mac-
Roberts, 1984).
As is evident from Table 1, including the 4 sen-
tence window around the citation more than dou-
bles the instances of subjective sentiment, and in the
case of negative sentiment, this proportion rises to 3.
In light of the overall sparsity of detectable citation
sentiment in a paper, and of the envisaged applica-
598
tions, this is a very positive result. The reason for
this effect is most likely ?sweetened criticism? ? au-
thors? strategic behaviour of softening the effect of
criticism among their peers (Hornsey et al, 2008).
Without Context With Context
o 87% 73%
n 5% 17%
p 8% 11%
Table 1: Distribution of classes.
3 Experiments and Results
We represent each citation as a feature set in a Sup-
port Vector Machine (SVM) (Cortes and Vapnik,
1995) framework and use n-grams of length 1 to 3
as well as dependency triplets as features. The de-
pendency triplets are constructed by merging the re-
lation, governor and dependent in a single string, for
instance, the relation nsubj(failed, method) is rep-
resented as nsubj failed method . This setup
has been shown to produce good results earlier as
well (Pang et al, 2002; Athar, 2011).
The first set of experiments focuses on simulta-
neous detection of sentiment and context sentences.
For this purpose, we use the four-class annotated
corpus described earlier. While the original anno-
tations were performed for a window of length 4,
we also experiment with asymmetrical windows of l
sentences preceding the citation and r sentences suc-
ceeding it. The detailed results are given in Table 2.
l r x o n p Fmacro Fmicro
0 0 - 1509 86 146 0.768 0.932
1 1 2823 1982 216 200 0.737 0.820
2 2 5984 2214 273 218 0.709 0.851
3 3 9170 2425 318 234 0.672 0.875
4 4 12385 2605 352 252 0.680 0.892
0 4 5963 2171 322 215 0.712 0.853
0 3 4380 2070 293 201 0.702 0.832
0 2 2817 1945 258 193 0.701 0.801
0 1 1280 1812 206 182 0.717 0.777
Table 2: Results for joint context and sentiment de-
tection.
Because of the skewed class distribution, we use
both the Fmacro and Fmicro scores with 10-fold
cross-validation. The baseline score, shown in bold,
is obtained with no context window and is compara-
ble to the results reported by Athar (2011). However,
we can observe that the F scores decrease as more
context is introduced. This may be attributed to the
increase in the vocabulary size of the n-grams and a
consequent reduction in the discriminating power of
the decision boundaries. These results show that the
task of jointly detecting sentiment and context is a
hard problem.
For our second set of experiments, we use the
three-class annotation scheme. We merge the text
of the sentences in the context windows as well as
their dependency triplets to obtain the features. The
results are reported in Table 3 with best results in
bold. Although these results are not better than the
context-less baseline, the reason might be data spar-
sity since existing work on citation sentiment analy-
sis uses more data (Athar, 2011).
l r Fmacro Fmicro
1 1 0.638 0.827
2 2 0.620 0.793
3 3 0.629 0.786
4 4 0.628 0.771
0 4 0.643 0.796
0 3 0.658 0.816
0 2 0.642 0.824
0 1 0.731 0.871
Table 3: Results using different context windows.
4 Related Work
While different schemes have been proposed for
annotating citations according to their function
(Spiegel-Rosing, 1977; Nanba and Okumura, 1999;
Garzone and Mercer, 2000), the only recent work on
citation sentiment detection using a relatively large
corpus is by Athar (2011). However, this work does
not handle citation context. Piao et al (2007) pro-
posed a system to attach sentiment information to
the citation links between biomedical papers by us-
ing existing semantic lexical resources.
A common approach for sentiment detection is to
use a labelled lexicon to score sentences (Hatzivas-
siloglou and McKeown, 1997; Turney, 2002; Yu and
Hatzivassiloglou, 2003). However, such approaches
599
have been found to be highly topic dependent (En-
gstro?m, 2004; Gamon and Aue, 2005; Blitzer et al,
2007).
Teufel et al (2006) worked on a 2,829 sentence ci-
tation corpus using a 12-class classification scheme.
Although they used context in their annotation, their
focus was on determining the author?s reason for cit-
ing a given paper. This task differs from citation sen-
timent, which is in a sense a ?lower level? of analy-
sis.
For implicit citation extraction, Kaplan et al
(2009) explore co-reference chains for citation ex-
traction using a combination of co-reference reso-
lution techniques. However, their corpus consists
of only 94 sentences of citations to 4 papers which
is likely to be too small to be representative. The
most relevant work is by Qazvinian and Radev
(2010) who extract only the non-explicit citations
for a given paper. They model each sentence as a
node in a graph and experiment with various win-
dow boundaries to create edges between neighbour-
ing nodes. However, their dataset consists of only 10
papers and their annotation scheme differs from our
four-class annotation as they do not deal with any
sentiment.
5 Conclusion
In this paper, we focus on automatic detection of
citation sentiment using the citation context. We
present a new corpus and show that ignoring the cita-
tion context would result in loss of a lot of sentiment,
specially criticism towards the cited paper. We also
report the results of the state-of-the-art citation sen-
timent detection systems on this corpus when using
this context-enhanced gold standard definition.
Future work directions may include improving
the detection algorithms by filtering the context sen-
tences more intelligently. For this purpose, exist-
ing work on coreference resolution (Lee et al, 2011)
may prove to be useful. Context features may also
be used for first filtering citations which have been
mentioned only in passing, and then applying con-
text based sentiment classification to the remaining
significant citations.
References
A. Abu-Jbara and D. Radev. 2011. Coherent citation-
based summarization of scientific papers. In Proc. of
ACL.
A. Athar. 2011. Sentiment analysis of citations using
sentence structure-based features. In Proc of ACL,
page 81.
S. Bird, R. Dale, B.J. Dorr, B. Gibson, M.T. Joseph, M.Y.
Kan, D. Lee, B. Powley, D.R. Radev, and Y.F. Tan.
2008. The acl anthology reference corpus: A ref-
erence dataset for bibliographic research in computa-
tional linguistics. In Proc. of LREC.
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies,
bollywood, boom-boxes and blenders: Domain adap-
tation for sentiment classification. In Proc. of ACL,
number 1.
C. Cortes and V. Vapnik. 1995. Support-vector networks.
Machine learning, 20(3):273?297.
C. Engstro?m. 2004. Topic dependence in sentiment clas-
sification. University of Cambridge.
M. Gamon and A. Aue. 2005. Automatic identification
of sentiment vocabulary: exploiting low association
with known sentiment terms. In Proc. of the ACL.
M. Garzone and R. Mercer. 2000. Towards an automated
citation classifier. Advances in Artificial Intelligence.
V. Hatzivassiloglou and K.R. McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Proc. of
ACL, page 181.
M.J. Hornsey, E. Robson, J. Smith, S. Esposo, and R.M.
Sutton. 2008. Sugaring the pill: Assessing rhetori-
cal strategies designed to minimize defensive reactions
to group criticism. Human Communication Research,
34(1):70?98.
K. Hyland. 1995. The Author in the Text: Hedging Sci-
entific Writing. Hong Kong papers in linguistics and
language teaching, 18:11.
D. Kaplan, R. Iida, and T. Tokunaga. 2009. Automatic
extraction of citation contexts for research paper sum-
marization: A coreference-chain based approach. In
Proc. of the 2009 Workshop on Text and Citation Anal-
ysis for Scholarly Digital Libraries.
H. Lee, Y. Peirsman, A. Chang, N. Chambers, M. Sur-
deanu, and D. Jurafsky. 2011. Stanford?s multi-pass
sieve coreference resolution system at the conll-2011
shared task. ACL HLT 2011.
M.H. MacRoberts and B.R. MacRoberts. 1984. The
negational reference: Or the art of dissembling. So-
cial Studies of Science, 14(1):91?94.
H. Nanba and M. Okumura. 1999. Towards multi-paper
summarization using reference information. In IJCAI,
volume 16, pages 926?931. Citeseer.
600
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learning
techniques. In Proc. of EMNLP.
S. Piao, S. Ananiadou, Y. Tsuruoka, Y. Sasaki, and J. Mc-
Naught. 2007. Mining opinion polarity relations of ci-
tations. In International Workshop on Computational
Semantics (IWCS). Citeseer.
L. Polanyi and A. Zaenen. 2006. Contextual valence
shifters. Computing attitude and affect in text: Theory
and applications, pages 1?10.
V. Qazvinian and D.R. Radev. 2010. Identifying non-
explicit citing sentences for citation-based summariza-
tion. In Proc. of ACL.
D.R. Radev, M.T. Joseph, B. Gibson, and P. Muthukrish-
nan. 2009. A Bibliometric and Network Analysis of
the field of Computational Linguistics. Journal of the
American Soc. for Info. Sci. and Tech.
A. Ritchie, S. Robertson, and S. Teufel. 2008. Com-
paring citation contexts for information retrieval. In
Proc. of ACM conference on Information and knowl-
edge management, pages 213?222. ACM.
I. Spiegel-Rosing. 1977. Science studies: Bibliometric
and content analysis. Social Studies of Science.
S. Teufel, A. Siddharthan, and D. Tidhar. 2006. Auto-
matic classification of citation function. In Proc. of
EMNLP, pages 103?110.
P.D. Turney. 2002. Thumbs up or thumbs down?: seman-
tic orientation applied to unsupervised classification of
reviews. In Proc. of ACL.
T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Rec-
ognizing contextual polarity: an exploration of fea-
tures for phrase-level sentiment analysis. Comp. Ling.,
35(3):399?433.
H. Yu and V. Hatzivassiloglou. 2003. Towards answering
opinion questions: Separating facts from opinions and
identifying the polarity of opinion sentences. In Proc.
of EMNLP, page 136.
J.M. Ziman. 1968. Public Knowledge: An essay con-
cerning the social dimension of science. Cambridge
Univ. Press, College Station, Texas.
601
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 18?26,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Detection of Implicit Citations for Sentiment Detection
Awais Athar Simone Teufel
Computer Laboratory, University of Cambridge
15 JJ Thomson Avenue, Cambridge CB3 0FD, UK
{awais.athar,simone.teufel}@cl.cam.ac.uk
Abstract
Sentiment analysis of citations in scientific pa-
pers is a new and interesting problem which
can open up many exciting new applications in
bibliometrics. Current research assumes that
using just the citation sentence is enough for
detecting sentiment. In this paper, we show
that this approach misses much of the exist-
ing sentiment. We present a new corpus in
which all mentions of a cited paper have been
annotated. We explore methods to automat-
ically identify these mentions and show that
the inclusion of implicit citations in citation
sentiment analysis improves the quality of the
overall sentiment assignment.
1 Introduction
The idea of using citations as a source of information
has been explored extensively in the field of biblio-
metrics, and more recently in the field of compu-
tational linguistics. State-of-the-art citations iden-
tification mechanisms focus either on detecting ex-
plicit citations i.e. those that consist of either the
author names and the year of publication or brack-
eted numbers only, or include a small sentence win-
dow around the explicit citation as input text (Coun-
cill et al, 2008; Radev et al, 2009; Ritchie et al,
2008). The assumption behind this approach is that
all related mentions of the paper would be concen-
trated in the immediate vicinity of the anchor text.
However, this assumption does not generally hold
true (Teufel, 2010; Sugiyama et al, 2010). The phe-
nomenon of trying to determine a citations?s cita-
tion context has a long tradition in library sciences
(O?Connor, 1982), and its connection with corefer-
ence has been duely noted (Kim et al, 2006; Kaplan
et al, 2009). Consider Figure 1, which illustrates a
typical case.
Figure 1: Example of the use of anaphora
While the first sentence cites the target paper ex-
plicitly using the name of the primary author along
with the year of publication of the paper, the re-
maining sentences mentioning the same paper ap-
pear after a gap and contain an indirect and implicit
reference to that paper. These mentions occur two
sentences after the formal citation in the form of
anaphoric it and the lexical hook METEOR. Most
current techniques, with the exception of Qazvinian
and Radev (2010), are not able to detect linguistic
mentions of citations in such forms. Ignoring such
mentions and examining only the sentences contain-
18
ing an explicit citation results in loss of information
about the cited paper. While this phenomenon is
problematic for applications like scientific summari-
sation (Abu-Jbara and Radev, 2011), it has a particu-
lar relevance for citation sentiment detection (Athar,
2011).
Citation sentiment detection is an attractive task.
Availability of citation polarity information can help
researchers in understanding the evolution of a field
on the basis of research papers and their critiques.
It can also help expert researchers who are in the
process of preparing opinion based summaries for
survey papers by providing them with motivations
behind as well as positive and negative comments
about different approaches (Qazvinian and Radev,
2008).
Current work on citation sentiment detection
works under the assumption that the sentiment
present in the citation sentence represents the true
sentiment of the author towards the cited paper
(Athar, 2011; Piao et al, 2007; Pham and Hoffmann,
2004). This assumption is so dominant because
current citation identification methods (Councill et
al., 2008; Ritchie et al, 2008; Radev et al, 2009)
can readily identify the citation sentence, whereas
it is much harder to determine the relevant context.
However, this assumption most certainly does not
hold true when the citation context spans more than
one sentence.
Concerning the sentiment aspect of the citation
context from Figure 1, we see that the citation sen-
tence does not contain any sentiment towards the
cited paper, whereas the following sentences act as
a critique and list its shortcomings. It is clear that
criticism is the intended sentiment, but if the gold
standard is defined by looking at the citation sen-
tence in isolation, a significant amount of sentiment
expressed in the text is lost. Given that overall most
citations in a text are neutral with respect to sen-
timent (Spiegel-Rosing, 1977; Teufel et al, 2006),
this makes it even more important to recover what
explicit sentiment there is in the article, wherever it
is to be found.
In this paper, we examine methods to extract all
opinionated sentences from research papers which
mention a given paper in as many forms as we can
identify, not just as explicit citations. We present
a new corpus in which all mentions of a cited paper
have been manually annotated, and show that our an-
notation treatment increases citation sentiment cov-
erage, particularly for negative sentiment. We then
explore methods to automatically identify all men-
tions of a paper in a supervised manner. In par-
ticular, we consider the recognition of named ap-
proaches and acronyms. Our overall system then
classifies explicit and implicit mentions according to
sentiment. The results support the claim that includ-
ing implicit citations in citation sentiment analysis
improves the quality of the overall sentiment assign-
ment.
2 Corpus Construction
We use the dataset from Athar (2011) as our starting
point, which consists of 8,736 citations in the ACL
Anthology (Bird et al, 2008) that cite a target set of
310 ACL Anthology papers. The citation summary
data from the ACL Anthology Network1 (Radev et
al., 2009) is used. This dataset is rather large, and
since manual annotation of context for each citation
is a time consuming task, a subset of 20 target pa-
pers (i.e., all citations to these) has been selected
for annotation. These 20 papers correspond to ap-
proximately 20% of incoming citations in the orig-
inal dataset. They contain a total of 1,555 citations
from 854 citing papers.
2.1 Annotation
We use a four-class scheme for annotation. Every
sentence which does not contain any direct or indi-
rect mention of the citation is labelled as being ex-
cluded (x) from the context. The rest of the sen-
tences are marked either positive (p), negative (n)
or objective/neutral (o). To speed up the annotation
process, we developed a customised annotation tool.
A total of 203,803 sentences have been annotated
from 1,034 paper?reference pairs. Although this an-
notation been performed by the first author only,
we know from previous work that similar styles
of annotation can achieve acceptable inter-annotator
agreement (Teufel et al, 2006). An example anno-
tation is given in Figure 2, where the first column
shows the line number and the second one shows
the class label for the citation to Smadja (1993). It
should be noted that since annotation is always per-
1http://www.aclweb.org
19
formed for a specific citation only, sentences such as
the one at line 32, which carry sentiment but refer to
a different citation, are marked as excluded from the
context.
If there are multiple sentiments in the same sen-
tence, the sentence has been labelled with the class
of the last sentiment mentioned. In this way, a total
of 3,760 citation sentences have been found in the
whole corpus, i.e. sentences belonging to class o, n
or p, and the rest have been labelled as x. Table 1
compares the number of sentences with only the ex-
plicit citations with all explicit and implicit mentions
of those citations. We can see that including the
citation context increases the subjective sentiment
by almost 185%. The resulting negative sentiment
also increases by more than 325%. This may be at-
tributed to the strategic behaviour of the authors of
?sweetening? the criticism in order to soften its ef-
fects among their peers (Hornsey et al, 2008).
Figure 2: Example annotation of a citation context.
Explicit mentions All mentions
o 1, 509 3, 100
n 86 368
p 146 292
Table 1: Distribution of classes.
Another view of the annotated data is available in
Figure 3a. This is in the form of interactive HTML
where each HTML page represents all the incoming
links to a paper. Each row represents the citing pa-
per and each column square represents a sentence.
The rows are sorted by increasing publication date.
Black squares are citations with the author name and
year of publication mentioned in the text. The red,
green and gray squares show negative, positive and
neutral sentiment respectively. Pointing the mouse
cursor at any square gives the text content of the cor-
responding sentence, as shown in the Figure 3a.
The ACL Id, paper title and authors? names are
also given at the top of the page. Similar data for the
corresponding citing paper is made available when
the mouse cursor is positioned on one of the orange
squares at the start of each row, as shown in the Fig-
ure 3b. Clicking on the checkboxes at the top hides
or shows the corresponding type of squares. There is
also an option to hide/show a grid so that the squares
are separated and rows are easier to trace. For ex-
ample, Figure 3b shows the grid with the neutral or
objective citations hidden.
In the next section, we describe the features set we
use to detect implicit citations from this annotated
corpus and discuss the results.
3 Experiments and Results
For the task of detecting all mentions of a citation,
we merge the class labels of sentences mentioning a
citation in any form (o n p). To make sure that the
easily detectable explicit citations do not influence
the results, we change the class label of all those
sentences to x which contain the first author?s name
within a 4-word window of the year of publication.
Our dataset is skewed as there are many more ob-
jective sentences than subjective ones. In such sce-
narios, average micro-F scores tend to be slightly
higher as they are a weighted measure. To avoid
this bias, we also report the macro-F scores. Fur-
thermore, to ensure there is enough data for training
each class, we use 10-fold cross-validation (Lewis,
1991) in all our experiments.
We represent each citation as a feature set in a
Support Vector Machine (SVM) (Cortes and Vapnik,
1995) framework. The corpus is processed using
WEKA (Hall et al, 2008) and the Weka LibSVM
library (EL-Manzalawy and Honavar, 2005; Chang
and Lin, 2001). For each ith sentence Si, we use the
following binary features.
? Si?1 contains the last name of the primary au-
thor, followed by the year of publication within
a four-word window.
20
(a) Sentence Text (b) Paper metadata
Figure 3: Different views of an annotated paper.
This feature is meant to capture the fact that
the sentence immediately after an explicit cita-
tion is more likely to continue talking about the
same work.
? Si contains the last name of the primary au-
thor followed by the year of publication within
a four-word window.
This feature should help in identifying sen-
tences containing explicit citations. Since such
sentences are easier to identify, including them
in the evaluation metric would result in a false
boost in the final score. We have thus excluded
all such sentences in our annotation and this
feature should indicate a negative instance to
the classifier.
? Si contains the last name of the primary au-
thor.
This feature captures sentences which contain
a reference to tools and algorithms which have
been named after their inventors, such as,
?One possible direction for future work is to
compare the search-based approach of Collins
and Roark with our DP-based approach.?
It should also capture the mentions of methods
and techniques used in the cited paper e.g.,
?We show that our approach outperforms Tur-
ney?s approach.?
? Si contains an acronym used in an explicit ci-
tation.
Acronyms are taken to be capitalised words
which are extracted from the vicinity of the
cited author?s last name using regular expres-
sions. For example, METEOR in Figure 1 is an
acronym which is used in place of a formal ci-
tation to refer to the original paper in the rest of
the citing paper.
? Si contains a determiner followed by a work
noun.
We use the following determiners D = {the,
this, that, those, these, his, her, their, such, pre-
vious, other}. The list of work nouns (tech-
nique, method, etc.) has been taken from Teufel
(2010). This feature extracts a pattern which
has been found to be useful for extracting cita-
tions in previous work (Qazvinian and Radev,
2010). Such phrases usually signal a continua-
tion of the topics related to citations in earlier
sentences. For example:
?Church et al(1989), Wettler & Rapp (1989)
and Church & Hanks (1990) describe algo-
rithms which do this. However, the validity of
these algorithms has not been tested by system-
atic comparisons with associations of human
subjects.?
? Si starts with a third person pronoun.
The feature also tries to capture the topic con-
tinuation after a citation. Sentences starting
with a pronoun (e.g. they, their, he, she, etc.)
are more likely to describe the subject citation
of the previous sentence in detail. For example:
21
?Because Daume III (2007) views the adapta-
tion as merely augmenting the feature space,
each of his features has the same prior mean
and variance, regardless of whether it is do-
main specific or independent. He could have
set these parameters differently, but he did not.?
? Si starts with a connector.
This feature also focuses on detecting the topic
continuity. Connectors have been shown to
be effective in other context related works as
well (Hatzivassiloglou and McKeown, 1997;
Polanyi and Zaenen, 2006). A list of 23 con-
nectors (e.g. however, although, moreover, etc.)
has been compiled by examining the high fre-
quency connectors from a separate set of papers
from the same domain. An example is:
?An additional consistent edge of a linear-
chain conditional random field (CRF) explicitly
models the dependencies between distant oc-
currences of similar words (Sutton and McCal-
lum, 2004; Finkel et al , 2005). However, this
approach requires additional time complexity
in inference/learning time and it is only suit-
able for representing constraints by enforcing
label consistency.?
? Si starts with a (sub)section heading.
? Si?1 starts with a (sub)section heading.
? Si+1 starts with a (sub)section heading.
The three features above are a consequence of
missing information about the paragraph and
section boundaries in the used corpus. Since
the text extraction has been done automatically,
the section headings are usually found to be
merged with the text of the succeeding sen-
tence. For example, the text below merges the
heading of section 4.2 with the next sentence.
?4.2 METEOR vs. SIA SIA is designed to take
the advantage of loose sequence-based metrics
without losing word-level information.?
Start and end of such section boundaries can
give us important information about the scope
of a citation. In order to exploit this informa-
tion, we use regular expressions to detect if the
sentences under review contains these merged
section titles and headings.
? Si contains a citation other than the one under
review.
It is more probable for the context of a citation
to end when other citations are mentioned in
a sentence, which is the motivation behind us-
ing this feature, which might contribute to the
discriminating power of the classifier in con-
junction with the presence of a citation in the
previous sentence. For example, in the extract
below, the scope of the first citation is limited
to the first sentence only.
?Blitzer et al(2006) proposed a structural
correspondence learning method for domain
adaptation and applied it to part-of-speech tag-
ging. Daume III (2007) proposed a simple fea-
ture augmentation method to achieve domain
adaptation.?
? Si contains a lexical hook.
The lexical hooks feature identifies lexical sub-
stitutes for the citations. We obtain these hooks
by examining all explicit citation sentences to
the cited paper and selecting the most frequent
capitalized phrase in the vicinity of the author?s
last name. The explicit citations come from all
citing papers and not just the paper for which
the features are being determined. For exam-
ple, the sentences below have been taken from
two different papers and cite the same target pa-
per (Cutting et al, 1992). While the acronym
HMM will be captured by the feature stated ear-
lier, the word Xerox will be missed.
E95-1014: ?This text was part-of-speech
tagged using the Xerox HMM tagger (Cutting
et al , 1992).?
J97-3003: ?The Xerox tagger (Cutting et al
1992) comes with a set of rules that assign an
unknown word a set of possible pos-tags (i.e. ,
POS-class) on the basis of its ending segment.?
This ?domain level? feature makes it possible
to extract the commonly used name for a tech-
nique which may have been missed by the
acronym feature due to long term dependen-
cies. We also extrapolate the acronym for such
22
phrases, e.g., in the example below, SCL would
also be checked along with Structural Corre-
spondence Learning.
?The paper compares Structural Correspon-
dence Learning (Blitzer et al, 2006) with (var-
ious instances of) self-training (Abney, 2007;
McClosky et al, 2006) for the adaptation of a
parse selection model to Wikipedia domains?
We also add n-grams of length 1 to 3 to this lexi-
cal feature set and compare the results obtained with
an n-gram only baseline in Table 2. N-grams have
been shown to perform consistently well in various
NLP tasks (Bergsma et al, 2010).
Class Baseline Our System
x 0.995 0.996
o n p 0.358 0.513
Avg. 0.990 0.992
Avg.(macro) 0.677 0.754
Table 2: Comparison of F -scores for non-explicit
citation detection.
By adding the new features listed above, the per-
formance of our system increases by almost 8% over
the n-gram baseline for the task of detecting citation
mentions. Using the pairwise Wilcoxon rank-sum
test at 0.05 significance level, we found that the dif-
ference between the baseline and our system is sta-
tistically significant2. While the micro-F score ob-
tained is quite high, the individual class scores show
that the task is hard and a better solution may require
a deeper analysis of the context.
4 Impact on Citation Sentiment Detection
We explore the effect of this context on citation sen-
timent detection. For a baseline, we use features of
the state-of-the-art system proposed in our earlier
work (Athar, 2011). While there we used n-gram
and dependency feature on sentences containing ex-
plicit citations only, our annotation is not restricted
to such citations and we may have more than one
2While this test may not be adequate as the data is highly
skewed, we are reporting the results since there is no obvious
alternative for discrete skewed data. In future, we plan to use
the continuous probability estimates produced by the classifier
for testing significance.
sentiment per each explicit citation. For example,
in Figure 2, our 2011 system will be restricted to
analysing sentence 33 only. However, it is clear
from our annotation that there is more sentiment
present in the succeeding sentences which belongs
to this explicit citation. While sentence 34 in Fig-
ure 2 is positive towards the cited paper, the next
sentence criticises it. Thus for this explicit citation,
there are three sentences with sentiment and all of
them are related to the same explicit citation. Treat-
ing these sentences separately will result in an artifi-
cial increase in the amount of data because they par-
ticipate in the same discourse. It would also make
it impossible to compare the sentiment annotated in
the previous work with our annotation.
To make sure the annotations are comparable,
we mark the true citation sentiment to be the last
sentiment mentioned in a 4-sentence context win-
dow, as this is pragmatically most likely to be the
real intention (MacRoberts and MacRoberts, 1984).
The window length is motivated by recent research
(Qazvinian and Radev, 2010) which favours a four-
sentence boundary for detecting non-explicit cita-
tions. Analysis of our data shows that more than
60% of the subjective citations lie in this window.
We include the implicit citations predicted by the
method described in the previous section in the con-
text. The results of the single-sentence baseline sys-
tem are compared with this context enhanced system
in Table 3.
Class Baseline Our System
o 0.861 0.887
n 0.138 0.621
p 0.396 0.554
Avg. 0.689 0.807
Avg.(macro) 0.465 0.687
Table 3: F -scores for citation sentiment detection.
The results show that our system outperforms the
baseline in all evaluation criteria. Performing the
pairwise Wilcoxon rank-sum testat 0.05 significance
level, we found that the improvement is statistically
significant. The baseline system does not use any
context and thus misses out on all the sentiment
information contained within. While this window-
based representation does not capture all the senti-
23
ment towards a citation perfectly, it is closer to the
truth than a system based on single sentence analysis
and is able to detect more sentiment.
5 Related Work
While different schemes have been proposed for
annotating citations according to their function
(Spiegel-Rosing, 1977; Nanba and Okumura, 1999;
Garzone and Mercer, 2000), the only recent work on
citation sentiment detection using a relatively large
corpus is by Athar (2011). However, this work does
not handle citation context. Other approaches to ci-
tation classification include work by Wilbur et al
(2006), who annotated a 101 sentence corpus on
focus, polarity, certainty, evidence and directional-
ity. Piao et al (2007) proposed a system to attach
sentiment information to the citation links between
biomedical papers by using existing semantic lexical
resources and NLP tools.
A common approach for sentiment detection is to
use a labelled lexicon to score sentences (Hatzivas-
siloglou and McKeown, 1997; Turney, 2002; Yu and
Hatzivassiloglou, 2003). However, such approaches
have been found to be highly topic dependent (En-
gstro?m, 2004; Gamon and Aue, 2005; Blitzer et al,
2007), which makes the creation of a general senti-
ment classifier a difficult task.
Teufel et al (2006) worked on a 2,829 sentence ci-
tation corpus using a 12-class classification scheme.
While the authors did make use of the context in
their annotation, their focus was on the task of deter-
mining the author?s reason for citing a given paper.
This task differs from citation sentiment detection,
which is in a sense a ?lower level? of analysis.
Some other recent work has focused on the prob-
lem of implicit citation extraction (Kaplan et al,
2009; Qazvinian and Radev, 2010). Kaplan et al
(2009) explore co-reference chains for citation ex-
traction using a combination of co-reference resolu-
tion techniques (Soon et al, 2001; Ng and Cardie,
2002). However, the corpus that they use consists of
only 94 citations to 4 papers and is likely to be too
small to be representative.
For citation extraction, the most relevant work is
by Qazvinian and Radev (2010) who proposed a
framework of Markov Random Fields to extract only
the non-explicit citations for a given paper. They
model each sentence as a node in a graph and ex-
periment with various window boundaries to cre-
ate edges between neighbouring nodes weighted by
lexical similarity between nodes. However, their
dataset consists of only 569 citations from 10 pa-
pers and their annotation scheme deals with neither
acronyms nor sentiment.
6 Discussion
What is the role of citation contexts in the overall
structure of scientific context? We assume a hier-
archical, rhetorical structure not unlike RST (Mann
and Thompson, 1987), but much flatter, where the
atomic units are textual blocks which carry a cer-
tain functional role in the overall scientific argument
for publication (Teufel, 2010; Hyland, 2000). Under
such a general model, citation blocks are certainly
a functional unit, and their recognition is a reward-
ing task in their own right. If citation blocks can be
recognised along with their sentiment, this is even
more useful, as it restricts the possibilities for which
rhetorical function the segment plays. For instance,
in the motivation section of a paper, before the pa-
per contribution is introduced, we often find nega-
tive sentiment assigned to citations, as any indica-
tion can serve as a justification for the current paper.
In contrast, positive sentiment is more likely to be
restricted to the description of an approach which
the authors include in their solution, or further de-
velop.
Another aspect concerns which features might
help in detecting coherent citation blocks. We have
here addressed coherence of citation contexts via
certain referring expressions, lexical hooks and also
coherence-indicating conjunctions (amongst oth-
ers). The reintroduction of citation contexts was
addressed via lexical hooks. Much more could be
done to explore this very interesting question. A
more fine-grained model of coherence might include
proper anaphora resolution (Lee et al, 2011), which
is still an unsolved task for scientific texts, and also
include models of lexical coherence such as lexical
chains (Barzilay and Elhadad, 1997) and entity co-
herence (Barzilay and Lapata, 2008).
24
7 Conclusion
In this paper, we focus on automatic detection of ci-
tation sentiment using citation context. We annotate
a new large corpus and show that ignoring the cita-
tion context would result in loss of a lot of sentiment,
specially criticism. We also report the results of the
state-of-the-art citation sentiment detection systems
on this corpus and when using this context-enhanced
gold standard definition.
References
A. Abu-Jbara and D. Radev. 2011. Coherent citation-
based summarization of scientific papers. In Proc. of
ACL.
A. Athar. 2011. Sentiment analysis of citations using
sentence structure-based features. In Proc of ACL,
page 81.
Regina Barzilay and Michael Elhadad. 1997. Using
lexical chains for text summarization. In Inderjeet
Mani and Mark T. Maybury, editors, Proceedings of
the ACL/EACL-97 Workshop on Intelligent Scalable
Text Summarization.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, (1):1?34.
Shane Bergsma, Emily Pitler, and Dekang Lin. 2010.
Creating robust supervised classifiers via web-scale n-
gram data. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 865?874, Uppsala, Sweden, July. Association
for Computational Linguistics.
S. Bird, R. Dale, B.J. Dorr, B. Gibson, M.T. Joseph, M.Y.
Kan, D. Lee, B. Powley, D.R. Radev, and Y.F. Tan.
2008. The acl anthology reference corpus: A ref-
erence dataset for bibliographic research in computa-
tional linguistics. In Proc. of LREC.
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies,
bollywood, boom-boxes and blenders: Domain adap-
tation for sentiment classification. In Proc. of ACL,
number 1.
C.C. Chang and C.J. Lin. 2001. LIBSVM: a li-
brary for support vector machines, 2001. Software
available at http://www.csie.ntu.edu.tw/
cjlin/libsvm.
C. Cortes and V. Vapnik. 1995. Support-vector networks.
Machine learning, 20(3):273?297.
I.G. Councill, C.L. Giles, and M.Y. Kan. 2008. Parscit:
An open-source crf reference string parsing package.
In Proc. of LREC, volume 2008. Citeseer.
Y. EL-Manzalawy and V. Honavar, 2005. WLSVM:
Integrating LibSVM into Weka Environment. Soft-
ware available at http://www.cs.iastate.
edu/?yasser/wlsvm.
C. Engstro?m. 2004. Topic dependence in sentiment clas-
sification. Unpublished MPhil Dissertation. Univer-
sity of Cambridge.
M. Gamon and A. Aue. 2005. Automatic identifica-
tion of sentiment vocabulary: exploiting low associa-
tion with known sentiment terms. In Proc. of the ACL,
pages 57?64.
M. Garzone and R. Mercer. 2000. Towards an automated
citation classifier. Advances in Artificial Intelligence.
D. Hall, D. Jurafsky, and C.D. Manning. 2008. Studying
the history of ideas using topic models. In EMNLP,
pages 363?371.
V. Hatzivassiloglou and K.R. McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Proc. of
ACL, page 181.
M.J. Hornsey, E. Robson, J. Smith, S. Esposo, and R.M.
Sutton. 2008. Sugaring the pill: Assessing rhetori-
cal strategies designed to minimize defensive reactions
to group criticism. Human Communication Research,
34(1):70?98.
Ken Hyland. 2000. Disciplinary Discourses; Social In-
teraction in Academic Writing. Longman, Harlow.
D. Kaplan, R. Iida, and T. Tokunaga. 2009. Automatic
extraction of citation contexts for research paper sum-
marization: A coreference-chain based approach. In
Proc. of the 2009 Workshop on Text and Citation Anal-
ysis for Scholarly Digital Libraries.
D. Kim, P. Webber, et al 2006. Implicit references to
citations: A study of astronomy papers.
H. Lee, Y. Peirsman, A. Chang, N. Chambers, M. Sur-
deanu, and D. Jurafsky. 2011. Stanford?s multi-pass
sieve coreference resolution system at the conll-2011
shared task. ACL HLT 2011.
D.D. Lewis. 1991. Evaluating text categorization. In
Proc. of Speech and Natural Language Workshop,
pages 312?318.
M.H. MacRoberts and B.R. MacRoberts. 1984. The
negational reference: Or the art of dissembling. So-
cial Studies of Science, 14(1):91?94.
William C. Mann and Sandra A. Thompson. 1987.
Rhetorical Structure Theory: A theory of text organ-
isation. ISI/RS-87-190. Technical report, Information
Sciences Institute, University of Southern California,
Marina del Rey, CA.
H. Nanba and M. Okumura. 1999. Towards multi-paper
summarization using reference information. In IJCAI,
volume 16, pages 926?931. Citeseer.
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In Proc. of ACL,
pages 104?111.
25
J. O?Connor. 1982. Citing statements: Computer recog-
nition and use to improve retrieval. Information Pro-
cessing & Management, 18(3):125?131.
S.B. Pham and A. Hoffmann. 2004. Extracting positive
attributions from scientific papers. In Discovery Sci-
ence, pages 39?45. Springer.
S. Piao, S. Ananiadou, Y. Tsuruoka, Y. Sasaki, and J. Mc-
Naught. 2007. Mining opinion polarity relations of ci-
tations. In International Workshop on Computational
Semantics (IWCS). Citeseer.
L. Polanyi and A. Zaenen. 2006. Contextual valence
shifters. Computing attitude and affect in text: Theory
and applications, pages 1?10.
V. Qazvinian and D.R. Radev. 2008. Scientific paper
summarization using citation summary networks. In
Proceedings of the 22nd International Conference on
Computational Linguistics-Volume 1, pages 689?696.
Association for Computational Linguistics.
V. Qazvinian and D.R. Radev. 2010. Identifying non-
explicit citing sentences for citation-based summariza-
tion. In Proc. of ACL.
D.R. Radev, M.T. Joseph, B. Gibson, and P. Muthukrish-
nan. 2009. A Bibliometric and Network Analysis of
the field of Computational Linguistics. Journal of the
American Soc. for Info. Sci. and Tech.
A. Ritchie, S. Robertson, and S. Teufel. 2008. Com-
paring citation contexts for information retrieval. In
Proc. of ACM conference on Information and knowl-
edge management, pages 213?222. ACM.
W.M. Soon, H.T. Ng, and D.C.Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Comp. Ling., 27(4):521?544.
I. Spiegel-Rosing. 1977. Science studies: Bibliometric
and content analysis. Social Studies of Science.
K. Sugiyama, T. Kumar, M.Y. Kan, and R.C. Tripathi.
2010. Identifying citing sentences in research papers
using supervised learning. In Information Retrieval &
Knowledge Management,(CAMP), 2010 International
Conference on, pages 67?72. IEEE.
S. Teufel, A. Siddharthan, and D. Tidhar. 2006. Auto-
matic classification of citation function. In Proc. of
EMNLP, pages 103?110.
Simone Teufel. 2010. The Structure of Scientific Arti-
cles: Applications to Citation Indexing and Summa-
rization. Stanford: CSLI Publications.
P.D. Turney. 2002. Thumbs up or thumbs down?: seman-
tic orientation applied to unsupervised classification of
reviews. In Proc. of ACL.
W.J. Wilbur, A. Rzhetsky, and H. Shatkay. 2006. New
directions in biomedical text annotation: definitions,
guidelines and corpus construction. BMC bioinfor-
matics, 7(1):356.
H. Yu and V. Hatzivassiloglou. 2003. Towards answering
opinion questions: Separating facts from opinions and
identifying the polarity of opinion sentences. In Proc.
of EMNLP, page 136.
26
