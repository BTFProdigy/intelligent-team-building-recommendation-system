Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 52?57,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Meeting TempEval-2: Shallow Approach for Temporal Tagger  
 
 
Oleksandr Kolomiyets 
Katholieke Universiteit Leuven 
Department of Computer Science 
Celestijnenlaan 200A, Heverlee, Belgium 
oleksandr.kolomiyets 
@cs.kuleuven.be 
 
 
Marie-Francine Moens 
Katholieke Universiteit Leuven 
Department of Computer Science 
Celestijnenlaan 200A, Heverlee, Belgium 
sien.moens@cs.kuleuven.be 
 
 
 
 
 
 
Abstract 
Temporal expressions are one of the important 
structures in natural language. In order to un-
derstand text, temporal expressions have to be 
identified and normalized by providing ISO-
based values. In this paper we present a shal-
low approach for automatic recognition of 
temporal expressions based on a supervised 
machine learning approach trained on an an-
notated corpus for temporal information, 
namely TimeBank. Our experiments demon-
strate a performance level comparable to a 
rule-based implementation and achieve the 
scores of 0.872, 0.836 and 0.852 for precision, 
recall and F1-measure for the detection task 
respectively, and 0.866, 0.796, 0.828 when an 
exact match is required.   
1 Introduction 
The task of recognizing temporal expressions 
(sometimes also referred as time expressions or 
simply TIMEX) was first introduced in the Mes-
sage Understanding Conference (MUC) in 1995. 
Temporal expressions were treated as a part of the 
Named Entity Recognition (NER) task, in which 
capitalized tokens in text were labeled with one of 
the predefined semantic labels, such as Date, Time, 
Person, Organization, Location, Percentage, and 
Money. As the types of temporal entities identified 
in this way were too restricted and provided little 
further information, the Automated Content Ex-
traction (ACE) launched a competition campaign 
for Temporal Expression Recognition and Norma-
lization (TERN 2004). The tasks were to identify 
temporal expressions in free text and normalize 
them providing an ISO-based date-time value. Lat-
er evaluations of ACE in 2005, 2006 and 2007 un-
fortunately did not set new challenges for temporal 
expression recognition and thus the participation 
interest in this particular task decreased.  
TempEval-2 is a successor of TempEval-2007 
and will take place in 2010. The new evaluation 
initiative sets new challenges for temporal text 
analysis. While TempEval-2007 was solely fo-
cused on recognition of temporal links, the     
TempEval-2 tasks aim at an all-around temporal 
processing with separate evaluations for recogni-
tion of temporal expressions and events, for the 
estimation of temporal relations between events 
and times in the same sentence, between events 
and document creation time, between two events in 
consecutive sentences and between two events, 
where one of them syntactically dominates the oth-
er (Pustejovsky et al, 2009). These evaluations 
became possible with a new freely available corpus 
with annotated temporal information, TimeBank 
(Pustejovsky et al, 2003a), and an annotation 
schema, called TimeML (Pustejovsky et al, 
2003b).  
For us all the tasks of TempEval-2 seem to be 
interesting. In this paper we make the first step 
towards a comprehensive temporal analysis and 
address the problem of temporal expression recog-
nition as it is set in TempEval-2. Despite a number 
of previous implementations mainly done in the 
context of the ACE TERN competition, very few, 
52
and exclusively rule-based methods were reported 
for temporal taggers on TimeBank developed by 
using the TimeML annotation scheme. As a main 
result of the deep analysis of relevant work (Sec-
tion 2), we decided to employ a machine learning 
approach for constituent-based classifications with 
generic syntactic and lexical features. 
    The remainder of the paper is organized as fol-
lows: in Section 2 we provide the details of rele-
vant work done in this field along with corpora and 
annotations schemes used; Section 3 describes the 
approach; experimental setup, results and error 
analysis are provided in Section 4. Finally, Section 
5 gives an outlook for further improvements and 
research.  
2 Related Work 
For better understanding of the performance levels 
provided in the paper we first describe evaluation 
metrics defined for the temporal expression recog-
nition task and then the methods and datasets used 
in previous research.   
2.1 Evaluation metrics 
With the start of the ACE TERN competition in 
2004, two major evaluation conditions were pro-
posed: Recognition+Normalization (full task) and 
Recognition only (TERN, 2004). 
Detection (Recognition): Detection is a prelimi-
nary task towards the full TERN task, in which 
temporally relevant expressions have to be found. 
The scoring is very generous and implies a minim-
al overlap in the extent of the reference and the 
system output tags. As long as there is at least one 
overlapping character, the tags will be aligned. 
Any alignment of the system output tags are scored 
as a correct detection. 
Sloopy span: Spans usually refer to strict match of 
both boundaries (the extent) of a temporal expres-
sion (see Exact Match). ?Sloopy? admits recog-
nized temporal expressions as long as their right 
boundary is the same as in the corresponding 
TimeBank?s extents (Boguraev and Ando, 2005). 
The motivation was to assess the correctness of 
temporal expressions recognized in TimeBank, 
which was reported as inconsistent with respect to 
some left boundary items, such as determiners and 
pre-determiners.   
Exact Match (Bracketing or Extent Recogni-
tion): Exact match measures the ability to correct-
ly identify the extent of the TIMEX. The extent of 
the reference and the system output tags must 
match exactly the system output tag to be scored as 
correct.  
2.2 Datasets  
To date, there are two annotated corpora used for 
temporal evaluations, the ACE TERN corpus and 
TimeBank (Pustejovsky et al, 2003a). In this sec-
tion we provide a brief description of the temporal 
corpora and annotation standards, which can sub-
stantially influence recognition results.  
Most of the implementations referred as the 
state-of-the-art were developed in the scope of the 
ACE TERN 2004. For evaluations, a training cor-
pus of 862 documents with about 306 thousand 
words was provided. Each document represents a 
news article formatted in XML, in which TIMEX2 
tags denote temporal expressions. The total num-
ber of temporal expressions for training is 8047 
TIMEX2 tags with an average of 10.5 per docu-
ment. The test set comprises 192 documents with 
1828 TIMEX2 tags (Ferro, 2004).  
The annotation of temporal expressions in the 
ACE corpus was done with respect to the TIDES 
annotation guidelines (Ferro et al, 2003). The 
TIDES standard specifies so-called markable ex-
pressions, whose syntactic head must be an appro-
priate lexical trigger, e.g. ?minute?, ?afternoon?, 
?Monday?, ?8:00?, ?future? etc. When tagged, the 
full extent of the tag must correspond to one of the 
grammatical categories: nouns (NN, NNP), noun 
phrases (NP), adjectives (JJ), adjective phrases 
(ADJP), adverbs (RB) and adverb phrases 
(ADVP). According to this, all pre- and postmo-
difiers as well as dependent clauses are also in-
cluded to the TIMEX2 extent, e.g. ?five days after 
he came back?, ?nearly four decades of expe-
rience?. Such a broad extent for annotations is of 
course necessary for correct normalization, but on 
the other hand, introduces difficulties for exact 
match. Another important characteristic of the 
TIDES standard are the nested temporal expres-
sions as for example: 
 
<TIMEX2>The<TIMEX2 VAL = "1994">1994 
</TIMEX2> baseball season </TIMEX2> 
 
53
The most recent annotation language for tem-
poral expressions, TimeML (Pustejovsky et al, 
2003b), with an underlying corpus TimeBank 
(Pustejovsky et al, 2003a), opens up new possibili-
ties for processing temporal information in text. 
Besides the specification for temporal expressions, 
i.e. TIMEX3, which is to a large extent inherited 
from TIDES, TimeML provides a means to capture 
temporal semantics by annotations with suitably 
defined attributes for fine-grained specification of 
analytical detail (Boguraev et al, 2007). The anno-
tation schema establishes new entity and relation 
marking tags along with numerous attributes for 
them. This advancement influenced the extent for 
event-based temporal expression, in which depen-
dent clauses are no longer included into TIMEX3 
tags. The TimeBank corpus includes 186 docu-
ments with 68.5 thousand words and 1423 
TIMEX3 tags.       
2.3 Approaches for temporal processing  
As for any recognition problem, there are two ma-
jor ways to solve it. Historically, rule-based sys-
tems were first implemented. Such systems are 
characterized by a great human effort in data anal-
ysis and rule writing. With a high precision such 
systems can be successfully employed for recogni-
tion of temporal expressions, whereas the recall 
reflects the effort put into the rule development. By 
contrast, machine learning methods require an an-
notated training set, and with a decent feature de-
sign and a minimal human effort can provide 
comparable or even better results than rule-based 
implementations. As the temporal expression rec-
ognition is not only about to detect them but also to 
provide an exact match, machine learning ap-
proaches can be divided into token-by-token classi-
fication following B(egin)-I(nside)-O(utside) 
encoding and binary constituent-based classifica-
tion, in which an entire chunk-phrase is under con-
sideration to be classified as a temporal expression 
or not. In this case, exact segmentation is the re-
sponsibility of the chunker or the parser used.  
Rule-based systems: One of the first well-known 
implementations of temporal taggers was presented 
in (Many and Wilson, 2000). The approach relies 
on a set of hand-crafted and machine-discovered 
rules, which are based upon shallow lexical fea-
tures. On average the system achieved a value of 
83.2% for F1-measure against hand-annotated da-
ta. The dataset used comprised a set of 22 New 
York Times articles and 199 transcripts of Voice of 
America taken from the TDT2 collection (Graff et 
al., 1999). It should be noted that the reported per-
formance was provided in terms of an exact match. 
Another example of rule-based temporal taggers is 
Chronos described in (Negri and Marseglia, 2004), 
which achieved the highest scores (F1-measure) in 
the TERN 2004 of 0.926 and 0.878 for recognition 
and exact match.  
Recognition of temporal expressions using 
TimeBank as an annotated corpus, is reported in 
(Boguraev and Ando, 2005) based on a cascaded 
finite-state grammar (500 stages and 16000 transi-
tions). A complex approach achieved an F1-
measure value of 0.817 for exact match and 0.896 
for detecting ?sloopy? spans.  Another known im-
plementation for TimeBank is an adaptation of 
(Mani and Wilson, 2000) from TIMEX2 to 
TIMEX3 with no reported performance level. 
Machine learning recognition systems: Success-
ful machine learning TIMEX recognition systems 
are described in (Ahn et al, 2005; Hacioglu et al, 
2005; Poveda et al, 2007). Proposed approaches 
made use of a token-by-token classification for 
temporal expressions represented by B-I-O encod-
ing with a set of lexical and syntactic features, e.g., 
token itself, part-of-speech tag, label in the chunk 
phrase and the same features for each token in the 
context window. The performance levels are pre-
sented in Table 1. All the results were obtained on 
the ACE TERN dataset.  
Approach F1 (detection) F1 (exact match) 
Ahn et al, 2005 0.914 0.798 
Hacioglu et al, 
2005 0.935 0.878 
Poveda et al, 
2007 0.986 0.757 
 
Table 1. Performance of Machine Learning Ap-
proaches with B-I-O Encoding 
 
Constituent-based classification approach for 
temporal expression recognition was presented in 
(Ahn et al, 2007). By comparing to the previous 
work (Ahn et al, 2005) on the same ACE TERN 
dataset, the method demonstrates a slight decrease 
in detection with F1-measure of 0.844 and a nearly 
equivalent F1-measure value for exact match of 
0.787.   
54
The major characteristic of machine learning 
approaches was a simple system design with a mi-
nimal human effort. Machine-learning based rec-
ognition systems have proven to have a 
comparable recognition performance level to state-
of-the-art rule-based detectors.  
3 Approach 
The approach we describe in this section employs a 
machine-learning technique and more specifically 
a binary constituent based classification. In this 
case the entire phrase is under consideration to be 
labeled as a TIMEX or not. We restrict the classifi-
cation for the following phrase types and grammat-
ical categories: NN, NNP, CD, NP, JJ, ADJP, RB, 
ADVP and PP. In order to make it possible, for 
each sentence we parse the initial input line with a 
Maximum Entropy parser (Ratnaparkhi, 1998) and 
extract all phrase candidates with respect the types 
defined above. Each phrase candidate is examined 
against the manual annotations for temporal ex-
pressions found in the sentence. Those phrases, 
which correspond to the temporal expressions in 
the sentence are taken as positive examples, while 
the rest are considered as negative ones. Only one 
sub-tree from a parse is marked as positive for a 
distinct TIMEX at once.  After that, for each can-
didate we produce a feature vector, which includes 
the following features: head phrase, head word, 
part-of-speech for head word, character type and 
character type pattern for head word as well as for 
the entire phrase. Character type and character type 
pattern1 features are implemented following Ahn et 
al. (2005). The patterns are defined by using the 
symbols X, x and 9. X and x are used for character 
type as well as for character type patterns for 
representing capital and lower-case letters for a 
token. 9 is used for representing numeric tokens. 
Once the character types are computed, the corres-
ponding character patterns are produced. A pattern 
consists of the same symbols as character types, 
and contains no sequential redundant occurrences 
of the same symbol. For example, the constituent 
?January 30th? has character type ?Xxxxxxx 
99xx? and pattern ?X(x) (9)(x)?.  
On this basis, we employ a classifier that im-
plements a Maximum Entropy model2 and per-
                                                          
1
 In literature such patterns are also known as shorttypes. 
2
 http://maxent.sourceforge.net/ 
forms categorization of constituent-phrases ex-
tracted from the input.  
4 Experiments, Results and Error Analy-
sis 
After processing the TimeBank corpus of 183 
documents we had 2612 parsed sentences with 
1224 temporal expressions in them. 2612 sentences 
resulted in 49656 phrase candidates. We separated 
the data in order to perform 10-fold cross valida-
tion, train the classifier and test it on an unseen 
dataset. The evaluations were conducted with re-
spect to the TERN 2004 evaluation plan (TERN, 
2004) and described in Section 2.1.    
After running experiments the classifier demon-
strated the performance in detection of TIMEX3 
tags with a minimal overlap of one character with 
precision, recall and F1-measure at 0.872, 0.836 
and 0.852 respectively. Since the candidate phrases 
provided by the parser do not always exactly align 
annotated temporal expressions, the results for the 
exact match experiments are constrained by an es-
timated upper-bound recall of 0.919. The experi-
ments on exact match demonstrated a small decline 
of performance level and received scores of 0.866, 
0.796 and 0.828 for precision, recall and F1-
measure respectively.  
Putting the received figures in context, we can 
say that with a very few shallow features and a 
standard machine learning algorithm the recogniz-
er of temporal expressions performed at a compa-
rable operational level to the rule-based approach 
of (Boguraev and Ando, 2005) and outperformed it 
in exact match. A comparative performance sum-
mary is presented in Table 2.  
Sometimes it is very hard even for humans to 
identify the use of obvious temporal triggers in a 
specific context. As a result, many occurrences of 
such triggers remained unannotated for which 
TIMEX3 identification could not be properly car-
ried out.   Apart of obvious incorrect parses, in-
exact alignment between temporal expressions and 
candidate phrases was caused by annotations that 
occurred at the middle of a phrase, for example 
?eight-years-long?, ?overnight?, ?yesterday?s?. In 
total there are 99 TIMEX3 tags (or 8.1%) misa-
ligned with the parser output, which resulted in 53 
(or 4.3%) undetected TIMEX3s. 
 
55
 P R F1 
Detection 
Our approach 0.872 0.836 0.852 
Sloopy Span 
(Boguraev and 
Ando, 2005) 0.852 0.952 0.896 
Exact Match 
Our approach 0.866 0.796 0.828 
(Boguraev and 
Ando, 2005) 0.776 0.861 0.817 
 
Table 2. Comparative Performance Summary 
 
Definite and indefinite articles are unsystemati-
cally left out or included into TIMEX3 extent, 
which may introduce an additional bias in classifi-
cation.    
5 Conclusion and Future Work 
In this paper we presented a machine learning 
approach for detecting temporal expression using a 
recent annotated corpus for temporal information, 
TimeBank. Employing shallow syntactic and lexi-
cal features, the performance level of the method 
achieved comparable results to a rule-based ap-
proach of Boguraev and Ando (2005) and for the 
exact match task even outperforms it. Although a 
direct comparison with other state-of-the-art sys-
tems is not possible, due to different evaluation 
corpora, annotation standards and size in particu-
lar, our experiments disclose a very important cha-
racteristic. While the recognition systems in the 
TERN 2004 reported a substantial drop of F1-
measure between detection and exact match results 
(6.5 ? 11.6%), our phrase-based detector demon-
strates a light decrease in F1-measure (2.4%), whe-
reas the precision declines only by 0.6%. This 
important finding leads us to the conclusion that 
most of TIMEX3s in TimeBank can be detected at 
a phrase-based level with a reasonably high per-
formance.  
Despite a good recognition performance level 
there is, of course, room for improvement. Many 
implementations in the TERN 2004 employ a set 
of apparent temporal tokens as one of the features. 
In our implementation, the classifier has difficul-
ties with very simple temporal expressions such as 
?now?, ?future?, ?current?, ?currently?, ?recent?, 
?recently?. A direct employment of vocabularies 
with temporal tokens may substantially increase 
the F1-measure of the method, however, it yet has 
to be proven. As reported in (Ahn et al, 2007) a 
precise recognition of temporal expressions is a 
prerequisite for accurate normalization.  
With our detector and a future normalizer we 
are able make the first step towards solving the 
TempEval-2 tasks, which introduce new challenges 
in temporal information processing: identification 
of events, identification of temporal expressions 
and identification of temporal relations (Puste-
jovsky et al, 2009). Our future work will be fo-
cused on improving current results by a new 
feature design, finalizing the normalization task 
and identification of temporal relations. All these 
components will result in a solid system infrastruc-
ture for all-around temporal analysis.  
Acknowledgments 
This work has been partly funded by the Flemish 
government (through IWT) and by Space Applica-
tions Services NV as part of the ITEA2 project  
LINDO (ITEA2-06011). 
References 
 
Ahn, D., Adafre, S. F., and de Rijke, M. 2005. Extract-
ing Temporal Information from Open Domain Text: 
A Comparative Exploration. Digital Information 
Management, 3(1):14-20, 2005.  
Ahn, D., van Rantwijk, J., and de Rijke, M. 2007. A 
Cascaded Machine Learning Approach to Interpret-
ing Temporal Expressions. In Proceedings NAACL-
HLT 2007. 
Boguraev, B., and Ando, R. K. 2005. TimeBank-Driven 
TimeML Analysis. In Annotating, Extracting and 
Reasoning about Time and Events. Dagstuhl Seminar 
Proceedings. Dagstuhl, Germany 
Boguraev, B., Pustejovsky, J., Ando, R., and Verhagen, 
M. 2007. TimeBank Evolution as a Community Re-
source for TimeML Parsing. Language Resource and 
Evaluation, 41(1): 91?115. 
Ferro, L., Gerber, L., Mani, I., Sundheim, B., and Wil-
son, G. 2003. TIDES 2003 Standard for the Annota-
tion of Temporal Expressions. Sept. 2003. 
timex2.mitre.org. 
Ferro, L. 2004. TERN Evaluation Task Overview and 
Corpus, 
<http://fofoca.mitre.org/tern_2004/ferro1_TERN200
4_task_corpus.pdf> (accessed: 5.03.2009) 
56
Graff, D., Cieri, C., Strassel, S., and Martey, N. 1999. 
The TDT-2 Text and Speech Corpus. In Proceedings 
of DARPA Broadcast News Workshop, pp. 57-60. 
Hacioglu, K., Chen, Y., and Douglas, B. 2005. Auto-
matic Time Expression Labeling for English and 
Chinese Text. In Proceedings of CICLing-2005, pp. 
348-359; Springer-Verlag, Lecture Notes in Comput-
er Science, vol. 3406. 
Mani, I. and Wilson, G. 2000. Robust Temporal 
Processing of News. In Proceedings of the 38th An-
nual Meeting on Association for Computational Lin-
guistics (Hong Kong, October 03 - 06, 2000). Annual 
Meeting of the ACL. Association for Computational 
Linguistics, Morristown, NJ, pp. 69-76. 
Negri, M. and Marseglia, L. 2004. Recognition and 
Normalization of Time Expressions: ITC-irst at 
TERN 2004. Technical Report, ITC-irst, Trento. 
Poveda, J., Surdeanu, M., and Turmo, J. 2007. A Com-
parison of Statistical and Rule-Induction Learners for 
Automatic Tagging of Time Expressions in English. 
In Proceedings of the International Symposium on 
Temporal Representation and Reasoning, pp. 141-
149.  
Pustejovsky, J., Hanks, P., Saur?, R., See, A., Day, D., 
Ferro, L., Gaizauskas, R., Lazo, M., Setzer, A., and 
Sundheim, B. 2003a. The TimeBank Corpus. In Pro-
ceedings of Corpus Linguistics 2003, pp. 647-656. 
Pustejovsky, J., Casta?o, J., Ingria, R., Saur?, R., Gai-
zauskas, R., Setzer, A., and Katz, G. 2003b. Time-
ML: Robust Specification of Event and Temporal 
Expressions in Text. In Proceedings of IWCS-5, Fifth 
International Workshop on Computational Seman-
tics. 
Pustejovsky, J., Verhagen, M., Nianwen, X., Gai-
zauskas, R., Hepple, M., Schilder, F., Katz, G., Saur?, 
R., Saquete, E., Caselli, T., Calzolari, N., Lee, K., 
and Im, S. 2009. TempEval2: Evaluating Events, 
Time Expressions and Temporal Relations. 
<http://www.timeml.org/tempeval2/tempeval2-
proposal.pdf> (accessed: 5.03.2009) 
Ratnaparkhi, A. 1999. Learning to Parse Natural Lan-
guage with Maximum Entropy Models. Machine 
Learning, 34(1): 151-175. 
TERN 2004 Evaluation Plan, 2004, 
<http://fofoca.mitre.org/tern_2004/tern_evalplan-
2004.29apr04.pdf> (accessed: 5.03.2009) 
 
57
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 271?276,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Model-Portability Experiments for Textual Temporal Analysis 
Oleksandr Kolomiyets, Steven Bethard and Marie-Francine Moens Department of Computer Science Katholieke Universiteit Leuven Celestijnenlaan 200A, Heverlee, 3001, Belgium {oleksandr.kolomiyets, steven.bethard, sien.moens}@cs.kuleuven.be  
 
 
Abstract 
We explore a semi-supervised approach for improving the portability of time expression recognition to non-newswire domains: we generate additional training examples by substituting temporal expression words with potential synonyms. We explore using synonyms both from WordNet and from the Latent Words Language Model (LWLM), which predicts synonyms in context using an unsupervised approach. We evaluate a state-of-the-art time expression recognition system trained both with and without the additional training examples using data from TempEval 2010, Reuters and Wikipedia. We find that the LWLM provides substan-tial improvements on the Reuters corpus, and smaller improvements on the Wikipedia corpus. We find that WordNet alne never improves performance, though intersecting the examples from the LWLM and WordNet provides more stable results for Wikipedia.  
1 Introduction The recognition of time expressions such as April 2011, mid-September and early next week is a cru-cial first step for applications like question answer-ing that must be able to handle temporally anchored queries. This need has inspired a variety of shared tasks for identifying time expressions, including the Message Understanding Conference named entity task (Grishman and Sundheim, 1996), the Automatic Content Extraction time 
normalization task (http://fofoca.mitre.org/tern.html) and the TempEval 2010 time expression task (Verhagen et al, 2010). Many researchers com-peted in these tasks, applying both rule-based and machine-learning approaches (Mani and Wilson, 2000; Negri and Marseglia, 2004; Hacioglu et al, 2005; Ahn et al, 2007; Poveda et al, 2007; Str?tgen and Gertz 2010; Llorens et al, 2010), and achieving F1 measures as high as 0.86 for recog-nizing temporal expressions. Yet in most of these recent evaluations, models are both trained and evaluated on text from the same domain, typically newswire.  Thus we know little about how well time expression recognition systems generalize to other sorts of text. We there-fore take a state-of-the-art time recognizer and eva-luate it both on TempEval 2010 and on two new test sets drawn from Reuters and Wikipedia. At the same time, we are interested in helping the model recognize more types of time expres-sions than are available explicitly in the newswire training data. We therefore introduce a semi-supervised approach for expanding the training data, where we take words from temporal expres-sions in the data, substitute these words with likely synonyms, and add the generated examples to the training set. We select synonyms both via Word-Net, and via predictions from the Latent Words Language Model (LWLM) (Deschacht and Moens, 2009). We then evaluate the semi-supervised mod-el on the TempEval, Reuters and Wikipedia test sets and observe how well the model has expanded its temporal vocabulary. 
271
2 Related Work Semi-supervised approaches have been applied to a wide variety of natural language processing tasks, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), and document classification (Sur-deanu et al, 2006). The most relevant research to our work here is that of (Poveda et al, 2009), which investigated a semi-supervised approach to time expression rec-ognition. They begin by selecting 100 time expres-sions as seeds, selecting only expressions that are almost always annotated as times in the training half of the Automatic Content Extraction corpus. Then they begin an iterative process where they search an unlabeled corpus for patterns given their seeds (with patterns consisting of surrounding to-kens, parts-of-speech, syntactic chunks etc.) and then search for new seeds given their patterns. The patterns resulting from this iterative process achieve F1 scores of up to 0.604 on the test half of the Automatic Content Extraction corpus. Our approach is quite different from that of (Po-veda et al, 2009) ? we use our training corpus for learning a supervised model rather than for se-lecting high precision seeds, we generate addi-tional training examples using synonyms rather than bootstrapping based on patterns, and we evaluate on Reuters and Wikipedia data that differ from the domain on which our model was trained. 3 Method The proposed method implements a supervised machine learning approach that classifies each chunk-phrase candidate top-down starting at the parse tree root provided by the OpenNLP parser. Time expressions are identified as phrasal chunks with spans derived from the parse as described in (Kolomiyets and Moens, 2010).  3.1 Basic TempEval Model We implemented a logistic regression model with the following features for each phrase-candidate: ? The head word of the phrase ? The part-of-speech tag of the head word ? All tokens and part-of-speech tags in the phrase as a bag of words 
? The word-shape representation of the head word and the entire phrase, e.g. Xxxxx 99 for the expression April 30  ? The condensed word-shape representation for the head word and the entire phrase, e.g. X(x) (9) for the expression April 30 ? The concatenated string of the syntactic types of the children of the phrase in the parse tree ? The depth in the parse tree  3.2 Lexical Resources for Bootstrapping Sparsity of annotated corpora is the biggest chal-lenge for any supervised machine learning tech-nique and especially for porting the trained models onto other domains. To overcome this problem we hypothesize that knowledge of semantically similar words, like temporal triggers, could be found by associating words that do not occur in the training set to similar words that do occur in the training set. Furthermore, we would like to learn these similarities automatically to be independent of knowledge sources that might not be available for all languages or domains. The first option is to use the Latent Words Language Model (LWLM) (Deschacht and Moens, 2009) ? a language model that learns from an unlabeled corpus how to pro-vide a weighted set of synonyms for words in con-text. The LWLM model is trained on the Reuters news article corpus of 80 million words.  WordNet (Miller, 1995) is another resource for synonyms widely used in research and applications of natural language processing. Synonyms from WordNet seem to be very useful for bootstrapping as they provide replacement words to a specific word in a particular sense. For each synset in WordNet there is a collection of other ?sister? syn-sets, called coordinate terms, which are topologi-cally located under the same hypernym.  3.3 Bootstrapping Strategies Having a list of synonyms for each token in the sentence, we can replace one of the original tokens by its synonym while still mostly preserving the sentence semantics. We choose to replace just the headword, under the assumption that since tempo-ral trigger words usually occur at the headword position, adding alternative synonyms for the headword should allow our model to learn tempo-ral triggers that did not appear in the training data.  
272
We designed the following bootstrapping strate-gies for generating new temporal expressions: ? LWLM: the phrasal head is replaced by one of the LWLM synonyms. ? WordNet 1st Sense: Synonyms and coordinate terms for the most common sense of the phrasal head are selected and used for generat-ing new examples of time expressions. ? WordNet Pseudo-Lesk: The synset for the phrasal head is selected as having the largest intersection between the synset?s words and the LWLM synonyms. Then, synonyms and coordinate terms are used for generating new examples of time expressions. ? LWLM+WordNet: The intersection of the LWLM synonyms and the WordNet synset found by pseudo-Lesk are used. In this way for every annotated time expression we generate n new examples (n?[1,10]) and use them for training bootstrapped classification models.  4 Experimental Setup The tested model is trained on the official Tem-pEval 2010 training data with 53450 tokens and 2117 annotated TIMEX3 tokens. For testing the portability of the model to other domains we anno-tated two small target domain document collec-tions with TIMEX3 tags. The first corpus is 12 Reuters news articles from the Reuters corpus 
(Lewis et al, 2004), containing 2960 total tokens and 240 annotated TIMEX3 tokens (inter-annotator agreement 0.909 F1-score). The second corpus is the Wikipedia article for Barak Obama (http://en.wikipedia.org/wiki/Obama), containing 7029 total tokens and 512 annotated TIMEX3 to-kens (inter-annotator agreement 0.901 F1-score). The basic TempEval model is evaluated on the source domain (TempEval 2010 evaluation set ? 9599 tokens in total and 269 TIMEX3 annotated tokens) and target domain data (Reuters and Wikipedia) using the TempEval 2010 evaluation metrics. Since porting the model onto other do-mains usually causes a performance drop, our ex-periments are focused on improving the results by employing different bootstrapping strategies1. 5 Results The recognition performance of the model is re-ported in Table 1 (column ?Basic TempEval Mod-el?) for the source and the target domains. The basic TempEval model itself achieves F1-score of 0.834 on the official TempEval 2010 evaluation corpus and has a potential rank 8 among 15 par-ticipated systems. The top seven TempEval-2 sys-tems achieved F1-score between 0.83 and 0.86.                                                            1 The annotated datasets are available at http://www.cs.kuleuven.be/groups/liir/software.php 
Bootstrapped Models   Basic TempEval Model LWLM WordNet 1st Sense WordNet Pseudo-Lesk LWLM+ WordNet # Syn 0 1 1 1 2 P 0.916 0.865 0.881 0.894 0.857 R 0.770 0.807 0.773 0.781 0.830 TempEval 2010 F1 0.834 0.835 0.824 0.833 0.829 # Syn 0 5 7 6 4 P 0.896 0.841 0.820 0.839 0.860 R 0.679 0.812 0.721 0.717 0.742 Reuters F1 0.773 0.826 0.767 0.773 0.796 # Syn 0 3 1 6 5 P 0.959 0.924 0.922 0.909 0.913 R 0.770 0.830 0.781 0.820 0.844 Wikipedia F1 0.859 0.874 0.858 0.862 0.877 Table 1: Precision, recall and F1 scores for all models on the source (TempEval 2010) and target (Reuters and Wikipedia) domains. Bootstrapped models were asked to generate between one and ten additional train-ing examples per instance. The maximum P, R, F1 and the number of synonyms at which this maximum was achieved are given in the P, R, F1 and # Syn rows. F1 scores more than 0.010 above the Basic Tem-pEval Model are marked in bold. 
273
However, this model does not port well to the Reuters corpus (0.773 vs. 0.834 F1-score). For the Wikipedia-based corpus, the basic TempEval mod-el actually performs a little better than on the source domain (0.859 vs. 0.834 F1-score). Four bootstrapping strategies were proposed and evaluated. Table 1 shows the maximum F1 score achieved by each of these strategies, along with the number of generated synonyms (between one and ten) at which this maximum was achieved. None of the bootstrapped models outperformed the basic TempEval model on the TempEval 2010 evalua-tion data, and the WordNet 1st Sense strategy and the WordNet Pseudo-Lesk strategy never outper-formed the basic TempEval model on any corpus. However, for the Reuters and Wikipedia cor-pora, the LWLM and LWLM+WordNet bootstrap-ping strategies outperformed the basic TempEval model. The LWLM strategy gives a large boost to model performance on the Reuters corpus from 0.773 up to 0.826 (a 23.3% error reduction) when using the first 5 synonyms. This puts performance on Reuters near performance on the TempEval domain from which the model was trained (0.834). This suggests that the (Reuters-trained) LWLM is finding exactly the right kinds of synonyms: those that were not originally present in the TempEval data but are present in the Reuters test data. On the Wikipedia corpus, the LWLM bootstrapping strat-egy results in a moderate boost, from 0.859 up to 0.874 (a 10.6% error reduction) when using the first three synonyms. Figure 1 shows that using more synonyms with this strategy drops perform-
ance on the Wikipedia corpus back down to the level of the basic TempEval model. The LWLM+WordNet strategy gives a moderate boost on the Reuters corpus from 0.773 up to 0.796 (a 10.1% error reduction) when four synonyms are used. Figure 2 shows that using six or more syno-nyms drops this performance back to just above the basic TempEval model. On the Wikipedia corpus, the LWLM+WordNet strategy results in a moder-ate boost, from 0.859 up to 0.877 (a 12.8% error reduction), with five synonyms. Using additional synonyms results in a small decline in perform-ance, though even with ten synonyms, the per-formance is better than the basic TempEval model. In general, the LWLM strategy gives the best performance, while the LWLM+WordNet strategy is less sensitive to the exact number of synonyms used when expanding the training data. 6 TempEval Error Analysis We were curious why synonym-based boot-strapping did not improve performance on the source-domain TempEval 2010 data. An error analysis suggested that some time expressions might have been left unannotated by the human annotators. Two of the authors re-annotated the TempEval evaluation data, finding inter-annotator agreement of 0.912 F1-score with each other, but only 0.868 and 0.887 F1-score with the TempEval annotators, primarily due to unannotated time ex-pressions such as 23-year, a few days and third-quarter. 
 Figure 1: F1 score of the LWLM bootstrapping strat-egy, generating from zero to ten additional training examples per instance. 
 Figure 2: F1 score of the LWLM+WordNet bootstrap-ping strategy, generating from zero to ten additional training examples per instance. 
274
Using this re-annotated TempEval 2010 data2, we re-evaluated the proposed bootstrapping tech-niques. Figure 3 and Figure 4 compare perform-ance on the original TempEval data to performance on the re-annotated version. We now see the same trends for the TempEval data as were observed for the Reuters and Wikipedia corpora: using a small number of synonyms from the LWLM to generate new training examples leads to performance gains. The LWLM bootstrapping model using the first synonym achieves 0.861 F1 score, a 22.8% error reduction over the baseline of 0.820 F1 score. 7 Discussion and Conclusions We have presented model-portability experiments on time expression recognition with a number of bootstrapping strategies. These bootstrapping strat-egies generate additional training examples by substituting temporal expression words with poten-tial synonyms from two sources: WordNet and the Latent Word Language Model (LWLM). Bootstrapping with LWLM synonyms provides a large boost for Reuters data and TempEval data and a decent boost for Wikipedia data when the top few synonyms are used. Additional synonyms do not help, probably because they are too newswire-specific: both the contexts from the TempEval training data and the synonyms from the Reuters-trained LWLM come from newswire text, so the 
                                                           2 Available at http://www.cs.kuleuven.be/groups/liir/software.php 
lower synonyms are probably more domain-specific. Intersecting the synonyms generated by the LWLM and by WordNet moderates the LWLM, making the bootstrapping strategy less sensitive to the exact number of synonyms used. However, while the intersected model performs as well as the LWLM model on Wikipedia, the gains over the non-bootstrapped model on Reuters and TempEval data are smaller. Overall, our results show that when porting time expression recognition models to other domains, a performance drop can be avoided by synonym-based bootstrapping. Future work will focus on using synonym-based expansion in the contexts (not just the time expressions headwords), and on incorporating contextual information and syntactic transformations. Acknowledgments This work has been funded by the Flemish gov-ernment as a part of the project AMASS++ (Ad-vanced Multimedia Alignment and Structured Summarization) (Grant: IWT-SBO-060051). References David Ahn, Joris van Rantwijk, and Maarten de Rijke. 2007. A Cascaded Machine Learning Approach to Interpreting Temporal Expressions. In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Lin-guistics (NAACL-HLT 2007). 
 Figure 3: F1 score of the LWLM bootstrapping strat-egy, comparing performance on the original TempEval data to the re-annotated version. 
 Figure 4: F1 score of the LWLM+WordNet bootstrap-ping strategy, comparing performance on the original TempEval data to the re-annotated version. 
275
Michael Collins and Yoram Singer. 1999. Unsupervised Models for Named Entity Classification. In Proceed-ings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pp. 100?110, College Park, MD. ACL. Koen Deschacht and Marie-Francine Moens. 2009. Us-ing the Latent Words Language Model for Semi-Supervised Semantic Role Labeling. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing. Ralph Grishman and Beth Sundheim. 1996. Message Understanding Conference-6: A Brief History. In Proceedings of the 16th Conference on Computa-tional Linguistics, pp. 466?471. Kadri Hacioglu, Ying Chen, and Benjamin Douglas 2005. Automatic Time Expression Labeling for Eng-lish and Chinese Text. In Gelbukh, A. (ed.) CICLing 2005. LNCS, vol. 3406, pp. 548?559. Springer, Hei-delberg. Oleksandr Kolomiyets, Marie-Francine Moens. 2010. KUL: Recognition and Normalization of Temporal Expressions. In Proceedings of SemEval-2 5th Work-shop on Semantic Evaluation. pp. 325-328. Uppsala, Sweden. ACL. David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. 2004. RCV1: A New Benchmark Collection for Text Categorization Research. Machine Learning Re-search. 5: 361-397 Inderjeet Mani, and George Wilson. 2000. Robust Tem-poral Processing of News. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pp. 69-76, Morristown, NJ. ACL. George A. Miller. 1995. WordNet: A Lexical Database for English. Communications of the ACM, 38(11): 39-41. Matteo Negri, and Luca Marseglia. 2004. Recognition and Normalization of Time Expressions: ITC-irst at TERN 2004. Technical Report, ITC-irst, Trento. Hector Llorens, Estela Saquete, and Borja Navarro. 2010. TIPSem (English and Spanish): Evaluating CRFs and Semantic Roles in TempEval 2. In Pro-ceedings of the 5th International Workshop on Se-mantic Evaluation, pp. 284?291, Uppsala, Sweden. ACL. Jordi Poveda, Mihai Surdeanu, and Jordi Turmo. 2007. A Comparison of Statistical and Rule-Induction Learners for Automatic Tagging of Time Expressions in English. In Proceedings of the International Sym-posium on Temporal Representation and Reasoning, pp. 141-149. 
Jordi Poveda, Mihai Surdeanu, and Jordi Turmo. 2009. An Analysis of Bootstrapping for the Recognition of Temporal Expressions. In Proceedings of the NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Processing, pp. 49-57, Stroudsburg, PA, USA. ACL. Jannik Str?tgen and Michael Gertz. 2010. HeidelTime: High Quality Rule-Based Extraction and Normaliza-tion of Temporal Expressions. In Proceedings of the 5th International Workshop on Semantic Evaluation, pp. 321?324, Uppsala, Sweden. ACL.  Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 2006. A Hybrid Approach for the Acquisition of Informa-tion Extraction Patterns. In Proceedings of the EACL 2006 Workshop on Adaptive Text Extraction and Mining (ATEM 2006). ACL. Marc Verhagen, Roser Sauri, Tommaso Caselli, and James Pustejovsky. 2010. SemEval-2010 Task 13: TempEval 2. In Proceedings of the 5th International Workshop on Semantic Evaluation, pp. 57?62, Upp-sala, Sweden. ACL. 	 ?David Yarowsky. 1995. Unsupervised word sense dis-ambiguation rivaling supervised methods. In Pro-ceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pp. 189?196, Cambridge, MA. ACL.   
276
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 88?97,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Extracting Narrative Timelines as Temporal Dependency Structures
Oleksandr Kolomiyets
KU Leuven
Celestijnenlaan 200A
B-3001 Heverlee, Belgium
Oleksandr.Kolomiyets@
cs.kuleuven.be
Steven Bethard
University of Colorado
Campus Box 594
Boulder, CO 80309, USA
Steven.Bethard@
colorado.edu
Marie-Francine Moens
KU Leuven
Celestijnenlaan 200A
B-3001 Heverlee, Belgium
Sien.Moens@
cs.kuleuven.be
Abstract
We propose a new approach to characterizing
the timeline of a text: temporal dependency
structures, where all the events of a narrative
are linked via partial ordering relations like BE-
FORE, AFTER, OVERLAP and IDENTITY. We
annotate a corpus of children?s stories with tem-
poral dependency trees, achieving agreement
(Krippendorff?s Alpha) of 0.856 on the event
words, 0.822 on the links between events, and
of 0.700 on the ordering relation labels. We
compare two parsing models for temporal de-
pendency structures, and show that a determin-
istic non-projective dependency parser outper-
forms a graph-based maximum spanning tree
parser, achieving labeled attachment accuracy
of 0.647 and labeled tree edit distance of 0.596.
Our analysis of the dependency parser errors
gives some insights into future research direc-
tions.
1 Introduction
There has been much recent interest in identifying
events, times and their relations along the timeline,
from event and time ordering problems in the Temp-
Eval shared tasks (Verhagen et al, 2007; Verhagen
et al, 2010), to identifying time arguments of event
structures in the Automated Content Extraction pro-
gram (Linguistic Data Consortium, 2005; Gupta and
Ji, 2009), to timestamping event intervals in the
Knowledge Base Population shared task (Artiles et
al., 2011; Amigo? et al, 2011).
However, to date, this research has produced frag-
mented document timelines, because only specific
types of temporal relations in specific contexts have
been targeted. For example, the TempEval tasks only
looked at relations between events in the same or ad-
jacent sentences (Verhagen et al, 2007; Verhagen et
al., 2010), and the Automated Content Extraction pro-
gram only looked at time arguments for specific types
of events, like being born or transferring money.
In this article, we propose an approach to temporal
information extraction that identifies a single con-
nected timeline for a text. The temporal language
in a text often fails to specify a total ordering over
all the events, so we annotate the timelines as tem-
poral dependency structures, where each event is a
node in the dependency tree, and each edge between
nodes represents a temporal ordering relation such
as BEFORE, AFTER, OVERLAP or IDENTITY. We
construct an evaluation corpus by annotating such
temporal dependency trees over a set of children?s
stories. We then demonstrate how to train a time-
line extraction system based on dependency parsing
techniques instead of the pair-wise classification ap-
proaches typical of prior work.
The main contributions of this article are:
? We propose a new approach to characterizing
temporal structure via dependency trees.
? We produce an annotated corpus of temporal
dependency trees in children?s stories.
? We design a non-projective dependency parser
for inferring timelines from text.
The following sections first review some relevant
prior work, then describe the corpus annotation and
the dependency parsing algorithm, and finally present
our evaluation results.
88
2 Related Work
Much prior work on the annotation of temporal in-
formation has constructed corpora with incomplete
timelines. The TimeBank (Pustejovsky et al, 2003b;
Pustejovsky et al, 2003a) provided a corpus anno-
tated for all events and times, but temporal relations
were only annotated when the relation was judged to
be salient by the annotator. In the TempEval compe-
titions (Verhagen et al, 2007; Verhagen et al, 2010),
annotated texts were provided for a few different
event and time configurations, for example, an event
and a time in the same sentence, or two main-clause
events from adjacent sentences. Bethard et al (2007)
proposed to annotate temporal relations one syntactic
construction at a time, producing an initial corpus of
only verbal events linked to events in subordinated
clauses. One notable exception to this pattern of
incomplete timelines is the work of Bramsen et al
(2006) where temporal structures were annotated as
directed acyclic graphs. However they worked on a
much coarser granularity, annotating not the order-
ing between individual events, but between multi-
sentence segments of text.
In part because of the structure of the available
training corpora, most existing temporal informa-
tion extraction models formulate temporal linking
as a pair-wise classification task, where each pair
of events and/or times is examined and classified as
having a temporal relation or not. Early work on the
TimeBank took this approach (Boguraev and Ando,
2005), classifying relations between all events and
times within 64 tokens of each other. Most of the top-
performing systems in the TempEval competitions
also took this pair-wise classification approach for
both event-time and event-event temporal relations
(Bethard and Martin, 2007; Cheng et al, 2007; UzZa-
man and Allen, 2010; Llorens et al, 2010). Systems
have also tried to take advantage of more global in-
formation to ensure that the pair-wise classifications
satisfy temporal logic transitivity constraints, using
frameworks such as integer linear programming and
Markov logic networks (Bramsen et al, 2006; Cham-
bers and Jurafsky, 2008; Yoshikawa et al, 2009; Uz-
Zaman and Allen, 2010). Yet the basic approach is
still centered around pair-wise classifications, not the
complete temporal structure of a document.
Our work builds upon this prior research, both
improving the annotation approach to generate the
fully connected timeline of a story, and improving
the models for timeline extraction using dependency
parsing techniques. We use the annotation scheme
introduced in more detail in Bethard et. al. (2012),
which proposes to annotate temporal relations as de-
pendency links between head events and dependent
events. This annotation scheme addresses the issues
of incoherent and incomplete annotations by guaran-
teeing that all events in a plot are connected along
a single timeline. These connected timelines allow
us to design new models for timeline extraction in
which we jointly infer the temporal structure of the
text and the labeled temporal relations. We employ
methods from syntactic dependency parsing, adapt-
ing them to our task by including features typical of
temporal relation labeling models.
3 Corpus Annotation
The corpus of stories for children was drawn from the
fables collection of (McIntyre and Lapata, 2009)1 and
annotated as described in (Bethard et al, 2012). In
this section we illustrate the main annotation princi-
ples for coherent temporal annotation. As an example
story, consider:
Two Travellers were on the road together,
when a Bear suddenly appeared on the
scene. Before he observed them, one made
for a tree at the side of the road, and
climbed up into the branches and hid there.
The other was not so nimble as his compan-
ion; and, as he could not escape, he threw
himself on the ground and pretended to be
dead. . . [37.txt]
Figure 1 shows the temporal dependency structure
that we expect our annotators to identify in this story.
The annotators were provided with guidelines both
for which kinds of words should be identified as
events, and for which kinds of events should be
linked by temporal relations. For identifying event
words, the standard TimeML guidelines for anno-
tating events (Pustejovsky et al, 2003a) were aug-
mented with two additional guidelines:
1Data available at http://homepages.inf.ed.ac.
uk/s0233364/McIntyreLapata09/
89
Figure 1: Event timeline for the story of the Travellers and the Bear. Nodes are events and edges are temporal relations.
Edges denote temporal relations signaled by linguistic cues in the text. Temporal relations that can be inferred via
transitivity are not shown.
? Skip negated, modal or hypothetical events (e.g.
could not escape, dead in pretended to be dead).
? For phrasal events, select the single word that
best paraphrases the meaning (e.g. in used to
snap the event should be snap, in kept perfectly
still the event should be still).
For identifying the temporal dependencies (i.e. the
ordering relations between event words), the anno-
tators were instructed to link each event in the story
to a single nearby event, similar to what has been
observed in reading comprehension studies (Johnson-
Laird, 1980; Brewer and Lichtenstein, 1982). When
there were several reasonable nearby events to choose
from, the annotators were instructed to choose the
temporal relation that was easiest to infer from the
text (e.g. preferring relations with explicit cue words
like before). A set of six temporal relations was used:
BEFORE, AFTER, INCLUDES, IS-INCLUDED, IDEN-
TITY or OVERLAP.
Two annotators annotated temporal dependency
structures in the first 100 fables of the McIntyre-
Lapata collection and measured inter-annotator agree-
ment by Krippendorff?s Alpha for nominal data (Krip-
pendorff, 2004; Hayes and Krippendorff, 2007). For
the resulting annotated corpus annotators achieved
Alpha of 0.856 on the event words, 0.822 on the links
between events, and of 0.700 on the ordering rela-
tion labels. Thus, we concluded that the temporal
dependency annotation paradigm was reliable, and
the resulting corpus of 100 fables2 could be used to
2Available from http://www.bethard.info/data/
fables-100-temporal-dependency.xml
train a temporal dependency parsing model.
4 Parsing Models
We consider two different approaches to learning a
temporal dependency parser: a shift-reduce model
(Nivre, 2008) and a graph-based model (McDonald
et al, 2005). Both models take as input a sequence
of event words and produce as output a tree structure
where the events are linked via temporal relations.
Formally, a parsing model is a function (W ? ?)
where W = w1w2 . . . wn is a sequence of event
words, and pi ? ? is a dependency tree pi = (V,E)
where:
? V = W ? {Root}, that is, the vertex set of the
graph is the set of words in W plus an artificial
root node.
? E = {(wh, r, wd) : wh ? V,wd ? V, r ? R =
{BEFORE, AFTER, INCLUDES, IS INCLUDED,
IDENTITY, OVERLAP}}, that is, in the edge set
of the graph, each edge is a link between a de-
pendent word and its head word, labeled with a
temporal relation.
? (wh, r, wd) ? E =? wd 6= Root, that is, the
artificial root node has no head.
? (wh, r, wd) ? E =? ((w?h, r
?, wd) ? E =?
wh = w?h? r = r
?), that is, for every node there
is at most one head and one relation label.
? E contains no (non-empty) subset of arcs
(wh, ri, wi), (wi, rj , wj), . . . , (wk, rl, wh), that
is, there are no cycles in the graph.
90
SHIFT Move all of L2 and the head of Q onto L1
([a1 . . . ai], [b1 . . . bj ], [wkwk+1 . . .], E) ? ([a1 . . . aib1 . . . bjwk], [], [wk+1 . . .], E)
NO-ARC Move the head of L1 to the head of L2
([a1 . . . aiai+1], [b1 . . . bj ], Q,E) ? ([a1 . . . ai], [ai+1b1 . . . bj ], Q,E)
LEFT-ARC Create a relation where the head of L1 depends on the head of Q
Not applicable if ai+1 is the root or already has a head, or if there is a path connecting wk and ai+1
([a1 . . . aiai+1], [b1 . . . bj ], [wk . . .], E) ? ([a1 . . . ai], [ai+1b1 . . . bj ], [wk . . .], E ? (wk, r, ai+1)
RIGHT-ARC Create a relation where the head of Q depends on the head of L1
Not applicable if wk is the root or already has a head, or if there is a path connecting wk and ai+1
([a1 . . . aiai+1], [b1 . . . bj ], [wk . . .], E) ? ([a1 . . . ai], [ai+1b1 . . . bj ], [wk . . .], E ? (ai+1, r, wk)
Table 1: Transition system for Covington-style shift-reduce dependency parsers.
4.1 Shift-Reduce Parsing Model
Shift-reduce dependency parsers start with an input
queue of unlinked words, and link them into a tree
by repeatedly choosing and performing actions like
shifting a node to a stack, or popping two nodes from
the stack and linking them. Shift-reduce parsers are
typically defined in terms of configurations and a tran-
sition system, where the configurations describe the
current internal state of the parser, and the transition
system describes how to get from one state to another.
Formally, a deterministic shift-reduce dependency
parser is defined as (C, T,CF , INIT, TREE) where:
? C is the set of possible parser configurations ci
? T ? (C ? C) is the set of transitions ti from
one configuration cj to another cj+1 allowed by
the parser
? INIT ? (W ? C) is a function from the input
words to an initial parser configuration
? CF ? C are the set of final parser configura-
tions cF where the parser is allowed to terminate
? TREE ? (CF ? ?) is a function that extracts a
dependency tree pi from a final parser state cF
Given this formalism and an oracle o ? (C ? T ),
which can choose a transition given the current con-
figuration of the parser, dependency parsing can be
accomplished by Algorithm 1. For temporal depen-
dency parsing, we adopt the Covington set of transi-
tions (Covington, 2001) as it allows for parsing the
non-projective trees, which may also contain ?cross-
ing? edges, that occasionally occur in our annotated
corpus. Our parser is therefore defined as:
Algorithm 1 Deterministic parsing with an oracle.
c? INIT(W )
while c /? CF do
t? o(c)
c? t(c)
end while
return TREE(c)
? c = (L1, L2, Q,E) is a parser configuration,
where L1 and L2 are lists for temporary storage,
Q is the queue of input words, and E is the set
of identified edges of the dependency tree.
? T = {SHIFT,NO-ARC,LEFT-ARC,RIGHT-ARC}
is the set of transitions described in Table 1.
? INIT(W ) = ([Root], [], [w1, w2, . . . , wn], ?)
puts all input words on the queue and the ar-
tificial root on L1.
? CF = {(L1, L2, Q,E) ? C : L1 = {W ?
{Root}}, L2 = Q = ?} accepts final states
where the input words have been moved off of
the queue and lists and into the edges in E.
? TREE((L1, L2, Q,E)) = (W ?{Root}, E) ex-
tracts the final dependency tree.
The oracle o is typically defined as a machine learn-
ing classifier, which characterizes a parser configu-
ration c in terms of a set of features. For temporal
dependency parsing, we learn a Support Vector Ma-
chine classifier (Yamada and Matsumoto, 2003) using
the features described in Section 5.
4.2 Graph-Based Parsing Model
One shortcoming of the shift-reduce dependency
parsing approach is that each transition decision
91
Figure 2: A setting for the graph-based parsing model: an initial dense graph G (left) with edge scores SCORE(e). The
resulting dependency tree as a spanning tree with the highest score over the edges (right).
made by the model is final, and cannot be revisited to
search for more globally optimal trees. Graph-based
models are an alternative dependency parsing model,
which assembles a graph with weighted edges be-
tween all pairs of words, and selects the tree-shaped
subset of this graph that gives the highest total score
(Fig. 2). Formally, a graph-based parser follows
Algorithm 2, where:
? W ? = W ? {Root}
? SCORE ? ((W ??R?W ) ? <) is a function
for scoring edges
? SPANNINGTREE is a function for selecting a
subset of edges that is a tree that spans over all
the nodes of the graph.
Algorithm 2 Graph-based dependency parsing
E ? {(e, SCORE(e)) : e ? (W ??R?W ))}
G? (W ?, E)
return SPANNINGTREE(G)
The SPANNINGTREE function is usually defined
using one of the efficient search techniques for find-
ing a maximum spanning tree. For temporal depen-
dency parsing, we use the Chu-Liu-Edmonds algo-
rithm (Chu and Liu, 1965; Edmonds, 1967) which
solves this problem by iteratively selecting the edge
with the highest weight and removing edges that
would create cycles. The result is the globally op-
timal maximum spanning tree for the graph (Geor-
giadis, 2003).
The SCORE function is typically defined as a ma-
chine learning model that scores an edge based on a
set of features. For temporal dependency parsing, we
learn a model to predict edge scores via the Margin
Infused Relaxed Algorithm (MIRA) (Crammer and
Singer, 2003; Crammer et al, 2006) using the set of
features defined in Section 5.
5 Feature Design
The proposed parsing algorithms both rely on ma-
chine learning methods. The shift-reduce parser
(SRP) trains a machine learning classifier as the or-
acle o ? (C ? T ) to predict a transition t from a
parser configuration c = (L1, L2, Q,E), using node
features such as the heads of L1, L2 and Q, and
edge features from the already predicted temporal
relations in E. The graph-based maximum spanning
tree (MST) parser trains a machine learning model
to predict SCORE(e) for an edge e = (wi, rj , wk),
using features of the nodes wi and wk. The full set
of features proposed for both parsing models, de-
rived from the state-of-the-art systems for temporal
relation labeling, is presented in Table 2. Note that
both models share features that look at the nodes,
while only the shift-reduce parser has features for
previously classified edges.
6 Evaluations
Evaluations were performed using 10-fold cross-
validation on the fables annotated in Section 3. The
corpus contains 100 fables, a total of 14,279 tokens
and a total of 1136 annotated temporal relations. As
92
Feature SRP MST
Word
?? ??
Lemma
?? ??
Part of speech (POS) tag
?? ??
Suffixes
?? ??
Syntactically governing verb
?? ??
Governing verb lemma
?? ??
Governing verb POS tag
?? ??
Governing verb POS suffixes
?? ??
Prepositional phrase occurrence
?? ??
Dominated by auxiliary verb?
?? ??
Dominated by modal verb?
?? ??
Temporal signal word is nearby?
?? ??
Head word lemma
?? ??
Temporal relation labels of ai and its
leftmost and rightmost dependents
?
Temporal relation labels of ai?1?s
leftmost and rightmost dependents
?
Temporal relation labels of b1 and its
leftmost and rightmost dependents
?
Table 2: Features for the shift-reduce parser (SRP) and the
graph-based maximum spanning tree (MST) parser. The?? features are extracted from the heads of L1, L2 and Q
for SRP and from each node of the edge for MST.
only 40 instances of OVERLAP relations were an-
notated when neither INCLUDES nor IS INCLUDED
label matched, for evaluation purposes all instances
of these relations were merged into the temporally
coarse OVERLAP relation. Thus, the total number of
OVERLAP relations in the corpus grew from 40 to
258 annotations in total.
To evaluate the parsing models (SRP and MST)
we proposed two baselines. Both are based on the
assumption of linear temporal structures of narratives
as the temporal ordering process that was evidenced
by studies in human text rewriting (Hickmann, 2003).
The proposed baselines are:
? LinearSeq: A model that assumes all events
occur in the order they are written, adding links
between each pair of adjacent events, and label-
ing all links with the relation BEFORE.
? ClassifySeq: A model that links each pair of
adjacent events, but trains a pair-wise classifier
to predict the relation label for each pair. The
classifier is a support vector machine trained us-
ing the same features as the MST parser. This is
an approximation of prior work, where the pairs
of events to classify with a temporal relation
were given as an input to the system. (Note that
Section 6.2 will show that for our corpus, apply-
ing the model only to adjacent pairs of events
is quite competitive for just getting the basic
unlabeled link structure right.)
The Shift-Reduce parser (SRP; Section 4.1) and the
graph-based, maximum spanning tree parser (MST;
Section 4.2) are compared to these baselines.
6.1 Evaluation Criteria and Metrics
Model performance was evaluated using standard
evaluation criteria for parser evaluations:
Unlabeled Attachment Score (UAS) The fraction
of events whose head events were correctly predicted.
This measures whether the correct pairs of events
were linked, but not if they were linked by the correct
relations.
Labeled Attachment Score (LAS) The fraction
of events whose head events were correctly pre-
dicted with the correct relations. This measures both
whether the correct pairs of events were linked and
whether their temporal ordering is correct.
Tree Edit Distance In addition to the UAS and
LAS the tree edit distance score has been recently in-
troduced for evaluating dependency structures (Tsar-
faty et al, 2011). The tree edit distance score
for a tree pi is based on the following operations
? ? ? : ? = {DELETE, INSERT, RELABEL}:
? ? =DELETE delete a non-root node v in pi with
parent u, making the children of v the children
of u, inserted in the place of v as a subsequence
in the left-to-right order of the children of u.
? ? =INSERT insert a node v as a child of u in
pi making it the parent of a consecutive subse-
quence of the children of u.
? ? =RELABEL change the label of node v in pi
Any two trees pi1 and pi2 can be turned one into an-
other by a sequence of edit operations {?1, ..., ?n}.
93
UAS LAS UTEDS LTEDS
LinearSeq 0.830 0.581 0.689 0.549
ClassifySeq 0.830 0.581 0.689 0.549
MST 0.837 0.614? 0.710 0.571
SRP 0.830 0.647?? 0.712 0.596?
Table 3: Performance levels of temporal structure pars-
ing methods. A ? indicates that the model outperforms
LinearSeq and ClassifiedSeq at p < 0.01 and a ? indicates
that the model outperforms MST at p < 0.05.
Taking the shortest such sequence, the tree edit dis-
tance is calculated as the sum of the edit operation
costs divided by the size of the tree (i.e. the number
of words in the sentence). For temporal dependency
trees, we assume each operation costs 1.0. The fi-
nal score subtracts the edit distance from 1 so that
a perfect tree has score 1.0. The labeled tree edit
distance score (LTEDS) calculates sequences over
the tree with all its labeled temporal relations, while
the unlabeled tree edit distance score (UTEDS) treats
all edges as if they had the same label.
6.2 Results
Table 3 shows the results of the evaluation. The
unlabeled attachment score for the LinearSeq base-
line was 0.830, suggesting that annotators were most
often linking adjacent events. At the same time,
the labeled attachment score was 0.581, indicating
that even in fables, the stories are not simply linear,
that is, there are many relations other than BEFORE.
The ClassifySeq baseline performs identically to the
LinearSeq baseline, which shows that the simple pair-
wise classifier was unable to learn anything beyond
predicting all relations as BEFORE.
In terms of labeled attachment score, both de-
pendency parsing models outperformed the base-
line models ? the maximum spanning tree parser
achieved 0.614 LAS, and the shift-reduce parser
achieved 0.647 LAS. The shift-reduce parser also
outperformed the baseline models in terms of labeled
tree edit distance, achieving 0.596 LTEDS vs. the
baseline 0.549 LTEDS. These results indicate that de-
pendency parsing models are a good fit to our whole-
story timeline extraction task.
Finally, in comparing the two different depen-
dency parsing models, we observe that the shift-
reduce parser outperforms the maximum spanning
Error Type Num. %
OVERLAP? BEFORE 24 43.7
Attach to further head 18 32.7
Attach to nearer head 6 11.0
Other types of errors 7 12.6
Total 55 100
Table 4: Error distribution from the analysis of 55 errors
of the Shift-Reduce parsing model.
tree parser in terms of labeled attachment score
(0.647 vs. 0.614). It has been argued that graph-
based models like the maximum spanning tree parser
should be able to produce more globally consistent
and correct dependency trees, yet we do not observe
that here. A likely explanation for this phenomenon
is that the shift-reduce parsing model allows for fea-
tures describing previous parse decisions (similar to
the incremental nature of human parse decisions),
while the joint nature of the maximum spanning tree
parser does not.
6.3 Error Analysis
To better understand the errors our model is still mak-
ing, we examined two folds (55 errors in total in
20% of the evaluation data) and identified the major
categories of errors:
? OVERLAP? BEFORE: The model predicts the
correct head, but predicts its label as BEFORE,
while the correct label is OVERLAP.
? Attach to further head: The model predicts
the wrong head, and predicts as the head an
event that is further away than the true head.
? Attach to nearer head: The model predicts the
wrong head, and predicts as the head an event
that is closer than the true head.
Table 4 shows the distribution of the errors over these
categories. The two most common types of errors,
OVERLAP ? BEFORE and Attach to further head,
account for 76.4% of all the errors.
The most common type of error is predicting
a BEFORE relation when the correct answer is an
OVERLAP relation. Figure 3 shows an example of
such an error, where the model predicts that the
Spendthrift stood before he saw, while the anno-
tator indicates that the seeing happened during the
94
Figure 3: An OVERLAP ? BEFORE parser error. True
links are solid lines; the parser error is the dotted line.
Figure 4: Parser errors attaching to further away heads.
True links are solid lines; parser errors are dotted lines.
time in which he was standing. An analysis of these
OVERLAP? BEFORE errors suggests that they occur
in scenarios like this one, where the duration of one
event is significantly longer than the duration of an-
other, but there are no direct cues for these duration
differences. We also observe these types of errors
when one event has many sub-events, and therefore
the duration of the main event typically includes the
durations of all the sub-events. It might be possible
to address these kinds of errors by incorporating auto-
matically extracted event duration information (Pan
et al, 2006; Gusev et al, 2011).
The second most common error type of the model
is the prediction of a head event that is further away
than the head identified by the annotators. Figure 4
gives an example of such an error, where the model
predicts that the gathering includes the smarting, in-
stead of that the gathering includes the stung. The
second error in the figure is also of the same type.
In 65% of the cases where this type of error occurs,
it occurs after the parser had already made a label
classification error such as BEFORE ? OVERLAP.
So these errors may be in part due to the sequen-
tial nature of shift-reduce parsing, where early errors
propagate and cause later errors.
7 Discussion and Conclusions
In this article, we have presented an approach to tem-
poral information extraction that represents the time-
line of a story as a temporal dependency tree. We
have constructed an evaluation corpus where such
temporal dependencies have been annotated over a
set of 100 children?s stories. We have introduced two
dependency parsing techniques for extracting story
timelines and have shown that both outperform a rule-
based baseline and a prior-work-inspired pair-wise
classification baseline. Comparing the two depen-
dency parsing models, we have found that a shift-
reduce parser, which more closely mirrors the incre-
mental processing of our human annotators, outper-
forms a graph-based maximum spanning tree parser.
Our error analysis of the shift-reduce parser revealed
that being able to estimate differences in event dura-
tions may play a key role in improving parse quality.
We have focused on children?s stories in this study,
in part because they typically have simpler temporal
structures (though not so simple that our rule-based
baseline could parse them accurately). In most of our
fables, there were only one or two characters with at
most one or two simultaneous sequences of actions.
In other domains, the timeline of a text is likely to
be more complex. For example, in clinical records,
descriptions of patients may jump back and forth
between the patient history, the current examination,
and procedures that have not yet happened.
In future work, we plan to investigate how to best
apply the dependency structure approach to such
domains. One approach might be to first group
events into their narrative containers (Pustejovsky
and Stubbs, 2011), for example, grouping together all
events linked to the time of a patient?s examination.
Then within each narrative container, our dependency
parsing approach could be applied. Another approach
might be to join the individual timeline trees into a
document-wide tree via discourse relations or rela-
tions to the document creation time. Work on how
humans incrementally process such timelines in text
may help to decide which of these approaches holds
the most promise.
Acknowledgements
We would like to thank the anonymous reviewers
for their constructive comments. This research was
partially funded by the TERENCE project (EU FP7-
257410) and the PARIS project (IWT SBO 110067).
95
References
[Amigo? et al2011] Enrique Amigo?, Javier Artiles, Qi Li,
and Heng Ji. 2011. An evaluation framework for aggre-
gated temporal information extraction. In SIGIR-2011
Workshop on Entity-Oriented Search.
[Artiles et al2011] Javier Artiles, Qi Li, Taylor Cas-
sidy, Suzanne Tamang, and Heng Ji. 2011.
CUNY BLENDER TAC-KBP2011 temporal slot fill-
ing system description. In Text Analytics Conference
(TAC2011).
[Bethard and Martin2007] Steven Bethard and James H.
Martin. 2007. CU-TMP: Temporal relation classifica-
tion using syntactic and semantic features. In Proceed-
ings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), pages 129?132, Prague,
Czech Republic, June. ACL.
[Bethard et al2007] Steven Bethard, James H. Martin, and
Sara Klingenstein. 2007. Finding temporal structure in
text: Machine learning of syntactic temporal relations.
International Journal of Semantic Computing (IJSC),
1(4):441?458, 12.
[Bethard et al2012] Steven Bethard, Oleksandr
Kolomiyets, and Marie-Francine Moens. 2012.
Annotating narrative timelines as temporal dependency
structures. In Proceedings of the International
Conference on Linguistic Resources and Evaluation,
Istanbul, Turkey, May. ELRA.
[Boguraev and Ando2005] Branimir Boguraev and
Rie Kubota Ando. 2005. TimeBank-driven TimeML
analysis. In Annotating, Extracting and Reasoning
about Time and Events. Springer.
[Bramsen et al2006] P. Bramsen, P. Deshpande, Y.K. Lee,
and R. Barzilay. 2006. Inducing temporal graphs.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 189?
198. ACL.
[Brewer and Lichtenstein1982] William F. Brewer and Ed-
ward H. Lichtenstein. 1982. Stories are to entertain: A
structural-affect theory of stories. Journal of Pragmat-
ics, 6(5-6):473 ? 486.
[Chambers and Jurafsky2008] N. Chambers and D. Juraf-
sky. 2008. Jointly combining implicit constraints im-
proves temporal ordering. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 698?706. ACL.
[Cheng et al2007] Yuchang Cheng, Masayuki Asahara,
and Yuji Matsumoto. 2007. NAIST.Japan: Tempo-
ral relation identification using dependency parsed tree.
In Proceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 245?248,
Prague, Czech Republic, June. ACL.
[Chu and Liu1965] Y. J. Chu and T.H. Liu. 1965. On
the shortest arborescence of a directed graph. Science
Sinica, pages 1396?1400.
[Covington2001] M.A. Covington. 2001. A fundamental
algorithm for dependency parsing. In Proceedings of
the 39th Annual ACM Southeast Conference, pages
95?102.
[Crammer and Singer2003] K. Crammer and Y. Singer.
2003. Ultraconservative online algorithms for multi-
class problems. Journal of Machine Learning Research,
3:951?991.
[Crammer et al2006] K. Crammer, O. Dekel, J. Keshet,
S. Shalev-Shwartz, and Y. Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551?585.
[Edmonds1967] J. Edmonds. 1967. Optimum branchings.
Journal of Research of the National Bureau of Stan-
dards, pages 233?240.
[Georgiadis2003] L. Georgiadis. 2003. Arborescence op-
timization problems solvable by Edmonds? algorithm.
Theoretical Computer Science, 301(1-3):427?437.
[Gupta and Ji2009] Prashant Gupta and Heng Ji. 2009.
Predicting unknown time arguments based on cross-
event propagation. In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, ACLShort ?09, pages
369?372, Stroudsburg, PA, USA. ACL.
[Gusev et al2011] Andrey Gusev, Nathanael Chambers,
Divye Raj Khilnani, Pranav Khaitan, Steven Bethard,
and Dan Jurafsky. 2011. Using query patterns to learn
the duration of events. In Proceedings of the Interna-
tional Conference on Computational Semantics, pages
145?154.
[Hayes and Krippendorff2007] A.F. Hayes and K. Krip-
pendorff. 2007. Answering the call for a standard
reliability measure for coding data. Communication
Methods and Measures, 1(1):77?89.
[Hickmann2003] Maya Hickmann. 2003. Children?s Dis-
course: Person, Space and Time Across Languages.
Cambridge University Press, Cambridge, UK.
[Johnson-Laird1980] P.N. Johnson-Laird. 1980. Men-
tal models in cognitive science. Cognitive Science,
4(1):71?115.
[Krippendorff2004] K. Krippendorff. 2004. Content anal-
ysis: An introduction to its methodology. Sage Publica-
tions, Inc.
[Linguistic Data Consortium2005] Linguistic Data Con-
sortium. 2005. ACE (Automatic Content Extraction)
English annotation guidelines for events version 5.4.3
2005.07.01.
[Llorens et al2010] Hector Llorens, Estela Saquete, and
Borja Navarro. 2010. TIPSem (English and Spanish):
Evaluating CRFs and semantic roles in TempEval-2. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 284?291, Uppsala, Sweden,
July. ACL.
96
[McDonald et al2005] R. McDonald, F. Pereira, K. Rib-
arov, and J. Hajic?. 2005. Non-projective dependency
parsing using spanning tree algorithms. In Proceedings
of the Conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 523?530. ACL.
[McIntyre and Lapata2009] N. McIntyre and M. Lapata.
2009. Learning to tell tales: A data-driven approach to
story generation. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 1-Volume 1, pages
217?225. ACL.
[Nivre2008] J. Nivre. 2008. Algorithms for determinis-
tic incremental dependency parsing. Computational
Linguistics, 34(4):513?553.
[Pan et al2006] Feng Pan, Rutu Mulkar, and Jerry R.
Hobbs. 2006. Learning event durations from event
descriptions. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational Lin-
guistics, pages 393?400, Sydney, Australia, July. ACL.
[Pustejovsky and Stubbs2011] J. Pustejovsky and
A. Stubbs. 2011. Increasing informativeness in
temporal annotation. In Proceedings of the 5th
Linguistic Annotation Workshop, pages 152?160. ACL.
[Pustejovsky et al2003a] James Pustejovsky, Jose?
Castan?o, Robert Ingria, Roser Saury?, Robert
Gaizauskas, Andrea Setzer, and Graham Katz. 2003a.
TimeML: Robust specification of event and temporal
expressions in text. In Proceedings of the Fifth
International Workshop on Computational Semantics
(IWCS-5), Tilburg.
[Pustejovsky et al2003b] James Pustejovsky, Patrick
Hanks, Roser Saury?, Andrew See, Robert Gaizauskas,
Andrea Setzer, Dragomir Radev, Beth Sundheim,
David Day, Lisa Ferro, and Marcia Lazo. 2003b.
The TimeBank corpus. In Proceedings of Corpus
Linguistics, pages 647?656.
[Tsarfaty et al2011] R. Tsarfaty, J. Nivre, and E. Ander-
sson. 2011. Evaluating dependency parsing: Robust
and heuristics-free cross-annotation evaluation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 385?396. ACL.
[UzZaman and Allen2010] Naushad UzZaman and James
Allen. 2010. TRIPS and TRIOS system for TempEval-
2: Extracting temporal information from text. In Pro-
ceedings of the 5th International Workshop on Seman-
tic Evaluation, pages 276?283, Uppsala, Sweden, July.
ACL.
[Verhagen et al2007] Marc Verhagen, Robert Gaizauskas,
Frank Schilder, Graham Katz, and James Pustejovsky.
2007. SemEval2007 Task 15: TempEval temporal rela-
tion identification. In SemEval-2007: 4th International
Workshop on Semantic Evaluations.
[Verhagen et al2010] Marc Verhagen, Roser Saur??, Tom-
maso Caselli, and James Pustejovsky. 2010. SemEval-
2010 Task 13: TempEval-2. In Proceedings of the 5th
International Workshop on Semantic Evaluation, Se-
mEval ?10, pages 57?62, Stroudsburg, PA, USA. ACL.
[Yamada and Matsumoto2003] H. Yamada and Y. Mat-
sumoto. 2003. Statistical dependency analysis with
support vector machines. In Proceedings of IWPT.
[Yoshikawa et al2009] K. Yoshikawa, S. Riedel, M. Asa-
hara, and Y. Matsumoto. 2009. Jointly identifying
temporal relations with Markov Logic. In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
405?413. ACL.
97
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 325?328,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
KUL: Recognition and Normalization of Temporal Expressions 
 
 
Oleksandr Kolomiyets, Marie-Francine Moens 
Department of Computer Science 
Katholieke Universiteit Leuven 
{oleksandr.kolomiyets, sien.moens}@cs.kuleuven.be 
 
 
  
 
Abstract 
 
In this paper we describe a system for the 
recognition and normalization of temporal 
expressions (Task 13: TempEval-2, Task 
A). The recognition task is approached as 
a classification problem of sentence con-
stituents and the normalization is imple-
mented in a rule-based manner. One of the 
system features is extending positive an-
notations in the corpus by semantically 
similar words automatically obtained from 
a large unannotated textual corpus. The 
best results obtained by the system are 
0.85 and 0.84 for precision and recall re-
spectively for recognition of temporal ex-
pressions; the accuracy values of 0.91 and 
0.55 were obtained for the feature values 
TYPE and VAL respectively. 
1 Introduction 
Recognition of temporal expressions1 is a task of 
proper identification of phrases with temporal 
semantics in running text. After several evalua-
tion campaigns targeted at temporal processing 
of text, such as MUC, ACE TERN and TempEv-
al-1 (Verhagen et al, 2007), the recognition and 
normalization task has been again newly reintro-
duced in TempEval-2 (Pustejovsky & Verhagen, 
2009). The task is defined as follows: determine 
the extent of the time expressions; in addition, 
determine the value of the features TYPE for the 
type of the temporal expression and its temporal 
value VAL. In this paper we describe the KUL 
system that has participated in this task.  
                                                 
1
 Temporal expressions are sometimes referenced as time 
expressions and timexes.  
Architecturally, the system employs a pipe-
lined information processing chain and imple-
ments a number of machine learning classifiers 
for extracting the necessary information for the 
temporal value estimation. The normalization 
step employs a number of hand-crafted vocabula-
ries for tagging single elements of a temporal 
expression and a rule-based system for estimat-
ing the temporal value. The performance of the 
system obtained the values of 0.85 and 0.84 for 
precision and recall respectively for the recogni-
tion of temporal expressions. The accuracy for 
the type and value is 0.91 and 0.55 respectively. 
 The remainder of the paper is organized as 
follows: Section 2 reports on the architecture of 
the system with single modules and describes 
theirs functions. Section 3 presents the results 
and error analysis; the conclusions are provided 
in Section 4. 
2 System Architecture 
The system is implemented in Java and follows a 
pipelined method for information processing. 
Regarding the problems it solves, it can be split 
in two sub-systems: recognition and normaliza-
tion.  
2.1 Recognition of Temporal Expressions  
This sub-system is employed for finding tempor-
al expressions in the text. It takes a sentence as 
input and looks for temporal expressions in it.   
Pre-processing: At this step the input text un-
dergoes syntactic analysis. Sentence detection, 
tokenization, part-of-speech tagging and parsing 
are applied2.  
Candidate selection: Since only certain lexi-
cal categories can be temporal expressions and 
they are defined in the TIDES standard (Ferro et 
                                                 
2
 For preprocessing we use the OpenNLP package 
(http://opennlp.sourceforge.net).  
325
al., 2003), in our implementation we consider the 
following chunk-phrases as candidates for tem-
poral expressions: nouns (week, day), proper 
names (Tuesday, May), noun phrases (last Tues-
day), adjectives (current), adjective phrases (then 
current), adverbs (currently), adverbial phrases 
(a year ago), and numbers (2000). As input it 
takes the sentences with provided syntactic in-
formation and marks phrases in the parse tree 
belonging to the above types for temporal ex-
pressions.   
Annotation alignment: If the system is used 
for training classifiers, all the candidates in a 
sentence are examined against the available an-
notations. The candidates, whose parse and anno-
tation extents aligned, are taken as positive ex-
amples and the rest is considered as negative.  
Feature Design: To produce a feature-vector 
we use most valuable features extracted for 
phrase-candidate. After a number of experiments 
the following features were selected:  
? Last token in the phrase, most probable 
token to be a temporal trigger; 
? Lemma of the last phrasal token; 
? Part-of-speech of the last phrasal token; 
? Character pattern of the last phrasal to-
ken as introduced in (Ahn et al, 2007); 
? Neighbor POS?s. The concatenated part-
of-speech tags of the last phrasal token 
and its preceding token;  
? Character pattern of the entire phrase; 
? Phrase surface. A concatenated string of 
sub-parse  types for the phrase; 
? A Boolean feature indicating nested 
complex phrasal parses, such as noun 
verb, adverbial, adjective or preposition-
al phrase; 
? Depth of the phrase. The number of the 
nested sub-parses to the deepest pre-
terminal sub-parse.  
All the features are considered as Boolean. 
Classification: Once the classifiers are trained 
they can be used for recognition of temporal ex-
pressions on test sentences. A preprocessed sen-
tence is taken as input and starting from its 
parse-tree root the candidate-phrases are classi-
fied. The most probable class will be assigned to 
the candidate under consideration. Once the 
phrase is classified as temporal expression no 
further classification of nested phrases is per-
formed, since no embedded timexes are allowed 
in the corpus. After a series of experiments with 
different machine learning techniques on the 
training data the maximum entropy classifier was 
chosen. 
Extending positive instances: Sparseness of 
annotated corpora is the biggest challenge for 
any supervised machine learning technique. To 
overcome this problem we hypothesize that 
knowledge of semantic similar words could be 
found by associating words that do not occur in 
the training set to similar words that did occur in 
the training set. Furthermore, we would like to 
learn these similarities automatically in order to 
be as much as possible independent of know-
ledge sources that might not be available for all 
languages or domains. For example, there is in 
TimeBank a temporal expression ?last summer? 
with the temporal trigger summer, but there is no 
annotation of temporal expressions built around 
the temporal trigger winter, and this means that 
no temporal expression with the trigger winter 
can be recognized. Something similar usually 
happens to any annotated corpus and we want to 
find a way how to find other temporal expres-
sions outside the available data, which can be 
used for training. On the other hand, we want to 
avoid a na?ve selection of words as, for example, 
from a gazetteer with temporal triggers, which 
may contradict with grammatical rules and the 
lexical context of a timex in text, e.g.: 
 
on Tuesday said.... 
 
But grammatically wrong by na?ve replacement 
from a gazetteer:  
? on week said*? 
? on day said*? 
? on month said* ? 
 
In order to find these words, which are legiti-
mate at a certain position in a certain context we 
use the latent word language model (LWLM) 
(Deschacht & Moens, 2009) with a Hidden Mar-
kov Model approach for estimating the latent 
word parameters.  
Complementary, we use WordNet (Miller, 
1995) as a source that can provide a most com-
plete set of words similar to the given one. One 
should note that the use of WordNet is not 
straight-forward. Due to the polysemy, the word 
sense disambiguation (WSD) problem has to be 
solved. Our system uses latent words obtained by 
the LWLM and chooses the synset with the high-
326
est overlap between WordNet synonyms and 
coordinate terms, and the latent words. The over-
lap value is calculated as the sum of LWLM 
probabilities for matching words. 
Having these two sets of synonyms and after a 
series of preliminary tests we found the setting, 
at which the system produces the highest results 
and submitted several runs with different strate-
gies:  
? Baseline (no expansion) (KUL Run 1) 
? 3 LWLM words with highest probabili-
ties (KUL Run 2) 
? 3 WordNet coordinate terms; WSD is 
solved by means of LWLM3 (KUL Run 
3)  
For each available annotation in the corpus a 
positive instance is generated. After that, the to-
ken at the most probable position for a temporal 
trigger is replaced by a synonym from the syn-
onym set found to the available token.  
2.2 Normalization of Temporal Expressions 
Normalization of temporal expressions is a 
process of estimating standardized temporal val-
ues and types. For example, the temporal expres-
sion ?summer 1990? has to be resolved to its 
value of 1990-SU and the type of DATE. In 
contrast, for the expression ?last year? the value 
cannot be estimated directly, rather it gets a mod-
ified value of another time expression.  
Due to a large variance of expressions denot-
ing the same date and vagueness in language, 
rule-based systems have been proven to perform 
better than machine-learning ones for the norma-
lization task. The current implementation follows 
a rule-based approach and takes a pre-processed 
document with recognized temporal expressions 
(as it is described in Section 2.1) and estimates a 
standardized ISO-based date/time value. In the 
following sections we provide implementation 
details of the system. 
Before the temporal value is estimated, we 
employ a classifier, which uses the same feature 
sets and classify the temporal expression among 
type classes DATE, TIME, DURATION and 
SET.  
Labeling: Labeling text is a process of provid-
ing tags to tokens of chunk-phrases from a de-
                                                 
3
 Preliminary experiments, when the most common sense in 
WordNet is chosen for increasing the number of positive 
examples, showed a low performance level and thus has not 
been proposed for evaluations. 
fined set of tags. We carefully examined availa-
ble annotated temporal expressions and annota-
tion standards to determine categories of words 
participating in temporal expressions. The fol-
lowing set of categories with labels based on se-
mantics of temporally relevant information and 
simple syntax was defined: ordinal numbers 
(first, 30th etc.), cardinal numbers (one, two, 10 
etc.), month names (Jan., January etc.), week 
day names (Mo., Monday etc.), season names 
(summer, winter etc.), parts of day (morning, 
afternoon etc.), temporal directions (ago, later, 
earlier etc.), quantifiers (several, few etc.), mod-
ifiers (recent, last etc.), approximators (almost, 
nearly etc.), temporal co-references (time, period 
etc.), fixed single token timexes (tomorrow, to-
day etc.), holidays (Christmas, Easter etc.) and 
temporal units (days, months, years etc.). Also 
fine-grained categories are introduced: day num-
ber, month number and year number. For each 
category we manually construct a vocabulary, in 
which each entry specifies a value of a temporal 
field or a final date/time value, or a method with 
parameters to apply.  
As input, the normalization takes a recognized 
temporal expression and its properties, such as 
the temporal type and the discourse type4. During 
labeling each token in a temporal expression is 
tagged with one or multiple labels corresponding 
to the categories defined above. For each of the 
categories a custom detector is implemented. The 
detector declares the method to run and the ex-
pected type of the result. The rules that imple-
ment the logics for the detector are inherited 
from an abstract class for this specific detector, 
so that if a new rule needs to be implemented its 
realization is limited to the development of one 
class, all the rest the detector does automatically. 
Besides, the order, in which detectors have to be 
run, can be specified (as for example, in case of 
fine-grained detectors). As output, the module 
provides labels of the categories to the tokens in 
the temporal expression. If there is no entry in 
the vocabulary for a token, its part-of-speech tag 
is used as the label.  
 Value estimation: Value estimation is 
implemented in the way of aggregating the 
values defined for entries in the vocabulary 
and/or executing instructions or methods 
specified. Also a set of predefined resolution 
                                                 
4
 Since in TempEval-2 the reference to the timex with re-
spect to which the value estimated is given, the normaliza-
tion module considers all timexes as deictic.   
327
rules is provided and can be extended with new 
implementations of resolution strategies.  
For resolution of complex relative temporal 
expressions, the value for which cannot be esti-
mated directly, we need to rely on additional in-
formation found at the recognition step. This in-
cludes the semantic type of the timex, discourse 
type and contextual temporal information 
(speech or document creation time, or previously 
mentioned timexes). Let?s consider the following 
temporal expression as an example: 10 days ago.  
In this example the temporal expression receives 
a modified value of another timex, namely the 
value of the document creation time. The tem-
poral expression is recognized and classified as a 
date (SEM TYPE: DATE), which refers to 
another timex (DISCOURSE TYPE: DEIC-
TIC). It takes the value of the referenced timex 
and modifies it with respect to the number (10), 
magnitude (days) and temporal direction (ago). 
Thus, the final value is calculated by subtracting 
a number of days for the value of the referenced 
timex.  
3 Results and Error Analysis  
In the Table 1 the results of the best-performing 
runs are presented.  
Table 1. Results of different runs of the system. 
As we can see the best results were obtained 
by extending available annotations with maxi-
mum 3 additional instances, which are extracted 
as coordinate terms in WordNet, whereas the 
WSD problem was solved as the greatest overlap 
between coordinate terms and latent words ob-
tained by the LWLM.  
Most of the errors at the recognition step were 
caused by misaligned parses and annotations.  
For normalization we acknowledge the signi-
ficance of estimating a proper temporal value 
with a correct link to the temporal expression 
with its value. In the TempEval-2 training data 
the links to the temporal expressions indicating 
how the value is calculated were not provided, 
and thus, the use of machine learning tools for 
training and automatic disambiguation was not 
possible. We choose a fixed strategy and all rela-
tive temporal expressions were resolved with 
respect to the document creation time, which 
caused errors with wrong temporal values and a 
low performance level.  
4 Conclusions 
For TempEval-2 we proposed a system for the 
recognition and normalization of temporal ex-
pressions. Multiple runs were submitted, among 
which the best results were obtained with auto-
matically expanded positive instances by words 
derived as coordinate terms from WordNet for 
which the proper sense was found as the greatest 
overlap between coordinate terms and latent 
words found by the LWLM.  
Acknowledgements 
This work has been funded by the Flemish gov-
ernment as a part of the project AMASS++ 
(Grant: IWT-60051) and by Space Applications 
Services NV as part of the ITEA2 project LIN-
DO (ITEA2-06011, IWT-70043). 
References  
Ahn, D., van Rantwijk, J., and de Rijke, M. 2007. A 
Cascaded Machine Learning Approach to Interpret-
ing Temporal Expressions. In Proceedings of 
NAACL-HLT 2007. 
Deschacht, K., and Moens M.-F. 2009. Using the La-
tent Words Language Model for Semi-Supervised 
Semantic Role Labeling. In Proceedings of the 
Conference on Empirical Methods in Natural Lan-
guage Processing.   
Ferro, L., Gerber, L., Mani, I., Sundheim, B., and 
Wilson, G. 2003. TIDES 2003 Standard for the 
Annotation of Temporal Expressions. 
Miller, G. A. 1995. WordNet: A Lexical Database for 
English. Communications of the ACM, 38(11): 39-
41. 
Pustejovsky, J. and Verhagen, M. 2009. SemEval-
2010 Task 13: Evaluating Events, Time Expres-
sions, and Temporal Relations (TempEval-2). In 
Proceedings of the Workshop on Semantic Evalua-
tions: Recent Achievements and Future Directions. 
Verhagen, M., Gaizauskas, R.,  Schilder, F., Hepple, 
M.,  and Pustejovsky, J. 2007. Semeval-2007 Task 
15: Tempeval Temporal Relation Identification. In 
SemEval-2007: 4th International Workshop on 
Semantic Evaluations. 
Run Recognition Normalization 
 P R F1 TYPE Acc. 
VAL 
Acc. 
1 0.78 0.82 0.8 0.91 0.55 
2 0.75 0.85 0.797 0.91 0.51 
3 0.85 0.84 0.845 0.91 0.55 
328
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 83?87, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
KUL: A Data-driven Approach to Temporal Parsing of Documents
Oleksandr Kolomiyets
KU Leuven
Celestijnenlaan 200A
Heverlee 3001, Belgium
Department of Computer Science
oleksandr.kolomiyets
@cs.kuleuven.be
Marie-Francine Moens
KU Leuven
Celestijnenlaan 200A
Heverlee 3001, Belgium
Department of Computer Science
sien.moens@cs.kuleuven.be
Abstract
This paper describes a system for temporal
processing of text, which participated in the
Temporal Evaluations 2013 campaign. The
system employs a number of machine learning
classifiers to perform the core tasks of: identi-
fication of time expressions and events, recog-
nition of their attributes, and estimation of
temporal links between recognized events and
times. The central feature of the proposed sys-
tem is temporal parsing ? an approach which
identifies temporal relation arguments (event-
event and event-timex pairs) and the semantic
label of the relation as a single decision.
1 Introduction
Temporal Evaluations 2013 (TempEval-3) is
the third iteration of temporal evaluations (after
TempEval-1 (Verhagen et al, 2007) and TempEval-
2 (Verhagen et al, 2010)) which addresses the
task of temporal information processing of text. In
contrast to the previous evaluation campaigns where
the temporal relation recognition task was simpli-
fied by restricting grammatical context (events in
adjacent sentences, events and times in the same
sentences) and proposed relation pairs, TempEval-3
does not set any context in which temporal re-
lations have to be identified. Thus, for temporal
relation recognition the challenges consist of: first,
detecting a pair of events, or an event and a time
that constitutes a temporal relation; and, second,
determining what semantic label to assign to the
proposed pair. Moreover, TempEval-3 proposes the
task of end-to-end temporal processing in which
events and times, their attributes and relations have
to be identified from a raw text input.
In this paper we present a data-driven approach
to all-around temporal processing of text. A num-
ber of machine-learning detectors were designed to
recognize temporal ?markables? (events and times)
and their attributes. The key feature of our approach
is that argument pairs, as well as relations between
them, are jointly estimated without specifying in ad-
vance the context in which these pairs have to occur.
2 Our Approach
2.1 Timex Processing
2.1.1 Timex Recognition and Normalization
The proposed method for timex recognition im-
plements a supervised machine learning approach
that processes each chunk-phrase derived from the
parse tree. Time expressions are detected by the
model as phrasal chunks in the parse with their cor-
responding spans. In addition, the model is boot-
strapped by substitutions of temporal triggers with
their synonyms learned by the Latent Words Lan-
guage Model (Deschacht et al, 2012) as described in
(Kolomiyets et al, 2011). We implemented a logis-
tic regression model that makes use of the following
features:
? the head word of the phrase and its POS tag;
? all tokens and POS tags in the phrase as a bag
of words;
? the word-shape representation of the head word
and the entire phrase, e.g. Xxxxx 99 for the
expression April 30;
83
? the condensed word-shape representation for
the head word and the entire phrase, e.g. X(x)
(9) for the expression April 30;
? the concatenated string of the syntactic types of
the children of the phrase in the parse tree;
? the depth in the parse tree.
In addition, we considered a special label for sin-
gle tokens of time expressions. In this way, we
detect parts of temporal expressions if they cannot
be found in the chunk-based fashion. In detail, if
a token is recognized as part of a timex and satis-
fies the pre-condition on its POS tag, we employ a
?look-behind? rule for the phrasal chunk to match
the begin token of the temporal expression. The le-
gitimate start POS tags are determiners, adjectives,
and cardinals. Another set of rules specifies unsuit-
able timexes, such as single cardinals with values
outside predefined ranges of day-of-month, month-
of-year and year numbers.
Normalization of temporal expressions is a pro-
cess of estimating standard temporal values and
types for temporal expressions. Due to a large vari-
ance of expressions denoting the same date and
vagueness in language, rule-based approaches are
usually employed for the normalization task, and our
implementation is a rule-based system. The nor-
malization procedure is the same as described in
(Kolomiyets and Moens, 2010), which participated
in TempEval-2.
2.2 Event Processing
The proposed method to event recognition imple-
ments a supervised machine learning approach that
classifies every single token in the input sentence as
an event instance of a specific semantic type. We im-
plemented a logistic regression model with features
largely derived from the work of Bethard and Martin
(2006):
? the token, its lemma, coarse and fine-grained
POS tags, token?s suffixes and affixes;
? token?s hypernyms and derivations in Word-
Net;
? the grammatical class of the chunk, in which
the token occurs;
? the lemma of the governing verb of the token;
? phrasal chunks in the contextual window;
? the light verb feature for the governing verb;
? the polarity of the token?s context;
? the determiner of the token and the sentence?s
subject;
In addition, we classify the tense attribute for the
detected event by applying a set of thirteen hand-
crafted rules.
2.3 Temporal Relation Processing
Temporal relation recognition is the most difficult
task of temporal information processing, as it re-
quires recognitions of argument pairs, and subse-
quent classifications of relation types. Our ap-
proach employs a shift-reduce parsing technique,
which treats each document as a dependency struc-
ture of annotations labeled with temporal relations
(Kolomiyets et al, 2012). On the one hand, the ad-
vantage of the model is that the relation arguments
and the relation between them are extracted as a sin-
gle decision of a statistical classification model. On
the other hand, such a decision is local and might
not lead to the optimal global solution1. The follow-
ing features for deterministic shift-reduce temporal
parsing are employed:
? the token, its lemma, suffixes, coarse and fine-
grained POS tags;
? the governing verb, its POS tag and suffixes;
? the sentence?s root verb, its lemma and POS
tag;
? features for a prepositional phrase occurrence,
and domination by an auxiliary or modal verb;
? features for the presence of a temporal signal in
the chunk and co-occurrence in the same sen-
tence;
? a feature indicating if the sentence root verb
lemmas of the arguments are the same;
? the temporal relation between the argument and
the document creation time (DCT) (see below);
? a feature indicating if one argument is labeled
as a semantic role of the other;
? timex value generation pattern (e.g. YYYY-MM
for 2013-02, or PXY for P5Y) and timex
granularity (e.g. DAY-OF-MONTH for Friday,
MONTH-OF-YEAR for February etc.);
1For further details on the deterministic temporal parsing
model we refer the reader to (Kolomiyets et al, 2012).
84
Training Test P R F1
TimeBank
TimeBank
10-fold
0.907 0.99 0.947
AQUAINT 0.755 0.972 0.850
Silver 0.736 0.963 0.834
AQUAINT
TimeBank 0.918 0.986 0.951
AQUAINT
10-fold
0.795 0.970 0.874
Silver 0.746 0.959 0.851
Silver
TimeBank 0.941 0.976 0.958
AQUAINT 0.822 0.955 0.883
Silver 10-fold 0.798 0.944 0.865
Table 1: Results for timex detection in different corpora.
As one of the features above provides information
about the temporal relation between the argument
and the DCT, we employ an interval-based algebra
to classify relations between timexes and the DCT.
In case the argument is an event, we use a simple
logistic regression classifier with the following fea-
tures:
? the event token, its lemma, coarse and fine-
grained POS tags;
? tense, polarity, modality and aspect attributes;
? the token?s suffixes;
? the governing verb, its POS tag, tense and the
grammatical class of the chunk, in which the
event occurs;
? preceding tokens of the chunk;
3 Results
3.1 Pre-Evaluation Results
The following results are obtained by 10-fold cross-
validations and corpus cross-validations with re-
spect to the evaluation criteria and metrics used in
TempEval-2. Tables 1 and 2 present the results for
the timex recognition and normalization tasks (Task
A), and, Tables 3 and 4 present the results for the
event recognition task (Task B).
As can be seen from the pre-evaluation results, the
most accurate classification of timexes on all cor-
pora in terms of F1 score is achieved for the model
trained on the Silver corpus. As for timex normaliza-
tion, the performances on TimeBank and the Silver
Test Corpus Type Acc. Value Acc.
TimeBank 0.847 0.742
AQUAINT 0.852 0.714
Silver 0.853 0.739
Table 2: Results for normalization in different corpora.
Training Test P R F1
TimeBank
TimeBank
10-fold
0.82 0.641 0.72
AQUAINT 0.864 0.649 0.741
Silver 0.888 0.734 0.804
AQUAINT
TimeBank 0.766 0.575 0.657
AQUAINT
10-fold
0.900 0.776 0.836
Silver 0.869 0.755 0.808
Silver
TimeBank 0.827 0.717 0.768
AQUAINT 0.906 0.807 0.854
Silver 10-fold 0.916 0.888 0.902
Table 3: Results for event detection in different corpora.
Training Test Class Acc.
TimeBank
TimeBank 10-fold 0.691
AQUAINT 0.717
Silver 0.804
AQUAINT
TimeBank 0.620
AQUAINT 10-fold 0.830
Silver 0.794
Silver
TimeBank 0.724
AQUAINT 0.829
Silver 10-fold 0.900
Table 4: Results for event classification in different cor-
pora.
corpus are not very different for type and value accu-
racies. Similarly, we observe the tendency for a bet-
ter performance on larger datasets with an exception
for 10-fold cross-validation using the AQUAINT
corpus.
3.2 Evaluation Results
For the official evaluations we submitted three runs
of the system, one of which addresses Tasks A
and B (timex and event recognition)2, one (KUL-
2During the official evaluation period, this run was re-
submitted with no changes in the output together with KUL-
TE3RunABC, which led to duplicate evaluation results known
85
Run Relaxed Evaluation
P R F1 Rank
KULRun-1 0.929 0.769 0.836 21/23
KUL-
TE3RunABC
0.921 0.754 0.829 22/23
Run Strict Evaluation
P R F1 Rank
KULRun-1 0.77 0.63 0.693 22/23
KUL-
TE3RunABC
0.814 0.667 0.733 15/23
Table 5: Results for the timex detection task.
TE3RunABC) provides a full temporal informa-
tion processing pipeline (Task ABC), and the one
for Task C only (KUL-TaskC). For KULRun-1 we
employed the recognition models described above,
all trained on the aggregated corpus comprising
all three available training corpora in the evalua-
tions. For KUL-TE3RunABC we also trained the
markable recognition models on the aggregated cor-
pus, but the event recognition output was slightly
changed in order to merge multiple consequent
events of the same semantic class into a single multi-
token event. The temporal dependency parsing
model was trained on the TimeBank and AQUAINT
corpora only, with a reduced set of relation labels.
This decision was motivated by the time constraints
and the training time needed. The final relation la-
bel set contains the following temporal relation la-
bels: BEFORE, AFTER, DURING, DURING INV,
INCLUDES and IS INCLUDED. Below we present
the obtained results for each task separately. The re-
sults for Task A are presented in Tables 5 and 6, for
Task B in Tables 7 and 8, and, for Task ABC and
Task-C-only in Table 9. It is worth mentioning that
for Task B the aspect value was provided as NONE,
thus this evaluation criterion is not representative for
our system.
4 Conclusion
For TempEval-3 we proposed a number of statisti-
cal and rule-based approaches. For Task A we em-
ployed a logistic regression classifier whose output
as KULRun-1 and KULRun-2. Further in the paper, we refer to
this run as simply to KULRun-1.
Run Rank
KULRun-1
F1
Value Type
18/23
0.629 0.741
Accuracy
Value Type
14/23
0.752 0.886
KUL-
TE3RunABC
F1
Value Type
19/23
0.621 0.733
Accuracy
Value Type
15/23
0.750 0.885
Table 6: Results for the timex normalization task.
Run P R F1 Rank
KULRun-1 0.807 0.779 0.792 5/15
KUL-
TE3RunABC
0.776 0.765 0.77 12/15
Table 7: Results for the event detection task.
Run Rank
KULRun-1
F1
Class Tense Aspect
3/15
0.701 n.a. n.a.
Accuracy
Class Tense Aspect
3/15
0.884 n.a. n.a.
KUL-
TE3RunABC
F1
Class Tense Aspect
5/15
0.687 0.497 0.632
Accuracy
Class Tense Aspect
1/15
0.891 0.644 0.82
Table 8: Results for the event attribute recognition task.
Run P R F1 Rank
KUL-
TE3RunABC
0.18 0.202 0.191 8/8
KUL-TaskC 0.234 0.265 0.248 10/13
Table 9: Results for Tasks ABC (end-to-end processing)
and C (gold entities are given).
was augmented by a small number of hand-crafted
rules to increase the recall. For the temporal ex-
86
pression normalization subtask we employed a rule-
based system which estimates the attribute values for
the recognized timexes. For Task B we proposed
a logistic regression classifier which processes in-
put tokens and classifies them as event instances of
particular semantic classes. The optional tense at-
tribute was estimated by a number of manually de-
signed rules. For the most difficult tasks, Task ABC
and Task C, we proposed a dependency parsing tech-
nique that jointly learns from data what arguments
constitute a temporal relation and what the temporal
relation label is. Due to evaluation time constraints
and the time needed to model training, we reduced
the set of relation labels and trained the model on
two small annotated corpora.
The evaluations evidenced that the use of larger
annotated data sets did not improve the timex recog-
nition performance as it was expected from the pre-
evaluations. Interestingly, we did not observe the ex-
pected improvement in terms of recall, as it was the
case in the pre-evaluations. Yet, the timex normal-
ization performance levels in the official evaluations
were slightly higher than in the pre-evaluations. In
contrast to timex recognition, the use of a large an-
notated corpus improved the results for event recog-
nition. The pilot implementation of a temporal
parser for newswire articles showed the lowest per-
formance in the evaluations for Task ABC, but still
provided decent results for Task C. One of the ad-
vantages of the proposed temporal parser is that the
parser selects arguments for a temporal relation and
classifies it at the same time. The decision is drawn
by a statistical model trained on the annotated data,
that is, the parser does not consider any particular
predefined grammatical context in which the relation
arguments have to be found. Another weak point of
the parser is that it requires a large volume of high-
quality annotations and long training times. The last
two facts made it impossible to fully evaluate the
proposed temporal parsing model, and we will fur-
ther investigate the effectiveness of the model.
Acknowledgments
The presented research was supporter by the TER-
ENCE (EU FP7-257410) and MUSE (EU FP7-
296703) projects.
References
Steven Bethard and James H Martin. 2006. Identification
of Event Mentions and their Semantic Class. In Pro-
ceedings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing, pages 146?154.
Association for Computational Linguistics.
Koen Deschacht, Jan De Belder, and Marie-Francine
Moens. 2012. The Latent Words Language Model.
Computer Speech & Language.
Oleksandr Kolomiyets and Marie-Francine Moens. 2010.
Kul: Recognition and Normalization of Temporal Ex-
pressions. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 325?328.
Association for Computational Linguistics.
Oleksandr Kolomiyets, Steven Bethard, and Marie-
Francine Moens. 2011. Model-Portability Experi-
ments for Textual Temporal Analysis. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 271?276.
Oleksandr Kolomiyets, Steven Bethard, and Marie-
Francine Moens. 2012. Extracting Narrative Time-
lines as Temporal Dependency Structures. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics, pages 88?97. Association
for Computational Linguistics.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 Task 15: TempEval Temporal
Relation Identification. In Proceedings of the 4th In-
ternational Workshop on Semantic Evaluations, pages
75?80. Association for Computational Linguistics.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 Task 13:
TempEval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 57?62. As-
sociation for Computational Linguistics.
87
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 255?262, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 3: Spatial Role Labeling
Oleksandr Kolomiyets?, Parisa Kordjamshidi?,
Steven Bethard? and Marie-Francine Moens?
?KU Leuven, Celestijnenlaan 200A, Heverlee 3001, Belgium
?University of Colorado, Campus Box 594 Boulder, Colorado, USA
Abstract
Many NLP applications require information
about locations of objects referenced in text,
or relations between them in space. For ex-
ample, the phrase a book on the desk contains
information about the location of the object
book, as trajector, with respect to another ob-
ject desk, as landmark. Spatial Role Label-
ing (SpRL) is an evaluation task in the infor-
mation extraction domain which sets a goal
to automatically process text and identify ob-
jects of spatial scenes and relations between
them. This paper describes the task in Se-
mantic Evaluations 2013, annotation schema,
corpora, participants, methods and results ob-
tained by the participants.
1 Introduction
Spatial Role Labeling at SemEval-2013 is the sec-
ond iteration of the task, which was initially in-
troduced at SemEval-2012 (Kordjamshidi et al,
2012a). The second iteration extends the previous
work with an additional training corpus, which con-
tains besides ?static? spatial relations, annotated mo-
tions. Motion detection is a novel task for annotating
trajectors (objects, which are moving), landmarks
(spatial context in which the motion is performed),
motion indicators (lexical triggers which signals tra-
jector?s motion), paths (a path along which the mo-
tion is performed), directions (absolute or relative
directions of trajector?s motion) and distances (a
distance as a product of motion). For annotating
motions the existing annotation scheme has been
adapted with additional markables which are, all to-
gether, described below.
2 Spatial Annotation Schema
In this Section we describe the annotation format of
spatial markables in text, and annotation guidelines
for the annotators.
2.1 Spatial Annotation Format
Building upon the previous work, we used the no-
tions of trajectors, landmarks and spatial indicators
as introduced by Kordjamshidi et al (2010). In ad-
dition, we further expanded the set of spatial roles
labels with motion indicators, paths, directions and
distances to capture fine-grained spatial semantics of
static spatial relations (as the ones which do not in-
volve motions), and to accommodate dynamic spa-
tial relations (the ones which do involve motions).
2.1.1 Static Spatial Relations and their Roles
Static spatial relations are defined as relations be-
tween still objects, whereas one object plays a cen-
tral role in the spatial scene, which is called tra-
jector, and the second one plays a secondary role,
and it is called landmark. In language, a spatial re-
lation between two objects is usually implemented
by a preposition (in, on, at, etc.) or a prepositional
phrase (on top of, inside of, etc.).
A static spatial relation is defined as a tuple that
contains a trajector, a landmark and a spatial indica-
tor. In the annotation schema, these annotations are
defined as follows:
Trajector: Trajector is a spatial role label as-
signed to a word or a phrase that denotes a central
object of a spatial scene. For example:
? [Trajector a lake] in the forest
255
? [Trajector a flag] on top of the building
Landmark: Landmark is a spatial role label as-
signed to a word or a phrase that denotes a secondary
object of a spatial scene, to which a possible spatial
relation (as between two objects in space) can be es-
tablished. For example:
? a lake in [Landmark the forest]
? a flag on top of [Landmark the building]
Spatial Indicator: Spatial Indicator is a spatial
role label assigned to a word or a phrase that sig-
nals a spatial relation between objects (trajectors and
landmarks) of a spatial scene. For example:
? a lake [Sp indicator in] the forest
? a flag] [Sp indicator on top of ] the building
Spatial Relation: Spatial Relation is a relation
that holds between spatial markables in text as, e.g.,
between a trajector and a landmark and triggered by
a spatial indicator. In spatial information theory the
relations and properties are usually grouped into the
domains of topological, directional, and distance re-
lations and also shape (Stock, 1998). Three semantic
classes for spatial relations were proposed:
? Region. This type refers to a region of space
which is always defined in relation to a land-
mark, e.g., the interior or exterior. For exam-
ple:
a lake in the forest =? ?Region, [Sp indicator
in], [Trajector a lake], [Landmark the forest]?
? Direction. This relation type denotes a direc-
tion along the axes provided by the different
frames of reference, in case the trajector of mo-
tion is not characterized in terms of its relation
to the region of a landmark. For example:
a flag on top of the building =? ?Direction,
[Sp indicator on top of ], [Trajector a flag],
[Landmark the building]?
? Distance. Type Distance states information
about the spatial distance of the objects and
could be a qualitative expression, such as close,
far or quantitative, such as 12 km. For example:
the kids are close to the blackboard =?
?Distance, [Distance close], [Trajector the kids],
[Landmark the blackboard]?
2.1.2 Dynamic Spatial Relations
In addition to static spatial relations and their
roles, SpRL-2013 introduces new spatial roles to
capture dynamic spatial relations which involve
motions. Let us demonstrate this with the following
example:
(1) In Brazil coming from the North-East I
stepped into the small forest and followed down a
dried creek.
The text above describes a motion, and the reader
can identify a number of concepts which are pecu-
liar for motions: there is an object whose location
is changing, the motion is performed in a specific
spatial context, with a specific direction, and with a
number of locations related to the object?s motion.
There has been an enormous effort in formalizing
and annotating motions in natural language. While
annotating motions was out of scope for the previ-
ous SpRL task and SpatialML (Mani et al, 2010),
the most recent work on the Dynamic Interval Tem-
poral Logic (DITL) (Pustejovsky and Moszkowicz,
2011) presents a framework for modeling motions
as a change of state, which adapts linguistic back-
ground considering path constructions and manner-
of-motion constructions. On this basis the Spa-
tiotemporal Markup Language (STML) has been in-
troduced for annotating motions in natural language.
In STML, a motion is treated as a change of location
over time, while differentiating between a number
of spatial configurations along the path. Being well-
defined for the formal representations of motion and
reasoning, in which representations either take ex-
plicit reference to temporal frames or reify a spatial
object for a path, all the previous work seems to be
difficult to apply in practice when annotating mo-
tions in natural language. It can be attributed to pos-
sible vague descriptions of path in natural language
when neither clear temporal event ordering, nor dis-
tinction between the start, end or intermediate path
point can be made.
In SpRL-2013, we simplify the previously intro-
duced notion of path in order to provide practical
motion annotations. For dynamic spatial relations
we introduce the following roles:
256
Trajector: Trajector is a spatial role label as-
signed to a word or a phrase which denotes an object
which moves, starts, interrupts, resumes a motion, or
is forcibly involved in a motion. For example:
? ... coming from the North-East [Trajector I]
stepped into ...
Motion Indicator: Motion indicator is a spatial
role label assigned to a word or a phrase which sig-
nals a motion of the trajector along a path. In Exam-
ple (1), a number of motion indicators can be identi-
fied:
? ... [Motion coming] from the North-East I
[Motion stepped into] ... and [Motion followed
down] ...
Path: Path is a spatial role label assigned to a word
or phrase that denotes the path of the motion as the
trajector is moving along, starting in, arriving in or
traversing it. In SpRL-2013, as opposite to STML,
the notion of path does not have the temporal dimen-
sion, thus whenever the motion is performed along a
path, for which either a start, an intermediate, an end
path point, or an entire path can be identified in text,
they are labeled as path. In Example (1), a number
of path labels can be identified:
? ... coming [Path from the North-East] I stepped
into [Path the small forest] and followed down
[Path a dried creek].
Landmark: The notion of path should not be con-
fused with landmarks. For spatial annotations, land-
mark has been introduced as a spatial role label for
a secondary object of the spatial scene. Being of
great importance for static spatial relations, in dy-
namic spatial relations, landmarks are used to cap-
ture a spatial context of a motion as for example:
? In [Landmark Brazil] coming from the North-
East ...
Distance: In contrast to the previous SpRL anno-
tation standard, in which distances and directions
have been uniformly treated as signals, in SpRL-
2013 if the motion is performed for a certain dis-
tance, and such a distance is mentioned in text, the
corresponding textual span is labeled as distance.
Distance is a spatial role label assigned to a word
or a phrase that denotes an absolute or relative dis-
tance of motion, or the distance between a trajector
and a landmark in case of a static spatial scene. For
example:
? [Distance 25 km]
? [Distance about 100 m]
? [Distance not far away]
? [Distance 25 min by car]
Direction: Additionally, if the motion is per-
formed in a certain (absolute or relative) direction,
and such a direction is mentioned in text, the corre-
sponding textual span is annotated as direction. Di-
rection is a spatial role label assigned to a word or
a phrase that denotes an absolute or relative direc-
tion of motion, or a spatial arrangement between a
trajector and a landmark. For example:
? [Direction the North-West]
? [Direction northwards]
? [Direction west]
? [Direction the left-hand side]
Spatial Relation: Similarly to static spatial rela-
tions, dynamic spatial relations are annotated by re-
lations that hold between a number of spatial roles.
The major difference to static spatial relations is the
mandatory motion indicator1. For example:
? In Brazil coming from the North-East I ...
=? ?Direction, [Sp indicator In], [Trajector I],
[Landmark Brazil], [Motion coming],[Path from
the North-East]?
? ... I stepped into the small forest and ...
=? ?Direction, [Trajector I], [Motion stepped
into],[Path the small forest]?
? ... I [...] and followed down a dried creek.
=? ?Direction, [Trajector I], [Motion followed
down],[Path a dried creek]?
1All dynamic spatial relations were annotated with type Di-
rection.
257
Corpus Files Sent. TR LM SI MI Path Dir Dis Relation
IAPR TC-12
Training 1 600 716 661 670 - - - - 765
Evaluation 1 613 872 743 796 - - - - 940
Confluence
Project
Training 95 1422 1701 1037 879 1039 945 223 307 2105
Evaluation 22 367 497 316 247 305 240 37 87 598
Table 1: Corpus statistics for SpRL-2013 with respect to annotated spatial roles (trajectors (TR), landmarks (LM),
spatial indicators (SI), motion indicators (MI), paths (Path), directions (Dir) and distances (Dis)) and spatial relations.
3 Corpora
The data for the shared task comprises two different
corpora.
3.1 IAPR TC-12 Image Benchmark Corpus
The first corpus is a subset of the IAPR TC-12 image
benchmark corpus (Grubinger et al, 2006). It con-
tains 613 text files that include 1213 sentences in to-
tal, and represents an extension of the dataset previ-
ously used in (Kordjamshidi et al, 2011). The orig-
inal corpus was available free of charge and without
copyright restrictions. The corpus contains images
taken by tourists with descriptions in different lan-
guages. The texts describe objects, and their abso-
lute and relative positions in the image. This makes
the corpus a rich resource for spatial information,
however, the descriptions are not always limited to
spatial information. Therefore, they are less domain-
specific and contain free explanations about the im-
ages. For training we released 600 sentences (about
50% of the corpus), and used remaining 613 sen-
tences for evaluations.
3.2 Confluence Project Corpus
The second corpus comes from the Confluence
project that targets the description of locations sit-
uated at each of the latitude and longitude inte-
ger degree intersection in the world. This corpus
contains user-generated content produced by, some-
times, non-native English speakers. We gathered the
content by keeping the original orthography and for-
mating. In addition, we stored the URLs of the de-
scriptions and extracted the coordinates of the de-
scribed confluence point, which might be interest-
ing for further research. In total, the entire corpus
contains 117 files with 1789 sentences (about 40,000
tokens). For training we released 95 annotated files
with 1422 sentences, 2105 annotated relations in to-
tal. For evaluation we used 22 annotated files with
367 sentences. The statistics on both corpora are
provided in Table 1.
3.3 Data Format
One important change to the data was made in
SpRL-2013. In contrast to SpRL-2012, where spa-
tial roles were annotated over ?head words? whose
indexes were part of unique identifiers, in SpRL-
2013 we switched to span-based annotations. More-
over, in order to provide a single data format for
the task, we transformed SpRL-2012 data into span-
based annotations, in course of which, we identified
a number of annotation errors and made further im-
provements for about 50 annotations.
For annotating the Confluence Project corpus we
used a freely available annotation tool MAE created
by Amber Stubbs (Stubbs, 2011). The resulting data
format uses the same annotation tags as in SpRL-
2012, but each role annotation refers to a character
offset in the original text2. Spatial relations are com-
posed of references to annotations by their unique
identifiers. Similarly to SpRL-2012, we allowed
annotators to provide non-consuming annotations,
where entity mentions, for which spatial roles can
be identified, are omitted in text but necessary for a
spatial relation triggered by either a spatial indicator
or a motion indicator. Two spatial roles are eligible
for non-consuming annotations: trajectors and land-
marks.
4 Tasks Descriptions
For the sake of consistency with SpRL-2012, in
SpRL-2013 we proposed the following tasks:
2Due to paper length constraints we omit the BNF specifica-
tions for spatial roles and relations. For further data format in-
formation we refer the reader to the task description web page:
www.cs.york.ac.uk/semeval-2013/task3/
258
? Task A: Identification of markable spans for
three types of spatial annotations such as tra-
jector, landmark and spatial indicator.
? Task B: Identification of tuples (triplets) that
connect trajectors, landmarks and spatial indi-
cators identified in Task A into spatial relations.
That is, identification of spatial relations with
three markables connected, and without se-
mantic relation classification.
? Task C: Identification of markable spans for all
spatial annotations such as trajector, landmark,
spatial indicator, motion indicator, path, direc-
tion and distance.
? Task D: Identification of n-tuples that connect
spatial markables identified in Task C into spa-
tial relations. That is, identification of spatial
relations with as many participating mark-
ables as possible, and without semantic rela-
tion classification.
? Task E: Semantic classification of spatial rela-
tions identified in Task D.
5 Evaluation Criteria and Metrics
System outputs were evaluated against the gold
annotations, which had to conform to the role?s
Backus-Naur form. For Tasks A and C, the system
annotations are spatial roles: spans of text associated
with spatial role types. A system annotation of a
role is considered correct if it has a minimal overlap
of one character with a gold annotation and matches
the role type of the gold annotation. For Tasks B and
D, the system annotations are spatial relation tuples
(of length 3 in task B, of length 3 to 5 in Task D) of
references to markable annotations. A system anno-
tation of a spatial relation tuple is considered correct
if it is of the same length as the gold annotation, and
if each spatial role in the system tuple matches each
role in the gold tuple. A spatial role estimated by a
system is considered correct if it matches a gold ref-
erence when having the same character offsets and
markable types (strict evaluation settings). In ad-
dition we introduced relaxed evaluation settings, in
which a minimal overlap of one character between
a system and a gold markable references is required
for a positive match under condition that the roles
match. For Task E, the system annotations are spa-
tial relation tuples of length 3 to 5, along with re-
lation type labels. A system annotation of a spatial
relation is considered correct if the spatial relation
tuple is correct under the evaluation of Task D and
the relation type of the system relation is the same
as the relation type of the gold relation.
Systems were evaluated for each of the tasks in
terms of precision (P), recall (R) and F1-score which
are defined as follows:
Precision =
tp
tp + fp
(1)
Recall =
tp
tp + fn
(2)
where tp is the number of true positives (the num-
ber of instances that are correctly found), fp is the
number of false positives (number of instances that
are predicted by the system but not a true instance),
and fn is the number of false negatives (missing re-
sults).
F1 = 2 ?
Precision ?Recall
Precision + Recall
(3)
6 System Description and Evaluation
Results
UNITOR. The UNITOR-HMM-TK system ad-
dressed Tasks A,B and C (Bastianelli et al, 2013).
In Tasks A and C, roles are labeled by a sequence-
based classifier: each word in a sentence is classi-
fied with respect to the possible spatial roles. An
approach based on the SVM-HMM learning algo-
rithm, formulated in (Tsochantaridis et al, 2006),
was used. It is in line with other methods based
on sequence-based classifier for Spatial Role La-
beling, such as Conditional Random Fields (Kord-
jamshidi et al, 2011), and the same SVM-HMM
learning algorithm (Kordjamshidi et al, 2012b).
UNITOR?s labeling approach has been inspired by
the work in (Croce et al, 2012), where an SVM-
HMM learning algorithm has been applied to the
classical FrameNet-based Semantic Role Labeling.
The main contribution of the proposed approach is
the adoption of shallow grammatical features instead
of the full syntax of the sentence, in order to avoid
over-fitting on the training data. Moreover, lexical
information has been generalized through the use
259
Run Task Evaluation Label P R F1-score
UNITOR.Run1.1
Task A relaxed
TR 0.684 0.681 0.682
LM 0.741 0.835 0.785
SI 0.967 0.889 0.926
Task B
relaxed Relation 0.551 0.391 0.458
strict Relation 0.431 0.306 0.358
UNITOR.Run1.2
Task A relaxed
TR 0.682 0.493 0.572
LM 0.801 0.560 0.659
SI 0.968 0.585 0.729
Task B
relaxed Relation 0.551 0.391 0.458
strict Relation 0.431 0.306 0.358
UNITOR.Run2.1
Task A relaxed
TR 0.565 0.317 0.406
LM 0.661 0.476 0.554
SI 0.612 0.481 0.538
Task C relaxed
TR 0.565 0.317 0.406
LM 0.662 0.476 0.554
SI 0.609 0.479 0.536
MI 0.892 0.294 0.443
Path 0.775 0.295 0.427
Dir 0.312 0.229 0.264
Dis 0.946 0.331 0.490
Table 2: Results of UNITOR for SpRL-2013 tasks (Task A, B and C).
of Word Space ? a Distributional Model of Lexi-
cal Semantics derived from the unsupevised anal-
ysis of an unlabeled large-scale corpus (Sahlgren,
2006). Similarly to the approaches demonstrated
in SpRL-2012, the proposed approach first classi-
fies spatial and motion indicators, then, using these
outcomes further spatial roles are determined. For
classifying indicators, the classifier makes use of
lexical and grammatical features like lemmas, part-
of-speech tags and lexical context representations.
The remaining spatial roles are estimated by another
classifier additionally employing the lemma of the
indicator, distance and relative position to the indi-
cator, and the number of tokens composing the indi-
cator as features.
In Task B, all roles found in a sentence for Task A
are combined to generate candidate relations, which
are verified by a Support Vector Machine (SVM)
classifier. As the entire sentence is informative
to determine the proper conjunction of all roles, a
Smoothed Partial Tree Kernel (SPTK) within the
classifier that enhances both syntactic and lexical in-
formation of the examples was applied (Croce et al,
2011). This is a convolution kernel that measures the
similarity between syntactic structures, which are
partially similar and whose nodes can be different,
but are, nevertheless, semantically related. Each ex-
ample is represented as a tree-structure which is di-
rectly derived from the sentence dependency parse,
and thus allows for avoiding manual feature engi-
neering as in contrast to the work of Roberts and
Harabagiu (2012). In the end, the similarity score
between lexical nodes is measured by the Word
Space model.
UNITOR submitted two runs for the IAPR TC-
12 Image benchmark corpus (we refer to them
as to UNITOR.Run1.1 and UNITOR.Run1.2) and
one run for the Confluence Project corpus (UN-
ITOR.Run2.1), based on the models individually
trained on the different corpora. The difference
between UNITOR.Run1.1 and UNITOR.Run1.2 is
that for UNITOR.Run1.1 the results are obtained for
all spatial roles (also the ones that have no spatial
relation), and UNITOR.Run1.2 only provided the
roles for which also spatial relations were identified.
The results are presented in Table 2.
260
Although, not directly comparable to the results in
SpRL-2012, one may observe some common trends.
First, similarly to the previous findings, the perfor-
mance for recognition of landmarks and spatial in-
dicators (Task A) on the IAPR TC-12 Image bench-
mark corpus is better than trajectors (F1-scores of
0.785, 0.926 and 0.682 respectively), and spatial in-
dicators is the ?easiest? spatial role to recognize (F1-
score of 0.926).
In contrast, spatial role labeling on the Confluence
Project corpus performs worse than on the IAPR
TC-12 Image benchmark corpus (with F1-scores of
0.406, 0.538 and 0.554 for trajectors, spatial indica-
tors and landmarks respectively). Interestingly, the
performance for landmarks is generally higher than
for trajectors, which is in line with previous findings
in SpRL-2012. The performance drop on the new
corpus can be attributed to more complex text and
descriptions, whereas multiple roles can be identi-
fied for the same span (for example, a path which
spans over trajectors, landmarks and spatial indica-
tors). For the new spatial roles of motion indicators,
paths, directions and distances, the performance lev-
els are overall higher than for trajectors with an ex-
ception of directions. Yet, the precision levels for
new roles is much higher than the recall (0.892 vs.
0.294 for motion indicators, 0.775 vs. 0.295 for
paths and 0.946 vs. 0.331 for distances). Directions
turned out to be the most difficult role to classify
(0.312, 0.229 and 0.264 for P , R and F1-score re-
spectively).
7 Conclusion
In this paper we described an evaluation task on Spa-
tial Role Labeling in the context of Semantic Evalu-
ations 2013. The task sets a goal to automatically
process text and identify objects of spatial scenes
and relations between them. Building largely upon
the previous evaluation campaign, SpRL-2012, in
SpRL-2013 we introduced additional spatial roles
and relations for capturing motions in text. In ad-
dition, a new annotated corpus for spatial roles (in-
cluding annotated motions) was produced and re-
leased to the participants. It comprises a set of 117
files with about 40,000 tokens in total.
With the registered number of 10 participants and
the final number of submissions (only one) we can
conclude that spatial role labeling is an interesting
task within the research community, however some-
times underestimated in its complexity. Our further
steps in promoting spatial role labeling will be a de-
tailed description of the annotation scheme and an-
notation guidelines, analysis of the corpora and ob-
tained results.
Acknowledgments
The presented research was supporter by the PARIS
project (IWT - SBO 110067), TERENCE (EU FP7?
257410) and MUSE (EU FP7?296703).
References
Emanuele Bastianelli, Danilo Croce, Roberto Basili, and
Daniele Nardi. 2013. UNITOR-HMM-TK: Struc-
tured Kernel-based learning for Spatial Role Labeling.
In Proceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval 2013). Association
for Computational Linguistics.
Danilo Croce, Alessandro Moschitti, and Roberto Basili.
2011. Structured Lexical Similarity via Convolution
Kernels on Dependency Trees. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1034?1046. Association for
Computational Linguistics.
Danilo Croce, Giuseppe Castellucci, and Emanuele Bas-
tianelli. 2012. Structured Learning for Semantic Role
Labeling. Intelligenza Artificiale, 6(2):163?176.
Michael Grubinger, Paul Clough, Henning Mu?ller, and
Thomas Deselaers. 2006. The IAPR TC-12 Bench-
mark: A New Evaluation Resource for Visual Informa-
tion Systems. In International Workshop OntoImage,
pages 13?23.
Parisa Kordjamshidi, Marie-Francine Moens, and Mar-
tijn van Otterlo. 2010. Spatial Role Labeling: Task
Definition and Annotation Scheme. In Proceedings
of the Seventh Conference on International Language
Resources and Evaluation (LREC?10), pages 413?420.
Parisa Kordjamshidi, Martijn Van Otterlo, and Marie-
Francine Moens. 2011. Spatial Role Labeling: To-
wards Extraction of Spatial Relations from Natural
Language. ACM Transactions on Speech and Lan-
guage Processing (TSLP), 8(3):4.
Parisa Kordjamshidi, Steven Bethard, and Marie-
Francine Moens. 2012a. Semeval-2012 Task 3: Spa-
tial Role Labeling. In Proceedings of the Sixth In-
ternational Workshop on Semantic Evaluation, pages
365?373. Association for Computational Linguistics.
Parisa Kordjamshidi, Paolo Frasconi, Martijn Van Ot-
terlo, Marie-Francine Moens, and Luc De Raedt.
261
2012b. Relational Learning for Spatial Relation Ex-
traction from Natural Language. In Inductive Logic
Programming, pages 204?220. Springer.
Inderjeet Mani, Christy Doran, Dave Harris, Janet Hitze-
man, Rob Quimby, Justin Richer, Ben Wellner, Scott
Mardis, and Seamus Clancy. 2010. SpatialML: Anno-
tation Scheme, Resources, and Evaluation. Language
Resources and Evaluation, 44(3):263?280.
James Pustejovsky and Jessica L Moszkowicz. 2011.
The Qualitative Spatial Dynamics of Motion in Lan-
guage. Spatial Cognition & Computation, 11(1):15?
44.
Kirk Roberts and Sanda M Harabagiu. 2012. UTD-
SpRL: A Joint Approach to Spatial Role Labeling. In
Proceedings of the Sixth International Workshop on
Semantic Evaluation, pages 419?424. Association for
Computational Linguistics.
Magnus Sahlgren. 2006. The Word-space Model. Ph.D.
thesis, Stockholm University.
Oliviero Stock. 1998. Spatial and Temporal Reasoning.
Springer-Verlag New York Incorporated.
Amber Stubbs. 2011. MAE and MAI: Lightweight An-
notation and Adjudication Tools. In Proceedings of
the 5th Linguistic Annotation Workshop, LAW V ?11,
pages 129?133, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-
mann, Yasemin Altun, and Yoram Singer. 2006. Large
Margin Methods for Structured and Interdependent
Output Variables. Journal of Machine Learning Re-
search, 6(2):1453.
262
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 263?271,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
KU Leuven at HOO-2012: A Hybrid Approach to Detection and Correction
of Determiner and Preposition Errors in Non-native English Text
Li Quan, Oleksandr Kolomiyets, Marie-Francine Moens
Department of Computer Science
KU Leuven
Celestijnenlaan 200A
3001 Heverlee, Belgium
li.quan@student.kuleuven.be
{oleksandr.kolomiyets, sien.moens}@cs.kuleuven.be
Abstract
In this paper we describe the technical im-
plementation of our system that participated
in the Helping Our Own 2012 Shared Task
(HOO-2012). The system employs a num-
ber of preprocessing steps and machine learn-
ing classifiers for correction of determiner and
preposition errors in non-native English texts.
We use maximum entropy classifiers trained
on the provided HOO-2012 development data
and a large high-quality English text collec-
tion. The system proposes a number of highly-
probable corrections, which are evaluated by a
language model and compared with the origi-
nal text. A number of deterministic rules are
used to increase the precision and recall of the
system. Our system is ranked among the three
best performing HOO-2012 systems with a
precision of 31.15%, recall of 22.08% and F1-
score of 25.84% for correction of determiner
and preposition errors combined.
1 Introduction
The Helping Our Own Challenge (Dale and Kilgar-
riff, 2010) is a shared task that was proposed to ad-
dress automated error correction of non-native En-
glish texts. In particular, the Helping Our Own 2012
Shared Task (HOO-2012) (Dale et al, 2012) focuses
on determiners and prepositions as they are well-
known sources for errors produced by non-native
English writers. For instance, Bitchener et al (2005)
reported error rates of respectively 20% and 29%.
Determiners are in particular challenging because
they depend on a large discourse context and world
knowledge, and moreover, they simply do not exist
in many languages, such as Slavic and South-East
Asian languages (Ghomeshi et al, 2009). The use
of prepositions in English is idiomatic and thus very
difficult for learners of English. On the one hand,
prepositions connect noun phrases to other words in
a sentence (e.g. . . . by bus), on the other hand, they
can also be part of phrasal verbs such as carry on,
hold on, etc.
In this paper we describe our system implemen-
tation and results in HOO-2012. The paper is struc-
tured as follows. Section 2 gives the task definition,
errors addressed, data resources and evaluation cri-
teria and metrics. Section 3 shows some background
and related work. Section 4 gives the full system de-
scription, while Section 5 reports and discusses the
results of the experiments. Section 6 concludes with
an error analysis and possible further improvements.
2 HOO-2012 Tasks and Resources
2.1 Tasks
In the scope of HOO-2012 the following six possible
error types1 are targeted:
? Replace determiner (RD):
Have the nice day. ? Have a nice day.
? Missing determiner (MD):
That is great idea. ? That is a great idea.
? Unnecessary determiner (UD):
I like the pop music. ? I like pop music.
1The set of error tags is based on the Cambridge University
Press Error Coding System, fully described in (Nicholls, 2003).
263
? Replace preposition (RT):
In the other hand. . . ? On the other hand. . .
? Missing preposition (MT):
She woke up 6 o?clock. ? She woke up at 6
o?clock.
? Unnecessary preposition (UT):
He must go to home. ? He must go home.
2.2 Data
The HOO development dataset consists of 1000
exam scripts drawn from a subset of the CLC FCE
Dataset (Yannakoudakis et al, 2011). This corpus
contains texts written by students who attended the
Cambridge ESOL First Certificate in English exam-
ination in 2000 and 2001. The entire development
dataset comprises 374680 words, with an average
of 375 words per file. The test data consists of a
further 100 files provided by Cambridge University
Press (CUP), with 18013 words, and an average of
180 words per file.
Type # Dev # Test A # Test B
RD 609 38 37
MD 2230 125 131
UD 1048 53 62
Det 3887 217 230
RT 2618 136 148
MT 1104 57 56
UT 822 43 39
Prep 4545 236 243
Total 8432 453 473
Words/Error 44.18 39.77 38.08
Table 1: Data error statistics.
Counts of the different error types are provided in
Table 1. The table shows counts for the development
dataset (?Dev?) and two versions of the gold stan-
dard test data: the original version as derived from
the CUP-provided dataset (?Test A?), and a revised
version (?Test B?) which was compiled in response
to requests for corrections from participating teams.
The datasets and the revision process are further ex-
plained in (Dale et al, 2012).
2.3 Evaluation Criteria and Metrics
For evaluation in the HOO framework, a distinction
is made between scores and measures. The com-
plete evaluation mechanism is described in detail in
(Dale and Narroway, 2012) and on the HOO-2012
website.2
Scores Three different scores are used:
1. Detection: does the system determine that an
edit of the specified type is required at some
point in the text?
2. Recognition: does the system correctly deter-
mine the extent of the source text that requires
editing?
3. Correction: does the system offer a correction
that is identical to that provided in the gold
standard?
Measures For each score, three measures are cal-
culated: precision (1), recall (2) and F -score (3).
precision =
tp
tp+ fp
(1)
recall =
tp
tp+ fn
(2)
where tp is the number of true positives (the num-
ber of instances that are correctly found by the sys-
tem), fp the number of false positives (the number
of instances that are incorrectly found), and fn the
number of false negatives (missing results).
F? = (1 + ?
2)
precision ? recall
?2 ? precision+ recall
(3)
where ? is used as a weight factor regulating the
trade-off between recall and precision. We use the
balanced F -score, i.e. ? = 1, such that recall and
precision are equally weighted.
Combined We provide results on prepositions and
determiners combined, and for each of these two
subcategories separately. We also report on each of
the different error types separately.
2See http://www.correcttext.org/hoo2012.
264
3 Related Work
HOO-2012 follows on from the HOO-2011 Shared
Task Pilot Round (Dale and Kilgarriff, 2011). That
task targeted a broader range of error types, and used
a much smaller dataset.
Most work on models for determiner and preposi-
tion generation has been developed in the context of
machine translation output (e.g. (Knight and Chan-
der, 1994), (Minnen et al, 2000), (De Felice and
Pulman, 2007) and (Toutanova and Suzuki, 2007)).
Some of these methods depend on full parsing of
text, which is not reliable in the context of noisy
non-native English texts.
Only more recently, models for automated error
detection and correction of non-native texts have
been explicitly developed and studied. Most of these
methods use large corpora of well-formed native En-
glish text to train statistical models, e.g. (Han et al,
2004), (Gamon et al, 2008) and (De Felice and Pul-
man, 2008). Yi et al (2008) used web counts to de-
termine correct article usage, while Han et al (2010)
trained a classifier solely on a large error-tagged
learner corpus for preposition error correction.
4 System Description
4.1 Global System Workflow
The system utilizes a hybrid approach that combines
statistical machine learning classifiers and a rule-
based system. The global system architecture is pre-
sented in Figure 1. This section describes the global
system workflow. The subsequent sections elabo-
rate on the machine learning classifiers and heuris-
tics implemented in the system.
The system workflow is divided in the following
processing steps:
1. Text Preprocessing: The system performs a
preliminary text analysis by automated spelling
correction and subsequent syntactic analysis,
such as tokenization and part-of-speech (POS)
tagging.
2. Error Detection, Recognition and Correction:
The system identifies if a correction is needed,
and the type and extent of that correction. Two
families of error correction tasks that separately
address determiners and prepositions are per-
formed in parallel.
3. Correction validation: Once a correction has
been proposed, it is validated by a language
model derived from a large corpus of high-
quality English text.
4.1.1 Text Preprocessing
In HOO-2012, texts submitted for automated cor-
rections are written by learners of English. Besides
the error types that are addressed in HOO-2012, mis-
spellings are another type of highly-frequent errors.
For example, one student writes the following: In my
point of vue, Internet is the most important discover
of the 2000 centery.
When using automated natural language process-
ing tools, incorrect spelling (and grammar) can in-
troduce an additional bias. To reduce the bias propa-
gated from the preprocessing steps, the text is first
automatically corrected by the open-source spell
checker GNU Aspell.3
At the next step, the text undergoes a shallow syn-
tactic analysis that includes sentence boundary de-
tection, tokenization, part-of-speech tagging, chunk-
ing, lemmatization, relation finding and preposi-
tional phrase attachment. These tasks are performed
by MBSP (De Smedt et al, 2010).4
4.1.2 Error Detection, Recognition and
Correction
In general, the task of automated error correction
is addressed by a number of subtasks of finding the
position in text, recognizing the type of error, and
the proposal for a correction. In our implementation
we approach these tasks in a two-step approach as
proposed in (Gamon et al, 2008). With two families
of errors, the system therefore employs four classi-
fiers in total.
For determiner error corrections, a classifier (C1
in Figure 1) first predicts whether a determiner is
required in the observed context. If it is required,
another classifier (C2 in Figure 1) estimates which
one. The same approach is employed for the prepo-
sition error correction task (classifiers C3 and C4 in
Figure 1). The details on how the classifiers were
implemented are highlighted in Section 4.2.
3http://aspell.net/
4MBSP is a text analysis system based on the TiMBL and
MBT memory based learning applications developed at CLiPS
and ILK (Daelemans and van den Bosch, 2005).
265
Figure 1: System architecture.
4.1.3 Correction Validation
Our error correction system implements a correc-
tion validation mechanism as proposed in (Gamon et
al., 2008). The validation mechanism makes use of
a language model that is derived from a large corpus
of English. We use a trigram language model trained
on the English Gigaword corpus with a 64K-word
vocabulary (using interpolated Kneser-Ney smooth-
ing with a bigram cutoff of 3 and trigram cutoff of
5).
The language model serves to increase the pre-
cision at the cost of recall as false positives can be
confusing for learners for English. The original sen-
tence and the error-corrected version are passed to
the language model. Only if the difference in proba-
bility of being generated by the language model ex-
ceeds a heuristic threshold (estimated using a tuning
set) is the correction finally accepted.
4.2 Machine Learning Classifiers
As already mentioned, the system employs four ma-
chine learning classifiers in total (C1?C4 ? two for
each family of errors). Classifiers C1 and C3 re-
spectively estimate the presence of determiners and
prepositions in the observed context. If one is ex-
pected, the second set of classifiers estimates which
one is the most likely.
For the determiner choice classifier (C2), we re-
strict the determiner choice class values to the indef-
inite and definite articles: a/an and the. The prepo-
sition choice class values for the preposition choice
classifier (C4) are restricted to set of the following
10 common prepositions: on, in, at, for, of, about,
from, to, by, with and (other).
All the classifiers are implemented by discrimina-
tive maximum entropy classification models (ME)
(Ratnaparkhi, 1998). Such models have been proven
effective for a number of natural language process-
ing tasks by combining heterogeneous forms of evi-
dence (Ratnaparkhi, 2010).
Training Classifiers and Inference As training
instances we consider each noun phrase (NP) in ev-
ery sentence of the training data. For the binary clas-
sifiers (C1 and C3), a positive example is a noun
phrase that follows a determiner/preposition, and a
negative example is one that does not. The multi-
class classifiers (C2 and C4) are trained respectively
to distinguish specific instances of determiners (defi-
nite and indefinite for C2) and the set of prepositions
mentioned above. For each classifier, a training in-
stance is represented by the following features:
? Tokens in NP.
? Tokens? POS tags in NP.
? Tokens? lemmas in NP.
? Tokens in a contextual window of 3 tokens to
the left and to the right from the potential cor-
rection position.
? Tokens? POS tags in a contextual window of 3
tokens from the potential correction position.
? Tokens? lemmas in a contextual window of 3
tokens from the potential correction position.
? Trigrams of concatenated tokens before and af-
ter NP.
266
? Trigrams of concatenated tokens? POS tags be-
fore and after NP.
? Trigrams of concatenated tokens? lemmas be-
fore and after NP.
? Head noun in NP.
? POS tag of head noun in NP.
? Lemma of head noun in NP.
Once the classification models have been derived,
the classifiers are ready to be employed in the sys-
tem. For the text correction task, each sentence
undergoes the same preprocessing analysis as de-
scribed in Section 4.1.1. Then, for each noun phrase
in the input sentence, we extract the feature con-
text, and use the models to predict the need for
the presence of a determiner or preposition, and if
so, which one. Our system only accepts classifier
predictions if they are obtained with a high confi-
dence. The confidence thresholds were empirically
estimated from pre-evaluation experiments with a
tuning dataset (Section 5.1).
4.3 Rule-based Modules
Our system also has a number of rule-based mod-
ules. The first rule-based module is in charge of
making the choice between a and an if the deter-
miner type classifier (C2) predicts the presence of
an indefinite determiner. The choice is determined
by a lookup in the CMU pronouncing dictionary5
(a/an CMU Dictionary in Figure 1). In this dictio-
nary each word entry is mapped to one or a number
of pronunciations in the phonetic transcription code
system Arpabet. If the pronunciation of the word
that follows the estimated correction position starts
with a consonant, a is used; if it starts with a vowel,
an is selected.
The second rule-based module corrects confusion
errors of determiner-noun agreement, e.g. this/these
and that/those (Definite Determiner in Figure 1). It
is implemented by introducing rules with patterns
based on whether the noun was tagged as singular
or plural.
The third rule-based module is used to filter out
unnecessary corrections proposed by the classifiers
5http://www.speech.cs.cmu.edu/cgi-bin/
cmudict
(C1-C4) and augmented by the already described
rule-based modules. Each correction is examined
against the input text and if it yields a different text
than the original input text, such a correction is con-
sidered as a necessary correction.
However, sometimes automatically proposed cor-
rections have to be rejected because they are out of
scope of the addressed errors. We do not replace
possessive determiners such as my, your, his, our,
their by the definite article the. Similarly, some
prepositions can be grouped in opposite pairs, for
example from and to, for which we do not propose
any correction as it requires a deep semantic analysis
of text.
5 Experiments and Results
In this section we describe the pre-evaluation exper-
iments and the results of the final evaluation on the
HOO-2012 test set. Table 2 shows the characteris-
tics of the datasets used in the experiments.
Dataset Sentences Tokens
HOO training 21925 340693
HOO tuning 2560 40966
HOO held-out 2749 42325
Reuters 207083 5487021
Wikipedia 53370 1430428
HOO test 1376 20606
Table 2: Datasets used.
5.1 Pre-Evaluation Experiments
In the course of system development, we split the
files in the HOO development dataset into a train-
ing set (80%), a tuning set (10%) and a held-out test
set (10%). From the beginning it was clear that the
provided development dataset alne was too small to
address the automated error correction tasks by em-
ploying machine learning classification techniques.
Additionally to that dataset, we used a set of Reuters
news data and the Wikipedia corpus for training the
classifiers.
Once the classification models had been derived,
the system was evaluated on the tuning data and ad-
justed in order to increase the overall performance.
267
After that, the system was evaluated on the held-out
test set for which the results are shown in Table 3.
Type Precision Recall F1-score
Det 64.11 14.89 24.17
Prep 52.32 16.38 25.32
All 60.19 15.38 24.50
Table 3: Correction results on held-out test set.
5.2 Final System Configuration and Evaluation
Results
For the final evaluation, we retrained the models us-
ing the complete HOO development data (again, in
addition to the Reuters and Wikipedia corpus men-
tioned above). The number of training instances are
shown in Table 4.
Classifier # Training instances
C1 1746128
C2 530885
C3 1763784
C4 706775
Table 4: Number of training instances used for the
ME models.
In the HOO framework, precision and recall are
weighted equally. However, in the domain of error
correction for non-native writers, precision is prob-
ably more important because false positives can be
very confusing and demotivating for learners of En-
glish. For this reason, we submitted two different
runs which also gave us insights into the impact of
the language model. ?Run 0? denotes the system ex-
cluding the language model and using lower thresh-
olds, such that neither precision nor recall is favored
in particular, while ?Run 1? focuses on precision
by using the language model as a filter, and having
higher thresholds. Thus, we present the results for
two different runs on the final HOO test set, both
before and after manual revision (see Section 2.2).
Table 5 presents the results for recognition and Ta-
ble 6 those for correction.
The difficulty of the HOO 2012 Shared Task is
reflected by rather low system performance levels
(Dale et al, 2012). Nonetheless, we observed some
interesting patterns. In terms of the overall system
performance, our system achieved better results for
determiner errors than for preposition errors.
With respect to determiners, missing determiners
are handled best by our system, while unnecessary
determiners and replacement errors are more diffi-
cult. Concerning prepositions, missing prepositions
are found to be the most challenging. This confirms
the difficulty of choosing the right preposition due to
the large number of possible alternatives, and their
sometimes subtle differences in usage and meaning.
While ?Run 1? achieved a higher precision (at the
cost of recall), ?Run 0? performed better in terms of
overall performance (F1-score). This result can be
explained by the relative small size and limited tun-
ing of the language model. Moreover, it also shows
that the use of the F1-score might not be the most
informative evaluation metric in this context.
6 Conclusions
Determiners and prepositions present real chal-
lenges for non-native English writers. For auto-
mated determiner and preposition error correction
in HOO-2012, we implemented a hybrid system
that combines statistical machine learning classifiers
and a rule-based system. By employing a language
model for correction validation, the system achieved
a precision of 42.16%, recall of 9.49% and F1-score
of 15.50%. Without the language model, a preci-
sion of 31.15%, recall of 22.08% and F1-score of
25.84% were reached, and our system was ranked
third in terms of F1-score.
Three major bottlenecks were identified in the im-
plementation: (i) spelling errors should first be cor-
rected due to the noisy input texts; (ii) classifier
thresholds must be carefully adjusted to minimize
false positives; and (iii) overall, preposition errors
are handled worse than determiner errors, although
there is also a large difference among the various er-
ror types.
For future work, we will focus on models that ex-
plicitly utilize the writer?s background. Also, a full
evaluation of the system should include a thorough
user-centric study with evaluation criteria and met-
rics beyond the traditional precision, recall and F -
score.
268
Type Precision Recall F1-score
RD 17.95 17.95 17.95
MD 60.76 38.40 47.06
UD 22.67 32.08 26.56
Det 37.31 33.18 35.12
RT 55.88 13.97 22.35
MT 50.00 5.26 9.52
UT 14.77 30.23 19.85
Prep 27.34 14.83 19.23
All 33.33 23.62 27.65
(a) Run 0 (before revision)
Type Precision Recall F1-score
RD 19.44 17.95 18.67
MD 65.82 39.69 49.52
UD 26.67 32.26 29.20
Det 40.93 34.50 37.44
RT 61.76 14.09 22.95
MT 50.00 5.36 9.68
UT 15.91 35.90 22.05
Prep 29.69 15.57 20.43
All 29.47 24.74 29.47
(b) Run 0 (after revision).
Type Precision Recall F1-score
RD 37.50 7.69 12.77
MD 66.67 12.80 21.48
UD 16.67 1.89 3.39
Det 52.63 9.22 15.69
RT 51.61 11.76 19.16
MT 40.00 3.51 6.45
UT 32.14 20.93 25.35
Prep 42.19 11.44 18.00
All 46.08 10.38 16.94
(c) Run 1 (before revision).
Type Precision Recall F1-score
RD 37.50 8.33 13.64
MD 79.17 14.50 24.52
UD 33.33 3.23 5.88
Det 63.16 10.48 17.98
RT 54.84 11.41 18.89
MT 40.00 3.57 6.56
UT 35.71 25.64 29.85
Prep 45.31 11.89 18.83
All 51.96 11.21 18.43
(d) Run 1 (after revision).
Table 5: Recognition results of the runs on the test set.
269
Type Precision Recall F1-score
RD 17.95 17.95 17.95
MD 54.43 34.40 42.16
UD 22.67 32.08 26.56
Det 34.72 30.88 32.68
RT 50.00 12.50 20.00
MT 50.00 5.26 9.52
UT 14.77 30.23 19.85
Prep 25.78 13.98 18.13
All 31.15 22.08 25.84
(a) Run 0 (before revision)
Type Precision Recall F1-score
RD 17.95 19.44 18.67
MD 59.49 35.88 44.76
UD 26.67 32.26 29.20
Det 38.34 32.31 35.07
RT 55.88 12.75 20.77
MT 50.00 5.36 9.68
UT 15.91 35.90 22.05
Prep 28.13 14.81 19.41
All 34.27 23.26 27.71
(b) Run 0 (after revision).
Type Precision Recall F1-score
RD 37.50 7.69 12.77
MD 62.50 12.00 20.13
UD 16.67 1.89 3.39
Det 50.00 8.76 14.90
RT 41.94 9.56 15.57
MT 40.00 3.51 6.45
UT 32.14 20.93 25.35
Prep 37.50 10.17 16.00
All 42.16 9.49 15.50
(c) Run 1 (before revision).
Type Precision Recall F1-score
RD 37.50 8.33 13.64
MD 75.00 13.74 23.23
UD 33.33 3.23 5.88
Det 60.05 10.04 17.23
RT 45.16 9.40 15.56
MT 40.00 3.57 6.56
UT 35.71 25.64 29.85
Prep 40.63 10.66 16.88
All 48.04 10.36 17.04
(d) Run 1 (after revision).
Table 6: Correction results of the runs on the test set.
270
References
John Bitchener, Stuart Young, and Denise Cameron.
2005. The effect of different types of corrective feed-
back on ESL student writing. Journal of Second Lan-
guage Writing, 14:191?205.
Walter Daelemans and Antal van den Bosch. 2005.
Memory-Based Language Processing. Studies in
Natural Language Processing. Cambridge University
Press.
Robert Dale and Adam Kilgarriff. 2010. Helping Our
Own: Text massaging for computational linguistics
as a new shared task. In Proceedings of the 6th In-
ternational Natural Language Generation Conference,
pages 261?266, Dublin, Ireland, 7?9 July 2010.
Robert Dale and Adam Kilgarriff. 2011. Helping Our
Own: The HOO 2011 pilot shared task. In Pro-
ceedings of the 13th European Workshop on Natural
Language Generation, pages 242?249, Nancy, France,
28?30 September 2011.
Robert Dale and George Narroway. 2012. A frame-
work for evaluating text correction. In Proceedings of
the Eight International Conference on Language Re-
sources and Evaluation, Istanbul, Turkey, 21?27 May
2012.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A report on the preposition and de-
terminer error correction shared task. In Proceedings
of the Seventh Workshop on Innovative Use of NLP for
Building Educational Applications, Montreal, Canada,
3?8 June 2012.
Rachele De Felice and Stephen G. Pulman. 2007. Au-
tomatically acquiring models of preposition use. In
Proceedings of the Fourth ACL-SIGSEM Workshop on
Prepositions, pages 45?50, Prague, Czech Republic,
28 June 2007.
Rachele De Felice and Stephen G. Pulman. 2008.
A classifier-based approach to preposition and deter-
miner error correction in L2 English. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics, pages 169?176, Manchester, United
Kingdom, 18?22 August 2008.
Tom De Smedt, Vincent Van Asch, and Walter Daele-
mans. 2010. Memory-based shallow parser for
Python. CLiPS Technical Report Series (CTRS), 2.
Michael Gamon, Lucy Vanderwende, Jianfeng Gao,
Chris Brockett, Alexandre Klementiev, William B.
Dolan, and Dmitriy Belenko. 2008. Using contex-
tual speller techniques and language modeling for ESL
error correction. In Proceedings of the International
Joint Conference on Natural Language Processing,
pages 449?456, Hyderabad, India, 7?12 January 2008.
Jila Ghomeshi, Paul Ileana, and Martina Wiltschko.
2009. Determiners: Universals and Variation. Lin-
guistik Aktuell/Linguistics Today. John Benjamins
Publishing Company.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2004. Detecting errors in English article usage with
a maximum entropy classifier trained on a large, di-
verse corpus. In Proceedings of the 4th International
Conference on Language Resources and Evaluation,
Lisbon, Portugal, 26?28 May 2004.
Na-Rae Han, Joel Tetreault, Soo-Hwa Lee, and Jin-
Young Ha. 2010. Using an error-annotated learner
corpus to develop an ESL/EFL error correction sys-
tem. In Proceedings of the Seventh International Con-
ference on Language Resources and Evaluation, Val-
letta, Malta, 19?21 May 2010.
Kevin Knight and Ishwar Chander. 1994. Automatic
postediting of documents. In Proceedings of the 12th
National Conference on Artificial Intelligence, pages
779?784, Seattle, Washington, USA, 31 July?4 Au-
gust 1994.
Guido Minnen, Francis Bond, and Ann Copestake. 2000.
Memory-based learning for article generation. In Pro-
ceedings of the 4th Conference on Computational Nat-
ural Language Learning and the Second Learning
Language in Logic Workshop, pages 43?48, Lisbon,
Portugal, 13?14 September 2000.
Diane Nicholls. 2003. The Cambridge Learner
Corpus?error coding and analysis for lexicography
and ELT. In Proceedings of the Corpus Linguis-
tics 2003 Conference, pages 572?581, Lancaster, UK,
29 March?2 April 2003.
Adwait Ratnaparkhi. 1998. Maximum entropy models
for natural language ambiguity resolution. Ph.D. the-
sis, Philadelphia, PA, USA. AAI9840230.
Adwait Ratnaparkhi. 2010. Maximum entropy models
for natural language processing. In Encyclopedia of
Machine Learning, pages 647?651.
Kristina Toutanova and Hisami Suzuki. 2007. Gener-
ating case markers in machine translation. In Human
Language Technology Conference of the North Ameri-
can Chapter of the Association of Computational Lin-
guistics, pages 49?56, Rochester, New York, USA,
22?27 April 2007.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading ESOL texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
180?189, Portland, Oregon, USA, 19?24 June 2011.
Xing Yi, Jianfeng Gao, and William B. Dolan. 2008. A
web-based English proofing system for English as a
second language users. In Proceedings of the Third
International Join Conference on Natural Language
Processing, Hyderabad, India, 7?12 January 2008.
271
