Sentence Compression for Automated Subtitling: A Hybrid Approach
Vincent Vandeghinste and Yi Pan
Centre for Computational Linguistics
Katholieke Universiteit Leuven
Maria Theresiastraat 21
BE-3000 Leuven
Belgium
vincent.vandeghinste@ccl.kuleuven.ac.be, yi.pan@ccl.kuleuven.ac.be
Abstract
In this paper a sentence compression tool is de-
scribed. We describe how an input sentence gets
analysed by using a.o. a tagger, a shallow parser
and a subordinate clause detector, and how, based
on this analysis, several compressed versions of this
sentence are generated, each with an associated es-
timated probability. These probabilities were esti-
mated from a parallel transcript/subtitle corpus. To
avoid ungrammatical sentences, the tool also makes
use of a number of rules. The evaluation was done
on three different pronunciation speeds, averaging
sentence reduction rates of 40% to 17%. The num-
ber of reasonable reductions ranges between 32.9%
and 51%, depending on the average estimated pro-
nunciation speed.
1 Introduction
A sentence compression tool has been built with
the purpose of automating subtitle generation for
the deaf and hard-of-hearing. Verbatim transcrip-
tions cannot be presented as the subtitle presentation
time is between 690 and 780 characters per minute,
which is more or less 5.5 seconds for two lines (ITC,
1997), (Dewulf and Saerens, 2000), while the aver-
age speech rate contains a lot more than the equiva-
lent of 780 characters per minute.
The actual amount of compression needed de-
pends on the speed of the speaker and on the amount
of time available after the sentence. In documen-
taries, for instance, there are often large silent in-
tervals between two sentences, the speech is often
slower and the speaker is off-screen, so the avail-
able presentation time is longer. When the speaker
is off-screen, the synchrony of the subtitles with
the speech is of minor importance. When subti-
tling the news the speech rate is often very high
so the amount of reduction needed to allow the
synchronous presentation of subtitles and speech is
much greater. The sentence compression rate is a
parameter which can be set for each sentence.
Note that the sentence compression tool de-
scribed in this paper is not a subtitling tool. When
subtitling, only when a sentence needs to be re-
duced, and the amount of reduction is known, the
sentence is sent to the sentence compression tool.
So the sentence compression tool is a module of an
automated subtitling tool. The output of the sen-
tence compression tool needs to be processed ac-
cording to the subtitling guidelines like (Dewulf and
Saerens, 2000), in order to be in the correct lay-out
which makes it usable for actual subtitling. Manu-
ally post-editing the subtitles will still be required,
as for some sentences no automatic compression is
generated.
In real subtitling it often occurs that the sentences
are not compressed, but to keep the subtitles syn-
chronized with the speech, some sentences are en-
tirely removed.
In section 2 we describe the processing of a sen-
tence in the sentence compressor, from input to out-
put. In section 3 we describe how the system was
evaluated and the results of the evaluation. Section
4 contains the conclusions.
2 From Full Sentence to Compressed
Sentence
The sentence compression tool is inspired by (Jing,
2001). Although her goal is text summarization
and not subtitling, her sentence compression system
could serve this purpose.
She uses multiple sources of knowledge on which
her sentence reduction is based. She makes use of
a corpus of sentences, aligned with human-written
sentence reductions which is similar to the parallel
corpus we use (Vandeghinste and Tjong Kim Sang,
2004). She applies a syntactic parser to analyse the
syntactic structure of the input sentences. As there
was no syntactic parser available for Dutch (Daele-
mans and Strik, 2002), we created ShaRPA (Van-
deghinste, submitted), a shallow rule-based parser
which could give us a shallow parse tree of the
input sentence. Jing uses several other knowl-
edge sources, which are not used (not available for
Dutch) or not yet used in our system (like WordNet).
In figure 1 the processing flow of an input sen-
tence is sketched.
Input Sentence
Tagger
Abbreviator
Numbers to Digits
Chunker
Subordinate Clause Detector
Shallow Parse
Tree
Compressor
Word ReducerCompressed
Sentence
Removal,
Non?removal,
Reduction
Database
Grammar
Rules
Figure 1: Sentence Processing Flow Chart
First we describe how the sentence is analysed
(2.1), then we describe how the actual sentence
compression is done (2.2), and after that we de-
scribe how words can be reduced for extra compres-
sion (2.3). The final part describes the selection of
the ouput sentence (2.4).
2.1 Sentence Analysis
In order to apply an accurate sentence compression,
we need a syntactic analysis of the input sentence.
In a first step, the input sentence gets tagged for
parts-of-speech. Before that, it needs to be trans-
formed into a valid input format for the part-of-
speech tagger. The tagger we use is TnT (Brants,
2000) , a hidden Markov trigram tagger, which was
trained on the Spoken Dutch Corpus (CGN), Inter-
nal Release 6. The accuracy of TnT trained on CGN
is reported to be 96.2% (Oostdijk et al, 2002).
In a second step, the sentence is sent to the
Abbreviator. This tool connects to a database
of common abbreviations, which are often pro-
nounced in full words (E.g. European Union be-
comes EU) and replaces the full form with its ab-
breviation. The database can also contain the tag
of the abbreviated part (E.g. the tag for EU is
N(eigen,zijd,ev,basis,stan) [E: singular non-neuter
proper noun]).
In a third step, all numbers which are written in
words in the input are replaced by their form in dig-
its. This is done for all numbers which are smaller
than one million, both for cardinal and ordinal nu-
merals.
In a fourth step, the sentence is sent to ShaRPa,
which will result in a shallow parse-tree of the sen-
tence. The chunking accuracy for noun phrases
(NPs) has an F-value of 94.7%, while the chunk-
ing accuracy of prepositional phrases (PPs) has an
F-value of 95.1% (Vandeghinste, submitted).
A last step before the actual sentence compres-
sion consists of rule-based clause-detection: Rel-
ative phrases (RELP), subordinate clauses (SSUB)
and OTI-phrases (OTI is om ... te + infinitive1) are
detected. The accuracy of these detections was eval-
uated on 30 files from the CGN component of read-
aloud books, which contained 7880 words. The
evaluation results are presented in table 1.
Type of S Precision Recall F-value
OTI 71.43% 65.22% 68.18%
RELP 69.66% 68.89% 69.27%
SSUB 56.83% 60.77% 58.74%
Table 1: Clause Detection Accuracy
The errors are mainly due to a wrong analysis
of coordinating conjunctions, which is not only the
weak point in the clause-detection module, but also
in ShaRPa. A full parse is needed to accurately
solve this problem.
2.2 Sentence Compression
For each chunk or clause detected in the previous
steps, the probabilities of removal, non-removal and
reduction are estimated. This is described in more
detail in 2.2.1.
Besides the statistical component in the compres-
sion, there are also a number of rules in the com-
pression program, which are described in more de-
tail in 2.2.2.
The way the statistical component and the rule-
based component are combined is described in
2.2.3.
1There is no equivalent construction in English. OTI is a
VP-selecting complementizer.
2.2.1 Use of Statistics
Chunk and clause removal, non-removal and reduc-
tion probabilities are estimated from the frequencies
of removal, non-removal and reduction of certain
types of chunks and clauses in the parallel corpus.
The parallel corpus consists of transcripts of tele-
vision programs on the one hand and the subti-
tles of these television programs on the other hand.
A detailed description of how the parallel corpus
was collected, and how the sentences and chunks
were aligned is given in (Vandeghinste and Tjong
Kim Sang, 2004).
All sentences in the source corpus (transcripts)
and the target corpus (subtitles) are analysed in the
same way as described in section 2.1, and are chunk
aligned. The chunk alignment accuracy is about
95% (F-value).
We estimated the removal, non-removal and re-
duction probabilities for the chunks of the types NP,
PP, adjectival phrase (AP), SSUB, RELP, and OTI,
based on their chunk removal, non-removal and re-
duction frequencies.
For the tokens not belonging to either of these
types, the removal and non-removal probabilities
were estimated based on the part-of-speech tag for
those words. A reduced tagset was used, as the orig-
inal CGN-tagset (Van Eynde, 2004) was too fine-
grained and would lead to a multiplication of the
number of rules which are now used in ShaRPa. The
first step in SharPa consists of this reduction.
For the PPs, the SSUBs and the RELPs, as well
as for the adverbs, the chunk/tag information was
considered as not fine-grained enough, so the es-
timation of the removal, non-removal and reduc-
tion probabilities for these types are based on the
first word of those phrases/clauses and the reduc-
tion, removal and non-removal probabilities of such
phrases in the parallel corpus, as the first words of
these chunk-types are almost always the heads of
the chunk. This allows for instance to make the
distinction between several adverbs in one sentence,
so they do not all have the same removal and non-
removal probabilities. A disadvantage is that this
approach leads to sparse data concerning the less
frequent adverbs, for which a default value (average
over all adverbs) will be employed.
An example : A noun phrase.
de grootste Belgische bank
[E: the largest Belgian bank]
After tagging and chunking the sentence and af-
ter detecting subordinate clauses, for every non-
terminal node in the shallow parse tree we retrieve
the measure of removal (X), of non-removal (=) and
of reduction2 (   ). For the terminal nodes, only the
measures of removal and of non-removal are used.
NP
= 0.54
X 0.27
 0.05
DET
= 0.68
X 0.28
de
ADJ
= 0.56
X 0.35
groot-
ste
ADJ
= 0.56
X 0.35
Bel-
gische
N
= 0.65
X 0.26
bank
For every combination the probability estimate
is calculated. So if we generate all possible com-
pressions (including no compression), the phrase
de grootste Belgische bank will get the
probability estimate 
				
Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 85?88,
Beijing, August 2010
An Efficient, Generic Approach to Extracting Multi-Word Expressions
from Dependency Trees
Scott Martens and Vincent Vandeghinste
Centrum voor Computerlingu??stiek
Katholieke Universiteit Leuven
scott@ccl.kuleuven.be & vincent@ccl.kuleuven.be
Abstract
The Varro toolkit offers an intuitive mech-
anism for extracting syntactically mo-
tivated multi-word expressions (MWEs)
from dependency treebanks by looking for
recurring connected subtrees instead of
subsequences in strings. This approach
can find MWEs that are in varying orders
and have words inserted into their compo-
nents. This paper also proposes descrip-
tion length gain as a statistical correlation
measure well-suited to tree structures.
1 Introduction
Automatic MWE extraction techniques operate
by using either statistical correlation tests on the
distributions of words in corpora, syntactic pat-
tern matching techniques, or by using hypothe-
ses about the semantic non-compositionality of
MWEs. This paper proposes a purely statistical
technique for MWE extraction that incorporates
syntactic considerations by operating entirely on
dependency treebanks. On the whole, dependency
trees have one node for each word in the sentence,
although most dependency schemes vary from this
to some extent in practice. See Figure 1 for an
example dependency tree produced automatically
by the Stanford parser from the English language
data in the Europarl corpus. (Marneffe, 2008;
Koehn, 2005)
Identifying MWEs with subtrees in dependency
trees is not a new idea. It is close to the formal def-
inition offered in Mel?c?uk (1998), and is applied
computationally in Debusmann (2004) However,
using dependency treebanks to automatically ex-
tract MWEs is fairly new and few MWE extrac-
Figure 1. A dependency tree of the sentence
?The Minutes of yesterday?s sitting have been dis-
tributed.?
tion projects to date take advantage of dependency
information directly. There are a number of rea-
sons why this is the case:
? String-based algorithms are not readily ap-
plicable to trees.
? Tree structures yield a potentially combina-
torial number of candidate MWEs, a prob-
lem shared with methods that look for strings
with gaps.
? Statistical techniques used in MWE extrac-
tion, like pointwise mutual information, are
two-variable tests that are not easy to apply
to larger sets of words.
The tool and statistical procedures used in this
research are not language dependent and can op-
erate on MWE of any size, producing depen-
85
(a) ?The Minutes (...)
have been distributed?
(b) ?(...) Minutes of
(...) distributed.?
Figure 2. Two induced subtrees of the dependency
tree in Figure 1. Note that both correspond to dis-
continuous phrases in the original sentence.
dency pairs, short phrases of any syntactic cate-
gory, lengthy formulas and idioms. There are no
underlying linguistic assumptions in this method-
ology except that a MWE must consist of words
that have a fixed set of dependency links in a
treebank. Even word order and distance between
words is not directly assumed to be significant.
The input, however, requires substantial linguis-
tic pre-processing ? particularly, the identification
of at least some of the dependency relations in
the corpora used. Retrieving MWEs that contain
abstract categories, like information about the ar-
guments of verbs or part-of-speech information
for unincluded elements, requires using treebanks
that contain that information, rather than purely
lexical dependency trees.
2 Varro Toolkit for Frequent Subtree
Discovery
The Varro toolkit is an open-source application for
efficiently extracting frequent closed unordered
induced subtrees from treebanks with labeled
nodes and edges. It is publicly available under an
open source license.1 For a fuller description of
Varro, including the algorithm and data structures
used and a formal definition of frequent closed un-
ordered induced subtrees, see Martens (2010).
Given some tree like the one in Figure 1, an in-
duced subtree is a connected subset of its nodes
and the edges that connect them, as shown in
Figure 2. Subtrees do not necessarily represent
1http://varro.sourceforge.net/
fixed sequences of words in the original text,
they include syntactically motivated discontinu-
ous phrases. This dramatically reduces the num-
ber of candidate discontinuous MWEs when com-
pared to string methods. An unordered induced
subtree is a subtree where the words may appear
with different word orders, but the subtree is still
identified as the same if the dependency structure
is the same. A frequent closed subtree is a sub-
tree of a treebank that appears more than some
fixed number of times and where there is no sub-
tree that contains it and appears the same number
of times. Finding only closed subtrees reduces the
combinatorial explosion of possible subtrees, and
ensures that each candidate MWE includes all the
words the that co-occur with it every time it ap-
pears.
3 Preprocessing and Extracting Subtrees
The English language portion of the Europarl
Corpus, version 3 was parsed using the Stanford
parser, which produces both a constituentcy parse
and a dependency tree as its output.2 The depen-
dency information for each sentence was trans-
formed into the XML input format used by Varro.
The result is a treebank of 1.4 million individual
parse trees, each representing a sentence, and a to-
tal of 36 million nodes.
In order to test the suitability of Varro for large
treebanks and intensive extractions, all recurring
closed subtrees that appear at least twice were ex-
tracted. This took a total of 129,312.27 seconds
(just over 34 hours), producing 9,976,355 frequent
subtrees, of which 9,909,269 contain more than
one word and are therefore candidate MWEs.
A fragment of the Varro output can be seen in
Figure 3. The nodes of the subtrees returned are
not in a grammatical surface order. However, the
original source order can be recovered by using
the locations where each subtree appears to find
the order in the treebank. Doing so for the tree
in Figure 3 shows what kinds of MWEs this ap-
proach can extract from treebanks. The under-
lined words in the following sentences are the
ones included in the subtree in Figure 3:
2This portion of the work was done by our colleagues
Jo?rg Tiedemann and Gideon Kotze? at RU Groningen.
86
Figure 3. An example of a found subtree and can-
didate MWE. This subtree appears in 2581 unique
locations in the treebank, and only the locations
of the first few places in the treebank where it ap-
pears are reproduced here, but all 2581 are in the
Varro output data.
The vote will take place tomorrow at 9 a.m.
The vote will take place today at noon.
The vote will take place tomorrow, Wednesday
at 11:30 a.m.
4 Statistical Methods for Evaluating
Subtrees as MWEs
To evaluate the quality of subtrees as MWEs,
we propose to use a simplified form of de-
scription length gain (DLG), a metric derived
from algorithmic information theory and Mini-
mum Description Length methods (MDL). (Ris-
sanen, 1978; Gru?nwald, 2005) Given a quantity of
data of any kind that can be stored as a digital in-
formation in a computer, and some process which
transforms the data in a way that can be reversed,
DLG is the measure of how the space required to
store that data changes when it is transformed.
To calculate DLG, one must first decide how to
encode the trees in the treebank. It is not neces-
sary to actually encode the treebank in any par-
ticular format. All that is necessary is to be able
to calculate how many bits the treebank would re-
quire to encode it.
Space prevents the full description of the en-
coding mechanism used or the way DLG is cal-
culated. The encoding mechanism is largely the
same as the one described in Luccio et al (2001)
Converting the trees to strings makes it possible to
calculate the encoding size by calculating the en-
tropy of the treebank in that encoding using clas-
sical information theoric methods.
In effect, the procedure for calculating DLG is
to calculate the entropy of the whole treebank,
given the encoding method chosen, and then to
recalculate its entropy given some subtree which
is removed from the treebank and replaced with a
symbol that acts as an abbreviation. That subtree
is then be added back to the treebank once as part
of a look-up table. These methods are largely the
same as those used by common data compression
software.
DLG is the difference between these two en-
tropy measures.3
Because of the sensitivity of DLG to low fre-
quencies, it can be viewed as a kind of non-
parametric significance test. Any frequent struc-
ture that cannot be used to compress the treebank
has a negative DLG and is not frequent enough or
large enough to be considered significant.
Varro reports several statistics related to DLG
for each extracted subtree, as shown in Figure 3:
? Unique appearances (reported by the root-
Count attribute) is the number of times the
extracted subtree appears with a different
root node.
? Entropy is the entropy of the extracted sub-
tree, given the encoding scheme that Varro
uses to calculate DLG.
? Algorithmic mutual information (AMI) (re-
ported with the mi attribute) is the DLG of
the extracted subtree divided by its number
of unique appearances in the treebank.
? Compression is the AMI divided by the en-
tropy.
AMI is comparable to pointwise mutual infor-
mation (PMI) in that both are measures of redun-
dant bits, while compression is comparable to nor-
malized mutual information metrics.
3This is a very simplified picture of MDL and DLG met-
rics.
87
5 Results and Conclusions
We used the metrics described above to sort the
nearly 10 million frequent subtrees of the parsed
English Europarl corpus. We found that:
? Compression and AMI metrics strongly fa-
vor very large subtrees that represent highly
formulaic language.
? DLG alone finds smaller, high frequency ex-
pressions more like MWEs favoured by ter-
minologists and collocation analysis.
For example, the highest DLG subtree matches
the phrase ?the European Union?. This is not
unexpected given the source of the data and con-
stitutes a very positive result. Among the nearly
10 million candidate MWEs extracted, it also
places near the top discontinuous phrases like
?... am speaking ... in my ... capacity as ...?.
Using both compression ratio and AMI, the
same subtree appears first. It is present 26 times
in the treebank, with a compression score of 0.894
and an AMI of 386.92 bits. It corresponds to the
underlined words in the sentence below:
The next item is the recommendation for
second reading (A4-0245/99), on behalf of
the Committee on Transport and Tourism, on
the common position adopted by the Council
(13651/3/98 - C4-0037/99-96/0182 (COD) with
a view to adopting a Council Directive on the
charging of heavy goods vehicles for the use of
certain infrastructures.
This is precisely the kind of formulaic speech,
with various gaps to fill in, which is of great inter-
est for sub-sentential translation memory systems.
(Gotti et al, 2005; Vandeghinste and Martens,
2010)
We believe this kind of strategy can substan-
tially enhance MWE extraction techniques. It in-
tegrates syntax into MWE extraction in an intu-
itive way. Furthermore, description length gain
offers a unified statistical account of an MWE as
a linguistically motivated structure that can com-
press relevant corpus data. It is similar to the types
of statistical tests already used, but is also non-
parametric and suitable for the study of arbitrary
MWEs, not just two-word MWEs or phrases that
occur without gaps.
6 Acknowledgements
This research is supported by the AMASS++
Project,4 directly funded by the Institute for the
Promotion of Innovation by Science and Technol-
ogy in Flanders (IWT) (SBO IWT 060051) and by
the PaCo-MT project (STE-07007).
References
Debusmann, Ralph. 2004. Multiword expressions as
dependency subgraphs. Proceedings of the 2004
ACL Workshop on Multiword Expressions, pp. 56-
63.
Gotti, Fabrizio, Philippe Langlais, Eliott Macklovitch,
Didier Bourigault, Benoit Robichaud and Claude
Coulombe. 2005. 3GTM: A third-generation trans-
lation memory. Proceedings of the 3rd Computa-
tional Linguistics in the North-East Workshop, pp.
8?15.
Gru?nwald, Peter. 2005. A tutorial introduction to
the minimum description length principle. In: Ad-
vances in Minimum Description Length: Theory
and Applications, (Peter Gru?nwald, In Jae Myung,
Mark Pitt, eds.), MIT Press, pp. 23?81.
Koehn, Philipp. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. Proceedings of the
10th MT Summit, pp. 79?86.
Luccio, Fabrizio, Antonio Enriquez, Pablo Rieumont
and Linda Pagli. 2001. Exact Rooted Subtree
Matching in Sublinear Time. Universita` di Pisa
Technical Report TR-01-14.
de Marneffe, Marie-Catherine and Christopher D.
Manning. 2008. The Stanford typed dependencies
representation. Proceedings of the 2008 CoLing
Workshop on Cross-framework and Cross-domain
Parser Evaluation, pp. 1?8.
Martens, Scott. 2010. Varro: An Algorithm and
Toolkit for Regular Structure Discovery in Tree-
banks. Proceedings of the 2010 Int?l Conf. on Com-
putational Linguistics (CoLing), in press.
Mel?c?uk, Igor. 1998. Collocations and Lexical Func-
tions. In: Phraseology. Theory, Analysis, and Ap-
plications, (Anthony Cowie ed.), pp. 23?53.
Rissanen, Jorma. 1978. Modeling by shortest data
description. Automatica, vol. 14, pp. 465?471.
Vandeghinste, Vincent and Scott Martens. 2010.
Bottom-up transfer in Example-based Machine
Translation. Proceedings of the 2010 Conf. of the
European Association for Machine Translation, in
press.
4http://www.cs.kuleuven.be/?liir/projects/amass/
88
Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 120?126,
Coling 2014, Dublin, Ireland, August 24 2014.
Improving the Precision of Synset Links Between Cornetto and Princeton
WordNet
Leen Sevens Vincent Vandeghinste
Centre for Computational Linguistics
KU Leuven
firstname@ccl.kuleuven.be
Frank Van Eynde
Abstract
Knowledge-based multilingual language processing benefits from having access to correctly es-
tablished relations between semantic lexicons, such as the links between different WordNets.
WordNet linking is a process that can be sped up by the use of computational techniques. Manual
evaluations of the partly automatically established synonym set (synset) relations between Dutch
and English in Cornetto, a Dutch lexical-semantic database associated with the EuroWordNet
grid, have confronted us with a worrisome amount of erroneous links. By extracting transla-
tions from various bilingual resources and automatically assigning a confidence score to every
pre-established link, we reduce the error rate of the existing equivalence relations between both
languages? synsets (section 2). We will apply this technique to reuse the connection of Sclera
and Beta pictograph sets and Cornetto synsets to Princeton WordNet and other WordNets, allow-
ing us to further extend an existing Dutch text-to-pictograph translation tool to other languages
(section 3).
1 Introduction
The connections between WordNets, large semantic databases grouping lexical units into synonym
sets or synsets, are an important resource in knowledge-based multilingual language processing.
EuroWordNet (Vossen, 1997) aims to build language-specific WordNets among the same lines as the
original WordNet
1
(Miller et al., 1990), using Inter-Lingual-Indexes to weave a web of equivalence re-
lations between the synsets contained within the databases. Cornetto
2
(Vossen et al., 2007), a Dutch
lexical-semantic collection of data associated with the Dutch EuroWordNet
3
, consists of more than 118
000 synsets. The equivalence relations establish connections between Dutch and English synsets in
Princeton WordNet version 1.5 and 2.0. We update these links to Princeton WordNet version 3.0 by the
mappings among WordNet versions made available by TALP-UPC
4
. The equivalence relations between
Cornetto and Princeton have been established semi-automatically by Vossen et al. (1999). Manual cod-
ing was carried out for the 14 749 most important concepts in the database. These include the most
frequent concepts, the concepts having a large amount of semantic relations and the concepts occupying
a high position in the lexical hierarchy. Automatic linkage was done by mapping the bilingual Van Dale
database
5
to WordNet 1.5. For every WordNet synset containing a dictionary?s translation for a particular
Dutch word, all its members were proposed as alternative translations. In the case of only one transla-
tion, the synset relation was instantly assumed correct, while multiple translations were weighted using
several heuristics, such as measuring the conceptual distance in the WordNet hierarchy. We decided to
verify the quality of these links and noticed that they were highly erroneous, making them not yet very
reliable for multilingual processing.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details:http://creativecommons.org/licenses/by/4.0/
1
http://wordnet.princeton.edu
2
http://tst-centrale.org/producten/lexica/cornetto/7-56
3
http://www.illc.uva.nl/EuroWordNet
4
http://www.talp.upc.edu
5
http://www.vandale.be
120
2 Improving the equivalence relations between Cornetto and Princeton WordNet
We manually evaluated the quality of the links between 300 randomly selected Cornetto synsets and
their supposedly related Princeton synsets. A Cornetto synset is often linked to more than one Princeton
synset. We found an erroneous link in 35.27% of the 998 equivalence relations we evaluated.
Each Cornetto synset has about 3.3 automatically derived English equivalents, allowing to roughly
compare our evaluation to an initial quality check of the equivalence relations performed by Vossen et
al. (1999). They note that, in the case of synsets with three to nine translations, the percentages of
correct automatically derived equivalents went down to 65% and 49% for nouns and verbs respectively.
Our manual evaluations are in line with these results, showing that only 64.73% of all the connections
in our sample are correct. An example of where it goes wrong is the Cornetto synset for the animal tor
?beetle?, which is not only appropriately linked to correct synsets (such as beetle and bug), but also
mistakenly to the Princeton synset for the computational glitch. This flaw is most probably caused by
the presence of the synonym bug, which is a commonly used word for errors in computer programs.
Examples like these are omnipresent in our data
6
and led us to conclude that the synset links between
Cornetto and Princeton WordNet definitely could be improved.
We build a bilingual dictionary for Dutch and English and use these translations as an automatic
indicator of the quality of equivalence relations. In order to create a huge list of translations we merge
several translation word lists, removing double entries. Some are manually compiled dictionaries, while
others are automatically derived word lists from parallel corpora: we extracted the 1-word phrases from
the phrase tables built with Moses (Koehn et al., 2007) based on the GIZA++ word alignments (Och and
Ney, 2003). Table 1 gives an overview.
This resulted in a coverage of 52.18% (43 970 out of 84 264) of the equivalence relations for which
translation information was available in order to possibly confirm the relation.
Translation dictionary Reference Method of compilation Nr of word pairs
Wiktionary www.wiktionary.org Manual 23,575
FreeDict www.freedict.com Manual 49,493
Europarl (Koehn, 2005) Automatic 2,970,501
Opus (Tiedemann, 2009) Automatic 6,223,539
Sclera translations www.pictoselector.eu Manual 12,381
Table 1: The used translation sources
Figure 1 visualizes how we used the bilingual dictionaries to automatically evaluate the quality of the
pre-established links between Cornetto and Princeton WordNet. We retrieve all the lemmas of the lexical
units that were contained within a synset S
i
(in our example, snoepgoed ?confectionary? and snoep
?candy? extracted from S
1
). Each of these lemmas is looked up in the bilingual dictionary, resulting
in a dictionary words list of English translations.
7
This list is used to estimate the correctness of the
equivalence relation between the Cornetto and the Princeton synset.
We retrieve the lexical units list from the English synset T
j
(in our example candy and confect extracted
from T
1
). We count the number of words in the lexical units list also appearing in the dictionary words
list (the overlap being represented as the multiset Q). Translations appearing more than once are given
more importance. For example, candy occurs twice, putting our overlap counter on 2. This overlap is
normalized. In the example it is divided by 3 (confect + candy + candy, as the double count is taken into
account), leaving us with a score of 66.67%. For the gloss words list we remove the stop words
8
and
make an analogous calculation. In our example, sweet is counted twice (the overlap being represented
as the multiset R) and this number is divided by the total number of gloss words available (again taking
6
Other examples: nederig ?humble? was linked to the synset for flexible (as a synonym for elastic), waterachtig ?aquatic?
was linked to the synsets for grey and mousy, rocker (hardrocker) was linked to the synset for rocking chair, etc.
7
Note that this list can contain doubles (such as candy and delicacy), as these translations would provide additional evidence
to our scoring algorithm. It is therefore not the case that the dictionary words list represents a set. It represents a multiset.
8
http://norm.al/2009/04/14/list-of-english-stop-words
121
Figure 1: The scoring mechanism with examples
into account the double count). Averaging this score of 25% with our first result, we obtain a confidence
score of 45.83% for this equivalence relation. We calculated this confidence score for every equivalence
relation in Cornetto.
We checked whether the automatic scoring algorithm (section 2) (dis)agreed with the manual judge-
ments in order to determine a satisfactory threshold value for the acceptance of synset links. Evaluation
results are shown in figure 2. While the precision (the proportion of accurate links that the system got
right) went slightly up as our criterium for link acceptance became stricter, the recall (the proportion of
correct links that the system retrieved) quickly made a rather deep dive. The F-score reveals that the best
trade-off is reached when synset links getting a score of 0% are rejected, retaining any link with a higher
confidence score. The results in Table 3 shows that we were able to reduce the error rate to 21.09%,
which is a relative improvement of 40.20% over the baseline.
3 Improving the equivalence relations in the context of text-to-pictograph translation
Being able to use the currently available technological tools is becoming an increasingly important factor
in today?s society. Augmentative and Alternative Communication (AAC) refers to the whole of commu-
nication methods which aim to assist people that are suffering from cognitive disabilities, helping them to
become more socially active in various domains of daily life. Text-to-pictograph translation is a particular
form of AAC technology that enables linguistically-impaired people to use the Internet independently.
Filtering away erroneous synset links in Cornetto has proven to be a useful way to improve the quality
of a text-to-pictograph translation tool. Vandeghinste and Schuurman (2014) have connected pictograph
sets to Cornetto synsets to enable text-to-pictograph translation. Equivalence relations are important to
allow reusing these connections in order to link pictographs to synsets for other languages than Dutch.
122
Figure 2: Precision (top line), recall (bottom line) and F-score (middle line) for different threshold values
of link acceptance.
Vandeghinste and Schuurman (2014) released Sclera2Cornetto, a resource linking Sclera
9
pictographs
to Cornetto synsets. Currently, over 13 000 Sclera pictographs are made available online, 5 710 of which
have been manually linked to Cornetto synsets. We want to build a text-to-pictograph conversion with
English and Spanish as source languages, reusing the Sclera2Cornetto data.
By improving Cornetto?s pre-established equivalence relations with Princeton synsets, we can connect
the Sclera pictographs with Princeton WordNet for English. The latter, in turn, will then be used as the
intermediate step in our process of assigning pictographs to Spanish synsets.
Manual evaluations were made for a randomly generated subset of the synsets that were previously
used by Vandeghinste and Schuurman (2014) for assigning Sclera and Beta
10
pictographs to Cornetto.
Beta pictographs are another pictograph set for which a link between the pictographs and Cornetto was
provided by Vandeghinste (2014).
Table 2 presents the coverage of our bilingual dictionary for synsets being connected to Sclera and
Beta pictographs, which is clearly higher than the coverage over all synsets.
Covered Total Difference with All synsets
All synsets 43 970 (52.18%) 84 264 -
Sclera baseline 5 294 (88.80%) 5 962 36.62%
Beta synsets 3 409 (88.94%) 3 833 36.76%
Table 2: Dictionary Coverage for different sets of synsets
Table 3 shows that the error rate of Cornetto?s equivalence relations on the Sclera and Beta subsets
is much lower than the error rate on the whole set (section 2). We attribute this difference to the fact
that Vossen et al. (1999) carried out manual coding for the most important concepts in the database (see
section 1), as the Sclera and Beta pictographs tend to belong to this category. In these cases, every
synset has between one and two automatically derived English equivalents on the average, allowing us
to roughly compare with the initial quality check of the equivalence relations performed by Vossen et al.
(1999) showing that, in the event of a Dutch synset having only one English equivalent, 86% of the nouns
and 78% of the verbs were correctly linked, while the ones having two equivalents were appropriate in
68% and 71% of the cases respectively.
The F-score in Figure 3 reveals that the best trade-off between precision and recall is reached at the
> 0% threshold value, improving the baseline precision for both Sclera and Beta. We now retrieve all
English synsets for which a non-zero score was obtained in order to assign Sclera and Beta pictographs
to Princeton WordNet.
9
http://www.sclera.be
10
http://www.betavzw.be
123
Figure 3: Precision (top line), recall (bottom line) and F-score (middle line) for Sclera and Beta synsets
respectively, for different threshold values of link acceptance.
Baseline Current Relative improvement
All 35.27% 21.09% 40.20%
Sclera 14.50% 9.95% 31.38%
Beta 15.77% 13.47% 14.58%
Table 3: The reduction in error rates of Cornetto?s equivalence relations.
4 Related work
Using bilingual dictionaries to initiate or improve WordNet linkage has been applied elsewhere. Linking
Chinese lemmata to English synsets (Huang et al., 2003) to create the Chinese WordNet is one such
example. The 200 most frequent Chinese words and the 10 most frequent adjectives were taken as
a starting set and found as translation equivalences for 496 English lemmata, making each Chinese
lemma corresponding to 2.13 English synsets on average. Evaluations showed that 77% of the 496
equivalent pairs were synonymous. This accuracy rate dropped to 62.7% when the list of equivalence
pairs was extended by including all WordNet synonyms. Sornlertlamvanich et al. (2008) assign synsets
to bilingual dictionaries for Asian languages by considering English equivalents and lexical synonyms,
listing all English translations and scoring synsets according to the amount of matching translations
found, yielding an average accuracy rate of 49.4% for synset assignment to a Thai-English dictionary
and an accuracy rate of 93.3% for synsets that are attributed the highest confidence score. Joshi et
al. (2012) generate candidate synsets in English, starting with synsets in Hindi. For each Hindi synset, a
bag of words is obtained by parsing its gloss, examples and synonyms. Using a bilingual dictionary, these
Hindi words are translated to English. Various heuristics are used to calculate the intersection between
the translated bag of words and the synset words, concepts or relations of the target language, such as
finding the closest hyperonym synset (accuracy rate of 79.76%), the closest common synset word bag
(accuracy rate of 74.48%) and the closest common concept word bag (accuracy rate of 55.20%). Finally,
Soria et al. (2009) develop a mechanism for enriching monolingual lexicons with new semantic relations
by relying on the use of Inter-Lingual-Indexes to link WordNets of different languages. However, the
quality of these links is not evaluated.
5 Conclusions and future work
We have shown that a rather large reduction in error rates (a relative improvement of 40.20% on the
whole set) concerning the equivalence relations between Cornetto and Princeton WordNet can be ac-
quired by applying a scoring algorithm based on bilingual dictionaries. The method can be used to create
new equivalence relations as well. Contrasting our results with related work shows that we reach at least
the same level of correctness, although results are hard to compare because of conceptual differences
between languages. An accuracy rate of 78.91% was obtained for the general set of Cornetto?s equiva-
lence relations, while its subset of Sclera and Beta synsets (denoting frequent concepts) acquired final
124
precision rates of 90.05% and 86.53% respectively (compare with section 4).
One advantage of our method is that it could easily be reused to automatically build reliable links
between Princeton WordNet and brand-new WordNets. Unsupervised clustering methods can provide us
with synonym sets in the source language, after which the bilingual dictionary technique and the scoring
algorithm can be applied in order to provide us with satisfactory equivalence relations between both
languages. Semantic relations between synsets can then also be transferred from Princeton to the source
language?s WordNet.
Our improved links will be integrated in the next version of Cornetto. Future work will consist of
scaling to other languages through other relations between WordNets.
6 Acknowledgements
We would like to thank Tom Vanallemeersch for providing us with the EuroParl and Opus alignment
tables. We are also grateful to Martijn van der Kooij for sending us the mappings between the Dutch and
English Sclera pictograph file names made in the Picto-selector project.
The research in this paper was partly funded by the support fund M.M. Delacroix under the Picto
project, by the EU-CIP project Able-To-Include, and by the Program Advisory Committee of the Ad-
vanced Master of Artificial Intelligence of the University of Leuven.
References
Chu-Ren Huang, Elanna Tseng, Dylan Tsai and Brian Murphy. 2003. Cross-lingual Portability of Semantic
relations: Bootstrapping Chinese WordNet with English WordNet Relations. Languages and Linguistics, 4(3):
509?532. Academia Sinica, Taipei.
Salil Joshi, Arindam Chatterjee, Arun Karthikeyan Karra and Pushpak Bhattacharyya. 2012. Eating Your Own
Cooking: Automatically Linking Wordnet Synsets of Two Languages. COLING (Demos): 239?246.
Philip Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. Conference Proceedings:
the tenth Machine Translation Summit: 79?86. Phuket, Thailand.
Philip Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin and
Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation Annual Meeting of the
Association for Computational Linguistics (ACL), demonstration session. Prague, Czech Republic.
George A. Miller, Richard Beckwidth, Christiane Fellbaum, Derek Gross and Katherine Miller. 1990. Introduction
to WordNet: An On-line Lexical Database. International Journal of Lexicography, 3(4): 235?244.
Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1): 19?51.
Claudia Soria, Monica Monachini, Francesca Bertagna, Nicoletta Calzolari, Chu-Ren Huang, Shu-Kai Hsieh,
Andrea Marchetti and Maurizio Tesconi. 2009. Exploring Interoperability of Language Resources: the Case of
Cross-lingual Semi-automatic Enrichment of Wordnets. In A. Witt, U. Heid, F. Sasaki and G. S?erasset (eds).
Special Issue on Interoperability of Multilingual Language Processing. Language Resources and Evaluation.,
43(1): 87?96.
Virach Sornlertlamvanich, Thatsanee Charoenporn, Chumpol Mokarat, Hitoshi Isahara, Hammam Riza and Purev
Jaimai. 2008. Synset Assignment for Bi-lingual Dictionary with Limited Resource. Proceedings of the Third
International Joint Conference on Natural Language Processing: 673?678.
Jrg Tiedemann. 2009. News from OPUS ? A Collection of Multilingual Parallel Corpora with Tools and Interfaces.
In N. Nicolov and K. Bontcheva and G. Angelova and R. Mitkov (eds). Recent Advances in Natural Language
Processing, Volume V. John Benjamins: Amsterdam/Philadelphia.
Vincent Vandeghinste. Linking Pictographs to Synsets: Beta2Cornetto. Technical report.
Vincent Vandeghinste and Ineke Schuurman. 2014. Linking Pictographs to Synsets: Sclera2Cornetto. LREC
2014. In press.
125
Piek Vossen. 1997. EuroWordNet: a multilingual database for information retrieval. Proceedings of the DELOS
workshop on Cross-language Information Retrieval. Zurich.
Piek Vossen, Laura Bloksma, and Paul Boersma. 1999. The Dutch Wordnet. EuroWordNet Paper. University of
Amsterdam, Amsterdam.
Piek Vossen, Katja Hofman, Maarten de Rijke, Erik Tjong Kim Sang and Koen Deschacht. 2007 The Cornetto
Database: Architecture and User-Scenarios. Proceedings of 7th Dutch-Belgian Information Retrieval Workshop
DIR2007. University of Leuven, Leuven.
126
