Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 34?41
Manchester, UK. August 2008
A Data Driven Approach to Query Expansion in Question Answering
Leon Derczynski, Jun Wang, Robert Gaizauskas and Mark A. Greenwood
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello
Sheffield S1 4DP UK
{aca00lad, acp07jw}@shef.ac.uk
{r.gaizauskas, m.greenwood}@dcs.shef.ac.uk
Abstract
Automated answering of natural language
questions is an interesting and useful prob-
lem to solve. Question answering (QA)
systems often perform information re-
trieval at an initial stage. Information re-
trieval (IR) performance, provided by en-
gines such as Lucene, places a bound on
overall system performance. For example,
no answer bearing documents are retrieved
at low ranks for almost 40% of questions.
In this paper, answer texts from previous
QA evaluations held as part of the Text
REtrieval Conferences (TREC) are paired
with queries and analysed in an attempt
to identify performance-enhancing words.
These words are then used to evaluate the
performance of a query expansion method.
Data driven extension words were found
to help in over 70% of difficult questions.
These words can be used to improve and
evaluate query expansion methods. Sim-
ple blind relevance feedback (RF) was cor-
rectly predicted as unlikely to help overall
performance, and an possible explanation
is provided for its low value in IR for QA.
1 Introduction
The task of supplying an answer to a question,
given some background knowledge, is often con-
sidered fairly trivial from a human point of view,
as long as the question is clear and the answer is
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
known. The aim of an automated question answer-
ing system is to provide a single, unambiguous re-
sponse to a natural language question, given a text
collection as a knowledge source, within a certain
amount of time. Since 1999, the Text Retrieval
Conferences have included a task to evaluate such
systems, based on a large pre-defined corpus (such
as AQUAINT, containing around a million news
articles in English) and a set of unseen questions.
Many information retrieval systems perform
document retrieval, giving a list of potentially rel-
evant documents when queried ? Google?s and Ya-
hoo!?s search products are examples of this type of
application. Users formulate a query using a few
keywords that represent the task they are trying to
perform; for example, one might search for ?eif-
fel tower height? to determine how tall the Eiffel
tower is. IR engines then return a set of references
to potentially relevant documents.
In contrast, QA systems must return an exact an-
swer to the question. They should be confident
that the answer has been correctly selected; it is
no longer down to the user to research a set of doc-
ument references in order to discover the informa-
tion themselves. Further, the system takes a natural
language question as input, instead of a few user-
selected key terms.
Once a QA system has been provided with a
question, its processing steps can be described in
three parts - Question Pre-Processing, Text Re-
trieval and Answer Extraction:
1. Question Pre-Processing TREC questions
are grouped into series which relate to a given
target. For example, the target may be ?Hinden-
burg disaster? with questions such as ?What type
of craft was the Hindenburg?? or ?How fast could
it travel??. Questions may include pronouns ref-
34
erencing the target or even previous answers, and
as such require processing before they are suitable
for use.
2. Text Retrieval An IR component will return
a ranked set of texts, based on query terms. At-
tempting to understand and extract data from an
entire corpus is too resource intensive, and so an IR
engine defines a limited subset of the corpus that
is likely to contain answers. The question should
have been pre-processed correctly for a useful set
of texts to be retrieved ? including anaphora reso-
lution.
3. Answer Extraction (AE) Given knowledge
about the question and a set of texts, the AE sys-
tem attempts to identify answers. It should be clear
that only answers within texts returned by the IR
component have any chance of being found.
Reduced performance at any stage will have a
knock-on effect, capping the performance of later
stages. If questions are left unprocessed and full
of pronouns (e.g.,?When did it sink??) the IR com-
ponent has very little chance of working correctly
? in this case, the desired action is to retrieve
documents related to the Kursk submarine, which
would be impossible.
IR performance with a search engine such as
Lucene returns no useful documents for at least
35% of all questions ? when looking at the top
20 returned texts. This caps the AE component
at 65% question ?coverage?. We will measure the
performance of different IR component configura-
tions, to rule out problems with a default Lucene
setup.
For each question, answers are provided in the
form of regular expressions that match answer text,
and a list of documents containing these answers
in a correct context. As references to correct doc-
uments are available, it is possible to explore a
data-driven approach to query analysis. We deter-
mine which questions are hardest then concentrate
on identifying helpful terms found in correct doc-
uments, with a view to building a system than can
automatically extract these helpful terms from un-
seen questions and supporting corpus. The avail-
ability and usefulness of these terms will provide
an estimate of performance for query expansion
techniques.
There are at least two approaches which could
make use of these term sets to perform query ex-
pansion. They may occur in terms selected for
blind RF (non-blind RF is not applicable to the
TREC QA task). It is also possible to build a cata-
logue of terms known to be useful according to cer-
tain question types, thus leading to a dictionary of
(known useful) expansions that can be applied to
previously unseen questions. We will evaluate and
also test blind relevance feedback in IR for QA.
2 Background and Related Work
The performance of an IR system can be quanti-
fied in many ways. We choose and define mea-
sures pertinent to IR for QA. Work has been done
on relevance feedback specific to IR for QA, where
it is has usually be found to be unhelpful. We out-
line the methods used in the past, extend them, and
provide and test means of validating QA relevance
feedback.
2.1 Measuring QA Performance
This paper uses two principle measures to describe
the performance of the IR component. Coverage
is defined as the proportion of questions where at
least one answer bearing text appears in the re-
trieved set. Redundancy is the average number
of answer bearing texts retrieved for each ques-
tion (Roberts and Gaizauskas, 2004).
Both these measures have a fixed limit n on the
number of texts retrieved by a search engine for a
query. As redundancy counts the number of texts
containing correct answers, and not instances of
the answer itself, it can never be greater than the
number of texts retrieved.
The TREC reference answers provide two ways
of finding a correct text, with both a regular expres-
sion and a document ID. Lenient hits (retrievals of
answer bearing documents) are those where the re-
trieved text matches the regular expression; strict
hits occur when the document ID of the retrieved
text matches that declared by TREC as correct and
the text matches the regular expression. Some doc-
uments will match the regular expression but not
be deemed as containing a correct answer (this
is common with numbers and dates (Baeza-Yates
and Ribeiro-Neto, 1999)), in which case a lenient
match is found, but not a strict one.
The answer lists as defined by TREC do not in-
clude every answer-bearing document ? only those
returned by previous systems and marked as cor-
rect. Thus, false negatives are a risk, and strict
measures place an approximate lower bound on
the system?s actual performance. Similarly, lenient
35
matches can occur out of context, without a sup-
porting document; performance based on lenient
matches can be viewed as an approximate upper
bound (Lin and Katz, 2005).
2.2 Relevance Feedback
Relevance feedback is a widely explored technique
for query expansion. It is often done using a spe-
cific measure to select terms using a limited set of
ranked documents of size r; using a larger set will
bring term distribution closer to values over the
whole corpus, and away from ones in documents
relevant to query terms. Techniques are used to
identify phrases relevant to a query topic, in or-
der to reduce noise (such as terms with a low cor-
pus frequency that relate to only a single article)
and query drift (Roussinov and Fan, 2005; Allan,
1996).
In the context of QA, Pizzato (2006) employs
blind RF using the AQUAINT corpus in an attempt
to improve performance when answering factoid
questions on personal names. This is a similar ap-
proach to some content in this paper, though lim-
ited to the study of named entities, and does not
attempt to examine extensions from the existing
answer data.
Monz (2003) finds a negative result when apply-
ing blind feedback for QA in TREC 9, 10 and 11,
and a neutral result for TREC 7 and 8?s ad hoc re-
trieval tasks. Monz?s experiment, using r = 10
and standard Rocchio term weighting, also found
a further reduction in performance when r was
reduced (from 10 to 5). This is an isolated ex-
periment using just one measure on a limited set
of questions, with no use of the available answer
texts.
Robertson (1992) notes that there are issues
when using a whole document for feedback, as
opposed to just a single relevant passage; as men-
tioned in Section 3.1, passage- and document-level
retrieval sets must also be compared for their per-
formance at providing feedback. Critically, we
will survey the intersection between words known
to be helpful and blind RF terms based on initial
retrieval, thus showing exactly how likely an RF
method is to succeed.
3 Methodology
We first investigated the possibility of an IR-
component specific failure leading to impaired
coverage by testing a variety of IR engines and
configurations. Then, difficult questions were
identified, using various performance thresholds.
Next, answer bearing texts for these harder ques-
tions were checked for words that yielded a per-
formance increase when used for query expansion.
After this, we evaluated how likely a RF-based ap-
proach was to succeed. Finally, blind RF was ap-
plied to the whole question set. IR performance
was measured, and terms used for RF compared to
those which had proven to be helpful as extension
words.
3.1 IR Engines
A QA framework (Greenwood, 2004a) was origi-
nally used to construct a QA system based on run-
ning a default Lucene installation. As this only
covers one IR engine in one configuration, it is
prudent to examine alternatives. Other IR engines
should be tested, using different configurations.
The chosen additional engines were: Indri, based
on the mature INQUERY engine and the Lemur
toolkit (Allan et al, 2003); and Terrier, a newer en-
gine designed to deal with corpora in the terabyte
range and to back applications entered into TREC
conferences (Ounis et al, 2005).
We also looked at both passage-level and
document-level retrieval. Passages can be de-
fined in a number of ways, such as a sentence,
a sliding window of k terms centred on the tar-
get term(s), parts of a document of fixed (and
equal) lengths, or a paragraph. In this case,
the documents in the AQUAINT corpus contain
paragraph markers which were used as passage-
level boundaries, thus making ?passage-level?
and ?paragraph-level? equivalent in this paper.
Passage-level retrieval may be preferable for AE,
as the number of potential distracters is some-
what reduced when compared to document-level
retrieval (Roberts and Gaizauskas, 2004).
The initial IR component configuration was with
Lucene indexing the AQUAINT corpus at passage-
level, with a Porter stemmer (Porter, 1980) and an
augmented version of the CACM (Jones and van
Rijsbergen, 1976) stopword list.
Indri natively supports document-level indexing
of TREC format corpora. Passage-level retrieval
was done using the paragraph tags defined in the
corpus as delimiters; this allows both passage- and
document-level retrieval from the same index, ac-
cording to the query.
All the IR engines were unified to use the Porter
36
Coverage Redundancy
Year Len. Strict Len. Strict
Lucene
2004 0.686 0.636 2.884 1.624
2005 0.703 0.566 2.780 1.155
2006 0.665 0.568 2.417 1.181
Indri
2004 0.690 0.554 3.849 1.527
2005 0.694 0.512 3.908 1.056
2006 0.691 0.552 3.373 1.152
Terrier
2004 - - - -
2005 - - - -
2006 0.638 0.493 2.520 1.000
Table 1: Performance of Lucene, Indri and Terrier at para-
graph level, over top 20 documents. This clearly shows the
limitations of the engines.
stemmer and the same CACM-derived stopword
list.
The top n documents for each question in the
TREC2004, TREC2005 and TREC2006 sets were
retrieved using every combination of engine, and
configuration1 . The questions and targets were
processed to produce IR queries as per the default
configuration for the QA framework. Examining
the top 200 documents gave a good compromise
between the time taken to run experiments (be-
tween 30 and 240 minutes each) and the amount
one can mine into the data. Tabulated results are
shown in Table 1 and Table 2. Queries have had
anaphora resolution performed in the context of
their series by the QA framework. AE compo-
nents begin to fail due to excess noise when pre-
sented with over 20 texts, so this value is enough to
encompass typical operating parameters and leave
space for discovery (Greenwood et al, 2006).
A failure analysis (FA) tool, an early version
of which is described by (Sanka, 2005), provided
reporting and analysis of IR component perfor-
mance. In this experiment, it provided high level
comparison of all engines, measuring coverage
and redundancy as the number of documents re-
trieved, n, varies. This is measured because a per-
fect engine will return the most useful documents
first, followed by others; thus, coverage will be
higher for that engine with low values of n.
3.2 Identification of Difficult Questions
Once the performance of an IR configuration over
a question set is known, it?s possible to produce
a simple report listing redundancy for each ques-
tion. A performance reporting script accesses the
1Save Terrier / TREC2004 / passage-level retrieval;
passage-level retrieval with Terrier was very slow using our
configuration, and could not be reliably performed using the
same Terrier instance as document-level retrieval.
Coverage Redundancy
Year Len. Strict Len. Strict
Indri
2004 0.926 0.837 7.841 2.663
2005 0.935 0.735 7.573 1.969
2006 0.882 0.741 6.872 1.958
Terrier
2004 0.919 0.806 7.186 2.380
2005 0.928 0.766 7.620 2.130
2006 0.983 0.783 6.339 2.067
Table 2: Performance of Indri and Terrier at document level
IR over the AQUAINT corpus, with n = 20
FA tool?s database and lists all the questions in
a particular set with the strict and lenient redun-
dancy for selected engines and configurations. En-
gines may use passage- or document-level config-
urations.
Data on the performance of the three engines is
described in Table 2. As can be seen, the cover-
age with passage-level retrieval (which was often
favoured, as the AE component performs best with
reduced amounts of text) languishes between 51%
and 71%, depending on the measurement method.
Failed anaphora resolution may contribute to this
figure, though no deficiencies were found upon vi-
sual inspection.
Not all documents containing answers are noted,
only those checked by the NIST judges (Bilotti
et al, 2004). Match judgements are incomplete,
leading to the potential generation of false nega-
tives, where a correct answer is found with com-
plete supporting information, but as the informa-
tion has not been manually flagged, the system will
mark this as a failure. Assessment methods are
fully detailed in Dang et al (2006). Factoid per-
formance is still relatively poor, although as only
1.95 documents match per question, this may be an
effect of such false negatives (Voorhees and Buck-
land, 2003). Work has been done into creating
synthetic corpora that include exhaustive answer
sets (Bilotti, 2004; Tellex et al, 2003; Lin and
Katz, 2005), but for the sake of consistency, and
easy comparison with both parallel work and prior
local results, the TREC judgements will be used to
evaluate systems in this paper.
Mean redundancy is also calculated for a num-
ber of IR engines. Difficult questions were those
for which no answer bearing texts were found by
either strict or lenient matches in any of the top n
documents, using a variety of engines. As soon as
one answer bearing document was found by an en-
gine using any measure, that question was deemed
non-difficult. Questions with mean redundancy of
37
zero are marked difficult, and subjected to further
analysis. Reducing the question set to just diffi-
cult questions produces a TREC-format file for re-
testing the IR component.
3.3 Extension of Difficult Questions
The documents deemed relevant by TREC must
contain some useful text that can help IR engine
performance. Such words should be revealed by
a gain in redundancy when used to extend an ini-
tially difficult query, usually signified by a change
from zero to a non-zero value (signifying that rele-
vant documents have been found where none were
before). In an attempt to identify where the use-
ful text is, the relevant documents for each difficult
question were retrieved, and passages matching the
answer regular expression identified. A script is
then used to build a list of terms from each passage,
removing words in the question or its target, words
that occur in the answer, and stopwords (based on
both the indexing stopword list, and a set of stems
common within the corpus). In later runs, num-
bers are also stripped out of the term list, as their
value is just as often confusing as useful (Baeza-
Yates and Ribeiro-Neto, 1999). Of course, answer
terms provide an obvious advantage that would not
be reproducible for questions where the answer is
unknown, and one of our goals is to help query ex-
pansion for unseen questions. This approach may
provide insights that will enable appropriate query
expansion where answers are not known.
Performance has been measured with both the
question followed by an extension (Q+E), as well
as the question followed by the target and then
extension candidates (Q+T+E). Runs were also
executed with just Q and Q+T, to provide non-
extended reference performance data points. Ad-
dition of the target often leads to gains in perfor-
mance (Roussinov et al, 2005), and may also aid
in cases where anaphora resolution has failed.
Some words are retained, such as titles, as in-
cluding these can be inferred from question or tar-
get terms and they will not unfairly boost redun-
dancy scores; for example, when searching for a
?Who? question containing the word ?military?,
one may want to preserve appellations such as
?Lt.? or ?Col.?, even if this term appears in the an-
swer.
This filtered list of extensions is then used to cre-
ate a revised query file, containing the base ques-
tion (with and without the target suffixed) as well
as new questions created by appending a candidate
extension word.
Results of retrievals with these new question are
loaded into the FA database and a report describ-
ing any performance changes is generated. The
extension generation process also creates custom
answer specifications, which replicate the informa-
tion found in the answers defined by TREC.
This whole process can be repeated with vary-
ing question difficulty thresholds, as well as alter-
native n values (typically from 5 to 100), different
engines, and various question sets.
3.4 Relevance Feedback Performance
Now that we can find the helpful extension words
(HEWs) described earlier, we?re equipped to eval-
uate query expansion methods. One simplistic ap-
proach could use blind RF to determine candidate
extensions, and be considered potentially success-
ful should these words be found in the set of HEWs
for a query. For this, term frequencies can be
measured given the top r documents retrieved us-
ing anaphora-resolved query Q. After stopword
and question word removal, frequent terms are ap-
pended to Q, which is then re-evaluated. This
has been previously attempted for factoid ques-
tions (Roussinov et al, 2005) and with a limited
range of r values (Monz, 2003) but not validated
using a set of data-driven terms.
We investigated how likely term frequency (TF)
based RF is to discover HEWs. To do this, the
proportion of HEWs that occurred in initially re-
trieved texts was measured, as well as the propor-
tion of these texts containing at least one HEW.
Also, to see how effective an expansion method is,
suggested expansion terms can be checked against
the HEW list.
We used both the top 5 and the top 50 documents
in formulation of extension terms, with TF as a
ranking measure; 50 is significantly larger than the
optimal number of documents for AE (20), without
overly diluting term frequencies.
Problems have been found with using entire
documents for RF, as the topic may not be the
same throughout the entire discourse (Robertson
et al, 1992). Limiting the texts used for RF to
paragraphs may reduce noise; both document- and
paragraph-level terms should be checked.
38
Engine
Year Lucene
Para
Indri
Para
Indri
Doc
Terrier
Doc
2004 76 72 37 42
2005 87 98 37 35
2006 108 118 59 53
Table 3: Number of difficult questions, as defined by those
which have zero redundancy over both strict and lenient mea-
sures, at n = 20. Questions seem to get harder each year.
Document retrieval yields fewer difficult questions, as more
text is returned for potential matching.
Engine
Lucene Indri Terrier
Paragraph 226 221 -
Document - 121 109
Table 4: Number of difficult questions in the 2006 task, as de-
fined above, this time with n = 5. Questions become harder
as fewer chances are given to provide relevant documents.
4 Results
Once we have HEWs, we can determine if these
are going to be of significant help when chosen as
query extensions. We can also determine if a query
expansion method is likely to be fruitful. Blind RF
was applied, and assessed using the helpful words
list, as well as RF?s effect on coverage.
4.1 Difficult Question Analysis
The number of difficult questions found at n =
20 is shown in Table 3. Document-level retrieval
gave many fewer difficult questions, as the amount
of text retrieved gave a higher chance of finding
lenient matches. A comparison of strict and lenient
matching is in Table 5.
Extensions were then applied to difficult ques-
tions, with or without the target. The performance
of these extensions is shown in Table 6. Results
show a significant proportion (74.4%) of difficult
questions can benefit from being extended with
non-answer words found in answer bearing texts.
4.2 Applying Relevance Feedback
Identifying HEWs provides a set of words that
are useful for evaluating potential expansion terms.
Match type
Strict Lenient
Year
2004 39 49
2005 56 66
2006 53 49
Table 5: Common difficult questions (over all three engines
mentioned above) by year and match type; n = 20.
Difficult questions used 118
Variations tested 6683
Questions that benefited 87 (74.4%)
Helpful extension words (strict) 4973
Mean helpful words per question 42.144
Mean redundancy increase 3.958
Table 6: Using Terrier Passage / strict matching, retrieving 20
docs, with TREC2006 questions / AQUAINT. Difficult ques-
tions are those where no strict matches are found in the top 20
IRT from just one engine.
2004 2005 2006
HEW found in IRT 4.17% 18.58% 8.94%
IRT containing HEW 10.00% 33.33% 34.29%
RF words in HEW 1.25% 1.67% 5.71%
Table 7: ?Helpful extension words?: the set of extensions that,
when added to the query, move redundancy above zero. r =
5, n = 20, using Indri at passage level.
Using simple TF based feedback (see Section 3.4),
5 terms were chosen per query. These words had
some intersection (see Table 7) with the exten-
sion words set, indicating that this RF may lead to
performance increases for previously unseen ques-
tions. Only a small number of the HEWs occur in
the initially retrieved texts (IRTs), although a no-
ticeable proportion of IRTs (up to 34.29%) contain
at least one HEW. However, these terms are prob-
ably not very frequent in the documents and un-
likely to be selected with TF-based blind RF. The
mean proportion of RF selected terms that were
HEWs was only 2.88%. Blind RF for question an-
swering fails here due to this low proportion. Strict
measures are used for evaluation as we are inter-
ested in finding documents which were not pre-
viously being retrieved rather than changes in the
distribution of keywords in IRT.
Document and passage based RF term selection
is used, to explore the effect of noise on terms, and
document based term selection proved marginally
superior. Choosing RF terms from a small set of
documents (r = 5) was found to be marginally
better than choosing from a larger set (r = 50).
In support of the suggestion that RF would be un-
r
5 50 Baseline
Rank Doc Para Doc Para
5 0.253 0.251 0.240 0.179 0.312
10 0.331 0.347 0.331 0.284 0.434
20 0.438 0.444 0.438 0.398 0.553
50 0.583 0.577 0.577 0.552 0.634
Table 8: Coverage (strict) using blind RF. Both document-
and paragraph-level retrieval used to determine RF terms.
39
Question:
Who was the nominal leader after the overthrow?
Target: Pakistani government overthrown in 1999
Extension word Redundancy
Kashmir 4
Pakistan 4
Islamabad 2.5
Question: Where did he play in college?
Target: Warren Moon
Extension word Redundancy
NFL 2.5
football 1
Question: Who have commanded the division?
Target: 82nd Airborne division
Extension word Redundancy
Gen 3
Col 2
decimated 2
officer 1
Table 9: Queries with extensions, and their mean redundancy
using Indri at document level with n = 20. Without exten-
sions, redundancy is zero.
likely to locate HEWs, applying blind RF consis-
tently hampered overall coverage (Table 8).
5 Discussion
HEWs are often found in answer bearing texts,
though these are hard to identify through sim-
ple TF-based RF. A majority of difficult questions
can be made accessible through addition of HEWs
present in answer bearing texts, and work to deter-
mine a relationship between words found in initial
retrieval and these HEWs can lead to coverage in-
creases. HEWs also provide an effective means
of evaluating other RF methods, which can be de-
veloped into a generic rapid testing tool for query
expansion techniques. TF-based RF, while finding
some HEWs, is not effective at discovering exten-
sions, and reduces overall IR performance.
There was not a large performance change
between engines and configurations. Strict
paragraph-level coverage never topped 65%, leav-
ing a significant number of questions where no
useful information could be provided for AE.
The original sets of difficult questions for in-
dividual engines were small ? often less than the
35% suggested when looking at the coverage fig-
ures. Possible causes could include:
Difficult questions being defined as those for
which average redundancy is zero: This limit
may be too low. To remedy this, we could increase
the redundancy limit to specify an arbitrary num-
ber of difficult questions out of the whole set.
The use of both strict and lenient measures: It
is possible to get a lenient match (thus marking a
question as non-difficult) when the answer text oc-
curs out of context.
Reducing n from 20 to 5 (Table 4) increased
the number of difficult questions produced. From
this we can hypothesise that although many search
engines are succeeding in returning useful docu-
ments (where available), the distribution of these
documents over the available ranks is not one that
bunches high ranking documents up as those im-
mediately retrieved (unlike a perfect engine; see
Section 3.1), but rather suggests a more even dis-
tribution of such documents over the returned set.
The number of candidate extension words for
queries (even after filtering) is often in the range
of hundreds to thousands. Each of these words
creates a separate query, and there are two varia-
tions, depending on whether the target is included
in the search terms or not. Thus, a large number
of extended queries need to be executed for each
question run. Passage-level retrieval returns less
text, which has two advantages: firstly, it reduces
the scope for false positives in lenient matching;
secondly, it is easier to scan result by eye and de-
termine why the engine selected a result.
Proper nouns are often helpful as extensions.
We noticed that these cropped up fairly regularly
for some kinds of question (e.g. ?Who?). Espe-
cially useful were proper nouns associated with
locations - for example, adding ?Pakistani? to
a query containing the word Pakistan lifted re-
dundancy above zero for a question on President
Musharraf, as in Table 9. This reconfirms work
done by Greenwood (2004b).
6 Conclusion and Future Work
IR engines find some questions very difficult and
consistently fail to retrieve useful texts even with
high values of n. This behaviour is common over
many engines. Paragraph level retrieval seems to
give a better idea of which questions are hard-
est, although the possibility of false negatives is
present from answer lists and anaphora resolution.
Relationships exist between query words and
helpful words from answer documents (e.g. with
a military leadership themes in a query, adding the
term ?general? or ?gen? helps). Identification of
HEWs has potential use in query expansion. They
could be used to evaluate RF approaches, or asso-
ciated with question words and used as extensions.
Previous work has ruled out relevance feedback
40
in particular circumstances using a single ranking
measure, though this has not been based on analy-
sis of answer bearing texts. The presence of HEWs
in IRT for difficult questions shows that guided RF
may work, but this will be difficult to pursue. Blind
RF based on term frequencies does not increase IR
performance. However, there is an intersection be-
tween words in initially retrieved texts and words
data driven analysis defines as helpful, showing
promise for alternative RF methods (e.g. based on
TFIDF). These extension words form a basis for
indicating the usefulness of RF and query expan-
sion techniques.
In this paper, we have chosen to explore only
one branch of query expansion. An alternative data
driven approach would be to build associations be-
tween recurrently useful terms given question con-
tent. Question texts could be stripped of stopwords
and proper nouns, and a list of HEWs associated
with each remaining term. To reduce noise, the
number of times a particular extension has helped
a word would be counted. Given sufficient sample
data, this would provide a reference body of HEWs
to be used as an aid to query expansion.
References
Allan, J., J. Callan, K. Collins-Thompson, B. Croft,
F. Feng, D. Fisher, J. Lafferty, L. Larkey, TN Truong,
P. Ogilvie, et al 2003. The Lemur Toolkit for Lan-
guage Modeling and Information Retrieval.
Allan, J. 1996. Incremental Relevance Feedback for
Information Filtering. In Research and Development
in IR, pages 270?278.
Baeza-Yates, R. and B. Ribeiro-Neto. 1999. Modern
Information Retrieval. Addison Wesley.
Bilotti, M.W., B. Katz, and J. Lin. 2004. What Works
Better for Question Answering: Stemming or Mor-
phological Query Expansion. Proc. IR for QA Work-
shop at SIGIR 2004.
Bilotti, M.W. 2004. Query Expansion Techniques for
Question Answering. Master?s thesis, Massachusetts
Institute of Technology.
Dang, H.T., J. Lin, and D. Kelly. 2006. Overview of
the TREC 2006 QA track. Proc. 15th Text REtrieval
Conf..
Greenwood, M.A., M. Stevenson, and R. Gaizauskas.
2006. The University of Sheffield?s TREC 2006
Q&A Experiments. In Proc. 15th Text REtrieval
Conference
Greenwood, M.A. 2004a. AnswerFinder: Question
Answering from your Desktop. In Proc. 7th Annual
Colloquium for the UK SIG for Computational Lin-
guistics (CLUK ?04).
Greenwood, M.A. 2004b. Using Pertainyms to Im-
prove Passage Retrieval for Questions Requesting
Information about a Location. In Proc. Workshop
on IR for QA (SIGIR 2004).
Jones, K.S. and C.J. van Rijsbergen. 1976. IR Test
Collections. J. of Documentation, 32(1):59?75.
Lin, J. and B. Katz. 2005. Building a Reusable Test
Collection for Question Answering. J. American So-
ciety for Information Science and Technology.
Monz, C. 2003. From Document Retrieval to Question
Answering. ILLC Dissertation Series 2003, 4.
Ounis, I., G. Amati, V. Plachouras, B. He, C. Macdon-
ald, and D. Johnson. 2005. Terrier IR Platform.
Proc. 27th European Conf. on IR (ECIR 05), San-
tiago de Compostela, Spain, pages 517?519.
Pizzato, L.A., D. Molla, and C. Paris. 2006. Pseudo-
Relevance Feedback using Named Entities for Ques-
tion Answering. Australasian Language Technology
Workshop (ALTW2006), pages 83?90.
Porter, M. 1980. An Algorithm for Suffix Stripping
Program. Program, 14(3):130?137.
Roberts, I and R Gaizauskas. 2004. Evaluating Passage
Retrieval Approaches for Question Answering. In
Proc. 26th European Conf. on IR.
Robertson, S.E., S. Walker, M. Hancock-Beaulieu,
A. Gull, and M. Lau. 1992. Okapi at TREC. In
Text REtrieval Conf., pages 21?30.
Roussinov, D. and W. Fan. 2005. Discretization Based
Learning Approach to Information Retrieval. In
Proc. 2005 Conf. on Human Language Technologies.
Roussinov, D., M. Chau, E. Filatova, and J.A. Robles-
Flores. 2005. Building on Redundancy: Fac-
toid Question Answering, Robust Retrieval and the
?Other?. In Proc. 14th Text REtrieval Conf.
Sanka, Atheesh. 2005. Passage Retrieval for Question
Answering. Master?s thesis, University of Sheffield.
Tellex, S., B. Katz, J. Lin, A. Fernandes, and G. Marton.
2003. Quantitative Evaluation of Passage Retrieval
Algorithms for Question Answering. Proc. 26th An-
nual Int?l ACM SIGIR Conf. on R&D in IR, pages
41?47.
Voorhees, E. and L. P. Buckland, editors. 2003. Proc.
12th Text REtrieval Conference.
41
Proceedings of the 5th Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 38?45,
Baltimore, Maryland USA, August 26 2014. c?2014 Association for Computational Linguistics
Preliminary Test of a Real-Time, Interactive Silent Speech Interface 
Based on Electromagnetic Articulograph 
 
 
Jun Wang 
Dept. of Bioengineering 
Callier Center for Communi-
cation Disorders 
University of Texas at Dallas 
wangjun@utdallas.edu 
Ashok Samal 
Dept. of Computer Science & 
Engineering 
University of Nebraska-
Lincoln 
samal@cse.unl.edu 
Jordan R. Green 
Dept. of Communication Sci-
ences & Disorders 
MGH Institute of Health Pro-
fessions 
jgreen2@mghihp.edu 
 
  
 
Abstract 
A silent speech interface (SSI) maps articula-
tory movement data to speech output. Alt-
hough still in experimental stages, silent 
speech interfaces hold significant potential 
for facilitating oral communication in persons 
after laryngectomy or with other severe voice 
impairments. Despite the recent efforts on si-
lent speech recognition algorithm develop-
ment using offline data analysis, online test 
of SSIs have rarely been conducted. In this 
paper, we present a preliminary, online test of 
a real-time, interactive SSI based on electro-
magnetic motion tracking. The SSI played 
back synthesized speech sounds in response 
to the user?s tongue and lip movements. 
Three English talkers participated in this test, 
where they mouthed (silently articulated) 
phrases using the device to complete a 
phrase-reading task. Among the three partici-
pants, 96.67% to 100% of the mouthed 
phrases were correctly recognized and corre-
sponding synthesized sounds were played af-
ter a short delay. Furthermore, one participant 
demonstrated the feasibility of using the SSI 
for a short conversation. The experimental re-
sults demonstrated the feasibility and poten-
tial of silent speech interfaces based on elec-
tromagnetic articulograph for future clinical 
applications. 
1 Introduction 
Daily communication is often a struggle for per-
sons who have undergone a laryngectomy, a sur-
gical removal of the larynx due to the treatment 
of cancer (Bailey et al., 2006). In 2013, about 
12,260 new cases of laryngeal cancer were esti-
mated in the United States (American Cancer 
Society, 2013). Currently, there are only limited 
treatment options for these individuals including 
(1) esophageal speech, which involves oscillation 
of the esophagus and is difficult to learn; (2) tra-
cheo-esophageal speech, in which a voice pros-
thesis is placed in a tracheo-esophageal puncture; 
and (3) electrolarynx, an external device held on 
the neck during articulation, which produces a 
robotic voice quality (Liu and Ng, 2007). Per-
haps the greatest disadvantage of these ap-
proaches is that they produce abnormal sounding 
speech with a fundamental frequency that is low 
and limited in range. The abnormal voice quality 
output severely affects the social life of people 
after laryngectomy (Liu and Ng, 2007). In addi-
tion, the tracheo-esophageal option requires an 
additional surgery, which is not suitable for eve-
ry patient (Bailey et al., 2006). Although re-
search is being conducted on improving the 
voice quality of esophageal or electrolarynx 
speech (Doi et al., 2010; Toda et al., 2012), new 
assistive technologies based on non-audio infor-
mation (e.g., visual or articulatory information) 
may be a good alternative approach for providing 
natural sounding speech output for persons after 
laryngectomy. 
Visual speech recognition (or automatic lip 
reading) typically uses an optical camera to ob-
tain lip and/or facial features during speech (in-
cluding lip contour, color, opening, movement, 
etc.) and then classify these features to speech 
units (Meier et al., 2000; Oviatt, 2003). Howev-
er, due to the lack of information from tongue, 
the primary articulator, visual speech recognition 
(i.e., using visual information only, without 
tongue and audio information) may obtain a low 
accuracy (e.g., 30% - 40% for phoneme classifi-
cation, Livescu et al., 2007). Furthermore, Wang 
and colleagues (2013b) have showed any single 
tongue sensor (from tongue tip to tongue body 
38
 Figure 1. Design of the real-time silent speech interface. 
back on the midsagittal line) encodes significant-
ly more information in distinguishing phonemes 
than do lips. However, visual speech recognition 
is well suited for applications with small-
vocabulary (e.g., a lip-reading based command-
and-control system for home appliance) or using 
visual information as an additional source for 
acoustic speech recognition, referred to as audio-
visual speech recognition (Potamianos et al., 
2003), because such a system based on portable 
camera is convenient in practical use. In contrast, 
SSIs, with tongue information, have potential to 
obtain a high level of silent speech recognition 
accuracy (without audio information). Currently, 
two major obstacles for SSI development are 
lack of (a) fast and accurate recognition algo-
rithms and (b) portable tongue motion tracking 
devices for daily use. 
SSIs convert articulatory information into text 
that drives a text-to-speech synthesizer. Although 
still in developmental stages (e.g., speaker-
dependent recognition, small-vocabulary), SSIs 
even have potential to provide speech output 
based on prerecorded samples of the patient?s 
own voice (Denby et al., 2010; Green et al., 
2011; Wang et al., 2009). Potential articulatory 
data acquisition methods for SSIs include ultra-
sound (Denby et al., 2011; Hueber et al., 2010), 
surface electromyography electrodes (Heaton et 
al., 2011; Jorgensen and Dusan, 2010), and elec-
tromagnetic articulograph (EMA) (Fagan et al., 
2008; Wang et al., 2009, 2012a). 
Despite the recent effort on silent speech in-
terface research, online test of SSIs has rarely 
been studied. So far, most of the published work 
on SSIs has focused on development of silent 
speech recognition algorithm through offline 
analysis (i.e., using prerecorded data) (Fagan et 
al., 2008;  Heaton et al., 2011; Hofe et al., 2013; 
Hueber et al., 2010; Jorgenson et al., 2010; Wang 
et al., 2009a, 2012a, 2012b, 2013c). Ultrasound-
based SSIs have been tested online with multiple 
subjects and encouraging results were obtained 
in a phrase reading task where the subjects were 
asked to silently articulate sixty phrases (Denby 
et al., 2011). SSI based on electromagnetic sens-
ing has been only tested using offline analysis 
(using pre-recorded data) collected from single 
subjects (Fagan et al., 2008; Hofe et al., 2013), 
although some work simulated online testing 
using prerecorded data (Wang et al., 2012a, 
2012b, 2013c). Online tests of SSIs using elec-
tromagnetic articulograph with multiple subjects 
are needed to show the feasibility and potential 
of the SSIs for future clinical applications. 
In this paper, we report a preliminary, online 
test of a newly-developed, real-time, and interac-
tive SSI based on a commercial EMA. EMA 
tracks articulatory motion by placing small sen-
sors on the surface of tongue and other articula-
tors (e.g., lips and jaw). EMA is well suited for 
the early state of SSI development because it (1) 
is non-invasive, (2) has a high spatial resolution  
in motion tracking, (3) has a high sampling rate, 
and (4) is affordable. In this experiment, partici-
pants used the real-time SSI to complete an 
online phrase-reading task and one of them had a 
short conversation with another person. The re-
sults demonstrated the feasibility and potential of 
SSIs based on electromagnetic sensing for future 
clinical applications. 
2 Design 
2.1 Major design 
Figure 1 illustrates the three-component design 
of the SSI: (a) real-time articulatory motion 
tracking using a commercial EMA, (b) online 
silent speech recognition (converting articulation 
information to text), and (c) text-to-speech syn-
thesis for speech output.  
The EMA system (Wave Speech Research 39
      
Figure 2. Demo of a participant using the silent speech interface. The left picture illustrates the 
coordinate system and sensor locations (sensor labels are described in text); in the right picture, a 
participant is using the silent speech interface to finish the online test. 
system, Northern Digital Inc., Waterloo, Canada) 
was used to track the tongue and lip movement 
in real-time. The sampling rate of the Wave sys-
tem was 100 Hz, which is adequate for this ap-
plication (Wang et al., 2012a, 2012b, 2013c). 
The spatial accuracy of motion tracking using 
Wave is 0.5 mm (Berry, 2011). 
The online recognition component recognized 
functional phrases from articulatory movements 
in real-time. The recognition component is mod-
ular such that alternative classifiers can easily 
replace and be integrated into the SSI. In this 
preliminary test, recognition was speaker-
dependent, where training and testing data were 
from the same speakers. 
The third component played back either pre-
recorded or synthesized sounds using a text-to-
speech synthesizer (Huang et al., 1997). 
2.2 Other designs 
A graphical user interface (GUI) is integrated 
into the silent speech interface for ease of opera-
tion. Using the GUI, users can instantly re-train 
the recognition engine (classifier) when new 
training samples are available. Users can also 
switch output voice (e.g., male or female). 
Data transfer through TCP/IP. Data transfer 
from the Wave system to the recognition unit 
(software) is accomplished through TCP/IP, the 
standard data transfer protocols on Internet. Be-
cause data bandwidth requirement is low (multi-
ple sensors, multiple spatial coordinates for each 
sensor, at 100 Hz sampling rate), any 3G or fast-
er network connection will be sufficient for fu-
ture use with wireless data transfer.  
Extensible (closed) vocabulary. In the early 
stage of this development, closed-vocabulary 
silent speech recognition was used; however, the 
vocabulary is extensible. Users can add new 
phrases into the system through the GUI. Adding 
a new phrase in the vocabulary is done in two 
steps. The user (the patient) first enters the 
phrase using a keyboard (keyboard input can also 
be done by an assistant or speech pathologist), 
and then produces a few training samples for the 
phrase (a training sample is articulatory data la-
beled with a phrase). The system automatically 
re-trains the recognition model integrating the 
newly-added training samples. Users can delete 
invalid training samples using the GUI as well. 
2.3 Real-time data processing 
The tongue and lip movement positional data 
obtained from the Wave system were processed 
in real-time prior to being used for recognition. 
This included the calculation of head-
independent positions of the tongue and lip sen-
sors and low pass filtering for removing noise.  
The movements of the 6 DOF head sensor 
were used to calculate the head-independent 
movements of other sensors. The Wave system 
represents object orientation or rotation (denoted 
by yaw, pitch, and roll in Euler angles) in qua-
ternions, a four-dimensional vector. Quaternion 
has its advantages over Euler angles. For exam-
ple, quaternion avoids the issue of gimbal lock 
(one degree of freedom may be lost in a series of 
rotation), and it is simpler to achieve smooth in-
terpolation using quaternion than using Euler 
angles (Dam et al., 1998). Thus, quaternion has 
been widely used in computer graphics, comput-
er vision, robotics, virtual reality, and flight dy-
namics (Kuipers, 1999). Given the unit quaterni-
on  
q = (a, b, c, d)                        (1) 
where a2 + b2 + c2 + d2 = 1, a 3 ? 3 rotation ma-
trix R can be derived using Equation (2): 40
?
?
?
?
?
?
?
?
?
?
+??+?
??+?+
+???+
=
2222
2222
2222
2222
2222
2222
dcbaabcdacbd
abcddcbaadbc
acbdadbcdcba
R
  (2) 
For details of how the quaternion is used in 
Wave system, please refer to the Wave Real-
Time API manual and sample application 
(Northern Digital Inc., Waterloo, Canada). 
3 A Preliminary Online Test  
3.1 Participants & Stimuli 
Three American English talkers participated in 
this experiment (two males and one female with 
average age 25 and SD 3.5 years). No history of 
speech, language, hearing, or any cognitive prob-
lems were reported. 
Sixty phrases that are frequently used in daily 
life by healthy people and AAC (augmentative 
and alternative communication) users were used 
in this experiment. Those phrases were selected 
from the lists in Wang et al., 2012a and Beukel-
man and Gutmann, 1999. 
3.2 Procedure 
Setup 
 
The Wave system tracks the motion of sensors 
attached on the articulators by establishing an 
electromagnetic field by a textbook-sized genera-
tor. Participants were seated with their head 
within the calibrated magnetic field (Figure 2, 
the right picture), facing a computer monitor that 
displays the GUI of the SSI. The sensors were 
attached to the surface of each articulator using 
dental glue (PeriAcryl Oral Tissue Adhesive). 
Prior to the experiment, each subject participated 
in a three-minute training session (on how to use 
the SSI), which also helped them adapt to the 
oral sensors. Previous studies have shown those 
sensors do not significantly affect their speech 
output after a short practice (Katz et al., 2006; 
Weismer and Bunton, 1999). 
Figure 2 (left) shows the positions of the five 
sensors attached to a participant?s forehead, 
tongue, and lips (Green et al., 2003; 2013; Wang 
et al., 2013a). One 6 DOF (spatial and rotational) 
head sensor was attached to a nose bridge on a 
pair of glasses (rather than on forehead skin di-
rectly), to avoid the skin artifact (Green et al., 
2007). Two 5 DOF sensors - TT (Tongue Tip) 
and TB (Tongue Body Back) - were attached on 
the midsagittal of the tongue. TT was located 
approximately 10 mm from the tongue apex 
(Wang et al., 2011, 2013a). TB was placed as far 
back as possible, depending on the participant?s 
tongue length (Wang et al., 2013b). Lip move-
ments were captured by attaching two 5 DOF 
sensors to the vermilion borders of the upper 
(UL) and lower (LL) lips at midline. The four 
sensors (i.e., TT, TB, UL, and LL) placements 
were selected based on literature showing that 
they are able to achieve as high recognition accu-
racy as that obtained using more tongue sensors 
for this application (Wang et al., 2013b). 
As mentioned previously, real-time prepro-
cessing of the positional time series was con-
ducted, including subtraction of head movements 
from tongue and lip data and noise reduction us-
ing a 20 Hz low pass filter (Green et al., 2003; 
Wang et al., 2013a). Although the tongue and lip 
sensors are 5D, only the 3D spatial data (i.e., x, y, 
and z) were used in this experiment. 
 
Training 
 
The training step was conducted to obtain a few 
samples for each phrase. The participants were 
asked to silently articulate all sixty phrases twice 
at their comfortable speaking rate, while the 
tongue and lip motion was recorded. Thus, each 
phrase has at least two samples for training. Dy-
namic Time Warping (DTW) was used as the 
classifier in this preliminary test, because of its 
rapid execution (Denby et al., 2011), although 
Gaussian mixture models may perform well too 
when the number of training samples is small 
(Broekx et al., 2013). DTW is typically used to 
compare two single-dimensional time-series, 
Training_Algorithm 
Let T1? Tn  be the sets of training samples for n 
phrases, where 
Ti = {Ti,1, ? Ti,j, ? Ti,mi} are mi samples for 
phrase i. 
1    for i = 1 to n     // n is the number of phrases 
2 Li = sum(length(Ti,j)) / mi,  j = 1 to mi; 
3 T = Ti,1;       // first sample of phrase i 
3 for j = 2 to mi 
4                (T', T'i,j) = MDTW(T, Ti,j); 
5         T  =  (T' + T'i,j) / 2;//amplitude mean 
6         T  =  time_normalize(T, Li); 
7 end 
8 Ri = T;   // representative sample for phrase i 
9     end   
10   Output(R); 
Figure 3. Training algorithm using DTW. The 
function call MDTW() returns the average 
DTW distances between the corresponding 
sensors and dimensions of two data samples. 41
 Subject 
Accuracy 
(%) 
Latency 
(s) 
# of Train-
ing Samples 
S01 100 3.086 2.0 
S02 96.67 1.403 2.4 
S03 96.67 1.524 3.1 
Table 1.  Phrase classification accuracy and 
latency for all three participants. 
 
thus we calculated the average DTW distance 
across the corresponding sensors and dimensions 
of two data samples. DTW was adapted as fol-
lows for training. 
The training algorithm generated a repre-
sentative sample based on all available training 
samples for each phrase. Pseudo-code of the 
training algorithm is provided in Figure 3, for the 
convenience of description. For each phrase i, 
first, MDTW was applied to the first two training 
samples, Ti,1 and Ti,2. Sample T is the amplitude-
mean of the warped samples T'i,1 and T'i,2 (time-
series) (Line 5). Next, T was time-normalized 
(stretched) to the average length of all training 
samples for this phrase (Li), which was to reduce 
the effects of duration change caused by DTW 
warping (Line 6). The procedure continued until 
the last training sample Ti, mi (mi is the number of 
training samples for phrase i). The final T was 
the representative sample for phrase i. 
The training procedure can be initiated by 
pressing a button on the GUI anytime during the 
use of the SSI. 
 
Testing 
 
During testing, each participant silently articulat-
ed the same list of phrases while the SSI recog-
nized each phrase and played corresponding syn-
thesized sounds. DTW was used to compare the 
test sample with the representative training sam-
ple for each phrase (Ri, Figure 3). The phrase that 
had the shortest DTW distance to the test sample 
was recognized. The testing was triggered by 
pressing a button on the GUI. If the phrase was 
incorrectly predicted, the participant was allowed 
to add at most two additional training samples 
for that phrase.  
Figure 2 (right) demonstrates a participant is 
using the SSI during the test. After the partici-
pant silently articulated ?Good afternoon?, the 
SSI displayed the phrase on the screen, and 
played the corresponding synthesized sound 
simultaneously. 
Finally, one participant used the SSI for a bidi-
rectional conversation with an investigator. Since 
this prototype SSI has a closed-vocabulary 
recognition component, the participant had to 
choose the phrases that have been trained. This 
task was intended to provide a demo of how the 
SSI is used for daily communication. The script 
of the conversation is as below: 
Investigator: Hi DJ, How are you? 
Subject: I?m fine. How are you doing? 
Investigator: I?m good. Thanks. 
Subject: I use a silent speech interface to talk. 
Investigator: That?s cool. 
Subject: Do you understand me? 
Investigator: Oh, yes. 
Subject: That?s good. 
4 Results and Discussion 
Table 1 lists the performance using the SSI for 
all three participants in the online, phrase-
reading task. The three subjects obtained a 
phrase recognition accuracy from 96.67% to 
100.00%, with a latency from 1.403 second to 
3.086 seconds, respectively. The high accuracy 
and relatively short delay demonstrated the fea-
sibility and potential of SSIs based on electro-
magnetic articulograph.  
The order of the participants in the experiment 
was S01, S02, and then S03. After the experi-
ment of S01, where all three dimensional data (x, 
y, and z) were used, we decided to use only y and 
z for S02 and S03 to reduce the latency. As listed 
in Table 1, the latencies of S02 and S03 did sig-
nificantly reduce, because less data was used for 
online recognition. 
Surprisingly, phrase recognition without using 
x dimension (left-right) data led to a decrease of 
accuracy and more training samples were re-
quired; prior research suggests that tongue 
movement in this dimension is not significant 
during speech in healthy talkers (Westbury, 
1994). This observation is supported by partici-
pant S01, who had the highest accuracy and 
needed fewer training samples for each phrase 
(column 3 in Table 1). S02 and S03 used data of 
only y and z dimensions. Of course, data from 
more subjects are needed to confirm the potential 
significance of the x dimension movement of the 
tongue to silent speech recognition accuracy.  
Data transfer between the Wave machine and 
the SSI recognition component was done through 
TCP/IP protocols and in real-time. In the future, 
this design feature will allow the recognition 
component to run on a smart phone or any wear-
able devices with an Internet connection (Cellu-
42
lar or Wi-Fi). In this preliminary test, the indi-
vidual delays caused by TCP/IP data transfer, 
online data preprocessing, and classification 
were not measured and thus unknown. The delay 
information may be useful for our future devel-
opment that the recognition component is de-
ployed on a smart-phone. A further study is 
needed to obtain and analyze the delay infor-
mation.  
The bidirectional dialogue by one of the par-
ticipants demonstrated how the SSI can be used 
in daily conversation. To our best knowledge, 
this is the first conversational demo using a SSI. 
An informal survey to a few colleagues provided 
positive feedback. The conversation was smooth, 
although it is noticeably slower than a conversa-
tion between two healthy talkers. Importantly, 
the voice output quality (determined by the text-
to-speech synthesizer) was natural, which strong-
ly supports the major motivation of SSI research: 
to produce speech with natural voice quality that 
current treatments cannot provide. A video demo 
is available online (Wang, 2014). 
The participants in this experiment were 
young and healthy. It is, however, unknown if 
the recognition accuracy may decrease or not for 
users after laryngectomy, although a single pa-
tient study showed the accuracy may decrease 
15-20% compared to healthy talkers using an 
ultrasound-based SSI (Denby et al., 2011). Theo-
retically, the tongue motion patterns in (silent) 
speech after the surgery should be no difference 
with that of healthy talkers. In practice, however, 
some patients after the surgery may be under 
treatment for swallowing using radioactive de-
vices, which may affect their tongue motion pat-
terns in articulation. Thus, the performance of 
SSIs may vary and depend on the condition of 
the patients after laryngectomy. A test of the SSI 
using multiple participants after laryngectomy is 
needed to understand the performance of SSIs 
for those patients under different conditions.  
Although a demonstration of daily conversa-
tion using the SSI is provided, SSI based on the 
non-portable Wave system is currently not ready 
for practical use. Fortunately, more affordable 
and portable electromagnetic devices are being 
developed as are small handheld or wearable de-
vices (Fagan et al., 2008). Researchers are also 
testing the efficacy of permanently implantable 
and wireless sensors (Chen et al., 2012; Park et 
al., 2012). In the future, those more portable, and 
wireless articulatory motion tracking devices, 
when they are ready, will be used to develop a 
portable SSI for practice use. 
In this experiment, a simple DTW algorithm 
was used to compare the training and testing 
phrases, which is known to be slower than most 
machine learning classifiers. Thus, in the future, 
the latency can be significantly reduced by using 
faster classifiers such as support vector machines 
(Wang et al., 2013c) or hidden Markov models 
(Heracleous and Hagita, 2011; King et al., 2007; 
Rudzicz et al., 2012; Uraga and Hain, 2006). 
Furthermore, in this proof-of-concept design, 
the vocabulary was limited to a small set of 
phrases, because our design required the whole 
experiment (including training and testing) to be 
done in about one hour. Additional work is need-
ed to test the feasibility of open-vocabulary 
recognition, which will be much more usable for 
people after laryngectomy or with other severe 
voice impairments. 
5 Conclusion and Future Work 
A preliminary, online test of a SSI based on elec-
tromagnetic articulograph was conducted. The 
results were encouraging revealing high phrase 
recognition accuracy and short playback laten-
cies among three participants in a phrase-reading 
task. In addition, a proof-of-concept demo of 
bidirectional conversation using the SSI was 
provided, which shows how the SSI can be used 
for daily communication. 
Future work includes: (1) testing the SSI with 
patients after laryngectomy or with severe voice 
impairment, (2) integrating a phoneme- or word-
level recognition (open-vocabulary) using faster 
machine learning classifiers (e.g., support vector 
machines or hidden Markov models), and (3) 
exploring speaker-independent silent speech 
recognition algorithms by normalizing the articu-
latory movement across speakers (e.g., due to the 
anatomical difference of their tongues). 
Acknowledgements 
This work was in part supported by the Callier 
Excellence in Education Fund, University of 
Texas at Dallas, and grants awarded by the Na-
tional Institutes of Health (R01 DC009890 and 
R01 DC013547). We would like to thank Dr. 
Thomas F. Campbell, Dr. William F. Katz, Dr. 
Gregory S. Lee, Dr. Jennell C. Vick, Lindsey 
Macy, Marcus Jones, Kristin J. Teplansky, Ve-
dad ?Kelly? Fazel, Loren Montgomery, and 
Kameron Johnson for their support or assistance. 
We also thank the anonymous reviewers for their 
comments and suggestions for improving the 
quality of this paper. 43
References 
American Cancer Society. 2013. Cancer Facts and 
Figures 2013. American Cancer Society, Atlanta, 
GA. Retrieved on February 18, 2014. 
Bailey, B. J., Johnson, J. T., and Newlands, S. D. 
2006. Head and Neck Surgery ? Otolaryngology, 
Lippincot, Williams & Wilkins, Philadelphia, PA, 
USA, 4th Ed., 1779-1780. 
Berry, J. 2011. Accuracy of the NDI wave speech 
research system, Journal of Speech, Language, and 
Hearing Research, 54:1295-1301. 
Beukelman, D. R., and Gutmann, M. 1999. Generic 
Message List for AAC users with ALS. 
http://aac.unl.edu/ALS_Message_List1.htm 
Broekx, L., Dreesen, K., Gemmeke, J. F., and Van 
Hamme, H. 2013. Comparing and combining clas-
sifiers for self-taught vocal interfaces, ACL/ISCA 
Workshop on Speech and Language Processing for 
Assistive Technologies, 21-28, 2013. 
Chen, W.-H., Loke, W.-F., Thompson, G., and Jung, 
B. 2012. A 0.5V, 440uW frequency synthesizer for 
implantable medical devices, IEEE Journal of Sol-
id-State Circuits, 47:1896-1907. 
Dam, E. B., Koch, M., and Lillholm, M. 1998. Qua-
ternions, interpolation and animation. Technical 
Report DIKU-TR-98/5, University of Copenhagen. 
Denby, B., Cai, J., Roussel, P., Dreyfus, G., Crevier-
Buchman, L., Pillot-Loiseau, C., Hueber, and T., 
Chollet, G. 2011. Tests of an interactive, phrase-
book-style post-laryngectomy voice-replacement 
system, the 17th International Congress on Pho-
netic Sciences, Hong Kong, China, 572-575. 
Denby, B., Schultz, T., Honda, K., Hueber, T., Gil-
bert, J. M., and Brumberg, J. S. 2010. Silent speech 
interface, Speech Communication, 52:270-287. 
Doi, H., Nakamura, K., Toda, T., Saruwatari, H., Shi-
kano, K. 2010. Esophageal speech enhancement 
based on statistical voice conversion with Gaussian 
mixture models, IEICE Transactions on Infor-
mation and Systems, E93-D, 9:2472-2482.  
Fagan, M. J., Ell, S. R., Gilbert, J. M., Sarrazin, E., 
and Chapman, P. M. 2008. Development of a (si-
lent) speech recognition system for patients follow-
ing laryngectomy, Medical Engineering & Physics, 
30(4):419-425. 
Green, P. D., Khan, Z., Creer, S. M. and Cunningham, 
S. P. 2011. Reconstructing the voice of an individ-
ual following Laryngectomy, Augmentative and Al-
ternative Communication, 27(1):61-66. 
Green, J. R., Wang, J., and Wilson, D. L. 2013. 
SMASH: A tool for articulatory data processing 
and analysis, Proc. Interspeech, 1331-35. 
Green, J. R. and Wang, Y. 2003. Tongue-surface 
movement patterns during speech and swallowing, 
Journal of the Acoustical Society of America, 
113:2820-2833. 
Hofe, R., Ell, S. R., Fagan, M. J., Gilbert, J. M., 
Green, P. D., Moore, R. K., and Rybchenko, S. I. 
2013. Small-vocabulary speech recognition using a 
silent speech interface based on magnetic sensing, 
Speech Communication, 55(1):22-32. 
Hofe, R.,  Ell, S. R., Fagan, M. J., Gilbert, J. M., 
Green, P. D., Moore, R. K., and Rybchenko, S. I. 
2011. Speech Synthesis Parameter Generation for 
the Assistive Silent Speech Interface MVOCA, 
Proc. Interspeech, 3009-3012. 
Huang, X. D., Acero, A., Hon, H.-W., Ju, Y.-C., Liu, 
J., Meredith, S., and Plumpe, M. 1997. Recent Im-
provements on Microsoft?s Trainable Text-to-
Speech System: Whistler, Proc. IEEE Intl. Conf. 
on Acoustics, Speech, and Signal Processing, 959-
962. 
Hueber, T., Benaroya, E.-L., Chollet, G., Denby, B., 
Dreyfus, G., Stone, M. 2010. Development of a si-
lent speech interface driven by ultrasound and opti-
cal images of the tongue and lips, Speech Commu-
nication, 52:288?300. 
Heaton, J. T., Robertson, M., and Griffin, C. 2011. 
Development of a wireless electromyographically 
controlled electrolarynx voice prosthesis, Proc. of 
the 33rd Annual Intl. Conf. of the IEEE Engineer-
ing in Medicine & Biology Society, Boston, MA, 
5352-5355. 
Heracleous, P., and Hagita, N. 2011. Automatic 
recognition of speech without any audio infor-
mation, Proc. IEEE Intl. Conf. on Acoustics, 
Speech, and Signal Processing, 2392-2395. 
Jorgensen, C. and Dusan, S. 2010. Speech interfaces 
based upon surface electromyography, Speech 
Communication, 52:354?366, 2010. 
Katz, W., Bharadwaj, S., Rush, M., and Stettler, M. 
2006. Influences of EMA receiver coils on speech 
production by normal and aphasic/apraxic talkers, 
Journal of Speech, Language, and Hearing Re-
search, 49:645-659. 
Kent, R. D., Adams, S. G., and Tuner, G. S. 1996. 
Models of speech production, in Principles of Ex-
perimental Phonetics, Ed., Lass, N. J., Mosby: St 
Louis, MO. 
King, S., Frankel, J. Livescu, K., McDermott, E., 
Richmond, K., Wester, M. 2007. Speech produc-
tion knowledge in automatic speech recognition, 
Journal of the Acoustical Society of America, 
121(2):723-742. 
Kuipers, J. B. 1999. Quaternions and rotation Se-
quences: a Primer with Applications to Orbits, 
Aerospace, and Virtual Reality, Princeton Univer-
sity Press, Princeton, NJ. 
44
Liu, H., and Ng, M. L. 2007. Electrolarynx in voice 
rehabilitation, Auris Nasus Larynx, 34(3): 327-332. 
Livescu, K., ?etin, O., Hasegawa-Johnson, Mark, 
King, S., Bartels, C., Borges, N., Kantor, A., et al. 
(2007). Articulatory feature-based methods for 
acoustic and audio-visual speech recognition: 
Summary from the 2006 JHU Summer Workshop. 
Proc. IEEE Intl. Conf. on Acoustics, Speech, and 
Signal Processing, 621-624. 
Meier, U., Stiefelhagen, R., Yang, J., and Waibel, A. 
(2000). Towards Unrestricted Lip Reading. Inter-
national Journal of Pattern Recognition and Artifi-
cial Intelligence, 14(5): 571-585. 
Oviatt, S. L. 2003. Multimodal interfaces, in Human?
Computer Interaction Handbook: Fundamentals, 
Evolving Technologies and Emerging Applications, 
Eds. Julie A. Jacko and Andrew Sears (Mahwah, 
NJ:Erlbaum): 286?304.  
Park, H., Kiani, M., Lee, H. M., Kim, J., Block, J., 
Gosselin, B., and Ghovanloo, M. 2012. A wireless 
magnetoresistive sensing system for an intraoral 
tongue-computer interface, IEEE Transactions on 
Biomedical Circuits and Systems, 6(6):571-585. 
Potamianos, G., Neti, C., Cravier, G., Garg, A. and 
Senior, A. W. 2003. Recent advances in the auto-
matic recognition of audio-visual speech, Proc. of  
IEEE, 91(9):1306-1326. 
Rudzicz, F., Hirst, G., Van Lieshout, P. 2012. Vocal 
tract representation in the recognition of cerebral 
palsied speech, Journal of Speech, Language, and 
Hearing Research, 55(4): 1190-1207. 
Toda, T., Nakagiri, M., Shikano, K. 2012. Statistical 
voice conversion techniques for body-conducted 
unvoiced speech enhancement, IEEE Transactions 
on Audio, Speech and Language Processing, 20(9): 
2505-2517. 
Uraga, E. and Hain, T. 2006. Automatic speech 
recognition experiments with articulatory data, 
Proc. Inerspeech, 353-356. 
Wang, J., Samal, A., Green, J. R., and Carrell, T. D. 
2009. Vowel recognition from articulatory position 
time-series data, Proc. IEEE Intl. Conf. on Signal 
Processing and Communication Systems, Omaha, 
NE, 1-6. 
Wang, J., Green, J. R., Samal, A., and Marx, D. B. 
2011. Quantifying articulatory distinctiveness of 
vowels, Proc. Interspeech, Florence, Italy, 277-
280. 
Wang, J., Samal, A., Green, J. R., and Rudzicz, F. 
2012a. Sentence recognition from articulatory 
movements for silent speech interfaces, Proc. IEEE 
Intl. Conf. on Acoustics, Speech, and Signal Pro-
cessing, 4985-4988. 
Wang, J., Samal, A., Green, J. R., and Rudzicz, F. 
2012b. Whole-word recognition from articulatory 
movements for silent speech interfaces, Proc. In-
terspeech, 1327-30. 
Wang, J., Green, J. R., Samal, A. and Yunusova, Y. 
2013a. Articulatory distinctiveness of vowels and 
consonants: A data-driven approach, Journal of 
Speech, Language, and Hearing Research, 56, 
1539-1551. 
Wang, J., Green, J. R., and Samal, A. 2013b. Individ-
ual articulator's contribution to phoneme produc-
tion, Proc. IEEE Intl. Conf. on Acoustics, Speech, 
and Signal Processing, Vancouver, Canada, 7795-
89. 
Wang, J., Balasubramanian, A., Mojica de La Vega, 
L.,  Green, J. R., Samal, A., and Prabhakaran, B. 
2013c. Word recognition from continuous articula-
tory movement time-series data using symbolic 
representations, ACL/ISCA Workshop on Speech 
and Language Processing for Assistive Technolo-
gies, Grenoble, France, 119-127. 
Wang J. 2014. DJ and his friend: A demo of conver-
sation using a real-time silent speech interface 
based on electromagnetic articulograph. [Video]. 
Available: http://www.utdallas.edu/~wangjun/ssi-
demo.html 
Weismer, G. and Bunton, K. (1999). Influences of 
pellet markers on speech production behavior: 
Acoustical and perceptual measures, Journal of the 
Acoustical Society of America, 105: 2882-2891. 
Westbury, J. 1994. X-ray microbeam speech produc-
tion database user?s handbook. University of Wis-
consin-Madison, Madison, Wisconsin. 
45
