Backward Beam Search Algorithm 
for Dependency Analysis of Japanese 
Satoshi  Sek ine 
Computer Science Department 
New York University 
715 Broadway, 7th floor 
New York, NY 10003, USA 
sekine@cs, nyu. edu 
K iyotaka  Uch imoto  H i tosh i  Isahara 
Communications Research Laboratory 
588-2 Iwaoka, Iwaoka-cho, Nishi-ku, 
Kobe, Hyogo, 651-2492, Japan 
\[uchimoto,  i sahara \ ]  @crl. go. j p 
Abst rac t  
Backward beam search tbr dependency analy- 
sis of Japanese is proposed. As dependencies 
normally go fl'om left to right in Japanese, it is 
effective to analyze sentences backwards (from 
right to left). The analysis is based on a statisti- 
cal method and employs a bemn search strategy. 
Based on experiments varying the bemn search 
width, we found that the accuracy is not sen- 
sitivc to the bemn width and even the analysis 
with a beam width of 1 gets ahnost he stone de- 
pendency accuracy as the best accuracy using a 
wider bemn width. This suggested a determin- 
istic algorithm for backwards Japanese depen- 
dency analysis, although still the bemn search 
is eitbctive as the N-best sentence accuracy is 
quite high. The time of analysis is observed to 
be quadratic in the sentence l ngth. 
1 In t roduct ion  
Dependency analysis is regarded as one of the 
standard methods of Japanese syntactic anal- 
ysis. The Japanese dependency structure is 
usually represented by the relationship between 
phrasal units called 'bunsetsu'. A bunsetsu nsu- 
ally contains one or more content words, like a 
noun, verb or adjective, and zero or more func- 
tion words, like a postposition (case marker) 
or verb/noun sul~\[ix. The relation between two 
bunsetsu has a direction front a dependent to 
its head. Figure 1 shows examples of 1)unsetsu 
and dependencies. Each bunsetsu is separated 
by "I"" The  first segment "KARE-HA" consists 
of two words, KARE (He) and HA (subject case 
marker). The  numbers  in the "head" line show 
the head ID of the corresponding bunsetsus. 
Note that the last segment does not have a head, 
and it is the head bunsetsu of the sentence. The 
task of the Japanese dependency analysis is to 
find the head ID for each bunsetsu. 
The analysis proposed in this paper has two 
conceptual steps. In the first step, dependency 
likelihoods are calculated for all possible pairs 
of bunsetsus. In the second step, an optimal de- 
pendency set for the entire sentence is retrieved. 
In this paper, we will mainly discuss the second 
step, a method fbr finding an optimal depen- 
dency set. In practice, the method proposed in 
this paper should be able to be combined with 
any systems which calculate dependency likeli- 
hoods. 
It is said that Japanese dependencies have the 
tbllowing characteristics1: 
(1) Dependencies are directed from left to right 
(2) Dependencies don't cross 
(3) Each seglnent except he rightmost one has 
only one head 
(4) In many cases, the left; context is not nec- 
essary to determine a dependency 
The analysis method proposed in this paper as- 
sumed these characteristics and is designed to 
utilize them. Based on these assumptions, we 
can analyze a sentence backwards (from right 
to left) in an efficient manner. There are two 
merits to this approach. Assume that we are 
analyzing the M-th segment of a sentence of 
length N and analysis has already been done 
for the (M + 1)-th to N-th segments (M < N). 
The first merit is that the head of the depen- 
dency of the M-th segment is one of the seg- 
1Of course, there are several exceptions (S.Shirai, 
1998), but the frequencies of such exceptions are neg- 
ligible compared to the current precision of the system. 
We believe those exceptions have to be treated when the 
problems we are facing at the moment are solved. As- 
sumption (4) has not been discussed very much, but our 
investigation with humans showed that it is true in more 
titan 90?./0 of the cases. 
754 
ID i 2 3 4 5 6 
KARE-HA \[ FUTATABI I PAI-W0 \[ TSUKURI, I KANOJO-NI I 0KUTTA. 
(He-subj) (again) (pie-obj) (made ,) (to her) (present) 
Head 6 4 4 6 6 - 
Translation: He made a pie again and presented it to her. 
Figure 1: Exmnt)le a JaI)anese sentence, 1)unsetsus and det)endencies 
ments between M + 1 and N (because of as- 
sumption 1), which are already analyzed. Be- 
cause of this, we don't have to kce 1) a huge lnlln- 
1)er of possible analyses, i.e. we can avoid some- 
thing like active edges in a chart parser, or mak- 
ing parallel stacks in GLR parsing, as we can 
make a decision at this time. Also, we can use 
the beam search mechanism, 1)y keet)ing only a 
certain nmnl)er of.analysis candidates at (',ach 
segment. The width of the 1)(;am search can 1)c, 
easily tuned and the memory size of the i)ro- 
(:ess is l)rot)ortional to the 1)roduct of the inl)ut 
sentence length and tile boron search width. 
The other merit is that the possit)le heads 
of tile d(~l)en(lency can t)e narrowed down 1)c- 
cause of the ~ssuml)tion of non-crossing det)en- 
(lencies (assumption 2). For exani1)le , if the 
K-th seglll(;nl; dCl)ends on the L-tll segnient 
(A4 < \]~ <~ L), then the \]~J-th segillent (:~l~n't 
depend on any segments between 1~ and L. 
According to our experilnent, this reduced the 
numl)er of heads to consider to less than 50(X~. 
The te(:hnique of backw~trd analysis of 
,lal)anese sentences has 1)een used in rule-based 
methods, for example (Fujita, 1988). How- 
ever, there are several difficulties with rule- 
based methods. First the rules are created by 
hmnans, so it is difficult to have wide cover- 
age and keel) consistency of the rules. Also, it 
is difficult to incorporate a scoring scheme in 
rule-1)ased methods. Many such met;hods used 
hem'isties to make deterministic decisions (and 
backtracking if it; fails in a sear(:hing) rather 
l;han using a scoring scheme. However, the com- 
1)ination of the backward analysis and the sta- 
tistical method has very strong advantages, one 
of which is the 1)emn search. 
2 Stat i s t i c  f ramework  
We. coin|lined tile backward beam search strat- 
egy with a statistical dependency analysis. 'rile 
det~fil of our statistic framework is described 
ill (Uehimoto et al, 1999). There have been 
a lot of prol)OS~fls for statistical analysis, in 
ninny languages, in particular in English and 
Japanese (Magerman, 1995) (Sekine and Grish- 
man, 1995) (Collins, 1997) (I/atnal)arkhi, 1997) 
(K.Shirai et.al, 1998) (Fujio and Matsnlnoto, 
1998) (Itaruno ct.al, 1997)(Ehara, 1998). One 
of the most advance(t systems in English is l)ro- 
posed 1)y I{atnaparkhi. It, uses the Maximum 
Entropy (ME) model and both of the accuracy 
and the speed of the system arc among the best 
ret)ortcd to date. Our  system uses the ME 
model, too. in the ME model, we define a set 
el! \]2~,atlll'eS which arc thought to l)e uscflfl in 
del)ealden(:y analysis, and it: learns the weights 
of the R~atures fl'om training data. Our t~ntttres 
in(:lude part-of-st)eech, inflections, lexical items, 
the existence of a contain or bra(:ket 1)etween 
the segments, and the distmme between the seg- 
ments. Also, confl)inations of those features are 
used as additional fe, atures. The system eal- 
(:ulates the probabilities of dependencies based 
on the model, which is trained using a training 
corpus. The probability of an entire sentence is 
derived from the 1)roduct of tile probal)ilities of 
all the dependencies in the sentence. We choose 
the analysis with the highest probafl)ility to be 
the analysis of the sentence. Although the ac- 
curacy of the analyzer is not the main issue of 
the t)al)er, as any types of models which use de- 
1)endency 1)rol)al)ilities can be iml)lelnented by 
our method, the 1)ertbrmance r t)orted in (Uchi- 
lnoto et al, 1999) is one of the best results re- 
ported by statistic~flly based systems. 
755 
3 A lgor i thm 
In this section, the analysis algorithm will be de- 
scribed. First the algorithm will be illustrated 
using an example, then the algorithm will be 
formally described. The main characteristics of 
the algorithm are the backward analysis and the 
beam search. 
The sentence "KARE-HA FUTATABI PAI-W\[I 
TSUKURI, KANOJ0-NI 0KUTTA. (He made a pie 
again and presented it to her)" is used as an in- 
put. We assume the POS tagging and segmen- 
tation analysis have been done correctly before 
starting the process. The border of each seg- 
ment is shown by "1". In the figures, the head of 
the dependency for each segment is represented 
by the segment number shown at the top of each 
segment. 
<Initial> 
ID 1 2 3 4 5 6 
RARE-HA \[ FUTATABI \[ PAI-WO \[ TSUKURI, \[ KANOJO-NI I OKUTTA. 
(He-subj) (again) (pie-obj) (made ,) (to her) (present) 
................................................................. 
Algorithm 
1. Analyze np to the second segment from the 
end 
The last segment has no dependency, sowe 
don't have to analyze it. The second seg- 
ment fl'om the end always depends on the 
last segment. So the result up to the sec- 
end segment from the end looks like the 
following. 
<Up to the second segment from the end> 
ID 1 2 3 4 5 6 
KARE-HA I FUTATABI \[ PAI-WO I TSUKURI, I KANOJO-NI I OKFITA. 
(He-subj) (again) (pie-obj) (made ,) (to her) (present) 
Cand 6 
................................................................. 
. The third segment from the end 
This segment ("TSUKURI," ) has two depen- 
dency candidates. One is the 5th segment 
("KANOJ0-NI") and the other is the 6th seg- 
ment ("0KUTTA"). Now, we use the proba- 
bilities calculated using the ME model in 
order to assign probabilities to the two can- 
didates (Candl and Cand2 in the following 
figure). Let's assume the probabilities 0.1 
and 0.9 respectively as an example. At the 
tail of each analysis, the total probability 
(the product of the probabilities of all de- 
pendencies) is shown. The candidates are 
sorted by the total probability. 
. 
<Up to the third segment from the end> 
ID 1 2 3 4 5 6 
KARE-HA I FUTATABI I PAI-WO I TSUKURI, I KANOJO-HI I OKUITA. 
(He-subj) (again) (pie-obj) (made ,) (to her) (present) 
Candl 6 0 - (0.9) 
Cand2 5 6 - (0.I) 
................................................................. 
The tburth segment from the end 
For each of the two candidates created at 
the previous tage, the dependencies of the 
fburth segment from the end ("PAI-W0") 
will be analyzed. For Candl, the segment 
can't have a dependency to the fifth seg- 
ment ("KANOJ0-1gI"), because of the non- 
crossing assmnption. So the probabili- 
ties of the dependencies only to the fourth 
(Candi-1) and the sixth (Candi-2) seg- 
ments are calculated. In the example, these 
probabilities are assmned to be 0.6 and 0.4. 
A similar analysis is conducted for Cand2 
(here probabilities are assumed to be 0.5, 
0.1 and 0.4) and three candidates are cre- 
ated (Cand2-1, Cand2-2 and Cand2-3). 
<Up to the fourth segment from the end> 
ID 1 2 3 4 5 6 
RARE-HA I FUTATABI I PAI-WO I TSUKURI, I KANOJO-NI I OKUTTA. 
(He-subj) (again) (pie-obj) (made ,) (to her) (present) 
C~dt - i  4 6 6 - (0.64) 
Candl-2 6 6 6 - (0.30) 
Cand2-1 4 5 6 - (0.05) 
Cand2-2 6 5 6 - (0.04) 
Caud2-3 5 5 6 - (0.01) 
................................................................. 
As tile analysis proceeds, a large number 
(almost L!) of candidates will he created. 
However, by linfiting the number of candi- 
dates at each stage, the total nmnber of 
candidates can be reduced. This is the 
beam search, one of the characteristics of 
the algorithm. By observing the analyses 
in the example, we can e~sily imagine that 
this beam search may not cause a serious 
problem in performance, because the candi- 
dates with low probabilities may be incor- 
rect anyway. For instance, when we set the 
beam search width = 3, then Canal2-2 and 
Cand2-3 in the figure will be discarded at 
this stage, and hence won't be used in the 
following analyses. The relationship of the 
beam search width and the accuracy oh- 
served in our experiments will be reported 
in the next section. 
756 
. Up to the, first segment 
The analyses are conducted in the, same 
way up to the first segment. For example, 
the result of tile analysis tbr the entire sell- 
tence will be shown below. (Appropriate, 
probabilities are used.) 
4.2 Beam search  w idth  and  accuracy  
In this subsection, the relationship between the 
beam width and the accuracy is discussed. In 
principle, the wider the beam search width, the 
more analyses can be retained and the better 
the accuracy cml be expected. However, the re- 
.................................................................. sultis somewhat different froan tile expectation. 
<Up to the first segment> 
ID 1 2 3 4 5 6 
KARE-IIA \[ FUTATABI \[ PAl-W0 { TSUKURI, \[ KANOJ0-NI \[ 0KUTTA. 
(Ile-subj) (again) (pie-obj) (made ,) (to her) (present) 
Candl 6 4 4 6 0 - (0. ii) 
Cand2 4 4 6 6 6 - (0 .09)  
Cand3 6 4 6 5 6 - (0.05) 
................................................................. 
Now, the formal algorithm is described induc- 
tiveJy in Figure 3. The order of the analysis is 
quadratic ill the length of the sentence. 
4 Exper iments  
In this section, experiments and evaluations will 
be reported. We use the Kyoto University Cor- 
pus (version 2) (Kurohashi el.el, 1{)97), a hand 
created Japanese corpus with POS-tags, bun- 
setsu segments and dependency information. 
The sentences in the articles from January 1, 
1994 to January 8, 1994 (7,960 sentences) a.re 
used t'or tim training of the ME model, and 
the sente, nccs in the artMes of Janum'y 9, 1994: 
(1,246 sentences) are used for the ewduation. 
The seid;ences ill the articles of Ja l luary 10, 1994 
are kept for future evaluations. 
4.1 Bas ic  Resu l t  
The evahlation result of our systenl is shown ill 
Table 1. The experiment uses the correctly seg- 
mente(1 and 1)art-oSsl)eet'h tagger1 sentences of 
the Kyoto University corpus. The bealn search 
width is sol; to 1, in other words, the systeln runs 
deterministically. Here, 'dependency accuracy' 
Table 1: lBvaluation 
Dependency accuracy 
Sentence accuracy 
Average analysis time 
87.14% (9814/11263) 
40.60% 0503/1239) 
0.03 sec 
is the percentage of correctly analyzed depen- 
dencies out of all dependencies. 'Sentence accu- 
racy' is the i)ercentage of the sentences in which 
all the dependencies are analyzed correctly. 
Table 2 shows the dependency accuracy and 
sentence accuracy for bemn widths 1 through 
20. The difference is very small, but the best 
Table 2: Relationship between beam width and 
accuracy 
Bemn width Dependency Sentence 
Accuracy Accuracy 
1 
2 
3 
4 
5 
6 
7 
10 
15 
20 
87.14 
87.16 
87.20 
87.1.5 
87.14 
87.16 
87.20 
87.20 
86.21 
86.21 
40.60 
40.76 
40.76 
40.68 
40.60 
40.60 
40.60 
40.60 
40.60 
40.60 
accuracy is obtained when the beain width is 11 
(fbr the dependency accuracy), and 2 and 3 (tbr 
the sentence accuracy). This proves that there 
are cases where the analysis with the highest 
product of probabilities is not correct, but the 
analysis decide(1 at each stage is correct. This is 
a very interesting result of our experiment, and 
it is related to assulnption 4 regarding Japanese 
dependency, lnentioned earlier. 
This suggests that when we analyze a 
.Japanese sentence backwards, we can do it de- 
terministically without great loss of accuracy. 
Table 3 shows where the mlalysis with bemn 
width 1 appears among the analyses with bealn 
width 200. It shows that most deterministic 
analyses appear as tile best analysis in the non- 
deterministic analyses. Also, mnong the deter- 
aninistic analyses which are correct (503 Sell- 
tences), 498 sentences (99.0%) have the same 
mmlysis at the best rank in the 200-beam-width 
analyses. (Followed by 3 sentences at the see-. 
end, 1 sentence ach at the third and fifth rank.) 
It means that in most of the cases, the mmlysis 
757 
<Variable> 
Length: 
W: 
C\[len\]: 
Length of the input sentence in segments 
The beam search width 
Candidate list; C for each segment keeps 
the top W partial analyses from that segment 
to the last segment. 
<Initial Operation> 
The second segment from the end depends on the last segment. 
This analysis is stored in C\[Length-l\]. 
<Inductive Operation> 
Assume the analysis up to the (M+l)-th segment has been finished. 
For each candidate ~c ' in C\[M+i\], do the following operation. 
Compute the possible dependencies of the M-th segment compatible 
with 'c'. For each dependency, create a new candidate Cd~ by 
adding the dependency to 'c'. Calculate the probability of 'd'. 
If C\[M\] has fewer than W entries, add ~d ~ to C\[M\]; 
else if the probability of Cd~ > the probability of the least 
probable entry of C\[M\], replace this entry by 'd'; 
else ignore 'd ' 
When the operation finishes for all candidates in C\[M+i\], 
proceed to the analysis of the (M-l)-th segment. 
Repeat the operation until the first segment is analyzed. 
The best analysis for the sentence is the best candidate in 
C\[1\]. 
Figure 2: Formal Algorithln 
with the highest probability at each stage also 
has the highest probability as a whole. This is 
related to assumption 4. The best analysis with 
the left context and the best analysis without 
tile left context are the same 95% of the time in 
general, and 99% of the time if the analysis is 
correct. These numbers are much higher than 
our human experinmnt mentioned in the ear- 
lier footnote (note that the number here is the 
percentage in terms of sentences, and the num- 
ber in the footnote is the percentage in terms of 
segnmnts.) It means that we may get good ac- 
curacy even without left contexts in analyz ing 
Japanese dependencies. 
4.3 N-Best  accuracy  
As we can generate N-best results, we measured 
N-best sentence accuracy. Figure 3 shows the 
N-best accuracy. N-best accuracy is the per- 
centage of tile sentences which have the correct 
analysis among its top N analyses. By setting 
a large beam width, we can observe N-best ac- 
curacy. The table shows the N-best accuracy 
when the beam width is set, to 20. When we set 
N = 20, 78.5% of the sentences have the cor- 
rect analysis in the top 20 analyses. If we have 
758 
Rank 1 
\]5"e<luc, n y 1175 
(%) (.{},5.8) 
Rank 11 
Frequen(:y 1 
(%) 
Table 3: The rank of the deterministic analysis 
2 3 4: 5 6 7 8 
20 11 8 4 2 1 2 
(1.6) (0.9) (0.6) (0.3) ( I ) .2 ) (0 .1 ) (0 .2 )  
12 j,5 1(i 17 18 
0 o 1 0 J J 
(0.1) ({}.1) (0.1) (0.1) 
9 10 
0 3 
(02) 
19 20 and more 
0 8 
(0.6) 
80 
70 
60 
50 
40 
30 
Sent;once Accuracy 
.53% 
#, 40.60% 
I I I I I I I I I I I - ~ T E  
0 5 10 11.5 20 
N 
Figure 3: N-best sentenc(~ Accuracy 
an ideal sysl;(ml for finding th(~ COl'lCCi; mmlysis 
a,lnOllg? th(;ln~ which maS, 11.%O SCllltl,lll;ic O1" COll- 
l,(;x{; inforlllt~I;io\]\]~ we can have a v(Ty a(:(;Hr~d;e 
an alyzer. 
\~TC Call llltl,l((; two interesting observations 
trom the result. The ac(:uracy of the 1--best 
mmlysis is about 40%, which is more tlm.n half 
of t, he accura(:y of 20-1)est analysis. This shows 
that although the system is not 1)erfb, ct, the 
computation of the 1)rolml)ilities is t)rol)ably 
good in order l;o find the correct mmlysis at the 
top rank. 
The other point is that the accm'aey is sat- 
urated at m'omM 80%. Iml)rovemel,t over 80% 
seelns very dit\[icult even if we use a very large 
bemn width W. (lf we set; W to the number 
of all possible combinations, which means al- 
most L! for sentence length L, we (21M gC{; 100(~0 
N-best accm'aey, lint this is not worth eonsidel'- 
ing.) This suggests tlmt wc h~we missed some- 
thing important. In part;icular, from our inves- 
tigation of the result, we believe that (:oordinate 
structure is one of the most important factors 
to iml)rove the accuracy. This remains one area 
of fllturc work. 
4.4 Speed of  the  analys is  
Based on the f'(n'nml algorithm, the analysis 
tinle can be estimated as t)rot)orl;ional to the 
square, of the inl)ut sentence length. Figure 4: 
shows the relationshi I) between the analysis 
time and the sentence length when wc set the 
beam width to 1. We use a Sun Ultra10 ma- 
chine and the process size is about 8M byte. 
We can see that the actual analyzing time al- 
Analysis time (see.) 
0.3 
0.2 * / "  
0 ~ ~  , r , , , 
0 10 20 30 40 
Sentence length 
\]?igure 4: \]~.elationshi 1) between sentence length 
and mmlyzing time 
most follows the quadratic urve. The ~verage 
amflysis time is 0.03 second and the ~werage sen- 
tence lengl:h is 10 segments. The analysis time 
for the longest sentence (41 segments) is 0.29 
second. W\; have not ot)l;imized the In'ogram in 
terms of speed aim there is room to shrink /;he 
process ize. 
759 
5 Conc lus ion  
In this paper, we proposed astatistical Jttpanese 
dependency analysis method which processes a
sentence backwards. As dependencies normally 
go from left to right in Japanese, it is eflhctive 
to analyze sentences backwards (from right to 
left). In this paper, we proposed a Japanese de- 
pendency analysis which combines a backward 
analysis and a statistical method. It can nat- 
urally incorporate a beam search strategy, an 
effective way of limiting the search space in the 
backwm'd analysis. We observed that the best 
perfbrmances were achieved when the width is 
very small. Actually, 95% of the analyses ob- 
tained with bemn width=l  were the stone as 
the best analyses with beam width=20. The 
analysis time was proportional to the square of 
the sentence length (nmnber of segments), as 
was predicted from the algorithm. The average 
analysis time was 0.03 second (average sentence 
length was 10.0 bunsetsus) and it took 0.29 sec- 
end to analyze the longest sentence, which has 
41 segments. This method can be ~tpplied to 
various languages which haw~ the stone or simi- 
lar characteristics of dependencies, for example 
Koran, Turkish etc. 
References  
Adam Berger and Harry Printz. 1998 : "A 
Comparison of Criteria for Maximum En- 
tropy / Mininmm Divergence Feature Selec- 
tion". Proceedings of the EMNLP-98 97-106 
Michael Collins. 1997 : "Three Generative, 
Lexicalized Models for Statistical Parsing". 
Proceedings of the ACL-97 16-23 
Terumasa Ehara. 1998 : "CMculation of 
Japanese dependency likelihood based on 
Maximmn Entropy model". Proceedings of 
the ANLP, Japan 382-385 
Masakazn t51jio and Yuuji Matsumoto. 1998 
: "Japanese Dependency Structure Analysis 
based on Lexicalized Statistics". Proceedings 
of the EMNLP-98 87-96 
Katsuhiko Fujita. 1988 : "A Trial of determin- 
istic dependency analysis". Proceedings of the 
Japanese Artificial Intelligence Annual meet- 
in9 399-402 
Masahiko Haruno and Satoshi Shirai and Yoshi- 
fumi Ooyama. 1998 : "Using Decision Trees 
to Construct a Practical Parser". Proceedings 
qf the the COLING/A CL-98 505-511 
Sadao Kurohashi and Makoto Nagao. 1994 : 
"KN Parser : ,J~tpanese Dependency/Case 
Structure Analyzer". Proceedings of The In- 
ternational Workshop on Sharable Natural 
Language Resources 48-55 
Sadao Kurohashi and Makoto Nagao. 1997 : 
"Kyoto University text corpus project". Pro- 
ceedings of the ANLP, Japan 115-118 
David Magerman. 1995 : "Statistical Decision- 
Tree Models for Parsiug". Proceedings of the 
ACL-95 276-283 
Adwait I/,atnaparkhi. 1997 : "A Linear Ob- 
served Time Statistical Parser Based on 
Maximum Entropy Models". Proceedings o.f 
EMNLP-97 
Satoshi Sekine and Ralph Grishman. 1995 : "A 
Corpus-based Probabilistic Grammar with 
Only Two Non-terminals". Proceedings of the 
IWPT-95 216-223 
Satoshi Shirai. 1998 : "Heuristics and its lira- 
itation". Jowrnal o\[ the ANLP, Japan Vol.5 
No.l, 1-2 
Kiyoaki Shirai, Kentaro Inui, Takenobu 'lbku- 
naga and Hozunli Tanaka. 1998 : "An Em- 
pirical Evaluation on Statistical Parsing of 
Japanese Sentences Using Lexical Association 
Statistics". P'roceedings ofEMNLP-98 80-86 
Kiyotaka Uchimoto, Satoshi Sekine, Hitoshi 
Isahara. 1999 : "Jat)anese Dependency 
Structm'e Analysis Based on Maximum En- 
tropy Models". P~vceedings o\[ the EACL-99 
pp196-203 
760 
Japanese Dependency Analysis 
using a Determinist ic Finite State Transducer 
Satosh i  Sek ine  
Computer  Science Depar tment  
New York University 
715 Broadway, 7th floor 
New York, NY 10003, USA 
Abst ract  
A deternfinistic finite state transducer is a fast 
device fbr analyzing strings. It takes O(n) time 
to analyze a string of length n. In this 1)al)er, an 
application of this technique to Japanese depen- 
dency analysis will be described. We achieved 
the speed at; a small cost in accuracy. It takes 
about 0.1.7 nfillisecond to analyze one sentence 
(average length is 10 bunsetsu, 1)ased on Pen- 
t:tahiti 650MHz PC, Linux) and we actually 
observed the analysis time to be proportional 
to the sentence length. Thb accuracy is about; 
81% even though very little lexical information 
is used. This is about 17% and 9% better than 
the default and a simple system, respectively. 
We believe the gap between our pertbrmm:ce 
and the best current 1)erforlnm:ce on the stone 
task, about 7%, can be filled by introducing lex- 
ical or sen:antic infornmtion. 
1 I n t roduct ion  
Syntactic analysis or parsing based on tradi- 
tional methods, like Chm't parsing or the GLR 
parsing algorithm, takes cubic or greater tin:e 
in the sentence length to analyze natural lan- 
guage sentences. For Japmmse, Sekine et al 
(Sekine et al, 2000) proposed a Japanese depen- 
dency analyzer which mmlyzes entences in time 
quadratic in the sentence length using a back- 
ward search algorithm. Recently, a mnnber of 
research efforts using Finite State Transducers 
(FST) have been reported. Roche built an En- 
glish syntactic mmlyzer by finding a fixed point 
in a non-deterministic FST (Roche, 1994). But 
it still can't anMyze a sentence in time linear in 
the sentence length. 
In this paper, we will propose a Japanese 
dependency analyzer using a Deterministic Fi- 
nile State Transducer (DFST). The Japmtese 
dependency structure is usually represented by 
relationships between phrasal units called %un- 
sets::'. A lmnsetsu sually contains one or more 
content words, like a noun, verb or adjective, 
and zero or more time:ion words, lil:e a post- 
position (case marker) or verb/noun suffix. A 
dependency between two bunsetsu has a direc- 
tion fl'om a dependent to its head. Figure 1 
shows examples of lmnsetsu and dependencies. 
Each lmnsetsu is separated by "1". The frst 
seglnent "KANOJO-HA" consists of two words, 
KANOJO (She) and HA (subject case, ,narl:er). 
The nmnbers in the "head" line show the head 
ID of corresponding bunsctsus. Note that the 
last segment does not have a head, and it is the 
head lmnsetsu of the sentence. The task of the 
dependency analysis is to find the head ID for 
each lnmsetsu. 
2 Backward  beam search  a lgor i thm 
l?irst, we wouhl lil:e to describe the backwm'd 
beam search algoril:lm: tbr ,\]at)m:ese d pendency 
analysis proposed by Sekinc et sd. (Sekine et al, 
2000). Their experin:ents suggested the method 
proposed in this paper. 
The following characteristics are known ibr 
Japanese dependency. Sekine et al assumed 
these characteristics in order to design the algo- 
rithm 1 
(1) l)ependencies are directed from left to right 
(2) Dependencies don't cross. 
(3) Each bunsetsu except the rightmost one 
has only one head 
(4) Left context is not necessary to deternfine 
a dependency. 
1We know that there are exceptions (Shirai, 1998), 
but the frequencies of such exceptions are very stuN1. 
Characteristic (4) is not well recognized, but based on 
our experiments with humans, it is true more than 90% 
of the time. 
761 
ID 1 2 3 4 5 6 
KANOJ0-HA I KARE-GA I TSUKUTTA I PAI-W0 I YOROKONDE I UKETOTTA. 
(She-subj) (he-subj) (made) (pie-obj) (with pleasure) (received) 
Head 6 3 4 6 6 - 
Translation: She received the pie made by him with pleasure. 
Figure h Example of a Japanese sentence, bunsetsus and dependencies 
Sekine et al proposed a backward beam search 
algorithm (analyze a sentence froln the tail to 
the head, or right to left). Backward search has 
two merits. Let's assume we have analyzed up 
to the/14 + l-st bunsetsu in a sentence of length 
N (0 < M < N). Now we are deciding on the 
head of the M-th bunsetsu. The first merit is 
that the head of the dependency of the /1J-th 
bunsetsu is one of the bunsetsus between t14 + 1 
and N, which are already analyzed. Becmlse of 
this, we don't have to keel) a huge number of 
possible analyses, i.e. we can avoid something 
like active edges in a chart parser, or making 
parallel stacks in GLR parsing, as we can make 
a decision at this time. Also, wc can use the 
bemn search mechanism, t)y keeping a certain 
nmnber of candidates of analyses at each t)un- 
setsu. The width of the beam search can be 
e~Lsily tuned and the memory size of the process 
is proportional to the product of the input sen- 
tence length and the beam search width. The 
other merit is that the 1)ossible heads of the de- 
pendency can be narrowed down because of the 
assmnption of non-crossing dependencies. For 
example, if the K-th bunsetsu depends on the 
L-th bunsetsn (/1J < K < L), then the 21//- 
th bunsetsu can't depend on any bunsetsus be- 
tween I(  and L. According to our experiment, 
this reduced the nmnber of heads to consider to 
less than 50%. 
Uchimoto et al implemented a Japanese de- 
pendency analyzer based on this algorithm in 
combination with the Maxinmm Entropy learn- 
ing method (Uchimoto et al, 2000). The an- 
alyzer demonstrated a high accuracy. Table 1 
shows the relationship between the beam width 
and the accuracy of the system. Froln the table, 
we can see that the accuracy is not sensitive to 
the beam width, and even when the width is 
Beam width Dependency Sentence 
Accuracy Accuracy 
\] 
2 
5 
10 
20 
87.14 
87.16 
87.15 
87.20 
86.21 
40.60 
40.76 
40.60 
40.60 
40.60 
Table h Bemn width and accuracy 
1, which Ineans that at each stage, the depen- 
dency is deterministieally decided, the accuracy 
is almost the same as the best accuracy. This 
means that the left, context of a dependency is
not necessary to decide the dependency, which 
is closely related to characteristic (4). This re- 
sult gives a strong motivation tbr st~rting the 
research reported in this paper. 
3 Idea  
Untbrtunately, the fact that the beam width can 
be 1 by itself can not lead to the analysis in time 
proportional to the input length. Theoretically, 
it takes time quadratic in the length and this is 
observed experimentally as well (see Figure 3 in 
(Sekine et al, 2000)). This is due to the process 
of finding the head among the candidate bun- 
setsns on its right. The time to find the head is 
proportional to the number of candidates, which 
is roughly proportional to the number of bun- 
setsus on its right. The key idea of getting linear 
speed is to make this time a constant. 
Table 2 shows the number of candidate bun- 
setsus and the relative location of the head 
among the candidates (the number of candi- 
date bunsetsus from the bmlsetsu being ana- 
lyzed). The data is derived fl'om 1st to 8th of 
762 
Nmn.of Location of head 
eand. 1 2 3 4 5 6 7 8 9 10 11 12 13 1.4 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
7918 
12824 4948 
10232 3292 
6774 2244 
3692 1294 
1843 614 
848 278 
34O 110 
137 33 
51 11 
17 5 
5 1 
3 () 
() \] 
3126 
1439 1968 
888 660 1114 
398 331. 291 551 
176 133 i33 125264 
83 47 ,78 57 68 121 
28 17 19 21 19 2/t 46 
10 7 3 10 3 6 8 22 
2 0 2 2 /1 2 /, 210  
1 2 0 0 0 0 22  13  
I. 0 0 1 0 0 0 0 0 0 0  
\] 0 0 0 0 0 0 0 0 0 0 2  
rl'able 2: Nmnber of candidate, and location of head 
January part of the Kyoto COl'pus, wlfich is a 
hand created cort)llS tagged.with POS, tmnsetsu 
and dependency relationships (l{urohashi, Na- 
gao, 1997). It is the same 1)ortion used as tile 
training data of the system described late, r. 'J~he 
nmnbc, r of cmMidates is shown vertically and 
the location of the head is shown horizontally. 
l;br exalnple, 2244 in the fom'th row and the 
se(:Olid (-ohmn~ means that there are 2244 bun- 
setsu which have 4 head Calldidates and the 2nd 
fl'om the left is the head of the bunsetsu. 
We can observe that the. data. is very biased. 
The head location is very lilnited. 98.7% of 
instances are covered 1)y the first 4 candidates 
and the last candidate, combined. In the table, 
the data which are not covered by the criteria 
are shown in italics. From this obserw~tion, we 
come to the key idea. We restrict he head can- 
didate locations to consider, mid we remember 
all patterns of categories at the limited loca- 
tions. For each remembered lmttern, we also 
remember where the. head (answer) is. This 
strategy will give us a constant ime select;ion 
of the head. For example, assmne, at ~ certain 
lmnsetsu in the training data, there are five Call- 
didates. Then we. will remember the categories 
of the current lmnsetsu, st\y "A", and five can- 
didates, "B C 1) E F", as wall as the head loca- 
tion, for example "2nd". At the analysis phase, 
if we encounter the same sittmtion, i.e. the same 
category bmlsetsu "A" to be mmlyzed, and the 
same categories of five candidates in the same 
order "13 C D E F", then we can just return tile 
remenfl)ered head location, "2nd". This process 
can be done in constant tilne and eventually, the 
analysis of a sentence can 1)e done in time 1)ro- 
portional to the sentence length. 
For the iml)lementation, we used a DFST in 
which each state represents the patterll of the 
candidates, the input of an edge is the category 
of the. l)llnsetsll 1)eing analyzed and the output 
of a.n edge. is the location of hea(t. 
4: Imp lementat ion  
We still have several 1)rol)lelns whi('h have to 
be solved in order to iint)lement the idea. As- 
SUlning the number of candidates to be 5, the 
t)roblelns are to 
(l) define the c~tegories of head bunsetsu can- 
didates, 
(2) limit the nunlber of patterns (the lmmber 
of states in DFST) to a mallageable range, 
because the Colnbilmtion of five categories 
could t)e huge 
(3) define the categories of intmt bunsetsus, 
(4) deal with unseen events (sparseness l)rob- 
lem). 
hi this section, we l)resent how we imple- 
mented the, system, wlfich at the stone tinle 
763 
shows the solution to the problems. At the end, 
we implemented the systeln in a very small size; 
1200 lines of C progrmn, 188KB data file and 
less than 1MB processing size. 
Structure of DFST 
For the categories of head bunsetsu candi- 
dates, we used JUMAN's POS categories as the 
basis and optimized them using held-out data. 
JUMAN has 42 parts-of-speech (POS) including 
the minor level POSs, and we used the POS of 
the head word of a candidate bunsetsu. We also 
used the information of whether the bunsetsu 
has a colnma or not. The nulnber of categories 
1)ecomes 18 after tuning it using the held-out 
data. 
The input of all edge is the information about 
the 1)unsetsu currently being analyzed. We 
used the inforination of the tail of the bunsetsu 
(mostly function words), and it becomes 40 cat- 
egories according to the stone tuning process. 
The output of an edge is the information of 
which candidate is the head. It is simply a nun> 
ber from 1 to 5. The training data contains 
examples which represent he same state and 
input, but different output. This is due to the 
rough definition of the categories or inherent im- 
possibility of to disambiguating the dependency 
relationship fi'om the given infbrmation only. In 
such cases, we pick the one which is the most 
frequent as the answer in that situation. We will 
discuss the problems caused by this strategy. 
Data size 
Based on the design described above, the 
number of states (or number of patterns) is 
1,889,568 (18 ~) and the number of edges is 
75,582,720 as each state has 40 outgoing edges. 
If we implement it as is, we may need several 
hundred megabytes of data. In order to keep 
it fast, all the data should be in memory, and 
the current memory requirement is too large to 
implement. 
To solve the problem, two ideas are employed. 
First, the states are represented by the combi- 
nation of the candidate categories, i.e. states 
can be located by the 5 candidate categories. 
So, once it transfers from one state to another, 
the new state can be identified from the previ- 
ous state, input bunsetsu and the output of the 
edge. Using this operation, the new state does 
not need to be remembered for each edge and 
this leads to a large data reduction. Second, we 
introduced the default dependency relationship. 
In the Japanese dependency relationship, 64% 
of bunsetsu debend on the next bunsetsu. So 
if this is the output, we don't record the edge 
as it is the default. In other words, if there is 
no edge in~brmation for a particular input at a 
particular state, the outtmt is the next bunsetsu 
(or 1). This gave unexpectedly a large benefit. 
For unseen events, it is reasonable to guess that 
the bunsetsu depends the next bunsetsu. Be- 
cause of the default, we don't have to keep such 
information. Actually the majority (more than 
99%) of states and edges are unseen events, so 
the default strategy helps a lot to reduce the 
data size. 
By the combination of the two ideas, there 
could be a state whidl has absolutely no in- 
formation. If this kind of state is reached in 
the DFST, the output for any input is the next 
bunsetsu and the next state can be calculated 
froln the information you have. In fact, we 
have a lot of states with absolutely no infor- 
mation. Before implementing the supplemen- 
tation, explained in the next section, the num- 
ber of recorded states is only 1,006 and there 
are 1,554 edges (among 1,889,568 possible states 
and 75,582,720 possible edges). After imple- 
menting the supplementation, westill have only 
10,457 states and 31.,316 edges. The data sizes 
(file sizes) are about 15KB and 188KB, respec- 
tively. 
Sparseness problem 
Tile amount of training data is about 8,000 
sentences. As the average number of bunsetsu 
in a sentence is 10, there are about 72,000 data 
points in the training data. This number is very 
much smaller than the nmnber of possible states 
(1,889,568), and it seems obvious that we will 
have a sparseness problem 2. 
In order to supplement the unseen events, 
we use the system developed by Udfimoto 
et.al (Uchimoto et al, 2000). A large cor- 
pus is parsed by the analyzer, and the results 
2However, we can make the system by using the de- 
fault strategy mM surprisingly the accuracy of the sys- 
tem is not so bad. This will be reported in the Experi- 
ment section 
764 
are added to the training cori)us. In prac- 
tice, we parsed two years of lmwspaper articles 
(2,536,229 sentences ofMainichi Shinbun 94 and 
95, excluding Jalmary 1-10, 95, as these are used 
in the Kyoto corpus). 
5 Exper iment  
In this section, we will report the experilnent. 
~?\r(, used the Kyoto corl)us (ve.rsion 2). The 
training is done using January 1-8, 95 (7,960 
sentences), the test is done using Jmmary 9, 
95 (1,246 sentences) mid the parameter tuning 
is done using Jmmary 10, 95 (1,519 sentences; 
held-out data). The input sentences to the sys- 
rein are morl)hologically analyzed and bunsetsu 
nre detected correctly. 
Dependency Accuracy 
Table 3 shows the accuracy of the systems. 
The 'dependency accuracy' metals the percent- 
age of the tmnsetsus whose head is correctly 
mmlyzed. The bmlsetsus are all but the last 
bunsetsu of the sentence, as the last 1)unsetsu 
has no head. The 'default nlethod' is the sys- 
rein in which the all bunsetsus are supt)osed 
to dei)end on the next bunsetsu. ~t'he 'base- 
line reel;hod' is a quite siml)le rule-based sys- 
toni which was created by hand an(t has a\])out 
30 rules. 'l'he details of the system are rel)orted 
in (Uchimoto et al, 1999). The 'Uchimoto's 
system' is tit(; system rel)orted in (Uchimoto ct 
al., 2000). They used the same training data 
m~d test (lata. The del)endency accuracies of 
System Det)en(tency 
Accuracy 
Our system (with supp.) 81.231% 
Our systenl (without sut)p. ) 77.972 % 
Default method (i4.14: % 
Baseline method 72.57 % 
Uchimoto's ystem 87.93 % 
Table 3: Dependency Accuracy 
our systems are 81% with supl)lenlentation a d 
78% without supt)lementation. The result is 
about 17% and 9% better than tile default mid 
the baseline nlethods respectively. Compared 
with Uchimoto's ystem, we are about 7% be- 
hind. But as Uchimoto's ystem used about 
40,()00 features including lexical features, and 
they also introduced combined features (up to 
5), it is natural that our system, which uses only 
18 categories and only combinations of two cat- 
cgories, has less accuracy a.
Analysis speed 
The main objective of the system is speed. 
Table 4 shows the analysis speed on three di f  
fcrent platforms. On the fastest machine, it an- 
alyzes a sentence in 0.17 millisecond. Tat)le 5 
shows a comlmrisou of the analysis speed of 
three difl'erent systems on SPARC-I. Our system 
runs about 100 times faster than Uchimoto's 
system and KNP(Kurohashi, Nagao, 1994). 
Platform Analysis time 
(millisec./sent.) 
PentiulnIII 650MHz 0.17 
SmlEnterprise, 400MHz 0.29 
Ultra SPARC-I, \]70MHz 1.{)3 
Table 4: Analysis Speed 
System Analysis time 
(millisec./sent.) 
Our system 1.03 
Uchimoto's ystem 170 
KNP 86 
Tal)le 5: Coml)arison of Analysis Speed 
Figure 2 shows tile relationship between tile 
sentence length and tile analysis tinle. We used 
tile slowest nlachine (Ultra SPARC-I, 170MHz) 
in order to minimize obserw~tional errors. We 
cml clearly observe that the anMysis tinle is pro- 
portional to the sentence length, as was pre- 
dicted by the algorithm. 
The speed of tile training is slightly slower 
l:han that of tile analysis. The training on the 
smaller training data (about 8000 sentences) 
t;akes M)out 10 seconds on Ultra SPAR.C-I. 
aHowever, our system uses context information offly( ~, 
bunsctsus, which is not used in Uchimoto's system. 
765 
1.5 
1.0 
0.5 
0.0 
Analysis time 
(millisec.) 
1 I 
0 5 I'0 15 2'0 
Sentence length (Num.of bunsetsu) 
Figure 2: Analysis time and Sentence length 
6 Discuss ion 
6.1 The restriction 
People may strongly argue that the main I)rol> 
lenl of the system is the restriction of the head 
location. We eonlpletely agree. We restrict the 
candidate to five bunsetsus, as we described ear- 
lie:', and we thcoretically ignored 1.3% of accu- 
racy. Obviously, this restriction can be loosened 
by widelfing the range of the candidates. For ex- 
ample, 1.3% will be reduced to 0.5% if we take 
6 instead of 5 candidates, hnplenlentation of 
this change is easy. However, the. problen: lies 
with the sparseness. As shown in Table 2, there 
are fewer examples with a large number of bml- 
setsu candidates. For examI)le, there are only 4 
instances which haw~ 14 candidates. It may be 
impossible to accumulate nough training ex- 
mnples of these kinds, even if we use a lot of 
untagged text fbr the supplementation. In such 
cases, we believe, we should have other kinds 
of remedies. One of them is a generalization of 
a large number candidates to a smaller number 
candidates by deleting nniniportant bunsetsus. 
This remains for fl:ture research. 
We can easily imagine that other systems may 
not analyze accurately the cases where the cor- 
rect head is far fl'om the bunsetsu. For example, 
Uchimoto's ystem achieves an accuracy of 41% 
in the cases shown in italics in Table 2, which is 
much lower than 88%, the system's ow~rall ac- 
curacy. So, relative to the Uchimoto's system, 
the restriction caused a loss of accuracy of only 
0.5% (1.3% x 41%) instead of 1.3%. 
6.2 Accuracy 
There are several things to do in order to achieve 
better accuracy. One of the major things is to 
use the information which has not been used, 
but is known to be useful to decide dependency 
relationships. Because the accuracy of the sys- 
tem against the training data is only 81.653% 
(without supplementation), it, is clear that we 
miss some important information, 
We believe the lexical relationships in verb 
frmne element preference (VFEP) is one of the 
most important types of information. Analyz- 
ing the data, we can find evidence that such 
information is crucial. For exmnplc, there are 
236 exmnples in the training corpus where there 
are 4 head candidates and they are bunsetsus 
whose heads are noun, verb, noun and verb, and 
the current bunsetsu ends with a kaku- j  oshi ,  
a major particle. Out of the 236 exami)les, the 
nulnber of cases where the first, second, third 
and last candidate is the head are 60, 142, 3 and 
31, respectively. The answer is widely spread, 
and as the current system takes the most fl'e- 
quent head as the answer, 94 eases out of 236 
(40%) are ignored. This is due to the level of 
categorization which uses only POS informa- 
tion. Looking at the example sentences in this 
ease, we can observe that the VFEP could solve 
the problenl. 
It is not straightfbrward to add such lexieal 
information to the current franlework. If such 
information is incorporated into the state inibr- 
marion, the nmnber of states will become enor- 
mous. We can alternatively take an approach 
which was taken by augmented CFG. In this ap- 
proach, the lexical infbrmation will be referred 
to only when needed. Such a process m~\y slow 
down the analyzer, but since the nuinber of in- 
vocation of the process needed in a :~entence 
may be proportional to the sentence length, we 
believe the entire process may still operate in 
tiine proportional to the sentence length. 
7 Conc lus ion 
We proposed a Japmiese dependency analy- 
sis using a Deterministic Finite State Trans- 
ducer (DFST). The system analyzes a sentence 
with fairly good accuracy of about 81% in 0.17 
millisecond on average. It can be at)plied to 
a wide range of NLP applications, including 
real time Machine Translation, Information Ex- 
766 
traction and Speech Recognition. There are 
a number of eflbrts to improve the accuracy 
of Japanese dependency analysis (Haruno et.al, 
1997) (Fujio, Matsumoto, 1998) (Kanaymna 
et.al, 2000). In particular, it is interesting to see 
that Kanwmna's method uses a similar idea, 
limiting the head candidates, in order to im- 
prove the accuracy. We are plmming to incor- 
porate the fruit of these works to improve our 
accuracy. We believe our resem'ch is comple- 
mentary to these research. We also believe the 
method proposed in this l)aper ln~\y be applic> 
ble to other languages which share the chaiac- 
teristics explained earlier, for example, Korean, 
~lhrkish and others. 
8 Acknowledgment  
The eXl)eriments in this paper were conducted 
using the Kyoto corpus. We really appreciate 
their effort alld their kindness in making it pub- 
lit: domain. We thank l)r. Kuroluu~hi and other 
people who worked in the project. Mr.Uchimoto 
helped us to prepare the d~l;a and the experi- 
ment, mid we would like to thank him. 
References  
Masakazu Fujio, Yuuji Matsumoto. 1998 : 
"JaI)an(;se Dctmndency Strtlctlll'( ~,Analysis 
base(l on l,cxicalizcd Statistics", P'rocccding.s" 
o.f Th, i'rd Co~:/'('.rcnce onEMNLP 1)t)87-9(i 
I:liroshi Kanayama, Kcntaro '\]brisawa, Yutaka 
Mitsuishi, .Jml'ichi Tsujii. 2000 : '"A hybrid 
,lalmnese Parser with Hand-crafl;ed Grmnmar 
and Statistics", Proceedings o/" COLING-O0, 
this pr'oceedings 
Masahiko Haruno, Satoshi Shirai, Yoshiflmfi 
Ooymna. 1998 : "Using Decision ~l~'ees to 
Construct a Practical Parser", Procccding.s o\[ 
COLING-A CL-08 pp505-511 
Sadao Kurohashi, Makoto Nagao. 1994 : "KN 
Parser : Japanese Dependency/Case Struc- 
ture Analyzer", Pwcccdin.qs of the Wor'k- 
shop on Sharable Natural Language l~.c,sourccs 
pp48-55 
Sadao Kurohashi, Makoto Nagao. 1.997 : "Ky- 
oto University text corpus project", Proceed- 
ings of the ANLP-9Z Japan pp115-118 
Emmmmel R.oche. 1994 : "Two Parsing Algo- 
rithms by Means of Finite State Transduc- 
ers", Proceedings of COLING-9/j pp431-435, 
Satoshi Sekine, Kiyotaka Ucllimoto, Hitoshi Isa- 
hara. 2000 : "Statistical Dependency Anal- 
ysis using Backwm'd Beam Search", Proceed- 
ings of COLING-O0, this proceedings 
Satoshi Shirai. 19!18 : "Heuristics and its lim- 
itation", ,JouTvtal of the ANLP, Japan Vol.5 
No.l, t)1)1-2 
Kiyotaka Uchimoto, Satoshi Sekine, Hitoshi Isa- 
hara. 1999 : "Jal)anese Dependency Struc- 
tllre Analysis Based on Maximum Entropy 
Models", ,\]our'nal of Information Processing 
Society of Japan Vol.40, No.9, pp3397-3407 
Kiyotaka Uchimoto, Masaki Mm'ata, Satoshi 
Sekine, Hitoshi Isahara. 2000 : "Dependency 
Model Using l?osterior Context", Pr'oceedings 
of the IIYPT-O0 
767 
Word Order  Acqu is i t ion  f rom Corpora  
Kiyotaka Uchimoto I, Masaki Murata t, Qing Ma*, 
Satoshi Sekine*, and Hitoshi Isahara t 
tCommunications Research Laboratory 
Ministry of Posts and Telecommunications 
588-2, Iwaoka, Iwaoka-cho, Nishi-ku 
Kobe, Hyogo, 651-2492, Japan 
\[uchimoto ,murata, qma, isahara\] @crl. go. jp 
*New York University 
715 Broadway, 7th floor 
New York, NY 10003, USA 
sekYne~cs, nyu. edu 
Abstract 
In this paper we describe a method of acquiring word 
order fl'om corpora. Word order is defined as the or- 
der of modifiers, or the order of phrasal milts called 
'bunsetsu' which depend on the stone modifiee. The 
method uses a model which automatically discovers 
what the tendency of the word order in Japanese is 
by using various kinds of information in and around 
the target bunsetsus. This model shows us to what 
extent each piece of information contributes to de- 
ciding the word order mid which word order tends to 
be selected when several kinds of information con- 
flict. The contribution rate of each piece of informa- 
tion in deciding word order is eiIiciently learned by a 
model within a maximum entropy framework. The 
performance of this traiimd model can be ewfluated 
by checking how many instances of word order st- 
letted by the model agree with those in the original 
text. In this paper, we show t, hat even a raw cor- 
pits that has not been tagged can be used to train 
the model, if it is first analyzed by a parser. This 
is possible because the word order of the text in the 
corpus is correct. 
1 Introduction 
Although it is said tha~ word order is free in 
Japanese, linguistic research shows that there art 
certain word order tendencies - -  adverbs of time, for 
example, tend to t)recede subjects, mM bunsetsus in 
a sentence that are modified by a long modifier tend 
to precede other bunsetsus in the sentence. Knowl- 
edge of these word order tendencies would be useful 
in analyzing and generating sentences. 
Ii1 this paper we define word order as the order of 
nrodifiers, or the order of bunsetsns wlfich depend on 
the same modifiee. There arc several elements which 
contribute to deciding the word order, and they are 
summarized by Saeki (Saeki, 1.998) as basic condi- 
tions that govern word order. When interpreting 
these conditions according to our definition, we era: 
summarize them ,~ tbllows. 
Component la l  eondit lons 
? A bunsetsu having a deep dependency tends 
to precede a bunsetsu having a shallow depen- 
dency. 
When there is a long distance between amodifier 
and its modifiee, the modifier is defined as a bun- 
setsu having a deep dependency. For example, 
the usual word order of modifiers in Japanese 
is tlm following: a bunsetsu which contains an 
interjection, a bunsetsu which contains an ad- 
verb of time, a bunsetsu which contains a sub- 
ject, and a bunsetsu which contains an object. 
Here, the bunsetsu containing an adverb of time 
is defined as a bunsetsu having deeper depen- 
dency than the one containing a subject. We 
call the concept representing the distance be- 
tween a modifier and its modifiee the depth of 
dependency. 
A bunsetsu having wide dependency tends to 
precede a bunsetsu having narrow dependency. 
A bunsetsu having wide dependency is defined 
as a bunsetsu which does not rigidly restrict its 
modifiee. For example, the bunsetsu "~btqlo_c 
(to Tokyo)" often depends on a bunsetsu whicll 
contains a verb of motion such as "ihu (go)" 
while the bunsetsu "watashi_.qa (I)" can depend 
on a bunsetsu which contains any kind of verb. 
Here, the bunsetsu "watashi_ga (I)" is defined as 
a bunsetsu having wider dependency than 1;11o 
tmnsetsu ':Tok~./o_c (to Tokyo)." We call the 
concept of how rigidly a modifier restricts its 
modifiee the width of dependency. 
Syntact i c  condit ions 
? A bunsetsu modified by a long inodifier ton(Is to 
precede a bunsetsu modified by a short lnodifier. 
A long modifier is a long clause, or a clause that 
contains many bunsetsus. 
? A bunsel, su containing a reference pronoun tends 
to precede other bunsetsus in the sentence. 
? A bunsetsu containing a repetition word tends 
to precede other bunsetsus in the sentence. 
A repetition word is a word referring to a word 
in a preceding sentence. For example, Taro 
mid Hanako in the following text are repetition 
words. "Taro and Hanako love each other. Taro 
is a civil servant and Hanako is a doctor." 
? A bunsetsu containing the case marker "wa" 
tends to precede other bunsetsus in the sentence. 
A mnnber of studies have tried to discover the rela- 
tionship between these conditions and word order in 
871 
Japanese. Tokunaga and Tanalca proposed a model 
for estimating JaI)anese word order based on a dic- 
tionary. They focused on the width of dependency 
(Tokunaga and Tanal~a, 1991). Under their model, 
however, word order is restricted to the order of case 
elements of verbs, and it is pointed out that the 
model can deal with only the obligatory case and 
it cmmot deal with contextual information (Saeki, 
1998). An N-gram model fbr detecting word order 
has also been proposed by Maruyama (Maruyama, 
1994), but under this model word order is defined as 
the order of morpheines in a sentence. The problem 
setting of Maruyama's study thus differed fl'om ours, 
and the conditions listed above were not taken into 
account in that study. As for estimating word or- 
der in English, a statistical model has been proposed 
by Shaw and Hatzivassiloglou (Shaw and Hatzivas- 
siloglou, 1999). Under their model, however, word 
order is restricted to the order of premodifiers or 
modifiers depending on nouns, and the model does 
not simultaneously take into account many elements 
that contribute to determining word order. It would 
be difficult to apply the model to estimating word 
order in Japanese when considering the many condi- 
tions as listed above. 
In this paper, we propose a method for acquiring 
from corpora the relationship between the conditions 
itemized above and word order in Japanese. The 
method uses a model which automatically discovers 
what the tendency of the word order in Japanese is 
by using various kinds of information in and around 
the target bunsetsus. This model shows us to what 
extent each piece of information contributes to decid- 
ing the word order and which word order tends to be 
selected when several kinds of information conflict. 
The contribution rate of each piece of information in 
deciding word order is efficiently learned by a model 
within a maximum entrot)y (M.E.) framework. The 
performance of the trained model can be evaluated 
according to how many instances of word order se- 
lected by the model agree with those in the original 
text. Because the word order of the text in the corpus 
is correct, the model can be trained using a raw co> 
pus instead of a tagged corpus, if it is first analyzed 
by a parser. In this paper, we show experimental re- 
sults demonstrating that this is indeed possible even 
when the parser is only 90% accurate. 
This work is a part of the corpus based text gen- 
eration. A whole sentence can be generated in the 
natural order by using the trained model, given de- 
pendencies between bunsetsus. It could be helpful 
for several applications uch as refinement support 
and text generation in machine translation. 
2 Word  Order  Acqu is i t ion  and 
Es t imat ion  
2.1 Word  Order  Mode l  
This section describes a model which estimates the 
likelihood of the appropriate word order. We call 
this model a word order model, and we implemented 
it within an M.E. framework. 
Given tokenization of a test corpus, the problem 
of word order estimation in Japanese can be reduced 
to the problem of assigning one of two tags to each 
relationship between two modifiers. A relationship 
could be tagged with "1" to indicate that the order 
of the two modifiers is appropriate, or with "0" to in- 
dicate that it is not. Ordering all modifiers so as to 
assign the tag "1" to all relationshit)s indicates that 
all modifiers art  in the appropriate word order. The 
two tags form the space of "futures" in the M.E. 
formulation of our estimation problem of word or- 
der between two modifiers. The M.E. model, as well 
as other similar models allows the computation of 
P(flh) for any f in the space of possible futures, F, 
and for every h in the space of possible histories, H. 
A "history" in maximum entropy is all of the condi- 
tioning data that enable us to make a decision in the 
space of futures. In the estimation problem of word 
order, we could reformulate this in terms of finding 
the probability of f associated with the relationship 
at index t in the test cortms as: 
P(flht) = P( f l  hfformation derivable 
from the test corpus 
related to relationship t) 
The computation of P(flh) in any M.E. models is 
dependent on a set of "features" which should be 
hdpful in making a prediction about the flmlre. Like 
most current M.E. models in computational linguis- 
tics, our model is restricted to features which are 
binary functions of the history and future. For in- 
stance, one of our features is 
1. : if has(h,x) = true, 
x = "Mdfl'l - Head-  
g(h,f) = POS(Major) : verb" (1) 
&f= l  
0 : otherwise. 
Here "has(h,x)" is a binary flmction which returns 
true if the history h has feature z. We focus on the 
attributes of a bunsetsu itself and on the features 
occurring between bunsetsus. 
Given a set of features and some training data, 
the maximum entropy estimation process produces a
model ill which every feature .qi has associated with it 
a parameter ai. This allows us to compute the con- 
ditional probability as follows (Berger et al, 1996): 
ag~ (h .f) 
P( / Ih ) -  1L ' (2) 
Z (h) 
ct i . (3) 
Y i 
The maximum entropy estimation technique guaran- 
tees that for every feature gi, the expected value of 
gi according to the M.E. model will equal the empir- 
ical expectation of gi in the training corpus. In other 
words: 
P(h,/). Mh, f) 
h,f 
= (4) 
h / 
Here /5 is an empirical probability and l~ Ie  is the 
872 
Table l: Example of estimating the probabilities of word orders. 
? ) I"I~{H (yesterday) / ?=x~ (tennis) / ~(f~l~l$ (Taro) / bLo ( l ) I t~yed. ) "  /I~,,'~<~,~ x llail.-)'nxt, x 1.7:-.x,~j~m ::: 0.6 x 0.8 x 0.3 0.144 ) \["NI!Iii:k (Taro) )  I~1!11 (yesterday) / ?:LX ~ (tennis) / bt:? (played.)" IP:mi~ ~H x I~*H,)-: x,,< x t?r,l~.?::x,: := 0.4 x 0.8 ? 0.7 0.224 
\[")kl'!l~l:~ (Taro) / -Y : :x '~ (tennis) / 'a~H (yesterday) / b?:o (plowed.)" I/)~:,,~ll x tg.=.x'~.,~H x P,k~a*~L-)::?~ =: 0.4 x 0.2 x 0 .7  0.05(i 
1"9:-':;? ~ (tennis) / II~{H (yesl.erd;ty) / >kfl\[~l~: ( l'&l'O) / \[,~:.? (played.)" \[13*lll,2<I,u:l. x l: ':.xt, jall x l!).:.x.~,.j<r~m :: 0.6 x 0.2 x 0.3 0.036 
\["5:=x ~ (tennis) / )kf~lll:t (Taro) / I~I~Ft (yesterday) / blz0 (played.)" ~ ? H  x l~;.:.xr, ~,~ZI? x l?y:.x~l~,~ =: 0.4 x 0.2 x 0.3 0.024 
prol)ability assigned by the M.E. model. 
We detine a word order model as a model which 
learns the at)l)ropriate order of each pair of nlodifiers 
which depend on the same modifiee. 'l'his model is 
derived from Eq. (2) as follows? Assmne that there 
are two bunsetsus 231 and 23~ which depend on the 
buusetsu B and that  It is the information derivable 
from the test corpus. \]?lie probability that "B\] B2" is 
the at)propriate order is given by the following equa- 
tion: 
\]iik: :1 ffi( l  ,b) (~'i ,i 
where .qi(1 < i < k) is a fl,atm'e and "1" indicates 
that the order is at)propriate. The terms cq,i and 
(~0,i are estimated fl'oln a eorl)us which is nlorpho- 
logically and syntactically analyzed. When there are 
three or more b\]msetsus that det)end on tit('. S}tlne 
? moditiee, the probability is estimated as follows: I or 
~'t bunsetsus 231, 232, . . . ,  23n which depend on the 
bmtsetsu B and for the information h derivaMe from 
the test corpus, the prot)ability t;hat "23\] 23~ . . .  23," 
is the at)propriate order, or P( l lh) ,  is represented ass 
the probability that every two bunsetsus "Bi ~i-Fj 
(1 _< i < n - 1,1 < j < 'n - i)" are the appropri~ 
ate. order, or  P({14~i, i_ l . j  = l \ [ l<  i <. n - -  1, l :< j < 
n - - i} lh ) ,  ilere "I4Li+j -- l" represents that ".l~i 
23i-Fj" is the appropriate order. Let us assume that 
every 14Q,~:+j is independent each other. Then 1)(1 Ih,) 
is derived as follows: 
2 ' ( l ib )  = \]1\]. i , . -  \ ] ,  
1 < .i _< , , , -  i}lh) 
n- - |  n - i  
i -- I  j --1 
= l l  1I j), 
i= l  j= l  
where \[Si,i+ j is the information derivable when fb- 
cusing on the bunsetsu 13 m~d its modifiers 13i and 
Bi+j. 
For example, in the sentence "I~ U (kinou, yester- 
day) / :kflll ~ (Taro_wa, Taro) / -P ~ x ~ (tcnnis_wo, 
tennis) / b t:o (sita., l)layed.)," where a "/" repre- 
sents a bunsetsu boundary, there are three bunset- 
sus that depend on the verb "b  ~: (sita)." We train 
a word order inodel under the assmnl)tion that the 
orders of three t)airs of modifiers -"I~l U" and "~ 
f$1.~," "Net\] " and "?  7-:7, ~ ," and ":kl~l*l~" and "5: 
m :7, ~"  .... are al)ttropriate. We use various ldnds of 
intormation in and around the target bunsetsus as 
features. For example, the information or the feature 
that a noun of time i)recedes a t)rot)er noun is deriv- 
able fl'om the order "IP} H (yesterday) / Y;fll~ I~ (Taro) 
/ b 1=o (pl~\yed.)," and the feature that a case fol- 
lowed by a case marker "w?' precedes a case followed 
by a caqe marker "wo" is derivable from the order ":~ 
fll~ It. ( Taro_wa, Taro) / ? ~ 7. ?k (tennis_wo, tennis) / 
b 2C_o (sita., t)layed.)." 
2.2 Word Order  Es t imat ion  
This section describes the algorithm of estimating 
the word order by using a trained word order model. 
The word order estimation is defined as deciding 
the order of ntoditiers or bunsetsus which depend 
on the same modifiee. The input of this task con- 
sists of modifiers and informat, ion necessary to know 
whether or not features are found. The output is 
the order of the inodifiers. We assume that lexical 
selection in each bunsetsu is already done and all 
del)endencies in a sentence are found. The informa- 
tion necessary to know whether or not features are 
found is morphological, syntactic, semmltic, and ('on- 
textual information, and the locations of bunsetsu 
bonndaries. The features used in our ext)eriments 
are described in Section 3. 
Word order is estimated in the following steps. 
Procedures 
1. All possible orders of modifiers are found. 
2. For each, the probability that it is apt)ropriate 
is estimated by a word order model, or Eq. (6). 
3. The order with the highest probability of 1)eing 
approl)riate is selected. 
l)br example, given the sentence "1~ U (kinou, 
yesterday) /:kfilIl~ (Taro_wa, Taro) /? : :x  ~ (tcn- 
nis_wo, temfis) / b t:o (sita., played.)," tim modi- 
tiers of a verb "b  ?:_ (played)" are three tmnsetsus, 
"l~ U (yesterday)," :k~/i ~ (Taro)," "?  = x ~ (ten- 
nis)." Their apt)ropriate order is estimated in the 
following steps. 
1. The probabilities that the orders of the three 
pairs of modifiers "N- LI " and ":is: BII l~ ," "I~ 
U" and "?~:7 ,~,"  and "~fllIl?" and "?  
c .x  ~" are appropriate are estimated. As- 
sume, for example, ~-H ,;k~l~ta, PrI~ It ,? :-~. ~, and 
P;kfzlIla,~ ca  ~ are respectively 0.6, 0.8, and 0.7. 
2. As shown in Table 1, probabilities are estimated 
for all six possible orders. The order "I~ U / :k 
fill IS / -7- ~- y. ~ / b \]Co ," which has the highest 
probability, is selected as the most apt)ropriate 
order. 
2.3 Per fo rmance  Eva luat ion  
The pcrformancc of a word order model can be eval- 
uated in the following way. First, extract from a 
test corpus bunsetsus having two or more modifiers. 
Then, using those 1)unsetsus and their modifiers as 
873 
Data 
~Jtlnsetsll lJtlnsetstl nHIYiber Of babel 
nlllllber modifier 
0 1 P 
1 5 
2 3 
3 4 
4 5 P 
5 
Table 2: Example of modifiers extracted fl'om a corpus. 
Modifiers (Bunsetsu number) 
Strings in a bunsetsu 
>kflli ~ ( Taro_to, Taro and) 
~Y'{a ( lIanako_to, llanako) 
-Y- = x q~ (tennis_no, tennis) 
~lc .  (sial_hi, tournament) 
lll'C, (dete,, participate,) 
~@ b t=, (yusyo_sita., won.) 
Moditiers whose modiliee is the bunsetsu 
in the left column. 
~,~ a (0) 
?=x0~ (2) 
~a  (0) ~-r-~a 0) '~ :  (a) 
~e  (o) ~?-~* (1) If'~ (4) 
input, estimate the orders of the modifiers as de- 
scribed in Section 2.2. The percentage of the modi- 
flees whose modifiers' word order agrees with that in 
the original text then gives what we call the agree- 
ment rate. It is a measure of how close the word 
order estimated by the model is to the actual word 
order in the training corpus. 
We use the following two measurements to calcu- 
late the agreement rate. 
Pa i r  of  modi f ie rs  The first measurement is the 
percentage of the pairs of modifiers whose word 
order agrees with that in the test corpus. For 
exmnple, given the sentence in a test corpus "N 
kl (kinou, yesterday) / ~t I la  (Taro_wa, Taro) 
/ -7- = 2` ~2 (tennis_wo, tennis) / t. ~:o (sita., 
played.)," if the word order estimated by the 
model is "~ H (yesterday) / -7" -- 2. ~ (tennis) / 
~1~ ~:~ (Taro) / b too (played.)," then the or- 
ders of the pairs of modifiers in the original sen- 
tence are "N H / ;k~l~  ," "15 H / -7- =- :7, ~ ," and 
"~lit:~ / ~--2` ~ ," and those in the estimated 
word order are "~H / -~---2`~," "~H / 
1~1~ la~ ," and "Y- = 2` ~ / %:t~ll lak ." The agreement 
rate is 67% (2/3) because two of the three orders 
are the same as those in the original sentence. 
Complete  agreement  The second measurement is 
the percentage of the modifiees whose modifiers' 
word order agrees with that in the test corpus. 
3 Exper iments  and  D iscuss ion  
In our experiment, we used the Kyoto University text 
corpus (Version 2) (Kurohashi mid Nagao, 1997), a 
tagged corpus of the Mainichi newspaper. For train- 
ing, we used 17,562 sentences from newspaper arti- 
cles appearing in 1995, from January 1st to Jmmary 
8th and from Jmmary 10th to June 9th. For testing, 
we used 2,394 sentences fl'om articles appearing on 
January 9th and from June 10th to June 30th. 
3.1 Def in i t ion of  Word  Order  in  a Corpus  
In the Kyoto University corpus, each bunsetsu has 
only one modifiee. When a bunsetsu Bm depends on 
a bunsetsu Bd and there is a bunsetsu /3p that de- 
pends on and is coordinate with \])d, Bp has not only 
the information that its modifiee is \]~d but also a la- 
bel indicating a coordination or the information that 
it is coordinate with B d. This information indirectly 
shows that the bunsetsu Bm can depend on both \]3p 
and Bd. In this case, we consider Bm a modifier of 
both Bv and B d. 
Under this condition, modifiers of a bunsetsu B 
are identified in the following steps. 
1. Bunsetsus that depend on a bunsetsu B are clas- 
sifted as modifiers of B. 
2. When B has a label indicating a coordination, 
bunsetsus that are to tile left of 13 and depend on 
the same modifiee as B are classified as modifiers 
of B. 
3. Bunsetsus that depend on a modifier of B and 
have a label indicating a coordination are clas- 
sifted as modifiers of B. The third step is re- 
peated. 
When the above procedure is completed, all bunset- 
sus that coordinate with each other are identified as 
modifiers which depend oi1 the same nmdifiee. For 
example, from the data listed on the left side of To- 
ble 2, the modifiers listed in the right-hand column 
are identified for each bunsetsu. "Nt~I; ~ (Taro_to, 
Taro and)," "?~g-~ IS (Hanako_to, Hanako)," "ql "(, 
(dete,, participate,)" are all identified as modifiers 
which depend on the same modifiee "~ b 7=? 
(yusyo_sita., won.)." 
3.2 Exper imenta l  Resu l ts  
The features used in our experiment are listed in Ta- 
bles 3 and 4. Each feature consists of a type and 
a value. The features consist basically of some at- 
tributes of the bunsetsu itself, and syntactic and con- 
textual information. We call the features listed in 
Tables 3 'basic features.' We selected them man- 
ually so that they reflect the basic conditions gov- 
erning word order that were sunmmrized by Saeki 
(Saeki, 1998). The features in Table 4 are combina- 
tions of basic features ('combined features') and were 
also selected manually. They are represented by the 
nmne of the target bunsetsu plus the feature type of 
the basic features. The total number of features was 
about 190,000, and 51,590 of them were observed in 
the training cortms three or more times. These were 
the ones we used in our experiment. 
The following terms are used in these tables: 
Mdf r l ,  Mdf r2 ,  Mdfe:  The word order model de- 
scribed in Section 2.1 estimates the probability 
that modifiers are in the appropriate order as 
the product of the probabilities of all pairs of 
modifiers. When estimating the probability tbr 
each pair of modifiers, the model assmnes that 
the two modifiers are in the appropriate order. 
Here we call the left modifier Mdfrl, the right 
modifier Mdfr2, and their modifiee Mdfe. 
Head:  the rightmost word in a bunsetsu other than 
those whose major pro't-of-speech I category is 
1Part-of-speech categories follow those of JUMAN (Kuro- 
hashi and Nagao, 1998). 
874 
Table 3: Basic features. 
Bas ic  features  
Feature values (Number of type) Feature type tegors\] Target 
1)tmsetsus 
1 Mdfrl, Mdfr2, 
Mdfe 
2 Mdfrl, Mdfr2, 
Mdfe 
3 Mdfrl, Mdfr2, 
Mdfe 
4 Mdfrl, Mdh'2, 
Mdfe 
m m  
\[\]ead-POS(Major) 
\[tead-POS(Minor) 
\[lead-hff(Major) 
\[Iead-Inf(Minor) 
\[Iead-SemFeat(110) 
\[1ead-SemFeat(111 ) 
\[Iead-SemFeat(433) 
5 Mdfrl, Mdff2, rype(String) 
Mdfe rype(Major) 
type(Minor) 
6 Mdfrl, Mdfr2, lOSIll l(String) 
Mdfe lOSIIIl(Minor) 
lOSII12(String) 
lOSUI2(Minor) 
7 Mdfrl, Mdfr2, Period 
Mdfe 
8 Mdfrl, Mdfr2 Numl)erOfMdfrs 
Mdfe NumberOfMdfrs 
9 Mdfrl, Mdfr2, 
Mdfe 
10 Mdfl'l, Mdfr2 
Coordination 
(Total : 90) 
Mdfrl-MdfrType-ll )to-Mdfr2-Typc 
Mdfl'2-MdfrTypeql )to-Mdfl-I -q'ype 
Mdfi'l -Md frType-lDto- Md fr2-MdfrType 
11 Mdfrl, Mdfr2, Rel)etition-llead-l,ex 
Mdfe Repetition-Md fr-lleadq,ex 
12 Mdfrl, Mdfr2 ReferencePronoun 
i~efereneePronou  (String) 
',.%066) 
~u (verb), s~u (adjective), ~,'~ (noun) . . . .  (11) 
~'~:~ (common oun), m'~ (quantifier) . . . .  (24) 
~1~ (vowel verb) . . . .  (30) 
'.'~ (stem), t~*~ (fandamental form) . . . .  (60) 
rrue (1) 
true (1) 
true (1) 
:~, :a ,  <-u<, t:u, ~, ~=, t . . . .  (7:3) 
uJ:~ (post-positional particle), . . .  (43) 
?,~JJ'~ (e~use marker), ~*$ (imperative form) . . .  (102) 
'~',5, ~<', a~, ,., l,~. . . . .  (63) 
)ill\], ~$m (ease marker), . . .  (5) 
~, ~, *, ~,*,, ... ((~3) 
~,~;~1 (ease marker) . . . .  (41 
\[nil\], \[exist\] (2) 
A(0), B(1), C(2), 1)(3 or more) (4) 
A(2), B(:3), C(4 or more) (3) 
P(Coordim~te), A(Apposition), I)(otherwise) (3) 
\]'rue, False (2) 
rrue, False (2) 
true, False (2) 
86.65% 73.87% 
\[--0.79%) (--1.54%) 
87.07% 75.03% 
',--O.37%) (--0.38%) 
87.39% 75.20% 
',-0.05%) (-0.m%) 
87.21% 75.20% 
\[--0.23%) (--0.21%) 
84.78% 70.03% 
(-2.66%) (-5.38%) 
87.32% 75.14% 
(-0.12% (--0.27%) 
87.39% 7 ~  
(-0.05%) (+0.13%) 
87.14% 74.86% 
(-0.30%) (-0.55%) 
87.40% 70.30?./o 
(-0.04%) (-0.0~%) 
8~.2~% 73.61% 
(--1.18% (--1.80%) 
87.34% 75.09% 
(--0.10%) (--0.'32%) 
\[nil\],\[exist\] (2) 87.31% 75.id% 
\[nil\], \[exist\] (2) (--0.13%) (--0.27%) 
\[nil\], \[exist\] (2) 87.27% 75.12% 
:~, :a,  :~ .~,~=.~, ,  ~. . . . .  (42) (--0.17%) (--0.29%) 
'%} @ (special marks)," "112 N (1)ost-posi~ioual 
particles)," or "}~N~? (suffixes)." 
Head-Lex :  the fllndalnental forth (unintlected 
forln) of the head word. Only words with a fre- 
quency of tlve or more are used. 
Head- In f :  the inflection type of a head. 
SemFeat :  We use the upper third layers of bunrui 
.qoihyou (NLl/I(National Language Research In- 
stitute), 19641 as semantic features. Bunrui goi- 
hyou is a Japanese thesaurus that has a tree 
structure and consists of seven layers. The tree 
has words in its leaves, and each word has a fig- 
ure indicating its category number. For exam- 
ple, the figure in parenthesis of a feature "Head- 
SemFeat( l l0)" in Table 3 shows the upper three 
digits of the category number of the head word 
or the ancestor node of the head word in the 
third layer in the tree. 
Type:  the rightmost word other than those whose 
major part-of-speech category is "~@ (special 
marks)." If the major category of the word 
is neither "NJN (post-positional particles)" nor 
"}~/~'~ (suffixes)," and the word is inflectable, 2 
then the type is represented by the inflection 
type. 
JOSHI1 ,  JOSHI2 : JOSHI1  is the rightmost post- 
positional particle in the bunsetsu. And if there 
are two or more post-positional particles in the 
bunsetsu, JOSHI2 is the second-rightmost post- 
positiolml particle. 
NmnberOfMdf rs :  number of modifiers. 
2The inflection types follow those of J UMAN. 
Mdfr l -Mdf rType ,  Mdf r2 -Mdf rType:  Types of 
tile modifiers of Mdfi'l and Mdfr2. 
X- IDto -Y :  X is identical to Y. 
Repet i t ion -Head-Lex :  a ret)etition word allpear- 
ing ill a preceding senteuce. 
Re ferencePronour l :  a reference pronoun appear- 
ing in the target bunsetsu or ill its modifiers. 
Categories 1 to 6 ill Table 3 reI)resent attributes 
in a bunsetsu, categories 7 to 10 represent syntac- 
tic information, and categories 11 and 12 represent 
contextual information. 
The results of our experiment are listed in Table 5. 
The first line shows tlle agreement rate when we esti- 
mated word order for 5,278 bunsetsus that have two 
or more modifiers and were extracted from 2,394 sen- 
tences al)pearing on Jmmary 9th and from June 10th 
to June 301tl. \Ve used bunsetsu boundary informa- 
tion and syntactic and contextual information which 
were derivable froln the test corpus and related to 
the input bunsetsus. As syntactic ilffOrlnation we 
used dependency inforlnation, coordinate structure, 
and information on whether the target bunsetsu is at 
the eM of a sentence. As contextual information we 
used the preceding sentence. The values in the row 
labeled Baseline1 in Table 5 are the agreement rates 
obtained when every order of all pairs of modifiers 
was selected randolnly. And values in the B&seline2 
row are the agreement rates obtained when we used 
the following equation instead of Eq. (5): 
freq(w12) 
PMu'(llh) = freq(w12) + frcq(w21)" (7) 
875 
Table 4: Combined features. 
Accuracy without 
the feature 
Pair of Complete 
modifiers :tgreement 
87.23% 74.65% 
(-0.21%) (-0.76%) 
Combined  features  
- -  Twin tbatures 
(Mdfr 1-Type, Mdfr2-Type) ,  
(Mdfr 1-Type,  Mdfe- I Iead-Lex) ,  
(Mdfr  1-Type,  Md fe- t tead-POS) ,  
(Mdfr  1-Type,  Mdfr  1-Coordlnrtt ion),  
(Mdfr  1-Type, Mdf r2 -Mdf rType- IDto -Md fr1-2"ypc), 
(MdfrE-Type,  Mdfe-Head-Lex) ,  
(Mdfrg-Type,  Mdfe-Head-POS) ,  
(Mdfr2-Type,  Mdfr2-Ooord inat ion) ,  
(Mdfr2-Type,  Md fr 1-MdfrType- lDto-Mdfr2-Type) ,  
Mdfr  1-Head-Lex,  Mdfe-Per iod) ,  
Mdfr  1 -nead-POS,  Mdfe-Per lod) ,  
Mdfr  1-1tead-POS, Mdfr  1-Repet l t lon-Head- I ,ex) ,  
Mdfr2- I tead-Lex,  Mdfe-Pet lod) ,  
Mdfr2-Ite,~d-POS, Mdfe-Per lod) ,  
Mdf r2 - I Iead-POS,  Mdfr2- I tepet l t lon- I lead-Lex)  
' t~iplet  tca tures  87.22% i 74.86% 
Mdfr l -Wype,  Mdfr2-Type,  Mdfe- l lead-Lex) ,  (--0.220./0) (--0.55%) 
Mdfr l -Type ,  Mdf r2 -Type,  Mdfe-Head-POS) ,  
Mdf r l -Type ,  Mdf r l -Coord inat lon ,  Mdfe-Type) ,  
MdfrE-Type,  Mdfr2-Coord lnat lon ,  Mdfe-Type) ,  
Mdf r l - JOSHI1 ,  Mdf r l - JOSHI2 ,  Mdfe- I Iead-Lex),  
Mdf r l -aOSHl l ,  Mdf r l - JOSHI2 ,  Mdfe- I Iead-POS) ,  
Mdf r2 - JOSHI1 ,  Mdfr2- JOSI I I2 ,  Mdfe-Head-Lex),  
Mdf r2-aOSH\[1 ,  Mdfr2-JOSl~12, Mdfo- l lead-POS)  
All of  above  combined  features  85.79% 77.67% 
~--1.65%) (--3.74%) 
Table 5: Results of agreement rates. 
Agreement  ra te  
Pair of modifiers Coml)lete agreement 
Our method 87.44%(72,367/14,137) 75.41% (3,980/5,278) 
Baseline1 48.96% (6,921/14,137) 35.10% (7,747/5,278) 
Baseline2 49.20% (6,956/14,137) 53.84% (1,786/5,278) 
IIere we assume that B1 and \]32 are modifiers, their 
modifiee is B, the word types of B1 and \]32 are re- 
spectively Wl and we. The values frcq(wr2) and 
frcq(w.27 ) then respectively represent the fl'equencies 
with which w7 and w,2 appeared in the order "WT, we, 
mid w" and "w2, WT, and w" in Malnichi newspaper 
articles fl'om 1991 to 1997. a Equation (7) means 
that given the sentence "~t~lt I:t (Taro_wa) / ~- -- ~, 
(tennis_wo) / b ~-:o (sita.)," one of two possibili- 
ties, "1$ (wa) / ~ (wo) / t, ~:o (sita.)" and "#c (wo) 
/ tS (wa) / b ~:o (sita.)," which has the higher fre- 
quency, is selected. 
3.3 Features  and Agreement  Rate  
This section describes how much each feature set con- 
tributes to improving the agreement rate. 
The values listed in the rightmost columns in Ta- 
bles 3 and 4 shows the performance of the word or- 
der estimation without each feature set. The values 
in parentheses are the percentage of improvement or
degradation to the formal experiment. In the exper- 
iments, when a basic feature was deleted, the com- 
bined features that included the basic feature were 
also deleted. The most useful feature is the type of 
3When wl and w2 were the same word, we used the head 
words in Bt  and 132 as Wl and w2. When one offreq(wt2) and 
freq(w21) was zero and the other was five or more, we used 
the f lequencies when they appeared in the order "Wl ws" and 
"w2 wt,"  respectively~ instead of frcq(wi2) al,d freq(wsl). 
When both freq(wl.2) and freq(w27) were zero, we instead 
used random figures between 0 and t. 
bunsetsu, which basically signifies the case marker or 
inflection type. This result is close to our expecta- 
tions. 
We selected features that, according to linguistic 
studies, as mudl  as possible reflect the basic condi- 
tions governing word order. The rightmost column 
in Tables 3 and 4 shows the extent o which each con- 
dition contributes to improving the agreement rate. 
However, each category of features might be rougher 
than that which is linguistically interesting. For ex- 
ample, all case markers uch as "wa" and "wo" were 
classified into the same category, and were deleted 
together in the experiment when single categories 
were removed. An experiment that considers each 
of these markers eparately would help us verify the 
importance of these markers separately. If we find 
new features in future linguistic research on word or- 
der, the experiments lacking each feature separately 
would help us verify their importance in the same 
manner .  
3.4 Tra in ing  Corpus  and Agreement  Rate  
The agreement rates for the training corpus and the 
test corpus are shown in Figure 1 as a function of 
the amount of training data (ntunber of sentences). 
The agreement rates in the "pair of modifiers" and 
'?!19~ . . . . .  %:I:: 'i, ~ . . . . . . . . .  ~':z~-,~',:: : : i 
= 
?I 90 90 
!' 
S5 ~5 i ~ - 
E 
75 ~ 75 
7O 7O 
65 . . . . . . . .  (,5 0 . . . .  
0 2000 400{) 6000 80O0 Io(mO 120{}0 14O00 16000 18000 2000 4000 6,300 ~000 IODO0 12(;00 14000 16O00 la00{1 
\] 11o NtllObor o~ Snnloncos \]o 111o Traif l in9 Data l lm Number ol Sentences in l lm T f 4dlli11U \[)ala 
Figure 1: Relationship between tile amount of training 
data and the agreement rate. 
"Complete agreement" measurements were respec- 
tiw~ly 82.54% and 68.40%. These values were ob- 
tained with very small training sets (250 sentences). 
These rates m'e considerably higher than those of 
the baselines, indicating that word order in Japanese 
can be acquired fl'om newspaper articles even with a 
small training set. 
With 17,562 training sentences, the agreemenl, 
rate in the "Complete agreement" measurement was 
75.41%. We randomly selected and analyzed 100 
modifiees from 1,298 modifiees whose modifiers' word 
order did not agree with those in the original text. 
We found that 48 of them were in a natural order 
and 52 of them were in an unnatural order. The 
former result shows that the word order was rela- 
tively fl'ee and several orders were acceptable. The 
latter result shows that the word order acquisition 
was not sufficient. To complete the acquisition we 
need more training corpora and features which take 
into account different information than that m Ta- 
bles 3 mid 4. We found many idiomatic expres- 
876 
sions in the uimatural word order results, such as "~ 
ffl\[~il~:5~ (houchi-kokka_ga,  country under the rule 
of law) / \ [ l} l~  (kiitc, to listen) /~#t~ (alcireru, 
to disgust), ~rj ~ b ?= a ~ (souan-s~,ta-no_ga, orlgl- 
,ration) / ~ *o ~- *o co (somosomo-no, at all) / ~t~  U 
(hg~'ima~'4 the beginning)," and ""~ l~ (g#-~4 taste) / 
~'~B (seikon, one's heart and soul) /g~ 6 (homcru, 
to trot somethil,g into soinething)." We think that 
the apt)ropriate word order for these idiomatic ex- 
pressions could be acquired if we had more training 
data. We also found several coordinate structures in 
the Ulnlatural word order results, suggesting that we 
should survey linguistic studies on coordinate struc- 
tures and try to find efllcient features for acquiring 
word order from coordinate structures. 
We (lid not use the results of semantic and con- 
textual analyses as input because corpora with se- 
mantic and contextuM tags were not available. If 
such corpora were available, we could more et\[iciently 
use features dealing with seinantic features, reference 
pronouns, and repetition words. We plan to make 
corpora with semantic and contextual tags and use 
these tags as input. 
3.5 Acqu is i t ion  f rom a Raw Corpus  
In this section, we show that a raw cortms instead of 
a tagged corpus can be used to train the lnodel, if it 
is first analyzed by a parser. We used the lnorl)holog- 
ical analyzer JUMAN and a tmrser KNP (Kurohashi, 
11198) which is based on a det)endency grainlnar, 
it, order to extract iuforumtion from a raw corpus 
for detecting whether or not each feature is found. 
'l?tm accuracy of JUMAN for detecting inorphologi- 
cal boundaries and part-of-speech tags is about 98%, 
and the parsecs dependency accuracy is about 90%. 
These results were obtained from analyzing Mainichi 
newspaper articles. 
We used 217,562 sentences for training. When 
these sel~t, ences were all extracted from a raw corlms , 
the agreement rate was 87.64% for "pair of modifiers" 
and was 75.77% for "Colnplete agreement." When 
the 217,562 training sentences were sentences fl'oln 
the tagged cortms (17,562 sentences) used in our for- 
real exl)eriment aInl froln a raw cortms, the agree- 
" e S :~ ment rate for "pair of lno(hfi.r, was 87.66% and 
for "Complete agreement" was 75.88%. These rates 
were about 0.5% higher than those obtained when we 
used only sentences from a tagged corlms. Thus, we 
can acquire word order by adding inforlnation froln 
a rmv corpus even if we do not have a large tagged 
corpus. The results also indicate that the parser ac- 
curacy is not so significant for word order acquisition 
and that an accuracy of about 90% is sufficient. 
4 Conc lus ion 
This paper described a method of acquiring word or- 
der froln corpora. We defined word order as the order 
of lnodifiers which depend on tile same lnodifiee. The 
lnethod uses a model which estimates the likelihood 
of the apt)ropriate word order. The lnodel automat- 
ically discovers what the tendency of the word order 
in Japanese is by nsing various ldnds of information 
in and arouud the target bunsetsus plus syntactic 
and contextual inforlnation. The contribution rate 
of each piece of inforination in deciding word order 
is efficiently learned by a model implemented within 
an ),,I.E. framework. Comparing results of experi- 
ments controlling for each piece of information, we 
found that the type of inforinatiou having the great~ 
est influence was the case marker or inflection type in 
a bunsetsu. Analyzing the relationship between the 
amount of training data and the agreement rate, we 
fimnd that word order could be acquired even with 
a small set of training data. We also folmd that a 
raw cortms as well as a tagged cortms can be used to 
train the model, if it is first, analyzed by a parser. The 
agreement rate was 75.41% for the Kyoto University 
corpus. We analyzed the lnodifiees whose modifiers' 
word order did not agree with that in the original 
text, and folmd that 48% of theln were in a natural 
order. This shows that, in umny cases, word order 
in Japanese is relatively free and several orders are 
acceptable. 
The text we used were lmwspaper articles, which 
tend to have a standard word order, but we think 
that word orders tend to differ between ditferent 
styles of writing. We would therefore like to carry 
out experiments with other types of texts, such as 
novels, having styles different froln that of newspa- 
pers. 
it has been (lift\]cult o evaluate tile reslflts of text 
generation objectively becmlse there have been no 
good stmldards for ewlllmtion. By using the stan- 
(lard we describe in this paper, however, we can evN- 
uate results objectively, at least for word order esti- 
mation in text, generation. 
We expect hat our lnodel can be used for several 
applications as well as linguistic veritication, such as 
text; refinement silt)port and text generation in nla- 
chine translation. 
References  
Adam L. Better, Stephen A. I)ella Pietra, and Vincent J. Della 
Pietra. 199(L A Maximum t'\]ntropy Approach to N~ttural ,~tn- 
gmtge Processing. Computational Linguistics, 22(11:39-71. 
Sadao Kurohashi and Makol.o Nagao. 1997. Kyoto University 
Text Corl)uS Project. In Proceedings of The Third Annual 
Mectin9 of The Association for Natural Language Process- 
ing, pages 115-118. (in Japanese). 
Sadao Kurohashi and Makoto Nagao, 1998. Japanese Morpho- 
logical Analysis System JUMAN Version 3.6. l)epartment of 
Informatics, Kyoto University. 
Sadao Kurohashi, 11198. Japanese Dependency/Case Structure 
Analyzer KNP Version 2.0b6. Department of Inforln~tties, Ky- 
ore University. 
IIiroshl Maruyama. 1994. Experhnents on V~rord-Order Recovery 
Using N-Cram Models. In YTte \]~9th Annual (;onvention IPS 
Japan. (in Japanese). 
NLRl(National Language Research Institute). 1964. Word List 
by Semantic Pri~ciples. Syuei Syuppan. (in Japmlese). 
Tetsuo Saekl. 1998. Yousetsu nihongo no 9ojun (Survey: Word 
Order in Japanese). Kuroshio Syupl)an. (in Japanese). 
James Shaw and Vasileios Ilatzivassiloglou. 1999. Ordering 
Among l'remodifiers. In Proceedings of the 37th Annual Meet- 
ing of the Association for Computational Linguistics (,4 CL), 
pages 135-143. 
Takenobu Tokunaga and IIozumi Tanaka. 1991. On Estimat- 
ing Japanese Word Order B~used on Valency Information. 
Keiryo Kokugogaku (Mathematical Linguistics), 18(21:53-(;5. 
(in Japanese). 
877 
J apanese  Named Ent i ty  Ext rac t ion  Eva luat ion  
- Ana lys i s  o f  Resu l t s  - 
Satosh i  Sek ine  
Computer  Science Depar tment  
New York University 
715 Broadway, 7th floor 
New York, NY 10003, USA 
sekine@cs, nyu. edu 
Yosh io  Er iguch i  
Research and Development Headquarters  
NTT  Data  Corporat ion 
1-21-2, Shinkawa, Chuo-ku, 
Tokyo, 104-0033, Japan 
er iguch i~rd ,  n t tdata ,  co. jp  
Abst ract  
We will report on one of the two tasks in the 
IREX (Information Retrieval and Extraction 
Exercise) project, an ew~luation-based project 
fbr Infbrmation Retriewfl and Intbrmation Ex- 
traction in Japanese (Sekine and Isahara, 2000) 
(IREX Committee, 1999). The project stm'ted 
in 1998 and concluded in September 1999 with 
many participants and collaborators (45 groups 
in total from Japan and the US). In this paper, 
the Nmned Entity (NE) task is reported. It is 
a task to extract NE's, such as names of orga- 
nizations, persons, locations and artifacts, time 
expressions and numeric expressions from news- 
p~t)er articles. First, we will explain the task 
and the definition, as well as the data we cre- 
ated and the results. Second, the analyses of the 
results will be described, which include analysis 
of task diifieulty across the NE types and sys- 
tern types, amflysis of dolnain dependency and 
comparison to hmnan per~brmance. 
1 I n t roduct ion  
The need for II1 and IE technologies i getting 
larger because of the improvements ill computer 
technology and the appearance of the Internet. 
Many researchers in the field feel that the ew~l- 
uation based projects in the USA, MUC (MUC 
Homepage, 1999) and TREC (TREC Home- 
page, 2000), have played a very important role 
in each field. In Japan, however, while there has 
been good research, we have had some dilficul- 
ties comparing systems based on the same plat- 
form, since our researdt is conducted at many 
different; Ulfiversities, companies, and laborato- 
ries using different data and evaluation lnea- 
sures. Our goal is to have a common platform in 
order to evaluate systems with the stone stan- 
dard. We believe such projects are nseflfl not 
only for comparing system performance lint also 
to address the following issues: 
1) To share and exchange problems among re- 
searchers, 
2) To aeeulntll~,te large quantities of data, 
3) To let other people know the importance 
and the quality of IR and IE techniques. 
Finishing the project, we believe we achieved 
these goals. 
In this paper we will describe one of the two 
tasks in the IR.EX project, the Nmned Entity 
task. 
2 IREX NE 
2.1 Task 
Named Entity extraction involves finding 
Named Entities, such as names of organizations, 
persons, locations, and artifimts, time expres- 
sions, and numeric expressions, uch as money 
and percentage expressions. It is one of the ha- 
sic techniques used in IR and IE. At the ewfl- 
uation, participants were asked to identit\[y NE 
expressions as correctly as possible. In order 
to avoid a copyright I)robleIn, we made a tool 
to convert a tagged text to a set of tag off'set 
information and wc only exchanged tag ott;et 
intbrlnation. 
2.2 Def in i t ion 
The definition of NE's is given in an 18-page 
document,  which is available through the II1EX 
homepage (IREX Homepage, 1999). There 
are 8 kinds of NE's shown in 'lhfl)le 1. In or- 
der to avoid requiring a unique decision ~br 
ambiguous cases where even a lnnnan could 
not tag unambiguously, we introduced a tag 
"OPTIONAL ''1. If a system tags an expression 
1This tag is newly introduced in IREX and does not 
exist in MUC. The tag accounts for 5.7% of all NE occur- 
1106 
NE Examl)le 
ORGANIZATION The Diet, 1REX Commit;tee 
PERSON Sekine, \?akanohana 
LOCATION Japan, '\]bkyo, Mt.Fuji 
ARTIFACT Pentiuln II, Nobel Prize 
DATE March 5, 1.965; Yesterday 
TIME 11 PM, nfidnight 
MONEY 100 yen, $12,345 
PERCENT 1.0%, a half 
~i~ble 1: NE Classes 
within the OPTIONAL tag, it; is just ignored for 
th.e scoring. The defilfition was created 1)ased on 
the MUC/MET definition; however, the process 
of lnaking the definition was not easy. In par- 
ticulaI', the definition of the newly introduced 
NE tyl)e "artifact" was Colitroversial. W'e ad- 
mit that more consideration is needed to make 
a clem'er definition of the NE typos. 
Comparing the NE task in Japanese to that 
in English, one of the ditIiculties comes from the 
fact that there is no word delinfiter in Japanese. 
Sysl;elns have to identity the })oundaries of ex- 
pressions. This will 1)ecome complicated when 
we want to tag a sul)string of what ix gener- 
ally considered a ,Japanes(~ wor(t, l/or (~xaml)le , 
il.t .Jal)allese there is a word "Ratnich?" which 
means "Visil; 3apa.n" and consists of two Chi- 
nese eh.aracters, "Ra?" (Visit;)and "Nichi" (ab- 
breviation of .Japan). Although mmly word seg- 
reenters identif~y it as a single, word, we expect 
to extrtmt only "Nichi" as a local;ion. '\]'his is 
a tricky prol)lem, as opposed to the ease in En- 
glish where a word is the unit of NE candidates. 
2+3 Runs  and  Data  
There were three kinds of NE exercises, the dry 
run, a restricted (hmlMn tbrmal rtm, and a gen- 
eral domain tbl'mal 1'1111, which will be explained 
later. Also we created three kinds of training 
(h~ta: the dry run trailfing data, the CI{.L_NE 
data and the formal run domain restricted trail> 
ing data. Td)le 2 shows the size of each data set. 
Note that CRL_NE (lata l)elongs to the Colllnltt- 
nication ll.esearch Laboratory (CI{L), but it is 
ronces ill the generM dolnaill evMuation and 2.1% in the 
restricted omain e, valuation (the t.ypes of the evaluation 
will be explained later). 
ineht(ted ill the tat)le, because the data was cre- 
ated by IREX participants, using the definition 
of II{EX-NE, +rod distributed through I\]{,EX. 
Data Number of 
articles 
Dry Run training 
Dry t.hm 
CIIL_NE data 
D)rlnal l'lln (restricted) training 
Formal run (restricted) 
Formal run (general) 
46 
36 
1.174 
23 
20 
71 
Table 2: Dnta size 
~n or(let to ensure the, fairness of the exercise 
in the formal \]'un~ we used newspaper articles 
which no one had ew~r seen. We, set the date to 
fl'eeze the system development (April 13, 1999). 
The date for the evahtation was set one month 
after that (lat;e (May 13 to \]7, 1999) so that 
we could select the test m'ticles fl'om the 1)cried 
t)etween those dates. \?e thank the Mainichi 
Newspaper CorI)oration for provi(ling this data 
for us t\]:ee of charge. 
2.4 Rest r i c ted  domain  
in the fbrmal run, in order to study system 
portability and the effect of domains on NE 
perfoilllanc(',, we had two kinds of evaluation: 
rest;rioted omain and general domMn. In the 
general domain ewthtation, w(, selected articles 
regardless of dolnain. The domain of the re- 
stricted domain evaluation was a.lmouneed one 
month before the develolmmnt freeze date. It; 
was an "arrest;" domain defined as follows and 
211 the articles in the restricted omain are se- 
lected based on the definition. 
77re articles arc 'related to an e'ucnt 
",,frost". The event is defined as th, c 
a'r'rc.st of a .suspect o1' s'~t,5'pects by po- 
lice, National \])olicc, State police of 
other police forces including the o'ncs 
of foreign countries. It includes arti- 
cles mentionirtg an arrest event in the 
past. It: excludes articles which have 
only i'n:formation about requesting an 
arrest warrant, art accusation or send- 
ing the pape'rs pc'training to a case to 
an Attorney's OJJicc. 
1107 
2.5 Resu l ts  
8 groups and 11 systems participated in the 
dry run, and 14 groups and 15 systems partici- 
pated in the %rmal run 2. Tim evaluation results 
were made public anonymously using systeln 
ID's. Table 3 shows the ew~luation results (F- 
measure) of the formal run. F-measure is cal- 
culated from recall and precision (IREX Coin- 
mittee, 1999). It ranges from 0 to 100, and the 
larger the better 
System ID 
1201 
1205 
1213 
1214 
1215 
1223 
1224 
1227 
1229 
1231 
1234 
1240 
1247 
1250a 
12501) 
general 
57.69 
80.05 
66.60 
70.34 
66.74 
72.18 
75.30 
77.37 
57.63 
74.82 
71.96 
60.96 
83.86 
69.82 
57.76 
restrict 
54.17 
78.08 
59.87 
80.37 
74.56 
74.90 
77.61 
85.02 
64.81 
81.94 
72.77 
58.46 
87.43 
70.12 
55.24 
diff. 
-3.52 
-1.97 
-6.73 
+10.03  
+7.82 
+2.72 
+2.31 
+7.65 
+7.18 
+7.12  
+0.81 
-2.50 
+3.57 
+0.30 
-2.52 
~li~ble 3: NE Formal run result 
3 Ana lyses  o f  the  resu l ts  
3.1 Diff iculty across NE  type 
In Table 4, tile F-measure of the best perform- 
ing system is shown in the "Best" column; the 
average F-measures are shown in the "Average" 
column tbr each NE type on the formal runs. It 
can be observed that identifying time and nu- 
lneric expressions is relatively easy, as the av- 
erage F-measures are more than 80%. In con- 
trast, the accuracy of the other types of NE is 
not so good. Ill particular, artifacts are quite 
difficult to identify. It is interesting to see that 
tagging artifacts in the general domain is mud1 
harder thins in the restricted domain. This is 
because of the limited types of artifacts in the 
restricted domain. Most of the artifacts in the 
2The participation to the dry run was not obligatory. 
This is why the number of participants is smaller in the 
dry run than that in the formal rmL 
restricted omain are the names of laws, as the 
doinain is the arrest domain. Systems lnight be 
able to find such types of names easily because 
they could be recognized by a small number of 
simple patterns or by a short list. The types 
of tim artifacts in the general donmin are quite 
diverse, including names of prizes, novels, ships, 
or paintings. It nfight be difficult to build pat- 
terns for these itelns, or systems may need very 
complicated rules or large dictionaries. 
3.2 Three types of  sys tems 
Based on the questionnaire for the particit)ants 
we gatlmred alter the formal runs, we found that 
there are three types of systems. 
? Hand created pattern based 
These are pattern based systems where the 
patterns are created by hand. A typical 
system used prefix, sutlqx and proper noun 
dictionaries. Patterns in these systems look 
like "If proper nouns are followed by a suffix 
of person name (for example, a common 
suflqx like "San", which is ahnost equivalent 
to Mr. and Ms.) then the proper nouns are 
a t)erson nmne". This type of system was 
very common; there were 8 systems in this 
category. 
? Automatically created pattern based 
These are pattern based systems where 
some or all of the patterns are created an-  
tolnatically using a training corl)us. There 
were three systems in this category, and 
these systems used quite different meth- 
ods. One of them used the "error driven 
method", in which hand created patterns 
were applied to tagged training data and 
the system learned from tlm mistakes. An- 
other system learned patterns for a wide 
range of information, including, syntax, 
verb frame and discourse information fl'om 
training data. The last system used the 
local context of training data and several 
filters were applied to get more accurate 
patterns. 
Fully automatic 
Systems in this category created their 
knowledge automatically from a training 
corpus. There were four systems in this 
category. These systems basicMly tried to 
assign one of the four tags, beginning, mid- 
dle or ending of an NE, or out-ofNE, to 
1108 
NE type 13est 
Orgmfization 78 
Person 87 
Location 8d: 
Artifact 44 
Date 90 
Time 82 
Money 86 
Percent 84 
Total 84 
General domain 
Average Expert 
57 96 
68 99 
70 98 
26 90 
86 98 
83 97 
86 100 
86 97 
70 98 
Best 
75 
87 
88 
83 
93 
97 
100 
87 
Table 4: Results 
Restrict domain 
Average Novice Expert 
55 
69 
68 
58 
89 
90 
91 
88 
97 
94 
74 
96 
98 
100 
98 
100 
99 
92 
100 
98 
100 
72 94 99 
each word or each character. The source 
information for the training was typically 
character type, POS, dictionary in%rma- 
tion or lexical information. As tile learn- 
ing meclmnism, Maximmn Entrot)y models, 
decision trees, and HMMs were used. 
It is interesting to see that tile top three sys- 
tems came, fi'om each category; the best sys- 
tem was a hand create, d pattern based system, 
tile second system was an automatically created 
pattern based system and the third system was 
a fully automatic system. So we believe we can 
not conclude which type is SUl)erior to the eth- 
el'S. 
Analyzing the results of the top three sys- 
to, ms, we observed the, importance of tile dic- 
tionaries. The best hand created pattern based 
system seems to have a wide coverage dictionary 
for person, organization and location names 
and achieved very good accuracy tbr those cat- 
egories. Howe.ver, the hand created pattern 
based system failed to capture the evahmtion 
specific pattenls like "the middle of April". Sys- 
tems wore required to extract the entire ex- 
1)ression as a date expression, but; the system 
only extracted "April". The best hand created 
rule based system, as well as the best ~mtolnat- 
ically created pattern lmsed system also missed 
other specific patterns which inchlde abbrevi- 
ations ("Rai -Nich i"  = Visit-Japan), conjmm- 
tions of locations ("Nichi-Be?" = ,\]alton-US), 
and street; addresse, s ("Meguro-ku, 0okayama 
2-12-1") .  The best hilly autolnatic system was 
successflfl in extracting lllOSt of these specific 
patterns. However, the flflly automatic system 
has a probleln in its coverage. In lmrticular, the 
training data was newspaper articles published 
in 1994 and the test; data was fro151 1999, so 
there are several new names, e.g. the prilne min- 
ister's name which is not so co1111noll (0buchi) 
and a location nmne like "Kosovo", wlfich were 
rarely mentioned in 1994 but apt)eared a lot in 
1999. '.Fhe system missed many of them. 
3.3 Domain  dependency  
In Table 3, the differences in performance be- 
tween the general domain and the restricted o- 
main ~r(' shown ill the cohmm "diff.". Many 
systems 1)erfl)nne(l better in the restri(:ted do- 
main, although ~ small ntllnl)er of systems per- 
forlned better ill the genera\] domain. There 
were two systems which intentiomflly tuned 
their systems towards the restricted domain, 
which are shown in bold in the table. Both of 
these were alnong the systelns which perfbrlned 
much better (more than 7%) ill the restricte.d 
domain. The system which achieved the largest 
improvement was a fully automatic system, and 
it only replaced the training data for the domain 
restricted task (so this is an intentionally tuned 
system). It shows the domain dependency of the 
task, although further investigation is needed to 
see why some other systems can perform nmch 
better even without domain tinting. 
3.4 Compar i son  to human per formance 
In Table 4, hulllan performance is shown in 
the "Novice" and "Expert" cohtmns. "Novice" 
means tile average F-measure of three gradu- 
ate students all(l "l;xpert" means the average 
F-measure of the two people who were most re- 
1109 
sponsible for creating tile definition and created 
the answer. They frst  created two answers in- 
dependently and checked them by themselves. 
The results after the checking are shown in the 
table, so many careless mistakes were deleted 
at this time. We Call say that 98-99 F-measure 
is the performance of experts who create them 
very carefully, and 94 is a usual person's perfof  
l nance .  
We can find a similar pattern of performance 
among different NEs. Hmnans also performed 
more poorly for artifacts and very well for time 
and numeric expressions. 
Tile difference t)etween the best system per- 
formance and hulnan perfbrlnance is 7 or more 
F-measure, as opposed to the case in English 
where the top systems perform at, a level compa- 
rable or superior to human perfornlancc. There 
could be several reasons for this. One obvious 
reason is that we introduced a ditficult NE type, 
artifact~ which degrades the overall performance 
more for the system side than the lmman side. 
Also, the difficulty of identifying the expression 
boundaries may contribute to the difference. Fi- 
nally, we believe that the systems can possibly 
improve, as IREX was the first evaluation based 
project in Japanese, whereas in English there 
trove been 7 MUC's and tile technology may 
have matured by now. 
5 Acknowledgment 
We would like to thank all the participants of 
IREX projects. The project would never have 
been as successful as it was without the partic- 
ipants, all of whom were very cooperative and 
constructive. 
References 
IREX Committee 1999 Proceedings of the 
IREX Workshop 
Satoshi Sekine, Hitoshi Isahara. 2000 : "IREX: 
IR and IE Ewdnation Project in Japanese" 
Proceedings of the LREC-2000 coT@fence 
IREX Hornepage 
http: / /cs.nyu.edu/projects/proteus/ i rex 
MUC Homepage http:/ /www.muc.saic.com/ 
TREC Homepage trec.nist.gov/ 
NTCIR H(nneI)age 
httl)://www.rd.nacsis.ac.jp/f itcadm/index- 
en.html 
TS C ttomeI)age 
httl,:// al  ga.jaist.ac.jp:S000/tsc/ 
4 Conclusion 
We reported on tile NE task of tim IREX 
project. We first explained tile task and the 
definition, as well a.s the d~ta we created and 
the results. The analyses of the result were de- 
scribed, which include analysis of task ditficulty 
across the NE types and system types, analysis 
of domain dependency and comparison to hu- 
man performance. 
As this is one of the first projects of this type 
in ,Japan, we may have a lot to do ill the fu- 
ture and holmflflly tile results of tile project 
will be beneficial for fllture projects. As tile 
next step, II/,EX will be merged with a simi- 
lar project NTCIR (NTCIR Homepage, 2000) 
which places more eml)hasis on IR., with a newly 
created project for sumlnarization, TSC (TSC 
Homepage, 2000), and contilme this kind of ef- 
fort for tile fllture. 
1110 
Text Generation from Keywords
Kiyotaka Uchimoto? Satoshi Sekine? Hitoshi Isahara?
?Communications Research Laboratory
2-2-2, Hikari-dai, Seika-cho, Soraku-gun,
Kyoto, 619-0289, Japan
{uchimoto,isahara}@crl.go.jp
?New York University
715 Broadway, 7th floor
New York, NY 10003, USA
sekine@cs.nyu.edu
Abstract
We describe a method for generating sentences
from ?keywords? or ?headwords?. This method
consists of two main parts, candidate-text con-
struction and evaluation. The construction part
generates text sentences in the form of depen-
dency trees by using complementary informa-
tion to replace information that is missing be-
cause of a ?knowledge gap? and other missing
function words to generate natural text sen-
tences based on a particular monolingual cor-
pus. The evaluation part consists of a model
for generating an appropriate text when given
keywords. This model considers not only word
n-gram information, but also dependency infor-
mation between words. Furthermore, it consid-
ers both string information and morphological
information.
1 Introduction
Text generation is an important technique used
for applications like machine translation, sum-
marization, and human/computer dialogue. In
recent years, many corpora have become avail-
able, and have been used to generate natural
surface sentences. For example, corpora have
been used to generate sentences for language
model estimation in statistical machine trans-
lation. In such translation, given a source lan-
guage text, S, the translated text, T , in the
target language that maximizes the probabil-
ity P (T |S) is selected as the most appropri-
ate translation, T
best
, which is represented as
(Brown et al, 1990)
Tbest = argmaxTP (T |S)
= argmaxT (P (S|T ) ? P (T )) . (1)
In this equation, P (S|T ) represents the model
used to replace words or phrases in a source lan-
guage with those in the target language. It is
called a translation model. P (T ) represents a
language model that is used to reorder trans-
lated words or phrases into a natural order in
the target language. The input of the language
model is a ?bag of words,? and the goal of the
model is basically to reorder the words. At this
point, there is an assumption that natural sen-
tences can be generated by merely reordering
the words given by a translation model. To give
such a complete set of words, however, a trans-
lation model needs a large number of bilingual
corpora. If we could automatically complement
the words needed to generate natural sentences,
we would not have to collect the large number
of bilingual corpora required by a translation
model. In this paper, we assume that the role of
the translation model is not to give a complete
set of words that can be used to generate nat-
ural sentences, but to give a set of headwords
or center words that a speaker might want to
express, and describe a model that can provide
the complementary information needed to gen-
erate natural sentences by using a target lan-
guage corpus when given a set of headwords.
If we denote a set of headwords in a target
language as K, we can express Eq. (1) as
P (T |S) = P (K|S) ? P (T |K). (2)
P (K|S) in this equation represents a model
that gives a set of headwords in the target lan-
guage when given a source-language text sen-
tence. P (T |K) represents a model that gener-
ates text sentence T when given a set of head-
words, K. We call the model represented by
P (T |K) a text-generation model. In this paper,
we describe a text-generation model and a gen-
eration system that uses the model. Given a set
of headwords or keywords, our system outputs
the text sentence that maximizes P (T |K) as an
appropriate text sentence, T
best
:
Tbest = argmaxTP (T |K)
= argmaxT (P (K|T )? P (T )) . (3)
In this equation, we call the model represented
by P (K|T ) a keyword-production model. This
equation is equal to Eq. (1) when a source-
text sentence is replaced with a set of key-
words. Therefore, this model can be regarded
as a model that translates keywords into text
sentences. The model represented by P (T ) in
Eq. (3) is a language model used in statistical
machine translation. The n-gram model is the
most popular one used as a language model.
We assume that there is one extremely proba-
ble ordered set of morphemes and dependencies
between words that produce keywords, and we
express P (K|T ) as
P (K|T ) ? P (K,M,D|T )
= P (K|M,D, T ) ? P (D|M,T )? P (M |T ). (4)
In this equation, M denotes an ordered set of
morphemes and D denotes an ordered set of de-
pendencies in a sentence. P (K|M,D,T ) rep-
resents a keyword-production model. To es-
timate the models represented by P (D|M,T )
and P (M |T ), we use a dependency model and
a morpheme model, respectively, for the depen-
dency analysis and morphological analysis.
Statistical machine translation and example-
based machine translation require numerous
high-quality bilingual corpora. Interlingual ma-
chine translation and transfer-based machine
translation require a parser with high precision.
Therefore, these approaches to translation are
not practical if we do not have enough bilingual
corpora or a good parser. This is especially so if
the source text-sentences are incomplete or have
errors like those often found in OCR and speech-
recognition output. In these cases, however, if
we translate headwords into words in the target
language and generate sentences from the trans-
lated words by using our method, we should be
able to generate natural sentences from which
we can grasp the meaning of the source-text sen-
tences.
The text-generation model represented by
P (T |K) in Eq. (2) can be applied to various
tasks besides machine translation.
? Sentence-generation support system
for people with aphasia: About 300,000
people are reported to suffer from aphasia
in Japan, and 40% of them can select only
a few words to describe a picture. If candi-
date sentences can be generated from these
few words, it would help these people com-
municate with their families and friends.
? Support system for second language
writing: Beginners writing in second lan-
guage usually fined it easy to produce cen-
ter words or headwords, but often have dif-
ficulty generating complete sentences. If
several possible sentences could be gener-
ated from those words, it would help begin-
ners communicate with foreigners or study
second-language writing.
These are just two examples. We believe that
there are many other possible applications.
2 Overview of the Text-Generation
System
In this section, we give an overview of our sys-
tem for generating text sentences from given
keywords. As shown in Fig. 1, this system con-
sists of three parts: generation-rule acquisition,
candidate-text sentence construction, and eval-
uation.
Figure 1: Overview of the text-generation sys-
tem.
Given keywords, text sentences are generated
as follows.
1. During generation-rule acquisition, genera-
tion rules for each keyword are automati-
cally acquired.
2. Candidate-text sentences are constructed
during candidate-text construction by ap-
plying the rules acquired in the first
step. Each candidate-text sentence is rep-
resented by a graph or dependency tree.
3. Candidate-text sentences are ranked ac-
cording to their scores assigned during eval-
uation. The scores are calculated as a
probability estimated by using a keyword-
production model and a language model
that are trained with a corpus.
4. The candidate-text sentence that maxi-
mizes the score or the candidate-text sen-
tences whose scores are over a threshold
are selected as output. The system can
also output candidate-text sentences that
are ranked within the top N sentences.
In this paper, we assume that the target lan-
guage is Japanese. We define a keyword as the
headword of a bunsetsu. A bunsetsu is a phrasal
unit that usually consists of several content and
function words. We define the headword of a
bunsetsu as the rightmost content word in the
bunsetsu, and we define a content word as a
word whose part-of-speech is a verb, adjective,
noun, demonstrative, adverb, conjunction, at-
tribute, interjection, or undefined word. We
define the other words as function words. We
define formal nouns and auxiliary verbs ?SURU
(do)? and ?NARU (become)? as function words,
except when there are no other content words
in the same bunsetsu. Part-of-speech categories
follow those in the Kyoto University text corpus
(Version 3.0) (Kurohashi and Nagao, 1997), a
tagged corpus of the Mainichi newspaper.
Figure 2: Example of text generated from key-
words.
For example, given the set of keywords
?kanojo (she),? ?ie (house),? and ?iku (go),? as
shown in Fig. 2, our system retrieves sentences
including each word, and extracts each bunsetsu
that includes each word as a headword of the
bunsetsu. If there is no tagged corpus such
as the Kyoto University text corpus, each bun-
setsu can be extracted by using a morphological-
analysis system and a dependency-analysis sys-
tem such as JUMAN (Kurohashi and Nagao,
1999) and KNP (Kurohashi, 1998). Our system
then acquires generation rules as follows.
? ?kanojo (she)?kanojo (she) no (of)?
? ?kanojo (she)?kanojo (she) ga?
? ?ie (house)?ie (house) ni (to)?
? ?iku (go)?iku (go)?
? ?iku (go)?itta (went)?
The system next generates candidate bunsetsus
for each keyword and candidate-text sentences
in the form of dependency trees, such as ?Can-
didate 1? and ?Candidate 2? in Fig. 2, with
the assumption that there are dependencies be-
tween keywords. Finally, the candidate-text
sentences are ranked by their scores, calculated
by a text-generation model, and transformed
into surface sentences.
In this paper, we focus on the keyword-
production model represented by Eq. (4) and
assume that our system outputs sentences in the
form of dependency trees.
3 Candidate-Text Construction
We automatically acquire generation rules from
a monolingual target corpus at the time of gen-
erating candidate-text sentences. Generation
rules are restricted to those that generate bun-
setsus, and the generated bunsetsus must in-
clude each input keyword as a headword in the
bunsetsu. We then generate candidate-text sen-
tences in the form of dependency trees by simply
combining the bunsetsus generated by the rules.
The simple combination of generated bunsetsus
may produce semantically or grammatically in-
appropriate candidate-text sentences, but our
goal in this work was to generate a variety of
text sentences rather than a few fixed expres-
sions with high precision 1.
3.1 Generation-Rule Acquisition
Let us denote a set of keywords as KS and a
set of rules, each of which generates a bunsetsu
when given keyword k(?KS), as R
k
. We then
restrict r
k
(?R
k
) to those represented as
k ? hkm?. (5)
In this rule, h
k
represents the head morpheme
whose word is equal to keyword k; m? repre-
sents zero, one, or a series of morphemes that
are connected to h
k
in the same bunsetsu. Here,
we define a morpheme as consisting of a word
and its morphological information or grammat-
ical attribute, such as part-of-speech, and we
define a head morpheme as consisting of a head-
word and its grammatical attribute. By apply-
ing these rules, we generate bunsetsus from in-
put keywords.
3.2 Construction of Dependency Trees
Given keywords K = k1k2 . . . kn, candidate bun-
setsus are generated by applying the generation
rules described in Section 3.1. Next, by as-
suming dependency relationships between the
bunsetsus, candidate dependency trees are con-
structed. Dependencies between the bunsetsus
are restricted in that they must have the follow-
ing characteristics of Japanese dependencies:
1Note that 83.33% (3,973/4,768) of the headwords in
the newspaper articles appearing on January 17, 1995
were found in those appearing from January 1st to 16th.
However, only 21.82% (2,295/10,517) of the headword
dependencies in the newspaper articles appearing on
January 17th were found in those appearing from Jan-
uary 1st to 16th.
(i) Dependencies are directed from left to
right.
(ii) Dependencies do not cross.
(iii) All bunsetsus except the rightmost one de-
pend on only one other bunsetsu.
For example, when three keywords are given
and candidate bunsetsus including each keyword
are generated as b1, b2, and b3, the candidate de-
pendency trees are (b1 (b2 b3)) and ((b1 b2) b3)
if we do not reorder keywords, but 16 trees re-
sult if we consider the order of keywords to be
arbitrary.
4 Text-Generation Model
We next describe the model represented by Eq.
(4); that is, a keyword-production model, a
morpheme model that estimates how likely a
string is to be a morpheme, and a dependency
model. The goal of this model is to select
optimal sets of morphemes and dependencies
that can generate natural sentences. We imple-
mented these models within an maximum en-
tropy framework (Berger et al, 1996; Ristad,
1997; Ristad, 1998).
4.1 Keyword-Production Models
This section describes five keyword-production
models which are represented by P (K|M,D,T )
in Eq. (4). In these models, we define the set of
headwords whose frequency in the corpus is over
a certain threshold as a set of keywords, KS,
and we restrict the bunsetsus to those generated
by the generation rules represented in form (5).
We assume that all keywords are independent
and that k
i
corresponds to word w
j
(1 ? j ? m)
when text is given as a series of words w1 . . . wm.
1. trigram model
We assume that k
i
depends only on the two
anterior words w
j?1 and wj?2.
P (K|M,D, T ) =
n
?
i=1
P (ki|wj?1, wj?2).(6)
2. posterior trigram model
We assume that k
i
depends only on the two
posterior words w
j+1 and wj+2.
P (K|M,D, T ) =
n
?
i=1
P (ki|wj+1, wj+2).(7)
3. dependency bigram model
We assume that k
i
depends only on the two
rightmost words w
l
and w
l?1 in the right-
most bunsetsu that modifies the bunsetsu
including k
i
(see Fig. 3).
P (K|M,D, T ) =
n
?
i=1
P (ki|wl, wl?1). (8)
Figure 3: Relationship between keywords and
words in bunsetsus.
4. posterior dependency bigram model
We assume that k
i
depends only on the
headword, w
s
, and the word on its right,
w
s+1, in the bunsetsu that is modified by
the bunsetsu including k
i
(see Fig. 3).
P (K|M,D, T ) =
n
?
i=1
P (ki|ws, ws+1). (9)
5. dependency trigram model
We assume that k
i
depends only on the two
rightmost words w
l
and w
l?1 in the right-
most bunsetsu that modifies the bunsetsu,
and on the two rightmost words w
h
and
w
h?1 in the leftmost bunsetsu that modi-
fies the bunsetsu including k
i
(see Fig. 3).
P (K|M,D, T ) =
n
?
i=1
P (k
i
|w
l
, w
l?1
, w
h
, w
h?1
). (10)
4.2 Morpheme Model
Let us assume that there are l grammatical
attributes assigned to morphemes. We call a
model that estimates the likelihood that a given
string is a morpheme and has the grammatical
attribute j(1 ? j ? l) a morpheme model.
Let us also assume that morphemes in the or-
dered set of morphemes M depend on the pre-
ceding morphemes. We can then represent the
probability of M , given text T ; namely, P (M |T )
in Eq. (4):
P (M |T ) =
n
?
i=1
P (mi|m
i?1
1
, T ), (11)
where m
i
can be one of the grammatical at-
tributes assigned to each morpheme.
4.3 Dependency Model
Let us assume that dependencies d
i
(1 ? i ? n)
in the ordered set of dependencies D are inde-
pendent. We can then represent P (D|M,T ) in
Eq. (4) as
P (D|M,T ) =
n
?
i=1
P (di|M,T ). (12)
5 Evaluation
To evaluate our system we made 30 sets of
keywords, with three keywords in each set, as
shown in Table 1. A human subject selected
the sets from headwords that were found ten
Table 1: Input keywords and examples of sys-
tem output.
Input (Keywords) Ex. of system output
??? ?? ?? (???? (??? ????))
?? ?? ??? ((??? ???) ???)
?? ?? ?? ((??? ???) ??)
??? ??? ?? (???? (??? ???))
?? ?? ???
?? ?? ??? ((??? ???) ???)
??? ?? ?? ((???? ???) ??)
??? ?? ?? (???? (??? ?????))
?? ?? ??
? ?? ?? ((?? ???) ????)
?? ?? ??? ((??? ???) ???)
?? ?? ?? ((??? ?????) ??)
?? ?? ?? (??? (??? ????))
?? ??? ?? ((??? ????) ??)
?? ?? ??? (??? (??? ????))
?? ??? ?? (??? (???? ?????))
?? ?? ??
?? ?? ?? ((??? ???) ??????)
??? ?? ? ((???? ??????) ?)
?? ?? ??? ((??? ???) ??????)
?? ?? ???? ((??? ???) ??????)
?? ?? ??? ((??? ???) ???)
?? ?? ??
??? ?? ?? ((???? ????) ??????)
?? ??? ?? ((??? ????) ????)
?? ?? ??? (??? (??? ???))
?? ??? ???? (??? (???? ???????))
?? ?? ??? ((??? ???) ???)
?? ??? ???
?? ?? ???
times or more in the newspaper articles on Jan-
uary 1st in the Kyoto University text corpus
(Version 3.0) without looking at the articles.
We evaluated each model by the percentage
of outputs that were subjectively judged as ap-
propriate by one of the authors. We used two
evaluation standards.
? Standard 1: If the dependency tree ranked
first is semantically and grammatically ap-
propriate, it is judged as appropriate.
? Standard 2: If there is at least one depen-
dency tree that is ranked within the top
ten and is semantically and grammatically
appropriate, it is judged as appropriate.
We used headwords that were found five times
or more in the newspaper articles appearing
from January 1st to 16th in the Kyoto Univer-
sity text corpus and also found in those appear-
ing on January 1st as the set of headwords, KS.
For headwords that were not in KS, we added
their major part-of-speech categories to the set.
We trained our keyword-production models by
using 1,129 sentences (containing 10,201 head-
words) from newspaper articles appearing on
January 1st. We used a morpheme model and a
dependency model identical to those proposed
by Uchimoto et al (Uchimoto et al, 2001; Uchi-
moto et al, 1999; Uchimoto et al, 2000b). To
train the models, we used 8,835 sentences from
newspaper articles appearing from January 1st
to 9th in 1995. Generation rules were acquired
from newspaper articles appearing from Jan-
uary 1st to 16th. The total number of sentences
was 18,435.
First, we evaluated the outputs generated
when the rightmost two keywords, such as ??
? and??,? on each line of Table 1 were input.
Table 2 shows the results. KM1 through KM5
stand for the five keyword-production models
described in Section 4.1, and MM and DM stand
for the morpheme and the dependency models,
respectively. The symbol + indicates a combi-
nation of models. In the models without MM,
DM, or both, P (M |T ) and P (D|M,T ) were as-
sumed to be 1. We carried out additional ex-
periments with models that considered both the
anterior and posterior words, such as the com-
bination of KM1 and KM2 or KM3 and KM4.
The results were at most 16/30 by standard 1
and 24/30 by standard 1.
Table 2: Results of subjective evaluation.
Model Standard 1 Standard 2
KM1 (trigram) 13/30 28/30
KM1 + MM 21/30 28/30
KM1 + DM 12/30 28/30
KM1 + MM + DM 26/30 28/30
KM2 (posterior trigram) 6/30 15/30
KM2 + MM 8/30 20/30
KM2 + DM 10/30 20/30
KM2 + MM + DM 9/30 25/30
KM3 (dependency bigram) 13/30 29/30
KM3 + MM 26/30 29/30
KM3 + DM 14/30 28/30
KM3 + MM + DM 27/30 29/30
KM4 (posterior dependency bigram) 10/30 18/30
KM4 + MM 9/30 26/30
KM4 + DM 9/30 22/30
KM4 + MM + DM 13/30 27/30
KM5 (dependency trigram) 12/30 26/30
KM5 + MM 17/30 28/30
KM5 + DM 12/30 27/30
KM5 + MM + DM 26/30 28/30
The models KM1+MM+DM,
KM3+MM+DM, and KM5+MM+DM
achieved the best results, as shown in Ta-
ble 2. For models KM1, KM3, and KM5, the
results with MM and DM were significantly
better than those without MM and DM in
the evaluation by standard 1. We believe this
was because cases are more tightly connected
with verbs than with nouns, so models KM1,
KM3, and KM5, which learn the connection
between cases and verbs, can better rank the
candidate-text sentences that have a natural
connection between cases and verbs than other
candidates.
Next, we conducted experiments using the
30 sets of keywords shown in Table 1 as in-
puts. We used two keyword-production mod-
els: model KM3+MM+DM, which achieved
the best results in the first experiment, and
model KM5+MM+DM, which considers the
richest information. We assumed that the in-
put keyword order was appropriate and did not
reorder the keywords. The results for both
models were the same: 19/30 in the evalu-
ation by standard 1 and 24/30 in the eval-
uation by standard 2. The right column of
Table 1 shows examples of the system out-
put. For example, for the input ??? (syourai,
in the future), ??? (shin-shin-tou, the New
Frontier Party), and ???? (umareru, to
be born)?, the dependency tree ?(???
[syourai wa] (???? [shin-shin-tou ga] ?
?????? [umareru darou]))? (?The New
Frontier Party will be born in the future.?)
was generated. This output was automati-
cally complemented by the appropriate modal-
ity ????? (darou, will), which agrees with
the word ???? (syourai, in the future), as
well as by post-positional particles such as ?
?? (wa, case marker) and ??? (ga). For
the input ???? (gaikoku-jin, a foreigner), ?
? (kanyuu, to join), and ?? (zouka, to in-
crease)?, the dependency tree ?(( ????
[gaikokujin no] ???? [kanyuu sya ga]) ?
????? [zouka shite iru] )? (?Foreigner
members are increasing in number.?) was
generated. This output was complemented
not only by the modality expression ???
??? (shite iru, the progressive form) and
post-positional particles such as ??? (no, of)
and ??? (ga), but also by the suffix ???
(sya, person), and a compound noun ?????
(kanyuu sya, member) was generated naturally.
In six cases, though, we did not obtain appro-
priate outputs because the candidate-text sen-
tences were not appropriately ranked. Improv-
ing the back-off ability of the model by using
classified words or synonyms as features should
enable us to rank sentences more appropriately.
6 Related Work
Many statistical generation methods have been
proposed. In this section, we describe the differ-
ences between our method and several previous
methods.
Japanese words are often followed by post-
positional particles, such as ?ga? and ?wo?,
to indicate the subject and object of a sen-
tence. There are no corresponding words in
English. Instead, English words are preceded
by articles, ?the? and ?a,? to distinguish def-
inite and indefinite nouns, and so on, and in
this case there are no corresponding words in
Japanese. Knight et al proposed a way to
compensate for missing information caused by
a lack of language-dependent knowledge, or a
?knowledge gap? (Knight and Hatzivassiloglou,
1995; Langkilde and Knight, 1998a; Langkilde
and Knight, 1998b). They use semantic expres-
sions as input, whereas we use keywords. Also,
they construct candidate-text sentences or word
lattices by applying rules, and apply their lan-
guage model, an n-gram model, to select the
most appropriate surface text. While we can-
not use their rules to generate candidate-text
sentences when given keywords, we can apply
their language model to our system to generate
surface-text sentences from candidate-text sen-
tences in the form of dependency trees. We can
also apply the formalism proposed by Langkilde
(Langkilde, 2000) to express the candidate-text
sentences.
Bangalore and Rambow proposed a method
to generate candidate-text sentences in the form
of trees (Bangalore and Rambow, 2000). They
consider dependency information when deriving
trees by using XTAG grammar, but they as-
sume that the input contains dependency infor-
mation. Our system generates candidate-text
sentences without relying on dependency infor-
mation in the input, and our model estimates
the dependencies between keywords.
Ratnaparkhi proposed models to generate
text from semantic attributes (Ratnaparkhi,
2000). The input of these models is semantic
attributes. His models are similar to ours if the
semantic attributes are replaced with keywords.
However, his models need a training corpus in
which certain words are replaced with seman-
tic attributes. Although our model also needs
a training corpus, the corpus can be automati-
cally created by using a morphological analyzer
and a dependency analyzer, both of which are
readily available.
Humphreys et al proposed using mod-
els developed for sentence-structure analysis to
rank candidate-text sentences (Humphreys et
al., 2001). As well as models developed for
sentence-structure analysis, we also use those
developed for morphological analysis and found
that these models contribute to the generation
of appropriate text.
Berger and Lafferty proposed a language
model for information retrieval (Berger and Laf-
ferty, 1999). Their concept is similar to that of
our model, which can be regarded as a model
that translates keywords into text, while their
model can be regarded as one that translates
query words into documents. However, the pur-
pose of their model is different: their goal is to
retrieve text that already exists while ours is to
generate new text.
7 Conclusion
We have described a method for generating sen-
tences from ?keywords? or ?headwords?. This
method consists of two main parts, candidate-
text construction and evaluation.
1. The construction part generates text sen-
tences in the form of dependency trees by
providing complementary information to
replace that missing due to a ?knowledge
gap? and other missing function words, and
thus generates natural text sentences based
on a particular monolingual corpus.
2. The evaluation part consists of a model
for generating an appropriate text sentence
when given keywords. This model consid-
ers the dependency information between
words as well as word n-gram informa-
tion. Furthermore, the model considers
both string and morphological information.
If a language model, such as a word n-gram
model, is applied to the generated-text sen-
tences in the form of dependency trees, an
appropriate surface-text sentence is generated.
The word-order model proposed by Uchimoto et
al. can also generate surface text in a natural
order (Uchimoto et al, 2000a).
There are several possible directions for our
future research. In particular,
? We would like to expand the generation
rules. We restricted the generation rules
automatically acquired from a corpus to
those that generate a bunsetsu. To gener-
ate a greater variety of candidate-text sen-
tences, we would like to expand the rules
that can generate a dependency tree. Ex-
pansion would lead to complementing with
content words as well as function words.
We also would like to prepare default rules
or to classify words into several classes
when no sentences including the keywords
are found in the target corpus.
? Some of the N-best text sentences gener-
ated by our system are semantically and
grammatically unnatural. To remove such
sentences from among the candidate-text
sentences, we must enhance our model so
that it can consider more information, such
as classified words or those in a thesaurus.
? We restricted keywords to the headwords or
rightmost content words in the bunsetsus.
We would like to expand the definition of
keywords to other content words and to
synonyms of the keywords.
Acknowledgments
We thank the Mainichi Newspapers for permis-
sion to use their data. We also thank Kimiko
Ohta, Hiroko Inui, Takehito Utsuro, Man-
abu Okumura, Akira Ushioda, Jun?ichi Tsujii,
Kiyosi Yasuda, and Masahisa Ohta for their
beneficial comments during the progress of this
work.
References
S. Bangalore and O. Rambow. 2000. Exploiting a Probabilis-
tic Hierarchical Model for Generation. In Proceedings of
the COLING, pages 42?48.
A. Berger and J. Lafferty. 1999. Information Retrieval as
Statistical Translation. In Proceedings of the ACM SIGIR,
pages 222?229.
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996.
A Maximum Entropy Approach to Natural Language Pro-
cessing. Computational Linguistics, 22(1):39?71.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra,
F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin.
1990. A Statistical Approach to Machine Translation.
Computational Linguistics, 16(2):79?85.
K. Humphreys, M. Calcagno, and D. Weise. 2001. Reusing a
Statistical Language Model for Generation. In Proceedings
of the EWNLG.
K. Knight and V. Hatzivassiloglou. 1995. Two-Level, Many-
Paths Generation. In Proceedings of the ACL, pages 252?
260.
S. Kurohashi and M. Nagao. 1997. Building a Japanese
Parsed Corpus while Improving the Parsing System. In
Proceedings of the NLPRS, pages 451?456.
S. Kurohashi and M. Nagao, 1999. Japanese Morphological
Analysis System JUMAN Version 3.61. Department of
Informatics, Kyoto University.
S. Kurohashi, 1998. Japanese Dependency/Case Structure
Analyzer KNP Version 2.0b6. Department of Informatics,
Kyoto University.
I. Langkilde and K. Knight. 1998a. Generation that Exploits
Corpus-Based Statistical Knowledge. In Proceedings of the
COLING-ACL, pages 704?710.
I. Langkilde and K. Knight. 1998b. The Practical Value of
N-grams in Generation. In Proceedings of the INLG.
I. Langkilde. 2000. Forest-Based Statistical Sentence Gener-
ation. In Proceedings of the NAACL, pages 170?177.
A. Ratnaparkhi. 2000. Trainable Methods for Surface Natu-
ral Language Generation. In Proceedings of the NAACL,
pages 194?201.
E. S. Ristad. 1997. Maximum Entropy Modeling for Natural
Language. ACL/EACL Tutorial Program, Madrid.
E. S. Ristad. 1998. Maximum Entropy Modeling Toolkit,
Release 1.6 beta. http://www.mnemonic.com/software/
memt.
K. Uchimoto, S. Sekine, and H. Isahara. 1999. Japanese De-
pendency Structure Analysis Based on Maximum Entropy
Models. In Proceedings of the EACL, pages 196?203.
K. Uchimoto, M. Murata, Q. Ma, S. Sekine, and H. Isahara.
2000a. Word Order Acquisition from Corpora. In Proceed-
ings of the COLING, pages 871?877.
K. Uchimoto, M. Murata, S. Sekine, and H. Isahara. 2000b.
Dependency Model Using Posterior Context. In Proceed-
ings of the IWPT, pages 321?322.
K. Uchimoto, S. Sekine, and H. Isahara. 2001. The Unknown
Word Problem: a Morphological Analysis of Japanese Us-
ing Maximum Entropy Aided by a Dictionary. In Proceed-
ings of the EMNLP, pages 91?99.
Morphological Analysis of The Spontaneous Speech Corpus
Kiyotaka Uchimoto?, Chikashi Nobata?, Atsushi Yamada?,
Satoshi Sekine?, and Hitoshi Isahara?
?Communications Research Laboratory
2-2-2, Hikari-dai, Seika-cho, Soraku-gun,
Kyoto, 619-0289, Japan
{uchimoto,nova,ark,isahara}@crl.go.jp
?New York University
715 Broadway, 7th floor
New York, NY 10003, USA
sekine@cs.nyu.edu
Abstract
This paper describes a project tagging a sponta-
neous speech corpus with morphological infor-
mation such as word segmentation and parts-of-
speech. We use a morphological analysis system
based on a maximum entropy model, which is
independent of the domain of corpora. In this
paper we show the tagging accuracy achieved by
using the model and discuss problems in tagging
the spontaneous speech corpus. We also show
that a dictionary developed for a corpus on a
certain domain is helpful for improving accu-
racy in analyzing a corpus on another domain.
1 Introduction
In recent years, systems developed for analyz-
ing written-language texts have become consid-
erably accurate. This accuracy is largely due
to the large amounts of tagged corpora and the
rapid progress in the study of corpus-based nat-
ural language processing. However, the accu-
racy of the systems developed for written lan-
guage is not always high when these same sys-
tems are used to analyze spoken-language texts.
The reason for this remaining inaccuracy is due
to several differences between the two types of
languages. For example, the expressions used
in written language are often quite different
from those in spoken language, and sentence
boundaries are frequently ambiguous in spoken
language. The ?Spontaneous Speech: Corpus
and Processing Technology? project was imple-
mented in 1999 to overcome this problem. Spo-
ken language includes both monologue and dia-
logue texts; the former (e.g. the text of a talk)
was selected as a target of the project because it
was considered to be appropriate to the current
level of study on spoken language.
Tagging the spontaneous speech corpus with
morphological information such as word seg-
mentation and parts-of-speech is one of the
goals of the project. The tagged corpus is help-
ful for us in making a language model in speech
recognition as well as for linguists investigat-
ing distribution of morphemes in spontaneous
speech. For tagging the corpus with morpholog-
ical information, a morphological analysis sys-
tem is needed. Morphological analysis is one of
the basic techniques used in Japanese sentence
analysis. A morpheme is a minimal grammat-
ical unit, such as a word or a suffix, and mor-
phological analysis is the process of segment-
ing a given sentence into a row of morphemes
and assigning to each morpheme grammatical
attributes such as part-of-speech (POS) and in-
flection type. One of the most important prob-
lems in morphological analysis is that posed by
unknown words, which are words found in nei-
ther a dictionary nor a training corpus. Two
statistical approaches have been applied to this
problem. One is to find unknown words from
corpora and put them into a dictionary (e.g.,
(Mori and Nagao, 1996)), and the other is to
estimate a model that can identify unknown
words correctly (e.g., (Kashioka et al, 1997;
Nagata, 1999)). Uchimoto et al used both
approaches. They proposed a morphological
analysis method based on a maximum entropy
(M.E.) model (Uchimoto et al, 2001). We used
their method to tag a spontaneous speech cor-
pus. Their method uses a model that can not
only consult a dictionary but can also identify
unknown words by learning certain characteris-
tics. To learn these characteristics, we focused
on such information as whether or not a string
is found in a dictionary and what types of char-
acters are used in a string. The model esti-
mates how likely a string is to be a morpheme.
This model is independent of the domain of cor-
pora; in this paper we demonstrate that this is
true by applying our model to the spontaneous
speech corpus, Corpus of Spontaneous Japanese
(CSJ) (Maekawa et al, 2000). We also show
that a dictionary developed for a corpus on a
certain domain is helpful for improving accu-
racy in analyzing a corpus on another domain.
2 A Morpheme Model
This section describes a model which estimates
how likely a string is to be a morpheme. We
implemented this model within an M.E. frame-
work.
Given a tokenized test corpus, the problem of
Japanese morphological analysis can be reduced
to the problem of assigning one of two tags to
each string in a sentence. A string is tagged
with a 1 or a 0 to indicate whether or not it is
a morpheme. When a string is a morpheme, a
grammatical attribute is assigned to it. The 1
tag is thus divided into the number, n, of gram-
matical attributes assigned to morphemes, and
the problem is to assign an attribute (from 0
to n) to every string in a given sentence. The
(n + 1) tags form the space of ?futures? in the
M.E. formulation of our problem of morpholog-
ical analysis. The M.E. model enables the com-
putation of P (f |h) for any future f from the
space of possible futures, F , and for every his-
tory, h, from the space of possible histories, H.
The computation of P (f |h) in any M.E. model
is dependent on a set of ?features? which would
be helpful in making a prediction about the fu-
ture. Like most current M.E. models in com-
putational linguistics, our model is restricted to
those features which are binary functions of the
history and future. For instance, one of our fea-
tures is
g(h, f) =
?
?
?
?
?
1 : if has(h, x) = true,
x = ?POS(?1)(Major) : verb,??
& f = 1
0 : otherwise.
(1)
Here ?has(h,x)? is a binary function that re-
turns true if the history h has feature x. In our
experiments, we focused on such information as
whether or not a string is found in a dictionary,
the length of the string, what types of characters
are used in the string, and what part-of-speech
the adjacent morpheme is.
Given a set of features and some training
data, the M.E. estimation process produces a
model, which is represented as follows (Berger
et al, 1996; Ristad, 1997; Ristad, 1998):
P (f |h) =
?
i
?
g
i
(h,f)
i
Z
?
(h)
(2)
Z
?
(h) =
?
f
?
i
?
g
i
(h,f)
i
. (3)
We define a model which estimates the like-
lihood that a given string is a morpheme and
has the grammatical attribute i(1 ? i ? n) as a
morpheme model. This model is represented by
Eq. (2), in which f can be one of (n + 1) tags
from 0 to n.
Given a sentence, it is divided into mor-
phemes, and a grammatical attribute is assigned
to each morpheme so as to maximize the sen-
tence probability estimated by our morpheme
model. Sentence probability is defined as the
product of the probabilities estimated for a par-
ticular division of morphemes in a sentence. We
use the Viterbi algorithm to find the optimal set
of morphemes in a sentence.
3 Experiments and Discussion
3.1 Experimental Conditions
We used the spontaneous speech corpus, CSJ,
which is a tagged corpus of transcriptions of
academic presentations and simulated public
speech. Simulated public speech is short speech
spoken specifically for the corpus by paid non-
professional speakers. For training, we used
805,954 morphemes from the corpus, and for
testing, we used 68,315 morphemes from the
corpus. Since there are no boundaries between
sentences in the corpus, we used two types of
boundaries, utterance boundaries, which are au-
tomatically detected at the place where a pause
of 200 ms or longer emerges in the CSJ, and
sentence boundaries assigned by the sentence
boundary identification system, which is based
on hand-crafted rules which use the pauses as
a clue. In the CSJ, fillers and disfluencies are
marked with tags (F) and (D). In the experi-
ments, we did not use those tags. Thus the in-
put sentences for testing are character strings
without any tags. The output is a sequence
of morphemes with grammatical attributes. As
the grammatical attributes, we define the part-
of-speech categories in the CSJ. There are 12
major categories. Therefore, the number of
grammatical attributes is 12, and f in Eq. (2)
can be one of 13 tags from 0 to 12.
Given a sentence, for every string consist-
ing of five or fewer characters and every string
appearing in a dictionary, whether or not the
string is a morpheme was determined and then
the grammatical attribute of each string deter-
mined to be a morpheme was identified and
assigned to that string. We collected all mor-
phemes from the training corpus except dis-
fluencies and used them as dictionary entries.
We denote the entries with a Corpus dictionary.
The maximum length for a morpheme was set
at five because morphemes consisting of six or
more characters are mostly compound words or
words consisting of katakana characters. We as-
sumed that compound words that do not appear
in the dictionary can be divided into strings con-
sisting of five or fewer characters because com-
pound words tend not to appear in dictionar-
ies. Katakana strings that are not found in the
dictionary were assumed to be included in the
dictionary as an entry having the part-of-speech
?Unknown(Major), Katakana(Minor).? An op-
timal set of morphemes in a sentence is searched
for by employing the Viterbi algorithm. The
assigned part-of-speech in the optimal set is se-
lected from all the categories of the M.E. model
except the one in which the string is not a mor-
pheme.
The features used in our experiments are
listed in Table 1. Each feature consists of a
type and a value, which are given in the rows of
the table. The features are basically some at-
tributes of the morpheme itself or attributes of
the morpheme to the left of it. We used the fea-
tures found three or more times in the training
corpus. The notations ?(0)? and ?(-1)? used in
the feature type column in Table 1 respectively
indicate a target string and the morpheme to
the left of it.
The terms used in the table are as follows:
String: Strings appearing as a morpheme three
or more times in the training corpus
Substring: Characters used in a string.
?(Left1)? and ?(Right1)? respectively rep-
resent the leftmost and rightmost charac-
ters of a string. ?(Left2)? and ?(Right2)?
respectively represent the leftmost and
rightmost character bigrams of a string.
Dic: Entries in the Corpus dictionary. As mi-
nor categories we used inflection types such
as a basic form as well as minor part-of-
speech categories. ?Major&Minor? indi-
cates possible combinations between major
and minor part-of-speech categories. When
the target string is in the dictionary, the
part-of-speech attached to the entry corre-
sponding to the string is used as a feature
value. If an entry has two or more parts-
of-speech, the part-of-speech which leads to
the highest probability in a sentence esti-
mated from our model is selected as a fea-
ture value.
Length: Length of a string
TOC: Types of characters used in a string.
?(Beginning)? and ?(End)?, respectively,
represent the leftmost and rightmost char-
acters of a string. When a string con-
sists of only one character, the ?(Begin-
ning)? and ?(End)? are the same character.
?TOC(0)(Transition)? represents the tran-
sition from the leftmost character to the
rightmost character in a string. ?TOC(-
1)(Transition)? represents the transition
from the rightmost character in the adja-
cent morpheme on the left to the leftmost
character in the target string. For example,
when the adjacent morpheme on the left
is ??? (sensei, teacher)? and the target
string is ?? (ni, case marker),? the feature
value ?Kanji?Hiragana? is selected.
POS: Part-of-speech.
3.2 Results and Discussion
Results of the morphological analysis obtained
by our method are shown in Table 2. Recall
is the percentage of morphemes in the test cor-
pus whose segmentation and major POS tag are
identified correctly. Precision is the percentage
of all morphemes identified by the system that
are identified correctly. The F-measure is de-
fined by the following equation.
F ? measure =
2 ? Recall ? Precision
Recall + Precision
This result shows that there is no significant
difference between accuracies obtained by us-
ing two types of sentence boundaries. However,
we found that the errors that occurred around
utterance boundaries were reduced in the re-
sult obtained with sentence boundaries assigned
by the sentence boundary identification system.
This shows that there is a high possibility that
we can achieve better accuracy if we use bound-
aries assigned by the sentence boundary identi-
fication system as sentence boundaries and if we
use utterance boundaries as features.
In these experiments, we used only the en-
tries with a Corpus dictionary. Next we show
the experimental results with dictionaries de-
veloped for a corpus on a certain domain. We
added to the Corpus dictionary all the approx-
imately 200,000 entries of the JUMAN dictio-
nary (Kurohashi and Nagao, 1999). We also
added the entries of a dictionary developed by
ATR. We call it the ATR dictionary.
Results obtained with each dictionary or each
combination of dictionaries are shown in Ta-
ble 3. In this table, OOV indicates Out-of-
Vocabulary rates. The accuracy obtained with
the JUMAN dictionary or the ATR dictionary
was worse than the accuracy obtained without
those dictionaries. This is because the segmen-
Table 1: Features.
Feature number Feature type Feature value (Number of value)
1 String(0) (223,457)
2 String(-1) (20,769)
3 Substring(0)(Left1) (2,492)
4 Substring(0)(Right1) (2,489)
5 Substring(0)(Left2) (74,046)
6 Substring(0)(Right2) (73,616)
7 Substring(-1)(Left1) (2,237)
8 Substring(-1)(Right1) (2,489)
9 Substring(-1)(Left2) (12,726)
10 Substring(-1)(Right2) (12,241)
11 Dic(0)(Major) Noun, Verb, Adj, . . . Undefined (13)
12 Dic(0)(Minor) Common noun, Topic marker, Basic form. . . (223)
13 Dic(0)(Major&Minor) Noun&Common noun, Verb&Basic form, . . . (239)
14 Length(0) 1, 2, 3, 4, 5, 6 or more (6)
15 Length(-1) 1, 2, 3, 4, 5, 6 or more (6)
16 TOC(0)(Beginning) Kanji, Hiragana, Number, Katakana, Alphabet (5)
17 TOC(0)(End) Kanji, Hiragana, Number, Katakana, Alphabet (5)
18 TOC(0)(Transition) Kanji?Hiragana, Number?Kanji, Katakana?Kanji, . . . (25)
19 TOC(-1)(End) Kanji, Hiragana, Number, Katakana, Alphabet (5)
20 TOC(-1)(Transition) Kanji?Hiragana, Number?Kanji, Katakana?Kanji, . . . (18)
21 POS(-1) Verb, Adj, Noun, . . . (12)
22 Comb(1,21) Combinations Feature 1 and 21 (142,546)
23 Comb(1,2,21) Combinations Feature 1, 2 and 21 (216,431)
24 Comb(1,13,21) Combinations Feature 1, 13 and 21 (29,876)
25 Comb(1,2,13,21) Combinations Feature 1, 2, 13 and 21 (158,211)
26 Comb(11,21) Combinations Feature 11 and 21 (156)
27 Comb(12,21) Combinations Feature 12 and 21 (1,366)
28 Comb(13,21) Combinations Feature 13 and 21 (1,518)
Table 2: Results of Experiments (Segmentation and major POS tagging).
Boundary Recall Precision F-measure
utterance 93.97% (64,198/68,315) 93.25% (64,198/68,847) 93.61
sentence 93.97% (64,195/68,315) 93.18% (64,195/68,895) 93.57
tation of morphemes and the definition of part-
of-speech categories in the JUMAN and ATR
dictionaries are different from those in the CSJ.
Given a sentence, for every string consisting
of five or fewer characters as well as every string
appearing in a dictionary, whether or not the
string is a morpheme was determined by our
morpheme model. However, we speculate that
we can ignore strings consisting of two or more
characters when they are not found in the dic-
tionary when OOV is low. Therefore, we carried
out the additional experiments ignoring those
strings. In the experiments, given a sentence,
for every string consisting of one character and
every string appearing in a dictionary, whether
or not the string is a morpheme is determined
by our morpheme model. Results obtained un-
der this condition are shown in Table 4. We
compared the accuracies obtained with dictio-
naries including the Corpus dictionary, whose
OOVs are relatively low. The accuracies ob-
tained with the additional dictionaries increased
while those obtained only with the Corpus dic-
tionary decreased. These results show that a
dictionary whose OOV in the test corpus is low
contributes to increasing the accuracy when ig-
noring the possibility that strings that consist
of two or more characters and are not found in
the dictionary become a morpheme.
These results show that a dictionary devel-
oped for a corpus on a certain domain can be
used to improve accuracy in analyzing a corpus
on another domain.
The accuracy in segmentation and major
POS tagging obtained for spontaneous speech
was worse than the approximately 95% obtained
for newspaper articles. We think the main rea-
son for this is the errors and the inconsistency
of the corpus, and the difficulty in recognizing
characteristic expressions often used in spoken
language such as fillers, mispronounced words,
and disfluencies. The inconsistency of the cor-
pus is due to the way the corpus was made, i.e.,
completely by human beings, and it is also due
Table 3: Results of Experiments (Segmentation and major POS tagging).
Dictionary Boundary Recall Precision F OOV
Corpus utterance 92.64% (63,288/68,315) 91.83% (63,288/68,917) 92.24 1.84%
Corpus sentence 92.61% (63,265/68,315) 91.79% (63,265/68,923) 92.20 1.84%
JUMAN utterance 90.28% (61,676/68,315) 90.07% (61,676/68,478) 90.17 6.13%
JUMAN sentence 90.33% (61,710/68,315) 90.22% (61,710/68,403) 90.27 6.13%
ATR utterance 89.80% (61,348/68,315) 90.12% (61,348/68,073) 89.96 8.14%
ATR sentence 89.96% (61,453/68,315) 90.30% (61,453/68,057) 90.13 8.14%
Corpus+JUMAN utterance 92.03% (62,872/68,315) 91.77% (62,872/68,507) 91.90 0.52%
Corpus+JUMAN sentence 92.09% (62,913/68,315) 91.80% (62,913/68,534) 91.95 0.52%
Corpus+ATR utterance 92.35% (63,086/68,315) 92.03% (63,086/68,547) 92.19 0.64%
Corpus+ATR sentence 92.30% (63,057/68,315) 91.94% (63,057/68,585) 92.12 0.64%
JUMAN+ATR utterance 91.60% (62,579/68,315) 91.57% (62,579/68,339) 91.59 4.61%
JUMAN+ATR sentence 91.66% (62,618/68,315) 91.67% (62,618/68,311) 91.66 4.61%
Corpus+JUMAN+ATR utterance 91.72% (62,658/68,315) 91.66% (62,658/68,357) 91.69 0.47%
Corpus+JUMAN+ATR sentence 91.72% (62,657/68,315) 91.62% (62,657/68,391) 91.67 0.47%
? For training 1/5 of all the training corpus (163,796 morphemes) was used.
Table 4: Results of Experiments (Segmentation and major POS tagging).
Dictionary Boundary Recall Precision F OOV
Corpus utterance 92.80% (63,395/68,315) 90.47% (63,395/70,075) 91.62 1.84%
Corpus sentence 92.71% (63,333/68,315) 90.48% (63,333/70,000) 91.58 1.84%
Corpus+JUMAN utterance 92.45% (63,154/68,315) 91.60% (63,154/68,942) 92.02 0.52%
Corpus+JUMAN sentence 92.48% (63,179/68,315) 91.71% (63,179/68,893) 92.09 0.52%
Corpus+ATR utterance 92.91% (63,474/68,315) 91.81% (63,474/69,137) 92.36 0.64%
Corpus+ATR sentence 92.75% (63,361/68,315) 91.76% (63,361/69,053) 92.25 0.64%
Corpus+JUMAN+ATR utterance 92.30% (63,055/68,315) 91.57% (63,055/68,858) 91.94 0.47%
Corpus+JUMAN+ATR sentence 92.28% (63,039/68,315) 91.55% (63,039/68,860) 91.91 0.47%
? For training 1/5 of all the training corpus (163,796 morphemes) was used.
to the definition of morphemes. Several incon-
sistencies in the test corpus existed, such as: ?
?? (tokyo, Noun)(Tokyo), ? (to, Other)(the
Metropolis), ? (ritsu, Other)(founded), ?
? (daigaku, Noun)(university),? and ???
(toritsu, Noun)(metropolitan), ?? (daigaku,
Noun)(university).? Both of these are the
names representing the same university. The
???? is partitioned into two in the first one
while it is not partitioned into two in the second
one according to the definition of morphemes.
When such inconsistencies in the corpus exist, it
is difficult for our model to discriminate among
these inconsistencies because we used only bi-
gram information as features. To achieve bet-
ter accuracy, therefore, we need to use trigram
or longer information. To correctly recognize
characteristic expressions often used in spoken
language, we plan to extract typical patterns
used in the expressions, to generalize the pat-
terns manually, and to generate possible expres-
sions using the generalized patterns, and finally,
to add such patterns to the dictionary. We also
plan to expand our model to skip fillers, mispro-
nounced words, and disfluencies because those
expressions are randomly inserted into text and
it is impossible to learn the connectivity be-
tween those randomly inserted expressions and
others.
References
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996.
A Maximum Entropy Approach to Natural Language Pro-
cessing. Computational Linguistics, 22(1):39?71.
H. Kashioka, S. G. Eubank, and E. W. Black. 1997. Decision-
Tree Morphological Analysis without a Dictionary for
Japanese. In Proceedings of the NLPRS, pages 541?544.
S. Kurohashi and M. Nagao, 1999. Japanese Morphological
Analysis System JUMAN Version 3.61. Department of
Informatics, Kyoto University.
K. Maekawa, H. Koiso, S. Furui, and H. Isahara. 2000. Spon-
taneous Speech Corpus of Japanese. In Proceedings of the
LREC, pages 947?952.
S. Mori and M. Nagao. 1996. Word Extraction from Cor-
pora and Its Part-of-Speech Estimation Using Distribu-
tional Analysis. In Proceedings of the COLING, pages
1119?1122.
M. Nagata. 1999. A Part of Speech Estimation Method
for Japanese Unknown Words using a Statistical Model
of Morphology and Context. In Proceedings of the ACL,
pages 277?284.
E. S. Ristad. 1997. Maximum Entropy Modeling for Natural
Language. ACL/EACL Tutorial Program, Madrid.
E. S. Ristad. 1998. Maximum Entropy Modeling Toolkit,
Release 1.6 beta. http://www.mnemonic.com/software/
memt.
K. Uchimoto, S. Sekine, and H. Isahara. 2001. The Unknown
Word Problem: a Morphological Analysis of Japanese Us-
ing Maximum Entropy Aided by a Dictionary. In Proceed-
ings of the EMNLP, pages 91?99.
Named Entity Discovery Using Comparable News Articles
Yusuke SHINYAMA and Satoshi SEKINE
Computer Science Department
New York University
715, Broadway, 7th Floor
New York, NY, 10003
yusuke@cs.nyu.edu, sekine@cs.nyu.edu
Abstract
In this paper we describe a way to discover
Named Entities by using the distribution of
words in news articles. Named Entity recog-
nition is an important task for today?s natural
language applications, but it still suffers from
data sparseness. We used an observation that a
Named Entity is likely to appear synchronously
in several news articles, whereas a common
noun is less likely. Exploiting this characteris-
tic, we successfully obtained rare Named Enti-
ties with 90% accuracy just by comparing time
series distributions of a word in two newspa-
pers. Although the achieved recall is not suf-
ficient yet, we believe that this method can be
used to strengthen the lexical knowledge of a
Named Entity tagger.
1 Introduction
Recently, Named Entity (NE) recognition has been
getting more attention as a basic building block for
practical natural language applications. A Named
Entity tagger identifies proper expressions such as
names, locations and dates in sentences. We are
trying to extend this to an Extended Named Entity
tagger, which additionally identifies some common
nouns such as disease names or products. We be-
lieve that identifying these names is useful for many
applications such as information extraction or ques-
tion answering (Sekine et al, 2002).
Normally a Named Entity tagger uses lexical or
contextual knowledge to spot names which appear
in documents. One of the major problem of this task
is its data sparseness. Names appear very frequently
in regularly updated documents such as news arti-
cles or web pages. They are, however, much more
varied than common nouns, and changing contin-
uously. Since it is hard to construct a set of pre-
defined names by hand, usually some corpus based
approaches are used for building such taggers.
However, as Zipf?s law indicates, most of the
names which occupy a large portion of vocabulary
are rarely used. So it is hard for Named Entity
tagger developers to keep up with a contemporary
set of words, even though a large number of docu-
ments are provided for learning. There still might
be a ?wild? noun which doesn?t appear in the cor-
pora. Several attempts have been made to tackle
this problem by using unsupervised learning tech-
niques, which make vast amount of corpora avail-
able to use. (Strzalkowski and Wang, 1996) and
(Collins and Singer, 1999) tried to obtain either lex-
ical or contextual knowledge from a seed given by
hand. They trained the two different kind of knowl-
edge alternately at each iteration of training. (Yan-
garber et al, 2002) tried to discover names with a
similar method. However, these methods still suffer
in the situation where the number of occurrences of
a certain name is rather small.
2 Synchronicity of Names
In this paper we propose another method to
strengthen the lexical knowledge for Named Entity
tagging by using synchronicity of names in com-
parable documents. One can view a ?comparable?
document as an alternative expression of the same
content. Now, two document sets where each doc-
ument of one set is associated with one in the other
set, is called a ?comparable? corpus. A compara-
ble corpus is less restricted than a parallel corpus
and usually more available. Several different news-
papers published on the same day report lots of the
same events, therefore contain a number of compa-
rable documents. One can also take another view of
a comparable corpus, which is a set of paraphrased
documents. By exploiting this feature, one can ex-
tract paraphrastic expressions automatically from
parallel corpora (Barzilay and McKeown, 2001) or
comparable corpora (Shinyama and Sekine, 2003).
Named Entities in comparable documents have
one notable characteristic: they tend to be preserved
across comparable documents because it is gener-
ally difficult to paraphrase names. We think that
it is also hard to paraphrase product names or dis-
ease names, so they will also be preserved. There-
fore, if one Named Entity appears in one document,
it should also appear in the comparable document.
 0
 5
 10
 15
 20
 25
 30
 0  50  100  150  200  250  300  350  400
Fre
que
ncy
Date
LATWPNYTREUTE
The occurrence of the word ?yigal?
 0
 20
 40
 60
 80
 100
 120
 140
 0  50  100  150  200  250  300  350  400
Fre
que
ncy
Date
LATWPNYTREUTE
The occurrence of the word ?killed?
Figure 1: The occurrence of two words in 1995
Consequently, if one has two sets of documents
which are associated with each other, the distribu-
tion of a certain name in one document set should
look similar to the distribution of the name in the
other document set.
We tried to use this characteristic of Named En-
tities to discover rare names from comparable news
articles. We particularly focused on the time series
distribution of a certain word in two newspapers.
We hypothesized that if a Named Entity is used
in two newspapers, it should appear in both news-
papers synchronously, whereas other words don?t.
Since news articles are divided day by day, it is easy
to obtain the time series distribution of words ap-
pearing in each newspaper.
Figure 2 shows the time series distribution of the
two words ?yigal? and ?killed?, which appeared in
several newspapers in 1995. The word ?yigal? (the
name of the man who killed Israeli Prime Minister
Yitzhak Rabin on Nov. 7, 1995) has a clear spike.
There were a total of 363 documents which included
the word that year and its occurrence is synchronous
between the two newspapers. In contrast, the word
?killed?, which appeared in 21591 documents, is
spread over all the year and has no clear character-
istic.
3 Experiment
To verify our hypothesis, we conducted an experi-
ment to measure the correlation between the occur-
rence of Named Entity and its similarity of time se-
ries distribution between two newspapers.
First, we picked a rare word, then obtained its
document frequency which is the number of articles
which contain the word. Since newspaper articles
are provided separately day by day, we sampled the
document frequency for each day. These numbers
form, for one year for example, a 365-element in-
teger vector per newspaper. The actual number of
news articles is oscillating weekly, however, we nor-
malized this by dividing the number of articles con-
taining the word by the total number of all articles
on that day. At the end we get a vector of fractions
which range from 0.0 to 1.0.
Next we compared these vectors and calculated
the similarity of their time series distributions across
different news sources. Our basic strategy was to
use the cosine similarity of two vectors as the likeli-
hood of the word?s being a Named Entity. However,
several issues arose in trying to apply this directly.
Firstly, it is not always true that the same event is re-
ported on the same day. An actual newspaper some-
times has a one or two-day time lag depending on
the news. To alleviate this effect, we applied a sim-
ple smoothing to each vector. Secondly, we needed
to focus on the salient use of each word, otherwise a
common noun which constantly appears almost ev-
ery day has an undesirable high similarity between
newspapers. To avoid this, we tried to intensify the
effect of a spike by comparing the deviation of the
frequency instead of the frequency itself. This way
we can degrade the similarity of a word which has a
?flat? distribution.
In this section we first explain a single-word ex-
periment which detects Named Entities that consist
of one word. Next we explain a multi-word exper-
iment which detects Named Entities that consist of
exactly two words.
3.1 Single-word Experiment
In a single-word experiment, we used two one-
year newspapers, Los Angeles Times and Reuters in
1995. First we picked a rare word which appeared
in either newspaper less than 100 times throughout
the year. We only used a simple tokenizer and con-
verted all words into lower case. A part of speech
tagger was not used. Then we obtained the docu-
ment frequency vector for the word. For each word
w which appeared in newspaper A, we got the doc-
ument frequency at date t:
fA(w, t) = dfA(w, t)/NA(t)
where dfA(w, t) is the number of documents which
contain the word w at date t in newspaper A. The
normalization constant NA(t) is the number of all
articles at date t. However comparing this value
between two newspapers directly cannot capture a
time lag. So now we apply smoothing by the fol-
lowing formula to get an improved version of fA:
f ?A(w, t) =
?
?W?i?W
r|i|fA(w, t+ i)
Here we give each occurrence of a word a ?stretch?
which sustains for W days. This way we can cap-
ture two occurrences which appear on slightly dif-
ferent days. In this experiment, we used W = 2
and r = 0.3, which sums up the numbers in a 5-day
window. It gives each occurrence a 5-day stretch
which is exponentially decreasing.
Then we make another modification to f ?A by
computing the deviation of f ?A to intensify a spike:
f ??A(w, t) =
f ?A(w, t)? f? ?A
?
where f? ?A and ? is the average and the standard de-
viation of f ?A(w):
f? ?A =
?
t f ?A(w, t)
T
? =
??
t (f ?A(w, t)? f? ?A)2
T
T is the number of days used in the experiment, e.g.
T = 365 for one year. Now we have a time series
vector FA(w) for word w in newspaper A:
FA(w) = {f ??A(w, 1), f ??A(w, 2), ..., f ??A(w, T )}
Similarly, we calculated another time series
FB(w) for newspaper B. Finally we computed
sim(w), the cosine similarity of two distributions
of the word w with the following formula:
sim(w) = FA(w) ? FB(w)|FA(w)||FB(w)|
Since this is the cosine of the angle formed by
the two vectors, the obtained similarity ranges from
?1.0 to 1.0. We used sim(w) as the Named Entity
score of the word and ranked these words by this
score. Then we took the highly ranked words as
Named Entities.
3.2 Multi-word Experiment
We also tried a similar experiment for compound
words. To avoid chunking errors, we picked all
consecutive two-word pairs which appeared in both
newspapers, without using any part of speech tagger
or chunker. Word pairs which include a pre-defined
stop word such as ?the? or ?with? were eliminated.
As with the single-word experiment, we measured
the similarity between the time series distributions
for a word pair in two newspapers. One different
point is that we compared three newspapers 1 rather
than two, to gain more accuracy. Now the ranking
score sim(w) given to a word pair is calculated as
follows:
sim(w) = simAB(w)? simBC(w)? simAC(w)
where simXY (w) is the similarity of the distribu-
tions between two newspapers X and Y , which can
be computed with the formula used in the single-
word experiment. To avoid incorrectly multiply-
ing two negative similarities, a negative similarity
is treated as zero.
4 Evaluation and Discussion
To evaluate the performance, we ranked 966 sin-
gle words and 810 consecutive word pairs which are
randomly selected. We measured how many Named
Entities are included in the highly ranked words.
We manually classified as names the words in the
following categories used in IREX (Sekine and Isa-
hara, 2000): PERSON, ORGANIZATION, LOCA-
TION, and PRODUCT. In both experiments, we re-
garded a name which can stand itself as a correct
Named Entity, even if it doesn?t stretch to the entire
noun phrase.
4.1 Single-word Experiment
Table 1 shows an excerpt of the ranking result. For
each word, the type of the word, the document fre-
quency and the similarity (score) sim(w) is listed.
Obvious typos are classified as ?typo?. One can ob-
serve that a word which is highly ranked is more
likely a Named Entity than lower ones. To show this
correlation clearly, we plot the score of the words
and the likelihood of being a Named Entity in Fig-
ure 2. Since the actual number of the words is
discrete, we computed the likelihood by counting
Named Entities in a 50-word window around that
score.
Table 3 shows the number of obtained Named En-
tities. By taking highly ranked words (sim(w) ?
1For the multi-word experiment, we used Los Angeles
Times, Reuters, and New York Times.
Word Type Freq. Score
sykesville LOCATION 4 1.000
khamad PERSON 4 1.000
zhitarenko PERSON 6 1.000
sirica PERSON 9 1.000
energiyas PRODUCT 4 1.000
hulya PERSON 5 1.000
salvis PERSON 5 0.960
geagea PERSON 27 0.956
bogdanor PERSON 6 0.944
gomilevsky PERSON 6 0.939
kulcsar PERSON 15 0.926
carseats noun 17 0.912
wilsons PERSON 32 0.897
yeud ORGANIZATION 10 0.893
yigal PERSON 490 0.878
bushey PERSON 10 0.874
pardew PERSON 17 0.857
yids PERSON 5 0.844
bordon PERSON 113 0.822
... ... ... ...
katyushas PRODUCT 56 0.516
solzhenitsyn PERSON 81 0.490
scheuer PERSON 9 0.478
morgue noun 340 0.456
mudslides noun 151 0.420
rump noun 642 0.417
grandstands noun 42 0.407
overslept verb 51 0.401
lehrmann PERSON 13 0.391
... ... ... ...
willowby PERSON 3 0.000
unknowable adj 48 0.000
taubensee PERSON 22 0.000
similary (typo) 3 0.000
recommitment noun 12 0.000
perorations noun 3 0.000
orenk PERSON 2 0.000
malarkey PERSON 34 0.000
gherardo PERSON 5 0.000
dcis ORGANIZATION 3 0.000
... ... ... ...
merritt PERSON 149 -0.054
echelon noun 97 -0.058
plugging verb 265 -0.058
normalcy noun 170 -0.063
lovell PERSON 238 -0.066
provisionally adv 74 -0.068
sails noun 364 -0.075
rekindled verb 292 -0.081
sublime adj 182 -0.090
afflicts verb 168 -0.116
stan PERSON 994 -0.132
Table 1: Ranking Result (Single-word)
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8
Lik
elih
ood
Score
Figure 2: Relationship of the score and the likeli-
hood of being a Named Entity (Single-word). The
horizontal axis shows the score of a word. The ver-
tical axis shows the likelihood of being a NE. One
can see that the likelihood of NE increases as the
score of a word goes up. However there is a huge
peak near the score zero.
0.6), we can discover rare Named Entities with 90%
accuracy. However, one can notice that there is a
huge peak near the score sim(w) = 0. This means
that many Named Entities still remain in the lower
score. Most such Named Entities only appeared in
one newspaper. Named Entities given a score less
than zero were likely to refer to a completely differ-
ent entity. For example, the word ?Stan? can be used
as a person name but was given a negative score, be-
cause this was used as a first name of more than 10
different people in several overlapping periods.
Also, we took a look at highly ranked words
which are not Named Entities as shown in Table 2.
The words ?carseats?, ?tiremaker?, or ?neurotripic?
happened to appear in a small number of articles.
Each of these articles and its comparable counter-
parts report the same event, but both of them use
the same word probably because there was no other
succinct expression to paraphrase these rare words.
This way these three words made a high spike. The
word ?officeholders? was misrecognized due to the
Word Type Freq. Score
carseats noun 17 0.9121
tiremaker noun 21 0.8766
officeholders noun 101 0.8053
neurotrophic adj 11 0.7850
mishandle verb 12 0.7369
Table 2: Errors (Single-word)
Words NEs
All words 966 462 (48%)
sim(w) ? 0.6 102 92 (90%)
sim(w) ? 0 511 255 (50%)
Table 3: Obtained NEs (Single-word)
Word pairs NEs
All word pairs 810 60 (7%)
sim(w) ? 0.05 27 11 (41%)
sim(w) ? 0 658 30 (5%)
Table 4: Obtained NEs (Multi-word)
repetition of articles. This word appeared a lot of
times and some of them made the spike very sharp,
but it turned out that the document frequency was
undesirably inflated by the identical articles. The
word ?mishandle? was used in a quote by a per-
son in both articles, which also makes a undesirable
spike.
4.2 Multi-word Experiment
In the multi-word experiment, the accuracy of the
obtained Named Entities was lower than in the
single-word experiment as shown in Table 4, al-
though correlation was still found between the score
and the likelihood. This is partly because there were
far fewer Named Entities in the test data. Also,
many word pairs included in the test data incorrectly
capture a noun phrase boundary, which may con-
tain an incomplete Named Entity. We think that this
problem can be solved by using a chunk of words
instead of two consecutive words. Another notable
example in the multi-word ranking is a quoted word
pair from the same speech. Since a news article
sometime quotes a person?s speech literally, such
word pairs are likely to appear at the same time in
both newspapers. However, since multi-word ex-
pressions are much more varied than single-word
ones, the overall frequency of multi-word expres-
sions is lower, which makes such coincidence easily
stand out. We think that this kind of problem can be
alleviated to some degree by eliminating completely
identical sentences from comparable articles.
The obtained ranking of word pairs are listed in
Table 5. The relationship between the score of word
pairs and the likelihood of being Named Entities is
plotted in Figure 3.
5 Conclusion and Future Work
In this paper we described a novel way to discover
Named Entities by using the time series distribution
Word Type Freq. Score
thai nation ORG. 82 0.425
united network ORG. 31 0.290
government open - 87 0.237
club royale ORG. 32 0.142
columnist pat - 81 0.111
muslim minister - 28 0.079
main antenna - 22 0.073
great escape PRODUCT 32 0.059
american black - 38 0.051
patrick swayze PERSON 112 0.038
finds unacceptable - 19 0.034
mayor ron PERSON 49 0.032
babi yar LOCATION 34 0.028
bet secret - 97 0.018
u.s. passport - 58 0.017
thursday proposed - 60 0.014
atlantic command ORG. 30 0.013
prosecutors asked - 73 0.011
unmistakable message - 25 0.010
fallen hero - 12 0.008
american electronics ORG. 65 0.007
primary goal - 138 0.007
beach boys ORG. 119 0.006
amnon rubinstein PERSON 31 0.005
annual winter - 43 0.004
television interviewer - 123 0.003
outside simpson - 76 0.003
electronics firm - 39 0.002
sanctions lifted - 83 0.001
netherlands antilles LOCATION 29 0.001
make tough - 60 0.000
permanent exhibit - 17 0.000
Table 5: Ranking Result (Multi-word)
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.02  0.04  0.06  0.08  0.1
Lik
elih
ood
Score
Figure 3: Relationship of the score and the likeli-
hood of being a Named Entity (Multi-word). The
horizontal axis shows the score of a word. The ver-
tical axis shows the likelihood of being a NE.
of names. Since Named Entities in comparable doc-
uments tend to appear synchronously, one can find a
Named Entity by looking for a word whose chrono-
logical distribution is similar among several compa-
rable documents. We conducted an experiment with
several newspapers because news articles are gener-
ally sorted chronologically, and they are abundant in
comparable documents. We confirmed that there is
some correlation between the similarity of the time
series distribution of a word and the likelihood of
being a Named Entity.
We think that the number of obtained Named En-
tities in our experiment was still not enough. So
we expect that better performance in actual Named
Entity tagging can be achieved by combining this
feature with other contextual or lexical knowledge,
mainly used in existing Named Entity taggers.
6 Acknowledgments
This research was supported in part by the De-
fense Advanced Research Projects Agency as part
of the Translingual Information Detection, Extrac-
tion and Summarization (TIDES) program, un-
der Grant N66001-001-1-8917 from the Space and
Naval Warfare Systems Center, San Diego, and by
the National Science Foundation under Grant ITS-
00325657. This paper does not necessarily reflect
the position of the U.S. Government.
References
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting paraphrases from a parallel corpus. In
Proceedings of ACL/EACL 2001.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In
Proceedings of EMNLP 1999.
Satoshi Sekine and Hitoshi Isahara. 2000. IREX:
IR and IE evaluation-based project in Japanese.
In Proceedings of LREC 2000.
Satoshi Sekine, Kiyoshi Sudo, and Chikashi No-
bata. 2002. Extended named entity hierarchy. In
Proceedings of LREC 2002.
Yusuke Shinyama and Satoshi Sekine. 2003. Para-
phrase acquisition for information extraction. In
Proceedings of International Workshop on Para-
phrasing 2003.
Tomek Strzalkowski and Jin Wang. 1996. A self-
learning universal concept spotter. In Proceed-
ings of COLING 1996.
Roman Yangarber, Winston Lin, and Ralph Grish-
man. 2002. Unsupervised learning of general-
ized names. In Proceedings of COLING 2002.
Cross-lingual Information Extraction System Evaluation
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman
Department of Computer Science
New York University
715 Broadway, 7th Floor,
New York, NY 10003
 
sudo,sekine,grishman  @cs.nyu.edu
Abstract
In this paper, we discuss the performance of cross-
lingual information extraction systems employing
an automatic pattern acquisition module. This mod-
ule, which creates extraction patterns starting from
a user?s narrative task description, allows rapid cus-
tomization to new extraction tasks. We compare two
approaches: (1) acquiring patterns in the source lan-
guage, performing source language extraction, and
then translating the resulting templates to the tar-
get language, and (2) translating the texts and per-
forming pattern discovery and extraction in the tar-
get language. We demonstrate an average of 8-10%
more recall using the first approach. We discuss
some of the problems with machine translation and
their effect on pattern discovery which lead to this
difference in performance.
1 Introduction
Research in information extraction (IE) and its re-
lated fields has led to a wide range of applications
in many domains. The portability issue of IE sys-
tems across different domains, however, remains a
serious challenge. This problem is being addressed
through automatic knowledge acquisition methods,
such as unsupervised learning for domain-specific
lexicons (Lin et al, 2003) and extraction patterns
(Yangarber, 2003), which require the user to pro-
vide only a small set of lexical items of the target
classes or extraction patterns for the target domain.
The idea of a self-customizing IE system emerged
recently with the improvement of pattern acquisi-
tion techniques (Sudo et al, 2003b), where the IE
system customizes itself across domains given by
the user?s query.
Furthermore, there are demands for access to in-
formation in languages different from the user?s
own. However, it is more challenging to provide
an IE system where the target language (here, En-
glish) is different from the source language (here,
Japanese): a cross-lingual information extraction
(CLIE) system.
In this research, we explore various methods
for efficient automatic pattern acquisition for the
CLIE system, including the translation of the en-
tire source document set into the target language.
To achieve efficiency, the resulting CLIE system
should (1) provide a reasonable level of extraction
performance (both accuracy and coverage) and (2)
require little or no knowledge on the user?s part of
the source language. Today, there are basic linguis-
tics tools available for many major languages. We
show how we can take advantage of the tools avail-
able for the source language to boost extraction per-
formance.
The rest of this paper is organized as follows.
Section 2 and 3 discuss the self-adaptive CLIE sys-
tem we assess throughout the paper. In Section 4,
we show the experimental result for entity detec-
tion. Section 5 discusses the problems in translation
that affect the pattern acquisition and Section 6 dis-
cusses related work. Finally, we conclude the paper
in Section 7 with future work.
2 Query-Driven Information Extraction
One approach to IE portability is to have a system
that takes the description of the event type from the
user as input and acquires extraction patterns for the
given scenario. Throughout the paper, we call this
kind of IE system QDIE (Query-Driven Information
Extraction) system, whose typical procedure is il-
lustrated in Figure 1.
QDIE (e.g. (Sudo et al, 2003a)) consists of three
phases to learn extraction patterns from the source
documents for a scenario specified by the user.
First, it applies morphological analysis, depen-
dency parsing and Named Entity (NE) tagging to the
entire source document set, and converts all the sen-
tences in the source document set into dependency
trees. The NE tagging replaces named entities by
their class, so the resulting dependency trees con-
tain some NE class names as leaf nodes. This is
crucial to identifying common patterns, and to ap-
plying these patterns to new text.
Second, the user provides a set of narrative sen-
Figure 1: QDIE Pattern Acquisition
tences describing the scenario (the events of inter-
est). Using these sentences as a retrieval query, the
information retrieval component of QDIE retrieves
representative documents of the scenario specified
by the user (relevant documents).
Then from among all the possible connected sub-
trees of all the sentences in the relevant documents,
the system calculates the score for each pattern can-
didate. The scoring function is based on TF/IDF
scoring in IR literature; a pattern is more relevant
when it appears more in the relevant documents
and less across the entire collection of source docu-
ments. The final output is the ordered list of pattern
candidates.
Note that a pattern candidate contains at least one
NE, so that it can be used to match a portion of a
sentence which contains an instance of the same NE
type. The matched NE instance is then extracted.
The pattern candidates may be simple predicate-
argument structures (e.g. (resign from   C-POST  )
in business domain) or even a complicated subtree
of a sentence which commonly appears in the rel-
evant documents (e.g. (   C-ORG  report personnel
affair (that   C-PERSON  resigns)) ).
3 Cross-lingual Information Extraction
(Riloff et al, 2002) present several approaches to
cross-lingual information extraction (CLIE). They
describe the use of ?cross-language projection? for
CLIE, exploiting the word alignment of documents
in one language and the same documents translated
into a different language by a machine translation
(MT) system. They conducted experiments between
two relatively close languages, English and French.
In the experiment reported here, we will explore
CLIE for two more disparate languages, English
and Japanese.
The QDIE system can be used in a cross-lingual
setting, and thus, the resulting cross-lingual version
of the QDIE system can minimize the requirement
of the user?s knowing the source language. Figure 2
shows two possible ways to achieve this goal.
It may be realized by translating all the docu-
ments of the source language into the target lan-
guage, and then running the monolingual ver-
sion of the QDIE system for the target language
(Translation-based QDIE). In our experiment, we
translated all the source Japanese documents into
English. Then we ran English-QDIE system to get
the extraction patterns, which are used to extract the
entities by pattern matching.
On the other hand, one can first translate the sce-
nario description into the source language and use
it for the monolingual QDIE system for the source
language, assuming that we have access to the tools
for pattern acquisition in the source language. Each
entity in the extracted table is translated into the
target language (Crosslingual-QDIE). In Figure 2,
we implemented this procedure by first translating
the English query into Japanese. 1 Then we ran
Japanese-QDIE system to identify Japanese extrac-
tion patterns. The extraction patterns are used to
extract items to fill the Japanese table. Finally, each
item in the extracted table is separately translated
into English. Note that translating names is easier
than translating the whole sentences.
As we shall demonstrate, the errors introduced by
the MT system impose a significant cost in extrac-
tion performance both in accuracy and coverage of
the target event. However, if basic linguistic anal-
ysis tools are available for the source language, it
is possible to boost CLIE performance by learning
patterns in the source language. In the next section,
we describe an experiment which compares these
two approaches. In the following section, we assess
the difficulty of learning extraction patterns from the
translated source language document set caused by
the errors of the MT system and/or the differences
of grammatical structure of the translated sentences.
We address specifically:
1. The accuracy of NE tagging on MT-ed source
documents and the use of cross-language pro-
jection.
2. How the structural difference in source and tar-
get language affects the extracted patterns.
3. The reduced frequency of the extracted pat-
terns, which makes it difficult for any mea-
surement of pattern relevance to distinguish the
1Note that our current implementation uses the output from
query translation by the MT system. As we note in Section 7,
we plan to investigate the possibility of additional performance
gain by using current crosslingual information retrieval tech-
niques.
Figure 2: Translation-based QDIE System(A) vs Crosslingual QDIE System(B): The user?s query (English),
the source document (Japanese) and the target extracted table (English) are highlighted.
effective patterns of low frequency from the
noise patterns.
4 Experiments
To evaluate the relevance of extraction patterns au-
tomatically learned for CLIE, we conducted exper-
iments for the Translation-based QDIE system and
the Cross-lingual QDIE system on the entity extrac-
tion task, which is to identify all the entities partic-
ipating in relevant events in a given set of Japanese
texts.
4.1 Experimental Setting
Since general NE taggers either are trained on En-
glish sentences or use manually created rules for
English sentences, the deterioration of NE tagger?s
performance cannot be avoided if it is applied to
the MT-ed English sentences. This causes the
Translation-based QDIE system to identify fewer
pattern candidates from the relevant documents
since a pattern candidate must contain at least one
of the NE types.
To remedy this problem, we incorporated ?cross-
language projection? (Riloff et al, 2002) only for
Named Entities. We used word alignment obtained
by using Giza++ (Och and Ney, 2003) to get names
in the English translation from names in the original
Japanese sentences. Note that it is extremely diffi-
cult to make an alignment of case markers where
one language explicitly renders a marker as a word
and the other does not. So, direct application of
(Riloff et al, 2002) is not suitable for this experi-
ment.
We compare the following three systems in this
experiment.
1. Crosslingual QDIE system
2. Translation-based QDIE system with word
alignment
3. Translation-based QDIE system without word
alignment
4.2 Data
The scenario for this experiment is the Management
Succession scenario of MUC-6(muc, 1995), where
corporate managers assumed and/or left their posts.
We used a much simpler template structure than the
one used in MUC-6, with Person, Organization, and
Post slots. To assess system performance, we mea-
sure the accuracy of the system at identifying the
participating entities in a management succession
event. This task does not involve grouping entities
associated with the same event into a single tem-
plate, in order to avoid possible effects of merging
failure on extraction performance for entities.
The source document set from which the extrac-
tion patterns are learned consists of 132,996 Yomi-
uri Newspaper articles from 1998. For our Crosslin-
gual QDIE system, all the documents are morpho-
logically analyzed by JUMAN (Kurohashi, 1997)
and converted into dependency trees by KNP (Kuro-
hashi and Nagao, 1994). For the Translation-based
QDIE system, all the documents are translated into
English by a commercial machine translation sys-
tem (IBM ?King of Translation?), and converted
into dependency trees by a corpus-based parser. We
retrieved 1500 documents as relevant documents.
We accumulated the test set of documents by a
simple keyword search. The test set consists of
100 Yomiuri Newspaper articles from 1999, out of
which only 61 articles contain at least one manage-
ment succession event. Note that all NE in the test
documents both in the original Japanese and in the
translated English sentences were identified man-
ually, so that the task can measure only how well
extraction patterns can distinguish the participating
entities from the entities that are not related to any
succession events. Table 1 shows the details of the
test data.
4.3 Results
Each pattern acquisition system outputs a list of the
pattern candidates ordered by the ranking function.
The resulting performance is shown as a precision-
Documents 100
(relevant + irrelevant)  
	
Names Person: 173 + 651
(relevant + irrelevant) Org: 111 + 709
Post: 210 + 626
Table 1: Statistics of Test Data
recall graph for each subset of top-  ranked patterns
where  ranges from 1 to the number of pattern can-
didates. The parameters for each system are tuned
to maximize the performance on separate validation
data.
The association of NE classes in the matched pat-
terns and slots in the template is made automati-
cally; Person, Organization, Post (slots) correspond
to C-PERSON, C-ORG, C-POST (NE-classes), re-
spectively, in the Management Succession scenario.
Figure 3 shows the precision-recall curve for the
top 1000 patterns acquired by each system on the
entity extraction task. Crosslingual QDIE system
reaches a maximum recall of 60%, which is sig-
nificantly better than Translation-based QDIE with
word alignment (52%) and Translation-based QDIE
without word alignment (41%). Within the high re-
call range, Crosslingual QDIE system generally had
better precision at the same recall than Translation-
based QDIE systems. At the low recall range ( 
 ), the performance is rather noisy.
Translation-based QDIE without word align-
ment performs similarly to Translation-based QDIE
with word alignment up to its maximum recall
(41%). Translation-based QDIE with word align-
ment reached 10% higher maximum recall (52%).
5 Problems in Translation
The detailed analysis of the result revealed the effect
of several problems caused by the MT system. The
current off-the-shelf MT system?s output resulted in
difficulty in using it as a source of extraction pat-
terns. In this section we will discuss the types of dif-
ferences between the source and target languages,
and their effect on pattern discovery.
Lexical differences Abbreviations in the source
language may not have their corresponding short
form in the target language. For example, ?Kei-
Dan-Ren? is an abbreviation of ?Keizai Dantai
Rengo-kai? which is an organization whose English
translation is ?Japan Federation of Economic Orga-
nizations?. Such abbreviations may not be listed in
the dictionary of the MT system. In such cases, the
literal translation of the abbreviation may be diffi-
cult to recognize as a name and is likely to be treated
as a common noun phrase.
Structural differences Some phrases in the
source language may have more than one relevant
translation. Depending upon the context where
a phrase appears, the MT system has to choose
one among the possible translations. Moreover,
the MT system may make a mistake, of course,
and output an erroneous translation. This results
in a diverse distribution of extraction patterns in
the target language. Figure 4 shows an exam-
ple of such a case. Suppose an extraction pattern
((   C-POST  -ni) shuninsuru) appears 20 times in
the original Japanese document set, out of which
it may be translated 10 times as (be appointed (to
(   C-POST  ))), 5 times as (assume (   C-POST  )),
3 times as (be inaugurated (as (   C-POST  ))), and
2 times as an erroneous translation. Some of the
lower frequency translated patterns will be ranked
lower by the scoring function and so will be hard to
distinguish from noise.
Figure 4: Example of Structural Difference in
Translation: The translation of a Japanese expression
into several English different expressions including erro-
neous ones.
Figure 5 shows an example of the case where
the context around the name did not seem to be
translated properly, so the dependency tree for the
sentence was not correct. The right translation is
?Okajima announced that President Hiroyuki Oka-
jima, 40 years old, resigned formally ...? which re-
sults in the dependency between the main verb ?an-
nounce? and the company ?Okajima?. The trans-
lation shown in Figure 5 not only shows incor-
rect word-translations, but also shows ungrammat-
ical structure, including too many relative clauses.
The structural error causes the errors in the depen-
dency parse tree including having ?end? as a root of
the entire tree and the wrong dependency from ?an-
nounced? to ?the major department? in Figure 5 2.
Thus, the accumulation of the errors resulted in
missing the organization name ?Okajima?.
Also, the conjunctions in Japanese sentences
could not be translated properly, and therefore, the
2The head is ?the major department? and ?announced? is
modifying the head.
020
40
60
80
100
0 10 20 30 40 50 60 70 80
Pr
ec
is
io
n 
(%
)
Recall (%)
Precision-Recall
Crosslingual
Translation with WA
Translation without WA
Figure 3: Performance Comparison on Entity Extraction Task
English dependency parser?s output is significantly
deteriorated. The example in Figure 6 shows the
case where both ?Mr. Suzuki? and ?Mr. Asada?
were inaugurated. In the original Japanese sentence,
?Mr. Suzuki? is closer to the verb ?be inaugurated?.
So, it seems that the MT system tries to find another
verb for ?Mr. Asada?, and attaches it (incorrectly)
to ?unofficially arranged?.
Out-of-Vocabulary Words The MT system may
not have a word in the source language dictionary, in
which case some MT systems output it in the origi-
nal script in the source language. This happens not
only for names but also for sentences which are er-
roneously segmented into words. Such problems, of
course, may make it hard to detect Named Entities
and get a correct dependency tree of the sentence.
However, translation of names is easier than
translation of contexts; the MT system can output
the transliteration of an unknown word. In fact,
name translation of the MT system we used for
this experiment is better than the sentence transla-
tion of the same MT system. The names appro-
priately extracted from Japanese documents by the
Crosslingual QDIE system, in most cases, are cor-
rectly translated or transliterated if no equivalent
translation exists.
6 Related Work
The work closest to ours is (Riloff et al, 2002).
They showed how IE learning tools, bitext align-
ment, and an MT system can be combined to cre-
ate CLIE systems between English and French.
They evaluated a variety of methods, including one
similar to our Translation-based QDIE. Their ap-
proaches were less reliant on language tools for the
?source? language (in their case, French) than our
Crosslingual-QDIE system. On the other hand, their
tests were made on a closer language pair (English
- French). We expect that the performance gap be-
tween Translation-based IE and Crosslingual IE is
more pronounced with a more divergent language
pair like Japanese and English.
There are interesting parallels between our work
and that of (Douzidia and Lapalme, 2004), who dis-
cussed the role of machine translation in a cross-
lingual summarization system which produces an
English summary from Arabic text. Their system
took the same path as our Crosslingual QDIE: sum-
marizing the Arabic text directly and only translat-
ing the summary, rather than translating the entire
Arabic text and summarizing the translation. They
had similar motivations: different translations pro-
duced by the MT system for the same word in dif-
ferent contexts, as well as translation errors, would
interfere with the summarization process.
The trade-offs, however, are not the same for the
two applications. For summarization either path
requires an MT system which can translate entire
sentences (either the original text or the summary).
Translation-based QDIE has a similar requirement,
Output of MT system:
From Muika the term settlement of accounts ended February , 99 having become
the prospect of the first deficit settlement of accounts after the war etc. ,
six of President Hiroyuki Okajima ( 40 ) , two managing directors , one man-
aging directors , the full-time directors that are 13 persons submitted the
resignation report , ?Okajima? of Marunouchi , Kofu-shi who is the major de-
partment store within the prefecture announced that he resigns formally by
the fixed general meeting of shareholders of the company planned at the end of
this month .
Output of Dependency Tree (part):
Figure 5: Example of Translation Errors: Figure also contains a part of the dependency parser?s output of the
sentence. Dashed lines show the correct dependencies.
but Crosslingual QDIE reduces the demands on MT:
only query translation and name translation are re-
quired.
7 Conclusion
We discussed the difficulty in cross-lingual infor-
mation extraction caused by the translation of the
source documents using an MT system. The ex-
perimental result for entity extraction suggests that
exploiting some basic tools available for the source
language will boost the performance of the whole
CLIE system.
We intend to investigate whether further perfor-
mance gain may be obtained by introducing ad-
ditional techniques for query translation. These
techniques, including query translation on expanded
queries and building a translation dictionary from
parallel corpora, are currently used in crosslingual
information retrieval (Larkey and Connell, 2003).
Acknowledgments
This research was supported in part by the De-
fense Advanced Research Projects Agency as part
of the Translingual Information Detection, Extrac-
tion and Summarization (TIDES) program, un-
der Grant N66001-001-1-8917 from the Space and
Naval Warfare Systems Center, San Diego, and by
the National Science Foundation under Grant ITS-
00325657. This paper does not necessarily reflect
the position of the U.S. Government.
References
Fouad Soufiane Douzidia and Guy Lapalme. 2004.
Lakhas, an Arabic summarization system. In
Proceedings of DUC2004.
Sadao Kurohashi and Makoto Nagao. 1994. KN
parser : Japanese dependency/case structure an-
alyzer. In Proceedings of the Workshop on
Sharable Natural Language Resources.
Sadao Kurohashi, 1997. Japanese Morphological
Analyzing System: JUMAN. http://www.kc.t.u-
tokyo.ac.jp/nl-resource/juman-e.html.
Leah Larkey and Margaret Connell. 2003. Struc-
tured Queries, Language Modeling, and Rele-
vance Modeling in Cross-Language Information
Retrieval. In Information Processing and Man-
agement, Special Issue on Cross Language Infor-
mation Retrieval.
Winston Lin, Roman Yangarber, and Ralph Grish-
man. 2003. Bootstrapped Learning of Seman-
tic Classes from Positive and Negative Examples.
In Proceedings of the ICML-2003 Workshop on
The Continuum from Labeled to Unlabeled Data,
Washington, D.C.
1995. Proceedings of the Sixth Message Under-
Output of MT system:
The personnel affairs to which managing director Shosei Suzuki ( 57 ) is in-
augurated as the Nippon Telegraph and Telephone president of the holding com-
pany which NTT will be the board of directors on the seventh , is inaugu-
rated by NTT reorganization on July 1 at President Jun-ichiro Miyatsu ( 63
) the Nippon Telegraph Telephone East Corporation ( NTT East Japan ) presi-
dent of a local communication company , he is inaugurated as Vice President
Shuichi Inoue ( 61 ) Nippon Telegraph Telephone West Corporation ( NTT west-
ern part of Japan ) were unofficially arranged Vice President Kazuo Asada
( 59 ) the NTT Communications president of a long distance international-
telecommunications company .
Output of Dependency Tree (part):
Figure 6: Example of Erroneous Conjunction Phrase: Figure also contains a part of the dependency parser?s
output of the sentence. Dashed lines show the correct dependencies.
standing Conference (MUC-6), Columbia, MD,
November. Morgan Kaufmann.
Franz Josef Och and Hermann Ney. 2003. A
Systematic Comparison of Various Statistical
Alignment Models. Computational Linguistics,
29(1):19?51.
Ellen Riloff, Charles Schafer, and David Yarowsky.
2002. Inducing Information Extraction Systems
for New Languages via Cross-Language Projec-
tion. In Proceedings of the 19th International
Conference on Computational Linguistics (COL-
ING 2002).
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003a. An Improved Extraction Pattern Repre-
sentation Model for Automatic IE Pattern Acqui-
sition. In Proceedings of the 41st Annual Meet-
ing of Association of Computational Linguistics
(ACL 2003), Sapporo, Japan.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grish-
man. 2003b. Pre-CODIE ? Crosslingual On-
Demand Information Extraction. In Proceedings
of HLT/NAACL 2003, Edmonton, Canada.
Roman Yangarber. 2003. Counter-Training in Dis-
covery of Semantic Patterns. In Proceedings
of the 41st Annual Meeting of Association of
Computational Linguistics (ACL 2003), Sapporo,
Japan.
Automatic Construction of Japanese KATAKANA Variant List
from Large Corpus
Takeshi Masuyama
Information Technology Center
University of Tokyo
7-3-1, Hongo, Bunkyo
Tokyo 113-0023
Japan
tak@r.dl.itc.u-tokyo.ac.jp
Satoshi Sekine
Computer Science Department
New York University
715 Broadway, 7th floor
New York NY 10003
USA
sekine@cs.nyu.edu
Hiroshi Nakagawa
Information Technology Center
University of Tokyo
7-3-1, Hongo, Bunkyo
Tokyo 113-0023
Japan
nakagawa@dl.itc.u-tokyo.ac.jp
Abstract
This paper presents a method to construct
Japanese KATAKANA variant list from
large corpus. Our method is useful for
information retrieval, information extrac-
tion, question answering, and so on, because
KATAKANA words tend to be used as
?loan words? and the transliteration causes
several variations of spelling. Our method
consists of three steps. At step 1, our sys-
tem collects KATAKANA words from large
corpus. At step 2, our system collects can-
didate pairs of KATAKANA variants from
the collected KATAKANA words using a
spelling similarity which is based on the edit
distance. At step 3, our system selects vari-
ant pairs from the candidate pairs using a
semantic similarity which is calculated by
a vector space model of a context of each
KATAKANA word. We conducted exper-
iments using 38 years of Japanese news-
paper articles and constructed Japanese
KATAKANA variant list with the perfor-
mance of 97.4% recall and 89.1% precision.
Estimating from this precision, our system
can extract 178,569 variant pairs from the
corpus.
1 Introduction
?Loan words? in Japanese are usually writ-
ten by a phonogram type of Japanese charac-
ter set, KATAKANA. Because of loan words,
the transliteration causes several variations of
spelling. Therefore, Japanese KATAKANA
words sometimes have several different or-
thographies for each original word. For exam-
ple, we found at least six different spellings of
?spaghetti? in 38 years of Japanese newspaper
articles, such as ???????,? ??????
??,? ???????,? ??????,? ????
???,? and ??????.? The different ex-
pression causes problems when we use search
engines, question answering systems, and so on
(Yamamoto et al, 2003). For example, when we
input ???????? as a query for a search en-
gine or a query for a question answering system,
we may not be able to find the web pages or the
answers for which we are looking, if a different
orthography for ???????? is used.
We investigated how many documents were
retrieved by Google 1 when each Japanese
KATAKANA variant of ?spaghetti? was used
as a query. The result is shown as Table 1.
For example, when we inputted ??????
?? as a query of Google, 104,000 documents
were retrieved and the percentage was 34.6%,
calculated by 104,000 divided by 300,556. From
Table 1, we see that each of six variants appears
frequently and thus we may not be able to find
the web pages for which we are looking.
Although we can manually create Japanese
KATAKANA variant list, it is a labor-intensive
task. In order to solve the problem, we propose
an automatic method to construct Japanese
KATAKANA variant list from large corpus.
Variant # of retrieved documents
?????? 104,000 (34.6%)
??????? 25,400 (8.5%)
?????? 1,570 (0.5%)
????? 131,000 (43.6%)
?????? 37,700 (12.5%)
????? 886 (0.3%)
Total 300,556 (100%)
Table 1: Number of retrieved documents when
we inputted each Japanese KATAKANA vari-
ant of ?spaghetti? as a query of Google.
Our method consists of three steps. First,
we collect Japanese KATAKANA words from
large corpus. Then, we collect candidate pairs of
KATAKANA variants based on a spelling simi-
larity from the collected Japanese KATAKANA
words. Finally, we select variant pairs using
1
http://www.google.co.jp/
a semantic similarity based on a vector space
model of a context of each KATAKANA word.
This paper is organized as follows. Section
2 describes related work. Section 3 presents
our method to construct Japanese KATAKANA
variant list from large corpus. Section 4 shows
some experimental results using 38 years of
Japanese newspaper articles, which we call ?the
Corpus? from now on, followed by evaluation
and discussion. Section 5 describes future work.
Section 6 offers some concluding remarks.
2 Related Work
There are some related work for the problems
with Japanese spelling variations. In (Shishi-
bori and Aoe, 1993), they have proposed a
method for generating Japanese KATAKANA
variants by using replacement rules, such as
? (be) ? ?? (ve) and ? (chi) ? ??(tsi).
Here, ??? represents ?substitution.? For ex-
ample, when we apply these rules to ?????
(Venezia),? three different spellings are gener-
ated as variants, such as ??????,? ????
??,? and ???????.?
Kubota et al have extracted Japanese
KATAKANA variants by first transforming
KATAKANA words to directed graphs based on
rewrite rules and by then checking whether the
directed graphs contain the same labeled path
or not (Kubota et al, 1993). A part of their
rewrite rules is shown in Table 2. For exam-
ple, when applying these rules to ??????
(Kuwait),? ?? a?c,? ?? b?c,? ??? d?c? are
generated as variants.
KATAKANA String ? Symbol
?? (we), ? (e) ? a
?? (we), ?? (ue) ? b
?? (twu), ? (to), ? (tsu) ? c
? (macron) ? ?
? (small e), ? (e) ? d
Table 2: A part of rewrite rules.
In (Shishibori and Aoe, 1993) and (Kubota
et al, 1993), they only paid attention to apply-
ing their replacement or rewrite rules to words
themselves and didn?t pay attention to their
contexts. Therefore, they wrongly decide that
?????? is a variant of ????.? Here, ??
???? represents ?wave? and ????? repre-
sents ?web.? In our method, we will decide if ?
????? and ????? convey the same mean-
ing or not using a semantic similarity based on
their contexts.
3 Construct Japanese KATAKANA
Variant List from Large Corpus
Our method consists of the following three
steps.
1. Collect Japanese KATAKANA words from
large corpus.
2. Collect candidate pairs of KATAKANA
variants from the collected KATAKANA
words using a spelling similarity.
3. Select variant pairs from the candidate
pairs based on a semantic similarity.
3.1 Collect KATAKANA Words from
Large Corpus
At the first step, we collected Japanese
KATAKANA words which consist of a
KATAKANA character, ? (bullet), ?
(macron-1), and ? (macron-2), which are
commonly used as a part of KATAKANA
words, using pattern matching. For example,
our system collects three KATAKANA words ?
???????????? (Ludwig Erhard-1),?
?? (Soviet),? ??????????????
(Ludwig Erhard-2),? ???? (Germany)?
from the following sentences. Note that two
mentions of ?Ludwig Erhard? have different
orthographies.
? ????????????????????
???????????(Defunct Ludwig
Erhard-1 is called ?Father of The Mirac-
ulous Economic Revival.?)
? ???????????????????
????????????????????
???????????????????
???????????????????
????????(If Soviet and East Eu-
ropean countries give up their controlling
concepts and pursue the economic deregu-
lation which Ludwig Erhard-2 of West Ger-
many did in 1948, they may achieve the
miraculous revival like West Germany.)
3.2 Spelling Similarity
At the second step, our system collects candi-
date pairs of two KATAKANA words, which
are similar in spelling, from the collected
KATAKANA words described in Section 3.1.
We used ?string penalty? to collect candidate
pairs. String penalty is based on the edit dis-
tance (Hall and DOWLING, 1980) which is a
similarity measure between two strings. We
used the following three types of operations.
? Substitution
Replace a character with another charac-
ter.
? Deletion
Delete a character.
? Insertion
Insert a character.
We also added some scoring heuristics to the
operations based on a pronunciation similarity
between characters. The rules are tuned by
hand using randomly selected training data.
Some examples are shown in Table 3. Here,
??? represents ?substitution? and lines with-
out ? represent ?deletion? or ?insertion.? Note
that ?Penalty? represents a score of the string
penalty from now on.
For example, we give penalty 1 between ??
????? and ??????,? because the strings
become the same when we replace ??? with ?
?? and its penalty is 1 as shown in Table 3.
Rules Penalty
? (a) ? ? (small a) 1
? (zi) ? ? (di) 1
? (macron) 1
? (ha) ? ? (ba) 2
? (u) ? ? (vu) 2
? (a) ? ? (ya) 3
? (tsu) ? ? (small tsu) 3
Table 3: A part of our string penalty rules.
We analyzed hundreds of candidate pairs
of training data and figured out that most
KATAKANA variations occur when the string
penalties were less than a certain threshold. In
this paper, we set 4 for the threshold and regard
KATAKANA pairs as candidate pairs when the
string penalties are less than 4. The thresh-
old was tuned by hand using randomly selected
training data.
For example, from the collected KATAKANA
words described in Section 3.1, our system col-
lects the pair of ???????????? and
?????????????, since the string
penalty is 3.
3.3 Context Similarity
At the final step, our system selects variant
pairs from the candidate pairs described in Sec-
tion 3.2 based on a semantic similarity. We used
a vector space model as a semantic similarity.
In the vector space model, we treated 10 ran-
domly selected articles from the Corpus as a
context of each KATAKANA word.
We divided sentences of the articles into
words using JUMAN2 (Kurohashi and Nagao,
1999) which is the Japanese morphological an-
alyzer, and then extracted content words which
consist of nouns, verbs, adjectives, adverbs, and
unknown words except stopwords. Stopwords
are composed of Japanese HIRAGANA charac-
ters, punctuations, numerals, common words,
and so on.
We used a cosine measure to calculate a se-
mantic similarity of two KATAKANA words.
Suppose that one KATAKANA word makes a
context vector a and the other one makes b.
The semantic similarity between two vectors a
and b is calculated as follows.
sim(a,b) = cos? = a ? b
|a||b|
(1)
The cosine measure tends to overscore fre-
quently appeared words. Therefore, in order to
avoid the problem, we treated log(N + 1) as a
score of a word appeared in a context. Here, N
represents the frequency of a word in a context.
We set 0.05 for the threshold of the seman-
tic similarity, i.e. we regard candidate pairs as
variant pairs when the semantic similarities are
more than 0.05. The threshold was tuned by
hand using randomly selected training data.
In the case of ????????????? (Lud-
wig Erhard-1)? and ?????????????
? (Ludwig Erhard-2)?, the semantic similarity
becomes 0.17 as shown in Table 4. Therefore,
we regard them as a variant pair.
Note that in Table 4, a decimal number rep-
resents a score of a word appeared in a context
calculated by log(N+1). For example, the score
of ?? (miracle) in the first context is 0.7.
4 Experiments
4.1 Data Preprocessing and
Performance Measures
We conducted the experiments using the Cor-
pus. The number of documents in the Cor-
2
http://www.kc.t.u-tokyo.ac.jp/nl-
resource/juman.html
Word ????????????
?? (miracle):0.7
?? (economy):1.9
Context ? (father):0.7
?? (revival):0.7
? ? ?
Word ?????????????
?? (miracle):1.1
??? (liberalization):1.4
Context ?? (economy):2.4
?? (revival):1.1
? ? ?
Similarity 0.17
Table 4: Semantic similarity between ????
?????????? and ???????????
???.?
pus was 4,678,040 and the distinct number
of KATAKANA words in the Corpus was
1,102,108.
As for a test set, we collected candidate
pairs whose string penalties range from 1 to
12. The number of collected candidate pairs
was 2,590,240. In order to create sample correct
KATAKANA variant data, 500 out of 2,590,240
were randomly selected and we evaluated them
manually by checking their contexts. Through
the evaluation, we found that no correct vari-
ant pairs appeared from 10 to 12. Thus, we
think that treating candidate pairs whose string
penalties range from 1 to 12 can cover almost
all of correct variant pairs.
To evaluate our method, we used recall (Re),
precision (Pr), and F measure (F ). These per-
formance measures are calculated by the follow-
ing formulas:
Re = number of pairs found and correcttotal number of pairs correct ,
Pr = number of pairs found and correcttotal number of pairs found ,
F = 2RePrRe + Pr .
4.2 Experiment-1
We conducted the first experiment based on
two settings; one method uses only the spelling
similarity and the other method uses both the
spelling similarity and the semantic similarity.
Henceforth, we use ?Method
p
,?
?Method
p&s
,? ?Ext,? and ?Cor? as the
following meanings.
Method
p
: The method using only the spelling
similarity
Method
p&s
: The method using both the
spelling similarity and the semantic
similarity
Ext: The number of extracted candidate pairs
Cor: The number of correct variant pairs among
the extracted candidate pairs
Note that in Method
p&s
, we ignored candi-
date pairs whose string penalties ranged from
4 to 12, since we set 4 for the threshold of the
string penalty as described in Section 3.2.
The result is shown in Table 5. For example,
when the penalty was 2, 81 out of 117 were se-
lected as correct variant pairs in Method
p
and
the precision was 69.2%. Also, 80 out of 98 were
selected as correct variant pairs in Method
p&s
and the precision was 81.6%.
As for Penalty 1-12 of Method
p
, i.e. we fo-
cused on the string penalties between 1 and 12,
the recall was 100%, because we regarded 269
out of 500 as correct variant pairs and Method
p
extracted all of them. Also, the precision was
53.8%, calculated by 269 divided by 500. Com-
paring Method
p&s
to Method
p
, the recall and
the precision of Method
p&s
were well-balanced,
since the recall was 97.4% and the precision was
89.1%.
In the same way, for Penalty 1-3, i.e. the
string penalties between 1 and 3, the recall of
Method
p
was 98.1%, since five correct variant
pairs between 4 and 12 were ignored and the
remaining 264 out of 269 were found. The preci-
sion of Method
p
was 77.2%. It was 23.4% higher
than the one of Penalty 1-12. Thus, F measure
also improved 16.4%. This result indicates that
setting 4 for the threshold works well to improve
overall performance.
Now, comparing Method
p&s
to Method
p
when the string penalties ranged from 1 to 3, the
recall of Method
p&s
was 0.7% lower. This was
because Method
p&s
couldn?t select two correct
variant pairs when the penalties were 1 and 2.
However, the precision of Method
p&s
was 16.2%
higher. Thus, F measure of Method
p&s
im-
proved 6.7% compared to the one of Method
p
.
From this result, we think that taking the se-
mantic similarity into account is a better strat-
egy to construct Japanese KATAKANA variant
list.
Penalty Method
p
Method
p&s
Cor/Ext (%) Cor/Ext (%)
1 130/134 (97.0) 129/129 (100)
2 81/117 (69.2) 80/98 (81.6)
3 53/91 (58.2) 53/67 (79.1)
4 2/14 (14.3)
5 0/30 (0.0)
6 1/14 (7.1)
7 1/20 (5.0)
8 0/14 (0.0)
9 1/12 (8.3)
10 0/16 (0.0)
11 0/17 (0.0)
12 0/21 (0.0)
Re 264/269 (98.1) 262/269 (97.4)
1-3 Pr 264/342 (77.2) 262/294 (89.1)
F 86.4% 93.1%
Re 269/269 (100)
1-12 Pr 269/500 (53.8)
F 70.0%
Table 5: Comparison of Method
p
and
Method
p&s
.
4.3 Experiment-2
We investigated how many variant pairs were
extracted in the case of six different spellings
of ?spaghetti? described in Section 1. Table 6
shows the result of all combination pairs when
we applied Method
p&s
.
For example, when the penalty was 1,
Method
p&s
selected seven candidate pairs and
all of them were correct. Thus, the recall was
100%. From Table 6, we see that the string
penalties of all combination pairs ranged from
1 to 3 and our system selected all of them by
the semantic similarity.
Penalty Method
p&s
1 7/7 (100%)
2 6/6 (100%)
3 2/2 (100%)
Total 15/15 (100%)
Table 6: A result of six different spellings of
?spaghetti? described in Section 1.
4.4 Estimation of expected correct
variant pairs
We estimated how many correct variant pairs
could be selected from the Corpus based on the
precision of Method
p&s
as shown in Table 5.
The result is shown in Table 7. We find that
the number of candidate pairs in the Corpus
was 100,746 for the penalty of 1, and 56,569 for
the penalty of 2, and 40,004 for the penalty of
3.
For example, when the penalty was 2, we esti-
mate that 46,178 out of 56,569 could be selected
as correct variant pairs, since the precision was
81.6% as shown in Table 5. In total, we estimate
that 178,569 out of 197,319 could be selected as
correct variant pairs from the Corpus.
Penalty # of expected variant pairs
1 100,746/100,746 (100%)
2 46,178/56,569 (81.6%)
3 31,645/40,004 (79.1%)
Total 178,569/197,319 (90.5%)
Table 7: Estimation of expected correct variant
pairs.
4.5 Error Analysis-1
As shown in Table 5, our system couldn?t select
two correct variant pairs using semantic sim-
ilarity when the penalties were 1 and 2. We
investigated the reason from the training data.
The problem was caused because the contexts
of the pairs were diffrent. For example, in the
case of ????????? and ????????
?,? which represent the same building material
company ?Aroc Sanwa? of Fukui prefecture in
Japan, their contexts were completely different
because of the following reason.
? ???????, ????????
??????? (Aroc Sanwa): This
word appeared with the name of
an athlete who took part in the
national athletic meet held in Toyama
prefecture in Japan, and the company
sponsored the athlete.
???????? (Aroc?Sanwa): This
word was used to introduce the
company in the article.
Note that each context of these words was
composed of only one article.
4.6 Error Analysis-2
From Table 5, we see that the numbers of incor-
rect variant pairs selected by Method
p&s
were
18 and 14 for each penalty of 2 and 3. We in-
vestigated such cases in the training data. The
example of ???? (Cart, Kart)? and ????
(Card)? is shown as follows.
? ???, ???
??? (Cart, Kart): This word was
used as the abbreviation of ?Shopping
Cart,? ?Racing Kart,? or ?Sport
Kart.?
??? (Card): This word was used as
the abbreviation of ?Credit Card? or
?Cash Card? and was also used as the
meaning of ?Schedule of Games.?
Although these were not a variant pair, our
system regarded the pair as the variant pair,
because their contexts were similar. In both
contexts, ??? (utilization),? ??? (record),?
?? (guest),? ???? (aim),? ???? (team),?
??? (victory),? ??? (high, expensive),? ??
? (success),? ??? (entry),? and so on were
appeared frequently and therefore the semantic
similarity became high.
5 Future Work
In this paper, we have used newspaper articles
to construct Japanese KATAKANA variant list.
We are planning to apply our method on differ-
ent types of corpus, such as patent documents
and Web data. We think that more variations
can be found from Web data. In the case of
?spaghetti? described in Section 1, we found at
least seven more different spellings, such as ?
??????,? ????????,? ??????
?,? ????????? ??????,? ?????
??,? and ????????.?
Although we have manually tuned scoring
rules of the string penalty using training data,
we are planning to introduce an automatic
method for learning the rules.
We will also have to consider other charac-
ter types of Japanese, i.e. KANJI variations
and HIRAGANA variations, though we have fo-
cused on only KATAKANA variations in this
paper. For example, both ????? and ???
??? mean ?move? in Japanese.
6 Conclusion
We have described the method to construct
Japanese KATAKANA variant list from large
corpus. Unlike the previous work, we focused
not only on the similarity in spelling but also
on the semantic similarity.
From the experiments, we found that
Method
p&s
performs better than Method
p
,
since it constructed Japanese KATAKANA
variant list with high performance of 97.4% re-
call and 89.1% precision.
Estimating from the precision, we found that
178,569 out of 197,319 could be selected as cor-
rect variant pairs from the Corpus. The result
could be helpful to solve the variant problems
of information retrieval, information extraction,
question answering, and so on.
References
Patrick A. V. Hall and GEOFF R. DOWLING.
1980. Approximate string matching. Com-
puting Surveys, 12(4):381?402.
Jun?ichi Kubota, Yukie Shoda, Masahiro
Kawai, Hirofumi Tamagawa, and Ryoichi
Sugimura. 1993. A method of detecting
KATAKANA variants in a document. IPSJ
NL97-16, pages 111?117.
Sadao Kurohashi and Makoto Nagao. 1999.
Japanese morphological analysis system JU-
MAN version 3.61. Department of Informat-
ics, Kyoto University.
Masami Shishibori and Jun?ichi Aoe. 1993. A
method for generation and normalization of
katakana variant notations. IPSJ NL94-5,
pages 33?40.
Eiko Yamamoto, Yoshiyuki Takeda, and Kyoji
Umemura. 2003. An IR similarity measure
which is tolerant for morphological variation.
Natural Language Processing, 10(1):63?80.
Coling 2008: Companion volume ? Posters and Demonstrations, pages 181?184
Manchester, August 2008
A Linguistic Knowledge Discovery Tool:  
Very Large Ngram Database Search with Arbitrary Wildcards 
Satoshi Sekine 
New York University 
715 Broadway, 7th floor, New York, NY 10003 
sekine@cs.nyu.edu 
Abstract 
In this paper, we will describe a search tool 
for a huge set of ngrams. The tool supports 
queries with an arbitrary number of wild-
cards. It takes a fraction of a second for a 
search, and can provide the fillers of the 
wildcards. The system runs on a single 
Linux PC with reasonable size memory (less 
than 4GB) and disk space (less than 400GB). 
This system can be a very useful tool for 
linguistic knowledge discovery and other 
NLP tasks. 
1 Introduction 
Currently, NLP research is shifting towards se-
mantic analysis. In order to understand what a 
sentence means, we require substantial back-
ground knowledge which must be gathered in 
advance. Building such knowledge is not an 
easy task. This is the so-called ?knowledge bot-
tleneck? problem, which was one of the major 
reasons for the failure of much AI research in 
the 1980's. However, now, the circumstances 
have quite changed. We have an almost unlim-
ited amount of text and machine power has dras-
tically improved. Using these fortunate assets, 
research on knowledge discovery in NLP is 
booming. The work by (Hearst 92) (Collins and 
Singer 99) (Brin 99) (Hasegawa et al 04) are 
only a few examples of this research direction. 
Notice that most of these methods use local con-
text. For example, a lexico-syntactic pattern, 
like ?NP such as NP? can extract hyponym rela-
tionships (Hearst 92), and contexts between two 
named entities can indicate a relationship be-
tween those names (Hasegawa et al 04). 
  
? 2008. Licensed under the Creative Commons 
Attribution-Noncommercial-Share Alike 3.0 Un-
ported license (http://creativecommons.org/ li-
censes/by-nc-sa/3.0/). Some rights reserved. 
It is quite natural to believe that the larger the 
corpus, the larger and the more reliable the dis-
covered knowledge can be. However, it leads to 
problems in terms of speed and machine power. 
In order to solve these problems, some people 
use commercial search engines (Chklovski and 
Pantel 04). However, using such search engines 
has serious problems: 1) Only a limited number 
of results available, 2) Only a limited number of 
accesses permitted, 3) Mysterious ranking func-
tion, 4) Unstable results over a long time period, 
5) Slow, as it is over the internet. Another idea 
to overcome this difficulty is to create one's own 
(maybe smaller) search engine (Cafarella and 
Etzioni 05) (Shinzato et al 08). Although creat-
ing one's own search engine has advantages (one 
of which is the freedom to design the form of 
the query; such as POS and dependency), it is a 
huge, expensive task; not everybody can afford 
to make a search engine. More seriously, not 
everybody can use it as he/she may want. 
In this paper, we will propose an alternative 
solution which should enable researchers with 
modest resources to conduct research using huge 
corpora for knowledge discovery. It is an ngram 
search tool with the following requirements. 
 
Requirements 
1. It searches for ngrams which include an ar-
bitrary number of wildcards, such as ?* such 
as * and?, ?Mr. * said?, ?from * to * by *? 
or ?* attack by * * on *?. 
2. It returns the fillers of the wildcards as well 
as ngram frequencies 
3. It produces the results in a fraction of a sec-
ond (for most reasonable queries) 
4. It runs on a single PC 
5. It needs only a reasonable amount of mem-
ory (4GB) for processing 
6. It needs only a reasonable amount of disk 
space (400GB) for indexing 108-109 ngrams 
181
2 Algorithm Overview 
There are two reasonable choices for the search 
algorithm. One is ?inverted indexing? (used by 
?lucene? and others) and the other is ?trie?. We 
used trie. Using inverted indexing, it is easy to 
create an index at the cost of runtime speed. We 
prefer an algorithm which requires more com-
plicated indexing in order to achieve the speed 
in searching. Trie is an indexing tree structure in 
which each node represents a symbol (in our 
case, words) and each link represents the se-
quence of the symbols (in our case n-gram). 
Searching can be done by traversing the tree, 
which is usually done in time constant in the 
size of the corpus. However, it is important to 
mention that the trie structure is order sensitive. 
For example, if the query includes wildcards, 
such as ?Mr.  * * said yesterday?, searching the 
trie is not an optimal solution. 
One naive solution is to create a search sys-
tem (or a trie) for each possible combination of 
wildcards. For example, for the query pattern 
?Mr. * * said yesterday?, we should prepare a 
search system for modified ngrams which have 
the first, fourth and fifth words of the original 
ngrams as the first three words. For ngrams of 
length N, the number of possible combinations 
of literals and wildcards is 2N. In theory, if we 
make that many search systems, we can solve 
the problem. However, the number of search 
systems is too large considering the number of 
ngrams we aim to handle (Table 1). Although 
we applied two implementation techniques to 
reduce the size of index (which will be de-
scribed in the next section), it is still likely that 
we could not satisfy requirement #6.  
We solved the problem by using a single 
search system for different kinds of search pat-
terns, reducing the number of needed search 
systems significantly. It can be observed that a 
pattern with wildcards at suffix positions can be 
searched using the same trie used for patterns 
without those wildcards. Also, we don?t always 
need to start the trie by indexing the first word. 
If we build an alternative trie which starts by 
indexing the second word, we can cover more 
patterns with fewer tries. For example, using the 
trie constructed to search for 5-grams ?DEABC? 
(We will call this a ?trie pattern?: each letter 
represents a literal, with A representing the first 
token in the original ngram), four ?search pat-
terns? (i.e. ngram pattern used in the queries), 
?AB*DE?, ?A**DE?, ?***DE? and ?***D*?, 
can be searched efficiently.  
We found that the minimum number of trie 
patterns needed to cover all possible search pat-
terns of length N is N/2 C N. We have con-
structed minimal sets of patterns for all N up to 
9. Once the system receives a query with one or 
more wildcards, it finds the trie pattern which 
covers the search pattern of the query. 
3 Implementation 
We also implemented two ideas to reduce the 
size of the index. One is related to a common 
technique to reduce the size of trie nodes by de-
leting the index of unique suffixes. In addition, 
we don?t store the remaining data within the trie. 
We just store the ngram ID at the node in order 
to further reduce the size of the index. Because 
there are many search systems, storing the 
ngram data in a single master database saves a 
lot of space. When the user wants to see the 
words of an ngram which was identified by the 
search, the system retrieves the ngram from the 
master database using the ngram ID. The benefit 
of this technique is quite large (more than 50% 
reduction of index size in 9gram), as many 
ngrams have long unique suffixes. 
The other idea is based on the fact that once 
the ngram data is provided, no update will be 
requested (i.e. insertion or replacement proce-
dure in the trie is not necessary). We can elimi-
nate the pointers to the parents and the siblings 
for each node, which contributes to about 30% 
additional reduction in the index size. 
Because the tries are very large, we divide 
them into segments (128 segments for 9-grams 
and 118 for 5-grams), so that an individual trie 
index segment is small enough to fit in a mem-
ory of modest size (requirement #5). Each seg-
ment contains a lexicographically contiguous 
sequence of ngrams. Furthermore, we use 
?mmap? to get the index into memory from the 
disk, so that only the portions of the trie seg-
ment that are actually used will be loaded.  
System Flow 
We will briefly describe the system flow 
1) First, all the words in the input query are 
looked up in the dictionary. If there are out of 
vocabulary words, then there is no ngram which 
matches the query. If all words are known, find 
the appropriate trie pattern for the input query 
based on the locations of wildcards.  
2) Then the appropriate segment(s) of the trie 
data is found for the search query. It was done 
182
by searching the table of segment index sorted 
in lexicographic order. 
3) Now, the search in the trie is performed. 
Note that there are 16,128 tries for 9 -gram (128 
segments for 126 trie patterns). If the search 
ends with an internal node, there is more than 
one matched ngram. If the search ends with a 
terminal node, just one ngram matches. If the 
trie ends before the end of query, you have to 
retrieve the ngram from the master database and 
match the retrieved ngram against the query. 
4) Once one or several ngram IDs are identi-
fied as matched ngram(s), the system will output 
the information. There are three output modes. 
a) Only instance and type frequency are printed. 
b) Only ngram IDs are printed. These two types 
can be achieved quickly without consulting the 
ngram master database. c) Ngram instances are 
printed using the ngram master database. 
4 Experiments and Evaluation 
4.1 Ngram data 
We implemented this using two ngram data sets.  
Google 1T Web 5gram 
This is a part of the ngram data provided by 
Google through LDC (Web 1T). The data was 
generated from approximately 1 trillion word 
tokens of publicly accessible Web pages.  
9-grams from 82 years of newspaper 
For knowledge discovery purposes, 5-grams 
are generally unsatisfactory. A 5-gram can only 
cover 2 words of context on each side of a single 
word term. So we extracted 9-grams from a 
number of newspaper corpora available to us. 
Including NANTC: LATWP(94-97), NYT, 
REUFF, REUTE, WSJ (94-96), BBN GigaWord 
corpus (news archive only): BBC(99-06), Peo-
ple Daily, Taipei Times, The Hindu (00-06), 
Arab News, Gulf News, India Times (01-06), 
AQUAINT corpus: APW, NYT (98-00), Xinhua 
(96-00), CSR corpus: WSJ (87-94). 
These corpora are cleaned up by several 
methods, because some of them have article du-
plications/minor variants and some of them con-
tain many non-sentences. We use a simple 
method to reduce such noise, which is to ?uniq? 
all the sentences for each year of each newspa-
per and count each distinct sentence only once. 
This is not a perfect solution, but it reduces a lot 
of noise to an amount almost unnoticeable for 
the ngram search result. The statistics of the data 
are shown in Table 1. 
 
Corpus Google 1T 82 yrs. News
Original Text 1 T words 1.8 G words 
Ngram 5-gram 9-gram 
Threshold 40 2 
#of ngrams 1,176,470,663 119,456,373 
# of patterns 10 126 
# of nodes 1.4G x 10 160M x 126 
Index size 277GB 322GB 
Table 1 Statistics of the data 
4.2 Example 
Obviously, the tool is useful for knowledge dis-
covery tasks for hyponym relations, binary rela-
tions, name extraction, relations between names 
and so on. It is also useful to extract more spe-
cific relations, such as ?what kind of attack oc-
curred by whom on what/when? by searching 
for ?* attack on * * by * **?. Figure 1 shows a 
snapshot of the tool?s output on this query. 
 
 
Figure 1 Snapshot of the output for ?* attack on * * by * * *? 
183
Importantly, because of the speed, we can mod-
ify the query in an interactive manner. The 
speed is helpful for batch processing, too, when 
we need to search millions of patterns. 
4.3 Evaluation 
The speed evaluation was conducted using 
9grams. We randomly selected 1017 9grams 
from the original data to form a test set, and also 
created another test set in which 1 to 3 words are 
randomly replaced by wildcards in those 9grams. 
We measure the average time to find the ngrams. 
In the case of a query with wildcards, we 
checked if the original ngram is found among all 
returned ngrams to verify the accuracy of the 
tool. The system runs in three output modes (we 
use a file output rather than stdout). Producing 
Ngram output takes a long time if the number of 
ngrams is very large, so we limited the number 
of ngrams to be printed to 100. In the experi-
ment, the number of returned ngrams ranged 
from 1 to 100 with an average of 1.26. The re-
sult is shown in Table 2 and the times are given 
in milliseconds. 
 
Print mode Freq. IDs Ngram
Original 19 19 20 
With wildcards 19 19 29 
Table 2 Speed of search (Newspaper 9gram) 
5 Related Work and Discussion 
One of the most related works is the CMU-
Cambridge Statistical LM toolkit (CMU-
Cambridge). It?s a tool to search ngrams in 
Speech fields. However, it does not handle 
wildcards, which is important for knowledge 
discovery purposes. There are several ngram 
tools in the field of genome sequence analysis 
(MUMmer) (REPuter). However, the size of the 
alphabets are quite different (handful of genome 
bases vs. about 1.08 million words), and their 
main purpose is to discover similarity or repeti-
tion. These systems are not directly useful for 
linguistic knowledge discovery purposes. As a 
document retrieval tool, there is a public domain 
search engine, such as ?lucene? (Lucene). 
However, its primary purpose is document re-
trieval and the inverted index algorithm can?t 
handle well very frequent terms. Searching is 
relatively slow. 
It should be noted that creating the index is a 
huge task. It took about 2 months using five 
4GB-memory machines. However, it took only 
about one week with 64GB-memory machine. 
As for the future direction, we are improving 
the tool to show the original sentences from 
which the ngram was extracted. We have some 
evidence that a longer context is needed for the 
knowledge discovery purpose. 
This tool is merely a tool, but we believe it?s 
a very powerful tool for linguistic knowledge 
discovery among other related objectives. The 
next step we would like to take is to apply this 
tool to find linguistic knowledge for NLP appli-
cations.  
Acknowledgements 
This research was supported in part by the Na-
tional Science Foundation under Grant IIS-
00325657. This paper does not necessarily re-
flect the position of the U.S. Government. We 
would like to thank our colleagues at New York 
University, who provided useful suggestions and 
discussions, including Prof. Grishman. 
References 
CMU-Cambridge Statistical LM toolkit homepage: 
http://www.speech.cs.cmu.edu/SLM/toolkit_docu
mentation.html 
Lucene homepage: http://lucene.apache.org/ 
MUMmer hompage: http://mummer.sourceforge.net/ 
REPuter homepage: http://bibiserv.techfak.uni-
bielefeld.de/reputer/ 
Web 1T 5-gram. by Google. LDC Catalog No.:  
LDC2006T13 
M. J. Cafarella and O. Etzioni. ?A Search Engine for 
Natural Language Applications?. 2005. In Proc. of 
World Wide Web Conference. 
S. Brin. ?Extracting Patterns and Relations from the 
World Wide Web?. 1998. In Proc. of Workshop 
on Web and DataBase-98. 
T. Chklovski and P. Pantel. ?VerbOcean: Mining the 
Web for Fine-Grained Semantic Verb Relations?. 
2004. In Proc. of EMNLP-04.  pp. 33-40. 
M. Collins and Y. Singer. ?Unsupervised Models for 
Named Entity Classification?. 1998. In Proc. of 
EMNLP-99. 
M. Hearst. ?Automatic Acquisition of Hyponyms 
from Large Text Corpora?. 1992. In Proc. of 
COLING-92. 
T. Hasegawa, S. Sekine and R. Grishman ?Discover-
ing Relations among Named Entities from Large 
Corpora?. 2004. In Proc. of ACL-04. 
K. Shinzato, T. Shibata, D. Kawahara, C. Hashimoto 
and S. Kurohashi. ?TSUBAKI: An Open Search 
Engine Infrastructure for Developing New Infor-
mation Access Methodology?, 2008. In Proc. of 
the 3rd IJCNLP-08. 
184
Automatic Pattern Acquisition
for Japanese Information Extraction
Kiyoshi Sudo
Computer Science
Department
New York University
715 Broadway, 7th floor,
New York, NY 10003 USA
sudo@cs.nyu.edu
Satoshi Sekine
Computer Science
Department
New York University
715 Broadway, 7th floor,
New York, NY 10003 USA
sekine@cs.nyu.edu
Ralph Grishman
Computer Science
Department
New York University
715 Broadway, 7th floor,
New York, NY 10003 USA
grishman@cs.nyu.edu
ABSTRACT
One of the central issues for information extraction is the cost of
customization from one scenario to another. Research on the auto-
mated acquisition of patterns is important for portability and scala-
bility. In this paper, we introduce Tree-Based Pattern representation
where a pattern is denoted as a path in the dependency tree of a sen-
tence. We outline the procedure to acquire Tree-Based Patterns in
Japanese from un-annotated text. The system extracts the relevant
sentences from the training data based on TF/IDF scoring and the
common paths in the parse tree of relevant sentences are taken as
extracted patterns.
Keywords
Information Extraction, Pattern Acquisition
1. INTRODUCTION
Information Extraction (IE) systems today are commonly based
on pattern matching. New patterns need to be written when we
customize an IE system for a new scenario (extraction task); this is
costly if done by hand. This has led to recent research on automated
acquisition of patterns from text with minimal pre-annotation. Riloff
[4] reported a successful result for her procedure that needs only
a pre-classified corpus. Yangarber [6] developed a procedure for
unannotated natural language texts.
One of their common assumption is that the relevant documents
include good patterns. Riloff implemented this idea by applying the
pre-defined heuristic rules to pre-classified (relevant) documents
and Yangarber advanced further so that the system can classify the
documents by itself given seed patterns specific to a scenario and
then find the best patterns from the relevant document set.
Considering how they represent the patterns, we can see that,
in general, Riloff and Yangarber relied on the sentence structure
of English. Riloff?s predefined heuristic rules are based on syn-
tactic structures, such as ?<subj> active-verb? and ?active-verb
.
<dobj>?. Yangarber used triples of a predicate and some of its
arguments, such as ?<pred> <subj> <obj>?.
The Challenges
Our careful examination of Japanese revealed some of the chal-
lenges for automated acquisition of patterns and information ex-
traction on Japanese(-like) language and other challenges which
arise regardless of the languages.
Free Word-ordering
Free word order is one of the most significant problems in analyz-
ing Japanese. To capture all the possible patterns given a predicate
and its arguments, we need to permute the arguments and list all the
patterns separately. For example, for ?<subj> <dobj> <iobj>
<predicate>? with the constraint that the predicate comes last in
the sentence, there would be six possible patterns (permutations of
three arguments). The number of patterns to cover even simple
facts would rise unacceptably high.
Flexible case marking system
There is also a difficulty in a language with a flexible case marking
system, like Japanese. In particular, we found that, in Japanese,
some of the arguments that are usually marked as object in En-
glish were variously marked by different post-positions, and some
case markers (postpositions) are used for marking more than one
grammatical category in different situations. For example, the topic
marker in Japanese, ?wa?, can mark almost any entity that would
have been variously marked in English. It is difficult to deal with
this variety by simply fixing the number of arguments of a predicate
for creating patterns in Japanese.
Relationships beyond direct predicate-argument
Furthermore, we may want to capture the relationship between a
predicate and a modifier of one of its arguments. In previous ap-
proaches, one had to introduce an ad hoc frame for such a relation-
ship, such as ?verb obj [PP <head-noun>]?, to extract the relation-
ship between ?to assume? and ?<organization>? in the sentence
?<person> will assume the <post> of <organization>?.
Relationships beyond clausal boundaries
Another problem lies in relationships beyond clause boundaries, es-
pecially if the event is described in a subordinate clause. For exam-
ple, for a sentence like ?<organization> announced that <person>
retired from <post>,? it is hard to find a relationship between
<organization> and the event of retiring without the global view
from the predicate ?announce?.
These problems lead IE systems to fail to capture some of the ar-
guments needed for filling the template. Overcoming the problems
above makes the system capable of finding more patterns from the
training data, and therefore, more slot-fillers in the template.
In this paper, we introduce Tree-based pattern representation and
consider how it can be acquired automatically.
2. TREE-BASED PATTERN REPRESENTA-
TION (TBP)
Definition
Tree-based representation of patterns (TBP) is a representation of
patterns based on the dependency tree of a sentence. A pattern is
defined as a path in the dependency tree passing through zero or
more intermediate nodes within the tree. The dependency tree is a
directed tree whose nodes are bunsetsus or phrasal units, and whose
directed arcs denote the dependency between two bunsetsus: A!B
denotes A?s dependency on B (e.g. A is a subject and B is a pred-
icate.) Here dependency relationships are not limited to just those
between a case-marked element and a predicate, but also include
those between a modifier and its head element, which covers most
relationships within sentences. 1
TBP for Information Extraction
Figure 2 shows how TBP is used in comparison with the word-
order based pattern, where A...F in the left part of the figure is a
sequence of the phrasal units in a sentence appearing in this or-
der and the tree in the right part is its dependency tree. To find
the relationship between B!F, a word-order based pattern needs a
dummy expression to hold C, D and E, while TBF can denote the
direct relationship as B!F. TBP can also represent a complicated
pattern for a node which is far from the root node in the depen-
dency tree, like C!D!E, which is hard to represent without the
sentence structure.
For matching with TBP, the target sentence should be parsed into
a dependency tree. Then all the predicates are detected and the
subtrees which have a predicate node as a root are traversed to find
a match with a pattern.
Benefit of TBP
TBP has some advantages for pattern matching over the surface
word-order based patterns in addressing the problems mentioned
in the previous section:
 Free word-order problem
TBP can offer a direct representation of the dependency re-
lationship even if the word-order is different.
 Free case-marking problem
TBP can freely traverse the whole dependency tree and find
any significant path as a pattern. It does not depend on pre-
defined case-patterns as Riloff [4] and Yangarber [6] did.
 Indirect relationships
TBP can find indirect relationships, such as the relationship
between a predicate and the modifier of the argument of the
1In this paper, we used the Japanese parser KNP [1] to obtain the
dependency tree of a sentence.
predicate. For example, the pattern
?<organization> of!<post> to!appoint? can capture the rela-
tionship between ?<organization>? and ?to be appointed?
in the sentence
?<person> was appointed to <post> of <organization>.?
 Relationships beyond clausal boundaries
TBP can capture relationships beyond clausal boundaries.
The pattern ?<post> to!appointCOMP! announce? can find the
relationship between ?<post>? and ?to announce?. This re-
lationship, later on, can be combined with the relationship
?<organization>? and ?to announce? and merged into one
event.
3. ALGORITHM
In this section, we outline our procedure for automatic acquisi-
tion of patterns. We employ a cascading procedure, as is shown
in Figure 3. First, the original documents are processed by a mor-
phological analyzer and NE-tagger. Then the system retrieves the
relevant documents for the scenario as a relevant document set. The
system, further, selects a set of relevant sentences as a relevant sen-
tence set from those in the relevant document set. Finally, all the
sentences in the relevant sentence set are parsed and the paths in
the dependency tree are taken as patterns.
3.1 Document Preprocessing
Morphological analysis and Named Entity (NE) tagging is per-
formed on the training data at this stage. We used JUMAN [2] for
the former and a NE-system which is based on a decision tree algo-
rithm [5] for the latter. Also the part-of-speech information given
by JUMAN is used in the later stages.
3.2 Document Retrieval
The system first retrieves the documents that describe the events
of the scenario of interest, called the relevant document set. A set
of narrative sentences describing the scenario is selected to create
a query for the retrieval. For this experiment, we set the size of
the relevant document set to 300 and retrieved the documents us-
ing CRL?s stochastic-model-based IR system [3], which performed
well in the IR task in IREX, Information Retrieval and Extraction
evaluation project in Japan 2. All the sentences used to create the
patterns are retrieved from this relevant document set.
3.3 Sentence Retrieval
The system then calculates the TF/IDF-based score of relevance
to the scenario for each sentence in the relevant document set and
retrieves the n most relevant sentences as the source of the patterns,
where n is set to 300 for this experiment. The retrieved sentences
will be the source for pattern extraction in the next subsection.
First, the TF/IDF-based score for every word in the relevant doc-
ument set is calculated. TF/IDF score of word w is:
score(w) =
(
TF (w) 
log(N+0:5)
DF (w)
log(N+1)
if w is Noun, Verb or Named Entity
0 otherwise
where N is the number of documents in the collection, TF(w) is
the term frequency of w in the relevant document set and DF(w) is
the document frequency of w in the collection.
Second, the system calculates the score of each sentence based
on the score of its words. However, unusually short sentences and
2IREX Homepage: http://cs.nyu.edu/cs/projects/proteus/irex
Dependency Tree Tree-Based Pattern
f
f f
f f
 
 
 
 
 
 
 
@
@
@
@
@
@
@
 
 
 
 
 
 
 
@
@
@
@
@
@
@
<organization>-wa
<organization>-TOPIC
<person>-ga
<person>-SUBJ
<post>-kara
<post>-FROM
taininsuru(retire)
happyosuru(announce)
 
 
 
 	
1
@
@
@
@R
3
@
@
@
@
@
 
 
 
 
 	
2
f
f
f
f
f
f
f
-
- -
-
happyosuru(announce)
taininsuru(retire)
<organization>-wa(<organization>-TOPIC)
<person>-ga(<person>-SUBJ)
<post>-kara(<post>-FROM)
1
2
3
Figure 1: Tree-Based Pattern Representation
Word-order Pattern
A B C D E F
6
Pattern [B * F]
F
E
BA
D
C


3
P
P
P
Pq


3
Q
Q
s
-
Pattern [B!F]
P
P
P
P
P
P
P
P
Pq
Pattern [C!E!F]TBP
Figure 2: Extraction using Tree-Based Pattern Representation
unusually long sentences will be penalized. The TF/IDF score of
sentence s is:
score(s) =
P
w2s
score(w)
length(s) + jlength(s) AVEj
where length(s) is the number of words in s, and AVE is the av-
erage number of words in a sentence.
3.4 Pattern Extraction
Based on the dependency tree of the sentences, patterns are ex-
tracted from the relevant sentences retrieved in the previous sub-
section. Figure 4 shows the procedure. First, the retrieved sentence
is parsed into a dependency tree by KNP [1] (Stage 1). This stage
also finds the predicates in the tree. Second, the system takes all
the predicates in the tree as the roots of their own subtrees, as is
shown in (Stage 2). Then each path from the root to a node is
extracted, and these paths are collected and counted across all the
relevant sentences. Finally, the system takes those paths with fre-
quency higher than some threshold as extracted patterns. Figure 5
shows examples of the acquired patterns.
4. EXPERIMENT
It is not a simple task to evaluate how good the acquired pat-
terns are without incorporating them into a complete extraction sys-
tem with appropriate template generation, etc. However, finding a
match of the patterns and a portion of the test sentences can be a
good measure of the performance of patterns.
The task for this experiment is to find a bunsetsu, a phrasal unit,
that includes slot-fillers by matching the pattern to the test sentence.
The performance is measured by recall and precision in terms of the
number of slot-fillers that the matched patterns can find; these are
calculated as follows.
Recall =
# of Matched Relevant SlotF illers
# of All Relevant SlotF illers
Original Document Set
Document
Preprocessing
-
Preprocessed Document Set
Document
Retrieval
-
Relevant Document Set
Sentence Retrieval
 
 
 
 
 
 
 
	
Relevant Sentence Set
(Tree representation)
f
f
f
f
f



P
P
P
P




P
P
P
P
Pattern
Extraction
-
f f f
f f f
f f
Extracted Patterns
Figure 3: Pattern Acquisition Procedure Overall Process
Precision =
# of Matched Relevant SlotF illers
# of All Matched SlotF illers
The procedure proposed in this paper is based on bunsetsus, and
an individual bunsetsu may contain more than one slot filler. In
such cases the procedure is given credit for each slot filler.
Strictly speaking, we don?t know how many entities in a matched
pattern might be slot-fillers when, actually, the pattern does not
contain any slot-fillers (in the case of over-generating). We ap-
proximate the potential number of slot-fillers by assigning 1 if the
(falsely) matched pattern does not contain any Named-Entities, or
assigning the number of Named-Entities in the (falsely) matched
pattern. For example, if we have a pattern ?go to dinner? for a
management succession scenario and it matches falsely in some
part of the test sentences, this match will gain one at the number
of All Matched Slot-fillers (the denominator of the precision). On
the other hand, if the pattern is ?<post> <person> laugh? and it
falsely matches ?President Clinton laughed?, this will gain two, the
number of the Named Entities in the pattern.
For the sake of comparison, we defined the baseline system with
the patterns acquired by the same procedure but only from the di-
rect relationships between a predicate and its arguments (PA in Fig-
ure 6 and 7).
We chose the following two scenarios.
 Executive Management Succession: events in which corpo-
rate managers left their positions or assumed new ones re-
gardless of whether it was a present (time of the report) or
past event.
Items to extract: Date, person, organization, title.
 Robbery Arrest: events in which robbery suspects were ar-
rested.
Items to extract: Date, suspect, suspicion.
4.1 Data
Management Succession
Documents 15
Sentences 79
DATE 43
PERSON 41
ORGANIZATION 22
OLD-ORGANIZATION 2
NEW-POST 30
OLD-POST 39
Table 1: Test Set for Management Succession scenario
Robbery Arrest
Documents 28
Sentences 182
DATE 26
SUSPICION 34
SUSPECT 50
Table 2: Test Set for Robbery Arrest scenario
For all the experiments, we used the Mainichi-Newspaper-95
corpus for training. As described in the previous section, the system
retrieved 300 articles for each scenario as the relevant document set
from the training data and it further retrieved 300 sentences as the
relevant sentence set from which all the patterns were extracted.
Test data was taken from Mainichi-Newspaper-94 by manually
reviewing the data for one month. The statistics of the test data are
shown in Table 1 and 2.
4.2 Results
Figure 6 and Figure 7 illustrates the precision-recall curve of this
Stage 1 (Dependency Tree)
<org>-wa
<psn>-ga
<post>-ni
shuninsuru(p)-to
happyoshita(p)
"
"
b
b
"
"
b
b
f
f
f
f
f
-
Stage 2 (Separated Trees)
<org>-wa
<psn>-ga
<post>-ni
happyoshita(p)


Q
Q
k
"
"
b
b
"
"
b
b
Q
Q
k




+


+
1
2
3
4
f
f
f
f
f
<psn>-ga
<post>-ni
shuninsuru(p)
b
b
"
"
Q
Q
k


+
5
6
f
f
f
(p indicates the node is a predicate.)
Extracted Patterns
1 <organization>-wa ! happyosuru
2 <person>-ga ! shuninsuru-to ! happyosuru
3 <post>-ni ! shuninsuru-to ! happyosuru
4 shuninsuru-to ! happyosuru
5 <person>-ga ! shuninsuru
6 <post>-ni ! shuninsuru
Japanese sentence :
English Translation :
<organization>-wa
<organization>-TOPIC
<person>-ga
<person>-SBJ
<post>-ni
<post>-TO
shuninsuru-to
start-COMP
happyoshita.
announced.
(<organization> announced that <person> was appointed to <post>.)
Figure 4: Pattern Acquisition from ?<org>-wa <psn>-ga <pst>-ni shuninsuru-to happyoshita.?
experiment for the executive management succession scenario and
robbery arrest scenario, respectively. We ranked all the acquired
patterns by calculating the sum of the TF/IDF-based score (same
as for sentence retrieval in Section 3.3) for each word in the pattern
and sorting them on this basis. Then we obtained the precision-
recall curve by changing the number of the top-ranked patterns in
the list.
Figure 6 shows that TBP is superior to the baseline system both
in recall and precision. The highest recall for TBP is 34% while the
baseline gets 29% at the same precision level. On the other hand,
at the same level of recall, TBP got higher precision (75%) than the
baseline (70%).
We can also see from Figure 6 that the curve has a slightly anoma-
lous shape where at lower recall (below 20%) the precision is also
low for both TBP and the baseline. This is due to the fact that
the pattern lists for both TBP and the baseline contains some non-
reliable patterns which get a high score because each word in the
patterns gets higher score than others.
Figure 7 shows the result of this experiment on the Robbery Ar-
rest scenario. Although the overall recall is low, TBP achieved
higher precision and recall (as high as 30% recall at 40% of pre-
cision) than the baseline except at the anomalous point where both
TBP and the baseline got a small number of perfect slot-fillers by a
highly ranked pattern, namely ?gotoyogi-de ! taihosuru (to arrest
0 20 40 60 80 100
Precision
0
20
40
60
Recall
+ +
+
+ +
+ +
++
++
+++
+
* *
*
* *
*
*
**
*
**
+ ... TBP
* ... Baseline (PA)
Figure 6: Result on Management Succession Scenario
Scenario Patterns
Executive Succession : <post>-ni ! shokakusuru (to be promoted to <post>)
<post>-ni ! shuninsuru (to assume <post>)
<post>-ni ! shokakusuru ! (to announce an informal decision of promoting
<jinji>-o ! happyosuru somebody to <post>)
Robbery Arrest : satsujin-yogi-de ! taihosuru (to arrest in suspicion of murder)
<date> ! taihosuru (to arrest on <date>)
satsujin-yogi-de ! taihosuru (to arrest in suspicion of murder)
<person>-yogisha ! #-o ! taihosuru (to arrest the suspect, <person>, age #)
Figure 5: Acquired Patterns
0 20 40 60 80 100
Precision
0
20
40
60
Recall
+
+
+
+++
+
++++
*
*
*
**
******
****
+ ... TBP
* ... Baseline (PA)
Figure 7: Result on Robbery Arrest Scenario
on suspicion of robbery)? for the baseline and ?<person> yogisha
! <number>-o ! taihosuru (to arrest the suspect, <person>,
age <number>)?.
5. DISCUSSION
Low Recall
It is mostly because we have not made a class of types of crimes
that the recall on the robbery arrest scenario is low. Once we have
a classifier as reliable as Named-Entity tagger, we can make a sig-
nificant gain in the recall of the system. And in turn, once we have
a class name for crimes in the training data (automatically anno-
tated by the classifier) instead of a separate name for each crime,
it becomes a good indicator to see if a sentence should be used to
acquire patterns. And also, incorporating the classes in patterns can
reduce the noisy patterns which do not carry any slot-fillers of the
template.
For example on the management succession scenario, all the
slot-fillers defined there were able to be tagged by the Named-
Entity tagger [5] we used for this experiment, including the title.
Since we knew all the slot-fillers were in one of the classes, we
also knew those patterns whose argument was not classified any
of the classes would not likely capture slot-fillers. So we could
put more weight on those patterns which contained <person>,
<organization>, <post> and <date> to collect the patterns with
higher performance, and therefore we could achieve high precision.
Erroneous Case Analysis
We also investigated other scenarios, namely train accident and air-
plane accident scenario, which we will not report in this paper.
However, some of the problems which arose may be worth men-
tioning since they will arise in other, similar scenarios.
 Results or Effects of the Target Event
Especially for the airplane accident scenario, most errors were
identified as matching the effect or result of the incident. A
typical example is ?Because of the accident, the airport had
been closed for an hour.? In the airplane accident scenario,
the performance of the document retrieval and the sentence
retrieval is not as good as the other two scenarios, and there-
fore, the frequency of relevant acquired patterns is rather low
because of the noise. Further improvement in retrieval and a
more robust approach is necessary.
 Related but Not-Desired Sentences
If the scenario is specific enough to make it difficult as an IR
task, the result of the document retrieval stage may include
many documents related to the scenario in a broader sense
but not specific enough for IE tasks. In this experiment, this
was the case for the airplane accident scenario. The result of
document retrieval included documents about other accidents
in general, such as traffic accidents. Therefore, the sentence
retrieval and pattern acquisition for these scenarios were af-
fected by the results of the document retrievals.
6. FUTURE WORK
Information Extraction
To apply the acquired patterns to an information extraction task,
further steps are required besides those mentioned above. Since
the patterns are a set of the binary relationships of a predicate and
another element, it is necessary to merge the matched elements into
a whole event structure.
Necessity for Generalization
We have not yet attempted any (lexical) generalization of pattern
candidates. The patterns can be expanded by using a thesaurus
and/or introducing a new (lexical) class suitable for a particular
domain. For example, the class of expressions of flight number
clearly helps the performance on the airplane accident scenario.
Especially, the generalized patterns will help improve recall.
Robust Pattern Extraction
As is discussed in the previous section, the performance of our sys-
tem relies on each component. If the scenario is difficult for the IR
task, for example, the whole result is affected. The investigation of
a more conservative approach would be necessary.
Translingualism
The presented results show that our procedure of automatic pat-
tern acquisition is promising. The procedure is quite general and
addresses problems which are not specific to Japanese. With an ap-
propriate morphological analyzer, a parser that produces a depen-
dency tree and an NE-tagger, our procedure should be applicable to
almost any language.
7. ACKNOWLEDGMENTS
This research is supported by the Defense Advanced Research
Projects Agency as part of the Translingual Information Detec-
tion, Extraction and Summarization (TIDES) program, under Grant
N66001-00-1-8917 from the Space and Naval Warfare Systems Cen-
ter San Diego. This paper does not necessarily reflect the position
or the policy of the U.S. Government.
8. REFERENCES
[1] S. Kurohashi and M. Nagao. Kn parser : Japanese
dependency/case structure analyzer. In the Proceedings of the
Workshop on Sharable Natural Language Resources, 1994.
[2] Y. Matsumoto, S. Kurohashi, O. Yamaji, Y. Taeki, and
M. Nagano. Japanese morphological analyzing system:
Juman. Kyoto University and Nara Institute of Science and
Technology, 1997.
[3] M. Murata, K. Uchimoto, H. Ozaku, and Q. Ma. Information
retrieval based on stochastic models in irex. In the
Proceedings of the IREX Workshop, 1994.
[4] E. Riloff. Automatically generating extraction patterns from
untagged text. In the Proceedings of Thirteenth National
Conference on Artificial Intelligence (AAAI-96), 1996.
[5] S. Sekine, R. Grishman, and H. Shinnou. A decision tree
method for finding and classifying names in japanese texts. In
the Proceedings of the Sixth Workshop on Very Large
Corpora, 1998.
[6] R. Yangarber, R. Grishman, P. Tapanainen, and S. Huttunen.
Unsupervised discovery of scnario-level patterns for
information extraction. In the Proceedings of the Sixth Applied
Natural Language Processing Conference, 2000.
A System to Solve Language Tests for Second Grade Students 
Manami Saito 
Nagaoka University of Technology 
saito@nlp.nagaokaut.ac.jp 
Kazuhide Yamamoto 
Nagaoka University of Technology 
yamamoto@fw.ipsj.or.jp 
Satoshi Sekine 
New York University 
Language Craft 
sekine@cs.nyu.edu 
Hitoshi Isahara 
National Institute of Information and Com-
munications Technology 
isahara@nict.go.jp 
 
 
Abstract 
This paper describes a system which 
solves language tests for second grade 
students (7 years old). In Japan, there 
are materials for students to measure 
understanding of what they studied, 
just like SAT for high school students 
in US. We use textbooks for the stu-
dents as the target material of this study. 
Questions in the materials are classified 
into four types: questions about Chi-
nese character (Kanji), about word 
knowledge, reading comprehension, 
and composition. This program doesn?t 
resolve the composition and some other 
questions which are not easy to be im-
plemented in text forms. We built a 
subsystem for each finer type of ques-
tions. As a result, we achieved 55% - 
83% accuracy in answering questions 
in unseen materials. 
1 Introduction 
This paper describes a system which solves lan-
guage tests for second grade students (7 years 
old). We have the following two objections. 
First, we aim to realize the NLP technologies 
into the form which can be easily observed by 
ordinary people. It is difficult to evaluate NLP 
technology clearly by ordinary people. Thus, we 
set the target to answer second grade Japanese 
language test, as an example of intelligible ap-
plication to ordinary people. The ability of this 
program will be shown by scores which are fa-
miliar to ordinary people. 
Second aim is to observe the problems of the 
NLP technologies by degrading the level of tar-
get materials. Those of the current NLP research 
are usually difficult, such as newspapers or tech-
nological texts. They require high accuracy lan-
guage processing, complex world knowledge or 
semantic processing. The NLP problems would 
become more apparent when we degrade the 
target materials. Although questions for second 
grade students also require world knowledge, it 
is expected that the questions become simpler 
and are resolved without tangled techniques.  
2 Related Works 
Hirschman et al (1999) and Charniak et al 
(2000) proposed systems to solve ?Reading 
Comprehension.? Hirschman et al (1999) de-
veloped ?Deep Read,? which is a system to se-
lect sentences in the text which include answers 
to a question. In their experiments, the types of 
questions are limited to ?When,? ?Who? and so 
on. The system is basically an information re-
trieval system which selects a sentence, instead 
of a document, based on the bag-of-words 
method. That system retrieves the sentence con-
taining the answer at 30-40% of the time on the 
tests of third to sixth grade materials. In short, 
Deep Read is very restricted compared to our 
system. Charniak et al (2000) built a system 
improved over the Deep Read by giving more 
weights for verb and subject, and introduced 
heuristic rules for ?Why? question. Though, the 
essential target and method are the same as that 
of Deep Read. 
43
3 Question Classification 
First, we bought five language test books for 
second grade students and one of them, pub-
lished by KUMON, was used as a training text 
to develop our system. The other four books are 
referred occasionally. Second, we classified the 
questions in the training text into four types: 
questions about Chinese character (Kanji), ques-
tions on word knowledge, reading comprehen-
sion, and composition. We will call these types 
as major types. Each of the ?major types? is 
classified into several ?minor types.? Table 1 
shows four major types and their minor types. In 
practice, each minor type farther has different 
style of questions; such as description question, 
choice question, and true-false question. The 
questions can be classified into approximately 
100 categories. We observed that some ques-
tions in other books are mostly similar; however 
there are several questions which are not cov-
ered by the categories. 
 
Major type Minor type 
Kanji Reading, Writing, Radical, The 
order of writing, Classification 
Word knowl-
edge 
Katakana, How to use Kana, Ap-
propriate Noun, To fill blanks for 
Verb, Adjective, and Adjunct, 
Synonym, Antonym, Particle, 
Conjunction, Onomatopoeia, Po-
lite Expression, Punctuation mark
Reading com-
prehension 
Who, What, When, Where, How, 
Why question, Extract specific 
phrases, Progress order of a story, 
Prose and Verse  
Composition Constructing sentence, How to 
write composition 
Table 1. Question types 
4 Answering questions and evaluation 
result 
In this section, we describe the programs to 
solve the questions for each of the 100 catego-
ries and these evaluation results. First we classi-
fied questions. Some questions are difficult to 
cover by the system such as the stroke order of 
writing Kanji. For about 90% of all the question 
types other than such questions, we created pro-
grams for basically one or two categories of 
questions. There are 47 programs for the catego-
ries found in the training data. 
In each section of 4.1 to 4.3, we describe 
how to solve questions of typical types and the 
evaluation results. The evaluation results of the 
total system will be reported in the following 
section. 
Table 2 to 4 show the distributions of minor 
types in each major type, ?Kanji,? ?Word 
knowledge,? and ?Reading comprehension,? in 
the training data and the evaluation results. 
Training and evaluation data have no overlap.  
4.1 Kanji questions 
Most of the Kanji questions are categorized into 
?reading? and ?writing.? Morphological analysis 
is used in the questions of both reading and writ-
ing Kanji; we found that large corpus is effec-
tive to choose the answer from Kanji candidates 
given from dictionary. Table 2 shows it in detail. 
This system cannot answer to the questions 
which are asking the order of writing Kanji, be-
cause it is difficult to put it into digital format.  
The system made 7 errors out of 334 ques-
tions. The most of the questions are the errors in 
reading Kanji by morphological analysis. 
In particular, morphological analysis is the 
only effective method to answer questions on 
this type. It would be very helpful, if we had a 
large corpus considering reading information, 
but there is no such corpus.  
 
Training 
data 
Test data Ques-
tion type
The rate 
of Q in 
training 
data[%]
The used 
knowledge 
and tools Correct 
ans. 
(total) 
Correct ans. 
(known type 
Q., total) 
Reading 27 Kanji dictionary, 
Morphological 
analysis 
 
96(100) 
 
6(8,8) 
Writing 61 Word diction-
ary, Large 
corpus 
 
220(222) 
 
63(66,66) 
Order of 
writing 
6 -  
0(20) 
 
0(0,2) 
Combi-
nation 
of Kanji 
parts 
3 -  
0(10) 
 
0(0,0) 
Classify 
Kanji 
3 Word diction-
ary, Thesaurus 
 
11(12) 
 
0(0,0) 
Total 100 - 327(364) 69(74,76) 
Table 2. Question types for Kanji 
4.2  Word knowledge questions 
The word knowledge question dealt with vo-
cabulary, different kinds of words and the struc-
ture of sentence. These don?t include Kanji 
questions and reading comprehension. Table 3 
shows different types of questions in this type. 
44
For the questions on antonym, the system can 
answer correctly by choosing most relevant an-
swer candidate using the large corpus out of 
multiple candidates found in the antonym dic-
tionary. 
The questions about synonyms ask relations 
of priority/inferiority between words and choos-
ing the word in a different group. These ques-
tions can usually be answered using thesaurus. 
Ex.1 shows a question about particle, Japa-
nese postposition, which asks to select the most 
appropriate particle for the sentence. 
The system produces all possible sentences 
with the particle choices, and finds most likely 
sentences in a corpus. In Ex.1, all combinations 
are not in a corpus, therefore shorter parts of the 
sentence are used to find in the corpus (e.g. ??
?? (1) ????, ???? (2) ????). In this 
case, the most frequent particle for (1) is ??? in 
a corpus, so this system outputs incorrect answer. 
Ex.1 [?/?/?]????()??????? 
?????? 
 Select particle which fits the sentence from 
{wo,to,ni} 
[1] ??? (1) ??? (2) ??? 
apple-(1) orange-(2) buy 
(1) correct=? (to) system=? (wo) 
(2) correct=? (wo) system=? (wo) 
 
The questions of Katakana can be answered 
mostly by the dictionary. The accuracy of this 
type is not so high, found in Table 2, because 
there are questions asking the origin of the 
words, most Katakana words in Japanese has a 
few origins: borrowed words, onomatopoeia, 
and others. Because we don?t have such knowl-
edge, we could not answer those questions.   
The questions of onomatopoeia include those 
shown in Ex.2. The system uses co-occurrence 
of words in the given sentence and each answer 
candidate to choose the correct answer in Ex.2, 
?????.? However, it was not chosen be-
cause the co-occurrence frequency of ????
?,? the word in the sentence, and ?????,? 
incorrect answer, is higher.  
Ex.2 ??? ???? ???? ??? 
? [ ] ?? ??????? ??????? 
Choose the most appropriate onomatopoeia 
(1) ??? ??? ???? ????  
????(A large object is rowing slowly) 
[??????????????] 
The questions of word knowledge are classi-
fied into 29 types. We made a subsystem for 
each type. As there are possibly more types in 
other books, making a subsystem for each type 
is very costly. One of the future directions of 
this study is to solve this problem. 
Training 
data 
Test data Question 
type 
The rate 
of Q in 
training 
data[%] 
The used 
knowledge and 
tools Correct 
ans. 
(total) 
Correct ans. 
(known type 
Q., total) 
Anonym 18 Antonym 
dictionary, 
Large corpus 
26(27) 12(15,21) 
Synonym 11 Thesaurus 14(17) 34(44,83) 
Particle 19 Large corpus 25(28) 16(17,17) 
Katakana 25 Word diction-
ary, Morpho-
logical analysis 
18(37) 19(22,52) 
Onomato-
poeia 
19 Large corpus, 
Morphological 
analysis 
18(29) 16(20,31) 
Structure 
of sen-
tence 
5 Morphological 
analysis 
7(7) 20(22,22) 
How to 
use kana 
2 - 0(3) 0(0,19) 
Dictation 
of verb 
2 - 0(3) 0(0,0) 
Total 100 - 108(151) 117(140,245)
Table 3. Question types for Word knowledge 
4.3 Reading comprehension questions 
The reading comprehension questions need to 
read a story and answer questions on the story. 
We will describe five typical techniques that are 
used at different types of questions, shown in 
Table 4. 
Pattern matching (a) is used in questions to 
fill blanks in an expression which describes a 
part of the story. In general, the sequence of 
word used in the matching is the entire expres-
sion, but if no match was found, smaller por-
tions are used in the matching. 
Ex.3 Fill blanks in the expression 
Story?partial??????? ???? 
?? ?? ????????? ????  
?? ???? ?????? 
(In a few days, the flower withers and gradu-
ally changes its color to black.) 
Expression???? (1)?(2) ?? ????? 
(The flower (1) and change its color to (2).) 
Answer?(1) ???? (withers) 
(2) ???? (black) 
The effectiveness of this technique is found 
in this example. The other methods will be 
needed when questions will be more difficult. At 
the time, this technique is very useful to solve 
many questions in reading comprehension.  
45
When the question is ?Why? question, key-
words such as ????  (thus)? and ????
(because)? are used. 
For example, when questions start with 
?When (??)? and ?Where (??),? we can 
restrict the type of answer word to time or loca-
tion, respectively. If the question includes the 
expression of ????? (??? is a particle to 
indicate direction/specification), the answer is 
also likely to be expressed with ?ni? right after 
the location in the story. (The kind of NE 
(Named Entity) and particle right after word 
(b)) 
For the questions asking the time or location 
about the entire story, this system outputs the 
appropriate type of the word which appeared 
first in the story. Although there are mistakes 
due to morphological analysis and NE extraction, 
this technique is also consistently very useful.  
The technique which is partial matching 
with keywords (c) is used to seek an answer 
from story for ?how,? ?why? or ?of what? ques-
tions. Keywords from the question are used to 
locate the answer in the story. 
Ex.4 ???????? ?????????
? ???
Frequency in the large corpus is used to 
find the appropriate sentence conjunction. (d) 
Answer is chosen by comparing the mutual in-
formation of the candidate conjunctions and the 
last basic block of the previous sentence. How-
ever, this technique turns out to be not effective. 
Discourse analysis considering wider context is 
required to solve this question. 
The technique which uses distance between 
keywords in question and answers (e) is sup-
plementary to the abovementioned methods. If 
multiple answers are found, the answer candi-
date that is the closest in the story text to the 
keywords in questions is generated. These key-
words are content words and unknown words in 
the text. This technique is found very effective. 
In Table 4, the column ?Used method? shows 
the techniques used to solve the type of ques-
tions, in the order of priority. ?f? in the table 
denotes means that we use a method which was 
not mentioned above.  
????(How big are chicks when 
they hatched?) 
Text?partial??????????? ??  
 ???????????? ??????? 
(The size of chicks when they hatched is about 
the size of your thumb.) 
 
 
 Answer?????? ???? (size of 
thumb)  
 
The rate of Training data Test data
questions in Used corrent wns. corrent ans.
training data [%] methods (total) (known type Q, total)
Who said 5 b,a,f
The others 0 b,e,f
Like what c,b,e
Of what c,f,e
What doing c,a,f
What is a,e
What do A say b,a,f,e
Whole story b
Part of story -
Whole story b
Part of story b,f,c
16 c,f 11(18) 0(1, 1)
10 c 8(11) 0(0, 1)
2 b,c,f 1(2) 0(0, 0)
10 a 10(12) 4(9, 9)
4 - 0(5) 0(0, 0)
2 d 1(2) 1(3, 3)
10 f 8(11) 0(0, 0)
10 f 7(12) 3(3, 3)
1 - 0(1) 0(0, 6)
100 - 74(116) 10(22, 34)
Why
How
How long, how often, how large
Total
Paragraph
The others
To fill blanks
Not have interrogative pronoun
Conjunction
Progress order of a story
Where 4 3(5) 0(1
When 4 3(5) 0(0
Who
Question type
17(26) 1(1, 6)What 22
5(6) 1(4, 4)
, 1)
, 0)
 
Table 4. Question types for Reading comprehension 
46
5 Evaluation 
We collected questions for the test from differ-
ent books of the training data. The proposition 
of the number of questions for different sections 
is not the same as that of the training data. Table 
2 to 4 show the evaluation results in the test data 
for each type. Table 5 shows the summary of the 
evaluation result. In the test, we use only the 
questions of the type in training data. The tables 
also show the total number of questions, the 
number of questions which are solved correctly, 
and the number of questions which are not one 
of the types the system targeted (not a type in 
the training data). 
The ratio of the questions covered by the sys-
tem, questions in test data which have the same 
type in the training data are 97.4% in Kanji, 
57.1% in word knowledge, and 64.7% in read-
ing comprehension. It indicates that about a half 
of the questions in word knowledge isn?t cov-
ered. As the result, accuracy on the questions of 
the covered types in word knowledge is 83.6%, 
but it drops to 47.8% for the entire questions. It 
is because our system classified the questions 
into many small types and builds a subsystem 
for each type. 
The accuracy for the questions of covered 
type is 83.4%. In particular, for the questions of 
Kanji and word knowledge, the scores in the test 
data are nearly the same as those in the training 
data. It presents that the accuracy of the system 
is provided that the question is in the covered 
type. However, the score of reading comprehen-
sion is lower in the test data. We believe that 
this is mainly due to the small test data of read-
ing comprehension (only 34) and that the accu-
racy for ?Who? questions and the questions to 
fill blanks in the test data are quite difficult com-
pared to the training data. 
Num. Num. Num. RCA * RCA* RCA*
of of of (known (total? in total
all Q known corrent type Q) [%] of known
type Q ans. [%] type Q [%]
Kanji 76 74 69 93.2 90.8 89.8
Word knowledge 245 140 117 83.6 47.8 71.5
Reading 
Comprehension
Total 355 235 196 83.4 55.2 80.3
45.5 29.4 63.834 22 10
 
Table 5. Evaluations at test data 
 
* Rate of Correct Answer 
6 Discussions 
We will discuss the problems and future direc-
tions found by the experiments. 
6.1 Recognition and Classification of Ques-
tions 
In order to solve a question in language test, 
students have to recognize the type of the ques-
tion. The current system skips this process. In 
this system, we set up about 100 types of ques-
tions referring the training data and a subpro-
gram solves questions corresponding to each 
type. There are two problems to be solved. First, 
we have to design the appropriate classification 
and avoid unknown types in the test data. From 
the experiment, we found that the current types 
are not enough to solve this problem. Second, 
the program has to classify the questions auto-
matically. We are building this system and are 
forecasting it quite optimistically once a good 
format is provided. 
6.2 Effectiveness of Large Corpus 
The large corpus of newspapers and the Web are 
used effectively in many different cases. We 
will describe several examples. 
In Japanese, there are different Kanji for the 
same reading. For example, Kanji for ???(au: 
to see, to solve correctly) are ???(to see)? or 
???(to solve correctly)? for ?????(to see 
people)? and ??????(to solve an answer 
correctly),? respectively. This type of questions 
can be solved by counting the expressions with 
Kanji in the corpus. It is similar to word sense 
disambiguation. 
In the questions of particle complement, such 
as ??? (umbrella) ??/?/? (locative-, con-
junctive-, and objective particles) ?? (home) 
??/?/?????? (to left) ? (Intentional 
sentence is ?I left the umbrella at home?)?, it can 
be solved by counting the expressions with each 
particle in a corpus. This method is mentioned in 
Matsui?2004?but the evaluation result was 
not reported. When the answer is not found for 
the entire expression, the answer is searched by 
deleting some contexts. Most questions of filling 
blank types, similar strategy is helpful to find 
the correct answer. 
In summary, the experiments showed that the 
large corpus is quite useful in several types of 
47
questions. We believe it would be quite difficult 
to achieve the same accuracy by compiled 
knowledge, such as a dictionary of verbs, anto-
nyms, synonyms, and relation words, and a the-
saurus.  
6.3 World Knowledge 
The questions sometimes need various types of 
world knowledge. For example, ?A student en-
ters junior high school after graduated from 
elementary school.? And ?People become happy, 
if he receives something nice from someone.? It 
is a difficult problem how to describe and how 
use that knowledge. Another type of world 
knowledge includes origin of words, such as 
foreign borrowed word or onomatopoeia. As far 
as we know, there is no comprehensive knowl-
edge of such in electronic form. It is required to 
design attributes of world knowledge and to use 
them flexibly when applying then to solve the 
questions. 
6.4 Difference between Reading Compre-
hension and Question Answering 
The current QA systems identify the NE type of 
questions and seek the answer candidate of the 
type. However, the questions in the reading 
comprehension don?t limit the answer types to 
person and organization, even if the question is 
?Who? type question. For example, ?raccoon 
dog behind our house? or ?the moon? can be the 
answer. Also, the answer is not always a noun 
phrase, but can be a clause, for example, ?the 
time when new leaves growing on a branch? for 
questions asking time. There are different kinds 
of questions, which are asking not the time of 
specific event but the time or season of the en-
tire story. For example ?When is this story 
about?? In this case, the question can?t be an-
swered by just extracting a noun phrase.  
However, at the moment, we can?t conclude 
if the question can or cannot be answered with-
out really understanding it. Sometime, we can 
find a correct answer without reading the story 
down the line or understanding the story per-
fectly. It is one of the future works. 
6.5 Other techniques: discourse and 
anaphora 
Some techniques other than morphological 
analysis, frequency of appearance in a corpus, 
and question answering methods are used in our 
system. We raise two issues. One of those is the 
discourse analysis. It is required in the questions 
to assign the order of paragraphs, and to select 
appropriate sentence conjunction. The other is 
anaphora analysis, which is very important, not 
only to indicate the antecedent, but also to find 
the link of mentions of entities.  
7 Conclusion 
 several inter-
esting NLP problems were found. 
Hi
Comprehension system?.  
Ch
er-based Language Understanding 
K.
of 
K. 
atural Language Processing, 2004, 
 
We develop a system to solve questions of sec-
ond grade language tests. Our objectives are to 
demonstrate the NLP technologies to ordinary 
people, and to observe the problems of NLP by 
degrading the level of target materials. We 
achieved 55% - 83% accuracy and
References 
rschman, L., Light, M., Breck, E. and Burger, J. D. 
?Deep READ: a Reading 
ACL, 1999, pp 325-332. 
arniak et al, ?Reading Comprehension Programs 
in a Statistical-language-Processing Class?. Work-
shop on Reading Comprehension Tests as Evalua-
tion for Comput
Systems. 2000. 
 Matsui: ?Search Technologies on WWW which 
utilize search engines?. (In Japanese) Journal 
Japanese Language, February, 2004, pp 34-43. 
Yoshihira, Y. Takeda, S. Sekine: ?KWIC System 
on WEB documents?, (In Japanese) 10th Annual 
Meeting of N
pp 137-139. 
 
F  
(http://languagecraft.jp/dennou/) 
igure 1. A Snapshot of the system
48
Automatic Paraphrase Discovery based on 
Context and Keywords between NE Pairs 
Satoshi Sekine 
New York University 
715 Broadway, 7th floor 
New York, NY 10003 USA 
sekine@cs.nyu.edu
Abstract
Automatic paraphrase discovery is an 
important but challenging task. We 
propose an unsupervised method to 
discover paraphrases from a large 
untagged corpus, without requiring any 
seed phrase or other cue. We focus on 
phrases which connect two Named En-
tities (NEs), and proceed in two stages. 
The first stage identifies a keyword in 
each phrase and joins phrases with the 
same keyword into sets. The second 
stage links sets which involve the same 
pairs of individual NEs. A total of 
13,976 phrases were grouped. The ac-
curacy of the sets in representing para-
phrase ranged from 73% to 99%, 
depending on the NE categories and set 
sizes; the accuracy of the links for two 
evaluated domains was 73% and 86%. 
1 Introduction 
One of the difficulties in Natural Language 
Processing is the fact that there are many ways 
to express the same thing or event. If the expres-
sion is a word or a short phrase (like ?corpora-
tion? and ?company?), it is called a ?synonym?. 
There has been a lot of research on such lexical 
relations, along with the creation of resources 
such as WordNet. If the expression is longer or 
complicated (like ?A buys B? and ?A?s purchase 
of B?), it is called ?paraphrase?, i.e. a set of 
phrases which express the same thing or event. 
Recently, this topic has been getting more atten-
tion, as is evident from the Paraphrase Work-
shops in 2003 and 2004, driven by the needs of 
various NLP applications. For example, in In-
formation Retrieval (IR), we have to match a 
user?s query to the expressions in the desired 
documents, while in Question Answering (QA), 
we have to find the answer to the user?s question 
even if the formulation of the answer in the 
document is different from the question. Also, in 
Information Extraction (IE), in which the system 
tries to extract elements of some events (e.g. 
date and company names of a corporate merger 
event), several event instances from different 
news articles have to be aligned even if these are 
expressed differently. 
We realize the importance of paraphrase; 
however, the major obstacle is the construction 
of paraphrase knowledge. For example, we can 
easily imagine that the number of paraphrases 
for ?A buys B? is enormous and it is not possi-
ble to create a comprehensive inventory by hand. 
Also, we don?t know how many such paraphrase 
sets are necessary to cover even some everyday 
things or events. Up to now, most IE researchers 
have been creating paraphrase knowledge (or IE 
patterns) by hand and for specific tasks. So, 
there is a limitation that IE can only be per-
formed for a pre-defined task, like ?corporate 
mergers? or ?management succession?. In order 
to create an IE system for a new domain, one 
has to spend a long time to create the knowledge. 
So, it is too costly to make IE technology ?open-
domain? or ?on-demand? like IR or QA. 
In this paper, we will propose an unsuper-
vised method to discover paraphrases from a 
large untagged corpus. We are focusing on 
phrases which have two Named Entities (NEs), 
as those types of phrases are very important for 
IE applications. After tagging a large corpus 
with an automatic NE tagger, the method tries to 
find sets of paraphrases automatically without 
being given a seed phrase or any kinds of cue.  
80
2 Algorithm 
2.1 Overview
Before explaining our method in detail, we pre-
sent a brief overview in this subsection. 
First, from a large corpus, we extract all the 
NE instance pairs. Here, an NE instance pair is 
any pair of NEs separated by at most 4 syntactic
chunks; for example, ?IBM plans to acquire Lo-
tus?. For each pair we also record the context,
i.e. the phrase between the two NEs (Step1).
Next, for each pair of NE categories, we collect
all the contexts and find the keywords which are 
topical for that NE category pair. We use a sim-
ple TF/IDF method to measure the topicality of
words. Hereafter, each pair of NE categories
will be called a domain; e.g. the ?Company ?
Company? domain, which we will call CC-
domain (Step 2). For each domain, phrases
which contain the same keyword are gathered to 
build a set of phrases (Step 3). Finally, we find
links between sets of phrases, based on the NE 
instance pair data (for example, different phrases
which link ?IBM? and ?Lotus?) (Step 4).  As we 
shall see, most of the linked sets are paraphrases.
This overview is illustrated in Figure 1. 
Figure 1. Overview of the method
2.2 Step by Step Algorithm 
In this section, we will explain the algorithm
step by step with examples. Because of their size,
the examples (Figures 2 to 4) appear at the end 
of the paper. 
Step 1. Extract NE instance pairs with contexts
First, we extract NE pair instances with their 
context from the corpus. The sentences in the
corpus were tagged by a transformation-based
chunker and an NE tagger. The NE tagger is a 
rule-based system with 140 NE categories [Se-
kine et al 2004]. These 140 NE categories are 
designed by extending MUC?s 7 NE categories 
with finer sub-categories (such as Company,
Institute, and Political Party for Organization; 
and Country, Province, and City for Location)
and adding some new types of NE categories 
(Position Title, Product, Event, and Natural Ob-
ject). All the NE pair instances which co-occur
separated by at most 4 chunks are collected
along with information about their NE types and
the phrase between the NEs (the ?context?). Fig-
ure 2 shows examples of extracted NE pair in-
stances and their contexts. The data is sorted 
based on the frequency of the context (?a unit 
of? appeared 314 times in the corpus) and the 
NE pair instances appearing with that context 
are shown with their frequency (e.g. ?NBC? and
?General Electric Co.? appeared 10 times with
the context ?a unit of?). 
Step 2. Find keywords for each NE pair
When we look at the contexts for each domain,
we noticed that there is one or a few important
words which indicate the relation between the 
NEs (for example, the word ?unit? for the phrase 
?a unit of?). Once we figure out the important
word (e.g. keyword), we believe we can capture 
the meaning of the phrase by the keyword. We
used the TF/ITF metric to identify keywords.
Corpus
All the contexts collected for a given domain 
are gathered in a bag and the TF/ITF scores are
calculated for all the words except stopwords in 
the bag. Here, the term frequency (TF) is the 
frequency of a word in the bag and the inverse 
term frequency (ITF) is the inverse of the log of
the frequency in the entire corpus. Figure 3
shows some keywords with their scores. 
Step 3. Gather phrases using keywords
Next, we select a keyword for each phrase ? the 
top-ranked word based on the TF/IDF metric.
(If the TF/IDF score of that word is below a 
threshold, the phrase is discarded.)  We then
NE pair instances 
keywords
Sets of phrases 
based on keywords
Links between
sets of phrases
Step 1 
Step 2 
Step 4 
Step 3 
81
gather all phrases with the same keyword.  Fig-
ure 4 shows some such phrase sets based on 
keywords in the CC-domain. 
Step 4. Cluster phrases based on Links
We now have a set of phrases which share a 
keyword. However, there are phrases which ex-
press the same meanings even though they do 
not share the same keyword. For example, in 
Figure 3, we can see that the phrases in the 
?buy?, ?acquire? and ?purchase? sets are mostly 
paraphrases. At this step, we will try to link 
those sets, and put them into a single cluster. 
Our clue is the NE instance pairs. If the same 
pair of NE instances is used with different 
phrases, these phrases are likely to be para-
phrases. For example, the two NEs ?Eastern 
Group Plc? and ?Hanson Plc? have the follow-
ing contexts. Here, ?EG? represents ?Eastern 
Group Plc?. and ?H? represents ?Hanson Plc?. 
xEG, has agreed to be bought by H 
xEG, now owned by H 
xH to acquire EG 
xH?s agreement to buy EG 
Three of those phrases are actually paraphrases, 
but sometime there could be some noise; such as 
the second phrase above. So, we set a threshold 
that at least two examples are required to build a 
link. More examples are shown in Figure 5. 
Notice that the CC-domain is a special case. 
As the two NE categories are the same, we can?t 
differentiate phrases with different orders of par-
ticipants ? whether the buying company or the 
to-be-bought company comes first. The links 
can solve the problem. As can be seen in the 
example, the first two phrases have a different 
order of NE names from the last two, so we can 
determine that the last two phrases represent a 
reversed relation. In figure 4, reverse relations 
are indicated by `*? next to the frequency. 
Now we have sets of phrases which share a 
keyword and we have links between those sets. 
3 Experiments 
3.1 Corpora
For the experiments, we used four newswire 
corpora, the Los Angeles Times/Washington 
Post, The New York Times, Reuters and the 
Wall Street Journal, all published in 1995. They 
contain about 200M words (25M, 110M, 40M 
and 19M words, respectively). All the sentences 
have been analyzed by our chunker and NE tag-
ger. The procedure using the tagged sentences to 
discover paraphrases takes about one hour on a 
2GHz Pentium 4 PC with 1GB of memory. 
3.2 Results
In this subsection, we will report the results of 
the experiment, in terms of the number of words, 
phrases or clusters. We will report the evalua-
tion results in the next subsection. 
Step 1. Extract NE pair instances with contexts
From the four years of newspaper corpus, we 
extracted 1.9 million pairs of NE instances. The 
most frequent NE category pairs are ?Person - 
Person (209,236), followed by ?Country - Coun-
try? (95,123) and ?Person - Country? (75,509). 
The frequency of the Company ? Company do-
main ranks 11th with 35,567 examples. 
As lower frequency examples include noise, 
we set a threshold that an NE category pair 
should appear at least 5 times to be considered 
and an NE instance pair should appear at least 
twice to be considered. This limits the number 
of NE category pairs to 2,000 and the number of 
NE pair instances to 0.63 million.  
Step 2. Find keywords for each NE pair
The keywords are found for each NE category 
pair. For example, in the CC-domain, 96 key-
words are found which have TF/ITF scores 
above a threshold; some of them are shown in 
Figure 3. It is natural that the larger the data in 
the domain, the more keywords are found. In the 
?Person ? Person? domain, 618 keywords are 
found, and in the ?Country ? Country? domain, 
303 keywords are found. In total, for the 2,000 
NE category pairs, 5,184 keywords are found. 
Step 3. Gather phrases using keywords
Now, the keyword with the top TF/ITF score is 
selected for each phrase. If a phrase does not 
contain any keywords, the phrase is discarded. 
For example, out of 905 phrases in the CC-
domain, 211 phrases contain keywords found in 
step 2. In total, across all domains, we kept 
13,976 phrases with keywords. 
82
Step 4. Link phrases based on instance pairs
Using NE instance pairs as a clue, we find links 
between sets of phrases. In the CC-domain,
there are 32 sets of phrases which contain more
than 2 phrases. We concentrate on those sets.
Among these 32 sets, we found the following
pairs of sets which have two or more links. Here 
a set is represented by the keyword and the 
number in parentheses indicates the number of
shared NE pair instances. 
buy - acquire (5) buy - agree (2) 
buy - purchase (5) buy - acquisition (7) 
buy - pay (2)* buy - buyout (3) 
buy - bid (2) acquire - purchase (2) 
acquire - acquisition (2) 
acquire - pay (2)*    purchase - acquisition (4) 
purchase - stake (2)* acquisition - stake (2)* 
unit - subsidiary (2) unit - parent (5) 
It is clear that these links form two clusters
which are mostly correct. We will describe the 
evaluation of such clusters in the next subsection.
3.3 Evaluation Results 
We evaluated the results based on two metrics.
One is the accuracy within a set of phrases 
which share the same keyword; the other is the 
accuracy of links. We picked two domains, the 
CC-domain and the ?Person ? Company? do-
main (PC-domain), for the evaluation, as the
entire system output was too large to evaluate. It
is not easy to make a clear definition of ?para-
phrase?. Sometimes extracted phrases by them-
selves are not meaningful to consider without
context, but we set the following criteria. If two 
phrases can be used to express the same rela-
tionship within an information extraction appli-
cation (?scenario?), these two phrases are
paraphrases. Although this is not a precise crite-
rion, most cases we evaluated were relatively
clear-cut. In general, different modalities
(?planned to buy?, ?agreed to buy?, ?bought?)
were considered to express the same relationship
within an extraction setting. We did have a prob-
lem classifying some modified noun phrases
where the modified phrase does not represent a
qualified or restricted form of the head, like 
?chairman? and ?vice chairman?, as these are
both represented by the keyword ?chairman?. In
this specific case, as these two titles could fill 
the same column of an IE table, we regarded
them as paraphrases for the evaluation. 
Evaluation within a set
The evaluation of paraphrases within a set of
phrases which share a keyword is illustrated in
Figure 4. For each set, the phrases with brack-
eted frequencies are considered not paraphrases
in the set. For example, the phrase ?'s New
York-based trust unit,? is not a paraphrase of the 
other phrases in the ?unit? set. As you can see in 
the figure, the accuracy for the domain is quite 
high except for the ?agree? set, which contains 
various expressions representing different rela-
tionships for an IE application. The accuracy is 
calculated as the ratio of the number of para-
phrases to the total number of phrases in the set.
The results, along with the total number of 
phrases, are shown in Table 1. 
Domain # of phrases totalphrases accuracy
7 or more 105 87.6%CC 6 or less 106 67.0%
7 or more 359 99.2%PC 6 or less 255 65.1%
Table 1. Evaluation results within sets 
Table 1 shows the evaluation result based on 
the number of phrases in a set. The larger sets 
are more accurate than the small sets. We can 
make several observations on the cause of errors. 
One is that smaller sets sometime have meaning-
less keywords, like ?strength? or ?add? in the 
CC-domain, or ?compare? in the PC-domain.
Eight out of the thirteen errors in the high fre-
quency phrases in the CC-domain are the
phrases in ?agree?. As can be seen in Figure 3, 
the phrases in the ?agree? set include completely
different relationships, which are not para-
phrases. Other errors include NE tagging errors 
and errors due to a phrase which includes other 
NEs. For example, in the phrase ?Company-A
last week purchased rival Marshalls from Com-
pany-B?, the purchased company is Marshalls,
not Company-B. Also there are cases where one 
of the two NEs belong to a phrase outside of the
relation. For example, from the sentence ?Mr. 
Smith estimates Lotus will make a profit this 
quarter??,  our system extracts ?Smith esti-
83
mates Lotus? as an instance. Obviously ?Lotus? 
is part of the following clause rather than being 
the object of ?estimates? and the extracted in-
stance makes no sense. We will return to these 
issues in the discussion section. 
Evaluation of links
A link between two sets is considered correct if 
the majority of phrases in both sets have the 
same meaning, i.e. if the link indicates para-
phrase. All the links in the ?CC-domain are 
shown in Step 4 in subsection 3.2. Out of those 
15 links, 4 are errors, namely ?buy - pay?, ?ac-
quire - pay?, ?purchase - stake? ?acquisition - 
stake?. When a company buys another company, 
a paying event can occur, but these two phrases 
do not indicate the same event. The similar ex-
planation applies to the link to the ?stake? set. 
We checked whether the discovered links are 
listed in WordNet.  Only 2 link in the CC-
domain (buy-purchase, acquire-acquisition) and 
2 links (trader-dealer and head-chief) in the PC-
domain are found in the same synset of Word-
Net 2.1 (http://wordnet.princeton.edu/). This 
result suggests the benefit of using the automatic 
discovery method. 
Domain Link accuracy WN coverage 
CC 73.3 % 2/11
PC 88.9% 2/8
Table 2. Evaluation results for links 
4 Related Work 
The work reported here is closely related to [Ha-
segawa et al 04]. First, we will describe their 
method and compare it with our method. They 
first collect the NE instance pairs and contexts, 
just like our method. However, the next step is 
clearly different. They cluster NE instance pairs 
based on the words in the contexts using a bag-
of-words method. In order to create good-sized 
vectors for similarity calculation, they had to set 
a high frequency threshold, 30. Because of this 
threshold, very few NE instance pairs could be 
used and hence the variety of phrases was also 
limited. Instead, we focused on phrases and set 
the frequency threshold to 2, and so were able to 
utilize a lot of phrases while minimizing noise. 
[Hasegawa et al 04] reported only on relation 
discovery, but one could easily acquire para-
phrases from the results. The number of NE in-
stance pairs used in their experiment is less than 
half of our method. 
There have been other kinds of efforts to dis-
cover paraphrase automatically from corpora. 
One of such approaches uses comparable docu-
ments, which are sets of documents whose con-
tent are found/known to be almost the same, 
such as different newspaper stories about the 
same event [Shinyama and Sekine 03] or differ-
ent translations of the same story [Barzilay 01]. 
The availability of comparable corpora is lim-
ited, which is a significant limitation on the ap-
proach.
Another approach to finding paraphrases is to 
find phrases which take similar subjects and ob-
jects in large corpora by using mutual informa-
tion of word distribution [Lin and Pantel 01].  
This approach needs a phrase as an initial seed 
and thus the possible relationships to be ex-
tracted are naturally limited. 
There has also been work using a bootstrap-
ping approach [Brin 98; Agichtein and Gravano 
00; Ravichandran and Hovy 02]. The basic 
strategy is, for a given pair of entity types, to 
start with some examples, like several famous 
book title and author pairs; and find expressions 
which contains those names; then using the 
found expressions, find more author and book 
title pairs. This can be repeated several times to 
collect a list of author / book title pairs and ex-
pressions. However, those methods need initial 
seeds, so the relation between entities has to be 
known in advance. This limitation is the obsta-
cle to making the technology ?open domain?. 
5 Discussion 
Keywords with more than one word
In the evaluation, we explained that ?chairman? 
and ?vice chairman? are considered paraphrases. 
However, it is desirable if we can separate them. 
This problem arises because our keywords con-
sist of only one word. Sometime, multiple words 
are needed, like ?vice chairman?, ?prime minis-
ter? or ?pay for? (?pay? and ?pay for? are differ-
ent senses in the CC-domain). One possibility is 
to use n-grams based on mutual information. If 
there is a frequent multi-word sequence in a 
domain, we could use it as a keyword candidate. 
84
Keyword detection error
Even if a keyword consists of a single word, 
there are words which are not desirable as key-
words for a domain. As was explained in the 
results section, ?strength? or ?add? are not de-
sirable keywords in the CC-domain. In our ex-
periment, we set the threshold of the TF/ITF 
score empirically using a small development 
corpus; a finer adjustment of the threshold could 
reduce the number of such keywords. 
Also, ?agree? in the CC-domain is not a de-
sirable keyword. It is a relatively frequent word 
in the domain, but it can be used in different 
extraction scenarios. In this domain the major 
scenarios involve the things they agreed on, 
rather than the mere fact that they agreed. 
?Agree? is a subject control verb, which domi-
nates another verb whose subject is the same as 
that of ?agree?; the latter verb is generally the 
one of interest for extraction.  We have checked 
if there are similar verbs in other major domains, 
but this was the only one. 
Using structural information
As was explained in the results section, we ex-
tracted examples like ?Smith estimates Lotus?, 
from a sentence like ?Mr. Smith estimates Lotus 
will make profit this quarter??. In order to 
solve this problem, a parse tree is needed to un-
derstand that ?Lotus? is not the object of ?esti-
mates?. Chunking is not enough to find such 
relationships. This remains as future work. 
Limitations
There are several limitations in the methods. 
The phrases have to be the expressions of length 
less than 5 chunks, appear between two NEs. 
Also, the method of using keywords rules out 
phrases which don?t contain popular words in 
the domain. We are not claiming that this 
method is almighty. Rather we believe several 
methods have to be developed using different 
heuristics to discover wider variety of para-
phrases.
Applications
The discovered paraphrases have multiple appli-
cations. One obvious application is information 
extraction. In IE, creating the patterns which 
express the requested scenario, e.g. ?manage-
ment succession? or ?corporate merger and ac-
quisition? is regarded as the hardest task. The 
discovered paraphrases can be a big help to re-
duce human labor and create a more comprehen-
sive pattern set. Also, expanding on the 
techniques for the automatic generation of ex-
traction patterns (Riloff 96; Sudo 03) using our 
method, the extraction patterns which have the 
same meaning can be automatically linked, ena-
bling us to produce the final table fully auto-
matically. While there are other obstacles to 
completing this idea, we believe automatic para-
phrase discovery is an important component for 
building a fully automatic information extraction 
system. 
6 Conclusion 
We proposed an unsupervised method to dis-
cover paraphrases from a large untagged corpus. 
We are focusing on phrases which have two 
Named Entities (NEs), as those types of phrases 
are very important for IE applications. After 
tagging a large corpus with an automatic NE 
tagger, the method tries to find sets of para-
phrases automatically without being given a 
seed phrase or any kind of cue. In total 13,976 
phrases are assigned to sets of phrases, and the 
accuracy on our evaluation data ranges from 65 
to 99%, depending on the domain and the size of 
the sets. The accuracies for link were 73% and 
86% on two evaluated domains. These results 
are promising and there are several avenues for 
improving on these results. 
7 Acknowledgements 
This research was supported in part by the De-
fense Advanced Research Projects Agency as 
part of the Translingual Information Detection, 
Extraction and Summarization (TIDES) pro-
gram, under Grant N66001-001-1-8917 from the 
Space and Naval Warfare Systems Center, San 
Diego, and by the National Science Foundation 
under Grant IIS-00325657. This paper does not 
necessarily reflect the position of the U.S. Gov-
ernment.  
We would like to thank Prof. Ralph Grish-
man, Mr. Takaaki Hasegawa and Mr. Yusuke 
Shinyama for useful comments, discussion and 
evaluation.
85
References
Agichtein, Eugene and Gravano, Luis. 2000. Snow-
ball: Extracting reations from large plain-text col-
locations. In Proc.  5th ACM Int?l Conf. on Digital 
Libruaries (ACM DL00) pp 85-94. 
Barzilay, Regina and McKeown, Kathleen. 2001. 
Extracting paraphrases from a parallel corpus. In 
Proc.  39th Annual Meeting  Association for Com-
putational Linguistics (ACL-EACL01), pp 50-57.
Brin, Sergey. 1998. Extracting patterns and relations 
from world wide web. In Proc. WebDB Workshop 
at 6th Int?l Conf. on Extending Database Technol-
ogy (WebDB98), pp172-183.
Hasegawa, Takaaki, Sekine, Satoshi and Grishman, 
Ralph. 2004. Discovering Relations among 
Named Entities from Large Corpora, In Proc.  
42nd Annual Meeting  Association for Computa-
tional Linguistics (ACL04), pp 415-422 
Hearst, Marti A. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc  Four-
teenth Int?l Conf. on Computational Linguistics 
(COLING92). 
Lin, Dekang and Pantel, Patrick. 2001. Dirt ? discov-
ery of inference rules from text. In Proc.  7th ACM 
SIGKDD Int?l Conf. on Knowledge Discovery and 
Data Mining (KDD01), pp323-328 
Ravichandran, Deepak and Hovy, Eduard. 2002. 
Learning Surface Text Patterns for a Question An-
swering System. In Proc.  Annual Meeting  Asso-
ciation for Computational Linguistics (ACL02) 
Riloff E. 1996. Automatically Generating Extraction 
Patterns from Untagged Text. In Proc. 13th Na-
tional Conf. on Artificial Intelligence (AAAI96), 
1044-1049.
Sekine, Satoshi and Nobata , Chikashi. 2004. Defini-
tion, Dictionary and Tagger for Extended Named 
Enties. In Proc. of the Fourth Int?l Conf. on Lan-
guage Resource and Evaluation (LREC 04)
Shinyama, Yusuke and Sekine, Satoshi. 2003. Para-
phrase acquisition for information extraction. In 
Proc. 2nd Int?l Workshop on Paraphrasing 
(IWP03) 
Sudo, Kiyoshi, Sekine, Satoshi and Grishman, Ralph. 
2003. An improved extraction pattern representa-
tion model for automatic IE pattern acquisition. In 
Proc. 41st Annual Meeting Association for Com-
putational Linguistics (ACL03)
# COMPANY COMPANY : 22535 
@ 314 , a unit of 
10 NBC   General Electric Co. 
9  Citibank  Citicorp 
7  Smith Barney  Travelers Group Inc. 
6  20th Century Fox the News Corp. 
5  Salomon Brothers Salomon Inc. 
5  Fidelity  FMR Corp. 
5  GTE Mobilnet  GTE Corp 
4  Smith Barney  Travelers Inc. 
?
@ 108 , a subsidiary of 
5  U.S. Ecology Inc. American Ecology Corp. 
3  Pulte Home Corp. Pulte Corp. 
?
Figure 2. Extracted NE pair instances and context 
4846.2 519 44778 buy 
3682.8 205 261 share 
3609.1 354 18186 unit 
2949.2 289 18021 parent 
2850.6 258 8523 acquire 
2709.9 275 25541 agree 
1964.1 163 4020 subsidiary 
1237.9 119 14959 purchase 
1036.9 94 8649 acquisition 
593.7 40 843 sell 
585.6 55 12000 stake 
581.3 63 50868 pay 
Figure 3. High TF/ITF words in ?Com-Com? 
(Numbers are TF/ITF score, frequency in the collec-
tion (TF), frequency in the corpus (TF) and word) 
=== buy === 
97 agreed to buy 
84 bought 
50 said it will buy 
45 said it agreed to buy 
25 will buy 
23 to buy 
20 plans to buy 
16 , which bought 
14 is buying 
11 said it would buy 
11 offered to buy 
10 's agreement to buy 
9 , which is buying 
9* agreed to be bought by 
8 is offering to buy 
7 said it wants to buy 
7 was buying 
6 tried to buy 
6 said it plans to buy 
6 said it intends to buy 
86
6* was bought by 
5 is offering to buy the portion of 
5 is expected to announce plans to buy 
5 is in talks to buy 
5 would buy 
5 succeeds in buying 
5 , said it 's buying 
=== unit === 
314 , a unit of 
24 is a unit of 
6* 's New York-based trust unit , 
5 a unit of 
=== parent === 
108 , the parent of 
81 , parent of 
56 , the parent company of 
14 , parent company of 
10* 's parent , 
9* 's parent company , 
6* , whose parent company is 
=== acquire === 
70 acquired 
38 said it will acquire 
23 agreed to acquire 
16 will acquire 
16* agreed to be acquired by 
14* , has agreed to be acquired by 
13 to acquire 
9 said it agreed to acquire 
8* was acquired by 
8* , which agreed to be acquired by 
7 would acquire 
7 said it would acquire 
7* is being acquired by 
6* , which was acquired by 
6* , which is being acquired by 
5 , which acquired 
5 succeeds in acquiring 
=== agree === 
(8) agreed to merge with 
(8) said it agreed to purchase 
(8) , agreed to accept any offer by 
(6) agreed to pay $ 19 billion for 
(6) has already agreed to make 
(5) agreed to pay 
(5) agreed to sell 
=== subsidiary === 
108 , a subsidiary of 
10 is a subsidiary of 
(8) 's Brown & Williamson subsidiary , 
7 , a wholly owned subsidiary of 
5 a subsidiary of 
5* 's U.S. subsidiary , 
5 , both subsidiaries of 
5 will become a subsidiary of 
=== purchase === 
51 's purchase of 
7 purchased 
7 an option to purchase 
6 for a six-year option to purchase 
(6) purchased Sterling Winthrop from 
6 recently completed its purchase of 
6 completes its purchase of 
6 's purchase of the 37 percent of 
(6) last week purchased rival Marshalls from 
(5) 's purchase of S.G. Warburg Group Plc , 
5 's $ 5.4 billion purchase of 
=== acquisition === 
41 's acquisition of 
21 's proposed acquisition of 
11 's planned acquisition of 
6 's $ 3.7 billion acquisition of 
(5) , Dresdner Bank AG 's planned acquisition 
of
5 's pending acquisition of 
5 completed the $1 billion stock acquisition of 
Figure 4. Gathered phrases using keywords 
(* indicates reverse relation, () indicates it is not 
paraphrase of the other phrases in the set) 
=== Union Pacific Corp. Southern Pacific Rail Corp. 
8 - in its takeover by 
2 + agreed to buy 
2 + said it will buy 
=== United Airlines UAL 
26 - , the parent of 
5 - , parent of 
4 - , the holding company for 
=== Eastern Group Plc Hanson Plc 
13 + , has agreed to be acquired by 
8 + , now owned by 
2 - to acquire 
2 - 's agreement to buy 
=== American Airlines AMR 
18 - , the parent of 
4 - , the holding company for 
2 - , the parent company of 
=== International Business Machines Corp. Lotus 
Development Corp. 
3 + said it would buy 
2 + 's bid for 
2 - agreed to be bought by 
Figure 5. Examples of NE instance pairs for links 
?+? indicates the same order of NEs, 
 ?-? indicates the reverse order 
87
pre-CODIE ? Crosslingual On-Demand Information Extraction
Kiyoshi Sudo Satoshi Sekine Ralph Grishman
Computer Science Department
New York University
715 Broadway, 7th Floor,
New York, NY
 
sudo,sekine,grishman  @cs.nyu.edu
1 Introduction
Our research addresses two central issues of informa-
tion extraction ? portability and multilinguality. We
are creating information extraction (IE) systems that take
foreign-language input and generate English tables of ex-
tracted information, and that can be easily adapted to new
extraction tasks. We want to minimize the human in-
tervention required for customization to a new scenario
(type of facts or events of interest), and allow the user
to interact with the system entirely in English. As a
prototype, we have developed the pre-CODIE system,
an experimental Crosss-lingual On-Demand Information
Extraction system that extracts facts or events of interest
from Japanese source text without requiring user knowl-
edge of Japanese.
2 Overview
To minimize the customization of the IE system across
scenarios, the extraction procedure of pre-CODIE is
driven by the query from the user. The user starts the
procedure by specifying the type of facts or events of
interest in the form of a narrative description, and then
pre-CODIE customizes itself to the topic by acquiring
extraction patterns based on the user?s description. Pre-
CODIE, as an early attempt at a fully-automated system,
still needs user interaction for template definition and slot
assignment; automating these steps is left as future work.
Pre-CODIE interacts with its user entirely in English;
even for slot assignment of the extraction patterns, the
system translates the Japanese extraction patterns, which
are based on subtrees of a dependency tree (Sudo et al,
2001), by word-to-word translation of each lexical item in
the patterns. For ease of use, the Japanese extraction pat-
terns are not only translated into English, but also shown
with translated example sentences which match the pat-
tern.
3 System Architecture
Pre-CODIE is implemented as an integration of several
modules, as shown in Figure 1: translation, information
retrieval, pattern acquisition, and extraction.
Figure 1: system architecture
First, the system takes the narrative description of
the scenario of interest as an English query, and the
Translation module (off-the-shelf IBM King of Transla-
tion system) generates a Japanese query. The IR mod-
ule retrieves a set of relevant documents from Japanese
Mainichi Newspaper from 1995. Then, the Pattern Ac-
quisition module produces a list of extraction patterns
from the relevant document set sorted by their relevance
to the scenario (Sudo et al, 2001). Pre-CODIE asks the
user to assign each placeholder in the patterns to one of
the slots in the template. Finally, the Extraction module
performs the pattern matching with slot-assigned patterns
to each text in the relevant document set and generates a
filled Japanese template, which is translated slot-by-slot
into English for the user.
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 25-26
                                                         Proceedings of HLT-NAACL 2003
4 An Example procedure: Management
Succession
From the user?s point of view, pre-CODIE works as fol-
lows with the screenshots in Figure 2.
1. Query: The user types in the narrative descrip-
tion of the scenario of interest, one phrase in ?de-
scription? text-box and more detail optionally given
in ?narrative? text-box: ?Management Succession:
...?.
2. Configuration: The user adds and/or deletes the
slots in the template; Add ?Person-In?, ?Person-
Out?, ?Post-In?, ?Post-Out?, and ?Organization?.
3. Slot Assignment: The user assigns a slot to each
placeholder in the pattern by choosing one of
the slots defined in step 2; Assign ?(be-promoted
(PERSON-SBJ))? to ?Person-In?.
Also, the user can see the example sentences with
the match of the pattern highlighted. This will make
it easier for the user to understand what each pattern
aims to extract.
4. Extraction: The user gets the extracted template
and repeats this procedure until the user gets the
right template by going back to step 3 to change
and/or add slot assignments, and by going back to
step 2 to delete and/or add slots in the template.
Acknowledgments
This research is supported by the Defense Advanced Re-
search Projects Agency under Grant N66001-00-1-8917
from the Space and Naval Warfare Systems Center San
Diego. This paper does not necessarily reflect the posi-
tion or the policy of the U.S. Government.
References
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2001. Automatic pattern acquisition for japanese in-
formation extraction. In Proceedings of the Human
Language Technology Conference (HLT2001), San
Diego, California.
(1)
(2)
(3)
(4)
Figure 2: Screenshots of an Example procedure: Each
image corresponds to the procedure in Section 4.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 304?311,
New York, June 2006. c?2006 Association for Computational Linguistics
Preemptive Information Extraction using Unrestricted Relation Discovery
Yusuke Shinyama Satoshi Sekine
New York University
715, Broadway, 7th Floor
New York, NY, 10003
{yusuke,sekine}@cs.nyu.edu
Abstract
We are trying to extend the boundary of
Information Extraction (IE) systems. Ex-
isting IE systems require a lot of time and
human effort to tune for a new scenario.
Preemptive Information Extraction is an
attempt to automatically create all feasible
IE systems in advance without human in-
tervention. We propose a technique called
Unrestricted Relation Discovery that dis-
covers all possible relations from texts and
presents them as tables. We present a pre-
liminary system that obtains reasonably
good results.
1 Background
Every day, a large number of news articles are cre-
ated and reported, many of which are unique. But
certain types of events, such as hurricanes or mur-
ders, are reported again and again throughout a year.
The goal of Information Extraction, or IE, is to re-
trieve a certain type of news event from past articles
and present the events as a table whose columns are
filled with a name of a person or company, accord-
ing to its role in the event. However, existing IE
techniques require a lot of human labor. First, you
have to specify the type of information you want and
collect articles that include this information. Then,
you have to analyze the articles and manually craft
a set of patterns to capture these events. Most exist-
ing IE research focuses on reducing this burden by
helping people create such patterns. But each time
you want to extract a different kind of information,
you need to repeat the whole process: specify arti-
cles and adjust its patterns, either manually or semi-
automatically. There is a bit of a dangerous pitfall
here. First, it is hard to estimate how good the sys-
tem can be after months of work. Furthermore, you
might not know if the task is even doable in the first
place. Knowing what kind of information is easily
obtained in advance would help reduce this risk.
An IE task can be defined as finding a relation
among several entities involved in a certain type of
event. For example, in the MUC-6 management
succession scenario, one seeks a relation between
COMPANY, PERSON and POST involved with hir-
ing/firing events. For each row of an extracted ta-
ble, you can always read it as ?COMPANY hired
(or fired) PERSON for POST.? The relation between
these entities is retained throughout the table. There
are many existing works on obtaining extraction pat-
terns for pre-defined relations (Riloff, 1996; Yangar-
ber et al, 2000; Agichtein and Gravano, 2000; Sudo
et al, 2003).
Unrestricted Relation Discovery is a technique to
automatically discover such relations that repeatedly
appear in a corpus and present them as a table, with
absolutely no human intervention. Unlike most ex-
isting IE research, a user does not specify the type
of articles or information wanted. Instead, a system
tries to find all the kinds of relations that are reported
multiple times and can be reported in tabular form.
This technique will open up the possibility of try-
ing new IE scenarios. Furthermore, the system itself
can be used as an IE system, since an obtained re-
lation is already presented as a table. If this system
works to a certain extent, tuning an IE system be-
comes a search problem: all the tables are already
built ?preemptively.? A user only needs to search
for a relevant table.
304
Article dump be-hit
2005-09-23 Katrina New Orleans
2005-10-02 Longwang Taiwan
2005-11-20 Gamma Florida
Keywords: storm, evacuate, coast, rain, hurricane
Table 1: Sample discovered relation.
We implemented a preliminary system for this
technique and obtained reasonably good perfor-
mance. Table 1 is a sample relation that was ex-
tracted as a table by our system. The columns of the
table show article dates, names of hurricanes and the
places they affected respectively. The headers of the
table and its keywords were also extracted automat-
ically.
2 Basic Idea
In Unrestricted Relation Discovery, the discovery
process (i.e. creating new tables) can be formulated
as a clustering task. The key idea is to cluster a set
of articles that contain entities bearing a similar rela-
tion to each other in such a way that we can construct
a table where the entities that play the same role are
placed in the same column.
Suppose that there are two articles A and B,
and both report hurricane-related news. Article A
contains two entities ?Katrina? and ?New Orleans?,
and article B contains ?Longwang? and ?Taiwan?.
These entities are recognized by a Named Entity
(NE) tagger. We want to discover a relation among
them. First, we introduce a notion called ?basic
pattern? to form a relation. A basic pattern is a
part of the text that is syntactically connected to
an entity. Some examples are ?X is hit? or ?Y?s
residents?. Figure 1 shows several basic patterns
connected to the entities ?Katrina? and ?New Or-
leans? in article A. Similarly, we obtain the basic
patterns for article B. Now, in Figure 2, both enti-
ties ?Katrina? and ?Longwang? have the basic pat-
tern ?headed? in common. In this case, we connect
these two entities to each other. Furthermore, there
is also a common basic pattern ?was-hit? shared by
?New Orleans? and ?Taiwan?. Now, we found two
sets of entities that can be placed in correspondence
at the same time. What does this mean? We can infer
that both entity sets (?Katrina?-?New Orleans? and
?Longwang?-?Taiwan?) represent a certain relation
that has something in common: a hurricane name
Katrina New Orleans
headed
threatened
is-category-5
...
was-hit
has-been-evacuated
-residents
...
Basic patternsfor entity "Katrina" Basic patternsfor entity "New Orleans"article A
1. 2.
. . . . . . . . .
Figure 1: Obtaining basic patterns.
Katrina New Orleans
headed
threatened
is-category-5
...
was-hit
has-been-evacuated
-residents
...article A
1. 2.
. . . . . . . . .
Longwang Taiwan
hit
headed
swirling
...
?s-coast
was-pounded
was-hit
...article B
1. 2.
. . . . . . . . .
Common Pattern"stroke" Common Pattern"was-hit"
article A Katrina New Orleans
article B Longwang Taiwan
ObtainedTable
Figure 2: Finding a similar relation from two articles.
and the place it affected. By finding multiple par-
allel correspondences between two articles, we can
estimate the similarity of their relations.
Generally, in a clustering task, one groups items
by finding similar pairs. After finding a pair of arti-
cles that have a similar relation, we can bring them
into the same cluster. In this case, we cluster articles
by using their basic patterns as features. However,
each basic pattern is still connected to its entity so
that we can extract the name from it. We can con-
sider a basic pattern to represent something like the
?role? of its entity. In this example, the entities that
had ?headed? as a basic pattern are hurricanes, and
the entities that had ?was-hit? as a basic pattern are
the places it affected. By using basic patterns, we
can align the entities into the corresponding column
that represents a certain role in the relation. From
this example, we create a two-by-two table, where
each column represents the roles of the entities, and
each row represents a different article, as shown in
the bottom of Figure 2.
We can extend this table by finding another article
305
in the same manner. In this way, we gradually extend
a table while retaining a relation among its columns.
In this example, the obtained table is just what an IE
system (whose task is to find a hurricane name and
the affected place) would create.
However, these articles might also include other
things, which could represent different relations. For
example, the governments might call for help or
some casualties might have been reported. To ob-
tain such relations, we need to choose different en-
tities from the articles. Several existing works have
tried to extract a certain type of relation by manu-
ally choosing different pairs of entities (Brin, 1998;
Ravichandran and Hovy, 2002). Hasegawa et al
(2004) tried to extract multiple relations by choos-
ing entity types. We assume that we can find such
relations by trying all possible combinations from
a set of entities we have chosen in advance; some
combinations might represent a hurricane and gov-
ernment relation, and others might represent a place
and its casualties. To ensure that an article can have
several different relations, we let each article belong
to several different clusters.
In a real-world situation, only using basic patterns
sometimes gives undesired results. For example,
?(President) Bush flew to Texas? and ?(Hurricane)
Katrina flew to New Orleans? both have a basic pat-
tern ?flew to? in common, so ?Bush? and ?Kat-
rina? would be put into the same column. But we
want to separate them in different tables. To allevi-
ate this problem, we put an additional restriction on
clustering. We use a bag-of-words approach to dis-
criminate two articles: if the word-based similarity
between two articles is too small, we do not bring
them together into the same cluster (i.e. table). We
exclude names from the similarity calculation at this
stage because we want to link articles about the same
type of event, not the same instance. In addition, we
use the frequency of each basic pattern to compute
the similarity of relations, since basic patterns like
?say? or ?have? appear in almost every article and it
is dangerous to rely on such expressions.
Increasing Basic Patterns
In the above explanation, we have assumed that we
can obtain enough basic patterns from an article.
However, the actual number of basic patterns that
one can find from a single article is usually not
enough, because the number of sentences is rather
small in comparison to the variation of expressions.
So having two articles that have multiple basic pat-
terns in common is very unlikely. We extend the
number of articles for obtaining basic patterns by
using a cluster of comparable articles that report the
same event instead of a single article. We call this
cluster of articles a ?basic cluster.? Using basic clus-
ters instead of single articles also helps to increase
the redundancy of data. We can give more confi-
dence to repeated basic patterns.
Note that the notion of ?basic cluster? is different
from the clusters used for creating tables explained
above. In the following sections, a cluster for creat-
ing a table is called a ?metacluster,? because this is
a cluster of basic clusters. A basic cluster consists
of a set of articles that report the same event which
happens at a certain time, and a metacluster consists
of a set of events that contain the same relation over
a certain period.
We try to increase the number of articles in a basic
cluster by looking at multiple news sources simulta-
neously. We use a clustering algorithm that uses a
vector-space-model to obtain basic clusters. Then
we apply cross-document coreference resolution to
connect entities of different articles within a basic
cluster. This way, we can increase the number of ba-
sic patterns connected to each entity. Also, it allows
us to give a weight to entities. We calculate their
weights using the number of occurrences within a
cluster and their position within an article. These
entities are used to obtain basic patterns later.
We also use a parser and tree normalizer to gen-
erate basic patterns. The format of basic patterns
is crucial to performance. We think a basic pat-
tern should be somewhat specific, since each pat-
tern should capture an entity with some relevant con-
text. But at the same time a basic pattern should
be general enough to reduce data sparseness. We
choose a predicate-argument structure as a natural
solution for this problem. Compared to traditional
constituent trees, a predicate-argument structure is
a higher-level representation of sentences that has
gained wide acceptance from the natural language
community recently. In this paper we used a logical
feature structure called GLARF proposed by Mey-
ers et al (2001a). A GLARF converter takes a syn-
tactic tree as an input and augments it with several
306
Katrina
hit
coast
SBJ OBJ
Louisiana
T-POS
?s
SUFFIX
Figure 3: GLARF structure of the sentence ?Katrina
hit Louisiana?s coast.?
features. Figure 3 shows a sample GLARF structure
obtained from the sentence ?Katrina hit Louisiana?s
coast.? We used GLARF for two reasons: first,
unlike traditional constituent parsers, GLARF has
an ability to regularize several linguistic phenom-
ena such as participial constructions and coordina-
tion. This allows us to handle this syntactic variety
in a uniform way. Second, an output structure can
be easily converted into a directed graph that rep-
resents the relationship between each word, without
losing significant information from the original sen-
tence. Compared to an ordinary constituent tree, it is
easier to extract syntactic relationships. In the next
section, we discuss how we used this structure to
generate basic patterns.
3 Implementation
The overall process to generate basic patterns and
discover relations from unannotated news articles is
shown in Figure 4. Theoretically this could be a
straight pipeline, but due to the nature of the im-
plementation we process some stages separately and
combine them in the later stage. In the following
subsection, we explain each component.
3.1 Web Crawling and Basic Clustering
First of all, we need a lot of news articles from mul-
tiple news sources. We created a simple web crawler
that extract the main texts from web pages. We ob-
served that the crawler can correctly take the main
texts from about 90% of the pages from each news
site. We ran the crawler every day on several news
sites. Then we applied a simple clustering algorithm
to the obtained articles in order to find a set of arti-
Web
Crawling
Basic
Clustering
Coreference
Resolution
Parsing
GLARFing
Basic Pattern
Generation
Metaclustering
Newspapers...
... BasicClusters
Basic Patterns
Metaclusters
(Tables)
Figure 4: System overview.
cles that talk about exactly the same news and form
a basic cluster.
We eliminate stop words and stem all the other
words, then compute the similarity between two ar-
ticles by using a bag-of-words approach. In news
articles, a sentence that appears in the beginning of
an article is usually more important than the others.
So we preserved the word order to take into account
the location of each sentence. First we computed a
word vector from each article:
Vw(A) = IDF(w)
?
i?POS(w,A)
exp(? iavgwords)
where Vw(A) is a vector element of word w in article
A, IDF (w) is the inverse document frequency of
word w, and POS(w,A) is a list of w?s positions
in the article. avgwords is the average number of
words for all articles. Then we calculated the cosine
value of each pair of vectors:
Sim(A1, A2) = cos(V (A1) ? V (A2))
We computed the similarity of all possible pairs of
articles from the same day, and selected the pairs
307
whose similarity exceeded a certain threshold (0.65
in this experiment) to form a basic cluster.
3.2 Parsing and GLARFing
After getting a set of basic clusters, we pass them
to an existing statistical parser (Charniak, 2000) and
rule-based tree normalizer to obtain a GLARF struc-
ture for each sentence in every article. The current
implementation of a GLARF converter gives about
75% F-score using parser output. For the details of
GLARF representation and its conversion, see Mey-
ers et al (2001b).
3.3 NE Tagging and Coreference Resolution
In parallel with parsing and GLARFing, we also ap-
ply NE tagging and coreference resolution for each
article in a basic cluster. We used an HMM-based
NE tagger whose performance is about 85% in F-
score. This NE tagger produces ACE-type Named
Entities 1: PERSON, ORGANIZATION, GPE, LO-
CATION and FACILITY 2. After applying single-
document coreference resolution for each article, we
connect the entities among different articles in the
same basic cluster to obtain cross-document coref-
erence entities with simple string matching.
3.4 Basic Pattern Generation
After getting a GLARF structure for each sentence
and a set of documents whose entities are tagged
and connected to each other, we merge the two out-
puts and create a big network of GLARF structures
whose nodes are interconnected across different sen-
tences/articles. Now we can generate basic patterns
for each entity. First, we compute the weight for
each cross-document entity E in a certain basic clus-
ter as follows:
WE =
?
e?E
mentions(e) ? exp(?C ? firstsent(e))
where e ? E is an entity within one article and
mentions(e) and firstsent(e) are the number of
mentions of entity e in a document and the position
1The ACE task description can be found at
http://www.itl.nist.gov/iad/894.01/tests/ace/ and the ACE
guidelines at http://www.ldc.upenn.edu/Projects/ACE/
2The hurricane names used in the examples were recognized
as PERSON.
Katrina
hit
coast
SBJ OBJ
Louisiana
T-POS
?s
SUFFIX
GPE+T-POS:coast
PER+SBJ:hit
PER+SBJ:hit-OBJ:coast
Figure 5: Basic patterns obtained from the sentence
?Katrina hit Louisiana?s coast.?
of the sentence where entity e first appeared, respec-
tively. C is a constant value which was 0.5 in this ex-
periment. To reduce combinatorial complexity, we
took only the five most highly weighted entities from
each basic cluster to generate basic patterns. We ob-
served these five entities can cover major relations
that are reported in a basic cluster.
Next, we obtain basic patterns from the GLARF
structures. We used only the first ten sentences
in each article for getting basic patterns, as most
important facts are usually written in the first few
sentences of a news article. Figure 5 shows all
the basic patterns obtained from the sentence ?Ka-
trina hit Louisiana?s coast.? The shaded nodes
?Katrina? and ?Louisiana? are entities from which
each basic pattern originates. We take a path
of GLARF nodes from each entity node until it
reaches any predicative node: noun, verb, or ad-
jective in this case. Since the nodes ?hit? and
?coast? can be predicates in this example, we ob-
tain three unique paths ?Louisiana+T-POS:coast
(Louisiana?s coast)?, ?Katrina+SBJ:hit (Katrina
hit something)?, and ?Katrina+SBJ:hit-OBJ:coast
(Katrina hit some coast)?.
To increase the specificity of patterns, we generate
extra basic patterns by adding a node that is imme-
diately connected to a predicative node. (From this
example, we generate two basic patterns: ?hit? and
?hit-coast? from the ?Katrina? node.)
Notice that in a GLARF structure, the type
of each argument such as subject or object is
preserved in an edge even if we extract a sin-
gle path of a graph. Now, we replace both
entities ?Katrina? and ?Louisiana? with variables
308
based on their NE tags and obtain parameter-
ized patterns: ?GPE+T-POS:coast (Louisiana?s
coast)?, ?PER+SBJ:hit (Katrina hit something)?,
and ?PER+SBJ:hit-OBJ:coast (Katrina hit some
coast)?.
After taking all the basic patterns from every basic
cluster, we compute the Inverse Cluster Frequency
(ICF) of each unique basic pattern. ICF is similar
to the Inverse Document Frequency (IDF) of words,
which is used to calculate the weight of each basic
pattern for metaclustering.
3.5 Metaclustering
Finally, we can perform metaclustering to obtain ta-
bles. We compute the similarity between each basic
cluster pair, as seen in Figure 6. XA and XB are
the set of cross-document entities from basic clusters
cA and cB , respectively. We examine all possible
mappings of relations (parallel mappings of multi-
ple entities) from both basic clusters, and find all the
mappings M whose similarity score exceeds a cer-
tain threshold. wordsim(cA, cB) is the bag-of-words
similarity of two clusters. As a weighting function
we used ICF:
weight(p) = ? log(clusters that include pall clusters )
We then sort the similarities of all possible pairs
of basic clusters, and try to build a metacluster by
taking the most strongly connected pair first. Note
that in this process we may assign one basic clus-
ter to several different metaclusters. When a link is
found between two basic clusters that were already
assigned to a metacluster, we try to put them into
all the existing metaclusters it belongs to. However,
we allow a basic cluster to be added only if it can
fill all the columns in that table. In other words, the
first two basic clusters (i.e. an initial two-row table)
determines its columns and therefore define the re-
lation of that table.
4 Experiment and Evaluation
We used twelve newspapers published mainly in the
U.S. We collected their articles over two months
(from Sep. 21, 2005 - Nov. 27, 2005). We obtained
643,767 basic patterns and 7,990 unique types. Then
we applied metaclustering to these basic clusters
Source articles 28,009
Basic clusters 5,543
Basic patterns (token) 643,767
Basic patterns (type) 7,990
Metaclusters 302
Metaclusters (rows ? 3) 101
Table 2: Articles and obtained metaclusters.
and obtained 302 metaclusters (tables). We then re-
moved duplicated rows and took only the tables that
had 3 or more rows. Finally we had 101 tables. The
total number the of articles and clusters we used are
shown in Table 2.
4.1 Evaluation Method
We evaluated the obtained tables as follows. For
each row in a table, we added a summary of the
source articles that were used to extract the rela-
tion. Then for each table, an evaluator looks into
every row and its source article, and tries to come
up with a sentence that explains the relation among
its columns. The description should be as specific as
possible. If at least half of the rows can fit the ex-
planation, the table is considered ?consistent.? For
each consistent table, the evaluator wrote down the
sentence using variable names ($1, $2, ...) to refer
to its columns. Finally, we counted the number of
consistent tables. We also counted how many rows
in each table can fit the explanation.
4.2 Results
We evaluated 48 randomly chosen tables. Among
these tables, we found that 36 tables were consis-
tent. We also counted the total number of rows that
fit each description, shown in Table 3. Table 4 shows
the descriptions of the selected tables. The largest
consistent table was about hurricanes (Table 5). Al-
though we cannot exactly measure the recall of each
table, we tried to estimate the recall by comparing
this hurricane table to a manually created one (Table
6). We found 6 out of 9 hurricanes 3. It is worth
noting that most of these hurricane names were au-
tomatically disambiguated although our NE tagger
didn?t distinguish a hurricane name from a person
3Hurricane Katrina and Longwang shown in the previous
examples are not included in this table. They appeared before
this period.
309
for each cluster pair (cA, cB) {
XA = cA.entities
XB = cB .entities
for each entity mapping M = [(xA1, xB1), ..., (xAn, xBn)] ? (2|XA| ? 2|XB |) {
for each entity pair (xAi, xBi) {
Pi = xAi.patterns ? xBi.patterns
pairscorei =
?
p?Pi weight(p)
}
mapscore =
?
pairscorei
if T1 < |M | and T2 < mapscore and T3 < wordsim(cA.words, cB .words) {
link cA and cB with mapping M .
}
}
}
Figure 6: Computing similarity of basic clusters.
Tables:
Consistent tables 36 (75%)
Inconsistent tables 12
Total 48
Rows:
Rows that fit the description 118 (73%)
Rows not fitted 43
Total 161
Table 3: Evaluation results.
Description Rows
Storm $1(PER) probably affected $2(GPE). 8/16
Nominee $2(PER) must be confirmed by $1(ORG). 4/7
$1(PER) urges $2(GPE) to make changes. 4/6
$1(GPE) launched an attack in $2(GPE). 3/5
$1(PER) ran against $2(PER) in an election. 4/5
$2(PER) visited $1(GPE) on a diplomatic mission. 2/4
$2(PER) beat $1(PER) in golf. 4/4
$2(GPE) soldier(s) were killed in $1(GPE). 3/3
$2(PER) ran for governor of $1(GPE). 2/3
Boxer $1(PER) fought boxer $2(PER). 3/3
Table 4: Description of obtained tables and the num-
ber of fitted/total rows.
name. The second largest table (about nominations
of officials) is shown in Table 7.
We reviewed 10 incorrect rows from various ta-
bles and found 4 of them were due to coreference er-
rors and one error was due to a parse error. The other
4 errors were due to multiple basic patterns distant
from each other that happened to refer to a different
event reported in the same cluster. The causes of the
one remaining error was obscure. Most inconsistent
tables were a mixture of multiple relations and some
of their rows still looked consistent.
We have a couple of open questions. First, the
overall recall of our system might be lower than ex-
isting IE systems, as we are relying on a cluster of
comparable articles rather than a single document to
discover an event. We might be able to improve this
in the future by adjusting the basic clustering algo-
rithm or weighting schema of basic patterns. Sec-
ondly, some combinations of basic patterns looked
inherently vague. For example, we used the two ba-
sic patterns ?pitched? and ??s-series? in the fol-
lowing sentence (the patterns are underlined):
Ervin Santana pitched 5 1-3 gutsy innings in his post-
season debut for the Angels, Adam Kennedy hit a go-
ahead triple that sent Yankees outfielders crashing to the
ground, and Los Angeles beat New York 5-3 Monday
night in the decisive Game 5 of their AL playoff series.
It is not clear whether this set of patterns can yield
any meaningful relation. We are not sure how much
this sort of table can affect overall IE performance.
5 Conclusion
In this paper we proposed Preemptive Information
Extraction as a new direction of IE research. As
its key technique, we presented Unrestricted Rela-
tion Discovery that tries to find parallel correspon-
dences between multiple entities in a document, and
perform clustering using basic patterns as features.
To increase the number of basic patterns, we used
a cluster of comparable articles instead of a single
document. We presented the implementation of our
preliminary system and its outputs. We obtained
dozens of usable tables.
310
Article 1:dump 2:coast
2005-09-21 (1) Rita Texas
2005-09-23 Rita New Orleans
2005-09-25 Bush Texas
2005-09-26 Damrey Hainan
2005-09-27 (2) Damrey Vietnam
2005-10-01 Rita Louisiana
2005-10-02 Otis Mexico
2005-10-04 Longwang China
2005-10-05 Stan Mexico
2005-10-06 Tammy Florida
2005-10-07 Tammy Georgia
2005-10-19 (3) Wilma Florida
2005-10-25 Wilma Cuba
2005-10-25 Wilma Massachusetts
2005-10-28 Beta Nicaragua
2005-11-20 Gamma Florida
1. More than 2,000 National Guard troops were put on
active-duty alert to assist as Rita slammed into the string
of islands and headed west, perhaps toward Texas. ...
2. Typhoon Damrey smashed into Vietnam on Tuesday af-
ter killing nine people in China, ...
3. Oil markets have been watching Wilma?s progress ner-
vously, ... but the threat to energy interests appears to
have eased as forecasters predict the storm will turn to-
ward Florida. ...
Table 5: Hurricane table (?Storm $1(PER) probably
affected $2(GPE).?) and the actual expressions we
used for extraction.
Hurricane Date (Affected Place) Articles
Philippe Sep 17-20 (?) 6
* Rita Sep 17-26 (Louisiana, Texas, etc.) 566
* Stan Oct 1-5 (Mexico, Nicaragua, etc.) 83
* Tammy Oct 5-? (Georgia, Alabama) 18
Vince Oct 8-11 (Portugal, Spain) 12
* Wilma Oct 15-25 (Cuba, Honduras, etc.) 368
Alpha Oct 22-24 (Haiti, Dominican Rep.) 80
* Beta Oct 26-31 (Nicaragua, Honduras) 55
* Gamma Nov 13-20 (Belize, etc.) 36
Table 6: Hurricanes in North America between mid-
Sep. and Nov. (from Wikipedia). Rows with a
star (*) were actually extracted. The number of the
source articles that contained a mention of the hurri-
cane is shown in the right column.
Article 1:confirm 2:be-confirmed
2005-09-21 Senate Roberts
2005-10-03 Supreme Court Miers
2005-10-20 Senate Bush
2005-10-26 Senate Sauerbrey
2005-10-31 Senate Mr. Alito
2005-11-04 Senate Alito
2005-11-17 Fed Bernanke
Table 7: Nomination table (?Nominee $2(PER)
must be confirmed by $1(ORG).?)
Acknowledgements
This research was supported by the National Science
Foundation under Grant IIS-00325657. This paper
does not necessarily reflect the position of the U.S.
Government. We would like to thank Prof. Ralph
Grishman who provided useful suggestions and dis-
cussions.
References
Eugene Agichtein and L. Gravano. 2000. Snowball: Ex-
tracting Relations from Large Plaintext Collections. In
Proceedings of the 5th ACM International Conference
on Digital Libraries (DL-00).
Sergey Brin. 1998. Extracting Patterns and Relations
from the World Wide Web. In WebDB Workshop at
EDBT ?98.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL-2000.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In Proceedings of the Annual
Meeting of Association of Computational Linguistics
(ACL-04).
Adam Meyers, Ralph Grishman, Michiko Kosaka, and
Shubin Zhao. 2001a. Covering Treebanks with
GLARF. In ACL/EACL Workshop on Sharing Tools
and Resources for Research and Education.
Adam Meyers, Michiko Kosaka, Satoshi Sekine, Ralph
Grishman, and Shubin Zhao. 2001b. Parsing and
GLARFing. In Proceedings of RANLP-2001, Tzigov
Chark, Bulgaria.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics (ACL).
Ellen Riloff. 1996. Automatically Generating Extrac-
tion Patterns from Untagged Text. In Proceedings of
the 13th National Conference on Artificial Intelligence
(AAAI-96).
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An Improved Extraction Pattern Representa-
tion Model for Automatic IE Pattern Acquisition. In
Proceedings of the Annual Meeting of Association of
Computational Linguistics (ACL-03).
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Unsupervised Discovery
of Scenario-Level Patterns for Information Extraction.
In Proceedings of the 18th International Conference
on Computational Linguistics (COLING-00).
311
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 133?136,
New York, June 2006. c?2006 Association for Computational Linguistics
Using Phrasal Patterns to Identify Discourse Relations  
Manami Saito 
Nagaoka University of  
Technology 
Niigata, JP 9402188 
saito@nlp.nagaokaut.ac.jp 
Kazuhide Yamamoto 
Nagaoka University of 
Technology 
Niigata, JP 9402188 
yamamoto@fw.ipsj.or.jp
Satoshi Sekine 
New York University 
New York, NY 10003 
sekine@cs.nyu.edu 
 
Abstract 
This paper describes a system which 
identifies discourse relations between two 
successive sentences in Japanese. On top 
of the lexical information previously 
proposed, we used phrasal pattern 
information. Adding phrasal information 
improves the system's accuracy 12%, 
from 53% to 65%. 
1 Introduction 
Identifying discourse relations is important for 
many applications, such as text/conversation 
understanding, single/multi-document 
summarization and question answering. (Marcu 
and Echihabi 2002) proposed a method to identify 
discourse relations between text segments using 
Na?ve Bayes classifiers trained on a huge corpus. 
They showed that lexical pair information 
extracted from massive amounts of data can have a 
major impact. 
We developed a system which identifies the 
discourse relation between two successive 
sentences in Japanese. On top of the lexical 
information previously proposed, we added phrasal 
pattern information. A phrasal pattern includes at 
least three phrases (bunsetsu segments) from two 
sentences, where function words are mandatory 
and content words are optional. For example, if the 
first sentence is ?X should have done Y? and the 
second sentence is ?A did B?, then we found it 
very likely that the discourse relation is 
CONTRAST (89% in our Japanese corpus). 
2 Discourse Relation Definitions 
There have been many definitions of discourse 
relation, for example (Wolf 2005) and (Ichikawa 
1987) in Japanese. We basically used Ichikawa?s 
classes and categorized 167 cue phrases in the 
ChaSen dictionary (IPADIC, Ver.2.7.0), as shown 
in Table 1. Ambiguous cue phrases were 
categorized into multiple classes. There are 7 
classes, but the OTHER class will be ignored in the 
following experiment, as its frequency is very 
small. 
Table 1. Discourse relations 
Discourse     
relation 
Examples of cue phrase 
(English translation) 
Freq. in  
corpus [%]
ELABORATION and, also, then, moreover 43.0 
CONTRAST although, but, while 32.2 
CAUSE-
EFFECT 
because, and so, thus, 
therefore 12.1 
EQUIVALENCE in fact, alternatively, similarly 6.0 
CHANGE-
TOPIC 
by the way, incidentally, 
and now, meanwhile, well 5.1 
EXAMPLE for example, for instance 1.5 
OTHER most of all, in general 0.2 
 
3 Identification using Lexical Information 
The system has two components; one is to identify 
the discourse relation using lexical information, 
described in this section, and the other is to 
identify it using phrasal patterns, described in the 
next section. 
A pair of words in two consecutive sentences 
can be a clue to identify the discourse relation of 
those sentences. For example, the CONTRAST 
relation may hold between two sentences which 
133
have antonyms, such as ?ideal? and ?reality? in 
Example 1. Also, the EXAMPLE relation may 
hold when the second sentence has hyponyms of a 
word in the first sentence. For example, ?gift shop?, 
?department store?, and ?supermarket? are 
hyponyms of ?store? in Example 2.  
Ex1) 
a. It is ideal that people all over the world 
accept independence and associate on an 
equal footing with each other. 
b. (However,) Reality is not that simple. 
Ex2) 
a. Every town has many stores.  
b. (For example,) Gift shops, department 
stores, and supermarkets are the main 
stores. 
 
In our experiment, we used a corpus from the 
Web (about 20G of text) and 38 years of 
newspapers. We extracted pairs of sentences in 
which an unambiguous discourse cue phrase 
appears at the beginning of the second sentence. 
We extracted about 1,300,000 sentence pairs from 
the Web and about 150,000 pairs from newspapers.  
300 pairs (50 of each discourse relation) were set 
aside as a test corpus. 
3.1 Extracting Word Pairs 
Word pairs are extracted from two sentences; i.e. 
one word from each sentence. In order to reduce 
noise, the words are restricted to common nouns, 
verbal nouns, verbs, and adjectives. Also, the word 
pairs are restricted to particular kinds of POS 
combinations in order to reduce the impact of word 
pairs which are not expected to be useful in 
discourse relation identification. We confined the 
combinations to the pairs involving the same part 
of speech and those between verb and adjective, 
and between verb and verbal noun. 
All of the extracted word pairs are used in base 
form. In addition, each word is annotated with a 
positive or negative label. If a phrase segment 
includes negative words like ?not?, the words in 
the same segment are annotated with a negative 
label. Otherwise, words are annotated with a 
positive label. We don?t consider double negatives. 
In Example 1-b, ?simple? is annotated with a 
negative, as it includes ?not? in the same segment. 
3.2 Score Calculation 
All possible word pairs are extracted from the 
sentence pairs and the frequencies of pairs are 
counted for each discourse relation. For a new 
(test) sentence pair, two types of score are 
calculated for each discourse relation based on all 
of the word pairs found in the two sentences. The 
scores are given by formulas (1) and (2). Here 
Freq(dr, wp) is the frequency of word pair (wp) in 
the discourse relation (dr). Score1 is the fraction of 
the given discourse relation among all the word 
pairs in the sentences. Score2 incorporates an 
adjustment based on the rate (RateDR) of the 
discourse relation in the corpus, i.e. the third 
column in Table 1. The score actually compares 
the ratio of a discourse relation in the particular 
word pairs against the ratio in the entire corpus. It 
helps the low frequency discourse relations get 
better scores.  
 
( )
( )
?
?
=
wpdr
wp
wpdrFreq
wpDRFreq
DRScore
,
1 ),(
,
                 (1) 
 
( )
( )
DR
wpdr
wp
RatewpdrFreq
wpDRFreq
DRScore ?= ?
?
,
2 ),(
,
 (2) 
 
4 Identification using Phrasal Pattern 
We can sometimes identify the discourse relation 
between two sentences from fragments of the two 
sentences. For example, the CONTRAST relation 
is likely to hold between the pair of fragments ?? 
should have done ?.? and ?? did ?.?, and the 
EXAMPLE relation is likely to hold between the 
pair of fragments ?There is?? and ?Those are ? 
and so on.?.  Here ??? represents any sequence of 
words. The above examples indicate that the 
discourse relation between two sentences can be 
recognized using fragments of the sentences even 
if there are no clues based on the sort of content 
words involved in the word pairs.  Accumulating 
such fragments in Japanese, we observe that these 
fragments actually form a phrasal pattern. A phrase 
(bunsetsu) in Japanese is a basic component of 
sentences, and consists of one or more content 
words and zero or more function words. We 
134
specify that a phrasal pattern contain at least three 
subphrases, with at least one from each sentence. 
Each subphrase contains the function words of the 
phrase, and may also include accompanying 
content words. We describe the method to create 
patterns in three steps using an example sentence 
pair (Example 3) which actually has the 
CONTRAST relation. 
Ex3)  
a. ?kanojo-no kokoro-ni donna omoi-ga at-ta-
ka-ha wakara-nai.? (No one knows what 
feeling she had in her mind.) 
b. ?sore-ha totemo yuuki-ga iru koto-dat-ta-
ni-chigai-nai.? (I think that she must have 
needed courage.) 
 
1) Deleting unnecessary phrases 
Noun modifiers using ?no? (a typical particle for a 
noun modifier) are excised from the sentences, as 
they are generally not useful to identify a discourse 
relation. For example, in the compound phrase 
?kanozyo-no (her) kokoro (mind)? in Example 3, 
the first phrase (her), which just modifies a noun 
(mind), is excised. Also, all of the phrases which 
modify excised phrases, and all but the last phrase 
in a conjunctive clause are excised.  
 
2) Restricting phrasal pattern 
In order to avoid meaningless phrases, we restrict 
the phrase participants to components matching the 
following regular expression pattern. Here, noun-x 
means all types of nouns except common nouns, i.e. 
verbal nouns, proper nouns, pronouns, etc. 
 
?(noun-x | verb | adjective)? (particle | auxiliary 
verb | period)+$?, or ?adverb$? 
 
3) Combining phrases and selecting words in a 
phrase 
All possible combinations of phrases including at 
least one phrase from each sentence and at least 
three phrases in total are extracted from a pair of 
sentences in order to build up phrasal patterns. For 
each phrase which satisfies the regular expression 
in 2), the subphrases to be used in phrasal patterns 
are selected based on the following four criteria (A 
to D). In each criterion, a sample of the result 
pattern (using all the phrases in Example 3) is 
expressed in bold face. Note that it is quite difficult 
to translate those patterns into English as many 
function words in Japanese are encoded as a 
position in English. We hope readers understand 
the procedure intuitively. 
 
A) Use all components in each phrase 
kanojo-no kokoro-ni donna omoi-ga at-ta-ka-ha wakara-nai. 
sore-ha totemo yuuki-ga iru koto-dat-ta-ni-chigai-nai. 
 
B) Remove verbal noun and proper noun 
kanojo-no kokoro-ni donna omoi-ga at-ta-ka-ha wakara-nai. 
sore-ha totemo yuuki-ga iru koto-dat-ta-ni-chigai-nai. 
 
C) In addition, remove verb and adjective 
kanojo-no kokoro-ni donna omoi-ga at-ta-ka-ha wakara-nai. 
sore-ha totemo yuuki-ga iru koto-dat-ta-ni-chigai-nai. 
 
D) In addition, remove adverb and remaining noun 
kanojo-no kokoro-ni donna omoi-ga at-ta-ka-ha wakara-nai. 
sore-ha totemo yuuki-ga iru koto-dat-ta-ni-chigai-nai. 
 
4.1 Score Calculation 
By taking combinations of 3 or more subphrases 
produced as described above, 348 distinct patterns 
can be created for the sentences in Example 3; all 
of them are counted with frequency 1 for the 
CONTRAST relation. Like the score calculation 
using lexical information, we count the frequency 
of patterns for each discourse relation over the 
entire corpus. Patterns appearing more than 1000 
times are not used, as those are found not useful to 
distinguish discourse relations. 
The scores are calculated replacing Freq(dr, 
wp) in formulas (1) and (2) by Freq(dr, pp). Here, 
pp is a phrasal pattern and Freq(dr, pp) is the 
number of times discourse relation dr connects 
sentences for which phrasal pattern pp is matched. 
These scores will be called Score3 and Score4, 
respectively. 
5 Evaluation 
The system identifies one of six discourse relations, 
described in Table 1, for a test sentence pair. Using 
the 300 sentence pairs set aside earlier (50 of each 
discourse relation type), we ran two experiments 
for comparison purposes: one using only lexical 
information, the other using phrasal patterns as 
well. In the experiment using only lexical 
information, the system selects the relation 
maximizing Score2 (this did better than Score1).  In 
the other, the system chooses a relation as follows: 
if one relation maximizes both Score1 and Score2, 
135
choose that relation; else, if one relation maximizes 
both Score3 and Score4, choose that relation; else 
choose the relation maximizing Score2. 
Table 2 shows the result. For all discourse relations, 
the results using phrasal patterns are better or the 
same. When we consider the frequency of 
discourse relations, i.e. 43% for ELABORATION, 
32% for CONTRAST etc., the weighted accuracy 
was 53% using only lexical information, which is 
comparable to the similar experiment by (Marcu 
and Echihabi 2002) of 49.7%. Using phrasal 
patterns, the accuracy improves 12% to 65%. Note 
that the baseline accuracy (by always selecting the 
most frequent relation) is 43%, so the improvement 
is significant. 
Table 2. The result 
Discourse relation Lexical info. Only 
With phrasal 
pattern 
ELABORATION 44% (22/50) 52% (26/50) 
CONTRAST 62% (31/50) 86% (43/50) 
CAUSE-EFFECT 56% (28/50) 56% (28/50) 
EQUIVALENCE 58% (29/50) 58% (29/50) 
CHANGE-TOPIC 66% (33/50) 72% (36/50) 
EXAMPLE 56% (28/50) 60% (30/50) 
Total 57% (171/300) 64% (192/300)
Weighted accuracy 53% 65% 
 
Since they are more frequent in the corpus, 
ELABORATION and CONTRAST are more 
likely to be selected by Score1 or Score3. But 
adjusting the influence of rate bias using Score2 
and Score4, it sometimes identifies the other 
relations.  
The system makes many mistakes, but people 
also may not be able to identify a discourse 
relation just using the two sentences if the cue 
phrase is deleted. We asked three human subjects 
(two of them are not authors of this paper) to do 
the same task. The total (un-weighted) accuracies 
are 63, 54 and 48%, which are about the same or 
even lower than the system performance. Note that 
the subjects are allowed to annotate more than one 
relation (Actually, they did it for 3% to 30% of the 
data). If the correct relation is included among 
their N choices, then 1/N is credited to the accuracy 
count. We measured inter annotator agreements. 
The average of the inter-annotator agreements is 
69%. We also measured the system performance 
on the data where all three subjects identified the 
correct relation, or two of them identified the 
correct relation and so on (Table 3). We can see 
the correlation between the number of subjects 
who answered correctly and the system accuracy. 
In short, we can observe from the result and the 
analyses that the system works as well as a human 
does under the condition that only two sentences 
can be read. 
Table 3. Accuracy for different agreements 
# of  subjects correct 3 2 1 0 
System accuracy 71% 63% 60% 47%
. 
6 Conclusion 
In this paper, we proposed a system which 
identifies discourse relations between two 
successive sentences in Japanese. On top of the 
lexical information previously proposed, we used 
phrasal pattern information. Using phrasal 
information improves accuracy 12%, from 53% to 
65%. The accuracy is comparable to human 
performance. There are many future directions, 
which include 1) applying other machine learning 
methods, 2) analyzing discourse relation 
categorization strategy, and 3) including a longer 
context beyond two sentences. 
Acknowledgements 
This research was partially supported by the 
National Science Foundation under Grant IIS-
00325657. This paper does not necessarily reflect 
the position of the U.S. Government. We would 
like to thank Prof. Ralph Grishman, New York 
University, who provided useful suggestions and 
discussions. 
References 
Daniel Marcu and Abdessamad Echihabi. 2002. An 
Unsupervised Approach to Recognizing Discourse 
Relations, Proceedings of the 40th Annual Meeting of 
the Association for Computational Linguistics, 368-
375. 
Florian Wolf and Edward Gibson. 2005. Representing 
Discourse Coherence: A Corpus-Based Study, 
Computational Linguistics, 31(2):249-287. 
Takashi Ichikawa. 1978. Syntactic Overview for 
Japanese Education, Kyo-iku publishing, 65-67 (in 
Japanese). 
136








An Improved Extraction Pattern Representation Model
for Automatic IE Pattern Acquisition
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman
Department of Computer Science
New York University
715 Broadway, 7th Floor,
New York, NY 10003 USA
 
sudo,sekine,grishman  @cs.nyu.edu
Abstract
Several approaches have been described
for the automatic unsupervised acquisi-
tion of patterns for information extraction.
Each approach is based on a particular
model for the patterns to be acquired, such
as a predicate-argument structure or a de-
pendency chain. The effect of these al-
ternative models has not been previously
studied. In this paper, we compare the
prior models and introduce a new model,
the Subtree model, based on arbitrary sub-
trees of dependency trees. We describe
a discovery procedure for this model and
demonstrate experimentally an improve-
ment in recall using Subtree patterns.
1 Introduction
Information Extraction (IE) is the process of identi-
fying events or actions of interest and their partici-
pating entities from a text. As the field of IE has de-
veloped, the focus of study has moved towards au-
tomatic knowledge acquisition for information ex-
traction, including domain-specific lexicons (Riloff,
1993; Riloff and Jones, 1999) and extraction pat-
terns (Riloff, 1996; Yangarber et al, 2000; Sudo
et al, 2001). In particular, methods have recently
emerged for the acquisition of event extraction pat-
terns without corpus annotation in view of the cost
of manual labor for annotation. However, there has
been little study of alternative representation models
of extraction patterns for unsupervised acquisition.
In the prior work on extraction pattern acquisition,
the representation model of the patterns was based
on a fixed set of pattern templates (Riloff, 1996), or
predicate-argument relations, such as subject-verb,
and object-verb (Yangarber et al, 2000). The model
of our previous work (Sudo et al, 2001) was based
on the paths from predicate nodes in dependency
trees.
In this paper, we discuss the limitations of prior
extraction pattern representation models in relation
to their ability to capture the participating entities in
scenarios. We present an alternative model based on
subtrees of dependency trees, so as to extract enti-
ties beyond direct predicate-argument relations. An
evaluation on scenario-template tasks shows that the
proposed Subtree model outperforms the previous
models.
Section 2 describes the Subtree model for extrac-
tion pattern representation. Section 3 shows the
method for automatic acquisition. Section 4 gives
the experimental results of the comparison to other
methods and Section 5 presents an analysis of these
results. Finally, Section 6 provides some concluding
remarks and perspective on future research.
2 Subtree model
Our research on improved representation models for
extraction patterns is motivated by the limitations of
the prior extraction pattern representations. In this
section, we review two of the previous models in
detail, namely the Predicate-Argument model (Yan-
garber et al, 2000) and the Chain model (Sudo et al,
2001).
The main cause of difficulty in finding entities by
extraction patterns is the fact that the participating
entities can appear not only as an argument of the
predicate that describes the event type, but also in
other places within the sentence or in the prior text.
In the MUC-3 terrorism scenario, WEAPON entities
occur in many different relations to event predicates
in the documents. Even if WEAPON entities appear
in the same sentence with the event predicate, they
rarely serve as a direct argument of such predicates.
(e.g., ?One person was killed as the result of a bomb
explosion.?)
Predicate-Argument model The Predicate-
Argument model is based on a direct syntactic rela-
tion between a predicate and its arguments1 (Yan-
garber et al, 2000). In general, a predicate provides
a strong context for its arguments, which leads to
good accuracy. However, this model has two major
limitations in terms of its coverage, clausal bound-
aries and embedded entities inside a predicate?s
arguments.
Figure 12 shows an example of an extraction task
in the terrorism domain where the event template
consists of perpetrator, date, location and victim.
With the extraction patterns based on the Predicate-
Argument model, only perpetrator and victim can
be extracted. The location (downtown Jerusalem) is
embedded as a modifier of the noun (heart) within
the prepositional phrase, which is an adjunct of the
main predicate, triggered3. Furthermore, it is not
clear whether the extracted entities are related to the
same event, because of the clausal boundaries.4
1Since the case marking for a nominalized predicate is sig-
nificantly different from the verbal predicate, which makes it
hard to regularize the nominalized predicates automatically, the
constraint for the Predicate-Argument model requires the root
node to be a verbal predicate.
2Throughout this paper, extraction patterns are defined as
one or more word classes with their context in the dependency
tree, where the actual word matched with the class is associ-
ated to one of the slots in the template. The notation of the
patterns in this paper is based on a dependency tree where (  
(  -  )..(  -  )) denotes   is the head, and, for each 	 in 
  ,
 is its argument and the relation between   and  is labeled
with   . The labels introduced in this paper are SBJ (subject),
OBJ (object), ADV (adverbial adjunct), REL (relative), APPOS
(apposition) and prepositions (IN, OF, etc.). Also, we assume
that the order of the arguments does not matter. Symbols begin-
ning with C- represent NE (Named Entity) types.
3Yangarber refers this as a noun phrase pattern in (Yangar-
ber et al, 2000).
4This is the problem of merging the result of entity extrac-
tion. Most IE systems have hard-coded inference rules, such
Chain model Our previous work, the Chain
model (Sudo et al, 2001)5 attempts to remedy the
limitations of the Predicate-Argument model. The
extraction patterns generated by the Chain model
are any chain-shaped paths in the dependency tree.6
Thus it successfully avoids the clausal boundary
and embedded entity limitation. We reported a 5%
gain in recall at the same precision level in the
MUC-6 management succession task compared to
the Predicate-Argument model.
However, the Chain model also has its own weak-
ness in terms of accuracy due to the lack of context.
For example, in Figure 1(c), (triggered (  C-DATE  -
ADV)) is needed to extract the date entity. However,
the same pattern is likely to be applied to texts in
other domains as well, such as ?The Mexican peso
was devalued and triggered a national financial cri-
sis last week.?
Subtree model The Subtree model is a general-
ization of previous models, such that any subtree of
a dependency tree in the source sentence can be re-
garded as an extraction pattern candidate. As shown
in Figure 1(d), the Subtree model, by its defini-
tion, contains all the patterns permitted by either the
Predicate-Argument model or the Chain model. It
is also capable of providing more relevant context,
such as (triggered (explosion-OBJ)(  C-DATE  -ADV)).
The obvious advantage of the Subtree model is
the flexibility it affords in creating suitable patterns,
spanning multiple levels and multiple branches. Pat-
tern coverage is further improved by relaxing the
constraint that the root of the pattern tree be a pred-
icate node. However, this flexibility can also be a
disadvantage, since it means that a very large num-
ber of pattern candidates ? all possible subtrees of
the dependency tree of each sentence in the corpus
? must be considered. An efficient procedure is re-
quired to select the appropriate patterns from among
the candidates.
Also, as the number of pattern candidates in-
creases, the amount of noise and complexity in-
as ?triggering an explosion is related to killing or injuring and
therefore constitutes one terrorism action.?
5Originally we called it ?Tree-Based Representation of Pat-
terns?. We renamed it to avoid confusion with the proposed
approach that is also based on dependency trees.
6(Sudo et al, 2001) required the root node of the chain to be
a verbal predicate, but we have relaxed that constraint for our
experiments.
(a)
JERUSALEM, March 21 ? A smiling Palestinian suicide bomber triggered a mas-
sive explosion in the heavily policed heart of downtown Jerusalem today, killing
himself and three other people and injuring scores.
(b)
(c)
Predicate-Argument Chain model
(triggered (  C-PERSON  -SBJ)(explosion-OBJ)(  C-DATE  -ADV)) (triggered (  C-PERSON  -SBJ))
(killing (  C-PERSON  -OBJ)) (triggered (heart-IN (  C-LOCATION  -OF)))
(injuring (  C-PERSON  -OBJ)) (triggered (killing-ADV (  C-PERSON  -OBJ)))
(triggered (injuring-ADV (  C-PERSON  -OBJ)))
(triggered (  C-DATE  -ADV))
(d)
Subtree model
(triggered (  C-PERSON  -SBJ)(explosion-OBJ)) (triggered (explosion-OBJ)(  C-DATE  -ADV))
(killing (  C-PERSON  -OBJ)) (triggered (  C-DATE  -ADV)(killing-ADV))
(injuring (  C-PERSON  -OBJ)) (triggered (  C-DATE  -ADV)(killing-ADV(  C-PERSON  -OBJ)))
(triggered (heart-IN (  C-LOCATION  -OF))) (triggered (  C-DATE  -ADV)(injuring-ADV))
(triggered (killing-ADV (  C-PERSON  -OBJ))) (triggered (explosion-OBJ)(killing (  C-PERSON  -OBJ)))
(triggered (  C-DATE  -ADV)) ...
Figure 1: (a) Example sentence on terrorism scenario. (b) Dependency Tree of the example sentence (The entities to be extracted
are shaded in the tree). (c) Predicate-Argument patterns and Chain-model patterns that contribute to the extraction task. (d) Subtree
model patterns that contribute the extraction task.
creases. In particular, many of the pattern candidates
overlap one another. For a given set of extraction
patterns, if pattern A subsumes pattern B (say, A is
(shoot (  C-PERSON  -OBJ)(to death)) and B is (shoot (  C-
PERSON  -OBJ))), there is no added contribution for
extraction by pattern matching with A (since all the
matches with pattern A must be covered with pattern
B). Therefore, we need to pay special attention to the
ranking function for pattern candidates, so that pat-
terns with more relevant contexts get higher score.
3 Acquisition Method
This section discusses an automatic procedure to
learn extraction patterns. Given a narrative descrip-
tion of the scenario and a set of source documents,
the following three stages obtain the relevant extrac-
tion patterns for the scenario; preprocessing, docu-
ment retrieval, and ranking pattern candidates.
3.1 Stage 1: Preprocessing
Morphological analysis and Named Entities (NE)
tagging are performed at this stage.7 Then all the
sentences are converted into dependency trees by an
appropriate dependency analyzer.8 The NE tagging
7We used Extended NE hierarchy based on (Sekine et al,
2002), which is structured and contains 150 classes.
8Any degree of detail can be chosen through entire proce-
dure, from lexicalized dependency to chunk-level dependency.
For the following experiment in Japanese, we define a node in
replaces named entities by their class, so the result-
ing dependency trees contain some NE class names
as leaf nodes. This is crucial to identifying common
patterns, and to applying these patterns to new text.
3.2 Stage 2: Document Retrieval
The procedure retrieves a set of documents that de-
scribe the events of the scenario of interest, the rel-
evant document set. A set of narrative sentences de-
scribing the scenario is selected to create a query
for the retrieval. Any IR system of sufficient accu-
racy can be used at this stage. For this experiment,
we retrieved the documents using CRL?s stochastic-
model-based IR system (Murata et al, 1999).
3.3 Stage 3: Ranking Pattern Candidates
Given the dependency trees of parsed sentences in
the relevant document set, all the possible subtrees
can be candidates for extraction patterns. The rank-
ing of pattern candidates is inspired by TF/IDF scor-
ing in IR literature; a pattern is more relevant when
it appears more in the relevant document set and less
across the entire collection of source documents.
The right-most expansion base subtree discovery
algorithm (Abe et al, 2002) was implemented to cal-
culate term frequency (raw frequency of a pattern)
and document frequency (the number of documents
where a pattern appears) for each pattern candidate.
The algorithm finds the subtrees appearing more fre-
quently than a given threshold by constructing the
subtrees level by level, while keeping track of their
occurrence in the corpus. Thus, it efficiently avoids
the construction of duplicate patterns and runs al-
most linearly in the total size of the maximal tree
patterns contained in the corpus.
The following ranking function was used to rank
each pattern candidate. The score of subtree   ,
	

, is
	


	Morphological Analysis of a Large Spontaneous Speech Corpus in Japanese
Kiyotaka Uchimoto? Chikashi Nobata? Atsushi Yamada?
Satoshi Sekine? Hitoshi Isahara?
?Communications Research Laboratory
3-5, Hikari-dai, Seika-cho, Soraku-gun,
Kyoto, 619-0289, Japan
{uchimoto,nova,ark,isahara}@crl.go.jp
?New York University
715 Broadway, 7th floor
New York, NY 10003, USA
sekine@cs.nyu.edu
Abstract
This paper describes two methods for de-
tecting word segments and their morpho-
logical information in a Japanese sponta-
neous speech corpus, and describes how
to tag a large spontaneous speech corpus
accurately by using the two methods. The
first method is used to detect any type of
word segments. The second method is
used when there are several definitions for
word segments and their POS categories,
and when one type of word segments in-
cludes another type of word segments. In
this paper, we show that by using semi-
automatic analysis we achieve a precision
of better than 99% for detecting and tag-
ging short words and 97% for long words;
the two types of words that comprise the
corpus. We also show that better accuracy
is achieved by using both methods than by
using only the first.
1 Introduction
The ?Spontaneous Speech: Corpus and Process-
ing Technology? project is sponsoring the construc-
tion of a large spontaneous Japanese speech corpus,
Corpus of Spontaneous Japanese (CSJ) (Maekawa
et al, 2000). The CSJ is a collection of mono-
logues and dialogues, the majority being mono-
logues such as academic presentations and simu-
lated public speeches. Simulated public speeches
are short speeches presented specifically for the cor-
pus by paid non-professional speakers. The CSJ in-
cludes transcriptions of the speeches as well as audio
recordings of them. One of the goals of the project
is to detect two types of word segments and cor-
responding morphological information in the tran-
scriptions. The two types of word segments were
defined by the members of The National Institute for
Japanese Language and are called short word and
long word. The term short word approximates a dic-
tionary item found in an ordinary Japanese dictio-
nary, and long word represents various compounds.
The length and part-of-speech (POS) of each are dif-
ferent, and every short word is included in a long
word, which is shorter than a Japanese phrasal unit,
a bunsetsu. If all of the short words in the CSJ
were detected, the number of the words would be
approximately seven million. That would be the
largest spontaneous speech corpus in the world. So
far, approximately one tenth of the words have been
manually detected, and morphological information
such as POS category and inflection type have been
assigned to them. Human annotators tagged every
morpheme in the one tenth of the CSJ that has been
tagged, and other annotators checked them. The hu-
man annotators discussed their disagreements and
resolved them. The accuracies of the manual tagging
of short and long words in the one tenth of the CSJ
were greater than 99.8% and 97%, respectively. The
accuracies were evaluated by random sampling. As
it took over two years to tag one tenth of the CSJ ac-
curately, tagging the remainder with morphological
information would take about twenty years. There-
fore, the remaining nine tenths of the CSJ must be
tagged automatically or semi-automatically.
In this paper, we describe methods for detecting
the two types of word segments and corresponding
morphological information. We also describe how
to tag a large spontaneous speech corpus accurately.
Henceforth, we call the two types of word segments
short word and long word respectively, or merely
morphemes. We use the term morphological anal-
ysis for the process of segmenting a given sentence
into a row of morphemes and assigning to each mor-
pheme grammatical attributes such as a POS cate-
gory.
2 Problems and Their Solutions
As we mentioned in Section 1, tagging the whole of
the CSJ manually would be difficult. Therefore, we
are taking a semi-automatic approach. This section
describes major problems in tagging a large sponta-
neous speech corpus with high precision in a semi-
automatic way, and our solutions to those problems.
One of the most important problems in morpho-
logical analysis is that posed by unknown words,
which are words found in neither a dictionary nor
a training corpus. Two statistical approaches have
been applied to this problem. One is to find un-
known words from corpora and put them into a
dictionary (e.g., (Mori and Nagao, 1996)), and the
other is to estimate a model that can identify un-
known words correctly (e.g., (Kashioka et al, 1997;
Nagata, 1999)). Uchimoto et al used both ap-
proaches. They proposed a morphological analysis
method based on a maximum entropy (ME) model
(Uchimoto et al, 2001). Their method uses a model
that estimates how likely a string is to be a mor-
pheme as its probability, and thus it has a potential
to overcome the unknown word problem. Therefore,
we use their method for morphological analysis of
the CSJ. However, Uchimoto et al reported that the
accuracy of automatic word segmentation and POS
tagging was 94 points in F-measure (Uchimoto et
al., 2002). That is much lower than the accuracy ob-
tained by manual tagging. Several problems led to
this inaccuracy. In the following, we describe these
problems and our solutions to them.
? Fillers and disfluencies
Fillers and disfluencies are characteristic ex-
pressions often used in spoken language, but
they are randomly inserted into text, so detect-
ing their segmentation is difficult. In the CSJ,
they are tagged manually. Therefore, we first
delete fillers and disfluencies and then put them
back in their original place after analyzing a
text.
? Accuracy for unknown words
The morpheme model that will be described
in Section 3.1 can detect word segments and
their POS categories even for unknown words.
However, the accuracy for unknown words is
lower than that for known words. One of the
solutions is to use dictionaries developed for a
corpus on another domain to reduce the num-
ber of unknown words, but the improvement
achieved is slight (Uchimoto et al, 2002). We
believe that the reason for this is that defini-
tions of a word segment and its POS category
depend on a particular corpus, and the defi-
nitions from corpus to corpus differ word by
word. Therefore, we need to put only words
extracted from the same corpus into a dictio-
nary. We are manually examining words that
are detected by the morpheme model but that
are not found in a dictionary. We are also
manually examining those words that the mor-
pheme model estimated as having low proba-
bility. During the process of manual exami-
nation, if we find words that are not found in
a dictionary, those words are then put into a
dictionary. Section 4.2.1 will describe the ac-
curacy of detecting unknown words and show
how much those words contribute to improving
the morphological analysis accuracy when they
are detected and put into a dictionary.
? Insufficiency of features
The model currently used for morphological
analysis considers the information of a target
morpheme and that of an adjacent morpheme
on the left. To improve the model, we need to
consider the information of two or more mor-
phemes on the left of the target morpheme.
However, too much information often leads to
overtraining the model. Using all the informa-
tion makes training the model difficult when
there is too much of it. Therefore, the best
way to improve the accuracy of the morpholog-
ical information in the CSJ within the limited
time available to us is to examine and revise
the errors of automatic morphological analysis
and to improve the model. We assume that the
smaller the probability estimated by a model
for an output morpheme is, then the greater
the likelihood is that the output morpheme is
wrong. Therefore, we examine output mor-
phemes in ascending order of their probabili-
ties. The expected improvement of the accu-
racy of the morphological information in the
whole of the CSJ will be described in Sec-
tion 4.2.1
Another problem concerning unknown words
is that the cost of manual examination is high
when there are several definitions for word seg-
ments and their POS categories. Since there
are two types of word definitions in the CSJ,
the cost would double. Therefore, to reduce the
cost, we propose another method for detecting
word segments and their POS categories. The
method will be described in Section 3.2, and
the advantages of the method will be described
in Section 4.2.2
The next problem described here is one that we
have to solve to make a language model for auto-
matic speech recognition.
? Pronunciation
Pronunciation of each word is indispensable for
making a language model for automatic speech
recognition. In the CSJ, pronunciation is tran-
scribed separately from the basic form writ-
ten by using kanji and hiragana characters as
shown in Fig. 1. Text targeted for morpho-
Basic form Pronunciation
0017 00051.425-00052.869 L:
(F??) (F??)
????? ?????????
0018 00053.073-00054.503 L:
???? ????
0019 00054.707-00056.341 L:
???????? ?????????
?Well, I?m going to talk about morphological analysis.?
Figure 1: Example of transcription.
logical analysis is the basic form of the CSJ
and it does not have information on actual pro-
nunciation. The result of morphological anal-
ysis, therefore, is a row of morphemes that
do not have information on actual pronuncia-
tion. To estimate actual pronunciation by using
only the basic form and a dictionary is impossi-
ble. Therefore, actual pronunciation is assigned
to results of morphological analysis by align-
ing the basic form and pronunciation in the
CSJ. First, the results of morphological anal-
ysis, namely, the morphemes, are transliterated
into katakana characters by using a dictionary,
and then they are aligned with pronunciation
in the CSJ by using a dynamic programming
method.
In this paper, we will mainly discuss methods for
detecting word segments and their POS categories in
the whole of the CSJ.
3 Models and Algorithms
This section describes two methods for detecting
word segments and their POS categories. The first
method uses morpheme models and is used to detect
any type of word segment. The second method uses
a chunking model and is only used to detect long
word segments.
3.1 Morpheme Model
Given a tokenized test corpus, namely a set of
strings, the problem of Japanese morphological
analysis can be reduced to the problem of assign-
ing one of two tags to each string in a sentence. A
string is tagged with a 1 or a 0 to indicate whether
it is a morpheme. When a string is a morpheme, a
grammatical attribute is assigned to it. A tag desig-
nated as a 1 is thus assigned one of a number, n, of
grammatical attributes assigned to morphemes, and
the problem becomes to assign an attribute (from 0
to n) to every string in a given sentence.
We define a model that estimates the likelihood
that a given string is a morpheme and has a gram-
matical attribute i(1 ? i ? n) as a morpheme
model. We implemented this model within an ME
modeling framework (Jaynes, 1957; Jaynes, 1979;
Berger et al, 1996). The model is represented by
Eq. (1):
p
?
(a|b) =
exp
(
?
i,j
?
i,j
g
i,j
(a, b)
)
Z
?
(b)
(1)
Short word Long word
Word Pronunciation POS Others Word Pronunciation POS Others
?? (form) ????(keitai) Noun ????? (morphological
analysis)
????????
?
(keitaisokaiseki) Noun
? (element) ? (so) Suffix
?? (analysis)????(kaiseki) Noun
? ? (ni) PPP case marker ???? (about) ???? (nitsuite) PPP case marker,
compound
word
?? (relate) ?? (tsui) Verb KA-GYO, ADF, eu-
phonic change
? ? (te) PPP conjunctive
? ? (o) Prefix ??????(talk) ??????? (ohanashiitasi) Verb SA-GYO,
ADF
?? (talk) ??? (hanashi) Verb SA-GYO, ADF
???(do) ??? (itashi) Verb SA-GYO, ADF
?? ?? (masu) AUX ending form ?? ?? (masu) AUX ending form
PPP : post-positional particle , AUX : auxiliary verb , ADF : adverbial form
Figure 2: Example of morphological analysis results.
Z
?
(b) =
?
a
exp
(
?
i,j
?
i,j
g
i,j
(a, b)
)
, (2)
where a is one of the categories for classification,
and it can be one of (n+1) tags from 0 to n (This is
called a ?future.?), b is the contextual or condition-
ing information that enables us to make a decision
among the space of futures (This is called a ?his-
tory.?), and Z
?
(b) is a normalizing constant deter-
mined by the requirement that
?
a
p
?
(a|b) = 1 for
all b. The computation of p
?
(a|b) in any ME model
is dependent on a set of ?features? which are binary
functions of the history and future. For instance, one
of our features is
g
i,j
(a, b) =
{
1 : if has(b, f
j
) = 1 & a = a
i
f
j
= ?POS(?1)(Major) : verb,??
0 : otherwise.
(3)
Here ?has(b, f
j
)? is a binary function that returns
1 if the history b has feature f
j
. The features used
in our experiments are described in detail in Sec-
tion 4.1.1.
Given a sentence, probabilities of n tags from 1
to n are estimated for each length of string in that
sentence by using the morpheme model. From all
possible division of morphemes in the sentence, an
optimal one is found by using the Viterbi algorithm.
Each division is represented as a particular division
of morphemes with grammatical attributes in a sen-
tence, and the optimal division is defined as a di-
vision that maximizes the product of the probabil-
ities estimated for each morpheme in the division.
For example, the sentence ???????????
??????? in basic form as shown in Fig. 1 is
analyzed as shown in Fig. 2. ??????? is ana-
lyzed as three morphemes, ??? (noun)?, ?? (suf-
fix)?, and ??? (noun)?, for short words, and as one
morpheme, ?????? (noun)? for long words.
In conventional models (e.g., (Mori and Nagao,
1996; Nagata, 1999)), probabilities were estimated
for candidate morphemes that were found in a dic-
tionary or a corpus and for the remaining strings
obtained by eliminating the candidate morphemes
from a given sentence. Therefore, unknown words
were apt to be either concatenated as one word or di-
vided into both a combination of known words and
a single word that consisted of more than one char-
acter. However, this model has the potential to cor-
rectly detect any length of unknown words.
3.2 Chunking Model
The model described in this section can be applied
when several types of words are defined in a cor-
pus and one type of words consists of compounds of
other types of words. In the CSJ, every long word
consists of one or more short words.
Our method uses two models, a morpheme model
for short words and a chunking model for long
words. After detecting short word segments and
their POS categories by using the former model,
long word segments and their POS categories are de-
tected by using the latter model. We define four la-
bels, as explained below, and extract long word seg-
ments by estimating the appropriate labels for each
short word according to an ME model. The four la-
bels are listed below:
Ba: Beginning of a long word, and the POS cat-
egory of the long word agrees with the short
word.
Ia: Middle or end of a long word, and the POS cat-
egory of the long word agrees with the short
word.
B: Beginning of a long word, and the POS category
of the long word does not agree with the short
word.
I: Middle or end of a long word, and the POS cat-
egory of the long word does not agree with the
short word.
A label assigned to the leftmost constituent of a long
word is ?Ba? or ?B?. Labels assigned to other con-
stituents of a long word are ?Ia?, or ?I?. For exam-
ple, the short words shown in Fig. 2 are labeled as
shown in Fig. 3. The labeling is done deterministi-
cally from the beginning of a given sentence to its
end. The label that has the highest probability as es-
timated by an ME model is assigned to each short
word. The model is represented by Eq. (1). In Eq.
(1), a can be one of four labels. The features used in
our experiments are described in Section 4.1.2.
Short word Long word
Word POS Label Word POS
?? Noun Ba ????? Noun
? Suffix I
?? Noun Ia
? PPP Ba ???? PPP
?? Verb I
? PPP Ia
? Prefix B ?????? Verb
?? Verb Ia
??? Verb Ia
?? AUX Ba ?? AUX
PPP : post-positional particle , AUX : auxiliary verb
Figure 3: Example of labeling.
When a long word that does not include a short
word that has been assigned the label ?Ba? or ?Ia?,
this indicates that the word?s POS category differs
from all of the short words that constitute the long
word. Such a word must be estimated individually.
In this case, we estimate the POS category by us-
ing transformation rules. The transformation rules
are automatically acquired from the training corpus
by extracting long words with constituents, namely
short words, that are labeled only ?B? or ?I?. A rule
is constructed by using the extracted long word and
the adjacent short words on its left and right. For
example, the rule shown in Fig. 4 was acquired in
our experiments. The middle division of the con-
sequent part represents a long word ???? (auxil-
iary verb), and it consists of two short words ???
(post-positional particle) and ??? (verb). If several
different rules have the same antecedent part, only
the rule with the highest frequency is chosen. If no
rules can be applied to a long word segment, rules
are generalized in the following steps.
1. Delete posterior context
2. Delete anterior and posterior contexts
3. Delete anterior and posterior contexts and lexi-
cal entries.
If no rules can be applied to a long word segment in
any step, the POS category noun is assigned to the
long word.
4 Experiments and Discussion
4.1 Experimental Conditions
In our experiments, we used 744,204 short words
and 618,538 long words for training, and 63,037
short words and 51,796 long words for testing.
Those words were extracted from one tenth of the
CSJ that already had been manually tagged. The
training corpus consisted of 319 speeches and the
test corpus consisted of 19 speeches.
Transcription consisted of basic form and pronun-
ciation, as shown in Fig. 1. Speech sounds were
faithfully transcribed as pronunciation, and also rep-
resented as basic forms by using kanji and hiragana
characters. Lines beginning with numerical digits
are time stamps and represent the time it took to
produce the lines between that time stamp and the
next time stamp. Each line other than time stamps
represents a bunsetsu. In our experiments, we used
only the basic forms. Basic forms were tagged with
several types of labels such as fillers, as shown in
Table 1. Strings tagged with those labels were han-
dled according to rules as shown in the rightmost
columns in Table 1.
Since there are no boundaries between sentences
in the corpus, we selected the places in the CSJ that
Anterior context Target words Posterior context
Entry ?? (it, go) ? (te)? (mi, try) ?? (tai, want)
POS Verb PPP Verb AUX
Label Ba B I Ba
Antecedent part
?
Anterior context Long word Posterior context
?? (it, go) ?? (temi, try) ?? (tai, want)
Verb AUX AUX
Consequent part
Figure 4: Example of transformation rules.
Table 1: Type of labels and their handling.
Type of Labels Example Rules
Fillers (F??) delete all
Disfluencies (D?)????? (D2?)? delete all
No confidence in
transcription
(? ?????) leave a candidate
Entirely (?) delete all
Several can- (? ???,????) leave the former
didates exist candidate
Citation on sound or
words
(M?)? (M?)??? leave a candidate
Foreign, archaic, or
dialect words
(O???????) leave a candidate
Personal name, dis-
criminating words,
and slander
???? (R??)??? leave a candidate
Letters and their
pronunciation in
katakana strings
(A????;EU) leave the former
candidate
Strings that cannot
be written in kanji
characters
(K? (F??)??;?) leave the latter can-
didate
are automatically detected as pauses of 500 ms or
longer and then designated them as sentence bound-
aries. In addition to these, we also used utterance
boundaries as sentence boundaries. These are au-
tomatically detected at places where short pauses
(shorter than 200 ms but longer than 50 ms) follow
the typical sentence-ending forms of predicates such
as verbs, adjectives, and copula.
4.1.1 Features Used by Morpheme Models
In the CSJ, bunsetsu boundaries, which are phrase
boundaries in Japanese, were manually detected.
Fillers and disfluencies were marked with the labels
(F) and (D). In the experiments, we eliminated fillers
and disfluencies but we did use their positional infor-
mation as features. We also used as features, bun-
setsu boundaries and the labels (M), (O), (R), and
(A), which were assigned to particular morphemes
such as personal names and foreign words. Thus, the
input sentences for training and testing were charac-
ter strings without fillers and disfluencies, and both
boundary information and various labels were at-
tached to them. Given a sentence, for every string
within a bunsetsu and every string appearing in a
dictionary, the probabilities of a in Eq. (1) were es-
timated by using the morpheme model. The output
was a sequence of morphemes with grammatical at-
tributes, as shown in Fig. 2. We used the POS cate-
gories in the CSJ as grammatical attributes. We ob-
tained 14 major POS categories for short words and
15 major POS categories for long words. Therefore,
a in Eq. (1) can be one of 15 tags from 0 to 14 for
short words, and it can be one of 16 tags from 0 to
15 for long words.
Table 2: Features.
Number Feature Type Feature value
(Number of value) (Short:Long)
1 String(0) (113,474:117,002)
2 String(-1) (17,064:32,037)
3 Substring(0)(Left1) (2,351:2,375)
4 Substring(0)(Right1) (2,148:2,171)
5 Substring(0)(Left2) (30,684:31,456)
6 Substring(0)(Right2) (25,442:25,541)
7 Substring(-1)(Left1) (2,160:2,088)
8 Substring(-1)(Right1) (1,820:1,675)
9 Substring(-1)(Left2) (11,025:12,875)
10 Substring(-1)(Right2) (10,439:13,364)
11 Dic(0)(Major) Noun, Verb, Adjective, . . . Unde-
fined (15:16)
12 Dic(0)(Minor) Common noun, Topic marker, Ba-
sic form. . . (75:71)
13 Dic(0)(Major&Minor) Noun&Common noun,
Verb&Basic form, . . . (246:227)
14 Dic(-1)(Minor) Common noun, Topic marker, Ba-
sic form. . . (16:16)
15 POS(-1) Noun, Verb, Adjective, . . . (14:15)
16 Length(0) 1, 2, 3, 4, 5, 6 or more (6:6)
17 Length(-1) 1, 2, 3, 4, 5, 6 or more (6:6)
18 TOC(0)(Beginning) Kanji, Hiragana, Number,
Katakana, Alphabet (5:5)
19 TOC(0)(End) Kanji, Hiragana, Number,
Katakana, Alphabet (5:5)
20 TOC(0)(Transition) Kanji?Hiragana,
Number?Kanji,
Katakana?Kanji, . . . (25:25)
21 TOC(-1)(End) Kanji, Hiragana, Number,
Katakana, Alphabet (5:5)
22 TOC(-1)(Transition) Kanji?Hiragana,
Number?Kanji,
Katakana?Kanji, . . . (16:15)
23 Boundary Bunsetsu(Beginning), Bun-
setsu(End), Label(Beginning),
Label(End), (4:4)
24 Comb(1,15) (74,602:59,140)
25 Comb(1,2,15) (141,976:136,334)
26 Comb(1,13,15) (78,821:61,813)
27 Comb(1,2,13,15) (156,187:141,442)
28 Comb(11,15) (209:230)
29 Comb(12,15) (733:682)
30 Comb(13,15) (1,549:1,397)
31 Comb(12,14) (730:675)
The features we used with morpheme models in
our experiments are listed in Table 2. Each feature
consists of a type and a value, which are given in the
rows of the table, and it corresponds to j in the func-
tion g
i,j
(a, b) in Eq. (1). The notations ?(0)? and
?(-1)? used in the feature-type column in Table 2 re-
spectively indicate a target string and the morpheme
to the left of it. The terms used in the table are ba-
sically as same as those that Uchimoto et al used
(Uchimoto et al, 2002). The main difference is the
following one:
Boundary: Bunsetsu boundaries and positional in-
formation of labels such as fillers. ?(Begin-
ning)? and ?(End)? in Table 2 respectively indi-
cate whether the left and right side of the target
strings are boundaries.
We used only those features that were found three or
more times in the training corpus.
4.1.2 Features Used by a Chunking Model
We used the following information as features
on the target word: a word and its POS cate-
gory, and the same information for the four clos-
est words, the two on the left and the two on
the right of the target word. Bigram and tri-
gram words that included a target word plus bigram
and trigram POS categories that included the tar-
get word?s POS category were used as features. In
addition, bunsetsu boundaries as described in Sec-
tion 4.1.1 were used. For example, when a target
word was ??? in Fig. 3, ???, ????, ???, ??
??, ???, ?Suffix?, ?Noun?, ?PPP?, ?Verb?, ?PPP?,
???&??, ??&???, ?? &?? &??, ??
&??&??, ?Noun&PPP?, ?PPP&Verb?, ?Suf-
fix&Noun&PPP?, ?PPP&Verb&PPP?, and ?Bun-
setsu(Beginning)? were used as features.
4.2 Results and Discussion
4.2.1 Experiments Using Morpheme Models
Results of the morphological analysis obtained by
using morpheme models are shown in Table 3 and
4. In these tables, OOV indicates Out-of-Vocabulary
rates. Shown in Table 3, OOV was calculated as the
proportion of words not found in a dictionary to all
words in the test corpus. In Table 4, OOV was cal-
culated as the proportion of word and POS category
pairs that were not found in a dictionary to all pairs
in the test corpus. Recall is the percentage of mor-
phemes in the test corpus for which the segmentation
and major POS category were identified correctly.
Precision is the percentage of all morphemes identi-
fied by the system that were identified correctly. The
F-measure is defined by the following equation.
F ? measure =
2? Recall ? Precision
Recall + Precision
Table 3: Accuracies of word segmentation.
Word Recall Precision F OOV
Short 97.47% (61,444
63,037
) 97.62% (61,444
62,945
) 97.54 1.66%
99.23% (62,553
63,037
) 99.11% (62,553
63,114
) 99.17 0%
Long 96.72% (50,095
51,796
) 95.70% (50,095
52,346
) 96.21 5.81%
99.05% (51,306
51,796
) 98.58% (51,306
52,047
) 98.81 0%
Table 4: Accuracies of word segmentation and POS
tagging.
Word Recall Precision F OOV
Short 95.72% (60,341
63,037
) 95.86% (60,341
62,945
) 95.79 2.64%
97.57% (61,505
63,037
) 97.45% (61,505
63,114
) 97.51 0%
Long 94.71% (49,058
51,796
) 93.72% (49,058
52,346
) 94.21 6.93%
97.30% (50,396
51,796
) 96.83% (50,396
52,047
) 97.06 0%
Tables 3 and 4 show that accuracies would im-
prove significantly if no words were unknown. This
indicates that all morphemes of the CSJ could be an-
alyzed accurately if there were no unknown words.
The improvements that we can expect by detecting
unknown words and putting them into dictionaries
are about 1.5 in F-measure for detecting word seg-
ments of short words and 2.5 for long words. For de-
tecting the word segments and their POS categories,
for short words we expect an improvement of about
2 in F-measure and for long words 3.
Next, we discuss accuracies obtained when un-
known words existed. The OOV for long words
was 4% higher than that for short words. In gen-
eral, the higher the OOV is, the more difficult de-
tecting word segments and their POS categories
is. However, the difference between accuracies
for short and long words was about 1% in recall
and 2% in precision, which is not significant when
we consider that the difference between OOVs for
short and long words was 4%. This result indi-
cates that our morpheme models could detect both
known and unknown words accurately, especially
long words. Therefore, we investigated the recall
of unknown words in the test corpus, and found
that 55.7% (928/1,667) of short word segments and
74.1% (2,660/3,590) of long word segments were
detected correctly. In addition, regarding unknown
words, we also found that 47.5% (791/1,667) of
short word segments plus their POS categories and
67.3% (2,415/3,590) of long word segments plus
their POS categories were detected correctly. The
recall of unknown words was about 20% higher for
long words than for short words. We believe that
this result mainly depended on the difference be-
tween short words and long words in terms of the
definitions of compound words. A compound word
is defined as one word when it is based on the def-
inition of long words; however it is defined as two
or more words when it is based on the definition of
short words. Furthermore, based on the definition of
short words, a division of compound words depends
on its context. More information is needed to pre-
cisely detect short words than is required for long
words. Next, we extracted words that were detected
by the morpheme model but were not found in a dic-
tionary, and investigated the percentage of unknown
words that were completely or partially matched to
the extracted words by their context. This percent-
age was 77.6% (1,293/1,667) for short words, and
80.6% (2,892/3,590) for long words. Most of the re-
maining unknown words that could not be detected
by this method are compound words. We expect that
these compounds can be detected during the manual
examination of those words for which the morpheme
model estimated a low probability, as will be shown
later.
The recall of unknown words was lower than that
of known words, and the accuracy of automatic mor-
phological analysis was lower than that of manual
morphological analysis. As previously stated, to
improve the accuracy of the whole corpus we take
a semi-automatic approach. We assume that the
smaller the probability is for an output morpheme
estimated by a model, the more likely the output
morpheme is wrong, and we examine output mor-
phemes in ascending order of their probabilities. We
investigated how much the accuracy of the whole
corpus would increase. Fig. 5 shows the relation-
ship between the percentage of output morphemes
whose probabilities exceed a threshold and their
93
94
95
96
97
98
99
100
20 30 40 50 60 70 80 90 100
Pr
ec
is
io
n 
(%
)
Output Rates (%)
"short_without_UKW"
"long_without_UKW"
"short_with_UKW"
"long_with_UKW"
Figure 5: Partial analysis.
precision. In this figure, ?short without UKW?,
?long without UKW??, ?short with UKW?, and
?long with UKW? represent the precision for short
words detected assuming there were no unknown
words, precision for long words detected assuming
there were no unknown words, precision of short
words including unknown words, and precision of
long words including unknown words, respectively.
When the output rate in the horizontal axis in-
creases, the number of low-probability morphemes
increases. In all graphs, precisions monotonously
decrease as output rates increase. This means that
tagging errors can be revised effectively when mor-
phemes are examined in ascending order of their
probabilities.
Next, we investigated the relationship between the
percentage of morphemes examined manually and
the precision obtained after detected errors were re-
vised. The result is shown in Fig. 6. Precision
represents the precision of word segmentation and
POS tagging. If unknown words were detected and
put into a dictionary by the method described in the
fourth paragraph of this section, the graph line for
short words would be drawn between the graph lines
?short without UKW? and ?short with UKW?, and
the graph line for long words would be drawn be-
tween the graph lines ?long without UKW? and
?long with UKW?. Based on test results, we can
expect better than 99% precision for short words
and better than 97% precision for long words in the
whole corpus when we examine 10% of output mor-
93
94
95
96
97
98
99
100
0 20 40 60 80 100 120
Pr
ec
is
io
n 
(%
)
Examined Morpheme Rates (%)
"short_without_UKW"
"long_without_UKW"
"short_with_UKW"
"long_with_UKW"
Figure 6: Relationship between the percentage of
morphemes examined manually and precision ob-
tained after revising detected errors (when mor-
phemes with probabilities under threshold and their
adjacent morphemes are examined).
0
10
20
30
40
50
60
0 5 10 15 20 25 30 35 40 45 50
Er
ro
r R
at
es
 in
 E
xa
m
in
ed
 M
or
ph
em
es
 (%
)
Examined Morpheme Rates (%)
"short_without_UKW"
"short_with_UKW"
"long_without_UKW"
"long_with_UKW"
Figure 7: Relationship between percentage of mor-
phemes examined manually and error rate of exam-
ined morphemes.
phemes in ascending order of their probabilities.
Finally, we investigated the relationship between
percentage of morphemes examined manually and
the error rate for all of the examined morphemes.
The result is shown in Fig. 7. We found that about
50% of examined morphemes would be found as er-
rors at the beginning of the examination and about
20% of examined morphemes would be found as
errors when examination of 10% of the whole cor-
pus was completed. When unknown words were de-
tected and put into a dictionary, the error rate de-
creased; even so, over 10% of examined morphemes
would be found as errors.
4.2.2 Experiments Using Chunking Models
Results of the morphological analysis of long
words obtained by using a chunking model are
shown in Table 5 and 6. The first and second lines
Table 5: Accuracies of long word segmentation.
Model Recall Precision F
Morph 96.72% (50,095
51,796
) 95.70% (50,095
52,346
) 96.21
Chunk 97.65% (50,580
51,796
) 97.41% (50,580
51,911
) 97.54
Chunk 98.84% (51,193
51,796
) 98.66% (51,193
51,888
) 98.75
Table 6: Accuracies of long word segmentation and
POS tagging.
Model Recall Precision F
Morph 94.71% (49,058
51,796
) 93.72% (49,058
52,346
) 94.21
Chunk 95.59% (49,513
51,796
) 95.38% (49,513
51,911
) 95.49
Chunk 98.56% (51,051
51,796
) 98.39% (51,051
51,888
) 98.47
Chunk w/o TR 92.61% (47,968
51,796
) 92.40% (47,968
51,911
) 92.51
TR : transformation rules
show the respective accuracies obtained when OOVs
were 5.81% and 6.93%. The third lines show the ac-
curacies obtained when we assumed that the OOV
for short words was 0% and there were no errors in
detecting short word segments and their POS cate-
gories. The fourth line in Table 6 shows the accuracy
obtained when a chunking model without transfor-
mation rules was used.
The accuracy obtained by using the chunking
model was one point higher in F-measure than that
obtained by using the morpheme model, and it was
very close to the accuracy achieved for short words.
This result indicates that errors newly produced by
applying a chunking model to the results obtained
for short words were slight, or errors in the results
obtained for short words were amended by apply-
ing the chunking model. This result also shows that
we can achieve good accuracy for long words by ap-
plying a chunking model even if we do not detect
unknown long words and do not put them into a dic-
tionary. If we could improve the accuracy for short
words, the accuracy for long words would be im-
proved also. The third lines in Tables 5 and 6 show
that the accuracy would improve to over 98 points
in F-measure. The fourth line in Tables 6 shows that
transformation rules significantly contributed to im-
proving the accuracy.
Considering the results obtained in this section
and in Section 4.2.1, we are now detecting short and
long word segments and their POS categories in the
whole corpus by using the following steps:
1. Automatically detect and manually examine
unknown words for short words.
2. Improve the accuracy for short words in the
whole corpus by manually examining short
words in ascending order of their probabilities
estimated by a morpheme model.
3. Apply a chunking model to the short words to
detect long word segments and their POS cate-
gories.
As future work, we are planning to use an active
learning method such as that proposed by Argamon-
Engelson and Dagan (Argamon-Engelson and Da-
gan, 1999) to more effectively improve the accuracy
of the whole corpus.
5 Conclusion
This paper described two methods for detecting
word segments and their POS categories in a
Japanese spontaneous speech corpus, and describes
how to tag a large spontaneous speech corpus accu-
rately by using the two methods. The first method is
used to detect any type of word segments. We found
that about 80% of unknown words could be semi-
automatically detected by using this method. The
second method is used when there are several defi-
nitions for word segments and their POS categories,
and when one type of word segments includes an-
other type of word segments. We found that better
accuracy could be achieved by using both methods
than by using only the first method alone.
Two types of word segments, short words and
long words, are found in a large spontaneous speech
corpus, CSJ. We found that the accuracy of auto-
matic morphological analysis for the short words
was 95.79 in F-measure and for long words, 95.49.
Although the OOV for long words was much higher
than that for short words, almost the same accuracy
was achieved for both types of words by using our
proposed methods. We also found that we can ex-
pect more than 99% of precision for short words,
and 97% for long words found in the whole corpus
when we examined 10% of output morphemes in as-
cending order of their probabilities as estimated by
the proposed models.
In our experiments, only the information con-
tained in the corpus was used; however, more appro-
priate linguistic knowledge than that could be used,
such as morphemic and syntactic rules. We would
like to investigate whether such linguistic knowl-
edge contributes to improved accuracy.
References
S. Argamon-Engelson and I. Dagan. 1999. Committee-Based
Sample Selection For Probabilistic Classifiers. Artificial In-
telligence Research, 11:335?360.
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A
Maximum Entropy Approach to Natural Language Process-
ing. Computational Linguistics, 22(1):39?71.
E. T. Jaynes. 1957. Information Theory and Statistical Me-
chanics. Physical Review, 106:620?630.
E. T. Jaynes. 1979. Where do we Stand on Maximum Entropy?
In R. D. Levine and M. Tribus, editors, The Maximum En-
tropy Formalism, page 15. M. I. T. Press.
H. Kashioka, S. G. Eubank, and E. W. Black. 1997. Decision-
Tree Morphological Analysis Without a Dictionary for
Japanese. In Proceedings of NLPRS, pages 541?544.
K. Maekawa, H. Koiso, S. Furui, and H. Isahara. 2000. Sponta-
neous Speech Corpus of Japanese. In Proceedings of LREC,
pages 947?952.
S. Mori and M. Nagao. 1996. Word Extraction from Cor-
pora and Its Part-of-Speech Estimation Using Distributional
Analysis. In Proceedings of COLING, pages 1119?1122.
M. Nagata. 1999. A Part of Speech Estimation Method for
Japanese Unknown Words Using a Statistical Model of Mor-
phology and Context. In Proceedings of ACL, pages 277?
284.
K. Uchimoto, S. Sekine, and H. Isahara. 2001. The Unknown
Word Problem: a Morphological Analysis of Japanese Using
Maximum Entropy Aided by a Dictionary. In Proceedings
of EMNLP, pages 91?99.
K. Uchimoto, C. Nobata, A. Yamada, S. Sekine, and H. Isahara.
2002. Morphological Analysis of The Spontaneous Speech
Corpus. In Proceedings of COLING, pages 1298?1302.
Discovering Relations among Named Entities from Large Corpora
Takaaki Hasegawa  
Cyberspace Laboratories
Nippon Telegraph and Telephone Corporation
1-1 Hikarinooka, Yokosuka,
Kanagawa 239-0847, Japan
hasegawa.takaaki@lab.ntt.co.jp
Satoshi Sekine and Ralph Grishman
Dept. of Computer Science
New York University
715 Broadway, 7th floor,
New York, NY 10003, U.S.A.

sekine,grishman  @cs.nyu.edu
Abstract
Discovering the significant relations embedded in
documents would be very useful not only for infor-
mation retrieval but also for question answering and
summarization. Prior methods for relation discov-
ery, however, needed large annotated corpora which
cost a great deal of time and effort. We propose
an unsupervised method for relation discovery from
large corpora. The key idea is clustering pairs of
named entities according to the similarity of con-
text words intervening between the named entities.
Our experiments using one year of newspapers re-
veals not only that the relations among named enti-
ties could be detected with high recall and precision,
but also that appropriate labels could be automati-
cally provided for the relations.
1 Introduction
Although Internet search engines enable us to ac-
cess a great deal of information, they cannot eas-
ily give us answers to complicated queries, such as
?a list of recent mergers and acquisitions of com-
panies? or ?current leaders of nations from all over
the world?. In order to find answers to these types
of queries, we have to analyze relevant documents
to collect the necessary information. If many rela-
tions such as ?Company A merged with Company
B? embedded in those documents could be gathered
and structured automatically, it would be very useful
not only for information retrieval but also for ques-
tion answering and summarization. Information Ex-
traction provides methods for extracting informa-
tion such as particular events and relations between
entities from text. However, it is domain depen-
dent and it could not give answers to those types of
queries from Web documents which include widely
various domains.
Our goal is automatically discovering useful re-
lations among arbitrary entities embedded in large

This work is supported by Nippon Telegraph and Telephone
(NTT) Corporation?s one-year visiting program at New York
University.
text corpora. We defined a relation broadly as an af-
filiation, role, location, part-whole, social relation-
ship and so on between a pair of entities. For ex-
ample, if the sentence, ?George Bush was inaugu-
rated as the president of the United States.? exists in
documents, the relation, ?George Bush?(PERSON)
is the ?President of? the ?United States? (GPE1),
should be extracted. In this paper, we propose
an unsupervised method of discovering relations
among various entities from large text corpora. Our
method does not need the richly annotated corpora
required for supervised learning ? corpora which
take great time and effort to prepare. It also does
not need any instances of relations as initial seeds
for weakly supervised learning. This is an advan-
tage of our approach, since we cannot know in ad-
vance all the relations embedded in text. Instead, we
only need a named entity (NE) tagger to focus on
the named entities which should be the arguments
of relations. Recently developed named entity tag-
gers work quite well and are able to extract named
entities from text at a practically useful level.
The rest of this paper is organized as follows. We
discuss prior work and their limitations in section 2.
We propose a new method of relation discovery in
section 3. Then we describe experiments and eval-
uations in section 4 and 5, and discuss the approach
in section 6. Finally, we conclude with future work.
2 Prior Work
The concept of relation extraction was introduced
as part of the Template Element Task, one of the
information extraction tasks in the Sixth Message
Understanding Conference (MUC-6) (Defense Ad-
vanced Research Projects Agency, 1995). MUC-7
added a Template Relation Task, with three rela-
tions. Following MUC, the Automatic Content Ex-
traction (ACE) meetings (National Institute of Stan-
dards and Technology, 2000) are pursuing informa-
1GPE is an acronym introduced by the ACE program to rep-
resent a Geo-Political Entity ? an entity with land and a gov-
ernment.
tion extraction. In the ACE Program2, Relation De-
tection and Characterization (RDC) was introduced
as a task in 2002. Most of approaches to the ACE
RDC task involved supervised learning such as ker-
nel methods (Zelenko et al, 2002) and need richly
annotated corpora which are tagged with relation in-
stances. The biggest problem with this approach is
that it takes a great deal of time and effort to prepare
annotated corpora large enough to apply supervised
learning. In addition, the varieties of relations were
limited to those defined by the ACE RDC task. In
order to discover knowledge from diverse corpora,
a broader range of relations would be necessary.
Some previous work adopted a weakly super-
vised learning approach. This approach has the ad-
vantage of not needing large tagged corpora. Brin
proposed the bootstrapping method for relation dis-
covery (Brin, 1998). Brin?s method acquired pat-
terns and examples by bootstrapping from a small
initial set of seeds for a particular relation. Brin
used a few samples of book titles and authors, col-
lected common patterns from context including the
samples and finally found new examples of book
title and authors whose context matched the com-
mon patterns. Agichtein improved Brin?s method
by adopting the constraint of using a named entity
tagger (Agichtein and Gravano, 2000). Ravichan-
dran also explored a similar method for question an-
swering (Ravichandran and Hovy, 2002). These ap-
proaches, however, need a small set of initial seeds.
It is also unclear how initial seeds should be selected
and how many seeds are required. Also their meth-
ods were only tried on functional relations, and this
was an important constraint on their bootstrapping.
The variety of expressions conveying the same re-
lation can be considered an example of paraphrases,
and so some of the prior work on paraphrase ac-
quisition is pertinent to relation discovery. Lin pro-
posed another weakly supervised approach for dis-
covering paraphrase (Lin and Pantel, 2001). Firstly
Lin focused on verb phrases and their fillers as sub-
ject or object. Lin?s idea was that two verb phrases
which have similar fillers might be regarded as para-
phrases. This approach, however, also needs a sam-
ple verb phrase as an initial seed in order to find
similar verb phrases.
3 Relation Discovery
3.1 Overview
We propose a new approach to relation discovery
from large text corpora. Our approach is based on
2A research and evaluation program in information extrac-
tion organized by the U.S. Government.
context based clustering of pairs of entities. We as-
sume that pairs of entities occurring in similar con-
text can be clustered and that each pair in a cluster
is an instance of the same relation. Relations be-
tween entities are discovered through this clustering
process. In cases where the contexts linking a pair
of entities express multiple relations, we expect that
the pair of entities either would not be clustered at
all, or would be placed in a cluster corresponding
to its most frequently expressed relation, because
its contexts would not be sufficiently similar to con-
texts for less frequent relations. We assume that use-
ful relations will be frequently mentioned in large
corpora. Conversely, relations mentioned once or
twice are not likely to be important.
Our basic idea is as follows:
1. tagging named entities in text corpora
2. getting co-occurrence pairs of named entities
and their context
3. measuring context similarities among pairs of
named entities
4. making clusters of pairs of named entities
5. labeling each cluster of pairs of named entities
We show an example in Figure 1. First, we find the
pair of ORGANIZATIONs (ORG) A and B, and the
pair of ORGANIZATIONs (ORG) C and D, after we
run the named entity tagger on our newspaper cor-
pus. We collect all instances of the pair A and B
occurring within a certain distance of one another.
Then, we accumulate the context words interven-
ing between A and B, such as ?be offer to buy?, ?be
negotiate to acquire?.3 In same way, we also ac-
cumulate context words intervening between C and
D. If the set of contexts of A and B and those of C
and D are similar, these two pairs are placed into
the same cluster. A ? B and C ? D would be in the
same relation, in this case, merger and acquisition
(M&A). That is, we could discover the relation be-
tween these ORGANIZATIONs.
3.2 Named entity tagging
Our proposed method is fully unsupervised. We
do not need richly annotated corpora or any ini-
tial manually selected seeds. Instead of them, we
use a named entity (NE) tagger. Recently devel-
oped named entity taggers work quite well and ex-
tract named entities from text at a practically usable
3We collect the base forms of words which are stemmed
by a POS tagger (Sekine, 2001). But verb past participles are
distinguished from other verb forms in order to distinguish the
passive voice from the active voice.
  
	
	



	


		
		
	
	
	



 
	Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 731?738,
Sydney, July 2006. c?2006 Association for Computational Linguistics
On-Demand Information Extraction 
Satoshi Sekine 
Computer Science Department 
New York University 
715 Broadway, 7th floor 
New York, NY 10003  USA 
sekine@cs.nyu.edu 
 
Abstract 
At present, adapting an Information Ex-
traction system to new topics is an expen-
sive and slow process, requiring some 
knowledge engineering for each new topic. 
We propose a new paradigm of Informa-
tion Extraction which operates 'on demand' 
in response to a user's query. On-demand 
Information Extraction (ODIE) aims to 
completely eliminate the customization ef-
fort. Given a user?s query, the system will 
automatically create patterns to extract sa-
lient relations in the text of the topic, and 
build tables from the extracted information 
using paraphrase discovery technology. It 
relies on recent advances in pattern dis-
covery, paraphrase discovery, and ex-
tended named entity tagging. We report on 
experimental results in which the system 
created useful tables for many topics, 
demonstrating the feasibility of this ap-
proach. 
1 Introduction 
Most of the world?s information is recorded, 
passed down, and transmitted between people in 
text form.  Implicit in most types of text are regu-
larities of information structure - events which 
are reported many times, about different indi-
viduals, in different forms, such as layoffs or 
mergers and acquisitions in news articles. The 
goal of information extraction (IE) is to extract 
such information:  to make these regular struc-
tures explicit, in forms such as tabular databases. 
Once the information structures are explicit, they 
can be processed in many ways: to mine infor-
mation, to search for specific information, to 
generate graphical displays and other summaries. 
However, at present, a great deal of knowl-
edge for automatic Information Extraction must 
be coded by hand to move a system to a new 
topic. For example, at the later MUC evaluations, 
system developers spent one month for the 
knowledge engineering to customize the system 
to the given test topic. Research over the last 
decade has shown how some of this knowledge 
can be obtained from annotated corpora, but this 
still requires a large amount of annotation in 
preparation for a new task.  Improving portability 
- being able to adapt to a new topic with minimal 
effort ? is necessary to make Information Extrac-
tion technology useful for real users and, we be-
lieve, lead to a breakthrough for the application 
of the technology. 
We propose ?On-demand information extrac-
tion (ODIE)?: a system which automatically 
identifies the most salient structures and extracts 
the information on the topic the user demands. 
This new IE paradigm becomes feasible due to 
recent developments in machine learning for 
NLP, in particular unsupervised learning meth-
ods, and it is created on top of a range of basic 
language analysis tools, including POS taggers, 
dependency analyzers, and extended Named En-
tity taggers.  
2 Overview 
The basic functionality of the system is the fol-
lowing. The user types a query / topic description 
in keywords (for example, ?merge? or ?merger?). 
Then tables will be created automatically in sev-
eral minutes, rather than in a month of human 
labor. These tables are expected to show infor-
mation about the salient relations for the topic. 
Figure 1 describes the components and how 
this system works. There are six major compo-
nents in the system. We will briefly describe 
each component and how the data is processed; 
then, in the next section, four important compo-
nents will be described in more detail. 
731
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Description of task (query) 
 
Figure 1. System overview 
 
1) IR system: Based on the query given by the 
user, it retrieves relevant documents from the 
document database. We used a simple TF/IDF 
IR system we developed. 
2) Pattern discovery: First, the texts in the re-
trieved documents are analyzed using a POS 
tagger, a dependency analyzer and an Ex-
tended NE (Named Entity) tagger, which will 
be described later. Then this component ex-
tracts sub-trees of dependency trees which are 
relatively frequent in the retrieved documents 
compared to the entire corpus. It counts the 
frequencies in the retrieved texts of all sub-
trees with more than a certain number of nodes 
and uses TF/IDF methods to score them. The 
top-ranking sub-trees which contain NEs will 
be called patterns, which are expected to indi-
cate salient relationships of the topic and will 
be used in the later components. 
3) Paraphrase discovery: In order to find semantic 
relationships between patterns, i.e. to find pat-
terns which should be used to build the same 
table, we use paraphrase discovery techniques. 
The paraphrase discovery was conducted off-
line and created a paraphrase knowledge base.  
4) Table construction: In this component, the 
patterns created in (2) are linked based on the 
paraphrase knowledge base created by (3), 
producing sets of patterns which are semanti-
cally equivalent. Once the sets of patterns are 
created, these patterns are applied to the docu-
ments retrieved by the IR system (1). The 
matched patterns pull out the entity instances 
and these entities are aligned to build the final 
tables. 
5) Language analyzers: We use a POS tagger and 
a dependency analyzer to analyze the text. The 
analyzed texts are used in pattern discovery 
and paraphrase discovery. 
6) Extended NE tagger: Most of the participants 
in events are likely to be Named Entities. 
However, the traditional NE categories are not 
sufficient to cover most participants of various 
events. For example, the standard MUC?s 7 
NE categories (i.e. person, location, organiza-
tion, percent, money, time and date) miss 
product names (e.g. Windows XP, Boeing 747), 
event names (Olympics, World War II), nu-
merical expressions other than monetary ex-
pressions, etc. We used the Extended NE 
categories with 140 categories and a tagger 
based on the categories. 
IR system 
Pattern discovery Paraphrase discovery 
Relevant 
documents 
Patterns 
Pattern sets 
Table 
Paraphrase 
Knowledge base 
Extended  
NE tagger
6) 5) Language 
Analyzer 
1) 
2) 
4) Table construction 
3)
732
3 Details of Components 
In this section, four important components will be 
described in detail. Prior work related to each 
component is explained and the techniques used in 
our system are presented. 
3.1 Pattern Discovery 
The pattern discovery component is responsible 
for discovering salient patterns for the topic. The 
patterns will be extracted from the documents 
relevant to the topic which are gathered by an IR 
system. 
Several unsupervised pattern discovery tech-
niques have been proposed, e.g. (Riloff 96), 
(Agichtein and Gravano 00) and (Yangarber et al 
00). Most recently we (Sudo et al 03) proposed a 
method which is triggered by a user query to dis-
cover important patterns fully automatically. In 
this work, three different representation models 
for IE patterns were compared, and the sub-tree 
model was found more effective compared to the 
predicate-argument model and the chain model. In 
the sub-tree model, any connected part of a de-
pendency tree for a sentence can be considered as 
a pattern. As it counts all possible sub-trees from 
all sentences in the retrieved documents, the com-
putation is very expensive. This problem was 
solved by requiring that the sub-trees contain a 
predicate (verb) and restricting the number of 
nodes. It was implemented using the sub-tree 
counting algorithm proposed by (Abe et al 02). 
The patterns are scored based on the relative fre-
quency of the pattern in the retrieved documents 
(fr) and in the entire corpus (fall). The formula uses 
the TF/IDF idea (Formula 1). The system ignores 
very frequent patterns, as those patterns are so 
common that they are not likely to be important to 
any particular topic, and also very rare patterns, as 
most of those patterns are noise. 
 
))(log(
)():(
ctf
tfsubtreetscore
all
r
+=          (1) 
 
The scoring function sorts all patterns which 
contain at least one extended NE and the top 100 
patterns are selected for later processing. Figure 2 
shows examples of the discovered patterns for the 
?merger and acquisition? topic. Chunks are shown 
in brackets and extended NEs are shown in upper 
case words. (COM means ?company? and MNY 
means ?money?) 
 
 
 
 <COM1> <agree to buy> <COM2> <for MNY> 
 
 
 
<COM1> <will acquire> <COM2> <for MNY> 
 
 
 
<a MNY merger> <of COM1> <and COM2> 
 
Figure 2. Pattern examples 
 
3.2 Paraphrase Discovery 
The role of the paraphrase discovery component is 
to link the patterns which mean the same thing for 
the task. Recently there has been a growing 
amount of research on automatic paraphrase dis-
covery. For example, (Barzilay 01) proposed a 
method to extract paraphrases from parallel trans-
lations derived from one original document. We 
proposed to find paraphrases from multiple news-
papers reporting the same event, using shared 
Named Entities to align the phrases (Shinyama et 
al. 02). We also proposed a method to find para-
phrases in the context of two Named Entity in-
stances in a large un-annotated corpus (Sekine 05). 
The phrases connecting two NEs are grouped 
based on two types of evidence. One is the iden-
tity of the NE instance pairs, as multiple instances 
of the same NE pair (e.g. Yahoo! and Overture) 
are likely to refer to the same relationship (e.g. 
acquisition). The other type of evidence is the 
keywords in the phrase. If we gather a lot of 
phrases connecting NE's of the same two NE 
types (e.g. company and company), we can cluster 
these phrases and find some typical expressions 
(e.g. merge, acquisition, buy). The phrases are 
clustered based on these two types of evidence 
and sets of paraphrases are created.  
Basically, we used the paraphrases found by 
the approach mentioned above. For example, the 
expressions in Figure 2 are identified as para-
phrases by this method; so these three patterns 
will be placed in the same pattern set.  
733
Note that there is an alternative method of 
paraphrase discovery, using a hand crafted syno-
nym dictionary like WordNet (WordNet Home 
page). However, we found that the coverage of 
WordNet for a particular topic is not sufficient. 
For example, no synset covers any combinations 
of the main words in Figure 2, namely ?buy?, ?ac-
quire? and ?merger?. Furthermore, even if these 
words are found as synonyms, there is the addi-
tional task of linking expressions. For example, if 
one of the expressions is ?reject the merger?, it 
shouldn?t be a paraphrase of ?acquire?. 
3.3 Extended NE tagging 
Named Entities (NE) were first introduced by the 
MUC evaluations (Grishman and Sundheim 96). 
As the MUCs concentrated on business and mili-
tary topics, the important entity types were limited 
to a few classes of names and numerical expres-
sions. However, along with the development of 
Information Extraction and Question Answering 
technologies, people realized that there should be 
more and finer categories for NE. We proposed 
one of those extended NE sets (Sekine 02). It in-
cludes 140 hierarchical categories. For example, 
the categories include Company, Company group, 
Military, Government, Political party, and Interna-
tional Organization as subcategories of Organiza-
tion. Also, new categories are introduced such as 
Vehicle, Food, Award, Religion, Language, Of-
fense, Art and so on as subcategories of Product, 
as well as Event, Natural Object, Vocation, Unit, 
Weight, Temperature, Number of people and so 
on. We used a rule-based tagger developed to tag 
the 140 categories for this experiment. 
Note that, in the proposed method, the slots of 
the final table will be filled in only with instances 
of these extended Named Entities. Most common 
nouns, verbs or sentences can?t be entries in the 
table. This is obviously a limitation of the pro-
posed method; however, as the categories are de-
signed to provide good coverage for a factoid type 
QA system, most interesting types of entities are 
covered by the categories. 
 
3.4 Table Construction 
Basically the table construction is done by apply-
ing the discovered patterns to the original corpus. 
The discovered patterns are grouped into pattern 
set using discovered paraphrase knowledge. Once 
the pattern sets are built, a table is created for each 
pattern set. We gather all NE instances matched 
by one of the patterns in the set. These instances 
are put in the same column of the table for the 
pattern set. When creating tables, we impose some 
restrictions in order to reduce the number of 
meaningless tables and to gather the same rela-
tions in one table. We require columns to have at 
least three filled instances and delete tables with 
fewer than three rows. These thresholds are em-
pirically determined using training data. 
 
Figure 3. Table Construction 
4 Experiments 
. Examples 
of 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
4.1 Data and Processing 
We conducted the experiments using the 1995 
New York Times as the corpus. The queries used 
for system development and threshold tuning were 
created by the authors, while queries based on the 
set of event types in the ACE extraction evalua-
tions were used for testing. A total of 31 test que-
ries were used; we discarded several queries 
which were ambiguous or uncertain. The test que-
ries were derived from the example sentences for 
each event type in the ACE guidelines
queries are shown in the Appendix. 
At the moment, the whole process takes about 
15 minutes on average for each query on a Pen-
tium 2.80GHz processor running Linux. The cor-
pus was analyzed in advance by a POS tagger, NE 
tagger and dependency analyzer. The processing 
News Paper  
 
* COM1 agree to buy 
ire 
 
COM1 and COM2 
COM2 for MNY 
* COM1 will acqu
COM2 for MNY 
* a MNY merger of
Newspaper Pattern Set 
Article1 
ABC agreed to 
buy CDE for $1M 
?.?????? 
Article 2 
a $20M merger of 
FGH and IJK
Article       Company                 Money
1        ABC, CDE                 $1M
2        FGH, IJK                  $20M
C no structed table
734
and counting of sub-trees takes the majority (more 
than 90%) of the time. We believe we can easily 
make it faster by programming techniques, for 
ple, using distributed puting. 
usually not 
full
e data, the evaluation data are 
sel
e more useful and interesting 
e information is. 
 
sefulness
Number of topics 
exam com
4.2 Result and Evaluation 
Out of 31 queries, the system is unable to build 
any tables for 11 queries. The major reason is that 
the IR component can?t find enough newspaper 
articles on the topic. It retrieved only a few arti-
cles for topics like ?born?, ?divorce? or ?injure? 
from The New York Times. For the moment, we 
will focus on the 20 queries for which tables were 
built. The Appendix shows some examples of 
queries and the generated tables. In total, 127 ta-
bles are created for the 20 topics, with one to thir-
teen tables for each topic. The number of columns 
in a table ranges from 2 to 10, including the 
document ID column, and the average number of 
columns is 3.0. The number of rows in a table 
range from 3 to 125, and the average number of 
rows is 16.9. The created tables are 
y filled; the average rate is 20.0%. 
In order to measure the potential and the use-
fulness of the proposed method, we evaluate the 
result based on three measures: usefulness, argu-
ment role coverage, and correctness. For the use-
fulness evaluation, we manually reviewed the 
tables to determine whether a useful table is in-
cluded or not. This is inevitably subjective, as the 
user does not specify in advance what table rows 
and columns are expected. We asked a subject to 
judge usefulness in three grades; A) very useful ? 
for the query, many people might want to use this 
table for the further investigation of the topic, B) 
useful ? at least, for some purpose, some people 
might want to use this table for further investiga-
tion and C) not useful ? no one will be interested 
in using this table for further investigation. The 
argument role coverage measures the percentage 
of the roles specified for each ACE event type 
which appeared as a column in one or more of the 
created tables for that event type. The correctness 
was measured based on whether a row of a table 
reflects the correct information. As it is impossi-
ble to evaluate all th
ected randomly.  
Table 1 shows the usefulness evaluation result. 
Out of 20 topics, two topics are judged very useful 
and twelve are judged useful. The very useful top-
ics are ?fine? (Q4 in the appendix) and ?acquit? 
(not shown in the appendix). Compared to the re-
sults in the ?useful? category, the tables for these 
two topics have more slots filled and the NE types 
of the fillers have fewer mistakes. The topics in 
the ?not useful? category are ?appeal?, ?execute?, 
?fired?, ?pardon?, ?release? and ?trial?. These are 
again topics with very few relevant articles. By 
increasing the corpus size or improving the IR 
component, we may be able to improve the per-
formance for these topics. The majority category, 
?useful?, has 12 topics. Five of them can be found 
in the appendix (all those besides Q4). For these 
topics, the number of relevant articles in the cor-
pus is relatively high and interesting relations are 
found. The examples in the appendix are selected 
from larger tables with many columns. Although 
there are columns that cannot be filled for every 
event instance, we found that the more columns 
that are filled in, th
th
Table 1. U  evaluation result 
Evaluation 
Very useful 2 
Useful 12 
Not useful 6 
 
For the 14 ?very useful? and ?useful? topics, 
the role coverage was measured. Some of the roles 
in the ACE task can be filled by different types of 
Named Entities, for example, the ?defendant? of a 
?sentence? event can be a Person, Organization or 
GPE. However, the system creates tables based on 
NE types; e.g. for the ?sentence? event, a Person 
column is created, in which most of the fillers are 
defendants. In such cases, we regard the column 
as covering the role. Out of 63 roles for the 14 
event types, 38 are found in the created tables, for 
a role coverage of 60.3%. Note that, by lowering 
the thresholds, the coverage can be increased to as 
much as 90% (some roles can?t be found because 
of Extended NE limitations or the rare appearance 
of roles) but with some sacrifice of precision. 
Table 2 shows the correctness evaluation re-
sults. We randomly select 100 table rows among 
the topics which were judged ?very useful? or 
?useful?, and determine the correctness of the in-
formation by reading the newspaper articles the 
information was extracted from. Out of 100 rows, 
84 rows have correct information in all slots. 4 
735
rows have some incorrect information in some of 
the columns, and 12 contain wrong information. 
Most errors are due to NE tagging errors (11 NE 
errors out of 16 errors). These errors include in-
stances of people which are tagged as other cate-
gories, and so on. Also, by looking at the actual 
articles, we found that co-reference resolution 
could help to fill in more information. Because the 
important information is repeatedly mentioned in 
newspaper articles, referential expressions are of-
ten used. For example, in a sentence ?In 1968 he 
was elected mayor of Indianapolis.?, we could not 
extract ?he? at the moment. We plan to add 
coreference resolution in the near future. Other 
? e entity is confused, i.e. victim 
? 
query (as both of them 
? 
He was sentenced 3 
ears and fined $1,000?. 
 
orrectness
n Numb
sources of error include: 
The role of th
and murderer 
Different kinds of events are found in one table, 
e.g., the victory of Jack Nicklaus was found in 
the political election 
use terms like ?win?) 
An unrelated but often collocate entity was 
included. For example, Year period expres-
sions are found in ?fine? events, as there are 
many expressions like ?
y
Table 2. C  evaluation result 
Evaluatio er of rows 
Correct 84 
Partially correct 4 
Incorrect 12 
5 Related Work 
As far as the authors know, there is no system 
similar to ODIE. Several methods have been pro-
posed to produce IE patterns automatically to fa-
cilitate IE knowledge creation, as is described in 
Section 3.1. But those are not targeting the fully 
automatic creation of a complete IE system for a 
new
vent detection follow 
thi
e a country 
and
ial where an ODIE-type 
system can be beneficial. 
 topic.  
There exists another strategy to extend the 
range of IE systems. It involves trying to cover a 
wide variety of topics with a large inventory of 
relations and events. It is not certain if there are 
only a limited number of topics in the world, but 
there are a limited number of high-interest topics, 
so this may be a reasonable solution from an engi-
neering point of view. This line of research was 
first proposed by (Aone and Ramos-Santacruz 00) 
and the ACE evaluations of e
s line (ACE Home Page). 
An unsupervised learning method has been ap-
plied to a more restricted IE task, Relation Dis-
covery. (Hasegawa et al 2004) used large corpora 
and an Extended Named Entity tagger to find 
novel relations and their participants. However, 
the results are limited to a pair of participants and 
because of the nature of the procedure, the discov-
ered relations are static relations lik
 its presidents rather than events. 
Topic-oriented summarization, currently pur-
sued by the DUC evaluations (DUC Home Page), 
is also closely related. The systems are trying to 
create summaries based on the specified topic for 
a manually prepared set of documents. In this case, 
if the result is suitable to present in table format, it 
can be handled by ODIE. Our previous study (Se-
kine and Nobata 03) found that about one third of 
randomly constructed similar newspaper article 
clusters are well-suited to be presented in table 
format, and another one third of the clusters can 
be acceptably expressed in table format. This sug-
gests there is a big potent
6 Future Work 
We demonstrated a new paradigm of Information 
Extraction technology and showed the potential of 
this method. However, there are problems to be 
solved to advance the technology. One of them is 
the coverage of the extracted information. Al-
though we have created useful tables for some 
topics, there are event instances which are not 
found. This problem is mostly due to the inade-
quate performance of the language analyzers (in-
formation retrieval component, dependency 
analyzer or Extended NE tagger) and the lack of a 
coreference analyzer. Even though there are pos-
sible applications with limited coverage, it will be 
essential to enhance these components and add 
coreference in order to increase coverage. Also, 
there are basic domain limitations. We made the 
system ?on-demand? for any topic, but currently 
only within regular news domains. As configured, 
the system would not work on other domains such 
as a medical, legal, or patent domain, mainly due 
to the design of the extended NE hierarchy.  
While specific hierarchies could be incorporated 
736
for new domains, it will also be desirable to inte-
grate bootstrapping techniques for rapid incre-
mental additions to the hierarchy. Also at the 
would like to investigate this problem in the future.  
7 Conclusion 
 
and demonstrates the feasibility of this approach. 
8 Acknowledgements  
arily reflect the position 
of 
-
suke Shinyama for useful comments, discussion. 
ACE Home Pag
.edu/Projects/ace 
Ke
d Practice of Knowledge in Database 
Ch
tural Lan-
Eu
Extracting Relations from Large Plaintext Collec-
moment, table column labels are simply Extended 
    NE categories, and do not indicate the role. We 
 
In this paper, we proposed ?On-demand Informa-
tion Extraction (ODIE)?. It is a system which 
automatically identifies the most salient structures 
and extracts the information on whatever topic the 
user demands.  It relies on recent advances in NLP 
technologies; unsupervised learning and several 
advanced NLP analyzers. Although it is at a pre-
liminary stage, we developed a prototype system 
which has created useful tables for many topics
 
This research was supported in part by the De-
fense Advanced Research Projects Agency under 
Contract HR0011-06-C-0023 and by the National 
Science Foundation under Grant IIS-0325657. 
This paper does not necess
the U.S. Government.  
We would like to thank Prof. Ralph Grishman, 
Dr. Kiyoshi Sudo, Dr. Chikashi Nobata, Mr. Ta-
kaaki Hasegawa, Mr. Koji Murakami and Mr. Yu
References 
e: 
http://www.ldc.upenn
DUC Home Page: http://duc.nist.gov 
WordNet Home Page:  http://wordnet.princeton.edu/ 
nji Abe, Shinji Kawasone, Tatsuya Asai, Hiroki 
Arimura and Setsuo Arikawa. 2002. ?Optimized 
Substructure Discovery for Semi-structured Data?. 
In Proceedings of the 6th European Conference on 
Principles an
(PKDD-02) 
inatsu Aone; Mila Ramos-Santacruz. 2000. ?REES: 
A Large-Scale Relation and Event Extraction Sys-
tem? In Proceedings of the 6th Applied Na
guage Processing Conference (ANLP-00) 
gene Agichtein and L. Gravano. 2000. ?Snowball: 
tionss?. In Proceedings of the 5th ACM International 
Conference on Digital Libraries (DL-00) 
Regina Barzilay and Kathleen McKeown. 2001. ?Ex-
tracting Paraphrases from a Parallel Corpus. In Pro-
ceedings of the Annual Meeting of Association of 
Computational Linguistics/ and European Chapter 
of Association of Computational Linguistics 
(ACL/EACL-01) 
Ralph Grishman and Beth Sundheim.1996. ?Message 
Understanding Conference - 6: A Brief History?, in 
Proceedings of the 16th International Conference on 
Computational Linguistics (COLING-96) 
Takaaki Hasegawa, Satoshi Sekine and Ralph Grish-
man 2004. ?Discovering Relations among Named 
Entities from Large Corpora?, In Proceedings of the 
Annual Meeting of the Association of Computa-
tional Linguistics (ACL-04)  
Ellen Riloff. 1996. ?Automatically Generating Extrac-
tion Patterns from Untagged Text?. In Proceedings 
of Thirteen National Conference on Artificial Intel-
ligence (AAAI-96) 
Satoshi Sekine, Kiyoshi Sudo and Chikashi Nobata. 
2002 ?Extended Named Entity Hierarchy? In Pro-
ceefings of the third International Conference on 
Language Resources and Evaluation (LREC-02) 
Satoshi Sekine and Chikashi Nobata. 2003. ?A survey 
for Multi-Document Summarization? In the pro-
ceedings of Text Summarization Workshop. 
Satoshi Sekine. 2005. ?Automatic Paraphrase Discov-
ery based on Context and Keywords between NE 
Pairs?. In Proceedings of International Workshop on 
Paraphrase (IWP-05) 
Yusuke Shinyama, Satoshi Sekine and Kiyoshi Sudo. 
2002. ?Automatic Paraphrase Acquisition from 
News Articles?. In Proceedings of the Human Lan-
guage Technology Conference (HLT-02) 
Kiyoshi Sudo, Satsohi Sekine and Ralph Grishman. 
2003. ?An Improved Extraction Pattern Representa-
tion Model for Automatic IE Pattern Acquisition?. 
In Proceedings of the Annual Meeting of Associa-
tion of Computational Linguistics (ACL-03) 
Roman Yangarber, Ralph Grishman, Pasi Tapanainen 
and Silja Huttunen. 2000. ?Unsupervised Discovery 
of Scenario-Level Patterns for Information Extrac-
tion?. In Proceedings of 18th International Confer-
ence on Computational Linguistics (COLING-00) 
737
Appendix: Sample queries and tables  
(Note that this is only a part of created tables) 
 
Q1: acquire, acquisition, merge, merger, buy purchase 
docid MONEY COMPANY DATE 
nyt950714.0324 About $3 billion PNC Bank Corp., Midlantic Corp.  
nyt950831.0485 $900 million Ceridian Corp., Comdata Holdings Corp. Last week 
nyt950909.0449 About $1.6 billion Bank South Corp  
nyt951010.0389 $3.1 billion CoreStates Financial Corp.  
nyt951113.0483 $286 million Potash Corp. Last month
nyt951113.0483 $400 million Chemicals Inc. Last year 
 
Q2: convict, guilty 
docid PERSON DATE AGE 
nyt950207.0001 Fleiss Dec. 2 28 
nyt950327.0402 Gerald_Amirault 1986 41 
nyt950720.0145 Hedayat_Eslaminia 1988  
nyt950731.0138 James McNally, James Johnson Bey, Jose Prieto, Pat-
terson 
1993, 1991, this 
year, 1984 
 
nyt951229.0525 Kane Last year  
 
Q3: elect 
Docid POSITION TITLE PERSON DATE 
nyt950404.0197 president Havel Dec. 29, 1989 
nyt950916.0222 president Ronald Reagan 1980 
nyt951120.0355 president Aleksander Kwasniewski  
 
Q4: fine 
Docid PERSON MONEY DATE 
nyt950420.0056 Van Halen $1,000  
nyt950525.0024 Derek Meredith $300  
nyt950704.0016 Tarango At least $15,500  
nyt951025.0501 Hamilton $12,000 This week 
nyt951209.0115 Wheatley Approximately $2,000  
 
Q5: arrest jail incarcerate imprison 
Docid PERSON YEAR PERIOD 
nyt950817.0544 Nguyen Tan Tri Four years 
nyt951018.0762 Wolf Six years 
nyt951218.0091 Carlos Mendoza-Lugo One year 
 
Q6: sentence 
Docid PERSON YEAR PERIOD 
nyt950412.0448 Mitchell Antar Four years 
nyt950421.0509 MacDonald 14 years 
nyt950622.0512 Aramony Three years 
nyt950814.0106 Obasanjo 25 years 
 
738
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 17?20,
Prague, June 2007. c?2007 Association for Computational Linguistics
System Demonstration of On-Demand Information Extraction 
Satoshi Sekine 
New York University 
715 Broadway, 7th floor 
New York, NY 10003 USA 
sekine@cs.nyu.edu  
Akira Oda 1)
Toyohashi University of Technology 
1-1 Hibarigaoka, Tenpaku-cho, 
Toyohashi, Aichi 441-3580 Japan 
oda@ss.ics.tut.ac.jp 
 
Abstract 
In this paper, we will describe ODIE, the 
On-Demand Information Extraction system. 
Given a user?s query, the system will pro-
duce tables of the salient information about 
the topic in structured form. It produces the 
tables in less than one minute without any 
knowledge engineering by hand, i.e. pat-
tern creation or paraphrase knowledge 
creation, which was the largest obstacle in 
traditional IE. This demonstration is based 
on the idea and technologies reported in 
(Sekine 06). A substantial speed-up over 
the previous system (which required about 
15 minutes to analyze one year of newspa-
per) was achieved through a new approach 
to handling pattern candidates; now less 
than one minute is required when using 11 
years of newspaper corpus. In addition, 
functionality was added to facilitate inves-
tigation of the extracted information. 
1 Introduction 
The goal of information extraction (IE) is to extract 
information about events in structured form from 
unstructured texts. In traditional IE, a great deal of 
knowledge for the systems must be coded by hand 
in advance. For example, in the later MUC evalua-
tions, system developers spent one month for the 
knowledge engineering to customize the system to 
the given test topic. Improving portability is neces-
sary to make Information Extraction technology 
useful for real users and, we believe, lead to a 
breakthrough for the application of the technology. 
 
1) This work was conducted when the first author was a 
junior research scientist at New York University. 
Sekine (Sekine 06) proposed ?On-demand in-
formation extraction (ODIE)?: a system which 
automatically identifies the most salient structures 
and extracts the information on the topic the user 
demands. This new IE paradigm becomes feasible 
due to recent developments in machine learning for 
NLP, in particular unsupervised learning methods, 
and is created on top of a range of basic language 
analysis tools, including POS taggers, dependency 
analyzers, and extended Named Entity taggers. 
This paper describes the demonstration system of 
the new IE paradigm, which incorporates some 
new ideas to make the system practical. 
2 Algorithm Overview 
We will present an overview of the algorithm in 
this section. The details can be found in (Sekine 
06).  
The basic functionality of the system is the fol-
lowing. The user types a query / topic description 
in keywords (for example, ?merge, acquire, pur-
chase?). Then tables will be created automatically 
while the user is waiting, rather than in a month of 
human labor. These tables are expected to show 
information about the salient relations for the topic. 
There are six major components in the system. 
1) IR system: Based on the query given by the 
user, it retrieves relevant documents from the 
document database. We used a simple TF/IDF 
IR system we developed. 
2) Pattern discovery: The texts are analyzed using 
a POS tagger, a dependency analyzer and an 
Extended Named Entity (ENE) tagger, which 
will be explained in (5). Then sub-trees of de-
pendency trees which are relatively frequent in 
the retrieved documents compared to the entire 
corpus are identified. The sub-trees to be used 
must satisfy some restrictions, including having 
17
between 2 and 6 nodes, having a predicate or 
nominalization as the head of the sub-tree, and 
having at least one NE. We introduced upper 
and lower frequency bounds for the sub-trees to 
be used, as we found the medium frequency 
sub-trees to be the most useful and least noisy. 
We compute a score for each pattern based on 
its frequency in the retrieved documents and in 
the entire collection.  The top scoring sub-trees 
will be called patterns, which are expected to 
indicate salient relationships of the topic and 
which will be used in the later components. We 
pre-compute such information as much as pos-
sible in order to enable usably prompt response 
to queries. 
3) Paraphrase discovery: In order to find semantic 
relationships between patterns, i.e. to find pat-
terns which should be used to build the same 
table, we use lexical knowledge such as Word-
Net and paraphrase discovery techniques. The 
paraphrase discovery was conducted off-line 
and created a paraphrase knowledge base.  
4) Table construction: In this component, the pat-
terns created in (2) are linked based on the 
paraphrase knowledge base created by (3), pro-
ducing sets of patterns which are semantically 
equivalent. Once the sets of patterns are created, 
these patterns are applied to the documents re-
trieved by the IR system (1). The matched pat-
terns pull out the entity instances from the sen-
tences and these entities are aligned to build the 
final tables. 
5) Extended NE tagger: Most of the participants in 
events are likely to be Named Entities. How-
ever, the traditional NE categories are not suffi-
cient to cover most participants of various 
events. For example, the standard MUC?s 7 NE 
categories (i.e. person, location, organization, 
percent, money, time and date) miss product 
names (e.g. Windows XP, Boeing 747), event 
names (Olympics, World War II), numerical 
expressions other than monetary expressions, 
etc. We used the Extended NE with 140 catego-
ries and a tagger developed for these categories. 
3 Speed-enhancing technology 
The largest computational load in this system is the 
extraction and scoring of the topic-relevant sub-
trees. In the previous system, 1,000 top-scoring 
sub-trees are extracted from all possible (on the 
order of hundreds of thousands) sub-trees in the 
top 200 relevant articles. This computation took 
about 14 minutes out of the total 15 minutes of the 
entire process. The difficulty is that the set of top 
articles is not predictable, as the input is arbitrary 
and hence the list of sub-trees is not predictable, 
too. Although a state-of-the-art tree mining algo-
rithm (Abe et al 02) was used, the computation is 
still impracticable for a real system.  
The solution we propose in this paper is to pre-
compute all possibly useful sub-trees in order to 
reduce runtime. We enumerate all possible sub-
trees in the entire corpus and store them in a data-
base with frequency and location information. To 
reduce the size of the database, we filter the pat-
terns, keeping only those satisfying the constraints 
on frequency and existence of predicate and named 
entities. However, it is still a big challenge, be-
cause in this system, we use 11 years of newspaper 
(AQUAINT corpus, with duplicate articles re-
moved) instead of the one year of newspaper (New 
York Times 95) used in the previous system. With 
this idea, the response time of the demonstration 
system is reduced significantly. 
The statistics of the corpus and sub-trees are as 
follows. The entire corpus includes 1,031,124 arti-
cles and 24,953,026 sentences. The frequency 
thresholds for sub-trees to be used is set to more 
than 10 and less than 10,000; i.e. sub-trees of those 
frequencies in the corpus are expected to contain 
most of the salient relationships with minimum 
noise. The sub-trees with frequency less than 11 
account for a very large portion of the data; 97.5% 
of types and 66.3% of instances, as shown in Table 
1. The sub-trees of frequency of 10,001 or more 
are relatively small; only 76 kinds and only 2.5% 
of the instances. 
 
Frequency 10,001 or 
more 
10,000-11 10 or less 
76 975,269 38,158,887# of type 
~0.0% 2.5% 97.5% 
2,313,347 29,257,437 62,097,271# of instance 
2.5% 31.2% 66.3% 
Table 1. Frequency of sub-trees 
 
We assign ID numbers to all 1 million sub-trees 
and 25 million sentences and those are mutually 
linked in a database. Also, 60 million NE occur-
rences in the sub-trees are identified and linked to 
18
the sub-tree and sentence IDs. In the process, the 
sentences found by the IR component are identi-
fied. Then the sub-trees linked to those sentences 
are gathered and the scores are calculated. Those 
processes can be done by manipulation of the data-
base in a very short time. The top sub-trees are 
used to create the output tables using NE occur-
rence IDs linked to the sub-trees and sentences. 
4 A Demonstration 
In this section, a simple demonstration scenario is 
presented with an example. Figure 1 shows the 
initial page. The user types in any keywords in the 
query box. This can be anything, but as a tradi-
tional IR system is used for the search, the key-
words have to include expressions which are nor-
mally used in relevant documents. Examples of 
such keywords are ?merge, acquisition, purchase?, 
?meet, meeting, summit? and ?elect, election?, 
which were derived from ACE event types. 
Then, normally within one minute, the system 
produces tables, such as those shown in Figure 2. 
All extracted tables are listed. Each table contains 
sentence ID, document ID and information ex-
tracted from the sentence. Some cells are empty if 
the information can?t be extracted. 
 
Figure 1. Screenshot of the initial page 
5 Evaluation 
The evaluation was conducted using scenarios 
based on 20 of the ACE event types. The accuracy 
of the extracted information was evaluated by 
judges for 100 rows selected at random. Of these 
rows, 66 were judged to be on target and correct. 
Another 10 were judged to be correct and related 
to the topic, but did not include the essential in-
formation of the topic. The remaining 24 included 
NE errors and totally irrelevant information (in 
some cases due to word sense ambiguity; e.g. 
?fine? weather vs.?fine? as a financial penalty). 
 
 
Figure 2. Screenshot of produced tables 
19
6 Other Functionality 
Functionality is provided to facilitate the user?s 
access to the extracted information. Figure 3 shows 
a screenshot of the document from which the in-
formation was extracted. Also the patterns used to 
create each table can be found by clicking the tab 
?patterns? (shown in Figure 4). This could help the 
user to understand the nature of the table. The in-
formation includes the frequency of the pattern in 
the retrieved documents and in the entire corpus, 
and the pattern?s score. 
 
 
Figure 3. Screenshot of document view 
 
 
Figure 4. Screenshot of pattern information 
 
7 Future Work 
We demonstrated the On-Demand Information Ex-
traction system, which provides usable response 
time for a large corpus. We still have several im-
provements to be made in the future. One is to in-
clude more advanced and accurate natural lan-
guage technologies to improve the accuracy and 
coverage.  For example, we did not use a corefer-
ence analyzer, and hence information which was 
expressed using pronouns or other anaphoric ex-
pressions can not be extracted. Also, more seman-
tic knowledge including synonym, paraphrase or 
inference knowledge should be included. The out-
put table has to be more clearly organized. In par-
ticular, we can?t display role information as col-
umn headings. The keyword input requirement is 
very inconvenient. For good performance, the cur-
rent system requires several keywords occurring in 
relevant documents; this is an obvious limitation. 
On the other hand, there are systems which don?t 
need any user input to create the structured infor-
mation (Banko et al 07) (Shinyama and Sekine 06). 
The latter system tries to identify all possible struc-
tural relations from a large set of unstructured 
documents. However, the user?s information needs 
are not predictable and the question of whether we 
can create structured information for all possible 
needs is still a big challenge. 
Acknowledgements 
This research was supported in part by the Defense Ad-
vanced Research Projects Agency as part of the 
Translingual Information Detection, Extraction  and 
Summarization (TIDES) program, under Grant N66001-
001-1-8917 from the Space and Naval Warfare Systems 
Center, San Diego, and by the National Science Founda-
tion under Grant IIS-00325657. This paper does not 
necessarily reflect the position of the U.S. Government. 
We would like to thank our colleagues at New York 
University, who provided useful suggestions and dis-
cussions, including, Prof. Ralph Grishman and Mr. Yu-
suke Shinyama. 
References 
Kenji Abe, Shinji Kawasone, Tatsuya Asai, Hiroki Ari-
mura and Setsuo Arikawa. 2002. ?Optimized Sub-
structure Discovery for Semi-structured Data?. 
PKDD-02. 
Michele Banko, Michael J Cafarella, Stephen Soderland, 
Matt Broadhead and Oren Etzioni. 2007. ?Open In-
formation Extraction from Web?. IJCAI-07. 
Satoshi Sekine. 2006. ?On-Demand Information Extrac-
tion?. COLING-ACL-06. 
Yusuke Shinyama and Satoshi Sekine, 2006. ?Preemp-
tive Information Extraction using Unrestricted Rela-
tion Discovery?. HLT-NAACL-2006. 
20
The Unknown Word Problem: a Morphological Analysis of
Japanese Using Maximum Entropy Aided by a Dictionary
Kiyotaka Uchimotoy, Satoshi Sekinez and Hitoshi Isaharay
yCommunications Research Laboratory
2-2-2, Hikari-dai, Seika-cho, Soraku-gun,
Kyoto, 619-0289 Japan
[uchimoto, isahara]@crl.go.jp
zNew York University
715 Broadway, 7th oor
New York, NY 10003, USA
sekine@cs.nyu.edu
Abstract
In this paper we describe a morphological analy-
sis method based on a maximum entropy model.
This method uses a model that can not only
consult a dictionary with a large amount of lex-
ical information but can also identify unknown
words by learning certain characteristics. The
model has the potential to overcome the un-
known word problem.
1 Introduction
Morphological analysis is one of the basic tech-
niques used in Japanese sentence analysis. A
morpheme is a minimal grammatical unit, such
as a word or a sux, and morphological analysis
is the process segmenting a given sentence into
a row of morphemes and assigning to each mor-
pheme grammatical attributes such as a part-
of-speech (POS) and an inection type. One of
the most important problems in morphological
analysis is that posed by unknown words, which
are words found in neither a dictionary nor a
training corpus, and there have been two sta-
tistical approaches to this problem. One is to
acquire unknown words from corpora and put
them into a dictionary (e.g., (Mori and Nagao,
1996)), and the other is to estimate a model
that can identify unknown words correctly (e.g.,
(Kashioka et al, 1997; Nagata, 1999)). We
would like to be able to make good use of both
approaches. If words acquired by the former
method could be added to a dictionary and a
model developed by the latter method could
consult the amended dictionary, then the model
could be the best statistical model which has
the potential to overcome the unknown word
problem. Mori and Nagao proposed a statisti-
cal model that can consult a dictionary (Mori
and Nagao, 1998). In their model the proba-
bility that a string of letters or characters is
a morpheme is augmented when the string is
found in a dictionary. The improvement of the
accuracy was slight, however, so we think that
it is dicult to eciently integrate the mecha-
nism for consulting a dictionary into an n-gram
model. In this paper we therefore describe a
morphological analysis method based on a max-
imum entropy (M.E.) model. This method uses
a model that can not only consult a dictionary
but can also identify unknown words by learn-
ing certain characteristics. To learn these char-
acteristics, we focused on such information as
whether or not a string is found in a dictio-
nary and what types of characters are used in a
string. The model estimates how likely a string
is to be a morpheme according to the informa-
tion on hand. When our method was used to
identify morpheme segments in sentences in the
Kyoto University corpus and to identify the ma-
jor parts-of-speech of these morphemes, the re-
call and precision were respectively 95.80% and
95.09%.
2 A Morpheme Model
This section describes a model which estimates
how likely a string is to be a morpheme. We
implemented this model within an M.E. frame-
work.
Given a tokenized test corpus, the problem
of Japanese morphological analysis can be re-
duced to the problem of assigning one of two
tags to each string in a sentence. A string is
tagged with a 1 or a 0 to indicate whether or
not it is a morpheme. When a string is a mor-
pheme, a grammatical attribute is assigned to
it. The 1 tag is thus divided into the num-
ber, n, of grammatical attributes assigned to
morphemes, and the problem is to assign an at-
tribute (from 0 to n) to every string in a given
sentence. The (n+1) tags form the space of \fu-
tures" in the M.E. formulation of our problem
of morphological analysis. The M.E. model, as
well as other similar models, enables the com-
putation of P (f jh) for any future f from the
space of possible futures, F , and for every his-
tory, h, from the space of possible histories, H.
A \history" in M.E. is all of the conditioning
data that enable us to make a decision in the
space of futures. In the problem of morphologi-
cal analysis, we can reformulate this in terms of
nding the probability of f associated with the
relationship at index t in the test corpus:
P (f jh
t
) = P (f jInformation derivable
from the test corpus
related to relationship t)
The computation of P (f jh) in any M.E. models
is dependent on a set of \features" which would
be helpful in making a prediction about the fu-
ture. Like most current M.E. models in com-
putational linguistics, our model is restricted to
those features which are binary functions of the
history and future. For instance, one of our fea-
tures is
g(h; f) =
8
>
<
>
:
1 : if has(h; x) = true;
x = \POS( 1)(Major) : verb;
00
& f = 1
0 : otherwise:
(1)
Here \has(h,x)" is a binary function that re-
turns true if the history h has feature x. In our
experiments, we focused on such information as
whether or not a string is found in a dictionary,
the length of the string, what types of characters
are used in the string, and the part-of-speech of
the adjacent morpheme.
Given a set of features and some training
data, the M.E. estimation process produces a
model in which every feature g
i
has an associ-
ated parameter 
i
. This enables us to compute
the conditional probability as follows (Berger et
al., 1996):
P (f jh) =
Q
i

g
i
(h;f)
i
Z

(h)
(2)
Z

(h) =
X
f
Y
i

g
i
(h;f)
i
: (3)
The M.E. estimation process guarantees that for
every feature g
i
, the expected value of g
i
accord-
ing to the M.E. model will equal the empirical
expectation of g
i
in the training corpus. In other
words,
X
h;f
~
P (h; f)  g
i
(h; f)
=
X
h
~
P (h) 
X
f
P
M:E:
(f jh)  g
i
(h; f): (4)
Here
~
P is an empirical probability and P
M:E:
is
the probability assigned by the model.
We dene part-of-speech and bunsetsu
boundaries as grammatical attributes. Here a
bunsetsu is a phrasal unit consisting of one or
more morphemes. When there are m types
of parts-of-speech, and the left-hand side of
each morpheme may or may not be a bunsetsu
boundary, the number, n, of grammatical at-
tributes assigned to morphemes is 2m.
1
We
propose a model which estimates the likelihood
that a given string is a morpheme and has the
grammatical attribute i(1  i  n). We call it
a morpheme model. This model is represented
by Eq. (2), in which f can be one of (n + 1)
tags from 0 to n.
A given sentence is divided into morphemes,
and a grammatical attribute is assigned to each
morpheme so as to maximize the sentence prob-
ability estimated by our morpheme model. Sen-
tence probability is dened as the product of the
probabilities estimated for a particular division
of morphemes in a sentence. We use the Viterbi
algorithm to nd the optimal set of morphemes
in a sentence and we use the method proposed
by Nagata (Nagata, 1994) to search for the N-
best sets.
3 Experiments and Discussion
3.1 Experimental Conditions
The part-of-speech categories that we used fol-
low those of JUMAN (Kurohashi and Nagao,
1999). There are 53 categories covering all pos-
sible combinations of major and minor cate-
gories as dened in JUMAN. The number of
grammatical attributes is 106 if we include the
detection of whether or not the left side of a
morpheme is a bunsetsu boundary. We do not
identify inection types probabilistically since
1
Not only morphemes but also bunsetsus can be iden-
tied by considering the information related to their bun-
setsu boundaries.
they can be almost perfectly identied by check-
ing the spelling of the current morpheme after
a part-of-speech has been assigned to it. There-
fore, f in Eq. (2) can be one of 107 tags from 0
to 106.
We used the Kyoto University text corpus
(Version 2) (Kurohashi and Nagao, 1997), a
tagged corpus of the Mainichi newspaper. For
training, we used 7,958 sentences from newspa-
per articles appearing from January 1 to Jan-
uary 8, 1995, and for testing, we used 1,246
sentences from articles appearing on January 9,
1995.
Given a sentence, for every string consisting
of ve or less characters and every string ap-
pearing in the JUMAN dictionary (Kurohashi
and Nagao, 1999), whether or not the string is
a morpheme was determined and then the gram-
matical attribute of each string determined to
be a morpheme was identied and assigned to
that string. The maximum length was set at ve
because morphemes consisting of six or more
characters are mostly compound words or words
consisting of katakana characters. The stipula-
tion that strings consisting of six or more char-
acters appear in the JUMAN dictionary was set
because long strings not present in the JUMAN
dictionary were rarely found to be morphemes
in our training corpus. Here we assume that
compound words that do not appear in the JU-
MAN dictionary can be divided into strings con-
sisting of ve or less characters because com-
pound words tend not to appear in dictionar-
ies, and in fact, compound words which con-
sist of six or more characters and do not ap-
pear in the dictionary were not found in our
training corpus. Katakana strings that are not
found in the JUMAN dictionary were assumed
to be included in the dictionary as an entry
having the part-of-speech \Unknown(Major),
Katakana(Minor)." An optimal set of mor-
phemes in a sentence is searched for by em-
ploying the Viterbi algorithm under the con-
dition that connectivity rules dened between
parts-of-speech in JUMAN must be met. The
assigned part-of-speech in the optimal set is
not always selected from the parts-of-speech at-
tached to entries in the JUMAN dictionary, but
may also be selected from the 53 categories of
the M.E. model. It is dicult to select an appro-
priate category from the 53 when there is little
training data, so we assume that every entry in
the JUMAN dictionary has all possible parts-of-
speech, and the part-of-speech assigned to each
morpheme is selected from those attached to the
entry corresponding to the morpheme string.
The features used in our experiments are
listed in Table 1. Each row in Table 1 contains a
feature type, feature values, and an experimen-
tal result that will be explained later. Each fea-
ture consists of a type and a value. The features
are basically some attributes of the morpheme
itself or those of the morpheme to the left of
it. We used the 31,717 features that were found
three or more times in the training corpus. The
notations \(0)" and \(-1)" used in the feature
type column in Table 1 respectively indicate a
target string and the morpheme on the left of
it.
The terms used in the table are the following:
String: Strings which appeared as a morpheme
ve or more times in the training corpus
Length: Length of a string
POS: Part-of-speech. \Major" and \Minor"
respectively indicate major and minor part-
of-speech categories as dened in JUMAN.
Inf: Inection type as dened in JUMAN
Dic: We use the JUMAN dictionary, which has
about 200,000 entries (Kurohashi and Na-
gao, 1999). \Major&Minor" indicates pos-
sible combinations between major and mi-
nor part-of-speech categories. When the
target string is in the dictionary, the part-
of-speech attached to the entry correspond-
ing to the string is used as a feature value.
If an entry has two or more parts-of-speech,
the part-of-speech which leads to the high-
est probability in a sentence estimated from
our model is selected as a feature value.
JUMAN has another type of dictionary,
which is called a phrase dictionary. Each
entry in the phrase dictionary consists of
one or more morphemes such as \? (to,
case marker), ? (wa, topic marker), ??
(ie, say)." JUMAN uses this dictionary to
detect morphemes which need a longer con-
text to be identied correctly. When the
target string corresponds to the string of
the left most morpheme in the phrase dic-
tionary in JUMAN, the part-of-speech at-
Table 1: Features.
Feature Accuracy without
number Feature type Feature value (Number of value) each feature set
Recall Precision F-measure
1 String(0) (4,331) 93.66% 93.81% 93.73
2 String(-1) (4,331) ( 2.14%) ( 1.28%) ( 1.71)
3 Dic(0)(Major) Verb, Verb&Phrase, Adj, Adj&Phrase, 94.64% 92.87% 93.75
: : : (28)
4 Dic(0)(Minor) Common noun, Common noun&Phrase, ( 1.16%) ( 2.22%) ( 1.69)
Topic marker, : : : (90)
5 Dic(0)(Major&Minor) Noun&Common noun,
Noun&Common noun&Phrase, : : : (103)
6 Length(0) 1, 2, 3, 4, 5, 6 or more (6) 95.52% 94.11% 94.81
7 Length(-1) 1, 2, 3, 4, 5, 6 or more (6) ( 0.28%) ( 0.98%) ( 0.63)
8 TOC(0)(Beginning) Kanji, Hiragana, Symbol, Number, 95.17% 93.89% 94.52
Katakana, Alphabet (6)
9 TOC(0)(End) Kanji, Hiragana, Symbol, Number, ( 0.63%) ( 1.20%) ( 0.92)
Katakana, Alphabet (6)
10 TOC(0)(Transition) Kanji!Hiragana, Number!Kanji,
Katakana!Kanji, : : : (30)
11 TOC(-1)(End) Kanji, Hiragana, Symbol, Number,
Katakana, Alphabet (6)
12 TOC(-1)(Transition) Kanji!Hiragana, Number!Kanji,
Katakana!Kanji, : : : (30)
13 POS(-1)(Major) Verb, Adj, Noun, Unknown, : : : (15) 95.60% 95.31% 95.45
14 POS(-1)(Minor) Common noun, Sahen noun, Numeral, ( 0.20%) (+0.22%) (+0.01)
: : : (45)
15 POS(-1)(Major&Minor) [nil], Noun&Common noun,
Noun&Common noun&Phrase, : : : (54)
16 Inf(-1)(Major) Vowel verb, : : : (33) 95.66% 95.00% 95.33
17 Inf(-1)(Minor) Stem, Basic form, Imperative form, : : : (60) ( 0.14%) ( 0.09%) ( 0.11)
18 BB(-1) [nil], [exist] (2) 95.82% 95.25% 95.53
19 BB(-1) & Noun&Common, noun&Bunsetsu boundary, (+0.02%) (+0.16%) (+0.09)
POS(-1)(Major&Minor) Noun&Common, noun&Within a bunsetsu,
: : : (106)
tached to the entry plus the information
that it is in the phrase dictionary (such as
\Verb&Phrase") is used as a feature value.
TOC: Types of characters used in a string.
\(Beginning)" and \(End)" respectively
represent the leftmost and rightmost char-
acters of a string. When a string con-
sists of only one character, the \(Begin-
ning)" and \(End)" are the same charac-
ter. \TOC(0)(Transition)" represents the
transition from the leftmost character to
the rightmost one in a string. \TOC(-
1)(Transition)" represents the transition
from the rightmost character in the adja-
cent morpheme on the left to the leftmost
one in the target string. For example, when
the adjacent morpheme on the left is \?
? (sensei, teacher)" and the target string
is \? (ni, case marker)," the feature value
\Kanji!Hiragana" is selected.
BB: Indicates whether or not the left side of a
morpheme is a bunsetsu boundary.
3.2 Results and Discussion
Some results of the morphological analysis are
listed in Table 2. Recall is the percentage of
morphemes in the test corpus whose segmen-
tation and major POS tag are identied cor-
rectly. Precision is the percentage of all mor-
phemes identied by the system that are iden-
tied correctly. F represents the F-measure and
is dened by the following equation.
F  measure =
2Recall  Precision
Recall + Precision
Table 2 shows results obtained by using our
method, by using JUMAN, and by using JU-
MAN plus KNP (Kurohashi, 1998). We show
the result obtained using JUMAN plus KNP
because JUMAN alone assigns an \Unknown"
tag to katakana strings when they are not in
the dictionary. All katakana strings not found
Table 2: Results of Experiments (Segmentation and major POS tagging).
Recall Precision F-measure
Our method 95.80% (29,986/31,302) 95.09% (29,986/31,467) 95.44
JUMAN 95.25% (29,814/31,302) 94.90% (29,814/31,417) 95.07
JUMAN+KNP 98.49% (30,830/31,302) 98.13% (30,830/31,417) 98.31
in the dictionary are therefore evaluated as er-
rors. KNP improves on JUMAN by replacing
the \Unknown" tag with a \Noun" tag and dis-
ambiguating part-of-speech ambiguities which
arise during the process of parsing when there is
more than one JUMAN analysis with the same
score.
The accuracy in segmentation and major
POS tagging obtained with our method and
that obtained with JUMAN were about 3%
worse than that obtained with JUMAN plus
KNP. We think the main reason for this was
an insucient amount of training data and fea-
ture sets and the inconsistency of the corpus.
The number of sentences in the training cor-
pus was only about 8,000, and we did not use
as many combined features as were proposed in
Ref. (Uchimoto et al, 1999). We were unable to
use more training data or more feature sets be-
cause every string consisting of ve or less char-
acters in our training corpus was used to train
our model, so the amount of tokenized train-
ing data would have become too large and the
training would not have been completed on the
available machine if we had used more training
data or more feature sets. The inconsistency
of the corpus was due to the way the corpus
was made. The Kyoto University corpus was
made by manually correcting the output of JU-
MAN plus KNP, and it is dicult to manually
correct all of the inconsistencies in the output.
The use of JUMAN plus KNP thus has an ad-
vantage over the use of our method when we
evaluate a system's accuracy by using the Ky-
oto University corpus. For example, the num-
ber of morphemes whose rightmost character is
\?" was 153 in the test corpus, and they were
all the same as those in the output of JUMAN
plus KNP. There were three errors (about 2%)
in the output of our system. There were several
inconsistencies in the test corpus such as \??
(seisan, Noun), ? (sha, Sux)(producer)," and
\??? (shouhi-sha, Noun)(consumer)." They
should have been corrected in the corpus-
making process to \?? (seisan, Noun),? (sha,
Sux)(producer)," and \?? (shouhi, Noun),
? (sha, Sux)(consumer)." It is dicult for
our model to discriminate among these with-
out over-training when there are such incon-
sistencies in the corpus. Other similar incon-
sistencies were, for example, \??? (geijutsu-
ka, Noun)(artist)" and \?? (kougei, Noun),
? (ka, Sux)(craftsman)," \??? (keishi-cho,
Noun)(the Metropolitan Police Board)" and \?
? (kensatsu, Noun), ? (cho, Noun)(the Pub-
lic Prosecutor's Oce)," and \??? (genjitsu-
teki, Adjective)(realistic)" and \?? (risou,
Noun), ? (teki, Sux)(ideal).". If these had
been corrected consistently when making the
corpus, the accuracy obtained by our method
could have been better than that shown in Ta-
ble 2. A study on corpus revision should be un-
dertaken to resolve this issue. We believe it can
be resolved by using our trained model. There
is a high possibility that a morpheme lacks con-
sistency in the training corpus when its proba-
bility, re-estimated by our model, is low. Thus
a method which detects morphemes having a
low probability can identify those lacking con-
sistency in the training corpus. We intend to
try this in the future.
3.3 Features and Accuracy
In our model, dictionary information and cer-
tain characteristics of unknown words are re-
ected as features, as shown in Table 1.
\String" and \Dic" reect the dictionary in-
formation,
2
and \Length" and \TOC"(types
of characters) reect the characteristics of un-
known words. Therefore, our model can not
only consult a dictionary but can also detect un-
known words. Table 1 shows the results of an
2
\String" indicates strings that make up a morpheme
and were found ve or more times in the training corpus.
Using this information as features in our M.E. model
corresponds to consulting a dictionary constructed from
the training corpus.
93
93.5
94
94.5
95
95.5
96
96.5
97
0 1000 2000 3000 4000 5000 6000 7000 8000
F-
m
ea
su
re
Number of Sentences
"training"
"testing"
Figure 1: Relation between accuracy and the number of training sentences.
analysis without the complete feature set. Al-
most all of the feature sets improved accuracy.
The contribution of the dictionary information
was especially signicant.
There were cases, however, in which the use
of dictionary information led to a decrease in
the accuracy. For example, we found these er-
roneous segmentations:
\?? (umi, sea)?? (ni, case marker)???
? (kaketa, bet)????? (romanha, the Ro-
mantic school)?" and \??? (aranami, rag-
ing waves)?? (ni, case marker)??? (make,
lose)???? (naishin, one's inmost heart)
?? (to, case marker)?" (Underlined strings
were errors.) when the correct segmentations
were:
\?? (umi, sea)?? (ni, case marker)???
? (kaketa, bet)???? (roman, romance)??
(wa, topic marker)?" and \??? (aranami,
raging waves)?? (ni, case marker)?????
(makenai, not to lose)?? (kokoro, heart)??
(to, case marker)?" (\?" indicates a morpho-
logical boundary.).
These errors were caused by nonstandard en-
tries in the JUMAN dictionary. The dictio-
nary had not only the usual notation using kanji
characters, \????" and \??," but also the
uncommon notation using hiragana strings, \?
???" and \???". To prevent this type of
error, it is necessary to remove nonstandard en-
tries from the dictionary or to investigate the
frequency of such entries in large corpora and
to use it as a feature.
3.4 Accuracy and the Amount of
Training Data
The accuracies (F-measures) for the training
corpus and the test corpus are shown in Figure 1
plotted against the number of sentences used
for training. The learning curve shows that we
can expect improvement if we use more training
data.
3.5 Unknown Words and Accuracy
The strength of our method is that it can iden-
tify morphemes when they are unknown words
and can assign appropriate parts-of-speech to
them. For example, the nouns \?? (Souseki)"
and \?? (Rohan)" are not found in the JU-
Table 3: Accuracy for unknown words (Recall).
Segmentation and Segmentation and
major POS tagging minor POS tagging
For words not found in the dictionary
nor in our training corpus
Our method 69.90% (432/618) 27.51% (170/618)
JUMAN+KNP 79.29% (490/618) 20.55% (127/618)
For words not found in the dictionary
nor in our features
Our method 76.17% (719/944) 32.20% (304/944)
JUMAN+KNP 85.70% (809/944) 27.22% (257/944)
For words not found in the dictionary
Our method 82.40% (1,138/1,381) 49.24% (680/1,381)
JUMAN+KNP 89.79% (1,240/1,381) 38.60% (533/1,381)
MAN dictionary. JUMAN plus KNP analyzes
them simply as \? (Noun)? (Noun)" and \?
(Adverb)? (Noun)," whereas our system ana-
lyzes both of them correctly. Our system cor-
rectly identied them as names of people even
though they were not in the dictionary and did
not appear as features in our M.E. model. Since
these names, or proper nouns, are newly coined
and can be represented by a variety of expres-
sions, no proper nouns can be included in a dic-
tionary, nor can they appear in a training cor-
pus; this means that proper nouns could easily
be unknown words. We investigated the accu-
racy of our method in identifying morphemes
when they are unknown words, and the re-
sults are listed in Table 3. The rst row in
each section shows the recall for the morphemes
that were unknown words. The second row in
each section shows the percentage of morphemes
whose segmentation and \minor" POS tag were
identied correctly. The dierence between the
rst and second lines, the third and fourth lines,
and fth and sixth lines is the denition of un-
known words. Unknown words were dened re-
spectively as words not found in the dictionary
nor in our training corpus, as words not found
in the dictionary nor in our features, and as
words not found in the dictionary. Our accu-
racy, shown as the second rows in Table 3 was
more than 5% better than that of JUMAN plus
KNP for each denition. These results show
that our model can eciently learn the char-
acteristics of unknown words, especially those
of proper nouns such as the names of people,
organizations, and locations.
4 Related Work
Several methods based on statistical models
have been proposed for the morphological anal-
ysis of Japanese sentences. An F-measure of
about 96% was achieved by a method based
on a hidden Markov model (HMM) (Takeuchi
and Matsumoto, 1997) and by one based on
a variable-memory Markov model (Haruno and
Matsumoto, 1997; Kitauchi et al, 1999). Al-
though the accuracy obtained with these meth-
ods was better than that obtained with ours,
their accuracy cannot be compared directly
with that of our method because their part-
of-speech categories dier from ours. And an
advantage of our model is that it can handle
unknown words, whereas their models do not
handle unknown words well. In their models,
unknown words are divided into a combination
of a word consisting of one character and known
words. Haruno and Matsumoto (Haruno and
Matsumoto, 1997) achieved a recall of about
96% when using trigram or greater information,
but achieved a recall of only 94% when using bi-
gram information. This leads us to believe that
we could obtain better accuracy if we use tri-
gram or greater information. We plan to do so
in future work.
Two approaches have been used to deal with
unknown words: acquiring unknown words from
corpora and putting them into a dictionary
(e.g., (Mori and Nagao, 1996)) and develop-
ing a model that can identify unknown words
correctly (e.g., (Kashioka et al, 1997; Nagata,
1999)). Nagata reported a recall of about 40%
for unknown words (Nagata, 1999). As shown
in Table 3, our method achieved a recall of
69.90% for unknown words. Our accuracy was
about 30% better than his. It is dicult to
compare his method with ours directly because
he used a dierent corpus (the EDR corpus),
but the part-of-speech categories and the def-
inition of morphemes he used were similar to
ours. Thus, this comparison is helpful in evalu-
ating our method. There are no spaces between
morphemes in Japanese. In general, therefore,
detecting whether a given string is an unknown
word or is not a morpheme is dicult when it
is not found in the dictionary, nor in the train-
ing corpus. However, our model learns whether
or not a given string is a morpheme and has a
huge amount of data for learning what in a cor-
pus is not a morpheme. Therefore, we believe
that the characteristics of our model led to its
good results for identifying unknown words.
Mori and Nagao proposed a model that can
consult a dictionary (Mori and Nagao, 1998);
they reported an F-measure of about 92 when
using the EDR corpus and of about 95 when
using the Kyoto University corpus. Their slight
improvement in accuracy by using dictionary in-
formation resulted in an F-measure of about 0.2,
while our improvement was about 1.7. Their
accuracy of 95% when using the Kyoto Univer-
sity corpus is similar to ours, but they added
to their dictionary all of the words appearing
in the training corpus. Therefore, their exper-
iment had to deal with fewer unknown words
than ours did.
With regard to the morphological analy-
sis of English sentences, methods for part-of-
speech tagging based on an HMM (Cutting et
al., 1992), a variable-memory Markov model
(Schutze and Singer, 1994), a decision tree
model (Daelemans et al, 1996), an M.E. model
(Ratnaparkhi, 1996), a neural network model
(Schmid, 1994), and a transformation-based
error-driven learning model (Brill, 1995) have
been proposed, as well as a combined method
(Marquez and Padro, 1997; van Halteren et al,
1998). On available machines, however, these
models cannot handle a large amount of lex-
ical information. We think that our model,
which can not only consult a dictionary with
a large amount of lexical information, but can
also identify unknown words by learning cer-
tain characteristics, has the potential to achieve
good accuracy for part-of-speech tagging in En-
glish. We plan to apply our model to English
sentences.
5 Conclusion
This paper described a method for morpho-
logical analysis based on a maximum entropy
(M.E.) model. This method uses a model
that can not only consult a dictionary but can
also identify unknown words by learning cer-
tain characteristics. To learn these characteris-
tics, we focused on such information as whether
or not a string is found in a dictionary and
what types of characters are used in a string.
The model estimates how likely a string is to
be a morpheme according to the information
on hand. When our method was used to iden-
tify morpheme segments in sentences in the Ky-
oto University corpus and to identify the ma-
jor parts-of-speech of these morphemes, the re-
call and precision were respectively 95.80% and
95.09%. In our experiments without each fea-
ture set shown in Tables 1, we found that dic-
tionary information signicantly contributes to
improving accuracy. We also found that our
model can eciently learn the characteristics of
unknown words, especially proper nouns such
as the names of people, organizations, and lo-
cations.
References
Adam L. Berger, Stephen A. Della Pietra,
and Vincent J. Della Pietra. 1996. A Max-
imum Entropy Approach to Natural Lan-
guage Processing. Computational Linguis-
tics, 22(1):39{71.
Eric Brill. 1995. Transformation-Based Error-
Driven Learning and Natural Language Pro-
cessing: A Case Study in Part-of-Speech Tag-
ging. Computational Linguistics, 21(4):543{
565.
Doung Cutting, Julian Kupiec, Jan Peder-
sen, and Penelope Sibun. 1992. A Practical
Part-of-Speech Tagger. In Proceedings of the
Third Conference on Applied Natural Lan-
guage Processing, pages 133{140.
Walter Daelemans, Jakub Zavrel, Peter Berck,
and Steven Gills. 1996. MBT: A Memory-
Based Part-of-Speech Tagger-Generator. In
Proceedings of the 4th Workshop on Very
Large Corpora, pages 1{14.
Masahiko Haruno and Yuji Matsumoto. 1997.
Mistake-Driven Mixture of Hierarchical-Tag
Context Trees. In Proceedings of the 35th An-
nual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 230{237.
Hideki Kashioka, Stephen G. Eubank, and
Ezra W. Black. 1997. Decision-Tree Mor-
phological Analysis without a Dictionary for
Japanese. In Proceedings of the Natural Lan-
guage Processing Pacic Rim Symposium,
pages 541{544.
Akira Kitauchi, Takehito Utsuro, and Yuji Mat-
sumoto. 1999. Probabilistic Model Learn-
ing for Japanese Morphological Analysis by
Error-driven Feature Selection. Transactions
of Information Processing Society of Japan,
40(5):2325{2337. (in Japanese).
Sadao Kurohashi and Makoto Nagao. 1997.
Building a Japanese Parsed Corpus while Im-
proving the Parsing System. In Proceedings
of the Natural Language Processing Pacic
Rim Symposium, pages 451{456.
Sadao Kurohashi and Makoto Nagao, 1999.
Japanese Morphological Analysis System JU-
MAN Version 3.61. Department of Informat-
ics, Kyoto University.
Sadao Kurohashi, 1998. Japanese Depen-
dency/Case Structure Analyzer KNP Ver-
sion 2.0b6. Department of Informatics, Ky-
oto University.
Llu

is Marquez and Llu

is Padro. 1997. A Flexi-
ble POS Tagger Using an Automatically Ac-
quired Language Model. In Proceedings of
the 35th Annual Meeting of the Association
for Computational Linguistics (ACL), pages
238{252.
Shinsuke Mori and Makoto Nagao. 1996.
Word Extraction from Corpora and Its Part-
of-Speech Estimation Using Distributional
Analysis. In Proceedings of the 16th Interna-
tional Conference on Computational Linguis-
tics (COLING96), pages 1119{1122.
Shinsuke Mori and Makoto Nagao. 1998. An
Improvement of a Morphological Analysis by
a Morpheme Clustering. Journal of Nat-
ural Language Processing, 5(2):75{103. (in
Japanese).
Masaaki Nagata. 1994. A Stochastic Japanese
Morphological Analyzer Using a Forward-DP
Backward-A

N-Best Search Algorithm. In
Proceedings of the 15th International Con-
ference on Computational Linguistics (COL-
ING94), pages 201{207.
Masaaki Nagata. 1999. A Part of Speech Esti-
mation Method for Japanese UnknownWords
using a Statistical Model of Morphology and
Context. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 277{284.
Adwait Ratnaparkhi. 1996. A Maximum En-
tropy Model for Part-Of-Speech Tagging. In
Conference on Empirical Methods in Natural
Language Processing, pages 133{142.
Helmut Schmid. 1994. Part-Of-Speech Tagging
with Neural Networks. In Proceedings of the
15th International Conference on Computa-
tional Linguistics (COLING94), pages 172{
176.
Hinrich Schutze and Yoram Singer. 1994. Part-
of-Speech Tagging Using a Variable Memory
Markov Model. In Proceedings of the 32nd
Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 181{187.
Koichi Takeuchi and Yuji Matsumoto. 1997.
HMM Parameter Learning for Japanese
Morphological Analyzer. Transactions of
Information Processing Society of Japan,
83(3):500{509. (in Japanese).
Kiyotaka Uchimoto, Satoshi Sekine, and Hi-
toshi Isahara. 1999. Japanese Dependency
Structure Analysis Based on Maximum En-
tropy Models. In Proceedings of the Ninth
Conference of the European Chapter of the
Association for Computational Linguistics
(EACL'99), pages 196{203.
Hans van Halteren, Jakub Zavrel, and Walter
Daelemans. 1998. Improving Data Driven
Wordclass Tagging by System Combination.
In Proceedings of the COLING-ACL '98,
pages 491{497.
A Survey for Multi-Document Summarization 
Satoshi Sekine 
New York University 
715 Broadway, 7th floor 
New York, NY, 10003, USA 
sekine@cs.nyu.edu 
 
 
Chikashi Nobata 
Communications Reserach Laboratory 
2-2-2 Hikaridai, Seika-chou, Soraku-gun 
Kyoto, 619-0289, Japan 
nova@crl.go.jp 
 
Abstract 
Automatic Multi-Document summarization is still hard 
to realize. Under such circumstances, we believe, it is 
important to observe how humans are doing the same 
task, and look around for different strategies. 
We prepared 100 document sets similar to the ones 
used in the DUC multi-document summarization task. 
For each document set, several people prepared the 
following data and we conducted a survey. 
A) Free style summarization 
B) Sentence Extraction type summarization 
C) Axis (type of main topic) 
D) Table style summary 
In particular, we will describe the last two in detail, 
as these could lead to a new direction for multi-
summarization research. 
1 Introduction 
Automatic Multi-Document summarization is still hard 
to realize. Like single document summarization for 
newspaper articles, where we don?t have a notably bet-
ter automatic summarization algorithm than a simple 
lead based method, automatic multi-document summa-
rization faces very difficult challenges. Under such 
circumstances, we believe, it is important to observe 
how humans are doing on the same task, and look for 
possible different strategies.  
Assume you are given several documents talking 
about the same topic, and are asked to summarize them, 
what might you do. The authors tried this by them-
selves. First we used a marker to mark the important 
phrases or sentences. Then we tried to connect them, in 
some cases by figuring out the main or common topics 
in the marked sentences, or in some cases, by making a 
list or a table to figure out the overview of the docu-
ments. When we looked at the result at this stage we 
noticed that these are very good summaries, even if 
they are not summaries in the conventional sense (a set 
of sentences to be read). The main topics are good to 
understand the overall issues in the document set and 
the table is a good digest of the issues throughout the 
document set. If we can automatically create such data 
from document sets, we might be able to make a good 
summary. The questions arising here are what kinds of 
?main topic? we can make in general, and what per-
centage of document sets are suitable for table-style 
summarization. 
The main topics we created in our hand summary 
experiment were like lists of keywords, but we found 
that there are more general types like ?these documents 
are talking about a single person?. As keyword extrac-
tion has been one of the techniques in summarization, 
we will focus on the types of the main topics in the 
following experiments. 
We will describe the definition of our types and re-
port on the experiment of manually creating table-style 
summarization, as well as analyses of free style sum-
maries and sentence extraction type summaries. We 
prepared 100 document sets similar to the ones used in 
the DUC multi-document summarization task (DUC 
homepage). For each document set, annotators prepared 
the following data. 
A) Free style summarization 
B) Sentence Extraction type summarization 
C) Axis (type of main topic) 
D) Table style summary 
In particular, we will describe the last two in detail, 
as these could lead to a new direction for multi-
document summarization research. 
2 Document Sets 
First, we describe how we accumulated our 100 multi-
document data. We found that the topics of DUC multi-
document data are a bit biased as it is pre-filtered for 
evaluation purposes, i.e. DUC document sets are care-
fully chosen as described in the guidelines. The pre-
filtering is useful for evaluation purposes, but it does 
not necessarily reflect the distribution of user needs or 
distribution of topics in the news. We would like to 
obtain relatively more balanced document sets. We 
adopted the procedure described in the following, 
where the entire experiment was done using a Japanese 
newspaper corpus (Mainichi 1998 and 1999). 
  Select an article randomly from the corpus 
(seed) 
  Choose keywords from each article. Keywords 
are all nouns of frequency more than 1, except 
for some special types of nouns 
  Use dice coefficient to retrieve articles similar 
to the seed article. Gather all documents that 
have coefficient more than 0.5. 
  Select article sets that have more than 3 articles. 
About 300 such sets are obtained and among 
them, we selected 100 document sets, preferring 
more documents in a set and avoiding overlap-
ping topics. 
The average number of articles in a document set 
was 4.7 and the average number of sentences in a 
document was 12.9. Annotators read the articles in each 
set and detected if there were articles that are different 
from the topic throughout the document set. Such arti-
cles, which turned out to be very few in number, were 
excluded in the following experiments. 
3 Task and annotator 
We have four tasks and three annotators (indicated by a 
number). Annotator 1 and 2 did the same task, but an-
notator 3 did only a part of it. All of them have college 
degrees, in particular annotators 1 and 2 are Japanese 
native speakers and have majors in linguistics at US 
universities. 
Some examples (free summaries for one document 
set, and axes and table data for three sets, all translated 
into English) are shown in the appendix. 
 
Annotator  
Task 1 2 3 
Free style summary 100 40  
Sent. Extraction 100 20 100 
Axis 100 100 100 
Table summary 100 100  
 
Table 1. Task and annotator 
4 Free style summarization 
The first task is a free style summarization. The inter-
annotator agreement based on the word vector metric 
adopted by TSC evaluation (TSC homepage) is calcu-
lated. This is a cosine metric of tf*idf measure of the 
words in the summaries. Most of the pairs (37 sets out 
of 40 sets) had values of 0.5 or more, which is much 
larger than that of automatic systems measured against 
the human made summaries in TSC-1 (ranging around 
0.4 in 10% summary, 0.5 in 40% summary). We can 
reasonably believe the summaries are very reliable. 
5 Sentence Extraction 
Now we will look at the summarization by sentence 
extraction. Annotators 1 and 3 conducted the task for 
the entire data, so we will compare the results of those 
two. We asked the annotators to extract about 20% of 
the sentences as a summary of each document set, but 
the actual numbers of extracted sentences are slightly 
different between the two. Table 2 shows the number of 
sentences selected by the two annotators with inter-
annotator agreement data. The number of sentences 
selected by both annotators (533) looks low, compared 
to the number of sentences selected by only one 
annotator (650 and 746). However, the chi-square test 
is 513.9, which means that these two results are 
strongly correlated (less than 0.0001%  chance). 
 
Annotator 1  
annotator 3 selected not selected 
 
Total 
selected 533 746 1279 
not selected 650 4050 4700 
Total 1183 4796 5979 
 
Table 2. Number of selected sentences 
6 Axis 
Axis is based on the idea of (McKeown et al 2001). 
They defined 4 categories of document sets based on 
the main topic of the document set for the purpose of 
using different summarization strategies (they actually 
used two sub-systems), shown in Table 3.  
 
Category and Description 
Single-Event (2) 
The documents center around one single event at one 
place and at roughly the same time, involving the 
same agents and actions 
Person-centered (10) 
The documents deal with one event concerning one 
person 
Multi-Event (7) 
Several events occurring at different places and times 
and usually with different protagonists, are reported 
together 
Other (11) 
Document sets contain even more loosely related 
documents 
 
Table 3. McKeown?s categories 
The number in brackets for each category indicates 
the number of document sets in the DUC 2001 training 
data. As can be seen, the number of ?person centered? 
sets is quite high. We believe this is due to the pre-
filtering in the DUC data. ?Other? is also high, which 
means more categories may be needed.  
We created new categories based on our study of 
document sets (other than the 100 sets reported here). 
We defined 13 categories, shown in Table 4, for what 
we will call the axis of the document set.  
 
Single-person Multi-Person 
Single-location Multi-location 
Single-organization Multi-organization 
Single-facility Multi-facility 
Single-product Multi-product 
Single-event Multi-event 
Others  
 
Table 4. 13 Axes 
 
The axis is a combination of two types of informa-
tion; single or multi, and 6 kinds of named entities 
(person, location, organization, facility, product and 
event). ?Single? means that all the articles are talking 
about a single event, person or other entity, whereas 
?Multi? articles are talking about multiple entities that 
might participate in similar types of events. We used 6 
categories of entity types, which are the major catego-
ries defined in the MUC (Grishman and Sundheim 
1996) or ACE project (ACE homepage). For example, 
if a document set is talking about Einstein?s biography, 
it should be tagged as ?single-person?, and if a set is 
talking about earthquakes in California last year, it 
should be tagged as ?multi-event?. 
In order to demonstrate the validity of the catego-
ries, we tried to categorize the training data of DUC 
2001?s multi-document sets into our categories. Two 
people assigned one or two categories to each set. We 
allow more than one axis to a document set, as some 
document sets should be inherently categorized into 
more than one axis. If we consider only the first 
choices, the inter annotator agreement ratio is 80% and 
if we include the second choices, the ratio is 93.3%. 
We believe the categorization is practical. Table 5 
shows the distribution of axis categories tagged on our 
100 data sets by three annotators. Note that annotators 
1 and 2 assigned more than one axis to some data sets, 
so the totals exceed 100. 
All the categories except multi-facility are used by 
at least two annotators. Because the axis ?other? is used 
rarely, the set of axes are empirically found to have 
quite good coverage. 
The inter-annotator agreements are 55, 61 and 67 % 
among the three annotators. Although the ratios are 
lower than that on the DUC data (we believe this is 
because of the pre-filtering of document sets), the 
agreement is still high at 55-67% even though there are 
13 kinds of axis. Note that chi-square test is not suit-
able to measure the data because of the data sparseness. 
 
Annotator  
Axis 1 2 3 
s-event 36 32 37 
s-facility 3 1 0 
s-location 7 2 4 
s-organization 6 12 11 
s-person 12 13 12 
s-product 14 14 8 
m-event 15 30 8 
m-facility 0 0 0 
m-location 2 3 2 
m-organization 9 12 13 
m-person 2 7 1 
m-product 2 2 0 
Other 1 0 3 
 
Table 5. Distribution of axis 
 
There are 39 document sets that have the same axis 
assigned by the three annotators, and there are only 7 
document sets that have three different axes by three 
annotators (no overlap at all). Even when different 
categories are tagged, sometimes all of them are under-
standable and we can say that these are all correct. So 
for some document sets, more than one category is in-
stinctively correct. This result indicates that, for some 
large percentage of document sets, it is possible to as-
sign axis(es). We believe for summarizing those docu-
ment sets, knowing the axis before summarization 
could be quite helpful. We are seeking a method to 
automate the process of finding the axis(es). 
7 Table 
A table is a good way to summarize a document set 
talking about multiple events of the same type, a collec-
tion of similar events or chronological events. We 
asked annotators to make a table for each document set. 
Table 6 shows some statistics of the created tables. The 
average number of columns is 3.47 and 5.25 for anno-
tator 1 and annotator 2, respectively. Regarding com-
parison between tables, the percentages of complete 
overlap (relationship of columns is 1 to 1 and the same 
information is collected in the column) are 58% and 
38%. The percentages of overlap (relationship of col-
umns is not 1 to 1, but the information of the columns 
is overlapping between the tables) are 94% and 70%. 
We can see that annotator 1 made fewer columns than 
annotator 2, and most columns made by annotator 1 
overlap columns made by annotator 2. So the differ-
ence is probably due to the fact that annotator 2 made 
more detailed tables. (As this is the first such survey, it 
was not easy to create good instructions) In other words, 
it might be the case that most important information 
(which was turned into columns) is simultaneously 
found by the two annotators. 
 
 
 Annotator 1 Annotator 2 
Ave. num. of column 3.47 5.25 
Complete overlap 58% 38% 
Overlap 94% 70% 
 
Table 6. Statistics of created tables 
 
When we compared the tables created by the two 
annotators one by one, we categorized the results into 5 
categories. 
A) Two tables are completely the same 
B) The information in the tables is the same, but 
the way of segmenting information into col-
umns is different. For example, one of the ta-
bles has a column ?visiting activity (of a 
diplomat)? including information about visiting 
place, person and purpose, whereas the other 
table has columns ?visiting place?, ?the person 
to meet? and ?purpose of the visit?. 
C) Missing one or two columns from either or 
both of tables (in total). This means one of the 
tables has one or two fewer columns and the in-
formation in the columns is not mentioned in the 
other table. As we can guess from Table 6, most 
of the missing columns were found in tables of 
annotator 1. 
D) Missing more than two columns from tables. 
E) The two tables are completely different in 
structure, because of the table creator?s different 
point of view. 
Table 7 shows the result of this survey.  
 
Description Num. of sets 
A) Same table 8 
B) Only segmentation 15 
C) Missing one or two column 34 
D) Missing more than two column 17 
E) Completely different table 26 
Total 100 
 
Table 7. Comparison of tables 
 
There are only a small number of document sets (8) 
from which the annotators made completely the same 
table. However, for more than half the document sets, 
the tables created by the two annotators are quite simi-
lar (including ?same table?, ?only segmentation? and 
?missing one or two columns?). This is complementary 
to the result shown in Table 6; for many document sets, 
the tables by annotator 2 have additional information 
compared to the tables by annotator 1. 
We also asked the annotators to judge if each docu-
ment set is suitable to summarize into a table. We made 
three categories for the survey. 
A) Table is natural for summarizing the document 
set 
B) Information can be summarized in table format 
C) Table is not suitable to summarize the docu-
ment set 
The result for the two annotators is shown in Table 
8. Annotators 1 and 2 judged 40 and 45 sets to be suit-
able for a table, 36 and 38 are OK and 24 and 17 are 
not suitable. This is an interesting result - that for so 
many document sets (40-45%) a table is judged to be 
natural for summarizing. Compared to that, only a 
smaller fraction (17-24%) are judged unsuitable. The 
relationships between the two annotators? judgments 
are also shown in Table 8. The Chi-test is 17.94 and the 
probability is 0.13%; that means that the two judges are 
highly correlated. 
 
Annotator 1  
Annotator 2 A B C 
 
total 
A 28 12 5 45 
B 9 16 13 38 
C 3 8 6 17 
total 40 36 24 100 
 
Table 8. Suitability of table 
 
8 Discussion 
We reported a survey for multi-document summariza-
tion. We believe the results are encouraging for the 
pursuit of some novel strategies of multi-document 
summarization.  
One of them is the notion of axis. As we observed 
that for some percentage of the document sets, the axis 
can be tagged with some certainty, we might be able to 
make an automatic system to find it. Once the axis is 
correctly found, it might be useful for multi document 
summarization. For example, if a set is ?single-person? 
then the summary for the set should be centered on the 
person. This may suggest, for example, generating a 
summary of type ?biography? (Mani 2001). If a docu-
ment set is found to be ?multi-event?, then the sum-
mary should focus on the differences of the events.  
The other result found in the experiment is that a 
quite large percentage of document sets can be summa-
rized in table format. As this is a preliminary experi-
ment, there is incompleteness in the instruction and we 
believe further study on this topic is necessary. In addi-
tion to setting guidelines for the degree of detail, the 
style of cell contents shall be more uniform. Currently, 
cells contain words, phrases and sentences. We believe 
that by making more careful instructions for annotation, 
the comparison between different tables can be more 
systematized. In other words, a systematic evaluation 
may be possible. 
9 Future Work 
Obviously, the future work suggested by these results 
includes automatic methods to find what the human 
found in this experiment. 
We have started finding the axis automatically by 
observing the distribution of named entities, words and 
phrases. 
Once we are able to find the axis and suitability of 
table summary automatically for a given document set, 
the next stage of research will involve using this infor-
mation to select an appropriate way to summarize the 
set, i.e. table summary, sentence extraction summary or 
summary including rewriting. This will be extended if 
the document set is created dynamically from a user?s 
query. The type of query could be a helpful clue in se-
lecting the way to summarize the retrieved document 
set. 
The technology to summarize a document set in ta-
ble format is studied in Information Extraction. How-
ever, it has a hard limitation that the topic of the 
document set has to be known in advance and the 
knowledge to build table has to be created by hand, 
which usually takes a long time. There have been ef-
forts to automate the knowledge creation (Riloff 1996) 
(Yangarber 2000) (Sudo 2001); we hope to make a 
bridge between such automatic IE knowledge discovery 
and automatic summarization efforts. 
 
10 Acknowledgements 
This research is supported by the Defense Advanced 
Research Projects Agency as part of the Translingual 
Information Detection, Extraction and Summarization 
(TIDES) program, under Grant N66001-001-1-8917 
from the Space and Naval Warfare Systems Center, 
San Diego, and by the National Science Foundation 
under Grant IIS-0081962. This paper does not neces-
sarily reflect the position of the U.S. Government. We 
would like to thank our colleagues at New York Uni-
versity, who provided useful suggestions and 
discussions, including, Prof. Ralph Grishman, Mr. 
Kiyoshi Sudo and Mr. Yusuke Shinyama. Also, we 
thank the three annotators to do the tedious job. 
References 
(ACE-homepage)  
http://www.nist.gov/speech/tests/ace/index.htm 
(DUC-homepage)                
http://www-nlpir.nist.gov/projects/duc/ 
(TSC-homepage)  http://lr-www.pi.titech.ac.jp/tsc/ 
 (McKeown et al 2001) K.R. McKeown, R. Barzilay, 
D. Evans, V. Hatzivassiloglou, M. Yen Kan, B. 
Schiffman, S. Teufel, ?Columbia Multi-Document 
Summarization: Approach and Evaluation?, Pro-
ceedings of the Document Understanding Confer-
ence (DUC-2001), 2001  
(Grishman and Sundheim 1996)  R. Grishman, B. 
Sundheim, ?Message Understanding Conference - 6: 
A Brief History?, Proceedings of the 16th Interna-
tional Conference on Computational Linguistics 
(COLING ?96), 1996  
(Mani 2001) I. Mani, ?Automatic Summarization?, 
John Benjamins Publishing Company, 2001 
(Riloff 1996) Ellen Riloff, ?Automatically Generating 
Extraction Patterns from Untagged Text?, Proceed-
ings of the 13th National Conference on Artificial 
Intelligence, 1996 
(Sudo et al 2001) Kiyoshi Sudo, Satoshi Sekine and 
Ralph Grishman, ?Automatic Pattern Acquisition for 
Japanese Information Extraction?, Procedings of 
Human Language Technologies (HLT ?01), 2001 
(Yangarber 2000) Roman Yangarber, Ralph Grishman, 
Pasi Tapanainen and Silja Huttunen: ?Automatic 
Acquisition of Domain Knowledge for Information 
Extraction?, Proceedings of the 18th International 
Conference on Computational Linguistics (COLING 
2000), 2000 
 
 
 
 
 
Appendix 
 
Original data is all in Japanese. The example data here is English translation. 
 
Sample Data (Article Set #mai9899.189565) 
<Axis> Annotator 1: s-event, s-person,     Annotator 2: s-person, s-event,    Annotator 3: s-person 
 
<Summary by annotator 1> 
Mr. Ikuo Kashima, 69, a yacht sailor in Hannnann-shi, Osaka-fu, departed a yacht harbor in Misaki-machi, Osaka, 
on September 15, with his yacht ?Koraasa 70? to achieve a solo sailing around the world without calling at any port 
as a world?s oldest challenger.  The original plan was to sail across the Pacific, the Atlantic, and the Indian Ocean in 
about 330 days, but he called at Honolulu, Hawaii, on October 26 due to the breakdown of fresh-water-generator.  
He repeatedly departed from and returned to Honolulu because of the engine and generator trouble on November 16, 
the leak and the breakdown of the helm in December, and he gave up on his plan after the fourth trial on January 2.  
In July 99, he left Misaki-machi to make another attempt to sail around the world, and the news of passing through 
the Cape Horn, the south most point in South America came on December 30.  He is expected to arrive at the harbor 
in Misaki-machi next July if the voyage goes smoothly. 
 
<Summary by annotator 2> 
To achieve a solo sailing around the world without calling at any port by the world?s oldest challenger, Mr. Ikuo 
Kashima left Tannowa Yacht Harbor in Misaki-machi, Osaka-fu, on September 15, 1998.  Around noon on October 
26, however, he had to call at Honolulu Harbor on Oahu Island, Hawaii, to repair the fresh-water-generator in his 
yacht ?Koraasa 70,? which he was using for this voyage.  Although he changed his plan and made Hawaii his start-
ing point for his around-the-world solo sailing and started sailing again on October 31, he had to return to Honolulu 
again on November 16, due to the breakdown of the engine and the generator.  Another trial started on December 4, 
again, did not succeed.  Mr. Kashima made his fourth trial on January 2, but again, he had to return to Honolulu on 
January 11 for the leak in the yacht and the trouble in the helm this time.  Later, Mr. Kashima, aiming at a solo sail-
ing around the world again, left Tannowa Yacht Harbor on July 20.  On December 30, he succeeded in passing 
through the most difficult navigation point, Cape Horn.  He is planed to be back to Misaki-machi, Osaka, in July 
2000. 
 
<Table summary by annotator 1>  Suitability for table: Suitable 
DATE PLACE EVENT PURPOSE 
9/15/98 Misaki-machi, Osaka Departure A non-stop solo sailing around the world 
10/26 Honolulu, Hawaii Calling To repair fresh-water-generator 
10/31 Honolulu, Hawaii Departure Second trial to sail around the world 
11/16 Honolulu, Hawaii Calling Troubles in the engine and the generator 
12/4 Honolulu, Hawaii Departure Third trial to sail around the world 
? Honolulu, Hawaii Calling Leak and the breakdown of helm 
1/2/99 Honolulu, Hawaii Departure Fourth trial to sail around the world 
7/20 Misaki-machi, Osaka Departure Another trial for the sailing around the world 
12/30 Cape Horn, the south most point in 
South America 
Passing  
 
<Table summary by annotator 2>  Suitability for table: Suitable 
DATE PLACE ACTION CAUSE 
9/15/1998 Tannowa Yacht Harbor, Misaki-
machi, Osaka-fu 
Departure To achieve a non-stop solo sailing around the 
world as the world?s oldest challenger. 
10/26/1998 Honolulu, Oahu Island, Hawaii Calling To repair fresh-water-generator in ?Koraasa70? 
10/31/1998 Honolulu, Oahu Island, Hawaii Departure To challenge a non-stop solo sailing around the 
world starting from Hawaii. 
11/16/1998 Honolulu, Oahu Island, Hawaii Calling To repair ?Koraasa 70?s engine and generator.  
12/4/1998 Honolulu, Oahu Island, Hawaii Departure To aim at a non-stop solo sailing around the 
world starting from Hawaii. 
1/2/1999 Honolulu, Oahu Island, Hawaii Departure To aim at a non-stop solo sailing around the 
world starting from Hawaii. 
1/11/1999 Honolulu, Oahu Island, Hawaii Calling For the leak and the troubles in the helm of 
?Koraasa 70.? 
7/20/1999 Tannowa Yacht Harbor, 
Misaki-machi, Osaka-fu 
Departure To achieve a non-stop solo sailing as a world?s 
oldest challenger 
12/30/1999 Cape Horn Passing  
July 2000 Tannowa Yacht Harbor, Misaki-
machi, Osaka-fu 
Expected to 
arrive 
Arrival from the non-stop solo sailing around 
the world as an oldest challenger. 
 
Sample Data (Article Set #mai9899.106495) 
<Axis> Annotator 1: s-person,     Annotator 2: s-person,    Annotator 3: s-person 
 
<Table summary by annotator 1>   Suitability for table: Not suitable 
DATE COHEN, DEFENSE SECRETARY 
3/27/98 Meeting with Mordechai, Israeli Minister of Defense. Promised to cooperate in Israel?s missile defense 
development. 
11/6/98 Visited Middle Eastern countries. Asking Iraq to withdraw its suspension on cooperating UNISCOM 
1/11/99 Visited Japan. Discussed the matters on US-Japan security guideline-related bills. 
2/22/99 Planned to visit Japan and China in April. (The plan was postponed due to the prolonged air raid in Yugo-
slavia.) 
7/9/99 Planned to visit Japan and South Korea in late-July. 
 
<Table summary by annotator 2>  Suitability for table: Suitable 
DATE PLACE MEETING WITH PURPOSE TIME OF 
VISIT 
3/27/98 Israel Mordechai, Israeli 
Minister of Defense 
To cooperate in Israel?s missile defense devel-
opment 
3/98 
11/6/98 Middle East  To discuss the issues on Iraq 11/98 
1/7/99 Japan Komura, Japanese For-
eign Minister 
Noroda, Japanese 
Chief of Defense 
To discuss the situation in Korean Peninsula and 
an outlook on passing US-Japan security guide-
line-related bill in Japan?s Diet 
1/11-
1/14/98 
1/7/99 South Korea   1/99 
2/2/99 Japan  To explain about the US-China defense coopera-
tion and to discuss the Guideline-related bill and 
its approval in the Diet. 
4/99 
2/2/99 China   4/99 
7/9/99 Japan Obuchi, Japanese 
Prime Minister 
To discuss the issues on North Korea?s ballistic 
missile and on relocation of Futemma Airport in 
Okinawa. 
Late July 
7/9/99 South Korea   Late July 
 
Sample Data (Article Set #mai9899.141141) 
<Axis> Annotator 1: s-location,    Annotator 2: m-event,    Annotator 3: s-product 
 
<Table summary by annotator 1>  Suitability for table: Suitable 
DATE EVENT KOSOVO LIBERATION 
ARMY 
YUGOSLAVIA/ SERBIAN 
GOVERNMENT 
CONTACT 
GROUP 
2/6/99 Peace Talk began Claimed for Kosovo?s in-
dependence 
Disagreed to Kosovo?s inde-
pendence 
Presented a peace 
plan including 
Kosovo?s auton-
omy 
2/10 Conflict in Peace talk Demanded Serbs to sign 
on agreement for immedi-
ate cease-fire 
Requested Kosovo to sign the 
peace plan 
Aimed at reaching 
an agreement 
2/19 A day before the ne-
gotiation deadline 
 Disagreed to NATO?s pres-
ence in Kosovo 
 
2/20 Agreement deadline. 
Talk continued. 
 Rejected NATO peace force?s 
presence in Kosovo 
Foreign Ministers 
arrived 
2/23 Final day for the 
Peace Talk 
Rejected to sign the peace 
plan.  Requested the vote 
for Kosovo independence. 
Rejected to dissolve Kos-
ovo Liberation Army. 
Rejected to sign the peace 
plan, and refused to have 
NATO?s peace force. 
Acknowledged the 
temporary agree-
ment on Kosovo?s 
autonomy. 
3/15 Second Peace Talk 
began 
   
3/16 Second day of Peace 
Talk 
Expressed the willingness 
to sign the peace plan 
Presented the disagreed items 
in peace plan in writing 
Rejected to change 
the peace plan 
3/18  Signed the peace plan  Requested Yugo-
slavia to accept the 
plan by 24th. 
 
<Table summary by annotator 2>  Suitability for table: can-be 
DATE NO. PERSON ACTION 
2/6/99 1 Representative of ?Kosovo Liberation Army? Arrived in Paris 
2/10/99 1 Yugoslavian government representative and 
groups of Albanian residents 
Each group handed in its own requirements and 
caused conflicts 
2/19/99 1 Cook, Foreign Minister, U.K. Arrived in Paris 
2/20/99 1 Albright, Secretary of State, U.S.A 
Fischer, Foreign Minister, Germany 
Arrived in Paris 
3/1/99 2 Albanian residents Expressed their willingness to sign the peace 
plan 
3/16/99 2 Contact Group of US, Europe, and Russia Pursued Serbian Republic representatives to 
accept the peace plan 
3/16/99 2 Representatives of Serbian Republic Presented objection to the peace plan in writing 
3/16/99 2 Hill, Special US envoy Rejected the big change in the peace plan 
3/18/99 2 Representatives of Serbian Republic Announced to sign the peace plan 
3/18/99 2 Contact Group of US, Europe, and Russia Requested Yugoslavia to accept the final peace 
plan by March 24. 
 
Evaluation of Features for Sentence Extraction
on Different Types of Corpora
Chikashi Nobata?, Satoshi Sekine? and Hitoshi Isahara?
? Communications Research Laboratory
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan
{nova, isahara}@crl.go.jp
? Computer Science Department, New York University
715 Broadway, 7th floor, New York, NY 10003, USA
sekine@cs.nyu.edu
Abstract
We report evaluation results for our sum-
marization system and analyze the result-
ing summarization data for three differ-
ent types of corpora. To develop a ro-
bust summarization system, we have cre-
ated a system based on sentence extraction
and applied it to summarize Japanese and
English newspaper articles, obtained some
of the top results at two evaluation work-
shops. We have also created sentence ex-
traction data from Japanese lectures and
evaluated our system with these data. In
addition to the evaluation results, we an-
alyze the relationships between key sen-
tences and the features used in sentence
extraction. We find that discrete combi-
nations of features match distributions of
key sentences better than sequential com-
binations.
1 Introduction
Our ultimate goal is to create a robust summariza-
tion system that can handle different types of docu-
ments in a uniform way. To achieve this goal, we
have developed a summarization system based on
sentence extraction. We have participated in eval-
uation workshops on automatic summarization for
both Japanese and English written corpora. We have
also evaluated the performance of the sentence ex-
traction system for Japanese lectures. At both work-
shops we obtained some of the top results, and for
the speech corpus we obtained results comparable
with those for the written corpora. This means that
the features we use are worth analyzing.
Sentence extraction is one of the main methods
required for a summarization system to reduce the
size of a document. Edmundson (1969) proposed a
method of integrating several features, such as the
positions of sentences and the frequencies of words
in an article, in order to extract sentences. He man-
ually assigned parameter values to integrate features
for estimating the significance scores of sentences.
On the other hand, machine learning methods can
also be applied to integrate features. For sentence
extraction from training data, Kupiec et al (1995)
and Aone et al (1998) used Bayes? rule, Lin (1999)
and Nomoto and Matsumoto (1997) generated a de-
cision tree, and Hirao et al (2002) generated an
SVM.
In this paper, we not only show evaluation results
for our sentence extraction system using combina-
tions of features but also analyze the features for dif-
ferent types of corpora. The analysis gives us some
indication about how to use these features and how
to combine them.
2 Summarization data
The summarization data we used for this research
were prepared from Japanese newspaper articles,
Japanese lectures, and English newspaper articles.
By using these three types of data, we could com-
pare two languages and also two different types of
corpora, a written corpus and a speech corpus.
2.1 Summarization data from Japanese
newspaper articles
Text Summarization Challenge (TSC) is an evalua-
tion workshop for automatic summarization, which
is run by the National Institute of Informatics in
Japan (TSC, 2001). Three tasks were presented at
TSC-2001: extracting important sentences, creating
summaries to be compared with summaries prepared
by humans, and creating summaries for informa-
tion retrieval. We focus on the first task here, i.e.,
the sentence extraction task. At TSC-2001, a dry
run and a formal run were performed. The dry run
data consisted of 30 newspaper articles and manu-
ally created summaries of each. The formal run data
consisted of another 30 pairs of articles and sum-
maries. The average number of sentences per article
was 28.5 (1709 sentences / 60 articles). The news-
paper articles included 15 editorials and 15 news re-
ports in both data sets. The summaries were created
from extracted sentences with three compression ra-
tios (10%, 30%, and 50%). In our analysis, we used
the extraction data for the 10% compression ratio.
In the following sections, we call these summa-
rization data the ?TSC data?. We use the TSC data
as an example of a Japanese written corpus to eval-
uate the performance of sentence extraction.
2.2 Summarization data from Japanese
lectures
The speech corpus we used for this experiment
is part of the Corpus of Spontaneous Japanese
(CSJ) (Maekawa et al, 2000), which is being cre-
ated by NIJLA, TITech, and CRL as an ongoing
joint project. The CSJ is a large collection of mono-
logues, such as lectures, and it includes transcrip-
tions of each speech as well as the voice data. We
selected 60 transcriptions from the CSJ for both sen-
tence segmentation and sentence extraction. Since
these transcription data do not have sentence bound-
aries, sentence segmentation is necessary before
sentence extraction. Three annotators manually gen-
erated sentence segmentation and summarization re-
sults. The target compression ratio was set to 10%.
The results of sentence segmentation were unified
to form the key data, and the average number of
sentences was 68.7 (4123 sentences / 60 speeches).
The results of sentence extraction, however, were
not unified, but were used separately for evaluation.
In the following sections, we call these summa-
rization data the ?CSJ data?. We use the CSJ data as
an example of a Japanese speech corpus to evaluate
the performance of sentence extraction.
2.3 Summarization data from English
newspaper articles
Document Understanding Conference (DUC) is an
evaluation workshop in the U.S. for automatic sum-
marization, which is sponsored by TIDES of the
DARPA program and run by NIST (DUC, 2001).
At DUC-2001, there were two types of tasks:
single-document summarization (SDS) and multi-
document summarization (MDS). The organizers of
DUC-2001 provided 30 sets of documents for a dry
run and another 30 sets for a formal run. These data
were shared by both the SDS and MDS tasks, and
the average number of sentences was 42.5 (25779
sentences / 607 articles). Each document set had a
topic, such as ?Hurricane Andrew? or ?Police Mis-
conduct?, and contained around 10 documents rele-
vant to the topic. We focus on the SDS task here, for
which the size of each summary output was set to
100 words. Model summaries for the articles were
also created by hand and provided. Since these sum-
maries were abstracts, we created sentence extrac-
tion data from the abstracts by word-based compar-
ison.
In the following sections, we call these summa-
rization data the ?DUC data?. We use the DUC data
as an example of an English written corpus to eval-
uate the performance of sentence extraction.
3 Overview of our sentence extraction
system
In this section, we give an overview of our sentence
extraction system, which uses multiple components.
For each sentence, each component outputs a score.
The system then combines these independent scores
by interpolation. Some components have more than
one scoring function, using various features. The
weights and function types used are decided by op-
timizing the performance of the system on training
data.
Our system includes parts that are either common
to the TSC, CSJ, and DUC data or specific to one of
these data sets. We stipulate which parts are specific.
3.1 Features for sentence extraction
3.1.1 Sentence position
We implemented three functions for sentence po-
sition. The first function returns 1 if the position of
the sentence is within a given threshold N from the
beginning, and returns 0 otherwise:
P1. Scorepst(Si)(1 ? i ? n) = 1(if i < N)
= 0(otherwise)
The threshold N is determined by the number of
words in the summary.
The second function is the reciprocal of the po-
sition of the sentence, i.e., the score is highest for
the first sentence, gradually decreases, and goes to a
minimum at the final sentence:
P2. Scorepst(Si) =
1
i
These first two functions are based on the hypoth-
esis that the sentences at the beginning of an article
are more important than those in the remaining part.
The third function is the maximum of the recipro-
cal of the position from either the beginning or the
end of the document:
P3. Scorepst(Si) = max(
1
i ,
1
n? i+ 1)
This method is based on the hypothesis that the sen-
tences at both the beginning and the end of an article
are more important than those in the middle.
3.1.2 Sentence length
The second type of scoring function uses sen-
tence length to determine the significance of sen-
tences. We implemented three scoring functions for
sentence length. The first function only returns the
length of each sentence (Li):
L1. Scorelen(Si) = Li
The second function sets the score to a negative
value as a penalty when the sentence is shorter than
a certain length (C):
L2. Scorelen(Si) = 0 (if Li ? C)
Li ? C (otherwise)
The third function combines the above two ap-
proaches, i.e., it returns the length of a sentence that
has at least a certain length, and otherwise returns a
negative value as a penalty:
L3. Scorelen(Si) = Li (if Li ? C)= Li ? C (otherwise)
The length of a sentence means the number of let-
ters, and based on the results of an experiment with
the training data, we set C to 20 for the TSC and
CSJ data. For the DUC data, the length of a sen-
tence means the number of words, and we set C to
10 during the training stage.
3.1.3 Tf*idf
The third type of scoring function is based on term
frequency (tf) and document frequency (df). We ap-
plied three scoring functions for tf*idf, in which the
term frequencies are calculated differently. The first
function uses the raw term frequencies, while the
other two are two different ways of normalizing the
frequencies, as follows, where DN is the number of
documents given:
T1. tf*idf(w) = tf(w) log DNdf(w)
T2. tf*idf(w) = tf(w)-1
tf(w) log
DN
df(w)
T3. tf*idf(w) = tf(w)
tf(w)+1 log
DN
df(w)
For the TSC and CSJ data, we only used the third
method (T3), which was reported to be effective
for the task of information retrieval (Robertson and
Walker, 1994). The target words for these functions
are nouns (excluding temporal or adverbial nouns).
For each of the nouns in a sentence, the system cal-
culates a Tf*idf score. The total score is the sig-
nificance of the sentence. The word segmentation
was generated by Juman3.61 (Kurohashi and Nagao,
1999). We used articles from the Mainichi newspa-
per in 1994 and 1995 to count document frequen-
cies.
For the DUC data, the raw term frequency (T1)
was selected during the training stage from among
the three tf*idf definitions. A list of stop words were
used to exclude functional words, and articles from
the Wall Street Journal in 1994 and 1995 were used
to count document frequencies.
3.1.4 Headline
We used a similarity measure of the sentence to
the headline as another type of scoring function. The
basic idea is that the more words in the sentence
overlap with the words in the headline, the more im-
portant the sentence is. The function estimates the
relevance between a headline (H) and a sentence
(Si) by using the tf*idf values of the words (w) in
the headline:
Scorehl(Si) =
?
w?H?Si
tf(w)
tf(w)+1 log
DN
df(w)
?
w?H
tf(w)
tf(w)+1 log
DN
df(w)
We also evaluated another method based on this
scoring function by using only named entities (NEs)
instead of words for the TSC data and DUC data.
Only the term frequency was used for NEs, because
we judged that the document frequency for an entity
was usually quite small, thereby making the differ-
ences between entities negligible.
3.1.5 Patterns
For the DUC data, we used dependency patterns
as a type of scoring function. These patterns were
extracted by pattern discovery during information
extraction (Sudo et al, 2001). The details of this ap-
proach are not explained here, because this feature
is not among the features we analyze in Section 5.
The definition of the function appears in (Nobata et
al., 2002).
3.2 Optimal weight
Our system set weights for each scoring function in
order to calculate the total score of a sentence. The
total score (Si) is defined from the scoring functions
(Scorej()) and weights (?j) as follows:
TotalScore(Si) =
?
j
?jScorej(Si) (1)
We estimated the optimal values of these weights
from the training data. After the range of each
weight was set manually, the system changed the
values of the weights within a range and summarized
the training data for each set of weights. Each score
was recorded after the weights were changed, and
the weights with the best scores were stored.
A particular scoring method was also selected in
the cases of features with more than one defined
scoring methods. We used the dry run data from
each workshop as TSC and DUC training data. For
the TSC data, since the 30 articles contained 15 ed-
itorials and 15 news reports, we estimated optimal
values separately for editorials and news reports. For
the CSJ data, we used 50 transcriptions for training
and 10 for testing, as mentioned in Section 2.2.
Table 1: Evaluation results for the TSC data.
Ratio 10% 30% 50% Avg.
System 0.363 (1) 0.435 (5) 0.589 (2) 0.463 (2)
Lead 0.284 0.432 0.586 0.434
4 Evaluation results
In this section, we show our evaluation results on the
three sets of data for the sentence extraction system
described in the previous section.
4.1 Evaluation results for the TSC data
Table 1 shows the evaluation results for our sys-
tem and some baseline systems on the task of sen-
tence extraction at TSC-2001. The figures in Ta-
ble 1 are values of the F-measure1. The ?System?
column shows the performance of our system and its
rank among the nine systems that were applied to the
task, and the ?Lead? column shows the performance
of a baseline system which extracts as many sen-
tences as the threshold from the beginning of a doc-
ument. Since all participants could output as many
sentences as the allowed upper limit, the values of
the recall, precision, and F-measure were the same.
Our system obtained better results than the baseline
systems, especially when the compression ratio was
10%. The average performance was second among
the nine systems.
4.2 Evaluation results for the DUC data
Table 2 shows the results of a subjective evalua-
tion in the SDS task at DUC-2001. In this subjec-
tive evaluation, assessors gave a score to each sys-
tem?s outputs, on a zero-to-four scale (where four is
the best), as compared with summaries made by hu-
mans. The figures shown are the average scores over
all documents. The ?System? column shows the per-
formance of our system and its rank among the 12
systems that were applied to this task. The ?Lead?
1The definitions of each measurement are as follows:
Recall (REC) = COR / GLD
Precision (PRE) = COR / SYS
F-measure = 2 * REC * PRE / (REC + PRE),
where COR is the number of correct sentences marked by the
system, GLD is the total number of correct sentences marked
by humans, and SYS is the total number of sentences marked by
the system. After calculating these scores for each transcription,
the average is calculated as the final score.
Table 2: Evaluation results for the DUC data (sub-
jective evaluation).
System Lead Avg.
Grammaticality 3.711 (5) 3.236 3.580
Cohesion 3.054 (1) 2.926 2.676
Organization 3.215 (1) 3.081 2.870
Total 9.980 (1) 9.243 9.126
Table 3: Evaluation results for the CSJ data.
Annotators
A B C Avg.
REC 0.407 0.331 0.354 0.364
PRE 0.416 0.397 0.322 0.378
F 0.411 0.359 0.334 0.368
column shows the performance of a baseline system
that always outputs the first 100 words of a given
document, while the ?Avg.? column shows the aver-
age for all systems. Our system ranked 5th in gram-
maticality and was ranked at the top for the other
measurements, including the total value.
4.3 Evaluation results for the CSJ data
The evaluation results for sentence extraction with
the CSJ data are shown in Table 3. We compared the
system?s results with each annotator?s key data. As
mentioned previously, we used 50 transcriptions for
training and 10 for testing.
These results are comparable with the perfor-
mance on sentence segmentation for written doc-
uments, because the system?s performance for the
TSC data was 0.363 when the compression ratio was
set to 10%. The results of our experiments thus show
that for transcriptions, sentence extraction achieves
results comparable to those for written documents,
if the are well defined.
4.4 Contributions of features
Table 4 shows the contribution vectors for each set
of training data. The contribution here means the
product of the optimized weight and the standard
deviation of the score for the test data. The vec-
tors were normalized so that the sum of the com-
ponents is equal to 1, and the selected function types
for the features are also shown in the table. Our sys-
tem used the NE-based headline function (HL (N))
for the DUC data and the word-based function (HL
Table 4: Contribution (weight? s.d.) of each feature
for each set of summarization data.
TSC
Features Editorial Report DUC CSJ
Pst. P3. 0.446 P1. 0.254 P1. 0.691 P3. 0.055
Len. L3. 0.000 L3. 0.000 L2. 0.020 L2. 0.881
Tf*idf T3. 0.169 T3. 0.185 T1. 0.239 T3. 0.057
HL (W) 0.171 0.292 - 0.007
HL (N) 0.214 0.269 0.045 -
Pattern - - 0.005 -
(W)) for the CSJ data, and both functions for the
TSC data. The columns for the TSC data show the
contributions when the compression ratio was 10%.
We can see that the feature with the biggest con-
tribution varies among the data sets. While the posi-
tion feature was the most effective for the TSC and
DUC data, the length feature was dominant for the
CSJ data. Most of the short sentences in the lectures
were specific expressions, such as ?This is the result
of the experiment.? or ?Let me summarize my pre-
sentation.?. Since these sentences were not extracted
as key sentences by the annotators, it is believed that
the function giving short sentences a penalty score
matched the manual extraction results.
5 Analysis of the summarization data
In Section 4, we showed how our system, which
combines major features, has performed well as
compared with current summarization systems.
However, the evaluation results alone do not suffi-
ciently explain how such a combination of features
is effective. In this section, we investigate the corre-
lations between each pair of features. We also match
feature pairs with distributions of extracted key sen-
tences as answer summaries to find effective combi-
nation of features for sentence extraction.
5.1 Correlation between features
Table 5 shows Spearman?s rank correlation coeffi-
cients among the four features. Significantly corre-
lated feature pairs are indicated by ???(? = 0.001).
Here, the word-based feature is used as the headline
feature. We see the following tendencies for any of
the data sets:
? ?Position? is relatively independent of the other features.
? ?Length? and ?Tf*idf? have high correlation2.
Table 5: Rank correlation coefficients between fea-
tures.
TSC Report
Features Length Tf*idf Headline
Position 0.019 -0.095 -0.139
Length ? 0.546? 0.338?
Tf*idf ? ? 0.696?
TSC Editorial
Features Length Tf*idf Headline
Position -0.047 -0.099 0.046
Length ? 0.532? 0.289?
Tf*idf ? ? 0.658?
DUC Data
Features Length Tf*idf Headline
Position -0.130? -0.108? -0.134?
Length ? 0.471? 0.293?
Tf*idf ? ? 0.526?
CSJ Data
Features Length Tf*idf Headline
Position -0.092? -0.069? -0.106?
Length ? 0.460? 0.224?
Tf*idf ? ? 0.533?
? ?TF*idf? and ?Headline ? also have high correlation.
These results show that while combinations of these
four features enabled us to obtain good evaluation
results, as shown in Section 4, the features are not
necessarily independent of one another.
5.2 Combination of features
Tables 6 and 7 show the distributions of extracted
key sentences as answer summaries with two pairs
of features: sentence position and the tf*idf value,
and sentence position and the headline information.
In these tables, each sentence is ranked by each of
the two feature values, and the rankings are split ev-
ery 10 percent. For example, if a sentence is ranked
in the first 10 percent by sentence position and the
last 10 percent by the tf*idf feature, the sentence be-
longs to the cell with a position rank of 0.1 and a
tf*idf rank of 1.0 in Table 6.
Each cell thus has two letters. The left letter is the
number of key sentences, and the right letter is the
ratio of key sentences to all sentences in the cell. The
left letter shows how the number of sentences differs
from the average when all the key sentences appear
equally, regardless of the feature values. Let T be
2Here we used equation T1 for the tf*idf feature, and the
score of each sentence was normalized with the sentence length.
Hence, the high correlation between ?Length? and ?Tf*idf? is
not trivial.
the total number of key sentences, M(= T100) be the
average number of key sentences in each range, and
S be the standard deviation of the number of key
sentences among all cells. The number of key sen-
tences for cell Ti,j is then categorized according to
one of the following letters:
A: Ti,j ? M + 2S
B: M + S ? Ti,j < M + 2S
C: M ? S ? Ti,j < M + S
D: M ? 2S ? Ti,j < M ? S
E: Ti,j < M ? 2S
O: Ti,j = 0
-: No sentences exist in the cell.
Similarly, the right letter in a cell shows how the ra-
tio of key sentences differs from the average ratio
when all the key sentences appear equally, regard-
less of feature values. Let N be the total number
of sentences, m(= TN ) be the average ratio of key
sentences, and s be the standard deviation of the ra-
tio among all cells. The ratio of key sentences for
cell ti,j is then categorized according to one of the
following letters:
a: ti,j ? m+ 2s
b: m+ s ? ti,j < m+ 2s
c: m? s ? ti,j < m+ s
d: m? 2s ? ti,j < m? s
e: ti,j < m? 2s
o: ti,j = 0
-: No sentences exist in the cell.
When key sentences appear uniformly regardless of
feature values, every cell is defined as ?Cc?. We
show both the range of the number of key sentences
and the ratio of key sentences, because both are nec-
essary to show how effectively a cell has key sen-
tences. If a cell includes many sentences, the num-
ber of key sentences can be large even though the
ratio is not. On the other hand, when the ratio of key
sentences is large and the number is not, the contri-
bution to key sentence extraction is small.
Table 6 shows the distributions of key sentences
when the features of sentence position and tf*idf
were combined. For the DUC data, both the num-
ber and ratio of key sentences were large when the
sentence position was ranked within the first 20 per-
cent and the value of the tf*idf feature was ranked
in the bottom 50 percent (i.e., Pst. ? 0.2, Tf*idf ?
0.5). On the other hand, both the number and ratio
of key sentences were large for the CSJ data when
the sentence position was ranked in the last 10 per-
cent and the value of the tf*idf feature was ranked
Table 6: Distributions of key sentences based on the combination of the sentence position (Pst.) and tf*idf
features.
DUC data
Tf*idf
Pst. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
0.1 Cc Cc Cc Cb Ba Ba Aa Aa Aa Aa
0.2 Cd Cc Cc Cc Cc Cc Bb Bb Bb Bb
0.3 Cd Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.4 Dd Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.5 Dd Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.6 Dd Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.7 Dd Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.8 Cd Dd Cc Cc Cc Cc Cc Cc Cc Cc
0.9 Dd Dd Cc Cc Cc Cc Cc Cc Cc Cc
1.0 Dd Dd Cc Cc Cc Cc Cc Cc Cc Cc
CSJ data
Tf*idf
Pst. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
0.1 Cc Cc Cc Cc Bc Cc Ab Bb Bb Bb
0.2 Oo Oo Cc Cc Cc Cc Cc Bb Bc Cc
0.3 Oo Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.4 Cc Cc Cc Cc Oo Cc Cc Cc Cc Cc
0.5 Oo Cc Oo Oo Cc Oo Cc Cc Cc Cc
0.6 Cc Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.7 Oo Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.8 Oo Cc Cc Oo Oo Cc Cc Cc Cc Cc
0.9 Cc Cc Cc Cc Cc Cc Cc Cc Cc Bb
1.0 Cc Cc Ba Bb Bb Aa Aa Bb Aa Aa
after the first 30 percent (i.e., Pst. = 1.0, Tf*idf ?
0.3),. When the tf*idf feature was low, the number
and ratio of key sentences were not large, regardless
of the sentence position values. These results show
that the tf*idf feature is effective when the values
are used as a filter after the sentences are ranked by
sentence position.
Table 7 shows the distributions of key sentences
with the combination of the sentence position and
headline features. About half the sentences did not
share words with the headlines and had a value of
0 for the headline feature. As a result, the cells in
the middle of the table do not have corresponding
sentences. The headline feature cannot be used as
a filter, unlike the tf*idf feature, because many key
sentences are found when the value of the headline
feature is 0. A high value of the headline feature is,
however, a good indicator of key sentences when it
is combined with the position feature. The ratio of
key sentences was large when the headline ranking
was high and the sentence was near the beginning
(at Pst. ? 0.2, Headline ? 0.7) for the DUC data.
For the CSJ data, the ratio of key sentences was also
large when the headline ranking was within the top
10 percent (Pst. = 0.1, Headline = 1.0), as well as
for the sentences near the ends of speeches.
These results indicate that the number and ratio
of key sentences sometimes vary discretely accord-
ing to the changes in feature values when features
are combined for sentence extraction. That is, the
performance of a sentence extraction system can be
improved by categorizing feature values into sev-
eral ranges and then combining ranges. While most
sentence extraction systems use sequential combi-
nations of features, as we do in our system based
on Equation 1, the performance of these systems
can possibly be improved by introducing the cat-
egorization of feature values, without adding any
new features. We have shown that discrete combi-
nations match the distributions of key sentences in
two different corpora, the DUC data and the CSJ
data. This indicates that discrete combinations of
corpora are effective across both different languages
and different types of corpora. Hirao et al (2002)
reported the results of a sentence extraction system
using an SVM, which categorized sequential feature
values into ranges in order to make the features bi-
nary. Some effective combinations of the binary fea-
Table 7: Distributions of key sentences based on
the combination of the sentence position (Pst.) and
headline features.
DUC data
Headline
Pst. 0.1 0.2?0.5 0.6 0.7 0.8 0.9 1.0
0.1 Ab -- -- Ca Ba Ba Aa
0.2 Ac -- -- Cb Cc Ca Ca
0.3 Ac -- -- Cc Cc Cb Cb
0.4 Ac -- -- Cc Cc Cc Cb
0.5 Ac -- -- Cc Cc Cc Cc
0.6 Bc -- -- Cc Cc Cc Cc
0.7 Bc -- -- Cc Cc Cc Cc
0.8 Ac -- -- Cd Cc Cc Cc
0.9 Bd -- -- Cd Cc Cc Cc
1.0 Bd -- -- Cd Cc Cc Cc
CSJ data
Headline
Pst. 0.1 0.2?0.5 0.6 0.7 0.8 0.9 1.0
0.1 Bc -- Cc Cc Bb Cc Aa
0.2 Bc -- Cc Cb Cc Cc Bb
0.3 Cc -- Cc Cc Cc Cc Cc
0.4 Cc -- Oo Cc Cc Cc Cc
0.5 Cc -- Oo Cc Oo Cc Cc
0.6 Cc -- Cc Cc Cc Cc Cc
0.7 Cc -- Oo Cc Cc Cc Cc
0.8 Cc -- Cc Cc Cc Cc Cc
0.9 Ac -- Ca Cc Cc Cc Cb
1.0 Ab -- Ca Aa Ba Ba Ba
tures in that report also indicate the effectiveness of
discrete combinations of features.
6 Conclusion
We have shown evaluation results for our sentence
extraction system and analyzed its features for dif-
ferent types of corpora, which included corpora dif-
fering in both language (Japanese and English) and
type (newspaper articles and lectures). The sys-
tem is based on four major features, and it achieved
some of the top results at evaluation workshops in
2001 for summarizing Japanese newspaper articles
(TSC) and English newspaper articles (DUC). For
Japanese lectures, the sentence extraction system
also obtained comparable results when the sentence
boundary was given.
Our analysis of the features used in this sentence
extraction system has shown that they are not neces-
sarily independent of one another, based on the re-
sults of their rank correlation coefficients. The anal-
ysis also indicated that the categorization of feature
values matches the distribution of key sentences bet-
ter than sequential feature values.
There are several features that were not described
here but are also used in sentence extraction sys-
tems, such as some specific lexical expressions and
syntactic information. In our future work, we will
analyze and use these features to improve the per-
formance of our sentence extraction system.
References
C. Aone, M. E. Okurowski, and J. Gorlinsky. 1998. Train-
able, Scalable Summarization Using Robust NLP and Ma-
chine Learning. In Proc. of COLING-ACL?98, pages 62?66.
DUC. 2001. http://duc.nist.gov. Document Understanding
Conference.
H. Edmundson. 1969. New methods in automatic abstracting.
Journal of ACM, 16(2):264?285.
T. Hirao, H. Isozaki, E. Maeda, and Y. Matsumoto. 2002. Ex-
tracting Important Sentences with Support Vector Machines.
In Proc. of COLING-2002.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A Trainable Docu-
ment Summarizaer. In Proc. of SIGIR?95, pages 68?73.
S. Kurohashi and M. Nagao, 1999. Japanese Morphological
Analyzing System: JUMAN version 3.61. Kyoto University.
Chin-Yew Lin. 1999. Training a selection function for extrac-
tion. In Proc. of the CIKM?99.
K. Maekawa, H. Koiso, S. Furui, and H. Isahara. 2000. Spon-
taneous Speech Corpus of Japanese. In Proc. of LREC2000,
pages 947?952.
C. Nobata, S. Sekine, H. Isahara, and R. Grishman. 2002. Sum-
marization System Integrated with Named Entity Tagging
and IE pattern Discovery. In Proceedings of the LREC-2002
Conference, pages 1742?1745, May.
T. Nomoto and Y. Matsumoto. 1997. The Reliability of Human
Coding and Effects on Automatic Abstracting (in Japanese).
In IPSJ-NL 120-11, pages 71?76, July.
S. E. Robertson and S. Walker. 1994. Some simple effec-
tive approximations to the 2-poisson model for probabilistic
weighted retreival. In Proc. of SIGIR?94.
K. Sudo, S. Sekine, and R. Grishman. 2001. Automatic pattern
acquisition for japanese information extraction. In Proc. of
HLT-2001.
TSC. 2001. Proceedings of the Second NTCIR Workshop
on Research in Chinese & Japanese Text Retrieval and Text
Summarization (NTCIR2). National Institute of Informat-
ics.
Paraphrase Acquisition for Information Extraction
Yusuke Shinyama
Department of Computer Science
New York University
715, Broadway, 7th Floor, NY, 10003
yusuke@cs.nyu.edu
Satoshi Sekine
Department of Computer Science
New York University
715, Broadway, 7th Floor, NY, 10003
sekine@cs.nyu.edu
Abstract
We are trying to find paraphrases from
Japanese news articles which can be used
for Information Extraction. We focused
on the fact that a single event can be re-
ported in more than one article in differ-
ent ways. However, certain kinds of noun
phrases such as names, dates and numbers
behave as ?anchors? which are unlikely to
change across articles. Our key idea is to
identify these anchors among comparable
articles and extract portions of expressions
which share the anchors. This way we
can extract expressions which convey the
same information. Obtained paraphrases
are generalized as templates and stored for
future use.
In this paper, first we describe our ba-
sic idea of paraphrase acquisition. Our
method is divided into roughly four steps,
each of which is explained in turn. Then
we illustrate several issues which we en-
counter in real texts. To solve these prob-
lems, we introduce two techniques: coref-
erence resolution and structural restriction
of possible portions of expressions. Fi-
nally we discuss the experimental results
and conclusions.
1 Introduction
We are trying to obtain paraphrases which can be
used for Information Extraction (IE) systems. IE
systems scan articles and retrieve specific informa-
tion which is required for a certain domain defined
in advance. Currently, many IE tasks are performed
by pattern matching. For example, if the system re-
ceives a sentence ?Two more people have died in
Hong Kong from SARS,? and the system has a pat-
tern ?NUMBER people die in LOCATION? in its in-
ventory, then the system can apply the pattern to
the sentence and fill the slots, and obtain informa-
tion such as ?NUMBER = two more, LOCATION =
Hong Kong?. In most IE systems, the performance
of the system is dependent on these well-designed
patterns.
In natural language sentences, a single event can
be expressed in many different ways. So we need
to prepare patterns for various kinds of expressions
used in articles. We are interested in clustering
IE patterns which capture the same information.
For example, a pattern such as ?LOCATION reports
NUMBER deaths? can be used for the same purpose
as the previous one, since this pattern could also cap-
ture the casualties occurring in a certain location.
Prior work to relate two IE patterns was reported by
(Shinyama et al, 2002). However, in this attempt
only limited forms of expressions could be obtained.
Furthermore, the obtained paraphrases were limited
to existing IE patterns only. We are interested in col-
lecting various kinds of clues, including similar IE
patterns themselves, to connect two patterns. In this
paper, we tried to obtain more varied paraphrases.
Although our current method is intended for use in
Information Extraction, we think the same approach
can be applied to obtain paraphrases for other pur-
poses, such as machine translation or text summa-
rization.
There have been several attempts to obtain para-
phrases. (Barzilay and McKeown, 2001) applied
text alignment to parallel translations of a single
text and used a part-of-speech tagger to obtain para-
phrases. (Lin and Pantel, 2001) used mutual infor-
mation of word distribution to calculate the simi-
larity of expressions. (Pang et al, 2003) also used
text alignment and obtained a finite state automaton
which generates paraphrases. (Ravichandran and
Hovy, 2002) used pairs of questions and answers
to obtain varied patterns which give the same an-
swer. Our approach is different from these works in
that we used comparable news articles as a source of
paraphrases and used Named Entity tagging and de-
pendency analysis to extract corresponding expres-
sions.
2 Overall Procedure of Paraphrase
Acquisition
Our main goal is to obtain pattern clusters for IE,
which consist of sets of equivalent patterns captur-
ing the same information. So we tried to discover
paraphrases contained in Japanese news articles for
a specific domain. Our basic idea is to search news
articles from the same day. We focused on the fact
that various newspapers describe a single event in
different ways. So if we can discover an event
which is reported in more then one newspaper, we
can hope these articles can be used as the source of
paraphrases. For example, the following articles ap-
peared in ?Health? sections in different newspapers
on Apr. 11:
1. ?The government has announced that two more
people have died in Hong Kong after contract-
ing the SARS virus and 61 new cases of the
illness have been detected.? (Reuters, Apr. 11)
2. ?Hong Kong reported two more deaths and 61
fresh cases of SARS Friday as governments
across the world took tough steps to stop the
killer virus at their borders.? (Channel News
Asia, Apr. 11)
In these articles, we can find several correspond-
ing parts, such as ?NUMBER people have died in
LOCATION? and ?LOCATION reported NUMBER
deaths?. Although their syntactic structures are dif-
ferent, they still convey the same single fact. Here
it is worth noting that even if a different expression
is used, some noun phrases such as ?Hong Kong?
or ?two more? are preserved across the two arti-
cles. We found that these words shared by the two
sentences provide firm anchors for two different ex-
pressions. In particular, Named Entities (NEs) such
as names, locations, dates or numbers can be the
firmest anchors since they are indispensable to re-
port an event and difficult to paraphrase.
We tried to obtain paraphrases by using this prop-
erty. First we collect a set of comparable articles
which reports the same event, and pull appropriate
portions out of the sentences which share the same
anchors. If we carefully choose appropriate portions
of the sentences, the extracted expressions will con-
vey the same information; i.e. they are paraphrases.
After corresponding portions are obtained, we gen-
eralize the expressions to templates of paraphrases
which can be used in future.
Our method is divided into four steps:
1. Find comparable sentences which report the
same event from different newspapers.
2. Identify anchors in the comparable sentences.
3. Extract corresponding portions from the sen-
tences.
4. Generalize the obtained expressions to para-
phrase templates.
Figure 1 shows the overall procedure. In the re-
mainder of this section, we describe each step in
turn.
2.1 Find Comparable Sentences
To find comparable articles and sentences, we used
methods developed for Topic Detection and Track-
ing (Wayne, 1998). The actual process is divided
into two parts: article level matching and sentence
level matching. Currently we assume that a pair
of paraphrases can be found in a single sentence
of each article and corresponding expressions don?t
range across two or more sentences. Article level
matching is first required to narrow the search space
and reduce erroneous matching of anchors.
Article
A
Article
B
SARS
1
SARS
2
Find comparable
articles and 
sentences
Identify
anchors
Extract
corrsponding
portions
(Articles on
the same day)
anchors paraphrases
"NUMBER
people die
in LOCATION"
"LOCTION
reports
NUMBER
deaths"
Generalize
expressions
Figure 1: The overall procedure
Before applying this technique, we first prepro-
cessed the articles by stripping off the strings which
are not considered as sentences. Then we used a
part-of-speech tagger to obtain segmented words. In
the actual matching process we used a method de-
scribed in (Papka et al, 1999) to find a set of com-
parable articles. Then we use a simple vector space
model for sentence matching.
2.2 Identify Anchors
Before extracting paraphrases, we find anchors in
comparable sentences. We used Extended Named
Entity tagging to identify anchors. A Named Entity
tagger identifies proper expressions such as names,
locations and dates in sentences. In addition to these
expressions, an Extended Named Entity tagger iden-
tifies some common nouns such as disease names or
numbers, that are also unlikely to change (Sekine
et al, 2002). For each corresponding pair of sen-
tences, we apply the tagger and identify the same
noun phrases which appear in both sentences as an-
chors.
2.3 Extract Corresponding Sentence Portions
Now we identify appropriate boundaries of expres-
sions which share the anchors identified in the pre-
vious stage. To avoid extracting non-grammatical
expressions, we operate on syntactically structured
text rather than sequences of words. Dependency
analysis is suitable for this purpose, since using de-
pendency trees we can reconstruct grammatically
correct expressions from a spanning subtree whose
root is a predicate. Dependency analysis also allows
us to extract expressions which are subtrees but do
not correspond to a single contiguous sequence of
words.
We applied a dependency analyzer to a pair of
corresponding sentences and obtained tree structures
for each sentence. Each node of the tree is either a
predicate such as a verb or an adjective, or an argu-
ment such as a noun or a pronoun. Each predicate
can take one or more arguments. We generated all
possible combinations of subtrees from each depen-
dency tree, and compared the anchors which are in-
cluded in both subtrees. After a pair of correspond-
ing subtrees which share the anchors is found, the
subtree pair can be recognized as paraphrases. In ac-
tual experiments, we put some restrictions on these
subtrees, which will be discussed later. This way
we can obtain grammatically well-formed portions
of sentences (Figure 2).
2.4 Generalize Expressions
After corresponding portions are obtained, we gen-
eralize the expressions to form usable templates of
paraphrases. Actually this is already done by Ex-
tended Named Entity tagging. An Extended Named
Entity tagger classifies proper expressions into sev-
eral categories. This is similar to a part-of-speech
tagger as it classifies words into several part-of-
speech categories. For example, ?Hong Kong? is
tagged as a location name, and ?two more? as a
number. So an expression such as ?two more peo-
ple die in Hong Kong? is finally converted into the
form ?NUMBER people die in LOCATION? where
NUMBER and LOCATION are slots to fill in. This
way we obtain expressions which can be used as IE
patterns.
the government
has announced
have died
in Hong Kong
two more people
subject
object
in
subject
Hong Kong
reported
two more deaths
subject
object
predicates predicates
paraphrases
LOCATION is included.
NUMBER is included.
anchors
Figure 2: Extracting portions of sentences
3 Handling Problems in Real Texts
In the previous section we described our method for
obtaining paraphrases in principle. However there
are several issues in actual texts which pose difficul-
ties for our method.
The first one is in finding anchors which refer to
the same entity. In actual articles, names are some-
time referred to in a slightly different form. For ex-
ample, ?President Bush? can also be referred to as
?Mr. Bush?. Additionally, sometime it is referred
to by a pronoun, such as ?he?. Since our method
relies on the fact that those anchors are preserved
across articles, anchors which appear in these var-
ied forms may reduce the actual number of obtained
paraphrases.
To handle this problem, we extended the notion
of anchors to include not just Extended Named En-
tities, but also pronouns and common nouns such
as ?the president?. We used a simple corefer-
ence resolver after Extended Named Entity tag-
ging. Currently this is done by simply assigning
the most recent antecedent to pronouns and finding
a longest common subsequence (LCS) between two
noun groups. Since it is possible to form a com-
pound noun such as ?President-Bush? in Japanese,
we computed LCS for each character in the two
noun groups. We used the following condition to
decide whether two noun groups s1 and s2 are coref-
erential:
? if 2 ? min(|s1|, |s2|) ? |LCS(s1, s2)|, then
s1 and s2 are considered coreferential.
Here |s| denotes the length of noun group s and
LCS(s1, s2) is the LCS of two noun groups s1 and
s2.
The second problem is to extract appropriate por-
tions as paraphrase expressions. Since we use a tree
structure to represent the expressions, finding com-
mon subtrees may take an exponential number of
steps. For example, if a dependency tree in one
article has one single predicate which has n argu-
ments, the number of possible subtrees which can
be obtained from the tree is 2n. So the matching
process between arbitrary combinations of subtrees
may grow exponentially with the length of the sen-
tences. Even worse, it can generate many combina-
tions of sentence portions which don?t make sense as
paraphrases. For example, from the expression ?two
more people have died in Hong Kong? and ?Hong
Kong reported two more deaths?, we could extract
expressions ?in Hong Kong? and ?Hong Kong re-
ported?. Although both of them share one anchor,
this is not a correct paraphrase. To avoid this sort of
error, we need to put some additional restrictions on
the expressions.
(Shinyama et al, 2002) used the frequency of ex-
pressions to filter these incorrect pairs of expres-
sions. First the system obtained a set of IE patterns
from corpora (Sudo and Sekine, 2001), and then cal-
culated the score for each candidate paraphrase by
counting how many times that expression appears as
an IE pattern in the whole corpus. However, with
this method, obtainable expressions are limited to
existing IE patterns only. Since we wanted to ob-
tain a broader range of expressions not limited to
IE patterns themselves, we tried to use other restric-
tions which can be acquired independently of the IE
system.
We partly solve this problem by calculating the
plausibility of each tree structure. In Japanese sen-
tences, the case of each argument which modifies
a predicate is represented by a case marker (post-
position or joshi) which follows a noun phrase, just
like prepositions in English but in the opposite order.
These arguments include subjects and objects that
are elucidated syntactically in English sentences.
We collected frequent cases occurring with a spe-
cific predicate in advance. We applied this restric-
tion when generating subtrees from a dependency
tree by calculating a score for each predicate as fol-
lows:
Let an instance of predicate p have cases C =
{c1, c2, ..., cn} and a function Np(I) be the number
of instances of p in the corpus whose cases are I =
{c1, c2, ..., cm}. We compute the score Sp(C) of the
instance:
Sp(C) =
?
I?C Np(I)
the number of instances of p in the corpus .
Using this metric, a predicate which doesn?t have
cases that it should usually have is given a lower
score. A subtree which includes a predicate whose
score is less than a certain threshold is filtered out.
This way we can filter out expressions such as
?Hong Kong reported? in Japanese since it would
lack an object case which normally the verb ?re-
port? should have. Moreover, this greatly reduces
the number of possible combinations of subtrees.
4 Experiments
We used Japanese news articles for this experi-
ment. First we collected articles for a specific do-
main from two different newspapers (Mainichi and
Nikkei). Then we used a Japanese part-of-speech
tagger (Kurohashi and Nagao, 1998) and Extended
Named Entity tagger to process documents, and put
them into a Topic Detection and Tracking system.
In this experiment, we used a modified version of a
Japanese Extended Named Entity tagger (Uchimoto
et al, 2000). This tagger tags person names, orga-
nization names, locations, dates, times and numbers.
Article pairs:
Obtained Correct
System 195 156
(80%)
Sentence pairs:
(from top 20 article pairs)
Obtained Correct
Manual 93 93
W/o coref. 55 41 (75%)
W coref. 75 52 (69%)
Paraphrase pairs:
Obtained Correct
W/o coref. or restriction 106 25 (24%)
W/o coref., w restriction 32 18 (56%)
W coref. and restriction 37 23 (62%)
Manual (in 5 hours) (100) (100)
Table 1: Results in the murder cases domain
Sample 1:
? PERSON1 killed PERSON2.
? PERSON1 let PERSON2 die from loss of blood.
Sample 2:
? PERSON1 shadowed PERSON2.
? PERSON1 kept his eyes on PERSON2.
Figure 3: Sample correct paraphrases obtained
(translated from Japanese)
Sample 3:
? PERSON1 fled to LOCATION.
? PERSON1 fled and lay in ambush to LOCATION.
Sample 4:
? PERSON1 cohabited with PERSON2.
? PERSON1 murdered in the room for cohabitation
with PERSON2.
Figure 4: Sample incorrect paraphrases obtained
(translated from Japanese)
Next we applied a simple vector space method to ob-
tain pairs of sentences which report the same event.
After that, we used a simple coreference resolver to
identify anchors. Finally we used a dependency an-
alyzer (Kurohashi, 1998) to extract portions of sen-
tences which share at least one anchor.
In this experiment, we used a set of articles which
reports murder cases. The results are shown in Ta-
ble 1. First, with Topic Detection and Tracking,
there were 156 correct pairs of articles out of 193
pairs obtained. To simplify the evaluation process,
we actually obtained paraphrases from the top 20
pairs of articles which had the highest similarities.
Obtained paraphrases were reviewed manually. We
used the following criteria for judging the correct-
ness of paraphrases:
1. They has to be describing the same event.
2. They should capture the same information if we
use them in an actual IE application.
We tried several conditions to extract paraphrases.
First we tried to extract paraphrases using neither
coreference resolution nor case restriction. Then we
applied only the case restriction with the threshold
0.3 < Sp(C), and observed the precision went up
from 24% to 56%. Furthermore, we added a sim-
ple coreference resolution and the precision rose to
62%. We got 23 correct paraphrases. We found
that several interesting paraphrases are obtained.
Some examples are shown in Figure 3 (correct para-
phrases) and Figure 4 (incorrect paraphrases).
It is hard to say how many paraphrases can be ul-
timately obtained from these articles. However, it is
worth noting that after spending about 5 hours for
this corpus we obtained 100 paraphrases manually.
5 Discussion
Some paraphrases were incorrectly obtained. There
were two major causes. The first one was depen-
dency analysis errors. Since our method recognizes
boundaries of expressions using dependency trees, if
some predicates in a tree take extra arguments, this
may result in including extraneous portions of the
sentence in the paraphrase. For example, the predi-
cate ?lay in ambush? in Sample 3 should have taken
a different noun as its subject. If so, the predicate
doesn?t share the anchors any more and could be
eliminated.
The second cause was the lack of recognizing
contexts. In Sample 4, we observed that even if two
expressions share multiple anchors, an obtained pair
can be still incorrect. We hope that this kind of error
can be reduced by considering the contexts around
expressions more extensively.
6 Future Work
We hope to apply our approach further to ob-
tain more varied paraphrases. After a certain
number of paraphrases are obtained, we can use
the obtained paraphrases as anchors to obtain
additional paraphrases. For example, if we know
?A dismantle B? and ?A destroy B? are para-
phrases, we could apply them to ?U.N. reported
Iraq dismantling more missiles? and ?U.N. official
says Iraq destroyed more Al-Samoud 2 missiles?,
and obtain another pair of paraphrases ?X reports Y?
and ?X says Y?.
This approach can be extended in the other direc-
tion. Some entities can be referred to by completely
different names in certain situations, such as ?North
Korea? and ?Pyongyang?. We are also planning to
identify these varied external forms of a single entity
by applying previously obtained paraphrases. For
example, if we know ?A restarted B? and ?A reac-
tivated B? as paraphrases, we could apply them to
?North Korea restarted its nuclear facility? and ?Py-
ongyang has reactivated the atomic facility?. This
way we know ?North Korea? and ?Pyongyang? can
refer to the same entity in a certain context.
In addition, we are planning to give some credi-
bility score to anchors for improving accuracy. We
found that some anchors are less reliable than oth-
ers even if they are considered as proper expres-
sions. For example, in most U.S. newspapers the
word ?U.S.? is used in much wider contexts than
word such as ?Thailand? although both of them are
country names. So we want to give less credit to
these widely used names.
We noticed that there are several issues in general-
izing paraphrases. Currently we simply label every
Named Entity as a slot. However expressions such
as ?the governor of LOCATION? can take only a cer-
tain kind of locations. Also some paraphrases might
require a narrower context than others and are not
truly interchangeable. For example, ?PERSON was
sworn? can be replaced with ?PERSON took office?,
but not vice versa.
7 Conclusions
In this paper, we described a method to obtain para-
phrases automatically from corpora. Our key notion
is to use comparable articles which report the same
event on the same day. Some noun phrases, espe-
cially Extended Named Entities such as names, lo-
cations and numbers, are preserved across articles
even if the event is reported using different expres-
sions. We used these noun phrases as anchors and
extracted portions which share these anchors. Then
we generalized the obtained expressions as usable
paraphrases.
We adopted dependency trees as a format for ex-
pressions which preserve syntactic constraints when
extracting paraphrases. We generate possible sub-
trees from dependency trees and find pairs which
share the anchors. However, simply generating all
subtrees ends up obtaining many inappropriate por-
tions of sentences. We tackled this problem by cal-
culating a score which tells us how plausible ex-
tracted candidates are. We confirmed that it con-
tributed to the overall accuracy. This metric was
also useful to trimming the search space for match-
ing subtrees. We used a simple coreference resolver
to handle some additional anchors such as pronouns.
Acknowledgments
This research is supported by the Defense Advanced
Research Projects Agency as part of the Translin-
gual Information Detection, Extraction and Sum-
marization (TIDES) program, under Grant N66001-
001-1-8917 from the Space and Naval Warfare Sys-
tems Center San Diego, and by the National Science
Foundation under Grant IIS-0081962. This paper
does not necessarily reflect the position or the pol-
icy of the U.S. Government.
References
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting Paraphrases from a Parallel Corpus. In Pro-
ceedings of the ACL/EACL.
Sadao Kurohashi and Makoto Nagao, 1998. Japanese
Morphological Analysis System JUMAN. Kyoto Uni-
versity, version 3.61 edition.
Sadao Kurohashi, 1998. Kurohashi-Nagao parser. Ky-
oto University, version 2.0 b6 edition.
Dekang Lin and Patrick Pantel. 2001. Discovery of In-
ference Rules for Question Answering. Natural Lan-
guage Engineering, 7(4):343?360.
Bo Pang, Kevin Knight, and Danial Marcu. 2003.
Syntax-based Alignment of Multiple Translations: Ex-
tracting Paraphrases and Generating New Sentences.
In NAACL-HLT.
Ron Papka, James Allen, and Victor Lavrenko. 1999.
UMASS Approaches to Detection and Tracking at
TDT2. In DARPA: Broadcast News Workshop.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics (ACL).
Satoshi Sekine, Kiyoshi sudo, and Chikashi Nobata.
2002. Extended Named Entity Hierarchy. In Proceed-
ings of the LREC.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and
Ralph Grishman. 2002. Automatic Paraphrase Ac-
quisition from News Articles. In Proceedings of the
Second International Conference on Human Language
Technology Research.
Kiyoshi Sudo and Satoshi Sekine. 2001. Automatic Pat-
tern Acquisition for Japanese Information Extraction.
In Proceedings of the HLT.
Kiyotaka Uchimoto, Masaki Murata, Qing Ma, Hiromi
Ozaku, and Hitoshi Isahara. 2000. Named Entity Ex-
traction Based on A Maximum Entropy Model and
Transformation Rules. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 326?335.
Charles L. Wayne. 1998. Topic Detection & Tracking: A
Case Study in Corpus Creation & Evaluation Method-
ologies. In Proceedings of the LREC.
Multilingual Aligned Parallel Treebank Corpus Reflecting
Contextual Information and Its Applications
Kiyotaka Uchimoto? Yujie Zhang? Kiyoshi Sudo?
Masaki Murata? Satoshi Sekine? Hitoshi Isahara?
?National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
{uchimoto,yujie,murata,isahara}@nict.go.jp
?New York University
715 Broadway, 7th floor
New York, NY 10003, USA
{sudo,sekine}@cs.nyu.edu
Abstract
This paper describes Japanese-English-Chinese
aligned parallel treebank corpora of newspaper
articles. They have been constructed by trans-
lating each sentence in the Penn Treebank and
the Kyoto University text corpus into a cor-
responding natural sentence in a target lan-
guage. Each sentence is translated so as to
reflect its contextual information and is anno-
tated with morphological and syntactic struc-
tures and phrasal alignment. This paper also
describes the possible applications of the par-
allel corpus and proposes a new framework to
aid in translation. In this framework, paral-
lel translations whose source language sentence
is similar to a given sentence can be semi-
automatically generated. In this paper we show
that the framework can be achieved by using
our aligned parallel treebank corpus.
1 Introduction
Recently, accurate machine translation systems
can be constructed by using parallel corpora
(Och and Ney, 2000; Germann et al, 2001).
However, almost all existing machine transla-
tion systems do not consider the problem of
translating a given sentence into a natural sen-
tence reflecting its contextual information in the
target language. One of the main reasons for
this is that we had many problems that had to
be solved by one-sentence to one-sentence ma-
chine translation before we could solve the con-
textual problem. Another reason is that it was
difficult to simply investigate the influence of
the context on the translation because sentence
correspondences of the existing bilingual doc-
uments are rarely one-to-one, and are usually
one-to-many or many-to-many.
On the other hand, high-quality treebanks
such as the Penn Treebank (Marcus et al, 1993)
and the Kyoto University text corpus (Kuro-
hashi and Nagao, 1997) have contributed to
improving the accuracies of fundamental tech-
niques for natural language processing such as
morphological analysis and syntactic structure
analysis. However, almost all of these high-
quality treebanks are based on monolingual cor-
pora and do not have bilingual or multilin-
gual information. There are few high-quality
bilingual or multilingual treebank corpora be-
cause parallel corpora have mainly been actively
used for machine translation between related
languages such as English and French, there-
fore their syntactic structures are not required
so much for aligning words or phrases. How-
ever, syntactic structures are necessary for ma-
chine translation between languages whose syn-
tactic structures are different from each other,
such as in Japanese-English, Japanese-Chinese,
and Chinese-English machine translations, be-
cause it is more difficult to automatically align
words or phrases between two unrelated lan-
guages than between two related languages. Ac-
tually, it has been reported that syntactic struc-
tures contribute to improving the accuracy of
word alignment between Japanese and English
(Yamada and Knight, 2001). Therefore, if we
had a high-quality parallel treebank corpus, the
accuracies of machine translation between lan-
guages whose syntactic structures are differ-
ent from each other would improve. Further-
more, if the parallel treebank corpus had word
or phrase alignment, the accuracy of automatic
word or phrase alignment would increase by
using the parallel treebank corpus as training
data. However, so far, there is no aligned par-
allel treebank corpus whose domain is not re-
stricted. For example, the Japanese Electronics
Industry Development Association?s (JEIDA?s)
bilingual corpus (Isahara and Haruno, 2000)
has sentence, phrase, and proper noun align-
ment. However, it does not have morphologi-
cal and syntactic information, the alignment is
partial, and the target is restricted to a white
paper. The Advance Telecommunications Re-
search dialogue database (ATR, 1992) is a par-
allel treebank corpus between Japanese and En-
glish. However, it does not have word or phrase
alignment, and the target domain is restricted
to travel conversation.
Therefore, we have been constructing aligned
parallel treebank corpora of newspaper articles
between languages whose syntactic structures
are different from each other since 2001; they
meet the following conditions.
1. It is easy to investigate the influence of the con-
text on the translation, which means the sen-
tences that come before and after a particular
sentence, and that help us to understand the
meaning of a particular word such as a pro-
noun.
2. The annotated information in the existing
monolingual high-quality treebanks can be uti-
lized.
3. They are open to the public.
To construct parallel corpora that satisfy these
conditions, each sentence in the Penn Tree-
bank (Release 2) and the Kyoto University text
corpus (Version 3.0) has been translated into
a corresponding natural sentence reflecting its
contextual information in a target language by
skilled translators, revised by native speakers,
and each parallel translation has been anno-
tated with morphological and syntactic struc-
tures, and phrasal alignment. Henceforth, we
call the parallel corpus that is constructed by
pursuing the above policy an aligned parallel
treebank corpus reflecting contextual informa-
tion. In this paper, we describe an aligned par-
allel treebank corpus of newspaper articles be-
tween Japanese, English, and Chinese, and its
applications.
2 Construction of Aligned Parallel
Treebank Corpus Reflecting
Contextual Information
2.1 Human Translation of Existing
Monolingual Treebank
The Penn Treebank is a tagged corpus of Wall
Street Journal material, and it is divided into 24
sections. The Kyoto University text corpus is a
tagged corpus of the Mainichi newspaper, which
is divided into 16 sections according to the cat-
egories of articles such as the sports section and
the economy section. To maintain the consis-
tency of expressions in translation, a few partic-
ular translators were assigned to translate arti-
cles in a particular section, and the same trans-
lator was assigned to the same section. The
instructions to translators for Japanese-English
translation is basically as follows.
1. One-sentence to one-sentence translation as a
rule
Translate a source sentence into a target sen-
tence. In case the translated sentence becomes
unnatural by pursuing this policy, leave a com-
ment.
2. Natural translation reflecting contextual infor-
mation
Except in the case that the translated sentence
becomes unnatural by pursuing policy 1, trans-
late a source sentence into a target sentence
naturally.
By deletion, replacement, or supplementation,
let the translated sentence be natural in the
context.
In an entire article, the translated sentences
must maintain the same meaning and informa-
tion as those of the original sentences.
3. Translations of proper nouns
Find out the translations of proper nouns by
looking up the nouns in a dictionary or by using
a web search. In case a translation cannot be
found, use a temporary name and report it.
We started the construction of Japanese-
Chinese parallel corpus in 2002. The Japanese
sentences of the Kyoto University text corpus
were also translated into Chinese by human
translators. Then each translated Chinese sen-
tence was revised by a second Chinese native.
The instruction to the translators is the same
as that given in the Japanese-English human
translations.
The breakdown of the parallel corpora is
shown in Table 1. We are planning to trans-
late the remaining 18,714 sentences of the Kyoto
University text corpus and the remaining 30,890
sentences of the Penn Treebank. As for the nat-
uralness of the translated sentences, there are
207 (1%) unnatural English sentences of the
Kyoto University text corpus, and 462 (2.5%)
unnatural Japanese sentences of the Penn Tree-
bank generated by pursuing policy 1.
2.2 Morphological and Syntactic
Annotation
In the following sections, we describe the anno-
tated information of the parallel treebank cor-
pus based on the Kyoto University text corpus.
2.2.1 Morphological and Syntactic
Information of Japanese-English
corpus
Translated English sentences were analyzed by
using the Charniak Parser (Charniak, 1999).
Then, the parsed sentences were manually re-
vised. The definitions of part-of-speech (POS)
categories and syntactic labels follow those of
the Treebank I style (Marcus et al, 1993).
We have finished revising the 10,328 parsed
sentences that appeared from January 1st to
11th. An example of morphological and syn-
tactic structures is shown in Figure 1. In this
figure, ?S-ID? means the sentence ID in the
Kyoto University text corpus. EOJ means the
boundary between a Japanese parsed sentence
and an English parsed sentence. The definition
of Japanese morphological and syntactic infor-
mation follows that of the Kyoto University text
corpus (Version 3.0). The syntactic structure is
represented by dependencies between Japanese
phrasal units called bunsetsus. The phrasal
Table 1: Breakdown of the parallel corpora
Original corpus Languages # of parallel sentences
Kyoto University text corpus Japanese-English 19,669 (from Jan. 1st to 17th in 1995)
Japanese-Chinese 38,383 (all)
Penn Treebank Japanese-English 18,318 (from section 0 to 9)
Total Japanese-English 37,987 (Approximately 900,000 English words)
Japanese-Chinese 38,383 (Approximately 900,000 Chinese words)
# S-ID:950104141-008
* 0 2D
???? ???? * ?? * * *
* 1 2D
?? ?????? * ?? ?? * *
? ?? * ??? ???????? * *
?? ??? * ??? ???????? * *
? ? * ?? ???? * *
* 2 6D
?? ???? * ?? ???? * *
? ? ? ??? * ??? ????????
? ? * ?? ?? * *
* 3 4D
?? ???? * ?? ???? * *
? ? * ?? ??? * *
* 4 5D
??? ???? ??? ?? * ???? ???
* 5 6D
?? ???? * ?? ???? * *
? ? * ?? ??? * *
* 6 -1D
??? ???? ?? ?? * ?????? ??????
? ? ?? ??? ?????? ???? ???
?? ?? ?? ??? ????????? ???????? ???
? ? * ?? ?? * *
EOJ
(S1 (S (NP (PRP They))
(VP (VP (VBD were)
(NP (DT all))
(ADJP (NP (QP (RB about)
(CD nineteen))
(NNS years))
(JJ old)))
(CC and)
(VP (VBD had)
(S (NP (DT no)
(NN strength))
(VP (VBN left)
(SBAR (S (VP (ADVP (RB even))
(TO to)
(VP (VB answer)
(NP (NNS questions))))))))))
(. .)))
EOE
Figure 1: Example of morphological and syn-
tactic information.
units or bunsetsus are minimal linguistic units
obtained by segmenting a sentence naturally in
terms of semantics and phonetics, and each of
them consists of one or more morphemes.
2.2.2 Chinese Morphological
Information of Japanese-Chinese
corpus
Chinese sentences are composed of strings of
Hanzi and there are no spaces between words.
The morphological annotation, therefore, in-
cludes providing tags of word boundaries and
POSs of words. We analyzed the Chinese sen-
tences by using the morphological analyzer de-
veloped by Peking University (Zhou and Duan,
1994). There are 39 categories in this POS set.
Then the automatically tagged sentences were
revised by the third native Chinese. In this
pass the Chinese translations were revised again
while the results of word segmentation and POS
tagging were revised. Therefore the Chinese
translations are obtained with a high quality.
We have finished revising the 12,000 tagged sen-
tences. The revision of the remaining sentences
is ongoing. An example of tagged Chinese sen-
tences is shown in Figure 2. The letters shown
Figure 2: Example of morphological informa-
tion of Chinese corpus.
after ?/? indicate POSs. The Chinese sentence is
the translation of the Japanese sentence in Fig-
ure 1. The Chinese sentences are GB encoded.
The 38,383 translated Chinese sentences have
1,410,892 Hanzi and 926,838 words.
2.3 Phrasal Alignment
This section describes the annotated informa-
tion of 19,669 sentences of the Kyoto University
text corpus.
The minimum alignment unit should be as
small as possible, because bigger units can be
constructed from units of the minimum size.
However, we decided to define a bunsetsu as the
minimum alignment unit. One of the main rea-
sons for this is that the smaller the unit is, the
higher the human annotation cost is. Another
reason is that if we define a word or a morpheme
as a minimum alignment unit, expressions such
as post-positional particles in Japanese and arti-
cles in English often do not have alignments. To
effectively absorb those expressions and to align
as many parts as possible, we found that a big-
ger unit than a word or a morpheme is suitable
as the minimum alignment unit. We call the
minimum alignment based on bunsetsu align-
ment units the bunsetsu unit translation pair.
Bigger pairs than the bunsetsu unit translation
pairs can be automatically extracted based on
the bunsetsu unit translation pairs. We call all
of the pairs, including bunsetsu unit transla-
tion pairs, translation pairs. The bunsetsu unit
translation pairs for idiomatic expressions often
become unnatural. In this case, two or more
bunsetsu units are combined and handled as a
minimum alignment unit. The breakdown of
the bunsetsu unit translation pairs is shown in
Table 2.
Table 2: Breakdown of the bunsetsu unit trans-
lation pairs.
(1) total # of translation pairs 172,255
(2) # of different translation pairs 146,397
(3) # of Japanese expressions 110,284
(4) # of English expressions 111,111
(5) average # of English expressions 1.33
corresponding to a Japanese expression ((2)/(3))
(6) average # of Japanese expressions 1.32
corresponding to a English expression ((2)/(4))
(7) # of ambiguous Japanese expressions 15,699
(8) # of ambiguous English expressions 12,442
(9) # of bunsetsu unit translation pairs 17,719
consisting of two or more bunsetsus
An example of phrasal alignment is shown in
Figure 3. A Japanese sentence is shown from
the line after the S-ID to the EOJ. Each line
indicates a bunsetsu. Each rectangular line in-
dicates a dependency between bunsetsus. The
leftmost number in each line indicates the bun-
setsu ID. The corresponding English sentence is
shown in the next line after that of the EOJ
(End of Japanese) until the EOE (End of En-
glish). The English expressions corresponding
to each bunsetsu are tagged with the corre-
sponding bunsetsu ID such as <P id=?bunsetsu
ID?></P>. When there are two or more fig-
ures in the tag id such as id=?1,2?, it means two
or more bunsetsus are combined and handled as
a minimum alignment unit.
For example, we can extract the following
translation pairs from Figure 3.
 (J) ??? (yunyuu-ga) / ????? (kaikin-sa-reta);
(E)that had been under the ban
 (J) ??????? (beikoku-san-ringo-no); (E)of apples
imported from the U.S.
 (J) ???? (dai-ichi-bin-ga); (E)The first cargo
 (J)???????(uridasa-reta); (E)was brought to the
market.
 (J) ??????? (beikoku-san-ringo-no) / ????
(dai-ichi-bin-ga); (E)The first cargo / of apples im-
ported from the U.S.
# S-ID:950110003-001
1 ????????????????
2 ????????????????
3 ????????????????
4 ????????????????
5 ????????????????
6 ????????????????
7 ????????????????
8 ????????????????
9 ????????????????
10 ???????????????
11 ????????????????
EOJ
<P id="4">The first cargo</P> <P id="3">of apples
imported from the U.S.</P> <P id="1,2">that had been
under the ban</P> <P id="7">completed</P> <P id="6">
quarantine</P> <P id="7">and</P> <P id="11">was brought
to the market</P> <P id="10">for the first time</P>
<P id="5">on the 9th</P> <P id="9">at major supermarket
chain stores</P> <P id="8">in the Tokyo metropolitan
area</P> <P id="11">.</P>
EOE
Figure 3: Example of phrasal alignment.
 (J) ??????? (beikoku-san-ringo-no) / ????
(dai-ichi-bin-ga) /???????(uridasa-reta); (E)The
first cargo / of apples imported from the U.S. / was
brought to the market.
Here, Japanese and English expressions are
divided by the symbol ?;?, and ?/? means a
bunsetsu boundary.
An overview of the criteria of the alignment
is as follows. Align as many parts as possible,
except if a certain part is redundant. More de-
tailed criteria will be attached with our corpus
when it is open to the public.
1. Alignment of English grammatical elements
that are not expressed in Japanese
English articles, possessive pronouns, infinitive
to, and auxiliary verbs are joined with nouns
and verbs.
2. Alignment between a noun and its substitute
expression
A noun can be aligned with its substitute ex-
pression such as a pronoun.
3. Alignment of Japanese ellipses
An English expression is joined with its related
elements. For example, the English subject is
joined with its related verb.
4. Alignment of supplementary or explanatory ex-
pression in English
Supplementary or explanatory expressions in
English are joined with their related words.
? Ex.?
# S-ID:950104142-003
1 ???????????
2 ???????????
3 ???????????
4 ???????????
5 ???????????
6 ???????????
EOJ
<P id="1">The Chinese character used for "ka"</P>
has such meanings as "beautiful" and "splendid."
EOE
?"?? (ka)??? (niwa)" corresponds to
"The Chinese character used for "ka""
5. Alignment of date and time
When a Japanese noun representing date and
time is adverbial, the English preposition is
joined with the date and time.
6. Alignment of coordinate structures
When English expressions represented by ?X
(A + B)? correspond to Japanese expressions
represented by ?XA + XB?, the alignment of
X overlaps.
? Ex.?
# S-ID:950106149-005
1 ?????????????
2 ?????????????
3 ?????????????
4 ?????????????
5 ?????????????
6 ?????????????
7 ?????????????
8 ?????????????
EOJ
In the Kinki Region, disposal of wastes started
<P id="2"><P id="4"> at offshore sites of</P>
Amagasaki</P> and <P id="4">Izumiotsu</P> from
1989 and 1991 respectively.
EOE
?"??? (Amagasaki-oki) ? (de)" corresponds to
"at offshore sites of Amagasaki"
?"???? (Izumiotsu-oki) ? (de)" corresponds to
"at offshore sites of ? Izumiotsu"
3 Applications of Aligned Parallel
Treebank Corpus
3.1 Use for Evaluation of Conventional
Methods
The corpus as described in Section 2 can be
used for the evaluation of English-Japanese and
Japanese-English machine translation. We can
directly compare various methods of machine
translation by using this corpus. It can be sum-
marized as follows in terms of the characteristics
of the corpus.
One-sentence to one-sentence translation
can be simply used for the evaluation of
various methods of machine translation.
Morphological and syntactic information
can be used for the evaluation of methods
that actively use morphological and syntactic
information, such as methods for example-
based machine translation (Nagao, 1981;
Watanabe et al, 2003), or transfer-based
machine translation (Imamura, 2002).
Phrasal alignment is used for the evaluation of
automatically acquired translation knowledge
(Yamamoto and Matsumoto, 2003).
An actual comparison and evaluation is our
future work.
3.2 Analysis of Translation
One-sentence to one-sentence translation
reflects contextual information. Therefore, it
is suitable to investigate the influence of the
context on the translation. For example, we
can investigate the difference in the use of
demonstratives and pronouns between English
and Japanese. We can also investigate the
difference in the use of anaphora.
Morphological and syntactic information
and phrasal alignment can be used to investi-
gate the appropriate unit and size of transla-
tion rules and the relationship between syntac-
tic structures and phrasal alignment.
3.3 Use in Conventional Systems
One-sentence to one-sentence translation
can be used for training a statistical translation
model such as GIZA++ (Och and Ney, 2000),
which could be a strong baseline system for
machine translation.
Morphological and syntactic information
and phrasal alignment can be used to acquire
translation knowledge for example-based ma-
chine translation and transfer-based machine
translation.
In order to show what kind of units are help-
ful for example-based machine translation, we
investigated whether the Japanese sentences of
newspaper articles appearing on January 17,
1995, which we call test-set sentences, could be
translated into English sentences by using trans-
lation pairs appearing from January 1st to 16th
as a database. First, we found that only one out
of 1,234 test-set sentences agreed with one out
of 18,435 sentences in the database. Therefore,
a simple sentence search will not work well. On
the other hand, 6,659 bunsetsus out of 12,632
bunsetsus in the test-set sentences agreed with
those in the database. If words in bunsetsus are
expanded into their synonyms, the combination
of the expanded bunsetsus sets in the database
may cover the test-set sentences. Next, there-
fore, we investigated whether the Japanese test-
set sentences could be translated into English
sentences by simply combining translation pairs
appearing in the database. Given a Japanese
sentence, words were extracted from it and
translation pairs that include those words or
their synonyms, which were manually evalu-
ated, were extracted from the database. Then,
the English sentence was manually generated by
just combining English expressions in the ex-
tracted translation pairs. One hundred two rel-
atively short sentences (the average number of
bunsetsus is about 9.8) were selected as inputs.
The number of equivalent translations, which
mean that the translated sentence is grammat-
ical and has the same meaning as the source
sentence, was 9. The number of similar transla-
tions, which mean that the translated sentence
is ungrammatical, or different or wrong mean-
ings of words, tenses, and prepositions are used
in the translated sentence, was 83. The num-
ber of other translations, which mean that some
words are missing, or the meaning of the trans-
lated sentence is completely different from that
of the original sentence, was 10. For example,
the original parallel translation is as follows:
Japanese:????????????????????????
????????????????????????
English: New Party Sakigake proposed that towards the or-
dinary session, both parties found a council to dis-
cuss policy and Diet management.
Given the Japanese sentence, the translated
sentence was:
Translation:Sakigake Party suggested to set up an organiza-
tion between the two parties towards the regular
session of the Diet to discuss under the theme of
policies and the management of the Diet.
This result shows that only 9% of input sen-
tences can be translated into sentences equiv-
alent to the original ones. However, we found
that approximately 90% of input sentences can
be translated into English sentences that are
equivalent or similar to the original ones.
3.4 Similar Parallel Translation
Generation
The original aim of constructing an aligned par-
allel treebank corpus as described in Section 2 is
to achieve a new framework for translation aid
as described below.
It would be very convenient if multilingual
sentences could be generated by just writing
sentences in our mother language. Today, it
can be formally achieved by using commercial
machine translation systems. However, the au-
tomatically translated sentences are often in-
comprehensible. Therefore, we have to revise
the original and translated sentences by find-
ing and referring to parallel translation whose
source language sentence is similar to the orig-
inal one. In many cases, however, we cannot
find such similar parallel translations to the in-
put sentence. Therefore, it is difficult for users
who do not have enough knowledge of the target
languages to generate comprehensible sentences
in several languages by just searching similar
parallel translations in this way. Therefore, we
propose to generate similar parallel translations
whose source language sentence is similar to
the input sentence. We call this framework for
translation aid similar parallel translation gen-
eration.
We investigated whether the framework can
be achieved by using our aligned parallel tree-
bank corpus. As the first step of this study,
we investigated whether an appropriate parallel
translation can be generated by simply combin-
ing translation pairs extracted from our aligned
parallel treebank corpus in the following steps.
1. Extract each content word with its adjacent
function word in each bunsetsu in a given sen-
tence
2. The extracted content words and their adjacent
function words are expanded into their syn-
onyms and class words whose major and minor
POS categories are the same
3. Find translation pairs including the expanded
content words with their expanded adjacent
function words in the given sentence
4. For each bunsetsu, select a translation pair that
has similar dependency relationship to those in
the given sentence
5. Generate a parallel translation by combining
the selected translation pairs
The input sentences were randomly selected
from 102 sentences described in Section 3.3.
The above steps, except the third step, were
basically conducted manually. The Examples
of the input sentences and generated parallel
translations are shown in Figure 4.
The basic unit of translation pairs in our
aligned parallel treebank corpus is a bunsetsu,
and the basic unit in the selection of transla-
tion pairs is also a bunsetsu. One of the ad-
vantages of using a bunsetsu as a basic unit is
that a Japanese expression represented as one
of various expressions in English, or omitted in
English, such as Japanese post-positional par-
ticles, is paired with a content word. There-
fore, the translation of such an expression is ap-
propriately selected together with the transla-
tion of a content word when a certain trans-
lation pair is selected. If the translation of
such an expression was selected independently
of the translation of a content word, the com-
bination of each translation would be ungram-
matical or unnatural. Another advantage of the
basic unit, bunsetsu, is that we can easily refer
to dependency information between bunsetsus
when we select an appropriate translation pair
because the original treebank has the depen-
dency information between bunsetsus. These
advantages are utilized in the above generation
steps. For example, in the first step, a content
word ??? (kokkai, Diet session)? in the sec-
ond example in Figure 4 was extracted from the
bunsetsu ????? (tsuujo-kokkai, the ordinary
Diet session) ? (ni, case marker)?, and it was
expanded into its class word ?? (kai, meeting)?
in the second step. Then, a translation pair
?(J)??????????? (kokuren-kodomo-
no-kenri-iinkai)? (ni, case marker); (E)the UN
Committee on the Rights of the Child /(J)
?? (taishi); (E)towards? was extracted as a
translation pair in the third step. Since the
dependency between ????????????
(kokuren-kodomo-no-kenri-iinkai, the UN Com-
mittee on the Rights of the Child)? and ???
(taishi, towards)? is similar to that between ?
???? (tsuujo-kokkai, the ordinary Diet ses-
sion)? (ni, case marker)? and ??? (muke, to-
wards)? in the input sentence, this translation
pair was selected in the fourth step. Finally,
the bunsetsu ???????????? (kokuren-
kodomo-no-kenri-iinkai, the UN Committee on
the Rights of the Child) ? (ni, case marker)?
and its translation ?the UN Committee on the
Rights of the Child? was used for generation of
a parallel translation in the fifth step.
When we use the generated parallel transla-
tion for the exact translation of the input sen-
tence, we should replace ??????????
?? (kokuren-kodomo-no-kenri-iinkai)? and its
translation ?the UN Committee on the Rights
of the Child? with ????? (tsuujo-kokkai, the
ordinary Diet session)? and its translation ?the
ordinary Diet session? by consulting a bilingual
dictionary. In this example, ??? (sono)? and
?them? should also be replaced with ??? (ry-
oto)? and ?both parties?. It is easy to identify
words in the generated translation that should
be replaced with words in the input sentence
because each bunsetsu in translation pairs is al-
ready aligned. In such cases, templates such as
?[?? (kaigi)]? (ni)?? (muke)? and ?towards
[council]? can be automatically generated by
generalizing content words expanded in the sec-
ond step and their translation in the generated
translation. The average number of English ex-
pressions corresponding to a Japanese expres-
sion is 1.3 as shown in Table 2. Even when there
are two or more possible English expressions, an
appropriate English expression can be chosen
by selecting a Japanese expression by referring
to dependencies in extracted translation pairs.
Therefore, in many cases, English sentences can
be generated just by reordering the selected ex-
pressions. The English word order was esti-
mated manually in this experiment. However,
we can automatically estimate English word or-
der by using a language model or an English
surface sentence generator such as FERGUS
(Bangalore and Rambow, 2000). Unnatural or
ungrammatical parallel translations are some-
times generated in the above steps. However,
comprehensible translations can be generated
as shown in Figure 4. The biggest advantage
of this framework is that comprehensible target
sentences can be generated basically by refer-
ring only to source sentences. Although it is
costly to search and select appropriate transla-
tion pairs, we believe that human labor can be
reduced by developing a human interface. For
example, when we use a Japanese text gener-
ation system from keywords (Uchimoto et al,
2002), users should only select appropriate key-
words.
We are investigating whether or not we can
generate similar parallel translations to all of
the Japanese sentences appearing on January
17, 1995. So far, we found that we can gen-
erate similar parallel translations to 691 out of
840 sentences (the average number of bunsetsus
is about 10.3) including the 102 sentences de-
scribed in Section 3.3. We found that we could
not generate similar parallel translations to 149
out of 840 sentences.
In the proposed framework of similar paral-
lel translation generation, the language appear-
ing in a corpus corresponds to a controlled lan-
guage, and users are allowed to use only the
controlled language to write sentences in the
source language. We believe that high-quality
bilingual or multilingual documents can be gen-
erated by letting us adapt ourselves to the con-
trolled environment in this way.
4 Conclusion
This paper described aligned parallel treebank
corpora of newspaper articles between lan-
guages whose syntactic structures are different
from each other; they meet the following condi-
tions.
1. It is easy to investigate the influence of the con-
text on the translation.
2. The annotated information in the existing
monolingual high-quality treebanks can be uti-
lized.
3. It is open to the public.
To construct parallel corpora that satisfy
these conditions, each sentence in the existing
monolingual high-quality treebanks has been
translated into a corresponding natural sentence
reflecting its contextual information in a target
language by skilled translators, and each par-
allel translation has been annotated with mor-
phological and syntactic structures and phrasal
alignment.
This paper also described the possible ap-
plications of the parallel corpus and proposed
a similar parallel translation generation frame-
work. In this framework, a parallel translation
whose source language sentence is similar to a
given sentence can be semi-automatically gen-
erated. In this paper we demonstrated that
the framework could be achieved by using our
aligned parallel treebank corpus.
In the near future, the aligned parallel tree-
bank corpora will be open to the public, and
expanded. We are planning to use the corpora
actively for machine translation, as a transla-
tion aid, and for second language learning. We
are also planning to develop automatic or semi-
automatic alignment system and an efficient in-
terface for machine translation aid.
Input sentence
(Japanese only)
???????????????????????????????????????????(Prime Minister
Murayama and Finance Minister Takemura met in the presidential office and they exchanged their
opinions, mainly on the issue of the new faction being formed by the New Democratic Union.)
Generated paral-
lel translation
(J) ????????????????????????????????????????????
(E) Finance Minister Takemura held the meeting at the official residence to exchange views about the
formation of the new party of the New Democratic Union.
Input sentence
(Japanese only)
????????????????????????????????????????????????(New
Party Sakigake proposed that towards the ordinary session, both parties found a council to discuss policy
and Diet management.)
Generated paral-
lel translation
(J) ????????????????????????????????????????????????
???????
(E) Sakigake proposed to set up an organization between them towards the UN Committee on the Rights
of the Child to discuss under the theme of policies and the management of the Diet.
Input sentence
(Japanese only)
?????????????????????????????????????????????(The meeting
was also intended to slow the movement towards the new party by the New Democratic Union, which is
trying to deepen the relationship with the New Frontier Party.)
Generated paral-
lel translation
(J) ?????????????????????????????????????????????
(E) The meeting had meanings to restrict the movement that the new party of New Democratic Union
is progressing to strengthen the coalition with The New Frontier Party.
Input sentence
(Japanese only)
?????????????????????????????????????????????????
???????(Lower House Diet Member Tatsuo Kawabata of the New Frontier Party decided on the
16th that he would hand in notification of his secession to the party on the 17th, in order to form a new
faction with Sadao Yamahana?s group.)
Generated paral-
lel translation
(J) ????????????????????????????????????????????????
?????????
(E) On 16th Tatsuo Kawabata, a member of the House of Representatives of the New Frontier Party
decided to submit The notice to leave the party to the Shinsei Party on the 17th in order to establish a
new faction with Yuukichi Amano and others.
Input sentence
(Japanese only)
???????????????????????????(As for the faction name in the Upper House,
they will decide after they consider how to form a relationship with Democratic Reform Union.)
Generated paral-
lel translation
(J) ?????????????????????
(E) The name of the faction will be decided after discussing the relationship with the JTUC.
Figure 4: Example of generated similar parallel translations.
Acknowledgments
We thank the Mainichi Newspapers for permis-
sion to use their data.
References
ATR. 1992. Dialogue Database. http://www.red.atr.co.jp/
database page/taiwa.html.
S. Bangalore and O. Rambow. 2000. Exploiting a Probabilis-
tic Hierarchical Model for Generation. In Proceedings of
the COLING, pages 42?48.
E. Charniak. 1999. A Maximum-Entropy-Inspired Parser.
Technical Report CS-99-12.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Yamada
2001. Fast Decoding and Optimal Decoding for Machine
Translation. In Proceedings of the ACL-EACL, pages 228?
235.
K. Imamura. 2002. Application of translation knowledge ac-
quired by hierarchical phrase alignment for pattern-based
MT. In Proceedings of the TMI, pages 74?84.
H. Isahara and M. Haruno. 2000. Japanese-English aligned
bilingual corpora. In Jean Veronis, editor, Parallel Text
Processing - Alignment and Use of Translation Corpora,
pages 313?334. Kluwer Academic Publishers.
S. Kurohashi and M. Nagao. 1997. Building a Japanese
Parsed Corpus while Improving the Parsing System. In
Proceedings of the NLPRS, pages 451?456.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
M. Nagao. 1981. A Framework of a Mechanical Translation
between Japanese and English by Analogy Principle. In
Proceedings of the International NATO Symposium on Ar-
tificial and Human Intelligence.
F. J. Och and H. Ney. 2000. Improved Statistical Alignment
Models. In Proceedings of the ACL, pages 440?447.
K. Uchimoto, S. Sekine, and H. Isahara. 2002. Text Gen-
eration from Keywords. In Proceedings of the COLING,
pages 1037?1043.
H. Watanabe, S. Kurohashi, and E. Aramaki. 2003. Finding
Translation Patterns from Paired Source and Target De-
pendency Structures. In Michael Carl and Andy Way, ed-
itors, Recent Advances in Example-Based Machine Trans-
lation, pages 397?420. Kluwer Academic Publishers.
K. Yamada and K. Knight. 2001. A Syntax-based Statistical
Translation Model. In Proceedings of the ACL, pages 523?
530.
K. Yamamoto and Y. Matsumoto. 2003. Extracting Transla-
tion Knowledge from Parallel Corpora. In Michael Carl
and Andy Way, editors, Recent Advances in Example-
Based Machine Translation, pages 365?395. Kluwer Aca-
demic Publishers.
Q. Zhou and H. Duan. 1994. Segmentation and POS Tag-
ging in the Construction of Contemporary Chinese Cor-
pus. Journal of Computer Science of China, Vol.85. (in
Chinese)
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 64?69,
Prague, June 2007. c?2007 Association for Computational Linguistics
The SemEval-2007 WePS Evaluation: Establishing a benchmark for the
Web People Search Task
Javier Artiles
UNED NLP & IR group
Madrid, Spain
javart@bec.uned.es
nlp.uned.es/?javier
Julio Gonzalo
UNED NLP & IR group
Madrid, Spain
julio@lsi.uned.es
nlp.uned.es/?julio
Satoshi Sekine
Computer Science Department
New York University, USA
sekine@cs.nyu.edu
nlp.cs.nyu.edu/sekine
Abstract
This paper presents the task definition, re-
sources, participation, and comparative re-
sults for the Web People Search task, which
was organized as part of the SemEval-2007
evaluation exercise. This task consists of
clustering a set of documents that mention
an ambiguous person name according to the
actual entities referred to using that name.
1 Introduction
Finding information about people in the World Wide
Web is one of the most common activities of Internet
users. Person names, however, are highly ambigu-
ous. In most cases, the results for a person name
search are a mix of pages about different people
sharing the same name. The user is then forced ei-
ther to add terms to the query (probably losing recall
and focusing on one single aspect of the person), or
to browse every document in order to filter the infor-
mation about the person he is actually looking for.
In an ideal system the user would simply type a
person name, and receive search results clustered ac-
cording to the different people sharing that name.
And this is, in essence, the WePS (Web People
Search) task we have proposed to SemEval-2007
participants: systems receive a set of web pages
(which are the result of a web search for a per-
son name), and they have to cluster them in as
many sets as entities sharing the name. This task
has close links with Word Sense Disambiguation
(WSD), which is generally formulated as the task
of deciding which sense a word has in a given con-
text. In both cases, the problem addressed is the res-
olution of the ambiguity in a natural language ex-
pression. A couple of differences make our prob-
lem different. WSD is usually focused on open-
class words (common nouns, adjectives, verbs and
adverbs). The first difference is that boundaries be-
tween word senses in a dictionary are often subtle
or even conflicting, making binary decisions harder
and sometimes even useless depending on the ap-
plication. In contrast, distinctions between people
should be easier to establish. The second difference
is that WSD usually operates with a dictionary con-
taining a relatively small number of senses that can
be assigned to each word. Our task is rather a case
of Word Sense Discrimination, because the number
of ?senses? (actual people) is unknown a priori, and
it is in average much higher than in the WSD task
(there are 90,000 different names shared by 100 mil-
lion people according to the U.S. Census Bureau).
There is also a strong relation of our proposed
task with the Co-reference Resolution problem, fo-
cused on linking mentions (including pronouns) in
a text. Our task can be seen as a co-reference reso-
lution problem where the focus is on solving inter-
document co-reference, disregarding the linking of
all the mentions of an entity inside each document.
An early work in name disambiguation (Bagga
and Baldwin, 1998) uses the similarity between doc-
uments in a Vector Space using a ?bag of words?
representation. An alternative approach by Mann
and Yarowsky (2003) is based on a rich feature space
of automatically extracted biographic information.
Fleischman and Hovy (2004) propose a Maximum
Entropy model trained to give the probability that
64
two names refer to the same individual 1.
The paper is organized as follows. Section 2 pro-
vides a description of the experimental methodol-
ogy, the training and test data provided to the par-
ticipants, the evaluation measures, baseline systems
and the campaign design. Section 3 gives a descrip-
tion of the participant systems and provides the eval-
uation results. Finally, Section 4 presents some con-
clusions.
2 Experimental Methodology
2.1 Data
Following the general SemEval guidelines, we have
prepared trial, training and test data sets for the task,
which are described below.
2.1.1 Trial data
For this evaluation campaign we initially deliv-
ered a trial corpus for the potential participants. The
trial data consisted of an adapted version of the
WePS corpus described in (Artiles et al, 2006). The
predominant feature of this corpus is a high number
of entities in each document set, due to the fact that
the ambiguous names were extracted from the most
common names in the US Census. This corpus did
not completely match task specifications because it
did not consider documents with internal ambiguity,
nor it did consider non-person entities; but it was,
however, a cost-effective way of releasing data to
play around with. During the first weeks after releas-
ing this trial data to potential participants, some an-
notation mistakes were noticed. We preferred, how-
ever, to leave the corpus ?as is? and concentrate our
efforts in producing clean training and test datasets,
rather than investing time in improving trial data.
2.1.2 Training data
In order to provide different ambiguity scenarios,
we selected person names from different sources:
US Census. We reused the Web03 corpus (Mann,
2006), which contains 32 names randomly picked
from the US Census, and was well suited for the
task.
Wikipedia. Another seven names were sampled
from a list of ambiguous person names in the En-
glish Wikipedia. These were expected to have a
1For a comprehensive bibliography on person name disam-
biguation refer to http://nlp.uned.es/weps
few predominant entities (popular or historical), and
therefore a lower ambiguity than the previous set.
ECDL. Finally, ten additional names were ran-
domly selected from the Program Committee listing
of a Computer Science conference (ECDL 2006).
This set offers a scenario of potentially low am-
biguity (computer science scholars usually have a
stronger Internet presence than other professional
fields) with the added value of the a priori knowl-
edge of a domain specific type of entity (scholar)
present in the data.
All datasets consist of collections of web pages
obtained from the 100 top results for a person name
query to an Internet search engine 2. Note that 100
is an upper bound, because in some occasions the
URL returned by the search engine no longer exists.
The second and third datasets (developed explic-
itly for our task) consist of 17 person names and
1685 associated documents in total (99 documents
per name in average). Each web page was down-
loaded and stored for off-line processing. We also
stored the basic metadata associated to each search
result, including the original URL, title, position in
the results ranking and the corresponding snippet
generated by the search engine.
In the process of generating the corpus, the se-
lection of the names plays an important role, poten-
tially conditioning the degree of ambiguity that will
be found later in the Web search results. The reasons
for this variability in the ambiguity of names are di-
verse and do not always correlate with the straight-
forward census frequency. A much more decisive
feature is, for instance, the presence of famous en-
tities sharing the ambiguous name with less popular
people. As we are considering top search results,
these can easily be monopolized by a single entity
that is popular in the Internet.
After the annotation of this data (see section
2.1.4.) we found our predictions about the average
ambiguity of each dataset not to be completely ac-
curate. In Table 1 we see that the ECDL-06 average
ambiguity is indeed relatively low (except for the
documents for ?Thomas Baker? standing as the most
ambiguous name in the whole training). Wikipedia
names have an average ambiguity of 23,14 entities
2We used the Yahoo! API from Yahoo! Search Web Ser-
vices (http://developer.yahoo.com/search/web/).
65
Name entities documents discarded
Wikipedia names
John Kennedy 27 99 6
George Clinton 27 99 6
Michael Howard 32 99 8
Paul Collins 37 98 6
Tony Abbott 7 98 9
Alexander Macomb 21 100 14
David Lodge 11 100 9
Average 23,14 99,00 8,29
ECDL-06 Names
Edward Fox 16 100 36
Allan Hanbury 2 100 32
Donna Harman 7 98 6
Andrew Powell 19 98 48
Gregory Crane 4 99 17
Jane Hunter 15 99 59
Paul Clough 14 100 35
Thomas Baker 60 100 31
Christine Borgman 7 99 11
Anita Coleman 9 99 28
Average 15,30 99,20 30,30
WEB03 Corpus
Tim Whisler 10 33 8
Roy Tamashiro 5 23 6
Cynthia Voigt 1 405 314
Miranda Bollinger 2 2 0
Guy Dunbar 4 51 34
Todd Platts 2 239 144
Stacey Doughty 1 2 0
Young Dawkins 4 61 35
Luke Choi 13 20 6
Gregory Brennan 32 96 38
Ione Westover 1 4 0
Patrick Karlsson 10 24 8
Celeste Paquette 2 17 2
Elmo Hardy 3 55 15
Louis Sidoti 2 6 3
Alexander Markham 9 32 16
Helen Cawthorne 3 46 13
Dan Rhone 2 4 2
Maile Doyle 1 13 1
Alice Gilbreath 8 74 30
Sidney Shorter 3 4 0
Alfred Schroeder 35 112 58
Cathie Ely 1 2 0
Martin Nagel 14 55 31
Abby Watkins 13 124 35
Mary Lemanski 2 152 78
Gillian Symons 3 30 6
Pam Tetu 1 4 2
Guy Crider 2 2 0
Armando Valencia 16 79 20
Hannah Bassham 2 3 0
Charlotte Bergeron 5 21 8
Average 5,90 47,20 18,00
Global average 10,76 71,02 26,00
Table 1: Training Data
per name, which is higher than for the ECDL set.
The WEB03 Corpus has the lowest ambiguity (5,9
entities per name), for two reasons: first, randomly
picked names belong predominantly to the long tail
of unfrequent person names which, per se, have low
ambiguity. Being rare names implies that in average
there are fewer documents returned by the search en-
gine (47,20 per name), which also reduces the pos-
sibilities to find ambiguity.
2.1.3 Test data
For the test data we followed the same process
described for the training. In the name selection we
tried to maintain a similar distribution of ambigu-
ity degrees and scenario. For that reason we ran-
domly extracted 10 person names from the English
Wikipedia and another 10 names from participants
in the ACL-06 conference. In the case of the US cen-
sus names, we decided to focus on relatively com-
mon names, to avoid the problems explained above.
Unfortunately, after the annotation was finished
(once the submission deadline had expired), we
found a major increase in the ambiguity degrees (Ta-
ble 2) of all data sets. While we expected a raise in
the case of the US census names, the other two cases
just show that there is a high (and unpredictable)
variability, which would require much larger data
sets to have reliable population samples.
This has made the task particularly challenging
for participants, because naive learning strategies
(such as empirical adjustment of distance thresholds
to optimize standard clustering algorithms) might be
misleaded by the training set.
2.1.4 Annotation
The annotation of the data was performed sepa-
rately in each set of documents related to an ambigu-
ous name. Given this set of approximately 100 doc-
uments that mention the ambiguous name, the an-
notation consisted in the manual clustering of each
document according to the actual entity that is re-
ferred on it.
When non person entities were found (for in-
stance, organization or places named after a person)
the annotation was performed without any special
rule. Generally, the annotator browses documents
following the original ranking in the search results;
after reading a document he will decide whether the
mentions of the ambiguous name refer to a new en-
tity or to a entity previously identified. We asked
the annotators to concentrate first on mentions that
strictly contained the search string, and then to pay
attention to the co-referent variations of the name.
For instance ?John Edward Fox? or ?Edward Fox
Smith? would be valid mentions. ?Edward J. Fox?,
however, breaks the original search string, and we
do not get into name variation detection, so it will
be considered valid only if it is co-referent to a valid
66
Name entities documents discarded
Wikipedia names
Arthur Morgan 19 100 52
James Morehead 48 100 11
James Davidson 59 98 16
Patrick Killen 25 96 4
William Dickson 91 100 8
George Foster 42 99 11
James Hamilton 81 100 15
John Nelson 55 100 25
Thomas Fraser 73 100 13
Thomas Kirk 72 100 20
Average 56,50 99,30 17,50
ACL06 Names
Dekang Lin 1 99 0
Chris Brockett 19 98 5
James Curran 63 99 9
Mark Johnson 70 99 7
Jerry Hobbs 15 99 7
Frank Keller 28 100 20
Leon Barrett 33 98 9
Robert Moore 38 98 28
Sharon Goldwater 2 97 4
Stephen Clark 41 97 39
Average 31,00 98,40 12,80
US Census Names
Alvin Cooper 43 99 9
Harry Hughes 39 98 9
Jonathan Brooks 83 97 8
Jude Brown 32 100 39
Karen Peterson 64 100 16
Marcy Jackson 51 100 5
Martha Edwards 82 100 9
Neil Clark 21 99 7
Stephan Johnson 36 100 20
Violet Howard 52 98 27
Average 50,30 99,10 14,90
Global average 45,93 98,93 15,07
Table 2: Test Data
mention.
In order to perform the clustering, the annotator
was asked to pay attention to objective facts (bi-
ographical dates, related names, occupations, etc.)
and to be conservative when making decisions. The
final result is a complete clustering of the docu-
ments, where each cluster contains the documents
that refer to a particular entity. Following the pre-
vious example, in documents for the name ?Edward
Fox? the annotator found 16 different entities with
that name. Note that there is no a priori knowledge
about the number of entities that will be discovered
in a document set. This makes the task specially
difficult when there are many different entities and
a high volume of scattered biographical information
to take into account.
In cases where the document does not offer
enough information to decide whether it belongs to
a cluster or is a new entity, it is discarded from the
evaluation process (not from the dataset). Another
common reason for discarding documents was the
absence of the person name in the document, usu-
ally due to a mismatch between the search engine
cache and the downloaded URL.
We found that, in many cases, different entities
were mentioned using the ambiguous name within a
single document. This was the case when a doc-
ument mentions relatives with names that contain
the ambiguous string (for instance ?Edward Fox?
and ?Edward Fox Jr.?). Another common case of
intra-document ambiguity is that of pages contain-
ing database search results, such as book lists from
Amazon, actors from IMDB, etc. A similar case is
that of pages that explicitly analyze the ambiguity of
a person name (Wikipedia ?disambiguation? pages).
The way this situation was handled, in terms of the
annotation, was to assign each document to as many
clusters as entities were referred to on it with the
ambiguous name.
2.2 Evaluation measures
Evaluation was performed in each document set
(web pages mentioning an ambiguous person name)
of the data distributed as test. The human annotation
was used as the gold standard for the evaluation.
Each system was evaluated using the standard pu-
rity and inverse purity clustering measures Purity is
related to the precision measure, well known in In-
formation Retrieval. This measure focuses on the
frequency of the most common category in each
cluster, and rewards the clustering solutions that in-
troduce less noise in each cluster. Being C the set
of clusters to be evaluated, L the set of categories
(manually annotated) and n the number of clustered
elements, purity is computed by taking the weighted
average of maximal precision values:
Purity =
?
i
|Ci|
n
max Precision(Ci, Lj)
where the precision of a cluster Ci for a given cat-
egory Lj is defined as:
Precision(Ci, Lj) =
|Ci
?
Lj |
|Ci|
Inverse Purity focuses on the cluster with maxi-
mum recall for each category, rewarding the clus-
tering solutions that gathers more elements of each
category in a corresponding single cluster. Inverse
Purity is defined as:
67
Inverse Purity =
?
i
|Li|
n
max Precision(Li, Cj)
For the final ranking of systems we used the har-
monic mean of purity and inverse purity F?=0,5 . The
F measure is defined as follows:
F =
1
? 1Purity + (1? ?)
1
Inverse Purity
F?=0,2 is included as an additional measure giv-
ing more importance to the inverse purity aspect.
The rationale is that, for a search engine user, it
should be easier to discard a few incorrect web
pages in a cluster containing all the information
needed, than having to collect the relevant infor-
mation across many different clusters. Therefore,
achieving a high inverse purity should be rewarded
more than having high purity.
2.3 Baselines
Two simple baseline approaches were applied to the
test data. The ALL-IN-ONE baseline provides a
clustering solution where all the documents are as-
signed to a single cluster. This has the effect of al-
ways achieving the highest score in the inverse pu-
rity measure, because all classes have their docu-
ments in a single cluster. On the other hand, the
purity measure will be equal to the precision of the
predominant class in that single cluster. The ONE-
IN-ONE baseline gives another extreme clustering
solution, where every document is assigned to a dif-
ferent cluster. In this case purity always gives its
maximum value, while inverse purity will decrease
with larger classes.
2.4 Campaign design
The schedule for the evaluation campaign was set by
the SemEval organisation as follows: (i) release task
description and trial data set; (ii) release of training
and test; (iii) participants send their answers to the
task organizers; (iv) the task organizers evaluate the
answers and send the results.
The task description and the initial trial data set
were publicly released before the start of the official
evaluation.
The official evaluation period started with the si-
multaneous release of both training and test data, to-
gether with a scoring script with the main evaluation
measures to be used. This period spanned five weeks
in which teams were allowed to register and down-
load the data. During that period, results for a given
task had to be submitted no later than 21 days af-
ter downloading the training data and no later than 7
days after downloading the test data. Only one sub-
mission per team was allowed.
Training data included the downloaded web
pages, their associated metadata and the human clus-
tering of each document set, providing a develop-
ment test-bed for the participant?s systems. We also
specified the source of each ambiguous name in the
training data (Wikipedia, ECDL conference and US
Census). Test data only included the downloaded
web pages and their metadata. This section of the
corpus was used for the systems evaluation. Partici-
pants were required to send a clustering for each test
document set.
Finally, after the evaluation period was finished
and all the participants sent their data, the task orga-
nizers sent the evaluation for the test data.
3 Results of the evaluation campaign
29 teams expressed their interest in the task; this
number exceeded our expectations for this pilot ex-
perience, and confirms the potential interest of the
research community in this highly practical prob-
lem. Out of them, 16 teams submitted results within
the deadline; their results are reported below.
3.1 Results and discussion
Table 3 presents the macro-averaged results ob-
tained by the sixteen systems plus the two baselines
on the test data. We found macro-average 3 prefer-
able to micro-average 4 because it has a clear inter-
pretation: if the evaluation measure is F, then we
should calculate F for every test case (person name)
and then average over all trials. The interpretation
of micro-average F is less clear.
The systems are ranked according to the scores
obtained with the harmonic mean measure F?=0,5 of
3Macro-average F consists of computing F for every test set
(person name) and then averaging over all test sets.
4Micro-average F consists of computing the average P and
IP (over all test sets) and then calculating F with these figures.
68
Macro-averaged Scores
F-measures
rank team-id ? =,5 ? =,2 Pur Inv Pur
1 CU COMSEM ,78 ,83 ,72 ,88
2 IRST-BP ,75 ,77 ,75 ,80
3 PSNUS ,75 ,78 ,73 ,82
4 UVA ,67 ,62 ,81 ,60
5 SHEF ,66 ,73 ,60 ,82
6 FICO ,64 ,76 ,53 ,90
7 UNN ,62 ,67 ,60 ,73
8 ONE-IN-ONE ,61 ,52 1,00 ,47
9 AUG ,60 ,73 ,50 ,88
10 SWAT-IV ,58 ,64 ,55 ,71
11 UA-ZSA ,58 ,60 ,58 ,64
12 TITPI ,57 ,71 ,45 ,89
13 JHU1-13 ,53 ,65 ,45 ,82
14 DFKI2 ,50 ,63 ,39 ,83
15 WIT ,49 ,66 ,36 ,93
16 UC3M 13 ,48 ,66 ,35 ,95
17 UBC-AS ,40 ,55 ,30 ,91
18 ALL-IN-ONE ,40 ,58 ,29 1,00
Table 3: Team ranking
purity and inverse purity. Considering only the par-
ticipant systems, the average value for the ranking
measure was 0, 60 and its standard deviation 0, 11.
Results with F?=0,2 are not substantially different
(except for the two baselines, which roughly swap
positions). There are some ranking swaps, but gen-
erally only within close pairs.
The good performance of the ONE-IN-ONE base-
line system is indicative of the abundance of single-
ton entities (entities represented by only one doc-
ument). This situation increases the inverse purity
score for this system giving a harmonic measure
higher than the expected.
4 Conclusions
The WEPS task ended with considerable success in
terms of participation, and we believe that a careful
analysis of the contributions made by participants
(which is not possible at the time of writing this re-
port) will be an interesting reference for future re-
search. In addition, all the collected and annotated
dataset will be publicly available 5 as a benchmark
for Web People Search systems.
At the same time, it is clear that building a re-
liable test-bed for the task is not simple. First of
all, the variability across test cases is large and un-
predictable, and a system that works well with the
5http://nlp.uned.es/weps
names in our test bed may not be reliable in practi-
cal, open search situations. Partly because of that,
our test-bed happened to be unintentionally chal-
lenging for systems, with a large difference be-
tween the average ambiguity in the training and test
datasets. Secondly, it is probably necessary to think
about specific evaluation measures beyond standard
clustering metrics such as purity and inverse purity,
which are not tailored to the task and do not be-
have well when multiple classification is allowed.
We hope to address these problems in a forthcom-
ing edition of the WEPS task.
5 Acknowledgements
This research was supported in part by the National
Science Foundation of United States under Grant
IIS-00325657 and by a grant from the Spanish gov-
ernment under project Text-Mess (TIN2006-15265-
C06). This paper does not necessarily reflect the po-
sition of the U.S. Government.
References
Javier Artiles, Julio Gonzalo, and Felisa Verdejo. 2005.
A Testbed for People Searching Strategies in the
WWW In Proceedings of the 28th annual Interna-
tional ACM SIGIR conference on Research and De-
velopment in Information Retrieval (SIGIR?05), pages
569-570.
Amit Bagga and Breck Baldwin. 1998. Entity-
Based Cross-Document Coreferencing Using the Vec-
tor Space Model In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and the 17th International Conference on Compu-
tational Linguistics (COLING-ACL?98), pages 79-85.
Michael B. Fleischman and Eduard Hovy 2004. Multi-
document person name resolution. In Proceedings of
ACL-42, Reference Resolution Workshop.
Gideon S. Mann. 2006. Multi-Document Statistical Fact
Extraction and Fusion Ph.D. Thesis.
Gideon S. Mann and David Yarowsky 2003. Unsuper-
vised Personal Name Disambiguation In Proceedings
of the seventh conference on Natural language learn-
ing at HLT-NAACL, pages 33-40.
69
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 39?43, Dublin, Ireland, August 23-29 2014.
Lightweight Client-Side Chinese/Japanese
Morphological Analyzer Based on Online Learning
Masato Hagiwara Satoshi Sekine
Rakuten Institute of Technology, New York
215 Park Avenue South, New York, NY
{masato.hagiwara, satoshi.b.sekine}@mail.rakuten.com
Abstract
As mobile devices and Web applications become popular, lightweight, client-side language
analysis is more important than ever. We propose Rakuten MA, a Chinese/Japanese
morphological analyzer written in JavaScript. It employs an online learning algorithm
SCW, which enables client-side model update and domain adaptation. We have achieved
a compact model size (5MB) while maintaining the state-of-the-art performance, via
techniques such as feature hashing, FOBOS, and feature quantization.
1 Introduction
Word segmentation (WS) and part-of-speech (PoS) tagging, often jointly called morphological
analysis (MA), are the essential component for processing Chinese and Japanese, where words
are not explicitly separated by whitespaces. There have been many word segmentater and PoS
taggers proposed in both Chinese and Japanese, such as Stanford Segmenter (Tseng et al.,
2005), zpar (Zhang and Clark, 2011), MeCab (Kudo et al., 2004), JUMAN (Kurohashi and
Nagao, 1994), to name a few. Most of them are intended for server-side use and provide limited
capability to extend or re-train models. However, as mobile devices such as smartphones and
tablets become popular, there is a growing need for client based, lightweight language analysis,
and a growing number of applications are built upon lightweight languages such as HTML, CSS,
and JavaScript. Techniques such as domain adaptation and model extension are also becoming
more important than ever.
In this paper, we present Rakuten MA, a morphological analyzer entirely written in JavaScript
based on online learning. We will be releasing the software as open source before the COLING
2014 conference at https://github.com/rakuten-nlp/rakutenma, under Apache License, ver-
sion 2.0. It relies on general, character-based sequential tagging, which is applicable to any
languages and tasks which can be processed in a character-by-character basis, including WS and
PoS tagging for Chinese and Japanese. Notable features include:
1. JavaScript based ? Rakuten MA works as a JavaScript library, the de facto ?lingua franca?
of the Web. It works on popular Web browsers as well as node.js, which enables a wide range
of adoption such as smartphones and Web browser extensions. Note that TinySegmenter1
is also entirely written in JavaScript, but it does not support model re-training or any
languages other than Japanese. It doesn?t output PoS tags, either.
2. Compact ? JavaScript-based analyzers pose a difficult technical challenge, that is, the
compactness of the model. Modern language analyzers often rely on a large number of
features and/or dictionary entries, which is impractical on JavaScript runtime environments.
In order to address this issue, Rakuten MA implements some notable techniques. First, the
features are character-based and don?t rely on dictionaries. Therefore, while it is inherently
incapable of dealing with words which are longer than features can capture, it may be robust
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and
proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1http://chasen.org/~taku/software/TinySegmenter/
39
S-N-nc S-P-k 
!! "!
B-V-c E-V-c 
#! $!
S-P-sj 
%!
tags 
characters x1:! c1:C x2:" c2:H x3:# c3:C x4:$ c4:H x5:% c5:H 
? 
Figure 1: Character-based tagging model
Feature Description
x
i?2
, x
i?1
, x
i
, x
i+1
, x
i+2
char. unigrams
x
i?2
x
i?1
, x
i?1
x
i
, x
i
x
i+1
, x
i+1
x
i+2
char. bigrams
c
i?2
, c
i?1
, c
i
, c
i+1
, c
i+2
type unigrams
c
i?2
c
i?1
, c
i?1
c
i
, c
i
c
i+1
, c
i+1
c
i+2
type bigrams
Table 1: Feature Templates Used for Tagging
x
i
and c
i
are the character and the character type at ia.
aEach feature template is instantiated and concate-
nated with possible tags. Character type bigram features
were only used for JA. In CN, we built a character type
dictionary, where character types are simply all the possi-
ble tags in the training corpus for a particular character.
to unknown words compared with purely dictionary-based systems. Second, it employs
techniques such as feature hashing (Weinberger et al., 2009), FOBOS (Duchi and Singer,
2009), and feature quantization, to make the model compact while maintaining the same
level of analysis performance.
3. Online Learning ? it employs a modern online learning algorithm called SCW (Wang et al.,
2012), and the model can be incrementally updated by feeding new instances. This enables
users to update the model if errors are found, without even leaving the Web browser or
node.js. Domain adaptation is also straightforward. Note that MeCab (Kudo et al., 2004),
also supports model re-training using a small re-training corpus. However, the training is
inherently a batch, iterative algorithm (CRF) thus it is hard to predict when it finishes.
2 Analysis Model and Compact Model Representation
Base Model Rakuten MA employs the standard character-based sequential tagging model.
It assigns combination of position tags2 and PoS tags to each character (Figure 1). The optimal
tag sequence y? for an input string x is inferred based on the features ?(y) and the weight vector
w as y? = arg max
y?Y (x)
w ? ?(y), where Y (x) denotes all the possible tag sequences for x, via
standard Viterbi decoding. Table 1 shows the feature template sets.
For training, we used soft confidence weighted (SCW) (Wang et al., 2012). SCW is an online
learning scheme based on Confidence Weighted (CW), which maintains ?confidence? of each
parameter as variance ? in order to better control the updates. Since SCW itself is a general
classification model, we employed the structured prediction model (Collins, 2002) for WS.
The code snippet in Figure 2 shows typical usage of Rakuten MA in an interactive way. Lines
starting with ?//? and ?>? are comments and user input, and the next lines are returned results.
Notice that the analysis of????????? ?President Barak Obama? get better as the model
observes more instances. The analyzer can only segment it into individual characters when the
model is empty ((1) in the code), whereas WS is partially correct after observing the first 10
sentences of the corpus ((2) in the code). After directly providing the gold standard, the result
(3) becomes perfect.
We used and compared the following three techniques for compact model representations:
Feature Hashing (Weinberger et al., 2009) applies hashing functions h which turn an arbi-
trary feature ?
i
(y) into a bounded integer value v, i.e., v = h(?
i
(y)) ? R, where 0 ? v < 2N ,
(N = hash size). This technique is especially useful for online learning, where a large, growing
number of features such as character/word n-grams could be observed on the fly, which the
model would otherwise need to keep track of using flexible data structures such as trie, which
could make training slower as the model observes more training instances. The negative effect of
hash collisions to the performance is negligible because most collisions are between rare features.
2As for the position tags, we employed the SBIEO scheme, where S stands for a single character word, BIE
for beginning, middle, and end of a word, respectively, and O for other positions.
40
// initialize with empty model
> var r = new RakutenMA({});
// (1) first attempt, failed with separate chars.
> r.tokenize("?????????").toString()
"?,,?,,?,,?,,?,,?,,?,,?,,?,"
// train with first 10 sentences in a corpus
> for (var i = 0; i < 10; i ++)
> r.train_one( rcorpus[i] );
// the model is no longer empty
> r.model
Object {mu: Object, sigma: Object}
// (2) second attempt -> getting closer
> r.tokenize("?????????").toString()
"???,N-nc,???,N-pn,?,,?,,?,Q-n"
// retrain with an answer
// return object suggests there was an update
> r.train_one([["???","N-np"],
... ["???","N-np"],["???","N-nc"]]);
Object {ans: Array[3], sys: Array[5], updated: true}
// (3) third attempt
> r.tokenize("?????????").toString()
"???,N-np,???,N-np,???,N-nc"
Figure 2: Rakuten MA usage example
Chinese Prec. Rec. F Japanese Prec. Rec. F
Stanford Parser 97.37 93.54 95.42 MeCab+UniDic 99.15 99.61 99.38
zpar 91.18 92.36 91.77 JUMAN **88.55 **83.06 **85.72
Rakuten MA 92.61 92.64 92.62 KyTea *80.57 *85.02 *82.73
TinySegmenter *86.93 *85.19 *86.05
Rakuten MA 96.76 97.30 97.03
Table 2: Segmentation Performance Comparison with Different Systems
* These systems use different WS criteria and their performance is shown simply for reference. ** We
postprocessed JUMAN?s WS result so that the WS criteria are closer to Rakuten MA?s.
Forward-Backward Splitting (FOBOS) (Duchi and Singer, 2009) is a framework to intro-
duce regularization to online learning algorithms. For each training instance, it runs uncon-
strained parameter update of the original algorithm as the first phase, then solves an instan-
taneous optimization problem to minimize a regularization term while keeping the parameter
close to the first phrase. Specifically, letting w
t+
1
2
,j
the j-th parameter after the first phrase of
iteration t and ? the regularization coefficient, parameter update of FOBOS with L1 regulariza-
tion is done by: w
t+1,j
= sign
(
w
t+
1
2
,j
) [
?
?
?
w
t+
1
2
,j
?
?
?
? ?
]
+
. The strength of regularization can be
adjusted by the coefficient ?. In combining SCW and FOBOS, we retained the confidence value
? of SCW unchanged.
Feature Quantization simply multiplies float numbers (e.g., 0.0165725659236262) by M
(e.g., 1,000) and round it to obtain a short integer (e.g., 16). The multiple M determines the
strength of quantization, i.e., the larger the finer grained, but the larger model size.
3 Experiments
We used CTB 7.0 (Xue et al., 2005) for Chinese (CN), and BCCWJ (Maekawa, 2008) for
Japanese (JA), with 50,805 and 60,374 sentences, respectively. We used the top two levels of
BCCWJ?s PoS tag hierarchy (38 unique tags) and all the CTB PoS tags (38 unique tags). The
average decoding time was 250 millisecond per sentence on Intel Xeon 2.13GHz, measured on
node.js. We used precision (Prec.), recall (Rec.), and F-value (F) of WS as the evaluation metrics,
averaged over 5-fold cross validation. We ignored the PoS tags in the evaluation because they
are especially difficult to compare across different systems with different PoS tag hierarchies.
Comparison with Other Analyzers First, we compare the performance of Rakuten MA
with other word segmenters. In CN, we compared with Stanford Segmenter (Tseng et al., 2005)
and zpar (Zhang and Clark, 2011). In JA, we compared with MeCab (Kudo et al., 2004),
JUMAN (Kurohashi and Nagao, 1994), KyTea (Neubig et al., 2011), and TinySegmenter.
Table 2 shows the result. Note that, some of the systems (e.g., Stanford Parser for CN and
MeCab+UniDic for JA) use the same corpus as the training data and their performance is
unfairly high. Also, other systems such as JUMAN and KyTea employ different WS criteria,
and their performance is unfairly low, although JUMAN?s WS result was postprocessed so that
41
!"#$
%"#$
&"#$
'"#$
($ )$ *$ +$ ,$ !$ %$ &$ '$ ("$
!"#
$%#
&'
()"
*
+',)-*./&0"#*
-./01$
2/01$
3$
Figure 3: Domain Adaptation Result for CN
!"#$
%"#$
&"#$
'""#$
'$ ($ )$ *$ +$ ,$ !$ %$ &$ '"$
!"#
$%#
&'
()"
*
+',)-*./&0"#*
-./01$
2/01$
3$
Figure 4: Domain Adaptation Result for JA
!"#"""$
%&'($
%&')$%&'*$%&'+$
,&'-"./($
,&*-"./($,&'-"./)$
,&*-"./)$
,&'-"./*$
012$0'2$
"-(*$
"-3"$
"-3*$
"-4"$
"-4*$
*""$ *#"""$
!"
#$%&'"()*&"+),"-./"
56789:;8$<86=-$>67?:;@$<A5AB$<86=-$>67?:;@C<A5AB$DE6;=-$<86=-$>67?:;@CDE6;=-$<A5ABCDE6;=-$F99$
Figure 5: Model Comparison
it gives a better idea how it compares with Rakuten MA. We can see that Rakuten MA can
achieve WS performance comparable with the state-of-the-art even without using dictionaries.
Domain Adaptation Second, we tested Rakuten MA?s domain adaptation ability. We chose
e-commerce as the target domain, since it is a rich source of out-of-vocabulary words and poses
a challenge to analyzers trained on newswire text. We sampled product titles and descriptions
from Rakuten Taiwan3 (for CN, 2,786 sentences) and Rakuten Ichiba4 (for JA, 13,268 sentences).
These collections were then annotated by human native speakers in each language, following the
tagging guidelines of CTB (for CN) and BCCWJ (for JA).
We divided the corpus into 5 folds, then used four of them for re-training and one for testing.
The re-training data is divided into ?batches,? consisting of mutually exclusive 50 sentences,
which were fed to the pre-trained model on CTB (for CN) and BCCWJ (for JA) one by one.
Figure 3 and 4 show the results. The performance quickly levels off after five batches for JA,
which gives an approximated number of re-training instances needed (200 to 300) for adaptation.
Note that adaptation took longer on CN, which may be attributed to the fact that Chinese WS
itself is a harder problem, and to the disparity between CTB (mainly news articles in mainland
China) and the adaptation corpus (e-commerce text in Taiwan).
3http://www.rakuten.com.tw/. Note that Rakuten Taiwan is written in Taiwanese traditional Chinese. It
was converted to simplified Chinese by using Wikipedia?s traditional-simplified conversion table http://svn.
wikimedia.org/viewvc/mediawiki/trunk/phase3/includes/ZhConversion.php. Still, having large Taiwan spe-
cific vocabulary poses additional challenges for domain adaptation.
4http://www.rakuten.co.jp/
42
Compact Model Representation Third, we consider the three techniques, and investigate
how these techniques affect the trade-off between WS performance and the model size, which is
measured by the feature trie byte size in raw JSON format5.
Figure 5 shows the scatter plot of F-value vs model size in KB, since we are rather interested
in the trade-off between the model size and the performance. The baseline is the raw model
without any techniques mentioned above. Notice that the figure?s x-axis is in log scale, and the
upper left corner in the figure corresponds to better trade-off (higher performance with smaller
model size). We can observe that all the three techniques can reduce the model size to some
extent while keeping the performance at the same level. In fact, these three techniques are
independent from each other and can be freely combined to achieve better trade-off. If we limit
strictly the same level of performance compared to the baseline, feature hashing (17bit hash
size) with quantization (Point (1) in the figure) seems to achieve the best trade-off, slightly
outperforming the baseline (F = 0.9457 vs F = 0.9455 of the baseline) with the model size of
as little as one fourth (5.2MB vs 20.6MB of the baseline). It is somewhat surprising to see that
feature quantization, which is a very simple method, achieves relatively good performance-size
trade-off (Point (2) in the figure).
4 Conclusion
In this paper, we proposed Rakuten MA, a lightweight, client-side morphological analyzer
entirely written in JavaScript. It supports online learning based on the SCW algorithm, which
enables quick domain adaptation, as shown in the experiments. We successfully achieved a
compact model size of as little as 5MB while maintaining the state-of-the-art performance, using
feature hashing, FOBOS, and feature quantization. We are planning to achieve even smaller
model size by adopting succinct data structure such as wavelet tree (Grossi et al., 2003).
Acknowledgements
The authors thank Satoko Marumoto, Keiji Shinzato, Keita Yaegashi, and Soh Masuko for
their contribution to this project.
References
Michael Collins. 2002. Discriminative training methods for hidden Markov models: theory and experiments with
perceptron algorithms. In Proc. of the EMNLP 2002, pages 1?8.
John Duchi and Yoram Singer. 2009. Efficient online and batch learning using forward backward splitting.
Journal of Machine Learning Research, 10:2899?2934.
Roberto Grossi, Ankur Gupta, and Jeffrey Scott Vitter. 2003. High-order entropy-compressed text indexes. In
Prof. of SODA 2003, pages 841?850.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to Japanese
morphological analysis. In Proceedings of EMNLP, pages 230?237.
Sadao Kurohashi and Makoto Nagao. 1994. Improvements of Japanese morphological analyzer JUMAN. In
Proceedings of the International Workshop on Sharable Natural Language Resources, pages 22?38.
Kikuo Maekawa. 2008. Compilation of the Kotonoha-BCCWJ corpus (in Japanese). Nihongo no kenkyu (Studies
in Japanese), 4(1):82?95.
Graham Neubig, Yosuke Nakata, and Shinsuke Mori. 2011. Pointwise prediction for robust, adaptable japanese
morphological analysis. In Proceedings of ACL-HLT, pages 529?533.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional
random field word segmenter. In Fourth SIGHAN Workshop on Chinese Language Processing.
Jialei Wang, Peilin Zhao, and Steven C. Hoi. 2012. Exact soft confidence-weighted learning. In Proc. of ICML
2012, pages 121?128.
Kilian Weinberger, Anirban Dasgupta, Josh Attenberg, John Langford, and Alex Smola. 2009. Feature hashing
for large scale multitask learning. In Proc. of ICML 2009, pages 1113?1120.
Naiwen Xue, Fei Xia, Fu-dong Chiou, and Marta Palmer. 2005. The penn Chinese treebank: Phrase structure
annotation of a large corpus. Natural Language Engineering, 11(2):207?238.
Yue Zhang and Stephen Clark. 2011. Syntactic processing using the generalized perceptron and beam search.
Computational Linguistics, 37(1):105?151.
5We used one fifth of the Japanese corpus BCCWJ for this experiment. Parameter ? of FOBOS was varied
over {1.0? 10?7, 5.0? 10?7, 1.0? 10?6, 5.0? 10?6, 1.0? 10?5}. The hash size of feature hashing was varied over
14, 15, 16, 17
6. The multiple of feature quantization M is set to M = 1000.
43
Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 13?16,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
SurfShop: combing a product ontology with topic model results for online
window-shopping.
Zofia Stankiewicz and Satoshi Sekine
Rakuten Institute of Technology, New York
215 Park Avenue South
New York, NY 10003, USA
{zofia.stankiewicz,satoshi.b.sekine}@mail.rakuten.com
Abstract
At present, online shopping is typically a
search-oriented activity where a user gains ac-
cess to products which best match their query.
Instead, we propose a surf-oriented online
shopping paradigm, which links associated
products allowing users to ?wander around?
the online store and enjoy browsing a variety
of items. As an initial step in creating this ex-
perience, we constructed a prototype of an on-
line shopping interface which combines pro-
duct ontology information with topic model
results to allow users to explore items from the
food and kitchen domain. As a novel task for
topic model application, we also discuss pos-
sible approaches to the task of selecting the
best product categories to illustrate the hidden
topics discovered for our product domain.
1 Introduction
Query based search remains the primary method of
access to large collections of data. However, new in-
terfacing options offered by mobile and touchscreen
applications lead to decreased reliance on typed
search queries. This trend further fuels the need for
technologies which allow users to browse and ex-
plore large amounts of data from a variety of view-
points. Online store product databases are a repre-
sentative example of such a data source. At present,
online shopping is typically a search-oriented ac-
tivity. Aside from suggestions of closely matching
products from a recommender system, internet shop-
pers have little opportunity to look around an online
store and explore a variety of related items. This ob-
servation led us to define a novel task of creating a
surf-oriented online shopping interface which facil-
itates browsing and access to multiple types of prod-
ucts. We created the prototype SurfShop application
in order to test whether we can combine knowledge
from a product ontology with topic modeling for a
better browsing experience.
Our aim is to design an application which offers
access to a variety of products, while providing a co-
herent and interesting presentation. While the pro-
duct ontology provides information on product types
which are semantically close (for example spaghetti
and penne), it does not provide information about
associations such as pasta and tomato sauce, which
may be mentioned implicitly in product descrip-
tions. In order to obtain semantically varied product
groupings from the data we integrated topic model
results into the application to display products which
are related through hidden topics.
The data used for this project consists of a snap-
shot from the product database of a Japanese Inter-
net shopping mall Rakuten Ichiba obtained in April
20111. We limited our prototype application to
the food and kitchen domain consisting of approx-
imately 4 million products. The textual information
available for each product includes a title and a short
description. Furthermore, each product is assigned
to a leaf category in the product hierarchy tree.
We use standard LDA (Blei et al, 2003) as the
topic model and our prototype can be treated as
an example of applied topic modeling. Although
there exist browsers of document collections based
1For a version of Rakuten product data made available for
research purposes see http://rit.rakuten.co.jp/rdr/index en.html.
13
on topic modeling 2, they have been constructed as
direct model result visualizations. In contrast, we
incorporate the LDA results into the output by com-
bining them with product category information and
search to produce a full blown application with a
topic model serving as one of its components. We
provide a more detailed overview of the entire sys-
tem in section 2.
In LDA literature, the topics discovered by the
model are typically represented by top n most prob-
able words for a given topic. In integrating topic
model results into our application we faced a chal-
lenge of creating theme pages which correspond
to hidden topics, and selecting product categories
which best illustrate a given topic. In section 3
we discuss a preliminary evaluation of the applica-
tion?s theme pages which suggests that combining
topic knowledge with ontology structure can lead to
more coherent product category groupings, and that
topic interpretation and labeling based solely on top
n words may not be sufficient for some applied tasks.
We conclude by summarizing plans for further de-
velopment of our prototype.
2 System overview
The initial input to the SurfShop system consists of a
product database and a product ontology with node
labels. All products were indexed for fast retrieval
by the application3. A chart of application compo-
nents is presented in Figure1.
Raw product descriptions from our data would
constitute a large corpus including meta-data such
as shipping or manufacturer information, which are
not relevant to our task. Thus, fitting a topic model
over this corpus is not guaranteed to provide use-
ful information about related product types. There-
fore, we decided to aggregate the product informa-
tion into a collection of product category documents,
where each document corresponds to a node in the
product ontology tree (1088 nodes total). Each doc-
ument consists of sentences extracted from product
descriptions which potentially describe its relation-
ship to other product categories (based on the oc-
currence of category name labels). We can then use
2For an example see http://www.sccs.swarthmore.edu/users/
08/ajb/tmve/wiki100k/browse/topic-list.html.
3We used an Apache Solr index and a JavaScript Ajax-Solr
library from https://github.com/evolvingweb/ajax-solr.
Figure 1: System overview
this artificially constructed corpus as input to LDA
to discover hidden topics in the collection4.
The topic model results, as well as product ontol-
ogy information are combined with product search
in order to build pages for our SurfShop application.
In the prototype the user can move between search,
browsing related categories, as well as browsing the-
matic product groupings. In the search mode, we use
the query and the top n search results to infer which
product category is most relevant to the query. This
allows us to display links to related category groups
next to the search results.
Given a product category, the users can also ex-
plore a related category map, such as the one shown
in Figure 2 for cheese. They can browse example
products in each related category by clicking on the
category to load product information into the right
column on the page. To provide example products, a
query is issued under the relevant ontology node us-
ing the product category label and topic keywords, to
ensure that we display items relevant to the current
page. The product browsing functionality is simi-
lar for theme pages which are discussed in the next
section.
4For LDA we used the Lingpipe package (http://alias-
i.com/lingpipe/).
14
Figure 2: Related category page example. Category and
theme labels have been translated into English.
3 Theme pages
An example of a breakfast theme page view is shown
in Figure 3. It includes clusters of product categories
which exemplify the page theme, such as bread and
jam or cheese and dairy. Each theme page corre-
sponds to a hidden topic discovered by the LDA
model5. Human interpretation of topic models has
been a focus of some recent work (Chang et al,
2009; Newman et al, 2010; Mimno et al, 2010).
However, previous approaches concentrate on repre-
senting a topic by its top n most probable words. In
contrast, our goal is to illustrate a topic by choosing
the most representative documents from the collec-
tion, which also correspond to product categories as-
sociated with the topic. Since this is a novel task, we
decided to concentrate on the issue of building and
evaluating theme pages before conducting broader
user studies of the prototype.
There are a few possible ways to select documents
which best represent a topic. The simplest would be
to consider the rank of this topic in the document.
Alternatively, since the model provides an estimate
of topic probability given a document, the proba-
bility that a product category document belongs to
a topic could be calculated straightforwardly using
the Bayes rule6. Yet another option for finding cat-
5We empirically set the number of topics to 100. We re-
moved top 10% most general topics, as defined by the number
of documents which include the topic in its top 10.
6We made an additional simplifying assumption that all doc-
uments are equiprobable.
Figure 3: Theme page fragment. Category and theme
labels have been translated into English.
egories related to a given topic would be to assign
a score based on KL divergence between the topic
word multinomial and a product category multino-
mial, with the probability of each word w in the vo-
cabulary defined as follows for a given category:
P (w) =
?
t
(P (w|ti) ? P (ti|cj)) (1)
Finally, we hypothesized that product ontology
structure may be helpful in creating the theme pages,
since if one product category is representative of
the topic, its sibling categories are also likely to be.
Conversely, if a category is the only candidate for a
given topic among its neighbors in the tree, it is less
likely to be relevant. Therefore, we clustered the
topic category candidates based on their distance in
the ontology, and retained only the clusters with the
highest average scores.
To evaluate which of the above methods is more
effective, we gave the following task to a group of
three Japanese annotators. For each topic we created
a list of category candidates which included product
categories where the topic ranked 1-3 (methods 1-3
in Table 1), top 25 Bayes score and KL divergence
score categories (methods 4 and 5), as well as the
categories based on ontology distance clusters com-
bined with the Bayes score averages for cluster reli-
ability (method 6). Each annotator was given a list
of top ten keywords for each of the topics and asked
to choose a suitable label based on the keywords.
Subsequently, they were asked to select product cat-
15
Scoring method Precision Recall F-score
1.Rank1 73.83% 43.21% 54.16%
2.Rank1+2 50.91% 59.56% 54.54%
3.Rank1+2+3 41.71% 73.08% 52.77%
4.Top25 KL 53.54% 70.44% 60.45%
5.Top25 Bayes 53.56% 71.25% 60.76%
6.Bayes+Ont 66.71% 69.17% 67.48%
Table 1: Result average for three annotators on Task 1.
egories from the candidate list which fit the topic
label they decided on.
In this manner, each annotator created their own
?golden standard? of best categories which allowed
us to compare the performance of different ap-
proaches to category selection. The amount of ac-
cepted categories varied, however a performance
comparison of candidate sets showed consistent
trends across annotators, which allows us to present
averages over annotator scores in Table 1. Rank
based selection increases in recall as lower ranks
are included but the precision of the results de-
creases. KL divergence and Bayes rule based scores
are comparable. Finally, combining the ontology
information with Bayes scoring improves the pre-
cision, while retaining the recall similar to that of
the top 25 Bayes score approach. We chose this last
method to create theme pages.
We also wanted to verify how the presence of top
topic words affects topic interpretation. In another
task, shown in Table 2, the same group of annota-
tors was presented only with product category lists
which combined method 5 and method 6 candidates
from the previous task. They were asked to assign a
topic label which summarized the majority of those
categories, as well as mark the categories which did
not fit the topic. Even though the annotators had
previously seen the same data, they tended to as-
sign broader labels than those based on the top topic
words, and included more categories as suitable for
a given topic. For example, for the breakfast theme
shown in Figure 3, one annotator labeled the topic
dairy products based on topic words, and bread and
dairy products based on the product category exam-
ples. The results of Task 2 led us to use manually
assigned theme page labels based on the product cat-
egory groupings rather than the topic keywords.
Scoring method Precision Recall F-score
5.Top25 Bayes 71.28% 81.83% 76.03%
6.Bayes+Ont 84.11% 75.46% 79.38%
Table 2: Result average for three annotators on Task 2.
The differences in results between Task 1 and
Task 2 indicate that, while top topic keywords aid
interpretation, they may suggest a narrower theme
than the documents selected to represent the topic
and thus may not be optimal for some applications.
This underscores the need for further research on hu-
man evaluation methods for topic models.
4 Future work
We demonstrated a prototype SurfShop system
which employs product ontology structure and LDA
model results to link associated product types and
provide an entertaining browsing experience.
In the future we plan to replace the LDA compo-
nent with a model which can directly account for the
links found through the product ontology tree, such
a version of the relational topic model (Chang and
Blei, 2009). In addition, we hope that further explo-
ration of theme page construction can contribute to
the development of topic visualization and evalua-
tion methods.
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn. Res.
3, 993-1022.
Jonathan Chang and David Blei. 2009. Relational topic
models for document networks. Proc. of Conf. on AI
and Statistics, 81-88.
J. Chang, J. Boyd-Graber, S. Gerrish, C. Wang, and D.
Blei. 2009. Reading tea leaves: How humans interpret
topic models. NIPS, 1-9.
David Mimno, Hanna Wallach, Edmund Talley, Miriam
Leenders, and Andrew McCallum 2011. Optimizing
semantic coherence in topic models. EMNLP, 2011.
David Newman, Jey Han Lau, Karl Grieser, and Timothy
Baldwin. 2010. Automatic evaluation of topic coher-
ence. Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics, 100-108.
16
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 521?529,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Semi-supervised Relation Extraction with Large-scale Word Clustering 
 
 
Ang Sun             Ralph Grishman             Satoshi Sekine 
  
Computer Science Department 
New York University 
{asun,grishman,sekine}@cs.nyu.edu 
 
 
 
 
 
 
Abstract 
We present a simple semi-supervised 
relation extraction system with large-scale 
word clustering. We focus on 
systematically exploring the effectiveness 
of different cluster-based features. We also 
propose several statistical methods for 
selecting clusters at an appropriate level of 
granularity. When training on different 
sizes of data, our semi-supervised approach 
consistently outperformed a state-of-the-art 
supervised baseline system. 
1 Introduction 
Relation extraction is an important information 
extraction task in natural language processing 
(NLP), with many practical applications. The goal 
of relation extraction is to detect and characterize 
semantic relations between pairs of entities in text. 
For example, a relation extraction system needs to 
be able to extract an Employment relation between 
the entities US soldier and US in the phrase US 
soldier.  
Current supervised approaches for tackling this 
problem, in general, fall into two categories: 
feature based and kernel based. Given an entity 
pair and a sentence containing the pair, both 
approaches usually start with multiple level 
analyses of the sentence such as tokenization, 
partial or full syntactic parsing, and dependency 
parsing. Then the feature based method explicitly 
extracts a variety of lexical, syntactic and semantic 
features for statistical learning, either generative or 
discriminative (Miller et al, 2000; Kambhatla, 
2004; Boschee et al, 2005; Grishman et al, 2005; 
Zhou et al, 2005; Jiang and Zhai, 2007). In 
contrast, the kernel based method does not 
explicitly extract features; it designs kernel 
functions over the structured sentence 
representations (sequence, dependency or parse 
tree) to capture the similarities between different 
relation instances (Zelenko et al, 2003; Bunescu 
and Mooney, 2005a; Bunescu and Mooney, 2005b; 
Zhao and Grishman, 2005; Zhang et al, 2006; 
Zhou et al, 2007; Qian et al, 2008). Both lines of 
work depend on effective features, either explicitly 
or implicitly.  
The performance of a supervised relation 
extraction system is usually degraded by the 
sparsity of lexical features. For example, unless the 
example US soldier has previously been seen in the 
training data, it would be difficult for both the 
feature based and the kernel based systems to 
detect whether there is an Employment relation or 
not. Because the syntactic feature of the phrase US 
soldier is simply a noun-noun compound which is 
quite general, the words in it are crucial for 
extracting the relation. 
This motivates our work to use word clusters as 
additional features for relation extraction. The 
assumption is that even if the word soldier may 
never have been seen in the annotated Employment 
relation instances, other words which share the 
same cluster membership with soldier such as 
president and ambassador may have been 
observed in the Employment instances. The 
absence of lexical features can be compensated by 
521
the cluster features. Moreover, word clusters may 
implicitly correspond to different relation classes. 
For example, the cluster of president may be 
related to the Employment relation as in US 
president while the cluster of businessman may be 
related to the Affiliation relation as in US 
businessman.   
The main contributions of this paper are: we 
explore the cluster-based features in a systematic 
way and propose several statistical methods for 
selecting effective clusters.  We study the impact 
of the size of training data on cluster features and 
analyze the performance improvements through an 
extensive experimental study. 
The rest of this paper is organized as follows: 
Section 2 presents related work and Section 3 
provides the background of the relation extraction 
task and the word clustering algorithm. Section 4 
describes in detail a state-of-the-art supervised 
baseline system. Section 5 describes the cluster-
based features and the cluster selection methods. 
We present experimental results in Section 6 and 
conclude in Section 7.  
2 Related Work 
The idea of using word clusters as features in 
discriminative learning was pioneered by Miller et 
al. (2004), who augmented name tagging training 
data with hierarchical word clusters generated by 
the Brown clustering algorithm (Brown et al, 1992) 
from a large unlabeled corpus. They used different 
thresholds to cut the word hierarchy to obtain 
clusters of various granularities for feature 
decoding. Ratinov and Roth (2009) and Turian et 
al. (2010) also explored this approach for name 
tagging. Though all of them used the same 
hierarchical word clustering algorithm for the task 
of name tagging and reported improvements, we 
noticed that the clusters used by Miller et al (2004) 
were quite different from that of Ratinov and Roth 
(2009) and Turian et al (2010). To our knowledge, 
there has not been work on selecting clusters in a 
principled way. We move a step further to explore 
several methods in choosing effective clusters. A 
second difference between this work and the above 
ones is that we utilize word clusters in the task of 
relation extraction which is very different from 
sequence labeling tasks such as name tagging and 
chunking. 
Though Boschee et al (2005) and Chan and 
Roth (2010) used word clusters in relation 
extraction, they shared the same limitation as the 
above approaches in choosing clusters. For 
example, Boschee et al (2005) chose clusters of 
different granularities and Chan and Roth (2010) 
simply used a single threshold for cutting the word 
hierarchy.  Moreover, Boschee et al (2005) only 
augmented the predicate (typically a verb or a 
noun of the most importance in a relation in their 
definition) with word clusters while Chan and Roth 
(2010) performed this for any lexical feature 
consisting of a single word. In this paper, we 
systematically explore the effectiveness of adding 
word clusters to different lexical features.  
3 Background  
3.1 Relation Extraction 
One of the well defined relation extraction tasks is 
the Automatic Content Extraction1 (ACE) program 
sponsored by the U.S. government. ACE 2004 
defined 7 major entity types: PER (Person), ORG 
(Organization), FAC (Facility), GPE (Geo-Political 
Entity: countries, cities, etc.), LOC (Location), 
WEA (Weapon) and VEH (Vehicle). An entity has 
three types of mention: NAM (proper name), NOM 
(nominal) or PRO (pronoun). A relation was 
defined over a pair of entity mentions within a 
single sentence. The 7 major relation types with 
examples are shown in Table 1. ACE 2004 also 
defined 23 relation subtypes. Following most of 
the previous work, this paper only focuses on 
relation extraction of major types. 
Given a relation instance ( , , )i jx s m m?
, where 
im  and jm
 are a pair of mentions and s  is the 
sentence containing the pair, the goal is to learn a 
function which maps the instance x to a type c, 
where c is one of the 7 defined relation types or the 
type Nil (no relation exists). There are two 
commonly used learning paradigms for relation 
extraction: 
Flat: This strategy performs relation detection 
and classification at the same time. One multi-class 
classifier is trained to discriminate among the 7 
relation types plus the Nil type. 
Hierarchical: This one separates relation 
detection from relation classification. One binary 
                                                        
1 Task definition: http://www.itl.nist.gov/iad/894.01/tests/ace/ 
ACE guidelines: http://projects.ldc.upenn.edu/ace/ 
522
classifier is trained first to distinguish between 
relation instances and non-relation instances. This 
can be done by grouping all the instances of the 7 
relation types into a positive class and the instances 
of Nil into a negative class. Then the thresholded 
output of this binary classifier is used as training 
data for learning a multi-class classifier for the 7 
relation types (Bunescu and Mooney, 2005b). 
 
Type Example 
EMP-ORG US president 
PHYS a military base in Germany 
GPE-AFF U.S. businessman 
PER-SOC a spokesman for the senator 
DISC each of whom 
ART US helicopters 
OTHER-AFF Cuban-American people 
 
Table 1:  ACE relation types and examples from the 
annotation guideline 2 . The heads of the two entity 
mentions are marked. Types are listed in decreasing 
order of frequency of occurrence in the ACE corpus. 
3.2 Brown Word Clustering 
The Brown algorithm is a hierarchical clustering 
algorithm which initially assigns each word to its 
own cluster and then repeatedly merges the two 
clusters which cause the least loss in average 
mutual information between adjacent clusters 
based on bigram statistics.  By tracing the pairwise 
merging steps, one can obtain a word hierarchy 
which can be represented as a binary tree. A word 
can be compactly represented as a bit string by 
following the path from the root to itself in the tree, 
assigning a 0 for each left branch, and a 1 for each 
right branch. A cluster is just a branch of that tree. 
A high branch may correspond to more general 
concepts while the lower branches it includes 
might correspond to more specific ones.  
Brown et al (1992) described an efficient 
implementation based on a greedy algorithm which 
initially assigned only the most frequent words into 
distinct clusters. It is worth pointing out that in this 
implementation each word occupies a leaf in the 
hierarchy, but each leaf might contain more than 
one word as can be seen from Table 2. The lengths 
of the bit strings also vary among different words. 
 
 
                                                        
2 http://projects.ldc.upenn.edu/ace/docs/EnglishRDCV4-3-
2.PDF 
Bit string Examples 
111011011100 US ? 
1110110111011 U.S. ? 
1110110110000 American ? 
1110110111110110 Cuban, Pakistani, Russian ?  
11111110010111 Germany, Poland, Greece ?  
110111110100 businessman, journalist, reporter 
1101111101111 president, governor, premier?  
1101111101100    senator, soldier, ambassador ? 
11011101110 spokesman, spokeswoman, ? 
11001100 people, persons, miners, Haitians 
110110111011111 base, compound, camps, camp ? 
110010111 helicopters, tanks, Marines ? 
 
Table 2: An example of words and their bit string 
representations obtained in this paper. Words in bold are 
head words that appeared in Table 1. 
4 Feature Based Relation Extraction 
Given a pair of entity mentions ,i jm m? ?
and the 
sentence containing the pair, a feature based 
system extracts a feature vector v  which contains 
diverse lexical, syntactic and semantic features. 
The goal is to learn a function which can estimate 
the conditional probability ( | )p c v , the probability 
of a relation type c given the feature vector v . The 
type with the highest probability will be output as 
the class label for the mention pair.  
We now describe a supervised baseline system 
with a very large set of features and its learning 
strategy.  
4.1 Baseline Feature Set 
We first adopted the full feature set from Zhou et 
al. (2005), a state-of-the-art feature based relation 
extraction system. For space reasons, we only 
show the lexical features as in Table 3 and refer the 
reader to the paper for the rest of the features.  
At the lexical level, a relation instance can be 
seen as a sequence of tokens which form a five 
tuple <Before, M1, Between, M2, After>. Tokens 
of the five members and the interaction between 
the heads of the two mentions can be extracted as 
features as shown in Table 3. 
In addition, we cherry-picked the following 
features which were not included in Zhou et al 
(2005) but were shown to be quite effective for 
relation extraction. 
Bigram of the words between the two mentions: 
This was extracted by both Zhao and Grishman 
(2005) and Jiang and Zhai (2007), aiming to 
523
provide more order information of the tokens 
between the two mentions. 
Patterns:  There are three types of patterns: 1) 
the sequence of the tokens between the two 
mentions as used in Boschee et al (2005); 2) the 
sequence of the heads of the constituents between 
the two mentions as used by Grishman et al (2005); 
3) the shortest dependency path between the two 
mentions in a dependency tree as adopted by 
Bunescu and Mooney (2005a). These patterns can 
provide more structured information of how the 
two mentions are connected.  
Title list: This is tailored for the EMP-ORG type 
of relations as the head of one of the mentions is 
usually a title. The features are decoded in a way 
similar to that of Sun (2009).  
 
Position Feature Description 
Before BM1F first word before M1 
BM1L second word before M1 
M1 WM1 bag-of-words in M1 
HM1 head3 word of M1 
Between WBNULL when no word in between 
WBFL the only word in between when 
only one word in between 
WBF first word in between when at 
least two words in between 
WBL last word in between when at 
least two words in between 
WBO other words in between except 
first and last words when at 
least three words in between 
M2 WM2 bag-of-words in M2 
HM2 head word of M2 
M12 HM12 combination of HM1 and HM2 
After AM2F  first word after M2 
AM2L  second word after M2 
 
Table 3: Lexical features for relation extraction. 
4.2 Baseline Learning Strategy 
We employ a simple learning framework that is 
similar to the hierarchical learning strategy as 
described in Section 3.1. Specifically, we first train 
a binary classifier to distinguish between relation 
instances and non-relation instances. Then rather 
than using the thresholded output of this binary 
classifier as training data, we use only the 
annotated relation instances to train a multi-class 
classifier for the 7 relation types. In the test phase, 
                                                        
3 The head word of a mention is normally set as the last word 
of the mention as in Zhou et al (2005). 
given a test instance x , we first apply the binary 
classifier to it for relation detection; if it is detected 
as a relation instance we then apply the multi-class 
relation classifier to classify it4. 
5 Cluster Feature Selection 
The selection of cluster features aims to answer the 
following two questions: which lexical features 
should be augmented with word clusters to 
improve generalization accuracy? How to select 
clusters at an appropriate level of granularity? We 
will describe our solutions in Section 5.1 and 5.2. 
5.1 Cluster Feature Decoding 
While each one of the lexical features in Table 3 
used by the baseline can potentially be augmented 
with word clusters, we believe the effectiveness of 
a lexical feature with augmentation of word 
clusters should be tested either individually or 
incrementally according to a rank of its importance 
as shown in Table 4. We will show the 
effectiveness of each cluster feature in the 
experiment section. 
 
Impor- 
tance 
Lexical 
Feature 
Description of 
lexical feature 
Cluster Feature 
1 HM HM1, HM2 and 
HM12 
HM1_WC, 
HM2_WC, 
HM12_WC 
2 BagWM WM1 and WM2 BagWM_WC 
3 HC a head5 of a chunk 
in context 
HC_WC 
4 BagWC word of context BagWC_WC 
 
Table 4: Cluster features ordered by importance. 
 
The importance is based on linguistic intuitions 
and observations of the contributions of different 
lexical features from various feature based systems. 
Table 4 simplifies a relation instance as a three 
tuple <Context, M1, M2> where the Context 
includes the Before, Between and After from the 
                                                        
4 Both the binary and multi-class classifiers output normalized 
probabilities in the range [0,1]. When the binary classifier?s 
prediction probability is greater than 0.5, we take the 
prediction with the highest probability of the multi-class 
classifier as the final class label. When it is in the range 
[0.3,0.5], we only consider as the final class label the 
prediction of the multi-class classifier with a probability which 
is greater than 0.9. All other cases are taken as non-relation 
instances. 
5 The head of a chunk is defined as the last word in the chunk. 
524
five tuple representation. As a relation in ACE is 
usually short, the words of the two entity mentions 
can provide more critical indications for relation 
classification than the words from the context. 
Within the two entity mentions, the head word of 
each mention is usually more important than other 
words of the mention; the conjunction of the two 
heads can provide an additional clue. And in 
general words other than the chunk head in the 
context do not contribute to establishing a 
relationship between the two entity mentions. 
The cluster based semi-supervised system works 
by adding an additional layer of lexical features 
that incorporate word clusters as shown in column 
4 of Table 4. Take the US soldier as an example, if 
we decide to use a length of 10 as a threshold to 
cut the Brown word hierarchy to generate word 
clusters, we will extract a cluster feature 
HM1_WC10=1101111101 in addition to the 
lexical feature HM1=soldier given that the full bit 
string of soldier is  1101111101100 in Table 2. 
(Note that the cluster feature is a nominal feature, 
not to be confused with an integer feature.) 
5.2 Selection of Clusters 
Given the bit string representations of all the words 
in a vocabulary, researchers usually use prefixes of 
different lengths of the bit strings to produce word 
clusters of various granularities. However, how to 
choose the set of prefix lengths in a principled way? 
This has not been answered by prior work. 
Our main idea is to learn the best set of prefix 
lengths, perhaps through the validation of their 
effectiveness on a development set of data. To our 
knowledge, previous research simply uses ad-hoc 
prefix lengths and lacks this training procedure. 
The training procedure can be extremely slow for 
reasons to be explained below. 
Formally, let l  be the set of available prefix 
lengths ranging from 1 bit to the length of the 
longest bit string in the Brown word hierarchy and 
let m  be the set of prefix lengths we want to use in 
decoding cluster features, then the problem of 
selecting effective clusters transforms to finding a 
| |m -combination of the set l which maximizes 
system performance. The training procedure can be 
extremely time consuming if we enumerate every 
possible | |m -combination of l , given that | |m  
can range from 1 to the size of l and the size of 
l equals the length of the longest bit string which is 
usually 20 when inducing 1,000 clusters using the 
Brown algorithm.                                  
One way to achieve better efficiency is to 
consider only a subset of l instead of the full set. In 
addition, we limit ourselves to use sizes 3 and 4 for 
m  for matching prior work. This keeps the cluster 
features to a manageable size considering that 
every word in your vocabulary could contribute to 
a lexical feature. For picking a subset of l , we 
propose below two statistical measures for 
computing the importance of a certain prefix 
length. 
Information Gain (IG): IG measures the 
quality or importance of a feature f by computing 
the difference between the prior entropy of classes 
C and the posterior entropy, given values V of the 
feature f (Hunt et al, 1966; Quinlan, 1986). For 
our purpose, C is the set of relation types, f is a 
cluster-based feature with a certain prefix length 
such as HM1_WC* where * means the prefix 
length and a value v is the prefix of the bit string 
representation of HM1. More formally, the IG of f 
is computed as follows: 
( ) ( ) log ( )
( ( ) ( | ) log ( | ))
c C
v V c C
IG f p c p c
p v p c v p c v
?
? ?
? ? ?
? ?
?
? ?
        (1) 
where the first and second terms refer to the prior 
and posterior entropies respectively. 
For each prefix length in the set l , we can 
compute its IG for a type of cluster feature and 
then rank the prefix lengths based on their IGs for 
that cluster feature. For simplicity, we rank the 
prefix lengths for a group of cluster features (a 
group is a row from column 4 in Table 4) by 
collapsing the individual cluster features into a 
single cluster feature. For example, we collapse the 
3 types: HM1_WC, HM2_WC and HM12_WC into 
a single type HM_WC for computing the IG.  
Prefix Coverage (PC): If we use a short prefix 
then the clusters produced correspond to the high 
branches in the word hierarchy and would be very 
general. The cluster features may not provide more 
informative information than the words themselves. 
Similarly, if we use a long prefix such as the length 
of the longest bit string, then maybe only a few of 
the lexical features can be covered by clusters. To 
capture this intuition, we define the PC of a prefix 
length i as below: 
525
( )( ) ( )
ic
l
count fPC i count f?
                        (2) 
where 
lf  stands for a lexical feature such as HM1 
and
icf
 a cluster feature with prefix length i such as 
HM1_WCi, (*)count  is the number of 
occurrences of that feature in training data. 
Similar to IG, we compute PC for a group of 
cluster features, not for each individual feature. 
In our experiments, the top 10 ranked prefix 
lengths based on IG and prefix lengths with PC 
values in the range [0.4, 0.9] were used. 
In addition to the above two statistical measures, 
for comparison, we introduce another two simple 
but extreme measures for the selection of clusters. 
Use All Prefixes (UA): UA produces a cluster 
feature at every available bit length with the hope 
that the underlying supervised system can learn 
proper weights of different cluster features during 
training. For example, if the full bit representation 
of ?Apple? is ?000?, UA would produce three 
cluster features: prefix1=0, prefix2=00 and 
prefix3=000. Because this method does not need 
validation on the development set, it is the laziest 
but the fastest method for selecting clusters.  
Exhaustive Search (ES): ES works by trying 
every possible combination of the set l and picking 
the one that works the best for the development set. 
This is the most cautious and the slowest method 
for selecting clusters. 
6 Experiments 
In this section, we first present details of our 
unsupervised word clusters, the relation extraction 
data set and its preprocessing. We then present a 
series of experiments coupled with result analyses. 
We used the English portion of the TDT5 
corpora (LDC2006T18) as our unlabeled data for 
inducing word clusters. It contains roughly 83 
million words in 3.4 million sentences with a 
vocabulary size of 450K. We left case intact in the 
corpora. Following previous work, we used 
Liang?s implementation of the Brown clustering 
algorithm (Liang, 2005).  We induced 1,000 word 
clusters for words that appeared at least twice in 
the corpora. The reduced vocabulary contains 
255K unique words. The clusters are available at 
http://www.cs.nyu.edu/~asun/data/TDT5_BrownW
C.tar.gz. 
For relation extraction, we used the benchmark 
ACE 2004 training data. Following most of the 
previous research, we used in experiments the 
nwire (newswire) and bnews (broadcast news) 
genres of the data containing 348 documents and 
4374 relation instances. We extracted an instance 
for every pair of mentions in the same sentence 
which were separated by no more than two other 
mentions. The non-relation instances generated 
were about 8 times more than the relation instances.  
Preprocessing of the ACE documents: We used 
the Stanford parser6 for syntactic and dependency 
parsing. We used chunklink7  to derive chunking 
information from the Stanford parsing. Because 
some bnews documents are in lower case, we 
recover the case for the head of a mention if its 
type is NAM by making the first character into its 
upper case. This is for better matching between the 
words in ACE and the words in the unsupervised 
word clusters. 
We used the OpenNLP 8  maximum entropy 
(maxent) package as our machine learning tool. 
We choose to work with maxent because the 
training is fast and it has a good support for multi-
class classification. 
6.1 Baseline Performance 
Following previous work, we did 5-fold cross-
validation on the 348 documents with hand-
annotated entity mentions. Our results are shown in 
Table 5 which also lists the results of another three 
state-of-the-art feature based systems. For this and 
the following experiments, all the results were 
computed at the relation mention level. 
 
System P(%) R(%) F(%) 
Zhou et al (2007)9 78.2 63.4 70.1 
Zhao and Grishman (2005)10 69.2 71.5 70.4 
Our Baseline 73.4 67.7 70.4 
Jiang and Zhai (2007) 11 72.4 70.2 71.3 
 
Table 5: Performance comparison on the ACE 2004 
data over the 7 relation types. 
                                                        
6 http://nlp.stanford.edu/software/lex-parser.shtml 
7 http://ilk.uvt.nl/team/sabine/chunklink/README.html 
8 http://opennlp.sourceforge.net/ 
9 Zhou et al (2005) tested their system on the ACE 2003 data; 
Zhou et al (2007) tested their system on the ACE 2004 data. 
10  The paper gives a recall value of 70.5, which is not 
consistent with the given values of P and F. An examination of 
the correspondence in preparing this paper indicates that the 
correct recall value is 71.5. 
11 The result is from using the All features in Jiang and Zhai 
(2007). It is not quite clear from the paper that whether they 
used the 348 documents or the whole 2004 training data. 
526
Note that although all the 4 systems did 5-fold 
cross-validation on the ACE 2004 data, the 
detailed data partition might be different. Also, we 
were doing cross-validation at the document level 
which we believe was more natural than the 
instance level. Nonetheless, we believe our 
baseline system has achieved very competitive 
performance. 
6.2 The Effectiveness of Cluster Selection 
Methods 
We investigated the tradeoff between performance 
and training time of each proposed method in 
selecting clusters. In this experiment, we randomly 
selected 70 documents from the 348 documents as 
test data which roughly equaled the size of 1 fold 
in the baseline in Section 6.1. For the baseline in 
this section, all the rest of the documents were used 
as training data. For the semi-supervised system, 
70 percent of the rest of the documents were 
randomly selected as training data and 30 percent 
as development data. The set of prefix lengths that 
worked the best for the development set was 
chosen to select clusters. We only used the cluster 
feature HM_WC in this experiment.  
 
System F ? Training  Time (in minute) 
Baseline 70.70  1 
UA 71.19 +0.49 1.5 
PC3 71.65 +0.95 30 
PC4 71.72 +1.02 46 
IG3 71.65 +0.95 45 
IG4 71.68 +0.98 78 
ES3 71.66 +0.96 465 
ES4 71.60 +0.90 1678 
 
Table 6: The tradeoff between performance and training 
time of each method in selecting clusters. PC3 means 
using 3 prefixes with the PC method. ? in this paper 
means the difference between a system and the baseline. 
 
Table 6 shows that all the 4 proposed methods 
improved baseline performance, with UA as the 
fastest and ES as the slowest. It was interesting that 
ES did not always outperform the two statistical 
methods which might be because of its overfitting 
to the development set. In general, both PC and IG 
had good balances between performance and 
training time. There was no dramatic difference in 
performance between using 3 and 4 prefix lengths.  
For the rest of this paper, we will only use PC4 
as our method in selecting clusters. 
6.3 The Effectiveness of Cluster Features 
The baseline here is the same one used in Section 
6.1. For the semi-supervised system, each test fold 
was the same one used in the baseline and the other 
4 folds were further split into a training set and a 
development set in a ratio of 7:3 for selecting 
clusters. We first added the cluster features 
individually into the baseline and then added them 
incrementally according to the order specified in 
Table 4. 
 
System F ? 
1 Baseline 70.4  
2 1 + HM_WC 71.5 + 1.1 
3 1 + BagWM_WC 71.0 + 0.6 
4 1 + HC_WC 69.6 - 0.8 
5 1 + BagWC_WC 46.1 - 24.3 
6 2 + BagWM_WC 71.0 + 0.6 
7 6 + HC_WC 70.6 + 0.2 
8 7+ BagWC_WC 50.3 - 20.1 
 
Table 7: Performance 12  of the baseline and using 
different cluster features with PC4 over the 7 types.  
 
We found that adding clusters to the heads of the 
two mentions was the most effective way of 
introducing cluster features. Adding clusters to the 
words of the mentions can also help, though not as 
good as the heads. We were surprised that the 
heads of chunks in context did not help. This might 
be because ACE relations are usually short and the 
limited number of long relations is not sufficient in 
generalizing cluster features. Adding clusters to 
every word in context hurt the performance a lot. 
Because of the behavior of each individual feature, 
it was not surprising that adding them 
incrementally did not give more performance gain.  
For the rest of this paper, we will only use 
HM_WC as cluster features. 
6.4 The Impact of Training Size 
We studied the impact of training data size on 
cluster features as shown in Table 8. The test data 
was always the same as the 5-fold used in the 
baseline in Section 6.1. no matter the size of the 
training data. The training documents for the  
                                                        
12  All the improvements of F in Table 7, 8 and 9 were 
significant at confidence levels >= 95%. 
527
# docs F of Relation Classification F of Relation Detection 
Baseline PC4 (?) Prefix10(?) Baseline PC4(?) Prefix10(?) 
50 62.9 63.8(+ 0.9) 63.7(+0.8) 71.4 71.9(+ 0.5) 71.6(+0.2) 
75 62.8 64.6(+ 1.8) 63.9(+1.1) 71.5 72.3(+ 0.8) 72.5(+1.0) 
125 66.1 68.1(+ 2.0) 67.5(+1.4) 74.5 74.8(+ 0.3) 74.3(-0.2) 
175 67.8 69.7(+ 1.9) 69.5(+1.7) 75.2 75.5(+ 0.3) 75.2(0.0) 
225 68.9 70.1(+ 1.2) 69.6(+0.7) 75.6 75.9(+ 0.3) 75.3(-0.3) 
?280 70.4 71.5(+ 1.1) 70.7(+0.3) 76.4 76.9(+ 0.5) 76.3(-0.1) 
 
Table 8: Performance over the 7 relation types with different sizes of training data. Prefix10 uses the single prefix 
length 10 to generate word clusters as used by Chan and Roth (2010). 
 
Type P R F 
Baseline PC4 (?) Baseline PC4 (?) Baseline PC4 (?) 
EMP-ORG 75.4 77.2(+1.8) 79.8 81.5(+1.7) 77.6 79.3(+1.7) 
PHYS 73.2 71.2(-2.0) 61.6 60.2(-1.4) 66.9 65.3(-1.7) 
GPE-AFF 67.1 69.0(+1.9) 60.0 63.2(+3.2) 63.3 65.9(+2.6) 
PER-SOC 88.2 83.9(-4.3) 58.4 61.0(+2.6) 70.3 70.7(+0.4) 
DISC 79.4 80.6(+1.2) 42.9 46.0(+3.2) 55.7 58.6(+2.9) 
ART 87.9 96.9(+9.0) 63.0 67.4(+4.4) 73.4 79.3(+5.9) 
OTHER-AFF 70.6 80.0(+9.4) 41.4 41.4(0.0) 52.2 54.6(+2.4) 
 
Table 9: Performance of each individual relation type based on 5-fold cross-validation. 
 
current size setup were randomly selected and 
added to the previous size setup (if applicable). For 
example, we randomly selected another 25 
documents and added them to the previous 50 
documents to get 75 documents. We made sure 
that every document participated in this experiment. 
The training documents for each size setup were 
split into a real training set and a development set 
in a ratio of 7:3 for selecting clusters.  
There are some clear trends in Table 8. Under 
each training size, PC4 consistently outperformed 
the baseline and the system Prefix10 for relation 
classification. For PC4, the gain for classification 
was more pronounced than detection. The mixed 
detection results of Prefix10 indicated that only 
using a single prefix may not be stable.   
We did not observe the same trend in the 
reduction of annotation need with cluster-based 
features as in Koo et al (2008) for dependency 
parsing. PC4 with sizes 50, 125, 175 outperformed 
the baseline with sizes 75, 175, 225 respectively. 
But this was not the case when PC4 was tested 
with sizes 75 and 225.  This might due to the 
complexity of the relation extraction task. 
6.5 Analysis 
There were on average 69 cross-type errors in the 
baseline in Section 6.1 which were reduced to 56 
by using PC4. Table 9 showed that most of the 
improvements involved EMP-ORG, GPE-AFF, 
DISC, ART and OTHER-AFF. The performance 
gain for PER-SOC was not as pronounced as the 
other five types. The five types of relations are 
ambiguous as they share the same entity type GPE 
while the PER-SOC relation only holds between 
PER and PER. This reflects that word clusters can 
help to distinguish between ambiguous relation 
types. 
As mentioned earlier the gain of relation 
detection was not as pronounced as classification 
as shown in Table 8. The unbalanced distribution 
of relation instances and non-relation instances 
remains as an obstacle for pushing the performance 
of relation extraction to the next level. 
7 Conclusion and Future Work 
We have described a semi-supervised relation 
extraction system with large-scale word clustering. 
We have systematically explored the effectiveness 
of different cluster-based features. We have also 
demonstrated that the two proposed statistical 
methods are both effective and efficient in 
selecting clusters at an appropriate level of 
granularity through an extensive experimental 
study. 
528
Based on the experimental results, we plan to 
investigate additional ways to improve the 
performance of relation detection. Moreover, 
extending word clustering to phrase clustering (Lin 
and Wu, 2009) and pattern clustering (Sun and 
Grishman, 2010) is worth future investigation for 
relation extraction. 
References  
Rie K. Ando and Tong Zhang. 2005 A Framework for 
Learning Predictive Structures from Multiple Tasks 
and Unlabeled Data. Journal of Machine Learning 
Research, Vol 6:1817-1853. 
Elizabeth Boschee, Ralph Weischedel, and Alex 
Zamanian. 2005. Automatic information extraction. 
In Proceedings of the International Conference on 
Intelligence Analysis. 
Peter F. Brown, Vincent J. Della Pietra, Peter V. 
deSouza,  Jenifer C. Lai, and Robert L. Mercer. 1992. 
Class-based n-gram models of natural language. 
Computational Linguistics, 18(4):467?479. 
Razvan C. Bunescu and Raymond J. Mooney. 2005a. A 
shortest path dependency kenrel for relation 
extraction. In Proceedings of HLT/EMNLP. 
Razvan C. Bunescu and Raymond J. Mooney. 2005b. 
Subsequence kernels for relation extraction. In 
Proceedings of NIPS. 
Yee Seng Chan and Dan Roth. 2010. Exploiting 
background knowledge for relation extraction. In 
Proc. of COLING. 
Ralph Grishman, David Westbrook and Adam Meyers. 
2005. NYU?s English ACE 2005 System Description. 
ACE 2005 Evaluation Workshop.  
Earl B. Hunt, Philip J. Stone and Janet Marin. 1966. 
Experiments in Induction. New York: Academic 
Press, 1966. 
Jing Jiang and ChengXiang Zhai. 2007. A systematic 
exploration of the feature space for relation 
extraction. In Proceedings of HLT-NAACL-07.  
Nanda Kambhatla. 2004. Combining lexical, syntactic, 
and semantic features with maximum entropy models 
for information extraction. In Proceedings of ACL-04. 
Terry Koo, Xavier Carreras, and Michael Collins. 2008. 
Simple Semi-supervised Dependency Parsing. In 
Proceedings of ACL-08: HLT. 
Percy Liang. 2005. Semi-Supervised Learning for 
Natural Language. Master?s thesis, Massachusetts 
Institute of Technology. 
Dekang Lin and Xiaoyun Wu. 2009. Phrase Clustering 
for Discriminative Learning. In Proc. of ACL-09. 
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph 
Weischedel. 2000. A novel use of statistical parsing 
to extract information from text. In Proc. of NAACL. 
Scott Miller, Jethran Guinness and Alex Zamanian. 
2004. Name Tagging with Word Clusters and 
Discriminative Training. In Proc. of HLT-NAACL. 
Longhua Qian, Guodong Zhou, Qiaoming Zhu and 
Peide Qian. 2008. Exploiting constituent 
dependencies for tree kernel-based semantic relation 
extraction . In Proc. of COLING. 
John Ross Quinlan. 1986. Induction of decision trees. 
Machine Learning, 1(1), 81-106. 
Lev Ratinov and Dan Roth. 2009. Design challenges 
and misconceptions in named entity recognition. In 
Proceedings of CoNLL-09. 
Ang Sun. 2009. A Two-stage Bootstrapping Algorithm 
for Relation Extraction. In RANLP-09. 
Ang Sun and Ralph Grishman. 2010. Semi-supervised 
Semantic Pattern Discovery with Guidance from 
Unsupervised Pattern Clusters. In Proc. of COLING. 
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 
2010. Word representations: A simple and general 
method for semi-supervised learning. In Proceedings 
of ACL. 
Dmitry Zelenko, Chinatsu Aone, and Anthony 
Richardella. 2003. Kernel methods for relation 
extraction. Journal of Machine Learning Research, 
3:1083?1106. 
Zhu Zhang. 2004. Weakly supervised relation 
classification for information extraction. In Proc. of 
CIKM?2004. 
Min Zhang, Jie Zhang, Jian Su, and GuoDong Zhou. 
2006. A composite kernel to extract relations 
between entities with both flat and structured features. 
In Proceedings of COLING-ACL-06. 
Shubin Zhao and Ralph Grishman. 2005. Extracting 
relations with integrated information using kernel 
methods. In Proceedings of ACL. 
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang. 
2005. Exploring various knowledge in relation 
extraction. In Proceedings of ACL-05. 
Guodong Zhou, Min Zhang, DongHong Ji, and 
QiaoMing Zhu. 2007. Tree kernel-based relation 
extraction with context-sensitive structured parse tree 
information. In Proceedings of EMNLPCoNLL-07. 
529
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 53?57,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Latent Class Transliteration based on Source Language Origin
Masato Hagiwara
Rakuten Institute of Technology, New York
215 Park Avenue South, New York, NY
masato.hagiwara@mail.rakuten.com
Satoshi Sekine
Rakuten Institute of Technology, New York
215 Park Avenue South, New York, NY
satoshi.b.sekine@mail.rakuten.com
Abstract
Transliteration, a rich source of proper noun
spelling variations, is usually recognized by
phonetic- or spelling-based models. How-
ever, a single model cannot deal with dif-
ferent words from different language origins,
e.g., ?get? in ?piaget? and ?target.? Li et
al. (2007) propose a method which explicitly
models and classifies the source language ori-
gins and switches transliteration models ac-
cordingly. This model, however, requires an
explicitly tagged training set with language
origins. We propose a novel method which
models language origins as latent classes. The
parameters are learned from a set of translit-
erated word pairs via the EM algorithm. The
experimental results of the transliteration task
of Western names to Japanese show that the
proposed model can achieve higher accuracy
compared to the conventional models without
latent classes.
1 Introduction
Transliteration (e.g., ??????? baraku obama /
Barak Obama?) is phonetic translation between lan-
guages with different writing systems. Words are
often transliterated when imported into differet lan-
guages, which is a major cause of spelling variations
of proper nouns in Japanese and many other lan-
guages. Accurate transliteration is also the key to
robust machine translation systems.
Phonetic-based rewriting models (Knight and
Jonathan, 1998) and spelling-based supervised mod-
els (Brill and Moore, 2000) have been proposed for
recognizing word-to-word transliteration correspon-
dence. These methods usually learn a single model
given a training set. However, single models cannot
deal with words from multiple language origins. For
example, the ?get? parts in ?piaget /???? piaje?
(French origin) and ?target / ????? ta?getto?
(English origin) may differ in how they are translit-
erated depending on their origins.
Li et al (2007) tackled this issue by proposing a
class transliteration model, which explicitly models
and classifies origins such as language and genders,
and switches corresponding transliteration model.
This method requires training sets of transliterated
word pairs with language origin. However, it is diffi-
cult to obtain such tagged data, especially for proper
nouns, a rich source of transliterated words. In ad-
dition, the explicitly tagged language origins are not
necessarily helpful for loanwords. For example, the
word ?spaghetti? (Italian origin) can also be found
in an English dictionary, but applying an English
model can lead to unwanted results.
In this paper, we propose a latent class transliter-
ation model, which models the source language ori-
gin as unobservable latent classes and applies appro-
priate transliteration models to given transliteration
pairs. The model parameters are learned via the EM
algorithm from training sets of transliterated pairs.
We expect that, for example, a latent class which is
mostly occupied by Italian words would be assigned
to ?spaghetti /?????supageti? and the pair will
be correctly recognized.
In the evaluation experiments, we evaluated the
accuracy in estimating a corresponding Japanese
transliteration given an unknown foreign word,
53
flextime
furekkusutaimu
s:
t:
?i
?i
Figure 1: Minimum edit operation sequence in the alpha-
beta model (Underlined letters are match operations)
using lists of Western names with mixed lan-
guages. The results showed that the proposed model
achieves higher accuracy than conventional models
without latent classes.
Related researches include Llitjos and Black
(2001), where it is shown that source language ori-
gins may improve the pronunciation of proper nouns
in text-to-speech systems. Another one by Ahmad
and Kondrak (2005) estimates character-based error
probabilities from query logs via the EM algorithm.
This model is less general than ours because it only
deals with character-based error probability.
2 Alpha-Beta Model
We adopted the alpha-beta model (Brill and Moore,
2000), which directly models the string substitu-
tion probabilities of transliterated pairs, as the base
model in this paper. This model is an extension to
the conventional edit distance, and gives probabil-
ities to general string substitutions in the form of
? ? ? (?, ? are strings of any length). The whole
probability of rewriting word s with t is given by:
PAB(t|s) = max
T?Part(t),S?Part(s)
|S|
?
i=1
P (?i ? ?i), (1)
where Part(x) is all the possible partitions of word
x. Taking logarithm and regarding ? logP (? ? ?)
as the substitution cost of ? ? ?, this maximiza-
tion is equivalent to finding a minimum of total sub-
stitution costs, which can be solved by normal dy-
namic programming (DP). In practice, we condi-
tioned P (? ? ?) by the position of ? in words,
i.e., at the beginning, in the middle, or at the end of
the word. This conditioning is simply omitted in the
equations in this paper.
The substitution probabilities P (? ? ?) are
learned from transliterated pairs. Firstly, we obtain
an edit operation sequence using the normal DP for
edit distance computation. In Figure 1 the sequence
is f?f, ? ?u, l?r, e?e,??k, x?k, ... and so on.
Secondly, non-match operations are merged with ad-
jacent edit operations, with the maximum length of
substitution pairs limited to W . When W = 2,
for example, the first non-match operation ? ?u is
merged with one operation on the left and right, pro-
ducing f?fu and l?ur. Finally, substitution prob-
abilities are calculated as relative frequencies of all
substitution operations created in this way. Note that
the minimum edit operation sequence is not unique,
so we take the averaged frequencies of all the possi-
ble minimum sequences.
3 Class Transliteration Model
The alpha-beta model showed better performance in
tasks such as spelling correction (Brill and Moore,
2000), transliteration (Brill et al, 2001), and query
alteration (Hagiwara and Suzuki, 2009). However,
the substitution probabilities learned by this model
are simply the monolithic average of training set
statistics, and cannot be switched depending on the
source language origin of given pairs, as explained
in Section 1.
Li et al (2007) pointed out that similar problems
arise in Chinese. Transliteration of Indo-European
names such as ????? / Alexandra? can be ad-
dressed by Mandarin pronunciation (Pinyin) ?Ya-Li-
Shan-Da,? while Japanese names such as ??? /
Yamamoto? can only be addressed by considering
the Japanese pronunciation, not the Chinese pro-
nunciation ?Shan-Ben.? Therefore, Li et al took
into consideration two additional factors, i.e., source
language origin l and gender / first / last names g,
and proposed a model which linearly combines the
conditioned probabilities P (t|s, l, g) to obtain the
transliteration probability of s ? t as:
P (t|s)soft =
?
l,g
P (t, l, g|s)
=
?
l,g
P (t|s, l, g)P (l, g|s) (2)
We call the factors c = (l, g) as classes in this paper.
This model can be interpreted as firstly computing
54
the class probability distribution given P (c|s) then
taking a weighted sum of P (t|s, c) with regard to
the estimated class c and the target t.
Note that this weighted sum can be regarded
as doing soft-clustering of the input s into classes
with probabilities. Alternatively, we can employ
hard-clustering by taking one class such that c? =
argmaxl,g P (l, g|s) and compute the transliteration
probability by:
P (t|s)hard ? P (t|s, c?). (3)
4 Latent Class Transliteration Model
The model explained in the previous section inte-
grates different transliteration models for words with
different language origins, but it requires us to build
class detection model c from training pairs explicitly
tagged with language origins.
Instead of assigning an explicit class c to each
transliterated pair, we can introduce a random vari-
able z and consider a conditioned string substitution
probability P (? ? ?|z). This latent class z cor-
responds to the classes of transliterated pairs which
share the same transliteration characteristics, such as
language origins and genders. Although z is not di-
rectly observable from sets of transliterated words,
we can compute it via EM algorithm so that it max-
imizes the training set likelihood as shown below.
Due to the space limitation, we only show the up-
date equations. Xtrain is the training set consisting
of transliterated pairs {(sn, tn)|1 ? n ? N}, N is
the number of training pairs, and K is the number of
latent classes.
Parameters: P (z = k) = pik, P (? ? ?|z)
(4)
E-Step: ?nk =
pikP (tn|sn, z = k)
?K
k=1 pikP (tn|sn, z = k)
, (5)
P (tn|sn, z) = max
T?Part(tn),S?Part(sn)
|S|
?
i=1
P (?i ? ?i|z)
M-Step: pi?k =
Nk
N
, Nk =
N
?
n=1
?nk (6)
P (? ? ?|z = k)? = 1
Nk
N
?
n=1
?nk
fn(? ? ?)
?
??? fn(? ? ?)
Here, fn(? ? ?) is the frequency of substitution
pair ? ? ? in the n-th transliterated pair, whose
calculation method is explained in Section 2. The
final transliteration probability is given by:
Platent(t|s) =
?
z
P (t, z|s) =
?
z
P (z|s)P (t|s, z)
?
?
z
pikP (s|z)P (t|s, z) (7)
The proposed model cannot explicitly model
P (s|z), which is in practice approximated by
P (t|s, z). Even omitting this factor only has a
marginal effect on the performance (within 1.1%).
5 Experiments
Here we evaluate the performance of the transliter-
ation models as an information retrieval task, where
the model ranks target t? for a given source s?, based
on the model P (t?|s?). We used all the t?n in the
test set Xtest = {(s?n, t?n)|1 ? n ? M} as target
candidates and s?n for queries. Five-fold cross vali-
dation was adopted when learning the models, that
is, the datasets described in the next subsections are
equally splitted into five folds, of which four were
used for training and one for testing. The mean re-
ciprocal rank (MRR) of top 10 ranked candidates
was used as a performance measure.
5.1 Experimental Settings
Dataset 1: Western Person Name List This
dataset contains 6,717 Western person names and
their Katakana readings taken from an European
name website ?????? 1, consisting of Ger-
man (de), English (en), and French (fr) person name
pairs. The numbers of pairs for these languages are
2,470, 2,492, and 1,747, respectively. Accent marks
for non-English languages were left untouched. Up-
percase was normalized to lowercase.
Dataset 2: Western Proper Noun List This
dataset contains 11,323 proper nouns and their
Japanese counterparts extracted from Wikipedia in-
terwiki. The languages and numbers of pairs con-
tained are: German (de): 2,003, English (en): 5,530,
Spanish (es): 781, French (fr): 1,918, Italian (it):
1http://www.worldsys.org/europe/
55
Language de en fr
Precision(%) 80.4 77.1 74.7
Table 1: Language Class Detection Result (Dataset 1)
1,091. Linked English and Japanese titles are ex-
tracted, unless the Japanese title contains any other
characters than Katakana, hyphen, or middle dot.
The language origin of titles were detected
whether appropriate country names are included in
the first sentence of Japanese articles. If they con-
tain ????? (of Germany),? ?????? (of
France),? ?????? (of Italy),? they are marked
as German, French, and Italian origin, respectively.
If the sentence contains any of Spain, Argentina,
Mexico, Peru, or Chile plus ???(of), it is marked
as Spanish origin. If they contain any of Amer-
ica, England, Australia or Canada plus ???(of), it
is marked as English origin. The latter parts of
Japanese/foreign titles starting from ?,? or ?(? were
removed. Japanese and foreign titles were split into
chunks by middle dots and ? ?, respectively, and re-
sulting chunks were aligned. Titles pairs with differ-
ent numbers of chunks, or ones with foreign char-
acter length less than 3 were excluded. All accent
marks were normalized (German ??? was converted
to ?ss?).
Implementation Details P (c|s) of the class
transliteration model was calculated by a charac-
ter 3-gram language model with Witten-Bell dis-
counting. Japanese Katakanas were all converted
to Hepburn-style Roman characters, with minor
changes so as to incorporate foreign pronunciations
such as ?wi / ??? and ?we / ??.? The hyphens
??? were replaced by the previous vowels (e.g., ??
??????? is converted to ?supagettii.?)
The maximum length of substitution pairs W de-
scribed in Section 2 was set W = 2. The EM al-
gorithm parameters P (? ? ?|z) were initialized to
the probability P (? ? ?) of the alpha-beta model
plus Gaussian noise, and pik were uniformly initial-
ized to 1/K. Based on the preliminary results, we
repeated EM iterations for 40 times.
5.2 Results
Language Class Detection We firstly show the
precision of language detection using the class
Language de en es fr it
Precision(%) 65.4 83.3 48.2 57.7 66.1
Table 2: Language Class Detection Result (Dataset 2)
Model Dataset 1 Dataset 2
AB 94.8 90.9
HARD 90.3 89.8
SOFT 95.7 92.4
LATENT 95.8 92.4
Table 3: Model Performance Comparison (MRR; %)
transliteration model P (c|s) and Equation (3) (Table
5.2, 5.2). The overall precision is relatively lower
than, e.g., Li et al (2007), which is attributed to the
fact that European names can be quite ambiguous
(e.g., ?Charles? can read ?????? cha?ruzu? or
????? sharuru?) The precision of Dataset 2 is
even worse because it has more classes. We can also
use the result of the latent class transliteration for
clustering by regarding k? = argmaxk ?nk as the
class of the pair. The resulting cluster purity way
was 0.74.
Transliteration Model Comparison We show
the evaluation results of transliteration candidate re-
trieval task using each of PAB(t|s) (AB), Phard(t|s)
(HARD), Psoft(t|s) (SOFT), and Platent(t|s) (LA-
TENT) (Table 5.2). The number of latent classes
was K = 3 for Dataset 1 and K = 5 for Dataset 2,
which are the same as the numbers of language ori-
gins. LATENT shows comparable performance ver-
sus SOFT, although it can be higher depending on
the value of K, as stated below. HARD, on the other
hand, shows lower performance, which is mainly
due to the low precision of class detection. The de-
tection errors are alleviated in SOFT by considering
the weighted sum of transliteration probabilities.
We also conducted the evaluation based on the
top-1 accuracy of transliteration candidates. Be-
cause we found out that the tendency of the results
is the same as MRR, we simply omitted the result in
this paper.
The simplest model AB incorrectly reads ?Felix
/ ??????,? ?Read / ???? as ?????
Firisu? and ????? Rea?do.? This may be because
English pronunciation ?x / ??? kkusu? and ?ea /
56
?? ?i? are influenced by other languages. SOFT
and LATENT can find correct candidates for these
pairs. Irregular pronunciation pairs such as ?Caen
/ ??? ka?n? (French; misread ????? sha?n?)
and ?Laemmle /??? Remuri? (English; misread
???? Riamu?) were misread by SOFT but not by
LATENT. For more irregular cases such as ?Hilda?
??? Iruda?(English), it is difficult to find correct
counterparts even by LATENT.
Finally, we investigated the effect of the number
of latent classes K. The performance is higher when
K is slightly smaller than the number of language
origins in the dataset (e.g., K = 4 for Dataset 2) but
the performance gets unstable for larger values of K
due to the EM algorithm initial values.
6 Conclusion
In this paper, we proposed a latent class translitera-
tion method which models source language origins
as latent classes. The model parameters are learned
from sets of transliterated words with different ori-
gins via the EM algorithm. The experimental re-
sult of Western person / proper name transliteration
task shows that, even though the proposed model
does not rely on explicit language origins, it achieves
higher accuracy versus conventional methods using
explicit language origins. Considering sources other
than Western languages as well as targets other than
Japanese is the future work.
References
Farooq Ahmad and Grzegorz Kondrak. 2005. Learning a
spelling error model from search query logs. In Proc.
of EMNLP-2005, pages 955?962.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling. In Proc. ACL-
2000, pages 286?293.
Eric Brill, Gary Kacmarcik, and Chris Brockett. 2001.
Automatically harvesting katakana-english term pairs
from search engine query logs. In Proc. NLPRS-2001,
pages 393?399.
Masato Hagiwara and Hisami Suzuki. 2009. Japanese
query alteration based on semantic similarity. In Proc.
of NAACL-2009, page 191.
Kevin Knight and Graehl Jonathan. 1998. Machine
transliteration. Computational Linguistics, 24:599?
612.
Haizhou Li, Khe Chai Sum, Jin-Shea Kuo, and Minghui
Dong. 2007. Semantic transliteration of personal
names. In Proc. of ACL 2007, pages 120?127.
Ariadna Font Llitjos and Alan W. Black. 2001. Knowl-
edge of language origin improves pronunciation accu-
racy. In Proc. of Eurospeech, pages 1919?1922.
57
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 183?189,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Accurate Word Segmentation using Transliteration
and Language Model Projection
Masato Hagiwara Satoshi Sekine
Rakuten Institute of Technology, New York
215 Park Avenue South, New York, NY
{masato.hagiwara, satoshi.b.sekine}@mail.rakuten.com
Abstract
Transliterated compound nouns not
separated by whitespaces pose diffi-
culty on word segmentation (WS). Of-
fline approaches have been proposed to
split them using word statistics, but
they rely on static lexicon, limiting
their use. We propose an online ap-
proach, integrating source LM, and/or,
back-transliteration and English LM.
The experiments on Japanese and Chi-
nese WS have shown that the pro-
posed models achieve significant im-
provement over state-of-the-art, reduc-
ing 16% errors in Japanese.
1 Introduction
Accurate word segmentation (WS) is the
key components in successful language pro-
cessing. The problem is pronounced in lan-
guages such as Japanese and Chinese, where
words are not separated by whitespaces. In
particular, compound nouns pose difficulties
to WS since they are productive, and often
consist of unknown words.
In Japanese, transliterated foreign com-
pound words written in Katakana are ex-
tremely difficult to split up into components
without proper lexical knowledge. For ex-
ample, when splitting a compound noun ?
???????? burakisshureddo, a traditional
word segmenter can easily segment this as ?
???/????? ?*blacki shred? since ????
? shureddo ?shred? is a known, frequent word.
It is only the knowledge that ????buraki
(*?blacki?) is not a valid word which prevents
this. Knowing that the back-transliterated un-
igram ?blacki? and bigram ?blacki shred? are
unlikely in English can promote the correct
WS, ??????/??? ?blackish red?. In Chi-
nese, the problem can be more severe since
the language does not have a separate script
to represent transliterated words.
Kaji and Kitsuregawa (2011) tackled
Katakana compound splitting using back-
transliteration and paraphrasing. Their ap-
proach falls into an offline approach, which
focuses on creating dictionaries by extract-
ing new words from large corpora separately
before WS. However, offline approaches have
limitation unless the lexicon is constantly
updated. Moreover, they only deal with
Katakana, but their method is not directly ap-
plicable to Chinese since the language lacks a
separate script for transliterated words.
Instead, we adopt an online approach, which
deals with unknown words simultaneously as
the model analyzes the input. Our ap-
proach is based on semi-Markov discrimina-
tive structure prediction, and it incorporates
English back-transliteration and English lan-
guage models (LMs) into WS in a seamless
way. We refer to this process of transliterat-
ing unknown words into another language and
using the target LM as LM projection. Since
the model employs a general transliteration
model and a general English LM, it achieves
robust WS for unknown words. To the best
of our knowledge, this paper is the first to use
transliteration and projected LMs in an online,
seamlessly integrated fashion for WS.
To show the effectiveness of our approach,
we test our models on a Japanese balanced cor-
pus and an electronic commerce domain cor-
pus, and a balanced Chinese corpus. The re-
sults show that we achieved a significant im-
provement in WS accuracy in both languages.
2 Related Work
In Japanese WS, unknown words are usu-
ally dealt with in an online manner with the
unknown word model, which uses heuristics
183
depending on character types (Kudo et al,
2004). Nagata (1999) proposed a Japanese un-
known word model which considers PoS (part
of speech), word length model and orthog-
raphy. Uchimoto et al (2001) proposed a
maximum entropy morphological analyzer ro-
bust to unknown words. In Chinese, Peng et
al. (2004) used CRF confidence to detect new
words.
For offline approaches, Mori and Nagao
(1996) extracted unknown word and estimated
their PoS from a corpus through distributional
analysis. Asahara and Matsumoto (2004)
built a character-based chunking model using
SVM for Japanese unknown word detection.
Kaji and Kitsuregawa (2011)?s approach is
the closest to ours. They built a model
to split Katakana compounds using back-
transliteration and paraphrasing mined from
large corpora. Nakazawa et al (2005) is
a similar approach, using a Ja-En dictionary
to translate compound components and check
their occurrence in an English corpus. Sim-
ilar approaches are proposed for other lan-
guages, such as German (Koehn and Knight,
2003) and Urdu-Hindi (Lehal, 2010). Correct
splitting of compound nouns has a positive ef-
fect on MT (Koehn and Knight, 2003) and IR
(Braschler and Ripplinger, 2004).
A similar problem can be seen in Korean,
German etc. where compounds may not be
explicitly split by whitespaces. Koehn and
Knight (2003) tackled the splitting problem in
German, by using word statistics in a mono-
lingual corpus. They also used the informa-
tion whether translations of compound parts
appear in a German-English bilingual corpus.
Lehal (2010) used Urdu-Devnagri translitera-
tion and a Hindi corpus for handling the space
omission problem in Urdu compound words.
3 Word Segmentation Model
Out baseline model is a semi-Markov struc-
ture prediction model which estimates WS and
the PoS sequence simultaneously (Kudo et al,
2004; Zhang and Clark, 2008). This model
finds the best output y? from the input sen-
tence string x as: y? = argmaxy?Y (x) w ??(y).
Here, Y (x) denotes all the possible sequences
of words derived from x. The best analysis is
determined by the feature function ?(y) the
ID Feature ID Feature
1 wi 13 w1i?1w1i
2 t1i 14 t1i?1t1i
3* t1i t2i 15* t1i?1t2i?1t1i t2i
4* t1i t2i t3i 16* t1i?1t2i?1t3i?1t1i t2i t3i
5* t1i t2i t5i t6i 17* t1i?1t2i?1t5i?1t6i?1t1i t2i t5i t6i
6* t1i t2i t6i 18* t1i?1t2i?1t6i?1t1i t2i t6i
7 wit1i 19 ?LMS1 (wi)
8* wit1i t2i 20 ?LMS2 (wi?1, wi)
9* wit1i t2i t3i 21 ?LMP1 (wi)
10* wit1i t2i t5i t6i 22 ?LMP2 (wi?1, wi)
11* wit1i t2i t6i
12 c(wi)l(wi)
Table 1: Features for WS & PoS tagging
weight vector w. WS is conducted by stan-
dard Viterbi search based on lattice, which
is illustrated in Figure 1. We limit the fea-
tures to word unigram and bigram features,
i.e., ?(y) = ?i[?1(wi) + ?2(wi?1, wi)] for y =
w1...wn. By factoring the feature function into
these two subsets, argmax can be efficiently
searched by the Viterbi algorithm, with its
computational complexity proportional to the
input length. We list all the baseline features
in Table 11. The asterisks (*) indicate the fea-
ture is used for Japanese (JA) but not for Chi-
nese (ZH) WS. Here, wi and wi?1 denote the
current and previous word in question, and tji
and tji?1 are level-j PoS tags assigned to them.
l(w) and c(w) are the length and the set of
character types of word w.
If there is a substring for which no dic-
tionary entries are found, the unknown word
model is invoked. In Japanese, our unknown
word model relies on heuristics based on char-
acter types and word length to generate word
nodes, similar to that of MeCab (Kudo et
al., 2004). In Chinese, we aggregated con-
secutive 1 to 4 characters add them as ?n
(common noun)?, ?ns (place name)?, ?nr (per-
sonal name)?, and ?nz (other proper nouns),?
since most of the unknown words in Chinese
are proper nouns. Also, we aggregated up to
20 consecutive numerical characters, making
them a single node, and assign ?m? (number).
For other character types, a single node with
PoS ?w (others)? is created.
1The Japanese dictionary and the corpus we used
have 6 levels of PoS tag hierarchy, while the Chinese
ones have only one level, which is why some of the
PoS features are not included in Chinese. As character
type, Hiragana (JA), Katakana (JA), Latin alphabet,
Number, Chinese characters, and Others, are distin-
guished. Word length is in Unicode.
184
Input: ? ? ? ? ? ? ? ? ? ? ? ? ?
? ? ? ? ? ?
?
? ?
? ?
? ? ? EOS? ? ? ?
? ? ? ? ?? ? ? ?
? ? ?
. . . 
. . . 
? ? ?
? ? ?
? ? ? ?
?BOS
very  popular       color                     blackish                            red
bu ki
bla bra kish
brakiblaki
brakiblaki
brackishblackish
ledreadred
shreadshred
node (a)
node (b)
edge (c)
TransliterationModel
English
LM
Figure 1: Example lattice with LM projection
4 Use of Language Model
Language Model Augmentation Analo-
gous to Koehn and Knight (2003), we can ex-
ploit the fact that ??? reddo (red) in the
example ????????? is such a common
word that one can expect it appears frequently
in the training corpus. To incorporate this
intuition, we used log probability of n-gram
as features, which are included in Table 1
(ID 19 and 20): ?LMS1 (wi) = log p(wi) and
?LMS2 (wi?1, wi) = log p(wi?1, wi). Here the
empirical probability p(wi) and p(wi?1, wi) are
computed from the source language corpus. In
Japanese, we applied this source language aug-
mentation only to Katakana words. In Chi-
nese, we did not limit the target.
4.1 Language Model Projection
As we mentioned in Section 2, English
LM knowledge helps split transliterated com-
pounds. We use (LM) projection, which is
a combination of back-transliteration and an
English model, by extending the normal lat-
tice building process as follows:
Firstly, when the lattice is being built, each
node is back-transliterated and the resulting
nodes are associated with it, as shown in
Figure 1 as the shaded nodes. Then, edges
are spanned between these extended English
nodes, instead of between the original nodes,
by additionally taking into consideration En-
glish LM features (ID 21 and 22 in Table 1):
?LMP1 (wi) = log p(wi) and ?LMP2 (wi?1, wi) =
log p(wi?1, wi). Here the empirical probabil-
ity p(wi) and p(wi?1, wi) are computed from
the English corpus. For example, Feature 21
is set to ?LMP1 (?blackish?) for node (a), to
?LMP1 (?red?) for node (b), and Feature 22 is
set to ?LMP2 (?blackish?, ?red?) for edge (c) in
Figure 1. If no transliterations were generated,
or the n-grams do not appear in the English
corpus, a small frequency ? is assumed.
Finally, the created edges are traversed from
EOS, and associated original nodes are chosen
as the WS result. In Figure 1, the bold edges
are traversed at the final step, and the corre-
sponding nodes ?? - ?? - ? - ??????-
???? are chosen as the final WS result.
For Japanese, we only expand and project
Katakana noun nodes (whether they are
known or unknown words) since transliterated
words are almost always written in Katakana.
For Chinese, only ?ns (place name)?, ?nr (per-
sonal name)?, and ?nz (other proper noun)?
nodes whose surface form is more than 1-
character long are transliterated. As the En-
glish LM, we used Google Web 1T 5-gram Ver-
sion 1 (Brants and Franz, 2006), limiting it to
unigrams occurring more than 2000 times and
bigrams occurring more than 500 times.
5 Transliteration
For transliterating Japanese/Chinese words
back to English, we adopted the Joint Source
Channel (JSC) Model (Li et al, 2004), a gen-
erative model widely used as a simple yet pow-
erful baseline in previous research e.g., (Hagi-
wara and Sekine, 2012; Finch and Sumita,
2010).2 The JSC model, given an input
of source word s and target word t, de-
fines the transliteration probability based on
transliteration units (TUs) ui = ?si, ti? as:
PJSC(?s, t?) =
?f
i=1 P (ui|ui?n+1, ..., ui?1),
where f is the number of TUs in a given source
/ target word pair. TUs are atomic pair units
of source / target words, such as ?la/?? and
?ish/????. The TU n-gram probabilities are
learned from a training corpus by following it-
erative updates similar to the EM algorithm3.
In order to generate transliteration candidates,
we used a stack decoder described in (Hagi-
wara and Sekine, 2012). We used the training
data of the NEWS 2009 workshop (Li et al,
2009a; Li et al, 2009b).
As reference, we measured the performance
on its own, using NEWS 2009 (Li et al, 2009b)
data. The percentage of correctly transliter-
ated words are 37.9% for Japanese and 25.6%
2Note that one could also adopt other generative /
discriminative transliteration models, such as (Jiampo-
jamarn et al, 2007; Jiampojamarn et al, 2008).
3We only allow TUs whose length is shorter than or
equal to 3, both in the source and target side.
185
for Chinese. Although the numbers seem low
at a first glance, Chinese back-transliteration
itself is a very hard task, mostly because
Chinese phonology is so different from En-
glish that some sounds may be dropped when
transliterated. Therefore, we can regard this
performance as a lower bound of the translit-
eration module performance we used for WS.
6 Experiments
6.1 Experimental Settings
Corpora For Japanese, we used (1) EC
corpus, consists of 1,230 product titles and
descriptions randomly sampled from Rakuten
(Rakuten-Inc., 2012). The corpus is manually
annotated with the BCCWJ style WS (Ogura
et al, 2011). It consists of 118,355 tokens, and
has a relatively high percentage of Katakana
words (11.2%). (2) BCCWJ (Maekawa, 2008)
CORE (60,374 sentences, 1,286,899 tokens,
out of which approx. 3.58% are Katakana
words). As the dictionary, we used UniDic
(Den et al, 2007). For Chinese, we used
LCMC (McEnery and Xiao, 2004) (45,697 sen-
tences and 1,001,549 tokens). As the dictio-
nary, we used CC-CEDICT (MDGB, 2011)4.
Training and Evaluation We used Aver-
aged Perceptron (Collins, 2002) (3 iterations)
for training, with five-fold cross-validation. As
for the evaluation metrics, we used Precision
(Prec.), Recall (Rec.), and F-measure (F). We
additionally evaluated the performance lim-
ited to Katakana (JA) or proper nouns (ZH)
in order to see the impact of compound split-
ting. We also used word error rate (WER) to
see the relative change of errors.
6.2 Japanese WS Results
We compared the baseline model, the
augmented model with the source language
(+LM-S) and the projected model (+LM-P).
Table 3 shows the result of the proposed mod-
els and major open-source Japanese WS sys-
tems, namely, MeCab 0.98 (Kudo et al, 2004),
JUMAN 7.0 (Kurohashi and Nagao, 1994),
4Since the dictionary is not explicitly annotated
with PoS tags, we firstly took the intersection of the
training corpus and the dictionary words, and assigned
all the possible PoS tags to the words which appeared
in the corpus. All the other words which do not appear
in the training corpus are discarded.
and KyTea 0.4.2 (Neubig et al, 2011) 5. We
observed slight improvement by incorporat-
ing the source LM, and observed a 0.48 point
F-value increase over baseline, which trans-
lates to 4.65 point Katakana F-value change
and 16.0% (3.56% to 2.99 %) WER reduc-
tion, mainly due to its higher Katakana word
rate (11.2%). Here, MeCab+UniDic achieved
slightly better Katakana WS than the pro-
posed models. This may be because it is
trained on a much larger training corpus (the
whole BCCWJ). The same trend is observed
for BCCWJ corpus (Table 2), where we gained
statistically significant 1 point F-measure in-
crease on Katakana word.
Many of the improvements of +LM-S over
Baseline come from finer grained splitting,
for example, * ?????? reinsuutsu ?rain
suits? to ???/???, while there is wrong
over-splitting, e.g., ???????terekyasutaa
?Telecaster? to * ??/?????. This type of
error is reduced by +LM-P, e.g., * ???/??
? purasu chikku ?*plus tick? to ??????
purasuchikku ?plastic? due to LM projection.
+LM-P also improved compounds whose com-
ponents do not appear in the training data,
such as * ???????? ruukasufirumu to
????/???? ?Lucus Film.? Indeed, we
randomly extracted 30 Katakana differences
between +LM-S and +LM-P, and found out
that 25 out of 30 (83%) are true improvement.
One of the proposed method?s advantages is
that it is very robust to variations, such as
?????????? akutibeitiddo ?activated,?
even though only the original form, ?????
?? akutibeito ?activate? is in the dictionary.
One type of errors can be attributed
to non-English words such as ??????
sunokobeddo, which is a compound of Japanese
word ??? sunoko ?duckboard? and an En-
glish word ??? beddo ?bed.?
6.3 Chinese WS Results
We compare the results on Chinese WS,
with Stanford Segmenter (Tseng et al, 2005)
(Table 4) 6. Including +LM-S decreased the
5Because MeCab+UniDic and KyTea models are
actually trained on BCCWJ itself, this evaluation is
not meaningful but just for reference. The WS granu-
larity of IPADic, JUMAN, and KyTea is also different
from the BCCWJ style.
6Note that the comparison might not be fair since
(1) Stanford segmenter?s criteria are different from
186
Model Prec. (O) Rec. (O) F (O) Prec. (K) Rec. (K) F (K) WER
MeCab+IPADic 91.28 89.87 90.57 88.74 82.32 85.41 12.87
MeCab+UniDic* (98.84) (99.33) (99.08) (96.51) (97.34) (96.92) (1.31)
JUMAN 85.66 78.15 81.73 91.68 88.41 90.01 23.49
KyTea* (81.84) (90.12) (85.78) (99.57) (99.73) (99.65) (20.02)
Baseline 96.36 96.57 96.47 84.83 84.36 84.59 4.54
+LM-S 96.36 96.57 96.47 84.81 84.36 84.59 4.54
+LM-S+LM-P 96.39 96.61 96.50 85.59 85.40 85.50 4.50
Table 2: Japanese WS Performance (%) on BCCWJ ? Overall (O) and Katakana (K)
Model Prec. (O) Rec. (O) F (O) Prec. (K) Rec. (K) F (K) WER
MeCab+IPADic 84.36 87.31 85.81 86.65 73.47 79.52 20.34
MeCab+UniDic 95.14 97.55 96.33 93.88 93.22 93.55 5.46
JUMAN 90.99 87.13 89.2 92.37 88.02 90.14 14.56
KyTea 82.00 86.53 84.21 93.47 90.32 91.87 21.90
Baseline 97.50 97.00 97.25 89.61 85.40 87.45 3.56
+LM-S 97.79 97.37 97.58 92.58 88.99 90.75 3.17
+LM-S+LM-P 97.90 97.55 97.73 93.62 90.64 92.10 2.99
Table 3: Japanese WS Performance (%) on the EC domain corpus
Model Prec. (O) Rec. (O) F (O) Prec. (P) Rec. (P) F (P) WER
Stanford Segmenter 87.06 86.38 86.72 ? ? ? 17.45
Baseline 90.65 90.87 90.76 83.29 51.45 63.61 12.21
+LM-S 90.54 90.78 90.66 72.69 43.28 54.25 12.32
+LM-P 90.90 91.48 91.19 75.04 52.11 61.51 11.90
Table 4: Chinese WS Performance (%) ? Overall (O) and Proper Nouns (P)
performance, which may be because one can-
not limit where the source LM features are
applied. This is why the result of +LM-
S+LM-P is not shown for Chinese. On the
other hand, replacing LM-S with LM-P im-
proved the performance significantly. We
found positive changes such as * ??/?
??? oumai/ersalihe to ???/???
oumaier/salihe ?Umar Saleh? and * ??/?
??? lingdao/renmandela to ???/???
lingdaoren/mandela?Leader Mandela?. How-
ever, considering the overall F-measure in-
crease and proper noun F-measure decrease
suggests that the effect of LM projection is
not limited to proper nouns but also promoted
finer granularity because we observed proper
noun recall increase.
One of the reasons which make Chinese LM
projection difficult is the corpus allows sin-
gle tokens with a transliterated part and Chi-
nese affices, e.g., ?????? makesizhuy-
izhe ?Marxists? (??? makesi ?Marx? + ?
?? zhuyizhe ?-ist (believers)?) and ???
niluohe ?Nile River? ( ?? niluo ?Nile? +
? he ?-river?). Another source of errors is
transliteration accuracy. For example, no ap-
ours, and (2) our model only uses the intersection of
the training set and the dictionary. Proper noun per-
formance for the Stanford segmenter is not shown since
it does not assign PoS tags.
propriate transliterations were generated for
??? weinasi ?Venus,? which is commonly
spelled ??? weinasi. Improving the JSC
model could improve the LM projection per-
formance.
7 Conclusion and Future Works
In this paper, we proposed a novel, on-
line WS model for the Japanese/Chinese
compound word splitting problem, by seam-
lessly incorporating the knowledge that back-
transliteration of properly segmented words
also appear in an English LM. The experi-
mental results show that the model achieves
a significant improvement over the baseline
and LM augmentation, achieving 16% WER
reduction in the EC domain.
The concept of LM projection is general
enough to be used for splitting other com-
pound nouns. For example, for Japanese per-
sonal names such as ???? Naka Riisa, if
we could successfully estimate the pronuncia-
tion Nakariisa and look up possible splits in
an English LM, one is expected to find a cor-
rect WS Naka Riisa because the first and/or
the last name are mentioned in the LM. Seek-
ing broader application of LM projection is a
future work.
187
References
Masayuki Asahara and Yuji Matsumoto. 2004.
Japanese unknown word identification by
character-based chunking. In Proceedings of
COLING 2004, pages 459?465.
Thorsten Brants and Alex Franz. 2006. Web 1T
5-gram Version 1. Linguistic Data Consortium.
Martin Braschler and B?rbel Ripplinger. 2004.
How effective is stemming and decompounding
for german text retrieval? Information Re-
trieval, pages 291?316.
Michael Collins. 2002. Discriminative training
methods for hidden markov models: theory and
experiments with perceptron algorithms. In
Proceedings of EMNLP 2012, pages 1?8.
Yasuharu Den, Toshinobu Ogiso, Hideki Ogura,
Atsushi Yamada, Nobuaki Minematsu, Kiy-
otaka Uchimoto, and Hanae Koiso. 2007.
The development of an electronic dictionary
for morphological analysis and its application
to Japanese corpus linguistics (in Japanese).
Japanese linguistics, 22:101?122.
Andrew Finch and Eiichiro Sumita. 2010. A
bayesian model of bilingual segmentation for
transliteration. In Proceedings of IWSLT 2010,
pages 259?266.
Masato Hagiwara and Satoshi Sekine. 2012. La-
tent class transliteration based on source lan-
guage origin. In Proceedings of NEWS 2012,
pages 30?37.
Sittichai Jiampojamarn, Grzegorz Kondrak, and
Tarek Sherif. 2007. Applying many-to-many
alignments and hidden markov models to letter-
to-phoneme conversion. In Proceedings of
NAACL-HLT 2007, pages 372?379.
Sittichai Jiampojamarn, Colin Cherry, and Grze-
gorz Kondrak. 2008. Joint processing and dis-
criminative training for letter-to-phoneme con-
version. In Proceedings of ACL 2008, pages
905?913.
Nobuhiro Kaji and Masaru Kitsuregawa. 2011.
Splitting noun compounds via monolingual and
bilingual paraphrasing: A study on japanese
katakana words. In Proceedings of the EMNLP
2011, pages 959?969.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proceedings
of EACL 2003, pages 187?193.
Taku Kudo, Kaoru Yamamoto, and Yuji Mat-
sumoto. 2004. Applying conditional random
fields to Japanese morphological analysis. In
Proceedings of EMNLP 2004, pages 230?237.
Sadao Kurohashi and Makoto Nagao. 1994. Im-
provements of Japanese morphological analyzer
juman. In Proceedings of the International
Workshop on Sharable Natural Language Re-
sources, pages 22?38.
Gurpreet Singh Lehal. 2010. A word segmentation
system for handling space omission problem in
urdu script. In Proceedings of the 1st Workshop
on South and Southeast Asian Natural Language
Processing (WSSANLP), pages 43?50.
Haizhou Li, Zhang Min, and Su Jian. 2004. A
joint source-channel model for machine translit-
eration. In Proceedings of ACL 2004, pages
159?166.
Haizhou Li, A Kumaran, Vladimir Pervouchine,
and Min Zhang. 2009a. Report of news 2009
machine transliteration shared task. In Proceed-
ings of NEWS 2009, pages 1?18.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009b. Whitepaper of news 2009
machine transliteration shared task. In Proceed-
ings of NEWS 2009, pages 19?26.
Kikuo Maekawa. 2008. Compilation of
the Kotonoha-BCCWJ corpus (in Japanese).
Nihongo no kenkyu (Studies in Japanese),
4(1):82?95.
Anthony McEnery and Zhonghua Xiao. 2004. The
lancaster corpus of mandarin chinese: A corpus
for monolingual and contrastive language study.
In Proceedings of LREC 2004, pages 1175?1178.
MDGB. 2011. CC-CEDICT,
Retreived August, 2012 from
http://www.mdbg.net/chindict/chindict.php?
page=cedict.
Shinsuke Mori and Makoto Nagao. 1996. Word
extraction from corpora and its part-of-speech
estimation using distributional analysis. In Pro-
ceedings of COLING 2006, pages 1119?1122.
Masaaki Nagata. 1999. A part of speech estima-
tion method for Japanese unknown words using
a statistical model of morphology and context.
In Proceedings of ACL 1999, pages 277?284.
Toshiaki Nakazawa, Daisuke Kawahara, and Sadao
Kurohashi. 2005. Automatic acquisition of ba-
sic katakana lexicon from a given corpus. In
Proceedings of IJCNLP 2005, pages 682?693.
Graham Neubig, Yosuke Nakata, and Shinsuke
Mori. 2011. Pointwise prediction for robust,
adaptable Japanese morphological analysis. In
Proceedings of ACL-HLT 2011, pages 529?533.
Hideki Ogura, Hanae Koiso, Yumi Fujike, Sayaka
Miyauchi, and Yutaka Hara. 2011. Mor-
phological Information Guildeline for BCCWJ:
Balanced Corpus of Contemporary Written
188
Japanese, 4th Edition. National Institute for
Japanese Language and Linguistics.
Fuchun Peng, Fangfang Feng, and Andrew McCal-
lum. 2004. Chinese segmentation and new word
detection using conditional random fields. In
Proceedings COLING 2004.
Rakuten-Inc. 2012. Rakuten Ichiba
http://www.rakuten.co.jp/.
Huihsin Tseng, Pichuan Chang, Galen Andrew,
Daniel Jurafsky, and Christopher Manning.
2005. A conditional random field word seg-
menter. In Fourth SIGHAN Workshop on Chi-
nese Language Processing.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi
Isahara. 2001. Morphological analysis based
on a maximum entropy model ? an ap-
proach to the unknown word problem ? (in
Japanese). Journal of Natural Language Pro-
cessing, 8:127?141.
Yue Zhang and Stephen Clark. 2008. Joint word
segmentation and pos tagging using a single per-
ceptron. In Proceedings of ACL 2008, pages
888?896.
189
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 30?37,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Latent Semantic Transliteration using Dirichlet Mixture
Masato Hagiwara Satoshi Sekine
Rakuten Institute of Technology, New York
215 Park Avenue South, New York, NY
{masato.hagiwara, satoshi.b.sekine}@mail.rakuten.com
Abstract
Transliteration has been usually recog-
nized by spelling-based supervised models.
However, a single model cannot deal with
mixture of words with different origins,
such as ?get? in ?piaget? and ?target?.
Li et al (2007) propose a class translit-
eration method, which explicitly models
the source language origins and switches
them to address this issue. In contrast
to their model which requires an explic-
itly tagged training corpus with language
origins, Hagiwara and Sekine (2011) have
proposed the latent class transliteration
model, which models language origins as
latent classes and train the transliteration
table via the EM algorithm. However, this
model, which can be formulated as uni-
gram mixture, is prone to overfitting since
it is based on maximum likelihood estima-
tion. We propose a novel latent seman-
tic transliteration model based on Dirichlet
mixture, where a Dirichlet mixture prior
is introduced to mitigate the overfitting
problem. We have shown that the pro-
posed method considerably outperform the
conventional transliteration models.
1 Introduction
Transliteration (e.g., ?????? baraku
obama ?Barak Obama?) is phonetic transla-
tion between languages with different writing
systems, which is a major way of importing
foreign words into different languages. Su-
pervised, spelling-based grapheme-to-grapheme
models such as (Brill and Moore, 2000; Li et
al., 2004), which directly align characters in the
training corpus without depending on phonetic
information, and statistically computing their
correspondence, have been a popular method to
detect and/or generate transliterations, in con-
trast to phonetic-based methods such as (Knight
and Jonathan, 1998). However, single, mono-
lithic models fail to deal with sets of foreign
words with multiple language origins mixed to-
gether. For example, the ?get? part of ?pi-
aget / ????piaje? and ?target / ????
? t?getto? differ in pronunciation and spelling
correspondence depending on their source lan-
guages, which are French and English in this
case.
To address this issue, Li et al (2007) have
proposed class transliteration model, which ex-
plicitly models and classifies classes of languages
(such as Chinese Hanzi, Japanese Katakana,
and so on) and genders, and switches corre-
sponding transliteration models based on the
input. This model requires training sets of
transliterated word pairs tagged with language
origin, which is difficult to obtain. Hagiwara
and Sekine proposed the latent class translitera-
tion (LCT) model (Hagiwara and Sekine, 2011),
which models source language origins as directly
unobservable latent classes and applies appro-
priate transliteration models to given transliter-
ation pairs. The model parameters are learned
from corpora without language origins in an un-
supervised manner. This enables us to correctly
assign latent classes for English and French to
?piaget / ????piaje? and ?target / ????
30
? t?getto? and to identify their transliteration
correspondence correctly. However, this model
is based on maximum likelihood estimation on
multinomials and thus sensitive to noise in the
training data such as transliteration pairs with
irregular pronunciation, and tends to overfit the
data.
Considering the atomic re-writing unit
(transliteration unit, or TU, e.g., ?get / ??
? getto?) as a word, and a transliteration pair
as a document consisting of a word sequence,
class-based transliteration can be modeled by
the perfect analogy to document topic models
proposed in tha past. In fact, the LCT model,
where the transliteration probability is defined
by a mixture of multinomials, can be regarded
as a variant of a topic model, namely Unigram
Mixuture (UM) (Nigam et al, 2000). There
has been an extension of unigram mixture pro-
posed (Sj?lander et al, 1996; Yamamoto and
Sadamitsu, 2005) which introduces a Dirichlet
mixture distribution as a prior and alleviates the
overfitting problem. We can expect to improve
the transliteration accuracy by formulating the
transliteration problem using a similar frame-
work to these topic models.
In this paper, we formalize class-based
transliteration based on language origins in the
framework of topic models. We then propose the
latent semantic transliteration model based on
Dirichlet mixture (DM-LST). We show through
experiments that it can significantly improve the
transliteration performance by alleviating the
overfitting issue.
Note that we tackle the task of transliteration
generation in this paper, in contrast to translit-
eration recognition. A transliteration generation
task is, given an input word s (such as ?piaget?),
the system is asked to generate from scratch
the most probable transliterated word t (e.g.,
?????piaje?). The transliteration recogni-
tion task, on the other hand, is to induce the
most probable transliteration t? ? T such that
t? = arg maxt?T P (?s, t?) given the input word
s and a pool of transliteration candidates T . We
call P (?s, t?) transliteration model in this paper.
This model can be regarded as the hy-
brid of an unsupervised alignment technique
for transliteration and class-based translitera-
tion. Related researches for the former in-
clude (Ahmad and Kondrak, 2005), who esti-
mate character-based error probabilities from
query logs via the EM algorithm. For the lat-
ter, Llitjos and Black (2001) showed that source
language origins may improve the pronunciation
of proper nouns in text-to-speech systems.
The structure of this paper is as follows:
we introduce the alpha-beta model(Brill and
Moore, 2000) in Section 2, which is the most ba-
sic spelling-based transliteration model on which
other models are based. In the following Section
3, we introduce and relate the joint source chan-
nel (JSC) model (Li et al, 2004) to the alpha-
beta model. We describe the LCT model as an
extension to the JSC model in Section 4. In
Section 5, we propose the DM-LST model, and
show the experimental results on transliteration
generation in Section 6.
2 Alpha-Beta Model
In this section, we describe the alpha-beta
model, which is one of the simplest spelling-
based transliteration models. Though simple,
the model has been shown to achieve better
performance in tasks such as spelling correction
(Brill and Moore, 2000), transliteration (Brill et
al., 2001), and query alteration (Hagiwara and
Suzuki, 2009).
The method directly models spelling-based
re-writing probabilities of transliteration pairs.
It is an extension to the normal edit distance,
where the cost of operations (substitution, in-
sertion, and deletion) is fixed to 1, and assigns a
probability to a string edit operation of the form
si ? ti (si and ti are any substrings of length 0
to w). We call the unit operation of string re-
writing ui = ?si, ti? as transliteration unit (TU)
as in (Li et al, 2004). The total transliteration
probability of re-writing a word s to t is given
by
PAB(?s, t?) = maxu1...uf
f
?
i=1
P (ui), (1)
where f is the number of TUs and u1...uf is any
sequence of TUs (e.g., ?pi ?? a ?? get ?
31
???) created by splitting up the input/output
transliteration pair ?s, t?. The above equa-
tion can be interpreted as a problem of find-
ing a TU sequence u1...uf which maximizes the
probability defined by the product of individ-
ual probabilities of independent TUs. After tak-
ing the logarithm of the both sides, and regard-
ing ? logP (ui) as the cost of string substitution
si ? ti, the problem is equivalent to minimizing
the sum of re-writing costs, and therefore can
be efficiently solved by dynamic programming
as done in the normal edit distance.
TU probabilities P (ui) are calculated from a
training set of transliteration pairs. However,
training sets usually lack alignment information
specifying which characters in s corresponding
which characters in t. Brill and Moore (2000)
resorted to heuristics to align same characters
and to induce the alignment of string chunks.
Hagiwara and Sekine (2011) converted Japanese
Katakana sequences into Roman alphabets be-
cause their model also assumed that the strings
si and ti are expressed in the same alphabet sys-
tem. Our method on the contrary, does not pose
such assumption so that strings in different writ-
ing systems (such as Japanese Katakana and
English alphabets, and Chinese characters and
English alphabets, etc.) can be aligned without
being converted to phonetic representation. For
this reason, we cannot adopt algorithms (such
as the one described in (Brill and Moore, 2000))
which heuristically infer alignment based on the
correspondence of the same characters.
When applying this alpha-beta model, we
computed TU probabilities by counting relative
frequencies of all the alignment possibilities for a
transliteration pair. For example, all the align-
ment possibilities for a pair of strings ?abc? and
?xy? are (a-x b-y c-?), (a-x b-? c-y), and (a-? b-x
c-y). By considering merging up to two adjacent
aligned characters in the first alignment, one ob-
tains the following five aligned string pairs: a-x,
b-y, c-?, ab-xy bc-y. Note that all the translit-
eration models described in this paper implic-
itly depend on the parameter w indicating the
maximum length of character n-grams. We fixed
w = 3 throughout this paper.
3 Joint Source Channel Model
The alpha-beta model described above has
shortcomings that the character alignment is
fixed based on heuristics, and it cannot cap-
ture the dependencies between TUs. One ex-
ample of such dependencies is the phenomenon
that the suffix ?-ed? in English verbs following
a voiced consonant is pronounced /d/, whereas
the one followed by an unvoiced consonant is
/t/. This section describes the JSC model(Li
et al, 2004), which was independently proposed
from the alpha-beta model. The JSC model is
essentially equivalent to the alpha-beta model
except: 1) it can also incorporate higher order
of n-grams of TUs and 2) the TU statistics is
taken not by fixing the heuristic initial align-
ment but by iteratively updating via an EM-like
algorithm.
In the JSC model, the transliteration proba-
bility is defined by the n-gram probabilities of
TUs ui = ?si, ti? as follows:
PJSC(?s, t?) =
f
?
i=1
P (ui|ui?n+1, ..., ui?1). (2)
Again, f is the number of TUs. The TU n-gram
probabilities P (ui|ui?n+1, ..., ui?1) can be calcu-
lated by the following iterative updates similar
to the EM algorithm:
1. Set the initial alignment randomly.
2. E-step: Take the TU n-gram statistics fix-
ing the current alignment, and update the
transliteration model.
3. M-step: Compute the alignment based on
the current transliteration model. The
alignment is inferred by dynamic program-
ming similar to the alpha-beta model.
4. Iterate the E- and M- step until conver-
gence.
Notice the alpha-beta model and the JSC
model are both transliteration recognition mod-
els. In order to output a transliterated word t
for a given input s, we generated transliteration
candidates with high probability using a stack
32
s
?s?
append reduce
(s,?)
(s,?)shift
s
?m?
(s,?)
(s,?)
sm
m
m
ap.
ap.
ap.
(s,?) (m,?)
(s,?) (m,?)
r.
?
reduce (sm,??)
smshift
s:? 5.22 s:? 6.69m: ? 6.14m: ? 8.47th: ? 6.72?
Figure 1: Overview of the stack decoder (generation
of ???? sumisu? from the input ?smith?)
decoder, whose overview is shown in Figure 1.
One character in the input string s (which is
?smith? in the figure) is given at a time, which
is appended at the end of the last TUs for each
candidate. (the append operation in the fig-
ure). Next, the last TU of each candidate is
either reduced or shifted. When it is reduced,
top R TUs with highest probabilities are gener-
ated and fixed referring to the TU table (shown
in the bottom-left of the figure). In Figure 1,
two candidates, namely (?s?, ?? su?) and (?s?,
?? zu?) are generated after the character ?s? is
given. When the last TU is shifted, it remains
unchanged and unfixed for further updates. Ev-
ery time a single character is given, the translit-
eration probability is computed using Eq. 2 for
each candidate, and all but the top-B candidates
with highest probabilities are discarded. The re-
duce width R and the beam width B were deter-
mined using the determined using development
sets, as mentioned in Section 6.
4 Latent Class Transliteration Model
As mentioned in Section 1, the alpha-beta
model and the JSC model build a single translit-
eration model which is simply the monolithic
average of training set statistics, failing to cap-
ture the difference in the source language ori-
gins. Li et al (2004) address this issue by defin-
ing classes c, i.e., the factors such as source lan-
guage origins, gender, and first/last names, etc.
which affect the transliteration probability. The
authors then propose the class transliteration
model which gives the probability of s ? t as
follows:
PLI(t|s) =
?
c
P (t, c|s) =
?
c
P (c|s)P (t|c, s) (3)
However, this model requires a training set
explicitly tagged with the classes. Instead of
assigning an explicit class c to each transliter-
ated pair, Hagiwara and Sekine (2011) introduce
a random variable z which indicates implicit
classes and conditional TU probability P (ui|z).
The latent class transliteration (LCT) model is
then defined as1:
PLCT(?s, t?) =
K
?
z=1
P (z)
f
?
i=1
P (ui|z) (4)
where K is the number of the latent classes.
The latent classes z correspond to classes such
as the language origins and genders mentioned
above, shared by sets of transliterated pairs with
similar re-writing characteristics. The classes z
are not directly observable from the training set,
but can be induced by maximizing the training
set likelihood via the EM algorithm as follows.
Parameters: P (z = k) = pik, P (ui|z) (5)
E-Step: ?nk =
pikP (?sn, tn?|z = k)
?K
k?=1 pik?P (?sn, tn?|z = k?)
, (6)
P (?sn, tn?|z) = maxu1..uf
fn
?
i=1
P (ui|z) (7)
M-Step: pinewk ?
N
?
n=1
?nk, (8)
P (ui|z = k)new =
1
Nk
N
?
n=1
?nk
fn(ui)
fn
(9)
where Nk =
?
n ?nk. Here, ?sn, tn? is the n-
th transliterated pair in the training set, and fn
and fn(ui) indicate how many TUs there are in
total in the n-th transliterated pair, and how
many times the TU ui appeared in it, respec-
tively. As done in the JSC model, we update the
alignment in the training set before the E-Step
for each iteration. Thus fn takes different values
1Note that this LCT model is formalized by intro-
ducing a latent variable to the transliteration generative
probability P (?s, t?) as in the JSC model, not to P (t|s).
33
from iteration to iteration in general. Further-
more, since the alignment is updated based on
P (ui|z) for each z = k, M different alignment
candidates are retained for each transliterated
pairs, which makes the value of fn dependent
on k, i.e., fkn . We initialize P (z = k) = 1/M
to and P (ui|z) = PAB(u) + ?, that is, the TU
probability induced by the alpha-beta algorithm
plus some random noise ?.
Considering a TU as a word, and a translit-
eration pair as a document consisting of a word
sequence, this LCT model defines the transliter-
ation probability as the mixture of multinomi-
als defined over TUs. This can be formulated
by unigram mixture (Nigam et al, 2000), which
is a topic model over documents. This follows a
generation story where documents (i.e., translit-
erated pairs) are generated firstly by choosing a
class z by P (z) and then by generating a word
(i.e., TU) by P (ui|z). Nevertheless, as men-
tioned in Section 1, since this model trains the
parameters based on the maximum likelihood
estimation over multinomials, it is vulnerable to
noise in the training set, thus prone to overfit
the data.
5 Latent Semantic Transliteration
Model based on Dirichlet Mixture
We propose the latent semantic translitera-
tion model based on Dirichlet mixture (DM-
LST), which is an extension to the LCT model
based on unigram mixture. This model enables
to prevent multinomials from being exceedingly
biased towards the given data, still being able to
model the transliteration generation by a mix-
ture of multiple latent classes, by introducing
Dirichlet mixture as a prior to TU multinomi-
als. The compound distribution of multinomi-
als when their parameters are given by Dirichlet
mixtures is given by the Polya mixture distribu-
tion(Yamamoto and Sadamitsu, 2005):
PDM (?s, t?) (10)
=
?
PMul(?s, t?;p)PDM (p;?,?K1 )dp
?
K
?
k=1
?kPPolya(?s, t?;?K1 ) (11)
=
K
?
k=1
?k
?(?k)
?(?k + f)
f
?
i=1
?(f(ui) + ?kui)
?(?kui)
where PMul(?;p) is multinomial with the pa-
rameter p. PDM is Dirichlet mixture, which
is a mixture (with co-efficients ?1, ..., ?K) of K
Dirichlet distributions with parameters ?K1 =
(?1,?2, ...,?K).
The model parameters can be induced by the
following EM algorithm. Notice that we adopted
a fast induction algorithm which extends an in-
duction method using leaving-one-out to mix-
ture distributions(Yamamoto et al, 2003).
Parameters: ? = (?1, ..., ?K),
(12)
?K1 = (?1,?2, ...,?K) (13)
E-Step: ?nk =
?kPPolya(?sn, tn?;?k)
?
k? ?k?PPolya(?sn, tn?;?k?)
(14)
M-Step: ?newk ?
N
?
n=1
?nk (15)
?newku = ?ku
?
n ?nk{fn(u)/(fn(u) ? 1 + ?ku)}
?
n ?nk{fn/(fn ? 1 + ?k)}
(16)
The prediction distribution when a sin-
gle TU u is the input is given PDM (u) =
?K
k=1 ?k?ku/?k. We therefore updated the
alignment in the training corpus, as done in the
JSC model updates, based on the probability
proportional to ?ku/?k for each k before ev-
ery M-Step. The parameters are initially set to
?k = 1/K, ?ku = PAB(u) + ?, as explained in
the previous section.
Since neither LCT nor DM-LST is a translit-
eration generation model, we firstly generated
transliteration candidates T by using the JSC
model and the stack decoder (Section 3) as a
34
baseline, then re-ranked the candidates using
the probabilities given by LCT (Eq. 4 or DM-
LST (Eq. 11), generating the re-ranked list
of transliterated outputs. Because the parame-
ters trained by the EM algorithm differ depend-
ing on the initial values, we trained 10 models
P 1DM , ..., P 10DM using the same training data and
random initial values and computed the aver-
age 110
?10
j=1 P
j
DM (?s, t?) to be used as the final
transliteration model.
It is worth mentioning that another topic
model, namely latent Dirichlet alocation (LDA)
(Blei et al, 2003), assumes that words in a doc-
ument can be generated from different topics
from each other. This assumption corresponds
to the notion that TUs in a single transliter-
ated pairs can be generated from different source
languages, which is presumably a wrong as-
sumption for transliteration tasks, probably ex-
cept for compound-like words with mixed ori-
gins such as ?na?veness?. In fact, we con-
firmed through a preliminary experiment that
LDA does not improve the transliteration per-
formance over the baseline.
6 Experiments
6.1 Evaluation
In this section, we compare the following
models: alpha-beta (AB), joint source channel
(JSC), latent class transliteration (LCT), and
latent semantic transliteration based on Dirich-
let mixture (DM-LST).
For the performance evaluation, we used three
language pairs, namely, English-Japanese (En-
Ja), English-Chinese (En-Ch), and English-
Korean (En-Ko), from the transliteration shared
task at NEWS 2009 (Li et al, 2009a; Li et al,
2009b). The size of each training/test set is
shown in the first column of Table 1. In general,
rn, a set of one or more reference transliterated
words, is associated with the n-th input sn in the
training/test corpus. Let cn,i, cn,2, ... be the out-
put of the transliteration system, i.e., the candi-
dates with highest probabilities assigned by the
transliteration model being evaluated. We used
the following three performance measures:
? ACC (averaged Top-1 accuracy): For ev-
ery ?sn, rn?, let an be an = 1 if the can-
didate with the highest probability cn,1 is
contained in the reference set rn and an =
0 otherwise. ACC is then calculated as
ACC 1N
?N
i=1 sn.
? MFS (mean F score): Let the
reference transliterated word clos-
est to the top-1 candidate cn, 1 be
r?n = arg minrn,j?rn ED(cn,1, rn,j), where
ED is the edit distance. The F-score of the
top candidate cn,1 for the n-th input sn is
then given by:
Pn = LSC(cn,1, r?n)/|cn,1| (17)
Rn = LCS(cn,1, r?n)/|r?n| (18)
Fn = 2RiPi/(Ri + Pi), (19)
where |x| is the length of string x, and
LCS(x, y) is the length of the longest com-
mon subsequence of x and y. Edit distance,
lengths of strings, and LCS are measured
in Unicode characters. Finally, MFS is de-
fined as MFS = 1N
?N
i=1 Fn.
? MRR (mean reciprocal rank): Of the
ranked candidates cn,1, cn,2, ..., let the high-
est ranked one which is also included in
the reference set rn be cn,j . We then
define reciprocal rank RRn = 1/j. If
none of the candidates are in the refer-
ence, RRn = 0. MRR is then defined by
MRR = 1N
?N
n=1RRn.
We used Kneser-Nay smoothing to smooth the
TU probabilities for LCT. The number of EM
iterations is fixed to 15 for all the models, based
on the result of preliminary experiments.
The reduce width R and the beam width B
for the stack decoder are fixed to R = 8 and
B = 32, because the transliteration generation
performance increased very little beyond these
widths based on the experiment using the de-
velopment set. We also optimized M , i.e., the
number of latent classes for LCT and DM-LST,
for each language pair and model in the same
way based on the development set.
35
Table 1: Performance comparison of transliteration
models
Language pair Model ACC MFS MRR
En-Ja AB 0.293 0.755 0.378
Train: 23,225 JSC 0.326 0.770 0.428
Test: 1,489 LCT 0.345 0.768 0.437
DM-LST 0.349 0.776 0.444
En-Ch AB 0.358 0.741 0.471
Train: 31,961 JSC 0.417 0.761 0.527
Test: 2,896 LCT 0.430 0.764 0.532
DM-LST 0.445 0.770 0.546
En-Ko AB 0.145 0.537 0.211
Train: 4,785 JSC 0.151 0.543 0.221
Test: 989 LCT 0.079 0.483 0.167
DM-LST 0.174 0.556 0.237
6.2 Results
We compared the performance of each
transliteration model in Table 1. For the lan-
guage pairs En-Ja and En-Ch, all the perfor-
mance increase in the order of AB < JSC <
LCT < DM-LST, showing the superiority our
proposed method. For the language pair En-
Ko, the performance for LCT re-ranking con-
siderably decreases compared to JSC. We sus-
pect this is due to the relatively small number
of training set, which caused the excessive fitting
to the data. We also found out that the optimal
value of M which maximizes the performance of
DM-LST is equal to or smaller than that of LCT.
This goes along with the findings (Yamamoto
and Sadamitsu, 2005) that Dirichlet mixture of-
ten achieves better language model perplexity
with smaller dimensionality compared to other
models.
Specific examples in the En-Ja test set whose
transliteration is improved by the proposed
methods include ?dijon ?????? dijon? and
?goldenberg ????????? g?rudenb?gu?.
Conventional methods, including LCT, sug-
gested ????? diyon? and ?????????
g?rudenberugu?, meaning that the translitera-
tion model is affected and biased towards non-
English pronunciation. The proposed method
can retain the major class of transliteration char-
acteristics (which is English in this case) and can
deal with multiple language origins depending
on transliteration pairs at the same time.
This trend can be also confirmed in other
language pairs, En-Ch and En-Ko. In En-Ch,
the transliterated words of ?covell? and ?nether-
wood? are improved ? ???? kefuer ???
? keweier? and ?????? neitehewude ??
??? neisewude?, respectively. in En-Ko, the
transliterated word of ?darling? is improved ??
?? dareuling? ? ??? dalling?.
We also observed that ?gutheim ?????
gutehaimu in En-Ch and martina ????
? mareutina in En-Ko are correctly translated
by the proposed method, even though they do
not have the English origin. Generally speak-
ing, however, how these non-English words are
pronounced depend on the context, as ?charles?
has different pronunciation in English and in
French, with the soft ?sh? sound at the begin-
ning. We need external clues to disambiguate
such transliteration, such as context information
and/or Web statistics.
7 Conclusion
In this paper, we proposed the latent seman-
tic transliteration model based on Dirichlet mix-
ture (DM-LST) as the extension to the latent
class transliteration model. The experimental
results showed the superior transliteration per-
formance over the conventional methods, since
DM-LST can alleviate the overfitting problem
and can capture multiple language origins. One
drawback is that it cannot deal with dependen-
cies of higher order of TU n-grams than bigrams.
How to incorporate these dependencies into the
latent transliteration models is the future work.
References
Farooq Ahmad and Grzegorz Kondrak. 2005. Learn-
ing a spelling error model from search query logs.
In Proc. of EMNLP-2005, pages 955?962.
David M. Blei, Andrew Y. Ng, and Michael I. Jor-
dan. 2003. Latent dirichlet alocation. Journal of
Machine Learning Research, 3:993?1022.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling. In Proc.
ACL-2000, pages 286?293.
36
Eric Brill, Gary Kacmarcik, and Chris Brockett.
2001. Automatically harvesting katakana-english
term pairs from search engine query logs. In Proc.
NLPRS-2001, pages 393?399.
Masato Hagiwara and Satoshi Sekine. 2011. Latent
class transliteration based on source language ori-
gin. In Proc. of ACL-HLT 2011, pages 53?57.
Masato Hagiwara and Hisami Suzuki. 2009.
Japanese query alteration based on semantic sim-
ilarity. In Proc. of NAACL-2009, page 191.
Kevin Knight and Graehl Jonathan. 1998. Ma-
chine transliteration. Computational Linguistics,
24:599?612.
Haizhou Li, Zhang Min, and Su Jian. 2004. A joint
source-channel model for machine transliteration.
In Proc. of ACL 2004, pages 159?166.
Haizhou Li, Khe Chai Sum, Jin-Shea Kuo, and
Minghui Dong. 2007. Semantic transliteration
of personal names. In Proc. of ACL 2007, pages
120?127.
Haizhou Li, A Kumaran, Vladimir Pervouchine, and
Min Zhang. 2009a. Report of news 2009 machine
transliteration shared task. In Proc. of the 2009
Named Entities Workshop, pages 1?18.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009b. Whitepaper of news 2009
machine transliteration shared task. In Proc. of
the 2009 Named Entities Workshop, pages 19?26.
Ariadna Font Llitjos and Alan W. Black. 2001.
Knowledge of language origin improves pronun-
ciation accuracy. In Proc. of Eurospeech, pages
1919?1922.
Kamal Nigam, Andrew Kachites McCallum, Sebas-
tian Thrun, and Tom Mitchell. 2000. Text clas-
sification from labeled and unlabeled documents
using em. Machine Learning, 39(2):103?134.
K. Sj?lander, K. Karplus, M. Brown, R. Hunghey,
A. Krogh, I.S. Mian, and D. Haussler. 1996.
Dirichlet mixtures:a method for improved detec-
tion of weak but significant protein sequence
homology. Computer Applications in the Bio-
sciences, 12(4):327?345.
Mikio Yamamoto and Kugatsu Sadamitsu. 2005.
Dirichlet mixtures in text modeling. CS Technical
Report, CS-TR-05-1.
Mikio Yamamoto, Kugatsu Sadamitsu, and Takuya
Mishina. 2003. Context modeling using dirichlet
mixtures and its applications to language models
(in japnaese). IPSJ, 2003-SLP-48:29?34.
37
