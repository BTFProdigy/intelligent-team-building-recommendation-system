Proceedings of the 12th Conference of the European Chapter of the ACL, pages 69?76,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Incremental Parsing with Parallel Multiple Context-Free Grammars
Krasimir Angelov
Chalmers University of Technology
Go?teborg, Sweden
krasimir@chalmers.se
Abstract
Parallel Multiple Context-Free Grammar
(PMCFG) is an extension of context-free
grammar for which the recognition problem is
still solvable in polynomial time. We describe
a new parsing algorithm that has the advantage
to be incremental and to support PMCFG
directly rather than the weaker MCFG formal-
ism. The algorithm is also top-down which
allows it to be used for grammar based word
prediction.
1 Introduction
Parallel Multiple Context-Free Grammar (PMCFG)
(Seki et al, 1991) is one of the grammar formalisms
that have been proposed for the syntax of natural lan-
guages. It is an extension of context-free grammar
(CFG) where the right hand side of the production rule
is a tuple of strings instead of only one string. Using tu-
ples the grammar can model discontinuous constituents
which makes it more powerful than context-free gram-
mar. In the same time PMCFG has the advantage to be
parseable in polynomial time which makes it attractive
from computational point of view.
A parsing algorithm is incremental if it reads the in-
put one token at the time and calculates all possible
consequences of the token, before the next token is
read. There is substantial evidence showing that hu-
mans process language in an incremental fashion which
makes the incremental algorithms attractive from cog-
nitive point of view.
If the algorithm is also top-down then it is possible
to predict the next word from the sequence of preced-
ing words using the grammar. This can be used for
example in text based dialog systems or text editors for
controlled language where the user might not be aware
of the grammar coverage. In this case the system can
suggest the possible continuations.
A restricted form of PMCFG that is still stronger
than CFG is Multiple Context-Free Grammar (MCFG).
In Seki and Kato (2008) it has been shown that
MCFG is equivalent to string-based Linear Context-
Free Rewriting Systems and Finite-Copying Tree
Transducers and it is stronger than Tree Adjoining
Grammars (Joshi and Schabes, 1997). Efficient recog-
nition and parsing algorithms for MCFG have been de-
scribed in Nakanishi et al (1997), Ljunglo?f (2004) and
Burden and Ljunglo?f (2005). They can be used with
PMCFG also but it has to be approximated with over-
generating MCFG and post processing is needed to fil-
ter out the spurious parsing trees.
We present a parsing algorithm that is incremental,
top-down and supports PMCFG directly. The algo-
rithm exploits a view of PMCFG as an infinite context-
free grammar where new context-free categories and
productions are generated during parsing. It is trivial to
turn the algorithm into statistical by attaching probabil-
ities to each rule.
In Ljunglo?f (2004) it has been shown that the Gram-
matical Framework (GF) formalism (Ranta, 2004) is
equivalent to PMCFG. The algorithm was implemented
as part of the GF interpreter and was evaluated with the
resource grammar library (Ranta, 2008) which is the
largest collection of grammars written in this formal-
ism. The incrementality was used to build a help sys-
tem which suggests the next possible words to the user.
Section 2 gives a formal definition of PMCFG. In
section 3 the procedure for ?linearization? i.e. the
derivation of string from syntax tree is defined. The
definition is needed for better understanding of the for-
mal proofs in the paper. The algorithm introduction
starts with informal description of the idea in section
4 and after that the formal rules are given in section
5. The implementation details are outlined in section 6
and after that there are some comments on the evalua-
tion in section 7. Section 8 gives a conclusion.
2 PMCFG definition
Definition 1 A parallel multiple context-free grammar
is an 8-tuple G = (N,T, F, P, S, d, r, a) where:
? N is a finite set of categories and a positive integer
d(A) called dimension is given for each A ? N .
? T is a finite set of terminal symbols which is dis-
joint with N .
? F is a finite set of functions where the arity a(f)
and the dimensions r(f) and di(f) (1 ? i ?
a(f)) are given for every f ? F . For every posi-
tive integer d, (T ?)d denote the set of all d-tuples
69
of strings over T . Each function f ? F is a to-
tal mapping from (T ?)d1(f) ? (T ?)d2(f) ? ? ? ? ?
(T ?)da(f)(f) to (T ?)r(f), defined as:
f := (?1, ?2, . . . , ?r(f))
Here ?i is a sequence of terminals and ?k; l?
pairs, where 1 ? k ? a(f) is called argument
index and 1 ? l ? dk(f) is called constituent
index.
? P is a finite set of productions of the form:
A? f [A1, A2, . . . , Aa(f)]
where A ? N is called result category,
A1, A2, . . . , Aa(f) ? N are called argument cat-
egories and f ? F is the function symbol. For
the production to be well formed the conditions
di(f) = d(Ai) (1 ? i ? a(f)) and r(f) = d(A)
must hold.
? S is the start category and d(S) = 1.
We use the same definition of PMCFG as is used by
Seki and Kato (2008) and Seki et al (1993) with the
minor difference that they use variable names like xkl
while we use ?k; l? to refer to the function arguments.
As an example we will use the anbncn language:
S ? c[N ]
N ? s[N ]
N ? z[]
c := (?1; 1? ?1; 2? ?1; 3?)
s := (a ?1; 1?, b ?1; 2?, c ?1; 3?)
z := (, , )
Here the dimensions are d(S) = 1 and d(N) = 3 and
the arities are a(c) = a(s) = 1 and a(z) = 0.  is the
empty string.
3 Derivation
The derivation of a string in PMCFG is a two-step pro-
cess. First we have to build a syntax tree of a category
S and after that to linearize this tree to string. The defi-
nition of a syntax tree is recursive:
Definition 2 (f t1 . . . ta(f)) is a tree of category A if
ti is a tree of category Bi and there is a production:
A? f [B1 . . . Ba(f)]
The abstract notation for ?t is a tree of category A?
is t : A. When a(f) = 0 then the tree does not have
children and the node is called leaf.
The linearization is bottom-up. The functions in the
leaves do not have arguments so the tuples in their defi-
nitions already contain constant strings. If the function
has arguments then they have to be linearized and the
results combined. Formally this can be defined as a
function L applied to the syntax tree:
L(f t1 t2 . . . ta(f)) = (x1, x2 . . . xr(f))
where xi = K(L(t1),L(t2) . . .L(ta(f))) ?i
and f := (?1, ?2 . . . ?r(f)) ? F
The function uses a helper function K which takes the
already linearized arguments and a sequence ?i of ter-
minals and ?k; l? pairs and returns a string. The string
is produced by simple substitution of each ?k; l? with
the string for constituent l from argument k:
K ? (?1?k1; l1??2?k2; l2? . . . ?n) = ?1?k1l1?2?k2l2 . . . ?n
where ?i ? T ?. The recursion in L terminates when a
leaf is reached.
In the example anbncn language the function z does
not have arguments and it corresponds to the base case
when n = 0. Every application of s over another tree
t : N increases n by one. For example the syntax tree
(s (s z)) will produce the tuple (aa, bb, cc). Finally the
application of c combines all elements in the tuple in
a single string i.e. c (s (s z)) will produce the string
aabbcc.
4 The Idea
Although PMCFG is not context-free it can be approx-
imated with an overgenerating context-free grammar.
The problem with this approach is that the parser pro-
duces many spurious parse trees that have to be filtered
out. A direct parsing algorithm for PMCFG should
avoid this and a careful look at the difference between
PMCFG and CFG gives an idea. The context-free ap-
proximation of anbncn is the language a?b?c? with
grammar:
S ? ABC
A?  | aA
B ?  | bB
C ?  | cC
The string ?aabbcc? is in the language and it can be
derived with the following steps:
S
? ABC
? aABC
? aaABC
? aaBC
? aabBC
? aabbBC
? aabbC
? aabbcC
? aabbccC
? aabbcc
70
The grammar is only an approximation because there
is no enforcement that we will use only equal number
of reductions for A, B and C. This can be guaranteed
if we replace B and C with new categories B? and C ?
after the derivation of A:
B? ? bB?? C ? ? cC ??
B?? ? bB??? C ?? ? cC ???
B??? ?  C ??? ? 
In this case the only possible derivation from aaB?C ?
is aabbcc.
The PMCFG parser presented in this paper works
like context-free parser, except that during the parsing
it generates fresh categories and rules which are spe-
cializations of the originals. The newly generated rules
are always versions of already existing rules where
some category is replaced with new more specialized
category. The generation of specialized categories pre-
vents the parser from recognizing phrases that are oth-
erwise withing the scope of the context-free approxi-
mation of the original grammar.
5 Parsing
The algorithm is described as a deductive process in
the style of (Shieber et al, 1995). The process derives
a set of items where each item is a statement about the
grammatical status of some substring in the input.
The inference rules are in natural deduction style:
X1 . . . Xn
Y
< side conditions on X1, . . . , Xn >
where the premises Xi are some items and Y is the
derived item. We assume that w1 . . . wn is the input
string.
5.1 Deduction Rules
The deduction system deals with three types of items:
active, passive and production items.
Productions In Shieber?s deduction systems the
grammar is a constant and the existence of a given pro-
duction is specified as a side condition. In our case the
grammar is incrementally extended at runtime, so the
set of productions is part of the deduction set. The pro-
ductions from the original grammar are axioms and are
included in the initial deduction set.
Active Items The active items represent the partial
parsing result:
[kjA? f [ ~B]; l : ? ? ?] , j ? k
The interpretation is that there is a function f with a
corresponding production:
A? f [ ~B]
f := (?1, . . . ?l?1, ??, . . . ?r(f))
such that the tree (f t1 . . . ta(f)) will produce the sub-
string wj+1 . . . wk as a prefix in constituent l for any
INITIAL PREDICT
S ? f [ ~B]
[00S ? f [ ~B]; 1 : ??]
S - start category, ? = rhs(f, 1)
PREDICT
Bd ? g[~C] [
k
jA? f [ ~B]; l : ? ? ?d; r? ?]
[kkBd ? g[~C]; r : ??]
? = rhs(g, r)
SCAN
[kjA? f [ ~B]; l : ? ? s ?]
[k+1j A? f [ ~B]; l : ? s ? ?]
s = wk+1
COMPLETE
[kjA? f [ ~B]; l : ??]
N ? f [ ~B] [kjA; l;N ]
N = (A, l, j, k)
COMBINE
[ujA? f [ ~B]; l : ? ? ?d; r? ?] [
k
uBd; r;N ]
[kjA? f [ ~B{d := N}]; l : ? ?d; r? ? ?]
Figure 1: Deduction Rules
sequence of arguments ti : Bi. The sequence ? is the
part that produced the substring:
K(L(t1),L(t2) . . .L(ta(f))) ? = wj+1 . . . wk
and ? is the part that is not processed yet.
Passive Items The passive items are of the form:
[kjA; l;N ] , j ? k
and state that there exists at least one production:
A? f [ ~B]
f := (?1, ?2, . . . ?r(f))
and a tree (f t1 . . . ta(f)) : A such that the constituent
with index l in the linearization of the tree is equal to
wj+1 . . . wk. Contrary to the active items in the passive
the whole constituent is matched:
K(L(t1),L(t2) . . .L(ta(f))) ?l = wj+1 . . . wk
Each time when we complete an active item, a pas-
sive item is created and at the same time we cre-
ate a new category N which accumulates all produc-
tions forA that produce thewj+1 . . . wk substring from
constituent l. All trees of category N must produce
wj+1 . . . wk in the constituent l.
There are six inference rules (see figure 1).
The INITIAL PREDICT rule derives one item spanning
the 0 ? 0 range for each production with the start cat-
egory S on the left hand side. The rhs(f, l) function
returns the constituent with index l of function f .
In the PREDICT rule, for each active item with dot be-
fore a ?d; r? pair and for each production for Bd, a new
active item is derived where the dot is in the beginning
of constituent r in g.
When the dot is before some terminal s and s is equal
to the current terminal wk then the SCAN rule derives a
new item where the dot is moved to the next position.
71
When the dot is at the end of an active item then it
is converted to passive item in the COMPLETE rule. The
category N in the passive item is a fresh category cre-
ated for each unique (A, l, j, k) quadruple. A new pro-
duction is derived for N which has the same function
and arguments as in the active item.
The item in the premise of COMPLETE was at some
point predicted in PREDICT from some other item. The
COMBINE rule will later replace the occurence A in the
original item (the premise of PREDICT) with the special-
ization N .
The COMBINE rule has two premises: one active item
and one passive. The passive item starts from position
u and the only inference rule that can derive items with
different start positions is PREDICT. Also the passive
item must have been predicted from active item where
the dot is before ?d; r?, the category for argument num-
ber d must have been Bd and the item ends at u. The
active item in the premise of COMBINE is such an item
so it was one of the items used to predict the passive
one. This means that we can move the dot after ?d; r?
and the d-th argument is replaced with its specialization
N .
If the string ? contains another reference to the d-th
argument then the next time when it has to be predicted
the rule PREDICT will generate active items, only for
those productions that were successfully used to parse
the previous constituents. If a context-free approxima-
tion was used this would have been equivalent to unifi-
cation of the redundant subtrees. Instead this is done at
runtime which also reduces the search space.
The parsing is successful if we had derived the
[n0S; 1;S
?] item, where n is the length of the text, S is
the start category and S? is the newly created category.
The parser is incremental because all active items
span up to position k and the only way to move to the
next position is the SCAN rule where a new symbol from
the input is consumed.
5.2 Soundness
The parsing system is sound if every derivable item rep-
resents a valid grammatical statement under the inter-
pretation given to every type of item.
The derivation in INITIAL PREDICT and PREDICT is
sound because the item is derived from existing pro-
duction and the string before the dot is empty so:
K ?  = 
The rationale for SCAN is that if
K ? ? = wj?1 . . . wk
and s = wk+1 then
K ? (? s) = wj?1 . . . wk+1
If the item in the premise is valid then it is based on
existing production and function and so will be the item
in the consequent.
In the COMPLETE rule the dot is at the end of the
string. This means that wj+1 . . . wk will be not just
a prefix in constituent l of the linearization but the full
string. This is exactly what is required in the semantics
of the passive item. The passive item is derived from
a valid active item so there is at least one production
for A. The category N is unique for each (A, l, j, k)
quadruple so it uniquely identifies the passive item in
which it is placed. There might be many productions
that can produce the passive item but all of them should
be able to generate wj+1 . . . wk and they are exactly
the productions that are added to N . From all this ar-
guments it follows that COMPLETE is sound.
The COMBINE rule is sound because from the active
item in the premise we know that:
K ? ? = wj+1 . . . wu
for every context ? built from the trees:
t1 : B1; t2 : B2; . . . ta(f) : Ba(f)
From the passive item we know that every production
forN produces thewu+1 . . . wk in r. From that follows
that
K ?? (??d; r?) = wj+1 . . . wk
where ?? is the same as ? except that Bd is replaced
withN . Note that the last conclusion will not hold if we
were using the original context because Bd is a more
general category and can contain productions that does
not derive wu+1 . . . wk.
5.3 Completeness
The parsing system is complete if it derives an item
for every valid grammatical statement. In our case we
have to prove that for every possible parse tree the cor-
responding items will be derived.
The proof for completeness requires the following
lemma:
Lemma 1 For every possible syntax tree
(f t1 . . . ta(f)) : A
with linearization
L(ft1 . . . ta(f)) = (x1, x2 . . . xd(A))
where xl = wj+1 . . . wk, the system will derive an item
[kjA; l;A
?] if the item [kjA ? f [ ~B]; l : ??l] was pre-
dicted before that. We assume that the function defini-
tion is:
f := (?1, ?2 . . . ?r(f))
The proof is by induction on the depth of the tree.
If the tree has only one level then the function f does
not have arguments and from the linearization defini-
tion and from the premise in the lemma it follows that
?l = wj+1 . . . wk. From the active item in the lemma
72
by applying iteratively the SCAN rule and finally the
COMPLETE rule the system will derive the requested
item.
If the tree has subtrees then we assume that the
lemma is true for every subtree and we prove it for the
whole tree. We know that
K ? ?l = wj+1 . . . wk
Since the function K does simple substitution it is pos-
sible for each ?d; s? pair in ?l to find a new range in the
input string j??k? such that the lemma to be applicable
for the corresponding subtree td : Bd. The terminals in
?l will be processed by the SCAN rule. Rule PREDICT
will generate the active items required for the subtrees
and the COMBINE rule will consume the produced pas-
sive items. Finally the COMPLETE rule will derive the
requested item for the whole tree.
From the lemma we can prove the completeness of
the parsing system. For every possible tree t : S such
that L(t) = (w1 . . . wn) we have to prove that the
[n0S; 1;S
?] item will be derived. Since the top-level
function of the tree must be from production for S the
INITIAL PREDICT rule will generate the active item in
the premise of the lemma. From this and from the as-
sumptions for t it follows that the requested passive
item will be derived.
5.4 Complexity
The algorithm is very similar to the Earley (1970) algo-
rithm for context-free grammars. The similarity is even
more apparent when the inference rules in this paper
are compared to the inference rules for the Earley al-
gorithm presented in Shieber et al (1995) and Ljunglo?f
(2004). This suggests that the space and time complex-
ity of the PMCFG parser should be similar to the com-
plexity of the Earley parser which is O(n2) for space
and O(n3) for time. However we generate new cate-
gories and productions at runtime and this have to be
taken into account.
Let theP(j) function be the maximal number of pro-
ductions generated from the beginning up to the state
where the parser has just consumed terminal number
j. P(j) is also the upper limit for the number of cat-
egories created because in the worst case there will be
only one production for each new category.
The active items have two variables that directly de-
pend on the input size - the start index j and the end
index k. If an item starts at position j then there are
(n ? j + 1) possible values for k because j ? k ? n.
The item also contains a production and there are P(j)
possible choices for it. In total there are:
n?
j=0
(n? j + 1)P(j)
possible choices for one active item. The possibilities
for all other variables are only a constant factor. The
P(j) function is monotonic because the algorithm only
adds new productions and never removes. From that
follows the inequality:
n?
j=0
(n? j + 1)P(j) ? P(n)
n?
i=0
(n? j + 1)
which gives the approximation for the upper limit:
P(n)
n(n+ 1)
2
The same result applies to the passive items. The only
difference is that the passive items have only a category
instead of a full production. However the upper limit
for the number of categories is the same. Finally the
upper limit for the total number of active, passive and
production items is:
P(n)(n2 + n+ 1)
The expression for P(n) is grammar dependent but
we can estimate that it is polynomial because the set
of productions corresponds to the compact representa-
tion of all parse trees in the context-free approximation
of the grammar. The exponent however is grammar de-
pendent. From this we can expect that asymptotic space
complexity will be O(ne) where e is some parameter
for the grammar. This is consistent with the results in
Nakanishi et al (1997) and Ljunglo?f (2004) where the
exponent also depends on the grammar.
The time complexity is proportional to the number
of items and the time needed to derive one item. The
time is dominated by the most complex rule which in
this algorithm is COMBINE. All variables that depend
on the input size are present both in the premises and
in the consequent except u. There are n possible values
for u so the time complexity is O(ne+1).
5.5 Tree Extraction
If the parsing is successful we need a way to extract the
syntax trees. Everything that we need is already in the
set of newly generated productions. If the goal item is
[n0S; 0;S
?] then every tree t of category S? that can be
constructed is a syntax tree for the input sentence (see
definition 2 in section 3 again).
Note that the grammar can be erasing; i.e., there
might be productions like this:
S ? f [B1, B2, B3]
f := (?1; 1??3; 1?)
There are three arguments but only two of them are
used. When the string is parsed this will generate a
new specialized production:
S? ? f [B?1, B2, B
?
3]
Here S,B1 and B3 are specialized to S?, B?1 and B
?
3
but the B2 category is still the same. This is correct
73
because actually any subtree for the second argument
will produce the same result. Despite this it is some-
times useful to know which parts of the tree were used
and which were not. In the GF interpreter such un-
used branches are replaced by meta variables. In this
case the tree extractor should check whether the cate-
gory also exists in the original set of categories N in
the grammar.
Just like with the context-free grammars the parsing
algorithm is polynomial but the chart can contain ex-
ponential or even infinite number of trees. Despite this
the chart is a compact finite representation of the set of
trees.
6 Implementation
Every implementation requires a careful design of the
data structures in the parser. For efficient access the set
of items is split into four subsets: A, Sj , C and P. A
is the agenda i.e. the set of active items that have to be
analyzed. Sj contains items for which the dot is before
an argument reference and which span up to position j.
C is the set of possible continuations i.e. a set of items
for which the dot is just after a terminal. P is the set
of productions. In addition the set F is used internally
for the generatation of fresh categories. The sets C,
Sj and F are used as association maps. They contain
associations like k 7? v where k is the key and v is the
value. All maps except F can contain more than one
value for one and the same key.
The pseudocode of the implementation is given in
figure 2. There are two procedures Init and Compute.
Init computes the initial values of S, P and A. The
initial agenda A is the set of all items that can be pre-
dicted from the start category S (INITIAL PREDICT rule).
Compute consumes items from the current agenda
and applies the SCAN, PREDICT, COMBINE or COMPLETE
rule. The case statement matches the current item
against the patterns of the rules and selects the proper
rule. The PREDICT and COMBINE rules have two
premises so they are used in two places. In both cases
one of the premises is related to the current item and a
loop is needed to find item matching the other premis.
The passive items are not independent entities but
are just the combination of key and value in the set F.
Only the start position of every item is kept because the
end position for the interesting passive items is always
the current position and the active items are either in
the agenda if they end at the current position or they
are in the Sj set if they end at position j. The active
items also keep only the dot position in the constituent
because the constituent definition can be retrieved from
the grammar. For this reason the runtime representation
of the items is [j;A ? f [ ~B]; l; p] where j is the start
position of the item and p is the dot position inside the
constituent.
The Compute function returns the updated S and P
sets and the set of possible continuations C. The set of
continuations is a map indexed by a terminal and the
Language Productions Constituents
Bulgarian 3516 75296
English 1165 8290
German 8078 21201
Swedish 1496 8793
Table 1: GF Resource Grammar Library size in number
of PMCFG productions and discontinuous constituents
0
200
400
600
800
1000
1200
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39
Number of Tokens
ms
German Bulgarian Swedish English
Figure 3: Parser performance in miliseconds per token
values are active items. The parser computes the set of
continuations at each step and if the current terminal is
one of the keys the set of values for it is taken as an
agenda for the next step.
7 Evaluation
The algorithm was evaluated with four languages from
the GF resource grammar library (Ranta, 2008): Bul-
garian, English, German and Swedish. These gram-
mars are not primarily intended for parsing but as a
resource from which smaller domain dependent gram-
mars are derived for every application. Despite this, the
resource grammar library is a good benchmark for the
parser because these are the biggest GF grammars.
The compiler converts a grammar written in the
high-level GF language to a low-level PMCFG gram-
mar which the parser can use directly. The sizes of
the grammars in terms of number of productions and
number of unique discontinuous constituents are given
on table 1. The number of constituents roughly cor-
responds to the number of productions in the context-
free approximation of the grammar. The parser per-
formance in terms of miliseconds per token is shown in
figure 3. In the evaluation 34272 sentences were parsed
and the average time for parsing a given number of to-
kens is drawn in the chart. As it can be seen, although
the theoretical complexity is polynomial, the real-time
performance for practically interesting grammars tends
to be linear.
8 Conclusion
The algorithm has proven useful in the GF system. It
accomplished the initial goal to provide suggestions
74
procedure Init() {
k = 0
Si = ?, for every i
P = the set of productions P in the grammar
A = ?
forall S ? f [ ~B] ? P do // INITIAL PREDICT
A = A+ [0;S ? f [ ~B]; 1; 0]
return (S,P,A)
}
procedure Compute(k, (S,P,A)) {
C = ?
F = ?
while A 6= ? do {
let x ? A, x ? [j;A? f [ ~B]; l; p]
A = A? x
case the dot in x is {
before s ? T ? C = C+ (s 7? [j;A? f [ ~B]; l; p+ 1]) // SCAN
before ?d; r? ? if ((Bd, r) 7? (x, d)) 6? Sk then {
Sk = Sk + ((Bd, r) 7? (x, d))
forall Bd ? g[~C] ? P do // PREDICT
A = A+ [k;Bd ? g[~C]; r; 0]
}
forall (k;Bd, r) 7? N ? F do // COMBINE
A = A+ [j;A? f [ ~B{d := N}]; l; p+ 1]
at the end ? if ?N.((j, A, l) 7? N ? F) then {
forall (N, r) 7? (x?, d?) ? Sk do // PREDICT
A = A+ [k;N ? f [ ~B]; r; 0]
} else {
generate fresh N // COMPLETE
F = F+ ((j, A, l) 7? N)
forall (A, l) 7? ([j?;A? ? f ?[ ~B?]; l?; p?], d) ? Sj do // COMBINE
A = A+ [j?;A? ? f ?[ ~B?{d := N}]; l?; p? + 1]
}
P = P+ (N ? f [ ~B])
}
}
return (S,P,C)
}
Figure 2: Pseudocode of the parser implementation
75
in text based dialog systems and in editors for con-
trolled languages. Additionally the algorithm has prop-
erties that were not envisaged in the beginning. It
works with PMCFG directly rather that by approxima-
tion with MCFG or some other weaker formalism.
Since the Linear Context-Free Rewriting Systems,
Finite-Copying Tree Transducers and Tree Adjoining
Grammars can be converted to PMCFG, the algorithm
presented in this paper can be used with the converted
grammar. The approach to represent context-dependent
grammar as infinite context-free grammar might be ap-
plicable to other formalisms as well. This will make it
very attractive in applications where some of the other
formalisms are already in use.
References
Ha?kan Burden and Peter Ljunglo?f. 2005. Parsing
linear context-free rewriting systems. In Proceed-
ings of the Ninth International Workshop on Parsing
Technologies (IWPT), pages 11?17, October.
Jay Earley. 1970. An efficient context-free parsing al-
gorithm. Commun. ACM, 13(2):94?102.
Aravind Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In Grzegorz Rozenberg and
Arto Salomaa, editors, Handbook of Formal Lan-
guages. Vol 3: Beyond Words, chapter 2, pages 69?
123. Springer-Verlag, Berlin/Heidelberg/New York.
Peter Ljunglo?f. 2004. Expressivity and Complexity of
the Grammatical Framework. Ph.D. thesis, Depart-
ment of Computer Science, Gothenburg University
and Chalmers University of Technology, November.
Ryuichi Nakanishi, Keita Takada, and Hiroyuki Seki.
1997. An Efficient Recognition Algorithm for Mul-
tiple ContextFree Languages. In Fifth Meeting
on Mathematics of Language. The Association for
Mathematics of Language, August.
Aarne Ranta. 2004. Grammatical Framework: A
Type-Theoretical Grammar Formalism. Journal of
Functional Programming, 14(2):145?189, March.
Aarne Ranta. 2008. GF Resource Grammar Library.
digitalgrammars.com/gf/lib/.
Hiroyuki Seki and Yuki Kato. 2008. On the Genera-
tive Power of Multiple Context-Free Grammars and
Macro Grammars. IEICE-Transactions on Info and
Systems, E91-D(2):209?221.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii,
and Tadao Kasami. 1991. On multiple context-
free grammars. Theoretical Computer Science,
88(2):191?229, October.
Hiroyuki Seki, Ryuichi Nakanishi, Yuichi Kaji,
Sachiko Ando, and Tadao Kasami. 1993. Par-
allel Multiple Context-Free Grammars, Finite-State
Translation Systems, and Polynomial-Time Recog-
nizable Subclasses of Lexical-Functional Grammars.
In 31st Annual Meeting of the Association for Com-
putational Linguistics, pages 130?140. Ohio State
University, Association for Computational Linguis-
tics, June.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and Implementation of
Deductive Parsing. Journal of Logic Programming,
24(1&2):3?36.
76
Proceedings of the EACL 2009 Demonstrations Session, pages 9?12,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
Grammatical Framework Web Service
Bjo?rn Bringert? and Krasimir Angelov and Aarne Ranta
Department of Computer Science and Engineering
Chalmers University of Technology and University of Gothenburg
{bringert,krasimir,aarne}@chalmers.se
Abstract
We present a web service for natural language
parsing, prediction, generation, and translation
using grammars in Portable Grammar Format
(PGF), the target format of the Grammatical
Framework (GF) grammar compiler. The web
service implementation is open source, works
with any PGF grammar, and with any web
server that supports FastCGI. The service ex-
poses a simple interface which makes it pos-
sible to use it for interactive natural language
web applications. We describe the function-
ality and interface of the web service, and
demonstrate several applications built on top
of it.
1 Introduction
Current web applications often consist of JavaScript
code that runs in the user?s web browser, with server-
side code that does the heavy lifting. We present a web
service for natural language processing with Portable
Grammar Format (PGF, Angelov et al, 2008) gram-
mars, which can be used to build interactive natural lan-
guage web applications. PGF is the back-end format
to which Grammatical Framework (GF, Ranta, 2004)
grammars are compiled. PGF has been designed to al-
low efficient implementations.
The web service has a simple API based solely on
HTTP GET requests. It returns responses in JavaScript
Object Notation (JSON, Crockford, 2006). The server-
side program is distributed as part of the GF software
distribution, under the GNU General Public License
(GPL). The program is generic, in the sense that it can
be used with any PGF grammar without any modifica-
tion of the program.
2 Grammatical Framework
Grammatical Framework (GF, Ranta, 2004) is a type-
theoretical grammar formalism. A GF grammar con-
sists of an abstract syntax, which defines a set of ab-
stract syntax trees, and one or more concrete syntaxes,
which define how abstract syntax trees are mapped to
(and from) strings. The process of producing a string
?Now at Google Inc.
(or, more generally, a feature structure) from an ab-
stract syntax tree is called linearization. The oppo-
site, producing an abstract syntax tree (or several, if the
grammar is ambiguous) from a string is called parsing.
In a small, semantically oriented application gram-
mar, the sentence ?2 is even? may correspond to the
abstract syntax tree Even 2. In a larger, more syn-
tactically oriented grammar, in this case the English
GF resource grammar (Ranta, 2007), the same sen-
tence can correspond to the abstract syntax tree PhrUtt
NoPConj (UttS (UseCl (TTAnt TPres ASimul)
PPos (PredVP (UsePN (NumPN (NumDigits (IDig
D 2)))) (UseComp (CompAP (PositA even A))))))
NoVoc.
2.1 Portable Grammar Format (PGF)
Portable Grammar Format (PGF, Angelov et al, 2008)
is a low-level format to which GF grammars are com-
piled. The PGF Web Service loads PGF files from disk,
and uses them to serve client requests. These PGF files
are normally produced by compiling GF grammars, but
they could also be produced by other means, for exam-
ple by a compiler from another grammar formalism.
Such compilers currently exist for context-free gram-
mars in BNF and EBNF formats, though they compile
via GF.
2.2 Parsing and Word Prediction
For each concrete syntax in a PGF file, there is a pars-
ing grammar, which is a Parallel Multiple Context Free
Grammar (PMCFG, Seki et al, 1991). The PGF inter-
preter uses an efficient parsing algorithm for PMCFG
(Angelov, 2009) which is similar to the Earley algo-
rithm for CFG. The algorithm is top-down and incre-
mental which makes it possible to use it for word com-
pletion. When the whole sentence is known, the parser
just takes the tokens one by one and computes the chart
of all possible parse trees. If the sentence is not yet
complete, then the known tokens can be used to com-
pute a partial parse chart. Since the algorithm is top-
down it is possible to predict the set of valid next tokens
by using just the partial chart.
The prediction can be used in applications to guide
the user to stay within the coverage of the grammar. At
each point the set of valid next tokens is shown and the
user can select one of them.
9
Figure 1: Translator interface. This example uses
the Bronzeage grammar, which consists of simple
syntactic rules along with lexica based on Swadesh
lists. Demo at http://digitalgrammars.com/
translate.
The word prediction is based entirely on the gram-
mar and not on any additional n-gram model. This
means that it works with any PGF grammar and no ex-
tra work is needed. In addition it works well even with
long distance dependencies. For example if the subject
is in a particular gender and the verb requires gender
agreement, then the the correct form is predicted, inde-
pendently on how far the verb is from the subject.
3 Applications
Several interactive web applications have been built
with the PGF Web Service. They are all JavaScript pro-
grams which run in the user?s web browser and send
asynchronous HTTP requests to the PGF Web Service.
3.1 Translator
The simplest application (see Figure 1) presents the
user with a text field for input, and drop-down boxes for
selecting the grammar and language to use. For every
change in the text field, the application asks the PGF
Web Service for a number of possible completions of
the input, and displays them below the text field. The
user can continue typing, or select one of the sugges-
tions. When the current input can be parsed completely,
the input is translated to all available languages.
3.2 Fridge Poetry
The second application is similar in functionality to the
first, but it presents a different user interface. The in-
terface (see Figure 2) mimics the popular refrigerator
magnet poetry sets. However, in contrast to physical
fridge magnets, this application handles inflection au-
tomatically and only allows the construction of gram-
matically correct sentences (as defined by the selected
grammar). It also shows translations for complete in-
puts and allows the user to switch languages.
Figure 2: Fridge poetry screenshot. Demo at http:
//digitalgrammars.com/fridge.
Figure 3: Reasoning screenshot. Demo at http://
digitalgrammars.com/mosg.
3.3 Reasoning
Another application is a natural language reasoning
system which accepts facts and questions from the
users, and tries to answer the questions based on the
facts given. The application uses the PGF Web Service
to parse inputs. It uses two other web services for se-
mantic interpretation and reasoning, respectively. The
semantic interpretation service uses a continuation-
based compositional mapping of abstract syntax terms
to first-order logic formulas (Bringert, 2008). The rea-
soning service is a thin layer on top of the Equinox the-
orem prover and the Paradox model finder (Claessen
and So?rensson, 2003).
4 API
Below, we will show URI paths for each function,
for example /pgf/food.pgf/parse. Arguments
to each function are given in the URL query string,
in application/x-www-form-urlencoded
(Raggett et al, 1999) format. Thus, if the service is
running on example.com, the URI for a request to
parse the string ?this fish is fresh? using the FoodEng
concrete syntax in the food.pgf grammar would
10
be: http://example.com/pgf/food.pgf/
parse?input=this+fish+is+fresh&from=
FoodEng. The functions described below each accept
some subset of the following arguments:
from The name of the concrete syntax to parse with
or translate from. Multiple from arguments can
be given, in which case all the specified languages
are tried. If omitted, all languages (that can be
used for parsing) are used.
cat The name of the abstract syntax category to parse
or translate in, or generate output in. If omitted,
the start category specified in the PGF file is used.
to The name of the concrete syntax to linearize or
translate to. Multiple to arguments can be given,
in which case all the specified languages are used.
If omitted, results for all languages are returned.
input The text to parse, complete or translate. If
omitted, the empty string is used.
tree The abstract syntax tree to linearize.
limit The maximum number of results to return.
All results are returned in UTF-8 encoded JSON or
JSONP format. A jsonp argument can be given to
each function to invoke a callback function when the
response is evaluated in a JavaScript interpreter. This
makes it possible to circumvent the Same Origin Policy
in the web browser and call the PGF Web Service from
applications loaded from another server.
4.1 Grammar List
/pgf retrieves a list of the available PGF files.
4.2 Grammar Info
/pgf/grammar.pgf, where grammar.pgf is the
name of a PGF file on the server, retrieves information
about the given grammar. This information includes
the name of the abstract syntax, the categories in the
abstract syntax, and the list of concrete syntaxes.
4.3 Parsing
/pgf/grammar.pgf/parse parses an input string
and returns a number of abstract syntax trees. Optional
arguments: input, from, cat.
4.4 Completion
/pgf/grammar.pgf/complete returns a list of
predictions for the next token, given a partial input.
Optional arguments: input, from, cat, limit. If
limit is omitted, all results are returned.
4.5 Linearization
/pgf/grammar.pgf/linearize accepts an ab-
stract syntax tree, and returns the results of lineariz-
ing it to one or more languages. Mandatory arguments:
tree. Optional arguments: to.
4.6 Random Generation
/pgf/grammar.pgf/random generates a number
of randomly generated abstract syntax trees for the se-
lected grammar. Optional arguments: cat, limit. If
limit is omitted, one tree is returned.
4.7 Translation
/pgf/grammar.pgf/translate performs text
to text translation. This is done by parsing, followed
by linearization. Optional arguments: input, from,
cat, to.
5 Application to Controlled Languages
The use of controlled languages is becoming more pop-
ular with the development of Web and Semantic Web
technologies. Related projects include Attempto (At-
tempto, 2008), CLOnE (Funk et al, 2007), and Com-
mon Logic Controlled English (CLCE) (Sowa, 2004).
All these projects provide languages which are subsets
of English and have semantic translations into first or-
der logic (CLCE), OWL (CLOnE) or both (Attempto).
In the case of Attempto, the translation is into first order
logic and if it is possible to the weaker OWL language.
The general idea is that since the controlled language
is a subset of some other language it should be under-
standable to everyone without special training. The op-
posite is not true - not every English sentence is a valid
sentence in the controlled language and the user must
learn how to stay within its limitations. Although this
is a disadvantage, in practice it is much easier to re-
member some subset of English phrases rather than to
learn a whole new formal language. Word suggestion
functionality such as that in the PGF Web Service can
help the user stay within the controlled fragment.
In contrast to the above mentioned systems, GF is
not a system which provides only one controlled lan-
guage, but a framework within which the developer can
develop his own language. The task is simplified by the
existence of a resource grammar library (Ranta, 2007)
which takes care of all low-level details such as word
order, and gender, number or case agreement. In fact,
the language developer does not have to be skilled in
linguistics, but does have to be a domain expert and
can concentrate on the specific task.
Most controlled language frameworks are focused
on some subset of English while other languages re-
ceive very little or no attention. With GF, the con-
trolled language does not have to be committed to only
one natural language but could have a parallel grammar
with realizations into many languages. In this case the
user could choose whether to use the English version
or, for example, the French version, and still produce
the same abstract representation.
6 Implementation
The PGF Web Service is a FastCGI program written in
Haskell. The program is a thin layer on top of the PGF
11
interpreter, which implements all the PGF functional-
ity, such as parsing, completion and linearization. The
web service also uses external libraries for FastCGI
communication, and JSON and UTF-8 encoding and
decoding.
The main advantage of using FastCGI instead of
plain CGI is that the PGF file does not have to be
reloaded for each request. Instead, each PGF file is
loaded the first time it is requested, and after that, it is
only reloaded if the file on disk is changed.
7 Performance
The web service layer introduces minimal overhead.
The typical response time for a parse request with a
small grammar, when running on a typical current PC,
is around 1 millisecond. For large grammars, response
times can be on the order of several seconds, but this is
entirely dependent on the PGF interpreter implementa-
tion.
The server is multi-threaded, with one lightweight
thread for each client request. A single instance of the
server can run threads on all cores of a multi-core pro-
cessor. Since the server maintains no state and requires
no synchronization, it can be easily replicated on mul-
tiple machines with load balancing. Since all requests
are cacheable HTTP GET requests, a caching proxy
could be used to improve performance if it is expected
that there will be repeated requests for the same URI.
8 Future Work
The abstract syntax in GF is based on Martin
Lo?f?s (1984) type theory and supports dependent types.
They can be used go beyond the pure syntax and to
check the sentences for semantic consistency. The cur-
rent parser completely ignores dependent types. This
means that the word prediction will suggest comple-
tions which might not be semantically meaningful.
In order to improve performance for high-traffic ap-
plications that use large grammars, the web service
could cache responses. As long as the grammar is not
modified, identical requests will always produce iden-
tical responses.
9 Conclusions
We have presented a web service for grammar-based
natural language processing, which can be used to build
interactive natural language web applications. The web
service has a simple API, based on HTTP GET requests
with JSON responses. The service allows high levels of
performance and scalability, and has been used to build
several applications.
References
Krasimir Angelov. 2009. Incremental Parsing with Par-
allel Multiple Context-Free Grammars. In European
Chapter of the Association for Computational Lin-
guistics.
Krasimir Angelov, Bjo?rn Bringert, and Aarne
Ranta. 2008. PGF: A Portable Run-Time For-
mat for Type-Theoretical Grammars. Journal
of Logic, Language and Information, submit-
ted. URL http://www.cs.chalmers.se/
?bringert/publ/pgf/pgf.pdf.
Attempto. 2008. Attempto Project Homepage -
http://attempto.ifi.uzh.ch/site/. URL http://
attempto.ifi.uzh.ch/site/.
Bjo?rn Bringert. 2008. Delimited Contin-
uations, Applicative Functors and Natu-
ral Language Semantics. URL http:
//www.cs.chalmers.se/?bringert/
publ/continuation-semantics/
continuation-semantics.pdf.
Koen Claessen and Niklas So?rensson. 2003. New
Techniques that Improve MACE-style Model Find-
ing. In Workshop on Model Computation
(MODEL). URL http://www.cs.chalmers.
se/?koen/pubs/model-paradox.ps.
Douglas Crockford. 2006. The application/json Media
Type for JavaScript Object Notation (JSON). RFC
4627 (Informational). URL http://www.ietf.
org/rfc/rfc4627.txt.
Adam Funk, Valentin Tablan, Kalina Bontcheva,
Hamish Cunningham, Brian Davis, and Siegfried
Handschuh. 2007. CLOnE: Controlled Language for
Ontology Editing. In Proceedings of the Interna-
tional Semantic Web Conference (ISWC 2007). Bu-
san, Korea.
Per Martin-Lo?f. 1984. Intuitionistic Type Theory. Bib-
liopolis, Naples.
Dave Raggett, Arnaud Le Hors, and Ian Jacobs.
1999. HTML 4.01 Specification. Technical report,
W3C. URL http://www.w3.org/TR/1999/
REC-html401-19991224/.
Aarne Ranta. 2004. Grammatical Framework: A
Type-Theoretical Grammar Formalism. Jour-
nal of Functional Programming, 14(2):145?189.
URL http://dx.doi.org/10.1017/
S0956796803004738.
Aarne Ranta. 2007. Modular Grammar Engineering
in GF. Research on Language and Computation,
5(2):133?158. URL http://dx.doi.org/10.
1007/s11168-007-9030-6.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii,
and Tadao Kasami. 1991. On multiple context-
free grammars. Theoretical Computer Science,
88(2):191?229. URL http://dx.doi.org/
10.1016/0304-3975(91)90374-B.
John Sowa. 2004. Common Logic Controlled En-
glish. Draft. URL http://www.jfsowa.com/
clce/specs.htm.
12
Proceedings of the EACL 2009 Demonstrations Session, pages 57?60,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
Grammar Development in GF
Aarne Ranta and Krasimir Angelov and Bjo?rn Bringert?
Department of Computer Science and Engineering
Chalmers University of Technology and University of Gothenburg
{aarne,krasimir,bringert}@chalmers.se
Abstract
GF is a grammar formalism that has a
powerful type system and module system,
permitting a high level of abstraction and
division of labour in grammar writing. GF
is suited both for expert linguists, who
appreciate its capacity of generalizations
and conciseness, and for beginners, who
benefit from its static type checker and,
in particular, the GF Resource Grammar
Library, which currently covers 12 lan-
guages. GF has a notion of multilingual
grammars, enabling code sharing, linguis-
tic generalizations, rapid development of
translation systems, and painless porting
of applications to new languages.
1 Introduction
Grammar implementation for natural languages is
a challenge for both linguistics and engineering.
The linguistic challenge is to master the complex-
ities of languages so that all details are taken into
account and work seamlessly together; if possible,
the description should be concise and elegant, and
capture the linguist?s generalizations on the level
of code. The engineering challenge is to make
the grammar scalable, reusable, and maintainable.
Too many grammars implemented in the history of
computational linguistics have become obsolete,
not only because of their poor maintainability, but
also because of the decay of entire software and
hardware platforms.
The first measure to be taken against the ?bit
rot? of grammars is to write them in well-defined
formats that can be implemented independently
of platform. This requirement is more or less an
axiom in programming language development: a
?Now at Google Inc.
language must have syntax and semantics specifi-
cations that are independent of its first implemen-
tation; otherwise the first implementation risks to
remain the only one.
Secondly, since grammar engineering is to a
large extent software engineering, grammar for-
malisms should learn from programming language
techniques that have been found useful in this re-
spect. Two such techniques are static type sys-
tems and module systems. Since grammar for-
malism implementations are mostly descendants
of Lisp and Prolog, they usually lack a static type
system that finds errors at compile time. In a com-
plex task like grammar writing, compile-time er-
ror detection is preferable to run-time debugging
whenever possible. As for modularity, traditional
grammar formalisms again inherit from Lisp and
Prolog low-level mechanisms like macros and file
includes, which in modern languages like Java and
ML have been replaced by advanced module sys-
tems akin in rigour to type systems.
Thirdly, as another lesson from software en-
gineering, grammar writing should permit an in-
creasing use of libraries, so that programmers can
build on ealier code. Types and modules are essen-
tial for the management of libraries. When a new
language is developed, an effort is needed in creat-
ing libraries for the language, so that programmers
can scale up to real-size tasks.
Fourthly, a grammar formalism should have a
stable and efficient implementation that works
on different platforms (hardware and operating
systems). Since grammars are often parts of larger
language-processing systems (such as translation
tools or dialogue systems), their interoperability
with other components is an important issue. The
implementation should provide compilers to stan-
dard formats, such as databases and speech recog-
nition language models. In addition to interoper-
ability, such compilers also help keeping the gram-
mars alive even if the original grammar formalism
57
ceases to exist.
Fifthly, grammar formalisms should have rich
documentation; in particular, they should have
accessible tutorials that do not demand the read-
ers to be experts in a linguistic theory or in com-
puter programming. Also the libraries should be
documented, preferably by automatically gener-
ated documentation in the style of JavaDoc, which
is guaranteed to stay up to date.
Last but not least, a grammar formalism, as well
its documentation, implementation, and standard
libraries, should be freely available open-source
software that anyone can use, inspect, modify, and
improve. In the domain of general-purpose pro-
gramming, this is yet another growing trend; pro-
prietary languages are being made open-source or
at least free of charge.
2 The GF programming language
The development of GF started in 1998 at Xe-
rox Research Centre Europe in Grenoble, within a
project entitled ?Multilingual Document Author-
ing? (Dymetman & al. 2000). Its purpose was
to make it productive to build controlled-language
translators and multilingual authoring systems,
previously produced by hard-coded grammar
rules rather than declarative grammar formalisms
(Power & Scott 1998). Later, mainly at Chalmers
University in Gothenburg, GF developed into a
functional programming language inspired by ML
and Haskell, with a strict type system and oper-
ational semantics specified in (Ranta 2004). A
module system was soon added (Ranta 2007), in-
spired by the parametrized modules of ML and
the class inheritance hierarchies of Java, although
with multiple inheritance in the style of C++.
Technically, GF falls within the class of so-
called Curry-style categorial grammars, inspired
by the distinction between tectogrammatical and
phenogrammatical structure in (Curry 1963).
Thus a GF grammar has an abstract syntax defin-
ing a system of types and trees (i.e. a free algebra),
and a concrete syntax, which is a homomorphic
mapping from trees to strings and, more generally,
to records of strings and features. To take a simple
example, the NP-VP predication rule, written
S ::= NP VP
in a context-free notation, becomes in GF a pair of
an abstract and a concrete syntax rule,
fun Pred : NP -> VP -> S
lin Pred np vp = np ++ vp
The keyword fun stands for function declara-
tion (declaring the function Pred of type NP ->
VP -> S), whereas lin stands for linearization
(saying that trees of form Pred np vp are con-
verted to strings where the linearization of np is
followed by the linearization of vp). The arrow
-> is the normal function type arrow of program-
ming languages, and ++ is concatenation.
Patterns more complex than string concatena-
tion can be used in linearizations of the same pred-
ication trees as the rule above. Thus agreement
can be expressed by using features passed from the
noun phrase to the verb phrase. The noun phrase
is here defined as not just a string, but as a record
with two fields?a string s and an agreement fea-
ture a. Verb-subject inversion can be expressed by
making VP into a discontinuous constituent, i.e.
a record with separate verb and complement fields
v and c. Combining these two phenomena, we
write
vp.v ! np.a ++ np.s ++ vp.c
(For the details of the notation, we refer to doc-
umentation on the GF web page.) Generalizing
strings into richer data structures makes it smooth
to deal accurately with complexities such as Ger-
man constituent order and Romance clitics, while
maintaining the simple tree structure defined by
the abstract syntax of Pred.
Separating abstract and concrete syntax makes
it possible to write multilingual grammars,
where one abstract syntax is equipped with several
concrete syntaxes. Thus different string configura-
tions can be mapped into the same abstract syntax
trees. For instance, the distinction between SVO
and VSO languages can be ignored on the abstract
level, and so can all other {S,V,O} patterns as well.
Also the differences in feature systems can be ab-
stracted away from. For instance, agreement fea-
tures in English are much simpler than in Arabic;
yet the same abstract syntax can be used.
Since concrete syntax is reversible between lin-
earization and parsing (Ljunglo?f 2004), multilin-
gual grammars can be used for translation, where
the abstract syntax works as interlingua. Experi-
ence from translation projects (e.g. Burke and Jo-
hannisson 2005, Caprotti 2006) has shown that the
interlingua-based translation provided by GF gives
good quality in domain-specific tasks. However,
GF also supports the use of a transfer component if
the compositional method implied by multilingual
grammars does not suffice (Bringert and Ranta
58
2008). The language-theoretical strenght of GF is
between mildly and fully context-sensitive, with
polynomial parsing complexity (Ljunglo?f 2004).
In addition to multilingual grammars, GF is
usable for more traditional, large-scale unilin-
gual grammar development. The ?middle-scale?
resource grammars can be extended to wide-
coverage grammars, by adding a few rules and
a large lexicon. GF provides powerful tools for
building morphological lexica and exporting them
to other formats, including Xerox finite state tools
(Beesley and Karttunen 2003) and SQL databases
(Forsberg and Ranta 2004). Some large lexica
have been ported to the GF format from freely
available sources for Bulgarian, English, Finnish,
Hindi, and Swedish, comprising up to 70,000 lem-
mas and over two million word forms.
3 The GF Resource Grammar Library
The GF Resource Grammar Library is a com-
prehensive multilingual grammar currently imple-
mented for 12 languages: Bulgarian, Catalan,
Danish, English, Finnish, French, German, Italian,
Norwegian, Russian, Spanish, and Swedish. Work
is in progress on Arabic, Hindi/Urdu, Latin, Pol-
ish, Romanian, and Thai. The library is an open-
source project, which constantly attracts new con-
tributions.
The library can be seen as an experiment on how
far the notion of multilingual grammars extends
and how GF scales up to wide-coverage gram-
mars. Its primary purpose, however, is to provide
a programming resource similar to the standard li-
braries of various programming languages. When
all linguistic details are taken into account, gram-
mar writing is an expert programming task, and
the library aims to make this expertise available to
non-expert application programmers.
The coverage of the library is comparable to the
Core Language Engine (Rayner & al. 2000). It has
been developed and tested in applications ranging
from a translation system for software specifica-
tions (Burke and Johannisson 2005) to in-car dia-
logue systems (Perera and Ranta 2007).
The use of a grammar as a library is made pos-
sible by the type and module system of GF (Ranta
2007). What is more, the API (Application Pro-
grammer?s Interface) of the library is to a large ex-
tent language-independent. For instance, an NP-
VP predication rule is available for all languages,
even though the underlying details of predication
vary greatly from one language to another.
A typical domain grammar, such as the one in
Perera and Ranta (2007), has 100?200 syntactic
combinations and a lexicon of a few hundred lem-
mas. Building the syntax with the help of the li-
brary is a matter of a few working days. Once it
is built for one language, porting it to other lan-
guages mainly requires writing the lexicon. By
the use of the inflection libraries, this is a matter of
hours. Thus porting a domain grammar to a new
language requires very effort and also very little
linguistic knowledge: it is expertise of the appli-
cation domain and its terminology that is needed.
4 The GF grammar compiler
The GF grammar compiler is usable in two ways:
in batch mode, and as an interactive shell. The
shell is a useful tool for developers as it provides
testing facilities such as parsing, linerization, ran-
dom generation, and grammar statistics. Both
modes use PGF, Portable Grammar Format,
which is the ?machine language? of GF permit-
ting fast run-time linearization and parsing (An-
gelov & al. 2008). PGF interpreters have been
written in C++, Java, and Haskell, permitting an
easy embedding of grammars in systems written
in these languages. PGF can moreover be trans-
lated to other formats, including language mod-
els for speech recognition (e.g. Nuance and HTK;
see Bringert 2007a), VoiceXML (Bringert 2007b),
and JavaScript (Meza Moreno and Bringert 2008).
The grammar compiler is heavily optimizing, so
that the use of a large library grammar in small
run-time applications produces no penalty.
For the working grammarian, static type check-
ing is maybe the most unique feature of the GF
grammar compiler. Type checking does not only
detect errors in grammars. It also enables aggres-
sive optimizations (type-driven partial evaluation),
and overloading resolution, which makes it pos-
sible to use the same name for different functions
whose types are different.
5 Related work
As a grammar development system, GF is compa-
rable to Regulus (Rayner 2006), LKB (Copestake
2002), and XLE (Kaplan and Maxwell 2007). The
unique features of GF are its type and module sys-
tem, support for multilingual grammars, the large
number of back-end formats, and the availability
of libraries for 12 languages. Regulus has resource
59
grammars for 7 languages, but they are smaller in
scope. In LKB, the LinGO grammar matrix has
been developed for several languages (Bender and
Flickinger 2005), and in XLE, the Pargram gram-
mar set (Butt & al. 2002). LKB and XLE tools
have been targeted to linguists working with large-
scale grammars, rather than for general program-
mers working with applications.
References
[Angelov et al2008] K. Angelov, B. Bringert, and
A. Ranta. 2008. PGF: A Portable Run-Time Format
for Type-Theoretical Grammars. Chalmers Univer-
sity. Submitted for publication.
[Beesley and Karttunen2003] K. Beesley and L. Kart-
tunen. 2003. Finite State Morphology. CSLI Publi-
cations.
[Bender and Flickinger2005] Emily M. Bender and
Dan Flickinger. 2005. Rapid prototyping of scal-
able grammars: Towards modularity in extensions
to a language-independent core. In Proceedings of
the 2nd International Joint Conference on Natural
Language Processing IJCNLP-05 (Posters/Demos),
Jeju Island, Korea.
[Bringert and Ranta2008] B. Bringert and A. Ranta.
2008. A Pattern for Almost Compositional Func-
tions. The Journal of Functional Programming,
18(5?6):567?598.
[Bringert2007a] B. Bringert. 2007a. Speech Recogni-
tion Grammar Compilation in Grammatical Frame-
work. In SPEECHGRAM 2007: ACL Workshop on
Grammar-Based Approaches to Spoken Language
Processing, June 29, 2007, Prague.
[Bringert2007b] Bjo?rn Bringert. 2007b. Rapid Devel-
opment of Dialogue Systems by Grammar Compi-
lation. In Simon Keizer, Harry Bunt, and Tim Paek,
editors, Proceedings of the 8th SIGdial Workshop on
Discourse and Dialogue, Antwerp, Belgium, pages
223?226. Association for Computational Linguis-
tics, September.
[Bringert2008] B. Bringert. 2008. Semantics of the GF
Resource Grammar Library. Report, Chalmers Uni-
versity.
[Burke and Johannisson2005] D. A. Burke and K. Jo-
hannisson. 2005. Translating Formal Software
Specifications to Natural Language / A Grammar-
Based Approach. In P. Blache and E. Stabler and
J. Busquets and R. Moot, editor, Logical Aspects
of Computational Linguistics (LACL 2005), volume
3492 of LNCS/LNAI, pages 51?66. Springer.
[Butt et al2002] M. Butt, H. Dyvik, T. Holloway King,
H. Masuichi, and C. Rohrer. 2002. The Parallel
Grammar Project. In COLING 2002, Workshop on
Grammar Engineering and Evaluation, pages 1?7.
URL
[Caprotti2006] O. Caprotti. 2006. WebALT! Deliver
Mathematics Everywhere. In Proceedings of SITE
2006. Orlando March 20-24.
[Copestake2002] A. Copestake. 2002. Implementing
Typed Feature Structure Grammars. CSLI Publica-
tions.
[Curry1963] H. B. Curry. 1963. Some logical aspects
of grammatical structure. In Roman Jakobson, edi-
tor, Structure of Language and its Mathematical As-
pects: Proceedings of the Twelfth Symposium in Ap-
plied Mathematics, pages 56?68. American Mathe-
matical Society.
[Dymetman et al2000] M. Dymetman, V. Lux, and
A. Ranta. 2000. XML and multilingual docu-
ment authoring: Convergent trends. In COLING,
Saarbru?cken, Germany, pages 243?249.
[Forsberg and Ranta2004] M. Forsberg and A. Ranta.
2004. Functional Morphology. In ICFP 2004,
Showbird, Utah, pages 213?223.
[Ljunglo?f2004] P. Ljunglo?f. 2004. The Expressivity
and Complexity of Grammatical Framework. Ph.D.
thesis, Dept. of Computing Science, Chalmers Uni-
versity of Technology and Gothenburg University.
[Meza Moreno and Bringert2008] M. S. Meza Moreno
and B. Bringert. 2008. Interactive Multilingual
Web Applications with Grammarical Framework. In
B. Nordstro?m and A. Ranta, editors, Advances in
Natural Language Processing (GoTAL 2008), vol-
ume 5221 of LNCS/LNAI, pages 336?347.
[Perera and Ranta2007] N. Perera and A. Ranta. 2007.
Dialogue System Localization with the GF Resource
Grammar Library. In SPEECHGRAM 2007: ACL
Workshop on Grammar-Based Approaches to Spo-
ken Language Processing, June 29, 2007, Prague.
[Power and Scott1998] R. Power and D. Scott. 1998.
Multilingual authoring using feedback texts. In
COLING-ACL.
[Ranta2004] A. Ranta. 2004. Grammatical Frame-
work: A Type-Theoretical Grammar Formal-
ism. The Journal of Functional Programming,
14(2):145?189.
[Ranta2007] A. Ranta. 2007. Modular Grammar Engi-
neering in GF. Research on Language and Compu-
tation, 5:133?158.
[Rayner et al2000] M. Rayner, D. Carter, P. Bouillon,
V. Digalakis, and M. Wire?n. 2000. The Spoken
Language Translator. Cambridge University Press,
Cambridge.
[Rayner et al2006] M. Rayner, B. A. Hockey, and
P. Bouillon. 2006. Putting Linguistics into Speech
Recognition: The Regulus Grammar Compiler.
CSLI Publications.
60
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 368?376,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Fast Statistical Parsing with Parallel Multiple Context-Free Grammars
Krasimir Angelov and Peter Ljungl
?
of
University of Gothenburg and Chalmers University of Technology
G?oteborg, Sweden
krasimir@chalmers.se
peter.ljunglof@cse.gu.se
Abstract
We present an algorithm for incremental
statistical parsing with Parallel Multiple
Context-Free Grammars (PMCFG). This
is an extension of the algorithm by An-
gelov (2009) to which we added statisti-
cal ranking. We show that the new al-
gorithm is several times faster than other
statistical PMCFG parsing algorithms on
real-sized grammars. At the same time the
algorithm is more general since it supports
non-binarized and non-linear grammars.
We also show that if we make the
search heuristics non-admissible, the pars-
ing speed improves even further, at the risk
of returning sub-optimal solutions.
1 Introduction
In this paper we present an algorithm for incre-
mental parsing using Parallel Multiple Context-
Free Grammars (PMCFG) (Seki et al., 1991). This
is a non context-free formalism allowing disconti-
nuity and crossing dependencies, while remaining
with polynomial parsing complexity.
The algorithm is an extension of the algorithm
by Angelov (2009; 2011) which adds statistical
ranking. This is a top-down algorithm, shown by
Ljungl?of (2012) to be similar to other top-down al-
gorithms (Burden and Ljungl?of, 2005; Kanazawa,
2008; Kallmeyer and Maier, 2009). None of the
other top-down algorithms are statistical.
The only statistical PMCFG parsing algorithms
(Kato et al., 2006; Kallmeyer and Maier, 2013;
Maier et al., 2012) all use bottom-up parsing
strategies. Furthermore, they require the gram-
mar to be binarized and linear, which means that
they only support linear context-free rewriting sys-
tems (LCFRS). In contrast, our algorithm natu-
rally supports the full power of PMCFG. By lift-
ing these restrictions, we make it possible to ex-
periment with novel grammar induction methods
(Maier, 2013) and to use statistical disambiguation
for hand-crafted grammars (Angelov, 2011).
By extending the algorithm with a statistical
model, we allow the parser to explore only parts
of the search space, when only the most proba-
ble parse tree is needed. Our cost estimation is
similar to the estimation for the Viterbi probabil-
ity as in Stolcke (1995), except that we have to
take into account that our grammar is not context-
free. The estimation is both admissible and mono-
tonic (Klein and Manning, 2003) which guaran-
tees that we always find a tree whose probability
is the global maximum.
We also describe a variant with a non-
admissible estimation, which further improves the
efficiency of the parser at the risk of returning a
suboptimal parse tree.
We start with a formal definition of a weighted
PMCFG in Section 2, and we continue with a pre-
sentation of our algorithm by means of a weighted
deduction system in Section 3. In Section 4,
we prove that our estimations are admissible and
monotonic. In Section 5 we calculate an esti-
mate for the minimal inside probability for every
category, and in Section 6 we discuss the non-
admissible heuristics. Sections 7 and 8 describe
the implementation and our evaluation, and the fi-
nal Section 9 concludes the paper.
2 PMCFG definition
Our definition of weighted PMCFG (Definition 1)
is the same as the one used by Angelov (2009;
2011), except that we extend it with weights for
the productions. This definition is also similar to
Kato et al (2006), with the small difference that we
allow non-linear functions.
As an illustration for PMCFG parsing, we use
a simple grammar (Figure 1) which can generate
phrases like ?both black and white? and ?either
red or white? but rejects the incorrect combina-
368
Definition 1
A parallel multiple context-free grammar is a tuple
G = (N,T, F, P, S, d, d
i
, r, a) where:
? N is a finite set of categories and a positive in-
teger d(A) called dimension is given for each
A ? N .
? T is a finite set of terminal symbols which is dis-
joint with N .
? F is a finite set of functions where the arity a(f)
and the dimensions r(f) and d
i
(f) (1 ? i ?
a(f)) are given for every f ? F . For every
positive integer d, (T
?
)
d
denote the set of all d-
tuples of strings over T . Each function f ? F
is a total mapping from (T
?
)
d
1
(f)
? (T
?
)
d
2
(f)
?
? ? ? ? (T
?
)
d
a(f)
(f)
to (T
?
)
r(f)
, defined as:
f := (?
1
, ?
2
, . . . , ?
r(f)
)
Here ?
i
is a sequence of terminals and ?k; l?
pairs, where 1 ? k ? a(f) is called argument
index and 1 ? l ? d
k
(f) is called constituent
index.
? P is a finite set of productions of the form:
A
w
?? f [A
1
, A
2
, . . . , A
a(f)
]
where A ? N is called result category,
A
1
, A
2
, . . . , A
a(f)
? N are called argument
categories, f ? F is the function symbol and
w > 0 is a weight. For the production to be
well formed the conditions d
i
(f) = d(A
i
) (1 ?
i ? a(f)) and r(f) = d(A) must hold.
? S is the start category and d(S) = 1.
tions both-or and either-and. We avoid these com-
binations by coupling the right pairs of words in a
single function, i.e. we have the abstract conjunc-
tions both and and either or which are linearized
as discontinuous phrases. The phrase insertion it-
self is done in the definition of conjA . It takes the
conjunction as its first argument, and it uses ?1; 1?
and ?1; 2? to insert the first and the second con-
stituent of the argument at the right places in the
complete phrase.
A tree of function applications that yelds a com-
plete phrase is the parse tree for the phrase. For
instance, the phrase ?both red and either black or
white? is represented by the tree:
(conjA both and red
(conjA either or black white))
A
w
1
?? conjA [Conj ,A ,A ]
A
w
2
?? black []
A
w
3
?? white[]
A
w
4
?? red []
Conj
w
5
?? both and []
Conj
w
6
?? either or[]
conjA := (?1; 1??2; 1??1; 2??3; 1?)
black := (?black?)
white := (?white?)
red := (?red?)
both and := (?both?, ?and?)
either or := (?either?, ?or?)
Figure 1: Example Grammar
The weight of a tree is the sum of the weights for
all functions that are used in it. In this case the
weight for the example isw
1
+w
5
+w
4
+w
1
+w
6
+
w
2
+ w
3
. If there are ambiguities in the sentence,
the algorithm described in Section 3 always finds
a tree which minimizes the weight.
Usually the weights for the productions are log-
arithmic probabilities, i.e. the weight of the pro-
duction A? f [
~
B] is:
w = ? logP (A? f [
~
B] | A)
where P (A ? f [
~
B] | A) is the probability to
choose this production when the result category is
fixed. In this case the probabilities for all produc-
tions with the same result category sum to one:
?
A
w
??f [
~
B] ?P
e
?w
= 1
However, the parsing algorithm does not depend
on the probabilistic interpretation of the weights,
so the same algorithm can be used with any other
kind of weights.
3 Deduction System
We define the algorithm as weighted deduction
system (Nederhof, 2003) which generalizes An-
gelov?s system.
A key feature in his algorithm is that the ex-
pressive PMCFG is reduced to a simple context-
free grammar which is extended dynamically at
parsing time in order to account for context de-
pendent features in the original grammar. This
369
can be exemplified with the grammar in Fig-
ure 1, where there are two productions for cat-
egory Conj . Given the phrase ?both black and
white?, after accepting the token both, only the
production Conj
w
5
?? both and [] can be applied
for parsing the second part of the conjunction.
This is achieved by generating a new category
Conj
2
which has just a single production:
Conj
2
w
5
?? both and [] (1)
The parsing algorithm is basically an extension of
Earley?s (1970) algorithm, except that the parse
items in the chart also keep track of the categories
for the arguments. In the particular case, the cor-
responding chart item will be updated to point to
Conj
2
instead of Conj . This guarantees that only
and will be accepted as a second constituent after
seeing that the first constituent is both.
Now since the set of productions is dynamic, the
parser must keep three kinds of items in the chart,
instead of two as in the Earley algorithm:
Productions The parser maintains a dynamic set
with all productions that are derived during the
parsing. The initial state is populated with the pro-
ductions from the set P in the grammar.
Active Items The active items play the same
role as the active items in the Earley algorithm.
They have the form:
[
k
j
A
w
?? f [
~
B]; l : ? ? ?;w
i
;w
o
]
and represent the fact that a constituent l of a cat-
egory A has been partially recognized from posi-
tion j to k in the sentence. Here A
w
?? f [
~
B] is
the production and the concatenation ?? is the se-
quence of terminals and ?k; r? pairs which defines
the l-th constituent of function f . The dot ? be-
tween ? and ? separates the part of the constituent
that is already recognized from the part which is
still pending. Finally w
i
and w
o
are the inside and
outside weights for the item.
Passive Items The passive items are of the form:
[
k
j
A; l;
?
A]
and state that a constituent with index l from cate-
gory A was recognized from position j to position
k in the sentence. As a consequence the parser has
created a new category
?
A. The set of productions
derived for
?
A compactly records all possible ways
to parse the j ? k fragment.
3.1 Inside and outside weights
The inside weight w
i
and the outside weight w
o
in
the active items deserve more attention since this
is the only difference compared to Angelov (2009;
2011). When the item is complete, it will yield the
forest of all trees that derive the sub-string cov-
ered by the item. For example, when the first con-
stituent for category Conj is completely parsed,
the forest will contain the single production in (1).
The inside weight for the active item is the cur-
rently best known estimation for the lowest weight
of a tree in the forest. The trees yielded by the item
do not cover the whole sentence however. Instead,
they will become part of larger trees that cover the
whole sentence. The outside weight is the esti-
mation for the lowest weight for an extension of a
tree to a full tree. The sum w
i
+ w
o
estimates the
weight of the full tree.
Before turning to the deduction rules we also
need a notation for the lowest possible weight for
a tree of a given category. If A ? N is a category
thenw
A
will denote the lowest weight that a tree of
categoryA can have. For convenience, we also use
w
~
B
as a notation for the sum
?
i
w
B
i
of the weight
of all categories in the vector
~
B. If the category
A is defined in the grammar then we assume that
the weight is precomputed as described in Section
5. When the parser creates the category, it will
compute the weight dynamically.
3.2 Deduction rules
The deduction rules are shown in Figure 2. Here
the assumption is that the active items are pro-
cessed in the order of increasing w
i
+ w
o
weight.
In the actual implementation we put all active
items in a priority queue and we always take first
the item with the lowest weight. We never throw
away items but the processing of items with very
high weight might be delayed indefinitely or they
may never be processed if the best tree is found
before that. Furthermore, we think of the deduc-
tion system as a way do derive a set of items, but in
our case we ignore the weights when we consider
whether two active items are the same. In this way,
every item is derived only once and the weights for
the active items are computed from the weights of
the first antecedents that led to its derivation.
Finally, we use two more notations in the rules:
rhs(g, r) denotes constituent with index r in func-
tion g; and ?
k
denotes the k-th token in the sen-
tence.
370
INITIAL PREDICT
S
w
?? f [
~
B]
[
0
0
S
w
?? f [
~
B]; 1 : ? ?;w + w
~
B
; 0]
S = start category, ? = rhs(f, 1)
PREDICT
B
d
w
1
?? g[
~
C] [
k
j
A
w
2
?? f [
~
B]; l : ? ? ?d; r? ?;w
i
;w
o
]
[
k
k
B
d
w
1
?? g[
~
C]; r : ? ?;w
1
+ w
~
C
;w
i
? w
B
d
+ w
o
]
? = rhs(g, r)
SCAN
[
k
j
A
w
?? f [
~
B]; l : ? ? s ?;w
i
;w
o
]
[
k+1
j
A
w
?? f [
~
B]; l : ? s ? ?;w
i
;w
o
]
s = ?
k+1
COMPLETE
[
k
j
A
w
?? f [
~
B]; l : ? ? ;w
i
;w
o
]
?
A
w
?? f [
~
B] [
k
j
A; l;
?
A]
?
A = (A, l, j, k), w
?
A
= w
i
COMBINE
[
u
j
A
w
?? f [
~
B]; l : ? ? ?d; r? ?;w
i
;w
o
] [
k
u
B
d
; r;
?
B
d
]
[
k
j
A
w
?? f [
~
B{d :=
?
B
d
}]; l : ? ?d; r? ? ?;w
i
+ w
?
B
d
? w
B
d
;w
o
]
Figure 2: Deduction Rules
The first rule on Figure 2 is INITIAL PREDICT and
here we predict the initial active items from the
productions for the start category S. Since this
is the start category, we set the outside weight to
zero. The inside weight is equal to the sum of the
weight w for the production and the lowest pos-
sible weight w
~
B
for the vector of arguments
~
B.
The reason is that despite that we do not know the
weight for the final tree yet, it cannot be lower than
w+w
~
B
since w
~
B
is the lowest possible weight for
the arguments of function f .
The interaction between inside and outside
weights is more interesting in the PREDICT rule.
Here we have an item where the dot is before ?d; r?
and from this we must predict one item for each
production B
d
w
1
?? g[
~
C] of category B
d
. The in-
side weight for the new item is w
1
+ w
~
C
for the
same reasons as for the INITIAL PREDICT rule. The
outside weight however is not zero because the
new item is predicted from another item. The in-
side weight for the active item in the antecedents
is now part of the outside weight of the new item.
We just have to subtract w
B
d
from w
i
because the
new item is going to produce a new tree which will
replace the d-th argument of f . For this reason the
estimation for the outside weight isw
i
?w
B
d
+w
o
,
where we also added the outside weight for the an-
tecedent item.
In the SCAN rule, we just move the dot past a
token, if it matches the current token ?
k+1
. Both
the inside and the outside weights are passed un-
touched from the antecedent to the consequent.
In the COMPLETE rule, we have an item where the
dot has reached the end of the constituent. Here we
generate a new category
?
A which is unique for the
combination (A, l, j, k), and we derive the produc-
tion
?
A
w
?? f [
~
B] for it. We set the weight w
?
A
for
?
A
to be equal to w
i
and in Section 4, we will prove
that this is indeed the lowest weight for a tree of
category
?
A.
In the last rule COMBINE, we combine an active
item with a passive item. The outside weight w
o
for the new active item remains the same. How-
ever, we must update the inside weight since we
have replaced the d-th argument in
~
B with the
newly generated category
?
B
d
. The new weight is
w
i
+ w
?
B
d
? w
B
d
, i.e. we add the weight for the
new category and we subtract the weight for the
previous category B
d
.
Now for the correctness of the weights we must
prove that the estimations are both admissible and
monotonic.
4 Admissibility and Monotonicity
We will first prove that the weights grow mono-
tonically, i.e. if we derive one active item from
another then the sum w
i
+ w
o
for the new item is
always greater or equal to the sum for the previous
371
item. PREDICT and COMBINE are the only two rules
with an active item both in the antecedents and in
the consequents.
Note that in PREDICT we choose one particular
production for category B
d
. We know that the
lowest possible weight of a tree of this category
is w
B
d
. If we restrict the set of trees to those
that not only have the same category B
d
but also
use the same production B
d
w
1
?? g[
~
C] on the top
level, then the best weight for such a tree will be
w
1
+ w
~
C
. According to the definition of w
B
d
, it
must follow that:
w
1
+ w
~
C
? w
B
d
From this we can trivially derive that:
(w
1
+ w
~
C
) + (w
i
? w
B
d
+ w
o
) ? w
i
+ w
o
which is the monotonicity condition for rule
PREDICT. Similarly in rule COMBINE, the condition:
w
?
B
d
? w
B
d
must hold because the forest of trees for
?
B
d
is in-
cluded in the forest forB
d
. From this we conclude
the monotonicity condition:
(w
i
+ w
?
B
d
? w
B
d
) + w
o
? w
i
+ w
o
The last two inequalities are valid only if we can
correctly compute w
?
B
d
for a dynamically gener-
ated category
?
B
d
. This happens in rule COMPLETE,
where we have a complete active item with a cor-
rectly computed inside weight w
i
. Since we pro-
cess the active items in the order of increasing
w
i
+ w
o
weight and since we create
?
A when we
find the first complete item for category A, it is
guaranteed that at this point we have an item with
minimal w
i
+ w
o
value. Furthermore, all items
with the same result category A and the same start
position j must have the same outside weight. It
follows that when we create
?
A we actually do it
from an active item with minimal inside weight
w
i
. This means that it is safe to assign that w
?
A
=
w
i
.
It is also easy to see that the estimation is ad-
missible. The only places where we use estima-
tions for the unseen parts of the sentence is in the
rules INITIAL PREDICT and PREDICT where we use
the weights w
~
B
and w
~
C
which may include com-
ponents corresponding to function argument that
are not seen yet. However by definition it is not
possible to build a tree with weight lower than the
weight for the category. This means that the esti-
mation is always admissible.
5 Initial Estimation
The minimal weight for a dynamically created cat-
egory is computed by the parser, but we must ini-
tialize the weights for the categories that are de-
fined in the grammar. The easiest way is to just
set all weights to zero, and this is safe since the
weights for the predefined categories are used only
as estimations for the yet unseen parts of the sen-
tence. Essentially this gives us a statistical parser
which performs Dijkstra search in the space of all
parse trees. Any other reasonable weight assign-
ment will give us an A
?
algorithm (Hart et al.,
1968).
In general it is possible to devise different
heuristics which will give us different improve-
ments in the parsing time. In our current im-
plementation of the parser we use a weight as-
signment which considers only the already known
probabilities for the productions in the grammar.
The weight for a category A is computed as:
w
A
= min
A
w
??f [
~
B] ? P
(w + w
~
B
)
Here the sum w + w
~
B
is the minimal weight for
a tree constructed with the production A
w
?? f [
~
B]
at the root. By taking the minimum over all pro-
ductions for A, we get the corresponding weight
w
A
. This is a recursive equation since its right-
hand side contains the valuew
~
B
which depends on
the weights for the categories in
~
B. It might hap-
pen that there are mutually dependent categories
which will lead to a recursion in the equation.
The solution is found with iterative assignments
until a fixed point is reached. In the beginning we
assign w
A
= 0 for all categories. After that we re-
compute the new weights with the equation above
until we reach a fixed point.
6 Non-admissible heuristics
The set of active items is kept in a priority queue
and at each step we process the item with the low-
est weight. However, when we experimented with
the algorithm we noticed that most of the time the
item that is selected would eventually contribute
with an alternative reading of the sentence but not
to the best parse. What happens is that despite that
there are already items ending at position k in the
sentence, the current best item might have a span
i ? j where j < k. The parser then picks the
best item only to discover later that the item be-
came much heavier until it reached the span i? k.
372
This suggests that when we compare the weights
of items with different end positions, then we must
take into account the weight that will be accumu-
lated by the item that ends earlier until the two
items align at the same end position.
We use the following heuristic to estimate the
difference. The first time when we extend an
item from position i to position i + 1, we record
the weight increment w
?
(i + 1) for that position.
The increment w
?
is the difference between the
weights for the best active item reaching position
i + 1 and the best active item reaching position i.
From now on when we compare the weights for
two items x
j
and x
k
, with end positions j and k
respectively (j < k), then we always add to the
score w
x
j
of the first item a fraction of the sum of
the increments for the positions between j and k.
In other words, instead of using w
x
j
when com-
paring with w
x
k
, we use
w
x
j
+ h ?
?
j<i?k
w
?
(i)
We call the constant h ? [0, 1] the ?heuristics fac-
tor?. If h = 0, we obtain the basic algorithm that
we described earlier which is admissible and al-
ways returns the best parse. However, the evalua-
tion in Section 8.3 shows that a significant speed-
up can be obtained by using larger values of h.
Unfortunately, if h > 0, we loose some accuracy
and cannot guarantee that the best parse is always
returned first.
Note that the heuristics does not change the
completeness of the algorithm ? it will succeed
for all grammatical sentences and fail for all non-
grammatical. But it does not guarantee that the
first parse tree will be the optimal.
7 Implementation
The parser is implemented in C and is distributed
as a part of the runtime system for the open-source
Grammatical Framework (GF) programming lan-
guage (Ranta, 2011).
1
Although the primary ap-
plication of the runtime system is to run GF appli-
cations, it is not specific to one formalism, and it
can serve as an execution platform for other frame-
works where natural language parsing and gener-
ation is needed.
The GF system is distributed with a library
of manually authored resource grammars (Ranta,
1
http://www.grammaticalframework.org/
2009) for over 25 languages, which are used as a
resource for deriving domain specific grammars.
Adding a big lexicon to the resource grammar re-
sults in a highly ambiguous grammar, which can
give rise to millions of trees even for moderately
complex sentences. Previously, the GF system has
not been able to parse with such ambiguous gram-
mars, but with our statistical algorithm it is now
feasible.
8 Evaluation
We did an initial evaluation on the GF English re-
source grammar augmented with a large-coverage
lexicon of 40 000 lemmas taken from the Oxford
Advanced Learner?s Dictionary (Mitton, 1986). In
total the grammar has 44 000 productions. The
rule weights were trained from a version of the
Penn Treebank (Marcus et al., 1993) which was
converted to trees compatible with the grammar.
The trained grammar was tested on Penn Tree-
bank sentences of length up to 35 tokens, and the
parsing times were at most 7 seconds per sentence.
This initial test was run on a computer with a 2.4
GHz Intel Core i5 processor with 8 GB RAM. This
result was very encouraging, given the complexity
of the grammar, so we decided to do a larger test
and compare with an existing state-of-the-art sta-
tistical PMCFG parser.
Rparse (Kallmeyer and Maier, 2013) is a an-
other state-of-the-art training and parsing system
for PMCFG.
2
It is written in Java and developed at
the Universities of T?ubingen and D?usseldorf, Ger-
many. Rparse can be used for training probabilis-
tic PMCFGs from discontinuous treebanks. It can
also be used for parsing new sentences with the
trained grammars.
In our evaluation we used Rparse to extract PM-
CFG grammars from the discontinuous German
Tiger Treebank (Brants et al., 2002). The rea-
son for using this treebank is that the extracted
grammars are non-context-free, and our parser is
specifically made for such grammars.
8.1 Evaluation data
In our evaluations we got the same general results
regardless of the size of the grammar, so we only
report the results from one of these runs.
In this particular example, we trained the gram-
mar on 40 000 sentences from the Tiger Treebank
with lengths up to 160 tokens. We evaluated on
2
https://github.com/wmaier/rparse
373
Count
Training sentences 40 000
Test sentences 4 607
Non-binarized grammar rules 30 863
Binarized grammar rules 26 111
Table 1: Training and testing data.
4 600 Tiger sentences, with a length of 5?60 to-
kens. The exact numbers are shown in Table 1.
All tests were run on a computer with a 2.3 GHz
Intel Core i7 processor with 16GB RAM.
As a comparison, Maier et al (2012) train on
approximately 15 000 sentences from the Negra
Treebank, and only evaluate on sentences of at
most 40 tokens.
8.2 Comparison with Rparse
We evaluated our parser by comparing it with
Rparse?s built-in parser. Note that we are only in-
terested in the efficiency of our implementation,
not the coverage and accuracy of the trained gram-
mar. In the comparison we used only the ad-
missible heuristics, and we did confirm that the
parsers produce optimal trees with exactly the
same weight for the same input.
Rparse extracts grammars in two steps. First
it converts the treebank into a PMCFG, and then
it binarizes that grammar. The binarization pro-
cess uses markovization to improve the precision
and recall of the final grammar (Kallmeyer and
Maier, 2013). We tested both Rparse?s standard
(Kallmeyer and Maier, 2013) and its new im-
proved parsing alogorithm (Maier et al., 2012).
The new algorithm unfortunately works only with
LCFRS grammars with a fan-out? 2 (Maier et al.,
2012).
In this test we used the optimal binarization
method described in Kallmeyer (2010, chapter
7.2). This was the only binarization algorithm in
Rparse that produced a grammar with fan-out? 2.
As can be seen in Figure 3, our parser outper-
forms Rparse for all sentence lengths. For sen-
tences longer than 15 tokens, the standard Rparse
parser needs on average 100 times longer time
than our parser. This difference increases with
sentence length, suggesting that our algorithm has
a better parsing complexity than Rparse.
The PGF parser also outperforms the improved
Rparse parser, but the relative difference seems to
stabilize on a speedup of 10?15 times.
0,01 
0,1 
1 
10 
100 
5 10 15 20 25 30 35 40 
 Rparse, standard  
 Rparse, fanout ? 2  
 PGF, admissible  
Figure 3: Parsing time (seconds) compared with
Rparse.
0,01 
0,1 
1 
10 
100 
5 10 15 20 25 30 35 40 45 50 55 60 
 PGF, admissible  
 PGF, h=0.50  
 PGF, h=0.75  
 PGF, h=0.95  
Figure 4: Parsing time (seconds) with different
heuristics factors.
8.3 Comparing different heuristics
In another test we compared the effect of the
heuristic factor h described in Section 6. We used
the same training and testing data as before, and
we tried four different heuristic factors: h = 0,
0.50, 0.75 and 0.95. As mentioned in Section 6,
a factor of 0 gives an admissible heuristics, which
means that the parser is guaranteed to return the
tree with the best weight.
The parsing times are shown in Figure 4. As
can be seen, a higher heuristics factor h gives a
considerable speed-up. For 40 token sentences,
h = 0.50 gives an average speedup of 5 times,
while h = 0.75 is 30 times faster, and h = 0.95 is
almost 500 times faster than using the admissible
heuristics h = 0. This is more clearly seen in Fig-
ure 5, where the parsing times are shown relative
to the admissible heuristics.
Note that all charts have a logarithmic y-axis,
which means that a straight line is equivalent to
exponential growth. If we examine the graph lines
374
0,001 
0,01 
0,1 
1 
5 10 15 20 25 30 35 40 
 PGF, admissible  
 PGF, h=0.50  
 PGF, h=0.75  
 PGF, h=0.95  
Figure 5: Relative parsing time for different values
of h, compared to admissible heuristic.
more closely, we can see that they are not straight.
The closest curves are in fact polynomial, with
a degree of 4?6 depending on the parser and the
value of h.
3
8.4 Non-admissibility and parsing quality
What about the loss of parsing quality when we
use a non-admissible heuristics? Firstly, as men-
tioned in Section 6, the parser still recognizes ex-
actly the same language as defined by the gram-
mar. The difference is that it is not guaranteed to
return the tree with the best weight.
In our evaluation we saw that for a factor h =
0.50, 80% of the trees are optimal, and only 3%
of the trees have a weight more than 5% from the
optimal weight. The performance gradually gets
worse for higher h, and with h = 0.95 almost 10%
of the trees have a weight more than 20% from the
optimum.
These numbers only show how the parsing qual-
ity degrades relative to the grammar. But since
the grammar is trained from a treebank it is more
interesting to evaluate how the parsing quality on
the treebank sentences is affected when we use a
non-admissible heuristics. Table 2 shows how the
labelled precision and recall are changed with dif-
ferent values for h. The evaluation was done us-
ing the EVALB measure which is implemented in
Rparse (Maier, 2010). As can be seen, a factor of
h = 0.50 only results in a f-score loss of 3 points,
which is arguably not very much. On the other
extreme, for h = 0.95 the f-score drops 14 points.
3
The exception is the standard Rparse parser, which has a
polynomial degree of 8.
Precision Recall F-score
admissible 71.1 67.7 69.3
h = 0.50 68.0 64.9 66.4
h = 0.75 63.0 60.8 61.9
h = 0.95 55.1 55.6 55.3
Table 2: Parsing quality for different values of h.
9 Discussion
The presented algorithm is an important general-
ization of the classical algorithms of Earley (1970)
and Stolcke (1995) for parsing with probabilistic
context-free grammars to the more general formal-
ism of parallel multiple context-free grammars.
The algorithm has been implemented as part of the
runtime for the Grammatical Framework (Ranta,
2011), but it is not limited to GF alone.
9.1 Performance
To show the universality of the algorithm, we eval-
uated it on large LCFRS grammars trained from
the Tiger Treebank.
Our parser is around 10?15 times faster than the
latest, optimized version of the Rparse state-of-
the-art parser. This improvement seems to be con-
stant, which means that it can be a consequence
of low-level optimizations. More important is that
our algorithm does not impose any restrictions at
all on the underlying PMCFG grammar. Rparse on
the other hand requires that the grammar is both
binarized and has a fan-out of at most 2.
By using a non-admissible heuristics, the speed
improves by orders of magnitude, at the expense
of parsing quality. This makes it possible to
parse long sentences (more than 50 tokens) in just
around a second on a standard desktop computer.
9.2 Future work
We would like to extend the algorithm to be able to
use lexicalized statistical models (Collins, 2003).
Furthermore, it would be interesting to develop
better heuristics for A
?
search, and to investigate
how to incorporate beam search pruning into the
algorithm.
375
References
Krasimir Angelov. 2009. Incremental parsing with
parallel multiple context-free grammars. In Pro-
ceedings of EACL 2009, the 12th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, Athens, Greece.
Krasimir Angelov. 2011. The Mechanics of the Gram-
matical Framework. Ph.D. thesis, Chalmers Univer-
sity of Technology, Gothenburg, Sweden.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of TLT 2002, the 1st Work-
shop on Treebanks and Linguistic Theories, So-
zopol, Bulgaria.
H?akan Burden and Peter Ljungl?of. 2005. Parsing lin-
ear context-free rewriting systems. In Proceedings
of IWPT 2005, the 9th International Workshop on
Parsing Technologies, Vancouver, Canada.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Lin-
guistics, 29(4):589?637.
Jay Earley. 1970. An efficient context-free parsing al-
gorithm. Communications of the ACM, 13(2):94?
102.
Peter Hart, Nils Nilsson, and Bertram Raphael. 1968.
A formal basis for the heuristic determination of
minimum cost paths. IEEE Transactions of Systems
Science and Cybernetics, 4(2):100?107.
Laura Kallmeyer and Wolfgang Maier. 2009. An in-
cremental Earley parser for simple range concatena-
tion grammar. In Proceedings of IWPT 2009, the
11th International Conference on Parsing Technolo-
gies, Paris, France.
Laura Kallmeyer and Wolfgang Maier. 2013. Data-
driven parsing using probabilistic linear context-
free rewriting systems. Computational Linguistics,
39(1):87?119.
Laura Kallmeyer. 2010. Parsing Beyond Context-Free
Grammars. Springer.
Makoto Kanazawa. 2008. A prefix-correct Earley
recognizer for multiple context-free grammars. In
Proceedings of TAG+9, the 9th International Work-
shop on Tree Adjoining Grammar and Related For-
malisms, T?ubingen, Germany.
Yuki Kato, Hiroyuki Seki, and Tadao Kasami. 2006.
Stochastic multiple context-free grammar for RNA
pseudoknot modeling. In Proceedings of TAGRF
2006, the 8th International Workshop on Tree Ad-
joining Grammar and Related Formalisms, Sydney,
Australia.
Dan Klein and Christopher D. Manning. 2003. A
?
parsing: fast exact Viterbi parse selection. In Pro-
ceedings of HLT-NAACL 2003, the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, Edmonton, Canada.
Peter Ljungl?of. 2012. Practical parsing of parallel
multiple context-free grammars. In Proceedings of
TAG+11, the 11th International Workshop on Tree
Adjoining Grammar and Related Formalisms, Paris,
France.
Wolfgang Maier, Miriam Kaeshammer, and Laura
Kallmeyer. 2012. PLCFRS parsing revisited: Re-
stricting the fan-out to two. In Proceedings of
TAG+11, the 11th International Workshop on Tree
Adjoining Grammar and Related Formalisms, Paris,
France.
Wolfgang Maier. 2010. Direct parsing of discontin-
uous constituents in German. In Proceedings of
SPRML 2010, the 1st Workshop on Statistical Pars-
ing of Morphologically-Rich Languages, Los Ange-
les, California.
Wolfgang Maier. 2013. LCFRS binarization and de-
binarization for directional parsing. In Proceedings
of IWPT 2013, the 13th International Conference on
Parsing Technologies, Nara, Japan.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn Treebank. Com-
putational Linguistics, 19:313?330.
Roger Mitton. 1986. A partial dictionary of English in
computer-usable form. Literary & Linguistic Com-
puting, 1(4):214?215.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and Knuth?s algorithm. Computational Linguis-
tics, 29(1):135?143.
Aarne Ranta. 2009. The GF resource grammar library.
Linguistic Issues in Language Technology, 2(2).
Aarne Ranta. 2011. Grammatical Framework: Pro-
gramming with Multilingual Grammars. CSLI Pub-
lications, Stanford.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii,
and Tadao Kasami. 1991. On multiple context-
free grammars. Theoretical Computer Science,
88(2):191?229.
Andreas Stolcke. 1995. An efficient probabilis-
tic context-free parsing algorithm that computes
prefix probabilities. Computational Linguistics,
21(2):165?201.
376
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 41?44,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Speech-Enabled Hybrid Multilingual Translation for Mobile Devices
Krasimir Angelov
University of Gothenburg
krasimir@chalmers.se
Bj
?
orn Bringert
Google Inc
bringert@google.com
Aarne Ranta
University of Gothenburg
aarne@chalmers.se
Abstract
This paper presents an architecture and a
prototype for speech-to-speech translation
on Android devices, based on GF (Gram-
matical Framework). From the user?s
point of view, the advantage is that the
system works off-line and yet has a lean
size; it also gives, as a bonus, gram-
matical information useful for language
learners. From the developer?s point of
view, the advantage is the open architec-
ture that permits the customization of the
system to new languages and for special
purposes. Thus the architecture can be
used for controlled-language-like transla-
tors that deliver very high quality, which
is the traditional strength of GF. However,
this paper focuses on a general-purpose
system that allows arbitrary input. It cov-
ers eight languages.
1 Introduction
Many popular applications (apps) on mobile
devices are about language. They range
from general-purpose translators to tourist phrase
books, dictionaries, and language learning pro-
grams. Many of the apps are commercial and
based on proprietary resources and software. The
mobile APIs (both Android and iOS) make it easy
to build apps, and this provides an excellent way to
exploit and demonstrate computational linguistics
research, perhaps not used as much as it could.
GF (Grammatical Framework, (Ranta, 2011)) is
a grammar formalism designed for building multi-
lingual grammars and interfacing them with other
software systems. Both multilinguality and inter-
facing are based on the use of an abstract syntax,
a tree structure that captures the essence of syntax
and semantics in a language-neutral way. Transla-
tion in GF is organized as parsing the source lan-
guage input into an abstract syntax tree and then
linearizing the tree into the target language. Here
is an example of a simple question, as modelled by
an abstract syntax tree and linearized to four lan-
guages, which use different syntactic structures to
express the same content:
Query (What Age (Name ?Madonna?))
English: How old is Madonna?
Finnish: Kuinka vanha Madonna on?
French: Quel ?age a Madonna?
Italian: Quanti anni ha Madonna?
In recent years much focus in GF has been
put on cloud applications (Ranta et al., 2010) and
on mobile apps, for both Android (D?etrez and
Enache, 2010) and iOS (Djupfeldt, 2013). They
all implement text-based phrasebooks, whereas
Alum?ae and Kaljurand (2012) have built a speech-
enabled question-answering system for Estonian.
An earlier speech translation system in GF is pre-
sented in Bringert (2008).
All embedded GF systems are based on a
standardized run-time format of GF, called PGF
(Portable Grammar Format; Angelov et al. 2009,
Angelov 2011). PGF is a simple ?machine lan-
guage?, to which the much richer GF source lan-
guage is compiled by the GF grammar compiler.
PGF being simple, it is relatively straightforward
to write interpreters that perform parsing and lin-
earizations with PGF grammars. The first mobile
implementations were explicitly designed to work
on small devices with limited resources. Thus they
work fine for small grammars (with up to hun-
dreds of rules and lexical entries per language), but
they don?t scale up well into open-domain gram-
mars requiring a lexicon size of tens of thousands
of lemmas. Moreover, they don?t support out-of-
grammar input, and have no means of choosing
between alternative parse results, which in a large
grammar can easily amount to thousands of trees.
A new, more efficient and robust run-time sys-
tem for PGF was later written in C (Angelov,
2011). Its performance is competitive with the
41
state of the art in grammar-based parsing (Angelov
and Ljungl?of, 2014). This system uses statisti-
cal disambiguation and supports large-scale gram-
mars, such as an English grammar covering most
of the Penn Treebank. In addition, it is lean
enough to be embedded as an Android application
even with full-scale grammars, running even on
devices as old as the Nexus One from early 2010.
Small grammars limited to natural language
fragments, such as a phrasebook, are usable when
equipped with predictive parsing that can suggest
the next words in context. However, there is no
natural device for word suggestions with speech
input. The system must then require the user to
learn the input language; alternatively, it can be
reduced to simple keyword spotting. This can
be useful in information retrieval applications, but
hardly in translation. Any useful speech-enabled
translator must have wide coverage, and it cannot
be restricted to just translating keywords.
In this paper, we show a mobile system that
has a wide coverage and translates both text and
speech. The system is modular and could be eas-
ily adapted to traditional GF applications as well:
since the PGF format is the same, one can combine
any grammar with any run-time PGF interpreter.
The rest of the paper is organized as follows:
Section 2 describes the system?s functionalities
from the user?s point of view. Section 3 explains
the technology from the developer?s point of view.
Section 4 presents some preliminary results on the
usability of the system, and discusses some ways
of improving it. Section 5 concludes.
A proper quantitative evaluation of the transla-
tion quality has to wait till another occasion, and
will be more properly done in a context that ad-
dresses hybrid GF-based translation as a research
topic. Early attempts in this area have not yet con-
verged into a stable methodology, but we believe
that setting translation in the context of a practical
use case, as here, can help identify what issues to
focus on.
2 Functionalities
The app starts with the last-used language pair pre-
selected for input and output. It waits for speech
input, which is invoked by touching the micro-
phone icon. Once the input is finished, it appears
in text on the left side of the screen. Its translation
appears below it, on the right, and is also rendered
as speech (Figure 1 (a)).
(a) (b)
Figure 1: Translation between various languages
with (a) speech (b) text input.
The source and target languages are selected by
the two drop-down lists on the top of the screen.
The icon with two arrows to the right of the lan-
guage selectors allows the two languages to be
swapped quickly.
The speech recognition and text-to-speech
(TTS) is done using public Android APIs. On
most devices, these make use of Google?s speech
recognizer and synthesizer, which are available in
both online and offline versions. The offline en-
gines tend to have a reduced choice of languages
and reduced quality compared to the online en-
gines, but don?t require an internet connection.
Alternatively, the user can select the keyboard
mode. The microphone icon is then changed to a
keyboard icon, which opens a software keyboard
and shows a text field for entering a new phrase.
Once the phrase is translated, it is shown on the
screen but also sent to TTS (Figure 1 (b)).
If the input consists of a single lexical unit,
the user can open a dictionary description for the
word. The resulting screen shows the base form
of the word, followed by a list of possible transla-
tions. The target language is shown on the top of
the screen and it can be changed to see the transla-
tions in the other languages (Figure 2 (a)). Touch-
ing one of the translations opens a full-form in-
flection table together with other grammatical in-
formation about the word, such as gender and verb
valency (Figure 2 (b)).
Finally, the translator also works as an input
mode for other apps such as SMS. It provides a
soft keyboard, which is similar to the standard An-
droid keyboard, except that it has two more keys
allowing the entered phrase to be translated in-
place from inside any other application.
42
(a) (b)
Figure 2: (a) Results of dictionary lookup. (b) Va-
lency and the inflection table for a Bulgarian verb.
3 Technology
3.1 Run-time processing
The core of the system is the C runtime for PGF
(Angelov, 2011). The runtime is compiled to na-
tive code with the Android NDK and is called via
foreign function interface from the user interface,
which is implemented in Java.
The main challenge in using the runtime on mo-
bile devices is that even the latest models are still
several times slower that a modern laptop. For in-
stance, just loading the grammars for English and
Bulgarian, on a mobile device initially took about
28 seconds, while the same task is a negligible
operation on a normal computer. We spent con-
siderable time on optimizing the grammar loader
and the translator in general. Now the same gram-
mar, when loaded sequentially, takes only about
5-6 seconds. Furthermore, we made the grammar
loader parallel, i.e. it loads each language in par-
allel. The user interface runs in yet another thread,
so while the grammar is loading, the user can al-
ready start typing or uttering a sentence. In addi-
tion, we made it possible to load only those lan-
guages that are actually used, i.e. only two at a
time instead of all eight at once.
Parsing is a challenge in itself. As the grammars
grow bigger, there tends to be more and more need
for disambiguation. This is performed by a statis-
tical model, where each abstract syntax tree node
has weight. We used the method of Angelov and
Ljungl?of (2014) to find the best tree.
Moreover, since any sound grammar is likely to
fail on some input, there is need for robustness.
This has been solved by chunking the input into
maximal parsable bits. As a result, the translations
are not always grammatically correct, because de-
Bulgarian 26664 French 19570
Chinese 17050 German 9992
English 65009 Hindi 33841
Finnish 57036 Swedish 24550
Table 1: Lexical coverage (lemmas)
pendencies between chunks, such as agreement,
get lost. This kind of errors are familiar to anyone
who has used a statistical system such as Google
translate. In the GF system it is easy to avoid them,
provided the parse is complete.
3.2 The language component
The language-specific component of the app is the
PGF grammar, which contains both the grammars
proper and the probabilistic model of the abstract
syntax. The app can be adaptad to a different PGF
grammar by changing a few lines of the source
code. Hence any grammar written in GF is readily
usable as the language component of an app. But
here we focus on the large-scale grammar meant
for robust translation.
The core of the grammar is the GF Resource
Grammar Library (Ranta, 2009), which currently
covers 29 languages. Of these, 8 have been ex-
tended with more syntax rules (about 20% in ad-
dition to the standard library) and a larger lexi-
con. Table 1 shows the list of languages together
with the size of the lexicon for each of them. The
abstract syntax is based on English lemmas and
some split word senses of them. The other lan-
guages, having fewer words than English, are thus
incomplete. Unknown words are rendered by ei-
ther showing them in English (if included in the
English lexicon) or just returning them verbatim
(typical for named entities).
The lexicon has been bootstrapped from various
freely available sources, such as linked WordNets
and the Wiktionary. Parts of the lexicon have been
checked or completely written manually.
4 First results
The most striking advantage of the translation app
is its lean size: currently just 18Mb for the whole
set of 8 languages, allowing translation for 56
language pairs. This can be compared with the
size of about 200Mb for just one language pair
in Google?s translation app used off-line. The
Apertium off-line app is between these two, using
around 2MB per language pair.
43
The speed is still an issue. While the app
now loads smoothly on modern hardware (such
as Nexus 5 phones), translation is usually much
slower than in Google and Apertium apps. The
speed depends heavily on the complexity of the
source language, with Finnish and French the
worst ones, and on sentence length. Only with
short sentences (under ten words) from Bulgarian,
Chinese, English, and Swedish, does the translator
deliver satisfactory speed. On the other hand, long
sentences entered via speech are likely to con-
tain speech recognition errors, which makes their
translation pointless anyway.
Translating single words is based on a simpler
algorithm (dictionary lookup) and is therefore im-
mediate; together with the grammatical informa-
tion displayed, this makes single word translation
into the most mature feature of the app so far.
The translation quality and coverage are rea-
sonable in phrasebook-like short and simple sen-
tences. The app has exploited some idiomatic con-
structions of the earlier GF phrasebook (D?etrez
and Enache, 2010), so that it can correctly switch
the syntactic structure and translate e.g. how old
are you to French as quel ?age as-tu. In many other
cases, the results are unidiomatic word-to-word
translations but still grammatical. For instance,
hur mycket ?ar klockan, which should give what is
the time, returns how mighty is the bell. Such short
idioms are typically correct in Google?s translation
app, and collecting them into the GF resources will
be an important future task.
On the plus side, grammar-based translation is
more predictable than statistical. Thus (currently)
when using Google translate from Swedish to En-
glish, both min far ?ar svensk and its negation min
far ?ar inte svensk come out as the positive sen-
tence my father is Swedish. With grammar-based
translation, such semantic errors can be avoided.
5 Conclusion
We have presented a platform for mobile transla-
tion apps based on GF grammars, statistical dis-
ambiguation, and chunking-based robustness, en-
hanced by Android?s off-the-shelf speech input
and output. The platform is demonstrated by a
system that translates fairly open text between 8
languages, with reasonable performance for short
sentences but slow parsing for longer ones, with
moreover lower quality due to more parse errors.
The processing modules, user interface, and the
language resources are available as open source
software and thereby usable for the community
for building other systems with similar function-
alities. As the app is a front end to a grammati-
cal language resource, it can also be used for other
language-aware tasks such as learning apps; this is
illustrated in the demo app by the display of inflec-
tion tables. The app and its sources are available
via http://www.grammaticalframework.org.
References
Tanel Alum?ae and Kaarel Kaljurand. 2012. Open and
extendable speech recognition application architec-
ture for mobile environments. The Third Interna-
tional Workshop on Spoken Languages Technologies
for Under-resourced Languages (SLTU 2012), Cape
Town, South Africa.
Krasimir Angelov and Peter Ljungl?of. 2014. Fast
statistical parsing with parallel multiple context-free
grammars. In European Chapter of the Association
for Computational Linguistics, Gothenburg.
Krasimir Angelov, Bj?orn Bringert, and Aarne Ranta.
2009. PGF: A Portable Run-Time Format for Type-
Theoretical Grammars. Journal of Logic, Language
and Information, 19(2), pp. 201?228.
Krasimir Angelov. 2011. The Mechanics of the Gram-
matical Framework. Ph.D. thesis, Chalmers Univer-
sity of Technology.
Bj?orn Bringert. 2008. Speech translation with Gram-
matical Framework. In Coling 2008: Proceedings of
the workshop on Speech Processing for Safety Crit-
ical Translation and Pervasive Applications, pages
5?8, Manchester, UK, August. Coling 2008 Orga-
nizing Committee.
Gr?egoire D?etrez and Ramona Enache. 2010. A frame-
work for multilingual applications on the android
platform. In Swedish Language Technology Confer-
ence.
Emil Djupfeldt. 2013. Grammatical framework on the
iphone using a C++ PGF parser. Technical report,
Chalmers Univerity of Technology.
Aarne Ranta, Krasimir Angelov, and Thomas Hallgren.
2010. Tools for multilingual grammar-based trans-
lation on the web. In Proceedings of the ACL 2010
System Demonstrations, ACLDemos ?10, pages 66?
71, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Aarne Ranta. 2009. The GF resource grammar library.
Linguistic Issues in Language Technology.
Aarne Ranta. 2011. Grammatical Framework: Pro-
gramming with Multilingual Grammars. CSLI Pub-
lications, Stanford. ISBN-10: 1-57586-626-9 (Pa-
per), 1-57586-627-7 (Cloth).
44
Proceedings of the ACL 2010 System Demonstrations, pages 66?71,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
Tools for Multilingual Grammar-Based Translation on the Web
Aarne Ranta and Krasimir Angelov and Thomas Hallgren
Department of Computer Science and Engineering
Chalmers University of Technology and University of Gothenburg
aarne@chalmers.se, krasimir@chalmers.se, hallgren@chalmers.se
Abstract
This is a system demo for a set of tools for
translating texts between multiple languages
in real time with high quality. The translation
works on restricted languages, and is based on
semantic interlinguas. The underlying model
is GF (Grammatical Framework), which is an
open-source toolkit for multilingual grammar
implementations. The demo will cover up to
20 parallel languages.
Two related sets of tools are presented: gram-
marian?s tools helping to build translators for
new domains and languages, and translator?s
tools helping to translate documents. The
grammarian?s tools are designed to make it
easy to port the technique to new applications.
The translator?s tools are essential in the re-
stricted language context, enabling the author
to remain in the fragments recognized by the
system.
The tools that are demonstrated will be ap-
plied and developed further in the European
project MOLTO (Multilingual On-Line Trans-
lation) which has started in March 2010 and
runs for three years.
1 Translation Needs for the Web
The best-known translation tools on the web are
Google translate1 and Systran2. They are targeted to
consumers of web documents: users who want to find
out what a given document is about. For this purpose,
browsing quality is sufficient, since the user has in-
telligence and good will, and understands that she uses
the translation at her own risk.
Since Google and Systran translations can be gram-
matically and semantically flawed, they don?t reach
publication quality, and cannot hence be used by
the producers of web documents. For instance, the
provider of an e-commerce site cannot take the risk that
the product descriptions or selling conditions have er-
rors that change the original intentions.
There are very few automatic translation systems ac-
tually in use for producers of information. As already
1www.google.com/translate
2www.systransoft.com
noted by Bar-Hillel (1964), machine translation is one
of those AI-complete tasks that involves a trade-off be-
tween coverage and precision, and the current main-
stream systems opt for coverage. This is also what web
users expect: they want to be able to throw just any-
thing at the translation system and get something useful
back. Precision-oriented approaches, the prime exam-
ple of which is METEO (Chandioux 1977), have not
been popular in recent years.
However, from the producer?s point of view, large
coverage is not essential: unlike the consumer?s tools,
their input is predictable, and can be restricted to very
specific domains, and to content that the producers
themselves are creating in the first place. But even in
such tasks, two severe problems remain:
? The development cost problem: a large amount
of work is needed for building translators for new
domains and new languages.
? The authoring problem: since the method does
not work for all input, the author of the source text
of translation may need special training to write in
a way that can be translated at all.
These two problems have probably been the main
obstacles to making high-quality restricted language
translation more wide-spread in tasks where it would
otherwise be applicable. We address these problems by
providing tools that help developers of translation sys-
tems on the one hand, and authors and translators?i.e.
the users of the systems?on the other.
In the MOLTO project (Multilingual On-Line Trans-
lation)3, we have the goal to improve both the devel-
opment and use of restricted language translation by an
order of magnitude, as compared with the state of the
art. As for development costs, this means that a sys-
tem for many languages and with adequate quality can
be built in a matter of days rather than months. As
for authoring, this means that content production does
not require the use of manuals or involve trial and er-
ror, both of which can easily make the work ten times
slower than normal writing.
In the proposed system demo, we will show how
some of the building blocks for MOLTO can already
now be used in web-based translators, although on a
3 www.molto-project.eu
66
Figure 1: A multilingual GF grammar with reversible
mappings from a common abstract syntax to the 15 lan-
guages currently available in the GF Resource Gram-
mar Library.
smaller scale as regards languages and application do-
mains. A running demo system is available at http:
//grammaticalframework.org:41296.
2 Multilingual Grammars
The translation tools are based on GF, Grammati-
cal Framework4 (Ranta 2004). GF is a grammar
formalism?that is, a mathematical model of natural
language, equipped with a formal notation for writ-
ing grammars and a computer program implementing
parsing and generation which are declaratively defined
by grammars. Thus GF is comparable with formalism
such as HPSG (Pollard and Sag 1994), LFG (Bresnan
1982) or TAG (Joshi 1985). The novel feature of GF is
the notion of multilingual grammars, which describe
several languages simultaneously by using a common
representation called abstract syntax; see Figure 1.
In a multilingual GF grammar, meaning-preserving
translation is provided as a composition of parsing and
generation via the abstract syntax, which works as an
interlingua. This model of translation is different from
approaches based on other comparable grammar for-
malisms, such as synchronous TAGs (Shieber and Sch-
abes 1990), Pargram (Butt & al. 2002, based on LFG),
LINGO Matrix (Bender and Flickinger 2005, based
on HPSG), and CLE (Core Language Engine, Alshawi
1992). These approaches use transfer rules between
individual languages, separate for each pair of lan-
guages.
Being interlingua-based, GF translation scales up
linearly to new languages without the quadratic blow-
up of transfer-based systems. In transfer-based sys-
4www.grammaticalframework.org
tems, as many as n(n? 1) components (transfer func-
tions) are needed to cover all language pairs in both di-
rections. In an interlingua-based system, 2n + 1 com-
ponents are enough: the interlingua itself, plus trans-
lations in both directions between each language and
the interlingua. However, in GF, n + 1 components
are sufficient, because the mappings from the abstract
syntax to each language (the concrete syntaxes) are
reversible, i.e. usable for both generation and parsing.
Multilingual GF grammars can be seen as an imple-
mentation of Curry?s distinction between tectogram-
matical and phenogrammatical structure (Curry
1961). In GF, the tectogrammatical structure is called
abstract syntax, following standard computer science
terminology. It is defined by using a logical frame-
work (Harper & al. 1993), whose mathematical basis
is in the type theory of Martin-Lo?f (1984). Two things
can be noted about this architecture, both showing im-
provements over state-of-the-art grammar-based trans-
lation methods.
First, the translation interlingua (the abstract syntax)
is a powerful logical formalism, able to express se-
mantical structures such as context-dependencies and
anaphora (Ranta 1994). In particular, dependent types
make it more expressive than the type theory used in
Montague grammar (Montague 1974) and employed in
the Rosetta translation project (Rosetta 1998).
Second, GF uses a framework for interlinguas,
rather than one universal interlingua. This makes the
interlingual approach more light-weight and feasible
than in systems assuming one universal interlingua,
such as Rosetta and UNL, Universal Networking Lan-
guage5. It also gives more precision to special-purpose
translation: the interlingua of a GF translation system
(i.e. the abstract syntax of a multilingual grammar) can
encode precisely those structures and distinctions that
are relevant for the task at hand. Thus an interlingua
for mathematical proofs (Hallgren and Ranta 2000) is
different from one for commands for operating an MP3
player (Perera and Ranta 2007). The expressive power
of the logical framework is sufficient for both kinds of
tasks.
One important source of inspiration for GF was the
WYSIWYM system (Power and Scott 1998), which
used domain-specific interlinguas and produced excel-
lent quality in multilingual generation. But the gener-
ation components were hard-coded in the program, in-
stead of being defined declaratively as in GF, and they
were not usable in the direction of parsing.
3 Grammars and Ontologies
Parallel to the first development efforts of GF in the
late 1990?s, another framework idea was emerging in
web technology: XML, Extensible Mark-up Language,
which unlike HTML is not a single mark-up language
but a framework for creating custom mark-up lan-
5www.undl.org
67
guages. The analogy between GF and XML was seen
from the beginning, and GF was designed as a for-
malism for multilingual rendering of semantic content
(Dymetman and al. 2000). XML originated as a format
for structuring documents and structured data serializa-
tion, but a couple of its descendants, RDF(S) and OWL,
developed its potential to formally express the seman-
tics of data and content, serving as the fundaments of
the emerging Semantic Web.
Practically any meaning representation format can
be converted into GF?s abstract syntax, which can then
be mapped to different target languages. In particular
the OWL language can be seen as a syntactic sugar for
a subset of Martin-Lo?f?s type theory so it is trivial to
embed it in GF?s abstract syntax.
The translation problem defined in terms of an on-
tology is radically different from the problem of trans-
lating plain text from one language to another. Many
of the projects in which GF has been used involve pre-
cisely this: a meaning representation formalized as GF
abstract syntax. Some projects build on previously ex-
isting meaning representation and address mathemati-
cal proofs (Hallgren and Ranta 2000), software speci-
fications (Beckert & al. 2007), and mathematical exer-
cises (the European project WebALT6). Other projects
start with semantic modelling work to build meaning
representations from scratch, most notably ones for di-
alogue systems (Perera and Ranta 2007) in the Euro-
pean project TALK7. Yet another project, and one clos-
est to web translation, is the multilingual Wiki sys-
tem presented in (Meza Moreno and Bringert 2008).
In this system, users can add and modify reviews of
restaurants in three languages (English, Spanish, and
Swedish). Any change made in any of the languages
gets automatically translated to the other languages.
To take an example, the OWL-to-GF mapping trans-
lates OWL?s classes to GF?s categories and OWL?s
properties to GF?s functions that return propositions.
As a running example in this and the next sec-
tion, we will use the class of integers and the
two-place property of being divisible (?x is divis-
ible by y?). The correspondences are as follows:
Class(pp:integer ...)
m
cat integer
ObjectProperty(pp:div
domain(pp:integer)
range(pp:integer))
m
fun div :
integer -> integer -> prop
4 Grammar Engineer?s Tools
In the GF setting, building a multilingual translation
system is equivalent to building a multilingual GF
6EDC-22253, webalt.math.helsinki.fi
7IST-507802, 2004?2006, www.talk-project.org
grammar, which in turn consists of two kinds of com-
ponents:
? a language-independent abstract syntax, giving
the semantic model via which translation is per-
formed;
? for each language, a concrete syntax mapping ab-
stract syntax trees to strings in that language.
While abstract syntax construction is an extra task com-
pared to many other kinds of translation methods, it is
technically relatively simple, and its cost is moreover
amortized as the system is extended to new languages.
Concrete syntax construction can be much more de-
manding in terms of programming skills and linguis-
tic knowledge, due to the complexity of natural lan-
guages. This task is where GF claims perhaps the high-
est advantage over other approaches to special-purpose
grammars. The two main assets are:
? Programming language support: GF is a modern
functional programming language, with a pow-
erful type system and module system supporting
modular and collaborative programming and reuse
of code.
? RGL, the GF Resource Grammar Library, im-
plementing the basic linguistic details of lan-
guages: inflectional morphology and syntactic
combination functions.
The RGL covers fifteen languages at the moment,
shown in Figure 1; see also Khegai 2006, El Dada and
Ranta 2007, Angelov 2008, Ranta 2009a,b, and Enache
et al 2010. To give an example of what the library
provides, let us first consider the inflectional morphol-
ogy. It is presented as a set of lexicon-building func-
tions such as, in English,
mkV : Str -> V
i.e. function mkV, which takes a string (Str) as its ar-
gument and returns a verb (V) as its value. The verb
is, internally, an inflection table containing all forms
of a verb. The function mkV derives all these forms
from its argument string, which is the infinitive form. It
predicts all regular variations: (mkV "walk") yields
the purely agglutinative forms walk-walks-walked-
walked-walking whereas (mkV "cry") gives cry-
cries-cried-cried-crying, and so on. For irregular En-
glish verbs, RGL gives a three-argument function tak-
ing forms such as sing,sang,sung, but it also has a fairly
complete lexicon of irregular verbs, so that the nor-
mal application programmer who builds a lexicon only
needs the regular mkV function.
Extending a lexicon with domain-specific vocabu-
lary is typically the main part of the work of a con-
crete syntax author. Considerable work has been put
into RGL?s inflection functions to make them as ?in-
telligent? as possible and thereby ease the work of the
68
users of the library, who don?t know the linguistic de-
tails of morphology. For instance, even Finnish, whose
verbs have hundreds of forms and are conjugated in
accordance with around 50 conjugations, has a one-
argument function mkV that yields the correct inflection
table for 90% of Finnish verbs.
As an example of a syntactic combination function
of RGL, consider a function for predication with two-
place adjectives. This function takes three arguments: a
two-place adjective, a subject noun phrase, and a com-
plement noun phrase. It returns a sentence as value:
pred : A2 -> NP -> NP -> S
This function is available in all languages of RGL, even
though the details of sentence formation are vastly dif-
ferent in them. Thus, to give the concrete syntax of the
abstract (semantical) predicate div x y (?x is divisi-
ble by y?), the English grammarian can write
div x y = pred
(mkA2 "divisible" "by") x y
The German grammarian can write
div x y = pred
(mkA2 "teilbar" durch_Prep) x y
which, even though superficially using different forms
from English, generates a much more complex struc-
ture: the complement preposition durch Prep takes
care of rendering the argument y in the accusative case,
and the sentence produced has three forms, as needed
in grammatically different positions (x ist teilbar durch
y in main clauses, ist x teilbar durch y after adverbs,
and x durch y teilbar ist in subordinate clauses).
The syntactic combinations of the RGL have their
own abstract syntax, but this abstract syntax is not the
interlingua of translation: it is only used as a library for
implementing the semantic interlingua, which is based
on an ontology and abstracts away from syntactic struc-
ture. Thus the translation equivalents in a multilingual
grammar need not use the same syntactic combinations
in different languages. Assume, for the sake of argu-
ment, that x is divisible by y is expressed in Swedish
by the transitive verb construction y delar x (literally,
?y divides x?). This can be expressed easily by using
the transitive verb predication function of the RGL and
switching the subject and object,
div x y = pred (mkV2 "dela") y x
Thus, even though GF translation is interlingua-based,
there is a component of transfer between English and
Swedish. But this transfer is performed at compile
time. In general, the use of the large-coverage RGL as a
library for restricted grammars is called grammar spe-
cialization. The way GF performs grammar specializa-
tion is based on techniques for optimizing functional
programming languages, in particular partial evalua-
tion (Ranta 2004, 2007). GF also gives a possibility to
run-time transfer via semantic actions on abstract syn-
tax trees, but this option has rarely been needed in pre-
vious applications, which helps to keep translation sys-
tems simple and efficient.
Figure 2: French word prediction in GF parser, sug-
gesting feminine adjectives that agree with the subject
la femme.
As shown in Figure 1, the RGL is currently avail-
able for 15 languages, of which 12 are official lan-
guages of the European Union. A similar number of
new languages are under construction in this collabo-
rative open-source project. Implementing a new lan-
guage is an effort of 3?6 person months.
5 Translator?s Tools
For the translator?s tools, there are three different use
cases:
? restricted source
? production of source in the first place
? modifying source produced earlier
? unrestricted source
Working with restricted source language recognizable
by a GF grammar is straightforward for the translating
tool to cope with, except when there is ambiguity in the
text. The real challenge is to help the author to keep in-
side the restricted language. This help is provided by
predictive parsing, a technique recently developed for
GF (Angelov 2009)8. Incremental parsing yields word
predictions, which guide the author in a way similar
to the T9 method9 in mobile phones. The difference
from T9 is that GF?s word prediction is sensitive to the
grammatical context. Thus it does not suggest all exist-
ing words, but only those words that are grammatically
correct in the context. Figure 2 shows an example of
the parser at work. The author has started a sentence as
la femme qui remplit le formulaire est co (?the woman
who fills the form is co?), and a menu shows a list of
words beginning with co that are given in the French
grammar and possible in the context at hand; all these
words are adjectives in the feminine form. Notice that
the very example shown in Figure 2 is one that is diffi-
cult for n-gram-based statistical translators: the adjec-
tive is so far from the subject with which it agrees that
it cannot easily be related to it.
Predictive parsing is a good way to help users pro-
duce translatable content in the first place. When mod-
ifying the content later, e.g. in a wiki, it may not be
optimal, in particular if the text is long. The text can
8 Parsing in GF is polynomial with an arbitrary exponent
in the worst case, but, as shown in Angelov 2009, linear in
practice with realistic grammars.
9www.t9.com
69
Pred known A (Rel woman N (Compl
fill V2 form N))
the woman who fills the form is known
la femme qui remplit le formulaire est connue
??
Pred known A (Rel man N (Compl fill V2
form N))
the man who fills the form is known
l? homme qui remplit le formulaire est connu
Figure 3: Change in one word (boldface) propagated to
other words depending on it (italics).
contain parts that depend on each other but are located
far apart. For instance, if the word femme (?woman?) in
the previous example is changed to homme, the preced-
ing article la has to be changed to l?, and the adjective
has to be changed to the masculine form: thus con-
nue (?known?) would become connu, and so on. Such
changes are notoriously difficult even for human au-
thors and translators, and can easily leave a document
in an inconsistent state. This is where another utility
of the abstract syntax comes in: in the abstract syntax
tree, all that is changed is the noun, and the regener-
ated concrete syntax string automatically obeys all the
agreement rules. The process is shown in Figure 3. The
one-word change generating the new set of documents
can be performed by editing any of the three represen-
tations: the tree, the English version, or the French ver-
sion. This functionality is implemented in the GF syn-
tax editor (Khegai & al. 2003).
Restricted languages in the sense of GF are close to
controlled languages, such as Attempto (Fuchs & al.
2008); the examples shown in this section are actually
taken from a GF implementation that generalizes At-
tempto Controlled English to five languages (Angelov
and Ranta 2009). However, unlike typical controlled
languages, GF does not require the absence of ambigu-
ity. In fact, when a controlled language is generalized
to new languages, lexical ambiguities in particular are
hard to avoid.
The predictive parser of GF does not try to resolve
ambiguities, but simply returns all alternatives in the
parse chart. If the target language has exactly the same
ambiguity, it remains hidden in the translation. But if
the ambiguity does make a difference in translation, it
has to be resolved, and the system has to provide a pos-
sibility of manual disambiguation by the user to guar-
antee high quality.
The translation tool snapshot in Figure 2 is from
an actual web-based prototype. It shows a slot in an
HTML page, built by using JavaScript via the Google
Web Toolkit (Bringert & al. 2009). The translation
is performed using GF in a server, which is called via
HTTP. Also client-side translators, with similar user in-
terfaces, can be built by converting the whole GF gram-
mar to JavaScript (Meza Moreno and Bringert 2008).
6 The Demo
In the demo, we will show
? how a simple translation system is built and com-
piled by using the GF grammar compiler and the
resource grammar library
? how the translator is integrated in a web page
? how the translator is used in a web browser by
means of an integrated incremental parser
A preliminary demo can be seen in http://
grammaticalframework.org:41296. All the
demonstrated tools are available as open-source soft-
ware from http://grammaticalframework.
org.
The work reported here is supported by MOLTO
(Multilingual On-Line Translation. FP7-ICT-247914).
References
Alshawi, H. (1992). The Core Language Engine. Cam-
bridge, Ma: MIT Press.
Angelov, K. (2008). Type-Theoretical Bulgarian
Grammar. In B. Nordstro?m and A. Ranta (Eds.),
Advances in Natural Language Processing (Go-
TAL 2008), Volume 5221 of LNCS/LNAI, pp. 52?
64. URL http://www.springerlink.com/
content/978-3-540-85286-5/.
Angelov, K. (2009). Incremental Parsing with Parallel
Multiple Context-Free Grammars. In Proceedings of
EACL?09, Athens.
Angelov, K. and A. Ranta (2009). Implementing Con-
trolled Languages in GF. In Proceedings of CNL-
2009, Marettimo, LNCS. to appear.
Bar-Hillel, Y. (1964). Language and Information.
Reading, MA: Addison-Wesley.
Beckert, B., R. Ha?hnle, and P. H. Schmitt (Eds.) (2007).
Verification of Object-Oriented Software: The KeY
Approach. LNCS 4334. Springer-Verlag.
Bender, E. M. and D. Flickinger (2005). Rapid
prototyping of scalable grammars: Towards mod-
ularity in extensions to a language-independent
core. In Proceedings of the 2nd International
Joint Conference on Natural Language Process-
ing IJCNLP-05 (Posters/Demos), Jeju Island, Ko-
rea. URL http://faculty.washington.
edu/ebender/papers/modules05.pdf.
Bresnan, J. (Ed.) (1982). The Mental Representation of
Grammatical Relations. MIT Press.
Bringert, B., K. Angelov, and A. Ranta (2009). Gram-
matical Framework Web Service. In Proceedings of
EACL?09, Athens.
70
Butt, M., H. Dyvik, T. H. King, H. Masuichi,
and C. Rohrer (2002). The Parallel Grammar
Project. In COLING 2002, Workshop on Gram-
mar Engineering and Evaluation, pp. 1?7. URL
http://www2.parc.com/isl/groups/
nltt/pargram/buttetal-coling02.pdf.
Chandioux, J. (1976). ME?TE?O: un syste`me
ope?rationnel pour la traduction automatique des bul-
letins me?te?reologiques destine?s au grand public.
META 21, 127?133.
Curry, H. B. (1961). Some logical aspects of gram-
matical structure. In R. Jakobson (Ed.), Structure of
Language and its Mathematical Aspects: Proceed-
ings of the Twelfth Symposium in Applied Mathemat-
ics, pp. 56?68. American Mathematical Society.
Dada, A. E. and A. Ranta (2007). Implementing an
Open Source Arabic Resource Grammar in GF. In
M. Mughazy (Ed.), Perspectives on Arabic Linguis-
tics XX, pp. 209?232. John Benjamin?s.
Dean, M. and G. Schreiber (2004). OWL Web On-
tology Language Reference. URL http://www.
w3.org/TR/owl-ref/.
Dymetman, M., V. Lux, and A. Ranta (2000).
XML and multilingual document author-
ing: Convergent trends. In COLING,
Saarbru?cken, Germany, pp. 243?249. URL
http://www.cs.chalmers.se/?aarne/
articles/coling2000.ps.gz.
Enache, R., A. Ranta, and K. Angelov (2010). An
Open-Source Computational Grammar for Roma-
nian. In A. Gelbukh (Ed.), Intelligent Text Process-
ing and Computational Linguistics (CICLing-2010),
Iasi, Romania, March 2010, LNCS, to appear.
Fuchs, N. E., K. Kaljurand, and T. Kuhn (2008).
Attempto Controlled English for Knowledge Rep-
resentation. In C. Baroglio, P. A. Bonatti,
J. Ma?uszyn?ski, M. Marchiori, A. Polleres, and
S. Schaffert (Eds.), Reasoning Web, Fourth Inter-
national Summer School 2008, Number 5224 in
Lecture Notes in Computer Science, pp. 104?124.
Springer.
Hallgren, T. and A. Ranta (2000). An exten-
sible proof text editor. In M. Parigot and
A. Voronkov (Eds.), LPAR-2000, Volume 1955
of LNCS/LNAI, pp. 70?84. Springer. URL
http://www.cs.chalmers.se/?aarne/
articles/lpar2000.ps.gz.
Harper, R., F. Honsell, and G. Plotkin (1993). A
Framework for Defining Logics. JACM 40(1), 143?
184.
Joshi, A. (1985). Tree-adjoining grammars: How
much context-sensitivity is required to provide rea-
sonable structural descriptions. In D. Dowty,
L. Karttunen, and A. Zwicky (Eds.), Natural Lan-
guage Parsing, pp. 206?250. Cambridge University
Press.
Khegai, J. (2006). GF Parallel Resource Grammars and
Russian. In Coling/ACL 2006, pp. 475?482.
Khegai, J., B. Nordstro?m, and A. Ranta (2003).
Multilingual Syntax Editing in GF. In A. Gel-
bukh (Ed.), Intelligent Text Processing and
Computational Linguistics (CICLing-2003),
Mexico City, February 2003, Volume 2588 of
LNCS, pp. 453?464. Springer-Verlag. URL
http://www.cs.chalmers.se/?aarne/
articles/mexico.ps.gz.
Martin-Lo?f, P. (1984). Intuitionistic Type Theory.
Napoli: Bibliopolis.
Meza Moreno, M. S. and B. Bringert (2008). Inter-
active Multilingual Web Applications with Gram-
marical Framework. In B. Nordstro?m and A. Ranta
(Eds.), Advances in Natural Language Processing
(GoTAL 2008), Volume 5221 of LNCS/LNAI, pp.
336?347. URL http://www.springerlink.
com/content/978-3-540-85286-5/.
Montague, R. (1974). Formal Philosophy. New
Haven: Yale University Press. Collected papers
edited by Richmond Thomason.
Perera, N. and A. Ranta (2007). Dialogue System
Localization with the GF Resource Grammar
Library. In SPEECHGRAM 2007: ACL Workshop
on Grammar-Based Approaches to Spoken Lan-
guage Processing, June 29, 2007, Prague. URL
http://www.cs.chalmers.se/?aarne/
articles/perera-ranta.pdf.
Pollard, C. and I. Sag (1994). Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Power, R. and D. Scott (1998). Multilingual authoring
using feedback texts. In COLING-ACL.
Ranta, A. (1994). Type Theoretical Grammar. Oxford
University Press.
Ranta, A. (2004). Grammatical Framework: A
Type-Theoretical Grammar Formalism. The Jour-
nal of Functional Programming 14(2), 145?
189. URL http://www.cs.chalmers.se/
?aarne/articles/gf-jfp.ps.gz.
Ranta, A. (2009a). Grammars as Software Li-
braries. In Y. Bertot, G. Huet, J.-J. Le?vy, and
G. Plotkin (Eds.), From Semantics to Computer
Science. Cambridge University Press. URL
http://www.cs.chalmers.se/?aarne/
articles/libraries-kahn.pdf.
Ranta, A. (2009b). The GF Resource Gram-
mar Library. In Linguistic Issues in Lan-
guage Technology, Vol. 2. URL http:
//elanguage.net/journals/index.
php/lilt/article/viewFile/214/158.
Rosetta, M. T. (1994). Compositional Translation.
Dordrecht: Kluwer.
Shieber, S. M. and Y. Schabes (1990). Synchronous
tree-adjoining grammars. In COLING, pp. 253?258.
71
Proceedings of the 5th Workshop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 55?64,
Dublin, Ireland, August 23-29 2014.
Developing an interlingual translation lexicon using WordNets
and Grammatical Framework
Shafqat Mumtaz Virk
University of Gothenburg,
University of Eng. & Tech. Lahore
virk.shafqat@gmail.com
K.V.S. Prasad
Chalmers University of Technology
prasad@chalmers.se
Aarne Ranta
University of Gothenburg
aarne@chalmers.se
Krasimir Angelov
University of Gothenburg
krasimir@chalmers.se
Abstract
The Grammatical Framework (GF) offers perfect translation between controlled subsets
of natural languages. E.g., an abstract syntax for a set of sentences in school mathematics
is the interlingua between the corresponding sentences in English and Hindi, say. GF
?resource grammars? specify how to say something in English or Hindi; these are re-
used with ?application grammars? that specify what can be said (mathematics, tourist
phrases, etc.). More recent robust parsing and parse-tree disambiguation allow GF to
parse arbitrary English text. We report here an experiment to linearise the resulting
tree directly to other languages (e.g. Hindi, German, etc.), i.e., we use a language-
independent resource grammar as the interlingua. We focus particularly on the last part
of the translation system, the interlingual lexicon and word sense disambiguation (WSD).
We improved the quality of the wide coverage interlingual translation lexicon by using
the Princeton and Universal WordNet data. We then integrated an existing WSD tool
and replaced the usual GF style lexicons, which give one target word per source word,
by the WordNet based lexicons. These new lexicons and WSD improve the quality of
translation in most cases, as we show by examples. Both WordNets and WSD in general
are well known, but this is the first use of these tools with GF.
1 Introduction
1.1 Translation via an interlingua
Interlingual translation scales easily up to a large number of languages. Google translate, for
example, deals with all pairs of 60 languages mostly by using English as a pivot language. In
this way, it can do with just 2 * 59 = 118 sets of bilingual training data, instead of 60 * 59 =
3540 sets. It would be hard to collect and maintain so many pairs, and in many cases, there is
very little data to be found.
The roots of an inter-lingua are perhaps in the medieval idea of a universal grammar (Lyons,
1968), in which a universal representation of meaning can be expressed. Translating via this
interlingua then also means that meaning is conserved in going from the source to the tar-
get language. In recent decades, this idea appears in (Curry, 1961) where the interlingua is
called tectogrammar, in the Rosetta project (Rosetta, 1994), building on the semantic models
of (Montague, 1974), and in the UNL (Universal Networking Language) project.
Incidentally, interlingua is also the heart of modern compiler technology. For instance, the
GNU Compiler Collection (Stallman, 2001) uses a shared tree representation to factor out the
majority of compilation phases between a large number of source and target languages. Compiler
writers save work, and semantics is preserved by design. A compiler, then, is built as a pipeline
with parsing from a source language to an abstract syntax tree, which is analyzed and
optimized in the language-independent phases, and finally linearized to a target language.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and
proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.
0/
55
It is easy to see an analogy between this pipeline and the way a human language translator
could work. But how to make it real? How to scale up to the full size of natural languages?
1.2 WordNets
In current machine translation research, interlingual methods are marginal, despite the wide use
of pivot languages in systems like Google translate. Closest to the mainstream perhaps is the
development of linked WordNets. The original Princeton Wordnet for English (Miller, 1995) de-
fines a set of word senses, which many other wordnets map to other languages. Implementations
of this idea are Finnish (Lind?n and Carlson., 2010) and Hindi (Hindi-WordNet, 2012).
In the linked Wordnet approach, the Princeton WordNet senses work as an interlingua, albeit
only on the level of the lexicon. (Lind?n and Carlson., 2010) give strong arguments why in fact
this is a good way to go, despite the often emphasized fact that different languages divide the
world in different ways, so that the senses of their word don?t map one to one. The evidence from
the English-Finnish case shows that 80% of the mappings are one-to-one and un-problematic.
As this part of the lexicon can be easily reused, linguists and system builders can concentrate
their effort on the remaining 20%.
The Universal WordNet (de Melo and Weikum, 2009) works on the same lines. Building on
the Princeton WordNet, it populates the mappings to over 200 different languages by collecting
data from different sources (such as the Wikipedia) and using supervised machine learning
techniques to propagate the knowledge and infer more of it. What makes it a particularly
interesting resource is that it is freely available under the most liberal licenses, as is the original
Princeton WordNet,
1.3 GF
Grammatical Framework (GF)(Ranta, 2004) is a grammar formalism tool based on Martin
L?f?s type theory (Martin-L?f, 1982). It can be seen as a tool to build interlingua based trans-
lation systems. GF works like a compiler: the source language is parsed to an abstract syntax
tree, which is then linearized to the target language. The parsing and linearization component
are defined by using Parallel Multiple Context-Free Grammars (PMCFG, (Seki et al., 1991),
(Ljungl?f, 2004)), which give GF an expressive power between mildly and fully context-sensitive
grammars. Thus GF can easily handle with language-specific variations in morphology, word
order, and discontinuous constituents, while maintaining a shared abstract syntax.
Historically, the main use of GF has been in controlled language implementations, e.g., (Ranta
and Angelov, 2010; Angelov and Enache, 2010; Ranta et al., 2012) and natural language
generation, e.g., (Dymetman et al., 2000), both applied in multilingual settings with up to 15
parallel languages. In recent years, the coverage of GF grammars and the processing performance
has enabled open-domain tasks such as treebank parsing (Angelov, 2011) and hybrid translation
of patents (Enache et al., 2012). The general purpose Resource Grammar Library (RGL)(Ranta,
2011) has grown to 30 languages. It includes the major European languages, South Asian
languages like Hindi/Urdu (Prasad and Shafqat, 2012), Nepali and Punjabi (Shafqat et al.,
2011), the Southeast Asian language Thai, and Japanese and Chinese.
However, GF has yet not been exploited for arbitrary text parsing and translation. To do
this, we have to meet several challenges: robust parsing, parse-tree disambiguation, word sense
disambiguation, and development of a wide-coverage interlingual translation lexicon. This paper
focuses on the latter two. We report first a method of using the WordNets (Princeton and
Universal) to build an interlingual full-form, multiple sense translation lexicon. Then, we show
how these lexicons together with a word sense disambiguation tool can be plugged in a translation
pipeline. Finally, we describe an experimental setup and give many examples to highlight the
effects of this work.
56
1.4 South Asian languages
While the work described here can apply to any language, it is particularly interesting for South
Asian languages. In these languages, statistical tools do not have much bilingual training data to
work on, so Google translate and similar tools are not as useful as they are with better resourced
languages. At the same time, there is an urgent and widely recognised need for translations from
English to the various languages of South Asia. Fortunately, word nets are being built for many
of them, so that the techniques described here can be applied.
2 From Universal WordNet to a GF Lexicon
The original Princeton WordNet (Miller, 1995) defines a set of word senses, and the Universal
WordNet (de Melo and Weikum, 2009) maps them to different languages. In this multilingual
scenario, the Princeton WordNet senses can be seen as an abstract representation, while the
Universal WordNet mappings can be seen as concrete representation of those senses in different
languages. GF grammars use very much the same technique of one common abstract and
multiple parallel concrete representations to achieve multilingualism. Due to this compatibility,
it is easy to build a multilingual GF lexicon using data from those two resources (i.e. Princeton
and Universal WordNets). This section briefly describes the experiment we did to build one
abstract and multiple concrete GF lexicons for a number of languages including German, French,
Finnish, Swedish, Hindi, and Bulgarian. The method is very general, so can be used to build a
similar lexicon for any other language for which data is available in the Universal WordNet.
2.1 GF Abstract Lexicon
The Princeton WordNet data is distributed in the form of different database files. For each of
the four lexical categories (i.e. noun, verb, adjective, and adverb), two files named ?index.pos?
and ?data.pos? are provided, where ?pos? is noun, verb, adj and adv. Each of the ?index.pos?
files contains all words, including synonyms of the words, found in the corresponding part of
speech category. Each of the ?data.pos? files contains information about unique senses belonging
to the corresponding part of speech category. For our purposes, there were two possible choices
to build an abstract representation of the lexicon:
1. To include all words of the four lexical categories, and also their synonyms (i.e. to build
the lexicon from ?index.pos? files)
2. To include only unique senses of the four categories with one word per sense, but not the
synonyms (i.e. to build the lexicon from the data.pos? files)
To better understand this difference, consider the words ?brother? and ?buddy?. The word
?brother? has five senses with sense offsets ?08111676?, ?08112052?, ?08112961?, ?08112265? and
?08111905? in the Princeton WordNet 1.7.11, while the word ?buddy? has only one sense with the
sense offset ?08112961?. Choosing option (1) means that we have to include the following entries
in our abstract lexicon.
brother_08111676_N
brother_08112052_N
brother_08112961_N
brother_08112265_N
brother_08111905_N
buddy_08112961_N
We can see that the sense with the offset ?08112961? is duplicated in the lexicon: once with
the lemma ?brother? and then with the lemma ?buddy?. However, if we choose option (2), we
end up with the following entries.
1We choose WordNet 1.7.1, because the word sense disambiguator that we are using in our translation pipeline
is based on WordNet 1.7.1
57
brother_08111676_N
brother_08112052_N
brother_08112265_N
brother_08111905_N
buddy_08112961_N
Since the file ?data.noun? lists the unique senses rather than the words, their will be no
duplication of the senses. However, the choice has an obvious effect on the lexicon coverage, and
depending on whether we want to use it as a parsing or as a linearization lexicon, the choice
becomes critical. Currently, we choose option (2) for the following two reasons:
1. The Universal WordNet provides mappings for synsets (i.e. unique senses) but not for the
individual synonyms of the synsets. If we choose option (1), as mentioned previously, we
have to list all synonyms in our abstract representation. But, as translations are available
only for synsets, we have to put the same translation against each of the synonyms of the
synset in our concrete representations. This will not gain us anything (as long as we use
these lexicon as linearization lexicons), but will increase the size of the lexicon and hence
may have reduce the processing speed of the translation system.
2. At the current stage of our experiments we are using these lexicons as linearization lexicons,
so one translation of each unique sense is enough.
Our abstract GF lexicon covers 91516 synsets out of around 111,273 synsets in the WordNet
1.7.1. We exclude some of the synsets with multi-word lemmas. We consider them more of a
syntactic category rather than a lexical category, and hence deal with them at the syntax level.
Here, we give a small segment of our abstract GF lexicon.
abstract LinkedDictAbs = Cat ** {
fun consecutive_01624944_A : A ;
fun consequently_00061939_Adv : Adv ;
fun conservation_06171333_N : N ;
fun conspire_00562077_V : V ;
fun sing_01362553_V2 : V2 ;
........
}
The first line in the above given code states that the module ?LinkedDictAbs? is an abstract
representation (note the keyword ?abstract?). This module extends (achieved by ?**? operator)
another module labeled ?Cat2? which, in this case, has definitions for the morphological categories
?A?, ?Adv?, ?N? and ?V?. These categories correspond to the ?adjective?, ?adverb?, ?noun?, and ?verb?
categories in the WordNet respectively. However, note that in GF resource grammars we have
a fine-grained morphological division for verbs. We sub-categorize them according to their
valencies i.e ?V? is for intransitive, and ?V2? for transitive verbs. We refer to (Bringert et al.,
2011) for more details on these divisions.
Each entry in this module is of the following general type:
fun lemma_senseOffset_t : t ;
Keyword ?fun? declares each entry as a function of the type ?t?. The function name is composed
of lemma, sense offset and a type ?t?, where lemma and sense offset are same as in the Princeton
WordNet, while ?t? is one of the morphological types in GF resource grammars.
This abstract representation will serve as a pivot for all concrete representations, which are
described next.
2This module has definitions of different morphological and syntactic categories in the GF resource grammar
library
58
2.2 GF Concrete Lexicons
We build the concrete representations for different languages using the translations obtained
from the Universal WordNet data and GF morphological paradigms (D?trez and Ranta, 2012;
Bringert et al., 2011). The Universal WordNet translations are tagged with a sense offset from
WordNet 3.03 and also with a confidence score. As, an example consider the following segment
form the Universal WordNet data, showing German translations for the noun synset with offset
?13810818? and lemma ?rest? (in the sense of ?remainder?).
n13810818 Rest 1.052756
n13810818 Abbrand 0.95462
n13810818 Ruckstand 0.924376
Each entry is of the following general type.
posSenseOffset translation confidence-score
If we have more than one candidate translation for the same sense (as in the above case),
we select the best one (i.e. with the maximum confidence score) and put it in the concrete
grammar. Next, we give a small segment from the German concrete lexicon.
concrete LinkedDictGer of LinkedDictAbs = CatGer ** open
ParadigmsGer, IrregGer,Prelude in {
lin consecutive_01624944_A = mkA "aufeinanderfolgend" ;
lin consequently_00061939_Adv = mkAdv "infolgedessen" ;
lin conservation_06171333_N = mkN "Konservierung" ;
lin conspire_00562077_V = mkV "anzetteln" ;
lin sing_01362553_V2 = mkV2 (mkV "singen" ) ;
......
}
The first line declares ?LinkedDictGer? to be the concrete representation of the previously
defined abstract representation (note the keyword ?concrete? at the start of the line). Each entry
in this representation is of the following general type:
lin lemma_senseOffset_t = paradigmName "translation" ;
Keyword ?lin? declares each entry to be a linearization of the corresponding function in the
abstract representation. ?paradigmName? is one of the morphological paradigms defined in the
?ParadigmsGer? module. So in the above code, ?mkA?, ?mkAdv?, ?mkN?, ?mkV? and ?mkV2? are
the German morphological paradigms4 for different lexical categories of ?adjective?, ?adverb?,
?noun?, ?intransitive verb?, and ?transitive verb? respectively. ?translation? is the best possible
translation obtained from the Universal WordNet. This translation is passed to a paradigm as
a base word, which then builds a full-form inflection table. These tables are then used in the
linearization phase of the translation system (see section 3)
Concrete lexicons for all other languages were developed using the same procedure. Table 1
gives some statistics about the coverage of these lexicons.
Language Number of Entries Language Number of Entries
Abstract 91516 German 49439
French 38261 Finnish 27673
Swedish 23862 Hindi 16654
Bulgarian 12425
Table 1: Lexicon Coverage Statistics
3However, in our concrete lexicons we match them to WordNet 1.7.1 for the reasons mentioned previously
4See (Bringert et al., 2011) for more details on these paradigms
59
3 System architecture
Figure 1 shows an architecture of the translation pipeline. The architecture is inter-lingual
and uses the Resource Grammar Library (RGL) of Grammatical Framework (Ranta, 2011) as
the syntax and semantics component, Penn Treebank data for parse-tree disambiguation and
IMS(It Makes Sense)(Zhong and Ng, 2010) as a word sense disambiguation tool. Even though
the syntax, semantics and parse-tree disambiguation are not the main topics of this paper,
we give the full architecture to show where the work reported in this paper fits. Internal GF
resources (e.g. resource grammars and dictionaries) are shown in rectangles while the external
components (e.g. PennTreebank and IMS(Zhong and Ng, 2010): a wide coverage word sense
disambiguation system for arbitrary text.) are shown in double-stroked rectangles.
With reference to Figure 1: The input is parsed using English resource grammar (EngRG)
and a comprehensive English dictionary (DictEng). If the input is syntactically ambiguous the
parser will return more than one parse-tree. These trees are disambiguated using a statistical
model build from the PennTreebank data. The best tree is further processed using the input
from the IMS to tag the lexical nodes with best sense identifiers. This tree is finally linearized
to the target language using the target language resource grammar (TLRG) together with the
target language lexicon (LinkedDict) discussed in section 2.
Inp
ut
Parsing
EngRG+DictEng
Pa
rse
-Tr
ee
s
Parse Tree 
Disambigu
ation
Be
st-
Tre
e
Word 
Sense 
Disambigu
ation
Linearizati
on
Sense-Ta
gged-Tre
e
IMS
TLRG+LinkedDict
Out
put
Penn Treebank
EngRG: English Resource Grammar
TLRG: Target Language Resource Grammar
Figure 1: The translation pipeline.
4 Experimental Setup and Evaluation
Our experimental setup is as follows: We take some English text as source, and translate it to a
target language (German and Hindi in these experiments) by passing it through the translation
pipeline described in section 3. To show the usefulness of the lexicons described in section 2 and
for comparison, we translate the same source twice: with and without word sense disambiguation.
For the first attempt, we used exactly the same translation pipeline as shown in Figure 1,
except that to overcome the deficiencies of our existing parse-tree disambiguator, for some of
the examples, we used trees directly from the PennTreebank, which are supposed to be correct.
However, this should not damage the claims made in this paper which is about developing
wide coverage interlingual translation lexicons and then using them for WSD in an interlingual
translation pipeline.
For the second attempt, we plugged out the word sense disambiguation form the translation
pipeline and used our old GF style lexicons (one target word per source word irrespective of its
sense) in the linearization phase.
Finally, we compared both candidate translations to see if we have gained anything. We did
both manual and automatic evaluations to confirm our findings.
For a set of 25 sentences for English-German pair we got marginal BLEU score improvements
(from 0.3904 to 0.399 with ?old? and ?new? dictionaries). Manual inspection, however, was much
more encouraging, and explained the reasons for very low improvements in the BLEU scores in
some cases. The reason was that even if the word sense disambiguation, and hence, our new
60
lexicon gives a better lexical choice, it will still be considered ?wrong? by the evaluation tool if the
gold-standard has a different choice. It was also observed that there were cases where the ?old?
lexicon produced a much better translation than the ?new? one. The reasons for this are obvious.
The word sense disambiguator has its own limitations and is known to make mistakes. Also, as
explained in Section 5, the lexicon cannot be guaranteed to always give the right translation.
Next, we give a number of example sentence with comments5 to show that how the new
lexicons improved the quality of translations, and also give some examples where it worked the
other way around.
4.1 German
1. Source He increases the board to seven
Without WSD er erh?ht das Brett nach einigen sieben
With WSD er vergr??ert die Beh?rde nach einigen sieben
Comments das Brett is a wooden board (wrong); erh?ht means ?to raise?. while
vergr??ert means ?increases the size?. Note the wrong preposition choice (?to? should
be zu rather than nach). Also, an indefinite determiner (einige, some) has been
wrongly added to the cardinal number is used as a noun phrase.
2. Source the index uses a base of 100 in 1,982
Without WSD das Verzeichnis verwendet eine Base nach einige 100 in einigen
1982
With WSD der [index_11688271_N] nutzt einen Operationsbasis von einigen
100 in einigen 1982
Comments Note the untranslated word in the WSD version. Base means a chemical base,
the wrong meaning here. Operationsbasis is not the best choice, but acceptable.
3. Source fear is the father of panic
With WSD Angst ist der Papa von Angst
Comment The traditional hilarious example, saying ?fear is fear?s daddy?.
4.2 Hindi
To represent Hindi, we use an IPA style alphabet, with the usual values and conventions.
Retroflexed sounds are written with a dot under the letter: ?, ?, and ? (a flap) are com-
mon, while ? and ? occur in Sanskritised Hindi (though many dialects pronounce them n and
?). The palatalised spirant is shown ? and aspirated stops are shown thus: kh. A macron over a
vowel denotes a long vowel, and ?, nasalisation. In Hindi, e and o are always long, so the macron
is dropped. Finally, we use ? to mean the nasal homorganic with the following consonant.
Here are examples from our evaluation showing that the WSD system works well; the versions
without WSD merely pick the first synonym in the lexicon.
1. Source Mr Baris is a lawyer in New York .
Without WSD Mr Baris New York m? k?n?n k? pa??it h?
With WSD Mr Baris New York m? vak?l h?
Word order Mr Baris New York in lawyer is
Comments k?n?n k? pa??it is ?expert/teacher in law?, while vak?l means ?lawyer?.
2. Source we don?t depend on pharmaceutical companies for our support
Without WSD ham au?adh?ya sahy?g? par ham?re bhara? p??a? ke liye nah??
nirte h??.
5For the comments on German, we are indebted to Erzsebet Galgoczy and Wolfgang Ahrendt, our colleagues
and German informants.
61
With WSD ham au?adh?ya ka?pan? par ham?re nirv?h vyay ke liye nah?? ?te h??.
Word order We pharmaceutical companies on our subsistence expenditure for
not ??? do
Comments sahy?g? means ?company? in the sense of ?colleagues?, nirv?h vyay means
?subsistence expenditure? , while bhara? p??a? means ?weight bearing?. The penul-
timate word in both versions is nonsense, and the lexicons need to be debugged.
3. Source you may recall that a triangle is also a polygon
Without WSD tum "recall may" ho ki ?r?yengl "also" bahubhuj h?
With WSD tum smara? kar sakte ho ki triko? bh? bahubhuj h?
Word order You recall do can that triangle also polygon is
Comments The version without WSD has several missing words. The WSD version of
?recall? is not idiomatic, but understandable.
It should be noted that the coverage of the Hindi lexicon is lowest of all the lexicons given
in Table 1. The result is that many sentences have missing words in the translations. Also,
there is considerable interference with Urdu words (some stemming from the shared base
grammar (Prasad and Shafqat, 2012)). Further, some mappings coming from the Universal
WordNet data are in roman, as opposed to Devanagari (the usual script for Hindi, and what
the grammar is based on), so these need to be transcribed. Finally, idiomatic phrases are
a problem (?before the law? is likely to be rendered ?(temporally) before the law? rather
than ?in the eyes of the law?).
5 The next steps
Since the Universal WordNet mappings are produced from parallel data by machine learning
techniques, the translations are not always accurate and do not always make the best possible
choice. This leaves a window for improvement in the quality of the reported lexicons. One
way of improvement is the manual inspection/correction, not an easy task for a wide-coverage
lexicon with around 100 thousand entries, but not impossible either. This would be a one-time
task with a strong impact on the quality of the lexicon. Another way is to use manually built
WordNets, such as the Finnish and Hindi WordNets. In our work, the availability of some of
these resources was an issue, so we leave it for the future. Further, as mentioned in Section 4,
the Hindi lexicon has some script-related issues which should be fixed in future.
When it comes to interlingua-based arbitrary machine translation, an important concern is
the size of lexicons. We are aware of the fact that the size of our lexicons is not comparable to
some of the other similar systems such as ATLAS-II (Fujitsu), where the size of lexicons is in
millions. We have plan to extend the size of lexicons using some of the other publicly available
resources (such as Hindi WordNet) and/or using parallel corpus. The development of bilingual
lexicons form parallel corpus have been previously explored (Delpech et al., 2012; Qian et al.,
2012), and the same ideas can be applied in our case.
6 Conclusion
We have shown how to use existing lexical resources such as WordNets to develop an interlingual
translation lexicon in GF, and how to use it for the WSD task in an arbitrary text translation
pipeline. The improvements in the translation quality (lexical), shown by examples in Section
4, are encouraging and motivate further work in this direction. However, it should be noted
that there is still a lot of work to be done (especially in the open domain text parsing and
parse-tree disambiguation phases of the translation pipeline) to bring the translation system to
a competitive level. For the reasons noted in the introduction, we expect our techniques to be
particularly useful for South Asian languages.
62
References
Angelov, K. (2011). The Mechanics of the Grammatical Framework. PhD thesis, Chalmers University
Of Technology. ISBN 978-91-7385-605-8.
Angelov, K. and Enache, R. (2010). Typeful Ontologies with Direct Multilingual Verbalization. In Fuchs,
N. and Rosner, M., editors, CNL 2010, Controlled Natural Language.
Bringert, B., Hallgren, T., and Ranta., A. (2011). GF resource grammar library synopsis.
www.grammaticalframework.org/lib/doc/synopsis.html.
Curry, H. B. (1961). Some logical aspects of grammatical structure. In Jakobson, R., editor, Structure of
Language and its Mathematical Aspects: Proceedings of the Twelfth Symposium in Applied Mathematics,
pages 56?68. American Mathematical Society.
de Melo, G. and Weikum, G. (2009). Towards a Universal Wordnet by learning from combined evidence.
In Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM 2009),
pages 513?522, New York, NY, USA. ACM.
Delpech, E., Daille, B., Morin, E., and Lemaire, C. (2012). Extraction of domain-specific bilingual lexicon
from comparable corpora: Compositional translation and ranking. In Proceedings of COLING 2012,
pages 745?762, Mumbai, India. The COLING 2012 Organizing Committee.
D?trez, G. and Ranta, A. (2012). Smart paradigms and the predictability and complexity of inflectional
morphology. In EACL, pages 645?653.
Dymetman, M., Lux, V., and Ranta, A. (2000). XML and multilingual document authoring: Conver-
gent trends. In Proc. Computational Linguistics COLING, Saarbr?cken, Germany, pages 243?249.
International Committee on Computational Linguistics.
Enache, R., Espa?a-Bonet, C., Ranta, A., and M?rquez, L. (2012). A hybrid system for patent translation.
In Proceedings of the 16th Annual Conference of the European Association for Machine Translation
(EAMT12), Trento, Italy.
Hindi-WordNet (2012). Hindi Wordnet. 2012. Universal Word?Hindi Lexicon.
http://www.cfilt.iitb.ac.in.
Lind?n, K. and Carlson., L. (2010). Finnwordnet?wordnet p? finska via ?vers?ttning. Lexi-
coNordica?Nordic Journal of Lexicography, 17:119?140.
Ljungl?f, P. (2004). The Expressivity and Complexity of Grammatical Framework. PhD thesis, Dept. of
Computing Science, Chalmers University of Technology and Gothenburg University. http://www.cs.
chalmers.se/~peb/pubs/p04-PhD-thesis.pdf.
Lyons, J. (1968). Introduction to theoretical linguistics. Cambridge: Cambridge University Press.
Martin-L?f, P. (1982). Constructive mathematics and computer programming. In Cohen, Los, Pfeif-
fer, and Podewski, editors, Logic, Methodology and Philosophy of Science VI, pages 153?175. North-
Holland, Amsterdam.
Miller, G. A. (1995). Wordnet: A lexical database for English. Communications of the ACM, 38:39?41.
Montague, R. (1974). Formal Philosophy. Yale University Press, New Haven. Collected papers edited
by Richmond Thomason.
Prasad, K. V. S. and Shafqat, M. V. (2012). Computational evidence that Hindi and Urdu share a
grammar but not the lexicon. In The 3rd Workshop on South and Southeast Asian NLP, COLING.
Qian, L., Wang, H., Zhou, G., and Zhu, Q. (2012). Bilingual lexicon construction from comparable
corpora via dependency mapping. In Proceedings of COLING 2012, pages 2275?2290, Mumbai, India.
The COLING 2012 Organizing Committee.
Ranta, A. (2004). Grammatical Framework: A Type-Theoretical Grammar Formalism. The Journal of
Functional Programming, 14(2):145?189. http://www.cse.chalmers.se/~aarne/articles/gf-jfp.
pdf.
Ranta, A. (2011). Grammatical Framework: Programming with Multilingual Grammars. CSLI Publica-
tions, Stanford. ISBN-10: 1-57586-626-9 (Paper), 1-57586-627-7 (Cloth).
63
Ranta, A. and Angelov, K. (2010). Implementing Controlled Languages in GF. In Proceedings of CNL-
2009, Athens, volume 5972 of LNCS, pages 82?101.
Ranta, A., D?trez, G., and Enache, R. (2012). Controlled language for everyday use: the MOLTO
phrasebook. In CNL 2012: Controlled Natural Language, volume 7175 of LNCS/LNAI.
Rosetta, M. T. (1994). Compositional Translation. Kluwer, Dordrecht.
Seki, H., Matsumura, T., Fujii, M., and Kasami, T. (1991). On multiple context-free grammars. Theo-
retical Computer Science, 88:191?229.
Shafqat, M., Humayoun, M., and Aarne, R. (2011). An open source Punjabi resource grammar. In Pro-
ceedings of the International Conference Recent Advances in Natural Language Processing 2011, pages
70?76, Hissar, Bulgaria. RANLP 2011 Organising Committee. http://aclweb.org/anthology/R11-1010.
Stallman, R. (2001). Using and Porting the GNU Compiler Collection. Free Software Foundation.
Zhong, Z. and Ng, H. T. (2010). It makes sense: A wide-coverage word sense disambiguation system
for free text. In Proceedings of the ACL 2010 System Demonstrations, pages 78?83, Uppsala, Sweden.
Association for Computational Linguistics. http://www.aclweb.org/anthology/P10-4014.
64
