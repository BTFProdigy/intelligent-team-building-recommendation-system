Proceedings of the EACL 2012 Student Research Workshop, pages 38?45,
Avignon, France, 26 April 2012. c?2012 Association for Computational Linguistics
What?s in a Name?
Entity Type Variation across Two Biomedical Subdomains
Claudiu Miha?ila? and Riza Theresa Batista-Navarro
National Centre for Text Mining
School of Computer Science, University of Manchester
Manchester Interdisciplinary Biocentre,
131 Princess Street, M1 7DN, Manchester, UK
claudiu.mihaila@cs.man.ac.uk
riza.batista-navarro@cs.man.ac.uk
Abstract
There are lexical, syntactic, semantic and
discourse variations amongst the languages
used in various biomedical subdomains. It
is important to recognise such differences
and understand that biomedical tools that
work well on some subdomains may not
work as well on others. We report here
on the semantic variations that occur in
the sublanguages of two biomedical subdo-
mains, i.e. cell biology and pharmacology,
at the level of named entity information. By
building a classifier using ratios of named
entities as features, we show that named en-
tity information can discriminate between
documents from each subdomain. More
specifically, our classifier can distinguish
between documents belonging to each sub-
domain with an accuracy of 91.1% F-score.
1 Introduction
Biomedical information extraction efforts in the
past decade have focussed on fundamental tasks
needed to create intelligent systems capable
of improving search engine results and easing
the work of biologists. More specifically, re-
searchers have concentrated mainly on named en-
tity recognition, mapping them to concepts in
curated databases (Krallinger et al 2008) and
extracting simple binary relations between enti-
ties. Recently, an increasing number of resources
that facilitate the training of systems to extract
more detailed information have become available,
e.g., PennBioIE (Kulick et al 2004), GENE-
TAG (Tanabe et al 2005), BioInfer (Pyysalo et
al., 2007), GENIA (Kim et al 2008), GREC
(Thompson et al 2009) and Metaknowledge GE-
NIA (Thompson et al 2011). Moreover, several
other annotated corpora have been developed for
shared task purposes, such as BioCreative I, II, III
(Arighi et al 2011) and BioNLP Shared Tasks
2009 and 2011 (Cohen et al 2009; Kim et al
2011).
Many of the tools currently used for biomedi-
cal language processing were trained and evalu-
ated on such popular corpora, most of which con-
sist of documents from the molecular biology sub-
domain. However, previous studies (discussed in
Section 2) have established that different biomed-
ical sublanguages exhibit linguistic variations. It
follows that tools which were developed and eval-
uated on corpora derived from one subdomain
might not always perform as well on corpora from
other subdomains. Understanding these linguistic
variations is essential to the process of adaptat-
ing natural language processing tools to new do-
mains.
In this paper, we highlight the variations be-
tween biomedical sublanguages by focussing on
the different types of named entities (NEs) that
are relevant to them. We show that the frequen-
cies of different named entity types vary enough
to allow a classifier for scientific subdomains to
be built based upon them.
The study is performed on open access jour-
nal articles present in the UK PubMed Central1
(UKPMC) (McEntyre et al 2010), an article
database that extends the functionality of the orig-
inal PubMed Central (PMC) repository2. This
database was chosen as our source, since most of
the documents within it are already tagged with
named entity information. We report here on the
results obtained for two biomedical subdomains,
1http://ukpmc.ac.uk/
2http://www.ncbi.nlm.nih.gov/pmc
38
i.e. cell biology and pharmacology. Our focus on
these two particular subdomains is motivated by
an increasing interest expressed by the biomedi-
cal research community, according to recent find-
ings that have shown their relevance to discover-
ing possible causes and treatments for incurable
diseases, such as cancer or Alzheimer?s Disease.
2 Related work
Harris (1968) introduced a formalisation of the
notion of sublanguage, which was defined as a
subset of general language. According to this
theory, it is possible to process specialised lan-
guages, since they have a structure that can be ex-
pressed in a computable form. More recently, sev-
eral works on the study of biomedical languages
substantiated his theory.
For instance, Sager et al(1987) worked on
pharmacological literature and lipid metabolism,
whereas Friedman et al(2002) analysed the prop-
erties of clinical and biomolecular sublanguages.
Other studies have investigated the differ-
ences between general and biomedical lan-
guages by focussing on specific linguistic aspects,
such as verb-argument relations and pronomi-
nal anaphora. For instance, Wattarujeekrit et al
(2004) analysed the predicate-argument structures
of 30 verbs used in biomedical articles. Their re-
sults suggest that, in certain cases, a significant
difference exists in the predicate frames compared
to those obtained from analysing news articles in
the PropBank project (Palmer et al 2005). Sim-
ilarly, based on the GENIA and PennBioIE cor-
pora, Cohen et al(2008) performed a study of
argument realisation with respect to the nominal-
isation and alternation of biomedical verbs. They
concluded that there is a high occurrence of these
phenomena in this semantically restricted do-
main, and underline that this sublanguage model
applies only to biomedical language.
Taking a different angle, Nguyen and Kim
(2008) examined the differences in the use of
pronouns by studying general domains (MUC
and ACE) and one biomedical domain (GENIA).
They observed that compared to the MUC and
ACE corpora, the GENIA corpus has significantly
more occurrences of neutral and third-person pro-
nouns, whilst first and second person pronouns
are non-existent.
Verspoor et al(2009) measured lexical and
structural variation in biomedical Open Access
journals and subscription-based journals, con-
cluding that there are no significant differences
between them. Therefore, a model trained on one
of these sources can be used successfully on the
other, as long as the subject matter is maintained.
Furthermore, they compared a mouse genomics
corpus with two reference corpora, one composed
of newswire texts and another of general biomed-
ical articles. In this case, unsurprisingly, signifi-
cant differences were found across many linguis-
tic dimensions. Relevant to our study is the com-
parison between the more specific mouse genome
corpus to the more general biomedical one: whilst
similar from some points of view, such as nega-
tion and passivisation, they differ in sentence
length and semantic features, such as the presence
of various named entities.
Our work is most similar to that of Lippincott
et al(2011), in which a clustering-based quantita-
tive analysis of the linguistic variations across 38
different biomedical sublanguages is presented.
They investigated four dimensions relevant to the
performance of NLP systems, i.e. vocabulary,
syntax, semantics and discourse structure. With
regard to semantic features, the authors induced
a topic model using Latent Dirichlet Analysis for
each word, and then extended the model to docu-
ments and subdomains according to observed dis-
tributions. Their conclusion is that a machine
learning system is able to create robust clusters
of subdomains, thus proving their hypothesis that
the commonly used molecular biology subdomain
is not representative of the domain as a whole.
In contrast, we examine the differences be-
tween biomedical sublanguages at the semantic
level, using only named entities. Furthermore,
we choose to perform our analysis only on two
subdomains (i.e. cell biology and pharmacology),
and try to classify these by using supervised ma-
chine learning algorithms.
3 Methodology
We designed an experiment in which various ma-
chine learning algorithms are trained and tested
on data obtained from open access journal arti-
cles. Firstly, a corpus of articles was created (Sec-
tion 3.1), after which the documents were auto-
matically annotated with named entities (Section
3.2). We then extracted a number features rele-
vant to the named entities present in the corpus
(Section 3.3).
39
3.1 Corpus development
Our corpus was created by first searching the
NLM Catalog3 for journals whose Broad Sub-
ject Term attributes contain only cell biology or
pharmacology, and then narrowing down the re-
sults to those which are in English and avail-
able via PubMed Central. Also, since we are
concentrating on full-text documents, we retained
only those journals that are available within the
PubMed Open Access subset4. According to this
procedure, we obtained a final list of two journals
for cell biology and six for pharmacology.
Using the PMC IDs of all articles published
in the selected journals, we retrieved documents
from UK PubMed Central. This database was
chosen as our source as the documents it contains
are already tagged with named entity information.
A total of 360 articles was retrieved for each cat-
egory, i.e. cell biology and pharmacology.
The retrieved documents were encoded in
XML format. Several unusable fragments were
removed before converting them to plain text. Ex-
amples of such fragments are article metadata (au-
thors, their affiliations, publishing history, etc.),
tables, figures and references. Table 1 shows the
statistics regarding the corpus following the ap-
plication of the pre-processing step. In the case
of pharmacology, the document collection con-
tains almost 1.4 million words, whilst the set of
cell biology articles consists of almost 2.5 million
words. The ratio of named entities to the total
number of words is almost the same in the two
collections, i.e. about 10%.
Subdomain Cell biology Pharmacology
No. of docs. 360 360
No. of words 2.49 m. 1.35 m.
No. of NEs 231761 103484
Table 1: Named entity types and their source.
3.2 Tagging of Named Entities
To extract named entities from the corpus, we
used a simple method that augments the named
entities present in the UKPMC articles with the
output of two named entity recognition tools
3http://www.ncbi.nlm.nih.gov/
nlmcatalog
4http://www.ncbi.nlm.nih.gov/pmc/
tools/openftlist
(NERs), i.e. NeMine and OSCAR. The types of
entities in the output be each of the two tools, to-
gether with the NE types present in the UKPMC
articles, are summarised in Table 2.
Named entities in the UKPMC database were
identified using NeMine (Sasaki et al 2008), a
dictionary-based statistical named entity recogni-
tion system. This system was later extended and
used by Nobata et al(2009) to recognise more
types, such as phenomena, processes, organs and
symptoms. We used this most recent version of
the software as our second source of more diverse
entity types.
The Open-Source Chemistry Analysis Rou-
tines (OSCAR) software (Jessop et al 2011) is
a toolkit for the recognition of named entities and
data in chemistry publications. Currently in its
fourth version, it uses three types of chemical en-
tity recognisers, namely regular expressions, pat-
terns and Maximum Entropy Markov models.
In total, 20 different classes of entities were
considered in this study. However, due to the
combination of several NERs, some NE types are
identified by more than one NER. Furthermore,
some of the NE types are more general and cover
other more specific types, which are also anno-
tated by one or mroe of the tools. This can lead to
double annotation. For instance, the Gene|Protein
type is more general than both Gene and Protein,
whereas the Chemical molecule type is a hyper-
nym of Gene, Protein, Drug and Metabolite. In
the case of multiple annotations over the same
span of text, we removed the more general labels,
so that each NE has only one label. Contradictory
cases, where two NERs label one NE with com-
pletely different tags, were not found.
After augmenting the existing NEs by running
the two NER tools on the corpus, the outputs were
combined to give a single ?silver? annotation list.
This operation was performed by computing the
mathematical union of the three individual anno-
tation sets, as shown in Equation 1.
ASilver = AUKPMC ? AOscar ? ANeMine (1)
Table 3 shows the ratios of named entities to the
number of words in each subcorpus. The ? sign
indicates strictly positive percentages, but which
are rounded down to zero in this table for for-
matting purposes. In the four places where it oc-
curs, the percentages lie between 0% and 0.005%,
40
Type UKPMC NeMine OSCAR
Gene X X
Protein X X
Gene|Protein X
Disease X X
Drug X X
Metabolite X X
Bacteria X
Diagnostic process X
General phenomenon X
Human phenomenon X
Indicator X
Natural phenomenon X
Organ X
Pathologic function X
Symptom X
Therapeutic process X
Chemical molecule X
Chemical adjective X
Enzyme X
Reaction X
Table 2: Named entity types and their source.
exclusively. It can be observed that some entity
types have approximately the same percentages in
the two subdomains, e.g. phenomena and reac-
tions. However, large differences can be observed
in the case of some of the other entity types. For
instance, chemical molecules occur twice as of-
ten in pharmacology articles than in cell biology,
whereas proteins appear almost three times more
often in cell biology than in pharmacology.
3.3 Experimental setup
Using the corpus described previously, we cre-
ated a training set for supervised machine learn-
ing algorithms. Every document in the corpus
was transformed into a vector consisting of 20
features. Each of these features corresponds to
an entity type in Table 2, having a numeric value
ranging from 0 to 1. This number represents the
ratio of the specific entity type to the total number
of named entities recognised in that document, as
shown in Equation 2.
? =
ntype
N
(2)
where ntype represents the number of NEs of a
certain type in a document and N represents the
total number of NEs in that document.
Furthermore, each vector was labelled with the
subdomain to which the respective document be-
longs (i.e., cell biology or pharmacology).
Weka (Witten and Frank, 2005; Hall et al
2009) was employed as the machine learning
framework, due to its large variety of classifica-
tion algorithms. We experimented with a large
number of classifiers, ranging from Bayesian nets
to functions, decision trees, decision rules and
meta-classifiers. The best performing classifiers
are shown in Table 4. BayesNet is an implemen-
tation of Bayesian Networks, SMO is an imple-
mentation of Support Vector Machines, J48 is an
implementation of decision trees, whilst Jrip is an
implementation of decision rules. Random For-
est is an ensemble classifier that consists of many
decision trees (in this study, J48 was used), out-
putting the class that occurs most frequently in the
output of individual trees.
The baseline that has been used is ZeroR, a sim-
ple algorithm that classifies all instances as per-
taining to the majority class. Since our classes
have equal numbers of instances, the F-score of
ZeroR is 50%.
41
Type CellBio Pharma
Enzyme 0.05% 0.09%
Bacteria 0.01% 0.16%
Chemical adjective ?0% ?0%
Chemical molecule 30.13% 60.86%
Diagnose process 0.03% 0.23%
Disease 3.35% 4.27%
Drug 1.25% 2.83%
Gene 0.87% 1.09%
Gene|Protein 5.02% 0.89%
General phenomenon ?0% 0.01%
Human phenomenon 0% ?0%
Indicator 0.36% 0.16%
Metabolite 3.26% 7.53%
Natural phenomenon 0.02% 0.1%
Organ 0.09% 0.27%
Pathologic function 0.04% 0.04%
Protein 53.31% 19.13%
Reaction 1.71% 1.31%
Symptom 0.03% 0.06%
Therapeutic process 0.47% 0.96%
Table 3: Ratios of NE types to the total number of NEs
in the two subdomains.
4 Results
The previously described features were used as in-
put to various supervised machine learning algo-
rithms; results and error analysis are provided in
Section 4.1 and Section 4.2, respectively.
4.1 Experimental results
As can be seen from Table 4, Random Forest
performs best, with 91.1% F-score. The other
three classifiers give lower results, varying be-
tween 86% and 89.5%.
Algorithm P R F1
BayesNet 89.5 89.4 89.4
SMO 86.1 86.1 86.1
JRip 87.8 87.8 87.8
J48 86.8 86.8 86.8
Random Forest 91.3 91.1 91.1
Table 4: Classification results for the best-performing
algorithms.
We also employed AdaBoost in conjunction
with the previously mentioned four classifiers,
and the results are given in Table 5. AdaBoost
is a meta-algorithm that adapts itself during the
course of several iterations in the sense that in
each iteration, classifiers built are tweaked to cor-
rect those instances misclassified by prior classi-
fiers. In this study, AdaBoost was run over 20
iterations, and it significantly improved the result
of J48, by almost 4%, to 90.3%. However, Ad-
aBoost decreased the F-score of Random Forest
by 1% and that of BayesNet by 0.3%.
Algorithm P R F1
BayesNet 89.2 89.2 89.2
SMO 86.1 86.1 86.1
JRip 87.9 87.9 87.9
J48 90.3 90.3 90.3
Random Forest 90.3 90.1 90.1
Table 5: Classification results for AdaBoost in con-
junction with the best-performing algorithms.
In order to determine which features have the
most influence on classification, regardless of
the classifying algorithm, two attribute evaluators
were used to measure the information gain for
each feature and to compute the value of the chi-
squared statistic with respect to the class. The val-
ues obtained are shown in Table 6, and to illustrate
their influence, are plotted in Figure 1, after being
normalised.
Unsurprisingly, Protein is the feature with the
most discriminatory power, considering it has the
highest count and it occurs almost three times
more often in the cell biology class than in the
pharmacology class. Chemical molecules follow
closely, again due to a high count and large differ-
ence between the classes. Due to their high scores
obtained from the attribute evaluators, we ran the
experiment again considering only these two fea-
tures. The Random Forest classifier achieved an
F-score of 80% using these parameters.
At the other end of the scale, there are five
features which have very little influence in dis-
criminating between the two classes. The corre-
sponding named entity types have the lowest oc-
currence counts in the corpora, with the exception
of Organ. When running Random Forest with
these five features only, an F-score of 50.5% is
obtained. This result is very close to the baseline,
surpassing it by only a small fraction.
4.2 Error analysis
As can be seen in Table 7, a total of 64 papers
were misclassified by the Random Forest classi-
42
Attribute InfoGain ChiSquare
Protein 0.4482 386.5648
Chemical molecule 0.3169 272.0111
Gene|Protein 0.2265 211.8034
Indicator 0.1805 170.0186
Gene 0.1718 156.9504
Metabolite 0.1667 155.8135
Reaction 0.1545 144.6946
Drug 0.1301 124.2604
Therapeutic process 0.1259 111.4571
Disease 0.1189 111.1882
Chemical adjective 0.0642 55.5556
Enzyme 0.0473 41.089
Diagnostic process 0.0388 32.1161
Bacteria 0.0297 26.0522
Natural phenomenon 0.0227 20.8004
Pathologic function 0 0
Symptom 0 0
General phenomenon 0 0
Organ 0 0
Human phenomenon 0 0
Table 6: Attribute selection output from two attribute
evaluators.
fier, the best performing algorithm. Of these, 45
(i.e. 70%) are cell biology papers which were in-
correctly classified as belonging to pharmacology,
whilst the remaining 19 belong to the pharmacol-
ogy class and are classified as cell biology.
Labelled as Cell_bio Pharma
Cell_bio 315 19
Pharma 45 341
Table 7: Confusion matrix for the Random Forest clas-
sifier.
As previously mentioned, the two features that
achieved the highest information gain are the ra-
tios for the Protein and Chemical molecule types.
Accordingly, only these two features were consid-
ered in this error analysis.
We firstly examined the features of the cell
biology documents which were incorrectly clas-
sified as pharmacology papers. It was notice-
able that the majority of the misclassified doc-
uments in this case have a small percentage of
Proteins (less than 0.35) and/or a large percent-
age of Chemical molecules (greater than 0.58). To
confirm this observation, a sample of documents
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45 InfoGainChiSquare
Prote
in
Chem
ical m
olecu
le
Gene
|Prote
in
Indica
tor GeneMetab
oliteReac
tion Drug
Thera
peutic
 proce
ss
Disea
se
Chem
ical a
djecti
ve
Enzym
e
Diagn
ostic 
proce
ss
Bacte
ria
Natur
al phe
nome
non
Patho
logic 
functi
on
Symp
tom
Gene
ral ph
enom
enonOrgan
Huma
n phe
nome
non
Figure 1: Normalised attribute selection output from
two attribute evaluators.
was accessed via the PubMed Central page which
provides links to identified entities such as com-
pounds, substances, genes and proteins. For in-
stance, the misclassified cell biology paper with
PMCID 2755470 was found to have no proteins,
whilst the one with PMCID 2679709 has quite a
large number of substances (chemical molecules).
We also analysed the features of papers in the
pharmacology subdomain which were misclassi-
fied as cell biology documents. In contrast to
the first type of misclassification, these documents
have a large percentage of Proteins and/or small
percentage of Chemical molecules. For example,
the pharmacology paper with PMCID 2817930
contains many protein instances, whilst the one
with PMCID 2680808 has no mentions of chemi-
cal molecules.
5 Conclusions and Future Work
We have shown that with the help of named en-
tity identification, classifiers can be built that are
able to distinguish between papers belonging to
different biomedical subdomains. The Random
Forest algorithm is able to discriminate between
cell biology and pharmacology open-access full-
text articles with an F-score of 91%. This result
supports the hypothesis that sublanguages used in
different biomedical domains exhibit significant
semantic variations. Such variations should there-
fore be considered when adapting automated tools
43
developed for a particular subdomain to new sub-
domains.
One possible future direction is to analyse mul-
tiple medical subdomains, such as neurology, vi-
rology and critical care. This could enable the
measurement of the distance between various sub-
domains with respect to specific named entity
types. Furthermore, a comparison of the method
described above with those using bag-of-words
or other non-semantic features could further en-
force the importance of named entities in doc-
ument classification and sublanguage identifica-
tion.
Acknowledgements
We would like to acknowledge the help given
by Dr. C.J. Rupp in obtaining the collection of
documents from the Open Access section of the
UKMPC.
References
Cecilia Arighi, Zhiyong Lu, Martin Krallinger, Kevin
Cohen, W Wilbur, Alfonso Valencia, Lynette
Hirschman, and Cathy Wu. 2011. Overview of
the BioCreative III Workshop. BMC Bioinformat-
ics, 12(Suppl 8):S1.
Kevin Bretonnel Cohen, Martha Palmer, and Lawrence
Hunter. 2008. Nominalization and alternations in
biomedical language. PLoS ONE, 3(9):e3158, 09.
Kevin Bretonnel Cohen, Dina Demner-Fushman,
Sophia Ananiadou, John Pestian, Jun?ichi Tsujii,
and Bonnie Webber, editors. 2009. Proceedings of
the BioNLP 2009 Workshop. Association for Com-
putational Linguistics, Boulder, Colorado, June.
Carol Friedman, Pauline Kra, and Andrey Rzhetsky.
2002. Two biomedical sublanguages: a description
based on the theories of Zellig Harris. Journal of
Biomedical Informatics, 35(4):222?235.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explorations, 11(1).
Zellig Harris. 1968. Mathematical Structures of Lan-
guage. John Wiley and Son, New York.
David Jessop, Sam Adams, Egon Willighagen, Lezan
Hawizy, and Peter Murray-Rust. 2011. Oscar4:
a flexible architecture for chemical text-mining.
Journal of Cheminformatics, 3(1):41.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008. Corpus annotation for mining biomedi-
cal events from literature. BMC Bioinformatics,
9(1):10.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011. Overview of genia event
task in bionlp shared task 2011. In Proceedings
of BioNLP Shared Task 2011 Workshop, pages 7?
15, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Martin Krallinger, Alexander Morgan, Larry Smith,
Florian Leitner, Lorraine Tanabe, John Wilbur,
Lynette Hirschman, and Alfonso Valencia. 2008.
Evaluation of text-mining systems for biology:
overview of the second biocreative community
challenge. Genome Biology, 9(Suppl 2):S1.
Seth Kulick, Ann Bies, Mark Liberman, Mark Mandel,
Ryan McDonald, Martha Palmer, Andrew Schein,
and Lyle Ungar. 2004. Integrated annotation for
biomedical information extraction. In Proceedings
of the BioLINK 2004.
Thomas Lippincott, Diarmuid Seaghdha, and Anna
Korhonen. 2011. Exploring subdomain varia-
tion in biomedical language. BMC Bioinformatics,
12(1):212.
Johanna R. McEntyre, Sophia Ananiadou, Stephen
Andrews, William J. Black, Richard Boulderstone,
Paula Buttery, David Chaplin, Sandeepreddy Che-
vuru, Norman Cobley, Lee-Ann Coleman, Paul
Davey, Bharti Gupta, Lesley Haji-Gholam, Craig
Hawkins, Alan Horne, Simon J. Hubbard, Jee-
Hyub Kim, Ian Lewin, Vic Lyte, Ross MacIn-
tyre, Sami Mansoor, Linda Mason, John Mc-
Naught, Elizabeth Newbold, Chikashi Nobata,
Ernest Ong, Sharmila Pillai, Dietrich Rebholz-
Schuhmann, Heather Rosie, Rob Rowbotham, C. J.
Rupp, Peter Stoehr, and Philip Vaughan. 2010.
UKPMC: a full text article resource for the life sci-
ences. Nucleic Acids Research.
Ngan L. T. Nguyen and Jin-Dong Kim. 2008. Explor-
ing domain differences for the design of pronoun
resolution systems for biomedical text. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics - Volume 1, COLING ?08,
pages 625?632, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Chikashi Nobata, Yutaka Sasaki, Noaki Okazaki,
C. J. Rupp, Jun?ichi Tsujii, and Sophia Ananiadou.
2009. Semantic search on digital document reposi-
tories based on text mining results. In International
Conferences on Digital Libraries and the Semantic
Web 2009 (ICSD2009), pages 34?48.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?105.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjorne, Jorma Boberg, Jouni Jarvinen, and Tapio
Salakoski. 2007. BioInfer: a corpus for infor-
mation extraction in the biomedical domain. BMC
Bioinformatics, 8(1):50.
44
Naomi Sager, Carol Friedman, and Margaret Lyman.
1987. Medical Language Processing: Computer
Management of Narrative Data. Addison-Wesley,
Reading, MA.
Yutaka Sasaki, Yoshimasa Tsuruoka, John McNaught,
and Sophia Ananiadou. 2008. How to make the
most of NE dictionaries in statistical NER. BMC
Bioinformatics, 9(Suppl 11):S5.
Lorraine Tanabe, Natalie Xie, Lynne Thom, Wayne
Matten, and W John Wilbur. 2005. GENETAG: a
tagged corpus for gene/protein named entity recog-
nition. BMC Bioinformatics, 6(Suppl 1):S3.
Paul Thompson, Syed Iqbal, John McNaught, and
Sophia Ananiadou. 2009. Construction of an an-
notated corpus to support biomedical information
extraction. BMC Bioinformatics, 10(1):349.
Paul Thompson, Raheel Nawaz, John McNaught, and
Sophia Ananiadou. 2011. Enriching a biomedi-
cal event corpus with meta-knowledge annotation.
BMC Bioinformatics, 12(1):393.
Karin Verspoor, Kevin Bretonnel Cohen, and
Lawrence Hunter. 2009. The textual characteristics
of traditional and open access scientific journals are
similar. BMC Bioinformatics, 10(1):183.
Tuangthong Wattarujeekrit, Parantu Shah, and Nigel
Collier. 2004. PASBio: predicate-argument struc-
tures for event extraction in molecular biology.
BMC Bioinformatics, 5(1):155.
Ian Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques (Sec-
ond Edition). Morgan Kaufmann.
45
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 43?48,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Extending an interoperable platform to facilitate the creation
of multilingual and multimodal NLP applications
Georgios Kontonatsios?, Paul Thompson?, Riza Theresa Batista-Navarro?,
Claudiu Miha?ila??, Ioannis Korkontzelos and Sophia Ananiadou
The National Centre for Text Mining,
School of Computer Science, The University of Manchester
131 Princess Street, Manchester M1 7DN, UK
{kontonag,batistar,thompsop,mihailac,
korkonti,ananiads}@cs.man.ac.uk
Abstract
U-Compare is a UIMA-based workflow
construction platform for building natu-
ral language processing (NLP) applica-
tions from heterogeneous language re-
sources (LRs), without the need for pro-
gramming skills. U-Compare has been
adopted within the context of the META-
NET Network of Excellence, and over
40 LRs that process 15 European lan-
guages have been added to the U-Compare
component library. In line with META-
NET?s aims of increasing communication
between citizens of different European
countries, U-Compare has been extended
to facilitate the development of a wider
range of applications, including both mul-
tilingual and multimodal workflows. The
enhancements exploit the UIMA Subject
of Analysis (Sofa) mechanism, that allows
different facets of the input data to be rep-
resented. We demonstrate how our cus-
tomised extensions to U-Compare allow
the construction and testing of NLP appli-
cations that transform the input data in dif-
ferent ways, e.g., machine translation, au-
tomatic summarisation and text-to-speech.
1 Introduction
Currently, there are many repositories that con-
tain a range of NLP components, e.g., OpenNLP1,
Stanford CoreNLP2, JULIE NLP Toolsuite3 and
NaCTeM software tools4. The ability to chain
components from these repositories into pipelines
is a prerequisite to facilitate the development of
?The authors have contributed equally to the development
of this work and production of the manuscript.
1http://opennlp.sourceforge.net/projects.html
2http://nlp.stanford.edu/software/corenlp.shtml
3http://www.julielab.de/Resources/Software/NLP Tools.html
4http://nactem.ac.uk/software.php
complex NLP applications. Combining together
heterogeneous components is not, however, al-
ways straightforward. The various components
used in a pipeline may be implemented using dif-
ferent programming languages, may have incom-
patible input/output formats, e.g., stand-off or in-
line annotations, or may require or produce incom-
patible data types, e.g., a particular named entity
recogniser (NER) may require specific types of
syntactic constituents as input, making it impor-
tant to choose the right type of syntactic parser to
run prior to the NER. Thus, the tools required to
build a new application may not be interoperable
with each other, and considerable extra work may
be required to make the tools talk to each other.
The Unstructured Information Management Ar-
chitecture (UIMA) (Ferrucci and Lally, 2004) was
created as a means to alleviate such problems. It
is a framework that facilitates the straightforward
combination of LRs, i.e., tools and corpora, into
workflow applications. UIMA is an OASIS stan-
dard that enables interoperability of LRs by defin-
ing a standard workflow metadata format and stan-
dard input/output representations.
U-Compare (Kano et al, 2011) is a graphical
NLP workflow construction platform built on top
of UIMA. It facilitates the rapid construction, test-
ing and evaluation of NLP workflows using drag-
and-drop actions within its graphical user inter-
face (GUI). U-Compare enhances interoperabil-
ity among UIMA-compliant LRs, by defining a
common and sharable Type System, i.e., a hier-
archy of annotation types, which models a wide
range of NLP data types, e.g., sentence, token,
part-of-speech tag, named entity and discourse
annotations. The aim is for all components in
U-Compare?s library to be compliant with this
type system. In the context of META-NET, U-
Compare?s library has been extended with 46 new
LRs supporting 15 European languages, all of
which are compliant with the same type system.
43
This makes U-Compare the world?s largest repos-
itory of type system-compatible LRs, allowing
users to seamlessly combine together resources to
create a range of NLP applications.
Previously, U-Compare was able to support the
development of a wide range of monolingual lex-
ical, syntactic and semantic processing tasks ap-
plications that enriched textual input documents
by adding annotations of various types. However,
not all NLP applications operate in this way; some
workflows transform the input data to create new
?views? of the input data. The META-NET project
aims to ensure equal access to information by all
European citizens. This aim implies the devel-
opment of both multilingual applications, which
transform input data from one language into an-
other, or multimodal applications, in which text
may be transformed into speech, or vice versa.
U-Compare has been extended in several ways
to support the construction of these more complex
workflow types. Specifically, information about
both the original and transformed data, together
with annotations associated with each view, can
now be visualised in a straightforward manner.
The changes support two new categories of work-
flow. Firstly, workflows that produce two or more
textual views of an input text are useful not only
for multilingual applications, such as those that
carry out machine translation, but also applica-
tions that transform the input text in other ways,
such as those that produce a summary of an in-
put text. Secondly, workflows that output audio as
well as textual views, e.g., text-to-speech applica-
tions, are also supported.
2 Related work
Over the past few years, an increasing num-
bers of researchers have begun to create and dis-
tribute their own workflow construction architec-
tures (Ferrucci and Lally, 2004; Cunningham et
al., 2002; Grishman et al, 1997; Scha?fer, 2006)
or platforms (Kano et al, 2011; Rak et al, 2012;
Ogrodniczuk and Karagiozov, 2011; Savova et al,
2010) that allow the rapid development of NLP ap-
plications.
GATE (Cunningham et al, 2002) is a workflow
construction framework that has been used to de-
velop several types of NLP applications, including
summarisation systems. It facilitates the develop-
ment of a wide range of NLP applications by pro-
viding a collection of components that can process
various languages, together with Java libraries that
handle character encoding for approximately 100
languages. However, GATE does not formally de-
fine any standards to model multilingual or mul-
timodal applications, but rather aims to boost the
development process of NLP applications.
TIPSTER (Grishman et al, 1997) is a generic
framework for the development of NLP applica-
tions. TIPSTER provides multilingual function-
alities by associating text segments of a paral-
lel document with one or more languages. This
allows language-dependent NLP components to
process only the appropriate mono-lingual sub-
documents. However, TIPSTER does not provide
explicit guidelines regarding the annotation types
and attributes that are produced by components.
This lack of a common and sharable system of
annotation types discourages interoperability be-
tween LRs. However, TIPSTER does not provide
a mechanism that facilitates the development of
multilingual or multimodal NLP applications.
Heart of Gold (Scha?fer, 2006) is an XML-
based workflow construction architecture that en-
ables interoperability of tools developed in dif-
ferent programming languages to be combined
into pipelines. Heart of Gold contains a rich li-
brary of shallow and deep parsing components
supporting several languages, e.g., English, Ger-
man, Japanese and Greek. Nonetheless, Heart of
Gold does not specifically support the construction
of multilingual or multimodal workflows.
In contrast to the other frameworks introduced
above, UIMA (Ferrucci and Lally, 2004) provides
an abstract-level mechanism that can be used to
support the development of workflows that carry
out transformations of the input data. This mech-
anism is called the Subject of Analysis or Sofa.
Multiple Sofas can be linked with an input file,
each of which stores different data and associ-
ated annotations. This mechanism can thus be ex-
ploited to represent alternative ?views? of the in-
put data, such as a source text and its translation.
The data stored in different Sofas is not restricted
to textual information; it can also correspond to
other modalities, such as audio data. This makes
the Sofa mechanism equally suitable for storing
the output of text-to-speech workflows. Our ex-
tensions to U-Compare are thus implemented by
reading and displaying the contents of different
types of Sofas.
The Sofa mechanism has previously been
44
under-exploited by UIMA developers, despite its
power in allowing more complex NLP workflows
to be constructed. Indeed, no other existing
UIMA-based platform (Kano et al, 2011; Rak et
al., 2012; Savova et al, 2010; Hahn et al, 2008)
has demonstrated the use of Sofas to construct
multilingual or multimodal applications. Thus, to
our knowledge, our enhancements to U-Compare
constitute the first attempt to make the construc-
tion of workflows that carry out transformations of
input data more readily available to UIMA users,
without the need for programming skills.
3 METANET4U Components in
U-Compare
The two dozen national and many regional lan-
guages of Europe present linguistic barriers that
can severely limit the free flow of goods, infor-
mation and services. The META-NET Network
of Excellence was created to respond to this is-
sue. Consisting of 60 research centres from 34
countries, META-NET has aimed to stimulate a
concerted, substantial and continent-wide effort to
push forward language technology research and
engineering, in order to ensure equal access to
information and knowledge for all European cit-
izens.
META-NET?s aims are dependent on the ready
availability of LRs that can carry out NLP and
text mining (TM) on a range of European lan-
guages. Such resources constitute the building
blocks for constructing language technology ap-
plications that can help European citizens to gain
easy access to the information they require. One
of the major outcomes of META-NET has been
the development of META-SHARE, an open, dis-
tributed facility for sharing and exchange of LRs
in a large number of European languages.
Within the context of META-NET, interoper-
ability of LRs is clearly of utmost importance, to
expedite the process of developing new NLP ap-
plications. In order to provide a concrete demon-
stration of the utility and power of promoting in-
teroperability within META-SHARE, one of the
sub-projects of META-NET, i.e., METANET4U,
has carried out a pilot study on interoperability,
making use of the UIMA framework and the U-
Compare platform. It is in this context that a set
of 46 new LRs, available in META-SHARE, were
wrapped as UIMA components and made avail-
able in U-Compare. Of these components, 37 op-
erate on one or more specific languages other than
English and 4 are language-independent. Table 1
shows the full set of categories of UIMA com-
ponents created during the METANET4U project,
together with the languages supported.
Several of these new components output mul-
tiple Sofas, i.e., two machine translation compo-
nents, two automatic summarisation components
and a text-to-speech component. It is hoped that
our U-Compare extensions will help to stimulate
the development of a greater number of related
UIMA components, and thus promote a new level
of complexity for future UIMA workflows.
Component Function Supported Languages
Language Identifier 54 modern languages
Paragraph breaker pt, mt
Sentence splitter en, pt ,mt, es, ca, ast,cy, gl, it
Tokeniser en, pt, mt, es, ca, ast,cy, gl, it, fr
Morph. Analyser en, pt, es, ca, ast,cy, gl, it, ro, eu, fr
POS Tagger en, es, ca, cy, gl, it,pt, ro, eu, fr, mt
Syntactic chunker en, es, ca, gl,ast, ro, fr
NP chunker ro
Segmenter ro, en
FDG Parser ro
Dependency Parser en, es, ca, gl, ast
Discourse Parser ro
NER Languageindependent
Summariser ro, en
Machine translation es?{gl,pt,ca}en?es, eu?es
Table 1: METANET4U UIMA components
4 Enhancements to U-Compare
In UIMA, an artefact, i.e., raw text, audio, im-
age, video, and its annotations, e.g., part-of-
speech tags, are represented in a standard format,
namely the Common Analysis Structure (CAS).
A CAS can contain any number of smaller sub-
CASes, i.e., Sofas, that carry different artefacts
with their linked annotations. Figure 1 illustrates
the different types of Sofas that are created by the
three types of workflows that we will demonstrate.
Firstly, for a machine translation workflow, at least
45
Multi-lingualMulti-modalWorkflowsDocuments aZ ??
CAS
?CASCAS
SOFA SOFA
SOFA SOFA
SOFA SOFA
Figure 1: UIMA based multilingual and multi-
modal workflow architecture
two CAS views, i.e., Sofas, are created, the first
corresponding to the text in the source language,
and the other Sofas corresponding to the transla-
tion(s) of the source text into target language(s).
The second type of workflow, i.e., automatic sum-
marisation, is related to the former workflow, in
that the two Sofas produced by the workflow are
both textual, one containing the input text and one
containing a summary of the original text. The
third type of workflow is different, in that a Sofa
containing audio data is used to represent the out-
put of a multimodal workflow.
Two specific extensions have been made to U-
Compare to handle both textual and audio So-
fas. When the output of a workflow consists of
multiple textual views (Sofas), the default anno-
tation viewer is automatically split to allow mul-
tiple views of the text to be displayed and side-
by-side. This can be useful, e.g., to allow careful
comparison of a source text and target translation
in a machine translation workflow. To handle au-
dio Sofas, we have developed a new, customised
viewer that can visualise and play audio data. The
visualisation consists of a graphical display of the
waveform, power information and spectrogram, as
well as segmentation of the audio data into re-
gions (such as individual tokens) and transcrip-
tions, if such information is present in the audio
Sofa. The viewer makes use the open-source li-
brary Java Speech Toolkit (JSTK)5.
5 Workflow applications
In order to provide a practical demonstration of
the enhanced capabilities of U-Compare, we show
5http://code.google.com/p/jstk
three different workflows that transform the input
data in different ways, namely translation, auto-
matic summarisation and speech synthesis. In this
section, we provide brief details of these work-
flows.
5.1 Machine translation
The University of Manchester has created UIMA
wrapper components corresponding to different
modules of Apertium (Corb??-Bellot et al, 2005), a
free rule-based machine translation engine. These
components consist of a morphological analyser,
POS tagger and translator. The three components
must be run in sequence to carry out translation,
although the first two components can be used
in other workflows to carry out monolingual
analyses. The UIMA components currently
handle a subset of the 27 languages dealt with
by the complete Apertium system, corresponding
to the languages of the METANET4U partners,
i.e., English?Spanish, Galician?Spanish,
Portuguese?Spanish, Catalan?Spanish and
Basque?Spanish. However, additional language
pairs can be added straightforwardly. Our sample
workflow includes as its initial component the
Language Identifier from the Romanian Academy
Research Institute for Artificial Intelligence
(RACAI), to automatically detect the language of
the text in the input Sofa. The subsequent compo-
nents in the workflow are the Apertium modules.
The workflow demonstrates how heterogeneous
components from different research groups can
be combined into workflows to create new NLP
applications. A sample output from running the
workflow is shown in Figure 2. The input text
was detected as English by the RACAI Language
Identifier. The English text was subsequently
analysed by the morphological analyser and POS
Tagger, and translated to Spanish by the translator.
Figure 2 illustrates the side-by-side display of the
contents of the two Sofas.
5.2 Automatic summarisation
Automatic summarisation for Romanian text can
be carried out by creating a workflow consisting
of two components developed by the Universitatea
?Alexandru Ioan Cuza? din Ias?i (UAIC). Firstly,
a segmenter (UAICSeg) splits the input text into
fragments, which are in turn used as input to the
summariser component (UAICSum). The length
of the output summary (percentage of the whole
document) is parameterised. As can be seen in
46
Figure 2: Translation of English text to Spanish
Figure 3: Summarisation of Romanian text
Figure 3, the output of this workflow is displayed
using the same parallel Sofa viewer. In this case,
the full text is displayed in the left-hand pane and
the summary is shown in the right-hand pane.
5.3 Speech synthesis
The Universitat Polite`cnica de Catalunya (UPC)
developed a speech synthesiser component that
is based around their Ogmios text-to-speech sys-
tem (Bonafonte et al, 2006). The UIMA com-
ponent version of this tool generates separate text
and audio Sofas; the former stores the textual to-
kens and textual representations of their pronun-
ciations, whilst the latter stores the start and end
time offsets of each of the tokens in the audio file,
together with their transcriptions. Fig. 4 shows
how the textual Sofa information is displayed in
U-Compare?s default annotation viewer, whilst the
audio Sofa information is shown in the new au-
dio visualiser mentioned above. The three differ-
ent types of visual information are displayed be-
low each other, and the segments (tokens) of the
audio file, together with their transcriptions, are
displayed at the bottom of the window. A ?Play?
button allows either the complete file or a selected
segment to be played.
6 Conclusions
The requirements of META-NET have motivated
several new enhancements to the U-Compare plat-
form, which, to our knowledge, make it the first
UIMA-based workflow construction platform that
is fully geared towards the development of NLP
applications that support a wide range of European
languages. The 46 new UIMA-wrapped LRs that
have been made available through U-Compare,
supporting 15 different European languages and
all compliant with the same type system, mean
that the improved U-Compare is essentially a hub
of multilingual resources, which can be freely and
flexibly combined to create new workflows. In
47
Figure 4: Speech Synthesis
addition, our enhancements to U-Compare mean
that various types of multilingual and multimodal
workflows can now be created with the minimum
effort. These enhancements are intended to make
U-Compare more attractive to users, and to help
stimulate the development of a new generation of
more complex UIMA-based NLP applications. As
future work, we intend to extend the library of
components that output multiple Sofas, and further
extend the functionalities of U-Compare to handle
other data modalities, e.g., video.
Acknowledgements
This work was partially funded by the Euro-
pean Community?s Seventh Framework Program
(FP7/2007-2013) [grant number 318736 (OSS-
METER)]; MetaNet4U project (ICT PSP Pro-
gramme) [grant number 270893]; and Engineer-
ing and Physical Sciences Research Council [grant
numbers EP/P505631/1, EP/J50032X/1].
References
A. Bonafonte, P. Agu?ero, J. Adell, J. Pe?rez, and
A. Moreno. 2006. Ogmios: The upc text-to-speech
synthesis system for spoken translation. In TC-
STAR Workshop on Speech-to-Speech Translation,
pages 199?204.
A. Corb??-Bellot, M. Forcada, S. Ortiz-Rojas, J. Pe?rez-
Ortiz, G. Ram??rez-Sa?nchez, F. Sa?nchez-Mart??nez,
I. Alegria, A. Mayor, and K. Sarasola. 2005.
An open-source shallow-transfer machine transla-
tion engine for the romance languages of Spain. In
Proceedings of the 10th Conference of the EAMT,
pages 79?86.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: an architecture for devel-
opment of robust HLT applications.
D. Ferrucci and A. Lally. 2004. Building an ex-
ample application with the unstructured information
management architecture. IBM Systems Journal,
43(3):455?475.
R. Grishman, B. Caid, J. Callan, J. Conley, H. Corbin,
J. Cowie, K. DiBella, P. Jacobs, M. Mettler, B. Og-
den, et al 1997. TIPSTER text phase ii architecture
design version 2.1 p 19 june 1996.
U. Hahn, E. Buyko, R. Landefeld, M. Mu?hlhausen,
M. Poprat, K. Tomanek, and J. Wermter. 2008. An
overview of JCoRe, the JULIE lab UIMA compo-
nent repository. In LREC?08 Workshop ?Towards
Enhanced Interoperability for Large HLT Systems:
UIMA for NLP?, pages 1?7, Marrakech, Morocco,
May.
Y. Kano, M. Miwa, K. Cohen, L. Hunter, S. Ananiadou,
and J. Tsujii. 2011. U-compare: A modular nlp
workflow construction and evaluation system. IBM
Journal of Research and Development, 55(3):11.
M. Ogrodniczuk and D. Karagiozov. 2011. Atlas - the
multilingual language processing platform. Proce-
samiento de Lenguaje Natural, 47(0):241?248.
R. Rak, A. Rowley, W. Black, and S. Ananiadou.
2012. Argo: an integrative, interactive, text mining-
based workbench supporting curation. Database:
The Journal of Biological Databases and Curation,
2012.
G. Savova, J. Masanz, P. Ogren, J. Zheng, S. Sohn,
K. Kipper-Schuler, and C. Chute. 2010. Mayo clin-
ical text analysis and knowledge extraction system
(ctakes): architecture, component evaluation and ap-
plications. Journal of the American Medical Infor-
matics Association, 17(5):507?513.
U. Scha?fer. 2006. Middleware for creating and com-
bining multi-dimensional nlp markup. In Proceed-
ings of the 5th Workshop on NLP and XML: Multi-
Dimensional Markup in Natural Language Process-
ing, pages 81?84. ACL.
48
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 83?91,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Building a Coreference-Annotated Corpus
from the Domain of Biochemistry
Riza Theresa Batista-Navarro1,2,3,? and Sophia Ananiadou1,2,??
1National Centre for Text Mining, University of Manchester, United Kingdom
2School of Computer Science, University of Manchester, United Kingdom
3Department of Computer Science, University of the Philippines Diliman, Philippines
?batistar@cs.man.ac.uk, ??sophia.ananiadou@manchester.ac.uk
Abstract
One of the reasons for which the resolution
of coreferences has remained a challenging
information extraction task, especially in the
biomedical domain, is the lack of training
data in the form of annotated corpora. In or-
der to address this issue, we developed the
HANAPIN corpus. It consists of full-text ar-
ticles from biochemistry literature, covering
entities of several semantic types: chemical
compounds, drug targets (e.g., proteins, en-
zymes, cell lines, pathogens), diseases, or-
ganisms and drug effects. All of the co-
referring expressions pertaining to these se-
mantic types were annotated based on the an-
notation scheme that we developed. We ob-
served four general types of coreferences in
the corpus: sortal, pronominal, abbreviation
and numerical. Using the MASI distance
metric, we obtained 84% in computing the
inter-annotator agreement in terms of Krip-
pendorff?s alpha. Consisting of 20 full-text,
open-access articles, the corpus will enable
other researchers to use it as a resource for
their own coreference resolution methodolo-
gies.
1 Introduction
Coreferences are linguistic expressions referring to
the same real-world entity (Jurafsky and Martin,
2009). The process of grouping all co-referring ex-
pressions in text into respective coreference chains is
known as coreference resolution. It was introduced
as one of the tasks of the sixth Message Understand-
ing Conference (MUC-6) in 1995 (Grishman and
Sundheim, 1995) and is one of the information ex-
traction tasks which have remained a challenge to
this day. One of the reasons it is still considered
an unresolved problem especially in the biomedical
domain is the lack of coreference-annotated corpora
which are needed for developing coreference resolu-
tion systems.
There exist only a handful of biomedical corpora
which are annotated with coreference information.
We have conducted a review of each of them, tak-
ing into consideration their sizes, document compo-
sition, domain, types of markable entities, types of
coreference annotated, availability, and reliability in
terms of inter-annotator agreement. Of these, only
two corpora have been used in coreference resolu-
tion systems developed outside the research group
that annotated them: MEDSTRACT (Castano et al,
2002), and the MEDCo1 corpus of abstracts which
was used by the different teams who participated
in the Coreference Supporting Task of the BioNLP
2011 Shared Task2. These two corpora are widely
used, despite the fact that they are composed only of
abstracts.
Previous studies have shown the advantages of
utilising full-text articles rather than abstracts in
information extraction systems (Shah et al, 2003;
Schumie et al, 2004; Cohen et al, 2010a). Further-
more, recent research on fact extraction (McIntosh
and Curran, 2009) has demonstrated the need for
processing full-text articles when identifying coref-
erent expressions pertaining to biomedical entities.
1http://nlp.i2r.a-star.edu.sg/medco.html
2http://sites.google.com/site/bionlpst/home/protein-gene-
coreference-task
83
However, coreference-annotated corpora composed
of full-text articles are not readily accessible. Cur-
rently, only the FlySlip corpus (Gasperin et al,
2007) is available for download. In this corpus,
only gene-related entities were considered for coref-
erence annotation. Thus, there is a need for devel-
oping full-text corpora with coreference annotations
for more semantic types. This is currently being ad-
dressed by the CRAFT project (Cohen et al, 2010b)
which seeks to develop a corpus of full-text articles
with coreference annotations for more types of en-
tities; it was not explicitly stated, however, exactly
which types are being covered. Similarly, we are
developing a corpus of full-text articles with corefer-
ence annotations, but to further the aim of covering
as many semantic types as possible, we selected a
domain that covers a variety of semantic concepts.
Research literature from this biochemistry subdo-
main, marine natural products chemistry, contains
references pertaining to chemical compounds, or-
ganisms, drug targets such as proteins, enzymes, nu-
cleic acids, tissues, cells, cell components, cell lines
and pathogens, drug effects, as well as diseases. We
cover a number of entity types with the intention of
providing more insight into how to disambiguate co-
referring expressions of different semantic types.
An annotation scheme was developed, taking into
consideration the coreference types which have been
observed from the corpus, namely: sortal, pronom-
inal, numerical and abbreviation. Three chemistry
graduates were employed to annotate the corpus. To
determine the reliability of the resulting annotations,
we measured inter-annotator agreement in terms of
Krippendorff?s alpha.
2 Related Work
Coreference is often associated with the phe-
nomenon of anaphora which is characterised by
an expression (called an anaphor) that points back
to an entity previously mentioned in the same dis-
course (called antecedent). Anaphora resolution
is the process of determining the antecedent of an
anaphor. While the output of anaphora resolution
is a set of anaphor-antecedent pairs, that of corefer-
ence resolution is a set of coreference chains which
can be treated as equivalence classes. Despite this
difference, an overlap between them may be ob-
served in several cases. Often, a number of anaphor-
antecedent pairs from a discourse are coreferential
or refer to the same entity in the same domain,
and may be placed in the same coreference chain.
For this reason, we also included in our review
of biomedical corpora those which were annotated
with anaphora information and refer to them hence-
forth as coreference-annotated corpora.
We determined the types of coreference anno-
tated in each corpus we have reviewed, adapting
Mitkov?s classification of anaphora (Mitkov et al,
2000) which is also applicable to coreference. Nom-
inal coreference is characterised by co-referring ex-
pressions pertaining to a noun. It is further divided
into pronominal coreference and sortal coreference
which use a pronoun and a lexical noun phrase,
respectively, as co-referring expressions. Unlike
nominal coreference, verbal coreference is char-
acterised by co-referring expressions pertaining to
verbs. Both nominal and verbal coreference can
be broadly categorised according to the kind of
relation as direct or indirect. In direct corefer-
ence, co-referring expressions are related by iden-
tity, synonymy or specialisation; in indirect corefer-
ence, they are related by associative relations such as
meronymy or holonymy for nouns, and troponymy
or entailment for verbs. Annotation of indirect
coreference is usually more challenging as it re-
quires more specialised domain knowledge.
Presently, there are five (5) different biomedical
corpora which are annotated with coreference in-
formation: MEDSTRACT (Castano et al, 2002),
MEDCo3, FlySlip (Gasperin et al, 2007), the Col-
orado Richly Annotated Full Text (CRAFT) cor-
pus (Cohen et al, 2010b) and DrugNerAr (Segura-
Bedmar et al, 2009).
The MEDCo corpus has two subsets, one consist-
ing of abstracts (which we shall refer to as MEDCo-
A) and another consisting of full papers (MEDCo-
B). The results of our review of all five corpora
are presented in Table 1. Included in the last row
(HANAPIN) are the attributes of the corpus that we
have developed for comparison with existing cor-
pora.
Three of them, MEDSTRACT, MEDCo and
DrugNerAr, adapted an annotation scheme similar
3http://nlp.i2r.a-star.edu.sg/medco.html
84
Ta
bl
e
1:
C
om
pa
ri
so
n
of
B
io
m
ed
ic
al
C
or
po
ra
w
it
h
C
or
ef
er
en
ce
A
nn
ot
at
io
ns
C
or
pu
s
S
ch
em
e
D
oc
um
en
t
D
om
ai
n/
C
or
ef
er
en
ce
A
va
il
ab
il
it
y
Fo
rm
at
R
el
ia
bi
li
ty
A
da
pt
ed
C
om
po
si
ti
on
M
ar
ka
bl
es
Ty
pe
s
M
E
D
S
T
R
A
C
T
M
U
C
C
S
10
0
ab
st
ra
ct
s
m
ol
ec
ul
ar
bi
ol
og
y/
di
re
ct
no
m
in
al
pu
bl
ic
ly
X
M
L
un
kn
ow
n
U
M
L
S
ty
pe
s
av
ai
la
bl
e
M
E
D
C
o-
A
M
U
C
C
S
19
99
ab
st
ra
ct
s
hu
m
an
bl
oo
d
ce
ll
di
re
ct
no
m
in
al
pu
bl
ic
ly
X
M
L
K
ri
pp
en
do
rf
f?
s
al
ph
a:
tr
an
sc
ri
pt
io
n
fa
ct
or
s/
av
ai
la
bl
e
83
%
on
15
ab
st
ra
ct
s
G
E
N
IA
Te
rm
O
nt
ol
og
y
ty
pe
s
M
E
D
C
o-
B
M
U
C
C
S
43
fu
ll
pa
pe
rs
hu
m
an
bl
oo
d
ce
ll
di
re
ct
no
m
in
al
cu
rr
en
tl
y
X
M
L
K
ri
pp
en
do
rf
f?
s
al
ph
a:
tr
an
sc
ri
pt
io
n
fa
ct
or
s/
un
av
ai
la
bl
e
80
.7
%
on
2
fu
ll
pa
pe
rs
G
E
N
IA
Te
rm
O
nt
ol
og
y
ty
pe
s
F
ly
S
li
p
do
m
ai
n-
5
fu
ll
pa
pe
rs
fr
ui
tfl
y
ge
no
m
ic
s/
di
re
ct
an
d
pu
bl
ic
ly
X
M
L
K
ap
pa
sc
or
e:
sp
ec
ifi
c
ge
ne
ti
c
en
ti
ti
es
in
di
re
ct
av
ai
la
bl
e
gr
ea
te
r
th
an
83
%
so
rt
al
on
ea
ch
pa
pe
r
C
R
A
F
T
O
nt
oN
ot
es
97
fu
ll
pa
pe
rs
m
ou
se
ge
no
m
ic
s/
di
re
ct
no
m
in
al
cu
rr
en
tl
y
S
G
M
L
K
ri
pp
en
do
rf
f?
s
al
ph
a:
al
le
nc
ou
nt
er
ed
an
d
ve
rb
al
an
d
un
av
ai
la
bl
e
61
.9
%
on
10
fu
ll
pa
pe
rs
D
ru
gN
er
A
r
M
U
C
C
S
49
D
ru
gB
an
k
dr
ug
-d
ru
g
in
te
ra
ct
io
ns
/
di
re
ct
no
m
in
al
pu
bl
ic
ly
X
M
L
un
kn
ow
n
te
xt
s
dr
ug
s
av
ai
la
bl
e
H
A
N
A
P
IN
M
E
D
C
o
20
fu
ll
pa
pe
rs
m
ar
in
e
na
tu
ra
l
di
re
ct
no
m
in
al
,
cu
rr
en
tl
y
X
M
L
K
ri
pp
en
do
rf
f?
s
al
ph
a:
pr
od
uc
ts
ch
em
is
tr
y/
nu
m
er
ic
al
&
un
av
ai
la
bl
e
75
%
av
er
ag
ed
ch
em
ic
al
co
m
po
un
ds
,
ab
br
ev
ia
ti
on
(t
o
be
re
le
as
ed
ov
er
20
pa
pe
rs
;
or
ga
ni
sm
s,
dr
ug
pu
bl
ic
ly
)
84
%
us
in
g
th
e
M
A
S
I
ta
rg
et
s,
dr
ug
di
st
an
ce
m
et
ri
c
ef
fe
ct
s,
di
se
as
es
85
to that of the Message Understanding Conference
scheme or MUCCS (Hirschman, 1997). Using the
Standard Generalized Markup Language (SGML) as
annotation format, MUCCS creates a link between
co-referring expressions by setting the value of an
attribute of the referring element to the ID of the ref-
erent.
The same mechanism is used in the annotation
of MEDSTRACT, MEDCo and DrugNerAr, but
with respective extensions to account for more spe-
cific relations (e.g., appositive relation in the case
of MEDCo). On the contrary, rather than link-
ing the referring expression to its referent, an an-
notator explicitly places co-referring expressions in
the same coreference chain with OntoNotes, the
scheme adapted in annotating the CRAFT corpus.
FlySlip can be considered unique in terms of its
annotation scheme as it adapted a domain-specific
scheme which was necessary since indirect corefer-
ences were annotated. All corpora are available in
the form of a mark-up language (SGML or XML).
The five corpora can be grouped into three accord-
ing to general domain: molecular biology (MED-
STRACT and MEDCo), genomics (FlySlip and
CRAFT), and pharmacology (DrugNerAr). MED-
STRACT and MEDCo both have coreference an-
notations for semantic types from the UMLS and
the GENIA ontology, respectively, which can be
broadly categorised into compound, organism, pro-
tein, gene and cell. Each of the FlySlip and
DrugNerAr corpora, on the other hand, have anno-
tations for only one general semantic type: gene-
related entities and drugs, respectively. CRAFT is
unique in this respect as its developers seek to anno-
tate all co-referring expressions regardless of seman-
tic type; the semantic types that have been encoun-
tered so far have not yet been reported, however.
In terms of coreference types for which annota-
tions have been added, CRAFT is the only corpus
with annotations for verbal coreference; all the rest
have annotations only for pronominal and/or sortal
coreference. With respect to coreference types ac-
cording to relation, FlySlip is the only corpus with
annotations for indirect coreference.
MEDCo-B, FlySlip and CRAFT are three exist-
ing corpora which are comprised of full-text arti-
cles. Among them, only FlySlip is currently publicly
available.
The corpus that we have developed, which we call
the HANAPIN corpus, is also intended for public
release in the near future and covers five general
semantic types. In the annotation scheme which
was designed and used in HANAPIN, two addi-
tional coreference types were considered: abbrevi-
ations and numerical coreferences which are com-
monly used in chemistry research literature. These
coreference types and the annotation scheme are fur-
ther described in the succeeding section.
3 Methodology
3.1 Composition of Corpus Documents
Taking into consideration that the corpus should
consist of full-text articles which can be distributed
to the public, we gathered full-text articles from the
journal Marine Drugs4 which is under the PubMed
Central Open Access subset5. The said journal cov-
ers subject areas such as marine natural products,
medicine analysis, marine pharmacology, pharma-
ceutical biology, marine drugs development and ma-
rine biotechnology, among many others. From all
of its articles from 2003 to 2009, we randomly se-
lected twenty (20) which seemed to be a reason-
able size considering that only five months were al-
located for the annotation of the corpus, and that
a previous study on biomedical corpora (Cohen et
al., 2005) has shown that a corpus can possibly be
widely used despite its small size. The experimen-
tal sections of the articles were not annotated as
they contain very detailed descriptions of the meth-
ods carried out by the authors; according to a study
(Shah et al, 2003), these usually contain technical
data, instruments and measurements ? types of in-
formation which are currently not of much interest
to researchers doing biomedical information extrac-
tion, although they may be in the future. The corpus
contains a total of 1,027 sentences or 27, 358 words.
3.2 Coreference Types
The coreferences observed in the corpus were cat-
egorised into four general nominal types: pronom-
inal, sortal, numerical and abbreviation. Table 2
presents the subtypes of sortal and pronominal
coreference, as well as examples for all types. We
4http://www.mdpi.com/journal/marinedrugs
5http://www.ncbi.nlm.nih.gov/pmc/about/openftlist.html
86
Table 2: Coreference Types with Examples
General Coreference Type Subtype Examples
pronominal
demonstrative this, that, these, those
personal it, they, its, their, theirs
indefinite another, few, other, some, all, any
distributive both, such, each, either, neither
relative which, that, whose
sortal
definite the loihichelins
indefinite an alkaloid, a mycalamide
demonstrative this metabolite, these compounds
distributive both compounds
predicate nominative ?Galactans are polysaccharides...?
appositive ?Radiosumin, an N-methyl dipeptide...?
numerical
N.A. ?The structures of 1 and 2...?
?Compounds 1-3 inhibit...?
abbreviation
N.A. ?...as a membrane type 1 matrix
metalloproteinase (MT1-MMP) inhibitor.
Compound 1 inhibited MT1-MMP with...?
have decided not to take into account verbal and in-
direct coreferences; only nominal and direct coref-
erences have been considered for the first release of
the corpus.
3.2.1 Pronominal Coreference
This type of coreference is characterised by a pro-
noun referring to a noun phrase. The pronoun is used
as a substitute to a noun. We have further identified
the following subtypes of pronominal coreference:
demonstrative, personal, indefinite, distributive and
relative.
3.2.2 Sortal Coreference
Also referred to as lexical noun phrase corefer-
ence, sortal coreference is characterised by a noun
phrase consisting of a head noun and its modifiers.
The subtypes of sortal coreference which have been
identified include: definite, indefinite, demonstra-
tive, distributive, predicate nominative and apposi-
tive.
3.2.3 Numerical Coreference
In chemistry research literature, a number is con-
ventionally used to refer to a chemical entity which
was introduced using the same number. Oftentimes,
a range of numbers is also used to refer to a number
of compounds previously mentioned.
3.2.4 Abbreviation
In annotating the HANAPIN corpus, abbrevia-
tions were also considered as co-referring expres-
sions. We distinguish them from the other corefer-
ence types to make the corpus of benefit to develop-
ers of abbreviation identification algorithms as well.
3.3 Annotation Scheme and Procedure
The annotation scheme used in MEDCo (which was
based on MUCCS) was adapted and modified for
the annotation of the HANAPIN corpus. We have
selected the MEDCo scheme as it already differen-
tiates between the pronominal and identity (equiva-
lent to sortal) types, whereas MUCCS has only the
identity type. There was a need, however, to extend
the MEDCo scheme to further specialise the corefer-
ence types. The XML Concordancer (XConc) tool6
was used in annotating the corpus. Configuring the
said tool for our needs is straightforward as it only
involved the customisation of a Document Type Def-
inition (DTD) file.
3.3.1 Term Annotations
As a preliminary step, the scheme required that
all terms which can be categorised into any of the
6http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/wiki.cgi?
page=XConc+Suite
87
Figure 1: Sample annotations as shown in the XConc annotation tool. The sentences in this example come from one
of the documents in the HANAPIN corpus, the Marine Drugs article with PubMed ID 19841723. For illustrative
purposes, the first sentence in the example was slightly modified to demonstrate the use of the cons element.
following semantic types be annotated:
1. chemical compound
2. organism
3. drug effect
4. disease
5. drug target (further categorised into: protein,
enzyme, nucleic acid, tissue, cell, cell compo-
nent, cell line, pathogen)
For each markable, the annotator creates a term
element which is assigned an ID and one of the se-
mantic types above. The scheme supports the anno-
tation of embedded terms, as well as terms in a dis-
continuous text region. The former entails placing
a term element within another. The latter is done
by dividing the discontinuous text into fragments
and annotating each fragment in the same manner
as an ordinary term element. The fragment elements
are then grouped together as a constituent element
(cons). Figure 1 presents a sample annotation of
a discontinuous term (constituent C5) as viewed in
XConc.
3.3.2 Co-referring Expressions
An annotator proceeds to the annotation of co-
referring expressions after annotating all terms
within a document. If an expression was found to
be co-referring with another term, the annotator as-
signs the ID of the latter as the value of the idref
attribute of the former. If the referring expression,
however, is a noun phrase and not a term that was
previously annotated during term annotation, it is
marked as a ref element and then linked to its ref-
erent. Annotators delimit these expressions by in-
cluding the necessary modifiers of the co-referring
element (e.g., the new jaspamide derivatives instead
of just jaspamide derivatives). A coreference type
which could be any of pronominal, numerical, ab-
breviation, and sortal (further categorised into def-
inite, indefinite, demonstrative, distributive, predi-
cate nominative and appositive) is also assigned as
the value of the type attribute of each link created.
We decided not to further divide pronominal coref-
erence into its subtypes as it became apparent dur-
ing the annotation dry runs that there is only a hand-
ful of pronominal coreferences. Figure 1 shows co-
referring expressions (connected by arrows) linked
by the mechanism just described.
Listed below are some of the main points of the
annotation guidelines:
1. A referring expression may be linked to multi-
ple referents.
2. The more specific one between two co-
referring expressions is considered as the ref-
erent. This means that there might be cases
when the referent occurs later than the refer-
ring expression. For example, R30:the new
natural products is the co-referring ex-
pression and C5:jaspamide Q and R is
the referent in Figure 1.
3. In cases where there are multiple choices for
the referent of a referring expression, the clos-
est one may be chosen as long as it is (or will
be) linked to the other choice expressions.
4. There are cases when more than one type of
coreference applies. For example, in Figure 1,
the new natural products is both an appositive
and a definite noun phrase. In such cases, the
appositive and predicate nominative types take
precedence over the other sortal types.
88
Figure 2: XML code generated by XConc for the sample annotations in Figure 1.
One could process the XML code (provided in
Figure 2 for the reader?s reference) to obtain the fol-
lowing coreference chains:
1. {R30:the new natural products,
C5:jaspamide Q and R, R10:the
new jaspamide derivatives,
R11:which, R12:both}
2. {T66:jaspamide Q, R34:2}
3. {T67:jaspamide R, R35:3}
4. {T70:jaspamide, R36:1}
The complete annotation guidelines will be pub-
licly released together with the annotated corpus.
4 Results
The three annotators were asked to complete the
coreference annotations within five months. A bi-
weekly meeting was held to address questions and
issues which could not be addressed or resolved by
means of the online project forum.
4.1 Statistics
As the HANAPIN corpus is the first of its kind from
the biochemistry domain and aims to cover several
semantic as well as coreference types, it is of interest
to determine which of the types are most prevalent.
To do this we computed statistics over the annota-
tions (Figure 3). For each type, we obtained the av-
erage over the annotations from the three coders.
There is a total of 395 coreference chains (not
including singleton chains or those with only one
mention) in the entire corpus. The coreference
chains are of the following semantic types: chemical
compounds (70.89%), drug targets (12.66% that ac-
counts for proteins, cell lines, pathogens, enzymes,
cells, cell parts, nucleic acids and tissues), organ-
isms (9.87%), drug effects (3.29%), and diseases
(3.29%). Among the drug targets, the most preva-
lent are proteins, cell lines and pathogens.
A total of 760 coreference links have been found
in the corpus. The most common among the types
is the numerical one (46%), followed by the sortal
type (33% that accounts for the definite, indefinite,
demonstrative, appositive, predicate nominative and
distributive types). Less common are the pronomi-
nal type (11%) and abbreviation (10%). Among the
sortal coreferences, the most common are the def-
inite and indefinite types, followed by the demon-
strative type.
89
Sheet5
Page 1
280
drug target (50) 50
organism (39) 39
drug effect (13) 13
disease (13) 13
395
numerical (352) 352
pronominal (83) 83
abbreviation (74) 74
definite (64) 64
indefinite (58) 58
demonstrative (42) 42
appositive (31) 31
pred. nom. (28) 28
distributive (28) 28
760
chem (280)
Semantic Types
chem (280)
drug target 
(50)
organism (39)
drug effect 
(13)
disease (13)
Coreference Types
numerical 
(352)
pronominal 
(83)
abbreviation 
(74)
definite (64)
indefinite (58)
demonstrative 
(42)
appositive 
(31)
pred. nom. 
(28)
distributive 
(28)
Figure 3: Distribution of semantic and coreference types in the HANAPIN corpus.
4.2 Corpus Reliability
Following Passoneau?s proposed method for com-
puting reliability for coreference annotation (Pas-
soneau, 2004), we computed for the reliability of
the corpus in terms of Krippendorff?s alpha, a co-
efficient of agreement that allows for partial dis-
agreement with the use of a distance metric based
on the similarity between coreference chains. Pas-
soneau?s first proposed distance metric (dP ) assigns
0 for identity, 0.33 for subsumption, 0.67 for inter-
section and 1 for disjunction. There are, however,
alternative distance metrics that consider the sizes
of the coreference chains, such as Jaccard?s coeffi-
cient of community (dJ ) and Dice?s coincidence in-
dex (dD) which can be computed as follows (Art-
stein and Peosio, 2004):
dJ = 1?
|A ?B|
|A ?B|
dD = 1?
2|A ?B|
|A|+ |B|
A new distance metric called Measuring Agree-
ment on Set-valued Items (MASI) was then later
proposed by Passoneau. It is obtained by getting the
product of the original distance metric dP and Jac-
card?s coefficient dJ .
Initially using Passoneau?s first proposed distance
metric dP in computing for Krippendorff?s alpha,
we obtained an average of 75% over all documents
in the HANAPIN corpus. Computing for alpha us-
ing the MASI distance metric gives 84%. Though
there is no value of alpha that has been established
to be an absolute indication of high agreement, pre-
vious works cited by Krippendorff have shown that
values of alpha less than 67% indicate unreliability
(Krippendorff, 1980). We can therefore regard the
obtained values of alpha as satisfactory.
5 Conclusion and Future Work
A coreference-annotated corpus from the domain
of biochemistry, consisting of full-text articles, has
been developed. It was annotated following guide-
lines which covered coreference and semantic types
that have not been covered in other biomedical cor-
pora before. This was done to further the aim of pro-
viding researchers with more insight into the phe-
nomenon of coreference in a cross-disciplinary do-
main. Results show that in this biochemistry do-
main, the most common types of coreference being
used by authors are the numerical and sortal types.
Verbal and indirect coreferences, however, have not
been considered at this stage; the annotation of these
types can be explored as part of future work on the
corpus.
To measure reliability of the corpus, we deter-
mined inter-annotator agreement on all documents
by computing for the value of Krippendorff?s al-
pha. Using Passoneau?s first proposed distance met-
ric and the MASI distance metric, we obtained sat-
isfactory values of 75% and 84%, respectively. The
corpus and annotation guidelines will be released to
the public to encourage and enable more researchers
to develop improved biomedical coreference resolu-
90
tion methodologies.
Acknowledgements
The UK National Centre for Text Mining is funded
by the UK Joint Information Systems Committee
(JISC). The authors would also like to acknowledge
the Office of the Chancellor, in collaboration with
the Office of the Vice-Chancellor for Research and
Development, of the University of the Philippines
Diliman for funding support through the Outright
Research Grant.
The authors also thank Paul Thompson for his
feedback on the annotation guidelines, and the
anonymous reviewers for their helpful comments.
References
Ron Artstein and Massimo Poesio. 2004. Inter-Coder
Agreement for Computational Linguistics. Computa-
tional Linguistics, 34(4):555-596.
Jose? Castan?o, Jason Zhang and James Pustejovsky. 2002.
Anaphora resolution in biomedical literature. Pro-
ceedings of the International Symposium on Reference
Resolution for NLP.
K. Bretonnel Cohen, Philip V. Ogren, Lynne Fox and
Lawrence E. Hunter. 2005. Empirical data on corpus
design and usage in biomedical natural language pro-
cessing. AMIA Annual Symposium Proceedings, pages
156-160.
K. Bretonnel Cohen, Helen L. Johnson, Karin Verspoor,
Christophe Roeder, Lawrence E. Hunter. 2010. The
structural and content aspects of abstracts versus bod-
ies of full text journal articles are different. BMC
Bioinformatics, 11(1):492.
K. Bretonnel Cohen, Arrick Lanfranchi, William Cor-
vey, William A. Baumgartner Jr., Christophe Roeder,
Philip V. Ogren, Martha Palmer and Lawrence E.
Hunter. 2010. Annotation of all coreference in
biomedical text: Guideline selection and adaptation.
Proceedings of the Second Workshop on Building
and Evaluating Resources for Biomedical Text Mining
(BioTxtM 2010), LREC 2010.
Caroline Gasperin, Nikiforos Karamanis and Ruth Seal.
2007. Annotation of anaphoric relations in biomedical
full-text articles using a domain-relevant scheme. Pro-
ceedings of the 6th Discourse Anaphora and Anaphor
Resolution Colloquium (DAARC 2007).
Ralph Grishman and Beth Sundheim. 1995. Design of
the MUC-6 Evaluation. MUC ?95: Proceedings of the
6th Message Understanding Conference, pages 1-11.
Lynette Hirschman. 1997. MUC-7 Coreference Task
Definition. Message Understanding Conference 7
Proceedings.
Daniel Jurafsky and James H. Martin. 2009. Speech
and Language Processing: An Introduction to Natu-
ral Language Processing, Computational Linguistics,
and Speech Recognition. Prentice-Hall, 2nd edition.
Klaus H. Krippendorff. 1980. Content Analysis: An
Introduction to Its Methodology. Beverly Hills, CA:
Sage Publications.
Tara McIntosh and James R.Curran. 2009. Chal-
lenges for automatically extracting molecular inter-
actions from full-text articles. BMC Bioinformatics,
10(1):311.
Ruslan Mitkov, Richard Evans, Constantin Orasan,
Catalina Barbu, Lisa Jones and Violeta Sotirova. 2005.
Coreference and anaphora: developing annotating
tools, annotated resources and annotation strategies.
Proceedings of the Discourse Anaphora and Anaphora
Resolution Colloquium (DAARC 2000), pages 49-58.
Rebecca J. Passoneau. 2004. Computing reliability for
coreference annotation. Proceedings of the Interna-
tional Conference on Language Resouces (LREC).
M. Schumie, M. Weeber, B. Schijvenaars, E. van Mul-
ligen, C. van der Eijk, R. Jelier, B. Mons and J.
Kors. 2004. Distribution of information in biomedi-
cal abstracts and full-text publications. Bioinformat-
ics, 20(16):2597-2604.
Isabel Segura-Bedmar, Mario Crespo, Ce?sar de Pablo-
Sa?nchez and Paloma Mart??nez. 2009. Resolving
anaphoras for the extraction of drug- drug interactions
in pharmacological documents. BMC Bioinformatics,
11(Suppl 2):S1.
Parantu K. Shah, Carolina Perez-Iratxeta, Peer Bork and
Miguel A. Andrade. 2003. Information extraction
from full text scientific articles: Where are the key-
words? BMC Bioinformatics, 4(1): 20.
91
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 79?88,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Towards a Better Understanding of Discourse:
Integrating Multiple Discourse Annotation Perspectives Using UIMA
Claudiu Miha?ila??, Georgios Kontonatsios?, Riza Theresa Batista-Navarro?,
Paul Thompson?, Ioannis Korkontzelos and Sophia Ananiadou
The National Centre for Text Mining,
School of Computer Science, The University of Manchester
{mihailac,kontonag,batistar,thompsop,
korkonti,ananiads}@cs.man.ac.uk
Abstract
There exist various different discourse an-
notation schemes that vary both in the
perspectives of discourse structure consid-
ered and the granularity of textual units
that are annotated. Comparison and inte-
gration of multiple schemes have the po-
tential to provide enhanced information.
However, the differing formats of cor-
pora and tools that contain or produce
such schemes can be a barrier to their
integration. U-Compare is a graphical,
UIMA-based workflow construction plat-
form for combining interoperable natu-
ral language processing (NLP) resources,
without the need for programming skills.
In this paper, we present an extension
of U-Compare that allows the easy com-
parison, integration and visualisation of
resources that contain or output annota-
tions based on multiple discourse anno-
tation schemes. The extension works by
allowing the construction of parallel sub-
workflows for each scheme within a single
U-Compare workflow. The different types
of discourse annotations produced by each
sub-workflow can be either merged or vi-
sualised side-by-side for comparison. We
demonstrate this new functionality by us-
ing it to compare annotations belonging
to two different approaches to discourse
analysis, namely discourse relations and
functional discourse annotations. Integrat-
ing these different annotation types within
an interoperable environment allows us to
study the correlations between different
types of discourse and report on the new
insights that this allows us to discover.
?The authors have contributed equally to the development
of this work and production of the manuscript.
1 Introduction
Over the past few years, there has been an increas-
ing sophistication in the types of available natural
language processing (NLP) tools, with named en-
tity recognisers being complemented by relation
and event extraction systems. Such relations and
events are not intended to be understood in isola-
tion, but rather they are arranged to form a coher-
ent discourse. In order to carry out complex tasks
such as automatic summarisation to a high degree
of accuracy, it is important for systems to be able
to analyse the discourse structure of texts automat-
ically. To facilitate the development of such sys-
tems, various textual corpora containing discourse
annotations have been made available to the NLP
community. However, there is a large amount of
variability in the types of annotations contained
within these corpora, since different perspectives
on discourse have led to the development of a
number of different annotation schemes.
Corpora containing discourse-level annotations
usually treat the text as a sequence of coherent tex-
tual zones (e.g., clauses and sentences). One line
of research has been to identify which zones are
logically connected to each other, and to charac-
terise these links through the assignment of dis-
course relations. There are variations in the com-
plexity of the schemes used to annotate these dis-
course relations. For example, Rhetorical Struc-
ture Theory (RST) (Mann and Thompson, 1988)
defines 23 types of discourse relations that are
used to structure the text into complex discourse
trees. Whilst this scheme was used to enrich the
Penn TreeBank (Carlson et al, 2001), the Penn
Discourse TreeBank (PDTB) (Prasad et al, 2008)
used another scheme to identify discourse rela-
tions that hold between pairs of text spans. It cate-
gorises the relations into types such as ?causal?,
?temporal? and ?conditional?, which can be ei-
ther explicit or implicit, depending on whether or
79
not they are represented in text using overt dis-
course connectives. In the biomedical domain, the
Biomedical Discourse Relation Bank (BioDRB)
(Prasad et al, 2011) annotates a similar set of re-
lation types, whilst BioCause focusses exclusively
on causality (Miha?ila? et al, 2013).
A second line of research does not aim to link
textual zones, but rather to classify them accord-
ing to their specific function in the discourse. Ex-
amples of functional discourse annotations include
whether a particular zone asserts new information
into the discourse or represents a speculation or
hypothesis. In scientific texts, knowing the type
of information that a zone represents (e.g., back-
ground knowledge, hypothesis, experimental ob-
servation, conclusion, etc.) allows for automatic
isolation of new knowledge claims (Sa?ndor and de
Waard, 2012). Several annotation schemes have
been developed to classify textual zones accord-
ing to their rhetorical status or general informa-
tion content (Teufel et al, 1999; Mizuta et al,
2006; Wilbur et al, 2006; de Waard and Pan-
der Maat, 2009; Liakata et al, 2012a). Related
to these studies are efforts to capture information
relating to discourse function at the level of events,
i.e., structured representations of pieces of knowl-
edge which, when identified, facilitate sophisti-
cated semantic searching (Ananiadou et al, 2010).
Since there can be multiple events in a sentence
or clause, the identification of discourse informa-
tion at the event level can allow for a more de-
tailed analysis of discourse elements than is possi-
ble when considering larger units of text. Certain
event corpora such as ACE 2005 (Walker, 2006)
and GENIA-MK (Thompson et al, 2011) have
been annotated with various types of functional
discourse information.
It has previously been shown that considering
several functional discourse annotation schemes in
parallel can be beneficial (Liakata et al, 2012b),
since each scheme offers a different perspective.
For a common set of documents, the cited study
analysed and compared functional discourse an-
notations at different levels of textual granular-
ity (i.e., sentences, clauses and events), showing
how the different schemes could complement each
other in order to lay the foundations for a possible
future harmonisation of the schemes. The results
of this analysis provide evidence that it would be
useful to carry out further such analyses involv-
ing other such schemes, including an investiga-
tion of how discourse relations and functional dis-
course annotations could complement each other,
e.g., which types of functional annotations occur
within the arguments of discourse relations. There
are, however, certain barriers to carrying out such
an analysis. For example, a comparison of an-
notation schemes would ideally allow the differ-
ent types of annotations to be visualised simul-
taneously or seamlessly merged together. How-
ever, the fact that annotations in different corpora
are encoded using different formats (e.g., stand-off
or in-line) and different encoding schemes means
that this can be problematic.
A solution to the challenges introduced above is
offered by the Unstructured Information Manage-
ment Architecture (UIMA) (Ferrucci and Lally,
2004), which defines a common workflow meta-
data format facilitating the straightforward combi-
nation of NLP resources into a workflow. Based
on the interoperability of the UIMA framework,
numerous researchers distribute their own tools as
UIMA-compliant components (Kano et al, 2011;
Baumgartner et al, 2008; Hahn et al, 2008;
Savova et al, 2010; Gurevych et al, 2007; Rak
et al, 2012b). However, UIMA is only intended
to provide an abstract framework for the interop-
erability of language resources, leaving the actual
implementation to third-party developers. Hence,
UIMA does not explicitly address interoperability
issues of tools and corpora.
U-Compare (Kano et al, 2011) is a UIMA-
based workflow construction platform that pro-
vides a graphical user interface (GUI) via which
users can rapidly create NLP pipelines using a
drag-and-drop mechanism. Conforming to UIMA
standards, U-Compare components and pipelines
are compatible with any UIMA application via a
common and sharable type system (i.e., a hier-
archy of annotation types). In defining this type
system, U-Compare promotes interoperability of
tools and corpora, by exhaustively modelling a
wide range of NLP data types (e.g., sentences, to-
kens, part-of-speech tags, named entities). This
type system was recently extended to include dis-
course annotations to model three discourse phe-
nomena, namely causality, coreference and meta-
knowledge (Batista-Navarro et al, 2013).
In this paper, we describe our extensions to U-
Compare, supporting the integration and visuali-
sation of resources annotated according to mul-
tiple discourse annotation schemes. Our method
80
decomposes pipelines into parallel sub-workflows,
each linked to a different annotation scheme.
The resulting annotations produced by each sub-
workflow can be either merged within a single
document or visualised in parallel views.
2 Related work
Previous studies have shown the advantages of
comparing and integrating different annotation
schemes on a corpus of documents (Guo et al,
2010; Liakata et al, 2010; Liakata et al, 2012b).
Guo et al (2010) compared three different dis-
course annotation schemes applied to a corpus
of biomedical abstracts on cancer risk assess-
ment and concluded that two of the schemes pro-
vide more fine-grained information than the other
scheme. They also revealed a subsumption rela-
tion between two schemes. Such outcomes from
comparing schemes are meaningful for users who
wish to select the most appropriate scheme for an-
notating their data. Liakata et al (2012) under-
line that different discourse annotation schemes
capture different dimensions of discourse. Hence,
there might be complementary information across
different schemes. Based on this hypothesis, they
provide a comparison of three annotation schemes,
namely CoreSC (Liakata et al, 2012a), GENIA-
MK (Thompson et al, 2011) and DiscSeg (de
Waard, 2007), on a corpus of three full-text pa-
pers. Their results showed that the categories in
the three schemes can complement each other. For
example, the values of the Certainty Level dimen-
sion of the GENIA-MK scheme can be used to as-
sign confidence values to the Conclusion, Result,
Implication and Hypothesis categories of CoreSC
and DiscSeg. In contrast to previous studies, our
proposed approach automatically integrates mul-
tiple annotation schemes. The proposed mecha-
nism allows users to easily compare, integrate and
visualise multiple discourse annotation schemes
in an interoperable NLP infrastructure, i.e., U-
Compare.
There are currently a number of freely-available
NLP workflow infrastructures (Ferrucci and Lally,
2004; Cunningham et al, 2002; Scha?fer, 2006;
Kano et al, 2011; Grishman, 1996; Baumgartner
et al, 2008; Hahn et al, 2008; Savova et al, 2010;
Gurevych et al, 2007; Rak et al, 2012b). Most
of the available infrastructures support the devel-
opment of standard NLP applications, e.g., part-
of-speech tagging, deep parsing, chunking, named
entity recognition and several of them allow the
representation and analysis of discourse phenom-
ena (Kano et al, 2011; Cunningham et al, 2002;
Savova et al, 2010; Gurevych et al, 2007). How-
ever, none of them has demonstrated the integra-
tion of resources annotated according to multiple
annotation schemes within a single NLP pipeline.
GATE (Cunningham et al, 2002) is an open
source NLP infrastructure that has been used for
the development of various language processing
tasks. It is packaged with an exhaustive number
of NLP components, including discourse analy-
sis modules, e.g., coreference resolution. Further-
more, GATE offers a GUI environment and wrap-
pers for UIMA-compliant components. However,
GATE implements a limited workflow manage-
ment mechanism that does not support the execu-
tion of parallel or nested workflows. In addition to
this, GATE does not promote interoperability of
language resources since it does not define any hi-
erarchy of NLP data types and components do not
formally declare their input/output capabilities.
In contrast to GATE, UIMA implements a more
sophisticated workflow management mechanism
that supports the construction of both parallel
and nested pipelines. In this paper, we exploit
this mechanism to integrate multiple annotation
schemes in NLP workflows. cTAKES (Savova
et al, 2010) and DKPro (Gurevych et al, 2007)
are two repositories containing UIMA-compliant
components that are tuned for the medical and
general domain, respectively. However, both of
these repositories support the representation of
only one discourse phenomenon, i.e., coreference.
Argo (Rak et al, 2012a; Rak et al, 2012b) is a
web-based platform that allows multiple branch-
ing and merging of UIMA pipelines. It incorpo-
rates several U-Compare components and conse-
quently, supports the U-Compare type system.
3 A UIMA architecture for processing
multiple annotation schemes
In UIMA, a document, together with its associated
annotations, is represented as a standardised data
structure, namely the Common Analysis Struc-
ture (CAS). Each CAS can contain any number
of nested sub-CASes, i.e., Subjects of Analysis
(Sofas), each of which can associate a different
type of annotation with the input document. In
this paper, we employ this UIMA mechanism to
allow the integration and comparison of multiple
81
Collection of Documents
Multi-SofaReader
Parallel Annotation Viewer Annotation Merger
ComparingSchemes Integrating Schemes
ComponentC_1
Sofa S_1
ComponentC_2
Sofa S_2
ComponentC_N-1
Sofa S_N-1
ComponentC_N
Sofa S_N
sub-workflows
Figure 1: Integrating annotations from multiple
annotation schemes in UIMA workflows
annotation schemes in a single U-Compare work-
flow. Assume that we have a corpus of documents
which has been annotated according to n different
schemes, S1, S2, ..., Sn?1, Sn. Also, assume that
we will use a library of m text analysis compo-
nents, C1, C2, ..., Cm?1, Cm, to enrich the corpus
with further annotations.
Our implemented architecture is illustrated in
Figure 1. Using multiple Sofas, we are able to split
a UIMA workflow into parallel sub-workflows.
Starting from a Multi-Sofa reader, we create n
sub-workflows, i.e., Sofas, each of which is linked
to a particular scheme for a different annotation
type. Each sub-workflow can then apply the anal-
ysis components that are most suitable for pro-
cessing the annotations from the corresponding
scheme.
U-Compare offers two different modes for visu-
alising corpora that have been annotated accord-
ing to multiple schemes. In the comparison mode,
the default annotation viewer is automatically split
to allow annotations from different schemes to be
displayed side-by-side. The second type of visu-
alisation merges the annotations produced by the
parallel sub-workflows into a single view. The
most appropriate view may depend on the prefer-
ences of the user and the task at hand, e.g., iden-
tifying similarities, differences or complementary
information between different schemes.
4 Application Workflows
In this section, we demonstrate two workflow ap-
plications that integrate multiple discourse anno-
tation schemes. The first workflow exploits U-
Compare?s comparison mode to visualise in par-
allel functional discourse annotations from two
schemes, namely, CoreSC (Liakata et al, 2012a)
and GENIA-MK (Thompson et al, 2011). The
second application integrates functional discourse
annotations in the ACE 2005 corpus with dis-
course relations obtained by an automated tool.
4.1 Visualising functional discourse
annotations from different schemes
The purpose of this workflow application is to re-
veal the different interpretations given by two dis-
course annotation schemes applied to a biomed-
ical corpus of three full-text papers (Liakata et
al., 2012b). The pipeline contains two read-
ers that take as input the annotations (in the
BioNLP Shared Task stand-off format) from the
two schemes and map them to U-Compare?s
type system. In this way, the annotations be-
come interoperable with existing components in
U-Compare?s library. U-Compare detects that the
workflow contains two annotation schemes and
automatically creates two parallel sub-workflows
as explained earlier. Furthermore, we configure
the workflow to use the comparison mode. There-
fore, the annotation viewer will display the two
different types of annotations based on the input
schemes side-by-side. Figure 2 illustrates the par-
allel viewing of a document annotated according
to both the CoreSC (left-hand side) and GENIA-
MK (right-hand side) annotation schemes. The
CoreSC scheme assigns a single category per sen-
tence. The main clause in the highlighted sen-
tence on the left-hand side constitutes the hypoth-
esis that transcription factors bind to exon-1. Ac-
cordingly, as can be confirmed from the annota-
tion table on the far right-hand side of the figure,
the (Hyp)othesis category has been assigned to the
sentence.
In the GENIA-MK corpus, the different pieces
of information contained within the sentence have
been separately annotated as structured events.
One of these events corresponds to the hypothe-
sis, but this is not the only information expressed:
information about a previous experimental out-
come from the authors, i.e., that exon1 is impli-
cated in CCR3 transcription, is annotated as a sep-
82
Figure 2: Comparing discourse annotations schemes in U-Compare. The pipeline uses two Sofas corre-
sponding to the CoreSC (left panel) and GENIA-MK (right panel) schemes.
arate event. Since functional discourse informa-
tion is annotated directly at the event level in the
GENIA-MK corpus, the bind event is considered
independently from the other event as represent-
ing an Analysis. Furthermore, the word hypoth-
esized is annotated as a cue for this categorisa-
tion. There are several ways in which the an-
notations of the two schemes can be seen to be
complementary to each other. For example, the
finer-grained categorisation of analytical informa-
tion in the CoreSC scheme could help to determine
that the analytical bind event in the GENIA-MK
corpus specifically represents a hypothesis, rather
than, e.g., a conclusion. Conversely, the event-
based annotation in the GENIA-MK corpus can
help to determine exactly which part of the sen-
tence represents the hypothesis. Furthermore, the
cue phrases annotated in the GENIA-MK corpus
could be used as additional features in a system
trained to assign CoreSC categories. Although in
this paper we illustrate only the visualisation of
different types of functional discourse annotations,
it is worth noting that U-Compare provides sup-
port for further processing. Firstly, unlike annota-
tion platforms such as brat (Stenetorp et al, 2012),
U-Compare allows for analysis components to be
integrated into workflows in a straightforward and
user-interactive manner. If, for example, it is of in-
terest to determine the tokens (and the correspond-
ing parts-of-speech) which frequently act as cues
in Analysis events, syntactic analysis components
(e.g., tokenisers and POS taggers) can be incorpo-
rated via a drag-and-drop mechanism. Also, U-
Compare allows the annotations to be saved in a
computable format using the provided Xmi Writer
CAS Consumer component. This facilitates fur-
ther automatic comparison of annotations.
4.2 Integrating discourse relations with
functional discourse annotations
To demonstrate the integration of annotations orig-
inating from two completely different perspectives
on discourse, we have created a workflow that
merges traditional discourse relations with func-
tional discourse annotations in a general domain
corpus. For this application, we used the ACE
2005 corpus, which consists of 599 documents
coming from broadcast conversation, broadcast
news, conversational telephone speech, newswire,
weblog and usenet newsgroups. This corpus
contains event annotations which have been en-
riched by attributes such as polarity (positive or
negative), modality (asserted or other), generic-
ity (generic or specific) and tense (past, present,
future or unspecified). We treat the values of
these attributes as functional discourse annota-
tions, since they provide further insight into the
interpretation of the events. We created a compo-
nent that reads the event annotations in the corpus
and maps them to U-Compare?s type system.
To obtain discourse relation annotations (which
are not available in the ACE corpus) we em-
ployed an end-to-end discourse parser trained
on the Penn Discourse TreeBank (Lin et al,
2012). It outputs three general types of anno-
tations, namely, explicit relations, non-explicit
relations and attribution spans. Explicit rela-
tions (i.e., those having overt discourse connec-
tives) are further categorised into the following 16
PDTB level-2 types: Asynchronous, Synchrony,
Cause, Pragmatic cause, Contrast, Concession,
Conjunction, Instantiation, Restatement, Alterna-
tive, List, Condition, Pragmatic condition, Prag-
matic contrast, Pragmatic concession and Excep-
83
Figure 3: Integrating different discourse annotation schemes in U-Compare.
tion. Non-explicit relations, on the other hand,
consist of EntRel and NoRel types, in addition to
the same first 11 explicit types mentioned above.
We created a workflow consisting of the ACE
corpus reader and the discourse parser (available
in U-Compare as a UIMA web service). This al-
lowed us to merge traditional discourse relations
with event-based functional discourse annotations,
and to visualise them in the same document (Fig-
ure 3). Furthermore, with the addition of the
Xmi Writer CAS Consumer in the workflow, the
merged annotations can be saved in a computable
format for further processing, allowing users to
perform deeper analyses on the discourse annota-
tions. This workflow has enabled us to gain some
insights into the correlations between functional
discourse annotations and discourse relations.
5 Correlations between discourse
relations and functional discourse
annotations
Based on the merged annotation format described
in the previous section, we computed cases in
which at least one of the arguments of a discourse
relation also contains an event. Figure 4 is a
heatmap depicting the correlations between differ-
ent types of discourse relations and the attribute
values of ACE events that co-occur with these re-
lations. The darker the colour, the smaller the ratio
of the given discourse relation co-occurring with
the specified ACE event attribute value. For in-
stance, the Cause relation co-occurs mostly with
positive events (over 95%) and the correspond-
ing cell is a very light shade of green. These are
discussed and exemplified below. In the exam-
ples, the following marking convention is used:
discourse connectives are capitalised, whilst argu-
ments are underlined. Event triggers are shown in
bold, and cues relating to functional discourse cat-
egories are italicised.
For all discourse relation types, at least 50% of
co-occurring events are assigned the specific value
of the Genericity attribute. Specific events are
those that describe a specific occurrence or situ-
ation, rather than a more generic situation. In gen-
eral, this high proportion of specific events is to be
expected. The types of text contained within the
corpus, consisting largely of news and transcrip-
tions of conversions, would be expected to intro-
duce a large amount of information about specific
events.
For two types of discourse relations, i.e. Condi-
tion and Concession, there are more or less equal
numbers of specific and generic events. The na-
ture of these relation types helps to explain these
proportions. Conditional relations often describe
how a particular, i.e., specific, situation will hold
if some hypothetical situation is true. Since hypo-
thetical situations do not denote specific instances,
they will usually be labelled as generic. Con-
cessions, meanwhile, usually describe how a spe-
cific situation holds, even though another (more
generic) situation would normally hold, that would
be inconsistent with this. For the Instantiation re-
lation category, it may once again be expected that
similar proportions of generic and specific events
would co-occur within their arguments, since an
instantiation describes a specific instance of a
more generic situation. However, contrary to these
84
AlternativeAsynchronousAttributionCauseConcessionConditionConjunctionContrastEntRelInstantiationListNoRelRestatementSynchronous
GenericSpecific AssertedOther NegativePositive FuturePast PresentUnspecif
ied
0
1
0.5
0.25
0.75
Genericity Modality Polarity Tense0
1
0.5
0.25
0.75
0
1
0.5
0.25
0.75
0
1
0.5
0.25
0.75
Figure 4: Heatmap showing the distribution of correlations between discourse relations and event-based
functional discourse categories. A darker shade indicates a smaller percentage of instances of a discourse
relation co-occurring with an event attribute.
expectations, the ratio of specific to generic events
is 3:1. A reason for this is that discourse argu-
ments corresponding to the description of a spe-
cific instance may contain several different events,
as illustrated in Example (1).
(1) Toefting has been convicted before. In
1999 he was given a 20-day suspended sentence
for assaulting a fan who berated him for
playing with German club Duisburg.
In terms of the Modality attribute, most dis-
course relations correlate with definite, asserted
events. Simillarly to the Genericity attribute, this
can be largely explained by the nature of the texts.
However, there are two relation types, i.e., Condi-
tion and Consession, which have particularly high
proportions of co-occurring events whose modal-
ity is other. Events that are assigned this attribute
value correspond to those that are not described as
though they are real occurrences. This includes,
e.g., speculated or hypothetical events. The fact
that Condition relations are usually hypothetical
in nature explain why 76% of events that co-occur
with such relations are assigned the other value
for the Modality attribute. Example (2) illustrates
a sentence containing this relation type.
(2) And I?ve said many times, IF we all
agreed on everything, everybody would want to
marry Betty and we would really be in a mess,
wouldn?t we, Bob.
An even higher proportion of Concession re-
lations co-occurs with events whose modality is
other. Example (3) helps to explain this. In the
first clause (the generic situation), the mention of
minimising civilian casualties is only described as
an effort, rather than a definite situation. The hedg-
ing of this generic situation is necessary in order to
concede that the more specific situation described
in the second clause could actually be true, i.e.,
that a large number of civilians have already been
killed. Due to the nature of news reporting, which
may come from potentially unreliable sources, the
killed event in this second clause is also hedged,
through the use of the word reportedly.
(3) ALTHOUGH the coalition leaders have
repeatedly assured that every effort would be
made to minimize civilian casualties in the
current Iraq war, at least 130 Iraqi civilians have
been reportedly killed since the war started five
days ago.
Almost 96% of events that co-occur with argu-
ments of discourse relations have positive polarity.
Indeed, for eight relation types, 100% of the cor-
responding events are positive. This can partly be
explained by the fact that, in texts reporting news,
85
there is an emphasis on reporting events that have
happened, rather than events that did not happen.
It can, however, be noted that events that co-occur
with certain discourse relation types have a greater
likelihood of having negative polarity. These rela-
tions include Contrast (9% of events having neg-
ative polarity) and Cause (5% negative events).
Contrasts can include comparisons of positive and
negative situations, as in Example (4), whilst for
Causes, it can sometimes be relevant to state that
a particular situation caused a specific event not to
take place, as shown in Example (5).
(4) The message from the Israeli government
is that its soldiers are not targeting journalists,
BUT that journalists who travel to places where
there could be live fire exchange between
Israeli forces and Palestinian gunmen have a
responsibility to take greater precautions.
(5) His father didn?t want to invade Iraq, BE-
CAUSE of all these problems they?re having
now.
For most relation types, around 60% of their co-
occurring events are annotated as describing past
tense situations. This nature of newswire and con-
versations mean that this is largely to be expected,
since they normally report mainly on events that
have already happened. The proportion of events
assigned the future tense value is highest when
they co-occur with discourse relations of type Al-
ternative. In this relation type, it is often the case
that one of the arguments describes a possible fu-
ture alternative to a current situation, as the case in
Example (6). This possible information pattern for
Alternative relations, where one of the arguments
represents a currently occurring situation, would
also help to explain why, even though very few
events in general are annotated as present tense,
almost 10% of events that co-occur with Alter-
native relations describe events that are currently
ongoing. As for events whose Tense value is un-
specified, two of the most common discourse re-
lation types with which they occur are Condition
and Concession. As exemplified above, Condition
relations are often hypothetical in nature, meaning
that no specific tense can be assigned. The generic
argument of a Concession relation can also remain
unmarked for tense. As in Example (3), it is not
clear whether the effort to minimise civilian casu-
alties has already been initiated, or will be initiated
in the future.
(6) Saddam wouldn?t be destroying missiles
UNLESS he thought he was going to be
destroyed if he didn?t.
6 Conclusions
Given the level of variability in existing discourse-
annotated corpora, it is meaningful for users to
identify the relative merits of different schemes.
In this paper, we have presented an extension of
the U-Compare infrastructure that facilitates the
comparison, integration and visualisation of doc-
uments annotated according to different annota-
tion schemes. U-Compare constructs multiple and
parallel annotation sub-workflows nested within a
single workflow, with each sub-workflow corre-
sponding to a distinct scheme. We have applied
the implemented method to visualise the similar-
ities and differences of two functional discourse
annotation schemes, namely CoreSC and GENIA-
MK. To demonstrate the integration of multiple
schemes in U-Compare, we developed a work-
flow that merged event annotations from the ACE
2005 corpus (which include certain types of func-
tional discourse information) with discourse rela-
tions obtained by an end-to-end parser. Moreover,
we have analysed the merged annotations obtained
by this workflow and this has allowed us to iden-
tify various correlations between the two different
types of discourse annotations.
Based on the intuition that there is comple-
mentary information across different types of dis-
course annotations, we intend to examine how the
integration of multiple discourse schemes, e.g.,
features obtained by merging annotations, affects
the performance of machine learners for discourse
analysis.
7 Acknowledgements
We are grateful to Dr. Ziheng Lin (Na-
tional University of Singapore) for providing us
with the discourse parser used for this work.
This work was partially funded by the Euro-
pean Community?s Seventh Framework Program
(FP7/2007-2013) [grant number 318736 (OSS-
METER)]; Engineering and Physical Sciences Re-
search Council [grant numbers EP/P505631/1,
EP/J50032X/1]; and MRC Text Mining and
Screening (MR/J005037/1).
86
References
Sophia Ananiadou, Sampo Pyysalo, Junichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology, 28(7):381 ? 390.
Riza Theresa B. Batista-Navarro, Georgios Kontonat-
sios, Claudiu Miha?ila?, Paul Thompson, Rafal Rak,
Raheel Nawaz, Ioannis Korkontzelos, and Sophia
Ananiadou. 2013. Facilitating the analysis of dis-
course phenomena in an interoperable NLP plat-
form. In Computational Linguistics and Intelligent
Text Processing, volume 7816 of Lecture Notes in
Computer Science, pages 559?571. Springer Berlin
Heidelberg, March.
William A. Baumgartner, Kevin Bretonnel Cohen, and
Lawrence Hunter. 2008. An open-source frame-
work for large-scale, flexible evaluation of biomedi-
cal text mining systems. Journal of biomedical dis-
covery and collaboration, 3:1+, January.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2001. Building a discourse-tagged cor-
pus in the framework of Rhetorical Structure Theory.
In Proceedings of the Second SIGdial Workshop on
Discourse and Dialogue - Volume 16, SIGDIAL ?01,
pages 1?10, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
an architecture for development of robust HLT
applications. In In Recent Advanced in Language
Processing, pages 168?175.
Anita de Waard and Henk Pander Maat. 2009. Epis-
temic segment types in biology research articles. In
Proceedings of the Workshop on Linguistic and Psy-
cholinguistic Approaches to Text Structuring (LPTS
2009).
Anita de Waard. 2007. A pragmatic structure for re-
search articles. In Proceedings of the 2nd inter-
national conference on Pragmatic web, ICPW ?07,
pages 83?89, New York, NY, USA. ACM.
David Ferrucci and Adam Lally. 2004. Building an
example application with the unstructured informa-
tion management architecture. IBM Systems Jour-
nal, 43(3):455?475.
Ralph Grishman. 1996. TIPSTER Text Phase II archi-
tecture design version 2.1p 19 june 1996. In Pro-
ceedings of the TIPSTER Text Program: Phase II,
pages 249?305, Vienna, Virginia, USA, May. Asso-
ciation for Computational Linguistics.
Yufan Guo, Anna Korhonen, Maria Liakata,
Ilona Silins Karolinska, Lin Sun, and Ulla Ste-
nius. 2010. Identifying the information structure of
scientific abstracts: An investigation of three differ-
ent schemes. In Proceedings of the 2010 Workshop
on Biomedical Natural Language Processing, pages
99?107. Association for Computational Linguistics.
Iryna Gurevych, Max Mu?hlha?user, Christof Mu?ller,
Ju?rgen Steimle, Markus Weimer, and Torsten Zesch.
2007. Darmstadt knowledge processing repository
based on UIMA. In Proceedings of the First Work-
shop on Unstructured Information Management Ar-
chitecture at Biannual Conference of the GSCL.
Udo Hahn, Ekaterina Buyko, Rico Landefeld, Matthias
Mu?hlhausen, Michael Poprat, Katrin Tomanek, and
Joachim Wermter. 2008. An overview of JCoRe,
the JULIE lab UIMA component repository. In
LREC?08 Workshop ?Towards Enhanced Interoper-
ability for Large HLT Systems: UIMA for NLP?,
pages 1?7, Marrakech, Morocco, May.
Yoshinobu Kano, Makoto Miwa, Kevin Cohen,
Lawrence Hunter, Sophia Ananiadou, and Jun?ichi
Tsujii. 2011. U-Compare: A modular NLP work-
flow construction and evaluation system. IBM Jour-
nal of Research and Development, 55(3):11.
Maria Liakata, Simone Teufel, Advaith Siddharthan,
and Colin Batchelor. 2010. Corpora for the concep-
tualisation and zoning of scientific papers. In Pro-
ceedings of LREC, volume 10.
Maria Liakata, Shyamasree Saha, Simon Dobnik,
Colin Batchelor, and Dietrich Rebholz-Schuhmann.
2012a. Automatic recognition of conceptualization
zones in scientific articles and two life science appli-
cations. Bioinformatics, 28(7):991?1000.
Maria Liakata, Paul Thompson, Anita de Waard, Ra-
heel Nawaz, Henk Pander Maat, and Sophia Ana-
niadou. 2012b. A three-way perspective on scien-
tic discourse annotation for knowledge extraction.
In Proceedings of the ACL Workshop on Detecting
Structure in Scholarly Discourse (DSSD), pages 37?
46, July.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2012. A
PDTB-styled end-to-end discourse parser. Natural
Language Engineering, FirstView:1?34, 10.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a functional
theory of text organization. Text, 8(3):243?281.
Claudiu Miha?ila?, Tomoko Ohta, Sampo Pyysalo, and
Sophia Ananiadou. 2013. BioCause: Annotating
and analysing causality in the biomedical domain.
BMC Bioinformatics, 14(1):2, January.
Yoko Mizuta, Anna Korhonen, Tony Mullen, and Nigel
Collier. 2006. Zone analysis in biology articles as a
basis for information extraction. International Jour-
nal of Medical Informatics, 75(6):468 ? 487. Re-
cent Advances in Natural Language Processing for
Biomedical Applications Special Issue.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bon-
nie Webber. 2008. The Penn Discourse Tree-
Bank 2.0. In Nicoletta Calzolari, Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios
87
Piperidis, and Daniel Tapias, editors, In Proceedings
of the 6th International Conference on language Re-
sources and Evaluation (LREC), pages 2961?2968.
Rashmi Prasad, Susan McRoy, Nadya Frid, Aravind
Joshi, and Hong Yu. 2011. The biomedical
discourse relation bank. BMC Bioinformatics,
12(1):188.
Rafal Rak, Andrew Rowley, and Sophia Ananiadou.
2012a. Collaborative development and evaluation
of text-processing workflows in a UIMA-supported
web-based workbench.
Rafal Rak, Andrew Rowley, William Black, and Sophia
Ananiadou. 2012b. Argo: an integrative, in-
teractive, text mining-based workbench supporting
curation. Database: The Journal of Biological
Databases and Curation, 2012.
A?gnes Sa?ndor and Anita de Waard. 2012. Identifying
claimed knowledge updates in biomedical research
articles. In Proceedings of the Workshop on De-
tecting Structure in Scholarly Discourse, ACL ?12,
pages 10?17, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Guergana Savova, James Masanz, Philip Ogren, Jiap-
ing Zheng, Sunghwan Sohn, Karin Kipper-Schuler,
and Christopher Chute. 2010. Mayo clini-
cal text analysis and knowledge extraction system
(cTAKES): architecture, component evaluation and
applications. Journal of the American Medical In-
formatics Association, 17(5):507?513.
Ulrich Scha?fer. 2006. Middleware for creating and
combining multi-dimensional nlp markup. In Pro-
ceedings of the 5th Workshop on NLP and XML:
Multi-Dimensional Markup in Natural Language
Processing, pages 81?84. ACL.
Pontus Stenetorp, Sampo Pyysalo, Goran Topic?,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. brat: a web-based tool for NLP-assisted
text annotation. In Proceedings of the Demonstra-
tions Session at EACL 2012, Avignon, France, April.
Association for Computational Linguistics.
Simone Teufel, Jean Carletta, and Marc Moens. 1999.
An annotation scheme for discourse-level argumen-
tation in research articles. In Proceedings of the
ninth conference on European chapter of the Asso-
ciation for Computational Linguistics, EACL ?99,
pages 110?117, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Paul Thompson, Raheel Nawaz, John McNaught, and
Sophia Ananiadou. 2011. Enriching a biomedi-
cal event corpus with meta-knowledge annotation.
BMC Bioinformatics, 12(1):393.
Christopher Walker. 2006. ACE 2005 Multilingual
Training Corpus.
W John Wilbur, Andrey Rzhetsky, and Hagit Shatkay.
2006. New directions in biomedical text annota-
tion: definitions, guidelines and corpus construction.
BMC Bioinformatics, 7(1):356.
88
