Intentions, Implicatures and Processing of Complex Questions
Sanda M. Harabagiu and Steven J. Maiorano and Alessandro Moschitti and Cosmin A. Bejan
University of Texas at Dallas
Human Language Technology Research Institute
Richardson, TX 75083-0688, USA
Abstract
In this paper we introduce two methods for
deriving the intentional structure of complex
questions. Techniques that enable the deriva-
tion of implied information are also presented.
We show that both the intentional structure
and the implicatures enabled by it are essen-
tial components of Q/A systems capable of suc-
cessfully processing complex questions. The
results of our evaluation support the claim that
there are multiple interactions between the pro-
cess of answer finding and the coercion of in-
tentions and implicatures.
1 Introduction
The Problem of Question Intentions.
When using a Question Answering system to find
information, the user cannot separate the intentions
and beliefs from the formulation of the question. A
direct consequence of this phenomenon is that the user
incorporates his or her intentions and beliefs into the
interrogation. For example, when asking the question:
Q1: What kind of assistance has North Korea received
from the USSR/Russia for its missile program?
the user associate with the question a number of in-
tentions, that maybe expressed a set of intended ques-
tions. Each intended question, in turn generates implied
information, that maybe expresses as implied questions.
For question Q1, a list of intended questions and implied
questions is detailed in Table1.
Most of the intended questions are similar with the
questions evaluated in TREC1. For example questions
1The Text REtrieval Conferences (TREC) are evaluation
workshops in which Information Retrieval tasks are annually
tested. Since 1999 the performance of question answering sys-
tems are measured in the TREC QA track.
Qi1, Qi2 and Qi3 are so-called definition questions, since
they ask about defining properties of an object. However
unlike the TREC definition questions, these questions ex-
press unstated intentions of the questioner and need to be
processed in the context of the original complex question
Q1. Questions Qi4 and Qi5 are factoid questions, request-
ing information about facts or events. Qi6 asks about the
source of information that enables the answers of ques-
tion Q1.
Questions Qi1, Qi2, Qi3, Qi4 and Qi5 result from the in-
tentional structure generated when processing question
Q1 or questions similar to it. When intended questions
are generated, their sequential processing (a) represents a
decomposition of the complex question and (b) generates
a scenario for finding information; thus questions like Q1
are also known as scenario questions.
Intentions and Implicatures.
As Table 1 suggests, the implied information takes the
form of alternatives that guide the answers to intended
questions. For example, question Qm11 lists alternatives
for the answer to Qi1 whereas Qm21 lists components of
the answer of Qi1. Implicatures may also involve tem-
poral inference, e.g. the implied questions pertaining
to Qi3 and Qi4. Additionally, the reliability of infor-
mation is commonly an implicature in the case of sce-
nario questions, since the causal and temporal inference
is based on the quality and correctness of the available
data sources. Neither intentions or implicatures are rec-
ognizable at syntactic or semantic level, but they both
play an important role in the question interpretation. In-
terpretations disregard the implied information or the user
intentions determine the extraction of incorrect answers,
thus influence the performance of Q/A systems.
Our solution.
In this paper we present two different mechanisms of
deriving the question implicatures. Both methods start
from the syntactic and semantic content of the interro-
gation. The first method considers only the semantic
Intended Questions Implied Questions
Qi1 :What is the USSR/Russia? Qm11 :Is this the Soviet/Russian government?
Qm12 :Does it include private firms, state-owned firms, educational
institutions, and individuals?
Qi2 :What is North Korea? Qm21 :Is this the North Korean government only?
Qm22 :Does it include private firms, state-owned firms, educational
institutions, and individuals?
Qi3 :What is assistance? Qm31 :Is it the transfer of complete missile systems, licensing agreements,
components, materials, or plans?
Qm32 :Is it the training of personnel?
Qm33 :What kind of training?
Qm34 :Does transfer include data, and, if so, what kind of data?
Qm35 :Does transfer include financial assistance, and, if so, what kind of
financial assistance?
Qi4 :What are the missiles in the North Qm41 :Are any based upon Soviet/Russian designs?
Korean inventory? Qm42 :If so, which ones?
Qm43 :What was the development timeline of the missiles?
Qm44 :Did any timeline differ significantly from others?
Qm45 :Did North Korea receive assistance from other sources besides
USSR/Russia to develop these missiles?
Qi5 :When did North Korea receive assistance Qm51 :Was any intended assistance halted, stopped or intercepted?
from the USSR/Russia?
Qi6 :What are the sources of information? Qm61 :Are the sources reliable?
Qm62 :Is some information contradictory?
Table 1: Question decomposition associated with question Q1
meaning of the words used in the question whereas the
second method considers the predicate-argument struc-
ture of the question and candidate answers as a form of
shallow semantics that enables the inference of the inten-
tional structure. Question implicatures are derived from
lexico-semantic paths retrieved from the WordNet lexico-
semantic database. These paths bring forward new con-
cepts, that may be associated with the question implica-
tures when testing the paths against the conversational
maxims introduced by Grice in (Grice, 1975a). For ex-
ample, if the user asks ?Will Prime Minister Mori survive
the crisis??, the first method detects the user?s belief that
the position of the Prime Minister is in jeopardy, since
the concept DANGER is coerced although none of the
question words directly imply it.
The second method generates the intentional structure
of the question, enabling a more structured representa-
tion of the pragmatics of question interpretation. The in-
tentional structure is based on a study that we have con-
ducted for capturing the motivations of a group of users
when asking series of questions in several scenarios. We
show how the intentional structures that we have gathered
guide the coercion of knowledge that helps to support the
acceptance of rejection of computational implicatures.
The derivation of intentional structures is made possi-
ble by predicate-argument structures that are recognized
both at the question level and at the candidate answer
level. In this paper we show how richer semantic ob-
jects can be derived around predicate-argument structures
and how inferential mechanisms can be associated with
such semantic objects for obtaining correct answers. The
rest of the paper is organized as follows. In Section 2
we describe several forms of complex questions that re-
quire the derivation of computational implicatures. Sec-
tion 3 details the models of Question Answering that we
considered and Section 4 shows our methods of deriving
predicate-argument structures and their usage in identi-
fying answers for questions. Section 5 details the inten-
tional structures whereas Section 6 summarizes the con-
clusions.
2 Question Complexity
Since 1999, the TREC QA evaluations focused on fac-
toid questions, such as ?In what year did Joe Di Maggio
compile his 56-game hitting streak?? or ?Name a film in
which Jude Law acted.?. The answers to most of these
questions belong to semantic categories associated with
each question class. For example, questions asking about
a date or a year can be answered because Named Entity
Recognizers identify a temporal expression in a candidate
text span. Similarly, names of people or organizations
are provided as answers to questions such as ?Who is the
first Russian astronaut?? or ?What is the largest software
company in the world??. Most Named Entity Recogniz-
ers detect names of PEOPLE, ORGANIZATIONS, LOCA-
TIONS, DATES, PRICES and NUMBERS. For factoid Q/A,
the list of name categories needs to be extended, as re-
ported in (Harabagiu et al, 2003) for recognizing many
What kind od assistance has North Korea received fromComplex Question:
What kind of assistance has X received from Y for Z?Question PATTERN:
X=North Korea FOCUS=Z=misile program
Intended Questions:
Definition Questions:
What is Y? "What is USSR/Russia?"
What is assistance? 
Elaboration of FOCUS:
Reliability: "What are the sources of information?"
the USSR/Russia for its missile program?
assistance from the USSR/Russia?"
(1) RESULTATIVE
(2) TEMPORAL
North Korean inventory?"
"When did North Korea receive
Y=USSR/Russia
What is X? "What is North Korea?"
"What are the missiles in the
Figure 1: Decomposition of scenario question into intended questions
more types of names, e.g. names of movies, names of
diseases, names of battles. Moreover, the semantic cat-
egories of the extended set of names need to be incor-
porated into an answer type taxonomy that enables the
recognition of (a) the expected answer type and (b) the
question class. The taxonomy of expected answer types
is useful because the answer is not always a name; it can
be a lexicalized concept or a concept that is expressed by
a paraphrase.
The TREC evaluations have also considered two more
classes of questions: (1) list questions and (2) definition
questions. The list questions have answers that are typ-
ically assembled from different documents. Such ques-
tions are harder to answer than factoid questions be-
cause the systems must detect duplications. Example of
list questions are ?Name singers performing the role of
Donna Elvira in performances of Mozart?s ?Don Gio-
vani?.? or ?What companies manufacture golf clubs??.
Definition questions require a different form of process-
ing that factoid questions because no taxonomy of answer
types needs to be used. The expected answer type is a def-
inition, which cannot be represented by a single concept.
Q/A systems assume that definitions are given by follow-
ing a set of linguistic patterns that need to be matched for
extracting the answer. Example of definition questions
are ?What is a golden parachute?? or ?What is ETA in
Spain??.
In (Echihabi and Marcu, 2003) a noisy channel model
for Q/A was introduced. This model is based on the
idea that if a given sentence SA contains an answer sub-
string A to a question Q, then SA can be re-written into
Q through a sequence of stochastic operators. Not only a
justification of the answer is produced, but the conditional
probability P(Q?SA) re-ranks all candidate answers.
A different viewpoint of Q/A was reported in (Itty-
cheriah et al, 2000). Finding the answers A to a ques-
tion Q was considered a classification problem that maxi-
mizes the conditional probability P(A?Q). This model is
not tractable currently, because (a) the search space is too
large for a text collection like the TREC or the AQUAINT
corpora; and (b) the training data is insufficient. There-
fore, Q/A is modeled by the distribution P(C?A,Q)
where C measures the ?correctness? of A to question
Q. By using a hidden variable E that represents the ex-
pected answer type, P(C?A,Q) = ?E p(C,E?Q,A) =
?E p(C?E,Q,A) * p(E?Q,A). Both distributions are
modeled by using the maximum entropy.
All three forms of questions are also useful when pro-
cessing complex questions, determined by a scenario re-
sulting from a problem-solving situation. As illustrated
in Figure 1, a scenario question may be associated with a
pattern. One of the pattern variables represents the focus
of the question. The notion of the question focus was first
introduced by (Lehnert, 1978). The focus represents the
most important concept of the question; a concept deter-
mining the domain of question. In the case of question
Q1, the focus is missile program. The identification of
the focus is based on the predicate-structure of the ques-
tion pattern and on the order of the arguments. Figure 3
shows both the question pattern associated with Q1 and
its predicate-argument structure. The argument with the
role of purpose is ranked highest, and thus it determines
the question focus.
With the exception of the focus, all arguments from
the predicate-argument structure may be used for gener-
ating definition questions. The focus is elaborated upon.
Several forms of elaborations are possible. One is a tem-
poral one, as illustrated in Figure 1. Other are resultative,
causative or manner-based. For example, the knowledge
that assistance in a missile program results in an inven-
When did North Korea receive from USSR/Russiaassistance
WRB VBD NNP NNP NNPVB NN
NPB
IN
PP
NPB
VP
NPB
SQ
WHADVP
SBARQ
OBJECTBENEFICIARY SOURCEDATE
when=DATE
Step 2(a): Binary Semantic Dependencies
receive North Korea USSR/Russia assistance
TypeExpected Answer
Step 2(b): Predicate?Argument Structures
Predicate: receive
Arguments: assistance=OBJECT
North Korea=BENEFICIARY
USSR/Russia=SOURCE
When=DATE=Expected Answer Type
Question: When did North Korea receive assistance from USSR/Russia?
Step 1: Syntactic Parse
Figure 2: Deriving the Expected Answer Type
Predicate?argument structure:
Predicate:
Arguments:
receive
Purpose: Z
Beneficiary: X Source: Y
Object: assistance
Question Pattern: What kind of assistance has X received
from Y for Z?
Figure 3: Predicate-argument structure
tory of missiles allows for resultative elaboration. Further
knowledge needs to be coerced for generating the implied
questions as possible follow-ups to intended questions.
The relationship between intended questions and im-
plied questions is marked by the presence of multiple
references, e.g. the pronouns it and this or any and
ones. The generation of implied questions is made pos-
sible by knowledge that is coerced from the intended
questions. For example, when asking Qi1 :?What is
the USSR/Russia?? the coercion process abstracts away
from the concept that needs to be defined, i.e. a coun-
try. The implied question requests confirmation of
the metonymy resolution involving USSR/Russia.This
named entity may represent a country but most likely it
refers to its government or, as Qm12 suggests, organiza-
tions or individuals acting on behalf of the country. Both
Qm11 and Qm12 , implied questions derived from the in-
tended question Qi1, refer to the metonymy by using the
pronouns this and it respectively. Different forms of coer-
cion are used for Qi3 because in this case the knowledge
is associated with the predicate. The implied questions
associated with the focus, i.e. the intended question Qi4,
coerce the design and development predicates which are
associated with the missiles as well as the timelines of
possible additional assistance.
3 Models of Question Answering
The processing of questions is typically performed as a
sequence of three processes: (1) Question Processing; (2)
Document Processing and (3) Answer Extraction. In the
case of factoid questions , question processing involves
the classification of questions with the purpose of pre-
dicting what semantic class the answer should belong
to. Thus we may have questions asking about PEOPLE,
ORGANIZATIONS, TIME or LOCATIONS. Since open-
domain Q/A systems process questions regardless of the
domain of interest, question processing must be based on
an extended ontology of answer types. The identification
of the expected answer type is based either on binary se-
mantic dependencies extracted from the syntactic parse of
the question (Harabagiu et al, 2001) or on the predicate-
argument structure of the question. In both cases, the re-
lation to the question stem (i.e. what, who, when) enables
the classification. Figure 2 illustrates a factoid question
generated as an intended question and the derivation of
its expected answer type.
However, many times the expected answer type needs
to be identified from an ontology that has high lexico-
semantic coverage. Many Q/A systems use the WordNet
database for this purpose. In contrast, definition ques-
tions do not require the identification of the expected an-
Answer Pattern:
Answer:
Question Pattern:
Question?Point, a Definition
has killed nearly 800 people since
taking up arms in 1968
for Basque Homeland and Freedom ?
ETA, a Basque language acronym
What is Question?Point in Country?
What is in SpainETA
NNP
NPB
IN
PP
NNP
NPB
NP
VBZ
SQ
WP
WHNP
SBARQ
Definition Question: What is ETA in Spain?
Question Parse:
Figure 4: Patterns for Processing Definition Questions
swer type, since they always request a definition. How-
ever, definition questions are matched against a set of pat-
terns, which enables the extraction of the definition from
the candidate answers. Figure 4 illustrates a definition
question, the pattern it matched as well as the extracted
answer.
Both factoid and definition questions can be answered
only if candidate passages are available. The retrieval of
these passages is made possible by keywords that are se-
lected from the question words. The Documents Process-
ing module implements a search engine that returns pas-
sages that are likely to contain the expected answer type
in the case of factoid questions or the definition pattern
in the case of definition questions. The answer extraction
module optimizes the extraction of the correct answer by
unifying the question information with the answer infor-
mation. The unification may be based on pattern match-
ing; on machine learning algorithms based on the ques-
tion and answer features or on abductive reasoning that
justifies the answer correctness.
Current state-of-the-art QA systems search for the can-
didate answer by assuming that the answers are single
concepts, that can be recognized from a hierarchy or by
a Named Entity Recognizer. This is a serious limitation,
but it works well for the factoid, list or definition ques-
tions evaluated in TREC.
The three modules of current Q/A systems reflect the
three functions that need to be considered by any Q/A
model: (1) understanding what the question asks; (2)
identify candidate text passage that might contain the an-
swer; and (3) the extraction of the correct answer. Cur-
rently, the expected answer type represents what ques-
tion asks about: a semantic concept, e.g. the name of a
person, location or organization, kinds of diseases, types
of animals or plants. Generally these semantic concepts
are lexicalized in a single word or in 2-word collocations.
Clearly, this represents a limitation, since often the ques-
tions ask for more than a single concept. As we have
seen in Table1, there is additional intended and implied
information that is requested. Therefore new models of
Question/Answering need to incorporate these additional
forms of knowledge.
When definition questions are processed in current
Q/A systems, they are matched against a pattern, which is
different from the question patterns associated with com-
plex questions similar to those illustrated in Figure 1. In
the case of a definition question like ?What is ETA in
Spain??, the pattern identifies the question-point (QP) as
ETA- the concept that needs to be defined and Spain as
its context. The definition question pattern also contains
several surface-form patterns that are matched in the can-
didate paragraphs. One such pattern is recognized in an
apposition, by [QP, a AP] where AP represents the an-
swer phrase. In the following passage:
?ETA, a Basque language acronym for Basque Homeland
and Freedom - has killed nearly 800 people since taking
up arms in 1968.?
the exact answer representing the definition is identified
in AP: Basque language acronym for Basque Homeland
and Freedom. The fact that Basque country is a region in
Spain allows a justification of the question context.
In this paper, by considering the intentional informa-
tion and the implied information that can be derived when
processing questions, we introduce a novel model of Q/A,
which has access to rich semantic structures and enables
the retrieval of more accurate answers as well as inference
processes that explain the validity and contextual cover-
age of answers.
Figure 5 shows the structure of the novel model of Q/A
we propose. Both Question Processing and Document
Processing have the recognition of predicate-argument
structures as a crux of their models. As reported in (Sur-
deanu et al, 2003), the recognition of predicate-argument
structures depends on features made available by full syn-
tactic parses and by Named Entity Recognizers. As we
shall show in this paper, the predicate-argument struc-
tures enable the recognition of question pattern, the ques-
tion focus and the intentional structure associated with
Question
Syntactic Parse Named EntityRecognition
Identification of
Predicate?Argument Structure
Structure
Recognition of Answer
based on extended
Indexing & Retrieval
lexico?semantic knowledge
Named EntityRecognitionSyntactic Parse
Intentional Structure
Identification ofPredicate?ArgumentStructures Question Pattern
Recognition of
Identification ofQuestion Focus
Recognition of Answer Structure
Keyword Extraction
Validation of Implied Information
Answer Structure
Recognition of
Recognition and extention
of intentional structure
Reference Resolution
Question Processing Answer ProcessingDocument Processing
Answer
Figure 5: Novel Question/Answering Architecture.
a question. When the intentions are known, the answer
structure can be identified and the keywords extracted.
For better retrieval of candidate answers, documents are
indexed and retrieved based on the predicate-argument
structures as well as on complex semantic structure asso-
ciated with different question patterns. Similarly, the in-
tentional structures are used for indexing/retrieving can-
didate passages. The Answer Processing function in-
volves the recognition of the answer structure and inten-
tional structure. Often this requires reference resolution.
The implied information coerced from both the question
and the candidate answer is also validated before decid-
ing on the answer correctness.
4 Predicate-Argument Structures
To identify predicate-argument structures in questions
and passages, we have: (1) used the Proposition Bank or
PropBank as training data; and (2) a mode for predicting
argument roles similar to the one employed by (Gildea
and Jurafsky, 2002).
PropBank is a one million word corpus annotated with
predicate-argument structures on top of the Penn Tree-
bank 2 Wall Street Journal texts. For any given predicate,
the expected arguments are labeled sequentially from Arg
0 to Arg 4. Generally, Arg 0 stands for agent, Arg 1 for
direct object or theme or patient, Arg 2 for indirect object
or benefactive or instrument or attribute or end state, Arg
3 for start point or benefactive or attribute and Arg4 for
end point. In addition to these core arguments, adjunc-
tative arguments are marked up. They include functional
tags from Treebank, e.g. ArgM-DIR indicates a direc-
tional, ArgM-LOC indicates a locative, and ArgM-TMP
stands for a temporal.
An example of PropBank markup is:
[Arg10 Analysts ] have been [predicate1 expecting ] [Arg11
a GM-Jaguar pact ] that would [predicate2 give ] [Arg22 the
U.S. car maker ] [Arg21 an eventual 30% state in the British
Company ].
The model of identifying the arguments of each pred-
icate consists of two tasks: (1) the recognition of the
boundaries of each argument in the syntactic parse tree;
(2) the identification of the argument role. Each task can
be cast as a separate classifier. Next section describes
our approach based on Support Vector Machines (SVM)
(Vapnik, 1995).
4.1 Automatic Predicate-Argument extraction
Given a sentence in natural language, all the predicates
associated with its verbs have to be identified along with
their arguments. This problem can be divided in two sub-
tasks: (a) detection of the target argument boundaries,
i.e. all its compounding words, and (b) classification of
the argument type, e.g. Arg0 or ArgM.
A direct approach to learn both detection and classifi-
cation of predicate arguments is summarized by the fol-
lowing steps:
1. Given a sentence from the training-set, generate a
full syntactic parse-tree;
2. let P and A be the set of predicates and the set of
parse-tree nodes (i.e. the potential arguments), re-
spectively;
3. for each pair <p, a> ? P ?A:
? extract the feature representation set, Fp,a;
? if the subtree rooted in a covers exactly the
words of one argument of p, put Fp,a in T+
(positive examples), otherwise put it in T?
(negative examples).
0.68
0.71
0.74
0.77
0.8
0.83
1 2 3 4 5
Polynomial Degree
(a)
F1
Arg0
Arg1
ArgM
0.65
0.68
0.71
0.74
0.77
0.8
1 2 3 4 5
Polynomial Degree
(b)
F1
Figure 6: Single classifiers and Multi-classifier performance for argument extraction.
The above T+ and T? sets can be re-organized as pos-
itive T+argi and negative T?argi examples for each argu-
ment i. In this way, an individual ONE-vs-ALL SVM
classifier for each argument i can be trained. We adopted
this solution as it is simple and effective (Pradhan et al,
2003). In the classification phase, given a sentence of the
test-set, all its Fp,a are generated and classified by each
individual SVM classifier. As a final decision, we select
the argument associated with the maximum value among
the scores provided by the SVMs2, i.e. argmaxi?S Ci,
where S is the target set of arguments.
The discovering of relevant features is a complex task.
Nevertheless there is a common consensus on the basic
features that should be adopted. These standard features,
first proposed in (Gildea and Jurafsky, 2002), are derived
from parse trees as illustrated by Table 2.
4.2 Parsing Sentence into Predicate Argument
Structures
For the experiments, we used PropBank
(www.cis.upenn.edu/?ace) along with Penn-
TreeBank3 2 (www.cis.upenn.edu/?treebank)
(Echihabi and Marcu, 2003). This corpus contains about
53,700 sentences and a fixed split between training and
testing which has been used in other researches (Gildea
and Jurafsky, 2002; Surdeanu et al, 2003; Hacioglu et
al., 2003; Chen and Rambow, 2003; Gildea and Hock-
enmaier, 2003; Gildea and Palmer, 2002; Pradhan et al,
2003). In this split, Sections from 02 to 21 are used for
training, section 23 for testing and sections 1 and 22 as
developing set. We considered all PropBank arguments
from Arg0 to Arg9, ArgA and ArgM even if only Arg0
from Arg4 and ArgM contain enough training/testing
2This is a basic method to pass from binary categorization
into a multi-class categorization problem; several optimization
have been proposed, e.g. (Goh et al, 2001).
3We point out that we removed from the Penn TreeBank the
special tags of noun phrases like Subj and TMP as parsers usu-
ally are not able to provide this information.
data to affect the global performance.
The classifier evaluations were carried out using
the SVM-light software (Joachims, 1999) available at
http://svmlight.joachims.org/ with the de-
fault polynomial kernel according to a degree d ?
{1, 2, 3, 4, 5}. The performances were evaluated using
the F1 measure for both single argument classifiers and
the multi-class classifier.
- PHRASE TYPE (pt): This feature indicates the syntactic
type of the phrase labeled as a predicate argument.
- PARSE TREE PATH (path): This feature contains the path
in the parse tree between the predicate phrase and the argu-
ment phrase, expressed as a sequence of nonterminal labels
linked by direction (up or down).
- POSITION (pos) Indicates if the constituent appears be-
fore or after the predicate in the sentence.
- VOICE (voice) This feature distinguishes between active
or passive voice for the predicate phrase.
- HEAD WORD (hw) This feature contains the head word
of the evaluated phrase. Case and morphological informa-
tion are preserved.
- GOVERNING CATEGORY (gov) This feature applies to
noun phrases only, and it indicates if the NP is dominated by
a sentence phrase (typical for subject arguments with active
voice predicates), or by a verb phrase (typical for object
arguments).
- PREDICATE WORD In our implementation this feature
consists of two components: (1) VERB: the word itself
with the case and morphological information preserved; and
(2) LEMMA which represents the verb normalized to lower
case and infinitive form.
Table 2: Standard Features used in Predicate Argument
Extraction.
Figure 6 illustrates the F1 measures for the overall ar-
gument extraction task (i.e. identification and classifica-
tion) according to different polynomial degrees. Figure
6(a) illustrates the F1-performance of single classifiers
for the arguments Arg0, Arg1 and ArgM. Figure 6(b) il-
lustrates the performance for all the arguments (i.e. the
multi-classifier). In general, we were able to recognize
predicate argument structures with an F1-score of 80%.
4.3 Using Predicate-Argument Structures in
Question Answering.
Predicate-argument structures are useful for identifying
candidate answers. Since they recognize long-distance
dependencies between a predicate and one its arguments,
they enable (1) the identification of the exact boundaries
of an answer; and (2) they unify the predicate-argument
relation sought by question with those recognized in can-
didate passages.
Moreover, they are very useful in situations when the
expected answer type of the question could not be recog-
nized. There are two causes when the expected answer
type cannot be identified:
Case1: the answer class is a name that cannot be correctly
classified by an available Named Entity Recognizer, be-
cause its class name is not encoded.
Case2: the answer class cannot be found in the Answer
Type hierarchy. The example from Figure 7 shows an in-
stance of case 1. In this figure, the TREC question Q2054
has a predicate that can be unified with PREDICATES
from the answer passage. The Arg1 of the predicate is
the expected answer, which is identified as ?the Declara-
tion of Independence?. The Arg0 in the question is But-
ton Gwinnett, whereas in the answer, it is underspecified,
and should be resolved to who. This relative pronoun has
Button Gwinnett as one of its antecedents.
In Figure 8 the second case is illustrated. The question
asked about the first argument of the predicate ?measure?,
when its Arg2 = ?a theodolite?. In the answer, Predicate
2, with its infinite form, has as Arg 2 the same ?theodo-
lite?. However, the predicates are lexicalized by different
verbs. In WordNet, the first sense of the verb ?measure?
as the verb ?determine? as a hypernym, therefore Arg1 =
?wind speeds? is the correct answer.
5 Intentional Structures
The correct interpretation of many questions requires
the inference of implicit information, that is not directly
stated in the question, but merely implied. The mecha-
nisms of recognizing the intentions of the questioner are
helpful means of identifying the implied information. For
example, in the question QI :?Will Prime Minister Mori
survive the crisis??, the user does not literally mean ?Will
Prime Minister Mori be still alive when the political cri-
sis is over ??, but rather (s)he implies her/his belief that
the current political crisis might cost the Japanese Prime
Minister his job. It is very unlikely that any expert knowl-
edge base covering Japanese politics will encode knowl-
edge covering all situations of political crisis and the pos-
sible outcomes of the prime minister. However, this prag-
matic knowledge is essential for the correct interpretation
of the question.
Q2054:
Answer: Button Gwinnett, George Walton and Lyman Hall were
Georgians who could have been hanged as traitors for
What document did Button Gwinnett sign on the upper left
hand side?
signing the Declaration of Independence on July 4, 1776.
Predicate?argument structure:
PREDICATE1: were
ARG1(PREDICATE1): Georgians
who
PREDICATE2: could have been hanged
ARG2(PREDICATE2): as traitors
PREDICATE3: signing
ARG1(PREDICATE3): The Declaration of Independence
ARGM?LOC(PREDICATE3): on July 4, 1776
ARG0: Button Gwinnett, George Walton and Lyman Hall
ARGM?LOC: on the upper left hand side
ARG0: Button Gwinnett
PREDICATE: sign
ARG1: What document Question Type
Predicate?argument structure:
Figure 7: Answer extraction from predicate-argument struc-
tures: Case1
measurePREDICATE:
What does a theodolite measure?
Predicate?argument structure:
ARG1: What
ARG2: a theodolite
Answer: The theodolite ? a 1940s gadget, no longer in production,
wind speeds.
that uses a helium balloon and trigonometry to determine
Predicate?argument structure:
a 1940s gadget
that
PREDICATE1: uses
PREDICATE2: to determine
ARG1(PREDICATE2): wind speeds
Q2145:
ARG1(PREDICATE1): The theodolite
ARG2(PREDICATE1): a helium balloon and trigonometry
Figure 8: Answer extraction from predicate-argument struc-
tures: Case 2
The design of advanced Question&Answering systems
capable of grasping the intention of a professional analyst
when (s)he poses a question depends both on the knowl-
edge of the domain referred by the question as well as on
a variety of rules and conventions that allow the commu-
nication of intentions and beliefs in addition to the literary
meaning of the question. Access to domain knowledge is
granted by a combination of retrieval mechanisms that
bring forward relevant document passages from unstruc-
tured collections of documents, specialized knowledge
Text Information RetrievalEngine
QUERY: Prime & Minister & Mori &DANGER (word)
Japanese Factual PoliticsKnowledge Base
Text Retrieval
"vote of non-confidence against 
Prime Minister Mori"
political crisis
survive crisis
adversity DANGER
WordNet 1.6
resignation
removal
strike
vote
vote of non-confidence =
   DANGER(Position)
continue in existence
Question: Will Prime Minister Mori survive the political crisis ?
DANGER ( Prime Minister Mori continues in Position)
Figure 9: Intentional Structure derived from Lexico-Semantic Knowledge.
bases and/or database access mechanisms. The research
proposed in this project focuses on the derivation and us-
age of pragmatic knowledge that supports the recognition
of question implications, also known as implicatures (cf.
(Grice, 1975b)).
5.1 Intentional structures Derived from
Lexico-Semantic Knowledge
The novel idea of this research is to link computa-
tional implicatures, similar to those defined by Grice
(Grice, 1975b), to inferences that can be drawn from
general lexico-semantic knowledge bases such as Word-
Net of FrameNet. Incipient work was described in
(Sanda Harabagiu and Yukawa, 1996), where a method
of using lexico-semantic path for recognizing textual im-
plicatures was presented. To our knowledge, this is the
only computational model of implicatures that was de-
veloped and tested on a large lexico-semantic knowledge
base (e.g. WordNet), enabling successful recognition of
implicatures.
The model proposed in (Sanda Harabagiu and Yukawa,
1996) uncovered a relationship between (a) the coherence
of a text segment; (b) its cohesion expressed by the lexical
paths and (c) the implicatures that can be drawn, mostly
to account for pragmatic knowledge. This relationship
can be extended across documents and across topics, to
learn patterns of textual and Q&A implicatures and the
methods of deriving knowledge that enables their recog-
nition.
The derivation of pragmatic knowledge combines in-
formation from three different sources:
(1) lexical knowledge bases (e.g. WordNet),
(2) expert knowledge bases that can be rapidly formatted
for many domains (e.g. Japanese political knowledge);
and
(3) knowledge supported from the textual information
available from documents. The methodology of combin-
ing these three sources of information is novel.
For question QI , the starting point is the concept iden-
tified as a cue for the expected answer type through meth-
ods described in (Harabagiu et al, 2000). This con-
cept is lexicalized by the verb-object pair survive-crisis.
Verb survive has four distinct senses in the WordNet 1.6
database, whereas noun crisis has two senses. The poly-
semy of the expected answer type increases the difficulty
of the derivation of pragmatic knowledge, but it does not
presupposes the word sense disambiguation of the ex-
pression. The information available in the glosses defin-
ing the WordNet synsets provides helpful information for
expanding the multi-word term defining the expected an-
swer type. By measuring the similarity between the two
senses of the noun crisis and the words encountered as
objects or prepositional attachments in the glosses of the
various senses of the verb survive, we distinguish the
noun adversity and the example cancer as expressing the
closest semantic orientation to the first sense of noun cri-
sis. The similarity is measured by counting the number
of common hypernyms and gloss concepts of hypernyms
of two synsets. Figure 9 illustrates the concepts related
to the question QI , as derived from WordNet lexico-
semantic knowledge base.
The fact that surviving a political crisis has a dangerous
component, indicated by the noun adversity, may also be
supported by inferences drawn from an expert knowledge
base, showing that a political crisis may be dangerous for
political figures in power. However, at this point, the ob-
ject of the dangerous situation is not specified. But sev-
eral concepts indicating dangerous political situations can
be inferred from the expert knowledge base and used in
the query for text evidence. Only when text passages in-
volving Prime Minister Mori are retrieved, clarifications
of the situation are brought to attention: a vote of non-
confidence against the prime minister is considered. This
new information helps inference from the expert knowl-
edge base. The expert knowledge base modeling the
 Intentional Structure of Questions
0* Evidence (     1-possess      (          2-Iraq,      3-biological weapons   ))
4* Means of finding (0)
a. reports 
b. inspections
c. assessments ? patterns of inspections
5* Source (0)
a. authority
b. reliability ? may, would
6* Consequence (0)
a. Enablement
b. Hiding/Presenting finding evidence 
Question: Does Iraq   have biological weapons  ?
x                           y
: have( Iraq, biological weapons )
Predicate-
Argument
Structure
Question
pattern
Does x have y ?
possess (x, y) 
Topic (3)
biological  weapons
a. Types of topic
b. Components
- chemical agents
- mustard gas, VX, sarin
c. Usage
- rockets, artillery shells
a. discover(1,2,3)
b. stockpile(2,3)
c. use(2,3)
d. 1-possess
a. develop(2,3)
b. acquire(2,3)
a. inspections( _,2,3)
b. ban( _,2,3)
Source/      fact/          reliability
reporter     evidence
5.a              0              5.b
coercion
Structure
Figure 10: Intentional structure derived from predicate-argument structures.
Japanese factional politics confirms that this is a danger-
ous situation for the Prime Minister and that in fact his
position is in jeopardy. Due to this inference from the
expert knowledge base, the concept POSITION replaces
noun existence from the gloss of the second sense of verb
survive, and the pragmatic knowledge required for the in-
terpretation of the implicature is assembled:
The interactions between the three information sources
derives the pragmatic knowledge on which relies the im-
plication of the question. The user had an inherent belief
that Prime Minister Mori might be replaced, and (s)he
queries the Q&A system not only to find information but
also to find support for his/her belief. The intentional
structure is represented as a set of concepts and the re-
lations that span them, as illustrated in Figure 9.
5.2 Coercion of Intentions
A second method of deriving the intentional structure of a
question is based on the predicate-argument structure that
is derived from the question and the candidate answers.
Figure 10 illustrates the Intentional Structure of one
such question. The structure of the intentions is deter-
mined by the predicate-argument structure of the ques-
tion and by its pattern. Generally, when asking whether
X posses Y, we want to find (1) evidence of this fact;
(2) we explore different means of finding the informa-
tion; (3) we are interested in the source of information
and (4) the enablers or inhibitors of finding the informa-
tion as well as the consequences of knowing it are of in-
terest. We assign a different index to each object from
the predicate-argument structure, and do the same for
each element of the intentional structure. For instance,
in Figure 2, source(0) is interpreted as source(index=0)
= source(evidence). Another feature of the intentional
structure is determined by the coercions that are associ-
ated with both forms of indexed objects. For example,
the coercion of evidence shows the most typical ways
of finding evidence in the context of the topic of the
question. Figure 2 lists such possibilities as (a) discov-
ering, (b) stockpiling, (c) using and even (d) possess-
ing. These possibilities are inserted in the context of the
topic, since they make use of the indexes for associat-
ing meaning to their representations. In fact, option (a)
discover(1,2,3) reads as discover(index=1, index=2, in-
dex=3) =discover(possesses(Iraq, biological weapons)).
Whereas option (b) stockpile(2,3) can be similarly inter-
preted as stockpile(Iraq, biological weapons). Note that
one of the indexed objects is the topic. The structure of
the topic is define along three semantic dimensions: (1)
hyponyms or examples of other types of the same cate-
gory as the topic; (2) the meronyms or components; and
(3) the functionality or the usage. The derivation of such
a large set of intentional structures helped us learn how
to coerce pragmatic knowledge. We have developed a
probabilistic approach extending the metonymy work of
(Lapata and Lascarides, 2003).
Lapata and Lascarides report a model of interpretation
of verbal metonymy as the point distribution P (e, o, v) of
three variables: the metonymy verb v, its object, and the
sought after interpretation i. For example a verb ? ob-
ject relation that needs to be metonymycally interpreted,
is enjoy ? movie. In this case v = enjoy, o = movie
and i ? {making, watching, directing}. The variables
of the distribution re ordered as <i, v, o> to help factor-
ing P (i, v, o) = P (i) ? P (v|i) ? P (o|i, v). Each of the
probabilities P (i), P (v|i) and P (o|i, v) can be estimated
using maximum likelihood. As it is illustrated in Fig-
ure 10, we have extended this model to account for: (1)
coercion of topic information; (2) coercion of evidence
of a fact; (3) interpretation of predicate and (4) inter-
pretation of arguments. Since the verb ? object rela-
tion translates in one of the predicate-argument relations,
we have coerced the predicate interpretations in the same
way as (Lapata and Lascarides, 2003), but we allowed
for any predicate-argument relation. Argument coercions
were produced by searching the most likely predicates
that used the same arguments. The topic model also in-
corporated topic signatures, similar to these reported in
(E.H. Hovy and Ravichandran, 2002).
6 Conclusions
In this paper we have described the problem of interpret-
ing the question intentions and proposed two methods of
generating the intentional structure of questions. The first
method is based on lexico-semantic chains between con-
cepts that are related to the question. The second method
generates intentional structures by using the predicate-
argument structures of questions and the topic represen-
tation of questions. To derive both forms of intentional
structures, we have relied on information available from
WordNet and on the parsing of questions and answers
in predicate-argument structures. Our experiments show
that the intentional structure may determine a different in-
terpretation of the question, and thus different keywords
can be used to retrieve the answers. Answer extraction
also depends on the semantic relations between the co-
erced interpretations of predicates and arguments. By
selecting a set of 100 questions for test, we have eval-
uated the correctness of the extracted answers when (1)
no intentional knowledge was coerced; (2) implicatures
were derived from lexico-semantic knowledge and (3)
intentional structures were derived based on predicate-
argument structures. An increase of 8structures and one
of 22the impact of each element of the intentional struc-
ture on the Q/A processing.
References
John Chen and Owen Rambow. 2003. Use of deep lin-
guistic features for the recognition and labeling of se-
mantic arguments. In Proceedings of the 2003 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In Pro-
ceedings of the 41st Annual Meeting of the ACL, Sap-
poro, Japan.
Chin-Yew Lin E.H. Hovy, U. Hermjakob and Deepak
Ravichandran. 2002. Using knowledge to facilitate
pinpointing of factoid answers. In Proceedings of the
19th International Conference on Computational Lin-
guistics (COLING 2002).
Daniel Gildea and Julia Hockenmaier. 2003. Identifying
semantic roles using combinatory categorial grammar.
In Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistic,
28(3):254?288.
Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition. In
Proceedings of the 40th Annual Conference of the
Association for Computational Linguistics (ACL-02),
Philadelphia, PA.
King-Shy Goh, Edward Chang, and Kwang-Ting Cheng.
2001. SVM binary classifier ensembles for image clas-
sification. In Proceedings of the tenth international
conference on Information and knowledge manage-
ment, pages 395?402.
Paul H. Grice. 1975a. Logic and conversation. In P. Cole
and New York J.L. Morgan (ed.), Academic Press, edi-
tors, Syntax and Sematics Vol.3:Speech Acts, pages 41?
58.
Paul J. Grice. 1975b. Syntax and Semantics Vol.3:Speech
Acts. P. Cole and J. Morgan, editors.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, Jim Mar-
tin, and Dan Jurafsky. 2003. Shallow semantic parsing
using support vector machines. Technical report.
Sanda Harabagiu, Marius Pas?ca, and Steven Maiorano.
2000. Experiments with open-domain textual question
answering. In Proceedings of the 18th International
Conference on Computational Linguistics (COLING-
2000), pages 292?298, Saarbrucken, Germany,.
Sanda M. Harabagiu, Dan I. Moldovan, Marius Pasca,
Rada Mihalcea, Mihai Surdeanu, Razvan C. Bunescu,
Roxana Girju, Vasile Rus, and Paul Morarescu. 2001.
The role of lexico-semantic feedback in open-domain
textual question-answering. In Meeting of the ACL,
pages 274?281.
S. Harabagiu, D. Moldovan, C. Clark, M. Bodwen,
J. Williams, and J. Bensley. 2003. Answer mining by
combining extraction techniques with abductive rea-
soning. In Notebook of the Twelveth Text REtrieval
Converence (TREC-2003), pages 46?53.
A. Ittycheriah, M. Franz, W. Zhu, and A. Ratnaparkhi.
2000. IBM?s statistical question answering system.
In Proceedings of the 9th Text REtrieval Conference,
Gaithersburg, MD.
T. Joachims. 1999. T. Joachims, Making large-Scale
SVM Learning Practical. In B. Scho?lkopf and C.
Burges and A. Smola (ed.), MIT-Press., editor, Ad-
vances in Kernel Methods - Support Vector Learning.
Maria Lapata and Alex Lascarides. 2003. A probabilistic
account of logical metonymy. Computational Linguis-
tics, 29:2:263?317.
Wendy Lehnert. 1978. The process of question answer-
ing. In Lawrence Erlbaum Assoc., Hillsdale.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H.
Martin, and Daniel Jurafsky. 2003. Semantic role
parsing: Adding semantic structure to unstructured
text. In Proceedings of the International Conference
on Data Mining (ICDM-2003).
Dan Moldovan Sanda Harabagiu and Takashi Yukawa.
1996. Testing gricean constraints on a wordnet-based
coherence evaluation system. In Working Notes of the
AAAI-96 Spring Symposium on Computational Impli-
cature, Stanford, CA.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of the
41th Annual Conference of the Association for Compu-
tational Linguistics (ACL-03), pages 8?15.
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer.
Experiments with Open-Domain Textual Question Answering 
Sanda M. Harabagiu and Marius A. Pa~ca and Steven J. Maiorano 
Department of Computer Science and Engineering 
Southern Methodist University 
Dallas, TX 75275-0122 
{sanda, marius, steve}@renoir, seas. smu. edu 
Abstract 
This paper describes the integration of several 
knowledge-based natural language processing tech- 
niques into a Question Answering system, capable 
of mining textual answers from large collections of 
texts. Surprizing quality is achieved when several 
lightweight knowledge-based NLP techniques com- 
I)lement mostly shallow, surface-based approaclms. 
1 Background 
The last decade has witnessed great advances and 
interest in the area of Information Extraction (IE) 
fi'om real-world texts. Systems that participated in 
the TIPSTER MUC competitions have been quite sue- 
cessflll at extracting information from newswire rues- 
sages and filling templates with inforInation pertain- 
ing to events or situations of interest. Typically, the 
templates model queries regarding who did what to 
wh, om, when and where, and eventually why. 
Recently, a new trend in information processing 
from texts has emerged. Textual Question Answer- 
ing (Q/A) aims at; identit~ying the answer of a ques- 
tion in large collections of on-line documents. In- 
stead of extracting all events of interest and their re- 
lated entities, a Q/A system higtflights only a short 
piece of text, accounting for the answer. Moreover, 
questions are expressed in natural anguage, are not 
constrained to a specific domain and are not limited 
to the six question types sought by IE systems (i.e. 
wh, ol (lid what.2 to whoma, when4 and wh, eres, and 
eventually whya). 
In open-domain Q/A systems, the finite-state 
technology and domain knowledge that made IE sys- 
tems successful are replaced by a combination of (1) 
kuowledge-based question processing, (2) new forms 
of text indexing and (3) lightweight abduction of 
queries. More generally, these systems coml)ine cre- 
atively components of tile NLP basic research ill- 
frastructure developed in the 80s (e.g. the compu- 
tational theory of Q/A reported in (Lehnert 1978) 
and tim theory of abductive interpretation of texts 
reported in (Hobbs et a1.1993)) with other shallow 
teclmiques that make possil)le the open-domain pro- 
cessing on real-world texts. 
Tim idea of building open-domain Q/A systems 
that perform on real-world ocument collections was 
initiated by the eighth Text REtrieval Conference 
(TREC-8), by organizing the first competition of an- 
swering fact-based questions uch as "Who came up 
with the name, El Nino?'. Resisting the tempta- 
tion of merely porting and integrating existing IE 
and IR technologies into Q/A systems, the develop- 
ers of the TREC Q/A systems have not only shalmd 
new processing methods, but also inspired new re- 
search in the challenging iutegration of surface-text- 
based methods with knowledge-based text inference. 
In particular, two clear knowledge processing needs 
are presented: (1) capturing the semantics of open- 
domain questions and (2) justifying the correctness 
of answers. 
In this paper, we present our experiments with 
integrating knowledge-based NLP with shallow pro- 
cessing techniques for these two aspects of Q/A. Our 
research was motivated by the need to enhance the 
precision of an implemented Q/A system and by the 
requirement to preI)are it for scaling to more com- 
plex questions than those t)resented in the TREC 
competition. In the remaining of the paper, we de- 
scribe a Q/A architecture that allows the integra- 
tion of knowledge-based NLP processing with shal- 
low processing and we detail tlmir interactions. Sec- 
tion 2 presents the flmctionality of several knowledge 
processing modules and describes tile NLP tech- 
niques for question and answer processing. Section 
3 explains the semantic and logical interactions of 
processing questions and answers whereas Sectiou 
4 higlllights the inference aspects that inlplement 
the justification option of a Q/A system. Section 
5 presents the results and the evaluations whereas 
Section 6 concludes tim paper. 
2 The NLP Techniques 
Surprising quality for open-donlain textual Q/A can 
be achieved when several lightweight knowledge- 
based NLP techniques eomt)lenmnt mostly shallow, 
surface-based approaches. The processing imposed 
by Q/A systems must be distinguished, oi1 the one 
band, from IR techniques, that locate sets of doc- 
292 
, :  : , .  
Queslion l Ioeumcnls Answer(s) 
Question ' Qul , ~._ k~\[ Semantic Logi 
~ ~  l,"LTransfimnati?n.- )j~J ('rransI 
, - \ [  - (Question ~ 
/ \ / \ / \  ..... " ' /  c/,-,.,..,. \ [!  
/ Ot, estion \ ~ = g ~  
Taxonomics ~/Expanded 
/ V ~.~ Expanded Word Classes \[ Question \ ] ) -  
f-~-:~2q ._1,'~ Expansion 
I / / P ~ ~ ? \ \  k.~J.j~J qSxpandcd 
Knowledge-lhrwd OuestioH l'roces,dng 
) 
tW 
Shallow Document l'roces.d~tg Kmm,ledge-Based Answer l'rocessing 
Figure 1: An architecture tbr knowh;dge-l)ased Question/Answering 
uments ('ontaining the required information, based 
on keywords tech, niques. Q/A systems are presented 
with natural anguage questions, far richer in seman- 
tics than a set of keywords eventually st, ru('flured 
around some, oi)erators. Ihnthernmre, the outtlut 
of Q/A systems is either the actual answer identi- 
fied in a text or small text; fragments containing the 
answer. This eliminates the user's trouble of tind- 
ing the required inibrnlation in sometime.s large sets 
of retrieved o(-uments. Op(m-donmin Q/A systems 
must also l)e distinguished, on the other hand, h'om 
IE syst(;ms that model the inforlnation eed through 
(latal)as(; t(;mt)lates , thus less naturally than a tex- 
tual answer. Moreovei', open-domain IE is still ditli- 
(:ult to achieve, beeause its linguistic t)atCern re(:og- 
nition relies on domain-dependent lexico-semantie 
knowledge. 
To t)e able to satisf~y tile ol)en-donmin constraints, 
textual Q/A systems replace the linguistic pattern 
matching capabilities of IE systems with methods 
that rely (m the recognitioil of tile question type and 
of the e.'rpectcd answer type. Generally, this informa- 
tion is available by accessing a classification based 
on the question stem (i.e. what, how much, who) 
and the head of the first nOml phrase of the ques- 
ti(m. Question 1)rocessing also includes the identifi- 
cation of the question keywords. Empirical methods, 
based on a set of ordered heuristics ot)erating on the 
phrasal parse of the question, extract keywords that 
are passed to the search engine. The overall preci- 
sion of tile Q/A system depends also on th(, recogni- 
tion of the question focus, since the answer extrac- 
tion, suet:ceding the IR phase, is centered around 
the question focus. Unl})rtmmtely, eml)irical ninth- 
ods fl)r t'oeus recognition are hard to develop without 
the availability of richer semantic knowledge. 
S1)eeial requir(nnents are set Oil the documeid; pro- 
(:essing COml)Olmnt of a Q/A system. To speed-u l) 
the answer extraction, the search engine returns only 
those 1)aragrai)hs from a document that contain all 
queried keywords. The paragraphs are ordered to 
1)roInote the, eases when the keywords not only art; as 
close as 1)ossibh~', lint also t)reserve the syntactic de- 
1)enden(:ies re(:ognized in the question. Answers are 
('.xtra(:ted whenever the question topic and the m> 
swer tyI)e are recognized iil a 1)aragraph. Thereafl;er 
the answers :/1(; scored 1)ased on several bag-of-words 
hem'isties. Throughout all this 1)roeessing, the NLP 
te(:hniques are limited to (21) named entity recogni- 
tion; (b) semantic lassification of the question tyt)e, 
l/ased oil information 1)rovided by an off-line ques- 
tion taxonon ly  21.i1(t senmntic lass intbrmation avail- 
able from WordNet (Felll)mml 1998); mid (c) phrasal 
parsing produced by enhancing Brill's part-of-sl)eech 
tagger with some rules tbr phrase tbrmation. 
Ilowever simt/le, this technology surl)asses 75% 
precision on trivia questions, as posed in the TREC- 
8 (:ompetit ion (of. (Moldovan et al1999)). An im- 
pressive improvenmnt of 14% is achieved when more 
knowledge-intensive NLP techniques are ai)plied a.t 
both question and answer processing level. Figure 1 
illustrates the architecture of a system that has en- 
hanced Q/A performance. 
As represented in Figure 1, all three modules 
of the Q/A system preserve the shallow processing 
eomi/onents that determine good t)erformanee. In 
t;t1(', Quest, ion Processing module, the Question Class 
re(:ognizer, working against a taxonomy of questions, 
293 
still constitutes the central processing that takes 
place at this stage. However, a far richer representa- 
tion of the quest;ion classes is employed. To be able 
to classify against the new question taxonomy each 
question is first flflly parsed and transfommd into 
a semantic representation that captures all relation- 
ships between I)hrase heads. 
The recognition of the question class is based on 
the comparison of the question smnantic representa- 
tion with the semantic representation of the nodes 
from tlm question taxonomy. Taxonomy nodes en- 
code also the answer type, the question focus and 
the semantic lass of question keywords. Multiple 
sets of keywords are generated based on their seman- 
tic class, all pertaining to the stone original ques- 
tion. This thature enables the search engine to re- 
trieve multiple sets of documents, pertaining to mul- 
tit)le sets of answers, that are extracted, combined 
and ranked based on several heuristics, reported in 
(Moklovan et a1.1999). This process of obtaining 
multiple sets of answers increases l;he likelihood of 
finding the correct answer. 
However, the big boost in the precision of the 
knowledge-based Q/A system is provided by the op- 
tion of enabling the justification of the extracted an- 
swer. All extracted answers are parsed and trans- 
formed in semantic representations. Thereafter, 
both semantic transformations for questions and an- 
swers are translated into logic forms and presented 
to a simplified theoreln prover. The proof back- 
chains Dora the question to the answer, its trace 
generating a justification. The prover may access 
a set of abduction rules that relax the justification 
process. Whenever an answer cmmot l)e 1)roven, it 
is discarded. This option solves multiple situations 
when the correct answer is not ranked as the first 
return, due to stronger surface-text-based in icators 
in some other answers, which unfortunately are not 
correct. 
This architecture allows for simple integration of 
semantic and axiomatic knowledge sources in a Q/A 
system and determines efficient interaction of text- 
surface-based and knowledge-based NLP techniques. 
3 In teract ions  
Three main interactions between text-surface-based 
and knowledge-based NLP techniques are designed 
in our Q/A architecture: 
1. When multiple sets of question keywords are passed 
to the search engine, increasing the chance of find- 
ing the text paragraph containing the answer. 
2. When the question focus and the answer type, re- 
suiting from the knowledge-based processing of the 
question, are used in the extraction of the answer, 
based on several empirical scores. 
3. When the justification option of the Q/A system is 
available. Instead of returning answers cored by 
some empirical measures, a proof of the correctness 
of the answer is produced, by accessing the logical 
transformations of the question and the answer, as 
well as axioms encoding world knowledge. 
All these interactions depend on two Nctors: (1) 
the l;ransformations of the question or answer into 
semantic or logical representations; and (2) the avail- 
ability of knowledge resources, e.g. the question tax- 
onomy and the world knowledge axioms. The avail- 
ability of new, high-performace parsers that operate 
on real world texts determines the transformation 
into semantic and logic formulae quite simple. In ad- 
dition, the acquisition of question taxonomies i alle- 
viated by machine learning techlfiques inspired from 
bootstrapping methods that learn linguistic patterns 
and semantic dictionaries for IE (of. (Riloff and 
Jones, 1999)). World knowledge axioms can also be 
easily derived by processing the gloss (lefinitions of 
WordNel; (Fellbaunl 1998). 
a.1 Semant ic  and  Logic  T rans format ions  
Semant ic  T ranstbrmat ions  
Instead of t)roducing only a phrasal parse for the 
question and answer, we lnake use of one of the new 
statistical parsers of large real-world text coverage 
(Collins, 1996). The parse trees produced by such a 
parser can be easily translated into a seinantic repre- 
sentation that (1) comprises all the phrase beads and 
(2) captures their int,er-relationships by anonymous 
links. Figure 2 illustrates both the I)arse tree and 
the associated semantic representation f a TI{EC-8 
question. 
Question: Why did I)avid Koresh ask the FBI for a word processor? 
Parse: 
SBAP.Q 
-- - . . . . .  SQ 
/ //------ v,, 
/ /  / /  4 -  - -~  --Pl' 
i / / / \  / 
WRB VBI) NNP NN\[' VB l)T NNI' IN DT NN NN 
I I I I I I I I I l I 
Why did David Koresh ask the Fill for a word processor 
Semantic representation: 
~ a s k  
word ~ . .  
REASON I )av id%/Y  - - -~"~\  processor 
Koresh FBI 
Figure 2: Question semantic transformation 
The actual transformation i to semantic repre- 
sentation of a question or an answer is obtained 
as a by-product of the parse tree traversal. Ini- 
tially, all leaves of the parse tree are classified as 
294 
.@@nodes or no'n-skipnodes. All n(mns, non-auxiliary 
verbs, adjectives a.nd adverl)s are categorized as 
non-skitmodes. All the other h~aves are skipnodes. 
Bottom-u 1) trav('.rsal of tim 1)arse tree (:ntails tlm 
t)roi)agation of leaf labels wh('amver l;hc 1)arcnt nod(; 
has more than one non-skipnod(; child. A rule based 
on the syntactic ategory (it' th(.' father selects one of 
the childr(m to 1)ropagatc its label a,t the next level 
in the tree. The winning node will then be consid- 
ered linked to all the other fornmr siblings thai; al'e 
non-skilmodes. The prot)agation (:ontimms mltil the 
l)arse 1;l'(~.c root receives a label, and thus a scmanti(" 
gral)h is (:rc;tl;(;(l as a 1)y-1)rodu(:t. Part of th('. la- 
bel i)roI)agation, we also (:onsider that whenever all 
('hildr(;n of a non-terminal are skilmo(l(;,% the parent; 
becomes a. skipnode as well. 
Figure 3 rel)r(~'s(;nts the lal)el I)rOl)agation for th(; 
1)arse tree of the question l'el)res(mt(M in Figure 2. 
The labels of Korcsh, ask, FB1 and processor are 
l)rot)agated to tlw, next level. This entails t;hat Ko- 
r'c.s'h is linked to David, ask to I,'BI and procc',s'sor mM 
procc, ssor to word. As a.@ be(:(/m(!s the lab(:l of th(; 
tree to(it, it is a.lso linked to I~ds'A,9ON, the qu(~sti()n 
type (l('~I;(n'lnin(',d l)y tlm question st('m: wh, y. The 
lal)el 1)rot)agation rules are id(mtical t<) the rules fl)r 
mapping from trees to d(~t)(m(hm(:y s\[.rlt(:l;lll.'cs llSC,(l 
my Mi('ha(,q Collins (of. (Collins, 199(5)). These rules 
i(lenti\[}, the head-child, and pl'Ol)agatc its label up 
in the tree. 
/ _  > 
( /  vV (>k) 
wl lm)w'  /~  Nl' (K,,,,,~IO' ' ' i , / '  NI' \[ I:IH,I 
/ / ~ ~ / / \ / ' \ 
WRB VBI) NNP NNP , ', VII I)T NNP, IN I)T NN NN ) 
I I I I / ' ,  I I I~" I I I I ,' 
Wily did l)avid Korcsh ask file \[:BI for a word lUl)ccssiw 
Figur(' .  3: \])~II'S(~ t,l'(~.(, ~ l;ra, v(',rsa\[ 
I'l'( processor ! -  
/ /  NI~ I)rOCCssor I / , . 
Logical  T rans format ions  
The logical formulae in whi(:h questions or answers 
arc translated are inspired \[)y the notal;i(m prol/os('.d 
in (ltobbs, 1.986-1.) and implemented in TACITUS 
(ttobbs, 1986-2). 
Based on the davidsonian tl"eatmenl; of action sen- 
|;(?llC(;S, in which events are tr(~at, ed as individuals, 
every question and every answer are transform('xl in
a tirst-order t)redicatc tbrmula for which (1) verbs 
are mapped in 1)red\[cares vcvb(e,x,y,z,. . .)  with the. 
(:onvention thai; variable e rel)res(;nts the evc'nl;ual- 
i(;y of that acl;ion or even(; (;o take place, wh('a'eas (;lm 
othcu arguments (e.g. z, y, z, ...) repr(~,s(mt l;lm t)rcd- 
icate argmnents of the verb; (2) nouns arc mal)l)ed 
into their lexicalized predicates; raM, (3) mo(lificws 
have the same argument as the predicate they mod- 
i\[y. For  (;Xalnpl('~, l;he qu(~si;ion i l l us t ra ted  in F igure  2 
has  l;he fo l low ing  log ica l  fo rn l  t rans for lnat ion  (LFT)  : 
\[REASON (x) ~David (y) ~Koresh (y) ~ask (e, x, y, z, p) 
~FBI (z) {qproces sot (p) ~word (p) \] 
The process of trmlslat;ing a sema.ntic ret)resenta.- 
tion into a logic form has the following steps: 
1. For each node in the semantic rcprcscntation, 
create a prcdicatc with a distinct argument. 
2.a. If  it noun and (tit ad, jective predicate arc linlccd 
tht:y should have, the sam(" argument. 
2.b. Tltc .qamc fin" verbs and adverbs, pairs of lwuns 
or an adjective and an advcrb. 
3. l,b'r each verb predicate, add aT~lumcnts corrc.sponding 
to each predicate to which it is directly 
linked in the semantic representation. 
Predicate argunmnts (:an be identified because tim 
soma,hi ; i t  l'ol)res(.~nl;al;ion us ing  &no l lymous  re la t ions  
represenl;s uniformly adjuncts mM thematic roles. 
Ilowevel:, sl;e 1) 2 of the l;l"mtsl~d;ion procedure l'eCOg- 
nixes the adjuncts, making predicate argmnenl;s the 
remaining (:()nn(~(:tions of tlm v(n'}) in t;}l(, ~semal~I;ic 
rq)resentation. 
3.2 Quest ion  Taxonomy 
The question taxonomy re l )rcscnl ;s  each  question 
nod(', as a qu in tup le :  (1) a setnan( ; ic  rcpr(~s(!ni;at ion 
of a qucsLion; (2) th(; question type; (3) the m~swer 
typ(,; (4) the question focus an,l (5) the question 
keywoMs. By using over 1500 questions provided by 
\]lcme(lia, as well as otlmr 2000 quest;ions retrieved 
from FAQFinder, we have b(!en abh ~. to learn classiti- 
cation rules mM buihl a (:oinplex question taxonomy. 
\ ..... , \[ \ ] I1SI I I I ICe I 
,,/- ;,,,\__ 
// ,, 
.2_ 
\[Artwo,k jl LI.ocation 1 
Figure 4: A snapshot of the top Question Taxonomy 
Initially, we stm'tcd with a seed hit;rarchy of 25 
question classes, manually built, in which all the se- 
mantic classes of the nodes fl'om the semantic repre- 
sentations were decided oil-line, by a hmnan expert. 
300 questions were processed to create this seed hi- 
erarchy. Figure 4 illustrates some of the nodes of 
295 
the top of this hierarchy. Later, as 500 more ques- 
tions were considered, we started classifying them 
semi-automatically, using the following two steps: 
(1) first a hmnan wonld decide the semantic lass of 
each node in the semantic representation f the new 
question; (2) then a classification procedure would 
decide whether the question belongs to one of the 
existing classes or a new class should be considered. 
To be able to classify a new question in one of the ex- 
isting question classes, two conditions must be satis- 
fled: (a) all nodes fi'om the taxonomy question must 
correspond to new question nodes with the same 
semantic lasses; and (b) unifyable nodes must be 
linked in the same way in both representations. The 
hierarchy grew to 68 question nodes. 
Later, 2700 more questions were classified fully 
automatically. To decide the semantic lasses of the 
nodes, we used the WordNet semantic hierarchies, 
by simply assigning to each semantic representation 
node the same class as that of any other question 
term from its WordNet hiera.rchy. 
The semantic representation, having the same for- 
mat for questions and answers, is a case fi'ame with 
anonymous relations, that allows the unification of 
the answer to the question regardless of the case re- 
lation. Figure 5 illustrates tbur nodes fi'om the ques- 
tion taxonomy, two for the "currency" question tyI)e 
attd two for the "person name" question type. The 
Figure also represents the mappings of four TREC- 
8 questions in these hier~rchy nodes. The mappings 
are represented by dashed arcs. In this Figm'c, the 
nodes front the semantic representations that con- 
rain a question mark are place holders for the ex- 
pected answer type. 
An additional set of classification rules is assoei- 
ated with this taxonomy, hfitially, all rules are based 
on the recognition of the question stem and of the 
answer type, obtained with class intbrmation from 
WordNet. However we could learn new rules when 
inorphologieal nd semantic variations of the seman- 
tic nodes arc allowed. Moreover, along with the new 
rules, we enrich the taxonomy, because often the new 
questions unify only partially with the current tax- 
enemy. All semantic and morphologic variations of 
the semantic representation nodes are grouped into 
word classes. Several of the word classes we used are 
listed in Table 1. 
\[I Word Ulass Words l\] 
Value words "monetary value", "money", "price" 
Expenditure words "spend", "buy", "rent", "invest" 
Creation words "author", "designer", ainvent~... 
Table 1: Examples of word classes. 
The bootstrapping algorithm that learns new 
classification rules and new classes of questions 
is based on an intbrmation extraction measure: 
scorc(rulei)--Ai * lo.q2(Ni), where Ai stands for the 
Class Name: cur rency  
\[ ~ l  O7: What debts did Qintex 
, . . . . . .  . .   rouploav : 
Class Name." I la lne perso l l  
Q32:Who received Ihe Will 
? Rogers Award in 1989? 
conlpctilioll " 
HFI2 
semalltic reprd~ - ~ representation 
Class Nanle: author 
Q92: Who released 1he lnlernet 
'~ worm in the late 1980s? 
el'talc " "  
| . 2 ~  . . . . . .  I 1980s1 
~ c  ~Bf i - )  ~ semanttc representation _ 
Figure 5: Mapping Questions in the Taxonomy 
number of different lexicon entries for the answer 
type of the question, whereas Ni = Ai /F i ,  where Fi 
is the number of different focus categories classified. 
The steps of the bootstrapping algorithm are: 
1. Retrieve concepts morphologically//semantically re ated 
to the semantic representations 
2. Apply the classification rules to all questions that 
contain any newly retrieved concepts. 
3. New_Classificatiton_Rules={} 
MUTUAL BOOTSTRAPPING LOOP 
4. Score. all new classification rules 
5. best_CR=thc highest scoring classification rule 
6. Add bcst_CR to the classification rules 
7. Add the questions classified by best_CR to the taxonomy 
8. Goto step 4 three times. 
9. Discard all new rules but the best scoring three. 
10. Goto 3. until the Q/A performance improves. 
296 
4 The Just i f icat ion Option 
A Q/A system that provides with the option of jus- 
til~ving the answer has the advantage that erroneous 
answers can be ruled out syst(,'matieally. In our quest 
of enh~mcing the precision of a Q /A  system by inco f  
porating additional knowledge, we fount1 this option 
very helpflH. However~ the generation of justifica- 
tions for ol)en-domain textual Q /A  systems poses 
some challenges. First;, we needed to develol) a very 
efficient prover, operating on logical form transfer- 
mat;ions. Our 1)rool'q are backchaining Do\]n the qlles- 
tions through a mixture of axioms. We use thl'ee 
forlllS of axioms: (1) axioms derived fl'om the facts 
stated in I;he l;eXtllal allswel; (2) ~/XiOlllS ro4)resent- 
ing world knowledge,; and (3) axioms (let;ermined by 
coreference resohlt;ion in the atiswer text;. For ex- 
alni)le , some of the axioms eml)loyed 1;o prove the 
answer to the TREC~8 question (26: Why did David 
Kor'csh, ask th, c 1,7\]1 for  a 'wo~'d p~vccssor?" are: 
SET 1 
Mr (71) := nu l l .  Koresh(71) :=nu l l .  word(72) :=nu l l .  
p rocessor (72) :=nu l l ,  sent (77  76  78  71) :=nu l l .  
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
SET 2 
Dav id (1) :=Mr(1) .  REASON(5) := enab le (5  3 6 ) .  
SET 3 
FB I  (i) : =nu l l .  
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
The first set represents fa(',l;s extraete(| through 
LFT \]~re(licat(',s of the textual answer: "0'?;(',~' t\]t(: 
'mc(~k(m,d My Kovcsh, .s'c'nt a vcq',,c.st fo'r a 'u;ord proce, s- 
so'r to c',,ablc h,i'H~, to 'record his vt:'~;clatio'~,s". The sec- 
Ol~d set  ret ) r ( ' , sen l ;s  worl(1 knowledge axioms that; we 
acquired senfi-auto\]na.tically. For instance we know 
that David is a male name, thus tlmt person cmt be 
addressed with Mr.. Similarly, events are (real>led 
for some reason. The third set of axioms represent 
the fact that the t,'l~l is in the context of the text 
answer. To be noted that the axioms derived from 
the answer have construct argument, s, relnesentc, d by 
convention with numl)ers larger titan 70. All the 
other arguments are variables. 
Q52 \?ho invented the road trallic cone,? 
Answer ST~iliT~g proudly for the caTl~cras , Gover~tor 
(shallow Pete Wilson, US TraTtsportatioTt Secretary 
methods) l,'edcrico l)eT~a aTtd Mayor l~ichavd l~iovda~t 
removed ~t hal f -  dozeTt phtstic or~t~tgc coTtcs 
f i rm the road~oay aTtd the first ca~'s passed 
I Answer David Mor~la~, the comp~tTty's maTtagiT~ 9 
(kb-based di~'ector t~d iT~?~eltlo~" of the plastic 
methods) co~te cycle, collects them. 
Table 2: Examples of trot)roved answer correctness. 
Tim justification of this answer is t)rovided by the 
r~ ( following proof I;r~lee. \] h ', 1)rover ;tttelllI)tS t;o 1)rove 
the LFT of the question (QLF) corre(:t 1} 3, proving 
from left to right each term of QLF. 
->Answer:0ver the weekend Mr Koresh sent a request for a word 
processor to enable him to record his revelations. 
->~LF:David(l)~Koresh(1)"word(2)^processor(2)'FBI(4) ~ 
ask(3 4 2 i 5) '_REASDN(5)'_PER(1)*0RG(4) 
->ALF:Mr(71)"Koresh(71)'word(72)'processor(72)'revelations(74)" 
record(Z3 74 75)^enable(75 73 76)~request(76)'sent(77 76 78 71) 
^weekend(78)"PEK(71) 'DATE(?8)  
-->Proving:David(1)^Koresh(1)^word(2)'processor(2)^FBI(4)" 
ask(3 4 2 i 5)'REASON(5)'_PER(1)'_0RG(4) 
There are i target axioms. Selected axiom: David(1):= Mr(1). 
Unifying: i to i. Ok 
--> Proving: Mr(1)'Koresh(J)~word(2)^processor(2)'FBl(4) 
^ask(3 4 2 i 5)^_REASDN(5)'_PER(1)^_0RG(4) 
There are i target axioms. Selected axiom: Mr(71):= null. 
Unifying: i to 71. Dk 
--> Proving: Koresh(71)*word(2)-processor(2)~FBl(4) " 
ask(3 4 2 Yl 5)* REASDN(5)^_PER(71)'_0RG(4) 
There are i target axioms. Selected axiom: Koresh(71):= null. 
Unifying: Y1 to 71. Dk 
--> Proving: word(2)'processor(2)^FBl(4)~ask(3 4 2 71 5)* 
_REASON(5)^_PER(71)'0RG(4) 
There are i target axioms. Selected axiom: word(72):= null, 
Unifying: 2 to 72. Ok 
--> Proving: processor(Y2)'FBl(4)^ask(3 4 72 71 5)^REASON(5) 
^_PER(71)^_DRG(4) 
There are i target axioms. Selected axiom: processor(72):= null. 
Unifying: ?2 to 72. Ok 
--> Proving: FBl(4)'ask(3 4 72 71 5)^REASON(5)^_PER(71)'_0RG(4) 
There are i target axioms, Selected axiom: FBI(1):= null. 
Unifying: 4 to i. Ok 
--> Proving: ask(3 4 72 71 5)~REASON(5)^_PER(ZI)^_BRG(4) 
There are 2 target axioms. Selected axiom: ask(l 2 3 4 5):= 
sent(l 6 7 4)'request(5). 
Unifying: i to 2. 3 to i. 5 to 5. 71 to 4. 72 to 3. Ok 
--> Proving: sent(l 6 7 ?I)^request(6)^_REASON(5)^_PER(YI)^_0RG(2) 
There are I target axioms, Selected axiom: sent(Z7 76 78 71):= null. 
Unifying: I to 77. 6 to Y6. Y to 78. 71 to 71. Ok 
--> Proving: request(76)^REASON(5)'_PER(71)^_0RG(2) 
There are i target axioms. Selected axiom: request(76):= null. 
Unifying: 76 to 76. Ok 
--> Proving: _REASON(5)?PER(71)?8}~G(2) 
There are 1 ta rget  axioms. Se lected axiom: _REASON(5):= enable(5 3 6). 
Unifying: 5 to 5. flk 
--> Proving: enable(5 3 6)^_PER(YI)-_flRG(2) 
There are i target axioms. Selected axiom: enable(75 73 76): = null. 
Unifying: 3 to ?3. 5 to 75. 6 to 76. Dk 
--> Proving: _PER(71)^_ORG(2) 
There are 3 target axioms. Selected axiom: _PER(71):= null. 
Unifying: 71 to gl. 0k 
--> Proving: _ORG(2) 
There are i target axioms. Selected axiom: _0RG(1):= FBI(1). 
Unifying: 2 to i. Ok 
--> Proving: nullJl\]J We found:Success. 
.................................................................. 
There are cases when our simple prover fails to 
prove a. correct answer. We have notice(1 that this 
hal)pens 1)ecause in the answer semantic representa- 
tion, st)me concepts that are connected in the ques- 
tion semantic representation are no longer directly 
linked. This is due to the f~mt that there are either 
parser errors or there are new syntactic dependen- 
cies between the two concepts. To acconmmdm;e 
this situation, we allow diflhrent constants that are 
arguments of the sanle predicate to be unifiable. The 
special cases in which this relaxation of the unifica- 
tion i)roeedul'e is allowed constitute our abduction 
r l t les .  
297 
5 Evaluat ion 
Both qualitative and quautitative valuation of the 
integration of surface text-based and knowledge- 
based methods for Q/A is imposed. Quantitatively, 
Tal)le 3 summarizes the scores obtained when only 
shallow methods were emI)loyed, in contrast with 
the results when knowledge-based methods were in- 
tegrated. We have sepm'ately measured the effect 
of tile integration of the knowledge-based methods 
at question processing and answer processing level. 
We have also evaluated the precision of the sys- 
tern when both integrations were implemented. The 
results were the first five answes's returned within 
250 bytes of text, when approximatively half mil- 
lion TREC documents are mined. Wc have used the 
200 questions from TREC-8, mid tile correct answers 
provided by NIST. The performance was measured 
both with the NIST scoring method employed in the 
TREC-8 and by simply assigning a score of 1 tbr the 
question having a correct answer, regardless of its 
position. 
Percentage of NIST score 
correct answers 
in top 5 returns 
Tezt-suTJ'acc-bascd 77.7% 64.5% 
Knowledge-based 83.2% 71.5% 
Question Processing 
(only) 
~l~:rt-su@zce-bascd 77.7% 73% 
only 'with Answer 
Justification 
Knowledge-based 89.5% 84.75% 
Question Pwces.sin9 
with Answer 
Justification 
Table 3: Accuracy t)erformance 
When using the NIST scoring method to eval- 
uate an individual answer, we used only six 
values:(1, .5, .33, .25, .2, 0), representing the score the 
answer's question obtains. If the first answer is cor- 
rect, it obtains a score of 1, if the second one is cor- 
rect, it is scored with .5, if the third one is correct, 
tile score becomes .aa, if the fourth is correct, the 
score is .25 and if the fifth one is correct, the score 
is .2. Otherwise, it is scored with 0. No credit is 
given if multiple answers are correct. Table 3 shows 
that both knowledge-based methods enhanced the 
precision, regardless of the scoring method. 
To further evaluate the contribution of tim justi- 
ficat, ion option, we evaluated separately the preci- 
sion of the prover tbr those questions for which tile 
surface-text-based methods of our system, when op- 
erating alone, emmet find correct answers. We had 
45 TREC-8 questions for which the evaluation of the 
prover was performed. Table 4 summarizes the ac- 
curacy of the prover. 
hlcorrect ~ilswors 
(no knowledge) 
Correct m~swers 
(KB-based) 
Incorrect answers 
(KB-based) 
P rovell 
con'ect 
3 
127 
l'roven Precision 
incorrect 
210 98.5% 
5 96.2% 
38 90.04% 
Table 4: Prover performmme 
Qualitatively, we find that the integration of 
knowledge-based methods is very beneficial. Table 2 
illustrates tile correct answer obtained with these 
methods, in contrast to tile incorrecl, answer pro- 
vided when only the shallow techniques m'e al)plied. 
6 Conc lus ions  
\Ve believe that the lmrfol'nmnce of a Q/A system 
del)ends on the knowledge sources it employs. In 
this paper we trove presented the effect of tile in- 
tegration of knowledge derived from question tax- 
enemies and produced by answer justifications on 
the Q/A precision. Our knowledge-based methods 
are lightweight, since, we do not generate precise se- 
mantic rel)resentations of questions or answo.rs, but  
mere attproximations determismd by syntactic de- 
1)en(lencies. Fm'thermore, our prover operates on 
very simple logical representations, its which syntac- 
tic and semantic ambiguities are completely ignored. 
Nevertheless, we have shown that these approxima- 
tions are functional, since we implemented a prover 
that justifies answers with high precision. Similarly, 
our knowledge-t)ased question l)rocessillg is a nlel'e 
combination of word class information and syntactic 
dep endencies. 
Re ferences  
Michael Collins. A New Statistical Parser Based on Bigram 
Lexical l)et)endencies. In Proceedings of the 2dst Annual 
Mectin9 of the Association for" Computational Lin.quistics, 
ACL-.96~ pages 184 19t, 1996. 
Christlane Fellbaum (Ed). WordNet - An Electronic Lexical 
l)atabase. MIT Press, 1998. 
Jerry R. IIobbs. Discourse and Inference. Unpublished 
malmscrlpt, 1986. 
Jerry R.. Itobbs. Overview of the TACITUS Project;. In Com- 
putational Linguistics, 12:(3), 1986. 
Jerry ltobbs, Mark StickeI, Doug Appelt, and Paul Mm'- 
tin. Interpretation as abduction. Artificial Intelligence, 63, 
pages 69 142, 1993. 
YVendy Lehnert. The processing of question answering. 
Lawrence Erlbaum Publishers, 1978. 
l)an Moldovan, Sanda llarabagiu, Marius Pasta, Rada Mi- 
halcea, Richard Goodrum, Roxana Gh:iu alK1 Vasile ll.us. 
Lasso: a tool for surfing the answer net. In Proceedings of 
TREC-8, 1999. 
Ellen Riloff and Rosie Jones. Learning l)ictionaries for hffor- 
mation Extraction by Multi-Level Bootstrat)plng. In Pro- 
cccdings of the 16th National Conference on Artificial In- 
telligence, AAAI-99, 1999. 
298 
 
		ffCOGEX: A Logic Prover for Question Answering
Dan Moldovan, Christine Clark, Sanda Harabagiu
Language Computer Corporation
Richardson, Texas 75080
moldovan@languagecomputer.com
Steve Maiorano
ATP
Washington, DC 20505
Abstract
Recent TREC results have demonstrated the
need for deeper text understanding methods.
This paper introduces the idea of automated
reasoning applied to question answering and
shows the feasibility of integrating a logic
prover into a Question Answering system. The
approach is to transform questions and answer
passages into logic representations. World
knowledge axioms as well as linguistic ax-
ioms are supplied to the prover which renders
a deep understanding of the relationship be-
tween question text and answer text. Moreover,
the trace of the proofs provide answer justifica-
tions. The results show that the prover boosts
the performance of the QA system on TREC
questions by 30%.
1 Introduction
Motivation
In spite of significant advances made recently in the
Question Answering technology, there still remain many
problems to be solved. Some of these are: bridging the
gap between question and answer words, pinpointing ex-
act answers, taking into consideration syntactic and se-
mantic roles of words, better answer ranking, answer jus-
tification, and others. The recent TREC results (Voorhees
2002) have demonstrated that many performing systems
reached a plateau; the systems ranked from 4th to 14th
answered correctly between 38.4% to 24.8% of the total
number of questions. It is clear that new ideas based on
a deeper language understanding are necessary to push
further the QA technology.
In this paper we introduce one such novel idea, the use
of automated reasoning in QA, and show that it is fea-
sible, effective, and scalable. We have implemented a
Logic Prover, called COGEX (from the permutation of
the first two syllables of the verb excogitate) which uni-
formly codifies the question and answer text, as well as
world knowledge resources, in order to use its inference
engine to verify and extract any lexical relationships be-
tween the question and its candidate answers.
Usefulness of a Logic Prover in QA
COGEX captures the syntax-based relationships such as
the syntactic objects, syntactic subjects, prepositional at-
tachments, complex nominals, and adverbial/adjectival
adjuncts provided by the logic representation of text. In
addition to the logic representations of questions and can-
didate answers, the QA Logic Prover needs world knowl-
edge axioms to link question concepts to answer con-
cepts. These axioms are provided by the WordNet glosses
represented in logic forms. Additionally, the prover needs
rewriting procedures for semantically equivalent lexical
patterns. With this deep and intelligent representation,
COGEX effectively and efficiently re-ranks candidate an-
swers by their correctness, extracts the exact answer, and
ultimately eliminates incorrect answers. In this way, the
Logic Prover is a powerful tool in boosting the accuracy
of the QA system. Moreover, the trace of a proof consti-
tutes a justification for that answer.
Technical challenges
The challenges one faces when using automated reason-
ing in the context of NLP include: logic representation
of open text, need of world knowledge axioms, logic rep-
resentation of semantically equivalent linguistic patterns,
and others. Logic proofs are accurate but costly, both in
terms of high failure rate due to insufficient input axioms,
as well as long processing time. Our solution is to inte-
grate the prover into the QA system and rely on reasoning
methods only to augment other previously implemented
answer extraction techniques.
                                                               Edmonton, May-June 2003
                                                               Main Papers , pp. 87-93
                                                         Proceedings of HLT-NAACL 2003
2 Integration of Logic Prover into a QA
System
The QA system includes traditional modules such as
question processing, document retrieval, answer extrac-
tion, built in ontologies, as well as many tools such as
syntactic parser, name entity recognizer, word sense dis-
ambiguation (Moldovan and Noviscki 2002), logic rep-
resentation of text (Moldovan and Rus 2001) and others.
The Logic Prover is integrated in this rich NLP environ-
ment and augments the QA system operation.
As shown in Figure 1, the inputs to COGEX consist of
logic representations of questions, potential answer para-
graphs, world knowledge and lexical information. The
term Answer Logic Form (ALF) refers to the candidate
answers in logic form. Candidate answers returned by the
Answer Extraction module are classified as open text due
to the unpredictable nature of their grammatical struc-
ture. The term Question Logic Form (QLF) refers to the
questions posed to the Question Answering system rep-
resented in logic form.
The prover also needs world knowledge axioms sup-
plied by the WordNet glosses transformed into logic rep-
resentations. Additionally there are many other axioms
representing equivalence classes of linguistic patterns,
called NLP axioms. All these are described below.
The Axiom Builder converts the Logic Forms for the
question, the glosses, and its candidate answers into ax-
ioms. Based on the parse tree patterns in the question
and answers, other NLP axioms are built to supplement
the existing general NLP axioms. Once the axioms are
complete and loaded, justification of the answer begins.
If a proof fails, the relaxation module is invoked. The
purpose of this module is twofold: (1) to compensate
for errors in the text parsing and Logic Form transfor-
mation phase, such as prepositional attachments and sub-
ject/object detection in verbs, (2) to detect correct an-
swers when the NLP and XWN (Extended WordNet) ax-
ioms fail to provide all the necessary inferences. During
the relaxation, arguments to predicates in the question are
incrementally uncoupled, the proof score is reduced, and
the justification is re-attempted. The loop between the
Justification and the Relaxation modules continues until
the proof succeeds, or the proof score is below a prede-
fined threshold. When all the candidate answers are pro-
cessed, the candidate answers are ranked based on their
proof scores, with the output from COGEX being the
ranked answers and the answer justifications.
3 Logic Representation of Text
A text logic form (LF) is an intermediary step between
syntactic parse and the deep semantic form. The LF cod-
ification acknowledges syntax-based relationships such
as: (1) syntactic subjects, (2) syntactic objects, (3) prepo-
sitional attachments, (4) complex nominals, and (5) ad-
jectival/adverbial adjuncts. Our approach is to derive the
LF directly from the output of the syntactic parser which
already resolves structural and syntactic ambiguities.
Essentially there is a one to one mapping of the words
of the text into the predicates in the logic form. The pred-
icate names consist of the base form of the word concate-
nated with the part of speech of the word. Each noun
has an argument that is used to represent it in other predi-
cates. One of the most important features of the Logic
Form representation is the fixed-slot allocation mecha-
nism of the verb predicates (Hobbs 1993). This allows
for the Logic Prover to see the difference between the
role of the subjects and objects in a sentence that is not
answerable in a keyword based situation.
Logic Forms are derived from the grammar rules found
in the parse tree of a sentence. There are far too many
grammar rules in the English language to efficiently and
realistically implement them all. We have observed that
the top ten most frequently used grammar rules cover
90% of the cases for WordNet glosses. This is referred
to as the 10-90 rule (Moldovan and Rus 2001). Below
we provide a sample sentence and its corresponding LF
representation.
Example:
Heavy selling of Standard & Poor?s 500-stock index fu-
tures in Chicago relentlessly beat stocks downward.
LF:
heavy JJ(x1) & selling NN(x1) & of IN(x1,x6) &
Standard NN(x2) & & CC(x13,x2,x3) & Poor NN(x3)
&?s POS(x6,x13) & 500-stock JJ(x6) & index NN(x4)
& future NN(x5) & nn NNC(x6,x4,x5) & in IN(x1,x8)
& Chicago NN(x8) & relentlessly RB(e12) &
beat VB(e12,x1,x9) & stocks NN(x9) & down-
ward RB(e12)
4 World Knowledge Axioms
Logic representation of WordNet glosses
A major problem in QA is that often an answer is
expressed in words different from the question keywords.
World knowledge is necessary to conceptually link ques-
tions and answers. WordNet glosses contain a source of
world knowledge. To be useful in automated reasoning,
the glosses need to be transformed into logic forms.
Taking the same approach as for open text, we have
parsed and represented in logic forms more than 50,000
WordNet glosses. For example, the gloss definition of
concept sport NN#1 is an active diversion requiring
physical exertion and competition, which yields the logic
representation:
active JJ(x1) & diversion NN(x1) & re-
quire VB(e1,x1,x2) & or CC(x2,x3,x4) & physi-
cal JJ(x3) & exertion NN(x3) & competition NN(x4)
Ranking
Answer answers
Answer
explanation
QLF
ALF
XWN axioms
Lexical chains
NLP axioms Builder
  Axiom
Ranked
Justification
Relaxation
       success
proof
fails
Figure 1: COGEX Architecture
Lexical Chains
A much improved source of world knowledge is obtained
when the gloss words are semantically disambiguated
(Moldovan and Noviscki 2002). By doing this, the con-
nectivity between synsets is dramatically increased. Lex-
ical chains can be established between synsets in different
hierarchies. These are sequences of semantically related
words that link two concepts.
Lexical chains improve the performance of question
answering systems in two ways: (1) increase the docu-
ment retrieval recall and (2) improve the answer extrac-
tion by providing the much needed world knowledge ax-
ioms that link question keywords with answers concepts.
We developed software that automatically provides
connecting paths between any two WordNet synsets
 
and
 
up to a certain distance (Moldovan and Noviscki
2002). The meaning of these paths is that the concepts
along a path are topically related. The path may contain
any of the WordNet relations augmented with a GLOSS
relation which indicates that a certain concept is present
in a synset gloss.
Examples
Below we provide some relevant lexical chains that link
a few selected TREC 2002 questions with their answers.
Q1394: What country did the game of croquet originate
in ?
Answer: Croquet is a 15th-century French sport that has
largely been dominated by older, wealthier people who
play at exclusive clubs.
Lexical chains:
(1) game:n#3  HYPERNYM  recreation:n#1 
HYPONYM  sport:n#1
(2) originate in:v#1  HYPONYM  stem:v#1 
GLOSS  origin:n#1  GLOSS  be:v#1
Q1403: When was the internal combustion engine
invented ?
Answer: The first internal - combustion engine was built
in 1867.
Lexical chains:
(1) invent:v#1  HYPERNYM  cre-
ate by mental act:v#1  HYPERNYM  create:v#1
 HYPONYM  build:v#1
Q: 1518 What year did Marco Polo travel to Asia ?
Answer: Marco Polo divulged the truth after returning in
1292 from his travels, which included several months on
Sumatra.
Lexical chains:
(1) travel to:v#1  GLOSS  travel:v#1  RGLOSS
 travel:n#1
(2) travel to#1  GLOSS  travel:v#1  HYPONYM
 return:v#1
(3) Sumatra:n#1  ISPART  Indonesia:n#1  IS-
PART  Southeast Asia:n#1  ISPART  Asia:n#1
Q: 1540 What is the deepest lake in America ?
Answer: Rangers at Crater Lake National Park in Oregon
have closed the hiking trail to the shore of the nation ?s
deepest lake
Lexical chains:
(1) America:n#1  HYPERNYM 
North American country:n#1  HYPERNYM 
country:n#1  GLOSS  nation:n#1
Q: 1570 What is the legal age to vote in Argentina ?
Answer: Voting is mandatory for all Argentines aged over
18
Lexical chains:
(1) legal:a#1  GLOSS  rule:n#1  RGLOSS 
mandatory:a#1
(2) age:n#1  RGLOSS  aged:a#3
(3) Argentine:a#1  GLOSS  Argentina:n#1
5 NLP Axioms
In additions to world knowledge axioms, a QA Logic
Prover needs linguistic knowledge. This is what distin-
guishes an NLP prover from a traditional mathematical
prover. General axioms that reflect equivalence classes
of linguistic patterns need to be created and instantiated
when invoked. We call these NLP axioms and present
below some examples together with questions that call
them.
Complex nominals and coordinated conjunctions
A question may refer to a subject/object by its full proper
name, and the answer will refer to the subject/object in
an abbreviated form. For example in the correct candi-
date answer for the question, ?Which company created
the Internet browser Mosaic??, Internet browser Mosaic
is referred to as Mosaic.
Using abduction, an axiom is built such that the head
noun of the complex nominal in the question implies the
remaining nouns in the complex nominal:
all x1 (mosaic nn(x1)  internet nn(x1) &
browser nn(x1))
An additional axiom is built such that all the nouns in the
complex nominal imply a complex nominal:
all x1 (internet nn(x1) & browser nn(x1) & mo-
saic nn(x1)  nn nnc(x1,x1,x1,x1))
So as not to restrict the ordering of the nouns in the noun
phrase from which the complex nominal is built, the
same argument is used for each of the noun predicates
in the complex nominal. Similar to the above issue, a
question may refer to the subject/object in an abbreviated
form, while the answer will refer to the subject/object
in its full, proper form. For example in the correct
candidate answer for the question, ?When was Microsoft
established??, Microsoft is referred to as Microsoft
Corp.
An axiom is built such that each noun of the complex
nominal takes on the identifying argument of the complex
nominal:
all x1 x2 x3 ( microsoft nn(x1) & corp nn(x2) &
nn nnc(x3,x1,x2)  microsoft nn(x3) & corp nn(x3))
Similar axioms are used for coordinated conjunctions de-
tected in the answer and the question. These are consid-
ered weak axioms, and any proof that uses them will be
penalized by being given a lower score than those that do
not.
Appositions
A candidate answer for a question may use an apposition
to describe the subject/object of the answer. The ques-
tion may refer to the subject/object by this apposition.
For example in the question, ?Name the designer of the
shoe that spawned millions of plastic imitations , known
as jellies?, the candidate answer, ?..Italian Andrea Pfis-
ter , designer of the 1979 ? bird cage ? shoe that spawned
millions of plastic imitations, known as ? jellies ...? uses
an apposition to describe the designer.
An axiom is built to link the head of the noun phrases
in the apposition such that they share the same argument:
all x12 x13 x14 x15 x17 x18 x19 (ital-
ian nn(x12) & andrea nn(x13) & pfister NN(x14)
& nn nnc(x15,x12,x13,x14)  designer nn(x15) &
of in(x15,x17) & 1979 nn(x17) & bird nn(x18) &
cage nn(x19))
Possesives
A question/answer substitutes the use of a possesive by
using an of or by preposition. For example, in the ques-
tion, ?What was the length of the Wright brothers? first
flight??, the candidate answer, ?Flying machines , which
got off the ground with a 120 - foot flight by the Wright
brothers in 1903...? implies ownership using the preposi-
tion by to connect the Wright brothers to flight.
An axiom is built to connect by to the possessive:
all x1 x2 (by in(x1,x2)  pos(x1,x2))
Equivalence classes for prepositions
Prepositions can be grouped into equivalence classes de-
pending on the context of the question, which is deter-
mined by the expected answer type. In location seeking
questions the prepositions at and in are often interchange-
able. Similarly for in and into, and from and of. In date
seeking questions in and of have interchangeable mean-
ings as do at and in. For example, in the question, ?What
body of water does the Colorado River flow into??, the
candidate answer, ?...the Colorado River flowed in the
Salton trough about 130 miles east of San Diego?, the
preposition in and into in the answer take in the same
meaning.
An axiom is built to link in to into:
all x1 x2 (in in(x1,x2)  into in(x1,x2))
Part of relations in location questions
A location seeking question may have a candidate
answer that identifies a location by referring to a part of
the location. For example, in the question, ?Where is
Devil ?s Tower??, the answer, ?American Indians won
another court battle over their right to worship without
interference at Devils Tower National Monument in the
northeast corner of Wyoming?, identifies Wyoming as
the location of Devil ?s Tower by referring to the part of
Wyoming in which it lies. An axiom is built to connect
Wyoming to its part:
all x1 x2 x3 (corner nn(x1) & of in(x1,x2) &
wyoming nn(x2)  wyoming nn(x1) )
Attribute of relations in quantity seeking questions
A question seeking a quantity may have a candidate an-
swer that implies quantity of subject by prefixing the
quantity to the subject. For example in the question
?What is the height of the tallest redwood?? the answer is
?329 feet Mother of Forest?s Big Basin tallest redwood..?
An axiom is built to connect the quantity to its subject,
redwood:
all x1 x2 ( quantity(x1) & redwood NN(x2) 
of in(x1,x2))
This is a weak axiom since the proximity of redwood to
quantity in the answer text is not guaranteed. As men-
tioned for the complex nominal and coordinated conjunc-
tion axioms, any proof that uses these axioms should be
penalized and ranked lower than those that do not. Note
that for this axiom to be effective, an axiom linking the
heads of the apposition is built:
all x8 x9 x10 x11 x12 (mother nn(x8) & of in(x8,x9)
& forest nn(x9)  big nn(x10) & basin nn(x11) &
nn nnc(x12,x10,x11) & s pos(x8,x12) & tall jj(x8) &
redwood nn(x8))
6 Control Strategy
Axiom partitioning mechanism
The search strategy used is the Set of Support Strategy,
which partitions the axioms used during the course of
a proof into those that have support and those that are
considered auxiliary (Wos 1988). The axioms with sup-
port are placed in the Set of Support (SOS) list and are
intended to guide the proof. The auxiliary axioms are
placed in the Usable list and are used to help the SOS
infer new clauses. This strategy restricts the search such
that a new clause is inferred if and only if one of its par-
ent clauses come from the Set of Support. The axioms
that are placed in the SOS are the candidate answers, the
question negated (to invoke the proof by contradiction),
and axioms related to linking named entities to answer
types.
Axioms placed in the Usable list are: (1) Extended
WordNet axioms, (2) NLP axioms, and (3) axioms based
on outside world knowledge, such as people and organi-
zations.
Inference rules
The inference rule sets are based on hyperresolution and
paramodulation. Hyperresolution is an inference rule that
does multiple binary resolution steps in one, where bi-
nary resolution is an inference mechanism that looks for
a positive literal in one clause and negative form of that
same literal in another clause such that the two literals
can be canceled, resulting in a newly inferred clause.
Paramodulation introduces the notion of equality substi-
tution so that axioms representing equality in the proof
do not need to be explicitly included in the axiom lists.
Additionally, similar to hyperresolution, paramodulation
combines multiple substitution steps into one.
All modern theorem provers use hyperresolution and
paramodulation inference rules since they allow for a
more compact and efficient proof by condensing multi-
ple steps into one.
COGEX will continue trying to find a proof until the
Set of Support becomes empty, a refutation is found, or
the proof score drops below a predefined threshold.
Two techniques have been implemented in COGEX to
deal with incomplete proofs:
1. Count the number of unifications/resolutions with
terms in the question along the longest search path in the
proof attempts, and
2. Relax the question logic form by incrementally un-
coupling arguments in the predicates, and/or removing
prepositions or modifiers that are not crucial to the mean-
ing of the text.
For example in question, ?How far is Yaroslavl from
Moscow?? a candidate answer is ?.. Yaroslavl, a city 250
miles north of Moscow.? By dropping the from predicate
in the question makes the proof succeed for the candidate
answer.
7 An example
The following example illustrates how all these pieces are
put together to generate answer proofs.
Question 108?:
Which company created the Internet Browser Mosaic ?
QLF:
organization AT(x2) ) & company NN(x2) & cre-
ate VB(e1,x2,x6) & Internet NN(x3) & browser NN(x4)
& Mosaic NN(x5) & nn NNC(x6,x3,x4,x5)
Question Axiom:
-(exists e1 x2 x3 x5 x6 ( organization at(x2) &
company nn(x2) & create vb(e1,x2,x6) & inter-
net nn(x3) & browser nn(x4) & mosaic nn(x5) &
nn nnc(x6,x3,x4,x5))).
Answer:
In particular, a program called Mosaic , developed by
the National Center for Supercomputing Applications (
NCSA ) at the University of Illinois at Urbana - Cham-
paign , is gaining popularity as an easy to use point and
click interface for searching portions of the Internet.
ALF:
In IN(x1,x28) & particular JJ(x29) & program NN(x1)
& call VB(e1,x27,x30) & Mosaic NN(x2) & de-
velop VB(e2,x2,x31) & by IN(e2,x8) & Na-
tional NN(x3) & Center NN(x4) & for NN(x5) &
Supercomputing NN(x6) & application NN(x7) &
nn NNC(x8,x3,x4,x5,x6,x7) & NCSA NN(x9) &
at IN(e2,x15) & University NN(x10) & of NN(x11)
& Illinois NN(x12) & at NN(x13) & Urbana NN(x14)
& nn NNC(x15,x10,x11,x12,x13,x14) & Cham-
paign NN(x16) & gain VB(e3,x1,x17) & popu-
larity NN(x17) & as IN(e3,x32) & easy JJ(x33)
& use VB(e4,x34,x26) & point NN(x18) &
and CC(x26,x18,x21) & click NN(x19) & in-
terface NN(x20) & nn NNC(x21,x19,x20) &
for IN(x26,e5) & search VB(e5,x26x22) & por-
tion NN(x22) & of IN(x22,x23) & Internet NN(x23)
Answer Axiom:
exists e1 e2 e3 e4 e5 x1 x10 x11 x12 x13 x14 x15 x16 x17
x18 x19 x2 x20 x21 x22 x23 x26 x27 x28 x3 x32 x33 x4
x5 x6 x7 x8 x9 (in in(e2,x28) & particular jj(x28) & pro-
gram nn(x1) & call vb(e1,x27,x1) & mosaic nn(x2) &
develop vb(e2,x8,x2) & by in(e2,x8) & national nn(x3)
& center nn(x4) & for nn(x5) & supercomputing nn(x6)
& application nn(x7) & nn nnc(x8,x3,x4,x5,x6,x7)
& ncsa nn(x9) & at in(x8,x15) & university nn(x10)
& of nn(x11) & illinois nn(x12) & at nn(x13) & ur-
bana nn(x14) & nn nnc(x15,x10,x11,x12,x13,x14)
& champaign nn(x16) & gain vb(e3,x2,x17)
& popularity nn(x17) & as in(e3,x32) &
easy jj(x33) & use vb(e4,x9,x2) & point nn(x18)
& and cc(x26,x18,x21) & click nn(x19) & inter-
face nn(x20) & nn nnc(x21,x19,x20) & for in(x26,e5)
& search vb(e5,x2,x22) & portion nn(x22) &
of in(x22,x23) & internet nn(x23)).
Named entity axioms:
(1) all x3 x4 x5 x6 x7 x8 (national nn(x3) &
center nn(x4) & for nn(x5) & supercomputing nn(x6)
& application nn(x7) & nn nnc(x8,x3,x4,x5,x6,x7) 
organization at(x8)).
(2) all x10 x11 x12 x13 x14 x15 (university nn(x10)
& of nn(x11) & illinois nn(x12) & at nn(x13) & ur-
bana nn(x14) & nn nnc(x15,x10,x11,x12,x13,x14) 
university at(x15)).
(3) all x16 (champaign nn(x16)  town at(x16)).
Wordnet Gloss axioms:
The question contained the verb create while the answer
contains the verb develop. In order to prove that this
answer is in fact correct, we need to detect and use a
lexical chain between develop and create. WordNet
supplies us with that chain such that
develop  make and make  create
Using WordNet glosses, this chain is transformed into
two axioms:
(4) exists x2 x3 x4 all e2 x1 x7 (develop vb(e2,x7,x1) 
make vb(e2,x7,x1) & something nn(x1) & new jj(x1)
& such jj(x1) & product nn(x2) & or cc(x4,x1,x3) &
mental jj(x3) & artistic jj(x3) & creation nn(x3)).
(5) all e1 x1 x2 (make vb(e1,x1,x2)  cre-
ate vb(e1,x1,x2) & manufacture vb(e1,x1,x2) &
man-made jj(x2) & product nn(x2)).
Furthermore, the question asks about the Internet
browser Mosiac, while the candidate answer refers to
Mosaic. To provide the knowledge that the Internet
browser Mosaic refers to the same thing as Mosaic, the
head of the complex nominal, Internet browser Mosaic,
implies its remaining components.
(6) all x1 (mosaic nn(x1)  internet nn(x1) &
browser nn(x1)).
(7) all x1 x2 x3 x4 (mosaic nn(x1) & internet nn(x1) &
browser nn(x1)  nn nnc(x1,x1,x1,x1)).
The next step is to build the Set of Support Axiom(s) for
the Question. The question is negated to invoke the proof
by contradiction
-(exists e1 x2 x3 x5 x6 ( organization at(x2) & com-
pany nn(x2) & create vb(e1,x2,x6) & internet nn(x3) &
browser(x4) & mosaic nn(x5) & nn nnc(x6,x3,x4,x5))).
Next, link the answer type term, its modifiers, and any
prepositional attachments to the answer type as a substi-
tute for more refined named entity recognition.
all x1 ( organization at(x1)  company nn(x1)).
It remains to create axioms for the ALF of the candidate
answer and to start the proof.
The Proof
34 [] -develop vb(x22,x7,x1) make vb(x22,x7,x1).
44 [] -make vb(x23,x1,x2)  create vb(x23,x1,x2).
74 [] - organization at(x1)  company nn(x1).
121 [] -mosaic nn(x1)  internet nn(x1).
122 [] -mosaic nn(x1)  browser nn(x1).
123 [] -mosaic nn(x1)  -internet nn(x1)  -
browser nn(x1)  nn nnc(x1,x1,x1,x1).
294 [] - organization at(x2)  -company nn(x2)  -
create vb(x34,x2,x6)  -internet nn(x3)  -browser nn(x4)
 -mosaic nn(x5)  -nn nnc(x6,x3,x4,x5).
299 [] mosaic nn($c111).
300 [] develop vb($c126,$c96,$c111).
302 [] national nn($c103).
303 [] center nn($c100).
304 [] for nn($c99).
305 [] supercomputing nn($c98).
306 [] application nn($c97).
307 [] nn nnc($c96,$c103,$c100,$c99,$c98,$c97).
332 [] -national nn(x3)  -center nn(x4)  -for nn(x5)
 -supercomputing nn(x6)  -application nn(x7)  -
nn nnc(x8,x3,x4,x5,x6,x7)  organization at(x8).
335 [hyper,299,122] browser nn($c111).
336 [hyper,299,121] internet nn($c111).
337 [hyper,336,123,299,335]
nn nnc($c111,$c111,$c111,$c111).
347 [hyper,300,34] make vb($c126,$c96,$c111).
356 [hyper,347,44] create vb($c126,$c96,$c111).
372 [hyper,332,302,303,304,305,306,307]
organization at($c96).
373 [hyper,372,74] company nn($c96).
374 [hyper,373,294,372,356,336,335,299,337] $F.
The numbers on the left hand side of the proof summary
indicate the step number in the search, not the step
number in the proof. Through step 332 we see that
COGEX has selected all the axioms it needs to prove
that the candidate answer is correct for the question
posed to the QA system. Steps 335 through 374 show
hyperresolutions that result in all the terms of the
question being derived in their positive form so the proof
by contradiction succeeds, which is indicated by the $F
in the final step and the hyperresolution of all the derived
terms with the negated question from step 1 of the proof.
The success of this proof boosts the candidate answer to
the first position.
When the proof fails, we devised a way to incremen-
tally relax some of the conditions that hinder the comple-
tion of the proof. This relaxation process puts weights
on the proof such that proofs weaker than a predefined
threshold are not accepted.
8 Results
COGEX was implemented and integrated into a state-of-
the-art Question Answering system that participated in
TREC 2002. All questions are attempted by the prover,
but if the proof fails the QA system resorts to other an-
swer extraction methods that were part of the system be-
fore the prover. Thus, some questions are answered by
the QA system without the prover, some only by the
prover and some by both the non-prover system and the
prover. The complete system answered 415 questions out
of 500 TREC 2002 questions. Of these, 206 were an-
swered by COGEX but some of these answers were also
provided by QA system without COGEX. A careful anal-
ysis indicates that the QA system without logic prover an-
swered 317 questions and the prover can answer only 98
additional questions for which the system without prover
failed. Table 1 summarizes these results.
Questions answered by the complete system 415
Questions answered by COGEX 206
Questions answered only by COGEX 98
Questions answered without COGEX 317
Table 1: Performance over 500 TREC 2002 questions
The added value of automated reasoning to the QA sys-
tem is 30.9% (98/317). This represents a significant im-
provement in the performance of the logic prover for QA
over the one reported in (Moldovan 2002). The failures
of the prover are due primarily to the lack of linguistic
axioms.
9 Discussion
A logic prover brings several advantages to question an-
swering, but at a high cost. Some advantages are: the ca-
pability of pinpointing exact answers that otherwise will
be missed, answer justification, and a quantifiable mea-
sure of how close a system is to providing an answer.
However, the implementation of a QA logic prover is ex-
pensive as it requires logic representation of text, world
knowledge axioms and a large number of linguistic ax-
ioms, that all take time to develop.
Acknowledgment
This work was supported in part by the ARDA
AQUAINT program.
We wish to thank Mihai Surdeanu and Marius Pasca from
LCC for their contribution to this work.
References
J. Hobbs, M. Stickel, and P. Martin. Interpretation as ab-
duction. Artificial Intelligence, 63 (1993), pp 69-142.
D. Moldovan, M. Pasca, S. Harabagiu and M. Surdeanu.
Performance issues and error analysis in an open-
domain question answering system. In Proceedings of
ACL 2002.
Dan Moldovan and Adrian Noviscki. Lexical Chains for
Questions. In Proceedings of Coling 2002.
Dan Moldovan and Vasile Rus. Logic Form Transfor-
mation of WordNet and its Applicability to Question
Answering. In Proceedings of ACL 2001.
Ellen Voorhees. Overview of the TREC 2002 Question
Answering Track. In TREC 2002 http://trec.nist.gov
Larry Wos. Automated Reasoning, 33 Basic Research
Problems. Prentice Hall, 1988.
Multilingual Coreference Resolution 
Sanda M.  Harabag iu  
Southern Methodist  University 
Dallas, TX  75275-0122 
sanda@seas, smu. edu 
Steven J .  Ma iorano  
IPO 
Washington,  D.C. 20505 
maiorano@cais, com 
Abst rac t  
In this paper we present a new, multi- 
lingual data-driven method for coreference 
resolution as implemented in the SWIZZLE 
system. The results obtained after training 
this system on a bilingual corpus of English 
and Romanian tagged texts, outperformed 
coreference r solution in each of the indi- 
vidual anguages. 
1 I n t roduct ion  
The recent availability of large bilingual corpora has 
spawned interest in several areas of multilingual text 
processing. Most of the research as focused on 
bilingual terminology identification, either as par- 
allel multiwords forms (e.g. the ChampoUion sys- 
tem (Smadja et a1.1996)), technical terminology (e.g. 
the Termight system (Dagan and Church, 1994) or 
broad-coverage translation lexicons (e.g. the SABLE 
system (Resnik and Melamed, 1997)). In addition, 
the Multilingual Entity Task (MET) from the TIP- 
STER program 1 (http://www-nlpir.nist.gov/related- 
projeets/tipster/met.htm) challenged the partici- 
pants in the Message Understanding Conference 
(MUC) to extract named entities across everal for- 
eign language corpora, such as Chinese, Japanese 
and Spanish. 
In this paper we present a new application of 
aligned multilinguai texts. Since coreference r so- 
lution is a pervasive discourse phenomenon causing 
performance impediments in current IE systems, we 
considered a corpus of aligned English and Roma- 
nian texts to identify coreferring expressions. Our 
task focused on the same kind of coreference as 
considered in the past MUC competitions, namely 
1The TIPSTER Text Program was a DARPA-Ied 
government effort o advance the state of the art in text 
processing technologies. 
the identity coreference. Identity coreference links 
nouns, pronouns and noun phrases (including proper 
names) to their corresponding antecedents. 
We created our bilingual collection by translating 
the MUC-6 and MUC-7 coreference training texts 
into Romanian using native speakers. The train- 
ing data set for Romanian coreference used, wher- 
ever possible, the same coreference identifiers as the 
English data and incorporated additional tags as 
needed. Our claim is that by adding the wealth 
of coreferential features provided by multilingual 
data, new powerful heuristics for coreference r solu- 
tion can be developed that outperform onolingual 
coreference r solution systems. 
For both languages, we resolved coreference by 
using SWIZZLE, our implementation f a bilingual 
coreference r solver. SWIZZLE is a multilingual en- 
hancement of COCKTAIL (Harabagiu and Maiorano, 
1999), a coreference r solution system that operates 
on a mixture of heuristics that combine semantic 
and textual cohesive information 2. When COCKTAIL 
was applied separately on the English and the Ro- 
manian texts, coreferring links were identified for 
each English and Romanian document respectively. 
When aligned referential expressions corefer with 
non-aligned anaphors, SWIZZLE derived new heuris- 
tics for coreference. Our experiments show that 
SWIZZLE outperformed COCKTAIL on both English 
and Romanian test documents. 
The rest of the paper is organized as follows. Sec- 
tion 2 presents COCKTAIL, a monolingnai coreference 
resolution system used separately on both the En- 
glish and Romanian texts. Section 3 details the 
data-driven approach used in SWIZZLE and presents 
some of its resources. Section 4 reports and discusses 
the experimental results. Section 5 summarizes the 
2The name of COCKTAIL is a pun on CogNIAC be- 
cause COCKTAIL combines a larger number of heuristics 
than those reported in (Baldwin, 1997). SWIZZLE, more- 
over, adds new heuristics, discovered from the bilingual 
aligned corpus. 
142 
conclusions. 
2 COCKTAIL  
Currently, some of the best-performing and 
most robust coreference r solution systems employ 
knowledge-based techniques. Traditionally, these 
techniques have combined extensive syntactic, se- 
mantic, and discourse knowledge. The acquisition 
of such knowledge is time-consuming, difficult, and 
error-prone. Nevertheless, recent results show that 
knowledge-poor methods perform with amazing ac- 
curacy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 
1996) (Kameyama, 1997)). For example, CogNIAC 
(Baldwin, 1997), a system based on seven ordered 
heuristics, generates high-precision resolution (over 
90%) for some cases of pronominal reference. For 
this research, we used a coreference r solution sys- 
tem ((Harabagiu and Malorano, 1999)) that imple- 
ments different sets of heuristics corresponding to 
various forms of coreference. This system, called 
COCKTAIL, resolves coreference by exploiting several 
textual cohesion constraints (e.g. term repetition) 
combined with lexical and textual coherence cues 
(e.g. subjects of communication verbs are more 
likely to refer to the last person mentioned in the 
text). These constraints are implemented as a set of 
heuristics ordered by their priority. Moreover, the 
COCKTAIL framework uniformly addresses the prob- 
lem of interaction between different forms of coref- 
erence, thus making the extension t  multilingual 
coreference very natural. 
2.1 Data -Dr iven  Coreference Reso lu t ion  
In general, we define a data-driven methodology as 
a sequence of actions that captures the data pat- 
terns capable of resolving a problem with both a 
high degree of precision and recall. Our data-driven 
methodology reported here generated sets of heuris- 
tics for the coreference r solution problem. Precision 
is the number of correct references out of the total 
number of coreferences resolved, whereas the recall 
measures the number of resolved references out of 
the total number of keys, i.e., the annotated coref- 
erence data. 
The data-driven methodology used in COCKTAIL is 
centered around the notion of a coreference chain. 
Due to the transitivity of coreference relations, k 
coreference r lations having at least one common ar- 
gument generate k + 1 core/erring expressions. The 
text position induces an order among coreferring ex- 
pressions. A coreference structure is created when 
a set of coreferring expressions are connected in an 
oriented graph such that each node is related only 
to one of its preceding nodes. In turn, a corefer- 
ence chain is the coreference structure in which ev- 
ery node is connected to its immediately preceding 
node. Clearly, multiple coreference structures for the 
same set of coreferring expressions can be mapped 
to a single coreference chain. As an example, both 
coreference structures illustrated in Figure l(a) and 
(c) are cast into the coreference chain illustrated in 
Figure l(b). 
TEXT TEXT TEXT 
i [ ]  
(a) (b) (c) 
Figure 1: Three coreference structures. 
Given a corpus annotated with coreference data, 
the data-driven methodology first generates all 
coreference chains in the data set and then con- 
siders all possible combinations of coreference re- 
lations that would generate the same coreference 
chains. For a coreference chain of length l with 
nodes nl, n2, ... nt+l, each node nk ( l<k~/)  can 
be connected to any of the l - k nodes preceding 
it. From this observation, we find that a number 
of 1 x 2 x ... x (l - k)... x I = l! coreference struc- 
tures can generate the same coreference chain. This 
result is very important, since it allows for the auto- 
matic generation of coreference data. For each coref- 
erence relation T~ from an annotated corpus we cre- 
ated a median of (l - 1)! new coreference r lations, 
where l is the length of the coreference hain contain- 
ing relation 7~. This observation gave us the possi- 
bility of expanding the test data provided by the 
coreference keys available in the MUC-6 and MUC- 
7 competitions (MUC-6 1996), (MUC-7 1998). The 
MUC-6 coreference annotated corpus contains 1626 
coreference r lations, while the MUC-7 corpus has 
2245 relations. The average length of a coreference 
chain is 7.21 for the MUC-6 data, and 8.57 for the 
MUC-7 data. We were able to expand the number 
of annotated coreference relations to 6,095,142 for 
the MUC-6 corpus and to 8,269,403 relations for the 
MUC-7 corpus; this represents an expansion factor 
of 3,710. We are not aware of any other automated 
way of creating coreference annotated ata, and we 
believe that much of the COCKTAIL's impressive per- 
formance is due to the plethora of data provided by 
this method. 
143 
Heuristics for 3rd person pronouns 
oHeuristie 1-Pronoun(H1Pron) 
Search in the same sentence for the same 
3rd person pronoun Pros' 
if (Pron' belongs to coreference chain CC) 
and there is an element from CC which is 
closest o Pron in Text, Pick that element. 
else Pick Pron'. 
oHeuristic 2-Pronoun(H2Pron) 
Search for PN, the closest proper name from Pron 
if (PN agrees in number and gender with Pros) 
if (PN belongs to coreference chain CC) 
then Pick the element from CC which is 
closest o Pros in Text. 
else Pick PN. 
o Heuristic 3- Pronoun( H3Pron )
Search for Noun, the closest noun from Pros 
if (Noun agrees in number and gender with Pros) 
i f  (Noun belongs to coreference chain CC) 
and there is an element from CC which is 
closest o Pros in Text, Pick that element. 
else Pick Noun 
Heuristics for nominal reference 
o Heuristic 1-Nominal(HINom ) 
if (Noun is the head of an appositive) 
then Pick the preceding NP. 
oHeuristic 2-Nominal(H2Nom) 
if (Noun belongs to an NP, Search for NP' 
such that Noun'=same_name(head(NP),head(NP')) 
or  
Noun'--same_name(adjunct(NP), adjunct(NP'))) 
then if (Noun' belongs to coreference chain CC) 
then Pick the element from CC which is 
closest o Noun in Text. 
else Pick Noun'. 
oHeuristic 3-Nominal(H3Nom) 
if Noun is the head of an NP 
then Search for proper name PN 
such that head(PN)=Noun 
if (PN belongs to coreference chain CC) 
and there is an element from CC which is 
closest o Noun in Text, Pick that element. 
else Pick PN. 
Table 1: Best performing heuristics implemented in COCKTAIL 
2.2 Knowledge-Poor  Core ference  
Reso lu t ion  
The result of our data-driven methodology is the 
set of heuristics implemented in COCKTAIL which 
cover both nominal and pronoun coreference. Each 
heuristic represents a pattern of coreference that 
was mined from the large set of coreference data. 
COCKTAIL uses knowledge-poor methods because (a) 
it is based only on a limited number of heuristics 
and (b) text processing is limited to part-of-speech 
tagging, named-entity recognition, and approximate 
phrasal parsing. The heuristics from COCKTAIL can 
be classified along two directions. First of all, they 
can be grouped according to the type of corefer- 
ence they resolve, e.g., heuristics that resolve the 
anaphors of reflexive pronouns operate differently 
than those resolving bare nominals. Currently, in 
COCKTAIL there are heuristics that resolve five types 
of pronouns (personal, possessive, reflexive, demon- 
strative and relative) and three forms of nominals 
(definite, bare and indefinite). 
Secondly, for each type of coreference, there are 
three classes of heuristics categorized according to 
their suitability to resolve coreference. The first 
class is comprised of strong indicators of coreference. 
This class resulted from the analysis of the distribu- 
tion of the antecedents in the MUC annotated ata. 
For example, repetitions of named entities and ap- 
positives account for the majority of the nominal 
coreferences, and, therefore, represent anchors for 
the first class of heuristics. 
The second class of coreference covers cases in 
which the arguments are recognized to be seman- 
tically consistent. COCKTAIL's test of semantic on- 
sistency blends together information available from 
WordNet and statistics gathered from Treebank. 
Different consistency checks are modeled for each of 
the heuristics. 
Example of the application of heuristic H2Pron 
Mr. Adams1, 69 years old, is the retired chairman 
of Canadian-based Emco Ltd., a maker of plumbing 
and petroleum equipment; he1 has served on the 
Woolworth board since 1981. 
Example of the application of heuristic H3Pron 
"We have got to stop pointing our fingers at these 
kids2 who have no future," he said, "and reach our 
hands out to them2. 
Example of the application of heuristic H2Nom 
The chairman and the chief executive officer3 
of Woolworth Corp. have temporarily relinquished 
their posts while the retailer conducts its investi- 
gation into alleged accounting irregularities4. 
Woolworth's board named John W. Adams, an 
outsider, to serve as interim chairman and executive 
officer3, while a special committee, appointed by 
the board last week and led by Mr. Adams, 
investigates the alleged irregularities4. 
Table 2: Examples of coreference resolution. The 
same annotated index indicates coreference. 
The third class of heuristics resolves coreference 
by coercing nominals. Sometimes coercions involve 
only derivational morphology - linking verbs with 
their nominalizations. On other occasions, coercions 
are obtained as paths of meronyms (e.g. is-part re- 
lations) and hypernyms (e.g. is-a relations). Con- 
144. 
sistency checks implemented for this class of coref- 
erence are conservative: ither the adjuncts must be 
identical or the adjunct of the referent must be less 
specific than the antecedent. Table 1 lists the top 
performing heuristics of COCKTAIL for pronominal 
and nominal coreference. Examples of the heuristics 
operation on the MUC data are presented presented 
in Table 2. Details of the top performing heuris- 
tics of COCKTAIL were reported in (Harabagiu and 
Maiorano, 1999). 
2.3 Bootstrapping for Coreferenee 
Resolution 
One of the major drawbacks of existing corefer- 
ence resolution systems is their inability to recog- 
nize many forms of coreference displayed by many 
real-world texts. Recall measures of current systems 
range between 36% and 59% for both knowledge- 
based and statistical techniques. Knowledge based- 
systems would perform better if more coreference 
constraints were available whereas tatistical meth- 
ods would be improved if more annotated data were 
available. Since knowledge-based techniques out- 
perform inductive methods, we used high-precision 
coreference heuristics as knowledge seeds for ma- 
chine learning techniques that operate on large 
amounts of unlabeled ata. One such technique 
is bootstrapping, which was recently presented in 
(Riloff and Jones 1999), (Jones et a1.1999) as an 
ideal framework for text learning tasks that have 
knowledge seeds. The method oes not require large 
training sets. We extended COCKTAIL by using meta- 
bootstrapping of both new heuristics and clusters of 
nouns that display semantic onsistency for corefer- 
ence. 
The coreference heuristics are the seeds of our 
bootstrapping framework for coreference r solution. 
When applied to large collections of texts, the 
heuristics determine classes of coreferring expres- 
sions. By generating coreference chains out of all 
these coreferring expressions, often new heuristics 
are uncovered. For example, Figure 2 illustrates the 
application of three heuristics and the generation of 
data for a new heuristic rule. In COCKTAIL, after a 
heuristic is applied, a new coreference chain is cal- 
culated. For the example illustrated in Figure 2, if 
the reference of expression A is sought, heuristic H1 
indicates expression B to be the antecedent. When 
the coreference chain is built, expression A is di- 
rectly linked to expression D, thus uncovering a new 
heuristic H0. 
As a rule of thumb, we do not consider a new 
heuristic unless there is massive vidence of its cov- 
erage in the data. To measure the coverage we use 
the FOIL_Gain measure, as introduced by the FOIL 
inductive algorithm (Cameron-Jones and Quinlan 
1993). Let Ho be the new heuristic and/-/1 a heuris- 
tic that is already in the seed set. Let P0 be the num- 
ber of positive coreference examples of Hn~w (i.e. 
the number of coreference r lations produced by the 
heuristic that can be found in the test data) and no 
the number of negative xamples of/-/new (i.e. the 
number of relations generated by the heuristic which 
cannot be found in the test data). Similarly, Pl and 
nl are the positive and negative xamples of Ha. 
The new heuristics are scored by their FOIL_Gain 
distance to the existing set of heuristics, and the best 
scoring one is added to the COCKTAIL system. The 
FOIL_Gain formula is: 
l og2- - -~ ) FOIL_Gain(H1, Ho) = k(log2 Pl nl  Po -k no 
where k is the number of positive examples cov- 
ered by both//1 and Ho. Heuristic Ho is added to 
the seed set if there is no other heuristic providing 
larger FOIL_Gain to any of the seed heuristics. 
H3 . j . . .~ IB  B 
\ [~  HO - New Heuristic 
Figure 2: Bootstrapping new heuristics. 
Since in COCKTAIL, semantic onsistency of core- 
ferring expressions is checked by comparing the sim- 
ilarity of noun classes, each new heuristic deter- 
mines the adjustment of the similarity threshold of 
all known coreferring noun classes. The steps of 
the bootstrapping algorithm that learns both new 
heuristics and adjusts the similarity threshold of 
coreferential expressions i : 
MUTUAL BOOTSTRAPPING LOOP 
1. Score all candidate heuristics with FOIL_Gain 
2. Best_h--closest candidate to heuristics(COCKTAIL) 
3. Add Best_h to heuristics(COCKTAIL) 
,f. Adjust semantic similarity threshold for semantic 
consistency o\[ coreferring nouns 
5. Goto step 1 if the precision and recall did not 
degrade under minimal performance. 
(Riloff and Jones 1999) note that the bootstrap- 
ping algorithm works well but its performance can 
deteriorate rapidly when non-coreferring data enter 
as candidate heuristics. To make the algorithm ore 
robust, a second level of bootstrapping can be intro- 
duced. The outer bootstrapping mechanism, called 
145 
recta-bootstrapping compiles the results of the inner 
(mutual) bootstrapping process and identifies the k 
most reliable heuristics, where k is a number de- 
termined experimentally. These k heuristics are re- 
tained and the rest of them are discarded. 
3 SWIZZLE  
3.1 Mul t iHngua l  Core ference  Data  
To study the performance of a data-driven multi- 
lingual coreference r solution system, we prepared a
corpus of Romanian texts by translating the MUC-6 
and MUC-7 coreference training texts. The transla- 
tions were performed by a group of four Romanian 
native speakers, and were checked for style by a cer- 
tified translator from Romania. In addition, the Ro- 
manian texts were annotated with coreference keys. 
Two rules were followed when the annotations were 
done: 
o 1: Whenever an expression ER represents a trans- 
lation of an expression EE from the corresponding 
English text, if Es  is tagged as a coreference key 
with identification umber ID, then the Romanian 
expression ER is also tagged with the same ID num- 
ber. This rule allows for translations in which the 
textual position of the referent and the antecedent 
have been swapped. 
o2: Since the translations often introduce new 
coreferring expressions in the same chain, the new 
expressions are given new, unused ID numbers. 
For example, Table 3 lists corresponding English 
and Romanian fragments of coreference chains from 
the original MUC-6 Wall Street Journal document 
DOCNO: 930729-0143. 
Table 3 also shows the original MUC coreference 
SGML annotations. Whenever present, the REF tag 
indicates the ID of the antecedent, whereas the MIN 
tag indicates the minimal reference xpression. 
3.2 Lex ica l  Resources  
The multilingual coreference resolution method im- 
plemented in SWIZZLE incorporates the heuristics de- 
rived from COKCTAIL's monolingual coreference res- 
olution processing in both languages. To this end, 
COCKTAIL required both sets of texts to be tagged 
for part-of-speech and to recognize the noun phrases. 
The English texts were parsed with Brill's part-of- 
speech tagger (Brill 1992) and the noun phrases were 
identified by the grammar ules implemented in the 
phrasal parser of FASTUS (Appelt et al, 1993). Cor- 
responding resources are not available in Romanian. 
To minimize COCKTAIL's configuration for process- 
ing Romanian texts, we implemented a Romanian 
part-of-speech rule-based tagger that used the same 
Economic adviser Gene Sperling described 
<COREF ID="29" TYPE=' IDENT" REF-"30"> 
i t</COREF> as "a true full-court press" to pass 
<COREF ID="31" TYPE="IDENT" REF="26" 
MIN='bilr '  >the <COREF ID="32" 
TYPE="IDENT" REF-----"10" MIN="reduction"> 
<COREF ID="33" TYPE="IDENT" REF="12"> 
def i c i t</COREF>-reduct ion</COREF> 
bill, the final version of which is now being 
hammered out by <COREF ID=" 43" >House 
</COREF> and <COREF ID="41" >Senate 
</COREF>negot ia tors</COREF>.  
<COREF ID=" 34" TYPE=" IDENT" REF-"  2" > 
The executives</COREF>' backing - however tepid 
- gives the administration a way to counter 
<COREF ID="35" TYPE="IDENT" REF="36"> 
bus iness</COREF:> critics of <COREF ID="500" 
TYPE="IDENT" REF="31" MIN="package" 
STATUS=" OPT" >the  overall package 
</COREF>,. . .  
Consilierul cu probleme conomice Gene Sperling a 
descris-<COREF ID=" 29" TYPE="IDENT" 
REF="30" >o</COREF> ca pe un efort de 
avengur~ menit s~ promoveze <COREF ID=" 1125" 
TYPE="IDENT" REF="26" MIN="legea">legea 
</COREF> pentru <COREF TYPE="IDENT" 
REF=" 10" MIN="reducerea" > reducerea 
</COREF> <COREF ID=" 33" TYPE=" IDENT" 
REF=" 12"> deficitului n bugetul SUA</COREF>. 
Versiunea finals a acestei <COREF ID="1126" 
TYPE="IDENT" REF="l125" MIN="legi">legi 
</COI~EF> este desfiin~at~ chiax in aceste 
zile in cadrul dezbaterilor ce au loc in 
<COREF ID="43" >Camera  Reprezentat lv i lor  
</CORJ~F> ?i in <COREF ID="41"> 
Senat</COREF></COREF>.  
Sprijinirea <COREF ID="127" TYPE="IDENT" 
REF=" 1126" MIN="legii" >leg i i>/COREF> 
de c~tre speciali~ti in economic - de?i 
in manier~ moderat~ - ofer~ administra~iei o 
modalitate de a contrabalansa criticile aduse 
<COREF ID="500" TYPE="IDENT" REF="31" 
MIN=" legii" STATUS=" OPT" >leg i i</COREF> 
de c~tre companiile americane,... 
Table 3: Example of parallel English and Romanian 
text annotated for coreference. The elements from a 
coreference chain in the respective texts are under- 
lined. The English text has only two elements in the 
coreference chain, whereas the Romanian text con- 
tains four different elements. The two additional ele- 
ments of the Romanian coreference chain are derived 
due to (1) the need to translate the relative clause 
from the English fragment into a separate sentence 
in Romanian; and (2) the reordering of words in the 
second sentence. 
146 
tags as generated by the Brill tagger. In addition, 
we implemented rules that identify noun phrases in 
Romanian. 
To take advantage of the aligned corpus, SWIZZLE 
also relied on bilingual exical resources that help 
translate the referential expressions. For this 
purpose, we used a core Romanian WordNet 
(Harabagiu, 1999) which encoded, wherever possi- 
ble, links between the English synsets and their Ro- 
manian counterparts. This resource also incorpo- 
rated knowledge derived from several bilingual dic- 
tionaries (e.g. (Bantam, 1969)). 
Having the parallel coreference annotations, we 
can easily identify their translations because they 
have the same identification coreference k y. Look- 
ing at the example given in Table 3, the expres- 
sion "legii', with ID=500 is the translation of the 
expression "package", having the same ID in the 
English text. However, in the test set, the REF 
fields are intentionally voided, entrusting COCKTAIL 
to identify the antecedents. The bilingual corefer- 
ence resolution performed in SWIZZLE, however, re- 
quires the translations ofthe English and Romanian 
antecedents. The principles guiding the translations 
of the English and Romanian antecedents (AE-R 
and A R-E, respectively) are: 
? Circularity: Given an English antecedent, due to 
semantic ambiguity, it can belong to several English 
WordNet sysnsets. For each such sysnset S/~ we con- 
sider the Romanian corresponding sysnet(s) Sff. We 
filter out all Sff that do not contain A E-R. If only 
one Romanian sysnset is left, then we identified a
translation. Otherwise, we start from the Roma- 
nian antecedent, find all synsets SR to which it be- 
longs, and obtain the corresponding English sysnets 
S F. Similarly, all English synsets not containing 
the English antecedent are filtered out. If only one 
synset remains, we have again identified a transla- 
tion. Finally, in the last case, the intersection of 
the multiple synsets in either language generates a 
legal translation. For example, the English synset 
S E ={bill, measure} translates into the Romanian 
synset S R ={lege}. First, none of the dictionary 
translations of bill into Romanian (e.g. politE, bac- 
notE, afi~) translate back into any of the elements 
of S E. However the translation of measure into the 
Romanian lege translates back into bill, its synonym. 
? Semantic density: Given an English and a Roma- 
nian antecedent, to establish whether they are trans- 
lations of one another, we disambiguate them by first 
collapsing all sysnsets that have common elements. 
Then we apply the circularity principle, relying on 
the semantic alignment encoded in the Romanian 
WordNet. When this core lexical database was first 
implemented, several other principles were applied. 
In our experiment, we were satisfied with the qual- 
ity of the translations recognized by following only 
these two principles. 
3.3 Mult i l ingual Coreference Resolution 
The SWIZZLE system was run on a corpus of 2335 
referential expressions in English (927 from MUC- 
6 and 1408 from MUC-7) and 2851 Romanian ex- 
pressions (1219 from MUC-6 and 1632 from MUC- 
7). Initially, the heuristics implemented in COCKTAIL 
were applied separately to the two textual collec- 
tions. Several special cases arose. 
English Text 
. . . .  :rr-Z, la ,on . . . . . . .  
Translation 
Romanian Text 
"~eference  
Figure 3: Case 1 of multilingual coreference 
Case 1, which is the ideal case, is shown in Fig- 
ure 3. It occurs when two referential expressions 
have antecedents hat are translations of one an- 
other. This situation occurred in 63.3% of the refer- 
ential expressions from MUC-6 and in 58.7% of the 
MUC-7 references. Over 50% of these are pronouns 
or named entities. However, all the non-ideal cases 
are more interesting for SWIZZLE, since they port 
knowledge that enhances system performance. 
Coref. English Text chains 
E . . . . . . . . . . . . . . . . . .  
H 4 ~  ................. 
Translation 
ER ~ .. . . . . . . . . . . . . . . . . . . . . . . . . . .  
Translation 
Romanian Text 
? ~ ......... RA 
'~  ?(R)RR 
ER: English reference RR: Romanian reference 
EA: English antecedent RA: Romanian antecedent 
ET: English translation RT: Romanian translation 
of Romanian antecedent of English antecedent 
Figure 4: Case 2 of multilingual coreference 
Case 2 occurs when the antecedents are not trans- 
lations, but belong to or corefer with elements of 
some coreference chains that were already estab- 
lished. Moreover, one of the antecedents is textually 
147 
closer to its referent. Figure 4 illustrates the case 
when the English antecedent is closer to the referent 
than the Romanian one. 
SWIZZLE Solutions: (1) If the heuristic H(E) used 
to resolve the reference inthe English text has higher 
priority than H(R), which was used to resolve the 
reference from the Romanian text, then we first 
search for RT, the Romanian translation of EA, the 
English antecedent. In the next step, we add heuris- 
tic H1 that resolves RR into RT, and give it a higher 
priority than H(R). Finally, we also add heuristic H2 
that links RTto RA when there is at least one trans- 
lation between the elements of the coreference hains 
containing EA and ET respectively. 
(2) If H(R) has higher priority than H(E), heuris- 
tic H3 is added while H(E) is removed. We also add 
//4 that relates ER to ET, the English translation of 
RA. 
Case 3 occurs when at least one of the antecedents 
starts a new coreference chain (i.e., no coreferring 
antecedent can be found in the current chains). 
SWIZZLE Solution: If one of the antecedents 
corefers with an element from a coreference chain, 
then the antecedent in the opposite language is its 
translation. Otherwise, SNIZZLE chooses the an- 
tecedent returned by the heuristic with highest pri- 
ority. 
4 Resu l ts  
The foremost contribution of SWIZZLE was that it 
improved coreference r solution over both English 
and Romanian texts when compared to monolingual 
coreference r solution performance in terms of preci- 
sion and recall. Also relevant was the contribution of 
SNIZZLE to the process of understanding the cultural 
differences expressed in language and the way these 
differences influence coreference r solution. Because 
we do not have sufficient space to discuss this issue 
in detail here, let us state, in short, that English is 
more economical than Romanian in terms of referen- 
tial expressions. However the referential expressions 
in Romanian contribute to the resolution of some of 
the most difficult forms of coreference in English. 
4.1 Precis ion and Recall 
Table 4 summarizes the precision results for both 
English and Romanian coreference. The results in- 
dicate that the English coreference is more pre- 
cise than the Romanian coreference, but SNIZZLE 
improves coreference r solution in both languages. 
There were 64% cases when the English coreference 
was resolved by a heuristic with higher priority than 
the corresponding heuristic for the Romanian coun- 
terpart. This result explains why there is better pre- 
cision enhancement for the English coreference. 
English 
Romanian 
SWIZZLE on 
English 
SWIZZLE on 
Romanian 
Nominal Pronominal 
73% 89% 
66% 78% 
76% 93% 
71?/o 82% 
Table 4: Coreference precision 
Total 
84% 
72% 
87% 
76% 
English 
Romanian 
SWIZZLE on 
English 
SWIZZLE on 
Romanian 
Nominal 
69% 
63% 
66% 
61% 
Pronominal Total 
89% 78% 
83% 72% 
87% 77% 
80% 70% 
Table 5: Coreference r call 
Table 5 also illustrates the recall results. The 
advantage of the data-driven coreference r solution 
over other methods is based on its better ecall per- 
formance. This is explained by the fact that this 
method captures a larger variety of coreference pat- 
terns. Even though other coreference r solution sys- 
tems perform better for some specific forms of refer- 
ence, their recall results are surpassed by the data- 
driven approach. Multilingual coreference in turn 
improves more the precision than the recall of the 
monolingual data-driven coreference systems. 
In addition, Table 5 shows that the English coref- 
erence results in better ecall than Romanian coref- 
erence. However, the recall shows a decrease for both 
languages for SNIZZLE because imprecise coreference 
links are deleted. As is usually the case, deleting 
data lowers the recall. All results were obtained by 
using the automatic scorer program developed for 
the MUC evaluations. 
5 Conc lus ions  
We have introduced a new data-driven method for 
multilingual coreference r solution, implemented in
the SWIZZLE system. The results of this method 
are encouraging since they show clear improvements 
over monolingual coreference r solution. Currently, 
we are also considering the effects of a bootstrap- 
ping algorithm for multilingual coreference r solu- 
tion. Through this procedure we would learn con- 
currently semantic onsistency knowledge and bet- 
ter performing heuristic rules. To be able to de- 
velop such a learning approach, we must first develop 
a method for automatic recognition of multilingual 
referential expressions. 
148 
We also believe that a better performance valu- 
ation of SidIZZLE can be achieved by measuring its 
impact on several complex applications. We intend 
to analyze the performance of SIdIZZLE when it is 
used as a module in an IE system, and separately in 
a Question/Answering system. 
Acknowledgements  This paper is dedicated to the 
memory of our friend Megumi Kameyama, who in- 
spired this work. 
Re ferences  
Douglas E. Appelt, Jerry R. Hobbs, John Bear, David 
Israel, Megumi Kameyama nd Mabry Tyson. 1993. 
The SRI MUC-5 JV-FASTUS Information Extraction 
System. In Proceedings of the Fifth Message Under- 
standing Conference (MUC-5). 
Brack Baldwin. 1997. CogNIAC: high precision corefer- 
ence with limited knowledge and linguistic resources. 
In Proceedings of the ACL '97/EACL '97 Workshop on 
Operational factors in practical, robust anaphora res- 
olution, pages 38-45, Madrid, Spain. 
Andrei Bantam. 1969. Dic~ionar Rom?n-Englez, Enlgez- 
Romfi~. Editura ~tiin~ific~, Bucure~ti. 
David Bean and Ellen Riloff. 1999. Corpus-Based I en- 
tification of Non-Anaphoric Noun Phrases. In Pro- 
ceedings of the 37th Conference of the Assosiation for 
Computatioanl Linguistics (A CL-99), pages 373-380. 
Eric Brill. A simple rule-based part of speech tagger. In 
Proceedings of the Third Conference on Applied Nat- 
ural Language Processing, pages 152-155, 1992. 
Joseph F. Cameron-Jones and Ross Quinlan. 1993. 
Avoiding Pitfalls When Learning Recursive Theories. 
In Proceedings of the 13th International Joint Confer- 
ence on Artificial Intelligence (IJCAI-93), pages 1050- 
1055. 
Claire Cardie and Kiri Wagstaff. 1999. Noun phrase 
coreference as clustering. In Proceedings of the Joint 
Conference on Empirical Methods in NLP and Very 
Large Corpora, pages 82-89. 
Niyu Ge, John Gale and Eugene Charniak. 1998. 
Anaphora Resolution: A Multi-Strategy Approach. In 
Proceedings of the 6th Workshop on Very Large Cor- 
pora, (COLING/A CL '98). 
Ido Dagan and Ken W. Church. 1994. TERMIGHT: 
Identifying and translating technical terminology. In 
Proceedings of the ~th ACL Conference on Applied 
Natural Language Processing (ANLP-94). 
Sanda M. Harabagiu. 1999. Lexical Acquisition for a 
Romanian WordNet. Proceeding of the 3rd European 
Summer School on Computational Linguistics. 
Sanda M. Harabagiu and Steve J. Maiorano. 1999. 
Knowledge-Lean Coreference Resolution and its Re- 
lation to Textual Cohesion and Coherence. In Pro- 
ceedings of the Workshop on the Relation of Dis- 
course/Dialogue Structure and Reference, ACL'98, 
pages 29-38. 
Jerry R. Hobbs. Resolving pronoun references. Lingua, 
44:311-338. 
Andrew Kehler. 1997. Probabilistic Coreference in In- 
formation Extraction. In Proceedings of the Second 
Conference on Empirical Methods in Natural Lan- 
guage Processing (SIGDAT), pages 163-173. 
Shalom Lappin and Herbert Leass. 1994. An algorithm 
for pronominal anaphora resolution. Computational 
Linguistics, 20(4):535-562. 
Rosie Jones, Andrew McCallum, Kevin Nigam and Ellen 
Riloff. 1999. Bootstrapping for Text Learning Tasks. 
In Proceedings of the IJCAI-99 Workshop on Text 
Mining: Foundations, Techniques, and Applications. 
Megumi Kameyama. 1997. Recognizing Referential 
Links: An Information Extraction Perspective. In 
Proceedings of the Workshop on Operational Factors 
in Practical, Robust Anaphora Resolution for Un- 
restricted Texts, (ACL-97/EACL-97), pages 46-53, 
Madrid, Spain. 
Christopher Kennedy and Branimir Bogureav. 1996. 
Anaphora for everyone: Pronominal anaphora reso- 
lution without a parser. In Proceedings of the 16th 
International Conference on Computational Linguis- 
tics (COLING-96). 
George A. Miller. 1995. WordNet: A Lexical Database. 
Communication of the A CM, 38(11):39-41. 
Ruslan Mitkov. 1998. Robust pronoun resolution 
with limited knowledge. In Proceedings of COLING- 
ACL'98, pages 869-875. 
1996. Proceedings of the Sixth Message Understanding 
Conference (MUC-6),Morgan Kaufmann, San Mateo, 
CA. 
1998. Proceedings of the Seventh Message Understand- 
ing Conference (MUC-7) ,Morgan Kaufmann, San 
Mateo, CA. 
Philip Resnik and I. Dan Melamed. 1997. Semi- 
Automatic Acquisition of Domain-Specific Translation 
Lexicons. In Proceedings of the 5th ACL Conference 
on Applied Natural Language Processing (ANLP-97). 
Ellen Riloff and Rosie Jones. 1999. Learning Dictionar- 
ies for Information Extraction by Multi-Level Boot- 
strapping. In Proceedings of the Sixteenth National 
Conference on Artificial Intelligence (AAAI-99). 
Frank Smadja, Katheleen R. McKeown and Vasileios 
Hatzivassiloglou. 1996. Translating collocations for 
bilingual exicons: A statistical approach. Computa- 
tional Linguistics , 21(1):1-38. 
149 
