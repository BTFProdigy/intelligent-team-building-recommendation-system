2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 91?102,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Intrinsic and Extrinsic Evaluation of an Automatic User Disengagement
Detector for an Uncertainty-Adaptive Spoken Dialogue System
Kate Forbes-Riley and Diane Litman and Heather Friedberg and Joanna Drummond?
University of Pittsburgh
Pittsburgh, PA 15260, USA
forbesk@pitt.edu, litman@pitt.edu, haf13@pitt.edu
Abstract
We present a model for detecting user dis-
engagement during spoken dialogue interac-
tions. Intrinsic evaluation of our model (i.e.,
with respect to a gold standard) yields results
on par with prior work. However, since our
goal is immediate implementation in a sys-
tem that already detects and adapts to user un-
certainty, we go further than prior work and
present an extrinsic evaluation of our model
(i.e., with respect to the real-world task). Cor-
relation analyses show crucially that our au-
tomatic disengagement labels correlate with
system performance in the same way as the
gold standard (manual) labels, while regres-
sion analyses show that detecting user disen-
gagement adds value over and above detecting
only user uncertainty when modeling perfor-
mance. Our results suggest that automatically
detecting and adapting to user disengagement
has the potential to significantly improve per-
formance even in the presence of noise, when
compared with only adapting to one affective
state or ignoring affect entirely.
1 Introduction
Spoken dialogue systems that can detect and adapt
to user affect1 are fast becoming reality (Schuller
et al, 2009b; Batliner et al, 2008; Prendinger and
Ishizuka, 2005; Vidrascu and Devillers, 2005; Lee
?Now at Univ. Toronto: jdrummond@cs.toronto.edu
1We use affect for emotions and attitudes that affect how
users communicate. Other speech researchers also combine
concepts of emotion, arousal, and attitudes where emotion is
not full-blown (Cowie and Cornelius, 2003).
and Narayanan, 2005; Shafran et al, 2003). The
benefits are clear: affect-adaptive systems have been
shown to increase task success (Forbes-Riley and
Litman, 2011a; D?Mello et al, 2010; Wang et al,
2008) or improve other system performance met-
rics such as user satisfaction (Liu and Picard, 2005;
Klein et al, 2002). However, to date most affec-
tive systems researchers have focused either only on
affect detection, or only on detecting and adapting
to a single affective state. The next step is thus to
develop and evaluate spoken dialogue systems that
detect and respond to multiple affective states.
We previously showed that detecting and re-
sponding to user uncertainty during spoken dialogue
computer tutoring significantly improves task suc-
cess (Forbes-Riley and Litman, 2011a). We are
now taking the next step: incorporating automatic
detection and adaptation to user disengagement as
well, with the goal of further improving task suc-
cess. We targeted user uncertainty and disengage-
ment because manual annotation showed them to be
the two most common user affective states in our
system and both are negatively correlated with task
success (Litman and Forbes-Riley, 2009; Forbes-
Riley and Litman, 2011b). Thus, we hypothesize
that providing appropriate responses to these states
would reduce their frequency, consequently improv-
ing task success. Although we address these user
states in the tutoring domain, spoken dialogue re-
searchers across domains and applications have in-
vestigated the automatic detection of both user un-
certainty (e.g. (Drummond and Litman, 2011; Pon-
Barry and Shieber, 2011; Paek and Ju, 2008; Alwan
et al, 2007)) and user disengagement (e.g., (Schuller
91
et al, 2010; Wang and Hirschberg, 2011; Schuller
et al, 2009a)), to improve system performance.
The detection of user disengagement in particular
has received substantial attention in recent years,
due to growing awareness of its potential for neg-
atively impacting commercial applications (Wang
and Hirschberg, 2011; Schuller et al, 2009a).
In this paper we present a model for automati-
cally detecting user disengagement during spoken
dialogue interactions. Intrinsic evaluation of our
model yields results on par with those of prior work.
However, we argue that while intrinsic evaluations
are necessary, they aren?t sufficient when immedi-
ate implementation is the goal, because there is no a
priori way to know when the model?s performance is
acceptable to use in a working system. This problem
is particularly relevant to affect detection because it
is such a difficult task, where no one achieves near-
perfect results. We argue that for such tasks some
extrinsic evaluation is also necessary, to show that
the automatic labels are useful and/or are a reason-
able substitute for a gold standard before undertak-
ing a labor-intensive and time-consuming evaluation
with real users. Here we use correlational analy-
ses to show that our automatic disengagement la-
bels are related to system performance in the same
way as the gold standard (manual) labels. We fur-
ther show through regression analyses that detecting
user disengagement adds value over and above de-
tecting only user uncertainty when modeling perfor-
mance. These results provide strong evidence that
enhancing a spoken dialogue system to detect and
adapt to multiple affective states (specifically, user
disengagement and uncertainty) has the potential to
significantly improve performance even in the pres-
ence of noise due to automatic detection, when com-
pared with only adapting to one affective state or ig-
noring affect entirely.
2 Related Work
Our focus in this paper is on first using machine
learning to develop a detector of user disengagement
for spoken dialogue systems, and then evaluating its
usefulness as fully as possible prior to its implemen-
tation and deployment with real users.
Disengaged users are highly undesirable in
human-computer interaction because they increase
the potential for user dissatisfaction and task fail-
ure; thus over the past decade there has already been
substantial prior work focused on detecting user dis-
engagement and the closely related states of bore-
dom, motivation and lack of interest (e.g., (Schuller
et al, 2010; Wang and Hirschberg, 2011; Jeon et
al., 2010; Schuller et al, 2009a; Bohus and Horvitz,
2009; Martalo et al, 2008; Porayska-Pomsta et al,
2008; Kapoor and Picard, 2005; Sidner and Lee,
2003; Forbes-Riley and Litman, 2011b)).
Within this work, specific affect definitions vary
slightly with the intention of being coherent within
the application and domain and being relevant to the
specific adaptation goal (Martalo et al, 2008). How-
ever, affective systems researchers generally agree
that disengaged users show little involvement in the
interaction, and often display facial, gestural and lin-
guistic signals such as gaze avoidance, finger tap-
ping, humming, sarcasm, et cetera.
The features used to detect disengagement also
vary depending on system domain and applica-
tion. For example, Sidner & Lee (2003) are in-
terested in modeling more natural and collabora-
tive human-robot interactions during basic conver-
sations. They define an algorithm for the engage-
ment process that involves appropriate eye gaze and
turn-taking. Martalo et al (2008) study how user
engagement influences dialogue patterns during in-
teractions with an embodied agent that gives ad-
vice about healthy dieting. They model engage-
ment using manually coded dialogue acts based on
the SWBDL-DAMSL scheme (Stolcke et al, 2000).
Bohus and Horvitz (2009) study systems that attract
and engage users for dynamic, multi-party dialogues
in open-world settings. They model user intentions
to engage the system with cues from facial sensors
and the dialogue. Within recent spoken dialogue
research, acoustic-prosodic, lexical and contextual
features have been found to be effective detectors
of disengagement (Schuller et al, 2010; Wang and
Hirschberg, 2011; Jeon et al, 2010); we will briefly
compare our own results with these in Section 5.
While all of the above-mentioned research has
presented intrinsic evaluations of their disengage-
ment modeling efforts that indicate a reasonable de-
gree of accuracy as compared to a gold standard
(e.g., manual coding), only a few have yet demon-
strated that the model?s detected values are useful
92
in practice and/or are a reasonable substitute for
the gold standard with respect to some practical
objective (e.g., a relationship to performance). In
particular, two studies (Bohus and Horvitz, 2009;
Schuller et al, 2009a) have gone directly from in-
trinsic evaluation of (dis)engagement models to per-
forming user studies with the implemented model,
thereby bypassing other less expensive and less
labor-intensive means of extrinsic evaluation to
quantify their model?s usefulness?and potentially in-
dicate its need to be further improved?before de-
ployment with real users. Neither study reports sta-
tistically significant improvements in system perfor-
mance as a result of detecting user (dis)engagement.
Finally, while substantial spoken dialogue and af-
fective systems research has shown that users dis-
play a range of affective states while interacting with
a system (e.g. (Schuller et al, 2009b; Conati and
Maclaren, 2009; Batliner et al, 2008; Devillers and
Vidrascu, 2006; Lee and Narayanan, 2005; Shafran
et al, 2003; Ang et al, 2002)), to date only a few af-
fective systems have been built that detect and adapt
to multiple user affective states (e.g., (D?Mello et al,
2010; Aist et al, 2002; Tsukahara andWard, 2001)),
and most of these have been deployed with cru-
cial natural language processing components ?wiz-
arded? by a hidden human agent (e.g., who performs
speech recognition or affect annotation on the user
turns); moreover, none have yet shown significant
improvements in system performance as a result of
adapting to multiple user affective states.
3 ITSPOKE: Spoken Dialogue Tutor
We develop and evaluate our disengagement detec-
tor using a corpus of spoken dialogues from a 2008
controlled experiment evaluating our uncertainty-
adaptive spoken dialogue tutoring system, IT-
SPOKE (Intelligent Tutoring SPOKEn dialog sys-
tem) (Forbes-Riley and Litman, 2011a).2
ITSPOKE tutors 5 Newtonian physics problems
(one per dialogue), using a Tutor Question - Stu-
dent Answer - Tutor Response format. After
each tutor question, the student speech is digi-
tized from head-mounted microphone input and sent
2ITSPOKE is a speech-enhanced and otherwise modified
version of the Why2-Atlas text-based qualitative physics tu-
tor (VanLehn et al, 2002).
to the Sphinx2 recognizer, which yields an auto-
matic transcript (Huang et al, 1993). This an-
swer?s (in)correctness is then automatically classi-
fied based on this transcript, using the TuTalk se-
mantic analyzer (Jordan et al, 2007), and the an-
swer?s (un)certainty is automatically classified by
inputting features of the speech signal, the automatic
transcript, and the dialogue context into a logistic
regression model. We will discuss these features
further in Section 5. All natural language process-
ing components were trained using prior ITSPOKE
corpora. The appropriate tutor response is deter-
mined based on the answer?s automatically labeled
(in)correctness and (un)certainty and then sent to the
Cepstral text-to-speech system3, whose audio output
is played through the student headphones and is also
displayed on a web-based interface.
The experimental procedure was as follows: col-
lege students with no college-level physics (1) read
a short physics text, (2) took a pretest, (3) worked
5 ?training? problems with ITSPOKE, where each
user received a varying level of uncertainty adapta-
tion based on condition, (4) took a user satisfaction
survey, (5) took a posttest isomorphic to the pretest,
and (6) worked a ?test? problem with ITSPOKE that
was isomorphic to the 5th training problem, where
no user received any uncertainty adaptation.
The resulting corpus contains 432 dialogues (6
per student) and 7216 turns from 72 students, 47
female and 25 male. All turns are used in the dis-
engagement detection experiments described next.
However, only the training problem dialogues (360,
5 per student, 6044 student turns) are used for the
performance analyses in Sections 6-7, because the
final test problem was given after the instruments
measuring performance (survey and posttest).
Our survey and tests are the same as those used in
multiple prior ITSPOKE experiments (c.f., (Forbes-
Riley and Litman, 2011a)). The pretest and posttest
each contain 26 multiple choice questions querying
knowledge of the topics covered in the dialogues.
Average pretest and posttest scores in the corpus
were 51.0% and 73.1% (out of 100%) with stan-
dard deviations of 14.5% and 13.8%, respectively.
The user satisfaction survey contains 16 statements
rated on a 5-point Likert scale. Average total sur-
3an outgrowth of Festival (Black and Taylor, 1997).
93
vey score was 60.9 (out of 80), with a standard de-
viation of 8.5. While the statements themselves are
listed elsewhere (Forbes-Riley and Litman, 2009),
9 statements concern the tutoring domain (e.g., The
tutor was effective/precise/useful), 7 of which were
taken from (Baylor et al, 2003) and 2 of which
were created for our system. 3 statements concern
user uncertainty levels and were created for our sys-
tem. 4 statements concern the spoken dialogue in-
teraction (e.g., It was easy to understand the tutor?s
speech) and were taken from (Walker et al, 2002).
Our survey has also been incorporated into other re-
cent work exploring user satisfaction in spoken dia-
logue computer tutors (Dzikovska et al, 2011). In
Section 6 we discuss how user scores on these in-
struments are used to measure system performance.
See (Forbes-Riley and Litman, 2011a) for further
details of ITSPOKE and the 2008 experiment.
Following the experiment, the entire corpus
was manually labeled for (in)correctness (cor-
rect, incorrect), (un)certainty (CER, UNC) and
(dis)engagement (ENG, DISE) by one trained an-
notator. Table 1 shows the distribution of the la-
beled turns in the 2008 ITSPOKE corpus. In prior
ITSPOKE corpora, our annotator displayed interan-
notator agreement of 0.85 and 0.62 Kappa on cor-
rectness and uncertainty, respectively (Forbes-Riley
and Litman, 2011a). For the disengagement label,
a reliability analysis was performed over several an-
notation rounds on subsets of the 2008 ITSPOKE
corpus by this and a second trained annotator, yield-
ing 0.55 Kappa (this analysis is described in detail
elsewhere (Forbes-Riley et al, 2011)). Our Kap-
pas indicate that user uncertainty and disengage-
ment can both be annotated with moderate reliabil-
ity in our dataset, on par with prior emotion anno-
tation work (c.f., (Pon-Barry and Shieber, 2011)).
Note however that the best way to label users? in-
ternal affective state(s) is still an open question.
Many system researchers (including ourselves) rely
on trained labelers (e.g., (Pon-Barry et al, 2006;
Porayska-Pomsta et al, 2008)) while others use self-
reports (e.g., (Conati and Maclaren, 2009; Gratch et
al., 2009; McQuiggan et al, 2008)). Both meth-
ods are problematic; for example both can be ren-
dered inaccurate when users mask their true feel-
ings. Two studies that have compared self-reports,
peer labelers, trained labelers, and combinations of
labelers (Afzal and Robinson, 2011; D?Mello et al,
2008) both illustrate the common finding that hu-
man annotators display low to moderate interannota-
tor reliability for affect annotation, and both studies
show that trained labelers yield the highest reliabil-
ity on this task. Despite the lack of high interan-
notator reliability, responding to affect detected by
trained human labels has still been shown to improve
system performance (see Section 1).
Table 1: 2008 ITSPOKE Corpus Description (N=7216)
Turn Label Total Percent
Disengaged 1170 16.21%
Correct 5330 73.86%
Uncertain 1483 20.55%
Uncertain+Disengaged 373 5.17%
4 Automatically Detecting User
Disengagement (DISE) in ITSPOKE
As noted in Section 1, we have developed a user dis-
engagement detector to incorporate into our existing
uncertainty-adaptive spoken dialogue system. The
result will be a state of the art system that adapts to
multiple affective states during the dialogue.
4.1 Binary DISE Label
Our disengagement annotation scheme (Forbes-
Riley et al, 2011) was derived from empirical ob-
servations in our data but draws on prior work,
including work mentioned in Section 2, appraisal
theory-based emotion models (e.g., Conati and Ma-
claren (2009))4, and prior approaches to annotating
disengagement or related states in tutoring (Lehman
et al, 2008; Porayska-Pomsta et al, 2008).
Briefly, our overall Disengagement label (DISE)
is used for turns expressing moderate to strong dis-
engagement towards the interaction, i.e., responses
given without much effort or without caring about
appropriateness. Responses might also be accompa-
nied by signs of inattention, boredom, or irritation.
Clear examples include answers spoken quickly in
leaden monotone, with sarcastic or playful tones,
or with off-task sounds such as rhythmic tapping or
4Appraisal theorists distinguish emotional behaviors from
their underlying causes, arguing that emotions result from an
evaluation of a context.
94
electronics usage.5 Note that our DISE label is de-
fined independently of the tutoring domain and thus
should generalize across spoken dialogue systems.
Figure 1 illustrates the DISE, (in)correctness, and
(un)certainty labels across 3 tutor/student turn pairs.
U1 is labeled DISE and UNC because the student
gave up immediately and with irritation when too
much prior knowledge was required. U2 is labeled
DISE and UNC because the student avoided giv-
ing a specific numerical value, offering instead a
vague (and obviously incorrect) answer. U3 is la-
beled DISE and CER because the student sang the
correct answer, indicating a lack of interest in the
larger purpose of the material being discussed.6
T1: What is the definition of Newton?s Second Law?
U1: I have no idea <sigh>. (DISE, incorrect, UNC)
. . .
T2: What?s the numerical value of the man?s accelera-
tion? Please specify the units too.
U2: The speed of the elevator. Meters per second. (DISE,
incorrect, UNC)
. . .
T3: What are the forces acting on the keys after the man
releases them?
U3: graaa-vi-tyyyyy <sings the answer> (DISE, cor-
rect, CER)
Figure 1: Corpus Example Illustrating the User Turn La-
bels ((Dis)Engagement, (In)Correctness, (Un)Certainty)
4.2 DISE Detection Method
Machine learning classification was done at the turn
level using WEKA software7 and 10-fold cross val-
idation. A J48 decision tree was chosen because of
its easily read output and the fact that previous ex-
periments with our data showed little variance be-
5Affective systems research has found total disengagement
rare in laboratory settings (Lehman et al, 2008; Martalo et al,
2008). As in that research, we equate the DISE label with no
or low engagement. Since total disengagement is common in
real-world unobserved human-computer interactions (deleting
unsatisfactory software being an extreme example) it remains
an open question as to how well laboratory findings generalize.
6Our original scheme distinguished six DISE subtypes
that trained annotators distinguished with a reliability of .43
Kappa (Forbes-Riley et al, 2011). However, pilot experiments
indicated that our models cannot accurately distinguish them,
thus our DISE detector focuses on the DISE label.
7http://www.cs.waikato.ac.nz/ml/weka/
tween different machine learning algorithms (Drum-
mond and Litman, 2011). We also use a cost matrix,
which heavily penalizes classifying a true DISE in-
stance as false, because our class distributions are
highly skewed (16.21% DISE turns) and the cost
matrix successfully mitigated the skew?s effect in
our prior work, where the uncertainty distribution is
also skewed (20.55% UNC turns) (Drummond and
Litman, 2011).
To train our DISE model, we first extracted the set
of speech and dialogue features shown in Figure 2
from the user turns in our corpus. As shown, the
acoustic-prosodic features represent duration, paus-
ing, pitch, and energy, and were normalized by the
first user turn, as well as totaled and averaged over
each dialogue. The lexical and dialogue features
consist of the current dialogue name (i.e., one of the
six physics problems) and turn number, the current
ITSPOKE question?s name (e.g.,T3 in Figure 1 has
a unique identifier) and depth in the discourse struc-
ture (e.g., an ITSPOKE remediation question after
an incorrect user answer would be at one greater
depth than the prior question), a word occurrence
vector for the automatically recognized text of the
user turn, an automatic (in)correctness label, and
lastly, the number of user turns since the last cor-
rect turn (?incorrect runs?). We also included two
user-based features, gender and pretest score.
? Acoustic-Prosodic Features
temporal features: turn duration, prior pause dura-
tion, turn-internal silence
fundamental frequency (f0) and energy (RMS) fea-
tures: maximum, minimum, mean, std. deviation
running totals and averages for all features
? Lexical and Dialogue Features
dialogue name and turn number
question name and question depth
ITSPOKE-recognized lexical items in turn
ITSPOKE-labeled turn (in)correctness
incorrect runs
? User Identifier Features:
gender and pretest score
Figure 2: Features Used to Detect Disengagement (DISE)
for each User Turn
95
Table 2: Results of 10-fold Cross-Validation Experiment with J48 Decision Tree Algorithm Detecting the Binary DISE
Label in the 2008 ITSPOKE Corpus (N=7216 user turns)
Algorithm Accuracy UA Precision UA Recall UA Fmeasure CC MLE
Decision Tree 83.1% 68.9% 68.7% 68.8% 0.52 0.25
Majority Label 83.8% 41.9% 50.0% 45.6% ? 0.27
Note that although our feature set was drawn pri-
marily from our prior uncertainty detection exper-
iments (Forbes-Riley and Litman, 2011a; Drum-
mond and Litman, 2011), we have also experi-
mented with other features, including state-of-the-
art acoustic-prosodic features used in the last Inter-
speech Challenges (Schuller et al, 2010; Schuller et
al., 2009b) and made freely available in the openS-
MILE Toolkit (Florian et al, 2010). To date, how-
ever, these features have only decreased the cross-
validation performance of our models.8 While some
of our features are tutoring-specific, these have sim-
ilar counterparts in other applications (i.e., answer
(in)correctness corresponds to a more general no-
tion of ?response appropriateness? in other domains,
while pretest score corresponds to the general no-
tion of domain expertise). Moreover, all of our fea-
tures are fully automatic and available in real-time,
so that the model can be directly implemented and
deployed. To that end, we now describe the results
of our intrinsic and extrinsic evaluations of our DISE
model, aimed at determining whether it is ready to
be evaluated with real users.
5 Intrinsic Evaluation: Cross-Validation
Table 2 shows the averaged results of the cross-
validation with the J48 decision tree algorithm. In
addition to accuracy, we use Unweighted Aver-
age (UA) Precision9, Recall, and F-measure be-
cause they are the standard measures used to eval-
uate current affect recognition technology, particu-
larly for unbalanced two-class problems (Schuller
et al, 2009b). In addition, we use the cross corre-
lation (CC) measure and mean linear error (MLE)
because these metrics were used in recent work for
evaluating disengagement (level of interest) detec-
tors for the Interspeech 2010 challenge (Schuller et
8We also tried using our automatic UNC label as a feature in
our DISE model, but our results weren?t significantly improved.
9simply ((Precision(DISE) + Precision(ENG))/2)
al., 2010; Wang and Hirschberg, 2011; Jeon et al,
2010)).10 Note however that the Interspeech 2010
task differs from ours not only in the corpus and fea-
tures, but also in the learning task: they used regres-
sion to detect a continuous level of interest ranging
from 0 to 1, while we detect a binary class. Thus
comparison between our results and those are only
suggestive rather than conclusive.
As shown in Table 2, we also compare our results
with those of majority class (ENG) labeling of the
same turns. Since (7216-1170)/7216 user turns in
the corpus are engaged (recall Table 1), always se-
lecting the majority class (ENG) label for these turns
thus yields 83.8% accuracy (with 0% precision and
recall for DISE, and 83.8% precision and 100% re-
call for ENG). While our DISE model does not out-
perform majority class labeling with respect to ac-
curacy, this is not surprising given the steep skew
in class distribution, and our learned model signif-
icantly outperforms the baseline with respect to all
the other measures (p<.001).11
Our CC and MLE results are on par with the best
results from the state-of-the-art systems competing
in the 2010 Interspeech Challenge, where the task
was to detect level of interest. In particular, the win-
ner obtained a CC of 0.428 (higher numbers are bet-
ter) and an MLE of 0.146 (lower numbers are bet-
ter) (Jeon et al, 2010), while a subsequent study
yielded a CC of 0.480 and an MLE of 0.131 on
the same corpus (Wang and Hirschberg, 2011). Our
results are also on par with the best results of the
other prior research on detecting disengagement dis-
cussed in Section 2 that detects a small number of
disengagement classes and reports accuracy and/or
recall and precision. For example, (Martalo et al,
2008) report average precision of 75% and recall
10Pearson product-moment correlation coefficient (CC) is a
measure of the linear dependence that is widely used in regres-
sion settings. MLE is a regression performance measure for the
mean absolute error between an estimator and the true value.
11CC is undefined for majority class labeling.
96
of 74% (detecting three levels of disengagement),
while (Kapoor and Picard, 2005) report an accuracy
of 86% for detecting binary (dis)interest.
Our final DISE model was produced by running
the J48 algorithm over our entire corpus. The re-
sulting decision tree contains 141 nodes and 75
leaves. Inspection of the tree reveals that all of the
feature types in Figure 2 (acoustic-prosodic, lexi-
cal/dialogue, user identifier) are used as decision
nodes in the tree, although not all variations on these
types were used. The upper-level nodes of the tree
are usually considered to be more informative fea-
tures as compared to lower-level nodes, since they
are queried for more leaves. The upper level of
the DISE model consists entirely of temporal, lex-
ical, pitch and energy features as well as question
name and depth and incorrect runs, while features
such as gender, turn number, and dialogue name
appear only near the leaves, and pretest score and
turn (in)correctness don?t appear at all. The amount
of pausing prior to the start of the user turn is the
most important feature for determining disengage-
ment, with pauses shorter than a quarter second be-
ing labeled DISE, suggesting that fast answers are a
strong signal of disengagement in our system. Users
who answer quickly may do so without taking the
time to think it through; the more engaged user, in
contrast, takes more time to prepare an answer.
Three lexical items from the student turns, ?fric-
tion?, ?light?, and ?greater?, are the next most im-
portant features in the tree, suggesting that particular
concepts and question types can be typically associ-
ated with user disengagement in a system. For ex-
ample, open-ended system questions may lead users
to disengage due to frustration from not knowing
when their answer is complete. One common case
in ITSPOKE involves asking users to name all the
forces on an object; some users don?t know how
many to list, so they start listing random forces, such
as ?friction.? On the other hand, multiple choice
questions can also lead users to disengage; they be-
gin with a reasonable chance of being correct and
thus don?t take the time to think through their an-
swer. One common case in ITSPOKE involves ask-
ing users to determine which of two objects has the
greater or lesser force, acceleration, and velocity.
While our feature set is highly generalizable to
other domains, it is an empirical question as to
whether the feature values we found maximally ef-
fective for predicting disengagement also general-
ize to other domains. Intuition is often unreliable,
and it has been widely shown in affect prediction
that the answer can depend on domain, dataset, and
learning algorithm employed. Moreover, there are
many types of spoken dialogue systems with dif-
ferent styles and no single type can represent the
entire field. That said, it is also important to note
that there are lessons to be learned from the features
selected for one particular domain, in terms of the
take-home message for other domains. For example,
the fact that ?prior pause? is selected as a strong sig-
nal of disengagement in ITSPOKE dialogues may
indicate that the feature itself (regardless of its se-
lected value) could be transferred to different do-
mains, alone or in the demonstrated combinations
with the other selected features.
6 Extrinsic Evaluation: Correlation
Next we use extrinsic evaluation to confirm that our
final DISE model is both useful and a reasonable
substitute for our gold standard manual DISE la-
bels. With respect to showing the utility of detecting
DISE, we use a correlational analysis to show that
the gold standard (manual) DISE values are signif-
icantly predictive of two different measures of sys-
tem performance.12 With respect to showing the ad-
equacy of our current level of detection performance
for the learned DISE model, we demonstrate that af-
ter replacing the manual DISE labels with the au-
tomatic DISE labels when running our correlations,
the automatic labels are related to performance in
the same way as the gold standard labels.
Thus for both our automatically detected DISE la-
bels (auto) and our gold standard DISE labels (man-
ual), we first computed the total number of occur-
rences for each student, and then computed a bivari-
ate Pearson?s correlation between this total and two
different metrics of performance: learning gain (LG)
and user satisfaction (US). In the tutoring domain,
learning is the primary performance metric and as is
common in this domain we compute it as normal-
ized learning gain ((posttest score-pretest score)/(1-
12Spoken dialogue research has shown that redesigning a sys-
tem in light of such correlational analysis can indeed yield per-
formance improvements (Rotaru and Litman, 2009).
97
Table 3: Correlations between Disengagement and both Satisfaction and Learning in ITSPOKE Corpus (N=72 users)
Measure Mean (SD) User Satisfaction Learning Gain
R p R p
Total Manual DISE 12.3 (7.3) -0.25 0.031 -0.35 0.002
Total Automatic DISE 12.6 (7.4) -0.26 0.029 -0.31 0.009
pretest score)). In spoken dialogue systems, user sat-
isfaction is the primary performance metric and as
is common in this domain we compute it by totaling
over the user satisfaction survey scores.13
Table 3 shows first the mean and standard devia-
tion for the DISE label over all students, the Pear-
son?s Correlation coefficient (R) and its significance
(p). As shown, both our manual and automatic DISE
labels are significantly related to performance, re-
gardless of whether we measure it as user satisfac-
tion or learning gain.14 Moreover, in both cases the
correlations are nearly identical between the man-
ual and automatic labels. These results indicate that
the detected DISE values are a useful substitute for
the gold standard, and suggest that redesigning IT-
SPOKE to recognize and respond to DISE can sig-
nificantly improve system performance.
7 Extrinsic Evaluation: Affective State
Multiple Regression
Because we are adding our disengagement detector
to a spoken dialogue system that already detects and
adapts to user uncertainty, we argue that it is also
necessary to evaluate whether greater performance
benefits are likely to be obtained by adapting to a
second state. In other words, given how difficult it is
to effectively detect and adapt to one user affective
state, is performance likely to improve by detecting
and adapting to multiple affective states?
To answer this question, we performed a multi-
ple linear regression analysis aimed at quantifying
the relative usefulness of the automatically detected
13Identical results were obtained by using an average instead
of a total, and only slightly weaker results were obtained when
normalizing the DISE totals as the percentages of total turns.
14We previously found a related correlation between different
DISE and learning measures, during the analysis of our DISE
annotation scheme (Forbes-Riley and Litman, 2011b). In par-
ticular, we showed a significant partial correlation between the
percentage of manual DISE labels and posttest controlled for
pretest score.
disengagement and uncertainty labels when predict-
ing our system performance metrics. We ran four
stepwise linear regressions. The first regression pre-
dicted learning gain, and gave the model two possi-
ble inputs: the total number of automatic DISE la-
bels and UNC labels per user. We then ran the same
regression again, this time predicting user satisfac-
tion. For comparison, we ran the same two regres-
sions using the manual DISE and UNC labels.
As the trained regression models in Figure 3 show,
when predicting learning gain, selecting both auto-
matically detected affective state metrics as inputs
significantly increases the model?s predictive power
as compared to only selecting one.15 The (stan-
dardized) feature weights indicate relative predic-
tive power in accounting for the variance in learn-
ing gain. As shown, both automatic affect metrics
have the same weight in the final model. This re-
sult suggests that adapting to our automatically de-
tected disengagement and uncertainty labels can fur-
ther improve learning over and above adapting to un-
certainty alone. Although the final model?s predic-
tive power is low (R2=0.15), our interest here is only
in investigating whether the two affective states are
more useful in combination than in isolation for pre-
dicting performance. In similar types of stepwise re-
gressions on prior ITSPOKE corpora, we?ve shown
that more complete models of system performance
incorporating many predictors of learning (i.e. af-
fective states in conjunction with other dialogue fea-
tures) can yield R2 values of over .5 (Forbes-Riley
et al, 2008).16
15Using the stepwise method, Automatic DISE was the first
feature selected, and Automatic UNC the second. However,
note that a model consisting of only the Automatic UNC metric
also yields significantly worse predictive power than selecting
both affective state metrics. Further note that almost identical
models were produced using percentages rather than totals.
16R2 is the standard reported metric for linear regressions.
However, for consistency with Table 3, note that the two models
in Figure 3 yield R values of -.31 and -.38, respectively.
98
Learning Gain = -.31 * Total Automatic DISE (R2=.09, p=.009)
Learning Gain = -.24 * Total Automatic DISE - .24 * Total Automatic UNC (R2=.15, p=.004)
Figure 3: Performance Model?s Predictive Power Increases Significantly with Multiple Affective Features
Interestingly, for the regression models of learn-
ing gain that used manual affect metrics, only the
DISE metric was selected as an input. This indi-
cates that the automatic affective state labels are use-
ful in combination for predicting performance in a
way that is not reflected in their gold standard coun-
terparts. Detecting multiple affective states might
thus be one way to compensate for the noise that is
introduced in a fully-automated affective spoken di-
alogue system.
Similarly, only the DISE metric was selected
for inclusion in the regression model of user sat-
isfaction, regardless of whether manual or auto-
matic labels were used. A separate correlation
analysis showed that user uncertainty is not sig-
nificantly correlated with user satisfaction in our
system, though we previously found that multiple
uncertainty-related metrics do significantly correlate
with learning (Litman and Forbes-Riley, 2009).
8 Summary and Current Directions
In this paper we used extrinsic evaluations to pro-
vide evidence for the utility of a new system de-
sign involving the complex task of user affect de-
tection, prior to undertaking an expensive and time-
consuming evaluation of an affect-adaptive system
with real users. In particular, we first presented a
novel model for automatically detecting user disen-
gagement in spoken dialogue systems. We showed
through intrinsic evaluations (i.e., cross-validation
experiments using gold-standard labels) that the
model yields results on par with prior work. We
then showed crucially through novel extrinsic eval-
uation that the resulting automatically detected dis-
engagement labels correlate with two primary per-
formance metrics (user satisfaction and learning) in
the same way as gold standard (manual) labels. This
suggests that adapting to the automatic disengage-
ment labels has the potential to significantly improve
performance even in the presence of noise from the
automatic labeling. Finally, further extrinsic anal-
yses using multiple regression suggest that adapt-
ing to our automatic disengagement labels can im-
prove learning (though not user satisfaction) over
and above the improvement achieved by only adapt-
ing to automatically detected user uncertainty.
We have already developed and implemented an
adaptation for user disengagement in ITSPOKE.
The disengagement adaptation draws on empiri-
cal analyses of our data and effective responses
to user disengagement presented in prior work
(c.f., (Forbes-Riley and Litman, 2011b)), We are
currently evaluating our disengagement adaptation
in the ?ideal? environment of a Wizard of Oz exper-
iment, where user disengagement, uncertainty, and
correctness are labeled by a hidden human during
user interactions with ITSPOKE.
Based on the evaluations here, we believe our dis-
engagement model is ready for implementation in
ITSPOKE. We will then evaluate the resulting spo-
ken dialogue system for detecting and adapting to
multiple affective states in an upcoming controlled
experiment with real users.
Acknowledgments
This work is funded by NSF award 0914615. We
thank Scott Silliman for systems support.
References
S. Afzal and P. Robinson. 2011. Natural affect data:
Collection and annotation. In Sidney D?Mello and
Rafael Calvo, editors, Affect and Learning Technolo-
gies. Springer.
G. Aist, B. Kort, R. Reilly, J. Mostow, and R. Pi-
card. 2002. Experimentally augmenting an intelli-
gent tutoring system with human-supplied capabili-
ties: Adding human-provided emotional scaffolding to
an automated reading tutor that listens. In Proc. In-
telligent Tutoring Systems Conference (ITS) Workshop
on Empirical Methods for Tutorial Dialogue Systems,
pages 16?28, San Sebastian, Spain.
A. Alwan, Y. Bai, M. Black, L. Caseyz, M. Gerosa,
M. Heritagez, M. Iseliy, M. Jonesz, A. Kazemzadeh,
S. Lee, S. Narayanan, P. Pricex, J. Tepperman, and
99
S. Wangy. 2007. A system for technology based as-
sessment of language and literacy in young children:
the role of multiple information sources. In Proceed-
ings of the 9th IEEE International Workshop on Multi-
media Signal Processing (MMSP), pages 26?30, Cha-
nia, Greece, October.
J. Ang, R. Dhillon, A. Krupski, E.Shriberg, and A. Stol-
cke. 2002. Prosody-based automatic detection of an-
noyance and frustration in human-computer dialog. In
J. H. L. Hansen and B. Pellom, editors, Proceedings
of the International Conference on Spoken Language
Processing (ICSLP), pages 2037?2039, Denver, USA.
A. Batliner, S. Steidl, C. Hacker, and E. Noth. 2008.
Private emotions vs. social interaction - a data-driven
approach towards analysing emotion in speech. User
Modeling and User-Adapted Interaction: The Journal
of Personalization Research, 18:175?206.
A. L. Baylor, J. Ryu, and E. Shen. 2003. The effect
of pedagogical agent voice and animation on learning,
motivation, and perceived persona. In Proceedings of
the ED-MEDIA Conference, Honolulu, Hawaii, June.
A. Black and P. Taylor. 1997. Festival speech synthe-
sis system: system documentation (1.1.1). The Centre
for Speech Technology Research, University of Edin-
burgh, http://www.cstr.ed.ac.uk/projects/festival/.
D. Bohus and E. Horvitz. 2009. Models for multiparty
engagement in open-world dialog. In Proceedings of
SIGdial, London, UK.
C. Conati and H. Maclaren. 2009. Empirically build-
ing and evaluating a probabilistic model of user af-
fect. User Modeling and User-Adapted Interaction,
19(3):267?303.
R. Cowie and R. R. Cornelius. 2003. Describing the
emotional states that are expressed in speech. Speech
Communication, 40(1-2):5?32.
L. Devillers and L. Vidrascu. 2006. Real-life emo-
tions detection with lexical and paralinguistic cues on
human-human call center dialogs. In Ninth Inter-
national Conference on Spoken Language Processing
(ICSLP, pages 801?804, Pittsburgh, PA, September.
S. D?Mello, S. Craig, A. Witherspoon, B. McDaniel, and
A. Graesser. 2008. Automatic detection of learner?s
affect from conversational cues. User Modeling and
User-Adapted Interaction: The Journal of Personal-
ization Research, 18:45?80.
S. D?Mello, B. Lehman, J. Sullins, R. Daigle, R. Combs,
K. Vogt, L. Perkins, and A. Graesser. 2010. A time
for emoting: When affect-sensitivity is and isn?t effec-
tive at promoting deep learning. In Intelligent Tutoring
Systems Conference, pages 245?254, Pittsburgh, PA,
USA, June.
J. Drummond and D. Litman. 2011. Examining the im-
pacts of dialogue content and system automation on
affect models in a spoken tutorial dialogue system.
In Proc. 12th Annual Meeting of the Special Interest
Group on Discourse and Dialogue (SIGDIAL), pages
312?318, Portland, Oregon, June.
M. Dzikovska, J. Moore, N. Steinhauser, and G. Camp-
bell. 2011. Exploring user satisfaction in a tutorial
dialogue system. In Proc. 12th Annual Meeting of
the Special Interest Group on Discourse and Dialogue
(SIGDIAL), pages 162?172, Portland, Oregon, June.
E. Florian, M. Wollmer, and B. Schuller. 2010. The Mu-
nich versatile and fast open-source audio feature ex-
tractor. In Proc. ACM Multimedia (MM), pages 1459?
1462, Florence, Italy.
K. Forbes-Riley and D. Litman. 2009. A user modeling-
based performance analysis of a wizarded uncertainty-
adaptive dialogue system corpus. In Proc. Inter-
speech, Brighton, UK, September.
K. Forbes-Riley and D. Litman. 2011a. Benefits and
challenges of real-time uncertainty detection and adap-
tation in a spoken dialogue computer tutor. Speech
Communication, 53(9?10):1115?1136.
K. Forbes-Riley and D. Litman. 2011b. When does
disengagement correlate with learning in spoken dia-
log computer tutoring? In Proceedings 15th Interna-
tional Conference on Artificial Intelligence in Educa-
tion (AIED), Auckland, NZ, June.
K. Forbes-Riley, M. Rotaru, and D. Litman. 2008. The
relative impact of student affect on performance mod-
els in a spoken dialogue tutoring system. User Model-
ing and User-Adapted Interaction, 18(1-2):11?43.
K. Forbes-Riley, D. Litman, and H. Friedberg. 2011. An-
notating disengagement for spoken dialogue computer
tutoring. In Sidney D?Mello and Rafael Calvo, editors,
Affect and Learning Technologies. Springer.
Jonathan Gratch, Stacy Marsella, Ning Wang, and
Brooke Stankovic. 2009. Assessing the validity of
appraisal-based models of emotion. In Proceedings of
ACII, Amsterdam, Netherlands.
X. D. Huang, F. Alleva, H. W. Hon, M. Y. Hwang, K. F.
Lee, and R. Rosenfeld. 1993. The SphinxII speech
recognition system: An Overview. Computer, Speech
and Language.
J. H. Jeon, R. Xia, and Y. Liu. 2010. Level of interest
sensing in spoken dialog using multi-level fusion of
acoustic and lexical evidence. In INTERSPEECH?10,
pages 2802?2805.
P. Jordan, B. Hall, M. Ringenberg, Y. Cui, and C.P. Rose.
2007. Tools for authoring a dialogue agent that par-
ticipates in learning studies. In Proc. Artificial Intelli-
gence in Education (AIED), pages 43?50.
A. Kapoor and R. W. Picard. 2005. Multimodal affect
recognition in learning environments. In 13th Annual
ACM International Conference on Multimedia, pages
677?682, Singapore.
100
J. Klein, Y. Moon, and R. Picard. 2002. This computer
responds to user frustration: Theory, design, and re-
sults. Interacting with Computers, 14:119?140.
C. M. Lee and S. Narayanan. 2005. Towards detect-
ing emotions in spoken dialogs. IEEE Transactions
on Speech and Audio Processing, 13(2), March.
B. Lehman, M. Matthews, S. D?Mello, and N. Per-
son. 2008. What are you feeling? Investigating
student affective states during expert human tutoring
sessions. In Intelligent Tutoring Systems Conference
(ITS), pages 50?59, Montreal, Canada, June.
D. Litman and K. Forbes-Riley. 2009. Spoken tutorial
dialogue and the feeling of another?s knowing. In Pro-
ceedings 10th Annual Meeting of the Special Interest
Group on Discourse and Dialogue (SIGDIAL), Lon-
don, UK, September.
K. Liu and R. W. Picard. 2005. Embedded empathy
in continuous, interactive health assessment. In CHI
Workshop on HCI Challenges in Health Assessment.
A. Martalo, N. Novielli, and F. de Rosis. 2008. Attitude
display in dialogue patterns. In Proc. AISB 2008 Sym-
posium on Affective Language in Human andMachine,
pages 1?8, Aberdeen, Scotland, April.
S. McQuiggan, B. Mott, and J. Lester. 2008. Model-
ing self-efficacy in intelligent tutoring systems: An in-
ductive approach. User Modeling and User-Adapted
Interaction (UMUAI), 18(1-2):81?123, February.
T. Paek and Y.-C. Ju. 2008. Accommodating explicit
user expressions of uncertainty in voice search or
something like that. In Proceedings of the 9th Annual
Conference of the International Speech Communica-
tion Association (INTERSPEECH 08), pages 1165?
1168, Brisbane, Australia, September.
H. Pon-Barry and S. Shieber. 2011. Recognizing uncer-
tainty in speech. EURASIP Journal on Advances in
Signal Processing.
H. Pon-Barry, K. Schultz, E. Owen Bratt, B. Clark, and
S. Peters. 2006. Responding to student uncertainty in
spoken tutorial dialogue systems. International Jour-
nal of Artificial Intelligence in Education, 16:171?194.
K. Porayska-Pomsta, M. Mavrikis, and H. Pain. 2008.
Diagnosing and acting on student affect: the tutor?s
perspective. User Modeling and User-Adapted In-
teraction: The Journal of Personalization Research,
18:125?173.
H. Prendinger and M. Ishizuka. 2005. The Empa-
thetic Companion: A character-based interface that ad-
dresses users? affective states. International Journal of
Applied Artificial Intelligence, 19(3):267?285.
M. Rotaru and D. Litman. 2009. Discourse structure and
performance analysis: Beyond the correlation. In Pro-
ceedings 10th Annual Meeting of the Special Interest
Group on Discourse and Dialogue (SIGDIAL), Lon-
don, UK.
B. Schuller, R. Muller, F. Eyben, J. Gast, B. Hrnler,
M. Wollmer, G. Rigoll, A. Hthker, and H. Konosu.
2009a. Being bored? recognising natural interest by
extensive audiovisual integration for real-life applica-
tion. Image and Vision Computing Journal, Special
Issue on Visual and Multimodal Analysis of Human
Spontaneous Behavior, 27:1760?1774.
B. Schuller, S. Steidl, and A. Batliner. 2009b. The
Interspeech 2009 Emotion Challenge. In Proceed-
ings of the 10th Annual Conference of the Inter-
national Speech Communication Association (Inter-
speech), ISCA, Brighton, UK, September.
B. Schuller, S. Steidl, A. Batliner, F. Burkhardt, L. Dev-
illers, C. Muller, and S. Narayanan. 2010. The
Interspeech 2010 Paralinguistic Challenge. In Pro-
ceedings of the 11th Annual Conference of the In-
ternational Speech Communication Assocation (Inter-
speech), pages 2794?2797, Chiba, Japan, September.
I. Shafran, M. Riley, and M. Mohri. 2003. Voice signa-
tures. In Proceedings of the IEEE Automatic Speech
Recognition and Understanding Workshop (ASRU),
pages 31?36, St. Thomas, US Virgin Islands.
C. Sidner and C. Lee. 2003. An architecture for engage-
ment in collaborative conversations between a robot
and a human. Technical Report TR2003-12, MERL.
A. Stolcke, N. Coccaro, R. Bates, P. Taylor, C. Van Ess-
Dykema, K. Ries, E. Shriberg, D. Jurafsky, R. Mar-
tin, and M. Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3).
W. Tsukahara and N. Ward. 2001. Responding to subtle,
fleeting changes in the user?s internal state. In Pro-
ceedings of the SIG-CHI on Human factors in comput-
ing systems, pages 77?84, Seattle, WA. ACM.
K. VanLehn, P. W. Jordan, C. Rose?, D. Bhembe,
M. Bo?ttner, A. Gaydos, M. Makatchev, U. Pap-
puswamy, M. Ringenberg, A. Roque, S. Siler, R. Sri-
vastava, and R. Wilson. 2002. The architecture of
Why2-Atlas: A coach for qualitative physics essay
writing. In Proc. Intl. Conf. on Intelligent Tutoring
Systems.
L. Vidrascu and L. Devillers. 2005. Detection of real-
life emotions in dialogs recorded in a call center. In
Proceedings of INTERSPEECH, Lisbon, Portugal.
M.Walker, A. Rudnicky, R. Prasad, J. Aberdeen, E. Bratt,
J. Garofolo, H. Hastie, A. Le, B. Pellom, A. Potami-
anos, R. Passonneau, S. Roukos, G. Sanders, S. Sen-
eff, and D. Stallard. 2002. DARPA communicator:
Cross-system results for the 2001 evaluation. In Proc.
ICSLP.
W. Wang and J. Hirschberg. 2011. Detecting levels of
interest from spoken dialog with multistream predic-
tion feedback and similarity based hierarchical fusion
101
learning. In Proc. 12th Annual Meeting of the Spe-
cial Interest Group on Discourse and Dialogue (SIG-
DIAL), pages 152?161, Portland, Oregon, June.
N. Wang, W.L. Johnson, R. E. Mayer, P. Rizzo, E. Shaw,
and H. Collins. 2008. The politeness effect: Peda-
gogical agents and learning outcomes. International
Journal of Human-Computer Studies, 66(2):98?112.
102
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 312?318,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Examining the Impacts of Dialogue Content and System Automation on
Affect Models in a Spoken Tutorial Dialogue System
Joanna Drummond
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
jmd73@cs.pitt.edu
Diane Litman
Department of Computer Science
Learning Research & Development Ctr.
University of Pittsburgh
Pittsburgh, PA 15260
litman@cs.pitt.edu
Abstract
Many dialogue system developers use data
gathered from previous versions of the dia-
logue system to build models which enable the
system to detect and respond to users? affect.
Previous work in the dialogue systems com-
munity for domain adaptation has shown that
large differences between versions of dialogue
systems affect performance of ported models.
Thus, we wish to investigate how more mi-
nor differences, like small dialogue content
changes and switching from a wizarded sys-
tem to a fully automated system, influence the
performance of our affect detection models.
We perform a post-hoc experiment where we
use various data sets to train multiple mod-
els, and compare against a test set from the
most recent version of our dialogue system.
Analyzing these results strongly suggests that
these differences do impact these models? per-
formance.
1 Introduction
Many dialogue system developers use data gathered
from previous versions of a system to train models
for analyzing users? interactions with later versions
of the system in new ways, e.g. detecting users? af-
fect enables the system to respond more appropri-
ately. However, this training data does not always
accurately reflect the current version of the system.
In particular, differences in the levels of automa-
tion and the presentation of dialogue content com-
monly vary between versions. For example, Raux et
al (2006) changed dialogue strategies for their Let?s
Go bus information system after real-world testing.
Previous work in dialogue systems with regards to
analyzing the impact of using differing training data
has primarily been in the domain adaptation field,
and has focused on two areas. First, previous work
empirically analyzed the need for domain adapta-
tion, i.e. methods for porting existing classifiers
to unrelated domains. For example, Webb and Liu
(2008) developed a cue-phrase-based dialogue act
classifier using the Switchboard corpus, and tested
on call center data. While this performed reason-
ably, training on the call center corpus and testing
on Switchboard performed poorly.
The second research direction involves propos-
ing methods for domain adaptation. Margolis et
al. (2010) observed similar poor performance when
porting their dialogue act classifier between three
corpora: Switchboard, the Meeting Recorder Dia-
log Act corpus, and a machine-translated version of
the Spanish Callhome corpus. They report promis-
ing results through varying their feature set. Blitzer
et al (2007) also observed poor performance and
the need for adaptation when porting product review
sentiment classifiers. They used four review corpora
from Amazon (books, DVDs, electronics, and small
appliances), which yielded 12 cross-domain train-
ing/testing pairs. Their algorithmic adaptation meth-
ods showed promising results.
Our work is in the first direction, as we also em-
pirically analyze the impact of differences in train-
ing and testing corpora to demonstrate the need for
adaptation methods. However, our work differs from
domain adaptation, as the corpora in this experiment
all come from one intelligent spoken physics tutor.
Instead, we analyze differences resulting from vary-
312
ing levels of automation and small changes in dia-
logue content between versions of our system.
With respect to analyzing automation, we em-
pirically compare the impact of differences in train-
ing on data from wizarded (WOZ) versus fully au-
tomated systems. Though many systems use data
from a WOZ version of the system to train models
which are then used in fully automated versions of
the system, the effectiveness of this method of dia-
logue system development has not been tested. We
hypothesize that models built with automated data
will outperform models built with wizarded data.
Additionally, minor dialogue content changes
typically exist between versions of systems. While
large changes, like changing domains, have been
shown to affect model performance, no work has in-
vestigated the impact of these more minute changes.
We hypothesize that these differences in dialogue
content presentation will also affect the models.
Finally, the amount of training data is a well
known factor which affects performance of models
built using supervised machine learning. We hy-
pothesize that combining some, but not all, types of
training corpora will improve the performance of the
trained models, e.g. adding automated data to WOZ
data will improve performance, as this provides fully
automated examples. We hypothesize only provid-
ing more WOZ data will not be as useful.
2 Data
The data used for this work comes from two prior
experiments using ITSPOKE, a spoken tutorial dia-
logue system, which tutors physics novices. Table
1 describes all data used, displaying the number of
users per data set, the number of dialogues between
the system and each user, the total number of user
turns per corpus, and the percentage of turns labeled
uncertain. See Appendix A for more information.
The first experiment, in 2007, compared two
dialogue-based strategies for remediating user un-
certainty over and above correctness (Forbes-Riley
and Litman, 2011b). The goal of this work was to
not only test the hypothesis that this uncertainty re-
mediation would improve users? learning, but to in-
vestigate what types of dialogue remediation would
improve users? learning the most. Since this experi-
ment, WOZ-07, was designed to be a gold-standard
case of uncertainty remediation, all natural language
understanding and uncertainty annotation was per-
formed by a human wizard, in real time (WOZ). All
annotations were made at the turn-level.
For WOZ-07, users? dialogue interactions with
the system would change based on which remedia-
tion strategy they were assigned to. There were two
different dialogue-based remediation strategies. In
addition to varying the strategies, the two control
conditions in this experiment also varied when the
remediation strategy was applied.
The simple remediation dialogue strategy pro-
vided additional information about the physics con-
cept the user was struggling with, or asked them
further questions about the concept. Both control
conditions used the simple remediation strategy; one
only applied the strategy when the user was incor-
rect, the other applied it if the user was incorrect and
randomly when the user was correct. The simple re-
mediation experimental condition applied the reme-
diation when the user was incorrect, or correct but
uncertain about their answer. The fourth condition
in WOZ-07 used the second dialogue strategy, com-
plex remediation. This strategy changed the way the
remediation was presented, depending on a combi-
nation of the user?s correctness and certainty in their
answer. Only users in the simple remediation exper-
imental condition learned more than users in other
conditions. Figure 1 shows an example of simple re-
mediation; the tutor acknowledges that the user is in-
correct, saying ?Well...?, and then explains the con-
cept the previous question tested. Appendix B com-
pares simple and complex remediation strategies.
Another experiment was performed in 2008,
where users interacted with either a fully automated
(ASR) version of ITSPOKE or a wizarded version.
The goal of this experiment was to see if the learn-
ing gains found in the 2007 experiment would hold
in the ASR version of the system. To mimic the
WOZ-07 experiment, the wizarded version (WOZ-
08) only used the simple remediation experimental
condition found in WOZ-07, while the ASR ver-
sion contained the simple remediation experimental
condition and both simple remediation control con-
ditions. The complex remediation strategy was not
included due to its poor performance in WOZ-07.
Thus, WOZ-08 and ASR-08 used identical dialogue
strategies, with minor differences in where the reme-
313
TUTORp5: (Response to an incorrect answer) Well...
We just discussed that by Newton?s Third law, when two
objects collide, the forces they exert on each other are
equal in magnitude and opposite in direction. This is true
regardless of the objects? differing masses. So the first
question?s answer is that the impact forces on the truck
and the car will have the same magnitude but opposite
direction. Now, the second question asks about the ve-
hicles? change in motion. We can use Newton?s Second
law to answer this. What does this law say?
TUTORp6: (Response to a correct, certain answer) Fine.
So the first question?s answer is that the impact forces on
the bus and the motorbike will have the same magnitude,
but opposite direction. Now, the second question asks
about the vehicles? change in motion. We can use New-
ton?s second law to answer this. What does this law say?
Figure 1: Corpus Excerpt: Remediation in Dialogue 5,
and No Remediation in Isomorphic Dialogue 6
diation would be applied. For the ASR conditions,
all models were trained on WOZ-07 data; users were
randomly assigned to the WOZ-08 or ASR-08 con-
dition as they participated.
In addition to eliminating the complex remedia-
tion condition, a sixth dialogue, completely isomor-
phic to the fifth dialogue, was added to all condi-
tions. See Appendix B dialogue examples, high-
lighting their content differences. Figure 1 displays
two ASR-08 tutor turns with the same user. These
turns are from the fifth problem, and the isomorphic
sixth problem. Note that two things change between
these two answers. First, the system responds to the
user?s incorrectness in the first example. Had the
user been correct and uncertain, this is also the di-
alogue s/he would have seen. Second, notice that
problem five discusses a car, while problem six dis-
cusses a motorcycle. To create a completely iso-
morphic problem, the scenario for the dialogue was
changed from a car to a motorcycle.
For both the 2007 and 2008 corpora, all gold-
standard uncertainty annotations were performed by
a trained human annotator. Development and pre-
vious testing of the annotation scheme between this
annotator and another trained annotator resulted in
kappa = 0.62. All wizarded conditions were an-
notated in real-time; all ASR conditions were anno-
Data Set #Usr #Dia #Turn %Unc
WOZ-07 81 5 6561 22.73
WOZ-08 19 6 1812 21.85
ASR-08 72 6 7216 20.55
ASR-08-Train 19 6 1911 21.51
ASR-08-Test 53 6 5305 20.21
Table 1: Description of data sets
tated in a post-hoc manner.
In sum, the main differences between the two sys-
tems? data are differences in automation (i.e. WOZ
and ASR) and content (i.e. presentation of content,
reflected by differing dialogue strategies, and num-
ber of physics dialogues).
3 Post-Hoc Experiment
In this post-hoc analysis, we will analyze the im-
pact of content differences by comparing the perfor-
mance of models built with WOZ-07 and WOZ-08,
and automation differences by comparing models
built with WOZ-08 and ASR-08 data. Instead of the
original study design, where WOZ-08 and ASR-08
subjects were run in parallel, we could have gathered
the WOZ data first, and used the WOZ data and the
first few ASR users for system evaluation and devel-
opment purposes. Thus, for the post-hoc analysis,
we mimic this by using WOZ-08 as a training set,
and splitting ASR-08 into two data sets?ASR-08-
Train (the first few users), and ASR-08-Test. (Please
see the last two rows of Table 1.) We held out the
first 19 users for ASR-08-Train, since this approx-
imates the amount of data used to train the model
built with WOZ-08. For our post-hoc study, the re-
maining 53 ASR users were used as a test set for
all training sets, to mimic an authentic development
lifestyle for a dialogue system. Additionally, this
guaranteed that no users appear in both the training
and testing set given any training set.
As all uncertainty remediation happens at the
turn-level, we classified uncertainty at the turn-level,
and compared these automated results with the gold-
standard annotations. We used all the features that
were designed for the original model. Since previ-
ous experiments with our data showed little variance
between different machine learning algorithms, we
chose a J48 decision tree, implemented by WEKA,1
1http://www.cs.waikato.ac.nz/ml/weka/
314
for all experiments due to its easy readability. Since
our class distribution is skewed (see Table 1), we
also used a cost matrix which heavily penalizes clas-
sifying an uncertain instance as certain.
We use simple lexical, prosodic and system-
specific features described in (Forbes-Riley and Lit-
man, 2011a) to build our models. These features
were kept constant through all experiments, so the
results could be directly comparable. For all lexical
features for all data sets, ASR text was used.2 For all
WOZ conditions, we gathered ASR text post-hoc.
We trained models on individual training sets, to
inspect the impact of content and automation dif-
ferences. We then trained new models on combi-
nations of these original training sets, to investigate
possible interactions. To allow for direct compari-
son, we used ASR-08-Test to evaluate all models.
Since detecting uncertainty is related to detecting
affective user states, we use the evaluation measures
Unweighted Average (UA) Recall and UA Precision,
presented in (Schuller et al, 2009).We also use UA
F-measure. Note that because only one hold-out
evaluation set was used, rather than using multiple
sets for cross-fold validation, we do not test for sta-
tistical significance between models? results.
4 Results
The first three rows of Table 2 present the results of
training a model on each possible training set indi-
vidually. Note that the number of instances per train-
ing set varies. WOZ-07 simply has more users in the
training set than WOZ-08 or ASR-08-Train. While
WOZ-08 and ASR-08-Train have the same number
of users, the number of turns slightly varies, since
dialogues vary depending on users? answers.
When comparing WOZ-08 to WOZ-07, first no-
tice that WOZ-08 outperforms WOZ-07 with a
much smaller amount of data. Both are wiz-
arded versions, but content differences exist be-
tween these experiments; WOZ-08 only used the
simple remediation strategy, and added a dialogue.
When comparing ASR-08-Train to the other two
individual training sets, note that it best approxi-
mates the test set. This training condition outper-
forms all others, while using less data than WOZ-
2We used ASR instead of manual transcriptions, to better
approximate automated data.
07. While WOZ-08 and ASR-08 have the same
content, the system changes from wizarded to au-
tomated language recognition. This allows us to di-
rectly compare how differences due to automation
(e.g. errors in detecting correct answers) can affect
performance of the models. Note that even though
we used ASR transcriptions of WOZ-08 turns, the
effects of ASR errors on later utterances are only
propagated in ASR-08-Train. As ASR-08-Train no-
ticeably outperforms WOZ-08, with approximately
the same amount of training data, we conclude that
using automated data for training better prepares the
model for the data it will be classifying.
As we also wish to investigate how incorporat-
ing more diverse training data would alter the per-
formance of the model, we combined ASR-08-Train
and WOZ-08 with the WOZ-07 training set, shown
in Table 2. We combined these sets practically, as
we wish to test how our model could have performed
if we had used our first few 2008 users to train the
model in the actual 2008 experiment.
First, note that all combination training sets out-
perform individual training sets. As ASR-08-Train
outperformed WOZ-08 for individual training sets,
it is not surprising that WOZ-07+ASR-08-Train out-
performs WOZ-07+WOZ-08.
However, we could have used WOZ-07 for feature
development only, and trained on WOZ-08 + ASR-
08-Train. Since the training and testing sets contain
identical content, it is unsurprising that the preci-
sion for this classifier is high. This classifier does
not perform as well with respect to recall, perhaps
since its training data is not as varied. Also note,
while this model trained on few data points, we used
additional data for feature development purposes.
Combining all three possible training sets does
not outperform WOZ-07+ASR-08-Train; it per-
forms equivalently, and uses much more data. We
hypothesize that, since WOZ-07 constitutes the ma-
jority of the training set, the benefit of including
WOZ-08 may be mitigated. Downsampling WOZ-
07 could test this hypothesis. Alternatively, the ben-
efit of combining WOZ-07+ASR-08-Train could be
that we provide many varied examples in this com-
bined training set. Since WOZ-07 already accounts
for differences in both content and automation,
WOZ-08 doesn?t introduce novel examples for the
classifier, and adding it may not be beneficial.
315
Training Set n UA Rec. UA Prec. UA F1
WOZ-07 6561 54.6% 53.0% 53.79%
WOZ-08 1812 58.0% 55.4% 56.67%
ASR-08-Train 1911 60.5% 57.2% 58.80%
WOZ-07 + WOZ-08 8373 66.1% 61.0% 63.45%
WOZ-07 + ASR-08-Train 8472 68.3% 63.5% 65.81%
WOZ-08 + ASR-08-Train 3723 64.0% 73.4% 68.38%
WOZ-07 + WOZ-08 + ASR-08-Train 10284 68.3% 63.6% 65.86%
Table 2: Results; Testing on ASR-08-Test (n = 5305). Bold denotes best performance per metric.
In sum, different training set combinations pro-
vide different benefits. With respect to UA F1 and
UA Precision, WOZ-08 + ASR-08-Train outper-
forms all other training sets. Using only 3723 turns
to train the model, this configuration uses the least
amount of training data. However, this requires pre-
viously collected data, such as WOZ-07, for fea-
ture development purposes. Alternatively, WOZ-
07 + ASR-08-Train performs better than WOZ-08 +
ASR-08-Train with respect to UA Recall, and does
not require a separate feature development set. Thus,
the ?best? training set would depend on both the ex-
perimental design, and the preferred metric.
5 Discussion and Future Work
In this paper, we provided evidence that the degree
of automation of a system used to collect training
data can impact the performance of a model when
used in a fully automated system. Since one com-
mon technique of building fully automated dialogue
systems uses a semi-automated wizarded version,
this result suggests incorporating a small amount of
automated data could greatly improve performance
of the models. Our results also suggest that the type
of data is more important than the quantity when
building these models, since well-performing mod-
els were built with small amounts of data. We also
investigated the impact of building models trained
with different dialogue content, another common
method of developing dialogue systems. As the
WOZ-08 model outperforms the WOZ-07 model, it
appears that this has a noticeable impact.
However, the WOZ-08 and WOZ-07 experiments
may not have had identical user population, due to
the timing differences between studies. We wish
to perform further post hoc-experiments to analyze
the impact of population differences in our data. To
do so, we will eliminate all dialogue strategy dif-
ferences between WOZ-07 and WOZ-08. To fur-
ther support our results regarding content differ-
ences, we wish to split WOZ-08 into two training
sets, one including the sixth problem, and one ex-
cluding it. After controlling for differences in quan-
tity of data, we will analyze the resulting models.
To further strength our results regarding automa-
tion differences, we will eliminate all differences in
when the remediation dialogue strategy was applied
between the WOZ-08 and ASR-08-Test corpus, and
try to replicate the results found in this paper.
As our results suggest the need for applying do-
main adaptation methods to improve models? per-
formance when there are differences in automation
and content, future work could investigate applying
already existing methods for domain adaptation, and
developing new ones for this problem. In particular,
the results we presented suggest a method for build-
ing a dialogue system that could mitigate the effects
of changes in automation and content. A small wiz-
arded condition, with changes in dialogue content,
could be used for feature development. This data, or
data from another small wizarded condition, could
then be used to train a preliminary model. This pre-
liminary model could be tested with a small num-
ber of users using an automated version. Then, the
data from the preliminary conditions could be used
to build the final model, which would be used for the
current, fully automated version of the system.
Acknowledgments
We thank Michal Valko, Michael Lipschultz,
Wenting Xiong, and the ITSPOKE group for helpful
comments and suggestions, and an REU supplement
to NSF Grant #0631930 for funding this work.
316
References
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biogra-
phies, bollywood, boom-boxes and blenders: Domain
adaptation for sentiment classification. In Annual
Meeting-Association For Computational Linguistics,
volume 45, page 440.
K. Forbes-Riley and D. Litman. 2011a. Benefits and
challenges of real-time uncertainty detection and adap-
tation in a spoken dialogue computer tutor. Speech
Communication.
K. Forbes-Riley and D. Litman. 2011b. Designing and
evaluating a wizarded uncertainty-adaptive spoken di-
alogue tutoring system. Computer Speech & Lan-
guage, 25(1):105?126.
A. Margolis, K. Livescu, and M. Ostendorf. 2010. Do-
main adaptation with unlabeled data for dialog act tag-
ging. In Proceedings of the 2010 Workshop on Do-
main Adaptation for Natural Language Processing,
pages 45?52. Association for Computational Linguis-
tics.
A. Raux, D. Bohus, B. Langner, A.W. Black, and M. Es-
kenazi. 2006. Doing research on a deployed spoken
dialogue system: One year of Lets Go! experience. In
Proc. Interspeech, pages 65?68. Citeseer.
B. Schuller, S. Steidl, and A. Batliner. 2009. The in-
terspeech 2009 emotion challenge. In Tenth Annual
Conference of the International Speech Communica-
tion Association.
N. Webb and T. Liu. 2008. Investigating the portability
of corpus-derived cue phrases for dialogue act classifi-
cation. In Proceedings of the 22nd International Con-
ference on Computational Linguistics-Volume 1, pages
977?984. Association for Computational Linguistics.
317
Data Set Dialogue #5 Dialogue #6Avg. Avg. Avg. Avg.
#Turn %Unc #Turn %Unc
WOZ-07 15.21 26.87 N/A N/A
WOZ-08 14.37 28.21 13.42 9.02
ASR-08 16.26 19.90 16.28 9.73
ASR-08
-Train 16.05 19.67 16.00 11.84
ASR-08
Test 16.38 19.98 16.38 8.99
Table 3: Dialogue-level description of corpora
Appendix A Differences between corpora
We note that one possible difficulty in using dia-
logues with differing content could be differing lev-
els of users? uncertainty between those dialogues.
Thus, Table 3 depicts the average percent of uncer-
tain turns over all users, per dialogue. Only Dia-
logues #5 and #6 are presented; average number of
turns and average percent uncertainty do change be-
tween all problems. While the average number of
turns are similar, the percentage of uncertain turns
drops greatly between the two isomorphic problems.
Appendix B Dialogue Examples
We also present dialogue examples from our cor-
pora to illustrate various content differences be-
tween versions of ITSPOKE. The changes between
Dialogues #5 and #6 are as follows: every instance
of ?car? in Dialogue #5 with ?motorbike,? and ev-
ery instance of ?truck? is replaced with ?PAT bus.?
Since users discuss Problem #6 with ITSPOKE im-
mediately after discussing Problem #5, the drop in
percentage of user uncertainty between Dialogue #5
and Dialogue #6 show in Table 3 might be explained
by this. However, all of the domain-specific words
and all of the knowledge concepts will remain the
same between these two problems.
Figures 1 (in the main paper) and 2 give examples
of the two types of remediation found in the 2007
corpus. ITSPOKE can change its dialogue based on
not only how it presents the content, but also what
feedback it gives. Complex remediation changes
both of these attributes. Table 4 displays possible
feedbacks given to the user, depending on their lev-
els of correctness and certainty. In Figure 2, the seg-
Tutor: What?s the overall net force on the truck equal to?
User: Zero?? [Incorrect & Uncertain]
Tutor: That?s not correct, but don?t worry. You seem
to be aware of your mistake. Let?s resolve it. [FEED-
BACK] The net force on the truck is equal to the impact
force on it. Let?s walk through this answer step by step.
[NEW SHORT ANSWER] We can derive the net force on
the truck by summing the individual forces on it, just like
we did for the car. First, what horizontal force is exerted
on the truck during the collision? [EXISTING SUBDIA-
LOGUE]
Figure 2: Example of Complex uncertainty remediation.
User Answer Examples of
Feedback Phrases
Simple Complex
Correct & That?s That?s right.
Certain right.
Correct & That?s That?s right, but you don?t
Uncertain right. sound very certain, so let?s
recap.
Incorrect & Well... Good try, but that?s not
Uncertain right. It sounds like you
knew there might be an
error in your answer.
Let?s fix it.
Incorrect & Well... I?m sorry, but there?s a
Certain mistake in your answer that
we need to work out.
Table 4: Example Feedback Phrases used in Simple and
Complex Remediation
ment of the tutor?s turn is labeled after that segment
is completed (e.g. the Feedback is ?That?s not cor-
rect... resolve it.?). The type of remediation can also
change. While Figure 1 depicts the normal remedi-
ation path as if the user had answered incorrectly or
correct but uncertain, complex remediation, shown
in Figure 2, first gives the user a short version of the
answer that they should have given, before moving
down the normal remediation path.
318
