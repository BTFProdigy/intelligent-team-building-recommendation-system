Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1051?1055,
Prague, June 2007. c?2007 Association for Computational Linguistics
Frustratingly Hard Domain Adaptation for Dependency Parsing
Mark Dredze1 and John Blitzer1 and Partha Pratim Talukdar1 and
Kuzman Ganchev1 and Joa?o V. Grac?a2 and Fernando Pereira1
1CIS Dept., University of Pennsylvania, Philadelphia, PA 19104
{mdredze|blitzer|partha|kuzman|pereira}@seas.upenn.edu
2L2F ? INESC-ID Lisboa/IST, Rua Alves Redol 9, 1000-029, Lisboa, Portugal
javg@l2f.inesc-id.pt
Abstract
We describe some challenges of adaptation
in the 2007 CoNLL Shared Task on Domain
Adaptation. Our error analysis for this task
suggests that a primary source of error is
differences in annotation guidelines between
treebanks. Our suspicions are supported by
the observation that no team was able to im-
prove target domain performance substan-
tially over a state of the art baseline.
1 Introduction
Dependency parsing, an important NLP task, can be
done with high levels of accuracy. However, adapt-
ing parsers to new domains without target domain
labeled training data remains an open problem. This
paper outlines our participation in the 2007 CoNLL
Shared Task on Domain Adaptation (Nivre et al,
2007). The goal was to adapt a parser trained on
a single source domain to a new target domain us-
ing only unlabeled data. We were given around
15K sentences of labeled text from the Wall Street
Journal (WSJ) (Marcus et al, 1993; Johansson and
Nugues, 2007) as well as 200K unlabeled sentences.
The development data was 200 sentences of labeled
biomedical oncology text (BIO, the ONCO portion
of the Penn Biomedical Treebank), as well as 200K
unlabeled sentences (Kulick et al, 2004). The two
test domains were a collection of medline chem-
istry abstracts (pchem, the CYP portion of the Penn
Biomedical Treebank) and the Child Language Data
Exchange System corpus (CHILDES) (MacWhin-
ney, 2000; Brown, 1973). We used the second or-
der two stage parser and edge labeler of McDonald
et al (2006), which achieved top results in the 2006
CoNLL-X shared task. Preliminary experiments in-
dicated that the edge labeler was fairly robust to do-
main adaptation, lowering accuracy by 3% in the de-
velopment domain as opposed to 2% in the source,
so we focused on unlabeled dependency parsing.
Our system did well, officially coming in 3rd
place out of 12 teams and within 1% of the top sys-
tem (Table 1). 1 In unlabeled parsing, we scored
1st and 2nd on CHILDES and pchem respectively.
However, our results were obtained without adap-
tation. Given our position in the ranking, this sug-
gests that no team was able to significantly improve
performance on either test domain beyond that of a
state-of-the-art parser.
After much effort in developing adaptation meth-
ods, it is critical to understand the causes of these
negative results. In what follows, we provide an er-
ror analysis that attributes domain loss for this task
to a difference in annotation guidelines between do-
mains. We then overview our attempts to improve
adaptation. While we were able to show limited
adaptation on reduced training data or with first-
order features, no modifications improved parsing
with all the training data and second-order features.
2 Parsing Challenges
We begin with an error analysis for adaptation be-
tween WSJ and BIO. We divided the available WSJ
data into a train and test set, trained a parser on
the train set and compared errors on the test set
and BIO. Accuracy dropped from 90% on WSJ to
84% on BIO. We then computed the fraction of er-
rors involving each POS tag. For the most common
1While only 8 teams participated in the closed track with us,
our score beat all of the teams in the open track.
1051
pchem l pchem ul childes ul bio ul
Ours 80.22 83.38 61.37 83.93
Best 81.06 83.42 61.37 -
Mean 73.03 76.42 57.89 -
Rank 3rd 2nd 1st -
Table 1: Official labeled (l) and other unlabeled (ul)
submitted results for the two test domains (pchem
and childes) and development data accuracy (bio).
The parser was trained on the provided WSJ data.
POS types, the loss (difference in source and tar-
get error) was: verbs (2%), conjunctions (5%), dig-
its (23%), prepositions (4%), adjectives (3%), de-
terminers (4%) and nouns (9%). 2 Two POS types
stand out: digits and nouns. Digits are less than
4% of the tokens in BIO. Errors result from the BIO
annotations for long sequences of digits which do
not appear in WSJ. Since these annotations are new
with respect to the WSJ guidelines, it is impossi-
ble to parse these without injecting knowledge of
the annotation guidelines. 3 Nouns are far more
common, comprising 33% of BIO and 30% of WSJ
tokens, the most popular POS tag by far. Addi-
tionally, other POS types listed above (adjectives,
prepositions, determiners, conjunctions) often attach
to nouns. To confirm that nouns were problem-
atic, we modified a first-order parser (no second or-
der features) by adding a feature indicating correct
noun-noun edges, forcing the parser to predict these
edges correctly. Adaptation performance rose on
BIO from 78% without the feature to 87% with the
feature. This indicates that most of the loss comes
from missing these edges.
The primary problem for nouns is the difference
between structures in each domain. The annota-
tion guidelines for the Penn Treebank flattened noun
phrases to simplify annotation (Marcus et al, 1993),
so there is no complex structure to NPs. Ku?bler
(2006) showed that it is difficult to compare the
Penn Treebank to other treebanks with more com-
plex noun structures, such as BIO. Consider theWSJ
phrase ?the New York State Insurance Department?.
The annotation indicates a flat structure, where ev-
2We measured these drops on several other dependency
parsers and found similar results.
3For example, the phrase ?(R = 28% (10/26); K=10% (3/29);
chi2 test: p=0.014).?
ery token is headed by ?Department?. In contrast,
a similar BIO phrase has a very different structure,
pursuant to the BIO guidelines. For ?the detoxi-
cation enzyme glutathione transferase P1-1?, ?en-
zyme? is the head of the NP, ?P1-1? is the head of
?transferase?, and ?transferase? is the head of ?glu-
tathione?. Since the guidelines differ, we observe no
corresponding structure in the WSJ. It is telling that
the parser labels this BIO example by attaching ev-
ery token to the final proper noun ?P1-1?, exactly as
the WSJ guidelines indicate. Unlabeled data cannot
indicate that BIO uses a different standard.
Another problem concerns appositives. For ex-
ample, the phrase ?Howard Mosher, president and
chief executive officer,? has ?Mosher? as the head
of ?Howard? and of the appositive NP delimited by
commas. While similar constructions occur in BIO,
there are no commas to indicate this. An example is
the above BIO NP, in which the phrase ?glutathione
transferase P1-1? is an appositive indicating which
?enzyme? is meant. However, since there are no
commas, the parser thinks ?P1-1? is the head. How-
ever, there are not many right to left attaching nouns.
In addition to a change in the annotation guide-
lines for NPs, we observed an important difference
in the distribution of POS tags. NN tags were almost
twice as likely in the BIO domain (14% in WSJ and
25% in BIO). NNP tags, which are close to 10% of
the tags in WSJ, are nonexistent in BIO (.24%). The
cause for this is clear when the annotation guide-
lines are considered. The proper nouns in WSJ are
names of companies, people and places, while in
BIO they are names of genes, proteins and chemi-
cals. However, for BIO these nouns are labeled NN
instead of NNP. This decision effectively removes
NNP from the BIO domain and renders all features
that depend on the NNP tag ineffective. In our above
BIO NP example, all nouns are labeled NN, whereas
the WSJ example contains NNP tags. The largest
tri-gram differences involve nouns, such as NN-NN-
NN, NNP-NNP-NNP, NN-IN-NN, and IN-NN-NN.
However, when we examine the coarse POS tags,
which do not distinguish between nouns, these dif-
ferences disappear. This indicates that while the
overall distribution of POS tags is similar between
the domains, the fine grained tags differ. These fine
grained tags provide more information than coarse
tags; experiments that removed fine grained tags
1052
hurt WSJ performance but did not affect BIO.
Finally, we examined the effect of unknown
words. Not surprisingly, the most significant dif-
ferences in error rates concerned dependencies be-
tween words of which one or both were unknown
to the parser. For two words that were seen in the
training data loss was 4%, for a single unknown
word loss was 15%, and 26% when both words were
unknown. Both words were unknown only 5% of
the time in BIO, while one of the words being un-
known was more common, reflecting 27% of deci-
sions. Upon further investigation, the majority of
unknown words were nouns, which indicates that
unknown word errors were caused by the problems
discussed above.
Recent theoretical work on domain adapta-
tion (Ben-David et al, 2006) attributes adaptation
loss to two sources: the difference in the distribu-
tion between domains and the difference in label-
ing functions. Adaptation techniques focus on the
former since it is impossible to determine the lat-
ter without knowledge of the labeling function. In
parsing adaptation, the former corresponds to a dif-
ference between the features seen in each domain,
such as new words in the target domain. The de-
cision function corresponds to differences between
annotation guidelines between two domains. Our er-
ror analysis suggests that the primary cause of loss
from adaptation is from differences in the annotation
guidelines themselves. Therefore, significant im-
provements cannot be made without specific knowl-
edge of the target domain?s annotation standards. No
amount of source training data can help if no rele-
vant structure exists in the data. Given the results
for the domain adaptation track, it appears no team
successfully adapted a state-of-the-art parser.
3 Adaptation Approaches
We survey the main approaches we explored for this
task. While some of these approaches provided a
modest performance boost to a simple parser (lim-
ited data and first-order features), no method added
any performance to our best parser (all data and
second-order features).
3.1 Features
A natural approach to improving parsing is to mod-
ify the feature set, both by removing features less
likely to transfer and by adding features that are
more likely to transfer. We began with the first ap-
proach and removed a large number of features that
we believed transfered poorly, such as most features
for noun-noun edges. We obtained a small improve-
ment in BIO performance on limited data only. We
then added several different types of features, specif-
ically designed to improve noun phrase construc-
tions, such as features based on the lexical position
of nouns (common position in NPs), frequency of
occurrence, and NP chunking information. For ex-
ample, trained on in-domain data, nouns that occur
more often tend to be heads. However, none of these
features transfered between domains.
A final type of feature we added was based on
the behavior of nouns, adjectives and verbs in each
domain. We constructed a feature representation
of words based on adjacent POS and words and
clustered words using an algorithm similar to that
of Saul and Pereira (1997). For example, our clus-
tering algorithm grouped first names in one group
and measurements in another. We then added the
cluster membership as a lexical feature to the parser.
None of the resulting features helped adaptation.
3.2 Diversity
Training diversity may be an effective source for
adaptation. We began by adding information from
multiple different parsers, which has been shown
to improve in-domain parsing. We added features
indicating when an edge was predicted by another
parser and if an edge crossed a predicted edge, as
well as conjunctions with edge types. This failed
to improve BIO accuracy since these features were
less reliable at test time. Next, we tried instance
bagging (Breiman, 1996) to generate some diversity
among parsers. We selected with replacement 2000
training examples from the training data and trained
three parsers. Each parser then tagged the remain-
ing 13K sentences, yielding 39K parsed sentences.
We then shuffled these sentences and trained a final
parser. This failed to improve performance, possibly
because of conflicting annotations or because of lack
of sufficient diversity. To address conflicting annota-
1053
tions, we added slack variables to the MIRA learn-
ing algorithm (Crammer et al, 2006) used to train
the parsers, without success. We measured diversity
by comparing the parses of each model. The dif-
ference in annotation agreement between the three
instance bagging parsers was about half the differ-
ence between these parsers and the gold annotations.
While we believe this is not enough diversity, it was
not feasible to repeat our experiment with a large
number of parsers.
3.3 Target Focused Learning
Another approach to adaptation is to favor training
examples that are similar to the target. We first mod-
ified the weight given by the parser to each training
sentence based on the similarity of the sentence to
target domain sentences. This can be done by mod-
ifying the loss to limit updates in cases where the
sentence does not reflect the target domain. We tried
a number of criteria to weigh sentences without suc-
cess, including sentence length and number of verbs.
Next, we trained a discriminative model on the pro-
vided unlabeled data to predict the domain of each
sentence based on POS n-grams in the sentence.
Training sentences with a higher probability of be-
ing in the target domain received higher weights,
also without success. Further experiments showed
that any decrease in training data hurt parser perfor-
mance. It would seem that the parser has no dif-
ficulty learning important training sentences in the
presence of unimportant training examples.
A related idea focused on words, weighing highly
tokens that appeared frequently in the target domain.
We scaled the loss associated with a token by a fac-
tor proportional to its frequency in the target do-
main. We found certain scaling techniques obtained
tiny improvements on the target domain that, while
significant compared to competition results, are not
statistically significant. We also attempted a sim-
ilar approach on the feature level. A very predic-
tive source domain feature is not useful if it does
not appear in the target domain. However, limiting
the feature space to target domain features had no
effect. Instead, we scaled each feature?s value by a
factor proportional to its frequency in the target do-
main and trained the parser on these scaled feature
values. We obtained small improvements on small
amounts of training data.
4 Future Directions
Given our pessimistic analysis and the long list of
failed methods, one may wonder if parser adapta-
tion is possible at all. We believe that it is. First,
there may be room for adaptation with our domains
if a common annotation scheme is used. Second,
we have stressed that typical adaptation, modifying
a model trained on the source domain, will fail but
there may be unsupervised parsing techniques that
improve performance after adaptation, such as a rule
based NP parser for BIO based on knowledge of the
annotations. However, this approach is unsatisfying
as it does not allow general purpose adaptation.
5 Acknowledgments
We thank Joel Wallenberg and Nikhil Dinesh for
their informative and helpful linguistic expertise,
Kevin Lerman for his edge labeler code, and Koby
Crammer for helpful conversations. Dredze is sup-
ported by a NDSEG fellowship; Ganchev and Taluk-
dar by NSF ITR EIA-0205448; and Blitzer by
DARPA under Contract No. NBCHD03001. Any
opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the
author(s) and do not necessarily reflect the views of
the DARPA or the Department of Interior-National
Business Center (DOI-NBC).
References
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2006. Analysis of representations for
domain adaptation. In NIPS.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
R. Brown. 1973. A First Language: The Early Stages.
Harvard University Press.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585, Mar.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
Sandra Ku?bler. 2006. How do treebank annotation
schemes influence parsing results? or how not to com-
pare apples and oranges. In RANLP.
1054
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-
Donald, M. Palmer, A. Schein, and L. Ungar. 2004.
Integrated annotation for biomedical information ex-
traction. In Proc. of the Human Language Technol-
ogy Conference and the Annual Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT/NAACL).
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency parsing with a two-
stage discriminative parser. In Conference on Natural
Language Learning (CoNLL).
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL).
Lawrence Saul and Fernando Pereira. 1997. Aggre-
gate and mixed-order markov models for statistical
language modeling. In EMNLP.
1055
Proceedings of ACL-08: HLT, pages 986?993,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Better Alignments = Better Translations?
Kuzman Ganchev
Computer & Information Science
University of Pennsylvania
kuzman@cis.upenn.edu
Joa?o V. Grac?a
L2F INESC-ID
Lisboa, Portugal
javg@l2f.inesc-id.pt
Ben Taskar
Computer & Information Science
University of Pennsylvania
taskar@cis.upenn.edu
Abstract
Automatic word alignment is a key step in
training statistical machine translation sys-
tems. Despite much recent work on word
alignment methods, alignment accuracy in-
creases often produce little or no improve-
ments in machine translation quality. In
this work we analyze a recently proposed
agreement-constrained EM algorithm for un-
supervised alignment models. We attempt to
tease apart the effects that this simple but ef-
fective modification has on alignment preci-
sion and recall trade-offs, and how rare and
common words are affected across several lan-
guage pairs. We propose and extensively eval-
uate a simple method for using alignment
models to produce alignments better-suited
for phrase-based MT systems, and show sig-
nificant gains (as measured by BLEU score)
in end-to-end translation systems for six lan-
guages pairs used in recent MT competitions.
1 Introduction
The typical pipeline for a machine translation (MT)
system starts with a parallel sentence-aligned cor-
pus and proceeds to align the words in every sen-
tence pair. The word alignment problem has re-
ceived much recent attention, but improvements in
standard measures of word alignment performance
often do not result in better translations. Fraser and
Marcu (2007) note that none of the tens of papers
published over the last five years has shown that
significant decreases in alignment error rate (AER)
result in significant increases in translation perfor-
mance. In this work, we show that by changing
the way the word alignment models are trained and
used, we can get not only improvements in align-
ment performance, but also in the performance of
the MT system that uses those alignments.
We present extensive experimental results evalu-
ating a new training scheme for unsupervised word
alignment models: an extension of the Expecta-
tion Maximization algorithm that allows effective
injection of additional information about the desired
alignments into the unsupervised training process.
Examples of such information include ?one word
should not translate to many words? or that direc-
tional translation models should agree. The gen-
eral framework for the extended EM algorithm with
posterior constraints of this type was proposed by
(Grac?a et al, 2008). Our contribution is a large scale
evaluation of this methodology for word alignments,
an investigation of how the produced alignments dif-
fer and how they can be used to consistently improve
machine translation performance (as measured by
BLEU score) across many languages on training cor-
pora with up to hundred thousand sentences. In 10
out of 12 cases we improve BLEU score by at least 14
point and by more than 1 point in 4 out of 12 cases.
After presenting the models and the algorithm in
Sections 2 and 3, in Section 4 we examine how
the new alignments differ from standard models, and
find that the newmethod consistently improves word
alignment performance, measured either as align-
ment error rate or weighted F-score. Section 5 ex-
plores how the new alignments lead to consistent
and significant improvement in a state of the art
phrase base machine translation by using posterior
decoding rather than Viterbi decoding. We propose
a heuristic for tuning posterior decoding in the ab-
sence of annotated alignment data and show im-
provements over baseline systems for six different
986
language pairs used in recent MT competitions.
2 Statistical word alignment
Statistical word alignment (Brown et al, 1994) is
the task identifying which words are translations of
each other in a bilingual sentence corpus. Figure
2 shows two examples of word alignment of a sen-
tence pair. Due to the ambiguity of the word align-
ment task, it is common to distinguish two kinds of
alignments (Och and Ney, 2003). Sure alignments
(S), represented in the figure as squares with bor-
ders, for single-word translations and possible align-
ments (P), represented in the figure as alignments
without boxes, for translations that are either not ex-
act or where several words in one language are trans-
lated to several words in the other language. Possi-
ble alignments can can be used either to indicated
optional alignments, such as the translation of an
idiom, or disagreement between annotators. In the
figure red/black dots indicates correct/incorrect pre-
dicted alignment points.
2.1 Baseline word alignment models
We focus on the hidden Markov model (HMM) for
alignment proposed by (Vogel et al, 1996). This is
a generalization of IBM models 1 and 2 (Brown et
al., 1994), where the transition probabilities have a
first-order Markov dependence rather than a zeroth-
order dependence. The model is an HMM, where the
hidden states take values from the source language
words and generate target language words according
to a translation table. The state transitions depend on
the distance between the source language words. For
source sentence s the probability of an alignment a
and target sentence t can be expressed as:
p(t,a | s) =
?
j
pd(aj |aj ? aj?1)pt(tj |saj ), (1)
where aj is the index of the hidden state (source lan-
guage index) generating the target language word at
index j. As usual, a ?null? word is added to the
source sentence. Figure 1 illustrates the mapping be-
tween the usual HMM notation and the HMM align-
ment model.
2.2 Baseline training
All word alignment models we consider are nor-
mally trained using the Expectation Maximization
s1 s1
s2 s3
we know
the way
sabemos       el       camino      null
usual HMM word alignment meaning
Si (hidden) source language word i
Oj (observed) target language word j
aij (transition) distortion model
bij (emission) translation model
Figure 1: Illustration of an HMM for word alignment.
(EM) algorithm (Dempster et al, 1977). The EM
algorithm attempts to maximize the marginal likeli-
hood of the observed data (s, t pairs) by repeatedly
finding a maximal lower bound on the likelihood and
finding the maximal point of the lower bound. The
lower bound is constructed by using posterior proba-
bilities of the hidden alignments (a) and can be opti-
mized in closed form from expected sufficient statis-
tics computed from the posteriors. For the HMM
alignment model, these posteriors can be efficiently
calculated by the Forward-Backward algorithm.
3 Adding agreement constraints
Grac?a et al (2008) introduce an augmentation of the
EM algorithm that uses constraints on posteriors to
guide learning. Such constraints are useful for sev-
eral reasons. As with any unsupervised induction
method, there is no guarantee that the maximum
likelihood parameters correspond to the intended
meaning for the hidden variables, that is, more accu-
rate alignments using the resulting model. Introduc-
ing additional constraints into the model often re-
sults in intractable decoding and search errors (e.g.,
IBM models 4+). The advantage of only constrain-
ing the posteriors during training is that the model
remains simple while respecting more complex re-
quirements. For example, constraints might include
?one word should not translate to many words? or
that translation is approximately symmetric.
The modification is to add a KL-projection step
after the E-step of the EM algorithm. For each sen-
tence pair instance x = (s, t), we find the posterior
987
distribution p?(z|x) (where z are the alignments). In
regular EM, p?(z|x) is used to complete the data and
compute expected counts. Instead, we find the distri-
bution q that is as close as possible to p?(z|x) in KL
subject to constraints specified in terms of expected
values of features f(x, z)
argmin
q
KL(q(z) || p?(z|x)) s.t. Eq[f(x, z)] ? b.
(2)
The resulting distribution q is then used in place
of p?(z|x) to compute sufficient statistics for the
M-step. The algorithm converges to a local maxi-
mum of the log of the marginal likelihood, p?(x) =?
z p?(z,x), penalized by the KL distance of the
posteriors p?(z|x) from the feasible set defined by
the constraints (Grac?a et al, 2008):
Ex[log p?(x)? min
q:Eq [f(x,z)]?b
KL(q(z) || p?(z|x))],
whereEx is expectation over the training data. They
suggest how this framework can be used to encour-
age two word alignment models to agree during
training. We elaborate on their description and pro-
vide details of implementation of the projection in
Equation 2.
3.1 Agreement
Most MT systems train an alignment model in each
direction and then heuristically combine their pre-
dictions. In contrast, Grac?a et al encourage the
models to agree by training them concurrently. The
intuition is that the errors that the two models make
are different and forcing them to agree rules out
errors only made by one model. This is best ex-
hibited in the rare word alignments, where one-
sided ?garbage-collection? phenomenon often oc-
curs (Moore, 2004). This idea was previously pro-
posed by (Matusov et al, 2004; Liang et al, 2006)
although the the objectives differ.
In particular, consider a feature that takes on value
1 whenever source word i aligns to target word j in
the forward model and -1 in the backward model. If
this feature has expected value 0 under the mixture
of the two models, then the forward model and back-
ward model agree on how likely source word i is to
align to target word j. More formally denote the for-
ward model??p (z) and backward model??p (z) where
??p (z) = 0 for z /?
??
Z and ??p (z) = 0 for z /?
??
Z
(
??
Z and
??
Z are possible forward and backward align-
ments). Define a mixture p(z) = 12
??p (z) + 12
??p (z)
for z ?
??
Z ?
??
Z . Restating the constraints that en-
force agreement in this setup: Eq[f(x, z)] = 0 with
fij(x, z) =
8
><
>:
1 z ?
??
Z and zij = 1
?1 z ?
??
Z and zij = 1
0 otherwise
.
3.2 Implementation
EM training of hidden Markov models for word
alignment is described elsewhere (Vogel et al,
1996), so we focus on the projection step:
argmin
q
KL(q(z) || p?(z|x)) s.t. Eq[f(x, z)] = 0.
(3)
The optimization problem in Equation 3 can be effi-
ciently solved in its dual formulation:
argmin
?
log
?
z
p?(z | x) exp {?
>f(x, z)} (4)
where we have solved for the primal variables q as:
q?(z) = p?(z | x) exp{?
>f(x, z)}/Z, (5)
with Z a normalization constant that ensures q sums
to one. We have only one dual variable per con-
straint, and we optimize them by taking a few gra-
dient steps. The partial derivative of the objective
in Equation 4 with respect to feature i is simply
Eq? [fi(x, z)]. So we have reduced the problem to
computing expectations of our features under the
model q. It turns out that for the agreement fea-
tures, this reduces to computing expectations under
the normal HMM model. To see this, we have by the
definition of q? and p?,
q?(z) =
??p (z | x) +??p (z | x)
2
exp{?>f(x, z)}/Z
=
??q (z) +??q (z)
2
.
(To make the algorithm simpler, we have assumed
that the expectation of the feature f0(x, z) =
{1 if z ?
??
Z ; ?1 if z ?
??
Z} is set to zero to
ensure that the two models ??q ,??q are each properly
normalized.) For ??q , we have: (??q is analogous)
??p (z | x)e?
>f(x,z)
=
?
j
??p d(aj |aj ? aj?1)
??p t(tj |saj )
?
ij
e?ijfij(x,zij)
=
?
j,i=aj
??p d(i|i? aj?1)
??p t(tj |si)e?ijfij(x,zij)
=
?
j,i=aj
??p d(i|i? aj?1)
??p ?t(tj |si).
988
Where we have let ??p ?t(tj |si) =
??p t(tj |si)e?ij , and
retained the same form for the model. The final pro-
jection step is detailed in Algorithm1.
Algorithm 1 AgreementProjection(??p ,??p )
1: ?ij ? 0 ?i, j
2: for T iterations do
3: ??p ?t(j|i)?
??p t(tj |si)e?ij ?i, j
4: ??p ?t(i|j)?
??p t(si|tj)e??ij ?i, j
5: ??q ? forwardBackward(??p ?t,
??p d)
6: ??q ? forwardBackward(??p ?t,
??p d)
7: ?ij ? ?ij ?E??q [ai = j] + E??q [aj = i] ?i, j
8: end for
9: return (??q ,??q )
3.3 Decoding
After training, we want to extract a single alignment
from the distribution over alignments allowable for
the model. The standard way to do this is to find
the most probable alignment, using the Viterbi al-
gorithm. Another alternative is to use posterior de-
coding. In posterior decoding, we compute for each
source word i and target word j the posterior prob-
ability under our model that i aligns to j. If that
probability is greater than some threshold, then we
include the point i? j in our final alignment. There
are two main differences between posterior decod-
ing and Viterbi decoding. First, posterior decod-
ing can take better advantage of model uncertainty:
when several likely alignment have high probabil-
ity, posteriors accumulate confidence for the edges
common to many good alignments. Viterbi, by con-
trast, must commit to one high-scoring alignment.
Second, in posterior decoding, the probability that a
0 1 2 3 4 5 6 7 8 0 1 2 3 4 5 6 7 80 ? ? ? ? ? ? ? ? ? 0 ? ? ? ? ? ? ? ? ? it1 ? ? ? ? ? ? ? ? ? 1 ? ? ? ? ? ? ? ? ? was2 ? ? ? ? ? ? ? ? ? 2 ? ? ? ? ? ? ? ? ? an3 ? ? ? ? ? ? ? ? ? 3 ? ? ? ? ? ? ? ? ? animated4 ? ? ? ? ? ? ? ? ? 4 ? ? ? ? ? ? ? ? ? ,5 ? ? ? ? ? ? ? ? ? 5 ? ? ? ? ? ? ? ? ? very6 ? ? ? ? ? ? ? ? ? 6 ? ? ? ? ? ? ? ? ? convivial7 ? ? ? ? ? ? ? ? ? 7 ? ? ? ? ? ? ? ? ? game8 ? ? ? ? ? ? ? ? ? 8 ? ? ? ? ? ? ? ? ? .jugaban
de una manera
animada
y muycordial
. jugaban
de una manera
animada
y muycordial
.
Figure 2: An example of the output of HMM trained on
100k the EPPS data. Left: Baseline training. Right: Us-
ing agreement constraints.
target word aligns to none or more than one word is
much more flexible: it depends on the tuned thresh-
old.
4 Word alignment results
We evaluated the agreement HMM model on two
corpora for which hand-aligned data are widely
available: the Hansards corpus (Och and Ney, 2000)
of English/French parliamentary proceedings and
the Europarl corpus (Koehn, 2002) with EPPS an-
notation (Lambert et al, 2005) of English/Spanish.
Figure 2 shows two machine-generated alignments
of a sentence pair. The black dots represent the ma-
chine alignments and the shading represents the hu-
man annotation (as described in the previous sec-
tion), on the left using the regular HMM model and
on the right using our agreement constraints. The
figure illustrates a problem known as garbage collec-
tion (Brown et al, 1993), where rare source words
tend to align to many target words, since the prob-
ability mass of the rare word translations can be
hijacked to fit the sentence pair. Agreement con-
straints solve this problem, because forward and
backward models cannot agree on the garbage col-
lection solution.
Grac?a et al (2008) show that alignment error rate
(Och and Ney, 2003) can be improved with agree-
ment constraints. Since AER is the standard metric
for alignment quality, we reproduce their results us-
ing all the sentences of length at most 40. For the
Hansards corpus we improve from 15.35 to 7.01 for
the English ? French direction and from 14.45 to
6.80 for the reverse. For English? Spanish we im-
prove from 28.20 to 19.86 and from 27.54 to 19.18
for the reverse. These values are competitive with
other state of the art systems (Liang et al, 2006).
Unfortunately, as was shown by Fraser and Marcu
(2007) AER can have weak correlation with transla-
tion performance as measured by BLEU score (Pa-
pineni et al, 2002), when the alignments are used
to train a phrase-based translation system. Conse-
quently, in addition to AER, we focus on precision
and recall.
Figure 3 shows the change in precision and re-
call with the amount of provided training data for
the Hansards corpus. We see that agreement con-
straints improve both precision and recall when we
989
 65 70 75 80 85 90 95 100  1
 10
 100
 1000
Thousand
s of traini
ng senten
ces
Agreemen
t Baseline
 65 70 75 80 85 90 95 100  1
 10
 100
 1000
Thousand
s of traini
ng senten
ces
Agreemen
t Baseline
Figure 3: Effect of posterior constraints on precision
(left) and recall (right) learning curves for Hansards
En?Fr.
 10 20 30 40 50 60 70 80 90 100  1
 10
 100
 1000
Thousand
s of traini
ng senten
ces
Rare Common Agreemen
t Baseline
 10 20 30 40 50 60 70 80 90 100  1
 10
 100
 1000
Thousand
s of traini
ng senten
ces
Rare Common  Agreeme
nt Baseline
Figure 4: Left: Precision. Right: Recall. Learning curves
for Hansards En?Fr split by rare (at most 5 occurances)
and common words.
use Viterbi decoding, with larger improvements for
small amounts of training data. We see a similar im-
provement on the EPPS corpus.
Motivated by the garbage collection problem, we
also analyze common and rare words separately.
Figure 4 shows precision and recall learning curves
for rare and common words. We see that agreement
constraints improve precision but not recall of rare
words and improve recall but not precision of com-
mon words.
As described above an alternative to Viterbi de-
coding is to accept all alignments that have probabil-
ity above some threshold. By changing the thresh-
old, we can trade off precision and recall. Figure
5 compares this tradeoff for the baseline and agree-
ment model. We see that the precision/recall curve
for agreement is entirely above the baseline curve,
so for any recall value we can achieve higher preci-
sion than the baseline for either corpus. In Figure 6
we break down the same analysis into rare and non
rare words.
Figure 7 shows an example of the same sentence,
using the same model where in one case Viterbi de-
coding was used and in the other case Posterior de-
coding tuned to minimize AER on a development set
 0 0.2 0.4 0.6 0.8 1  0
 0.2 0.4
 0.6 0.8
 1
Recall
PrecisionBaseline Agreemen
t
 0 0.2 0.4 0.6 0.8 1  0
 0.2 0.4
 0.6 0.8
 1
Recall 
PrecisionBaseline Agreemen
t
Figure 5: Precision and recall trade-off for posterior de-
coding with varying threshold. Left: Hansards En?Fr.
Right: EPPS En?Es.
 0 0.2 0.4 0.6 0.8 1  0
 0.2 0.4
 0.6 0.8
 1
Recall
PrecisionBaseline Agreemen
t
 0 0.2 0.4 0.6 0.8 1  0
 0.2 0.4
 0.6 0.8
 1
Recall
PrecisionBaseline Agreemen
t
Figure 6: Precision and recall trade-off for posterior on
Hansards En?Fr. Left: rare words only. Right: common
words only.
was used. An interesting difference is that by using
posterior decoding one can have n-n alignments as
shown in the picture.
A natural question is how to tune the threshold in
order to improve machine translation quality. In the
next section we evaluate and compare the effects of
the different alignments in a phrase based machine
translation system.
5 Phrase-based machine translation
In this section we attempt to investigate whether our
improved alignments produce improved machine
0 1 2 3 4 5 6 7 8 0 1 2 3 4 5 6 7 80 ? ? ? ? ? ? ? ? ? 0 ? ? ? ? ? ? ? ? ? firstly1 ? ? ? ? ? ? ? ? ? 1 ? ? ? ? ? ? ? ? ? ,2 ? ? ? ? ? ? ? ? ? 2 ? ? ? ? ? ? ? ? ? we3 ? ? ? ? ? ? ? ? ? 3 ? ? ? ? ? ? ? ? ? have4 ? ? ? ? ? ? ? ? ? 4 ? ? ? ? ? ? ? ? ? a5 ? ? ? ? ? ? ? ? ? 5 ? ? ? ? ? ? ? ? ? legal6 ? ? ? ? ? ? ? ? ? 6 ? ? ? ? ? ? ? ? ? framework8 ? ? ? ? ? ? ? ? ? 8 ? ? ? ? ? ? ? ? ? .en primero
lugar
, tenemos
un marco
jur??dico
. en primero
lugar
, tenemos
un marco
jur??dico
.
Figure 7: An example of the output of HMM trained on
100k the EPPS data using agreement HMM. Left: Viterbi
decoding. Right: Posterior decoding tuned to minimize
AER. The addition is en-firstly and tenemos-have.
990
translation. In particular we fix a state of the art
machine translation system1 and measure its perfor-
mance when we vary the supplied word alignments.
The baseline system uses GIZA model 4 alignments
and the open source Moses phrase-based machine
translation toolkit2, and performed close to the best
at the competition last year.
For all experiments the experimental setup is as
follows: we lowercase the corpora, and train lan-
guage models from all available data. The reason-
ing behind this is that even if bilingual texts might
be scarce in some domain, monolingual text should
be relatively abundant. We then train the com-
peting alignment models and compute competing
alignments using different decoding schemes. For
each alignment model and decoding type we train
Moses and use MERT optimization to tune its pa-
rameters on a development set. Moses is trained us-
ing the grow-diag-final-and alignment symmetriza-
tion heuristic and using the default distance base
distortion model. We report BLEU scores using a
script available with the baseline system. The com-
peting alignment models are GIZA Model 4, our im-
plementation of the baseline HMM alignment and
our agreement HMM. We would like to stress that
the fair comparison is between the performance of
the baseline HMM and the agreement HMM, since
Model 4 is more complicated and can capture more
structure. However, we will see that for moderate
sized data the agreement HMM performs better than
both its baseline and GIZA Model 4.
5.1 Corpora
In addition to the Hansards corpus and the Europarl
English-Spanish corpus, we used four other corpora
for the machine translation experiments. Table 1
summarizes some statistics of all corpora. The Ger-
man and Finnish corpora are also from Europarl,
while the Czech corpus contains news commentary.
All three were used in recent ACL workshop shared
tasks and are available online3. The Italian corpus
consists of transcribed speech in the travel domain
and was used in the 2007 workshop on spoken lan-
guage translation4. We used the development and
1www.statmt.org/wmt07/baseline.html
2www.statmt.org/moses/
3http://www.statmt.org
4http://iwslt07.itc.it/
Corpus Train Len Test Rare (%) Unk (%)
En, Fr 1018 17.4 1000 0.3, 0.4 0.1, 0.2
En, Es 126 21.0 2000 0.3, 0.5 0.2, 0.3
En, Fi 717 21.7 2000 0.4, 2.5 0.2, 1.8
En, De 883 21.5 2000 0.3, 0.5 0.2, 0.3
En, Cz 57 23.0 2007 2.3, 6.6 1.3, 3.9
En, It 20 9.4 500 3.1, 6.2 1.4, 2.9
Table 1: Statistics of the corpora used in MT evaluation.
The training size is measured in thousands of sentences
and Len refers to average (English) sentence length. Test
is the number of sentences in the test set. Rare and Unk
are the percentage of tokens in the test set that are rare
and unknown in the training data, for each language.
 26 28 30 32 34 36  1000
0
 10000
0
 1e+06
Traini
ng dat
a size 
(sente
nces)
Agree
ment P
ost-pts Model
 4
Baseli
ne Vit
erbi
Figure 8: BLEU score as the amount of training data is
increased on the Hansards corpus for the best decoding
method for each alignment model.
tests sets from the workshops when available. For
Italian corpus we used dev-set 1 as development and
dev-set 2 as test. For Hansards we randomly chose
1000 and 500 sentences from test 1 and test 2 to be
testing and development sets respectively.
Table 1 summarizes the size of the training corpus
in thousands of sentences, the average length of the
English sentences as well as the size of the testing
corpus. We also report the percentage of tokens in
the test corpus that are rare or not encountered in the
training corpus.
5.2 Decoding
Our initial experiments with Viterbi decoding and
posterior decoding showed that for our agreement
model posterior decoding could provide better align-
ment quality. When labeled data is available, we can
tune the threshold to minimize AER. When labeled
data is not available we use a different heuristic to
991
tune the threshold: we choose a threshold that gives
the same number of aligned points as Viterbi decod-
ing produces. In principle, we would like to tune
the threshold by optimizing BLEU score on a devel-
opment set, but that is impractical for experiments
with many pairs of languages. We call this heuristic
posterior-points decoding. As we shall see, it per-
forms well in practice.
5.3 Training data size
The HMM alignment models have a smaller param-
eter space than GIZA Model 4, and consequently we
would expect that they would perform better when
the amount of training data is limited. We found that
this is generally the case, with the margin by which
we beat model 4 slowly decreasing until a crossing
point somewhere in the range of 105 - 106 sentences.
We will see in section 5.3.1 that the Viterbi decoding
performs best for the baseline HMM model, while
posterior decoding performs best for our agreement
HMM model. Figure 8 shows the BLEU score for
the baseline HMM, our agreement model and GIZA
Model 4 as we vary the amount of training data from
104 - 106 sentences. For all but the largest data sizes
we outperform Model 4, with a greater margin at
lower training data sizes. This trend continues as we
lower the amount of training data further. We see a
similar trend with other corpora.
5.3.1 Small to Medium Training Sets
Our next set of experiments look at our perfor-
mance in both directions across our 6 corpora, when
we have small to moderate amounts of training data:
for the language pairs with more than 100,000 sen-
tences, we use only the first 100,000 sentences. Ta-
ble 2 shows the performance of all systems on these
datasets. In the table, post-pts and post-aer stand
for posterior-points decoding and posterior decod-
ing tuned for AER. With the notable exception of
Czech and Italian, our system performs better than
or comparable to both baselines, even though it uses
a much more limited model than GIZA?s Model 4.
The small corpora for which our models do not per-
form as well as GIZA are the ones with a lot of rare
words. We suspect that the reason for this is that we
do not implement smoothing, which has been shown
to be important, especially in situations with a lot of
rare words.
X? En En? X
Base Agree Base Agree
GIZA M4 23.92 17.89
De Viterbi 24.08 23.59 18.15 18.13
post-pts 24.24 24.65(+) 18.18 18.45(+)
GIZA M4 18.29 11.05
Fi Viterbi 18.79 18.38 11.17 11.54
post-pts 18.88 19.45(++) 11.47 12.48(++)
GIZA M4 33.12 26.90
Fr Viterbi 32.42 32.15 25.85 25.48
post-pts 33.06 33.09(?) 25.94 26.54(+)
post-aer 31.81 33.53(+) 26.14 26.68(+)
GIZA M4 30.24 30.09
Es Viterbi 29.65 30.03 29.76 29.85
post-pts 29.91 30.22(++) 29.71 30.16(+)
post-aer 29.65 30.34(++) 29.78 30.20(+)
GIZA M4 51.66 41.99
It Viterbi 52.20 52.09 41.40 41.28
post-pts 51.06 51.14(??) 41.63 41.79(?)
GIZA M4 22.78 12.75
Cz Viterbi 21.25 21.89 12.23 12.33
post-pts 21.37 22.51(++) 12.16 12.47(+)
Table 2: BLEU scores for all language pairs using up to
100k sentences. Results are after MERT optimization.
The marks (++)and (+)denote that agreement with poste-
rior decoding is better by 1 BLEU point and 0.25 BLEU
points respectively than the best baseline HMM model;
analogously for (??), (?); while (?)denotes smaller dif-
ferences.
5.3.2 Larger Training Sets
For four of the corpora we have more than 100
thousand sentences. The performance of the sys-
tems on all the data is shown in Table 3. German
is not included because MERT optimization did not
complete in time. We see that even on over a million
instances, our model sometimes performs better than
GIZA model 4, and always performs better than the
baseline HMM.
6 Conclusions
In this work we have evaluated agreement-
constrained EM training for statistical word align-
ment models. We carefully studied its effects on
word alignment recall and precision. Agreement
training has a different effect on rare and com-
mon words, probably because it fixes different types
of errors. It corrects the garbage collection prob-
lem for rare words, resulting in a higher preci-
sion. The recall improvement in common words
992
X? En En? X
Base Agree Base Agree
GIZA M4 22.78 14.72
Fi Viterbi 22.92 22.89 14.21 14.09
post-pts 23.15 23.43 (+) 14.57 14.74 (?)
GIZA M4 35.65 31.15
Fr Viterbi 35.19 35.17 30.57 29.97
post-pts 35.49 35.95 (+) 29.78 30.02 (?)
post-aer 34.85 35.48 (+) 30.15 30.07 (?)
GIZA M4 31.62 32.40
Es Viterbi 31.75 31.84 31.17 31.09
post-pts 31.88 32.19 (+) 31.16 31.56 (+)
post-aer 31.93 32.29 (+) 31.23 31.36 (?)
Table 3: BLEU scores for all language pairs using all
available data. Markings as in Table 2.
can be explained by the idea that ambiguous com-
mon words are different in the two languages, so the
un-ambiguous choices in one direction can force the
choice for the ambiguous ones in the other through
agreement constraints.
To our knowledge this is the first extensive eval-
uation where improvements in alignment accuracy
lead to improvements in machine translation per-
formance. We tested this hypothesis on six differ-
ent language pairs from three different domains, and
found that the new alignment scheme not only per-
forms better than the baseline, but also improves
over a more complicated, intractable model. In or-
der to get the best results, it appears that posterior
decoding is required for the simplistic HMM align-
ment model. The success of posterior decoding us-
ing our simple threshold tuning heuristic is fortu-
nate since no labeled alignment data are needed:
Viterbi alignments provide a reasonable estimate of
aligned words needed for phrase extraction. The na-
ture of the complicated relationship between word
alignments, the corresponding extracted phrases and
the effects on the final MT system still begs for
better explanations and metrics. We have investi-
gated the distribution of phrase-sizes used in transla-
tion across systems and languages, following recent
investigations (Ayan and Dorr, 2006), but unfortu-
nately found no consistent correlation with BLEU
improvement. Since the alignments we extracted
were better according to all metrics we used, it
should not be too surprising that they yield better
translation performance, but perhaps a better trade-
off can be achieved with a deeper understanding of
the link between alignments and translations.
Acknowledgments
J. V. Grac?a was supported by a fellowship from
Fundac?a?o para a Cie?ncia e Tecnologia (SFRH/ BD/
27528/ 2006). K. Ganchev was partially supported
by NSF ITR EIA 0205448.
References
N. F. Ayan and B. J. Dorr. 2006. Going beyond AER: An
extensive analysis of word alignments and their impact
on MT. In Proc. ACL.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, M. J.
Goldsmith, J. Hajic, R. L. Mercer, and S. Mohanty.
1993. But dictionaries are data too. In Proc. HLT.
P. F. Brown, S. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1994. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263?311.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Royal Statistical Society, Ser. B, 39(1):1?
38.
A. Fraser and D. Marcu. 2007. Measuring word align-
ment quality for statistical machine translation. Com-
put. Linguist., 33(3):293?303.
J. Grac?a, K. Ganchev, and B. Taskar. 2008. Expecta-
tion maximization and posterior constraints. In Proc.
NIPS.
P. Koehn. 2002. Europarl: A multilingual corpus for
evaluation of machine translation.
P. Lambert, A.De Gispert, R. Banchs, and J. B. Marin?o.
2005. Guidelines for word alignment evaluation and
manual alignment. In Language Resources and Eval-
uation, Volume 39, Number 4.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proc. HLT-NAACL.
E. Matusov, Zens. R., and H. Ney. 2004. Symmetric
word alignments for statistical machine translation. In
Proc. COLING.
R. C. Moore. 2004. Improving IBM word-alignment
model 1. In Proc. ACL.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In ACL.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Comput. Lin-
guist., 29(1):19?51.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: A Method for Automatic Evaluation of Ma-
chine Translation. In Proc. ACL.
S. Vogel, H. Ney, and C. Tillmann. 1996. Hmm-based
word alignment in statistical translation. In Proc.
COLING.
993
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 369?377,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Dependency Grammar Induction via Bitext Projection Constraints
Kuzman Ganchev and Jennifer Gillenwater and Ben Taskar
Department of Computer and Information Science
University of Pennsylvania, Philadelphia PA, USA
{kuzman,jengi,taskar}@seas.upenn.edu
Abstract
Broad-coverage annotated treebanks nec-
essary to train parsers do not exist for
many resource-poor languages. The wide
availability of parallel text and accurate
parsers in English has opened up the pos-
sibility of grammar induction through par-
tial transfer across bitext. We consider
generative and discriminative models for
dependency grammar induction that use
word-level alignments and a source lan-
guage parser (English) to constrain the
space of possible target trees. Unlike
previous approaches, our framework does
not require full projected parses, allowing
partial, approximate transfer through lin-
ear expectation constraints on the space
of distributions over trees. We consider
several types of constraints that range
from generic dependency conservation to
language-specific annotation rules for aux-
iliary verb analysis. We evaluate our ap-
proach on Bulgarian and Spanish CoNLL
shared task data and show that we con-
sistently outperform unsupervised meth-
ods and can outperform supervised learn-
ing for limited training data.
1 Introduction
For English and a handful of other languages,
there are large, well-annotated corpora with a vari-
ety of linguistic information ranging from named
entity to discourse structure. Unfortunately, for
the vast majority of languages very few linguis-
tic resources are available. This situation is
likely to persist because of the expense of creat-
ing annotated corpora that require linguistic exper-
tise (Abeill?, 2003). On the other hand, parallel
corpora between many resource-poor languages
and resource-rich languages are ample, motivat-
ing recent interest in transferring linguistic re-
sources from one language to another via parallel
text. For example, several early works (Yarowsky
and Ngai, 2001; Yarowsky et al, 2001; Merlo
et al, 2002) demonstrate transfer of shallow pro-
cessing tools such as part-of-speech taggers and
noun-phrase chunkers by using word-level align-
ment models (Brown et al, 1994; Och and Ney,
2000).
Alshawi et al (2000) and Hwa et al (2005)
explore transfer of deeper syntactic structure:
dependency grammars. Dependency and con-
stituency grammar formalisms have long coex-
isted and competed in linguistics, especially be-
yond English (Mel?c?uk, 1988). Recently, depen-
dency parsing has gained popularity as a simpler,
computationally more efficient alternative to con-
stituency parsing and has spurred several super-
vised learning approaches (Eisner, 1996; Yamada
and Matsumoto, 2003a; Nivre and Nilsson, 2005;
McDonald et al, 2005) as well as unsupervised in-
duction (Klein and Manning, 2004; Smith and Eis-
ner, 2006). Dependency representation has been
used for language modeling, textual entailment
and machine translation (Haghighi et al, 2005;
Chelba et al, 1997; Quirk et al, 2005; Shen et al,
2008), to name a few tasks.
Dependency grammars are arguably more ro-
bust to transfer since syntactic relations between
aligned words of parallel sentences are better con-
served in translation than phrase structure (Fox,
2002; Hwa et al, 2005). Nevertheless, sev-
eral challenges to accurate training and evalua-
tion from aligned bitext remain: (1) partial word
alignment due to non-literal or distant transla-
tion; (2) errors in word alignments and source lan-
guage parses, (3) grammatical annotation choices
that differ across languages and linguistic theo-
ries (e.g., how to analyze auxiliary verbs, conjunc-
tions).
In this paper, we present a flexible learning
369
framework for transferring dependency grammars
via bitext using the posterior regularization frame-
work (Gra?a et al, 2008). In particular, we ad-
dress challenges (1) and (2) by avoiding com-
mitment to an entire projected parse tree in the
target language during training. Instead, we ex-
plore formulations of both generative and discrim-
inative probabilistic models where projected syn-
tactic relations are constrained to hold approxi-
mately and only in expectation. Finally, we ad-
dress challenge (3) by introducing a very small
number of language-specific constraints that dis-
ambiguate arbitrary annotation choices.
We evaluate our approach by transferring from
an English parser trained on the Penn treebank to
Bulgarian and Spanish. We evaluate our results
on the Bulgarian and Spanish corpora from the
CoNLL X shared task. We see that our transfer
approach consistently outperforms unsupervised
methods and, given just a few (2 to 7) language-
specific constraints, performs comparably to a su-
pervised parser trained on a very limited corpus
(30 - 140 training sentences).
2 Approach
At a high level our approach is illustrated in Fig-
ure 1(a). A parallel corpus is word-level aligned
using an alignment toolkit (Gra?a et al, 2009) and
the source (English) is parsed using a dependency
parser (McDonald et al, 2005). Figure 1(b) shows
an aligned sentence pair example where depen-
dencies are perfectly conserved across the align-
ment. An edge from English parent p to child c is
called conserved if word p aligns to word p? in the
second language, c aligns to c? in the second lan-
guage, and p? is the parent of c?. Note that we are
not restricting ourselves to one-to-one alignments
here; p, c, p?, and c? can all also align to other
words. After filtering to identify well-behaved
sentences and high confidence projected depen-
dencies, we learn a probabilistic parsing model us-
ing the posterior regularization framework (Gra?a
et al, 2008). We estimate both generative and dis-
criminative models by constraining the posterior
distribution over possible target parses to approxi-
mately respect projected dependencies and other
rules which we describe below. In our experi-
ments we evaluate the learned models on depen-
dency treebanks (Nivre et al, 2007).
Unfortunately the sentence in Figure 1(b) is
highly unusual in its amount of dependency con-
servation. To get a feel for the typical case, we
used off-the-shelf parsers (McDonald et al, 2005)
for English, Spanish and Bulgarian on two bi-
texts (Koehn, 2005; Tiedemann, 2007) and com-
pared several measures of dependency conserva-
tion. For the English-Bulgarian corpus, we ob-
served that 71.9% of the edges we projected were
edges in the corpus, and we projected on average
2.7 edges per sentence (out of 5.3 tokens on aver-
age). For Spanish, we saw conservation of 64.4%
and an average of 5.9 projected edges per sentence
(out of 11.5 tokens on average).
As these numbers illustrate, directly transfer-
ring information one dependency edge at a time
is unfortunately error prone for two reasons. First,
parser and word alignment errors cause much of
the transferred information to be wrong. We deal
with this problem by constraining groups of edges
rather than a single edge. For example, in some
sentence pair we might find 10 edges that have
both end points aligned and can be transferred.
Rather than requiring our target language parse to
contain each of the 10 edges, we require that the
expected number of edges from this set is at least
10?, where ? is a strength parameter. This gives
the parser freedom to have some uncertainty about
which edges to include, or alternatively to choose
to exclude some of the transferred edges.
A more serious problem for transferring parse
information across languages are structural differ-
ences and grammar annotation choices between
the two languages. For example dealing with aux-
iliary verbs and reflexive constructions. Hwa et al
(2005) also note these problems and solve them by
introducing dozens of rules to transform the trans-
ferred parse trees. We discuss these differences
in detail in the experimental section and use our
framework introduce a very small number of rules
to cover the most common structural differences.
3 Parsing Models
We explored two parsing models: a generative
model used by several authors for unsupervised in-
duction and a discriminative model used for fully
supervised training.
The discriminative parser is based on the
edge-factored model and features of the MST-
Parser (McDonald et al, 2005). The parsing
model defines a conditional distribution p?(z | x)
over each projective parse tree z for a particular
sentence x, parameterized by a vector ?. The prob-
370
(a)
(b)
Figure 1: (a) Overview of our grammar induction approach via bitext: the source (English) is parsed and word-aligned with
target; after filtering, projected dependencies define constraints over target parse tree space, providing weak supervision for
learning a target grammar. (b) An example word-aligned sentence pair with perfectly projected dependencies.
ability of any particular parse is
p?(z | x) ?
?
z?z
e???(z,x), (1)
where z is a directed edge contained in the parse
tree z and ? is a feature function. In the fully su-
pervised experiments we run for comparison, pa-
rameter estimation is performed by stochastic gra-
dient ascent on the conditional likelihood func-
tion, similar to maximum entropy models or con-
ditional random fields. One needs to be able to
compute expectations of the features ?(z,x) under
the distribution p?(z | x). A version of the inside-
outside algorithm (Lee and Choi, 1997) performs
this computation. Viterbi decoding is done using
Eisner?s algorithm (Eisner, 1996).
We also used a generative model based on de-
pendency model with valence (Klein and Man-
ning, 2004). Under this model, the probability of
a particular parse z and a sentence with part of
speech tags x is given by
p?(z,x) = proot(r(x)) ? (2)
(?
z?z
p?stop(zp, zd, vz) pchild(zp, zd, zc)
)
?
(?
x?x
pstop(x, left, vl) pstop(x, right, vr)
)
where r(x) is the part of speech tag of the root
of the parse tree z, z is an edge from parent zp
to child zc in direction zd, either left or right, and
vz indicates valency?false if zp has no other chil-
dren further from it in direction zd than zc, true
otherwise. The valencies vr/vl are marked as true
if x has any children on the left/right in z, false
otherwise.
4 Posterior Regularization
Gra?a et al (2008) introduce an estimation frame-
work that incorporates side-information into un-
supervised problems in the form of linear con-
straints on posterior expectations. In grammar
transfer, our basic constraint is of the form: the
expected proportion of conserved edges in a sen-
tence pair is at least ? (the exact proportion we
used was 0.9, which was determined using un-
labeled data as described in Section 5). Specifi-
cally, let Cx be the set of directed edges projected
from English for a given sentence x, then given
a parse z, the proportion of conserved edges is
f(x, z) = 1|Cx|
?
z?z 1(z ? Cx) and the expected
proportion of conserved edges under distribution
p(z | x) is
Ep[f(x, z)] =
1
|Cx|
?
z?Cx
p(z | x).
The posterior regularization framework (Gra?a
et al, 2008) was originally defined for gener-
ative unsupervised learning. The standard ob-
jective is to minimize the negative marginal
log-likelihood of the data : E?[? log p?(x)] =
E?[? log
?
z p?(z,x)] over the parameters ? (we
use E? to denote expectation over the sample sen-
tences x). We typically also add standard regular-
ization term on ?, resulting from a parameter prior
? log p(?) = R(?), where p(?) is Gaussian for the
MST-Parser models and Dirichlet for the valence
model.
To introduce supervision into the model, we de-
fine a set Qx of distributions over the hidden vari-
ables z satisfying the desired posterior constraints
in terms of linear equalities or inequalities on fea-
ture expectations (we use inequalities in this pa-
per):
Qx = {q(z) : E[f(x, z)] ? b}.
371
Basic Uni-gram Features
xi-word, xi-pos
xi-word
xi-pos
xj-word, xj-pos
xj-word
xj-pos
Basic Bi-gram Features
xi-word, xi-pos, xj-word, xj-pos
xi-pos, xj-word, xj-pos
xi-word, xj-word, xj-pos
xi-word, xi-pos, xj-pos
xi-word, xi-pos, xj-word
xi-word, xj-word
xi-pos, xj-pos
In Between POS Features
xi-pos, b-pos, xj-pos
Surrounding Word POS Features
xi-pos, xi-pos+1, xj-pos-1, xj-pos
xi-pos-1, xi-pos, xj-pos-1, xj-pos
xi-pos, xi-pos+1, xj-pos, xj-pos+1
xi-pos-1, xi-pos, xj-pos, xj-pos+1
Table 1: Features used by the MSTParser. For each edge (i, j), xi-word is the parent word and xj-word is the child word,
analogously for POS tags. The +1 and -1 denote preceeding and following tokens in the sentence, while b denotes tokens
between xi and xj .
In this paper, for example, we use the conserved-
edge-proportion constraint as defined above. The
marginal log-likelihood objective is then modi-
fied with a penalty for deviation from the de-
sired set of distributions, measured by KL-
divergence from the set Qx, KL(Qx||p?(z|x)) =
minq?Qx KL(q(z)||p?(z|x)). The generative
learning objective is to minimize:
E?[? log p?(x)] +R(?) + E?[KL(Qx||p?(z | x))].
For discriminative estimation (Ganchev et al,
2008), we do not attempt to model the marginal
distribution of x, so we simply have the two regu-
larization terms:
R(?) + E?[KL(Qx||p?(z | x))].
Note that the idea of regularizing moments is re-
lated to generalized expectation criteria algorithm
of Mann and McCallum (2007), as we discuss in
the related work section below. In general, the
objectives above are not convex in ?. To opti-
mize these objectives, we follow an Expectation
Maximization-like scheme. Recall that standard
EM iterates two steps. An E-step computes a prob-
ability distribution over the model?s hidden vari-
ables (posterior probabilities) and an M-step that
updates the model?s parameters based on that dis-
tribution. The posterior-regularized EM algorithm
leaves the M-step unchanged, but involves project-
ing the posteriors onto a constraint set after they
are computed for each sentence x:
argmin
q
KL(q(z) ? p?(z|x))
s.t. Eq[f(x, z)] ? b,
(3)
where p?(z|x) are the posteriors. The new poste-
riors q(z) are used to compute sufficient statistics
for this instance and hence to update the model?s
parameters in the M-step for either the generative
or discriminative setting.
The optimization problem in Equation 3 can be
efficiently solved in its dual formulation:
argmin
??0
b>?+log
?
z
p?(z | x) exp {??
>f(x, z)}.
(4)
Given ?, the primal solution is given by: q(z) =
p?(z | x) exp{??>f(x, z)}/Z, where Z is a nor-
malization constant. There is one dual variable per
expectation constraint, and we can optimize them
by projected gradient descent, similar to log-linear
model estimation. The gradient with respect to ?
is given by: b ? Eq[f(x, z)], so it involves com-
puting expectations under the distribution q(z).
This remains tractable as long as features factor by
edge, f(x, z) =
?
z?z f(x, z), because that en-
sures that q(z) will have the same form as p?(z |
x). Furthermore, since the constraints are per in-
stance, we can use incremental or online version
of EM (Neal and Hinton, 1998), where we update
parameters ? after posterior-constrained E-step on
each instance x.
5 Experiments
We conducted experiments on two languages:
Bulgarian and Spanish, using each of the pars-
ing models. The Bulgarian experiments transfer a
parser from English to Bulgarian, using the Open-
Subtitles corpus (Tiedemann, 2007). The Span-
ish experiments transfer from English to Spanish
using the Spanish portion of the Europarl corpus
(Koehn, 2005). For both corpora, we performed
word alignments with the open source PostCAT
(Gra?a et al, 2009) toolkit. We used the Tokyo
tagger (Tsuruoka and Tsujii, 2005) to POS tag
the English tokens, and generated parses using
the first-order model of McDonald et al (2005)
with projective decoding, trained on sections 2-21
of the Penn treebank with dependencies extracted
using the head rules of Yamada and Matsumoto
(2003b). For Bulgarian we trained the Stanford
POS tagger (Toutanova et al, 2003) on the Bul-
372
Discriminative model Generative model
Bulgarian Spanish Bulgarian Spanish
no rules 2 rules 7 rules no rules 3 rules no rules 2 rules 7 rules no rules 3 rules
Baseline 63.8 72.1 72.6 67.6 69.0 66.5 69.1 71.0 68.2 71.3
Post.Reg. 66.9 77.5 78.3 70.6 72.3 67.8 70.7 70.8 69.5 72.8
Table 2: Comparison between transferring a single tree of edges and transferring all possible projected edges. The transfer
models were trained on 10k sentences of length up to 20, all models tested on CoNLL train sentences of up to 10 words.
Punctuation was stripped at train time.
gtreebank corpus from CoNLL X. The Spanish
Europarl data was POS tagged with the FreeLing
language analyzer (Atserias et al, 2006). The dis-
criminative model used the same features as MST-
Parser, summarized in Table 1.
In order to evaluate our method, we a baseline
inspired by Hwa et al (2005). The baseline con-
structs a full parse tree from the incomplete and
possibly conflicting transferred edges using a sim-
ple random process. We start with no edges and
try to add edges one at a time verifying at each
step that it is possible to complete the tree. We
first try to add the transferred edges in random or-
der, then for each orphan node we try all possible
parents (both in random order). We then use this
full labeling as supervision for a parser. Note that
this baseline is very similar to the first iteration of
our model, since for a large corpus the different
random choices made in different sentences tend
to smooth each other out. We also tried to cre-
ate rules for the adoption of orphans, but the sim-
ple rules we tried added bias and performed worse
than the baseline we report. Table 2 shows at-
tachment accuracy of our method and the baseline
for both language pairs under several conditions.
By attachment accuracy we mean the fraction of
words assigned the correct parent. The experimen-
tal details are described in this section. Link-left
baselines for these corpora are much lower: 33.8%
and 27.9% for Bulgarian and Spanish respectively.
5.1 Preprocessing
Preliminary experiments showed that our word
alignments were not always appropriate for syn-
tactic transfer, even when they were correct for
translation. For example, the English ?bike/V?
could be translated in French as ?aller/V en
v?lo/N?, where the word ?bike? would be aligned
with ?v?lo?. While this captures some of the se-
mantic shared information in the two languages,
we have no expectation that the noun ?v?lo?
will have a similar syntactic behavior to the verb
?bike?. To prevent such false transfer, we filter
out alignments between incompatible POS tags. In
both language pairs, filtering out noun-verb align-
ments gave the biggest improvement.
Both corpora also contain sentence fragments,
either because of question responses or frag-
mented speech in movie subtitles or because of
voting announcements and similar formulaic sen-
tences in the parliamentary proceedings. We over-
come this problem by filtering out sentences that
do not have a verb as the English root or for which
the English root is not aligned to a verb in the
target language. For the subtitles corpus we also
remove sentences that end in an ellipsis or con-
tain more than one comma. Finally, following
(Klein and Manning, 2004) we strip out punctu-
ation from the sentences. For the discriminative
model this did not affect results significantly but
improved them slightly in most cases. We found
that the generative model gets confused by punctu-
ation and tends to predict that periods at the end of
sentences are the parents of words in the sentence.
Our basic model uses constraints of the form:
the expected proportion of conserved edges in a
sentence pair is at least ? = 90%.1
5.2 No Language-Specific Rules
We call the generic model described above ?no-
rules? to distinguish it from the language-specific
constraints we introduce in the sequel. The no
rules columns of Table 2 summarize the perfor-
mance in this basic setting. Discriminative models
outperform the generative models in the majority
of cases. The left panel of Table 3 shows the most
common errors by child POS tag, as well as by
true parent and guessed parent POS tag.
Figure 2 shows that the discriminative model
continues to improve with more transfer-type data
1We chose ? in the following way: we split the unlabeled
parallel text into two portions. We trained a models with dif-
ferent ? on one portion and ran it on the other portion. We
chose the model with the highest fraction of conserved con-
straints on the second portion.
373
 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68
 0.1
 1
 10
accuracy (%)
trainin
g data 
size (th
ousand
s of se
ntence
s)
our me
thod baselin
e
Figure 2: Learning curve of the discriminative no-rules
transfer model on Bulgarian bitext, testing on CoNLL train
sentences of up to 10 words.
Figure 3: A Spanish example where an auxiliary verb dom-
inates the main verb.
up to at least 40 thousand sentences.
5.3 Annotation guidelines and constraints
Using the straightforward approach outlined
above is a dramatic improvement over the standard
link-left baseline (and the unsupervised generative
model as we discuss below), however it doesn?t
have any information about the annotation guide-
lines used for the testing corpus. For example, the
Bulgarian corpus has an unusual treatment of non-
finite clauses. Figure 4 shows an example. We see
that the ?da? is the parent of both the verb and its
object, which is different than the treatment in the
English corpus.
We propose to deal with these annotation dis-
similarities by creating very simple rules. For
Spanish, we have three rules. The first rule sets
main verbs to dominate auxiliary verbs. Specifi-
cally, whenever an auxiliary precedes a main verb
the main verb becomes its parent and adopts its
children; if there is only one main verb it becomes
the root of the sentence; main verbs also become
Figure 4: An example where transfer fails because of
different handling of reflexives and nonfinite clauses. The
alignment links provide correct glosses for Bulgarian words.
?Bh? is a past tense marker while ?se? is a reflexive marker.
parents of pronouns, adverbs, and common nouns
that directly preceed auxiliary verbs. By adopt-
ing children we mean that we change the parent
of transferred edges to be the adopting node. The
second Spanish rule states that the first element
of an adjective-noun or noun-adjective pair domi-
nates the second; the first element also adopts the
children of the second element. The third and fi-
nal Spanish rule sets all prepositions to be chil-
dren of the first main verb in the sentence, unless
the preposition is a ?de? located between two noun
phrases. In this later case, we set the closest noun
in the first of the two noun phrases as the preposi-
tion?s parent.
For Bulgarian the first rule is that ?da? should
dominate all words until the next verb and adopt
their noun, preposition, particle and adverb chil-
dren. The second rule is that auxiliary verbs
should dominate main verbs and adopt their chil-
dren. We have a list of 12 Bulgarian auxiliary
verbs. The ?seven rules? experiments add rules for
5 more words similar to the rule for ?da?, specif-
ically ?qe?, ?li?, ?kakvo?, ?ne?, ?za?. Table 3
compares the errors for different linguistic rules.
When we train using the ?da? rule and the rules for
auxiliary verbs, the model learns that main verbs
attach to auxiliary verbs and that ?da? dominates
its nonfinite clause. This causes an improvement
in the attachment of verbs, and also drastically re-
duces words being attached to verbs instead of par-
ticles. The latter is expected because ?da? is an-
alyzed as a particle in the Bulgarian POS tagset.
We see an improvement in root/verb confusions
since ?da? is sometimes errenously attached to a
the following verb rather than being the root of the
sentence.
The rightmost panel of Table 3 shows similar
analysis when we also use the rules for the five
other closed-class words. We see an improvement
in attachments in all categories, but no qualitative
change is visible. The reason for this is probably
that these words are relatively rare, but by encour-
aging the model to add an edge, it also rules out in-
correct edges that would cross it. Consequently we
are seeing improvements not only directly from
the constraints we enforce but also indirectly as
types of edges that tend to get ruled out.
5.4 Generative parser
The generative model we use is a state of the art
model for unsupervised parsing and is our only
374
No Rules Two Rules Seven Rules
child POS parent POS
acc(%) errors errors
V 65.2 2237 T/V 2175
N 73.8 1938 V/V 1305
P 58.5 1705 N/V 1112
R 70.3 961 root/V 555
child POS parent POS
acc(%) errors errors
N 78.7 1572 N/V 938
P 70.2 1224 V/V 734
V 84.4 1002 V/N 529
R 79.3 670 N/N 376
child POS parent POS
acc(%) errors errors
N 79.3 1532 N/V 1116
P 75.7 998 V/V 560
R 69.3 993 V/N 507
V 86.2 889 N/N 450
Table 3: Top 4 discriminative parser errors by child POS tag and true/guess parent POS tag in the Bulgarian CoNLL train data
of length up to 10. Training with no language-specific rules (left); two rules (center); and seven rules (right). POS meanings:
V verb, N noun, P pronoun, R preposition, T particle. Accuracies are by child or parent truth/guess POS tag.
 0.6 0.65 0.7 0.75
 20 4
0 60
 80 1
00 12
0 140
accuracy (%)
supervis
ed traini
ng data s
ize
supervis
ed no rules two rule
s
seven ru
les
 0.65 0.7 0.75 0.8
 20 4
0 60
 80 1
00 12
0 140
accuracy (%)
supervis
ed traini
ng data s
ize
supervis
ed no rules three rul
es
 0.65 0.7 0.75 0.8
 20 4
0 60
 80 1
00 12
0 140
accuracy (%)
supervis
ed train
ing data
 size
supervis
ed no rules two rule
s
seven ru
les
 0.65 0.7 0.75 0.8
 20 4
0 60
 80 1
00 12
0 140
accuracy (%)
supervis
ed train
ing data
 sizesupervis
ed no rules three ru
les
Figure 5: Comparison to parsers with supervised estimation and transfer. Top: Generative. Bottom: Discriminative. Left:
Bulgarian. Right: Spanish. The transfer models were trained on 10k sentences all of length at most 20, all models tested
on CoNLL train sentences of up to 10 words. The x-axis shows the number of examples used to train the supervised model.
Boxes show first and third quartile, whiskers extend to max and min, with the line passing through the median. Supervised
experiments used 30 random samples from CoNLL train.
fully unsupervised baseline. As smoothing we add
a very small backoff probability of 4.5 ? 10?5 to
each learned paramter. Unfortunately, we found
generative model performance was disappointing
overall. The maximum unsupervised accuracy it
achieved on the Bulgarian data is 47.6% with ini-
tialization from Klein and Manning (2004) and
this result is not stable. Changing the initialization
parameters, training sample, or maximum sen-
tence length used for training drastically affected
the results, even for samples with several thousand
sentences. When we use the transferred informa-
tion to constrain the learning, EM stabilizes and
achieves much better performance. Even setting
all parameters equal at the outset does not prevent
the model from learning the dependency structure
of the aligned language. The top panels in Figure 5
show the results in this setting. We see that perfor-
mance is still always below the accuracy achieved
by supervised training on 20 annotated sentences.
However, the improvement in stability makes the
algorithm much more usable. As we shall see be-
low, the discriminative parser performs even better
than the generative model.
5.5 Discriminative parser
We trained our discriminative parser for 100 iter-
ations of online EM with a Gaussian prior vari-
ance of 100. Results for the discriminative parser
are shown in the bottom panels of Figure 5. The
supervised experiments are given to provide con-
text for the accuracies. For Bulgarian, we see that
without any hints about the annotation guidelines,
the transfer system performs better than an unsu-
375
pervised parser, comparable to a supervised parser
trained on 10 sentences. However, if we spec-
ify just the two rules for ?da? and verb conjuga-
tions performance jumps to that of training on 60-
70 fully labeled sentences. If we have just a lit-
tle more prior knowledge about how closed-class
words are handled, performance jumps above 140
fully labeled sentence equivalent.
We observed another desirable property of the
discriminative model. While the generative model
can get confused and perform poorly when the
training data contains very long sentences, the dis-
criminative parser does not appear to have this
drawback. In fact we observed that as the maxi-
mum training sentence length increased, the pars-
ing performance also improved.
6 Related Work
Our work most closely relates to Hwa et al (2005),
who proposed to learn generative dependency
grammars using Collins? parser (Collins, 1999) by
constructing full target parses via projected de-
pendencies and completion/transformation rules.
Hwa et al (2005) found that transferring depen-
dencies directly was not sufficient to get a parser
with reasonable performance, even when both
the source language parses and the word align-
ments are performed by hand. They adjusted for
this by introducing on the order of one or two
dozen language-specific transformation rules to
complete target parses for unaligned words and
to account for diverging annotation rules. Trans-
ferring from English to Spanish in this way, they
achieve 72.1% and transferring to Chinese they
achieve 53.9%.
Our learning method is very closely related to
the work of (Mann and McCallum, 2007; Mann
and McCallum, 2008) who concurrently devel-
oped the idea of using penalties based on pos-
terior expectations of features not necessarily in
the model in order to guide learning. They call
their method generalized expectation constraints
or alternatively expectation regularization. In this
volume (Druck et al, 2009) use this framework
to train a dependency parser based on constraints
stated as corpus-wide expected values of linguis-
tic rules. The rules select a class of edges (e.g.
auxiliary verb to main verb) and require that the
expectation of these be close to some value. The
main difference between this work and theirs is
the source of the information (a linguistic infor-
mant vs. cross-lingual projection). Also, we de-
fine our regularization with respect to inequality
constraints (the model is not penalized for exceed-
ing the required model expectations), while they
require moments to be close to an estimated value.
We suspect that the two learning methods could
perform comparably when they exploit similar in-
formation.
7 Conclusion
In this paper, we proposed a novel and effec-
tive learning scheme for transferring dependency
parses across bitext. By enforcing projected de-
pendency constraints approximately and in expec-
tation, our framework allows robust learning from
noisy partially supervised target sentences, instead
of committing to entire parses. We show that dis-
criminative training generally outperforms gener-
ative approaches even in this very weakly super-
vised setting. By adding easily specified language-
specific constraints, our models begin to rival
strong supervised baselines for small amounts of
data. Our framework can handle a wide range of
constraints and we are currently exploring richer
syntactic constraints that involve conservation of
multiple edge constructions as well as constraints
on conservation of surface length of dependen-
cies.
Acknowledgments
This work was partially supported by an Integra-
tive Graduate Education and Research Trainee-
ship grant from National Science Foundation
(NSFIGERT 0504487), by ARO MURI SUB-
TLE W911NF-07-1-0216 and by the European
Projects AsIsKnown (FP6-028044) and LTfLL
(FP7-212578).
References
A. Abeille?. 2003. Treebanks: Building and Using
Parsed Corpora. Springer.
H. Alshawi, S. Bangalore, and S. Douglas. 2000.
Learning dependency translation models as collec-
tions of finite state head transducers. Computational
Linguistics, 26(1).
J. Atserias, B. Casas, E. Comelles, M. Gonza?lez,
L. Padro?, and M. Padro?. 2006. Freeling 1.3: Syn-
tactic and semantic services in an open-source nlp
library. In Proc. LREC, Genoa, Italy.
376
P. F. Brown, S. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1994. The mathematics of statistical ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
C. Chelba, D. Engle, F. Jelinek, V. Jimenez, S. Khudan-
pur, L. Mangu, H. Printz, E. Ristad, R. Rosenfeld,
A. Stolcke, and D. Wu. 1997. Structure and perfor-
mance of a dependency language model. In Proc.
Eurospeech.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
G. Druck, G. Mann, and A. McCallum. 2009. Semi-
supervised learning of dependency parsers using
generalized expectation criteria. In Proc. ACL.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: an exploration. In Proc. CoLing.
H. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proc. EMNLP, pages 304?311.
K. Ganchev, J. Graca, J. Blitzer, and B. Taskar.
2008. Multi-view learning over structured and non-
identical outputs. In Proc. UAI.
J. Grac?a, K. Ganchev, and B. Taskar. 2008. Expec-
tation maximization and posterior constraints. In
Proc. NIPS.
J. Grac?a, K. Ganchev, and B. Taskar. 2009. Post-
cat - posterior constrained alignment toolkit. In The
Third Machine Translation Marathon.
A. Haghighi, A. Ng, and C. Manning. 2005. Ro-
bust textual inference via graph matching. In Proc.
EMNLP.
R. Hwa, P. Resnik, A. Weinberg, C. Cabezas, and
O. Kolak. 2005. Bootstrapping parsers via syntactic
projection across parallel texts. Natural Language
Engineering, 11:11?311.
D. Klein and C. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency
and constituency. In Proc. of ACL.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT Summit.
S. Lee and K. Choi. 1997. Reestimation and best-
first parsing algorithm for probabilistic dependency
grammar. In In WVLC-5, pages 41?55.
G. Mann and A. McCallum. 2007. Simple, robust,
scalable semi-supervised learning via expectation
regularization. In Proc. ICML.
G. Mann and A. McCallum. 2008. Generalized expec-
tation criteria for semi-supervised learning of con-
ditional random fields. In Proc. ACL, pages 870 ?
878.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proc. ACL, pages 91?98.
I. Mel?c?uk. 1988. Dependency syntax: theory and
practice. SUNY. inci.
P. Merlo, S. Stevenson, V. Tsang, and G. Allaria. 2002.
A multilingual paradigm for automatic verb classifi-
cation. In Proc. ACL.
R. M. Neal and G. E. Hinton. 1998. A new view of the
EM algorithm that justifies incremental, sparse and
other variants. In M. I. Jordan, editor, Learning in
Graphical Models, pages 355?368. Kluwer.
J. Nivre and J. Nilsson. 2005. Pseudo-projective de-
pendency parsing. In Proc. ACL.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc.
EMNLP-CoNLL.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proc. ACL.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: syntactically informed
phrasal smt. In Proc. ACL.
L. Shen, J. Xu, and R. Weischedel. 2008. A new
string-to-dependency machine translation algorithm
with a target dependency language model. In Proc.
of ACL.
N. Smith and J. Eisner. 2006. Annealing structural
bias in multilingual weighted grammar induction. In
Proc. ACL.
J. Tiedemann. 2007. Building a multilingual parallel
subtitle corpus. In Proc. CLIN.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proc. HLT-NAACL.
Y. Tsuruoka and J. Tsujii. 2005. Bidirectional infer-
ence with the easiest-first strategy for tagging se-
quence data. In Proc. HLT/EMNLP.
H. Yamada and Y. Matsumoto. 2003a. Statistical de-
pendency analysis with support vector machines. In
Proc. IWPT, pages 195?206.
H. Yamada and Y. Matsumoto. 2003b. Statistical de-
pendency analysis with support vector machines. In
Proc. IWPT.
D. Yarowsky and G. Ngai. 2001. Inducing multilin-
gual pos taggers and np bracketers via robust pro-
jection across aligned corpora. In Proc. NAACL.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001.
Inducing multilingual text analysis tools via robust
projection across aligned corpora. In Proc. HLT.
377
TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 37?44,
Rochester, April 2007 c?2007 Association for Computational Linguistics
Transductive Structured Classification through Constrained
Min-Cuts
Kuzman Ganchev Fernando Pereira
Computer and Information Science
University of Pennsylvania
Philadelphia PA
{kuzman,pereira}@cis.upenn.edu
Abstract
We extend the Blum and Chawla
(2001) graph min-cut algorithm to
structured problems. This extension
can alternatively be viewed as a joint
inference method over a set of train-
ing and test instances where parts of
the instances interact through a pre-
specified associative network. The
method has has an efficient approxima-
tion through a linear-programming re-
laxation. On small training data sets,
the method achieves up to 34.8% rela-
tive error reduction.
1 Introduction
We describe a method for transductive classifi-
cation in structured problems. Our method ex-
tends the Blum and Chawla (2001) algorithm for
transductive classification. In that algorithm,
each training and test instance is represented
by a vertex in a graph. The algorithm finds the
min-cut that separates the positively and nega-
tively labeled instances. We give a linear pro-
gram that implements an approximation of this
algorithm and extend it in several ways. First,
our formulation can be used in cases where there
are more than two labels. Second, we can use
the output of a classifier to provide a prior pref-
erence of each instance for a particular label.
This lets us trade off the strengths of the min-
cut algorithm against those of a standard classi-
fier. Finally, we extend the algorithm further to
deal with structured output spaces, by encoding
parts of instances as well as constraints that en-
sure a consistent labeling of an entire instance.
The rest of this paper is organized as follows.
Section 2 explains what we mean by transduc-
tive classification and by structured problems.
Section 3 reviews the Blum and Chawla (2001)
algorithm, how we formulate it as a linear pro-
gram and our proposed extensions. Section 4
relates our proposal to previous work. Section 5
describes our experimental results on real and
synthetic data and Section 6 concludes the pa-
per.
2 Concepts and Notation
In this work we combine two separate ap-
proaches to learning: transductive methods, in
which classification of test instances arises from
optimizing a single objective involving both
training and test instances; and structured clas-
sification, in which instances involve several in-
terdependent classification problems. The de-
scription of structured problems also introduces
useful terminology for the rest of the paper.
2.1 Transductive Classification
In supervised classification, training instances
are used to induce a classifier that is then ap-
plied to individual test instances that need to
be classified. In transductive classification, a
single optimization problem is set up involving
all training and test instances; the solution of
the optimization problem yields labels for the
test instances. In this way, the test instances
provide evidence about the distribution of the
data, which may be useful when the labeled data
is limited and the distribution of unlabeled data
37
Figure 1: An example where unlabeled data
helps to reveal the underlying distribution of
the data points, borrowed from Sindhwani et al
(2005). The circles represent data points (unla-
beled are empty, positive have a ?+? and neg-
ative have a ?-?). The dashed lines represent
decision boundaries for a classifier. The first fig-
ure shows the labeled data and the max-margin
decision boundary (we use a linear boundary to
conform with Occam?s razor principle). The sec-
ond figure shows the unlabeled data points re-
vealing the distribution from which the training
examples were selected. This distribution sug-
gests that a linear boundary might not be ap-
propriate for this data. The final figure shows
a more appropriate decision boundary given the
distribution of the unlabeled data.
is informative about the location of the decision
boundary. Figure 1 illustrates this.
2.2 Structured Classification
The usual view of structured classification is as
follows. An instance consists of a set of classifi-
cation problems in which the labels of the differ-
ent problems are correlated according to a cer-
tain graphical structure. The collection of clas-
sification labels in the instance forms a single
structured label. A typical structured problem
is part of speech (POS) tagging. The parts of
speech of consecutive words are strongly corre-
lated, while the POS of words that are far away
do not influence each other much. In the natu-
ral language processing tasks that motivate this
work, we usually formalize this observation with
a Markov assumption, implemented by breaking
up the instance into parts consisting of pairs of
consecutive words. We assign a score for each
possible label of each part and then use a dy-
namic programming algorithm to find the high-
est scoring label of the entire instance.
In the rest of this paper, it will be sometimes
more convenient to think of all the (labeled and
unlabeled) instances of interest as forming a sin-
gle joint classification problem on a large graph.
In this joint problem, the atomic classification
problems are linked according to the graphical
structure imposed by their partition into struc-
tured classification instances. As we will see,
other links between atomic problems arise in our
setting that may cross between different struc-
tured instances.
2.3 Terminology
For structured problems, instance refers to an
entire problem (for example, an entire sentence
for POS tagging). A token refers to the smallest
unit that receives a label. In POS tagging, a to-
ken is a word. A part is one or more tokens and
is a division used by a learning algorithm. For
all our experiments, a part is a pair of consecu-
tive tokens, but extension to other types of parts
is trivial. If two parts share a token then a con-
sistent label for those parts has to have the same
label on the shared token. For example in the
sentence ?I love learning .? we have parts for
?I love? and ?love learning?. These share the
token ?love? and two labels for the two parts
has to agree on the label for the token in order
to be consistent. In all our experiments, a part
is a pair of consecutive tokens so two parts are
independent unless one immediately follows the
other.
3 Approach
We extend the min-cut formulation of Blum and
Chawla (2001) to multiple labels and structured
variables by adapting a linear-programming en-
coding of metric labeling problems. By relaxing
the linear program, we obtain an efficient ap-
proximate inference algorithm. To understand
our method, it is useful to review the min-
cut transductive classification algorithm (Sec-
tion 3.1) as well as the metric labeling prob-
lem and its linear programming relaxation (Sec-
tion 3.2). Section 3.3 describes how to encode
a multi-way min-cut problem as an instance of
metric labeling as well as a trivial extension that
lets us introduce a bias when computing the cut.
38
Section 3.4 extends this formalism to structured
classification.
3.1 Min-Cuts for Transductive
Classification
Blum and Chawla (2001) present an efficient
algorithm for semi-supervised machine learning
in the unstructured binary classification setting.
At a high level, the algorithm is as follows:
? Construct a graph where each instance cor-
responds to a vertex;
? Add weighted edges between similar ver-
tices with weight proportional to a measure
of similarity;
? Find the min-cut that separates positively
and negatively labeled training instances;
? Label all instances on the positive side of
the cut as positive and all others as nega-
tive.
For our purposes we need to consider two exten-
sions to this problem: multi-way classification
and constrained min-cut.
For multi-way classification, instead of com-
puting the binary min-cut as above, we need
to find the multi-way min-cut. Unfortunately,
doing this in general is NP-hard, but a poly-
nomial time approximation exists (Dahlhaus et
al., 1992). In Section 3.3 we describe how we
approximate this problem.
We extend this approach to structured data
by constructing a graph whose vertices corre-
spond to different parts of the instance, and add
weighted edges between similar parts. We then
find the multi-way min-cut that separates ver-
tices with different labels subject to some con-
straints: if two parts overlap then the labels have
to be consistent. Our main contribution is an al-
gorithm that approximately computes this con-
strained multi-way min-cut with a linear pro-
gramming relaxation.
3.2 Metric Labeling
Kleinberg and Tardos (1999) introduce the met-
ric labeling problem as a common inference
problem in a variety of fields. The inputs to
the problem are a weighted graph G = (V,E), a
set of labels L = {i|i ? 1 . . . k}, a cost function
c(v, i) which represents the preference of each
vertex for each possible label and a metric d(i, j)
between labels i and j. The goal is to assign a
label to each vertex l : V ? L so as to minimize
the cost given by:
c(l) =
?
v?V c(v, l(v))
+
?
(u,v)?E d(l(u), l(v)) ? w(u, v) .
(1)
Kleinberg and Tardos (1999) give a linear pro-
gramming approximation for this problem with
an approximation factor of two and explain how
this can be extended to an O(log k) approxima-
tion for arbitrary metrics by creating a hierar-
chy of labels. Chekuri et al (2001) present an
improved linear program that incorporates arbi-
trary metrics directly and provides an approxi-
mation at least as good as that of Kleinberg and
Tardos (1999). The idea in the new linear pro-
gram is to have a variable for each edge labeling
as well as one for each vertex labeling.
Following Chekuri et al (2001), we represent
the event that vertex u has label i by the vari-
able x(u, i) having the value 1; if x(u, i) = 0 then
vertex v must have some other label. Similarly,
we use the variable and value x(u, i, v, j) = 1 to
mean that the vertices u and v (which are con-
nected by an edge) have label i and j respec-
tively. The edge variables allow us to encode
the costs associated with violated edges in the
metric labeling problem. Edge variables should
agree with vertex labels, and by symmetry we
should have x(u, i, v, j) = x(v, j, u, i). If the
linear program gives an integer solution, this is
clearly the optimal solution to the original met-
ric labeling instance. Chekuri et al (2001) de-
scribe a rounding procedure to compute an in-
teger solution to the LP that is guaranteed to
be an approximation of the optimal integer so-
lution. For the problems we considered, this was
very rarely necessary. Their linear program re-
laxation is shown in Figure 2. The cost function
is the sum of the vertex costs and edge costs.
The first constraint requires that each vertex
have a total of one labeling unit distributed over
its labels, that is, we cannot assign more or less
than one label per vertex. The second constraint
39
min
X
u?V
X
i?L
c(u, i)x(u, i)
+
X
(u,v)?E
X
k,j?L
w(u, v)d(i, j)x(u, i, v, j)
subject to
X
i?L
x(u, i) = 1 ?u ? V
x(u, i)?
X
j?L
x(u, i, v, j) = 0 ?u ? V, v ? N(u), i ? L
x(u, i, v, j)? x(v, j, u, i) = 0 ?u, v ? V, i, j ? L
x(u, i, v, j), x(u, i) ? [0, 1] ?u, v ? V, i, j ? L
Figure 2: The Chekuri et al (2001) linear pro-
gram used to approximate metric labeling. See
text for discussion.
requires that vertex- and edge-label variables are
consistent: the label that vertex variables give
a vertex should agree with the labels that edge
variables give that vertex. The third constraint
imposes the edge-variable symmetry condition,
and the final constraint requires that all the vari-
ables be in the range [0, 1].
3.3 Min Cut as an Instance of Metric
Labeling
Given an instance of the (multi-way) min-cut
problem, we can translate it to an instance of
metric labeling as follows. The underlying graph
and edge weights will be the same as min-cut
problem. We add vertex costs (c(u, i) ?u ?
V, i ? L) and a label metric (d(i, j) ?i, j ? L).
For all unlabeled vertices set the vertex cost to
zero for all labels. For labeled vertices set the
cost of the correct label to zero and all other la-
bels to infinity. Finally let d(i, j) be one if i 6= j
and zero otherwise.
The optimal solution to this instance of metric
labeling will be the same as the optimal solution
of the initial min cut instance: the cost of any
labeling is the number of edges that link vertices
with different labels, which is exactly the num-
ber of cut edges. Also by the same argument,
every possible labeling will correspond to some
cut and approximations of the metric labeling
formulation will be approximations of the origi-
nal min-cut problem.
Since the metric labeling problem allows ar-
bitrary affinities between a vertex in the graph
and possible labels for that vertex, we can triv-
ially extend the algorithm by introducing a bias
at each vertex for labels more compatible with
that vertex. We use the output of a classifier to
bias the cost towards agreement with the clas-
sifier. Depending on the strength of the bias,
we can trade off our confidence in the perfor-
mance of the min-cut algorithm against the our
confidence in a fully-supervised classifier.
3.4 Extension to Structured
Classification
To extend this further to structured classifica-
tion we modify the Chekuri et al (2001) linear
program (Figure 2). In the structured case, we
construct a vertex for every part of an instance.
Since we want to find a consistent labeling for an
entire instance composed of overlapping parts,
we need to add some more constraints to the lin-
ear program. We want to ensure that if two ver-
tices correspond to two overlapping parts, then
they are assigned consistent labels, that is, the
token shared by two parts is given the same label
by both. First we add a new zero-weight edge
between every pair of vertices corresponding to
overlapping parts. Since its weight is zero, this
edge will not affect the cost. We then add a
constraint to the linear-program that the edge
variables for inconsistent labelings of the new
edges have a value of zero.
More formally, let (u, i, v, j) ? ? denote that
the part u having label i is consistent with the
part v having label j; if u and v do not share any
tokens, then any pair of labels for those parts are
consistent. Now add zero-weight edges between
overlapping parts. Then the only modification
to the linear program is that
x(u, i)?
?
j?L x(u, i, v, j) = 0
?u ? V, v ? N(u), i ? L
will become
x(u, i)?
?
j:(u,i,v,j)?? x(u, i, v, j) = 0
?u ? V, v ? N(u), i ? L .
40
min
X
u?V
X
i?L
c(u, i)x(u, i)
+
X
(u,v)?E
X
k,j?L
w(u, v)d(i, j)x(u, i, v, j)
subject to
X
i?L
x(u, i) = 1 ?u ? V
x(u, i)?
X
j:(u,i,v,j)??
x(u, i, v, j) = 0 ?u ? V, v ? N(u), i ? L
x(u, i, v, j)? x(v, j, u, i) = 0 ? (u, i, v, j) ? ?
x(u, i, v, j), x(u, i) ? [0, 1] ?u, v ? V, i, j ? L
Figure 3: The modified linear program used to
approximate metric labeling. See text for dis-
cussion.
What this modification does is to ensure that all
the mass of the edge variables between vertices
u and v lies in consistent labelings for their edge.
The modified linear program is shown in Figure
3. We can show that this can be encoded as
a larger instance of the metric labeling problem
(with roughly |V |+|E| more vertices and a label
set that is four times as large), but modifying the
linear program directly results in a more efficient
implementation. The final LP has one variable
for each labeling of each edge in the graph, so
we have O(|E||L|2) variables. Note that |L| is
the number of labelings of a pair of tokens for
us ? even so, computation of a single dataset
took on the order of minutes using the Xpress
MP package.
4 Relation to Previous work
Our work is set of extensions to the work of-
Blum and Chawla (2001), which we have already
described. Our extensions allow us to handle
multi-class and structured data, as well as to
take hints from a classifier. We can also spec-
ify a similarity metric between labels so that a
cut-edge can cost different amounts depending
on what partitions it spans.
Taskar et al (2004a) describe a class of
Markov networks with associative clique poten-
tials. That is, the clique potentials always prefer
that all the nodes in the clique have the same
label. The inference problem in these networks
is to find the assignment of labels to all nodes in
the graph that maximizes the sum of the clique
potentials. Their paper describes a linear pro-
gramming relaxation to find (or approximate)
this inference problem which is very similar to
the LP formulation of Chekuri et al (2001) when
all cliques are of size 2. They generalize this
to larger cliques and prove that their LP gives
an integral solution when the label alphabet has
size 2 (even for large cliques). For the learn-
ing problem they exploit the dual of the LP for-
mulation and use a maximum margin objective
similar to the one used by Taskar et al (2004b).
If we ignore the learning problem and focus on
inference, one could view our work as inference
over a Markov network created by combining a
set of linear chain conditional random fields with
an associative Markov network (with arbitrary
structure). A direction for future work would be
to train the associative Markov network either
independently from the chain-structured model
or jointly with it. This would be very similar to
the joint inference work described in the next
paragraph, and could be seen as a particular
instantiation of either a non-linear conditional
random field (Lafferty et al, 2001) or relational
Markov network (Taskar et al, 2002).
Sutton and McCallum (2004) consider the use
of linear chain CRFs augmented with extra skip
edges which encode a probabilistic belief that
the labels of two entities might be correlated.
They provide experimental results on named en-
tity recognition for e-mail messages announcing
seminars, and their system achieves a 13.7% rel-
ative reduction in error on the ?Speaker? field.
Their work differs from ours in that they add
skip edges only between identical capitalized
words and only within an instance, which for
them is an e-mail message. In particular, they
can never have an edge between labeled and un-
labeled parts. Their approach is useful for iden-
tification of personal names but less helpful for
other named entity tasks where the names may
not be capitalized.
Lafferty et al (2004) show a representer the-
orem allowing the use of Mercer kernels with
41
CRFs. They use a kernel CRF with a graph
kernel (Smola and Kondor, 2003) to do semi-
supervised learning. For them, the graph de-
fines an implicit representation of the data, but
inference is still performed only on the (chain)
structure of the CRF. By contrast, we perform
inference over the whole set of examples at the
same time.
Altun et al (2006) extend the use of graph-
based regularization to structured variables.
Their work is in the framework of maximum
margin learning for structured variables where
learning is framed as an optimization problem.
They modify the objective function by adding
a penalty whenever two parts that are expected
to have a similar label assign a different score to
the same label. They show improvements of up
to 5.3% on two real tasks: pitch accent predic-
tion and optical character recognition (OCR).
Unfortunately, to solve their optimization prob-
lem they have to invert an n?n matrix, where n
is the number of parts in the training and test-
ing data times the number of possible labels for
each part. Because of this they are forced to
train on an unrealistically small amount of data
(4-40 utterances for pitch accent prediction and
10 words for OCR).
5 Experiments
We performed experiments using our approach
on three different datasets using a conditional
random field as the base classifier. Unless oth-
erwise noted this was regularized using a zero-
mean Gaussian prior with a variance of 1.
The first dataset is the pitch-accent prediction
dataset used in semi-supervised learning by Al-
tun et al (2006). There are 31 real and binary
features (all are encoded as real values) and only
two labels. Instances correspond to an utterance
and each token corresponds to a word. Altun
et al (2006) perform experiments on 4 and 40
training instances using at most 200 unlabeled
instances.
The second dataset is the reference part of
the Cora information extraction dataset.1 This
1The Cora IE dataset has been used in Seymore et
al. (1999), Peng and McCallum (2004), McCallum et
al. (2000) and Han et al (2003), among others. We
consists of 500 computer science research paper
citations. Each token in a citation is labeled as
being part of the name of an author, part of the
title, part of the date or one of several other
labels that we combined into a single category
(?other?).
The third dataset is the chunking dataset
from the CoNLL 2000 (Sang and Buchholz,
2000) shared task restricted to noun phrases.
The task for this dataset is, given the words in a
sentence as well as automatically assigned parts
of speech for these words, label each word with
B-NP if it is the first word in a base noun phrase,
I-NP if it is part of a base noun phrase but not
the first word and O if it is not part of a noun
phrase.
For all experiments, we let each word be a
token and consider parts consisting of two con-
secutive tokens.
5.1 Pitch Accent Prediction
For the pitch accent prediction dataset, we used
the 5-nearest neighbors of each instance accord-
ing to the Euclidean distance in the original fea-
ture space to construct the graph for min-cut.
Table 1 shows the results of our experiments on
this data, as well as the results reported by Al-
tun et al (2006). The numbers in the table are
per-token accuracy and each entry is the mean
of 10 random train-test data selections.
For this problem, our method improves per-
formance over the base CRF classifier (except
when the training data consists of only 4 utter-
ances), but we do not see improvements as dra-
matic as those observed by Altun et al (2006).
Note that even the larger dataset here is quite
small ? 40 utterances where each token has been
annotated with a binary value.
5.2 Cora-IE
For the Cora information extraction dataset, we
used the first 100 principal components of the
feature space to find 5 nearest neighbors of each
part. This approximation is due to the cost of
comuting nearest neighbors in high dimensions.
In these experiments we trained on 40 instances
obtained the dataset from http://www.cs.umass.edu/
~mccallum/data/cora-ie.tar.gz.
42
Method 4:80 40:80 40:200
CRF 71.2 72.5 73.1
MinCut 69.4 74.4 74.3
STR 70.7 75.7 77.5
SVM 69.9 72.0 73.1
Table 1: Results on the pitch accent prediction
task. The methods we compare are as follows.
CRF is supervised CRF training. MinCut is our
method with a CRF as base classifier. STR and
SVM are the semi-supervised results reported in
Altun et al (2006). The experiments are 4 la-
beled and 80 unlabeled, 40 labeled and 80 unla-
beled and 40 labeled and 200 unlabeled respec-
tively.
Variance 10 100 1000
CRF 84.5% 84.3% 83.9%
MinCut 88.8% 89.6% 89.9%
Table 2: Accuracy on the Cora-IE dataset as
a percentage of tokens correctly classified at dif-
ferent settings for the CRF variance. Results for
training on 40 instances and testing on 80. In
all cases the scores are the mean of 10 random
selections of 120 instances from the set of 500
available.
and used 80 as testing data. In all cases we
randomly selected training and testing instances
10 times from the total set of 500. Table 2
shows the average accuracies for the 10 repe-
titions, with different values for the variance of
the Gaussian prior used to regularize the CRF.
If we choose the optimal value for each method,
our approach gives a 34.8% relative reduction
in error over the CRF, and improves over it in
each of the 10 random data selections, and all
settings of the Guassian prior variance.
5.3 CoNLL NP-Chunking
Our results are worst for the CoNLL NP-
Chunking dataset. As above, we used 10 ran-
dom selections of training and test sets, and
used the 100 principal components of the fea-
ture space to find 5 nearest neighbors of each
part. Table 3 shows the results of our experi-
ments. The numbers in the table are per-token
Method 20:40 40:80
CRF 87.6 90.6
MinCut(CRF) 88.2 89.6
Table 3: Results on the NP-chunking task. The
table compares a CRF with our method using a
CRF as a base classifier. The experiments use
20 labeled and 40 unlabeled and 40 labeled and
80 unlabeled instances.
accuracy as before. When the amount of train-
ing data is very small (20 instances) we improve
slightly over the base CRF classifier, but with
an increased amount of training data, the small
improvement is replaced with a small loss.
6 Discussion
We have presented a new transductive algorithm
for structured classification, which achieves er-
ror reductions on some real-world problems. Un-
fortunately, those gains are not always realized,
and sometimes our approach leads to an increase
in error. The main reason that our approach
does not always work seems to be that our mea-
sure of similarity between different parts is very
coarse. In general, finding all the pairs of parts
have the same label is as difficult as finding the
correct labeling of all instances, but it might be
possible to use unlabeled data to learn the sim-
ilarity measure.
References
Yasemin Altun, David McAllester, and Mikhail
Belkin. 2006. Maximum margin semi-supervised
learning for structured variables. In Y. Weiss,
B. Scho?lkopf, and J. Platt, editors, Advances in
Neural Information Processing Systems 18, pages
33?40. MIT Press, Cambridge, MA.
Avrim Blum and Shuchi Chawla. 2001. Learn-
ing from labeled and unlabeled data using graph
mincuts. In Proceedings of the 18th International
Conf. on Machine Learning, pages 19?26. Morgan
Kaufmann, San Francisco, CA.
Chandra Chekuri, Sanjeev Khanna, Joseph Naor,
and Leonid Zosin. 2001. Approximation algo-
rithms for the metric labeling problem via a new
linear programming formulation. In Symposium
on Discrete Algorithms, pages 109?118.
43
E. Dahlhaus, D. S. Johnson, C. H. Papadimitriou,
P. D. Seymour, and M. Yannakakis. 1992. The
complexity of multiway cuts (extended abstract).
In Proceedings of the twenty-fourth annual ACM
symposium on Theory of computing, pages 241?
251, New York, NY, USA. ACM Press.
H. Han, C. Giles, E. Manavoglu, H. Zha, Z. Zhang,
and E. Fox. 2003. Automatic document meta-
data extraction using support vector machines. In
Joint Conference on Digital Libraries.
Jon Kleinberg and Eva Tardos. 1999. Approx-
imation algorithms for classification problems
with pairwise relationships: Metric labeling and
markov random fields. In Proceedings of the 40th
Annual Symposium on Foundations of Computer
Science, page 14, Washington, DC, USA. IEEE
Computer Society.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In Proceedings of the 10th Inter-
national Conference on Machine Learning, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
John Lafferty, Xiaojin Zhu, and Yan Liu. 2004.
Kernel conditional random fields: representation
and clique selection. In Proceedings of the twenty-
first international conference on Machine learn-
ing, page 64, New York, NY, USA. ACM Press.
A. McCallum, K. Nigam, J. Rennie, and K. Sey-
more. 2000. Automating the construction of in-
ternet portals with machine learning. Information
Retrieval, 3:127?163.
Fuchun Peng and Andrew McCallum. 2004.
Accurate information extraction from research
papers using conditional random fields. In
Daniel Marcu Susan Dumais and Salim Roukos,
editors, Main Proceedings of HLT-NAACL, pages
329?336, Boston, Massachusetts, USA, May 2 -
May 7. Association for Computational Linguistics.
Erik Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the CoNLL-2000 shared task:
Chunking. In Proceedings of the Fourth Confer-
ence on Computational Natural Language Learn-
ing and of the Second Learning Language in Logic
Workshop. Association for Computational Lin-
guistics.
K. Seymore, A. McCallum, and R. Rosenfeld. 1999.
Learning hidden markov model structure for in-
formation extraction. In AAAI?99 Workshop on
Machine Learning for Information Extraction.
Vikas Sindhwani, Partha Niyogi, and Mikhail Belkin.
2005. Beyond the point cloud: from transductive
to semi-supervised learning. In Proceedings of the
22nd International Conference on Machine Learn-
ing, pages 824?831.
Alexander Smola and Risi Kondor. 2003. Kernels
and regularization on graphs. In M. Warmuth and
B. Scholkopf, editors, Proceedings of the Sixteenth
Annual Conference on Learning Theory and Ker-
nels Workshop.
Charles Sutton and Andrew McCallum. 2004. Col-
lective segmentation and labeling of distant enti-
ties in information extraction. Technical Report
TR # 04-49, University of Massachusetts, July.
Presented at ICML Workshop on Statistical Re-
lational Learning and Its Connections to Other
Fields.
Ben Taskar, Abbeel Pieter, and Daphne Koller.
2002. Discriminative probabilistic models for re-
lational data. In Proceedings of the 18th An-
nual Conference on Uncertainty in Artificial Intel-
ligence (UAI-02), pages 485?492, San Francisco,
CA. Morgan Kaufmann Publishers.
B. Taskar, V. Chatalbashev, and D. Koller. 2004a.
Learning associative markov networks. In Pro-
ceedings of the Twenty-First International Con-
ference on Machine Learning (ICML).
Ben Taskar, Carlos Guestrin, and Daphne Koller.
2004b. Max-margin markov networks. In Se-
bastian Thrun, Lawrence Saul, and Bernhard
Scho?lkopf, editors, Advances in Neural Informa-
tion Processing Systems 16. MIT Press, Cam-
bridge, MA.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large mar-
gin methods for structured and interdependent
output variables. JMLR, 6:1453?1484.
44
BioNLP 2007: Biological, translational, and clinical language processing, pages 129?136,
Prague, June 2007. c?2007 Association for Computational Linguistics
Automatic Code Assignment to Medical Text
Koby Crammer and Mark Dredze and Kuzman Ganchev and Partha Pratim Talukdar
Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA
{crammer|mdredze|kuzman|partha}@seas.upenn.edu
Steven Carroll
Division of Oncology, The Children?s Hospital of Philadelphia, Philadelphia, PA
carroll@genome.chop.edu
Abstract
Code assignment is important for handling
large amounts of electronic medical data in
the modern hospital. However, only expert
annotators with extensive training can as-
sign codes. We present a system for the
assignment of ICD-9-CM clinical codes to
free text radiology reports. Our system as-
signs a code configuration, predicting one or
more codes for each document. We com-
bine three coding systems into a single learn-
ing system for higher accuracy. We compare
our system on a real world medical dataset
with both human annotators and other auto-
mated systems, achieving nearly the maxi-
mum score on the Computational Medicine
Center?s challenge.
1 Introduction
The modern hospital generates tremendous amounts
of data: medical records, lab reports, doctor notes,
and numerous other sources of information. As hos-
pitals move towards fully electronic record keeping,
the volume of this data only increases. While many
medical systems encourage the use of structured in-
formation, including assigning standardized codes,
most medical data, and often times the most impor-
tant information, is stored as unstructured text.
This daunting amount of medical text creates
exciting opportunities for applications of learning
methods, such as search, document classification,
data mining, information extraction, and relation ex-
traction (Shortliffe and Cimino, 2006). These ap-
plications have the potential for considerable bene-
fit to the medical community as they can leverage
information collected by hospitals and provide in-
centives for electronic record storage. Much of the
data generated by medical personnel is unused past
the clinical visit, often times because there is no way
to simply and quickly apply the wealth of informa-
tion. Medical NLP holds the promise of both greater
care for individual patients and enhanced knowledge
about health care.
In this work we explore the assignment of ICD-9-
CM codes to clinical reports. We focus on this prac-
tical problem since it is representative of the type
of task faced by medical personnel on a daily ba-
sis. Many hospitals organize and code documents
for later retrieval using different coding standards.
Often times, these standards are extremely complex
and only trained expert coders can properly perform
the task, making the process of coding documents
both expensive and unreliable since a coder must se-
lect from thousands of codes a small number for a
given report. An accurate automated system would
reduce costs, simplify the task for coders, and create
a greater consensus and standardization of hospital
data.
This paper addresses some of the challenges asso-
ciated with ICD-9-CM code assignment to clinical
free text, as well as general issues facing applica-
tions of NLP to medical text. We present our auto-
mated system for code assignment developed for the
Computational Medicine Center?s challenge. Our
approach uses several classification systems, each
with the goal of predicting the exact code configu-
ration for a medical report. We then use a learning
129
system to combine our predictions for superior per-
formance.
This paper is organized as follows. First, we ex-
plain our task and difficulties in detail. Next we de-
scribe our three automated systems and features. We
combine the three approaches to create a single su-
perior system. We evaluate our system on clinical
reports and show accuracy approaching human per-
formance and the challenge?s best score.
2 Task Overview
The health care system employs a large number of
categorization and classification systems to assist
data management for a variety of tasks, including
patient care, record storage and retrieval, statistical
analysis, insurance, and billing. One of these sys-
tems is the International Classification of Diseases,
Ninth Revision, Clinical Modification (ICD-9-CM)
which is the official system of assigning codes to di-
agnoses and procedures associated with hospital uti-
lization in the United States. 1 The coding system
is based on World Health Organization guidelines.
An ICD-9-CM code indicates a classification of a
disease, symptom, procedure, injury, or information
from the personal history. Codes are organized hier-
archically, where top level entries are general group-
ings (e.g. ?diseases of the respiratory system?) and
bottom level codes indicate specific symptoms or
diseases and their location (e.g. ?pneumonia in as-
pergillosis?). Each specific, low-level code consists
of 4 or 5 digits, with a decimal after the third. Higher
level codes typically include only 3 digits. Overall,
there are thousands of codes that cover a broad range
of medical conditions.
Codes are assigned to medical reports by doc-
tors, nurses and other trained experts based on com-
plex coding guidelines (National Center for Health
Statistics, 2006). A particular medical report can be
assigned any number of relevant codes. For exam-
ple, if a patient exhibits a cough, fever and wheez-
ing, all three codes should be assigned. In addi-
tion to finding appropriate codes for each condition,
complex rules guide code assignment. For exam-
ple, a diagnosis code should always be assigned if a
diagnosis is reached, a diagnosis code should never
1http://www.cdc.gov/nchs/about/otheract/
icd9/abticd9.htm
be assigned when the diagnosis is unclear, a symp-
tom should never be assigned when a diagnosis is
present, and the most specific code is preferred. This
means that codes that seem appropriate to a report
should be omitted in specific cases. For example,
a patient with hallucinations should be coded 780.1
(hallucinations) but for visual hallucinations, the
correct code is 368.16. The large number of codes
and complexity of assignment rules make this a diffi-
cult problem for humans (inter-annotator agreement
is low). Therefore, an automated system that sug-
gested or assigned codes could make medical data
more consistent.
These complexities make the problem difficult
for NLP systems. Consider the task as multi-class,
multi-label. For a given document, many codes may
seem appropriate but it may not be clear to the algo-
rithm how many to assign. Furthermore, the codes
are not independent and different labels can inter-
act to either increase or decrease the likelihood of
the other. Consider a report that says, ?patient re-
ports cough and fever.? The presence of the words
cough and fever indicate codes 786.2 (cough) and
780.6 (fever). However, if the report continues to
state that ?patient has pneumonia? then these codes
are dropped in favor of 486 (pneumonia). Further-
more, if the report then says ?verify clinically?, then
the diagnosis is uncertain and only codes 786.2 and
780.6 apply. Clearly, this is a challenging problem,
especially for an automated system.
2.1 Corpus
We built and evaluated our system in accordance
with the Computational Medicine Center?s (CMC)
2007 Medical Natural Language Processing Chal-
lenge.2 Since release of medical data must strictly
follow HIPAA standards, the challenge corpus un-
derwent extensive treatment for disambiguation,
anonymization, and careful scrubbing. A detailed
description of data preparation is found in Compu-
tational Medicine Center (2007). We describe the
corpus here to provide context for our task.
The training corpus is comprised of 978 radiolog-
ical reports taken from real medical records. A test
corpus contains 976 unlabeled documents. Radiol-
ogy reports have two text fields, clinical history and
2www.computationalmedicine.org/challenge
130
impression. The physician ordering the x-ray writes
the clinical history, which contains patient informa-
tion for the radiologist, including history and current
symptoms. Sometimes a guess as to the diagnosis
appears (?evaluate for asthma?). The descriptions
are sometimes whole sentences and other times sin-
gle words (?cough?). The radiologist writes the im-
pression to summarize his or her findings. It con-
tains a short analysis and often times a best guess as
to the diagnosis. At times this field is terse, (?pneu-
monia? or ?normal kidneys?) and at others it con-
tains an entire paragraph of text. Together, these two
fields are used to assign ICD-9-CM codes, which
justify a certain procedure, possibly for reimburse-
ment by the insurance company.
Only a small percentage of ICD-9-CM codes ap-
pear in the challenge. In total, the reports include 45
different codes arranged in 94 configurations (com-
binations). Some of these codes appear frequently,
while others are rare, appearing only a single time.
The test set is restricted so that each configuration
appears at least once in the training set, although
there is no further guarantee as to the test set?s distri-
bution over codes. Therefore, in addition to a large
number of codes, there is variability in the amount
of data for each code. Four codes have over 100
examples each and 24 codes have 10 or fewer doc-
uments, with 10 of these codes having only a single
document.
Since code annotation is a difficult task, each doc-
ument in the corpus was evaluated by three expert
annotators. A gold annotation was created by tak-
ing the majority of the annotators; if two of the three
annotators provided a code, that code is used in the
gold configuration. This approach means that a doc-
ument?s configuration may be a construction of mul-
tiple annotators and may not match any of the three
annotators exactly. Both the individual and the ma-
jority annotations are included with the training cor-
pus.
While others have attempted ICD-9 code classi-
fication, our task differs in two respects (Section 7
provides an overview of previous work). First, pre-
vious work has used discharge reports, which are
typically longer with more text fields. Second, while
most systems are evaluated as a recommendation
system, offering the top k codes and then scoring
recall at k, our task is to provide the exact configu-
ration. The CMC challenge evaluated systems using
an F1 score, so we are penalized if we suggest any
label that does not appear in the majority annotation.
To estimate task difficulty we measured the inter-
annotator score for the training set using the three
annotations provided. We scored two annotations
with the micro average F1, which weighs each code
assignment equally (see Section 5 for details on
evaluation metrics). If an annotator omitted a code
and included an extra code, he or she is penalized
with a false positive (omitting a code) and a false
negative (adding an extra code). We measured anno-
tators against each other; the average f-measure was
74.85 (standard deviation of .06). These scores were
low since annotators chose from an unrestricted set
of codes, many of which were not included in the fi-
nal majority annotation. However, these scores still
indicate the human accuracy for this task using an
unrestricted label set. 3
3 Code Assignment System
We developed three automated systems guided by
our above analysis. First, we designed a learning
system that used natural language features from the
official code descriptions and the text of each re-
port. It is general purpose and labels all 45 codes
and 94 configurations (labels). Second, we built a
rule based system that assigned codes based on the
overlap between the reports and code descriptions,
similar to how an annotator may search code de-
scriptions for appropriate labels. Finally, a special-
ized system aimed at the most common codes imple-
mented a policy that mimics the guidelines a medical
staffer would use to assign these codes.
3.1 Learning System
We begin with some notational definitions. In what
follows, x denotes the generic input document (ra-
diology report), Y denotes the set of possible label-
ings (code configurations) of x, and y?(x) the cor-
rect labeling of x. For each pair of document x
and labeling y ? Y , we compute a vector-valued
feature representation f(x, y). A linear model is
3We also measured each annotator with the majority codes,
taking the average score (87.48), and the best annotator with
the majority label (92.8). However, these numbers are highly
biased since the annotator influences the majority labeling. We
observe that our final system still exceeds the average score.
131
given by a weight vector w. Given this weight vec-
tor w, the score w ? f(x, y) ranks possible labelings
of x, and we denote by Yk,w(x) the set of k top
scoring labelings for x. For some structured prob-
lems, a factorization of f(x, y) is required to enable
a dynamic program for inference. For our problem,
we know all the possible configurations in advance
(there are 94 of them) so we can pick the highest
scoring y ? Y by trying them all. For each docu-
ment x and possible labeling y, we compute a score
using w and the feature representation f(x, y). The
top scoring y is output as the correct label. Section
3.1.1 describes our feature function f(x, y) while
Section 3.1.2 describes how we find a good weight
vector w.
3.1.1 Features
Problem representation is one of the most impor-
tant aspects of a learning system. In our case, this
is defined by the set of features f(x, y). Ideally we
would like a linear combination of our features to ex-
actly specify the true labeling of all the instances, but
we want to have a small total number of features so
that we can accurately estimate their values. We sep-
arate our features into two classes: label specific fea-
tures and transfer features. For simplicity, we index
features by their name. Label specific features are
only present for a single label. For example, a simple
class of label specific features is the conjunction of a
word in the document with an ICD-9-CM code in the
label. Thus, for each word we create 94 features, i.e.
the word conjoined with every label. These features
tend to be very powerful, since weights for them can
encode very specific information about the way doc-
tors talk about a disease, such as the feature ?con-
tains word pneumonia and label contains code 486?.
Unfortunately, the cost of this power is that there are
a large number of these features, making parameter
estimation difficult for rare labels. In contrast, trans-
fer features can be present in multiple labels. An
example of a transfer feature might be ?the impres-
sion contains all the words in the code descriptions
of the codes in this label?. Transfer features allow us
to generalize from one label to another by learning
things like ?if all the words of the label description
occur in the impression, then this label is likely? but
have the drawback that we cannot learn specific de-
tails about common labels. For example, we cannot
learn that the word ?pneumonia? in the impression
is negatively correlated with the code cough. The
inclusion of both label specific and transfer features
allows us to learn specificity where we have a large
number of examples and generality for rare codes.
Before feature extraction we normalized the re-
ports? text by converting it to lower case and by
replacing all numbers (and digit sequences) with a
single token ?NUM?. We also prepared a synonym
dictionary for a subset of the tokens and n-grams
present in the training data. The synonym dictionary
was based onMeSH4, the Medical Subject Headings
vocabulary, in which synonyms are listed as terms
under the same concept. All ngrams and tokens
in the training data which had mappings defined in
the synonym dictionary were then replaced by their
normalized token; e.g. all mentions of ?nocturnal
enuresis? or ?nighttime urinary incontinence? were
replaced by the token ?bedwetting?. Additionally,
we constructed descriptions for each code automati-
cally from the official ICD-9-CM code descriptions
in National Center for Health Statistics (2006). We
also created a mapping between code and code type
(diagnosis or symptom) using the guidelines.
Our system used the following features. The de-
scriptions of particular features are in quotes, while
schemes for constructing features are not.
? ?this configuration contains a disease code?,
?this configuration contains a symptom code?,
?this configuration contains an ambiguous
code? and ?this configuration contains both dis-
ease and symptom codes?.5
? With the exception of stop-words, all words of
the impression and history conjoined with each
label in the configuration; pairs of words con-
joined with each label; words conjoined with
pairs of labels. For example, ?the impression
contains ?pneumonia? and the label contains
codes 786.2 and 780.6?.
? A feature indicating when the history or im-
pression contains a complete code description
4www.nlm.nih.gov/mesh
5We included a feature for configurations that had both dis-
ease and symptom codes because they appeared in the training
data, even though coding guidelines prohibit these configura-
tions.
132
for the label; one for a word in common with
the code description for one of the codes in the
label; a common word conjoined with the pres-
ence of a negation word nearby (?no?, ?not?,
etc.); a word in common with a code descrip-
tion not present in the label. We applied similar
features using negative words associated with
each code.
? A feature indicating when a soft negation word
appears in the text (?probable?, ?possible?,
?suspected?, etc.) conjoined with words that
follow; the token length of a text field (?im-
pression length=3?); a conjunction of a feature
indicating a short text field with the words in
the field (?impression length=1 and ?pneumo-
nia? ?)
? A feature indicating each n-gram sequence that
appears in both the impression and clinical his-
tory; the conjunction of certain terms where
one appears in the history and the other in the
impression (e.g. ?cough in history and pneu-
monia in impression?).
3.1.2 Learning Technique
Using these feature representations, we now learn
a weight vector w that scores the correct labelings
of the data higher than incorrect labelings. We used
a k-best version of the MIRA algorithm (Crammer,
2004; McDonald et al, 2005). MIRA is an online
learning algorithm that for each training document
x updates the weight vector w according to the rule:
wnew = argmin
w
?w ? wold?
s.t. ?y ? Yk,wold(x) :
w ? f(x, y?(x)) ? w ? f(x, y) ? L(y?(x), y)
where L(y?(x), y) is a measure of the loss of label-
ing y with respect to the correct labeling y?(x). For
our experiments, we set k to 30 and iterated over the
training data 10 times. Two standard modifications
to this approach also helped. First, rather than using
just the final weight vector, we average all weight
vectors. This has a smoothing effect that improves
performance on most problems. The second modifi-
cation is the introduction of slack variables:
wnew = argmin
w
?w ? wold? + ?
?
i
?i
s.t. ?y ? Yk,wold(x) :
w ? f(x, y?(x)) ? w ? f(x, y) ? L(y?(x), y) ? ?i
?i ? {1 . . . k} : ?i ? 0.
We used a ? of 10?3 in our experiments.
The most straightforward loss function is the 0/1
loss, which is one if y does not equal y?(x) and zero
otherwise. Since we are evaluated based on the num-
ber of false negative and false positive ICD-9-CM
codes assigned to all the documents, we used a loss
that is the sum of the number of false positive and the
number of false negative labels that y assigns with
respect to y?(x).
Finally, we only used features that were possi-
ble for some labeling of the test data by using only
the test data to construct our feature alphabet. This
forced the learner to focus on hypotheses that could
be used at test time and resulted in a 1% increase in
F-measure in our final system on the test data.
3.2 Rule Based System
Since some of the configurations appear a small
number of times in our corpus (some only once),
we built a rule based system that requires no train-
ing. The system uses a description of the ICD-9-CM
codes and their types, similar to the list used by our
learning system (Section 3.1.1). The code descrip-
tions include between one and four short descrip-
tions, such as ?reactive airway disease?, ?asthma?,
and ?chronic obstructive pulmonary disease?. We
treat each of these descriptions as a bag of words.
For a given report, the system parses both the clini-
cal history and impression into sentences, using ?.?
as a sentence divider. Each sentence is the checked
to see if all of the words in a code description appear
in the sentence. If a match is found, we set a flag
corresponding to the code. However, if the code is
a disease, we search for a negation word in the sen-
tence, removing the flag if a negation word is found.
Once all code descriptions have been evaluated, we
check if there are any flags set for disease codes. If
so, we remove all symptom code flags. We then emit
a code corresponding to each set flag. This simple
system does not enforce configuration restrictions;
133
we may predict a code configuration that does not
appear in our training data. Adding this restriction
improved precision but hurt recall, leading to a slight
decrease in F1 score. We therefore omitted the re-
striction from our system.
3.3 Automatic Coding Policies
As we described in Section 2, enforcing coding
guidelines can be a complex task. While a learning
system may have trouble coding a document, a hu-
man may be able to define a simple policy for cod-
ing. Since some of the most frequent codes in our
dataset have this property, we decided to implement
such an automatic coding policy. We selected two
related sets of codes to target with a rule based sys-
tem, a set of codes found in pneumonia reports and
a set for urinary tract infection/reflux reports.
Reports related to pneumonia are the most com-
mon in our dataset and include codes for pneumo-
nia, asthma, fever, cough and wheezing; we handle
them with a single policy. Our policy is as follows:
? Search for a small set of keywords (e.g.
?cough?, ?fever?) to determine if a code should
be applied.
? If ?pneumonia? appears unnegated in the im-
pression and the impression is short, or if it oc-
curs in the clinical history and is not preceded
by phrases such as ?evaluate for? or ?history
of?, apply pneumonia code and stop.
? Use the same rule to code asthma by looking
for ?asthma? or ?reactive airway disease?.
? If no diagnosis is found, code all non-negated
symptoms (cough, fever, wheezing).
We selected 80% of the training set to evaluate in the
construction of our rules. We then ran the finished
system on both this training set and the held out 20%
of the data. The system achieved F1 scores of 87%
on the training set and 84% on the held out data for
these five codes. The comparable scores indicates
that we did not over-fit the training data.
We designed a similar policy for two other related
codes, urinary tract infection and vesicoureteral re-
flux. We found these codes to be more complex as
they included a wide range of kidney disorders. On
these two codes, our system achieved 78% on the
train set and 76% on the held out data. Overall, au-
tomatically applying our two policies yielded high
confidence predictions for a significant subset of the
corpus.
4 Combined System
Since our three systems take complimentary ap-
proaches to the problem, we combined them to im-
prove performance. First, we took our automatic
policy and rule based systems and cascaded them; if
the automatic policy system does not apply a code,
the rule based system classifies the report. We used
a cascaded approach since the automatic policy sys-
tem was very accurate when it was able to assign
a code. Therefore, the rule based system defers to
the policy system when it is triggered. Next, we in-
cluded the prediction of the cascaded system as a
feature for our learning system. We used two fea-
ture rules: ?cascaded-system predicted exactly this
label? and ?cascaded-system predicted one of the
codes in this label?. As we show, this yielded our
most accurate system. While we could have used a
meta-classifier to combine the three systems, includ-
ing the rule based systems as features to the learning
system allowed it to learn the appropriate weights
for the rule based predictions.
5 Evaluation Metric
Evaluation metrics for this task are often based on
recommendation systems, where the system returns
a list of the top k codes for selection by the user. As
a result, typical metrics are ?recall at k? and aver-
age precision (Larkey and Croft, 1995). Instead, our
goal was to predict the exact configuration, returning
exactly the number of codes predicted to be on the
report. The competition used a micro-averaged F1
score to evaluate predictions. A contingency table
(confusion matrix) is computed by summing over
each predicted code for each document by predic-
tion type (true positive, false positive, false negative)
weighing each code assignment equally. F1 score
is computed based on the resultant table. If specific
codes or under-coding is favored, we can modify our
learning loss function as described in Section 3.1.2.
A detailed treatment of this evaluation metric can be
found in Computational Medicine Center (2007).
134
System Precision Recall F1
BL 61.86 72.58 66.79
RULE 81.9 82.0 82.0
CASCADE 86.04 84.56 85.3
LEARN 85.5 83.6 84.6
CASCADE+LEARN 87.1 85.9 86.5
Table 1: Performance of our systems on the provided
labeled training data (F1 score). The learning sys-
tems (CASCADE+LEARN and LEARN ) were eval-
uated on ten random split of the data while RULE
was evaluated on all of the training data. We include
a simple rule based system (BL ) as a baseline.
6 Results
We evaluated our systems on the labeled training
data of 978 radiology reports. For each report, each
system predicted an exact configuration of codes
(i.e. one of 94 possible labels). We score each sys-
tem using a micro-averaged F1 score. Since we only
had labels for the training data, we divided the data
using an 80/20 training test split and averaged results
over 10 runs for our learning systems. We evaluated
the following systems:
? RULE : The rule based system based on ICD-
9-CM code descriptions (Section 3.2).
? CASCADE : The automatic code policy system
(Section 3.3) cascaded with RULE (Section 4).
? LEARN : The learning system with both label
specific and transfer features (Section 3.1).
? CASCADE+LEARN : Our combined system
that incorporates CASCADE predictions as a
feature to LEARN (Section 4).
For a baseline, we built a simple system that ap-
plies the official ICD-9-CM code descriptions to find
the correct labels (BL ). For each code in the train-
ing set, the system generates text-segments related to
it. During testing, for each new document, the sys-
tem checks if any text-segment (as discovered dur-
ing training) appears in the document. If so, the cor-
responding code is predicted. The results from our
four systems and baseline are shown in Table 1.
System Train Test
CASCADE 85.3 84
CASCADE+LEARN 86.5 87.60
Average - 76.6
Best - 89.08
Table 2: Performance of two systems on the train
and test data. Results obtained from the web sub-
mission interface were rounded. Average and Best
are the average and best f-measures of the 44 sub-
mitted systems (standard deviation 13.40).
Each of our systems easily beats the baseline, and
the average inter-annotator score for this task. Ad-
ditionally, we were able to evaluate two of our sys-
tems on the test data using a web interface as pro-
vided by the competition. The test set contains 976
documents (about the same as the training set) and
is drawn the from same distribution as the training
data. Our test results were comparable to perfor-
mance on the training data, showing that we did
not over-fit to the training data (Table 2). Addi-
tionally, our combined system (CASCADE+LEARN
) achieved a score of 87.60%, beating our training
data performance and exceeding the average inter-
annotator score. Out of 44 submitted systems, the
average score on test data was 76.7% (standard devi-
ation of 13.40) and the maximum score was 89.08%.
Our system scored 4th overall and was less than
1.5% behind the best system. Overall, in comparison
with our baselines and over 40 systems, we perform
very well on this task.
7 Related Work
There have been several attempts at ICD-9-CM
code classification and related problems for med-
ical records. The specific problem of ICD-9-CM
code assignment was studied by Lussier et al (2000)
through an exploratory study. Larkey and Croft
(1995) designed classifiers for the automatic assign-
ment of ICD-9 codes to discharge summaries. Dis-
charge summaries tend to be considerably longer
than our data and contain multiple text fields. Ad-
ditionally, the number of codes per document has
a larger range, varying between 1 and 15 codes.
Larkey and Croft use three classifiers: K-nearest
neighbors, relevance feedback, and bayesian inde-
135
pendence. Similar to our approach, they tag items
as negated and try to identify diagnosis and symp-
tom terms. Additionally, their final system combines
all three models. A direct comparison is not possi-
ble due to the difference in data and evaluation met-
rics; they use average precision and recall at k. On
a comparable metric, ?principal code is top candi-
date?, their best system achieves 59.9% accuracy. de
Lima et al (1998) rely on the hierarchical nature of
medical codes to design a hierarchical classification
scheme. This approach is likely to help on our task
as well but we were unable to test this since the lim-
ited number of codes removes any hierarchy. Other
approaches have used a variety of NLP techniques
(Satomura and Amaral, 1992).
Others have used natural language systems for the
analysis of medical records (Zweigenbaum, 1994).
Chapman and Haug (1999) studied radiology re-
ports looking for cases of pneumonia, a goal sim-
ilar to that of our automatic coding policy system.
Meystre and Haug (2005) processed medical records
to harvest potential entries for a medical problem
list, an important part of electronic medical records.
Chuang et al (2002) studied Charlson comorbidi-
ties derived from processing discharge reports and
chest x-ray reports and compared them with admin-
istrative data. Additionally, Friedman et al (1994)
applies NLP techniques to radiology reports.
8 Conclusion
We have presented a learning system that processes
radiology reports and assigns ICD-9-CM codes.
Each of our systems achieves results comparable
with an inter-annotator baseline for our training data.
A combined system improves over each individ-
ual system. Finally, we show that on test data un-
available during system development, our final sys-
tem continues to perform well, exceeding the inter-
annotator baseline and achieving the 4th best score
out of 44 systems entered in the CMC challenge.
9 Acknowledgements
We thank Andrew Lippa for his extensive medical
wisdom. Dredze is supported by an NDSEG fel-
lowship; Ganchev and Talukdar by NSF ITR EIA-
0205448; and Crammer by DARPA under Contract
No. NBCHD03001. Any opinions, findings, and
conclusions or recommendations expressed in this
material are those of the author(s) and do not nec-
essarily reflect the views of the DARPA or the De-
partment of Interior-National Business Center (DOI-
NBC).
References
W.W. Chapman and P.J. Haug. 1999. Comparing expert sys-
tems for identifying chest x-ray reports that support pneu-
monia. In AMIA Symposium, pages 216?20.
JH Chuang, C Friedman, and G Hripcsak. 2002. A com-
parison of the charlson comorbidities derived from medical
language processing and administrative data. AMIA Sympo-
sium, pages 160?4.
Computational Medicine Center. 2007. The
computational medicine center?s 2007 med-
ical natural language processing challenge.
http://computationalmedicine.org/challenge/index.php.
Koby Crammer. 2004. Online Learning of Complex Categorial
Problems. Ph.D. thesis, Hebrew Univeristy of Jerusalem.
Luciano R. S. de Lima, Alberto H. F. Laender, and Berthier A.
Ribeiro-Neto. 1998. A hierarchical approach to the auto-
matic categorization of medical documents. In CIKM.
C Friedman, PO Alderson, JH Austin, JJ Cimino, and SB John-
son. 1994. A general natural-language text processor for
clinical radiology. Journal of the American Medical Infor-
matics Association, 1:161?74.
Leah S. Larkey and W. Bruce Croft. 1995. Automatic assign-
ment of icd9 codes to discharge summaries. Technical re-
port, University of Massachusetts at Amherst, Amherst, MA.
YA Lussier, C Friedman, L Shagina, and P Eng. 2000. Au-
tomating icd-9-cm encoding using medical language pro-
cessing: A feasibility study.
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005.
Flexible text segmentation with structured multilabel classi-
fication. In HLT/EMNLP.
Stephane Meystre and Peter J Haug. 2005. Automation of a
problem list using natural language processing. BMC Medi-
cal Informatics and Decision Making.
National Center for Health Statistics. 2006. Icd-
9-cm official guidelines for coding and reporting.
http://www.cdc.gov/nchs/datawh/ftpserv/ftpicd9/ftpicd9.htm.
Y Satomura and MB Amaral. 1992. Automated diagnostic in-
dexing by natural language processing. Medical Informat-
ics, 17:149?163.
Edward H. Shortliffe and James J. Cimino, editors. 2006.
Biomedical Informatics: Computer Applications in Health
Care and Biomedicine. Springer.
P. Zweigenbaum. 1994. Menelas: an access system for medical
records using natural language. Comput Methods Programs
Biomed, 45:117?20.
136
Proceedings of the Linguistic Annotation Workshop, pages 53?56,
Prague, June 2007. c?2007 Association for Computational Linguistics
Semi-Automated Named Entity Annotation
Kuzman Ganchev and Fernando Pereira
Computer and Information Science,
University of Pennsylvania,
Philadelphia PA
{ kuzman and pereira } @cis.upenn.edu
Mark Mandel
Linguistic Data Consortium,
University of Pennsylvania, Philadelphia PA
mamandel@ldc.upenn.edu
Steven Carroll and Peter White
Division of Oncology, Children?s Hospital of Philadelphia Philadelphia PA
{ carroll and white }@genome.chop.edu
Abstract
We investigate a way to partially automate
corpus annotation for named entity recogni-
tion, by requiring only binary decisions from
an annotator. Our approach is based on a lin-
ear sequence model trained using a k-best
MIRA learning algorithm. We ask an an-
notator to decide whether each mention pro-
duced by a high recall tagger is a true men-
tion or a false positive. We conclude that our
approach can reduce the effort of extending
a seed training corpus by up to 58%.
1 Introduction
Semi-automated text annotation has been the subject
of several previous studies. Typically, a human an-
notator corrects the output of an automatic system.
The idea behind our approach is to start annota-
tion manually and to partially automate the process
in the later stages. We assume that some data has
already been manually tagged and use it to train a
tagger specifically for high recall. We then run this
tagger on the rest of our corpus and ask an annotator
to filter the list of suggested gene names.
The rest of this paper is organized as follows. Sec-
tion 2 describes the model and learning algorithm.
Section 3 relates our approach to previous work.
Section 4 describes our experiments and Section 5
concludes the paper.
2 Methods
Throughout this work, we use a linear sequence
model. This class of models includes popular tag-
ging models for named entities such as conditional
random fields, maximum entropy Markov models
and max-margin Markov networks. Linear sequence
models score possible tag sequences for a given in-
put as the dot product between a learned weight vec-
tor and a feature vector derived from the input and
proposed tas sequence. Linear sequence models dif-
fer principally on how the weight vector is learned.
Our experiments use the MIRA algorithm (Cram-
mer et al, 2006; McDonald et al, 2005) to learn
the weight vector.
2.1 Notation
In what follows, x denotes the generic input sen-
tence, Y (x) the set of possible labelings of x, and
Y +(x) the set of correct labelings of x. There is
also a distinguished ?gold? labeling y(x) ? Y +(x).
For each pair of a sentence x and labeling y ?
Y (x), we compute a vector-valued feature represen-
tation f(x, y). Given a weight vector w, the score
w ? f(x, y) ranks possible labelings of x, and we de-
note by Yk,w(x) the set of k top scoring labelings for
x.
We use the standard B,I,O encoding for named
entities (Ramshaw and Marcus, 1995). Thus Y (x)
for x of length n is the set of all sequences of length
n matching the regular expression (O|(BI?))?. In a
linear sequence model, for suitable feature functions
f , Yk,w(x) can be computed efficiently with Viterbi
decoding.
2.2 k-best MIRA and Loss Functions
The learning portion of our method finds a weight
vector w that scores the correct labelings of the test
data higher than incorrect labelings. We used a k-
53
best version of the MIRA algorithm (Crammer et
al., 2006; McDonald et al, 2005). This is an online
learning algorithm that starts with a zero weight vec-
tor and for each training sentence makes the small-
est possible update that would score the correct la-
bel higher than the old top k labels. That is, for each
training sentence x we update the weight vector w
according to the rule:
wnew = argminw ?w ? wold?
s. t. w ? f(x, y(x)) ? w ? f(x, y) ? L(Y +(x), y)
?y ? Yk,wold(x)
where L(Y +(x), y) is the loss, which measures the
errors in labeling y relative to the set of correct la-
belings Y +(x).
An advantage of the MIRA algorithm (over many
other learning algorithms such as conditional ran-
dom fields) is that it allows the use of arbitrary loss
functions. For our experiments, the loss of a label-
ing is a weighted combination of the number of false
positive mentions and the number of false negative
mentions in that labeling.
2.3 Semi-Automated Tagging
For our semi-automated annotation experiments, we
imagine the following scenario: We have already an-
notated half of our training corpus and want to anno-
tate the remaining half. The goal is to save annotator
effort by using a semi-automated approach instead
of annotating the rest entirely manually.
In particular we investigate the following method:
train a high-recall named entity tagger on the anno-
tated data and use that to tag the remaining corpus.
Now ask a human annotator to filter the resulting
mentions. The mentions rejected by the annotator
are simply dropped from the annotation, leaving the
remaining mentions.
3 Relation to Previous Work
This section relates our approach to previous work
on semi-automated approaches. First we discuss
how semi-automated annotation is different from ac-
tive learning and then discuss some previous semi-
automated annotation work.
3.1 Semi-Automated versus Active Learning
It is important not to confuse semi-automated anno-
tation with active learning. While they both attempt
to alleviate the burden of creating an annotated cor-
pus, they do so in a completely orthogonal manner.
Active learning tries to select which instances should
be labeled in order to make the most impact on learn-
ing. Semi-automated annotation tries to make the
annotation of each instance faster or easier. In par-
ticular, it is possible to combine active learning and
semi-automated annotation by using an active learn-
ing method to select which sentences to label and
then using a semi-automated labeling method.
3.2 Previous work on semi-automated
annotation
The most common approach to semi-automatic an-
notation is to automatically tag an instance and then
ask an annotator to correct the results. We restrict
our discussion to this paradigm due to space con-
straints. Marcus et al (1994), Chiou et al (2001)
and Xue et al (2002) apply this approach with some
minor modifications to part of speech tagging and
phrase structure parsing. The automatic system of
Marcus et al only produces partial parses that are
then assembled by the annotators, while Chiou et al
modified their automatic parser specifically for use
in annotation. Chou et al (2006) use this tag and
correct approach to create a corpus of predicate ar-
gument structures in the biomedical domain. Culota
et al (2006) use a refinement of the tag and correct
approach to extract addressbook information from e-
mail messages. They modify the system?s best guess
as the user makes corrections, resulting in less anno-
tation actions.
4 Experiments
We now evaluate to what extent our semi-automated
annotation framework can be useful, and how much
effort it requires. For both questions we compare
semi-automatic to fully manual annotation. In our
first set of experiments, we measured the usefulness
of semi-automatically annotated corpora for training
a gene mention tagger. In the second set of exper-
iments, we measured the annotation effort for gene
mentions with the standard fully manual method and
with the semi-automated methods.
4.1 Measuring Effectiveness
The experiments in this section use the training data
from the the Biocreative II competition (Tanabe et
54
Sentence Expression of SREBP-1a stimulated StAR promoter activity in the context of COS-1 cells
gold label Expression of SREBP-1a stimulated StAR promoter activity in . . .
alternative Expression of SREBP-1a stimulated StAR promoter activity in . . .
alternative Expression of SREBP-1a stimulated StAR promoter activity in . . .
Figure 1: An example sentence and its annotation in Biocreative II. The evaluation metric would give full
credit for guessing one of the alternative labels rather than the ?gold? label.
al., 2005). The data is supplied as a set of sentences
chosen randomly fromMEDLINE and annotated for
gene mentions.
Each sentence in the corpus is provided as a list of
?gold? gene mentions as well as a set of alternatives
for each mention. The alternatives are generated by
the annotators and count as true positives. Figure 1
shows an example sentence with its gold and alter-
native mentions. The evaluation metric for these ex-
periments is F-score augmented with the possibility
of alternatives (Yeh et al, 2005).
We used 5992 sentences as the data that has al-
ready been annotated manually (set Data-1), and
simulated different ways of annotating the remain-
ing 5982 sentences (set Data-2). We compare the
quality of annotation by testing taggers trained us-
ing these corpora on a 1493 sentence test set.
We trained a high-recall tagger (recall of 89.6%)
on Data-1, and ran it on Data-2. Since we have
labels available for Data-2, we simulated an anno-
tator filtering these proposed mentions by accepting
them only if they exactly match a ?gold? or alterna-
tive mention. This gave us an F-score of 94.7% on
Data-2 and required 9981 binary decisions.
Figure 2 shows F1 score as a function of the num-
ber of extra sentences annotated. Without any ad-
ditional data, the F-measure of the tagger is 81.0%.
The two curves correspond to annotation with and
without alternatives. The horizontal line at 82.8%
shows the level achieved by the semi-automatic
method (when using all of Data-2).
From the figure, we can see that to get compa-
rable performance to the semi-automatic approach,
we need to fully manually annotate roughly a third
as much data with alternatives, or about two thirds as
much data without alternatives. The following sec-
tion examines what this means in terms of annotator
time by providing timing results for semi-automatic
and fully-manual annotation without alternatives.
 81 81.5 82 82.5 83 83.5 84 84.5 85
 0
 1000
 2000
 3000
 4000
 5000
 6000
Extra
 Anno
tated 
Sente
nces (
from 
Data-
2)
Manu
al Wi
th Alt
ernati
ves
Manu
al w/o
 Alter
native
s
Semi-
Autom
atic (o
n all o
f Data
-2)
Figure 2: Effect of the number of annotated in-
stances on F1 score. In all cases the original 5992
instances were used; the curves show manual an-
notation while the level line is the semi-automatic
method. The curves are averages over 3 trials.
4.2 Measuring Effort
The second set of experiments compares annotator
effort between fully manual and semi-automatic an-
notation. Because we did not have access to an expe-
rienced annotator from the Biocreative project, and
gene mention annotations vary subtly among anno-
tation efforts, we evaluated annotator effort on on the
PennBioIE named entity corpus.1 Furthermore, we
have not yet annotated enough data locally to per-
form both effectiveness and effort experiments on
the local corpus alone. However, both corpora an-
notate gene mentions in MEDLINE abstracts, so we
expect that the timing results will not be significantly
different.
We asked an experienced annotator to tag 194
MEDLINE abstracts: 96 manually and 98 using the
semi-automated method. Manual annotation was
done using annotation software familiar to the an-
notator. Semi-automatic annotation was done with a
1Available from http://bioie.ldc.upenn.edu/
55
Web-based tool developed for the task. The new tool
highlights potential gene mentions in the text and al-
lows the annotator to filter them with a mouse click.
The annotator had been involved in the creation of
the local manually annotated corpus, and had a lot of
experience annotating named entities. The abstracts
for annotation were selected randomly so that they
did not contain any abstracts tagged earlier. There-
fore, we did not expect the annotator to have seen
any of them before the experiment.
To generate potential gene mentions for the semi-
automated annotation, we ran two taggers on the
data: a high recall tagger trained on the local corpus
and a high recall tagger trained on the Biocreative
corpus. At decode time, we took the gene mentions
from the top two predictions of each of these taggers
whenever there were any gene mentions predicted.
As a result, the annotator had to make more binary
decisions per sentence than they would have for ei-
ther training corpus alone. For the semi-automated
annotation, the annotator had to examine 682 sen-
tences and took on average 10 seconds per sentence.
For the fully-manual annotation, they examined 667
sentences and took 40 seconds per sentence on av-
erage. We did not ask the annotator to tag alterna-
tives because they did not have any experience with
tagging alternatives and we do not have a tool that
makes the annotation of alternatives easy. Conse-
quently, effort totals for annotation with alternatives
would have been skewed in our favor. The four-fold
speedup should be compared to the lower curve in
Figure 2.
5 Discussion and Further Work
We can use the effort results to estimate the relative
effort of annotating without alternatives and of semi-
automated annotation. To obtain the same improve-
ment in F-score, we need to semi-automatically an-
notate roughly a factor of 1.67 more data than using
the fully manual approach. Multiplying that by the
0.25 factor reduction in annotation time, we get that
the time required for a comparable improvement in
F-score is 0.42 times as long ? a 58% reduction in
annotator time.
We do not have any experiments on annotating
alternatives, but the main difference between semi-
automated and fully-manual annotation is that the
former does not require the annotator to decide on
boundaries. Consequently, we expect that annota-
tion with alternatives will be considerably more ex-
pensive than without alternatives, since more bound-
aries have to be outlined.
In future work, it would be interesting to compare
this approach to the traditional approach of manually
correcting output of a system. Due to constraints
on annotator time, it was not possible to do these
experiments as part of the current work.
References
Fu-Dong Chiou, David Chiang, and Martha Palmer.
2001. Facilitating treebank annotation using a statisti-
cal parser. In HLT ?01. ACL.
Wen-Chi Chou, Richard Tzong-Han Tsai, Ying-Shan Su,
Wei Ku, Ting-Yi Sung, and Wen-Lian Hsu. 2006.
A semi-automatic method for annotating a biomedical
proposition bank. In FLAC?06. ACL.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. JMLR, 7.
Aron Culota, Trausti Kristjansson, Andrew McCallum,
and Paul Viola. 2006. Corrective feedback and per-
sistent learning for information extraction. Artificial
Intelligence, 170:1101?1122.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In ACL?05. ACL.
Lance Ramshaw and Mitch Marcus. 1995. Text chunk-
ing using transformation-based learning. In David
Yarovsky and Kenneth Church, editors, Proceedings
of the Third Workshop on Very Large Corpora. ACL.
Lorraine Tanabe, Natalie Xie, Lynne H. Thom, Wayne
Matten, and W. John Wilbur. 2005. GENETAG: a
tagged corpus for gene/protein named entity recogni-
tion. BMC Bioinformatics, 6(Suppl. 1).
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated chinese corpus.
In Proceedings of the 19th international conference on
Computational linguistics. ACL.
Alexander Yeh, Alexander Morgan, Marc Colosimo, and
Lynette Hirschman. 2005. BioCreAtIvE Task 1A:
gene mention finding evaluation . BMC Bioinformat-
ics, 6(Suppl. 1).
56
Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 19?20,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Small Statistical Models by Random Feature Mixing
Kuzman Ganchev and Mark Dredze
Department of Computer and Information Science
University of Pennsylvania, Philadelphia, PA
{kuzman,mdredze}@cis.upenn.edu
Abstract
The application of statistical NLP systems to
resource constrained devices is limited by the
need to maintain parameters for a large num-
ber of features and an alphabet mapping fea-
tures to parameters. We introduce random
feature mixing to eliminate alphabet storage
and reduce the number of parameters without
severely impacting model performance.
1 Introduction
Statistical NLP learning systems are used for many
applications but have large memory requirements, a
serious problem for mobile platforms. Since NLP
applications use high dimensional models, a large
alphabet is required to map between features and
model parameters. Practically, this means storing
every observed feature string in memory, a pro-
hibitive cost for systems with constrained resources.
Offline feature selection is a possible solution, but
still requires an alphabet and eliminates the poten-
tial for learning new features after deployment, an
important property for adaptive e-mail or SMS pre-
diction and personalization tasks.
We propose a simple and effective approach to
eliminate the alphabet and reduce the problem of di-
mensionality through random feature mixing. We
explore this method on a variety of popular datasets
and classification algorithms. In addition to alpha-
bet elimination, this reduces model size by a factor
of 5?10 without a significant loss in performance.
2 Method
Linear models learn a weight vector over features
constructed from the input. Features are constructed
as strings (e.g. ?w=apple? interpreted as ?contains
the word apple?) and converted to feature indices
maintained by an alphabet, a map from strings to
integers. Instances are efficiently represented as a
sparse vector and the model as a dense weight vec-
tor. Since the alphabet stores a string for each fea-
ture, potentially each unigram or bigram it encoun-
ters, it is much larger than the weight vector.
Our idea is to replace the alphabet with a random
function from strings to integers between 0 and an
intended size. This size controls the number of pa-
rameters in our model. While features are now eas-
ily mapped to model parameters, multiple features
can collide and confuse learning. The collision rate
is controlled by the intended size. Excessive colli-
sions can make the learning problem more difficult,
but we show significant reductions are still possible
without harming learning. We emphasize that even
when using an extremely large feature space to avoid
collisions, alphabet storage is eliminated. For the
experiments in this paper we use Java?s hashCode
function modulo the intended size rather than a ran-
dom function.
3 Experiments
We evaluated the effect of random feature mix-
ing on four popular learning methods: Perceptron,
MIRA (Crammer et al, 2006), SVM and Maximum
entropy; with 4 NLP datasets: 20 Newsgroups1,
Reuters (Lewis et al, 2004), Sentiment (Blitzer
et al, 2007) and Spam (Bickel, 2006). For each
dataset we extracted binary unigram features and
sentiment was prepared according to Blitzer et al
(2007). From 20 Newsgroups we created 3 binary
decision tasks to differentiate between two similar
1
http://people.csail.mit.edu/jrennie/20Newsgroups/
19
 70 75 80 85 90  0 10 2
0 30 40 50
 60 70 80 9
0
thousands of
 featuresfeature mixi
ng
no feature m
ixing
 70 75 80 85 90  0 10 2
0 30 40 50
 60 70 80 9
0
thousands of
 featuresfeature mixi
ng
no feature m
ixing
Figure 1: Kitchen appliance reviews. Left: Maximum en-
tropy. Right: Perceptron. Shaded area and vertical lines
extend one standard deviation from the mean.
labels from computers, science and talk. We cre-
ated 3 similar problems from Reuters from insur-
ance, business services and retail distribution. Senti-
ment used 4 Amazon domains (book, dvd, electron-
ics, kitchen). Spam used the three users from task
A data. Each problem had 2000 instances except for
20 Newsgroups, which used between 1850 and 1971
instances. This created 13 binary classification prob-
lems across four tasks. Each model was evaluated
on all problems using 10-fold cross validation and
parameter optimization. Experiments varied model
size to observe the effect of feature collisions on per-
formance.
Results for sentiment classification of kitchen ap-
pliance reviews (figure 1) are typical. The original
model has roughly 93.6k features and its alphabet
requires 1.3MB of storage. Assuming 4-byte float-
ing point numbers the weight vector needs under
0.37MB. Consequently our method reduces storage
by over 78% when we keep the number of param-
eters constant. A further reduction by a factor of 2
decreases accuracy by only 2%.
Figure 2 shows the results of all experiments
for SVM and MIRA. Each curve shows normalized
dataset performance relative to the full model as the
percentage of original features decrease. The shaded
rectangle extends one standard deviation above and
 1.02  1  0.98  0.96  0.94  0
 0.5 1
 1.5 2
Relative # fe
atures
 1.02  1  0.98  0.96  0.94  0
 0.5 1
 1.5 2
Relative # fe
atures
Figure 2: Relative performance on all datasets for SVM
(left) and MIRA (right).
 76 78 80 82 84 86 88  0 2 
4 6 8 1
0 12 14 16
thousands of
 featuresfeature mixi
ng
no feature m
ixing
 76 78 80 82 84 86 88  0 2 
4 6 8 1
0 12 14 16
thousands of
 featuresfeature mixi
ng
no feature m
ixing
Figure 3: The anomalous Reuters dataset from figure 2
for Perceptron (left) and MIRA (right).
below full model performance. Almost all datasets
perform within one standard deviation of the full
model when using feature mixing set to the total
number of features for the problem, indicating that
alphabet elimination is possible without hurting per-
formance. One dataset (Reuters retail distribution) is
a notable exception and is illustrated in detail in fig-
ure 3. We believe the small total number of features
used for this problem is the source of this behavior.
On the vast majority of datasets, our method can re-
duce the size of the weight vector and eliminate the
alphabet without any feature selection or changes to
the learning algorithm. When reducing weight vec-
tor size by a factor of 10, we still obtain between
96.7% and 97.4% of the performance of the original
model, depending on the learning algorithm. If we
eliminate the alphabet but keep the same size weight
vector, model the performance is between 99.3%
of the original for MIRA and a slight improvement
for Perceptron. The batch learning methods are be-
tween those two extremes at 99.4 and 99.5 for max-
imum entropy and SVM respectively. Feature mix-
ing yields substantial reductions in memory require-
ments with a minimal performance loss, a promising
result for resource constrained devices.
References
S. Bickel. 2006. Ecml-pkdd discovery challenge
overview. In The Discovery Challenge Workshop.
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies,
bollywood, boom-boxes and blenders: Domain adap-
tation for sentiment classification. In ACL.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Ressearch, 7.
D. D. Lewis, Y. Yand, T. Rose, and F. Li. 2004. Rcv1:
A new benchmark collection for text categorization re-
search. JMLR, 5:361?397.
20
Proceedings of the Workshop on BioNLP: Shared Task, pages 95?98,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Tunable Domain-Independent Event Extraction in the MIRA Framework
Georgi Georgiev1 Kuzman Ganchev1 Vassil Momtchev1
georgi.georgiev@ontotext.com kuzman.ganchev@ontotext.com vassil.momtchev@ontotext.com
Deyan Peychev1 Preslav Nakov1 Angus Roberts2
deyan.peychev@ontotext.com preslav.nakov@ontotext.com a.roberts@dcs.shef.ac.uk
1 Ontotext AD, 135 Tsarigradsko Chaussee, Sofia 1784, Bulgaria
2 The Department of Computer Science, Regent Court 211 Portobello, Sheffield, S1 4DP. UK.
Abstract
We describe the system of the PIKB team
for BioNLP?09 Shared Task 1, which targets
tunable domain-independent event extraction.
Our approach is based on a three-stage clas-
sification: (1) trigger word tagging, (2) sim-
ple event extraction, and (3) complex event
extraction. We use the MIRA framework for
all three stages, which allows us to trade pre-
cision for increased recall by appropriately
changing the loss function during training. We
report results for three systems focusing on re-
call (R = 28.88%), precision (P = 65.58%),
and F1-measure (F1 = 33.57%), respectively.
1 Introduction
Molecular interactions have been the focus of inten-
sive research in the development of in-silico biology.
Recent developments like the Pathway and Interac-
tion Knowledge Base (PIKB) aim to make available
to the user the large semantics of the existing molec-
ular interactions data using massive knowledge syn-
dication. PIKB is part of LinkedLifeData1, a plat-
form for semantic data integration based on RDF2
syndication and lightweight reasoning.
Our system is based on the MIRA framework
where, by appropriately changing the loss function
on training, we can achieve any desirable balance
between precision and recall. For example, low pre-
cision with high recall would be appropriate in a
search that aims to identify as many potential candi-
dates as possible to be further examined by the user,
1http://www.linkedlifedata.com
2http://www.w3.org/RDF/
while high precision might be essential when adding
relations to a knowledge base. Such a tunable sys-
tem is practical for a variety of important tasks, in-
cluding but not limited to, populating extracted facts
in PIKB and reasoning on top of new and old data.
Our system is based on a three-stage classification
process: (1) trigger word tagging using a linear se-
quence model, (2) simple event extraction, and (3)
complex event extraction. In stage (2), we generate
relations between a trigger word and one or more
proteins, while in stage (3), we look for complex in-
teractions between simple events, trigger words and
proteins. We use MIRA for all three stages with a
loss function tuned for high recall.
2 One-best MIRA and Loss Functions
In what follows, xi will denote a generic input sen-
tence, and yi will be the ?gold? labeling of xi. For
each pair of a sentence xi and a labeling y, we com-
pute a vector-valued feature representation f(xi, y).
Given a weight vector w, the dot-product w ? f(x, y)
ranks the possible labelings y of x; we will denote
the top scoring labeling as yw(x). As with hidden
Markov models (Rabiner, 1989), yw(x) can be com-
puted efficiently for suitable feature functions using
dynamic programming.
The learning portion of our method requires find-
ing a weight vector w that scores the correct labeling
of the training data higher than any incorrect label-
ing. We used a one-best version of MIRA (Cram-
mer, 2004; McDonald et al, 2005) to choose w.
MIRA is an online learning algorithm that updates
the weight vector w for each training sentence xi
according to the following rule:
95
wnew = argmin
w
?w ? wold?
s.t. w ? f(xi, yi) ? w ? f(x, y?) ? L(yi, y?)
where L(yi, y) is a measure of the loss of using y in-
stead of the correct labeling yi, and y? is a shorthand
for ywold(xi). In case of a single constraint, this pro-
gram has a closed-form solution. The most straight-
forward and the most commonly used loss function
is the Hamming loss, which sets the loss of labeling
y with respect to the gold labeling yi as the number
of training examples where the two labelings dis-
agree. Since Hamming loss is not flexible enough
for targeted training towards recall or precision, we
use a number of task-specific loss functions (see
Sections 3 and 5 for details). We implemented one-
best MIRA and the corresponding loss functions in
an in-house toolkit called Edlin. Edlin provides gen-
eral machine learning architecture for linear models
and a framework with implementations of popular
learning algorithms including Naive Bayes, percep-
tron, maximum entropy, one-best MIRA, and condi-
tional random fields (CRF) among others.
3 Trigger Word Tagging
The training and the development abstracts were
first tokenized and split into sentences using maxi-
mum entropy models trained on the Genia3 corpora.
Subsequently, we trained several sequence taggers
in order to identify the trigger words in text. All
our experiments used the standard BIO encoding
(Ramshaw and Marcus, 1995) with different feature
sets and learning procedures. We focused on recall
since it determines the upper bound on the perfor-
mance of our final system. In our experiments, we
found that simultaneously identifying trigger words
and the event types they trigger yielded low recall;
thus, we settled on identifying trigger words in text
as one kind of entity, regardless of event types.
In our initial experiments, we used a CRF-
based sequence tagger (Lafferty et al, 2001), which
yielded R=43.51%. We further tried feature induc-
tion (McCallum, 2003) and second-order Markov
assumptions for the CRF, achieving 44.72% and
49.64% recall, respectively.
3http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/wiki.cgi
Feature Set R P F1
Baseline (current word) 44.82 2.86 05.38
+ POS & char 3-gram 77.41 27.96 41.09
+ previous POS tag 79.77 29.32 42.88
+ lexicon (final tagger) 80.44 29.65 43.33
Table 1: Recall (R), precision (P), and F1-measure for the
trigger words tagger (in %s) on the development dataset
for different feature sets using MIRA training with false
negatives as a loss function.
Feature Sets
entity type of e1 and e2
words in e1 and e2
word bigrams in e1 and e2
POS of e1 and e2
words between e1 and e2
word bigrams between e1 and e2
POS between e1 and e2
distance between e1 and e2
distance between e1 and e2 in the dependency graph
steps in parse tree to get e1 and e2 in the same phrase
various combinations of the above features
Table 2: Our feature set for the MIRA classifier that pre-
dicts binary relations. Here e1 and e2 can be proteins
and/or trigger words.
Subsequently, we settled on using MIRA so that
we can trade-off precision for recall. In order to
boost recall, we defined the loss function as the num-
ber of false negative trigger chunks. Thus, a larger
loss update was made whenever the model failed to
discover a trigger word, while discovering spurious
trigger words was penalized less severely. We ex-
perimented with popular feature sets previously used
for named entity (McCallum and Li, 2003) and gene
(McDonald and Pereira, 2005) recognition including
orthographic, part-of-speech (POS), shallow parsing
and gazetteers. However, we found that only a small
number of them was really helpful; a summary is
presented in Table 1. In order to boost recall even
further, we prepared a gazetteer of trigger chunks
derived from the training data, and we extended it
with the corresponding WordNet synsets; we thus
achieved 80.44% recall for our final tagger.
4 Event Extraction
The input to our event extraction algorithm is a list
of trigger words and a list of genes or gene prod-
96
ucts (e.g., proteins); the output is a set of relations
as defined for Task 1. Our algorithm works in two
stages. First, we generate events corresponding to
relations between a trigger word and one or more
proteins (simple events); then we generate events for
relations between trigger words, proteins and simple
events (complex events). The two stages differ only
in the input data; thus, below we will describe our
system for the first stage only.
For each sentence, we considered all pairs of en-
tities (trigger words and proteins), and we used an
unstructured classifier to determine the relationship
for a given pair. These relationships encoded both
the type of event (e.g., binding, regulation) and enti-
ties? roles in that event (e.g., theme, cause); there
was also a special relationship for unrelated enti-
ties. We constructed labeled examples to train a
MIRA classifier using the training data provided by
the task organizers; n-ary relations were then recon-
structed from classifier?s predictions. The features
we used are summarized in Table 2: they are over
the words separating the two entities and their part-
of-speech tags. We further used some simple fea-
tures from syntactic phrases (OpenNLP4 parser) and
dependency parse trees (McDonald et al, 2005), ex-
tracted using parsers trained on Genia corpora.
After some initial experiments, we found that our
features were not sufficiently rich to allow us to learn
the relationships between proteins that are part of the
same event: we achieved a very low recall of about
20%. Consequently, we focused on the relationships
between a trigger word and a protein. Since the com-
petition stipulated that each trigger could be associ-
ated with only one type of event, we first chose the
event type for each trigger by selecting the protein-
label pair with the highest score. We then fixed the
event type for this trigger word, and we discarded all
proteins for which our classifier assigned a different
event type to the target trigger-protein pair. Finally,
we added to our output list all binary relations where
the role of the protein was theme.
For some event classes ? binding, regulation, pos-
itive regulation and negative regulation ? the output
of the binary classifier was further transformed so
that n-ary relations can be formed. However, the
way we did this was somewhat ad-hoc. For bind-
4http://opennlp.sourceforge.net
Event Class R P F1
Localization 10.92 82.61 19.29
Binding 7.20 39.68 12.20
Gene expression 30.47 74.58 43.26
Transcription 10.95 39.47 17.14
Protein catabolism 28.57 57.14 38.10
Phosphorylation 34.07 86.79 48.94
Event Total 21.52 68.68 32.77
Regulation 1.37 26.67 2.61
Positive regulation 1.12 25.58 2.14
Negative regulation 0.26 100.00 0.53
Regulation Total 0.97 27.12 1.87
Overall 10.84 64.13 18.55
Table 3: Our official results: for an erroneous submission.
ing events, we added a 3-ary relation between the
trigger, the highest scoring protein, and the second
highest scoring protein. For regulation events, we
added a 3-ary relation between the trigger and every
pair of proteins where one was a theme and the other
one was a cause. This aggressive addition of poten-
tial matches slightly reduced the overall precision,
but helped improve the recall for the final system.
5 Results and Discussion
Unfortunately, we made an error when making our
official submission, which resulted in low scores;
Table 3 shows the results for that submission.
The rest of this section describes the results and
the implementation for the system we intended to
submit. All reported results are for exact span
matches and were obtained using the online tool pro-
vided by the task organizers.
As stated in Section 4, we used a linear model
trained using one-best MIRA with ten runs over
the data for the event extraction system. We over-
sampled the unstructured training instances that cor-
responded to a relation so that they become roughly
equal in number to those that do not correspond to a
relation. Finally, we performed parameter averaging
as described in (Freund and Schapire, 1999). These
details turned out to be very important for the system
performance.
Table 4 shows the results for three different loss
functions that gave the best results in our experi-
ments. In describing the loss functions, we define
three different types of errors: (1) if the system cor-
rectly predicted that a relation should be present,
97
0-1 Loss High Recall High Precision
Event Class R P F1 R P F1 R P F1
Localization 33.33 69.05 44.96 39.08 48.23 43.17 25.86 86.54 39.82
Binding 38.33 32.60 35.23 46.97 24.51 32.21 24.50 37.95 29.77
Gene expression 57.89 65.72 61.56 64.82 53.49 58.61 47.65 76.27 58.65
Transcription 30.66 33.87 32.18 33.58 22.12 26.67 21.17 47.54 29.29
Protein catabolism 42.86 85.71 57.14 42.86 60.00 50.00 42.86 85.71 57.14
Phosphorylation 75.56 77.86 76.69 77.78 65.22 70.95 52.59 82.56 64.25
Event total 49.64 54.60 52.00 55.98 41.55 47.70 37.93 65.83 48.13
Regulation 0.00 0.00 0.00 2.41 22.58 4.35 0.00 0.00 0.00
Positive regulation 1.73 30.91 3.28 5.29 25.24 8.75 0.20 28.57 0.40
Negative regulation 0.53 40.00 1.04 1.06 23.53 2.02 0.26 100.00 0.53
Regulation Total 1.15 30.16 2.21 3.81 24.80 6.61 0.18 37.50 0.36
Overall 24.45 53.54 33.57 28.88 39.71 33.44 18.32 65.58 28.64
Table 4: Results (in %s) for one-best MIRA with different loss functions.
but guessed the wrong type, we call this a cross-
labeling; (2) a false positive occurs when the learner
guessed some relation while there should have been
none; (3) the reverse is a false negative. All loss
functions we considered had a cross-labeling loss of
1. The 0-1 loss also has a loss of 1 for false positives
and false negatives. The high-recall loss function
penalizes false positives with 0.1 and false negatives
with 5. The high-precision loss function penalizes
false negatives with 0.1 and false positives with 5.
The values 0.1 and 5 were chosen on the develop-
ment data, but were not optimized aggressively.
In conclusion, we have built three domain-
independent event extraction systems based on the
MIRA framework, each using a different loss func-
tion. Overall, they perform quite well and would
have been ranked second on precision5, and 6th on
recall, and 7th on F1-measure.
6 Future Work
After integrating domain knowledge, which should
improve the recall for complex events and should
boost the overall precision, we intend to transform
the system output into RDF and add it to the PIKB
repository. The required efforts discouraged us from
building a middle ontology between the BioNLP and
the PIKB data models, especially given the time lim-
itations for the present task competition. However,
we believe this is a promising direction, which we
plan to pursue in future work.
5Our official submission is second on precision as well.
Acknowledgments
The work reported in this paper was partially sup-
ported by the EU FP7 - 215535 LarKC.
References
Koby Crammer. 2004. Online Learning of Complex Cat-
egorial Problems. Ph.D. thesis, Hebrew University of
Jerusalem.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. In
Machine Learning, pages 277?296.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML. Morgan Kaufmann.
Andrew McCallum and Wei Li. 2003. Early results
for named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In Proceedings of CoNLL.
Andrew McCallum. 2003. Efficiently inducing features
of conditional random fields. In Proceedings of UAI.
Ryan McDonald and Fernando Pereira. 2005. Identify-
ing gene and protein mentions in text using conditional
random fields. BMC Bioinformatics, (Suppl 1):S6(6).
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL. ACL.
Lawrence Rabiner. 1989. A tutorial on hidden Markov
models and selected applications in speech recogni-
tion. Proceedings of the IEEE, 77(2).
Lance Ramshaw and Mitch Marcus. 1995. Text chunk-
ing using transformation-based learning. In David
Yarovsky and Kenneth Church, editors, Proceedings
of the Third Workshop on Very Large Corpora. ACL.
98
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1996?2006,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Cross-Lingual Discriminative Learning of Sequence Models
with Posterior Regularization
Kuzman Ganchev
Google Research
76 9th Avenue
New York, NY 10011
kuzman@google.com
Dipanjan Das
Google Research
76 9th Avenue
New York, NY 10011
dipanjand@google.com
Abstract
We present a framework for cross-lingual
transfer of sequence information from a
resource-rich source language to a resource-
impoverished target language that incorporates
soft constraints via posterior regularization. To
this end, we use automatically word aligned
bitext between the source and target language
pair, and learn a discriminative conditional ran-
dom field model on the target side. Our poste-
rior regularization constraints are derived from
simple intuitions about the task at hand and
from cross-lingual alignment information. We
show improvements over strong baselines for
two tasks: part-of-speech tagging and named-
entity segmentation.
1 Introduction
Supervised systems for NLP tasks are available for
a handful of languages. These systems achieve high
accuracy for many applications; a variety of robust
algorithms to train them from labeled data have been
developed. Here, we focus on learning sequence mod-
els for the languages that lack annotated resources.
For a given resource-poor target language of inter-
est, we assume that parallel data with a resource-rich
source language exists. With the help of this bitext
and a supervised system in the source language, we
infer constraints over the label distribution in the tar-
get language, and train a discriminative model using
posterior regularization (Ganchev et al, 2010).
Cross-lingual learning of structured prediction
models via parallel data has been applied for several
natural language processing problems, including part-
of-speech (POS) tagging (Yarowsky and Ngai, 2001),
syntactic parsing (Hwa et al, 2005) and named-entity
recognition (Kim et al, 2012). These methods are
useful in several ways. First, they help in fast proto-
typing of natural language systems for new languages
that do not boast human annotations. Second, the
output of such systems could be used to bootstrap
more extensive human annotation projects (Vlachos,
2006). Finally, they are significantly more accurate
than purely unsupervised systems (McDonald et al,
2011; Das and Petrov, 2011).
Recently, Ta?ckstro?m et al (2013) presented a tech-
nique for coupling token constraints derived from pro-
jected cross-lingual information and type constraints
derived from noisy tag dictionaries to learn POS tag-
gers. Although this technique resulted in state-of-
the-art weakly supervised taggers, the authors used a
heuristic to combine the aforementioned two sources
of constraints: the dictionary constraints pruned the
tagger?s search space, and the intersected token-level
projections were treated as hard observations. On
the other hand, Ganchev et al (2009) presented a
framework for learning weakly-supervised systems
(in their case, dependency parsers) that incorporated
alignment-based information too, but used the cross-
lingual information only as soft constraints, via poste-
rior regularization. The advantage of this framework
lay in the fact that the projections were only trusted
to a certain degree, determined by a strength hyper-
parameter, which unfortunately the authors did not
have an elegant way to tune. In this paper, we ex-
ploit the better aspects of these two lines of work:
first, we extend the framework of Ta?ckstro?m et al
by treating the alignment-based projections only as
soft constraints (see ?3.4); second, we choose the
constraint strength by utilizing the tag ambiguity of
tokens for a given resource-poor language (see ?6.1).
Other than validating our framework on part-of-
speech tagging, we experiment on named-entity seg-
mentation in a cross-lingual framework. For this
1996
task, we present a novel method to perform high-
precision phrase-level entity transfer (?5.2.2); we
also provide ways to balance precision and recall
with posterior regularization (?6.2) by incorporating
intuitive soft constraints during learning. We mea-
sure performance on standard benchmark datasets for
both of these tasks, and report improvements over
state-of-the-art baselines.
2 Prior Work
Cross-lingual projection methods can be classified
by their use of two very broad ideas. The first idea
utilizes parallel data to create full or partial annota-
tions in the low-resource language and trains from
this data. This was popularized by Yarowsky and
Ngai (2001) who applied this to POS tagging and
shallow parsing. It was later applied to parsing (Hwa
et al, 2005) and named entity recognition (Kim et
al., 2012). The second idea, first proposed by Ze-
man and Resnik (2008) and applied more broadly
by McDonald et al (2011), is to train a model on
a resource-rich language and apply it to a resource-
poor language directly. The disparity between the
languages is mitigated by the choice of features. In
addition to cross-lingual projection, purely unsuper-
vised methods have been explored but with limited
success (Christodoulopoulos et al, 2010). Here, we
resort to cross-lingual projection and incorporate the
first idea; we also follow Li et al (2012) and use
Wiktionary to further constrain the POS tagging task.
Our learning setup is similar to that of Ganchev et
al. (2009), who also use posterior regularization but
focus on dependency parsing alone. Our work differs
with respect to the tasks, the learning algorithm and
also in that we use corpus-wide constraints, while
Ganchev et al use one constraint per sentence. For
the part-of-speech tagging task, our approach is sim-
ilar to that of Ta?ckstro?m et al (2013), who use an
almost identical learning setup but only make use of
hard constraints. By relaxing these constraints, we
allow the model to identify and ignore inconsistently
labeled parts of sentences, and achieve better results
using identical training and test data.
3 Approach
We give an overview of our approach, and present the
details of our model used for cross-lingual learning.
Algorithm 1 Cross-Lingual Learning with Posterior
Regularization
Require: Parallel source and target language data
De and Df , source language model (M)e, task-
specific target language constraints C.
Ensure: ?f , a set of target language parameters.
1: De?f ? word-align-bitext(De,Df )
2: D?e ? label-supervised(De)
3: D?f ? project-and-filter-labels(De?f , D?e)
4: ?f ? learn-posterior-constrained(D?f , C)
5: Return ?f
3.1 General Overview
The general overview of our framework is provided
in Algorithm 1. The process of learning parame-
ters for a target language for a given task involves
four subtasks. First, we run word alignment over a
large corpus of parallel data between the resource-
rich source language and the resource-impoverished
target language (see ?4.3). In the second step, we
use a supervised model to label the source side of
the parallel data (see ?5.1.1 and ?5.2.1). The third
step involves a task-specific word-alignment filter-
ing step; this step involves heuristics for which we
use cues from prior state-of-the-art (Das and Petrov,
2011; Ta?ckstro?m et al, 2013, see ?5.1.2) and also
introduce some novel ones for the NE segmentation
problem (see ?5.2.2). In the fourth step, we train a
linear chain conditional random field (Lafferty et al,
2001, CRF henceforth) using posterior regularization.
In the next subsection, we turn to a brief summary of
this final step of estimating parameters of a discrimi-
native model with posterior regularization.
3.2 Learning with Posterior Regularization
In this work, we utilize discriminative CRF mod-
els, and use posterior regularization (PR) to optimize
their parameters. As a framework, posterior regular-
ization is described in detail by Ganchev et al (2010).
However in our work, we adopt a different optimiza-
tion technique; in what follows, we summarize the
optimization algorithm in the context of CRF models.
Let x be an input sentence with a set of possible
labelings Y(x) and let y ? Y(x) be a particular la-
beling for sentence x. We use bold capital letters
X = {x1 . . .xn} and Y = {y1 . . .yn} to denote
1997
a corpus of sentences and labelings for the corpus
respectively. A CRF models the probability distri-
bution over possible labels for a sentence p?(y|x)
as:
p?(y | x) ? exp(? ? f(x,y)) (1)
where ? are the model parameters and f(.) is a fea-
ture function. The model examines sentences in iso-
lation, and the probability of a particular labeling for
a corpus is defined as a product over the individual
sentences:
p?(Y | X) =
?
(x,y)?(X,Y)
p?(y | x). (2)
Traditionally, CRF models have been trained to op-
timize the regularized log-likelihood of the training
data
max
?
L(?) = max
?
log(p?(Y | X))? ? ||?|| (3)
In our setting, we do not have a fully labeled cor-
pus, but we have constraints on the distribution of
labels. For example, we may know that a particular
token could be labeled only by a label inventory li-
censed by a dictionary, or that a labeling projected
from a source language is usually (but not always)
correct. We define these constraints in terms of fea-
ture expectations. Let q(Y) be a distribution over all
possible labelings of our corpus Y(X). Let Q be a
set of distributions defined by:
Q = {q(Y) : Eq[?(X,Y)] ? b}, (4)
where ? is a constraint feature function and b is a vec-
tor of non-negative values that serve as upper bounds
to the expectations of every constraint feature. The
vector b is used to encode our prior knowledge about
desirable distributions q(Y). Note that the constraint
features ? are not related to the model features f . The
model features, together with the model parameters ?
define the CRF model; the model features need to be
computed at inference time for prediction. By con-
trast, the constraint features and their corresponding
constraint values are used to define our training ob-
jective function (and are only used during learning).
The PR objective with no labeled data is defined with
respect to Q as:
PR: max
?
JQ(?) =
max
?
?KL(Q?p?(Y | X))? ? ||?|| (5)
where KL(Q||p) = minq?QKL(q||p) is the
KL-divergence (Kullback and Leibler, 1951)
from a set to a point. Note that as we add more
constraints, Q becomes a smaller set. In the
limit, Q = {q(Y) : q(Y?) = 1} contains just one
distribution concentrated on a single labeling Y?.
In this limit, posterior regularization degenerates
into the convex log-likelihood objective normally
used for supervised data JQ(?) = L(?). However,
in the general case, the PR objective JQ is not
necessarily convex. Prior work, including that of
Ganchev et al propose an algorithm similar to
Expectation-Maximization (Dempster et al, 1977,
EM henceforth) to optimize JQ, but we follow Liang
et al (2009) in using a schochastic update-based
algorithm described below.
Note: To make it easier to reason about constraint
values b, we scale constraint features ?(X,Y) to lie
in [0, 1] by computing maxY ?(X,Y) for the corpus
to which ? is applied.
3.3 Optimization
The optimization procedure proposed by
Ganchev et al is similar to the EM algorithm,
and computes the minimization minq?QKL(q||p)
at each step, using its dual form; this minimization is
convex, so there is no duality gap. They show that
the optimal primal variables q?(Y) are related to the
optimal dual variables ?? by:
q?(Y) =
p?(Y|X)e??
???(X,Y)
Z(??)
. (6)
where Z(??) is the normalizer. The dual problem is
given by:
max
??0
?b ? ?? logZ(?). (7)
Substituting Eq. 7 into the objective in Eq. 5, we get
the saddle-point problem:
max
?
min
??0
b ? ?+ log
?
Y
p?(Y|X)e
?????(X,Y)
? ? ||?|| . (8)
To optimize the above objective function, we need to
compute partial derivatives with respect to both ? and
?. First, to compute the partial derivatives of Eq. 8
1998
with respect to ?, we need to find expectations of the
model features f given the current distribution p? and
the constraint distribution q. To perform tractable
inference, a linear-chain CRF model assumes that
the feature function factorizes according to smaller
parts; in particular the factorization uses the follow-
ing structure:
f(x,y) =
?
i
f(x, yi, yi?1) (9)
where i ranges over the tokens in the sentence. This
factorization allows us to efficiently compute expec-
tations over the labels yi and label-pairs (yi, yi+1).
To compute the partial gradient of Eq. 8 with respect
to ?, we need to find the expectations of the con-
straint features ?. In order to be tractable here too,
we ensure that ? also factorize according to the same
structure as f . Therefore, the gradient computation
w.r.t. ? turns out to be straightforward.
For all the experiments in this paper, we optimize
Eq. 8 using stochastic projected gradient. For each
training sentence, we compute the gradient of ? and
? with respect to Eq. 8, take a gradient step in each
one, and truncate the negative entries in ? to zero.
We use a step size of 1 for all experiments.1
3.4 Relationship with Ta?ckstro?m et al (2013)
In this subsection, we focus briefly on the relationship
between this work and the work of Ta?ckstro?m et al
(2013), who focused on constrained learning of POS
taggers. Ta?ckstro?m et al define constrained lattices
and train by optimizing marginal conditional log-
likelihood. In our notation, they define their objective
as:
max
?
log
?
Y?Y?(X)
p?(Y|X)? ???? (10)
where Y?(X) are the constrained lattices of label se-
quences that agree with both a dictionary and cross-
lingually projected POS tags for each sentence of
the training corpus. Let us define a constraint fea-
ture ?(X,Y) which counts the number of tags in Y
which are outside the constraint set Y?(X) and require
?(X,Y) ? 0. Note that,
arg min
q
KL(q||p?(Y|X)) s. t. ?(X,Y) ? 0
1Note that we did not implement regularization of ? in the
stochastic optimizer, hence our PR objective (Eq. 8) was unregu-
larized; however, the baseline models use `2 regularization.
gives the same distribution as Eq. 10. Given this
equivalence, it is easy to see that the gradient of
Eq. 5 with respect to ? is the same as that of Eq. 10.
By using such constrained lattices, Ta?ckstro?m et al
avoid maintaining a parameter for the constraint, but
lose the ability to relax the constraint value and al-
low some probability mass outside the pruned lat-
tice. Their paper also differs from ours in that they
use L-BFGS (Liu and Nocedal, 1989), while we use
an online optimization procedure. Since the objec-
tives are non-convex, the two optimization techniques
could lead to different local optima even when the
constraint is not relaxed (b = 0).
4 Tasks and Data
In this section, we focus on the nature of the two tasks
that we attempt to solve, describe the source language
datasets we use to train our supervised models for
transfer, the target language datasets on which we
evaluate our models and the parallel data we use for
cross-lingual transfer.
4.1 Part-of-Speech Tagging
First, we focus on the task of part-of-speech tagging.
Following previous work on cross-lingual POS tag-
ging (Das and Petrov, 2011; Ta?ckstro?m et al, 2013),
we adopt the POS tags of Petrov et al (2012), ver-
sion 1.03;2 we use the October 2012 version of Wik-
tionary3 as our tag dictionary.
After pruning the search space with the dictionary,
we place soft constraints derived by projecting POS
tags across word alignments. The alignments are fil-
tered for confidence (see ?5.1.2), but we also filter
any projected tags that are not licensed by the dictio-
nary. The example in Figure 1 illustrates why this
dictionary filtering step is important. Consider the
English-Spanish phrase pair from Figure 1, which
we observed in our training data. Our supervised tag-
ger correctly tags Asian with the ADJ tag as shown
in the figure. Asian is aligned to the Spanish word
Asia, which should be tagged NOUN. Because the
Spanish Wiktionary only allows the NOUN tag for
Asia, we do not project the ADJ tag from the English
word Asian. By contrast, we do project the NOUN
tag from the English word sponges to the Spanish
2http://code.google.com/p/universal-pos-tags
3http://meta.wikimedia.org/wiki/Wiktionary
1999
of    [ Asian ]        sponges
de   las   esponjas   de   Asia
ADP
ADJ NOUN
MISC
Figure 1: An English (top) ? Spanish (bottom) phrase pair
from our parallel data. The correct POS tags and NER
annotations are shown for the English phrase. Word align-
ments are shown as links between English and Spanish
words.
word esponjas because this tag is in our dictionary
for the latter word.
For all our POS experiments, we evaluate on sev-
enteen target languages. Fifteen of these languages
were part of the experiments conducted by Ta?ckstro?m
et al (2013); we add Arabic and Hungarian to the set.
The first column of Table 1 lists all seventeen lan-
guages using their two-letter abbreviation codes from
the ISO 639-1 standard. The evaluation datasets cor-
respond to the test sets from the CoNLL shared tasks
on dependency parsing (Buchholz and Marsi, 2006;
Nivre et al, 2007). For French we use the treebank
of Abeille? et al (2003). English serves as our source
language and we use the Penn Treebank (Marcus et
al., 1993, with tags mapped to the universal tags) to
train our supervised source-side model.
4.2 Named-Entity Segmentation
Second, we investigate the task of named-entity seg-
mentation. The goal of this task is to identify the
boundaries of named-entities for a given language
without classifying them by type. This is the un-
labeled version of named-entity recognition, and is
more amenable to cross-lingual supervision. To un-
derstand why that is, consider again the example
from Figure 1. The English supervised NE tagger
correctly identifies Asian as a named entity of type
MISC (miscellaneous). The word-alignments sug-
gest we should transfer this annotation to the Spanish
word Asia which is also an entity. However, this
should be labeled LOC (location) according to the
CoNLL annotation guidelines (Tjong Kim Sang and
De Meulder, 2003). Because syntactic variations
of this kind are common, it makes cross-lingual de-
tection of NE boundaries as well as types hard.4 In
this paper, we focus on named-entity segmentation
alone, consider the full NER task out of scope. We
use English as a source language and train a super-
vised English named-entity tagger with the labels in
place, using the CoNLL 2003 shared task data (Tjong
Kim Sang and De Meulder, 2003). We project the
spans using the maximal-span heuristic (Yarowsky
and Ngai, 2001). We project into Dutch, German and
Spanish and evaluate on the standard CoNLL 2002
and 2003 shared task data sets (Tjong Kim Sang,
2002; Tjong Kim Sang and De Meulder, 2003).
4.3 Parallel Data
For both tasks we use parallel data gathered automat-
ically from the web using the method of Uszkoreit
et al (2010), as well as data from Europarl (Koehn,
2005) and the UN parallel corpus (UN, 2006), for
languages covered by the latter two corpora. The
parallel sentences are word aligned with the aligner
of DeNero and Macherey (2011). The size of the
parallel corpus is larger than we need for our tasks,
so we follow Ta?ckstro?m et al (2013) in sampling
500k tokens for POS tagging and 10k sentences for
named-entity segmentation (see ?5.1.2 and ?5.2.2).
5 Experimental Details
In this section, we provide details about task-
specific implementations of the supervised source-
side model and the word-alignment filtering tech-
niques (steps 2 and 3 in Algorithm 1 respectively);
we also briefly describe the setup of the cross-lingual
experiments for each task.
5.1 Part-of-Speech Tagging
We first focus on the experimental setup for the POS
tagging task. When describing feature sets we refer to
features conjoined with just a single tag as emission
features and with consecutive tag pairs as transition
features.
4We tried using English and German gazetteers from the
CoNLL 2002 and 2003 shared tasks as a label dictionary similar
to the way we use Wiktionary for POS tagging. This did not work
well because the CoNLL gazetteers do not have good coverage
on our parallel datasets, which we use for training.
2000
5.1.1 Supervised Source-Side Model
We tag the English side of our parallel data with
a supervised first-order linear-chain CRF POS tag-
ger. We use standard features for tagging. Our emis-
sion features are a bias feature, the current word,
its suffixes up to length 3, its capitalization shape,
whether it contains a hyphen, digit or punctuation
and its cluster identity. Our transition features are a
bias feature and the cluster identities of each word
in the transition. For the cluster-based features, we
use monolingual word clusters induced with the ex-
change algorithm of Uszkoreit and Brants (2008),
which implements the same objective as Brown et al
(1992); these clusters have shown improvements for
sequence labeling tasks (Turian et al, 2010; Ta?ck-
stro?m et al, 2012). We set the number of clusters to
256 for both the source side tagger and all the other
languages. On Section 23 of the WSJ section of the
Penn Treebank, the source side tagger achieves an
accuracy of 96.2%.
5.1.2 Word Alignment Filtering
Following Ta?ckstro?m et al (2013), we tag the En-
glish side of our parallel data using the source-side
POS tagger, intersect the word alignments and filter
alignments with confidence below 0.95. We sam-
ple 500,000 tokens of target side sentences for each
language, and use this as training data for learning
weakly-supervised taggers.
5.1.3 Setup for Cross-Lingual Experiments
Following Ta?ckstro?m et al (2013) we use a re-
duced feature set for the cross-lingual models. The
emission features are the same as the supervised
model but without the punctuation feature,5 and we
use only the bias transition feature. Because this
limits the ability of the model to use context, we
also experiment with an extended feature set that
has transition features for the clusters of each word
in the transition, and their suffixes up to length 3.
We refer to the extended-feature models as ?BASE+?
and ?PR+? to distinguish them from the models with
fewer features, labeled ?BASE? and ?PR?.
We train BASE and BASE+ using L-BFGS with
an `2 regularization weight of 1 for 100 iterations to
reproduce the setup used by Ta?ckstro?m et al (2013).
5The dictionary licenses punctuations, only by the ?.? tag.
We have only one constraint feature in our poste-
rior regularization models that fires for the unpruned
projected tags on words xi. This feature controls
how often our model trusts a projected tag; we ex-
plain how its strength is chosen in ?6.1. The PR and
PR+ models are trained using the stochastic gradient
method described in ?3.3.
5.2 Named-Entity Segmentation
In this subsection, we turn to the experimental details
of the named-entity segmentation system.
5.2.1 Supervised Source-Side Model
To train our supervised source-side NER model,
we implemented a linear-chain first order CRF model.
Our feature set was inspired by the model of Kazama
and Torisawa (2007, ?6.1); we used all the local fea-
tures from their model except the gazetteer features,
and added cluster emission features for offsets in the
range [-2, 2] and transition features for offsets in the
range [-1, 1] as well as a sentence-start feature. We
use automatic POS tags for all the experiments.
We use a BIO encoding of the four NER labels
(PER, LOC, ORG and MISC). We also experi-
mented with omitting the NE labels from the tagger,
still with a BIO encoding for segments, but the results
were worse on average than what we report in Table 2.
We train the source-side model on the CoNLL 2003
English training set with log-loss using L-BFGS for
100 iterations with `2 regularization weight of 0.1.
The model gets 90.9% and 87.5% labeled F1 on the
CoNLL development and test sets respectively.6
5.2.2 Word-Alignment Filtering
Projecting named entities across languages can
be error prone for several reasons. Mistakes intro-
duced by the automatic word aligner is one of them.
Word alignment errors are particularly problematic
for entity mentions because of the garbage collector
effect (Brown et al, 1993); due to differences in the
word order between languages, a few alignment er-
rors can result in many errors in the other language.
Additionally, entities can occur on just one side of
the bitext.7 Another source of error is the automatic
6These performance values would place us among the top
three competitors of the CoNLL 2003 shared task.
7For example, ?It?s all Greek to me.? in one language and ?I
don?t understand it.? in another.
2001
labeling on the source side, which is inaccurate if the
parallel corpus is out of domain. To mitigate these
errors, we aggressively filter the training data for this
task. We discard sentence pairs where more than
30% of the source language tokens are unaligned,
where any source entities are unaligned or where
any source entities are more than 4 tokens long. We
also compute a confidence score over entity anno-
tations as the minimum posterior over the tags that
comprise the entity and discard sentence pairs that
have an entity with confidence below 0.9. Finally,
we discard any sentences that contain no projected
entities. These filtering steps allow us to keep 7.4%,
9.7% and 10.4% of the aligned sentence pairs for Ger-
man, Spanish and Dutch, respectively, resulting in
very high-precision named-entity projections (see Ta-
ble 2). For comparison, we also perform experiments
without this filtering step.
5.2.3 Setup for Cross-Lingual Experiments
We use a CRF with the same feature set and BIO
encoding for the cross-lingual models as the source-
side NER model. We compare our approach (?PR?
in Table 2) to a baseline (?BASE? in Table 2) which
treats the projected annotations as fully observed.
The PR model treats the projected NE spans of a
sentence as observed, and allows all labels on the
remaining tokens. Since the ?O? tag is never seen, an
unconstrained model would learn to never predict it.
We add two features that fire when the current word
is tagged ?O?: a bias feature and a feature that fires
when the automatic POS tag is a proper noun. We set
upQ so the desired expectations are at least 0.98 and
at most 0.1 for these constraint features respectively.
6 Results
In this section, we turn to our experimental results;
first, we focus on POS tagging and then turn to the
NE segmentation task.
6.1 Part-of-Speech Tagging
Constraint Strength: As discussed in ?4.1, it is
important to filter out projected annotations not li-
censed by Wiktionary. Thus, the quality of weakly-
supervised POS taggers learned from projections
is closely correlated with the coverage of the Wik-
tionary. To quantify the effect of Wiktionary cover-
age, we counted the expected number of possible tags
 0.7
 0.8
 0.9
 1
 0.1  0.2  0.3  0.4
optim
al b
1/TpT
Figure 2: Correlation between optimal constraint value b
and dictionary pruning efficiency. Each blue square is a
language, the green line is a linear approximation of the
data.
per token (TpT) for our unlabeled corpora. Specif-
ically, for each token, we counted the number of
tags licensed by the dictionary, or all tags for word
forms not in the dictionary. For each language, we
also ran our system with constraint strengths in {0.7,
0.75, 0.8, 0.85, 0.9, 0.92, 0.95, 0.98, 1.00}, and com-
puted the optimal constraint strength from this set.
We found that the best constraint strength is closely
correlated with the average number of tags available
for each token. Figure 2 shows the best constraint
strength as a function of the inverse of the number of
unpruned tags per token. As observed in the figure,
the relationship between the optimal strength and
1/TpT is roughly linear. Figure 2 also shows a linear
approximation to the data plotted. When applying
this technique to a new language, we would not be
able to estimate the optimal constraint strength, but
we could use the linear approximation and knowl-
edge of 1/TpT to estimate it. For our experiments
below, we perform this estimation for each language
using the linear approximation computed from the
remaining languages.
Results: The results for our part-of-speech tagging
experiments are in Table 1. We compare our results
to BASE, which corresponds to reruns of the best
model of Ta?ckstro?m et al (2013, Column 9 of Ta-
ble 2), and closely aligns with the numbers reported
by the authors. We see in Table 1 that for both fea-
ture sets (i.e., with and without the ?+? extension),
our estimated constraint strength is usually better
than using a constraint strength of 1. The results
in the column labeled PR are better than BASE for
12 out of 17 languages, and the results for PR+ are
2002
BASE BASE+ PR PR+
ar 37.84 44.96 ? 49.04? 50.10?
bg 88.04 87.93 88.02 88.42?
cs 79.67 80.01 ? 80.20? 80.68?
da 88.14 87.92 88.24? 87.90
de 90.32 89.97 90.41? 90.29
el 90.03 89.03 90.63? 90.24?
es 86.99 86.81 87.20? 87.21?
fr 87.07 87.53 ? 87.44? 87.48?
hu 82.05 82.05 82.14? 83.13?
it 89.48 89.89 ? 89.52 89.72?
ja 80.63 78.54 80.02 79.68
nl 85.89 85.77 85.59 85.98?
pt 90.93 91.60 ? 91.48? 91.56?
sl 82.46 82.08 83.16? 83.49?
sv 89.06 88.72 89.25? 88.77
tr 64.39 65.74 ? 63.88 66.47?
zh 73.98 72.82 74.51? 68.43
Avg 81.59 81.85 ? 82.40? 82.33?
-zh-ar 85.01 84.91 85.15? 85.40?
Table 1: POS tagging results. BASE represents the best
model of Ta?ckstro?m et al (2013). PR is a system with
the same features but with relaxed constraints. BASE+
and PR+ add additional model features (see ?5.2.3). ? in-
dicates improvements over the previous state of the art
(BASE), and bold values indicate the best score for a lan-
guage. ?Avg? indicates averaged results for all 17 lan-
guages, while ?-zh-ar? shows averaged results without
Chinese and Arabic.
better than BASE+ for 13 out of 17 languages. Ad-
ditionally, adding features does not tend to help the
baseline model to a large extent (the wins are for
6 languages), but does tend to help the PR model
(for 11 languages); however, there is a large drop in
performance for Chinese.
Error Analysis: Here, we analyze the nature of
improvements that the PR models get. For the lan-
guages where PR results in large improvements, it
stems from the ability to allow the sentential con-
text to sometimes override the tag projected via the
parallel data. For example, the Czech word se can
either be a reflexive pronoun (such as ourselves in
English) or translate to the preposition with. The
pronominal sense comprises about 95% of occur-
rences in the Czech annotations, but it would not
appear in an English translation. For example, the
phrase ?pod??vali jsme se? translates to ?we looked?,
and the word jsme would typically be aligned to we;
se, which serves as a reflexive pronoun here, remains
unaligned. Consequently, in our data, over 7000 oc-
currences of se appear, but only 17 instances have a
tag projection that is not filtered by Wiktionary. Since
the remaining are tagged with the preposition tag, the
hard-constrained baseline always tags se as a prepo-
sition. By contrast, the soft-constrained PR model
predicts the pronominal sense in cases where the con-
text is most indicative of a pronoun ? 38% of the time.
It still mistags many of the pronominal cases where
the contextual evidence is not strong enough. We get
very similar behavior with the Hungarian word hogy
which can translate to the conjunction that (as in ?I
see that you are here?) or the adverb how.
We found that the drastic drop in performance for
Chinese under the PR+ model is due to the possessive
marker ??? which serves exclusively as a particle
in the test data. Wiktionary also allows the noun and
adverb tags. The adverbial use is actually a different
token (?? ? really, truly) containing the same
character. Because the cross-lingual training data is
based on machine-learned alignments, 99.4% of the
training examples of? have no annotations, and only
0.6% have the particle annotation projected from the
English ?s possessive marker. If we remove the noun
and adverb senses from the Wiktionary performance
of PR+ improves to 72.87%. Alternatively, we could
add another constraint to prefer closed-class words
over open-class words when both are licensed by the
dictionary. When we add such a constraint to Chinese
with a constraint value of 0.95, we recover most of the
loss (68.43? 72.94); however, we do not report this
specific change to the Chinese experimental setup in
Table 1 to maintain generality.
6.2 Named-Entity Segmentation
Results: Table 2 shows the results for the named en-
tity segmentation experiments. First, we observe that
the word alignment filtering step (?5.2.2) improves
results for all three languages by significant margins,
for both the BASE and PR models. Both with and
without filtering, we observe that the baseline mod-
els are very strongly biased towards precision. The
filtering step tends to help with recall more than pre-
cison for both models. By having a soft constraint via
PR and allowing some segmentations to fall outside
of the transferred one, we get an increase in recall,
2003
No Filtering Filtering (?5.2.2)
Lang Metric BASE PR BASE PR
Prec 74.29 73.85 75.36 76.47
de Recall 41.69 54.50 54.71 64.61
F1 53.41 62.71 63.39 70.04
Prec 74.53 62.10 82.50 70.22
es Recall 56.39 78.33 67.27 81.10
F1 64.20 69.28 74.11 75.27
Prec 81.90 75.12 86.39 76.09
nl Recall 50.54 76.11 65.45 79.11
F1 62.51 75.61 74.47 77.57
Above: dev, below: test
Prec 73.23 71.67 69.90 70.94
de Recall 39.70 51.81 52.52 61.42
F1 51.49 60.14 59.97 65.84
Prec 75.38 65.40 83.50 73.68
es Recall 56.00 80.30 67.55 83.31
F1 64.26 72.09 74.68 78.20
Prec 79.45 73.55 86.01 77.05
nl Recall 47.45 75.37 65.16 80.11
F1 59.42 74.45 74.14 78.55
Table 2: Result for the named-entity segmentation exper-
iments. The highest score in each category is shown in
bold. Note that ?No Filtering? still discards sentences with
no projected entities.
and in turn an improved F1 score. On average the
PR model improves F-score by 3.6% on the develop-
ment set and 4.6% on the test set over the baseline
(when filtering is used). Note that because we focus
on named entity segmentation, our results are not
directly comparable to those of Ta?ckstro?m (2012),
who train a de-lexicalized named entity recognizer
on one language and apply it to other languages.
Error Analysis: In order to get a sense for the types
of errors made by the baseline which are corrected
by the PR model, we collected statistics about the
most frequent errors in the segments extracted by
the baseline and by our model. We divided the er-
rors into missing segments, extraneous segments and
overlapping segments.
From Table 2, it is clear that the most common
errors for the baseline models are missing entities.
From our analysis of the CoNLL development data,
we found that the entities that occur with little context
(such as the location and publisher of an item) at the
onset of news articles are most frequently missed. For
German, dpa (Deutsche Presse-Agentur) and Reuter
are the two most common missing segmentations;
the Spanish counterparts are Gobierno (Government)
and Barcelona, while for Dutch they are De Morgen
and Brussel. While filtering parallel sentences and
using a soft constraint both increase recall, even our
strongest model does not get enough information to
predict these entities, and they continue to be major
sources of error. By contrast, the names mentioned
in context are the ones that are most frequently added
to the analysis when PR is used. In a sense this
is desirable, since a machine-learned named-entity
segmentation system is most useful for the long tail
of entity mentions.
If we filter the training data and use the PR model
to further increase recall, precision errors tend to
become relatively more frequent (this trend is ob-
servable in Table 2). For German, the most frequent
precision error is Mark referring to the Deutsche
Mark. For Spanish, the most frequent precision er-
rors are due to boundary errors. The Spanish an-
notation guidelines include enclosing quotes as part
of the entity name, and failing to include them ac-
counts for just under 1% of the precision errors of
the PR system that uses filtering. The second most
frequent error is failing to segment Inter de Mila?n.
The model segments out either Inter or Mila?n or both
by themselves depending on context.
7 Conclusions
In this paper, we presented a framework for cross-
lingual transfer of sequence information from a
resource-rich source language to a resource-poor tar-
get language. Our framework incorporates soft con-
straints while training with projected information
via posterior regularization. We presented the effi-
cacy of our framework on two very useful natural
language tasks: POS tagging and named-entity seg-
mentation. The soft constraints used in our work
model intuitions about a given task. For the POS
tagging problem, we designed constraints that also
incorporate projected token-level information, and
presented a principled method for choosing the extent
to which this information should be trusted within
the PR framework. This approach generalizes the
state of the art in cross-lingual projection work in
the context of POS tagging, and improves upon it.
2004
Across seventeen languages, our models outperform
the previous state of the art by an average of 0.8%
(greater than 4% error reduction), and outperforms
it on twelve out of seventeen languages. For named-
entity segmentation, our model results in 3.6% and
4.6% absolute improvements in F1-score on our de-
velopment and test sets respectively, when averaged
across three languages.
Acknowledgments
We would like to thank Ryan McDonald, Fernando
Pereira, Slav Petrov and Oscar Ta?ckstro?m for numer-
ous discussions on this topic and providing detailed
feedback on early drafts of this paper. We are also
grateful to the four anonymous reviewers for their
valuable comments.
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel.
2003. Building a Treebank for French. In A. Abeille?,
editor, Treebanks: Building and Using Parsed Corpora,
chapter 10. Kluwer.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18:467?479.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, Meredith J. Goldsmith, Jan Hajic,
Robert L. Mercer, and Surya Mohanty. 1993. But
dictionaries are data too. In Proceedings of the Work-
shop on Human Language Technology.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
POS induction: How far have we come? In Proceed-
ings of EMNLP.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of ACL-HLT.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the em algorithm. Journal of the Royal Statistical
Society, Series B, 39(1):1?38.
John DeNero and Klaus Macherey. 2011. Model-based
aligner combination using dual decomposition. In Pro-
ceedings of ACL-HLT.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency grammar induction via bitext pro-
jection constraints. In Proceedings of ACL-IJCNLP.
Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. Journal of Machine Learn-
ing Research, 11:2001?2049.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(03):311?325.
Jun?ichi Kazama and Kentaro Torisawa. 2007. A new
perceptron algorithm for sequence labeling with non-
local features. In Proceedings of EMNLP.
Sungchul Kim, Kristina Toutanova, and Hwanjo Yu. 2012.
Multilingual named entity recognition using parallel
data and metadata from wikipedia. In Proceedings of
ACL.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT
Summit.
Solomon Kullback and Richard A. Leibler. 1951. On
information and sufficiency. Annals of Mathematical
Statistics, 22:49?86.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
Proceedings of ICML.
Shen Li, Joa?o Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings of
EMNLP-CoNLL.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning from measurements in exponential families.
In Proceedings of ICML.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45:503?528.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated corpus
of English: the Penn treebank. Computational Linguis-
tics, 19(2):313?330.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of EMNLP-CoNLL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC.
Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszkoreit.
2012. Cross-lingual word clusters for direct transfer of
linguistic structure. In Proceedings of NAACL-HLT.
2005
Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan Mc-
Donald, and Joakim Nivre. 2013. Token and type con-
straints for cross-lingual part-of-speech tagging. Trans-
actions of the Association for Computational Linguis-
tics, 1:1?12.
Oscar Ta?ckstro?m. 2012. Nudging the envelope of direct
transfer methods for multilingual named entity recogni-
tion. In Proceedings of the NAACL-HLT Workshop on
the Induction of Linguistic Structure.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the CoNLL-2003 shared task: language-
independent named entity recognition. In Proceedings
of CoNLL.
Erik F. Tjong Kim Sang. 2002. Introduction to
the CoNLL-2002 shared task: Language-independent
named entity recognition. In Proceedings of CoNLL.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceedings
of ACL.
UN. 2006. ODS UN parallel corpus.
Jakob Uszkoreit and Thorsten Brants. 2008. Distributed
word clustering for large scale class-based language
modeling in machine translation. In Proceedings of
ACL-HLT.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe
Dubiner. 2010. Large scale parallel document mining
for machine translation. In Proceedings of COLING.
Andreas Vlachos. 2006. Active annotation. Proceedings
of EACL.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
NAACL.
Daniel Zeman and Philip Resnik. 2008. Cross-language
parser adaptation between related languages. In Pro-
ceedings of IJCNLP Workshop: NLP for Less Privi-
leged Languages.
2006
Learning Tractable Word Alignment Models
with Complex Constraints
Joa?o V. Grac?a?
L2F INESC-ID
Kuzman Ganchev??
University of Pennsylvania
Ben Taskar?
University of Pennsylvania
Word-level alignment of bilingual text is a critical resource for a growing variety of tasks. Proba-
bilistic models for word alignment present a fundamental trade-off between richness of captured
constraints and correlations versus efficiency and tractability of inference. In this article, we
use the Posterior Regularization framework (Grac?a, Ganchev, and Taskar 2007) to incorporate
complex constraints into probabilistic models during learning without changing the efficiency
of the underlying model. We focus on the simple and tractable hidden Markov model, and
present an efficient learning algorithm for incorporating approximate bijectivity and symmetry
constraints. Models estimated with these constraints produce a significant boost in performance
as measured by both precision and recall of manually annotated alignments for six language
pairs. We also report experiments on two different tasks where word alignments are required:
phrase-based machine translation and syntax transfer, and show promising improvements over
standard methods.
1. Introduction
The seminal work of Brown et al (1993b) introduced a series of probabilistic models
(IBM Models 1?5) for statistical machine translation and the concept of ?word-by-
word? alignment, the correspondence between words in source and target languages.
Although no longer competitive as end-to-end translation models, the IBM Models,
as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996),
are still widely used for word alignment. Word alignments are used primarily for
extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn,
Och, and Marcu 2003] and rules [Galley et al 2004; Chiang et al 2005]) as well as for
? INESC-ID Lisboa, Spoken Language Systems Lab, R. Alves Redol 9, 1000-029 LISBOA, Portugal.
E-mail: joao.graca@l2f.inesc-id.pt.
?? University of Pennsylvania, Department of Computer and Information Science, Levine Hall, 3330 Walnut
Street, Philadelphia, PA 19104-6309. E-mail: kuzman@cis.upenn.edu.
? University of Pennsylvania, Department of Computer and Information Science, 3330 Walnut Street,
Philadelphia, PA 19104-6389. E-mail: taskar@cis.upenn.edu.
Submission received: 1 August 2009; revised submission received: 24 December 2009; accepted for
publication: 10 March 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
MT system combination (Matusov, Ueffing, and Ney 2006). But their importance has
grown far beyond machine translation: for instance, transferring annotations between
languages (Yarowsky and Ngai 2001; Hwa et al 2005; Ganchev, Gillenwater, and
Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint
unsupervised POS and parser induction across languages (Snyder and Barzilay 2008).
IBM Models 1 and 2 and the HMM are simple and tractable probabilistic models,
which produce the target sentence one target word at a time by choosing a source word
and generating its translation. IBM Models 3, 4, and 5 attempt to capture fertility (the
tendency of each source word to generate several target words), resulting in probabilis-
tically deficient, intractable models that require local heuristic search and are difficult to
implement and extend. Many researchers use the GIZA++ software package (Och and
Ney 2003) as a black box, selecting IBM Model 4 as a compromise between alignment
quality and efficiency. All of the models are asymmetric (switching target and source
languages produces drastically different results) and the simpler models (IBM Models 1,
2, and HMM) do not enforce bijectivity (the majority of words translating as a single
word). Although there are systematic translation phenomena where one cannot hope to
obtain 1-to-1 alignments, we observe that in over 6 different European language pairs
the majority of alignments are in fact 1-to-1 (86?98%). This leads to the common practice
of post-processing heuristics for intersecting directional alignments to produce nearly
bijective and symmetric results (Koehn, Och, and Marcu 2003).
In this article we focus on the HMM word alignment model (Vogel, Ney, and
Tillmann 1996), using a novel unsupervised learning framework that significantly
boosts its performance. The new training framework, called Posterior Regulariza-
tion (Grac?a, Ganchev, and Taskar 2007), incorporates prior knowledge in the form of
constraints on the model?s posteriors. The constraints are expressed as inequalities on
the expected value under the posterior distribution of user-defined features. Although
the base model remains unchanged, learning guides the model to satisfy these con-
straints. We propose two such constraints: (i) bijectivity: one word should not translate
to many words; and (ii) symmetry: directional alignments should agree. Both of these
constraints significantly improve the performance of the model both in precision and
recall, with the symmetry constraint generally producing more accurate alignments.
Section 3 presents the Posterior Regularization (PR) framework and describes how to
encode such constraints in an efficient manner, requiring only repeated inference in the
original model to enforce the constraints. Section 4 presents a detailed evaluation of
the alignments produced. The constraints over posteriors consistently and significantly
outperform the unconstrained HMM model, evaluated against manual annotations.
Moreover, this training procedure outperforms the more complex IBM Model 4 nine
times out of 12. We examine the influence of constraints on the resulting posterior dis-
tributions and find that they are especially effective for increasing alignment accuracy
for rare words. We also demonstrate a new methodology to avoid overfitting using a
small development corpus. Section 5 evaluates the new framework on two different
tasks that depend on word alignments. Section 5.1 focuses on MT and shows that the
better alignments also lead to better translation systems, adding to similar evidence
presented in Ganchev, Grac?a, and Taskar (2008). Section 5.2 shows that the alignments
we produce are better suited for transfer of syntactic dependency parse annotations.
An implementation of this work (Grac?a, Ganchev, and Taskar 2009) is available under a
GPL license.1
1 www.seas.upenn.edu/?strctlrn/CAT/.
482
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
2. Background
A word alignment for a parallel sentence pair represents the correspondence between
words in a source language and their translations in a target language (Brown et al
1993b). There are many reasons why a simple word-to-word (1-to-1) correspondence
is not possible for every sentence pair: for instance, auxiliary verbs used in one lan-
guage but not the other (e.g., English He walked and French Il est alle?), articles required
in one language but optional in the other (e.g., English Cars use gas and Portuguese
Os carros usam gasolina), cases where the content is expressed using multiple words
in one language and a single word in the other language (e.g., agglutination such as
English weapons of mass destruction and German Massenvernichtungswaffen), and expres-
sions translated indirectly. Due to this inherent ambiguity, manual annotations usually
distinguish between sure correspondences for unambiguous translations, and possible,
for ambiguous translations (Och and Ney 2003). The top row of Figure 1 shows two
word alignments between an English?French sentence pair. We use the following nota-
tion: the alignment on the left (right) will be referenced as source?target (target?source)
and contains source (target) words as rows and target (source) words as columns. Each
entry in the matrix corresponds to a source?target word pair, and is the candidate for an
alignment link. Sure links are represented as squares with borders, and possible links
Figure 1
Posterior marginal distributions for different models for an English to French sentence
translation. Left: EN?FR model. Right: FR? EN model. Top: Regular HMM posteriors.
Middle: After applying bijective constraint. Bottom: After applying symmetric constraint. Sure
alignments are squares with borders; possible alignments are squares without borders. Circle
size indicates probability value. Circle color in the middle and bottom rows indicates differences
in posterior from the top row: green = higher probability; red = lower probability.
483
Computational Linguistics Volume 36, Number 3
Table 1
Test corpora statistics: English?French, English?Spanish, English?Portuguese,
Portuguese?Spanish, Portuguese?French, and Spanish?French.
Corpus Sentence Pairs Ave Length Max Length % Sure % 1-1
En/Fr 447 16/17 30/30 21 98
En/Es 400 29/31 90/99 67 86
En/Pt 60 11/11 20/20 54 91
Pt/Es 60 11/11 20/20 69 92
Pt/Fr 60 11/12 20/20 77 88
Es/Fr 60 11/12 20/20 79 87
are represented as squares without borders. Circles indicate the posterior probability
associated with a given link and will be explained latter.
We use six manually annotated corpora whose characteristics are summarized in
Table 1. The corpora are: the Hansard corpus (Och and Ney 2000) of English/French
Canadian Parliamentary proceedings (En-Fr), and the English/Spanish portion of the
Europarl corpus (Koehn 2005) where the annotation is from EPPS (Lambert et al 2005)
(En-Es) using standard test and development set split. We also used the English/
Portuguese (En-Pt), Portuguese/Spanish (Pt-Es), Portuguese/French (Pt-Fr), and
Spanish/French (Es-Fr) portions of the Europarl corpus using annotations described
by Grac?a et al (2008), where we split the gold alignments into a dev/test set in a ratio of
40%/60%. Table 1 shows some of the variety of challenges presented by each corpus.
For example, En-Es has longer sentences and hence more ambiguity for alignment.
Furthermore, it has a smaller percentage of bijective (1-to-1) alignments, which makes
word fertility more important. Overall, the great majority of links are bijective across
the corpora (86?98%). This characteristic will be explored by the constraints described
in this article. For the evaluations in Section 4, the percentage of sure links (out of all
links) will correlate with difficulty because only sure links are considered for recall.
2.1 HMM Word Alignment Model
In this article we focus on the HMM for word alignment proposed by Vogel, Ney, and
Tillmann (1996). This model generalizes IBM Models 1 and 2 (Brown et al 1993b),
by introducing a first-order Markov dependence between consecutive alignment link
decisions. The model is an (input?output) HMM with I positions whose hidden state
sequence z = (z1, . . . , zI ) with zi ? {null, 1, . . . , J} corresponds to a sequence of source
word positions, where J is the source sentence length, and with null representing un-
aligned target words. Each observation corresponds to a word in the target language xi.
The probability of an alignment z and target sentence x given a source sentence y can
be expressed as:
p?(x, z | y) =
I
?
i=1
pd(zi | zi?1)pt(xi | yzi ) (1)
where pt(xi | yzi ) is the probability of a target word at position i being a translation of the
source word at position zi (translation probability), and pd(zi | zi?1) is the probability
484
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
of translating a word at position zi, given that the previous translated word was at
position zi?1 (distortion probability). Note that this model is directional: Each target
word (observation) can be aligned to at most one source word (hidden state), whereas a
source word could be used multiple times.
We refer to translation parameters pt and distortions parameters pd jointly as ?.
There are several important standard details of the parametrization: The distortion
probability pd(zi | zi?1) depends only on the distance (zi ? zi?1) between the source po-
sitions the states represent. Only distances in the range ?5 are modeled explicitly, with
larger distances assigned equal probabilities. The probability of the initial hidden state,
pd(z1 | z0) is modeled separately from the other distortion probabilities. To incorporate
null links, we add a translation probability given null: pt(xi | ynull). Following standard
practice, null links also maintain position information and do not allow distortion. To
implement this, we create position-specific null hidden states for each source position,
and set pd(nulli|yi? ) = 0 and pd(nulli|nulli? ) = 0 for all i = i?. The model is simple, with
complexity of inference O(I ? J2). There are several problems with the model that arise
from its directionality, however.
 Non-bijective: Multiple target words can be linked to a single source
word. This is rarely desirable. For instance, the model produces
non-bijective links 22% of the time for En-Fr instead of 2%.
 Asymmetric: By switching the (arbitrary) choice of which language is
source and which is target, the HMM produces very different results. For
example, intersecting the sets of alignments produced by the two possible
choices for source preserves less than half of their union for both En-Fr
and En-Pt.2
2.2 Training
Standard HMM training seeks model parameters ? that maximize the log-likelihood of
the parallel corpus:
Log-Likelihood : L(?) = ?E[log p?(x | y)] = ?E[log
?
z
p?(x, z | y)] (2)
where ?E[ f (x, y)] = 1N
?N
n=1 f (x
n, yn) denotes the empirical average of a function f (xn, yn)
over the N pairs of sentences {(x1, y1) . . . , (xN, yN )} in the training corpus. Because
of the latent alignment variables z, the log-likelihood function for the HMM model
is not concave, and the model is fit using the Expectation Maximization (EM) algo-
rithm (Dempster, Laird, and Rubin 1977). EM maximizes L(?) via block-coordinate
ascent on a lower bound F(q, ?) using an auxiliary distribution over the latent variables
q(z | x, y) (Neal and Hinton 1998):
EM Lower Bound : L(?) ? F(q, ?) = ?E
[
?
z
q(z | x, y) log p?(x, z | y)
q(z | x, y)
]
(3)
2 For both of these points, see the experimental setup in Section 4.1.
485
Computational Linguistics Volume 36, Number 3
To simplify notation, we will drop the dependence on y and will write p?(x, z | y) as
p?(x, z), p?(z | x, y) as p?(z | x) and q(z | x, y) as q(z | x). The alternating E and M steps
at iteration t + 1 are given by:
E : qt+1(z | x) = arg max
q(z|x)
F(q, ?t) = arg min
q(z|x)
KL(q(z | x) || p?t (z | x)) = p?t (z | x) (4)
M : ?t+1 = arg max
?
F(qt+1, ?) = arg max
?
?E
[
?
z
qt+1(z | x) log p?(x, z)
]
(5)
where KL(q||p) = Eq[log q(?)p(?) ] is the Kullback-Leibler divergence. The EM algorithm is
guaranteed to converge to a local maximum of L(?) under mild conditions (Neal and
Hinton 1998). The E step computes the posteriors qt+1(z | x) = p?t (z | x) over the latent
variables (alignments) given the observed variables (sentence pair) and current param-
eters ?t, which is accomplished by the forward-backward algorithm for HMMs. The M
step uses qt+1 to ?softly fill in? the values of alignments z and estimate parameters ?t+1.
This step is particularly easy for HMMs, where ?t+1 simply involves normalizing (ex-
pected) counts. This modular split into two intuitive and straightforward steps accounts
for the vast popularity of EM.
In Figure 1, each entry in the alignment matrix contains a circle indicating the align-
ment link posterior for that particular word pair after training an HMM model with the
EM algorithm (see the experimental set up in Section 4.1). Note that the link posteriors
are concentrated around particular source words (rare words occurring less than five
times in the corpus) in both directions, instead of being spread across different words.
This is a well-known problem when training using EM called the ?garbage collector ef-
fect? (Brown et al 1993a). A rare word in the source language links to many words in the
target language that we would ideally like to see unaligned, or aligned to other words
in the sentence. The reason this happens is that the generative model has to distribute
translation probability for each source word among different candidate target words.
If one translation is much more common than another, but the rare translation is used
in the sentence, the model might have a very low translation probability for the correct
alignment. On the other hand, because the rare source word occurs only in a few sen-
tences it needs to spread its probability mass over fewer competing translations. In this
case, choosing to align the rare word to all of these words leads to a higher likelihood
than correctly linking them or linking them to the special null word, because it increases
the likelihood of this sentence without lowering the likelihood of many other sentences.
2.3 Decoding
Alignments are normally predicted using the Viterbi algorithm (which selects the single
most probable path through the HMM?s lattice).
Another possibility that often works better is to use Minimum Bayes-Risk (MBR)
decoding (Kumar and Byrne 2002; Liang, Taskar, and Klein 2006; Grac?a, Ganchev, and
Taskar 2007). Using this decoding we include an alignment link i ? j if the posterior
probability that word i aligns to word j is above some threshold. This allows the
accumulation of probability from several low-scoring alignments that agree on one
alignment link. The threshold is tuned on some small amount of labeled data?in
our case the development set?to minimize some loss. Kumar and Byrne (2002) study
different loss functions that incorporate linguistic knowledge, and show significant
486
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
improvement over likelihood decoding. Note that this could potentially result in an
alignment having zero probability under the model, as many-to-many alignments can
be produced in this way. MBR decoding has several advantages over Viterbi decoding.
First, independently of the particular choice of the loss function, by picking a specific
threshold we can trade off precision and recall of the predicted word alignments. In
fact, in this work when comparing different alignment sets we do not commit to any
loss function but instead compare precision vs recall curves, by generating alignments
for different thresholds (0..1). Second, with this method we can ignore the null word
probabilities, which tend to be poorly estimated.
3. Posterior Regularization
Word alignment models in general and the HMM in particular are very gross over-
simplifications of the translation process and the optimal likelihood parameters learned
often do not correspond to sensible alignments. One solution to this problem is to
add more complexity to the model to better reflect the translation process. This is the
approach taken by IBM Models 4+ (Brown et al 1993b; Och and Ney 2003), and more
recently by the LEAF model (Fraser and Marcu 2007). Unfortunately, these changes
make the models probabilistically deficient and intractable, requiring approximations
and heuristic learning and inference prone to search errors. Instead, we propose to
use a learning framework called Posterior Regularization (Grac?a, Ganchev, and Taskar
2007) that incorporates side information into unsupervised estimation in the form of
constraints on the model?s posteriors. The constraints are expressed as inequalities on
the expected values under the posterior distribution of user-defined constraint features
(not necessarily the same features used by the model). Because in most applications
what we are interested in are the latent variables (in this case the alignments), con-
straining the posteriors allows a more direct way to achieve the desired behavior.
On the other hand, constraining the expected value of the features instead of adding
them to the model allows us to express features that would otherwise make the model
intractable. For example, enforcing that each hidden state of an HMM model should be
used at most once per sentence would break the Markov property and make the model
intractable. In contrast, we will show how to enforce the constraint that each hidden
state is used at most once in expectation. The underlying model remains unchanged,
but the learning method changes. During learning, our method is similar to the EM
algorithm with the addition of solving an optimization problem similar to a maximum
entropy problem inside the E Step. The following subsections present the Posterior
Regularization framework, followed by a description of how to encode two pieces of
prior information aimed at solving the problems described at the end of Section 2.
3.1 Posterior Regularization Framework
The goal of the Posterior Regularization (PR) framework is to guide a model during
learning towards satisfying some prior knowledge about the desired latent variables
(in this case word alignments), encoded as constraints over their expectations. The
key advantage of using regularization on posterior expectations is that the base model
remains unchanged, but during learning, it is driven to obey the constraints by setting
appropriate parameters ?. Moreover, experiments show that enforcing constraints in ex-
pectation results in predicted alignments that also satisfy the constraints. More formally,
posterior information in PR is specified with sets Qx of allowed distributions over the
487
Computational Linguistics Volume 36, Number 3
hidden variables z which satisfy inequality constraints on some user-defined feature
expectations, with violations bounded by  ? 0:
Constrained Posterior Set : Qx = {q(z | x) : ??, Eq[f(x, z)] ? bx ? ?; ||?||22 ? 2} (6)
Qx denotes the set of valid distributions where some feature expectations are bounded
by bx and  ? 0 is an allowable violation slack. Setting  = 0 enforces inequality
constraints strictly. In order to introduce equality constraints, we use two inequality
constraints with opposite signs. We assume that Qx is non-empty for each example x.
Furthermore, the set Qx needs to be convex. In this work we restrict ourselves to
linear inequalities because, as will be shown, subsequently this simplifies the learning
algorithm. Note that Qx, f(x, z), and bx also depend on y, the corresponding source
sentence, but we suppress the dependence for brevity. In PR, the log-likelihood of a
model is penalized with the KL-divergence between the desired distribution space Qx
and the model posteriors, KL(Qx ? p?(z|x)) = min
q(z|x)?Qx
KL(q(z | x) ? p?(z|x)). The regu-
larized objective is:
Posterior Regularized Likelihood : L(?) ? ?E[KL(Qx ? p?(z|x))]. (7)
The objective trades off likelihood and distance to the desired posterior subspace (mod-
ulo getting stuck in local maxima) and provides an effective method of controlling the
posteriors.
Another way of interpreting the objective is to express the marginal log-likelihood
L(?) as a KL distance: KL(?(xn) ? p?(x)) where ?(xn) is a delta function at xn. Hence the
objective is a sum of two average KL terms, one in the space of distributions over x and
one in the space of distributions over z:
?L(?) + ?E[KL(Qx ? p?(z|x))] = 1N
N
?
n=1
KL(?(xn) ? p?(x)) + KL(Qxn ? p?(z|xn)) (8)
This view of the PR objective is illustrated in Figure 2.
Figure 2
Maximizing the PR objective is equivalent to minimizing the empirical average of two
KL divergences: The negative log-likelihood ?L(?) = 1N
?N
n=1 KL(?(x
n) ? p?(x)) plus posterior
regularization 1N
?N
n=1 KL(Qxn ? p?(z|xn)), where ?(xn) is a delta function at xn. The diagram
illustrates the effect of the likelihood term and the regularization term operating over the two
spaces of distributions: the observed variables x and the latent variables z. (The effect of the prior
on ? is not shown.)
488
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
Computing the PR objective involves solving the optimization problem for each x:
Primal Projection : KL(Qx ? p?(z|x)) = min
q(z|x)?Qx
KL(q(z | x) ? p?(z|x)) (9)
Directly minimizing this objective is hard because there is an exponential number of
alignments z; however, the problem becomes easy to solve in its dual formulation (see
Appendix A for derivation):
Dual Projection : arg min
??0
bx ? + log Z(?) +  ||?||2 (10)
where Z(?) =
?
z p?(z|x) exp(?? ? f(x, z)) is the normalization constant and the primal
solution is q(z|x) = p?(z|x) exp{??f(x, z)}/Z(?). There is one dual variable per ex-
pectation constraint, and the dual gradient at ? = 0 is ?(?) = bx ? Eq[f(x, z)] +  ?i||?||2 .
Note that this primal?dual relationship is very similar to the one between maximum
likelihood and maximum entropy. If bx corresponds to empirical expectations and
p?(z|x) is uniform, then Equation (10) would be a log-likelihood and Equation (14) (fol-
lowing) would be a maximum entropy problem. As with maximum entropy, gradient
computation involves computing an expectation under q(z | x), which can be performed
efficiently if the features f(x, z) factor in the same way as the model p?(x, z), and the
constraints are linear. The conditional distribution over z represented by a graphical
model such as HMM can be written as a product of factors over cliques C:
Factored Posterior : p(z | x) = 1Z
?
c?C
?(x, zc) (11)
In an HMM, the cliques C are simply the nodes zi and the edges (zi, zi+1) and the factors
correspond to the distortion and translation probabilities. We will assume f is factorized
as a sum over the same cliques (we will show below how symmetry and bijectivity
constraints can be expressed in this way):
Factored Features : f(x, z) =
?
c?C
f(x, zc) (12)
Then q(z | x) has the same form as p?(z | x):
q(z | x) = 1Zp(z | x) exp(??
f(x, z)) = 1Z
?
c?C
?(x, zc) exp
??f(x,zc ) (13)
Hence the projection step uses the same inference algorithm (forward?backward for
HMMs) to compute the gradient, only modifying the local factors using the current
setting of ?.
?i ? 0;1
while ||?(?)||2 > ? do2
??(x, zc) ? ?(x, zc) exp??
f(x,zc );3
q(z | x) ? forwardBackward(??(x, zc));4
? ? ? + ???(?);5
end6
Algorithm 1: Computing KL(Qx ? p?(z|x)) = min
q?Qx
KL(q(z|x) ? p?(z|x))
489
Computational Linguistics Volume 36, Number 3
We optimize the dual objective using the gradient based methods shown in
Algorithm 1. Here ? is an optimization precision, ? is a step size chosen with the strong
Wolfe?s rule (Nocedal and Wright 1999). Here, ??(?) represents an ascent direction
chosen as follows: For inequality constraints, it is the projected gradient (Bertsekas
1999); for equality constraints with slack, we use conjugate gradient (Nocedal and
Wright 1999), noting that when ? = 0, the objective is not differentiable. In practice
this only happens at the start of optimization and we use a sub-gradient for the first
direction.
Computing the projection requires an algorithm for inference in the original model,
and uses that inference as a subroutine. For HMM word alignments, we need to make
several calls to forward?backward in order to choose ?. Setting the optimization pre-
cision ? more loosely allows the optimization to terminate more quickly but at a less
accurate value. We found that aggressive optimization significantly improves alignment
quality for both constraints we used and consequently choose ? so that tighter values
do not significantly improve performance. This explains why we report better results
here in this paper than in Ganchev, Grac?a, and Taskar (2008), which uses a more naive
optimization (see Section 4.1).
3.2 Posterior Regularization via Expectation Maximization
We can optimize the PR objective using a procedure very similar to the expectation
maximization (EM) algorithm. Recall from Equation (4) that in the E step, q(z | x) is
set to the posterior over hidden variables given the current ?. To converge to the PR
objective, we must modify the E step so that q(z | x) is a projection of the posteriors onto
the constraint set Qx for each example x (Grac?a, Ganchev, and Taskar 2007).
E? : arg min
q,?
KL(q(z|x) ? p?t (z|x)) s.t. Eq[f(x, z)] ? bx ? ?; ||?||22 ? 2 (14)
The new posteriors q(z|x) are used to compute sufficient statistics for this instance and
hence to update the model?s parameters in the M step (Equation (5)), which remains
unchanged. This scheme is illustrated in Figure 3 and in Algorithm 2. The only imple-
mentation difference is that we must now perform the KL projection before collecting
sufficient statistics. We found it can help to also perform this projection at test time,
using q(z | x) = arg min
q(z|x)?Qx
KL(q(z | x)|p?(z | x)) instead of p?(z | x) to decode.
for t = 1..T do1
for each training sentence x do2
E?-Step: qt+1(z | x) = arg min
q(z|x)?Qx
KL(q(z | x)||p?t (z | x))
3
end4
M-Step: ?t+1 = arg max? ?E
[
?
z q
t+1(z | x) log p?(z, x)
]
5
end6
Algorithm 2: PR optimization via modified EM. E?-Step is computed using
Algorithm 1.
490
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
Figure 3
Modified EM for optimizing PR objective L(?) ? ?E[KL(Qx ? p?(z|x))].
3.3 Bijectivity Constraints
We observed in Table 1 that most alignments are 1-to-1 and we would like to introduce
this prior information into the model. Unfortunately including such a constraint in the
model directly breaks the Markov property in a fairly fundamental way. In particular
computing the normalization would require the summation of 1-to-1 or near 1-to-1
weighted matchings, which is a classic #P-complete problem. Introducing alignment
degree constraints in expectation using the PR framework is easy and tractable. We
encode them as the constraint E[f(x, z)] ? 1 where we have one feature f for each source
word j that counts how many times it is aligned to a target word in the alignment z:
Bijective Features : fj(x, z) =
?
i
1(zi = j).
The second row of Figure 1 shows an example of the posteriors after applying bijectivity
constraints; the first row is before the projection. Green (respectively, red) circles indicate
that the probability mass for that particular link increased (respectively, decreased)
when compared with the EM-trained HMM. For example, in the top left panel, the
word schism is used more than once, causing erroneous alignments. Projecting to the
bijectivity constraint set prevents this and most of the mass is (for this example) moved
to the correct word pairs. Enforcing the constraint at training and decoding increases
the fraction of 1-to-1 alignment links from 78% to 97.3% for En-Fr (manual annotations
have 98.1%); for En-Pt the increase is from 84.7% to 95.8% (manual annotations have
90.8%) (see Section 4.1).
3.4 Symmetry Constraints
The directional nature of the generative models used to recover word alignments con-
flicts with their interpretation as translations. In practice, we see that the choice of which
language is source versus target matters and changes the mistakes made by the model
(the first row of panels in Figure 1). The standard approach is to train two models
independently and then intersect their predictions (Och and Ney 2003). However, we
show that it is much better to train two directional models concurrently, coupling
their posterior distributions over alignments to approximately agree. Let the directional
models be defined as: ??p (??z ) (source?target) and ??p (??z ) (target?source). We suppress
dependence on x and y for brevity. Define z to range over the union of all possible
491
Computational Linguistics Volume 36, Number 3
directional alignments
??
Z ???Z . We define a mixture model p(z) = 12
??p (z) + 12
??p (z)
where ??p (??z ) = 0 and vice versa (i.e., the alignment of one directional model has prob-
ability zero according to the other model). We then define the following feature for each
target?source position pair i, j:
Symmetric Features : fij(x, z) =
?
?
?
?
?
+1 z ? ??Z and ??z i = j
?1 z ? ??Z and ??z j = i
0 otherwise
.
If the feature fij has an expected value of zero, then both models predict the i, j link
with equal probability. We therefore impose the constraint Eq[ fij(x, z)] = 0 (possibly with
some small slack). Note that satisfying this implies satisfying the bijectivity constraint
presented earlier. To compute expectations of these features under the model q we only
need to be able to compute them under each directional HMM. To see this, we have by
the definition of q? and p?,
q?(z|x) =
??p (z | x) + ??p (z | x)
2
exp{??f(x, z)}
Z?
=
??q (z|x) Z??q??p (x) +
??q (z|x) Z??q??p (x)
2Z?
(15)
where we have defined:
??q (z|x) = 1Z??q
??p (z, x) exp{??f(x, z)} with Z??q =
?
z
??p (z, x) exp{??f(x, z)}
??q (z|x) = 1Z??q
??p (z, x) exp{??f(x, z)} with Z??q =
?
z
??p (z, x) exp{??f(x, z)}
All these quantities can be computed separately in each model using forward?backward
and, furthermore, Z? = 12 (
Z??q
??p (x) +
Z??q
??p (x) ). The effect of this constraint is illustrated in
the bottom panels of Figure 1. The projected link posteriors are equal for the two
models, and in most cases the probability mass was moved to the correct alignment
links. The exception is the word pair internal/le. In this case, the model chose to incor-
rectly have a high posterior for the alignment link rather than generating internal from
null in one direction and le from null in the other.
We can measure symmetry of predicted alignments as the ratio of the size of the
intersection to the size of the union. Symmetry constraints increase symmetry from 48%
to 89.9% for En-Fr and from 48% to 94.2% for En-Pt (see Section 4.1).
4. Alignment Quality Evaluation
We begin with a comparison of word alignment quality evaluated against manually
annotated alignments as measured by precision and recall. We use the six parallel
corpora with gold annotations described in the beginning of Section 2.
4.1 Experimental Setup
We discarded all training data sentence pairs where one of the sentences contained
more than 40 words. Following common practice, we added the unlabeled development
and test data sets to the pool of unlabeled sentences. We initialized the IBM Model 1
translation table with uniform probabilities over word pairs that occur together in the
same sentence and trained the IBM Model 1 for 5 iterations. All HMM alignment models
492
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
were initialized with the translation table from IBM Model 1 and uniform distortion
probabilities. We run each training procedure until the area under the precision/recall
curve measured on a development corpus stops increasing (see Figure 4 for an example
of such a curve). Using the precision/recall curve gives a broader sense of the model?s
performance than using a single point (by tuning a threshold for a particular metric). In
most cases this meant four iterations for normal EM training and two iterations using
posterior regularization. We suspect that the constraints make the space easier to search.
The convergence criterion for the projection algorithm was the normalized l2 norm
of the gradient (gradient norm divided by number of constraints) being smaller than
? (see Algorithm 1). For bijective constraints, we set ? to 0.005 and used zero slack.
For symmetric constraints, ? and slack were set to 0.001. We chose ? aggressively
and lower values did not significantly increase performance. Less aggressive settings
cause degradation of performance: For example, for En-Fr using 10k sentences, and
running four iterations of constrained EM, the area under the precision/recall curve for
the symmetric model changed from 70% with ? = 0.1 to 85% using ? = 0.001. On the
other hand, the number of iterations required to project the constraints increases for
smaller values of ?. The number of forward?backward calls for normal HMM is 40k
(one for each sentence and EM iteration), for the symmetric model using ? = 0.1 was
around 41k and using ? = 0.001 was around 26M (14 minutes to 4 hours 14 minutes
of training time, 17 times slower, for the different settings of ?). We note that better
optimization methods, such as L-BFGS, or using a warm start for the parameters at each
EM iteration (parameters from the previous iteration), or training the models online,
would potentially decrease the running time of our method.
The intent of this experimental section is to evaluate the gains from using con-
straints during learning, hence the main comparison is between HMM trained with
normal EM vs. trained with PR plus constraints. We also report results for IBM Model 4,
because it is often used as the default word alignment model, and can be used as a
reference. However, we would like to note that IBM Model 4 is a more complex model,
able to capture more structure, albeit at the cost of intractable inference. Because our
approach is orthogonal to the base model used, the constraints described here could
be applied in principle to IBM Model 4 if exact inference was efficient, hopefully
yielding similar improvements. We used a standard implementation of IBM Model
4 (Och and Ney 2003) and because changing the existing code is not trivial, we could
not use the same stopping criterion to avoid overfitting and we are not able to produce
precision/recall curves. We trained IBM Model 4 using the default configuration of the
Figure 4
Precision/Recall curves for different models using 1,000k sentences. Precision on the horizontal
axis. Left: Hansard EN-FR direction. Right: EN-PT Portuguese-English direction.
493
Computational Linguistics Volume 36, Number 3
Figure 5
Word alignment precision when the threshold is chosen to achieve IBM Model 4 recall with a
difference of ? 0.005. The average relative increase in precision (against the HMM model) is
10% for IBM Model 4, 11% for B-HMM, and 14% for S-HMM.
MOSES training script.3 This performs five iterations of IBM Model 1, five iterations of
HMM, and five iterations of IBM Model 4.
4.2 Alignment Results
In this section we present results on alignment quality. All comparisons are made using
MBR decoding because this decoding method always outperforms Viterbi decoding.4
For the models with constraints we project the posteriors at decode time (i.e., we use
q(z | x) to decode). This gives a small but consistent improvement. Figure 4 shows
precision/recall curves for the different models on the En-Fr corpus using English as
the source language (left), and on the En-Pt corpus using Portuguese as the source.
Precision/recall curves are obtained by varying the posterior threshold from 0 to 1 and
then plotting the different precision and recall values obtained.
We observe several trends from Figure 4. First, both types of constraints improve
over the HMM in terms of both precision and recall (their precision/recall curve is
always above). Second, S-HMM performs slightly better than B-HMM. IBM Model 4
is comparable with both constraints (after symmetrization). The results for all language
pairs are in Figure 5. For ease of comparison, we choose a decoding threshold for HMM
models to achieve the recall of the corresponding IBM Model 4 and report precision.
Our methods always improve over the HMM by 10% to 15%, and improve over IBM
Model 4 nine times out of 12. Comparing the constraints with each other we see that
3 www.statmt.org/moses/?n=FactoredTraining.HomePage.
4 IBM Model 4 uses Viterbi decoding as Giza++ does not support MBR decoding.
494
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
Figure 6
Word alignment precision as a function of training data size (number of sentence pairs).
Posterior decoding threshold chosen to achieve IBM Model 4 recall in the Hansard corpus. Right:
English as source. Left: French as source.
S-HMM performs better than B-HMM in 10 out of 12 cases. Because S-HMM indirectly
enforces bijectivity and models sequential correlations on both sides, this is perhaps not
surprising.
Figure 6 shows performance as a function of training data size. As before, we decode
to achieve the recall of IBM Model 4. For small training corpora adding the constraints
provides larger improvements (20?30%) but we still achieve significant gains even with
a million parallel sentences (15%). Greater improvements for small data sizes indicate
that our approach can be especially effective for resource-poor language pairs.
4.3 Rare vs. Common Words
One of the main benefits of using the posterior regularization constraints described is
an alleviation of the garbage collector effect (Brown et al 1993a). Figure 7 breaks down
performance improvements by common versus rare words. As before, we use posterior
decoding, tuning the threshold to match IBM Model 4 recall. For common words, this
tuning maintains recall very close for all models so we do not show this in the figure. In
the top left panel of Figure 7, we see that precision of common words follows the pattern
we saw for the corpus overall: Symmetric and bijective outperform both IBM Model 4
and the baseline HMM, with symmetric slightly better than bijective. The results for
common words vary more slowly as we increase the quantity of training data than they
did for the full corpus. In the top right panel of Figure 7 we show the precision for rare
words. For the baseline HMM as well as for IBM Model 4, this is very low precisely
because of the garbage collector problem: Rare words become erroneously aligned to
untranslated words, leading to low precision. In fact the constrained models achieve
absolute precision improvements of up to 50% over the baseline. By removing these
erroneous alignments the translation table becomes more accurate, allowing higher re-
call on the full corpus. In the bottom panel of Figure 7, we observe a slightly diminished
recall for rare words. This slight drop in recall is due to moving the mass corresponding
to rare words to null.
4.4 Symmetrization
As discussed earlier, the word alignment models are asymmetric, whereas most appli-
cations require a single alignment for each sentence pair. Typically this is achieved by
a symmetrization heuristic that takes two directional alignments and produces a single
495
Computational Linguistics Volume 36, Number 3
Figure 7
Precision and Recall as a function of training data size for En-Fr by common and rare words.
Top Left: Common Precision, Top Right: Rare Precision, Bottom: Rare Recall.
alignment. For MT the most commonly used heuristic is called grow diagonal final
(Och and Ney 2003). This starts with the intersection of the sets of aligned points and
adds points around the diagonal that are in the union of the two sets of aligned points.
The alignment produced has high recall relative to the intersection and only slightly
lower recall than the union. In syntax transfer the intersection heuristic is normally
used, because one wants to have high precision links to transfer knowledge between
languages. One pitfall of these symmetrization heuristics is that they can obfuscate the
link between the original alignment and the ones used for a specific task, making errors
more difficult to analyze. Because they are heuristics tuned for a particular phrase-
based translation system, it is not clear when they will help and when they will hinder
system performance. In this work we followed a more principled approach that uses
Figure 8
Precision/recall curves for the different models after soft union symmetrization. Precision is on
the horizontal axis. Left EN-FR, Right PT-ES.
496
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
the knowledge about the posterior distributions of each directional model. We include a
point in the final alignment if the average of the posteriors under the two models for that
point is above a threshold. This heuristic is called soft union (DeNero and Klein 2007).
Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.
The posterior regularization?trained models still performed better, but the differences
get smaller after doing the symmetrization. This should not be very surprising, because
the soft union symmetrization can be viewed as an approximation of our symmetry
constraint applied only at decode time. Applying the symmetrization to the model with
symmetry constraints does not affect performance.
4.5 Analysis
In this section we discuss some scenarios in which the constraints make the alignments
better, and some scenarios where they fail. We have already discussed the garbage
collector effect and how both models address it. Both of the constraints also bias the
model to have at most probability one in any row or column of the posterior matrix,
encouraging 1-to-1 alignments. Obviously whenever alignments are systematically not
1-to-1 , this can lead to errors (for instance the examples described in Section 2).
An example presented in Figure 9 shows the posterior marginal distributions for an
English/French sentence pair using the same notation as in Figure 1. In the top panel of
Figure 9
Posterior distributions for different models for an English to French sentence translation. Left:
EN?FR model. Right: FR? EN model. Top: Regular HMM posteriors. Middle: After applying
the bijective constraint. Bottom: After applying the symmetric constraint. Sure alignments are
squares with borders; possible alignments are squares without borders. Circle size indicates
probability value. Circle color in the middle and bottom rows indicates differences in posterior
from the top row; green = higher probability; red = lower probability.
497
Computational Linguistics Volume 36, Number 3
Figure 9, we see the baseline models, where the English word met is incorrectly being
aligned to se?ance est ouverte. This makes it impossible to recover the correct alignment
house/se?ance. Either constraint corrects this problem. On the other hand, by enforcing
a 1-to-1 mapping the correct alignment met / est ouverte is lost. Going back to the first
row (regular HMM) this alignment is correct in one direction and absent in the other
(due to the many-to-1 model restriction) but we can recover that information using the
symmetrization heuristics, since the point is present at least in one direction with high
probability mass. This is not the case for the constraint-based models that reduce the
mass of that alignment in both directions. Going back to the right panel of Figure 8, we
can see that for low values of precision the HMM model actually achieves better recall
than the constraint-based methods. There are two possible solutions to alleviate this
type of problem, both with their caveats. One solution is to model the fertility of each
word in a way similar to IBM Model 4, or more generally to model alignments of multi-
ple words. This can lead to significant computational burden, and is not guaranteed to
improve results. A more complicated model may require approximations that destroy
its performance gain, or require larger corpora to estimate its parameters. Another
option is to perform some linguistically motivated pre-processing of the language pair
to conjoin words. This of course has the disadvantage that it needs to be specific to a
language pair in order to include information such as ?English simple past is written
using a single word, so join together French passe? compose?.? An additional problem
with joining words to alleviate inter-language divergences is that it can increase data
sparsity.
5. Task-Specific Alignment Evaluation
In this section we evaluate the alignments resulting from using the proposed constraints
in two different tasks: Statistical machine translation where alignments are used to
restrict the number of possible minimal translation units; and syntax transfer, where
alignments are used to decide how to transfer dependency links.
5.1 Phrase-Based Machine Translation
We now investigate whether our alignments produce improvements in an end-to-end
phrase-based machine translation system. We use a state-of-the-art machine translation
system,5 and follow the experimental setup used for the 2008 shared task on machine
translation (ACL 2008 Third Workshop on Statistical Machine Translation). The full
pipeline consists of the following steps: (1) prepare the data (lowercase, tokenize, and
filter long sentences); (2) build language models; (3) create word alignments in each
direction; (4) symmetrize directional word alignments; (5) build phrase table; (6) tune
weights for the phrase table. For more details consult the shared task description.6 To
evaluate the quality of the produced alignments, we keep the pipeline unchanged, and
use the models described earlier to generate the word alignments in Step 3. For Step 4,
we use the soft union symmetrization heuristic. Symmetrization has almost no effect on
alignments produced by S-HMM, but we use it for uniformity in the experiments. We
tested three values of the threshold (0.2, 0.4, 0.6) which try to capture different tradeoffs
5 The open source Moses (Hoang et al 2007) toolkit from www.statmt.org/moses/.
6 www.statmt.org/wmt08/baseline.html.
498
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
Table 2
BLEU scores for all language pairs. The best threshold was selected according to the
development set after the last MERT iteration. Bold denotes the best score.
Fr ? En En ? Fr Es ? En En ? Es Pt ? En En ? Pt
IBM M4 GDF 35.7 31.2 32.4 31.6 31.4 28.9
HMM SU 35.9 28.9 32.3 31.6 30.9 31.6
B-HMM SU 36.0 31.5 32.6 31.7 31.0 32.2
S-HMM SU 35.5 31.2 31.9 32.5 31.4 32.3
of precision vs. recall, and pick the best according to the translation performance on
development data. Table 2 summarizes the results for the different corpora. For refer-
ence we include IBM Model 4 as suggested in the task description. PR training always
outperforms EM training and outperforms IBM Model 4 in all but one experiment.
Differences in BLEU range from 0.2 to 0.9. The two constraints help to a different extent
for different corpora and translation directions, in a somewhat unpredictable manner.
In general our impression is that the connection between alignment quality and BLEU
scores is complicated, and changes are difficult to explain and justify. The number of
iterations for MERT optimization to converge varied from 2 to 28; and the best choice of
threshold on the development set did not always correspond to the best on the test set.
Contrary to conventional wisdom in the MT community, bigger phrase tables did not
always perform better. In 14 out of 18 cases, the threshold picked was 0.4 (medium size
phrase tables) and the other four times 0.2 was picked (smaller phrase tables). When
we include only high confidence alignments, more phrases are extracted but many of
these are erroneous. Potentially this leads to a poor estimate of the phrase probabilities.
See Lopez and Resnik (2006) for further discussion.
5.2 Syntax Transfer
In this section, we compare the different alignments produced with and without PR
based on how well they can be used for transfer of linguistic resources across languages.
We used the system proposed by Ganchev, Gillenwater, and Taskar (2009). This system
uses a word-aligned corpus and a parser for a resource-rich language (source language)
in order to create a parser for a resource-poor language (target language). We consider
a parse tree on the source language as a set of dependency edges to be transferred. For
each such edge, if both end points are aligned to words in the target language, then
the edge is transferred. These edges are then used as weak supervision when training
a generative or discriminative dependency parser. In order to evaluate the alignments
we computed the fraction of correctly transferred edges as a function of the average
number of edges transferred by using supervised parse trees on the target side. By
changing the threshold in MBR decoding of alignments, we can trade off accuracy of the
transferred edges vs. transferring more edges. We generated supervised parses using
the first-order model from the MST parser (McDonald, Crammer, and Pereira 2005)
trained on the Penn Treebank for English and the CoNLL X parses for Bulgarian and
Spanish. Following Ganchev, Gillenwater, and Taskar (2009), we filter alignment links
between words with incompatible POS tags. Figure 10 shows our results for transferring
from English to Bulgarian (En?Bg) and from English to Spanish (En?Es). The En?Bg
499
Computational Linguistics Volume 36, Number 3
Figure 10
Edge conservation for cross-lingual grammar induction. Left: En?Bg subtitle corpus; Right:
En?Es parliamentary proceedings. Vertical axis: percentage of transferred edges that are correct.
Horizontal axis: average number of transferred edges per sentence.
results are based on a corpus of movie subtitles (Tiedemann 2007), and are consequently
shorter sentences, whereas the En?Es results are based on a corpus of parliamentary
proceedings (Koehn 2005). We see in Figure 10 that for both domains, the models trained
using posterior regularization perform better than the baseline model trained using EM.
6. Related Work
The idea of introducing constraints over a model to better guide the learning process
has appeared before. In the context of word alignment, Deng and Byrne (2005) use a
state-duration HMM in order to model word-to-phrase translations. The fertility of each
source word is implicitly encoded in the durations of the HMM states. Without any
restrictions, likelihood prefers to always use longer phrases and the authors try to con-
trol this behavior by multiplying every transition probability by a constant ? > 1. This
encourages more transitions and hence shorter phrases. For the task of unsupervised
dependency parsing, Smith and Eisner (2006) add a constraint of the form ?the average
length of dependencies should be X? to capture the locality of syntax (at least half
of the dependencies are between adjacent words), using a scheme they call structural
annealing. They modify the model?s distribution over trees p?(y) by a penalty term
as: p
?
?(y) ? p?(y)e
(?
?
e?y length(e)), where length(e) is the surface length of edge e. The
factor ? changes from a high value to a lower one so that the preference for short edges
(hence a smaller sum) is stronger at the start of training.
These two approaches also have the goal of controlling unsupervised learning, and
the form of the modified distributions is reminiscent of the form that the projected
posteriors take. However, the approaches differ substantially from PR. Smith and Eisner
(2006) make a statement of the form ?scale the total length of edges?, which depending
on the value of ? will prefer to have more shorter/longer edges. Such statements are
not data dependent. Depending on the value of ?, for instance if ? ? 0, even if the data
is such that the model already uses too many short edges on average, this value of
? will push for more short edges. By contrast the statements we can make in PR are
of the form ?there should be more short edges than long edges?. Such a statement is
data-dependent in the sense that if the model satisfies the constraints then we do not
need to change it; if it is far from satisfying it we might need to make very dramatic
changes.
500
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
PR is closely related to the work of Mann and McCallum (2007, 2008), who concur-
rently developed the idea of using penalties based on posterior expectations of features
to guide semi-supervised learning. They call their method generalized expectation (GE)
constraints or alternatively expectation regularization. In the original GE framework,
the posteriors of the model on unlabeled data are regularized directly. They train a
discriminative model, using conditional likelihood on labeled data and an ?expectation
regularization? penalty term on the unlabeled data:
arg max
?
Llabeled(?) ? ??E[||Ep? [f(x, z) ? b||
2
2]. (16)
Notice that there is no intermediate distribution q. For some kinds of constraints this
objective is difficult to optimize in ? and in order to improve efficiency, Bellare, Druck,
and McCallum (2009) propose interpreting the PR framework as an approximation
to the GE objective in Equation (16). They compare the two frameworks on several
data sets and find that performance is similar. Liang, Jordan, and Klein (2009) cast
the problem of incorporating partial information about latent variables into a Bayesian
framework using ?measurements,? and after several approximation steps, they arrive
at the objective we optimize.
The idea of jointly training two directional models has been explored by Liang,
Taskar, and Klein (2006), although under a very different formalization. They de-
fine a joint objective max
?1,?2
?E
[
log??p ?1 (x) + log
??p ?2 (x) + log
?
z
??p ?1 (z | x)
??p ?2 (z | x)
]
. However, the
product distribution ??p ?1 (z | x)
??p ?2 (z | x) ranges over all one-to-one alignments and
computing it is #P-complete (Liang, Taskar, and Klein 2006). They approximate this
distribution as a product of marginals: q(z) =
?
i,j
??p ?1 (zi,j | x)
??p ?2 (zi,j | x), but it is not
clear what objective the approximate procedure actually optimizes.
7. Conclusion
In this article we explored a novel learning framework, Posterior Regularization, for
incorporating rich constraints over the posterior distributions of word alignments. We
focused on the HMM word alignment model, and showed how we could incorpo-
rate complex constraints like bijectivity and symmetry while keeping the inference
in the model tractable. Using these constraints we showed consistent and significant
improvements in six different language pairs even when compared to a more complex
model such as IBM Model 4. In addition to alleviating the ?garbage collector? effect, we
show that the obtained posterior distributions better reflect the desired alignments. Both
constraints are biasing the models towards 1-to-1 alignments, which may be inappro-
priate in some situations, and we show some systematic mistakes that the constraints
introduce and suggest possible fixes.
We experimented with two different tasks that rely on word alignments, phrase-
based MT and syntax transfer. For phrase-based MT, the improved alignments lead
to a modest increase in BLEU performance. For syntax transfer, we have shown that
the number of edges of a dependency tree that can be accurately transferred from one
language to another increases as a result of improved alignments.
Our framework opens up the possibility of efficiently adding many other con-
straints that are directly applicable to word alignments, such as preferring alignments
that respect dependency tree structure, part of speech tags, or syntactic boundaries.
501
Computational Linguistics Volume 36, Number 3
Appendix A: Modified E-Step Dual Derivation
The modified E step involves a projection step that minimizes the Kullback-Leibler
divergence:
E? : arg min
q(z|x),?
KL( q(z|x) ? p?(z|x)) s.t. Eq[f(x, z)] ? bx ? ?; ||?||22 ? 2.
Assuming the set Qx = { q(z|x) : ??, Eq[f(x, z)] ? bx ? ?; ||?||22 ? 2} is non-empty, the
corresponding Lagrangian is max
?,?,?
min
q(z|x),?
L( q(z|x), ?, ?, ?, ?) with ? ? 0 and ? ? 0,
where
L( q(z|x), ?, ?, ?, ?) = KL( q(z|x) ? p?(z|x)) + ?(Eq[f(x, z)] ? bx ? ?)
+ ?(||?||22 ? 2) + ?(
?
z
q(z|x) ? 1)
?L( q(z|x), ?, ?, ?, ?)
? q(z|x) = log( q(z|x)) + 1 ? log( p?(z|x)) + ?
f(x, z) + ? = 0
=? q(z|x) = p?(z|x) exp(??
f(x, z))
e exp(?)
?L( q(z|x), ?, ?, ?, ?)
??i
= 2??i ? ?i = 0 =? ?i =
?i
2?
Plugging q(z|x) and ? in L( q(z|x), ?, ?, ?, ?) and taking the derivative with respect to ?:
?L(?, ?, ?)
?? =
?
z
p?(z|x) exp(??f(x, z))
e exp(?)
? 1 = 0 =? ? = log(
?
z p?(z|x) exp(??f(x, z))
e )
Simplifying q(z|x) = p?(z|x) exp(??
f(x,z))
Z?
where Z? =
?
z p?(z|x) exp(??f(x, z)) en-
sures that q(z|x) is properly normalized. Plugging ? into L(?, ?, ?) and taking the
derivative with respect to ?, we get:
L(?, ?) = ? log(Z?) ? bx ??
||?||22
2? +
||?||22
4? ? ?
2 (A.1)
?L(?, ?)
?? =
||?||22
2?2
? ||?||
2
2
4?2
? 2 = 0 =? ? = ||?||22 (A.2)
Replacing back into L(?, ?) we get the dual objective:
Dual E? : arg max
??0
?bx ?? log(Z?) ? ||?||2  (A.3)
Acknowledgments
J. V. Grac?a was supported by a fellowship
from Fundac?a?o para a Cie?ncia e Tecnologia
(SFRH/ BD/ 27528/ 2006) and by FCT
project CMU-PT/HuMach/0039/2008.
K. Ganchev was partially supported by
NSF ITR EIA 0205448. Ben Taskar was
partially supported by DARPA CSSG
2009 grant.
References
Bannard, Colin and Chris Callison-Burch.
2005. Paraphrasing with bilingual parallel
corpora. In ACL ?05: Proceedings of the
43rd Annual Meeting of the Association for
Computational Linguistics, pages 597?604,
Morristown, NJ.
Bellare, Kedar, Gregory Druck, and Andrew
McCallum. 2009. Alternating projections
502
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
for learning with expectation constraints.
In Proceedings of the Twenty-Fifth Conference
Annual Conference on Uncertainty in
Artificial Intelligence, pages 43?50,
Corvallis, OR.
Bertsekas, Dimitri P. 1999. Nonlinear
Programming: 2nd Edition. Athena
Scientific, Nashua, NH.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, Meredith J.
Goldsmith, Jan Hajic, Robert L. Mercer,
and Surya Mohanty. 1993a. But
dictionaries are data too. In HLT ?93:
Proceedings of the Workshop on Human
Language Technology, pages 202?205,
Morristown, NJ.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993b. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Chiang, David, Adam Lopez, Nitin
Madnani, Christof Monz, Philip Resnik,
and Michael Subotin. 2005. The Hiero
machine translation system: Extensions,
evaluation, and analysis. In Proceedings
of the Human Language Technology
Conference and Conference on Empirical
Methods in Natural Language Processing,
pages 779?786, Vancouver.
Dempster, Arthur P., Nan M. Laird, and
Donald B. Rubin. 1977. Maximum
likelihood from incomplete data via the
em algorithm. Royal Statistical Society,
Series B, 39(1):1?38.
DeNero, John and Dan Klein. 2007. Tailoring
word alignments to syntactic machine
translation. In Proceedings of the 45th
Annual Meeting of the Association of
Computational Linguistics, pages 17?24,
Prague.
Deng, Yonggang and William Byrne. 2005.
HMM word and phrase alignment for
statistical machine translation. In HLT ?05:
Proceedings of the Conference on Human
Language Technology and Empirical
Methods in Natural Language Processing,
pages 169?176, Morristown, NJ.
Association for Computational Linguistics.
Fraser, Alexander and Daniel Marcu. 2007.
Getting the structure right for word
alignment: Leaf. In Proceedings of the Joint
Conference on Empirical Methods in Natural
Language Processing and Computational
Natural Language Learning
(EMNLP-CoNLL), pages 51?60, Prague.
Galley, Michel, Mark Hopkins, Kevin Knight,
and Daniel Marcu. 2004. What?s in a
translation rule? In HLT-NAACL 2004:
Main Proceedings, pages 273?280,
Boston, MA.
Ganchev, Kuzman, Jennifer Gillenwater, and
Ben Taskar. 2009. Dependency grammar
induction via bitext projection constraints.
In ACL-IJCNLP ?09: Proceedings of the Joint
Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint
Conference on Natural Language Processing
of the AFNLP: Volume 1, pages 369?377,
Morristown, NJ.
Ganchev, Kuzman, Joa?o V. Grac?a, and Ben
Taskar. 2008. Better alignments = better
translations? In Proceedings of ACL-08: HLT,
pages 986?993, Columbus, OH.
Grac?a, Joa?o V., Kuzman Ganchev, and Ben
Taskar. 2007. Expectation maximization
and posterior constraints. In J. C. Platt,
D. Koller, Y. Singer, and S. Roweis, editors,
Advances in Neural Information Processing
Systems 20. MIT Press, Cambridge, MA,
pages 569?576.
Grac?a, Joa?o V., Kuzman Ganchev, and
Ben Taskar. 2009. Postcat - posterior
constrained alignment toolkit. The Prague
Bulletin Of Mathematical Linguistics - Special
Issue: Open Source Tools for Machine
Translation, 91:27?37.
Grac?a, Joa?o V., Joana P. Pardal, Lu??sa Coheur,
and Diamantino Caseiro. 2008. Building
a golden collection of parallel
multi-language word alignment. In
Proceedings of the Sixth International
Language Resources and Evaluation
(LREC?08), Marrakech.
Hoang, Hieu, Alexandra Birch, Chris
Callison-Burch, Richard Zens, Rwth
Aachen, Alexandra Constantin, Marcello
Federico, Nicola Bertoldi, Chris Dyer,
Brooke Cowan, Wade Shen, Christine
Moran, and Ondrej Bojar. 2007. Moses:
Open source toolkit for statistical machine
translation. In Proceedings of the 45th
Annual Meeting of the Association for
Computational Linguistics Companion
Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague.
Hwa, Rebecca, Philip Resnik, Amy
Weinberg, Clara Cabezas, and Okan Kolak.
2005. Bootstrapping parsers via syntactic
projection across parallel texts. Natural
Language Engineering, 11:11?311.
Koehn, Philipp. 2005. Europarl: A parallel
corpus for statistical machine translation.
In Machine Translation Summit,
12?15 September, Phuket.
Koehn, Philipp, Franz Josef Och, and Daniel
Marcu. 2003. Statistical phrase-based
503
Computational Linguistics Volume 36, Number 3
translation. In Proceedings of the 2003
Conference of the North American Chapter of
the Association for Computational Linguistics
on Human Language Technology (NAACL),
pages 48?54, Morristown, NJ.
Kumar, Shankar and William Byrne. 2002.
Minimum Bayes-Risk word alignments of
bilingual texts. In Proceedings of the ACL-02
Conference on Empirical Methods in Natural
Language Processing, pages 140?147,
Philadelphia, PA.
Lambert, Patrik, Adria` De Gispert, Rafael
Banchs, and Jose? B. Marino. 2005.
Guidelines for word alignment evaluation
and manual alignment. Language Resources
and Evaluation, 39(4):267?285.
Liang, Percy, Michael I. Jordan, and Dan
Klein. 2009. Learning from measurements
in exponential families. In ICML ?09:
Proceedings of the 26th Annual International
Conference on Machine Learning,
pages 641?648, New York, NY.
Liang, Percy, Ben Taskar, and Dan Klein.
2006. Alignment by agreement. In
Proceedings of the Human Language
Technology Conference of the NAACL, Main
Conference, pages 104?111, New York, NY.
Lopez, Adam and Philip Resnik. 2006.
Word-based alignment, phrase-based
translation: Whats the link? In Proceedings
of the 7th Conference of the Association for
Machine Translation in the Americas
(AMTA): Visions for the Future of Machine
Translation, pages 90?99, Boston, MA.
Mann, G. and A. McCallum. 2007. Simple,
robust, scalable semi-supervised learning
via expectation regularization. In
Proceedings of the 24th International
Conference on Machine Learning, page 600,
Corvallis, OR.
Mann, Gideon S. and Andrew McCallum.
2008. Generalized expectation criteria
for semi-supervised learning of
conditional random fields. In Proceedings
of ACL-08: HLT, pages 870?878,
Columbus, OH.
Matusov, Evgeny, Nicola Ueffing, and
Hermann Ney. 2006. Computing
consensus translation from multiple
machine translation systems using
enhanced hypotheses alignment. In
Proceedings of the EACL, pages 33?40,
Cambridge.
McDonald, Ryan, Koby Crammer, and
Fernando Pereira. 2005. Online
large-margin training of dependency
parsers. In ACL ?05: Proceedings of the
43rd Annual Meeting of the Association for
Computational Linguistics, pages 91?98,
Morristown, NJ.
Neal, Radford M. and Geoffrey E. Hinton.
1998. A new view of the EM algorithm that
justifies incremental, sparse and other
variants. In M. I. Jordan, editor, Learning in
Graphical Models. Kluwer, Amsterdam,
pages 355?368.
Nocedal, Jorge and Stephen J. Wright.
1999. Numerical Optimization. Springer,
Berlin.
Och, Franz Josef and Hermann Ney. 2000.
Improved statistical alignment models.
In ACL ?00: Proceedings of the 38th
Annual Meeting on Association for
Computational Linguistics, pages 440?447,
Morristown, NJ.
Och, Franz Josef and Hermann Ney. 2003.
A systematic comparison of various
statistical alignment models. Computational
Linguistics, 29(1):19?51.
Smith, Noah A. and Jason Eisner. 2006.
Annealing structural bias in multilingual
weighted grammar induction. In ACL-44:
Proceedings of the 21st International
Conference on Computational Linguistics and
the 44th Annual Meeting of the Association for
Computational Linguistics, pages 569?576,
Morristown, NJ.
Snyder, Benjamin and Regina Barzilay.
2008. Unsupervised multilingual learning
for morphological segmentation. In
Proceedings of ACL-08: HLT, pages 737?745,
Columbus, OH.
Tiedemann, Jo?rg. 2007. Building a
multilingual parallel subtitle corpus. In
Proceedings of the 17th Conference on
Computational Linguistics in the Netherlands
(CLIN 17), Leuven.
Vogel, Stephan, Hermann Ney, and
Christoph Tillmann. 1996. Hmm-based
word alignment in statistical translation.
In Proceedings of the 16th Conference on
Computational Linguistics, pages 836?841,
Morristown, NJ.
Yarowsky, David and Grace Ngai. 2001.
Inducing multilingual POS taggers and NP
bracketers via robust projection across
aligned corpora. In Proceedings of the North
American Chapter Of The Association For
Computational Linguistics, pages 1?8,
Morristown, NJ.
504
Proceedings of the ACL 2010 Conference Short Papers, pages 194?199,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Sparsity in Dependency Grammar Induction
Jennifer Gillenwater and Kuzman Ganchev
University of Pennsylvania
Philadelphia, PA, USA
{jengi,kuzman}@cis.upenn.edu
Jo?o Gra?a
L2F INESC-ID
Lisboa, Portugal
joao.graca@l2f.inesc-id.pt
Fernando Pereira
Google Inc.
Mountain View, CA, USA
pereira@google.com
Ben Taskar
University of Pennsylvania
Philadelphia, PA, USA
taskar@cis.upenn.edu
Abstract
A strong inductive bias is essential in un-
supervised grammar induction. We ex-
plore a particular sparsity bias in de-
pendency grammars that encourages a
small number of unique dependency
types. Specifically, we investigate
sparsity-inducing penalties on the poste-
rior distributions of parent-child POS tag
pairs in the posterior regularization (PR)
framework of Gra?a et al (2007). In ex-
periments with 12 languages, we achieve
substantial gains over the standard expec-
tation maximization (EM) baseline, with
average improvement in attachment ac-
curacy of 6.3%. Further, our method
outperforms models based on a standard
Bayesian sparsity-inducing prior by an av-
erage of 4.9%. On English in particular,
we show that our approach improves on
several other state-of-the-art techniques.
1 Introduction
We investigate an unsupervised learning method
for dependency parsing models that imposes spar-
sity biases on the dependency types. We assume
a corpus annotated with POS tags, where the task
is to induce a dependency model from the tags for
corpus sentences. In this setting, the type of a de-
pendency is defined as a pair: tag of the dependent
(also known as the child), and tag of the head (also
known as the parent). Given that POS tags are de-
signed to convey information about grammatical
relations, it is reasonable to assume that only some
of the possible dependency types will be realized
for a given language. For instance, in English it
is ungrammatical for nouns to dominate verbs, ad-
jectives to dominate adverbs, and determiners to
dominate almost any part of speech. Thus, the re-
alized dependency types should be a sparse subset
of all possible types.
Previous work in unsupervised grammar induc-
tion has tried to achieve sparsity through priors.
Liang et al (2007), Finkel et al (2007) and John-
son et al (2007) proposed hierarchical Dirichlet
process priors. Cohen et al (2008) experimented
with a discounting Dirichlet prior, which encour-
ages a standard dependency parsing model (see
Section 2) to limit the number of dependent types
for each head type.
Our experiments show a more effective sparsity
pattern is one that limits the total number of unique
head-dependent tag pairs. This kind of sparsity
bias avoids inducing competition between depen-
dent types for each head type. We can achieve the
desired bias with a constraint on model posteri-
ors during learning, using the posterior regulariza-
tion (PR) framework (Gra?a et al, 2007). Specifi-
cally, to implement PR we augment the maximum
marginal likelihood objective of the dependency
model with a term that penalizes head-dependent
tag distributions that are too permissive.
Although not focused on sparsity, several other
studies use soft parameter sharing to couple dif-
ferent types of dependencies. To this end, Cohen
et al (2008) and Cohen and Smith (2009) inves-
tigated logistic normal priors, and Headden III et
al. (2009) used a backoff scheme. We compare to
their results in Section 5.
The remainder of this paper is organized as fol-
194
lows. Section 2 and 3 review the models and sev-
eral previous approaches for learning them. Sec-
tion 4 describes learning with PR. Section 5 de-
scribes experiments across 12 languages and Sec-
tion 6 analyzes the results. For additional details
on this work see Gillenwater et al (2010).
2 Parsing Model
The models we use are based on the generative de-
pendency model with valence (DMV) (Klein and
Manning, 2004). For a sentence with tags x, the
root POS r(x) is generated first. Then the model
decides whether to generate a right dependent con-
ditioned on the POS of the root and whether other
right dependents have already been generated for
this head. Upon deciding to generate a right de-
pendent, the POS of the dependent is selected by
conditioning on the head POS and the direction-
ality. After stopping on the right, the root gener-
ates left dependents using the mirror reversal of
this process. Once the root has generated all its
dependents, the dependents generate their own de-
pendents in the same manner.
2.1 Model Extensions
For better comparison with previous work we
implemented three model extensions, borrowed
from Headden III et al (2009). The first exten-
sion alters the stopping probability by condition-
ing it not only on whether there are any depen-
dents in a particular direction already, but also on
how many such dependents there are. When we
talk about models with maximum stop valency Vs
= S, this means it distinguishes S different cases:
0, 1, . . . , S?2, and? S?1 dependents in a given
direction. The basic DMV has Vs = 2.
The second model extension we implement is
analogous to the first, but applies to dependent tag
probabilities instead of stop probabilities. Again,
we expand the conditioning such that the model
considers how many other dependents were al-
ready generated in the same direction. When we
talk about a model with maximum child valency
Vc = C, this means we distinguish C different
cases. The basic DMV has Vc = 1. Since this
extension to the dependent probabilities dramati-
cally increases model complexity, the third model
extension we implement is to add a backoff for the
dependent probabilities that does not condition on
the identity of the parent POS (see Equation 2).
More formally, under the extended DMV the
probability of a sentence with POS tags x and de-
pendency tree y is given by:
p?(x,y) = proot(r(x))?
Y
y?y
pstop(false | yp, yd, yvs)pchild(yc | yp, yd, yvc)?
Y
x?x
pstop(true | x, left, xvl) pstop(true | x, right, xvr )
(1)
where y is the dependency of yc on head yp in di-
rection yd, and yvc , yvs , xvr , and xvl indicate va-
lence. For the third model extension, the backoff
to a probability not dependent on parent POS can
be formally expressed as:
?pchild(yc | yp, yd, yvc) + (1? ?)pchild(yc | yd, yvc) (2)
for ? ? [0, 1]. We fix ? = 1/3, which is a crude
approximation to the value learned by Headden III
et al (2009).
3 Previous Learning Approaches
In our experiments, we compare PR learning
to standard expectation maximization (EM) and
to Bayesian learning with a sparsity-inducing
prior. The EM algorithm optimizes marginal like-
lihood L(?) = log
?
Y p?(X,Y), where X =
{x1, . . . ,xn} denotes the entire unlabeled corpus
and Y = {y1, . . . ,yn} denotes a set of corre-
sponding parses for each sentence. Neal and Hin-
ton (1998) view EM as block coordinate ascent on
a function that lower-bounds L(?). Starting from
an initial parameter estimate ?0, the algorithm it-
erates two steps:
E : qt+1 = argmin
q
KL(q(Y) ? p?t(Y | X)) (3)
M : ?t+1 = argmax
?
Eqt+1 [log p?(X,Y)] (4)
Note that the E-step just sets qt+1(Y) =
p?t(Y|X), since it is an unconstrained minimiza-
tion of a KL-divergence. The PR method we
present modifies the E-step by adding constraints.
Besides EM, we also compare to learning with
several Bayesian priors that have been applied to
the DMV. One such prior is the Dirichlet, whose
hyperparameter we will denote by ?. For ? < 0.5,
this prior encourages parameter sparsity. Cohen
et al (2008) use this method with ? = 0.25 for
training the DMV and achieve improvements over
basic EM. In this paper we will refer to our own
implementation of the Dirichlet prior as the ?dis-
counting Dirichlet? (DD) method. In addition to
195
the Dirichlet, other types of priors have been ap-
plied, in particular logistic normal priors (LN) and
shared logistic normal priors (SLN) (Cohen et al,
2008; Cohen and Smith, 2009). LN and SLN aim
to tie parameters together. Essentially, this has a
similar goal to sparsity-inducing methods in that it
posits a more concise explanation for the grammar
of a language. Headden III et al (2009) also im-
plement a sort of parameter tying for the E-DMV
through a learning a backoff distribution on child
probabilities. We compare against results from all
these methods.
4 Learning with Sparse Posteriors
We would like to penalize models that predict a
large number of distinct dependency types. To en-
force this penalty, we use the posterior regular-
ization (PR) framework (Gra?a et al, 2007). PR
is closely related to generalized expectation con-
straints (Mann and McCallum, 2007; Mann and
McCallum, 2008; Bellare et al, 2009), and is also
indirectly related to a Bayesian view of learning
with constraints on posteriors (Liang et al, 2009).
The PR framework uses constraints on posterior
expectations to guide parameter estimation. Here,
PR allows a natural and tractable representation of
sparsity constraints based on edge type counts that
cannot easily be encoded in model parameters. We
use a version of PR where the desired bias is a
penalty on the log likelihood (see Ganchev et al
(2010) for more details). For a distribution p?, we
define a penalty as the (generic) ?-norm of expec-
tations of some features ?:
||Ep? [?(X,Y)]||? (5)
For computational tractability, rather than penaliz-
ing the model?s posteriors directly, we use an aux-
iliary distribution q, and penalize the marginal log-
likelihood of a model by the KL-divergence of p?
from q, plus the penalty term with respect to q.
For a fixed set of model parameters ? the full PR
penalty term is:
min
q
KL(q(Y) ? p?(Y|X)) + ? ||Eq[?(X,Y)]||? (6)
where ? is the strength of the regularization. PR
seeks to maximize L(?) minus this penalty term.
The resulting objective can be optimized by a vari-
ant of the EM (Dempster et al, 1977) algorithm
used to optimize L(?).
4.1 `1/`? Regularization
We now define precisely how to count dependency
types. For each child tag c, let i range over an enu-
meration of all occurrences of c in the corpus, and
let p be another tag. Let the indicator ?cpi(X,Y)
have value 1 if p is the parent tag of the ith occur-
rence of c, and value 0 otherwise. The number of
unique dependency types is then:
X
cp
max
i
?cpi(X,Y) (7)
Note there is an asymmetry in this count: occur-
rences of child type c are enumerated with i, but
all occurrences of parent type p are or-ed in ?cpi.
That is, ?cpi = 1 if any occurrence of p is the par-
ent of the ith occurrence of c. We will refer to PR
training with this constraint as PR-AS. Instead of
counting pairs of a child token and a parent type,
we can alternatively count pairs of a child token
and a parent token by letting p range over all to-
kens rather than types. Then each potential depen-
dency corresponds to a different indicator ?cpij ,
and the penalty is symmetric with respect to par-
ents and children. We will refer to PR training
with this constraint as PR-S. Both approaches per-
form very well, so we report results for both.
Equation 7 can be viewed as a mixed-norm
penalty on the features ?cpi or ?cpij : the sum cor-
responds to an `1 norm and the max to an `?
norm. Thus, the quantity we want to minimize
fits precisely into the PR penalty framework. For-
mally, to optimize the PR objective, we complete
the following E-step:
argmin
q
KL(q(Y)||p?(Y|X)) + ?
X
cp
max
i
Eq[?(X,Y)],
(8)
which can equivalently be written as:
min
q(Y),?cp
KL(q(Y) ? p?(Y|X)) + ?
X
cp
?cp
s. t. ?cp ? Eq[?(X,Y)]
(9)
where ?cp corresponds to the maximum expecta-
tion of ? over all instances of c and p. Note that
the projection problem can be solved efficiently in
the dual (Ganchev et al, 2010).
5 Experiments
We evaluate on 12 languages. Following the ex-
ample of Smith and Eisner (2006), we strip punc-
tuation from the sentences and keep only sen-
tences of length ? 10. For simplicity, for all mod-
els we use the ?harmonic? initializer from Klein
196
Model EM PR Type ?
DMV 45.8 62.1 PR-S 140
2-1 45.1 62.7 PR-S 100
2-2 54.4 62.9 PR-S 80
3-3 55.3 64.3 PR-S 140
4-4 55.1 64.4 PR-AS 140
Table 1: Attachment accuracy results. Column 1: Vc-
Vs used for the E-DMV models. Column 3: Best PR re-
sult for each model, which is chosen by applying each of
the two types of constraints (PR-S and PR-AS) and trying
? ? {80, 100, 120, 140, 160, 180}. Columns 4 & 5: Con-
straint type and ? that produced the values in column 3.
and Manning (2004), which we refer to as K&M.
We always train for 100 iterations and evaluate
on the test set using Viterbi parses. Before eval-
uating, we smooth the resulting models by adding
e?10 to each learned parameter, merely to remove
the chance of zero probabilities for unseen events.
(We did not tune this as it should make very little
difference for final parses.) We score models by
their attachment accuracy ? the fraction of words
assigned the correct parent.
5.1 Results on English
We start by comparing English performance for
EM, PR, and DD. To find ? for DD we searched
over five values: {0.01, 0.1, 0.25, 1}. We found
0.25 to be the best setting for the DMV, the same
as found by Cohen et al (2008). DD achieves ac-
curacy 46.4% with this ?. For the E-DMV we
tested four model complexities with valencies Vc-
Vs of 2-1, 2-2, 3-3, and 4-4. DD?s best accuracy
was 53.6% with the 4-4 model at ? = 0.1. A
comparison between EM and PR is shown in Ta-
ble 1. PR-S generally performs better than the PR-
AS for English. Comparing PR-S to EM, we also
found PR-S is always better, independent of the
particular ?, with improvements ranging from 2%
to 17%. Note that in this work we do not perform
the PR projection at test time; we found it detri-
mental, probably due to a need to set the (corpus-
size-dependent) ? differently for the test set. We
also note that development likelihood and the best
setting for ? are not well-correlated, which un-
fortunately makes it hard to pick these parameters
without some supervision.
5.2 Comparison with Previous Work
In this section we compare to previously published
unsupervised dependency parsing results for En-
glish. It might be argued that the comparison is
unfair since we do supervised selection of model
Learning Method Accuracy
? 10 ? 20 all
PR-S (? = 140) 62.1 53.8 49.1
LN families 59.3 45.1 39.0
SLN TieV & N 61.3 47.4 41.4
PR-AS (? = 140) 64.4 55.2 50.5
DD (? = 1, ? learned) 65.0 (?5.7)
Table 2: Comparison with previous published results. Rows
2 and 3 are taken from Cohen et al (2008) and Cohen and
Smith (2009), and row 5 from Headden III et al (2009).
complexity and regularization strength. However,
we feel the comparison is not so unfair as we per-
form only a very limited search of the model-?
space. Specifically, the only values of ? we search
over are {80, 100, 120, 140, 160, 180}.
First, we consider the top three entries in Ta-
ble 2, which are for the basic DMV. The first en-
try was generated using our implementation of
PR-S. The second two entries are logistic nor-
mal and shared logistic normal parameter tying re-
sults (Cohen et al, 2008; Cohen and Smith, 2009).
The PR-S result is the clear winner, especially as
length of test sentences increases. For the bot-
tom two entries in the table, which are for the E-
DMV, the last entry is best, corresponding to us-
ing a DD prior with ? = 1 (non-sparsifying), but
with a special ?random pools? initialization and a
learned weight ? for the child backoff probabil-
ity. The result for PR-AS is well within the vari-
ance range of this last entry, and thus we conjec-
ture that combining PR-AS with random pools ini-
tialization and learned ? would likely produce the
best-performing model of all.
5.3 Results on Other Languages
Here we describe experiments on 11 additional
languages. For each we set ? and model complex-
ity (DMV versus one of the four E-DMV exper-
imented with previously) based on the best con-
figuration found for English. This likely will not
result in the ideal parameters for all languages, but
provides a realistic test setting: a user has avail-
able a labeled corpus in one language, and would
like to induce grammars for many other languages.
Table 3 shows the performance for all models and
training procedures. We see that the sparsifying
methods tend to improve over EM most of the
time. For the basic DMV, average improvements
are 1.6% for DD, 6.0% for PR-S, and 7.5% for
PR-AS. PR-AS beats PR-S in 8 out of 12 cases,
197
Bg Cz De Dk En Es Jp Nl Pt Se Si Tr
DMV Model
EM 37.8 29.6 35.7 47.2 45.8 40.3 52.8 37.1 35.7 39.4 42.3 46.8
DD 0.25 39.3 30.0 38.6 43.1 46.4 47.5 57.8 35.1 38.7 40.2 48.8 43.8
PR-S 140 53.7 31.5 39.6 44.0 62.1 61.1 58.8 31.0 47.0 42.2 39.9 51.4
PR-AS 140 54.0 32.0 39.6 42.4 61.9 62.4 60.2 37.9 47.8 38.7 50.3 53.4
Extended Model
EM (3,3) 41.7 48.9 40.1 46.4 55.3 44.3 48.5 47.5 35.9 48.6 47.5 46.2
DD 0.1 (4,4) 47.6 48.5 42.0 44.4 53.6 48.9 57.6 45.2 48.3 47.6 35.6 48.9
PR-S 140 (3,3) 59.0 54.7 47.4 45.8 64.3 57.9 60.8 33.9 54.3 45.6 49.1 56.3
PR-AS 140 (4,4) 59.8 54.6 45.7 46.6 64.4 57.9 59.4 38.8 49.5 41.4 51.2 56.9
Table 3: Attachment accuracy results. The parameters used are the best settings found for English. Values for hyperparameters
(? or ?) are given after the method name. For the extended model (Vc, Vs) are indicated in parentheses. En is the English Penn
Treebank (Marcus et al, 1993) and the other 11 languages are from the CoNLL X shared task: Bulgarian [Bg] (Simov et al,
2002), Czech [Cz] (Bohomov? et al, 2001), German [De] (Brants et al, 2002), Danish [Dk] (Kromann et al, 2003), Spanish
[Es] (Civit and Mart?, 2004), Japanese [Jp] (Kawata and Bartels, 2000), Dutch [Nl] (Van der Beek et al, 2002), Portuguese
[Pt] (Afonso et al, 2002), Swedish [Se] (Nilsson et al, 2005), Slovene [Sl] (D?eroski et al, 2006), and Turkish [Tr] (Oflazer et
al., 2003).
Unad
papeleranc esvs und
objetonc civilizadoaq
Unad
papeleranc esvs und
objetonc civilizadoaq
1.00
1.00 1.000.49
0.51
1.00
0.57
0.43
Unad
papeleranc esvs und
objetonc civilizadoaq
1.00 0.83 0.75 0.990.92
0.35
0.48
Figure 1: Posterior edge probabilities for an example sen-
tence from the Spanish test corpus. At the top are the gold
dependencies, the middle are EM posteriors, and bottom are
PR posteriors. Green indicates correct dependencies and red
indicates incorrect dependencies. The numbers on the edges
are the values of the posterior probabilities.
though the average increase is only 1.5%. PR-S
is also better than DD for 10 out of 12 languages.
If we instead consider these methods for the E-
DMV, DD performs worse, just 1.4% better than
the E-DMV EM, while both PR-S and PR-AS con-
tinue to show substantial average improvements
over EM, 6.5% and 6.3%, respectively.
6 Analysis
One common EM error that PR fixes in many lan-
guages is the directionality of the noun-determiner
relation. Figure 1 shows an example of a Span-
ish sentence where PR significantly outperforms
EM because of this. Sentences such as ?Lleva
tiempo entenderlos? which has tags ?main-verb
common-noun main-verb? (no determiner tag)
provide an explanation for PR?s improvement?
when PR sees that sometimes nouns can appear
without determiners but that the opposite situation
does not occur, it shifts the model parameters to
make nouns the parent of determiners instead of
the reverse. Then it does not have to pay the cost
of assigning a parent with a new tag to cover each
noun that doesn?t come with a determiner.
7 Conclusion
In this paper we presented a new method for unsu-
pervised learning of dependency parsers. In con-
trast to previous approaches that constrain model
parameters, we constrain model posteriors. Our
approach consistently outperforms the standard
EM algorithm and a discounting Dirichlet prior.
We have several ideas for further improving our
constraints, such as: taking into account the direc-
tionality of the edges, using different regulariza-
tion strengths for the root probabilities than for the
child probabilities, and working directly on word
types rather than on POS tags. In the future, we
would also like to try applying similar constraints
to the more complex task of joint induction of POS
tags and dependency parses.
Acknowledgments
J. Gillenwater was supported by NSF-IGERT
0504487. K. Ganchev was supported by
ARO MURI SUBTLE W911NF-07-1-0216.
J. Gra?a was supported by FCT fellowship
SFRH/BD/27528/2006 and by FCT project CMU-
PT/HuMach/0039/2008. B. Taskar was partly
supported by DARPA CSSG and ONR Young
Investigator Award N000141010746.
198
References
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002.
Floresta Sinta(c)tica: a treebank for Portuguese. In
Proc. LREC.
K. Bellare, G. Druck, and A. McCallum. 2009. Al-
ternating projections for learning with expectation
constraints. In Proc. UAI.
A. Bohomov?, J. Hajic, E. Hajicova, and B. Hladka.
2001. The prague dependency treebank: Three-level
annotation scenario. In Anne Abeill?, editor, Tree-
banks: Building and Using Syntactically Annotated
Corpora.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and
G. Smith. 2002. The TIGER treebank. In Proc.
Workshop on Treebanks and Linguistic Theories.
M. Civit and M.A. Mart?. 2004. Building cast3lb: A
Spanish Treebank. Research on Language & Com-
putation.
S.B. Cohen and N.A. Smith. 2009. The shared logistic
normal distribution for grammar induction. In Proc.
NAACL.
S.B. Cohen, K. Gimpel, and N.A. Smith. 2008. Lo-
gistic normal priors for unsupervised probabilistic
grammar induction. In Proc. NIPS.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical So-
ciety, 39(1):1?38.
S. D?eroski, T. Erjavec, N. Ledinek, P. Pajas,
Z. ?abokrtsky, and A. ?ele. 2006. Towards a
Slovene dependency treebank. In Proc. LREC.
J. Finkel, T. Grenager, and C. Manning. 2007. The
infinite tree. In Proc. ACL.
K. Ganchev, J. Gra?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
J. Gillenwater, K. Ganchev, J. Gra?a, F. Pereira, and
B. Taskar. 2010. Posterior sparsity in unsupervised
dependency parsing. Technical report, MS-CIS-10-
19, University of Pennsylvania.
J. Gra?a, K. Ganchev, and B. Taskar. 2007. Expec-
tation maximization and posterior constraints. In
Proc. NIPS.
W.P. Headden III, M. Johnson, and D. McClosky.
2009. Improving unsupervised dependency pars-
ing with richer contexts and smoothing. In Proc.
NAACL.
M. Johnson, T.L. Griffiths, and S. Goldwater. 2007.
Adaptor grammars: A framework for specifying
compositional nonparametric Bayesian models. In
Proc. NIPS.
Y. Kawata and J. Bartels. 2000. Stylebook for the
Japanese Treebank in VERBMOBIL. Technical re-
port, Eberhard-Karls-Universitat Tubingen.
D. Klein and C. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency
and constituency. In Proc. ACL.
M.T. Kromann, L. Mikkelsen, and S.K. Lynge. 2003.
Danish Dependency Treebank. In Proc. TLT.
P. Liang, S. Petrov, M.I. Jordan, and D. Klein. 2007.
The infinite PCFG using hierarchical Dirichlet pro-
cesses. In Proc. EMNLP.
P. Liang, M.I. Jordan, and D. Klein. 2009. Learn-
ing from measurements in exponential families. In
Proc. ICML.
G. Mann and A. McCallum. 2007. Simple, robust,
scalable semi-supervised learning via expectation
regularization. In Proc. ICML.
G. Mann and A. McCallum. 2008. Generalized expec-
tation criteria for semi-supervised learning of condi-
tional random fields. In Proc. ACL.
M. Marcus, M. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
R. Neal and G. Hinton. 1998. A new view of the EM
algorithm that justifies incremental, sparse and other
variants. In M. I. Jordan, editor, Learning in Graph-
ical Models, pages 355?368. MIT Press.
J. Nilsson, J. Hall, and J. Nivre. 2005. MAMBA meets
TIGER: Reconstructing a Swedish treebank from
antiquity. NODALIDA Special Session on Tree-
banks.
K. Oflazer, B. Say, D.Z. Hakkani-T?r, and G. T?r.
2003. Building a Turkish treebank. Treebanks:
Building and Using Parsed Corpora.
K. Simov, P. Osenova, M. Slavcheva, S. Kolkovska,
E. Balabanova, D. Doikoff, K. Ivanova, A. Simov,
E. Simov, and M. Kouylekov. 2002. Building a lin-
guistically interpreted corpus of bulgarian: the bul-
treebank. In Proc. LREC.
N. Smith and J. Eisner. 2006. Annealing structural
bias in multilingual weighted grammar induction. In
Proc. ACL.
L. Van der Beek, G. Bouma, R. Malouf, and G. Van No-
ord. 2002. The Alpino dependency treebank. Lan-
guage and Computers.
199
Rich Prior Knowledge in 
Learning for NLP
Gregory Druck, Kuzman Ganchev, Jo?o Gra?a
Why Incorporate Prior Knowledge?
have: unlabeled data
option: hire
linguist
annotators
Why Incorporate Prior Knowledge?
have: unlabeled data
option: hire
linguist
annotators
This approach does not 
scale to every task and 
domain of interest.
However, we already 
know a lot about most 
problems of interest.
Example: Document Classification 
?
Prior Knowledge: 
?
labeled features: information about the label 
distribution when word w is present
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
Documents
Labels
newsgroups classification
baseball Mac politics
...
hit Apple senate
...
Braves Macintosh taxes
...
runs Powerbook liberal
...
sentiment polarity
positive negative
memorable terrible
perfect boring
exciting mess
Example: Information Extraction
?
Prior Knowledge: 
?
labeled features: 
?
the word ACM should be labeled either journal or 
booktitle most of the time
?
non-Markov (long-range) dependencies:
?
each reference has at most one segment of each type
W. H. Enright. Improving the efficiency of matrix operations 
in the numerical solution of stiff ordinary differential 
equations. ACM Trans. Math. Softw., 4(2), 127-136, June 1978.
extraction from 
research papers:
Example: Part-of-speech Induction
?
Prior Knowledge: 
?
linguistic knowledge: each sentence should have a verb
?
posterior sparsity: the total number of different POS tags 
assigned to each word type should be small
Tags
A career with the European 
institutions must become more 
attractive. Too many young, new...
Text
Example: Dependency Grammar Induction
?
Prior Knowledge: 
?
linguistic rules: nouns are usually dependents of verbs
?
noisy labeled data: target language parses should be 
similar to aligned parses in a resource-rich source language 
Example: Word Alignment
?
Prior Knowledge: 
?
Bijectivity: alignment should be mostly one-to-one
?
Symmetry: source?target and target?source 
alignments should agree
A career with the European institutions must become more attractive. 
Uma carreira nas institui??es europeias t?m de se tornar mais atractiva. 
This Tutorial
In general, how can we leverage such knowledge 
and an unannotated corpus during learning?
Notation & Models
input variables (documents, sentences):
structured output variables (parses, sequences):
unstructured output variables (labels):
input / output variables for entire corpus: 
probabilistic model parameters:
generative models:
discriminative models:
model feature function:  
p ?(y|x)
p?(x,y)
x
y
?
f(x , y)
X Y
y
Learning Scenarios
?
Unsupervised: 
?
unlabeled data + prior knowledge
?
Lightly Supervised: 
?
unlabeled data + ?informative? prior knowledge
?
i.e. provides specific information about labels 
?
Semi-Supervised: 
?
labeled data + unlabeled data + prior knowledge
Running Example #1:
Document Classification
?
model: Maximum Entropy Classifier (Logistic Regression) 
?
setting: lightly supervised; no labeled data
?
prior knowledge: 
?
labeled features: information about the label 
distribution when word w is present
?
label is often hockey or baseball when game is present
p?(y|x) =
1
Z(x)
exp(? ? f(x, y))
Running Example #2:
Word Alignment
?
model: first-order Hidden Markov Model (HMM)
?
setting: unsupervised
?
prior knowledge: 
?
Bijectivity: alignment should be mostly one-to-one
1 1
2 3
we know
the way
sabemos       el       camino      null
1 2 3 0
p?(y,x) = p?(y0)
N?
i=1
p?(yi|yi?1)p?( x i|yi)
Problem
?
This output does not agree with prior knowledge!
?
 six target words align to source word animada
?
 five source words do not align with any target word 
gameconvivialvery,animatedanwasit
cordialmuyyanimadamaneraunadejugaban
model
data output
x
1
x
2
x
3
y
1
y
2
y
3
+
Limited Approach: Labeling Data
limitation: Often unclear how to do conversion
?
Example #1: often (not always) game ? {hockey,baseball} 
?
Example #2: alignment should be mostly one-to-one
prior 
knowledge
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
approach: Convert prior knowledge to labeled data.
Prototypes (+ cluster features):   
?
 [Haghighi & Klein 06]
Others: 
?
[Raghavan & Allan 07]       
?
[Schapire et al 02]
Limited Approach: Bayesian Approach
approach: Encode prior knowledge with a prior on parameters.
limitation: Our prior knowledge is not about parameters! 
Parameters are difficult to interpret; hard to get desired effect.
?
Example #1: often (not always) game ? {hockey,baseball}
?
Example #2: alignment should be mostly one-to-one
natural: ?   should be small (or sparse)??
( informative prior )
possible: ?    should be close to   ??i ??i
p (?)
specifying 
x
1
x
2
x
3
y
1
y
2
y
3
??
[Dayanik et al 06]
[Johnson 07], among many others
Limited Approach: Augmenting Model
limitation: can be difficult to get desired effect
?
Example #1: often (not always) game ? {hockey,baseball}
limitation: may make exact inference intractable
?
Example #2: Bijectivity makes inference #P-complete
x
1
x
2
x
3
y
1
y
2
y
3
z
1
approach: Encode prior knowledge with 
additional variables and dependencies.
This Tutorial
develop:
?
a language for directly encoding prior knowledge
?
methods for learning with knowledge in this language
?
( approximations to modeling this language directly )
?
(loosely) these methods perform mappings for us: 
?
encoded prior knowledge            parameters
?
encoded prior knowledge            labeling 
?
--- --- 
----- -- -- 
--- ---- 
--- --- --- 
--- --- 
----- -- -- 
--- ---- 
--- --- --- 
--- --- 
----- -- -- 
--- ---- 
--- --- --- 
?
?
A Language for Encoding Prior Knowledge
Our prior knowledge is about distributions over latent 
output variables. (output variables are interpretable)
Specifically, we know some properties of this distribution:
?
Example #1: often (not always) game?{hockey,baseball}
Formulation: know about the expectations of some 
functions under distribution over latent output variables
Constraint Features
?
constraint feature function: 
?
Example #1: 
?
for document x, returns a vector with a 1 in the lth 
position if y is the lth label and the word w is in x
?
Example #2: 
?
returns a vector with mth value = number of target 
words in sentence x that align with source word m
?(x , y )
?w(x, y) = 1 (y = l )1( w ? x)
?(x,y) =
N?
i=1
1(yi = m)
Expectations of Constraint Features
?
Example #1:  Corpus expectation: 
?
vector with expected distribution over labels for 
documents that contain w (     is the count of w)
?
Example #2:  Per-example expectation: 
?
vector with mth value = expected number of target 
words that align with source word m 
Ep? [?(X,Y)] =
1
c w
?
x
?
y
p?(y|x)?w(x, y)
E p ? [?(x,y)] =
?
y
p?(y|x)?(x,y)
c w
Expressing Preferences
?
express preferences using target values: 
?
Example #1:                           
?
label distribution for game is close to [40% 40% 20%]
?
Example #2:                           
?
expected number of target words that align with each 
source word is at most one
??
E p ? [? w ( X , Y )] ? ??
E p ? [?(x , y)] ? ??
Preview: Labeled Features
User Experiments [Druck et al 08]
0 100 200 300 400 500 600 700 8000.4
0.5
0.6
0.7
0.8
0.9
1
labeling time in seconds
tes
ting
 ac
cur
acy
 
 
GEER
~2 minutes, 100 
features labeled 
(or skipped): 
82% accuracy
~15 minutes, 100 
documents labeled 
(or skipped):
78% accuracy
PC vs. Mac
complete set of 
labeled features
PC Mac
dos mac
ibm apple
hp quadra
dx
targets set with 
simple heuristic: 
majority label gets 
90% of mass
Preview:  Word Alignment
[Gra?a et al 10]
60
68.75
77.5
86.25
95
En-Pt Pt-En En-Es Es-En
HMM HMM + Bijectivity Constraint
Overview of the Frameworks
Running Example
Model Family: conditional exponential models
                   are model features
p?(Y|X) =
exp( ? ? f(X,Y))
Z(X)
Z(X) =
?
Y
e x p( ? ? f(X , Y))
f( X , Y )
                Choosing parameters
Model Family: conditional exponential models
Objective: maximize observed data likelihood
Note: Frameworks also suitable for 
generative models (no labeled data necessary)   
?
p?(Y|X) =
exp( ? ? f(X,Y))
Z(X)
max
?
log p?( Y L | X L ) + log p(?)
d e f
= L (?; D L )
Visual Example: Maximum Likelihood
Model:                                      
Objective:
-
+
oo
o
o o
o o o
max
?
log p?( Y L | X L )? 0. 1???
2
2
p(Y|X) =
?
i
exp( y i x i ? ?)
Z ( x i)
A language for prior information
The expectations of user-defined constraint 
features             are close to some value 
?( X , Y ) ??
E [?( X , Y )] ? ??
Running Example:
Want to ensure that 25% of unlabeled 
documents are about politics
?
constraint features
 
?
preferred expected value
?
Expectation w.r.t. unlabeled data
?(x , y) =
?
1 if y is ?politics?
0 otherwise
?? = 0 . 25
Constraint-Driven Learning
Motivation: Hard EM algorithm with preferences
Hard EM: 
Constraint Driven Learning:
M - Step: set ? = arg max
?
log p ?(
?Y | X )
E-Step: set ?Y = argmax
Y
log p ?( Y | X )?penalty( Y )
M - Step: set ? = arg max
?
log p ?(
?Y | X )
E-Step: set ?Y = argmax
Y
log p ?( Y | X )
M. Chang, L. Ratinov, D. Roth (2007).
Constraint-Driven Learning
Motivation: Hard EM algorithm with preferences
Constraint Driven Learning:
?
penalties encode similar information as 
* more on this later *
?
E-Step can be hard; use beam search
E-Step: set ?Y = argmax
Y
log p ?( Y | X )?penalty( Y )
M - Step: set ? = arg max
?
log p ?(
?Y | X )
E [ ? ] ? ??
Visual Example: Constraint Driven Learning
    where     are ?imagined? labels and
?Y
-
+
oo
o
o o
o o o
?[ ?Y ] = count(+ , ?Y )
max
?,Y?
log p?( Y L| X L)? 0. 1???
2
2 s.t. ?(
?Y ) = 2
Posterior Regularization
Motivation: EM algorithm with sane posteriors
EM:
Constrained EM:
E-Step: set q ( Y ) = argmin
q
D KL ( q ( Y )||p?( Y | X ))
M-Step: set ? = argmax
?
E q ( Y ) [ p ?( Y | X )]
E-Step: set q ( Y ) = argmin
q ?Q
D KL ( q ( Y )||p?(y|x))
M-Step: set ? = argmax
?
E q ( Y )[ p ?( Y | X )]
J. Gra?a, K. Ganchev, B. Taskar (2007).
Posterior Regularization
Motivation: EM algorithm with sane posteriors
Idea:                  provide constraints
Objective:
E [ ? ] ? ??
define Q : set of q such that E q [?] ? ??
m ax
?
L(? ; D L )?D KL (Q || p?( Y | X ))
run EM-like procedure but use proposal q ? Q
where
D KL is Kullback-Leibler divergence
X = D U are the input variables for unlabeled corpus
Y is label for entire unlabeled corpus
Posterior Regularization
Hard constraints:
Soft constraints:
max
?
L(?; D L) ? min
q ?Q
D KL ( q ( Y )|| p ?( Y | X ))
Q =
?
q ( Y ) :
?
?
? E q [?( Y )] = ??
?
?
?
2
2
? ?
?
max
?
L(?;D L ) ? min
q
?
D KL (q( Y )|| p?( Y | X )) +
?
?
?
? E q [?( Y )] = ??
?
?
?
2
2
?
Visual Example: Posterior Regularization 
 where: 
-
+
oo
o
o o
o o o
max
?
log p?( Y L | X L )? 0. 1???
2
2 ? D KL ( Q|| p?)
D KL ( Q|| p?) = min
q
D KL ( q ||p?) s.t. E q [?] = 2
Generalized Expectation Constraints
Motivation: augment log-likelihood with cost for ?bad? 
posteriors.
Objective:
where
                                                                    is short-hand
Optimization: gradient descent on    
max
?
L (?; D L)?
?
?
? E p ? ( Y | X ) [?] ? ??
?
?
?
?
E p ? ( Y | X )
[?] = E p ? ( Y | X ) [?( X , Y )]
=
?
Y
p?( Y | X )?( X , Y )
?
G. Mann, A. McCallum (2007). 
A visual comparison of the frameworks
Objective: Generalized Expectation Constraints
-
+
oo
o
o o
o o o
max
?
log p?( Y L | X L ) ? 0. 1???
2
2 ? 500?E p ? [?] ? 2?
2
2
Types of constraints
Constraint Driven Learning: Penalized Viterbi
?
Easy if                           decompose as the model.
                 and
?
Otherwise:
?
Beam search
?
Integer linear program 
p ( Y | X ) =
?
c
p c (y c | X )
argmax
Y
log p?( Y | X ) ? ??( X , Y )? ????
??( X , Y )? ????
??( X , Y )? ???? =
?
c
? c ( X , y c )
Types of constraints
Posterior Regularization: KL projection
?
Usually easy if               decompose as the model:
     and
?
Otherwise: Sample (e.g. K. Bellare, G. Druck, and A. McCallum, 2009)
?( Y , X )
p ( Y | X ) =
?
c
p c (y c | X )
q ( Y | X ) =
?
c
q c (y c | X )
?( X , Y ) =
?
c
? c ( X , y c )
?
min
q
D KL ( q ||p?) s.t. ?E q [?] ? ???? ? ?
Types of constraints
Generalized Expectation Constraints: Direct gradient
?
Usually easy if:
?
decomposes as the model
?
Can compute                * more on this later *
?
Unstructured
?
Sequence, Grammar (semiring trick)
?
Otherwise: sample or approximate the gradient.
?( Y , X )
max
?
L (?; D L)?
?
?
? E p ? ( Y | X ) [?] ? ??
?
?
?
?
?( X , Y ) =
?
c
? c ( X , y c )
E [ ?? f ]
A Bayesian View: Measurements
Objective: mode of    given observations
X L ? X
Y L Y
?( X , Y )
b
Figure 4.1: The model used by Liang et al [2009], using our notation. We have separated
treatment of the labeled data (XL,YL) from treatment of the unlabeled data X.
and produce some value ?(X,Y), which is never observed directly. Instead, we observe
some noisy version b ? ?(X,Y). The measured values b are distributed according to
some noise model pN(b|?(X,Y)). Liang et al [2009] note that the optimization is convex
for log-concave noise and use box noise in their experiments, giving b uniform probability
in some range near ?(X,Y).
In the Bayesian setting, the model parameters ? as well as the observed measurement
values b are random variables. Liang et al [2009] use the mode of p(?|XL,YL,X,b) as a
point estimate for ?:
argmax
?
p(?|XL,YL,X,b) = argmax
?
?
Y
p(?,Y,b|X,XL,YL), (4.6)
with equality because p(?|XL,YL,X,b) ? p(?,b|XL,YL,X) =
?
Y p(?,Y,b|X,XL,YL). Liang et al [2009] focus on computing p(?,Y,b|X,XL,YL).
They define their model for this quantity as follows:
p(?,Y,b|X,XL,YL) = p(?|XL,YL) p?(Y|X) pN(b|?(X,Y)) (4.7)
where the Y and X are particular instantiations of the random variables in the entire unla-
beled corpusX. Equation 4.7 is a product of three terms: a prior on ?, the model probability
p?(Y|X), and a noise model pN(b|?). The noise model is the probability that we observe
a value, b, of the measurement features ?, given that its actual value was ?(X,Y). The
idea is that we model errors in the estimation of the posterior probabilities as noise in the
measurement process. Liang et al [2009] use a uniform distribution over ?(X,Y) ? ?,
which they call ?box noise?. Under this model, observing b farther than ? from ?(X,Y)
has zero probability. In log space, the exact MAP objective, becomes:
max
?
L(?) + logEp?(Y|X)
?
pN(b|?(X,Y))
?
. (4.8)
31
max
?
l og p(?) +
?
( x , y ) ? D L
l og p?( y |x) = L (? ; D L )
?
P. Liang, M. Jordan, D. Klein (2009)
Objective: mode of    given observations
A Bayesian View: Measurements
X L ? X
Y L Y
?( X , Y )
b
Figure 4.1: The model used by Liang et l. [2009], using our notation. We have separated
treatment of the labeled data (XL,YL) from treatment of the unlabeled data X.
and produce some value ?(X,Y), which is never observed directly. Instead, we observe
some noisy version b ? ?(X,Y). The measured values b are distributed according to
some noise model pN(b|?(X,Y)). Liang et al [2009] note that the optimization is convex
for log-concave noise and use box noise in their experiments, giving b uniform probability
in some range near ?(X,Y).
In the Bayesian setting, the model parameters ? as well as the observed measurement
values b are random variables. Liang et al [2009] use the mode of p(?|XL,YL,X,b) as a
point estimate for ?:
argmax
?
p(?|XL,YL,X,b) = argmax
?
?
Y
p(?,Y,b|X,XL,YL), (4.6)
with equality because p(?|XL,YL,X,b) ? p(?,b|XL,YL,X) =
?
Y p(?,Y,b|X,XL,YL). Liang et al [2009] focus on computing p(?,Y,b|X,XL,YL).
They define their model for this quantity as follows:
p(?,Y,b|X,XL,YL) = p(?|XL,YL) p?(Y|X) pN(b|?(X,Y)) (4.7)
where the Y and X are particular instantiations of the random variables in the entire unla-
beled corpusX. Equation 4.7 is a product of three terms: a prior on ?, the model probability
p?(Y|X), and a noise model pN(b|?). The noise model is the probability that we observe
a value, b, of the measurement features ?, given that its actual value was ?(X,Y). The
idea is that we model errors in the estimation of the posterior probabilities as noise in the
measurement process. Liang et al [2009] use a uniform distribution over ?(X,Y) ? ?,
which they call ?box noise?. Under this model, observing b farther than ? from ?(X,Y)
has zero probability. In log space, the exact MAP objective, becomes:
max
?
L(?) + logEp?(Y|X)
?
pN(b|?(X,Y))
?
. (4.8)
31
max
?
L (?;D L ) + log E p ? ( Y | X )
?
p(??|?( X , Y ))
?
?
What's wrong with this picture?
Objective: mode of    given observations
Example: Exactly 25% of articles are ?politics?
What is the probability exactly 25% of the articles are 
labeled ``politics''?
How do we optimize this with respect to  ?
max
?
L (? ; D L ) + log E p ?(Y | X)
?
p(??|?( X , Y ))
?
?
?
p(??|?( X , Y )) = 1
?
?? = ?( X , Y )
?
E p ?(Y | X)
?
1 (?? = ?(X , Y))
?
What's wrong with this picture?
Example: Compute prob:  25% of docs are ?politics?.
    Naively:
      in this case we can use a DP, but if 
there are many constraints, that doesn?t 
work.
Easier: What is the expected number of ?politics? articles?
Article p(?politics?)
1 0.2
2 0.4
3 0.1
4 0.6
0 . 2 + 0 . 4 + 0 . 1 + 0 . 6
0 . 2 ? (1 ? 0 . 4) ? (1 ? 0 . 1) ? (1 ? 0 . 6)
+ . . . +
+(1 ? 0 . 2) ? (1 ? 0 . 4) ? (1 ? 0 . 1) ? 0 . 6
Probabilities and Expectations
difficult to compute expectations of arbitrary functions but...
Usually:             decomposes as a sum
e.g. 25% of articles are ?politics?
Idea: approximate 
?( X , Y )
?(X , Y) =
?
instances
?( x , y )
E p ?(Y | X)
?
p
?
?? | ?( X , Y )
??
? p
?
?? | E p ?(Y | X) [?( X , Y )]
?
Probabilities and Expectations
Approximation:
Objective:
Example:                      is Gaussian     
                                              is 
so for appropriate                           this is identical to GE!
E p ? ( Y | X )
?
p
?
?? | ?
??
? p
?
?? | E p ? ( Y | X ) [ ? ]
?
max
?
L (?;D L ) + log p
?
?? | E p ?(Y | X) [?]
?
l og p
?
?? | E [?]
?
?
p
?
?? | E [ ? ]
?
l og p
?
?? | E [ ? ]
?
?
?
?
? E [ ? ] ? ??
?
?
?
2
2
Optimizing GE objective
GE Objective:
?
Gradient involves covariance
this can be hard because
and the usual dynamic programs (inside outside, forward 
backward) can?t compute this.
C ov( ? , f) = E [ ?? f ] ? E [ ? ] ? E [ f ]
E [?? f ] =
?
Y
p ( Y )?( Y )? f ( Y )
O GE = max
?
L (? ; D L )?
?
?
? E p ?(Y | X) [ ?( X , Y )] ? ??
?
?
?
?
Optimizing GE Objective
Maintaining both     and      in the DP is expensive
* Semiring trick can help for some problems *
x1 x2 x3 x3
y1 y2 y3 y4
E [?? f ] =
?
Y
p ( Y )?( Y )? f ( Y )
?( Y )? f ( Y ) =
?
?
i
?(yi)
?
?
?
?
?
j
f (y j )
?
?
yi y j
   E.g. if inference is a hypergraph problem.
A Variational Approximation
GE Objective:
?
Can be hard to compute                   in gradient. 
Idea: use variational approximation
* Note: this is the PR objective *
q ( Y ) ? p?( Y | X )
max? , q (Y) L(?; D L )?D KL
?
q ( Y ) || p?( Y | X )
?
?
?
?
? E q [?( X , Y )] ? ??
?
?
?
?
C ov( ? , f )
O GE = max
?
L (? ; D L )?
?
?
???? E p ?(Y|X) [ ?( X , Y )]
?
?
?
?
Approximating with the mode
PR Objective: 
sometimes minimizing the KL is hard.  
Idea: use hard assignment                               :
?
                                    becomes 
?
                                 becomes 
?
use EM-like procedure to optimize
Constraint Driven Learning Objective:
max? , q ( Y ) L(?; D L )?D KL
?
q ( Y ) || p?( Y | X )
?
?
?
?
? E q [?( X , Y )] ? ??
?
?
?
?
q ( Y ) ? 1 ( Y = ?Y )
?
?
? E q [?( X , Y )] ? ??
?
?
?
?
l og p ( ?Y )D KL
?
q ( Y ) || p?( Y | X )
?
l og p(?? | ?( X , ?Y ))
m ax
?, ?Y
L (? ; D L) + log p ?(
?Y ) + log p (??|?( X , ?Y ))
Visual Summary
Measurements
Generalized
Expectation
Distribution
Matching
Posterior
Regularization
Coupled Semi-
Supervised
Learning
Constraint
Driven
Learning
variational approximation;
Jensen?s inequality
variational
approximation
MAP
approximation
MAP
approximation
logE[ p N (??|?)] ? log p N (??|E[?])
Applications
?
Unstructured problems:
?
Document Classification
?
Sequence problems:
?
Information Extraction
?
Pos-Induction 
?
Word Alignment
?
Tree problems:
?
Grammar Induction
Document Classification
?
Model: Max. Entropy Classifier (Logistic Regression)
?
Challenge: What if we have no labeled data?
?
cannot use standard unsupervised learning:
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
Documents
Labels
p ?(y|x) =
exp(? ? f(x, y))
?
y
exp(? ? f(x, y))
?
y
p?( y | x )= 1
Labeled Features
?
often we can still provide some light supervision
?
prior knowledge: labeled features
?
formally: have an estimate of the distribution over labels 
for documents that contain word w:
?? w
newsgroups classification
baseball Mac politics
...
hit Apple senate
...
Braves Macintosh taxes
...
runs Powerbook liberal
...
sentiment polarity
positive negative
memorable terrible
perfect boring
exciting mess
Leveraging Labeled Features with GE
[Mann & McCallum 07], [Druck et al 08]
?
constraint feature: 
?
for a document x, returns a vector with a 1 in the lth 
position if y is the lth label and the word w is in x
?
expectation: label distribution for docs that contain w
?
GE penalty: KL divergence from target distribution
? w (x, y) = 1 (y = l )1( w ? x)
1
c w
?
x
E p? ( y | x ) [ ?w(x, y)]
D KL
?
??w||
1
c w
?
x
Ep?(y | x)[?w(x , y )]
?
User Experiments with Labeled Features
[Druck et al 08]
0 100 200 300 400 500 600 700 8000.4
0.5
0.6
0.7
0.8
0.9
1
labeling time in seconds
tes
ting
 ac
cur
acy
 
 
GEER
~2 minutes, 100 
features labeled 
(or skipped): 
82% accuracy
~15 minutes, 100 
documents labeled 
(or skipped):
78% accuracy
PC vs. Mac
complete set of 
labeled features
PC Mac
dos mac
ibm apple
hp quadra
dx
targets set with 
simple heuristic: 
majority label gets 
90% of mass
Experiments with Labeled Features
[Druck et al 08]
60
65
70
75
80
sentiment (50) webkb (100) newsgroups (500)
GE (model contains only labeled features)
GE (model also contains unlabeled features)
15x
3.5x
6.5x
learning about ?unlabeled features? through 
covariance improves generalization
estimated speed-up over 
labeling documents
Information Extraction: Example Tasks
?
citation extraction: 
?
apartment listing extraction: 
Detached single family house. 3 bedrooms 1 1/2 baths.  Almost 
1000 square feet in living area. 1 car garage. New pergo floor 
and tile kitchen floor. New interior/exterior paint. Close to 
shopping mall and bus stop. Near 101/280. Available July 1, 
2004. If you are interested, email for more details.
Cousot, P. and Cousot, R. 1978. Static determination of 
dynamic properties of recursive procedures. In Proceedings of 
the IFIP Conference on Programming Concepts, E. Neuhold, 
Ed. North-Holland Pub. Co., 237-277.
Information Extraction: Markov Models
?
models for sequence labeling based IE
?
Hidden Markov Model (HMM):  
?
Conditional Random Field (CRF):  
p?(y,x) = p?(y0)
N?
i=1
p?(yi|yi?1)p?( x i|yi)
p?(y|x) =
1
Z(x)
exp(
N?
i =1
? ? f (x, yi? 1 , yi))
expectation:
label distribution when q is true
model: Linear Chain CRF
note: Semiring trick makes GE 
O(L
2
) instead of O(L
3
) as in 
[Mann & McCallum 08]
Information Extraction: Labeled Features
[Mann & McCallum 08], [Liang et al 09]
ROOMMATES respectful
CONTACT *phone*
FEATURES laundry
apartments example 
labeled features:
1
c q
?
x
?
i
E p?(yi | x )[?q(x, yi, i )]
constraint features:
vector with a 1 in the lth 
position if y is the lth label 
and predicate q is true (i.e. w 
is present at i)
? q (x, yi , i) = 1 (y i = l)q(x, i)
Information Extraction: Labeled Features
[Haghighi & Klein 06], [Mann & McCallum 08], [Liang et al 09]
apartment listing extraction
Prototype
GE (KL)
Measurements/PR
650
700
750
800
850
0 labeled 10 labeled 100 labeled
supervised CRF (100) [MM08]
?
accurate with constraints alone 
?
outperform fully supervised with 
constraints and labeled data
Limitations of Markov Models
?
predicted: 
?
prediction has two author and two title segments:
?
error #1: Neuhold, Ed. should be editor
?
error #2: North-Holland Pub. Co., should be 
publisher
?
A Markov model cannot represent that at most one segment 
of each type appears in each reference.
Cousot, P. and Cousot, R. 1978. Static determination of 
dynamic properties of recursive procedures. In Proceedings of 
the IFIP Conference on Programming Concepts, E. Neuhold, 
Ed. North-Holland Pub. Co., 237-277.
Long-Range Constraints
[Chang et al 07] [Bellare et al 09]
?
?Each field is a contiguous sequence of tokens and appears 
at most once in a citation.?
?
constraint feature: counts the number of segments of 
each type
?
constrained to be ? 1 using PR or CODL
?
additional constraints: 10 labeled features such as:
?
pages?pages 
?
proc.?booktitle
Long-Range Constraints
[Chang et al 07] [Bellare et al 09]
constraints improve both 
CRF (PR) and HMM (CODL)
50
60
70
80
90
5 labeled 20 labeled
CRF CRF + PR
HMM HMM + CODL
citation model method description
[Mann et al 07] MaxEnt GE
constraints on 
label marginals
[Druck et al 09] CRF GE
actively labeled 
features
[Bellare & 
McCallum 09] 
alignment 
CRF
GE labeled features
[Singh et al 10] 
semi-Markov 
CRF
PR labeled gazetteers 
[Druck et al 10] HMM PR
constraints derived 
from labeled data
Other Applications in 
Information Extraction
Pos Induction
Low Tag Ambiguity
[Gra?a et al 09] 
JJ
VB
NN
car
object
romantic
offensive
being
E[degree] = 1.5E[degree] = 10000  0 2 4 6 8 10  0  200  400  600  800  1000 1200 1400 1600 1800L 1L ! rank of word by L1L!SupervisedHMMDistribution of word ambiguity
N V ADJ Prep ADV
0.9 0.1 0 0 0
0.7 0.1 0.1 0 0.1
0.1 0.3 0 0.6 0
0.3 0.6 0 0 0.1
0.3 0.7 0 0 0
?
Pick a particular word type: run
?
Stack all occurrences
?
Calculate posterior probability 
?
Take the maximum for each tag
?
Sum the maxes
a run into town.
of the mile run.
run gold.
run errands.
run for mayor.
Sum
1
Sum
1
1
1
1
0.9 0.7 0.1 0.6 0.2
Max
Sum
2.5
Measuring Tag Ambiguity
[Gra?a et al 09] 
?wti :Word type w  has hidden state t at occurrence i
m i n
cwt
E q ( y ) [?wti] ? c wt
?1 / ?? =
?
w t
c w t
Tag Sparsity
[Gra?a et al 09] 
0
1.25
2.5
3.75
5
En Pt Es
A
m
b
i
g
u
i
t
y
 
d
i
f
f
e
r
e
n
c
e
HMM L1LMax
Average ambiguity 
difference 0 2 4 6 8 10  0  200  400  600  800  1000 1200 1400 1600 1800L 1L ! rank of word by L1L!SupervisedHMMHMM+SpDistribution of word ambiguity
Results
[Gra?a et al 09] 
50
57.5
65
72.5
80
En Pt Bg Es Dk Tr
HMM HMM+Sp
3.8
6.7
7.4
7.6 9.6
3.8
6.5 % Average Improvement
Word Alignments
[Gra?a et al 10] 
?
Bijectivity constraints:
?
Each word should align to at most one other word
?
Symmetry constraints:
?
Directional models should agree
Bijectivity Constraints
[Gra?a et al 10]
Bijective Constraints
0 1 2 3 4 5 6 7 8
0 ? ? ? ? ? ? ? ? ? jugaban ? ? 1
1 ? ? ? ? ? ? ? ? ? de ? ? 1
2 ? ? ? ? ? ? ? ? ? una ? ? 1
3 ? ? ? ? ? ? ? ? ? manera ? ? 1
4 ? ? ? ? ? ? ? ? ? animada ? ? 1
5 ? ? ? ? ? ? ? ? ? y ? ? 1
6 ? ? ? ? ? ? ? ? ? muy ? ? 1
7 ? ? ? ? ? ? ? ? ? cordial ? ? 1
8 ? ? ? ? ? ? ? ? ?. ? ? 1
it was an animated
, very convivial
game
.
50 / 74
Bijective Constraints - After projection
0 1 2 3 4 5 6 7 8
0 ? ? ? ? ? ? ? ? ? jugaban ? ? 1
1 ? ? ? ? ? ? ? ? ? de ? ? 1
2 ? ? ? ? ? ? ? ? ? una ? ? 1
3 ? ? ? ? ? ? ? ? ? manera ? ? 1
4 ? ? ? ? ? ? ? ? ? animada ? ? 1
5 ? ? ? ? ? ? ? ? ? y ? ? 1
6 ? ? ? ? ? ? ? ? ? muy ? ? 1
7 ? ? ? ? ? ? ? ? ? cordial ? ? 1
8 ? ? ? ? ? ? ? ? ? . ? ? 1
it was an animated
, very convivial
game
.
51 / 74Feature: 
Constraint: 
?(x,y) =
N?
i=1
1(yi = m)
E q [?(x , y)] ? 1
Symmetry Constraints
[Gra?a et al 10]
Feature:
Constraint: 
Sym metric - Original posteriors
0 1 2 3 4
??p ?t (z | x)
0 ? ? ? ? ? no
1 ? ? ? ? ? hay
2 ? ? ? ? ? estad??sticas
3 ? ? ? ? ?.
0 1 2 3 4
??p ?t (z | x)
0 ? ? ? ? ? no
1 ? ? ? ? ? hay
2 ? ? ? ? ? estad??sticas
3 ? ? ? ? ?.
no statistical
data
exists
.
p ? t
q
??
p ? t
??
p ? t
55 / 74
Eq [ ?(x , y)] = 0
?(x, y ) =
?
??
??
+ 1 y ?
??
y and
??
y i = j
? 1 y ?
??
y and
??
y j = i
0 otherwise
??
p ?( y | )
??
p ?( y )
Symmetry Constraints
[Gra?a et al 10]
Before projection: After projection:
Symmetric - After projection
E-Step qs(z) = arg min
q(z)? Q s
KL [qs(z) || p?t (z | xs)]
0 1 2 3 4
??p ?t (z | x)
0 ? ? ? ? ? no
1 ? ? ? ? ? hay
2 ? ? ? ? ? estad??sticas
3 ? ? ? ? ?.
0 1 2 3 4
??p ?t (z | x)
0 ? ? ? ? ? no
1 ? ? ? ? ? hay
2 ? ? ? ? ? estad??sticas
3 ? ? ? ? ?.
no statistical
data
exists
.
0 1 2 3 4
0 ? ? ? ? ? no
??q (z)1
? ? ? ? ? hay
2 ? ? ? ? ? estad??sticas
3 ? ? ? ? ?.
0 1 2 3 4
0 ? ? ? ? ? no
??q (z)1
? ? ? ? ? hay
2 ? ? ? ? ? estad??sticas
3 ? ? ? ? ?.
no statistical
data
exists
.
M-Step Does not change
56 / 74
??
p ?( y |x)
??
p ?( y |x
?
q ( y )
?
q ( y )
Results
[Gra?a et al 10]
 50 60 70 80 90 100 1000  10000  100000  1e+06precision size S-HMMB-HMMHMM 50 60 70 80 90 100 1000  10000  100000  1e+06precision size S-HMMB-HMMHMM
Evolution with data size 50 60 70 80 90 100 1000  10000  100000  1e+06precision size S-HMMB-HMMM4HMM  50 60 70 80 90 100 1000  10000  100000  1e+06precision size S-HMMB-HMMM4HMM
? Specially useful for low data situations
6 1 / 74
Evolution with data size 50 60 70 80 90 100 1000  10000  100000  1e+06precision size S-HMMB-HMMM4HMM  50 60 70 80 90 100 1000  10000  100000  1e+06precision size S-HMMB-HMMM4HMM
? Specially useful for low data situations
6 1 / 74
Results
[Gra?a et al 10]
 60 65 70
 75 80 85
 90 95
En-Pt Pt-En Pt-Fr Fr-Pt En-Es Es-En Es-Fr Fr-Es Pt-Es Es-Pt En-Fr Fr-EnLanguagesHMM70.5 67.5
73.0 77.6 75.7 74.9 80.9 84.0 82.4 79.8 76.3 78.3
B-HMM
85.0 74.4 71.3
86.3 88.4 87.2 87.2 86.5 82.5 90.1 90.8 91.6
S-HMM
86.2 85.0 82.4 87.9 82.7 84.6 89.1 88.9 84.6 91.8 93.4 9
4.6
Dependency Parsing
DMV Model
[Gra?a et al 04]
Dependency model with valence
(Klein and Manning, ACL 2004)
x
y
Regularization
N
creates
V
sparse
ADJ
grammars
N
p?(x, y) = ?root(V )
? ?stop(nostop|V ,right,false) ? ?child(N|V ,right)
? ?stop(stop|V ,right,true) ? ?stop(nostop|V ,left,false) ? ?child(N|V ,left)
. . .
3/9
Dependency Parsing
?
Transfer annotations from another language
?
[Ganchev et al 09]
?
Constrain the number of child/parent 
relations
?
[Gillenwater et al 11]
?
Use linguistic rules
?
[Druck et al 09] [Naseem et al 10]
Dependency Parsing
Transfer annotations
[Ganchev et al 09]
?
Use information from a resource rich 
language
?
Make the annotation transfer robust
?
Preserve n % of the edges
Dependency Parsing
Transfer annotations
[Ganchev et al 09]
E q [?(x,y)] =
1
| C
x
|
?
y? C x
q (y|x)
E q [ ?(x,y)] ? b
Dependency Parsing
Transfer annotations
[Ganchev et al 09]
66
67
68
69
70
ES BG
DMV PR-Transfer
Dependency Parsing
Posterior Sparsity
[Gra?a et al 10]
?
ML learns very ambiguous grammars
?
all productions have some probability
?
constrain the number of possible 
productions
Dependency Parsing
Posterior Sparsity
[Gillenwater et al 11]
Measuring ambiguity on distributions over trees
N
?
N
V
?
N
AD
J
?
N
N
?
V
V
?
V
AD
J
?
V
N
?
AD
J
V
?
AD
J
AD
J
?
AD
J
SparsityN isV workingV
0.40.6 0 1 0
SparsityN isV workingV
0.4 0.6 .4 .6 0
UseV goodADJ grammarsN
0.70.3 0 .7 .3
UseV goodADJ grammarsN
0.40.6 .4 .6 0
max ?
sum = 3.3 ? 0 1 .3 .4 .6 0 .4 .6 0
7/9
Dependency Parsing
Posterior Sparsity
[Gillenwater et al 11]
GILLENWATER, GANCHEV, GRA?A, PEREIRA, TASKAR
Una
d
papelera
nc
es
vs
un
d
objeto
nc
civilizadoaq
Una
d
papelera
nc
es
vs
un
d
objeto
nc
civilizadoaq
1.00
1.00 1.000.49
0.51
1.00
0.57
0.43
Una
d
papelera
nc
es
vs
un
d
objeto
nc
civilizadoaq
1.00 0.83 0.75 0.990.92
0.35
0.48
Figure 14: Posterior edge probabilities for an example sentence from the Spanish test corpus. Top
is Gold, middle is EM, and bottom is PR.
since then it does not have to pay the cost of assigning a parent with a new tag to cover each noun
that does not come with a determiner.
Table 4 contrasts the most frequent types of errors EM, SDP, and PR make on several test sets
where PR does well. The ?acc? column is accuracy and the ?errs? column is the absolute number
of errors of the key type. Accuracy for the key ?parent POS truth/guess? child POS? is computed
as a function of the true relation. So, if the key is pt /p g ? c , then accuracy is:
acc =
# of pt ? c in Viterbi parses
# of pt ? c in gold parses
. (25)
In the following subsections we provide some analysis of the results from Table 4.
7.1 English Corrections
Considering English first, there are several notable differences between EM and PR errors. Similar
to the example for Spanish, the direction of the noun-determiner relation is corrected by PR. This is
reflected by the VB/DT? NN key, the NN/VBZ? DT key, the NN/IN? DT key, the IN/DT?
NN key, the NN/VBD? DT key, the NN/VBP? DT key, and the NN/VB? DT key, which for
EM and SDP have accuracy 0. PR corrects these errors.
A second correction PR makes is reflected in the VB/TO? VB key. One explanation for the
reason PR is able to correctly identify VBs as the parents of other VBs instead of mistakenly making
TO the parent of VBs is that ?VB CC VB? is a frequently occurring sequence. For example, ?build
and hold? and ?panic and bail? are two instances of the ?VB CC VB? pattern from the test corpus.
Presented with such scenarios, where there is no TO present to be the parent of VB, PR chooses the
first VB as the parent of the second. It maintains this preference for making the first VB a parent of
the second when encountered with ?VB TO VB? sequences, such as ?used to eliminate?, because it
would have to pay an additional penalty to make TO the parent of the second VB. In this manner,
PR corrects the VB/TO? VB key error of EM and SDP.
26
Gold:
DVM:
DMV+Sparsity:
Dependency Parsing
Posterior Sparsity
[Gillenwater t al. 11]
0
17.5
35
52.5
70
English Bulgarian Portuguese Checz Spanish German
DMV DMV+Sparsity
Dependency Parsing
Linguistic Rules
[Naseem et al 10]
Using Universal Linguistic Knowledge to Guide Grammar Induction
Tahira Naseem, Harr Chen, Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{ tahira, harr, regina} @csail.mit.edu
Mark Johnson
Department of Computing
Macquarie University
mark.johnson@mq.edu.au
Abstract
We present an approach to grammar induc-
tion that utilizes syntactic universals to im-
prove dependency parsing across a range of
languages. Our method uses a single set
of manually-specified language-independent
rules that identify syntactic dependencies be-
tween pairs of syntactic categories that com-
monly occur across languages. During infer-
ence of the probabilistic model, we use pos-
terior expectation constraints to require that a
minimum proportion of the dependencies we
infer be instances of these rules. We also auto-
matically refine the syntactic categories given
in our coarsely tagged input. Across six lan-
guages our approach outperforms state-of-the-
art unsupervised methods by a significant mar-
gin.1
1 Introduction
Despite surface differences, human languages ex-
hibit striking similarities in many fundamental as-
pects of syntactic structure. These structural corre-
spondences, referred to as syntactic universals, have
been extensively studied in linguistics (Baker, 2001;
Carnie, 2002; White, 2003; Newmeyer, 2005) and
underlie many approaches in multilingual parsing.
In fact, much recent work has demonstrated that
learning cross-lingual correspondences from cor-
pus data greatly reduces the ambiguity inherent in
syntactic analysis (Kuhn, 2004; Burkett and Klein,
2008; Cohen and Smith, 2009a; Snyder et al, 2009;
Berg-Kirkpatrick and Klein, 2010).
1The source code for the work presented in this paper is
available at http://groups.csail.mit.edu/rbg/code/dependency/
Root? Auxiliary Noun? Adjective
Root? Verb Noun? Article
Verb? Noun Noun? Noun
Verb? Pronoun Noun? Numeral
Verb? Adverb Preposition? Noun
Verb? Verb Adjective? Adverb
Auxiliary? Verb
Table 1: The manually-specified universal dependency
rules used in our experiments. These rules specify head-
dependent relationships between coarse (i.e., unsplit)
syntactic categories. An explanation of the ruleset is pro-
vided in Section 5.
In this paper, we present an alternative gram-
mar induction approach that exploits these struc-
tural correspondences by declaratively encoding a
small set of universal dependency rules. As input
to the model, we assume a corpus annotated with
coarse syntactic categories (i.e., high-level part-of-
speech tags) and a set of universal rules defined over
these categories, such as those in Table 1. These
rules incorporate the definitional properties of syn-
tactic categories in terms of their interdependencies
and thus are universal across languages. They can
potentially help disambiguate structural ambiguities
that are difficult to learn from data alone ? for
example, our rules prefer analyses in which verbs
are dependents of auxiliaries, even though analyz-
ing auxiliaries as dependents of verbs is also consis-
tent with the data. Leveraging these universal rules
has the potential to improve parsing performance
for a large number of human languages; this is par-
ticularly relevant to the processing of low-resource
Small set of 
universal rules
= 1 if edge in rule set
E q [ ?(x,y)] ? b
?(x , y)
Dependency Parsing
Linguistic Rules
[Nas em et al 10]
0
20
40
60
80
English Danish Portuguese Slovene Spanish Swedish
DMV DMV+Rules
Dependency Parsing:
Applications using Other Models
?
Tree CRF
?
[Druck et al 09]
?
MST Parser
?
[Ganchev et al 09]
Other Applications
?
Multi view learning:
?
[Ganchev et al 08]
?
Relation extraction:
?
[Chen et al 11]
Implementation Tips and Tricks
Off-the-Shelf Tools: MALLET
http://mallet.cs.umass.edu
?
off-the-shelf support for labeled features
?
models: MaxEnt Classifier, Linear Chain CRF (one and two 
label constraints)
?
methods: GE and PR
?
constraints on label distributions for input features
?
GE penalties:  KL divergence,     (+ soft inequalities)
?
PR penalties:     (+ soft inequalities)
?
in development: Tree CRF,      and other penalties
?
2
2
?
2
2
?1
Off-the-Shelf Tools: MALLET
http://mallet.cs.umass.edu
?
import data in SVMLight-like or CoNLL03-like formats
?
import constraints in a simple text format:
?
easily specify method options (i.e. SimpleTagger):
positive interesting:2 film:1 ...
negative tired:1 sequel:1 ...
positive best:1 recommend:2 ...
U.N.       NNP  B-NP  B-ORG 
official   NN   I-NP  O 
heads      VBZ  B-VP  O 
tired negative:0.8 positive:0.2
best positive:0.9 negative:0.1
U.N. B-ORG:0.7,0.9
B-VP O:0.95,
java cc.mallet.fst.semi_supervised.tui.SemiSupSimpleTagger \
--train true --test lab --loss l2 --learning ge \
unlabeled.txt test.txt constraints.txt
New GE Constraints: MALLET
http://mallet.cs.umass.edu
?
Java Interfaces for implementing new GE constraints
?
covariance computation implemented (MaxEnt, CRF)
?
primarily need to write methods to:
?
restriction: constraints must factor with model
?
restriction: GE objective must be differentiable
compute constraint features and expectations
compute GE objective value
compute GE objective gradient (but not covariance)
New PR Constraints: MALLET
http://mallet.cs.umass.edu
?
Java Interfaces for implementing new PR constraints
?
inference algorithms implemented (MaxEnt, CRF)
?
primarily need to write methods for E-step (projection):
?
restriction: constraints must factor with model
compute constraint features and expectations
compute scores under q for E-step
compute objective function for E-step
compute gradient for E-step
GE Implementation Advice
?
computing covariance (required for gradient): 
?
trick: compute cov. of composite constraint feature
?
example:     penalty: 
?
result: only need to store vectors of size            in 
computation, rather than covariance matrix
?
trick: efficient gradient computation in hypergraphs
?
use semiring algorithms of [Li & Eisner 09] 
?
result: same time complexity as supervised (w. both)
? c (x , y) =
?
?
2( ??? E [?])?(x , y)?22
d i m( f )
GE Implementation Advice
?
parameter regularization: 
?
    regularization encourages bootstrapping by penalizing 
very large parameter values:
?
optimization: non-convex
?
usually L-BFGS still preferable (use ?restart trick?)
?
zero initialization usually works well
?
other init: supervised, MaxEnt, GE in simpler model
?
2
2
>
Off-the-Shelf Tools: PR Toolkit
http://code.google.com/p/pr-toolkit/
?
off-the-shelf support for PR
?
models:  
?
MaxEnt Classifier, HMM,DMV
?
applications:  
?
Word Alignment, Pos Induction, Grammar Induction
?
constraints: posterior sparsity, bijectivity, agreement
?
No command line mode
?
Smaller support base
PR Implementation example:
Word Alignment - Bijectivity
?
Learning: EM, PR
?
void eStep(counts, lattices);
?
void mStep(counts);
?
lattice constraint.project(lattice);
?
Model: HMM
?
lattice computePosteriors(lattice);
?
void addCount(lattice, counts);
?
void updateParameters(counts);
?
Constraints: Bijectivity
?
lattice project(lattice);
PR Implementation example:
EM
class EM {
 model;
 	
void em(n){
 lattices= model.getLattices();
 counts = model.counts();	 	 	
 for(i=0; i< n; i++) {	 	 	
	 eStep(counts, lattices);
	 mStep(counts);
 }
}
	
void eStep(counts, lattices) {	
	 counts.clear();
	 for(l : lattices)  {		 	
	  model.computePosterior(l);
	  model.addCount(l,counts);	
	 }
}	
void mStep(counts) {
	 model.updateParameters(counts);
}
......
}
PR Implementation example:
PR
class PR {
 model;
 constraint;
	
void em(n){
 lattices= model.getLattices();
 counts = model.counts();	 	 	
 for(i=0; i< n; i++) {	 	 	
	 eStep(counts, lattices);
	 mStep(counts);
 }
}
	
void eStep(counts, lattices) {	
	 counts.clear();
	 for(l : lattices){	 	 	
	  model.computePosterior(l);
    constraint.project(l);
	  model.addCount(l,counts);	
	 }
}	
void mStep(counts) {
	 model.updateParameters(counts);
}
......
}
PR Implementation example:
HMM
class HMM {
 obsProb, transProbs,initProbs;
	
lattice computerPosteriors(lattice){
 ?Run forward backward?
}
	
void addCount(lattice,counts){
 ?Add posteriors to count table?
}
void updateParams(counts){
 ?Normalize counts?
 ?Copy counts to params table?
}
void getCounts(){
 ?return copy of params structures?
}
void getLattices(){
 ?return structure of all lattices 
in the corpus?
}
......
}
PR Implementation example:
Bijective constraints
?
Constraint: returns a vector with mth value = number of 
target words in sentence x that align with source word m
?(x,y) =
N?
i=1
1(yi = m) Q = { q : E q [?(x,y)] ? 1}
?
Primal: Hard
D KL ( Q| p?) = arg min
q
D KL ( q |p?)
?
Dual: Easy
arg max
?? 0
? b T ? ?? l og Z (?)? ||?|| 2
Z (?) =
?
y
p?( y |x) exp(?? ? ?(x , y ))
PR Implementation example:
Bijective Constraints
class BijectiveConstraints {
model;
lattice project(lattice){
 obj = BijectiveObj(model,lattice);
 Optimizer.optimize(obj);
}
	
}
class BijectiveObj {
  lattice;
  
void updateModel(newLambda){
 lattice_ = lattice*exp(newLambda);
 computerPosteriors(lattice)
}
double getObj(){
  obj = -dot(lambda,b);
  obj -= lattice.likelihood;
  obj -= l2Norm(lambda);
}
double[] getGrad(){
 grad = lattice.posteriors - b;
 grad -= norm(lambda);
 return grad;
}
Other Software Packages
?
Learning Based Java:  
?
http://cogcomp.cs.illinois.edu/page/software_view/11
?
support for Constraint-Driven Learning
?
Factorie:   
?
http://code.google.com/p/factorie/
?
support for GE and PR in development
Rich Prior Knowledge in Learning for Natural
Language Processing
Bibliography
For a more up-to-date bibliography as well as additional information about
these methods, point your browser to: http://sideinfo.wikkii.com/
1 Constraint-Driven Learning
Constraint driven learning (CoDL) was first introduced in Chang et al [2007],
and has been used also in Chang et al [2008]. A further paper on the topic is
in submission [Chang et al, 2010].
2 Generalized Expectation
Generalized Expectation (GE) constraints were first introduced by Mann and
McCallum [2007] 1 and were used to incorporate prior knowledge about the label
distribution into semi-supervised classification. GE constraints have also been
used to leverage ?labeled features? in document classification [Druck et al, 2008]
and information extraction [Mann and McCallum, 2008, Druck et al, 2009b,
Bellare and McCallum, 2009], and to incorporate linguistic prior knowledge
into dependency grammar induction [Druck et al, 2009a].
3 Posterior Regularization
The most clearly written overview of Posterior Regularization (PR) is Ganchev
et al [2010]. PR was first introduced in Graca et al [2008], and has been
applied to dependency grammar induction [Ganchev et al, 2009, Gillenwater
et al, 2009, 2011, Naseem et al, 2010], part of speech induction [Grac?a et al,
2009a], multi-view learning [Ganchev et al, 2008], word alignment [Graca et al,
2008, Ganchev et al, 2009, Grac?a et al, 2009b], and cross-lingual semantic
alignment [Platt et al, 2010]. The framework was independently discovered
by Bellare et al [2009] as an approximation to GE constraints, under the name
Alternating Projections, and used under that name also by Singh et al [2010]
and Druck and McCallum [2010] for information extraction. The framework
was also independently discovered by Liang et al [2009] as an approximation to
1In Mann and McCallum [2007] the method was called Expectation Regularization.
a Bayesian model motivated by modeling prior information as measurements,
and applied to information extraction.
4 Closely related frameworks
Quadrianto et al [2009] introduce a distribution matching framework very
closely related to GE constraints, with the idea that the model should pre-
dict the same feature expectations on labeled and undlabeled data for a set of
features, formalized as a kernel.
Carlson et al [2010] introduce a framework for semi-supervised learning
based on constraints, and trained with an iterative update algorithm very similar
to CoDL, but introducing only confident constraints as the algorithm progresses.
Gupta and Sarawagi [2011] introduce a framework for agreement that is
closely related to the PR-based work in Ganchev et al [2008], with a slightly
different objective and a different training algorithm.
References
K. Bellare, G. Druck, and A. McCallum. Alternating projections for learning
with expectation constraints. In Proc. UAI, 2009.
Kedar Bellare and Andrew McCallum. Generalized expectation criteria for boot-
strapping extractors using record-text alignment. In EMNLP, pages 131?140,
2009.
Andrew Carlson, Justin Betteridge, Richard C. Wang, Estevam R. Hruschka Jr.,
and Tom M. Mitchell. Coupled Semi-Supervised Learning for Information
Extraction. In Proceedings of the Third ACM International Conference on
Web Search and Data Mining (WSDM), 2010.
M. Chang, L. Ratinov, and D. Roth. Guiding semi-supervision with constraint-
driven learning. In Proc. ACL, 2007.
Ming-Wei Chang, Lev Ratinov, and Dan Roth. Structured learning with con-
strained conditional models. 2010. In submission.
M.W. Chang, L. Ratinov, N. Rizzolo, and D. Roth. Learning and inference
with constraints. In Proceedings of the National Conference on Artificial
Intelligence (AAAI). AAAI, 2008.
G. Druck, G. Mann, and A. McCallum. Learning from labeled features using
generalized expectation criteria. In Proc. SIGIR, 2008.
G. Druck, G. Mann, and A. McCallum. Semi-supervised learning of dependency
parsers using generalized expectation criteria. In Proc. ACL-IJCNLP, 2009a.
Gregory Druck and Andrew McCallum. High-performance semi-supervised
learning using discriminatively constrained generative models. In Proceedings
of the International Conference on Machine Learning (ICML 2010), pages
319?326, 2010.
Gregory Druck, Burr Settles, and Andrew McCallum. Active learning by label-
ing features. In EMNLP, pages 81?90, 2009b.
K. Ganchev, J. Grac?a, J. Blitzer, and B. Taskar. Multi-view learning over
structured and non-identical outputs. In Proc. UAI, 2008.
K. Ganchev, J. Gillenwater, and B. Taskar. Dependency grammar induction via
bitext projection constraints. In Proc. ACL-IJCNLP, 2009.
Kuzman Ganchev, Joo Graa, Jennifer Gillenwater, and Ben Taskar. Posterior
sparsity in unsupervised dependency parsing. Journal of Machine Learn-
ing Research, 11:2001?2049, July 2010. URL http://jmlr.csail.mit.edu/
papers/v11/ganchev10a.html.
Jennifer Gillenwater, Kuzman Ganchev, Joo Graa, Ben Taskar, and Fernando
Pereira. Sparsity in grammar induction. In NIPS Workshop on Grammar
Induction, Representation of Language and Language Learning, 2009.
Jennifer Gillenwater, Kuzman Ganchev, Joo Graa, Fernando Pereira, and Ben
Taskar. Posterior sparsity in unsupervised dependency parsing. Journal of
Machine Learning Research, 12:455?490, February 2011. URL http://jmlr.
csail.mit.edu/papers/v12/gillenwater11a.html.
Joao Graca, Kuzman Ganchev, and Ben Taskar. Expectation maximization
and posterior constraints. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis,
editors, Advances in Neural Information Processing Systems 20, pages 569?
576. MIT Press, Cambridge, MA, 2008.
J. Grac?a, K. Ganchev, F. Pereira, and B. Taskar. Parameter vs. posterior
sparisty in latent variable models. In Proc. NIPS, 2009a.
J. Grac?a, K. Ganchev, and B. Taskar. Postcat - posterior constrained alignment
toolkit. In The Third Machine Translation Marathon, 2009b.
Rahul Gupta and Sunita Sarawagi. Joint training for open-domain extraction
on the web: exploiting overlap when supervision is limited. In Proceedings of
the Fourth ACM International Conference on Web Search and Data Mining
(WSDM), 2011.
P. Liang, M. I. Jordan, and D. Klein. Learning from measurements in exponen-
tial families. In Proc. ICML, 2009.
G. S. Mann and A. McCallum. Simple, robust, scalable semi-supervised learning
via expectation regularization. In Proc. ICML, 2007.
G. S. Mann and A. McCallum. Generalized expectation criteria for semi-
supervised learning of conditional random fields. In Proc. ACL, 2008.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark Johnson. Using uni-
versal linguistic knowledge to guide grammar induction. In Proceedings of
the 2010 Conference on Empirical Methods in Natural Language Processing,
pages 1234?1244, Cambridge, MA, October 2010. Association for Computa-
tional Linguistics. URL http://www.aclweb.org/anthology/D10-1120.
John Platt, Kristina Toutanova, and Wen-tau Yih. Translingual document rep-
resentations from discriminative projections. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language Processing, pages 251?261,
Cambridge, MA, October 2010. Association for Computational Linguistics.
URL http://www.aclweb.org/anthology/D10-1025.
Novi Quadrianto, James Petterson, and Alex Smola. Distribution matching for
transduction. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams,
and A. Culotta, editors, Advances in Neural Information Processing Systems
22, pages 1500?1508. MIT Press, 2009.
Sameer Singh, Dustin Hillard, and Chris Leggetter. Minimally-supervised ex-
traction of entities from text advertisements. In Human Language Tech-
nologies: The 2010 Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages 73?81, Los Angeles,
California, June 2010. Association for Computational Linguistics. URL
http://www.aclweb.org/anthology/N10-1009.
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 238?242,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Using Search-Logs to Improve Query Tagging
Kuzman Ganchev Keith Hall Ryan McDonald Slav Petrov
Google, Inc.
{kuzman|kbhall|ryanmcd|slav}@google.com
Abstract
Syntactic analysis of search queries is im-
portant for a variety of information-retrieval
tasks; however, the lack of annotated data
makes training query analysis models diffi-
cult. We propose a simple, efficient proce-
dure in which part-of-speech tags are trans-
ferred from retrieval-result snippets to queries
at training time. Unlike previous work, our
final model does not require any additional re-
sources at run-time. Compared to a state-of-
the-art approach, we achieve more than 20%
relative error reduction. Additionally, we an-
notate a corpus of search queries with part-
of-speech tags, providing a resource for future
work on syntactic query analysis.
1 Introduction
Syntactic analysis of search queries is important for
a variety of tasks including better query refinement,
improved matching and better ad targeting (Barr
et al, 2008). However, search queries differ sub-
stantially from traditional forms of written language
(e.g., no capitalization, few function words, fairly
free word order, etc.), and are therefore difficult
to process with natural language processing tools
trained on standard corpora (Barr et al, 2008). In
this paper we focus on part-of-speech (POS) tagging
queries entered into commercial search engines and
compare different strategies for learning from search
logs. The search logs consist of user queries and
relevant search results retrieved by a search engine.
We use a supervised POS tagger to label the result
snippets and then transfer the tags to the queries,
producing a set of noisy labeled queries. These la-
beled queries are then added to the training data and
the tagger is retrained. We evaluate different strate-
gies for selecting which annotation to transfer and
find that using the result that was clicked by the user
gives comparable performance to using just the top
result or to aggregating over the top-k results.
The most closely related previous work is that of
Bendersky et al (2010, 2011). In their work, un-
igram POS tag priors generated from a large cor-
pus are blended with information from the top-50
results from a search engine at prediction time. Such
an approach has the disadvantage that it necessitates
access to a search engine at run-time and is com-
putationally very expensive. We re-implement their
method and show that our direct transfer approach is
more effective, while being simpler to instrument:
since we use information from the search engine
only during training, we can train a stand-alone POS
tagger that can be run without access to additional
resources. We also perform an error analysis and
find that most of the remaining errors are due to er-
rors in POS tagging of the snippets.
2 Direct Transfer
The main intuition behind our work, Bendersky et
al. (2010) and Ru?d et al (2011), is that standard NLP
annotation tools work better on snippets returned by
a search engine than on user supplied queries. This
is because snippets are typically well-formed En-
glish sentences, while queries are not. Our goal is to
leverage this observation and use a supervised POS
tagger trained on regular English sentences to gen-
erate annotations for a large set of queries that can
be used for training a query-specific model. Perhaps
the simplest approach ? but also a surprisingly pow-
erful one ? is to POS tag some relevant snippets for
238
a given query, and then to transfer the tags from the
snippet tokens to matching query tokens. This ?di-
rect? transfer idea is at the core of all our experi-
ments. In this work, we provide a comparison of
techniques for selecting snippets associated with the
query, as well as an evaluation of methods for align-
ing the matching words in the query to those in the
selected snippets.
Specifically, for each query1 with a corresponding
set of ?relevant snippets,? we first apply the baseline
tagger to the query and all the snippets. We match
any query terms in these snippets, and copy over the
POS tag to the matching query term. Note that this
can produce multiple labelings as the relevant snip-
pet set can be very diverse and varies even for the
same query. We choose the most frequent tagging
as the canonical one and add it to our training set.
We then train a query tagger on all our training data:
the original human annotated English sentences and
also the automatically generated query training set.
The simplest way to match query tokens to snip-
pet tokens is to allow a query token to match any
snippet token. This can be problematic when we
have queries that have a token repeated with differ-
ent parts-of-speech such as in ?tie a tie.? To make a
more precise matching we try a sequence of match-
ing rules: First, exact match of the query n-gram.
Then matching the terms in order, so the query ?tiea
a tieb? matched to the snippet ?to tie1 a neck tie2?
would match tiea:tie1 and tieb:tie2. Finally, we
match as many query terms as possible. An early
observation showed that when a query term occurs
in the result URL, e.g., searching for ?irs mileage
rate? results in the page irs.gov, the query term
matching the URL domain name is usually a proper
noun. Consequently we add this rule.
In the context of search logs, a relevant snippet
set can refer to the top k snippets (including the case
where k = 1) or the snippet(s) associated with re-
sults clicked by users that issued the query. In our
experiments we found that different strategies for se-
lecting relevant snippets, such as selecting the snip-
pets of the clicked results, using the top-10 results
or using only the top result, perform similarly (see
Table 1).
1We skip navigational queries, e.g, amazon or amazon.com,
since syntactic analysis of such queries is not useful.
Query budget/NN rent/VB a/DET car/NN Clicks
Snip 1 . . . Budget/NNP Rent/NNP 2
A/NNP Car/NNP . . .
Snip 2 . . . Go/VB to/TO Budget/NNP 1
to/TO rent/VB a/DET car/NN . . .
Snip 3 . . . Rent/VB a/DET car/NN 1
from/IN Budget/NNP . . .
Figure 1: Example query and snippets as tagged by a
baseline tagger as well as associated clicks.
By contrast Bendersky et al (2010) use a lin-
ear interpolation between a prior probability and the
snippet tagging. They define pi(t|w) as the relative
frequency of tag t given by the baseline tagger to
word w in some corpus and ?(t|w, s) as the indica-
tor function for word w in the context of snippet s
has tag t. They define the tagging of a word as
argmax
t
0.2pi(t|w) + 0.8mean
s:w?s
?(t|w, s) (1)
We illustrate the difference between the two ap-
proaches in Figure 1. The numbered rows of the
table correspond to three snippets (with non-query
terms elided). The strategy that uses the clicks to se-
lect the tagging would count two examples of ?Bud-
get/NNP Rent/NNP A/NNP Car/NNP? and one for
each of two other taggings. Note that snippet 1
and the query get different taggings primarily due
to orthographic variations. It would then add ?bud-
get/NNP rent/NNP a/NNP car/NNP? to its training
set. The interpolation approach of Bendersky et al
(2010) would tag the query as ?budget/NNP rent/VB
a/DET car/NN?. To see why this is the case, consider
the probability for rent/VB vs rent/NNP. For rent/VB
we have 0.2 + 0.8? 23 , while for rent/NNP we have
0 + 0.8? 13 assuming that pi(VB|rent) = 1.
3 Experimental Setup
We assume that we have access to labeled English
sentences from the PennTreebank (Marcus et al,
1993) and the QuestionBank (Judge et al, 2006), as
well as large amounts of unlabeled search queries.
Each query is paired with a set of relevant results
represented by snippets (sentence fragments con-
taining the search terms), as well as information
about the order in which the results were shown to
the user and possibly the result the user clicked on.
Note that different sets of results are possible for the
239
same query, because of personalization and ranking
changes over time.
3.1 Evaluation Data
We use two data sets for evaluation. The first is the
set of 251 queries from Microsoft search logs (MS-
251) used in Bendersky et al (2010, 2011). The
queries are annotated with three POS tags represent-
ing nouns, verbs and ?other? tags (MS-251 NVX).
We additionally refine the annotation to cover 14
POS tags comprising the 12 universal tags of Petrov
et al (2012), as well as proper nouns and a special
tag for search operator symbols such as ?-? (for
excluding the subsequent word). We refer to this
evaluation set as MS-251 in our experiments. We
had two annotators annotate the whole of the MS-
251 data set. Before arbitration, the inter-annotator
agreement was 90.2%. As a reference, Barr et al
(2008) report 79.3% when annotating queries with
19 POS tags. We then examined all the instances
where the annotators disagreed, and corrected
the discrepancy. Our annotations are available at
http://code.google.com/p/query-syntax/.
The second evaluation set consists of 500 so
called ?long-tail? queries. These are queries that oc-
curred rarely in the search logs, and are typically
difficult to tag because they are searching for less-
frequent information. They do not contain naviga-
tional queries.
3.2 Baseline Model
We use a linear chain tagger trained with the aver-
aged perceptron (Collins, 2002). We use the follow-
ing features for our tagger: current word, suffixes
and prefixes of length 1 to 3; additionally we use
word cluster features (Uszkoreit and Brants, 2008)
for the current word, and transition features of the
cluster of the current and previous word. When
training on Sections 1-18 of the Penn Treebank
and testing on sections 22-24, our tagger achieves
97.22% accuracy with the Penn Treebank tag set,
which is state-of-the-art for this data set. When we
evaluate only on the 14 tags used in our experiments,
the accuracy increases to 97.88%.
We experimented with 4 baseline taggers (see Ta-
ble 2). WSJ corresponds to training on only the
standard training sections of Wall Street Journal por-
tion of the Penn Treebank. WSJ+QTB adds the
Method
MS-251
NVX
MS-251 long-tail
DIRECT-CLICK 93.43 84.11 78.15
DIRECT-ALL 93.93 84.39 77.73
DIRECT-TOP-1 93.93 84.60 77.60
Table 1: Evaluation of snippet selection strategies.
QuestionBank as training data. WSJ NOCASE and
WSJ+QTB NOCASE use case-insensitive version of
the tagger (conceptually lowercasing the text before
training and before applying the tagger). As we will
see, all our baseline models are better than the base-
line reported in Bendersky et al (2010); our lower-
cased baseline model significantly outperforms even
their best model.
4 Experiments
First, we compared different strategies for selecting
relevant snippets from which to transfer the tags.
These systems are: DIRECT-CLICK, which uses
snippets clicked on by users; DIRECT-ALL, which
uses all the returned snippets seen by the user;2
and DIRECT-TOP-1, which uses just the snippet in
the top result. Table 1 compares these systems on
our three evaluation sets. While DIRECT-ALL and
DIRECT-TOP-1 perform best on the MS-251 data
sets, DIRECT-CLICK has an advantage on the long
tail queries. However, these differences are small
(<0.6%) suggesting that any strategy for selecting
relevant snippet sets will return comparable results
when aggregated over large amounts of data.
We then compared our method to the baseline
models and a re-implementation of Bendersky et al
(2010), which we denote BSC. We use the same
matching scheme for both BSC and our system, in-
cluding the URL matching described in Section 2.
The URL matching improves performance by 0.4-
3.0% across all models and evaluation settings.
Table 2 summarizes our final results. For com-
parison, Bendersky et al (2010) report 91.6% for
their final system, which is comparable to our im-
plementation of their system when the baseline tag-
ger is trained on just the WSJ corpus. Our best sys-
tem achieves a 21.2% relative reduction in error on
their annotations. Some other trends become appar-
2Usually 10 results, but more if the user viewed the second
page of results.
240
Method
MS-251
NVX
MS-251 long-tail
WSJ 90.54 75.07 53.06
BSC 91.74 77.82 57.65
DIRECT-CLICK 93.36 85.81 76.13
WSJ + QTB 90.18 74.86 53.48
BSC 91.74 77.54 57.65
DIRECT-CLICK 93.01 85.03 76.97
WSJ NOCASE 92.87 81.92 74.31
BSC 93.71 84.32 76.63
DIRECT-CLICK 93.50 84.46 77.48
WSJ + QTB NOCASE 93.08 82.70 74.65
BSC 93.57 83.90 77.27
DIRECT-CLICK 93.43 84.11 78.15
Table 2: Tagging accuracies for different baseline settings
and two transfer methods.DIRECT-CLICK is the approach
we propose (see text). Column MS-251 NVX evaluates
with tags from Bendersky et al (2010). Their baseline
is 89.3% and they report 91.6% for their method. MS-
251 and Long-tail use tags from Section 3.1. We observe
snippets for 2/500 long-tail queries and 31/251 MS-251
queries.
ent in Table 2. Firstly, a large part of the benefit of
transfer has to do with case information that is avail-
able in the snippets but is missing in the query. The
uncased tagger is insensitive to this mismatch and
achieves significantly better results than the cased
taggers. However, transferring information from the
snippets provides additional benefits, significantly
improving even the uncased baseline taggers. This
is consistent with the analysis in Barr et al (2008).
Finally, we see that the direct transfer method from
Section 2 significantly outperforms the method de-
scribed in Bendersky et al (2010). Table 3 confirms
this trend when focusing on proper nouns, which are
particularly difficult to identify in queries.
We also manually examined a set of 40 queries
with their associated snippets, for which our best
DIRECT-CLICK system made mistakes. In 32 cases,
the errors in the query tagging could be traced back
to errors in the snippet tagging. A better snippet
tagger could alleviate that problem. In the remain-
ing 8 cases there were problems with the matching
? either the mis-tagged word was not found at all,
or it was matched incorrectly. For example one of
the results for the query ?bell helmet? had a snippet
containing ?Bell cycling helmets? and we failed to
match helmet to helmets.
Method P R F
WSJ + QTB NOCASE 72.12 79.80 75.77
BSC 82.87 69.05 75.33
BSC + URL 83.01 70.80 76.42
DIRECT-CLICK 79.57 76.51 78.01
DIRECT-ALL 75.88 78.38 77.11
DIRECT-TOP-1 78.38 76.40 77.38
Table 3: Precision and recall of the NNP tag on the long-
tail data for the best baseline method and the three trans-
fer methods using that baseline.
5 Related Work
Barr et al (2008) manually annotate a corpus of
2722 queries with 19 POS tags and use it to train
and evaluate POS taggers, and also describe the lin-
guistic structures they find. Unfortunately their data
is not available so we cannot use it to compare to
their results. Ru?d et al (2011) create features based
on search engine results, that they use in an NER
system applied to queries. They report report sig-
nificant improvements when incorporating features
from the snippets. In particular, they exploit capital-
ization and query terms matching URL components;
both of which we have used in this work. Li et al
(2009) use clicks in a product data base to train a tag-
ger for product queries, but they do not use snippets
and do not annotate syntax. Li (2010) and Manshadi
and Li (2009) also work on adding tags to queries,
but do not use snippets or search logs as a source of
information.
6 Conclusions
We described a simple method for training a search-
query POS tagger from search-logs by transfer-
ring context from relevant snippet sets to query
terms. We compared our approach to previous work,
achieving an error reduction of 20%. In contrast to
the approach proposed by Bendersky et al (2010),
our approach does not require access to the search
engine or index when tagging a new query. By ex-
plicitly re-training our final model, it has the ability
to pool knowledge from several related queries and
incorporate the information into the model param-
eters. An area for future work is to transfer other
syntactic information, such as parse structures or su-
pertags using a similar transfer approach.
241
References
Cory Barr, Rosie Jones, and Moira Regelson. 2008.
The linguistic structure of English web-search queries.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1021?1030, Honolulu, Hawaii, October. Association
for Computational Linguistics.
M. Bendersky, W.B. Croft, and D.A. Smith. 2010.
Structural annotation of search queries using pseudo-
relevance feedback. In Proceedings of the 19th ACM
international conference on Information and knowl-
edge management, pages 1537?1540. ACM.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. of EMNLP.
John Judge, Aoife Cahill, and Josef van Genabith. 2006.
Questionbank: Creating a corpus of parse-annotated
questions. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational
Linguistics, pages 497?504, Sydney, Australia, July.
Association for Computational Linguistics.
X. Li, Y.Y. Wang, and A. Acero. 2009. Extracting
structured information from user queries with semi-
supervised conditional random fields. In Proceedings
of the 32nd international ACM SIGIR conference on
Research and development in information retrieval,
pages 572?579. ACM.
X. Li. 2010. Understanding the semantic structure of
noun phrase queries. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 1337?1345. Association for Com-
putational Linguistics.
M. Manshadi and X. Li. 2009. Semantic tagging of web
search queries. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 2-Volume 2, pages
861?869. Association for Computational Linguistics.
M. P. Marcus, Mary Ann Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus of
English: the Penn treebank. Computational Linguis-
tics, 19.
S. Petrov, D. Das, and R. McDonald. 2012. A universal
part-of-speech tagset. In Proc. of LREC.
Stefan Ru?d, Massimiliano Ciaramita, Jens Mu?ller, and
Hinrich Schu?tze. 2011. Piggyback: Using search en-
gines for robust cross-domain named entity recogni-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 965?975, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
J. Uszkoreit and T. Brants. 2008. Distributed word clus-
tering for large scale class-based language modeling in
machine translation. In Proc. of ACL.
242
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92?97,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Universal Dependency Annotation for Multilingual Parsing
Ryan McDonald? Joakim Nivre?? Yvonne Quirmbach-Brundage? Yoav Goldberg??
Dipanjan Das? Kuzman Ganchev? Keith Hall? Slav Petrov? Hao Zhang?
Oscar Ta?ckstro?m?? Claudia Bedini? Nu?ria Bertomeu Castello?? Jungmee Lee?
Google, Inc.? Uppsala University? Appen-Butler-Hill? Bar-Ilan University?
Contact: ryanmcd@google.com
Abstract
We present a new collection of treebanks
with homogeneous syntactic dependency
annotation for six languages: German,
English, Swedish, Spanish, French and
Korean. To show the usefulness of such a
resource, we present a case study of cross-
lingual transfer parsing with more reliable
evaluation than has been possible before.
This ?universal? treebank is made freely
available in order to facilitate research on
multilingual dependency parsing.1
1 Introduction
In recent years, syntactic representations based
on head-modifier dependency relations between
words have attracted a lot of interest (Ku?bler et
al., 2009). Research in dependency parsing ? com-
putational methods to predict such representations
? has increased dramatically, due in large part to
the availability of dependency treebanks in a num-
ber of languages. In particular, the CoNLL shared
tasks on dependency parsing have provided over
twenty data sets in a standardized format (Buch-
holz and Marsi, 2006; Nivre et al, 2007).
While these data sets are standardized in terms
of their formal representation, they are still hetero-
geneous treebanks. That is to say, despite them
all being dependency treebanks, which annotate
each sentence with a dependency tree, they sub-
scribe to different annotation schemes. This can
include superficial differences, such as the renam-
ing of common relations, as well as true diver-
gences concerning the analysis of linguistic con-
structions. Common divergences are found in the
1Downloadable at https://code.google.com/p/uni-dep-tb/.
analysis of coordination, verb groups, subordinate
clauses, and multi-word expressions (Nilsson et
al., 2007; Ku?bler et al, 2009; Zeman et al, 2012).
These data sets can be sufficient if one?s goal
is to build monolingual parsers and evaluate their
quality without reference to other languages, as
in the original CoNLL shared tasks, but there are
many cases where heterogenous treebanks are less
than adequate. First, a homogeneous represen-
tation is critical for multilingual language tech-
nologies that require consistent cross-lingual anal-
ysis for downstream components. Second, consis-
tent syntactic representations are desirable in the
evaluation of unsupervised (Klein and Manning,
2004) or cross-lingual syntactic parsers (Hwa et
al., 2005). In the cross-lingual study of McDonald
et al (2011), where delexicalized parsing models
from a number of source languages were evalu-
ated on a set of target languages, it was observed
that the best target language was frequently not the
closest typologically to the source. In one stun-
ning example, Danish was the worst source lan-
guage when parsing Swedish, solely due to greatly
divergent annotation schemes.
In order to overcome these difficulties, some
cross-lingual studies have resorted to heuristics to
homogenize treebanks (Hwa et al, 2005; Smith
and Eisner, 2009; Ganchev et al, 2009), but we
are only aware of a few systematic attempts to
create homogenous syntactic dependency anno-
tation in multiple languages. In terms of auto-
matic construction, Zeman et al (2012) attempt
to harmonize a large number of dependency tree-
banks by mapping their annotation to a version of
the Prague Dependency Treebank scheme (Hajic?
et al, 2001; Bo?hmova? et al, 2003). Addition-
ally, there have been efforts to manually or semi-
manually construct resources with common syn-
92
tactic analyses across multiple languages using al-
ternate syntactic theories as the basis for the repre-
sentation (Butt et al, 2002; Helmreich et al, 2004;
Hovy et al, 2006; Erjavec, 2012).
In order to facilitate research on multilingual
syntactic analysis, we present a collection of data
sets with uniformly analyzed sentences for six lan-
guages: German, English, French, Korean, Span-
ish and Swedish. This resource is freely avail-
able and we plan to extend it to include more data
and languages. In the context of part-of-speech
tagging, universal representations, such as that of
Petrov et al (2012), have already spurred numer-
ous examples of improved empirical cross-lingual
systems (Zhang et al, 2012; Gelling et al, 2012;
Ta?ckstro?m et al, 2013). We aim to do the same for
syntactic dependencies and present cross-lingual
parsing experiments to highlight some of the bene-
fits of cross-lingually consistent annotation. First,
results largely conform to our expectations of
which target languages should be useful for which
source languages, unlike in the study of McDon-
ald et al (2011). Second, the evaluation scores
in general are significantly higher than previous
cross-lingual studies, suggesting that most of these
studies underestimate true accuracy. Finally, un-
like all previous cross-lingual studies, we can re-
port full labeled accuracies and not just unlabeled
structural accuracies.
2 Towards A Universal Treebank
The Stanford typed dependencies for English
(De Marneffe et al, 2006; de Marneffe and Man-
ning, 2008) serve as the point of departure for our
?universal? dependency representation, together
with the tag set of Petrov et al (2012) as the under-
lying part-of-speech representation. The Stanford
scheme, partly inspired by the LFG framework,
has emerged as a de facto standard for depen-
dency annotation in English and has recently been
adapted to several languages representing different
(and typologically diverse) language groups, such
as Chinese (Sino-Tibetan) (Chang et al, 2009),
Finnish (Finno-Ugric) (Haverinen et al, 2010),
Persian (Indo-Iranian) (Seraji et al, 2012), and
Modern Hebrew (Semitic) (Tsarfaty, 2013). Its
widespread use and proven adaptability makes it a
natural choice for our endeavor, even though ad-
ditional modifications will be needed to capture
the full variety of grammatical structures in the
world?s languages.
Alexandre re?side avec sa famille a` Tinqueux .
NOUN VERB ADP DET NOUN ADP NOUN P
NSUBJ ADPMOD
ADPOBJ
POSS
ADPMOD
ADPOBJ
P
Figure 1: A sample French sentence.
We use the so-called basic dependencies (with
punctuation included), where every dependency
structure is a tree spanning all the input tokens,
because this is the kind of representation that most
available dependency parsers require. A sample
dependency tree from the French data set is shown
in Figure 1. We take two approaches to generat-
ing data. The first is traditional manual annotation,
as previously used by Helmreich et al (2004) for
multilingual syntactic treebank construction. The
second, used only for English and Swedish, is to
automatically convert existing treebanks, as in Ze-
man et al (2012).
2.1 Automatic Conversion
Since the Stanford dependencies for English are
taken as the starting point for our universal annota-
tion scheme, we begin by describing the data sets
produced by automatic conversion. For English,
we used the Stanford parser (v1.6.8) (Klein and
Manning, 2003) to convert the Wall Street Jour-
nal section of the Penn Treebank (Marcus et al,
1993) to basic dependency trees, including punc-
tuation and with the copula verb as head in cop-
ula constructions. For Swedish, we developed a
set of deterministic rules for converting the Tal-
banken part of the Swedish Treebank (Nivre and
Megyesi, 2007) to a representation as close as pos-
sible to the Stanford dependencies for English.
This mainly consisted in relabeling dependency
relations and, due to the fine-grained label set used
in the Swedish Treebank (Teleman, 1974), this
could be done with high precision. In addition,
a small number of constructions required struc-
tural conversion, notably coordination, which in
the Swedish Treebank is given a Prague style anal-
ysis (Nilsson et al, 2007). For both English and
Swedish, we mapped the language-specific part-
of-speech tags to universal tags using the map-
pings of Petrov et al (2012).
2.2 Manual Annotation
For the remaining four languages, annotators were
given three resources: 1) the English Stanford
93
guidelines; 2) a set of English sentences with Stan-
ford dependencies and universal tags (as above);
and 3) a large collection of unlabeled sentences
randomly drawn from newswire, weblogs and/or
consumer reviews, automatically tokenized with a
rule-based system. For German, French and Span-
ish, contractions were split, except in the case of
clitics. For Korean, tokenization was more coarse
and included particles within token units. Annota-
tors could correct this automatic tokenization.
The annotators were then tasked with producing
language-specific annotation guidelines with the
expressed goal of keeping the label and construc-
tion set as close as possible to the original English
set, only adding labels for phenomena that do not
exist in English. Making fine-grained label dis-
tinctions was discouraged. Once these guidelines
were fixed, annotators selected roughly an equal
amount of sentences to be annotated from each do-
main in the unlabeled data. As the sentences were
already randomly selected from a larger corpus,
annotators were told to view the sentences in or-
der and to discard a sentence only if it was 1) frag-
mented because of a sentence splitting error; 2) not
from the language of interest; 3) incomprehensible
to a native speaker; or 4) shorter than three words.
The selected sentences were pre-processed using
cross-lingual taggers (Das and Petrov, 2011) and
parsers (McDonald et al, 2011).
The annotators modified the pre-parsed trees us-
ing the TrEd2 tool. At the beginning of the annota-
tion process, double-blind annotation, followed by
manual arbitration and consensus, was used itera-
tively for small batches of data until the guidelines
were finalized. Most of the data was annotated
using single-annotation and full review: one an-
notator annotating the data and another reviewing
it, making changes in close collaboration with the
original annotator. As a final step, all annotated
data was semi-automatically checked for annota-
tion consistency.
2.3 Harmonization
After producing the two converted and four an-
notated data sets, we performed a harmonization
step, where the goal was to maximize consistency
of annotation across languages. In particular, we
wanted to eliminate cases where the same label
was used for different linguistic relations in dif-
ferent languages and, conversely, where one and
2Available at http://ufal.mff.cuni.cz/tred/.
the same relation was annotated with different la-
bels, both of which could happen accidentally be-
cause annotators were allowed to add new labels
for the language they were working on. Moreover,
we wanted to avoid, as far as possible, labels that
were only used in one or two languages.
In order to satisfy these requirements, a number
of language-specific labels were merged into more
general labels. For example, in analogy with the
nn label for (element of a) noun-noun compound,
the annotators of German added aa for compound
adjectives, and the annotators of Korean added vv
for compound verbs. In the harmonization step,
these three labels were merged into a single label
compmod for modifier in compound.
In addition to harmonizing language-specific la-
bels, we also renamed a small number of relations,
where the name would be misleading in the uni-
versal context (although quite appropriate for En-
glish). For example, the label prep (for a mod-
ifier headed by a preposition) was renamed adp-
mod, to make clear the relation to other modifier
labels and to allow postpositions as well as prepo-
sitions.3 We also eliminated a few distinctions in
the original Stanford scheme that were not anno-
tated consistently across languages (e.g., merging
complm with mark, number with num, and purpcl
with advcl).
The final set of labels is listed with explanations
in Table 1. Note that relative to the universal part-
of-speech tagset of Petrov et al (2012) our final
label set is quite rich (40 versus 12). This is due
mainly to the fact that the the former is based on
deterministic mappings from a large set of annota-
tion schemes and therefore reduced to the granu-
larity of the greatest common denominator. Such a
reduction may ultimately be necessary also in the
case of dependency relations, but since most of our
data sets were created through manual annotation,
we could afford to retain a fine-grained analysis,
knowing that it is always possible to map from
finer to coarser distinctions, but not vice versa.4
2.4 Final Data Sets
Table 2 presents the final data statistics. The num-
ber of sentences, tokens and tokens/sentence vary
3Consequently, pobj and pcomp were changed to adpobj
and adpcomp.
4The only two data sets that were created through con-
version in our case were English, for which the Stanford de-
pendencies were originally defined, and Swedish, where the
native annotation happens to have a fine-grained label set.
94
Label Description
acomp adjectival complement
adp adposition
adpcomp complement of adposition
adpmod adpositional modifier
adpobj object of adposition
advcl adverbial clause modifier
advmod adverbial modifier
amod adjectival modifier
appos appositive
attr attribute
aux auxiliary
auxpass passive auxiliary
cc conjunction
ccomp clausal complement
Label Description
compmod compound modifier
conj conjunct
cop copula
csubj clausal subject
csubjpass passive clausal subject
dep generic
det determiner
dobj direct object
expl expletive
infmod infinitival modifier
iobj indirect object
mark marker
mwe multi-word expression
neg negation
Label Description
nmod noun modifier
nsubj nominal subject
nsubjpass passive nominal subject
num numeric modifier
p punctuation
parataxis parataxis
partmod participial modifier
poss possessive
prt verb particle
rcmod relative clause modifier
rel relative
xcomp open clausal complement
Table 1: Harmonized label set based on Stanford dependencies (De Marneffe et al, 2006).
source(s) # sentences # tokens
DE N, R 4,000 59,014
EN PTB? 43,948 1,046,829
SV STB? 6,159 96,319
ES N, B, R 4,015 112,718
FR N, B, R 3,978 90,000
KO N, B 6,194 71,840
Table 2: Data set statistics. ?Automatically con-
verted WSJ section of the PTB. The data release
includes scripts to generate this data, not the data
itself. ?Automatically converted Talbanken sec-
tion of the Swedish Treebank. N=News, B=Blogs,
R=Consumer Reviews.
due to the source and tokenization. For example,
Korean has 50% more sentences than Spanish, but
?40k less tokens due to a more coarse-grained to-
kenization. In addition to the data itself, anno-
tation guidelines and harmonization rules are in-
cluded so that the data can be regenerated.
3 Experiments
One of the motivating factors in creating such a
data set was improved cross-lingual transfer eval-
uation. To test this, we use a cross-lingual transfer
parser similar to that of McDonald et al (2011).
In particular, it is a perceptron-trained shift-reduce
parser with a beam of size 8. We use the features
of Zhang and Nivre (2011), except that all lexical
identities are dropped from the templates during
training and testing, hence inducing a ?delexical-
ized? model that employs only ?universal? proper-
ties from source-side treebanks, such as part-of-
speech tags, labels, head-modifier distance, etc.
We ran a number of experiments, which can be
seen in Table 3. For these experiments we ran-
domly split each data set into training, develop-
ment and testing sets.5 The one exception is En-
glish, where we used the standard splits. Each
row in Table 3 represents a source training lan-
guage and each column a target evaluation lan-
guage. We report both unlabeled attachment score
(UAS) and labeled attachment score (LAS) (Buch-
holz and Marsi, 2006). This is likely the first re-
liable cross-lingual parsing evaluation. In partic-
ular, previous studies could not even report LAS
due to differences in treebank annotations.
We can make several interesting observations.
Most notably, for the Germanic and Romance tar-
get languages, the best source language is from
the same language group. This is in stark contrast
to the results of McDonald et al (2011), who ob-
serve that this is rarely the case with the heteroge-
nous CoNLL treebanks. Among the Germanic
languages, it is interesting to note that Swedish
is the best source language for both German and
English, which makes sense from a typological
point of view, because Swedish is intermediate be-
tween German and English in terms of word or-
der properties. For Romance languages, the cross-
lingual parser is approaching the accuracy of the
supervised setting, confirming that for these lan-
guages much of the divergence is lexical and not
structural, which is not true for the Germanic lan-
guages. Finally, Korean emerges as a very clear
outlier (both as a source and as a target language),
which again is supported by typological consider-
ations as well as by the difference in tokenization.
With respect to evaluation, it is interesting to
compare the absolute numbers to those reported
in McDonald et al (2011) for the languages com-
5These splits are included in the release of the data.
95
Source
Training
Language
Target Test Language
Unlabeled Attachment Score (UAS) Labeled Attachment Score (LAS)
Germanic Romance Germanic Romance
DE EN SV ES FR KO DE EN SV ES FR KO
DE 74.86 55.05 65.89 60.65 62.18 40.59 64.84 47.09 53.57 48.14 49.59 27.73
EN 58.50 83.33 70.56 68.07 70.14 42.37 48.11 78.54 57.04 56.86 58.20 26.65
SV 61.25 61.20 80.01 67.50 67.69 36.95 52.19 49.71 70.90 54.72 54.96 19.64
ES 55.39 58.56 66.84 78.46 75.12 30.25 45.52 47.87 53.09 70.29 63.65 16.54
FR 55.05 59.02 65.05 72.30 81.44 35.79 45.96 47.41 52.25 62.56 73.37 20.84
KO 33.04 32.20 27.62 26.91 29.35 71.22 26.36 21.81 18.12 18.63 19.52 55.85
Table 3: Cross-lingual transfer parsing results. Bolded are the best per target cross-lingual result.
mon to both studies (DE, EN, SV and ES). In that
study, UAS was in the 38?68% range, as compared
to 55?75% here. For Swedish, we can even mea-
sure the difference exactly, because the test sets
are the same, and we see an increase from 58.3%
to 70.6%. This suggests that most cross-lingual
parsing studies have underestimated accuracies.
4 Conclusion
We have released data sets for six languages with
consistent dependency annotation. After the ini-
tial release, we will continue to annotate data in
more languages as well as investigate further au-
tomatic treebank conversions. This may also lead
to modifications of the annotation scheme, which
should be regarded as preliminary at this point.
Specifically, with more typologically and morpho-
logically diverse languages being added to the col-
lection, it may be advisable to consistently en-
force the principle that content words take func-
tion words as dependents, which is currently vi-
olated in the analysis of adpositional and copula
constructions. This will ensure a consistent analy-
sis of functional elements that in some languages
are not realized as free words or are not obliga-
tory, such as adpositions which are often absent
due to case inflections in languages like Finnish. It
will also allow the inclusion of language-specific
functional or morphological markers (case mark-
ers, topic markers, classifiers, etc.) at the leaves of
the tree, where they can easily be ignored in appli-
cations that require a uniform cross-lingual repre-
sentation. Finally, this data is available on an open
source repository in the hope that the community
will commit new data and make corrections to ex-
isting annotations.
Acknowledgments
Many people played critical roles in the pro-
cess of creating the resource. At Google, Fer-
nando Pereira, Alfred Spector, Kannan Pashu-
pathy, Michael Riley and Corinna Cortes sup-
ported the project and made sure it had the re-
quired resources. Jennifer Bahk and Dave Orr
helped coordinate the necessary contracts. Andrea
Held, Supreet Chinnan, Elizabeth Hewitt, Tu Tsao
and Leigha Weinberg made the release process
smooth. Michael Ringgaard, Andy Golding, Terry
Koo, Alexander Rush and many others provided
technical advice. Hans Uszkoreit gave us per-
mission to use a subsample of sentences from the
Tiger Treebank (Brants et al, 2002), the source of
the news domain for our German data set. Anno-
tations were additionally provided by Sulki Kim,
Patrick McCrae, Laurent Alamarguy and He?ctor
Ferna?ndez Alcalde.
References
Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and Barbora
Hladka?. 2003. The Prague Dependency Treebank:
A three-level annotation scenario. In Anne Abeille?,
editor, Treebanks: Building and Using Parsed Cor-
pora, pages 103?127. Kluwer.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
Treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Miriam Butt, Helge Dyvik, Tracy Holloway King,
Hiroshi Masuichi, and Christian Rohrer. 2002.
The parallel grammar project. In Proceedings of
the 2002 workshop on Grammar engineering and
evaluation-Volume 15.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative
reordering with Chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation (SSST-3)
at NAACL HLT 2009.
96
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of ACL-HLT.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation.
Marie-Catherine De Marneffe, Bill MacCartney, and
Chris D. Manning. 2006. Generating typed depen-
dency parses from phrase structure parses. In Pro-
ceedings of LREC.
Tomaz Erjavec. 2012. MULTEXT-East: Morphosyn-
tactic resources for Central and Eastern European
languages. Language Resources and Evaluation,
46:131?142.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction
via bitext projection constraints. In Proceedings of
ACL-IJCNLP.
Douwe Gelling, Trevor Cohn, Phil Blunsom, and Joao
Grac?a. 2012. The pascal challenge on grammar in-
duction. In Proceedings of the NAACL-HLT Work-
shop on the Induction of Linguistic Structure.
Jan Hajic?, Barbora Vidova Hladka, Jarmila Panevova?,
Eva Hajic?ova?, Petr Sgall, and Petr Pajas. 2001.
Prague Dependency Treebank 1.0. LDC, 2001T10.
Katri Haverinen, Timo Viljanen, Veronika Laippala,
Samuel Kohonen, Filip Ginter, and Tapio Salakoski.
2010. Treebanking finnish. In Proceedings of
The Ninth International Workshop on Treebanks and
Linguistic Theories (TLT9).
Stephen Helmreich, David Farwell, Bonnie Dorr, Nizar
Habash, Lori Levin, Teruko Mitamura, Florence
Reeder, Keith Miller, Eduard Hovy, Owen Rambow,
and Advaith Siddharthan. 2004. Interlingual anno-
tation of multilingual text corpora. In Proceedings
of the HLT-EACL Workshop on Frontiers in Corpus
Annotation.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of NAACL.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(03):311?325.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL.
Dan Klein and Chris D. Manning. 2004. Corpus-based
induction of syntactic structure: models of depen-
dency and constituency. In Proceedings of ACL.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Morgan and Claypool.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP.
Jens Nilsson, Joakim Nivre, and Johan Hall. 2007.
Generalizing tree transformations for inductive de-
pendency parsing. In Proceedings of ACL.
Joakim Nivre and Bea?ta Megyesi. 2007. Bootstrap-
ping a Swedish treebank using cross-corpus harmo-
nization and annotation projection. In Proceedings
of the 6th International Workshop on Treebanks and
Linguistic Theories.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on
dependency parsing. In Proceedings of EMNLP-
CoNLL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC.
Mojgan Seraji, Bea?ta Megyesi, and Nivre Joakim.
2012. Bootstrapping a Persian dependency tree-
bank. Linguistic Issues in Language Technology,
7(18):1?10.
David A. Smith and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of EMNLP.
Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the ACL.
Ulf Teleman. 1974. Manual fo?r grammatisk beskrivn-
ing av talad och skriven svenska. Studentlitteratur.
Reut Tsarfaty. 2013. A unified morpho-syntactic
scheme of stanford dependencies. Proceedings of
ACL.
Daniel Zeman, David Marecek, Martin Popel,
Loganathan Ramasamy, Jan S?tepa?nek, Zdene?k
Z?abokrtsky`, and Jan Hajic. 2012. Hamledt: To
parse or not to parse. In Proceedings of LREC.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL-HLT.
Yuan Zhang, Roi Reichart, Regina Barzilay, and Amir
Globerson. 2012. Learning to map into a universal
pos tagset. In Proceedings of EMNLP.
97
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1448?1458,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Semantic Frame Identification with Distributed Word Representations
Karl Moritz Hermann
??
Dipanjan Das
?
Jason Weston
?
Kuzman Ganchev
?
?
Department of Computer Science, University of Oxford, Oxford OX1 3QD, United Kingdom
?
Google Inc., 76 9th Avenue, New York, NY 10011, United States
karl.moritz.hermann@cs.ox.ac.uk
{dipanjand,kuzman}@google.com jaseweston@gmail.com
Abstract
We present a novel technique for semantic
frame identification using distributed rep-
resentations of predicates and their syntac-
tic context; this technique leverages auto-
matic syntactic parses and a generic set
of word embeddings. Given labeled data
annotated with frame-semantic parses, we
learn a model that projects the set of word
representations for the syntactic context
around a predicate to a low dimensional
representation. The latter is used for se-
mantic frame identification; with a stan-
dard argument identification method in-
spired by prior work, we achieve state-of-
the-art results on FrameNet-style frame-
semantic analysis. Additionally, we report
strong results on PropBank-style semantic
role labeling in comparison to prior work.
1 Introduction
Distributed representations of words have proved
useful for a number of tasks. By providing richer
representations of meaning than what can be en-
compassed in a discrete representation, such ap-
proaches have successfully been applied to tasks
such as sentiment analysis (Socher et al, 2011),
topic classification (Klementiev et al, 2012) or
word-word similarity (Mitchell and Lapata, 2008).
We present a new technique for semantic frame
identification that leverages distributed word rep-
resentations. According to the theory of frame se-
mantics (Fillmore, 1982), a semantic frame rep-
resents an event or scenario, and possesses frame
elements (or semantic roles) that participate in the
?
The majority of this research was carried out during an
internship at Google.
event. Most work on frame-semantic parsing has
usually divided the task into two major subtasks:
frame identification, namely the disambiguation of
a given predicate to a frame, and argument iden-
tification (or semantic role labeling), the analysis
of words and phrases in the sentential context that
satisfy the frame?s semantic roles (Das et al, 2010;
Das et al, 2014).
1
Here, we focus on the first sub-
task of frame identification for given predicates;
we use our novel method (?3) in conjunction with
a standard argument identification model (?4) to
perform full frame-semantic parsing.
We present experiments on two tasks. First, we
show that for frame identification on the FrameNet
corpus (Baker et al, 1998; Fillmore et al, 2003),
we outperform the prior state of the art (Das et al,
2014). Moreover, for full frame-semantic parsing,
with the presented frame identification technique
followed by our argument identification method,
we report the best results on this task to date. Sec-
ond, we present results on PropBank-style seman-
tic role labeling (Palmer et al, 2005; Meyers et al,
2004; M`arquez et al, 2008), that approach strong
baselines, and are on par with prior state of the art
(Punyakanok et al, 2008).
2 Overview
Early work in frame-semantic analysis was pio-
neered by Gildea and Jurafsky (2002). Subsequent
work in this area focused on either the FrameNet
or PropBank frameworks, and research on the lat-
ter has been more popular. Since the CoNLL
2004-2005 shared tasks (Carreras and M`arquez,
1
There are exceptions, wherein the task has been modeled
using a pipeline of three classifiers that perform frame iden-
tification, a binary stage that classifies candidate arguments,
and argument identification on the filtered candidates (Baker
et al, 2007; Johansson and Nugues, 2007).
1448
John     bought    a   car   .
COMMERCE_BUY
buy.V
Buyer Goods
John     bought    a   car   .
buy.01
buy.V
A0 A1
Mary      sold        a   car   .
COMMERCE_BUY
sell.V
Seller Goods
Mary      sold        a   car   .
sell.01
sell.V
A0 A1
(a) (b)
Figure 1: Example sentences with frame-semantic analyses.
FrameNet annotation conventions are used in (a) while (b)
denotes PropBank conventions.
2004; Carreras and M`arquez, 2005) on PropBank
semantic role labeling (SRL), it has been treated
as an important NLP problem. However, research
has mostly focused on argument analysis, skipping
the frame disambiguation step, and its interaction
with argument identification.
2.1 Frame-Semantic Parsing
Closely related to SRL, frame-semantic parsing
consists of the resolution of predicate sense into
a frame, and the analysis of the frame?s argu-
ments. Work in this area exclusively uses the
FrameNet full text annotations. Johansson and
Nugues (2007) presented the best performing sys-
tem at SemEval 2007 (Baker et al, 2007), and Das
et al (2010) improved performance, and later set
the current state of the art on this task (Das et al,
2014). We briefly discuss FrameNet, and subse-
quently PropBank annotation conventions here.
FrameNet The FrameNet project (Baker et al,
1998) is a lexical database that contains informa-
tion about words and phrases (represented as lem-
mas conjoined with a coarse part-of-speech tag)
termed as lexical units, with a set of semantic
frames that they could evoke. For each frame,
there is a list of associated frame elements (or
roles, henceforth), that are also distinguished as
core or non-core.
2
Sentences are annotated us-
ing this universal frame inventory. For exam-
ple, consider the pair of sentences in Figure 1(a).
COMMERCE BUY is a frame that can be evoked by
morphological variants of the two example lexical
units buy.V and sell.V. Buyer, Seller and Goods are
some example roles for this frame.
2
Additional information such as finer distinction of the
coreness properties of roles, the relationship between frames,
and that of roles are also present, but we do not leverage that
information in this work.
PropBank The PropBank project (Palmer et al,
2005) is another popular resource related to se-
mantic role labeling. The PropBank corpus has
verbs annotated with sense frames and their ar-
guments. Like FrameNet, it also has a lexi-
cal database that stores type information about
verbs, in the form of sense frames and the possi-
ble semantic roles each frame could take. There
are modifier roles that are shared across verb
frames, somewhat similar to the non-core roles
in FrameNet. Figure 1(b) shows annotations for
two verbs ?bought? and ?sold?, with their lemmas
(akin to the lexical units in FrameNet) and their
verb frames buy.01 and sell.01. Generic core role
labels (of which there are seven, namely A0-A5 and
AA) for the verb frames are marked in the figure.
3
A key difference between the two annotation sys-
tems is that PropBank uses a local frame inven-
tory, where frames are predicate-specific. More-
over, role labels, although few in number, take spe-
cific meaning for each verb frame. Figure 1 high-
lights this difference: while both sell.V and buy.V
are members of the same frame in FrameNet, they
evoke different frames in PropBank. In spite of
this difference, nearly identical statistical models
could be employed for both frameworks.
Modeling In this paper, we model the frame-
semantic parsing problem in two stages: frame
identification and argument identification. As
mentioned in ?1, these correspond to a frame dis-
ambiguation stage,
4
and a stage that finds the var-
ious arguments that fulfill the frame?s semantic
roles within the sentence, respectively. This re-
sembles the framework of Das et al (2014), who
solely focus on FrameNet corpora, unlike this pa-
per. The novelty of this paper lies in the frame
identification stage (?3). Note that this two-stage
approach is unusual for the PropBank corpora
when compared to prior work, where the vast ma-
jority of published papers have not focused on the
verb frame disambiguation problem at all, only fo-
cusing on the role labeling stage (see the overview
paper of M`arquez et al (2008) for example).
3
NomBank (Meyers et al, 2004) is a similar resource for
nominal predicates, but we do not consider it in our experi-
ments.
4
For example in PropBank, the lexical unit buy.V has
three verb frames and in sentential context, we want to disam-
biguate its frame. (Although PropBank never formally uses
the term lexical unit, we adopt its usage from the frame se-
mantics literature.)
1449
2.2 Distributed Frame Identification
We present a model that takes word embeddings
as input and learns to identify semantic frames.
A word embedding is a distributed representa-
tion of meaning where each word is represented
as a vector in R
n
. Such representations allow a
model to share meaning between similar words,
and have been used to capture semantic, syntac-
tic and morphological content (Collobert and We-
ston, 2008; Turian et al, 2010, inter alia). We use
word embeddings to represent the syntactic con-
text of a particular predicate instance as a vector.
For example, consider the sentence ?He runs the
company.? The predicate runs has two syntac-
tic dependents ? a subject and direct object (but
no prepositional phrases or clausal complements).
We could represent the syntactic context of runs as
a vector with blocks for all the possible dependents
warranted by a syntactic parser; for example, we
could assume that positions 0 . . . n in the vector
correspond to the subject dependent, n+1 . . . 2n
correspond to the clausal complement dependent,
and so forth. Thus, the context is a vector in R
nk
with the embedding of He at the subject position,
the embedding of company in direct object posi-
tion and zeros everywhere else. Given input vec-
tors of this form for our training data, we learn a
matrix that maps this high dimensional and sparse
representation into a lower dimensional space. Si-
multaneously, the model learns an embedding for
all the possible labels (i.e. the frames in a given
lexicon). At inference time, the predicate-context
is mapped to the low dimensional space, and we
choose the nearest frame label as our classifica-
tion. We next describe this model in detail.
3 Frame Identification with Embeddings
We continue using the example sentence from
?2.2: ?He runs the company.? where we want to
disambiguate the frame of runs in context. First,
we extract the words in the syntactic context of
runs; next, we concatenate their word embeddings
as described in ?2.2 to create an initial vector space
representation. Subsequently, we learn a map-
ping from this initial representation into a low-
dimensional space; we also learn an embedding
for each possible frame label in the same low-
dimensional space. The goal of learning is to
make sure that the correct frame label is as close as
possible to the mapped context, while competing
frame labels are farther away.
Formally, let x represent the actual sentence
with a marked predicate, along with the associated
syntactic parse tree; let our initial representation
of the predicate context be g(x). Suppose that the
word embeddings we start with are of dimension
n. Then g is a function from a parsed sentence
x to R
nk
, where k is the number of possible syn-
tactic context types. For example g selects some
important positions relative to the predicate, and
reserves a block in its output space for the embed-
ding of words found at that position. Suppose g
considers clausal complements and direct objects.
Then g : X ? R
2n
and for the example sentence
it has zeros in positions 0 . . . n and the embedding
of the word company in positions n+1 . . . 2n.
g(x) = [0, . . . , 0, embedding of company].
Section 3.1 describes the context positions we use
in our experiments. Let the low dimensional space
we map to be R
m
and the learned mapping be M :
R
nk
? R
m
. The mapping M is a linear trans-
formation, and we learn it using the WSABIE algo-
rithm (Weston et al, 2011). WSABIE also learns an
embedding for each frame label (y, henceforth).
In our setting, this means that each frame corre-
sponds to a point in R
m
. If we have F possi-
ble frames we can store those parameters in an
F ?m matrix, one m-dimensional point for each
frame, which we will refer to as the linear map-
ping Y . Let the lexical unit (the lemma conjoined
with a coarse POS tag) for the marked predicate
be `. We denote the frames that associate with
` in the frame lexicon
5
and our training corpus
as F
`
. WSABIE performs gradient-based updates
on an objective that tries to minimize the distance
between M(g(x)) and the embedding of the cor-
rect label Y (y), while maintaining a large distance
between M(g(x)) and the other possible labels
Y (y?) in the confusion set F
`
. At disambiguation
time, we use a simple dot product similarity as our
distance metric, meaning that the model chooses
a label by computing the argmax
y
s(x, y) where
s(x, y) = M(g(x)) ?Y (y), where the argmax iter-
ates over the possible frames y ? F
`
if ` was seen
in the lexicon or the training data, or y ? F , if it
was unseen.
6
Model learning is performed using
the margin ranking loss function as described in
5
The frame lexicon stores the frames, corresponding se-
mantic roles and the lexical units associated with the frame.
6
This disambiguation scheme is similar to the one adopted
by Das et al (2014), but they use unlemmatized words to
define their confusion set.
1450
Figure 2: Context representation extraction for the
embedding model. Given a dependency parse (1)
the model extracts all words matching a set of paths
from the frame evoking predicate and its direct de-
pendents (2). The model computes a composed rep-
resentation of the predicate instance by using dis-
tributed vector representations for words (3) ? the
(red) vertical embedding vectors for each word are
concatenated into a long vector. Finally, we learn a
linear transformation function parametrized by the
context blocks (4).
Weston et al (2011), and in more detail in section
3.2.
Since WSABIE learns a single mapping from g(x)
to R
m
, parameters are shared between different
words and different frames. So for example ?He
runs the company? could help the model disam-
biguate ?He owns the company.? Moreover, since
g(x) relies on word embeddings rather than word
identities, information is shared between words.
For example ?He runs the company? could help
us to learn about ?She runs a corporation?.
3.1 Context Representation Extraction
In principle g(x) could be any feature function, but
we performed an initial investigation of two partic-
ular variants. In both variants, our representation
is a block vector where each block corresponds to
a syntactic position relative to the predicate, and
each block?s values correspond to the embedding
of the word at that position.
Direct Dependents The first context function we
considered corresponds to the examples in ?3. To
elaborate, the positions of interest are the labels of
the direct dependents of the predicate, so k is the
number of labels that the dependency parser can
produce. For example, if the label on the edge be-
tween runs and He is nsubj, we would put the em-
bedding of He in the block corresponding to nsubj.
If a label occurs multiple times, then the embed-
dings of the words below this label are averaged.
Unfortunately, using only the direct dependents
can miss a lot of useful information. For exam-
ple, topicalization can place discriminating infor-
mation farther from the predicate. Consider ?He
runs the company.? vs. ?It was the company that
he runs.? In the second sentence, the discrim-
inating word, company dominates the predicate
runs. Similarly, predicates in embedded clauses
may have a distant agent which cannot be captured
using direct dependents. Consider ?The athlete
ran the marathon.? vs. ?The athlete prepared him-
self for three months to run the marathon.? In the
second example, for the predicate run, the agent
The athlete is not a direct dependent, but is con-
nected via a longer dependency path.
Dependency Paths To capture more relevant
context, we developed a second context function
as follows. We scanned the training data for a
given task (either the PropBank or the FrameNet
domains) for the dependency paths that connected
the gold predicates to the gold semantic argu-
ments. This set of dependency paths were deemed
as possible positions in the initial vector space rep-
resentation. In addition, akin to the first context
function, we also added all dependency labels to
the context set. Thus for this context function, the
block cardinality k was the sum of the number of
scanned gold dependency path types and the num-
ber of dependency labels. Given a predicate in its
sentential context, we therefore extract only those
context words that appear in positions warranted
by the above set. See Figure 2 for an illustration
of this process.
We performed initial experiments using con-
text extracted from 1) direct dependents, 2) de-
pendency paths, and 3) both. For all our experi-
ments, setting 3) which concatenates the direct de-
pendents and dependency path always dominated
the other two, so we only report results for this
setting.
3.2 Learning
We model our objective function following We-
ston et al (2011), using a weighted approximate-
rank pairwise loss, learned with stochastic gradi-
ent descent. The mapping from g(x) to the low
dimensional space R
m
is a linear transformation,
so the model parameters to be learnt are the matrix
M ? R
nk?m
as well as the embedding of each
possible frame label, represented as another ma-
trix Y ? R
F?m
where there are F frames in total.
The training objective function minimizes:
?
x
?
y?
L
(
rank
y
(x)
)
max(0, ?+s(x, y)?s(x, y?)).
1451
where x, y are the training inputs and their cor-
responding correct frames, and y? are negative
frames, ? is the margin. Here, rank
y
(x) is the
rank of the positive frame y relative to all the neg-
ative frames:
rank
y
(x) =
?
y?
I(s(x, y) ? ? + s(x, y?)),
and L(?) converts the rank to a weight. Choos-
ing L(?) = C? for any positive constant C opti-
mizes the mean rank, whereas a weighting such as
L(?) =
?
?
i=1
1/i (adopted here) optimizes the
top of the ranked list, as described in (Usunier
et al, 2009). To train with such an objective,
stochastic gradient is employed. For speed the
computation of rank
y
(x) is then replaced with a
sampled approximation: sample N items y? until
a violation is found, i.e. max(0, ? + s(x, y?) ?
s(x, y))) > 0 and then approximate the rank with
(F ? 1)/N , see Weston et al (2011) for more
details on this procedure. For the choices of the
stochastic gradient learning rate, margin (?) and
dimensionality (m), please refer to ?5.4-?5.5.
Note that an alternative approach could learn
only the matrixM , and then use a k-nearest neigh-
bor classifier in R
m
, as in Weinberger and Saul
(2009). The advantage of learning an embedding
for the frame labels is that at inference time we
need to consider only the set of labels for classi-
fication rather than all training examples. Addi-
tionally, since we use a frame lexicon that gives
us the possible frames for a given predicate, we
usually only consider a handful of candidate la-
bels. If we used all training examples for a given
predicate for finding a nearest-neighbor match at
inference time, we would have to consider many
more candidates, making the process very slow.
4 Argument Identification
Here, we briefly describe the argument identifi-
cation model used in our frame-semantic parsing
experiments, post frame identification. Given x,
the sentence with a marked predicate, the argu-
ment identification model assumes that the pred-
icate frame y has been disambiguated. From a
frame lexicon, we look up the set of semantic roles
R
y
that associate with y. This set alo contains the
null role r
?
. From x, a rule-based candidate argu-
ment extraction algorithm extracts a set of spans
A that could potentially serve as the overt
7
argu-
7
By overtness, we mean the non-null instantiation of a
semantic role in a frame-semantic parse.
? starting word of a ? POS of the starting word of a
? ending word of a ? POS of the ending word of a
? head word of a ? POS of the head word of a
? bag of words in a ? bag of POS tags in a
? a bias feature ? voice of the predicate use
? word cluster of a?s head
? word cluster of a?s head conjoined with word cluster
of the predicate
?
? dependency path between a?s head and the predicate
? the set of dependency labels of the predicate?s children
? dependency path conjoined with the POS tag of a?s
head
? dependency path conjoined with the word cluster of
a?s head
? position of a with respect to the predicate (before, after,
overlap or identical)
? whether the subject of the predicate is missing (miss-
ingsubj)
? missingsubj, conjoined with the dependency path
? missingsubj, conjoined with the dependency path from
the verb dominating the predicate to a?s head
Table 1: Argument identification features. The span in con-
sideration is termed a. Every feature in this list has two ver-
sions, one conjoined with the given role r and the other con-
joined with both r and the frame y. The feature with a
?
su-
perscript is only conjoined with the role to reduce its sparsity.
mentsA
y
for y (see ?5.4-?5.5 for the details of the
candidate argument extraction algorithms).
Learning Given training data of the form
??x
(i)
, y
(i)
,M
(i)
??
N
i=1
, where,
M = {(r, a} : r ? R
y
, a ? A ?A
y
}, (1)
a set of tuples that associates each role r in R
y
with a span a according to the gold data. Note that
this mapping associates spans with the null role r
?
as well. We optimize the following log-likelihood
to train our model:
max
?
N
?
i=1
|M
(i)
|
?
j=1
log p?
(
(r, a)
j
|x, y,R
y
)
? C???
2
2
where p? is a log-linear model normalized over the
set R
y
, with features described in Table 1. We
set C = 1.0 and use L-BFGS (Liu and Nocedal,
1989) for training.
Inference Although our learning mechanism
uses a local log-linear model, we perform infer-
ence globally on a per-frame basis by applying
hard structural constraints. Following Das et al
(2014) and Punyakanok et al (2008) we use the
log-probability of the local classifiers as a score in
an integer linear program (ILP) to assign roles sub-
ject to hard constraints described in ?5.4 and ?5.5.
We use an off-the-shelf ILP solver for inference.
1452
5 Experiments
In this section, we present our experiments and
the results achieved. We evaluate our novel frame
identification approach in isolation and also con-
joined with argument identification resulting in
full frame-semantic structures; before presenting
our model?s performance we first focus on the
datasets, baselines and the experimental setup.
5.1 Data
We evaluate our models on both FrameNet- and
PropBank-style structures. For FrameNet, we use
the full-text annotations in the FrameNet 1.5 re-
lease
8
which was used by Das et al (2014, ?3.2).
We used the same test set as Das et al contain-
ing 23 documents with 4,458 predicates. Of the
remaining 55 documents, 16 documents were ran-
domly chosen for development.
9
For experiments with PropBank, we used the
Ontonotes corpus (Hovy et al, 2006), version 4.0,
and only made use of the Wall Street Journal doc-
uments; we used sections 2-21 for training, sec-
tion 24 for development and section 23 for testing.
This resembles the setup used by Punyakanok et
al. (2008). All the verb frame files in Ontonotes
were used for creating our frame lexicon.
5.2 Frame Identification Baselines
For comparison, we implemented a set of baseline
models, with varying feature configurations. The
baselines use a log-linear model that models the
following probability at training time:
p(y|x, `) =
e
??f(y,x,`)
?
y??F
`
e
??f(y?,x,`)
(2)
At test time, this model chooses the best frame as
argmax
y
? ? f(y, x, `) where argmax iterates over
the possible frames y ? F
`
if ` was seen in the
lexicon or the training data, or y ? F , if it was un-
seen, like the disambiguation scheme of ?3. We
train this model by maximizing L
2
regularized
log-likelihood, using L-BFGS; the regularization
constant was set to 0.1 in all experiments.
For comparison with our model from ?3, which
we call WSABIE EMBEDDING, we implemented two
baselines with the log-linear model. Both the
baselines use features very similar to the input rep-
resentations described in ?3.1. The first one com-
putes the direct dependents and dependency paths
8
See https://framenet.icsi.berkeley.edu.
9
These documents are listed in appendix A.
as described in ?3.1 but conjoins them with the
word identity rather than a word embedding. Ad-
ditionally, this model uses the un-conjoined words
as backoff features. This would be a standard NLP
approach for the frame identification problem, but
is surprisingly competitive with the state of the art.
We call this baseline LOG-LINEAR WORDS. The sec-
ond baseline, tries to decouple the WSABIE training
from the embedding input, and trains a log linear
model using the embeddings. So the second base-
line has the same input representation as WSABIE
EMBEDDING but uses a log-linear model instead of
WSABIE. We call this model LOG-LINEAR EMBED-
DING.
5.3 Common Experimental Setup
We process our PropBank and FrameNet training,
development and test corpora with a shift-reduce
dependency parser that uses the Stanford conven-
tions (de Marneffe and Manning, 2013) and uses
an arc-eager transition system with beam size of 8;
the parser and its features are described by Zhang
and Nivre (2011). Before parsing the data, it is
tagged with a POS tagger trained with a condi-
tional random field (Lafferty et al, 2001) with the
following emission features: word, the word clus-
ter, word suffixes of length 1, 2 and 3, capitaliza-
tion, whether it has a hyphen, digit and punctua-
tion. Beyond the bias transition feature, we have
two cluster features for the left and right words in
the transition. We use Brown clusters learned us-
ing the algorithm of Uszkoreit and Brants (2008)
on a large English newswire corpus for cluster fea-
tures. We use the same word clusters for the argu-
ment identification features in Table 1.
We learn the initial embedding representations
for our frame identification model (?3) using a
deep neural language model similar to the one pro-
posed by Bengio et al (2003). We use 3 hidden
layers each with 1024 neurons and learn a 128-
dimensional embedding from a large corpus con-
taining over 100 billion tokens. In order to speed
up learning, we use an unnormalized output layer
and a hinge-loss objective. The objective tries to
ensure that the correct word scores higher than a
random incorrect word, and we train with mini-
batch stochastic gradient descent.
5.4 Experimental Setup for FrameNet
Hyperparameters For our frame identification
model with embeddings, we search for the WSA-
BIE hyperparameters using the development data.
1453
SEMAFOR LEXICON FULL LEXICON
Development Data
Model All Ambiguous Rare All Ambiguous Rare
LOG-LINEAR WORDS 96.21 90.41 95.75 96.37 90.41 96.07
LOG-LINEAR EMBEDDING 96.06 90.56 95.38 96.19 90.49 95.70
WSABIE EMBEDDING (?3) 96.90 92.73 96.44 96.99 93.12 96.39
SEMAFOR LEXICON FULL LEXICON
Model All Ambiguous Rare Unseen All Ambiguous Rare
Test Data
Das et al (2014) supervised 82.97 69.27 80.97 23.08
Das et al (2014) best 83.60 69.19 82.31 42.67
LOG-LINEAR WORDS 84.71 70.97 81.70 27.27 87.44 70.97 87.10
LOG-LINEAR EMBEDDING 83.42 68.70 80.95 27.97 86.20 68.70 86.03
WSABIE EMBEDDING (?3) 86.58 73.67 85.04 44.76 88.73 73.67 89.38
Table 2: Frame identification results for FrameNet. See ?5.6.
SEMAFOR LEXICON FULL LEXICON
Model Precision Recall F
1
Precision Recall F
1
Development Data
LOG-LINEAR WORDS 89.43 75.98 82.16 89.41 76.05 82.19
WSABIE EMBEDDING (?3) 89.89 76.40 82.59 89.94 76.27 82.54
Test Data
Das et al supervised 67.81 60.68 64.05
Das et al best 68.33 61.14 64.54
LOG-LINEAR WORDS 71.16 63.56 67.15 73.35 65.27 69.08
WSABIE EMBEDDING (?3) 72.79 64.95 68.64 74.44 66.17 70.06
Table 3: Full structure prediction results for FrameNet; this reports frame and argument identification performance jointly. We
skip LOG-LINEAR EMBEDDING because it underperforms all other models by a large margin.
We search for the stochastic gradient learning
rate in {0.0001, 0.001, 0.01}, the margin ? ?
{0.001, 0.01, 0.1, 1} and the dimensionality of the
final vector space m ? {256, 512}, to maximize
the frame identification accuracy of ambiguous
lexical units; by ambiguous, we imply lexical units
that appear in the training data or the lexicon with
more than one semantic frame. The underlined
values are the chosen hyperparameters used to an-
alyze the test data.
Argument Candidates The candidate argument
extraction method used for the FrameNet data, (as
mentioned in ?4) was adapted from the algorithm
of Xue and Palmer (2004) applied to dependency
trees. Since the original algorithm was designed
for verbs, we added a few extra rules to handle
non-verbal predicates: we added 1) the predicate
itself as a candidate argument, 2) the span ranging
from the sentence position to the right of the pred-
icate to the rightmost index of the subtree headed
by the predicate?s head; this helped capture cases
like ?a few months? (where few is the predicate and
months is the argument), and 3) the span ranging
from the leftmost index of the subtree headed by
the predicate?s head to the position immediately
before the predicate, for cases like ?your gift to
Goodwill? (where to is the predicate and your gift
is the argument).
10
10
Note that Das et al (2014) describe the state of the art
in FrameNet-based analysis, but their argument identifica-
tion strategy considered all possible dependency subtrees in
Frame Lexicon In our experimental setup, we
scanned the XML files in the ?frames? directory
of the FrameNet 1.5 release, which lists all the
frames, the corresponding roles and the associ-
ated lexical units, and created a frame lexicon to
be used in our frame and argument identification
models. We noted that this renders every lexical
unit as seen; in other words, at frame disambigua-
tion time on our test set, for all instances, we only
had to score the frames in F
`
for a predicate with
lexical unit ` (see ?3 and ?5.2). We call this setup
FULL LEXICON. While comparing with prior state
of the art on the same corpus, we noted that Das et
al. (2014) found several unseen predicates at test
time.
11
For fair comparison, we took the lexical
units for the predicates that Das et al considered
as seen, and constructed a lexicon with only those;
training instances, if any, for the unseen predicates
under Das et al?s setup were thrown out as well.
We call this setup SEMAFOR LEXICON.
12
We also
experimented on the set of unseen instances used
by Das et al
ILP constraints For FrameNet, we used three
ILP constraints during argument identification
(?4). 1) each span could have only one role, 2)
each core role could be present only once, and 3)
all overt arguments had to be non-overlapping.
a parse, resulting in a much larger search space.
11
Instead of using the frame files, Das et al built a frame
lexicon from FrameNet?s exemplars and the training corpus.
12
We got Das et al?s seen predicates from the authors.
1454
Model All Ambiguous Rare
LOG-LINEAR WORDS 94.21 90.54 93.33
LOG-LINEAR EMBEDDING 93.81 89.86 93.73
WSABIE EMBEDDING (?3) 94.79 91.52 92.55
Dev data ? ? Test data
Model All Ambiguous Rare
LOG-LINEAR WORDS 94.74 92.07 91.32
LOG-LINEAR EMBEDDING 94.04 90.95 90.97
WSABIE EMBEDDING (?3) 94.56 91.82 90.62
Table 4: Frame identification accuracy results for PropBank.
The model and the column names have the same semantics
as Table 2.
Model P R F
1
LOG-LINEAR WORDS 80.02 75.58 77.74
WSABIE EMBEDDING (?3) 80.06 75.74 77.84
Dev data ? ? Test data
Model P R F
1
LOG-LINEAR WORDS 81.55 77.83 79.65
WSABIE EMBEDDING (?3) 81.32 77.97 79.61
Table 5: Full frame-structure prediction results for Propbank.
This is a metric that takes into account frames and arguments
together. See ?5.7 for more details.
5.5 Experimental Setup for PropBank
Hyperparameters As in ?5.4, we made a hyper-
parameter sweep in the same space. The chosen
learning rate was 0.01, while the other values were
? = 0.01 and m = 512. Ambiguous lexical units
were used for this selection process.
Argument Candidates For PropBank we use
the algorithm of Xue and Palmer (2004) applied
to dependency trees.
Frame Lexicon For the PropBank experiments
we scanned the frame files for propositions in
Ontonotes 4.0, and stored possible core roles for
each verb frame. The lexical units were simply
the verb associating with the verb frames. There
were no unseen verbs at test time.
ILP constraints We used the constraints of Pun-
yakanok et al (2008).
5.6 FrameNet Results
Table 2 presents accuracy results on frame iden-
tification.
13
We present results on all predicates,
ambiguous predicates seen in the lexicon or the
training data, and rare ambiguous predicates that
appear ? 11 times in the training data. The WS-
ABIE EMBEDDING model from ?3 performs signif-
icantly better than the LOG-LINEAR WORDS base-
line, while LOG-LINEAR EMBEDDING underperforms
in every metric. For the SEMAFOR LEXICON setup,
we also compare with the state of the art from Das
13
We do not report partial frame accuracy that has been
reported by prior work.
Model P R F
1
LOG-LINEAR WORDS 77.29 71.50 74.28
WSABIE EMBEDDING (?3) 77.13 71.32 74.11
Dev data ? ? Test data
Model P R F
1
LOG-LINEAR WORDS 79.47 75.11 77.23
WSABIE EMBEDDING (?3) 79.36 75.04 77.14
Punyakanok et al Collins 75.92 71.45 73.62
Punyakanok et al Charniak 77.09 75.51 76.29
Punyakanok et al Combined 80.53 76.94 78.69
Table 6: Argument only evaluation (semantic role labeling
metrics) using the CoNLL 2005 shared task evaluation script
(Carreras and M`arquez, 2005). Results from Punyakanok et
al. (2008) are taken from Table 11 of that paper.
et al (2014), who used a semi-supervised learn-
ing method to improve upon a supervised latent-
variable log-linear model. For unseen predicates
from the Das et al system, we perform better as
well. Finally, for the FULL LEXICON setting, the ab-
solute accuracy numbers are even better for our
best model. Table 3 presents results on the full
frame-semantic parsing task (measured by a reim-
plementation of the SemEval 2007 shared task
evaluation script) when our argument identifica-
tion model (?4) is used after frame identification.
We notice similar trends as in Table 2, and our re-
sults outperform the previously published best re-
sults, setting a new state of the art.
5.7 PropBank Results
Table 4 shows frame identification results on the
PropBank data. On the development set, our best
model performs with the highest accuracy on all
and ambiguous predicates, but performs worse on
rare ambiguous predicates. On the test set, the
LOG-LINEAR WORDS baseline performs best by a
very narrow margin. See ?6 for a discussion.
Table 5 presents results where we measure pre-
cision, recall and F
1
for frames and arguments to-
gether; this strict metric penalizes arguments for
mismatched frames, like in Table 3. We see the
same trend as in Table 4. Finally, Table 6 presents
SRL results that measures argument performance
only, irrespective of the frame; we use the eval-
uation script from CoNLL 2005 (Carreras and
M`arquez, 2005). We note that with a better frame
identification model, our performance on SRL im-
proves in general. Here, too, the embedding model
barely misses the performance of the best baseline,
but we are at par and sometimes better than the sin-
gle parser setting of a state-of-the-art SRL system
(Punyakanok et al, 2008).
14
14
The last row of Table 6 refers to a system which used the
1455
6 Discussion
For FrameNet, the WSABIE EMBEDDING model we
propose strongly outperforms the baselines on all
metrics, and sets a new state of the art. We be-
lieve that the WSABIE EMBEDDING model performs
better than the LOG-LINEAR EMBEDDING baseline
(that uses the same input representation) because
the former setting allows examples with differ-
ent labels and confusion sets to share informa-
tion; this is due to the fact that all labels live in
the same label space, and a single projection ma-
trix is shared across the examples to map the input
features to this space. Consequently, the WSABIE
EMBEDDING model can share more information be-
tween different examples in the training data than
the LOG-LINEAR EMBEDDING model. Since the LOG-
LINEAR WORDS model always performs better than
the LOG-LINEAR EMBEDDING model, we conclude
that the primary benefit does not come from the
input embedding representation.
15
On the PropBank data, we see that the LOG-
LINEAR WORDS baseline has roughly the same per-
formance as our model on most metrics: slightly
better on the test data and slightly worse on the
development data. This can be partially explained
with the significantly larger training set size for
PropBank, making features based on words more
useful. Another important distinction between
PropBank and FrameNet is that the latter shares
frames between multiple lexical units. The ef-
fect of this is clearly observable from the ?Rare?
column in Table 4. WSABIE EMBEDDING performs
poorly in this setting while LOG-LINEAR EMBEDDING
performs well. Part of the explanation has to do
with the specifics of WSABIE training. Recall that
the WSABIE EMBEDDING model needs to estimate
the label location in R
m
for each frame. In other
words, it must estimate 512 parameters based on
at most 10 training examples. However, since the
input representation is shared across all frames,
every other training example from all the lexical
units affects the optimal estimate, since they all
modify the joint parameter matrixM . By contrast,
in the log-linear models each label has its own
set of parameters, and they interact only via the
normalization constant. The LOG-LINEAR WORDS
model does not have this entanglement, but cannot
share information between words. For PropBank,
combination of two syntactic parsers as input.
15
One could imagine training a WSABIE model with word
features, but we did not perform this experiment.
these drawbacks and benefits balance out and we
see similar performance for LOG-LINEAR WORDS
and LOG-LINEAR EMBEDDING. For FrameNet, esti-
mating the label embedding is not as much of a
problem because even if a lexical unit is rare, the
potential frames can be frequent. For example, we
might have seen the SENDING frame many times,
even though telex.V is a rare lexical unit.
In comparison to prior work on FrameNet, even
our baseline models outperform the previous state
of the art. A particularly interesting comparison is
between our LOG-LINEAR WORDS baseline and the
supervised model of Das et al (2014). They also
use a log-linear model, but they incorporate a la-
tent variable that uses WordNet (Fellbaum, 1998)
to get lexical-semantic relationships and smooths
over frames for ambiguous lexical units. It is
possible that this reduces the model?s power and
causes it to over-generalize. Another difference is
that when training the log-linear model, they nor-
malize over all frames, while we normalize over
the allowed frames for the current lexical unit.
This would tend to encourage their model to ex-
pend more of its modeling power to rule out pos-
sibilities that will be pruned out at test time.
7 Conclusion
We have presented a simple model that outper-
forms the prior state of the art on FrameNet-
style frame-semantic parsing, and performs at par
with one of the previous-best single-parser sys-
tems on PropBank SRL. Unlike Das et al (2014),
our model does not rely on heuristics to con-
struct a similarity graph and leverage WordNet;
hence, in principle it is generalizable to varying
domains, and to other languages. Finally, we pre-
sented results on PropBank-style semantic role la-
beling with a system that included the task of au-
tomatic verb frame identification, in tune with the
FrameNet literature; we believe that such a sys-
tem produces more interpretable output, both from
the perspective of human understanding as well as
downstream applications, than pipelines that are
oblivious to the verb frame, only focusing on ar-
gument analysis.
Acknowledgments
We thank Emily Pitler for comments on an early
draft, and the anonymous reviewers for their valu-
able feedback.
1456
References
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998.
The berkeley framenet project. In Proceedings of
COLING-ACL.
C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-
2007 Task 19: Frame semantic structure extraction.
In Proceedings of SemEval.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137?1155.
X. Carreras and L. M`arquez. 2004. Introduction to the
CoNLL-2004 shared task: Semantic role labeling.
In Proceedings of CoNLL.
X. Carreras and L. M`arquez. 2005. Introduction to the
CoNLL-2005 shared task: semantic role labeling. In
Proceedings of CoNLL.
R. Collobert and J. Weston. 2008. A unified architec-
ture for natural language processing: Deep neural
networks with multitask learning. In Proceedings of
ICML.
D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010.
Probabilistic frame-semantic parsing. In Proceed-
ings of NAACL-HLT.
D. Das, D. Chen, A. F. T. Martins, N. Schneider, and
N. A. Smith. 2014. Frame-semantic parsing. Com-
putational Linguistics, 40(1):9?56.
M.-C. de Marneffe and C. D. Manning, 2013. Stanford
typed dependencies manual.
C. Fellbaum, editor. 1998. WordNet: an electronic
lexical database.
C. J. Fillmore, C. R. Johnson, and M. R. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16(3):235?250.
C. J. Fillmore. 1982. Frame Semantics. In Linguis-
tics in the Morning Calm, pages 111?137. Hanshin
Publishing Co., Seoul, South Korea.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: The 90 In Pro-
ceedings of NAACL-HLT.
R. Johansson and P. Nugues. 2007. LTH: semantic
structure extraction using nonprojective dependency
trees. In Proceedings of SemEval.
A. Klementiev, I. Titov, and B. Bhattarai. 2012. In-
ducing crosslingual distributed representations of
words. In Proceedings of COLING.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML.
D. C. Liu and J. Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45(3):503 ? 528.
L. M`arquez, X. Carreras, K. C. Litkowski, and
S. Stevenson. 2008. Semantic role labeling: an in-
troduction to the special issue. Computational Lin-
guistics, 34(2):145?159.
A. Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The NomBank project: An interim report. In Pro-
ceedings of NAACL/HLT Workshop on Frontiers in
Corpus Annotation.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In Proceedings of ACL-
HLT.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
V. Punyakanok, D. Roth, and W. Yih. 2008. The im-
portance of syntactic parsing and inference in se-
mantic role labeling. Computational Linguistics,
34(2):257?287.
R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and
C. D. Manning. 2011. Semi-supervised recursive
autoencoders for predicting sentiment distributions.
In Proceedings of EMNLP.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word
representations: A simple and general method for
semi-supervised learning. In Proceedings of ACL,
Stroudsburg, PA, USA.
N. Usunier, D. Buffoni, and P. Gallinari. 2009. Rank-
ing with ordered weighted pairwise classification. In
ICML.
J. Uszkoreit and T. Brants. 2008. Distributed word
clustering for large scale class-based language mod-
eling in machine translation. In Proceedings of
ACL-HLT.
K. Q. Weinberger and L. K. Saul. 2009. Distance met-
ric learning for large margin nearest neighbor clas-
sification. Journal of Machine Learning Research,
10:207?244.
J. Weston, S. Bengio, and N. Usunier. 2011. Wsabie:
Scaling up to large vocabulary image annotation. In
Proceedings of IJCAI.
N. Xue and M. Palmer. 2004. Calibrating features for
semantic role labeling. In Proceedings of EMNLP
2004.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Pro-
ceedings of ACL-HLT.
1457
Number Filename
dev-1 LUCorpus-v0.3 20000420 xin eng-NEW.xml
dev-2 NTI SouthAfrica Introduction.xml
dev-3 LUCorpus-v0.3 CNN AARONBROWN ENG 20051101 215800.partial-NEW.xml
dev-4 LUCorpus-v0.3 AFGP-2002-600045-Trans.xml
dev-5 PropBank TicketSplitting.xml
dev-6 Miscellaneous Hijack.xml
dev-7 LUCorpus-v0.3 artb 004 A1 E1 NEW.xml
dev-8 NTI WMDNews 042106.xml
dev-9 C-4 C-4Text.xml
dev-10 ANC EntrepreneurAsMadonna.xml
dev-11 NTI LibyaCountry1.xml
dev-12 NTI NorthKorea NuclearOverview.xml
dev-13 LUCorpus-v0.3 20000424 nyt-NEW.xml
dev-14 NTI WMDNews 062606.xml
dev-15 ANC 110CYL070.xml
dev-16 LUCorpus-v0.3 CNN ENG 20030614 173123.4-NEW-1.xml
Table 7: List of files used as development set for the FrameNet 1.5 corpus.
A Development Data
Table 7 features a list of the 16 randomly selected
documents from the FrameNet 1.5 corpus, which
we used for development. The resultant develop-
ment set consists of roughly 4,500 predicates. We
use the same test set as in Das et al (2014), con-
taining 23 documents and 4,458 predicates.
1458
