Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1142?1151,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
On the Role of Lexical Features in Sequence Labeling
Yoav Goldberg
?
and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
{yoavg|elhadad}@cs.bgu.ac.il
Abstract
We use the technique of SVM anchoring to
demonstrate that lexical features extracted
from a training corpus are not necessary to
obtain state of the art results on tasks such
as Named Entity Recognition and Chunk-
ing. While standard models require as
many as 100K distinct features, we derive
models with as little as 1K features that
perform as well or better on different do-
mains. These robust reduced models in-
dicate that the way rare lexical features
contribute to classification in NLP is not
fully understood. Contrastive error analy-
sis (with and without lexical features) in-
dicates that lexical features do contribute
to resolving some semantic and complex
syntactic ambiguities ? but we find this
contribution does not generalize outside
the training corpus. As a general strat-
egy, we believe lexical features should not
be directly derived from a training corpus
but instead, carefully inferred and selected
from other sources.
1 Introduction
Common NLP tasks, such as Named Entity
Recognition and Chunking, involve the identifi-
cation of spans of words belonging to the same
phrase. These tasks are traditionally reduced to
a tagging task, in which each word is to be clas-
sified as either Beginning a span, Inside a span,
or Outside of a span. The decision is based on
the word to be classified and its neighbors. Fea-
tures supporting the classification usually include
?
Supported by the Lynn and William Frankel Center for
Computer Sciences, Ben Gurion University
the word forms themselves and properties derived
from the word forms, such as prefixes, suffixes,
capitalization information, and parts-of-speech.
While early approaches to the NP-chunking task
(Cardie and Pierce, 1998) relied on part-of-speech
information alone, it is widely accepted that lexi-
cal information (word forms) is crucial for build-
ing accurate systems for these tasks. Indeed,
all the better-performing systems in the CoNLL
shared tasks competitions for Chunking (Sang and
Buchholz, 2000) and Named Entity Recognition
(Tjong Kim Sang, 2002; Tjong Kim Sang and
De Meulder, 2003) make extensive use of such
lexical information.
Is this belief justified? In this paper, we show
that the influence of lexical features on such se-
quence labeling tasks is more complex than is gen-
erally assumed. We find that exact word forms
aren?t necessary for accurate classification. This
observation is important because relying on the
exact word forms that appear in a training corpus
leads to over-fitting, as well as to larger models.
In this work, we focus on learning with Support
Vector Machines (SVMs) (Vapnik, 1995). SVM
classifiers can handle very large feature spaces,
and produce state-of-the-art results for NLP ap-
plications (see e.g. (Kudo and Matsumoto, 2000;
Nivre et al, 2006)). Alas, when trained on pruned
feature sets, in which rare lexical items are re-
moved, SVM models suffer a loss in classifica-
tion accuracy. It would seem that rare lexical
items are indeed crucial for SVM classification
performance. However, in Goldberg and Elhadad
(2007), we suggested that the SVM learner is us-
ing the rare lexical features for singling out hard
cases rather than for learning meaningful general-
izations. We provide further evidence to support
this claim in this paper.
1142
We show that by using a variant of SVM ?
Anchored SVM Learning (Goldberg and Elhadad,
2007) with a polynomial kernel, one can learn
accurate models for English NP-chunking (Mar-
cus and Ramshaw, 1995), base-phrase chunking
(CoNLL 2000), and Dutch Named Entity Recog-
nition (CoNLL 2002), on a heavily pruned feature
space. Our models make use of only a fraction
of the lexical features available in the training set
(less than 1%), and yet provide highly-competitive
accuracies.
For the Chunking and NP-Chunking tasks, the
most heavily pruned experiments, in which we
consider only features appearing at least 100 times
in the training corpus, do show a small but signif-
icant drop in accuracy on the testing corpus com-
pared to the non-pruned models exposed to all
available features in the training data. We pro-
vide detailed error analysis of a development set
in Section 6, revealing the causes for these differ-
ences. We suggest one additional binary feature
in order to account for some of the performance
gap. Moreover, we show that the differences in
accuracy vanish when the lexicalized and unlexi-
calized models are tested on text from slightly dif-
ferent sources than the training corpus (Section 7).
This goes to show that with an appropriate
learning method, orthographic and structural (in
the form of POS tag sequences) information is suf-
ficient for achieving state-of-the-art performance
on these kind of sequence labeling tasks. This
does not mean semantic information is not needed
for these tasks. It does mean that current models
capture only a tiny amount of such semantic in-
formation through rare lexical features, and in a
manner that does not generalize well.
We believe this data motivates a different strat-
egy to incorporate lexical features into classifica-
tion models: instead of collecting the raw lexical
forms appearing in a training corpus, we should at-
tempt to actively construct a feature space includ-
ing lexical features derived from external sources.
The feature representation of (Collobert and We-
ston, 2008) could be a step in that direction. We
also believe that hard cases for sequence labeling
(POS ambiguity, coordination, long syntactic con-
structs) could be directly approached with special-
ized classifiers.
1.1 Related Work
This work complements a similar line of results
from the parsing literature. While it was ini-
tially believed that lexicalization of PCFG parsers
(Collins, 1997; Charniak, 2000) is crucial for
obtaining good parsing results, Gildea (2001)
demonstrated that the lexicalized Model-1 parser
of Collins (1997) does not benefit from bilexical
information when tested on a new text domain,
and only marginally benefits from such informa-
tion when tested on the same text domain as the
training corpora. This was followed by (Bikel,
2004) who showed that bilexical-information is
used in only 1.49% of the decisions in Collins?
Model-2 parser, and that removing this informa-
tion results in ?an exceedingly small drop in per-
formance?. However, uni-lexical information was
still considered crucial. Klein and Manning (2003)
bridged the gap between lexicalized and unlexi-
calized parsing performance, providing a compet-
itive unlexicalized parsing model, relying on lex-
ical information for only a few closed-class lex-
ical items. This was recently followed by (Mat-
suzaki et al, 2005; Petrov et al, 2006) who intro-
duce state-of-the-art nearly unlexicalized PCFG
parsers.
Similarly for discriminative dependency pars-
ing, state-of-the-art parsers (McDonald, 2006;
Nivre et al, 2006) are highly lexicalized. How-
ever, the model analysis in (McDonald, 2006)
reveals that bilexical features hardly contribute
to the performance of a discriminative MST-
based dependency parser, while Kawahara and
Uchimoto (2007) demonstrate that minimally-
lexicalized shift-reduce based dependency parsers
can produce near state-of-the-art accuracy.
In this work, we address the same question of
determining the impact of lexical features on a dif-
ferent family of tasks: sequence labeling, as illus-
trated by named entity recognition and chunking.
As discussed above, all state-of-the-art published
methods rely on lexical features for such tasks
(Zhang et al, 2001; Sha and Pereira, 2003; Finkel
et al, 2005; Ratinov and Roth, 2009). Sequence
labeling includes both a structural aspect (bracket-
ing the chunks) and a tagging aspect (classifying
the chunks). While we expect the structural aspect
can benefit from techniques similar to those used
in the parsing literature, it is unclear whether the
tagging component could perform well without
detailed lexical information. We demonstrate in
this work that, indeed, lexical features are not nec-
essary to obtain competitive performance. Our ap-
proach consists in performing a detailed analysis
1143
of the role played by rare lexical features in SVM
models. We distinguish the information brought to
the model by such features from the role they play
in a specific learning method.
2 Learning with Less Features
We adopt the common feature representation in
which each data-point is represented as a sparse
D dimensional binary-valued vector f . Each of
the D possible features f
i
is an indicator func-
tion. The indicator functions look at properties of
the current or neighbouring words. An example
of such function f
i
is 1 iff the previous
word-form is DOG, 0 otherwise. The
lexical (word-form) features result in extremely
high-dimensional (yet very sparse) feature vectors
? each word-form in the vocabulary of the training
set correspond to (at-least) one indicator function.
Due to the Zipfian distribution of language data,
many of the lexical features are very rare, and ap-
pear only a couple times in the training set. Ide-
ally, we would like our classifiers to learn only
from robust features: consider only features that
appear at least k times in the training data (rare-
feature pruning). These features are more likely to
appear in unseen test data, and thus such features
can support more robust generalization.
However, we find empirically that performing
such feature pruning prior to learning SVM mod-
els hurts the performance of the learned models.
Our intuition is that this sensitivity to rare lexi-
cal features is not explained by the richness of in-
formation such rare features bring to the model.
Instead, we believe that rare lexical features help
the classifier because they make the data artifi-
cially more separable. To demonstrate this claim,
we experiment with anchored SVM, which intro-
duces artificial mechanical anchors into the model
to achieve separability, and make rare lexical fea-
tures unnecessary.
3 Learning Method
SVM are discriminative, max-margin, linear clas-
sifiers (Vapnik, 1995), which can be kernelized.
For the formulation of SVMs in the context of
NLP applications, see (Kudo and Matsumoto,
2001). SVMs with a polynomial kernel of degree
2 were shown to provide state-of-the-art perfor-
mance in many NLP application, see for example
(Kudo and Matsumoto, 2000; Nivre et al, 2006;
Isozaki and Kazawa, 2002; Goldberg et al, 2006).
SVMs cope with inseparable data by introduc-
ing a soft-margin ? allowing some of the training
instances to be classified incorrectly subject to a
penalty, controlled by a parameter C.
Anchored SVM As we show in Section 5, the
soft-margin heuristic performs sub-optimally for
NLP tasks when the data is inseparable. We use in-
stead the Anchored Learning heuristic, introduced
in (Goldberg and Elhadad, 2007). The idea behind
anchored learning is that some training instances
are inherently ambiguous. This ambiguity stems
from ambiguity in language structure, which can-
not be resolved with a given feature representa-
tion. When a data-point cannot be classified, it
might be due to missing information, which is not
available in the data representation. Instead of al-
lowing ambiguous items to be misclassified during
training, we make the training data artificially sep-
arable. This is achieved by adding a unique feature
to each training example (an anchor). These an-
chor features cause each data-point to be slightly
more similar to itself than to any other data point.
At test time, we remove anchor features.
In terms of kernel-based learning, anchored learn-
ing can be achieved by redefining the dot product
between two vectors to take into account the iden-
tity of the vectors: x
i
?
anc
x
j
= x
i
? x
j
+ ?
ij
.
The classifier learned over the anchored data
takes into account the fine interactions between
the various inseparable data points. In our ex-
periments, SVM models over anchored data have
many more support vectors than soft-margin SVM
models. However, the anchored models generalize
much better when less features are available.
Relation to L2 SVM The classic soft-margin
SVM formulation uses L1-penalty for misclassi-
fied instances. Specifically, the objective of the
learner is to minimize
1
2
||w||
2
+ C
?
i
?
i
subject
to some margin constraints, where w is a weight
vector to be learned and ?
i
is the misclassification
error for instance i. This is equivalent to maximiz-
ing the dual problem:
?
M
i=1
?
i
?
1
2
?
i,j
?
i
?
j
y
i
y
j
K(x
i
, x
j
)
Another variant is L2-penalty SVM (Koshiba
and Abe, 2003), in which there is a quadratic
penalty for misclassified instances.
Here, the learning objective is to minimize:
1
2
||w||
2
+
1
2
C
?
i
?
2
i
or alternatively maximize the
dual:
?
i
?
i
?
1
2
?
i,j
?
i
?
j
y
i
y
j
(K(x
i
, x
j
) +
?
ij
C
).
Interestingly, for the linear kernel, SVM-
1144
anchoring reduces to L2-SVM with C=1. How-
ever, for the case of non-linear kernels, anchored
and L2-SVM produce different results, as the an-
choring is applied prior to the kernel expansion.
Specifically for the case of the second-degree
polynomial kernel, L2-SVM aims to maximize:
?
i
?
i
?
1
2
?
i,j
?
i
?
j
y
i
y
j
((x
i
? x
j
+ 1)
2
+
?
ij
C
),
while the anchored-SVM variant would maxi-
mizes:
?
i
?
i
?
1
2
?
i,j
?
i
?
j
y
i
y
j
(x
i
?x
j
+?
ij
+1)
2
.
In our experiments, as discussed in Section
5.4, we find that anchored-SVM and soft-margin
SVM with tuned C value both reach good re-
sults when we reduce the amount of lexical fea-
tures. Anchored-SVM, however, does not require
fine-tuning of the error-parameter C since it in-
sures separability. As a result, we learn anchored-
SVM models quickly (few hours) as opposed to
several days per model for C-tuned soft-margin
SVM. Anchored-SVMs also provide an easy ex-
planation of the role of features in terms of sepa-
rability. Therefore, we use anchored-SVMs in our
experiments as the learning method, but we expect
that other learning methods are capable of learning
with the same reduced feature sets.
4 Experiment Setup
How important are the rare lexical features for
learning accurate NLP models? To investigate
this question, we experiment with 3 different NLP
sequence-labeling tasks. For each task, we train a
sequence of polynomial kernel (d=2) SVM classi-
fiers, using both soft-margin (C=1) and anchored
SVM. Each classifier is trained on a pruned fea-
ture set, in which only features appearing at least
k times in the training data are kept. We vary the
pruning parameter k. Pruning is performed over
all the features in the model, but lexical features
are most affected by it.
For all the models, we use the B-I-O represen-
tation, and perform multiclass classification using
pairwise-voting. For our features, we consider
properties of tokens in a 5-token window centered
around the token to be classified, as well as the
two previous classifier predictions. Results are re-
ported as F-measure over labeled identified spans.
Polynomial vs. Linear models The polynomial
kernel of degree 2 allows us to efficiently and im-
plicitly include in our models all feature pairs.
Syntactic structure information as captured by
pairs of POS-tags and Word-POS pairs is certainly
important for such syntactic tasks as Chunking
and NER, as demonstrated by the many systems
described in (Sang and Buchholz, 2000; Tjong
Kim Sang, 2002). By using the polynomial ker-
nel, we can easily make use of this information
without intensive feature-tuning for the most suc-
cessful feature pairs.
L1-SVM, L2-SVM and the choice of the C pa-
rameter Throughout our experiments, we use the
?standard? variant of SVM, L1-penalty soft mar-
gin SVM, as implemented by the TinySVM
1
soft-
ware package, with the default C value of 1. This
setting is shown to produce good results for se-
quence labeling tasks in previous work (Kudo and
Matsumoto, 2000), and is what most end-users of
SVM classifiers are likely to use. As we show
in Sect.5.4, fine-tuning the C parameter reaches
better accuracy than L1-SVM with C=1. How-
ever, as this fine-tuning is computationally expen-
sive, we first report the comparison L1-SVM/C=1
vs. anchored-SVM, which consistently reached
the best results, and was the quickest to train.
Feature Pruning vs. Feature Selection Our aim
in this set of experiments is not to find the optimal
set of lexical features, but rather to demonstrate
that most lexical items are not needed for accurate
classification in sequence labeling tasks. To this
end, we perform very crude frequency based fea-
ture pruning. We believe better motivated feature
selection technique taking into account linguistic
(e.g. prune only open-class words) or statistic in-
formation could result in slightly more accurate
models with even fewer lexical items.
5 Experiments and Results
5.1 Named Entity Recognition (NER)
We use the Dutch data set from the CoNLL 2002
shared task (Tjong Kim Sang, 2002). The aim is to
identify named entities (persons, locations, orga-
nizations and miscellaneous) in text. The task has
two stages: identification of the entities, and clas-
sification of the identified entities into their corre-
sponding types. We focus here on the identifica-
tion task.
Features: We use the following properties for
each of the relevant tokens: word-form, POS,
ORT, prefix1, prefix2, prefix3, suffix1, suffix2,
suffix3. The ORT feature can take one of the fol-
lowing values: {number, contains-digit, contains-
hyphen, capitalized, all-capitalized, URL, punctu-
ation, regular}.
1
http://chasen.org/?taku/software/TinySVM/
1145
PRUNING #FEATURES SOFT-MARGIN ANCHORED
0 186,421 90.92 90.78
100 5,804 90.73 90.75
1000 1,207 88.56 90.10
1500 821 85.92 89.29
Table 1: Named Entity Identification results (F-
score) on dev set, with various pruning thresholds.
Results are presented in Table 1. Without fea-
ture pruning, we achieve an F-score of 90.9. This
dataset proved to be quite resilient to feature prun-
ing. Pruning features appearing less than 100
times results in just a slight decrease in F-score.
Extremely aggressive pruning, keeping only fea-
tures appearing more than 1,000 or 1,500 times in
the training data, results in a big drop in F-score
for the soft-margin SVM (from about 91 to 86).
Much less so for the Anchored-SVM. Using An-
chored SVM we achieve an F-score of 90.1 after
pruning with k = 1, 000. This model has 1207 ac-
tive features, and 27 unique active lexical forms.
5.2 NP Chunking
The goal of this task (Marcus and Ramshaw, 1995)
is the identification of non-recursive NPs. We use
the data from the CoNLL 2000 shared task: NP
chunks are extracted from Sections 15-18 (train)
and 20 (test) of the Penn WSJ corpus. POS tagged
are automatically assigned by the Brill Tagger.
Features: We consider the POS and word-form of
each token.
PRUNING #FEATURES SOFT-MARGIN ANCHORED
0 92,805 94.12 94.08
1 46,527 93.78 94.09
2 32,583 93.58 94.00
5 18,092 93.42 94.01
10 10,812 93.00 93.98
20 5,952 92.48 93.92
50 2,436 92.33 93.96
100 1,168 91.94 93.83
Table 2: NP-Chunking results (F-score), with var-
ious pruning thresholds.
Results are presented in Table 2. Without fea-
ture pruning (k = 0), the soft-margin SVM per-
forms slightly better than the Anchored-SVM. Ei-
ther of the results are state-of-the-art for this task.
However, even modest pruning (k = 2) hurts
the soft-margin model significantly. Not so for
the anchored-SVM. Even with relatively aggres-
sive pruning (k = 100), the anchored model still
achieves an impressive F-score of 93.83. Remark-
ably, in that last model, there are only 1,168 active
features, and only 209 unique active lexical forms.
5.3 Chunking
The goal of the Chunking task (Sang and Buch-
holz, 2000) is the identification of an assortment
of linguistic base-phrases. We use the data from
the CoNLL 2000 shared task.
Features: We perform two experiments. In the
first experiment, we consider the POS and word-
form of each token. In this setting, feature pruning
resulted in a bigger loss in performance than in
the two previous tasks. Preliminary error analysis
revealed that many errors are due to tagger errors,
especially of the present participle forms. This led
us to the second experiment, in which we added as
features the 2- and 3- letter suffixes for the word to
be classified (but not for the surrounding words).
Results are presented in Tables 3 and 4. In the
first experiment (POS + Word), the non-pruned
soft-margin model is the same system as the top-
performing system in the original shared task,
and yields state-of-the-art results. Unlike the NP-
chunking case, here feature pruning has a rela-
tively large impact on the results even for the an-
chored models. However, the anchored models
are still far more robust than the soft-margin ones.
With k = 100 pruning, the soft-margin model suf-
fers a drop of 2.5 F points, while the anchored
model suffers a drop of only 0.84 F points. Even
after this drop, the anchored k = 100 model still
performs above the top-third system in the CoNLL
2000 shared task. This anchored k = 100 model
has 1,180 active features, and only 209 unique ac-
tive lexical features.
The second experiment (POS + word-form +
suffixes for main word) adds crude morphological
information to the learner, helping it to avoid com-
mon tagger mistakes. This additional information
is helpful: pruning with k = 100 leads to an ac-
curate anchored model (93.12 F) with only 209
unique lexical items. Note that with the addition
of the suffix features, the pruned model k = 20
beats the purely lexical model (no suffix features)
with no pruning (93.51 vs. 93.44) with 10 times
less features. When we combine suffixes and all
lexical forms, we still see a slight advantage to
the lexical model (93.73 vs. 93.12 with pruning
at k = 100).
Even less lexicalization How robust are the suf-
fixes? We performed a third experiment, in which
1146
PRUNING #FEATURES SOFT-MARGIN ANCHORED
0 92,837 93.44 93.40
1 46,557 93.20 93.32
2 32,614 93.10 93.31
5 18,126 92.89 93.29
10 10,834 92.73 93.23
20 5,975 92.18 93.16
50 2,463 91.80 92.89
100 1,180 90.94 92.56
Table 3: Chunking results (F), with various prun-
ing thresholds. Experiment 1. Features: POS,
Word.
PRUNING #FEATURES SOFT-MARGIN ANCHORED
0 104,304 93.73 93.69
1 72,228 93.56 93.68
2 57,578 93.50 93.64
5 37,210 93.35 93.62
10 23,968 93.26 93.56
20 14,060 92.84 93.51
50 6,326 92.28 93.37
100 3,340 91.83 93.12
Table 4: Chunking results (F), with various prun-
ing thresholds. Experiment 2. Features: POS,
Word, {Suff2, Suff3} of main Word.
we replaced any explicit word-forms by 2- and 3-
letter suffixes. This gives us the complete word
form of many function words, and a reasonable
amount of morphological marking. Results are
presented in Table 5. Surprisingly, this infor-
mation proves to be quite robust. Without fea-
ture pruning, both the anchored and soft-margin
model achieve near state-of-the-art performance
of 93.25F. Pruning with k = 100 hurts the re-
sult of the soft-margin model, but the anchored
model remains robust with an F-score of 93.18.
This last model has 2,563 active features. With
further pruning (k = 250), the result of the an-
chored model drops to 92.87F (still 3rd place in
the CoNLL shared task), with only 1,508 active
features in the model.
5.4 Fine-tuned soft-margin SVMs
For the sake of completeness, and to serve as a bet-
ter comparison to the soft-margin SVM, we report
results of some experiments with both L1 and L2
SVMs, with tuned C values. NP-chunking perfor-
mance with tuned C values and various pruning
thresholds is presented in Table 6.
For these results, the C parameter was tuned
on a development set using Brent?s 1-dimension
minimization method (Brent, 1973). While tak-
ing about 40 hours of computation to fit, the fi-
PRUNING #FEATURES SOFT-MARGIN ANCHORED
0 19,910 93.25 93.23
100 2,563 92.87 93.18
250 1,508 92.40 92.87
Table 5: Chunking results (F), with various prun-
ing thresholds. Experiment 3. Features: POS ,
Suff2, Suff3 .
K L1 (C) L2 (C) ANCHORED
0 94.12 (1.0001) 94.09 (2.6128) 94.08
50 93.79 (0.0524) 93.71 (0.0082) 93.96
100 93.72 (0.0567) 93.59 (0.0072) 93.83
Table 6: NP-Chunking results (F), with various
pruning thresholds K, for L1 and L2 SVMs with
tuned C values
nal results catch up with those of the anchored-
SVM but still remain slightly lower. This further
highlights our main point: accurate models can
be achieved also with mostly unlexicalized mod-
els, and the lexical features do not contribute sub-
stantial semantic information, but rather affect the
separability of the data. This is nicely demon-
strated by SVM-anchoring, in which lexical infor-
mation is practically replaced by artificial seman-
tically void indexes, but similar performance can
also be achieved by fine-tuning other learning pa-
rameters.
6 Error Analysis
Our experiments so far indicate that very aggres-
sive feature pruning hurts performance slightly (by
about 0.5F point). The feature-pruned models are
still accurate, indicating that lexical features con-
tribute little to the classification accuracy. We now
investigate the differences between the lexicalized
and pruned models, in order to characterize the
kind of information that is available to the lexi-
calized models but missing from the pruned ones.
In the next section, we also verify that pruned-
models are more stable than the fully lexicalized
ones when tested over different text genres and do-
mains.
We focus our analysis on the chunking task, which
is a superset of the NP-chunking task. We compare
the fully lexicalized soft-margin SVM model with
the POS+suffix2+suffix3 anchored-SVM model
with k = 100 pruning. We analyze the mod-
els? respective performance on section 05 of the
WSJ corpus. This dataset is different than the of-
ficial test set. It is, however, part of the same an-
notated corpus as both the training and test sets.
On this dataset, the fully lexicalized SVM model
1147
achieves an F-score of 93.24, vs. 92.59 for the
suffix-based pruned anchored-SVM model. (The
pruned anchored-SVM model (k = 100) from ex-
periment 2, achieve a slightly higher F-score of
92.84)
We investigate only those chunks which are
identified correctly by one model but not by the
other. Overall, there are 440 chunks (363 unique)
which are identified correctly only by the lexical-
ized model, and 258 chunks (232 unique) only by
the pruned model.
Where the pruned model is always wrong
Some errors are unique to the pruned model.
Over 45 of the cases that are identified correctly
only in the lexicalized model (more than 10%) are
due to the words ?including? (18 cases) and ?If?
(9 cases), as well as other -ing forms such as ?fol-
lowing?, ?according?, ?rising? and ?suspecting.?
The word ?including? appears 80 times in the
training data, always tagged as VBG and func-
tioning as a PP chunk, which is an odd chunk
for VBGs. The lexicalized model easily picked
up on this behaviour, while the pruned model
couldn?t. Similarly, the word ?following/VBG?
appears 32 times, 20 of which as PP, and the
word ?according/VBG? 53 times, all of them as
PP. The pruned model could not distinguish those
from the rest of the VBGs and tagged them as
VPs. What seems to happen in these cases, is
that certain verbal forms participate in idiomatic
constructions and behave syntactically as preposi-
tions. The POS tagger does not pick this ambigu-
ity in function and contributes only the most likely
tag for the words (VBG). Lexical models learn that
certain VBGs are ?becoming? prepositions in the
observed dataset. These words do not appear as
specific features in the pruned models, and hence
these usage shifts are often misclassified. Interest-
ingly, the pruned model did learn that verbal forms
can sometimes be PPs: it made use of that infor-
mation by mis-identifying 11 verbal VBGs and 6
verbal VBNs as PPs.
The word ?If/IN?, unlike most prepositions, it
always starts an SBAR rather than a PP chunk in
the corpus. The pruned model learned this be-
haviour correctly for the lower-cased ?if/IN?, but
missed the upper-cased version appearing in 79
sentence initial locations in the corpus.
These cases are caused by a mismatch between
the POS tag and the syntactic function observed in
the chunked dataset.
Additional cases include the adverbs (Already,
Nearby, Soon, Maybe, Perhaps, once, Then): they
are sometimes not chunked as ADVP but are left
outside of any chunk. Some one-word ADJP
chunks being chunked as NPs (short, general, sure,
worse, . . . ) (6 cases) and some are chunked as
ADVPs (hard, British-born, . . . ) (4 cases).
There are 10 cases where the pruned model
splits an NP into ADVP and NP, such as:
[later] [this week], [roughly][18 more U.S. stores]. In
addition, the pruned model failed to learn the con-
struction ?typical of?, resulting in 2 NP chunks
such as: [The more intuitive approach typical].
Some mistakes of the pruned model seem
like mistakes/pecularities of the annotated corpus,
which the lexicalized model found a way to work
around. Consider the following gold-standard
cases from the annotated corpus:
- [ VP seems ] [ ADVP rarely ] [ VP to cut ]
- [ ADVP just ] [ PP after ]
- [ VP is ] [ NP anything ] [ O but ] [ VP fixing ]
- [ ADJP as high ] [ PP as ] [ NP 8.3 % ]
- [ ADJP less ] [ PP than ] [ ADJP rosy ]
- [ NP 40 % ] [ PP to ] [ NP 45 % ]
Which were each identified as a single chunk by
the pruned model. It can be argued these are mis-
takes in the tagged dataset.
Where the lexical model is sometimes better
Both models fail on conjunctions, but the lexical-
ized model do slightly better. Conjunction error
types come in two main varieties, either chunking
[x][and][y] instead of [x and y] (pruned: 21 cases,
lex: 14 cases) or chunking [x and y] instead of
[x][and][y] (pruned: 26 cases, lex: 24 cases).
Joining VP and NP into an NP, due
to a verb/adj ambiguity. For exam-
ple chunking [NP fired six executives] in-
stead of [VP fired] [NP six executives],
or [NP keeping viewers] instead of
[VP keeping] [NP viewers]. 12 such cases are
resolved correctly only by the lexicalized model,
and 5 only by the pruned one.
SBAR/PP confusion for words such as:
?as?,?after?,?with?,?since? (both ways). 13 cases
for the pruned model, 6 for lexicalized one.
Where both model are similar
Merging back-to-back NPs: Both models tend
to erroneously join back-to-back NPs to a sin-
gle NP, e.g. : [NP Westinghouse this year], or
[NP themselves fashion enterprises]. No model is bet-
ter than the other on these cases, each model failed
1148
on 16 cases the other model succeeded on.
Joining NP and VP into an NP due to
Verb/Noun ambiguity and tagger mistakes:
- [NP the weekend] [VP making] ? [NP the weekend making]
- [NP the competition] [VP can] ? [NP the competition can]
(lexicalized: 6 errors, pruned: 8 errors)
And splitting some NPs to VP+NP due to the
same reasons:
- [VP operating] [NP profit]
- [VP improved] [NP average yield]
(lexicalized: 5 errors, pruned: 7 errors)
The word ?that? is confused between SBAR and
NP (5 mistakes for each model)
Erroneously splitting range NPs, e.g. :
- [about $115][to][$125] (2 cases for each model).
Where the pruned model is better
There are some cases where the pruned models is
doing better than the lexicalized one:
VP wrongly split into VP and ADJP:
- [remains] [banned]
4 mistakes for lexicalized, 1 for pruned
VP wrongly split into VP and VP:
- [were scheduled] [to meet]
- [used] [to complain]
3 mistakes for lexicalized, 1 for pruned
VP wrongly split into ADVP and VP:
- [largly][reflecting]
- [selectively][leaking]
6 mistakes for lexicalized, 1 for pruned
PP and SBAR confusion:
- of, with, As, after
9 mistakes for lex, 5 for pruned
VP chunked as NP due to tagger mistake:
- [NP ruling], [NP drives], [NP cuts]
6 mistakes for lex, 2 for pruned
?that? tagged as NP instead of SBAR:
2 mistakes for lex, 0 for pruned
To conclude
Both the pruned and the fully lexicalized models
have problems dealing with non-local phenomena
such as coordination and relative clauses, as well
as verb/adjective ambiguities and VBG/Noun am-
biguities. They also perform poorly on embeded
syntactic constructions (such as an NP containing
an ADJP), and on identification of back-to-back
NPs, which often requires semantic knowledge.
Both models suffer from tagging mistakes of the
underlying tagger and systematic ambiguity be-
tween the morphological tag assigned by the tag-
ger and the syntactic tag in which the word oper-
ates (e.g., ?including? used as a preposition).
The main advantage of the fully lexcialized
model is in dealing with:
? Some coordinated constructions.
? Some cases of verb/adjective ambiguities.
? Specific function words not seen much in
training.
? Idiomatic usages of some VBG/VBN forms
functioning as prepositions.
The first two items are semantic in nature, and hint
that lexical features do capture some semantic in-
formation. While this might be true on the spe-
cific corpus, we believe that such corpus-derived
semantic knowledge is very restricted, is not gen-
eralizable, and will not transfer well to other cor-
pora, even on the same genre. We provide evi-
dence for this claim in Section 7.
The last two items are syntactic. We address
them by introducing a slightly modified feature
model.
6.1 Another chunking Experiment
Based on the observations from the error analy-
sis, we performed another pruned-chunking exper-
iment, with the following features:
? Word and POS for a -2,+2 window around
the current token, and 2-and-3-letter suffixes
of the token to be classified (same as Experi-
ment 2 in Section 5.2 above).
? Features of words appearing as a preposi-
tion (IN) anywhere in the training set are
not pruned (this result in a model with 310
unique lexical items after k = 100 pruning).
? An additional binary feature indicating for
each token whether it can function as a PP.
The list of possible-PP forms is generated by
considering all tokens seen inside a PP in the
training corpus. It can be easily extended if
additional lexicographic resources are avail-
able, without retraining the model.
This last proposed feature incorporates important
lexical knowledge without relying on features for
specific lexical forms, and is more generalizable.
The accuracy of this new model on the develop-
ment and test set with various pruning thresholds
is presented in Table 7.
The addition of the CanBePrep feature im-
proves the fully-lexicalized model accuracy on the
development set (93.24 to 93.68), and does not af-
fect fully lexicalized result on the test set (93.71
1149
CORPUS SOURCE CONTENT #TOKENS
WSJ 4 articles from wsj.com business Magazine, business 2,671
Jaguar Wikipedia page on Jaguar Well edited text, animals 5,396
FreeWill Wikipedia page on Free Will Well edited text, philosophy 9,428
LJ-Life 4 LiveJournal posts Noisy teenage writing, life 870
Table 8: Corpus Variation Text Sources
PRUNING #FEATURES SOFT-MARGIN ANCHORED
Dev Set
0 92,989 93.71 ?
100 4,066 ? 93.22
Test Set
0 92,989 93.68 ?
100 4,066 ? 93.26
Table 7: Chunking results (F), with various prun-
ing thresholds. Experiment 4. Features: POS,
Word , Suff2, Suff3 for main word, CanBePrep .
vs. 93.73). The pruned model performance im-
proves in both cases, more so on the development
set (93.12 to 93.22 on the test set, 92.84 to 93.26
on the development set). The new model helps
bridging the gap between the fully lexicalized and
the pruned model, yet we still observe a lead of
0.4F for the fully lexicalized model. We now turn
to explore how meaningful this difference is in
real-world situation in which one does not operate
on the Penn-WSJ corpus.
7 Corpus Variation and Model
Performance
When tested on the exact same resource as the
models are trained on, the fully lexicalized model
still has a slight edge over the pruned ones. How
well does this lexical knowledge transfer to dif-
ferent text genres? We compare the models? per-
formance on text from various genres, ranging
from very similar to the training material (re-
cent articles from the WSJ Business section) to a
well-edited but different domain text (?Featured-
content? wikipedia pages) to a non-edited noisy
text (live-journal blog posts from the ?life? cate-
gory). As we do not have gold-annotated data for
these text genres, we analyze the few differences
between the models, manually inspecting the in-
stances on which the models disagree.
Table 8 describes our test corpora for this ex-
periment. We applied the fully-lexicalized and
the pruned (k = 100) anchored models described
in Section 6.1 to these texts, and compared the
chunking results. The results are presented in Ta-
ble 9.
When moving outside of the canonic training
TEXT #DIFF PRUNED LEX BOTH
CORRECT CORRECT WRONG
WSJ 13 9 4 0
Jaguar 45 20 20 7
FreeWill 118 51 38 29
LJ-Life 15 8 6 1
Table 9: Comparison of Models? performance on
different text genres
corpus, the fully lexicalized model have no advan-
tage over the heavily pruned one. On the contrary,
the pruned models seem to have a small advantage
in most cases (though it is hard to tell if the differ-
ences are significant). This is true even for texts
in the very same domain, genre and editing guide-
lines as the training corpus was derived from.
8 Discussion
For all the sequence labeling tasks we analyzed,
the anchored-SVM proved to be robust to feature
pruning. The experiments support the claim that
rare lexical features do not provide substantial in-
formation to the model, but instead play a role in
maintaining separability. When this role is taken
over by anchoring, we can obtain the same level
of performance with very few robust lexical fea-
tures. Yet, we cannot conclude that lexical infor-
mation is not needed. There is a significant differ-
ence between the pruned and non-pruned models
for the chunking task. We showed that this dif-
ference can be bridged to some extent by a binary
feature relating to idiomatic word usage, and that
the difference vanishes when testing outside of the
annotated corpus. The high classification accura-
cies achieved with the heavily pruned anchored-
SVM models sheds new light on the actual role
of lexical features, and indicating that there is still
a lot to be learned regarding the effective incor-
poration of lexical and semantic information into
our models. It is our view that semantic knowl-
edge should not be expected to be learned by in-
spection of raw lexical counts from an annotated
text corpus, but instead collected from sources ex-
ternal to the annotated corpora ? either based on
a very large unannotated corpora, or on manually
constructed lexical resources.
1150
References
Daniel M. Bikel. 2004. Intricacies of collins? parsing
model. Computational Linguistics, 30(4).
Richard P. Brent, 1973. Algorithms for Minimization
without Derivatives, chapter 4. Prentice-Hall.
Claire Cardie and David Pierce. 1998. Error-driven
pruning of treebank grammars for base noun phrase
identification. In ACL-1998.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proc of NAACL.
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Proc of EACL.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Proc. of
ICML.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proc of ACL.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proc of EMNLP.
Yoav Goldberg and Michael Elhadad. 2007. SVM
Model Tampering and Anchored Learning: A Case
Study in Hebrew. NP Chunking. In ACL2007.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2006. Noun Phrase Chunking in Hebrew: Influence
of Lexical and Morphological Features. In COL-
ING/ACL2006.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient
Support Vector Classifiers For Named Entity Recog-
nition. In COLING2002.
Daisuke Kawahara and Kiyotaka Uchimoto. 2007.
Miniamlly lexicalized dependency parsing. In Proc
of ACL (Short papers).
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proc. of ACL.
Yoshiaki Koshiba and Shigeo Abe. 2003. Comparison
of L1 and L2 support vector machines. In Proc. of
the International Joint Conference on Neural Net-
works, volume 3.
Taku Kudo and Yuji Matsumoto. 2000. Use of Sup-
port Vector Learning for Chunk Identification. In
CoNLL-2000.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In NAACL ?01.
Mitch P. Marcus and Lance A. Ramshaw. 1995.
Text Chunking Using Transformation-Based Learn-
ing. In 3rd ACL Workshop on Very Large Corpora.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic cfg with latent annotations. In
Proc of ACL.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Joakim Nivre, Johan Hall, and Jens Nillson. 2006.
MaltParser: A Data-Driven Parser-Generator for
Dependency Parsing. In LREC2006.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc of ACL.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proc of CONLL.
Erik F. Tjong Kim Sang and S. Buchholz. 2000. Intro-
duction to the CoNLL-2000 shared task: chunking.
In CoNLL-2000.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proc of NAACL.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 Shared
Task: Language-Independent Named Entity Recog-
nition. In CoNLL-2003.
Erik F. Tjong Kim Sang. 2002. Introduction to the
CoNLL-2002 Shared Task: Language-Independent
Named Entity Recognition. In CoNLL-2002.
Vladimir Vapnik. 1995. The nature of statistical learn-
ing theory. Springer-Verlag New York, Inc.
Tong Zhang, Fred Damerau, and David Johnson. 2001.
Text chunking using regularized winnow. In Proc of
ACL.
1151
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 327?335,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Enhancing Unlexicalized Parsing Performance
using a Wide Coverage Lexicon, Fuzzy Tag-set Mapping,
and EM-HMM-based Lexical Probabilities
Yoav Goldberg1? Reut Tsarfaty2? Meni Adler1? Michael Elhadad1
1Department of Computer Science, Ben Gurion University of the Negev
{yoavg|adlerm|elhadad}@cs.bgu.ac.il
2Institute for Logic, Language and Computation, University of Amsterdam
R.Tsarfaty@uva.nl
Abstract
We present a framework for interfacing
a PCFG parser with lexical information
from an external resource following a dif-
ferent tagging scheme than the treebank.
This is achieved by defining a stochas-
tic mapping layer between the two re-
sources. Lexical probabilities for rare
events are estimated in a semi-supervised
manner from a lexicon and large unanno-
tated corpora. We show that this solu-
tion greatly enhances the performance of
an unlexicalized Hebrew PCFG parser, re-
sulting in state-of-the-art Hebrew parsing
results both when a segmentation oracle is
assumed, and in a real-word parsing sce-
nario of parsing unsegmented tokens.
1 Introduction
The intuition behind unlexicalized parsers is that
the lexicon is mostly separated from the syntax:
specific lexical items are mostly irrelevant for ac-
curate parsing, and can be mediated through the
use of POS tags and morphological hints. This
same intuition also resonates in highly lexicalized
formalism such as CCG: while the lexicon cate-
gories are very fine grained and syntactic in na-
ture, once the lexical category for a lexical item is
determined, the specific lexical form is not taken
into any further consideration.
Despite this apparent separation between the
lexical and the syntactic levels, both are usually es-
timated solely from a single treebank. Thus, while
?Supported by the Lynn and William Frankel Center for
Computer Sciences, Ben Gurion University
?Funded by the Dutch Science Foundation (NWO), grant
number 017.001.271.
?Post-doctoral fellow, Deutsche Telekom labs at Ben Gu-
rion University
PCFGs can be accurate, they suffer from vocabu-
lary coverage problems: treebanks are small and
lexicons induced from them are limited.
The reason for this treebank-centric view in
PCFG learning is 3-fold: the English treebank is
fairly large and English morphology is fairly sim-
ple, so that in English, the treebank does provide
mostly adequate lexical coverage1; Lexicons enu-
merate analyses, but don?t provide probabilities
for them; and, most importantly, the treebank and
the external lexicon are likely to follow different
annotation schemas, reflecting different linguistic
perspectives.
On a different vein of research, current POS tag-
ging technology deals with much larger quantities
of training data than treebanks can provide, and
lexicon-based unsupervised approaches to POS
tagging are practically unlimited in the amount
of training data they can use. POS taggers rely
on richer knowledge than lexical estimates de-
rived from the treebank, have evolved sophisti-
cated strategies to handle OOV and can provide
distributions p(t|w, context) instead of ?best tag?
only.
Can these two worlds be combined? We pro-
pose that parsing performance can be greatly im-
proved by using a wide coverage lexicon to sug-
gest analyses for unknown tokens, and estimating
the respective lexical probabilities using a semi-
supervised technique, based on the training pro-
cedure of a lexicon-based HMM POS tagger. For
many resources, this approach can be taken only
on the proviso that the annotation schemes of the
two resources can be aligned.
We take Modern Hebrew parsing as our case
study. Hebrew is a Semitic language with rich
1This is not the case with other languages, and also not
true for English when adaptation scenarios are considered.
327
morphological structure. This rich structure yields
a large number of distinct word forms, resulting in
a high OOV rate (Adler et al, 2008a). This poses
a serious problem for estimating lexical probabili-
ties from small annotated corpora, such as the He-
brew treebank (Sima?an et al, 2001).
Hebrew has a wide coverage lexicon /
morphological-analyzer (henceforth, KC Ana-
lyzer) available2, but its tagset is different than the
one used by the Hebrew Treebank. These are not
mere technical differences, but derive from dif-
ferent perspectives on the data. The Hebrew TB
tagset is syntactic in nature, while the KC tagset
is lexicographic. This difference in perspective
yields different performance for parsers induced
from tagged data, and a simple mapping between
the two schemes is impossible to define (Sec. 2).
A naive approach for combining the use of the
two resources would be to manually re-tag the
Treebank with the KC tagset, but we show this ap-
proach harms our parser?s performance. Instead,
we propose a novel, layered approach (Sec. 2.1),
in which syntactic (TB) tags are viewed as contex-
tual refinements of the lexicon (KC) tags, and con-
versely, KC tags are viewed as lexical clustering
of the syntactic ones. This layered representation
allows us to easily integrate the syntactic and the
lexicon-based tagsets, without explicitly requiring
the Treebank to be re-tagged.
Hebrew parsing is further complicated by the
fact that common prepositions, conjunctions and
articles are prefixed to the following word and
pronominal elements often appear as suffixes. The
segmentation of prefixes and suffixes can be am-
biguous and must be determined in a specific con-
text only. Thus, the leaves of the syntactic parse
trees do not correspond to space-delimited tokens,
and the yield of the tree is not known in advance.
We show that enhancing the parser with external
lexical information is greatly beneficial, both in an
artificial scenario where the token segmentation is
assumed to be known (Sec. 4), and in a more re-
alistic one in which parsing and segmentation are
handled jointly by the parser (Goldberg and Tsar-
faty, 2008) (Sec. 5). External lexical informa-
tion enhances unlexicalized parsing performance
by as much as 6.67 F-points, an error reduction
of 20% over a Treebank-only parser. Our results
are not only the best published results for pars-
ing Hebrew, but also on par with state-of-the-art
2http://mila.cs.technion.ac.il/hebrew/resources/lexicons/
lexicalized Arabic parsing results assuming gold-
standard fine-grained Part-of-Speech (Maamouri
et al, 2008).3
2 A Tale of Two Resources
Modern Hebrew has 2 major linguistic resources:
the Hebrew Treebank (TB), and a wide coverage
Lexicon-based morphological analyzer developed
and maintained by the Knowledge Center for Pro-
cessing Hebrew (KC Analyzer).
The Hebrew Treebank consists of sentences
manually annotated with constituent-based syn-
tactic information. The most recent version (V2)
(Guthmann et al, 2009) has 6,219 sentences, and
covers 28,349 unique tokens and 17,731 unique
segments4.
The KC Analyzer assigns morphological analy-
ses (prefixes, suffixes, POS, gender, person, etc.)
to Hebrew tokens. It is based on a lexicon of
roughly 25,000 word lemmas and their inflection
patterns. From these, 562,439 unique word forms
are derived. These are then prefixed (subject to
constraints) by 73 prepositional prefixes.
It is interesting to note that even with these
numbers, the Lexicon?s coverage is far from com-
plete. Roughly 1,500 unique tokens from the He-
brew Treebank cannot be assigned any analysis
by the KC Lexicon, and Adler et al(2008a) report
that roughly 4.5% of the tokens in a 42M tokens
corpus of news text are unknown to the Lexicon.
For roughly 400 unique cases in the Treebank, the
Lexicon provides some analyses, but not a correct
one. This goes to emphasize the productive nature
of Hebrew morphology, and stress that robust lex-
ical probability estimates cannot be derived from
an annotated resource as small as the Treebank.
Lexical vs. Syntactic POS Tags The analyses
produced by the KC Analyzer are not compatible
with the Hebrew TB.
The KC tagset (Adler et al, 2008b; Netzer et
al., 2007; Adler, 2007) takes a lexical approach to
POS tagging (?a word can assume only POS tags
that would be assigned to it in a dictionary?), while
the TB takes a syntactic one (?if the word in this
particular positions functions as an Adverb, tag it
as an Adverb, even though it is listed in the dictio-
nary only as a Noun?). We present 2 cases that em-
phasize the difference: Adjectives: the Treebank
3Our method is orthogonal to lexicalization and can be
used in addition to it if one so wishes.
4In these counts, all numbers are conflated to one canoni-
cal form
328
treats any word in an adjectivial position as an Ad-
jective. This includes also demonstrative pronouns
?? ??? (this boy). However, from the KC point of
view, the fact that a pronoun can be used to modify
a noun does not mean it should appear in a dictio-
nary as an adjective. The MOD tag: similarly,
the TB has a special POS-tag for words that per-
form syntactic modification. These are mostly ad-
verbs, but almost any Adjective can, in some cir-
cumstances, belong to that class as well. This cat-
egory is highly syntactic, and does not conform to
the lexicon based approach.
In addition, many adverbs and prepositions in
Hebrew are lexicalized instances of a preposition
followed by a noun (e.g., ?????, ?in+softness?,
softly). These can admit both the lexical-
ized and the compositional analyses. Indeed,
many words admit the lexicalized analyses in
one of the resource but not in the other (e.g.,
????? ?for+benefit? is Prep in the TB but only
Prep+Noun in the KC, while for ??? ?from+side?
it is the other way around).
2.1 A Unified Resource
While the syntactic POS tags annotation of the TB
is very useful for assigning the correct tree struc-
ture when the correct POS tag is known, there are
clear benefits to an annotation scheme that can be
easily backed by a dictionary.
We created a unified resource, in which every
word occurrence in the Hebrew treebank is as-
signed a KC-based analysis. This was done in a
semi-automatic manner ? for most cases the map-
ping could be defined deterministically. The rest
(less than a thousand instances) were manually as-
signed. Some Treebank tokens had no analyses
in the KC lexicon, and some others did not have
a correct analysis. These were marked as ?UN-
KNOWN? and ?MISSING? respectively.5
The result is a Treebank which is morpho-
logically annotated according to two different
schemas. On average, each of the 257 TB tags
is mapped to 2.46 of the 273 KC tags.6 While this
resource can serve as a basis for many linguisti-
cally motivated inquiries, the rest of this paper is
5Another solution would be to add these missing cases to
the KC Lexicon. In our view this act is harmful: we don?t
want our Lexicon to artificially overfit our annotated corpora.
6A ?tag? in this context means the complete morphologi-
cal information available for a morpheme in the Treebank: its
part of speech, inflectional features and possessive suffixes,
but not prefixes or nominative and accusative suffixes, which
are taken to be separate morphemes.
devoted to using it for constructing a better parser.
Tagsets Comparison In (Adler et al, 2008b),
we hypothesized that due to its syntax-based na-
ture, the Treebank morphological tagset is more
suitable than the KC one for syntax related tasks.
Is this really the case? To verify it, we simulate a
scenario in which the complete gold morpholog-
ical information is available. We train 2 PCFG
grammars, one on each tagged version of the Tree-
bank, and test them on the subset of the develop-
ment set in which every token is completely cov-
ered by the KC Analyzer (351 sentences).7 The
input to the parser is the yields and disambiguated
pre-terminals of the trees to be parsed. The parsing
results are presented in Table 1. Note that this sce-
nario does not reflect actual parsing performance,
as the gold information is never available in prac-
tice, and surface forms are highly ambiguous.
Tagging Scheme Precision Recall
TB / syntactic 82.94 83.59
KC / dictionary 81.39 81.20
Table 1: evalb results for parsing with Oracle
morphological information, for the two tagsets
With gold morphological information, the TB
tagging scheme is more informative for the parser.
The syntax-oriented annotation scheme of the
TB is more informative for parsing than the lexi-
cographic KC scheme. Hence, we would like our
parser to use this TB tagset whenever possible, and
the KC tagset only for rare or unseen words.
A Layered Representation It seems that learn-
ing a treebank PCFG assuming such a different
tagset would require a treebank tagged with the
alternative annotation scheme. Rather than assum-
ing the existence of such an alternative resource,
we present here a novel approach in which we
view the different tagsets as corresponding to dif-
ferent aspects of the morphosyntactic representa-
tion of pre-terminals in the parse trees. Each of
these layers captures subtleties and regularities in
the data, none of which we would want to (and
sometimes, cannot) reduce to the other. We, there-
fore, propose to retain both tagsets and learn a
fuzzy mapping between them.
In practice, we propose an integrated represen-
tation of the tree in which the bottommost layer
represents the yield of the tree, the surface forms
7For details of the train/dev splits as well as the grammar,
see Section 4.2.
329
are tagged with dictionary-based KC POS tags,
and syntactic TB POS tags are in turn mapped onto
the KC ones (see Figure 1).
TB: KC: Layered:
...
JJ-ZYTB
??
...
PRP-M-S-3-DEMKC
??
...
JJ-ZYTB
PRP-M-S-3-DEMKC
??
...
INTB
??????
...
INKC
?
...
NN-F-SKC
?????
...
INTB
INKC
?
NN-F-SKC
?????
Figure 1: Syntactic (TB), Lexical (KC) and
Layered representations
This representation helps to retain the informa-
tion both for the syntactic and the morphologi-
cal POS tagsets, and can be seen as capturing the
interaction between the morphological and syn-
tactic aspects, allowing for a seamless integra-
tion of the two levels of representation. We re-
fer to this intermediate layer of representation as
a morphosyntactic-transfer layer and we formally
depict it as p(tKC |tTB).
This layered representation naturally gives rise
to a generative model in which a phrase level con-
stituent first generates a syntactic POS tag (tTB),
and this in turn generates the lexical POS tag(s)
(tKC). The KC tag then ultimately generates the
terminal symbols (w). We assume that a morpho-
logical analyzer assigns all possible analyses to a
given terminal symbol. Our terminal symbols are,
therefore, pairs: ?w, t?, and our lexical rules are of
the form t? ?w, t?. This gives rise to the follow-
ing equivalence:
p(?w, tKC?|tTB) = p(tKC |tTB)p(?w, tKC?|tKC)
In Sections (4, 5) we use this layered gener-
ative process to enable a smooth integration of
a PCFG treebank-learned grammar, an external
wide-coverage lexicon, and lexical probabilities
learned in a semi-supervised manner.
3 Semi-supervised Lexical Probability
Estimations
A PCFG parser requires lexical probabilities
of the form p(w|t) (Charniak et al, 1996).
Such information is not readily available in
the lexicon. However, it can be estimated
from the lexicon and large unannotated cor-
pora, by using the well-known Baum-Welch
(EM) algorithm to learn a trigram HMM tagging
model of the form p(t1, . . . , tn, w1, . . . , wn) =
argmax
?
p(ti|ti?1, ti?2)p(wi|ti), and taking
the emission probabilities p(w|t) of that model.
In Hebrew, things are more complicated, as
each emission w is not a space delimited token, but
rather a smaller unit (a morphological segment,
henceforth a segment). Adler and Elhadad (2006)
present a lattice-based modification of the Baum-
Welch algorithm to handle this segmentation am-
biguity.
Traditionally, such unsupervised EM-trained
HMM taggers are thought to be inaccurate, but
(Goldberg et al, 2008) showed that by feeding the
EM process with sufficiently good initial proba-
bilities, accurate taggers (> 91% accuracy) can be
learned for both English and Hebrew, based on a
(possibly incomplete) lexicon and large amount of
raw text. They also present a method for automat-
ically obtaining these initial probabilities.
As stated in Section 2, the KC Analyzer (He-
brew Lexicon) coverage is incomplete. Adler
et al(2008a) use the lexicon to learn a Maximum
Entropy model for predicting possible analyses for
unknown tokens based on their orthography, thus
extending the lexicon to cover (even if noisily) any
unknown token. In what follows, we use KC Ana-
lyzer to refer to this extended version.
Finally, these 3 works are combined to create
a state-of-the-art POS-tagger and morphological
disambiguator for Hebrew (Adler, 2007): initial
lexical probabilities are computed based on the
MaxEnt-extended KC Lexicon, and are then fed
to the modified Baum-Welch algorithm, which is
used to fit a morpheme-based tagging model over
a very large corpora. Note that the emission prob-
abilities P (W |T ) of that model cover all the mor-
phemes seen in the unannotated training corpus,
even those not covered by the KC Analyzer.8
We hypothesize that such emission probabili-
ties are good estimators for the morpheme-based
P (T ? W ) lexical probabilities needed by a
PCFG parser. To test this hypothesis, we use it
to estimate p(tKC ? w) in some of our models.
4 Parsing with a Segmentation Oracle
We now turn to describing our first set of exper-
iments, in which we assume the correct segmen-
8P (W |T ) is defined also for words not seen during train-
ing, based on the initial probabilities calculation procedure.
For details, see (Adler, 2007).
330
tation for each input sentence is known. This is
a strong assumption, as the segmentation stage
is ambiguous, and segmentation information pro-
vides very useful morphological hints that greatly
constrain the search space of the parser. However,
the setting is simpler to understand than the one
in which the parser performs both segmentation
and POS tagging, and the results show some in-
teresting trends. Moreover, some recent studies on
parsing Hebrew, as well as all studies on parsing
Arabic, make this oracle assumption. As such, the
results serve as an interesting comparison. Note
that in real-world parsing situations, the parser is
faced with a stream of ambiguous unsegmented to-
kens, making results in this setting not indicative
of real-world parsing performance.
4.1 The Models
The main question we address is the incorporation
of an external lexical resource into the parsing pro-
cess. This is challenging as different resources fol-
low different tagging schemes. One way around
it is re-tagging the treebank according to the new
tagging scheme. This will serve as a baseline
in our experiment. The alternative method uses
the Layered Representation described above (Sec.
2.1). We compare the performance of the two ap-
proaches, and also compare them against the per-
formance of the original treebank without external
information.
We follow the intuition that external lexical re-
sources are needed only when the information
contained in the treebank is too sparse. There-
fore, we use treebank-derived estimates for reli-
able events, and resort to the external resources
only in the cases of rare or OOV words, for which
the treebank distribution is not reliable.
Grammar and Notation For all our experi-
ments, we use the same grammar, and change
only the way lexical probabilities are imple-
mented. The grammar is an unlexicalized
treebank-estimated PCFG with linguistically mo-
tivated state-splits.9
In what follows, a lexical event is a word seg-
ment which is assigned a single POS thereby func-
tioning as a leaf in a syntactic parse tree. A rare
9Details of the grammar: all functional information is re-
moved from the non-terminals, finite and non-finite verbs, as
well as possessive and other PPs are distinguished, definite-
ness structure of constituents is marked, and parent annota-
tion is employed. It is the same grammar as described in
(Goldberg and Tsarfaty, 2008).
(lexical) event is an event occurring less than K
times in the training data, and a reliable (lexical)
event is one occurring at least K times in the train-
ing data. We use OOV to denote lexical events ap-
pearing 0 times in the training data. count(?) is
a counting function over the training data, rare
stands for any rare event, and wrare is a specific
rare event. KCA(?) is the KC Analyzer function,
mapping a lexical event to a set of possible tags
(analyses) according to the lexicon.
Lexical Models
All our models use relative frequency estimated
probabilities for reliable lexical events: p(t ?
w|t) = count(w,t)count(t) . They differ only in their treat-
ment of rare (including OOV) events.
In our Baseline, no external resource is used.
We smooth for rare and OOV events using a per-
tag probability distribution over rare segments,
which we estimate using relative frequency over
rare segments in the training data: p(wrare|t) =
count(rare,t)
count(t) . This is the way lexical probabilities
in treebank grammars are usually estimated.
We experiment with two flavours of lexical
models. In the first, LexFilter, the KC Analyzer is
consulted for rare events. We estimate rare events
using the same per-tag distribution as in the base-
line, but use the KC Analyzer to filter out any in-
compatible cases, that is, we force to 0 the proba-
bility of any analysis not supported by the lexicon:
p(wrare|t) =
{
count(rare,t)
count(t) t ? KCA(wrare)
0 t /? KCA(wrare)
Our second flavour of lexical models, Lex-
Probs, the KC Analyzer is consulted to propose
analyses for rare events, and the probability of an
analysis is estimated via the HMM emission func-
tion described in Section 3, which we denote B:
p(wrare|t) = B(wrare, t)
In both LexFilter and LexProbs, we resort to
the relative frequency estimation in case the event
is not covered in the KC Analyzer.
Tagset Representations
In this work, we are comparing 3 different rep-
resentations: TB, which is the original Treebank,
KC which is the Treebank converted to use the KC
Analyzer tagset, and Layered, which is the layered
representation described above.
The details of the lexical models vary according
to the representation we choose to work with.
For the TB setting, our lexical rules are of the form
331
ttb ? w. Only the Baseline models are relevant
here, as the tagset is not compatible with that of
the external lexicon.
For the KC setting, our lexical rules are of the form
tkc ? w, and their probabilities are estimated as
described above. Note that this setting requires our
trees to be tagged with the new (KC) tagset, and
parsed sentences are also tagged with this tagset.
For the Layered setting, we use lexical rules of
the form ttb ? w. Reliable events are esti-
mated as usual, via relative frequency over the
original treebank. For rare events, we estimate
p(ttb ? w|ttb) = p(ttb ? tkc|ttb)p(tkc ? w|tkc),
where the transfer probabilities p(ttb ? tkc) are
estimated via relative frequencies over the layered
trees, and the emission probabilities are estimated
either based on other rare events (LexFilter) or
based on the semi-supervised method described in
Section 3 (LexProbs).
The layered setting has several advantages:
First, the resulting trees are all tagged with the
original TB tagset. Second, the training proce-
dure does not require a treebank tagged with the
KC tagset: Instead of learning the transfer layer
from the treebank we could alternatively base our
counts on a different parallel resource, estimate it
from unannotated data using EM, define it heuris-
tically, or use any other estimation procedure.
4.2 Experiments
We perform all our experiments on Version 2 of
the Hebrew Treebank, and follow the train/test/dev
split introduced in (Tsarfaty and Sima?an, 2007):
section 1 is used for development, sections 2-12
for training, and section 13 is the test set, which
we do not use in this work. All the reported re-
sults are on the development set.10 After removal
of empty sentences, we have 5241 sentences for
training, and 483 for testing. Due to some changes
in the Treebank11, our results are not directly com-
parable to earlier works. However, our baseline
models are very similar to the models presented
in, e.g. (Goldberg and Tsarfaty, 2008).
In order to compare the performance of the
model on the various tagset representations (TB
tags, KC tags, Layered), we remove from the test
set 51 sentences in which at least one token is
marked as not having any correct segmentation in
the KC Analyzer. This introduces a slight bias in
10This work is part of an ongoing work on a parser, and the
test set is reserved for final evaluation of the entire system.
11Normalization of numbers and percents, correcting of
some incorrect trees, etc.
favor of the KC-tags setting, and makes the test
somewhat easier for all the models. However, it
allows for a relatively fair comparison between the
various models.12
Results and Discussion
Results are presented in Table 2.13
Baseline
rare: < 2 rare: < 10
Prec Rec Prec Rec
TB 72.80 71.70 67.66 64.92
KC 72.23 70.30 67.22 64.31
LexFilter
rare: < 2 rare: < 10
Prec Rec Prec Rec
KC 77.18 76.31 77.34 76.20
Layered 76.69 76.40 76.66 75.74
LexProbs
rare: < 2 rare: < 10
Prec Rec Prec Rec
KC 77.29 76.65 77.22 76.36
Layered 76.81 76.49 76.85 76.08
Table 2: evalb results for parsing with a
segmentation Oracle.
As expected, all the results are much lower than
those with gold fine-grained POS (Table 1).
When not using any external knowledge (Base-
line), the TB tagset performs slightly better than
the converted treebank (KC). Note, however, that
the difference is less pronounced than in the gold
morphology case. When varying the rare words
threshold from 2 to 10, performance drops consid-
erably. Without external knowledge, the parser is
facing difficulties coping with unseen events.
The incorporation of an external lexical knowl-
edge in the form of pruning illegal tag assignments
for unseen words based on the KC lexicon (Lex-
Filter) substantially improves the results (? 72 to
? 77). The additional lexical knowledge clearly
improves the parser. Moreover, varying the rare
words threshold in this setting hardly affects the
parser performance: the external lexicon suffices
to guide the parser in the right direction. Keep-
ing the rare words threshold high is desirable, as it
reduces overfitting to the treebank vocabulary.
We expected the addition of the semi-
supervised p(t ? w) distribution (LexProbs) to
improve the parser, but found it to have an in-
significant effect. The correct segmentation seems
12We are forced to remove these sentences because of the
artificial setting in which the correct segmentation is given. In
the no-oracle setting (Sec. 5), we do include these sentences.
13The layered trees have an extra layer of bracketing
(tTB ? tKC ). We remove this layer prior to evaluation.
332
to remove enough ambiguity as to let the parser
base its decisions on the generic tag distribution
for rare events.
In all the settings with a Segmentation Oracle,
there is no significant difference between the KC
and the Layered representation. We prefer the lay-
ered representation as it provides more flexibility,
does not require trees tagged with the KC tagset,
and produces parse trees with the original TB POS
tags at the leaves.
5 Parsing without a Segmentation Oracle
When parsing real world data, correct token seg-
mentation is not known in advance. For method-
ological reasons, this issue has either been set-
aside (Tsarfaty and Sima?an, 2007), or dealt with
in a pipeline model in which a morphological dis-
ambiguator is run prior to parsing to determine the
correct segmentation. However, Tsarfaty (2006)
argues that there is a strong interaction between
syntax and morphological segmentation, and that
the two tasks should be modeled jointly, and not
in a pipeline model. Several studies followed this
line, (Cohen and Smith, 2007) the most recent of
which is Goldberg and Tsarfaty (2008), who pre-
sented a model based on unweighted lattice pars-
ing for performing the joint task.
This model uses a morphological analyzer to
construct a lattice over all possible morphologi-
cal analyses of an input sentence. The arcs of
the lattice are ?w, t? pairs, and a lattice parser
is used to build a parse over the lattice. The
Viterbi parse over the lattice chooses a lattice path,
which induces a segmentation over the input sen-
tence. Thus, parsing and segmentation are per-
formed jointly.
Lexical rules in the model are defined over the
lattice arcs (t? ?w, t?|t), and smoothed probabil-
ities for them are estimated from the treebank via
relative frequency over terminal/preterminal pairs.
The lattice paths themselves are unweighted, re-
flecting the intuition that all morphological anal-
yses are a-priori equally likely, and that their per-
spective strengths should come from the segments
they contain and their interaction with the syntax.
Goldberg and Tsarfaty (2008) use a data-driven
morphological analyzer derived from the treebank.
Their better models incorporated some external
lexical knowledge by use of an Hebrew spell
checker to prune some illegal segmentations.
In what follows, we use the layered represen-
tation to adapt this joint model to use as its mor-
phological analyzer the wide coverage KC Ana-
lyzer in enhancement of a data-driven one. Then,
we further enhance the model with the semi-
supervised lexical probabilities described in Sec 3.
5.1 Model
The model of Goldberg and Tsarfaty (2008) uses a
morphological analyzer to constructs a lattice for
each input token. Then, the sentence lattice is built
by concatenating the individual token lattices. The
morphological analyzer used in that work is data
driven based on treebank observations, and em-
ploys some well crafted heuristics for OOV tokens
(for details, see the original paper). Here, we use
instead a morphological analyzer which uses the
KC Lexicon for rare and OOV tokens.
We begin by adapting the rare vs. reliable events
distinction from Section 4 to cover unsegmented
tokens. We define a reliable token to be a token
from the training corpus, which each of its possi-
ble segments according to the training corpus was
seen in the training corpus at least K times.14 All
other tokens are considered to be rare.
Our morphological analyzer works as follows:
For reliable tokens, it returns the set of analyses
seen for this token in the treebank (each analysis
is a sequence of pairs of the form ?w, tTB?).
For rare tokens, it returns the set of analyses re-
turned by the KC analyzer (here, analyses are se-
quences of pairs of the form ?w, tKC?).
The lattice arcs, then, can take two possible
forms, either ?w, tTB? or ?w, tKC?.
Lexical rules of the form tTB ? ?w, tTB? are reli-
able, and their probabilities estimated via relative
frequency over events seen in training.
Lexical rules of the form tTB ? ?w, tKC?
are estimated in accordance with the transfer
layer introduced above: p(tTB ? ?w, tKC?) =
p(tKC |tTB)p(?w, tKC?|tKC).
The remaining question is how to estimate
p(?w, tKC?|tKC). Here, we use either the LexFil-
ter (estimated over all rare events) or LexProbs
(estimated via the semisupervised emission prob-
abilities)models, as defined in Section 4.1 above.
5.2 Experiments
As our Baseline, we take the best model of (Gold-
berg and Tsarfaty, 2008), run against the current
14Note that this is more inclusive than requiring that the
token itself is seen in the training corpus at least K times, as
some segments may be shared by several tokens.
333
version of the Treebank.15 This model uses the
same grammar as described in Section 4.1 above,
and use some external information in the form of a
spell-checker wordlist. We compare this Baseline
with the LexFilter and LexProbs models over the
Layered representation.
We use the same test/train splits as described in
Section 4. Contrary to the Oracle segmentation
setting, here we evaluate against all sentences, in-
cluding those containing tokens for which the KC
Analyzer does not contain any correct analyses.
Due to token segmentation ambiguity, the re-
sulting parse yields may be different than the gold
ones, and evalb can not be used. Instead, we use
the evaluation measure of (Tsarfaty, 2006), also
used in (Goldberg and Tsarfaty, 2008), which is
an adaptation of parseval to use characters instead
of space-delimited tokens as its basic units.
Results and Discussion
Results are presented in Table 3.
rare: < 2 rare: < 10
Prec Rec Prec Rec
Baseline 67.71 66.35 ? ?
LexFilter 68.25 69.45 57.72 59.17
LexProbs 73.40 73.99 70.09 73.01
Table 3: Parsing results for the joint parsing+seg
task, with varying external knowledge
The results are expectedly lower than with the
segmentation Oracle, as the joint task is much
harder, but the external lexical information greatly
benefits the parser also in the joint setting. While
significant, the improvement from the Baseline to
LexFilter is quite small, which is due to the Base-
line?s own rather strong illegal analyses filtering
heuristic. However, unlike the oracle segmenta-
tion case, here the semisupervised lexical prob-
abilities (LexProbs) have a major effect on the
parser performance (? 69 to ? 73.5 F-score), an
overall improvement of ? 6.6 F-points over the
Baseline, which is the previous state-of-the art for
this joint task. This supports our intuition that rare
lexical events are better estimated using a large
unannotated corpus, and not using a generic tree-
bank distribution, or sparse treebank based counts,
and that lexical probabilities have a crucial role in
resolving segmentation ambiguities.
15While we use the same software as (Goldberg and Tsar-
faty, 2008), the results reported here are significantly lower.
This is due to differences in annotation scheme between V1
and V2 of the Hebrew TB
The parsers with the extended lexicon were un-
able to assign a parse to about 10 of the 483 test
sentences. We count them as having 0-Fscore
in the table results.16 The Baseline parser could
not assign a parse to more than twice that many
sentences, suggesting its lexical pruning heuris-
tic is quite harsh. In fact, the unparsed sen-
tences amount to most of the difference between
the Baseline and LexFilter parsers.
Here, changing the rare tokens threshold has
a significant effect on parsing accuracy, which
suggests that the segmentation for rare tokens is
highly consistent within the corpus. When an un-
known token is encountered, a clear bias should
be taken toward segmentations that were previ-
ously seen in the same corpus. Given that that ef-
fect is remedied to some extent by introducing the
semi-supervised lexical probabilities, we believe
that segmentation accuracy for unseen tokens can
be further improved, perhaps using resources such
as (Gabay et al, 2008), and techniques for incor-
porating some document, as opposed to sentence
level information, into the parsing process.
6 Conclusions
We present a framework for interfacing a parser
with an external lexicon following a differ-
ent annotation scheme. Unlike other studies
(Yang Huang et al, 2005; Szolovits, 2003) in
which such interfacing is achieved by a restricted
heuristic mapping, we propose a novel, stochastic
approach, based on a layered representation. We
show that using an external lexicon for dealing
with rare lexical events greatly benefits a PCFG
parser for Hebrew, and that results can be further
improved by the incorporation of lexical probabil-
ities estimated in a semi-supervised manner using
a wide-coverage lexicon and a large unannotated
corpus. In the future, we plan to integrate this
framework with a parsing model that is specifi-
cally crafted to cope with morphologically rich,
free-word order languages, as proposed in (Tsar-
faty and Sima?an, 2008).
Apart from Hebrew, our method is applicable
in any setting in which there exist a small tree-
bank and a wide-coverage lexical resource. For
example parsing Arabic using the Arabic Tree-
bank and the Buckwalter analyzer, or parsing En-
glish biomedical text using a biomedical treebank
and the UMLS Specialist Lexicon.
16When discarding these sentences from the test set, result
on the better LexProbs model leap to 74.95P/75.56R.
334
References
M. Adler and M. Elhadad. 2006. An unsupervised
morpheme-based hmm for hebrew morphological
disambiguation. In Proc. of COLING/ACL2006.
Meni Adler, Yoav Goldberg, David Gabay, and
Michael Elhadad. 2008a. Unsupervised lexicon-
based resolution of unknown words for full morpho-
logical analysis. In Proc. of ACL 2008.
Meni Adler, Yael Netzer, David Gabay, Yoav Goldberg,
and Michael Elhadad. 2008b. Tagging a hebrew
corpus: The case of participles. In Proc. of LREC
2008.
Meni Adler. 2007. Hebrew Morphological Disam-
biguation: An Unsupervised Stochastic Word-based
Approach. Ph.D. thesis, Ben-Gurion University of
the Negev, Beer-Sheva, Israel.
Eugene Charniak, Glenn Carroll, John Adcock, An-
thony Cassandra, Yoshihiko Gotoh, Jeremy Katz,
Michael Littman, and John McCann. 1996. Taggers
for parsers. Artif. Intell., 85(1-2):45?57.
Shay B. Cohen and Noah A. Smith. 2007. Joint mor-
phological and syntactic disambiguation. In Pro-
ceedings of EMNLP-CoNLL-07, pages 208?217.
David Gabay, Ziv Ben Eliahu, and Michael Elhadad.
2008. Using wikipedia links to construct word seg-
mentation corpora. In Proc. of the WIKIAI-08 Work-
shop, AAAI-2008 Conference.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gen-
erative model for joint morphological segmentation
and syntactic parsing. In Proc. of ACL 2008.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. Em can find pretty good hmm pos-taggers
(when given a good start). In Proc. of ACL 2008.
Noemie Guthmann, Yuval Krymolowski, Adi Milea,
and Yoad Winter. 2009. Automatic annotation of
morpho-syntactic dependencies in a modern hebrew
treebank. In Proc. of TLT.
Mohamed Maamouri, Ann Bies, and Seth Kulick.
2008. Enhanced annotation and parsing of the ara-
bic treebank. In INFOS 2008, Cairo, Egypt, March
27-29, 2008.
Yael Netzer, Meni Adler, David Gabay, and Michael
Elhadad. 2007. Can you tag the modal? you should!
In ACL07 Workshop on Computational Approaches
to Semitic Languages, Prague, Czech.
K. Sima?an, A. Itai, Y. Winter, A. Altman, and N. Nativ.
2001. Building a tree-bank of modern hebrew text.
Traitement Automatique des Langues, 42(2).
P. Szolovits. 2003. Adding a medical lexicon to an
english parser. In Proc. AMIA 2003 Annual Sympo-
sium.
Reut Tsarfaty and Khalil Sima?an. 2007. Three-
dimensional parametrization for parsing morpholog-
ically rich languages. In Proc. of IWPT 2007.
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
realizational parsing. In Proc. of CoLING, pages
889?896, Manchester, UK, August. Coling 2008.
Reut Tsarfaty. 2006. Integrated Morphological and
Syntactic Disambiguation for Modern Hebrew. In
Proceedings of ACL-SRW-06.
MS Yang Huang, MD Henry J. Lowe, PhD Dan Klein,
and MS Russell J. Cucina, MD. 2005. Improved
identification of noun phrases in clinical radiology
reports using a high-performance statistical natural
language parser augmented with the umls specialist
lexicon. J Am Med Inform Assoc, 12(3), May.
335
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 689?696,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Noun Phrase Chunking in Hebrew 
Influence of Lexical and Morphological Features 
 
 
Yoav Goldberg  and  Meni Adler  and  Michael Elhadad 
Computer Science Department 
Ben Gurion University of the Negev 
P.O.B 653 Be'er Sheva 84105, Israel  
{yoavg,adlerm,elhadad}@cs.bgu.ac.il 
 
  
 
Abstract 
We present a method for Noun Phrase 
chunking in Hebrew. We show that the 
traditional definition of base-NPs as non-
recursive noun phrases does not apply in 
Hebrew, and propose an alternative defi-
nition of Simple NPs.  We review syntac-
tic properties of Hebrew related to noun 
phrases, which indicate that the task of 
Hebrew SimpleNP chunking is harder 
than base-NP chunking in English. As a 
confirmation, we apply methods known 
to work well for English to Hebrew data. 
These methods give low results (F from 
76 to 86) in Hebrew. We then discuss our 
method, which applies SVM induction 
over lexical and morphological features. 
Morphological features improve the av-
erage precision by ~0.5%, recall by ~1%, 
and F-measure by ~0.75, resulting in a 
system with average performance of 93% 
precision, 93.4% recall and 93.2 F-
measure.* 
1 Introduction 
Modern Hebrew is an agglutinative Semitic lan-
guage, with rich morphology.  Like most other 
non-European languages, it lacks NLP resources 
and tools, and specifically there are currently no 
available syntactic parsers for Hebrew.  We ad-
dress the task of NP chunking in Hebrew as a 
                                                 
*
 This work was funded by the Israel Ministry of Sci-
ence and Technology under the auspices of the 
Knowledge Center for Processing Hebrew.  Addi-
tional funding was provided by the Lynn and William 
Frankel Center for Computer Sciences.  
first step to fulfill the need for such tools.  We 
also illustrate how this task can successfully be 
approached with little resource requirements, and 
indicate how the method is applicable to other 
resource-scarce languages. 
NP chunking is the task of labelling noun 
phrases in natural language text. The input to this 
task is free text with part-of-speech tags.  The 
output is the same text with brackets around base 
noun phrases.  A base noun phrase is an NP 
which does not contain another NP (it is not re-
cursive).  NP chunking is the basis for many 
other NLP tasks such as shallow parsing, argu-
ment structure identification, and information 
extraction 
We first realize that the definition of base-NPs 
must be adapted to the case of Hebrew (and 
probably other Semitic languages as well) to cor-
rectly handle its syntactic nature.  We propose 
such a definition, which we call simple NPs and 
assess the difficulty of chunking such NPs by 
applying methods that perform well in English to 
Hebrew data.  While the syntactic problem in 
Hebrew is indeed more difficult than in English, 
morphological clues do provide additional hints, 
which we exploit using an SVM learning 
method.  The resulting method reaches perform-
ance in Hebrew comparable to the best results 
published in English. 
2 Previous Work 
Text chunking (and NP chunking in particular), 
first proposed by Abney (1991), is a well studied 
problem for English. The CoNLL2000 shared 
task (Tjong Kim Sang et al, 2000) was general 
chunking. The best result achieved for the shared 
task data was by Zhang et al(2002), who 
achieved NP chunking results of 94.39% preci-
sion, 94.37% recall and 94.38 F-measure using a 
689
generalized Winnow algorithm, and enhancing 
the feature set with the output of a dependency 
parser. Kudo and Matsumoto (2000) used an 
SVM based algorithm, and achieved NP chunk-
ing results of 93.72% precision, 94.02% recall 
and 93.87 F-measure for the same shared task 
data, using only the words and their PoS tags. 
Similar results were obtained using Conditional 
Random Fields on similar features (Sha and 
Pereira, 2003). 
The NP chunks in the shared task data are 
base-NP chunks ? which are non-recursive NPs, 
a definition first proposed by Ramshaw and 
Marcus (1995). This definition yields good NP 
chunks for English, but results in very short and 
uninformative chunks for Hebrew (and probably 
other Semitic languages). 
Recently, Diab et al(2004) used SVM based 
approach for Arabic text chunking.  Their chunks 
data was derived from the LDC Arabic TreeBank 
using the same program that extracted the chunks 
for the shared task.  They used the same features 
as Kudo and Matsumoto (2000), and achieved 
over-all chunking performance of 92.06% preci-
sion, 92.09% recall and 92.08 F-measure (The 
results for NP chunks alone were not reported).  
Since Arabic syntax is quite similar to Hebrew, 
we expect that the issues reported below apply to 
Arabic results as well. 
3 Hebrew Simple NP Chunks 
The standard definition of English base-NPs is 
any noun phrase that does not contain another 
noun phrase, with possessives treated as a special 
case, viewing the possessive marker as the first 
word of a new base-NP (Ramshaw and Marcus, 
1995).  To evaluate the applicability of this defi-
nition to Hebrew, we tested this definition on the 
Hebrew TreeBank (Sima?an et al 2001) pub-
lished by the Hebrew Knowledge Center. We 
extracted all base-NPs from this TreeBank, 
which is similar in genre and contents to the 
English one.  This results in extremely simple 
chunks.  
 
English 
BaseNPs 
Hebrew 
BaseNPs 
Hebrew 
SimpleNPs 
Avg # of words 2.17 1.39 2.49 
% length 1 30.95 63.32 32.83 
% length 2 39.35 35.48 32.12 
% length 3 18.68 0.83 14.78 
% length 4 6.65 0.16 9.47 
% length 5 2.70 0.16 4.56 
% length > 5 1.67 0.05 6.22 
Table 1.  Size of Hebrew and English NPs 
Table 1 shows the average number of words in a 
base-NP for English and Hebrew.  The Hebrew 
chunks are basically one-word groups around 
Nouns, which is not useful for any practical pur-
pose, and so we propose a new definition for He-
brew NP chunks, which allows for some nested-
ness. We call our chunks Simple NP chunks.  
3.1 Syntax of NPs in Hebrew 
One of the reasons the traditional base-NP defi-
nition fails for the Hebrew TreeBank is related to 
syntactic features of Hebrew ? specifically, 
smixut (construct state ? used to express noun 
compounds), definite marker and the expression 
of possessives. These differences are reflected to 
some extent by the tagging guidelines used to 
annotate the Hebrew Treebank and they result in 
trees which are in general less flat than the Penn 
TreeBank ones.  
Consider the example base noun phrase [The 
homeless people]. The Hebrew equivalent is 
(1)  	
  
 which by the non-recursive NP definition will be 
bracketed as: 
   	
  , or, loosely translating 
back to English: [the home]less [people].  
In this case, the fact that the bound-morpheme 
less appears as a separate construct state word 
with its own definite marker (ha-) in Hebrew 
would lead the chunker to create two separate 
NPs for a simple expression.  We present below 
syntactic properties of Hebrew which are rele-
vant to NP chunking. We then present our defini-
tion of Simple NP Chunks.  
 
Construct State: The Hebrew genitive case is 
achieved by placing two nouns next to each other. 
This is called ?noun construct?, or smixut. The 
semantic interpretation of this construct is varied 
(Netzer and Elhadad, 1998), but it specifically 
covers possession. The second noun can be 
treated as an adjective modifying the next noun. 
The first noun is morphologically marked in a 
form known as the construct form (denoted by 
const). The definite article marker is placed on 
the second word of the construction: 
(2)  
 beit sefer / house-[const] book 
 School 
(3)  
 beit ha-sefer / house-[const] the-book 
 The school 
 
The construct form can also be embedded: 
(4) 	


 
690
misrad ro$ ha-mem$ala  
Office-[const poss] head-[const] the-government 
The prime-minister?s office 
 
Possessive: the smixut form can be used to indi-
cate possession. Other ways to express posses-
sion include the possessive marker  - ?$el? / 
?of? - (5), or adding a possessive suffix on the 
noun (6). The various forms can be mixed to-
gether, as in (7): 
(5) 	
 
ha-bait $el-i / the-house of-[poss 1st person] 
My house 
(6)  
beit-i / house-[poss 1st person] 
My house 
(7) 	

	

  
misrad-o $el ro$ ha-mem$ala 
Office-[poss 3rd] of head-[const] the-government 
The prime minister office 
 
Adjective: Hebrew adjectives come after the 
noun, and agree with it in number, gender and 
definite marker: 
(8)  
ha-tapu?ah ha-yarok / the-Apple the-green 
The green apple 
 
Some aspects of the predicate structure in He-
brew directly affect the task of NP chunking, as 
they make the decision to ?split? NPs more or 
less difficult than in English. 
 
Word order and the preposition 'et': Hebrew 
sentences can be either in SVO or VSO form. In 
order to keep the object separate from the sub-
ject, definite direct objects are marked with the 
special preposition 'et', which has no analog in 
English.  
 
Possible null equative: The equative form in 
Hebrew can be null. Sentence (9) is a non-null 
equative, (10) a null equative, while (11) and 
(12) are predicative NPs, which look very similar 
to the null-equative form: 
 
(9) 	 
ha-bait hu gadol 
The-house is big 
The house is big 
 
(10) 	 
ha-bait gadol 
The-house big 
The house is big 
 
(11) 	 
bait gadol 
House big 
A big house 
(12) 	 
ha-bait ha-gadol 
The-house the-big 
The big house 
 
Morphological Issues: In Hebrew morphology, 
several lexical units can be concatenated into a 
single textual unit.  Most prepositions, the defi-
nite article marker and some conjunctions are 
concatenated as prefixes, and possessive pro-
nouns and some adverbs are concatenated as suf-
fixes.  The Hebrew Treebank is annotated over a 
segmented version of the text, in which prefixes 
and suffixes appear as separate lexical units.  On 
the other hand, many bound morphemes in Eng-
lish appear as separate lexical units in Hebrew.  
For example, the English morphemes re-, ex-, 
un-, -less, -like, -able, appear in Hebrew as sepa-
rate lexical units ? , 	, 

 , , , 
, . 
  
In our experiment, we use as input to the 
chunker the text after it has been morphologi-
cally disambiguated and segmented. Our 
analyzer provides segmentation and PoS tags 
with 92.5% accuracy and full morphology with 
88.5% accuracy (Adler and Elhadad, 2006). 
3.2 Defining Simple NPs 
Our definition of Simple NPs is pragmatic. We 
want to tag phrases that are complete in their 
syntactic structure, avoid the requirement of tag-
ging recursive structures that include full clauses 
(relative clauses for example) and in general, tag 
phrases that have a simple denotation. To estab-
lish our definition, we start with the most com-
plex NPs, and break them into smaller parts by 
stating what should not appear inside a Simple 
NP. This can be summarized by the following 
table: 
 
Outside SimpleNP Exceptions 
Prepositional Phrases 
Relative Clauses 
Verb Phrases 
Apposition1 
Some conjunctions 
(Conjunctions are 
marked according to the 
TreeBank guidelines)2. 
% related PPs are 
allowed:  

5% of the sales 
 
Possessive  - '$el' / 
'of' - is not consid-
ered a PP 
Table 2.   Definition of Simple NP chunks 
Examples for some Simple NP chunks resulting 
from that definition: 
 
                                                 
1
 Apposition structure is not annotated in the TreeBank. As 
a heuristic, we consider every comma inside a non conjunct-
ive NP which is not followed by an adjective or an adjective 
phrase to be marking the beginning of an apposition. 
2
 As a special case, Adjectival Phrases and possessive con-
junctions are considered to be inside the Simple NP.  
691
   	   
	

Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 224?231,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
SVM Model Tampering and Anchored Learning: A Case Study in Hebrew
NP Chunking
Yoav Goldberg and Michael Elhadad
Computer Science Department
Ben Gurion University of the Negev
P.O.B 653 Be?er Sheva 84105, Israel
yoavg,elhadad@cs.bgu.ac.il
Abstract
We study the issue of porting a known NLP
method to a language with little existing NLP
resources, specifically Hebrew SVM-based
chunking. We introduce two SVM-based
methods ? Model Tampering and Anchored
Learning. These allow fine grained analysis
of the learned SVM models, which provides
guidance to identify errors in the training cor-
pus, distinguish the role and interaction of
lexical features and eventually construct a
model with ?10% error reduction. The re-
sulting chunker is shown to be robust in the
presence of noise in the training corpus, relies
on less lexical features than was previously
understood and achieves an F-measure perfor-
mance of 92.2 on automatically PoS-tagged
text. The SVM analysis methods also provide
general insight on SVM-based chunking.
1 Introduction
While high-quality NLP corpora and tools are avail-
able in English, such resources are difficult to obtain
in most other languages. Three challenges must be
met when adapting results established in English to
another language: (1) acquiring high quality anno-
tated data; (2) adapting the English task definition
to the nature of a different language, and (3) adapt-
ing the algorithm to the new language. This paper
presents a case study in the adaptation of a well
known task to a language with few NLP resources
available. Specifically, we deal with SVM based He-
brew NP chunking. In (Goldberg et al, 2006), we
established that the task is not trivially transferable
to Hebrew, but reported that SVM based chunking
(Kudo and Matsumoto, 2000) performs well. We
extend that work and study the problem from 3 an-
gles: (1) how to deal with a corpus that is smaller
and with a higher level of noise than is available in
English; we propose techniques that help identify
?suspicious? data points in the corpus, and identify
how robust the model is in the presence of noise;
(2) we compare the task definition in English and in
Hebrew through quantitative evaluation of the differ-
ences between the two languages by analyzing the
relative importance of features in the learned SVM
models; and (3) we analyze the structure of learned
SVM models to better understand the characteristics
of the chunking problem in Hebrew.
While most work on chunking with machine
learning techniques tend to treat the classification
engine as a black-box, we try to investigate the re-
sulting classification model in order to understand
its inner working, strengths and weaknesses. We in-
troduce two SVM-based methods ? Model Tamper-
ing and Anchored Learning ? and demonstrate how
a fine-grained analysis of SVM models provides in-
sights on all three accounts. The understanding of
the relative contribution of each feature in the model
helps us construct a better model, which achieves
?10% error reduction in Hebrew chunking, as well
as identify corpus errors. The methods also provide
general insight on SVM-based chunking.
2 Previous Work
NP chunking is the task of marking the bound-
aries of simple noun-phrases in text. It is a well
studied problem in English, and was the focus of
CoNLL2000?s Shared Task (Sang and Buchholz,
224
2000). Early attempts at NP Chunking were rule
learning systems, such as the Error Driven Prun-
ing method of Pierce and Cardie (1998). Follow-
ing Ramshaw and Marcus (1995), the current dom-
inant approach is formulating chunking as a clas-
sification task, in which each word is classified as
the (B)eginning, (I)nside or (O)outside of a chunk.
Features for this classification usually involve local
context features. Kudo and Matsumoto (2000) used
SVM as a classification engine and achieved an F-
Score of 93.79 on the shared task NPs. Since SVM
is a binary classifier, to use it for the 3-class classi-
fication of the chunking task, 3 different classifiers
{B/I, B/O, I/O} were trained and their majority vote
was taken.
NP chunks in the shared task data are BaseNPs,
which are non-recursive NPs, a definition first pro-
posed by Ramshaw and Marcus (1995). This defini-
tion yields good NP chunks for English. In (Gold-
berg et al, 2006) we argued that it is not applica-
ble to Hebrew, mainly because of the prevalence
of the Hebrew?s construct state (smixut). Smixut
is similar to a noun-compound construct, but one
that can join a noun (with a special morphologi-
cal marking) with a full NP. It appears in about
40% of Hebrew NPs. We proposed an alterna-
tive definition (termed SimpleNP) for Hebrew NP
chunks. A SimpleNP cannot contain embedded rel-
atives, prepositions, VPs and NP-conjunctions (ex-
cept when they are licensed by smixut). It can
contain smixut, possessives (even when they are
attached by the ???/of? preposition) and partitives
(and, therefore, allows for a limited amount of re-
cursion). We applied this definition to the Hebrew
Tree Bank (Sima?an et al, 2001), and constructed
a moderate size corpus (about 5,000 sentences) for
Hebrew SimpleNP chunking. SimpleNPs are differ-
ent than English BaseNPs, and indeed some meth-
ods that work well for English performed poorly
on Hebrew data. However, we found that chunk-
ing with SVM provides good result for Hebrew Sim-
pleNPs. We analyzed that this success comes from
SVM?s ability to use lexical features, as well as two
Hebrew morphological features, namely ?number?
and ?construct-state?.
One of the main issues when dealing with Hebrew
chunking is that the available tree bank is rather
small, and since it is quite new, and has not been
used intensively, it contains a certain amount of in-
consistencies and tagging errors. In addition, the
identification of SimpleNPs from the tree bank also
introduces some errors. Finally, we want to investi-
gate chunking in a scenario where PoS tags are as-
signed automatically and chunks are then computed.
The Hebrew PoS tagger we use introduces about 8%
errors (compared with about 4% in English). We
are, therefore, interested in identifying errors in the
chunking corpus, and investigating how the chunker
operates in the presence of noise in the PoS tag se-
quence.
3 Model Tampering
3.1 Notation and Technical Review
This section presents notation as well as a technical
review of SVM chunking details relevant to the cur-
rent study. Further details can be found in Kudo and
Matsumoto (2000; 2003).
SVM (Vapnik, 1995) is a supervised binary clas-
sifier. The input to the learner is a set of l train-
ing samples (x1, y1), . . . , (xl, yl), x ? Rn, y ?
{+1,?1}. xi is an n dimensional feature vec-
tor representing the ith sample, and yi is the la-
bel for that sample. The result of the learning pro-
cess is the set SV of Support Vectors, the asso-
ciated weights ?i, and a constant b. The Support
Vectors are a subset of the training vectors, and to-
gether with the weights and b they define a hyper-
plane that optimally separates the training samples.
The basic SVM formulation is of a linear classifier,
but by introducing a kernel function K that non-
linearly transforms the data from Rn into a higher
dimensional space, SVM can be used to perform
non-linear classification. SVM?s decision function
is: y(x) = sgn
(
?
j?SV yj?jK(xj , x) + b
)
where
x is an n dimensional feature vector to be classi-
fied. In the linear case, K is a dot product oper-
ation and the sum w = ? yj?jxj is an n dimen-
sional weight vector assigning weight for each of
the n features. The other kernel function we con-
sider in this paper is a polynomial kernel of degree
2: K(xi, xj) = (xi ? xj + 1)2. When using binary
valued features, this kernel function essentially im-
plies that the classifier considers not only the explic-
itly specified features, but also all available pairs of
features. In order to cope with inseparable data, the
learning process of SVM allows for some misclas-
sification, the amount of which is determined by a
225
parameter C, which can be thought of as a penalty
for each misclassified training sample.
In SVM based chunking, each word and its con-
text is considered a learning sample. We refer to
the word being classified as w0, and to its part-of-
speech (PoS) tag, morphology, and B/I/O tag as p0,
m0 and t0 respectively. The information consid-
ered for classification is w?cw . . . wcw, p?cp . . . pcp,
m?cm . . .mcm and t?ct . . . t?1. The feature vector
F is an indexed list of all the features present in
the corpus. A feature fi of the form w+1 = dog
means that the word following the one being clas-
sified is ?dog?. Every learning sample is repre-
sented by an n = |F | dimensional binary vector x.
xi = 1 iff the feature fi is active in the given sample,
and 0 otherwise. This encoding leads to extremely
high dimensional vectors, due to the lexical features
w?cw . . . wcw.
3.2 Introducing Model Tampering
An important observation about SVM classifiers is
that features which are not active in any of the Sup-
port Vectors have no effect on the classifier deci-
sion. We introduce Model Tampering, a procedure
in which we change the Support Vectors in a model
by forcing some values in the vectors to 0.
The result of this procedure is a new Model in
which the deleted features never take part in the clas-
sification.
Model tampering is different than feature selec-
tion: on the one hand, it is a method that helps us
identify irrelevant features in a model after training;
on the other hand, and this is the key insight, re-
moving features after training is not the same as re-
moving them before training. The presence of the
low-relevance features during training has an impact
on the generalization performed by the learner as
shown below.
3.3 The Role of Lexical Features
In Goldberg et al (2006), we have established that
using lexical features increases the chunking F-
measure from 78 to over 92 on the Hebrew Tree-
bank. We refine this observation by using Model
Tampering, in order to assess the importance of lex-
ical features in NP Chunking. We are interested in
identifying which specific lexical items and contexts
impact the chunking decision, and quantifying their
effect. Our method is to train a chunking model
on a given training corpus, tamper with the result-
ing model in various ways and measure the perfor-
mance1 of the tampered models on a test corpus.
3.4 Experimental Setting
We conducted experiments both for English and He-
brew chunking. For the Hebrew experiments, we use
the corpora of (Goldberg et al, 2006). The first one
is derived from the original Treebank by projecting
the full syntactic tree, constructed manually, onto a
set of NP chunks according to the SimpleNP rules.
We refer to the resulting corpus as HEBGold since
PoS tags are fully reliable. The HEBErr version
of the corpus is obtained by projecting the chunk
boundaries on the sequence of PoS and morphology
tags obtained by the automatic PoS tagger of Adler
& Elhadad (2006). This corpus includes an error
rate of about 8% on PoS tags. The first 500 sen-
tences are used for testing, and the rest for training.
The corpus contains 27K NP chunks. For the En-
glish experiments, we use the now-standard training
and test sets that were introduced in (Marcus and
Ramshaw, 1995)2. Training was done using Kudo?s
YAMCHA toolkit3. Both Hebrew and English mod-
els were trained using a polynomial kernel of de-
gree 2, with C = 1. For English, the features used
were: w?2 . . . w2, p?2 . . . p2, t?2 . . . t?1. The same
features were used for Hebrew, with the addition of
m?2 . . .m2. These are the same settings as in (Kudo
and Matsumoto, 2000; Goldberg et al, 2006).
3.5 Tamperings
We experimented with the following tamperings:
TopN ? We define model feature count to be the
number of Support Vectors in which a feature is ac-
tive in a given classifier. This tampering leaves in the
model only the top N lexical features in each classi-
fier, according to their count.
NoPOS ? all the lexical features corresponding to
a given part-of-speech are removed from the model.
For example, in a NoJJ tampering, all the features of
the form wi = X are removed from all the support
vectors in which pi = JJ is active.
Loc6=i ? all the lexical features with index i are
removed from the model e.g., in a Loc6=+2 tamper-
1The performance metric we use is the standard Preci-
sion/Recall/F measures, as computed by the conlleval program:
http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt
2ftp://ftp.cis.upenn.edu/pub/chunker
3http://chasen.org/?taku/software/yamcha/
226
ing, features of the form w+2 = X are removed).
Loc=i ? all the lexical features with an index other
than i are removed from the model.
3.6 Results and Discussion
Highlights of the results are presented in Tables (1-
3). The numbers reported are F measures.
TopN HEBGold HEBErr ENG
ALL 93.58 92.48 93.79
N=0 78.32 76.27 90.10
N=10 90.21 88.68 90.24
N=50 91.78 90.85 91.22
N=100 92.25 91.62 91.72
N=500 93.60 92.23 93.12
N=1000 93.56 92.41 93.30
Table 1: Results of TopN Tampering.
The results of the TopN tamperings show that for
both languages, most of the lexical features are irrel-
evant for the classification ? the numbers achieved
by using all the lexical features (about 30,000 in He-
brew and 75,000 in English) are very close to those
obtained using only a few lexical features. This
finding is very encouraging, and suggests that SVM
based chunking is robust to corpus variations.
Another conclusion is that lexical features help
balance the fact that PoS tags can be noisy: we
know both HEBErr and ENG include PoS tag-
ging errors (about 8% in Hebrew and 4% in En-
glish). While in the case of ?perfect? PoS tagging
(HEBGold), a very small amount of lexical features
is sufficient to reach the best F-result (500 out of
30,264), in the presence of PoS errors, more than
the top 1000 lexical features are needed to reach the
result obtained with all lexical features.
More striking is the fact that in Hebrew, the
top 10 lexical features are responsible for an im-
provement of 12.4 in F-score. The words cov-
ered by these 10 features are the following: Start
of Sentence marker and comma, quote,
?of/???, ?and/??, ?the/?? and ?in/??.
This finding suggests that the Hebrew PoS tagset
might not be informative enough for the chunking
task, especially where punctuation 4 and preposi-
tions are concerned. The results in Table 2 give fur-
ther support for this claim.
4Unlike the WSJ PoS tagset in which most punctuations get
unique tags, our tagset treat punctuation marks as one group.
NoPOS HEBG HEBE NoPOS HEBG HEBE
Prep 85.25 84.40 Pronoun 92.97 92.14
Punct 88.90 87.66 Conjunction 92.31 91.67
Adverb 92.02 90.72 Determiner 92.55 91.39
Table 2: Results of Hebrew NoPOS Tampering.
Other scores are ? 93.3(HEBG), ? 92.2(HEBE).
When removing lexical features of a specific
PoS, the most dramatic loss of F-score is reached
for Prepositions and Punctuation marks, followed
by Adverbs, and Conjunctions. Strikingly, lexi-
cal information for most open-class PoS (including
Proper Names and Nouns) has very little impact on
Hebrew chunking performance.
From this observation, one could conclude that
enriching a model based only on PoS with lexical
features for only a few closed-class PoS (prepo-
sitions and punctuation) could provide appropri-
ate results even with a simpler learning method,
one that cannot deal with a large number of fea-
tures. We tested this hypothesis by training the
Error-Driven Pruning (EDP) method of (Cardie and
Pierce, 1998) with an extended set of features. EDP
with PoS features only produced an F-result of 76.3
on HEBGold. By adding lexical features only for
prepositions {? ? ? ? ??}, one conjunction {?} and
punctuation, the F-score on HEBGold indeed jumps
to 85.4. However, when applied on HEBErr, EDP
falls down again to 59.4. This striking disparity, by
comparison, lets us appreciate the resilience of the
SVM model to PoS tagging errors, and its gener-
alization capability even with a reduced number of
lexical features.
Another implication of this data is that commas
and quotation marks play a major role in deter-
mining NP boundaries in Hebrew. In Goldberg
et al (2006), we noted the Hebrew Treebank is not
consistent in its treatment of punctuation, and thus
we evaluated the chunker only after performing nor-
malization of chunk boundaries for punctuations.
We now hypothesize that, since commas and quo-
tation marks play such an important role in the clas-
sification, performing such normalization before the
training stage might be beneficial. Indeed results on
the normalized corpus show improvement of about
1.0 in F score on both HEBErr and HEBGold. A
10-fold cross validation experiment on punctuation
normalized HEBErr resulted in an F-Score of 92.2,
improving the results reported by (Goldberg et al,
227
2006) on the same setting (91.4).
Loc=I HEBE ENG Loc6=I HEBE ENG
-2 78.26 89.79 -2 91.62 93.87
-1 76.96 90.90 -1 91.86 93.03
0 90.33 92.37 0 79.44 91.16
1 76.90 90.47 1 92.33 93.30
2 76.55 90.06 2 92.18 93.65
Table 3: Results of Loc Tamperings.
We now turn to analyzing the importance of con-
text positions (Table 3). For both languages, the
most important lexical feature (by far) is at position
0, that is, the word currently being classified. For
English, it is followed by positions 1 and -1, and
then positions 2 and -2. For Hebrew, back context
seems to have more effect than front context. In
Hebrew, all the positions positively contribute to the
decision, while in English removing w2/?2 slightly
improves the results (note also that including only
feature w2/?2 performs worse than with no lexical
information in English).
3.7 The Real Role of Lexical Features
Model tampering (i.e., removing features after the
learning stage) is not the same as learning without
these features. This claim is verified empirically:
training on the English corpus without the lexical
features at position ?2 yields worse results than with
them (93.73 vs. 93.79) ? while removing the w?2
features via tampering on a model trained with w?2
yields better results (93.87). Similarly, for all cor-
pora, training using only the top 1,000 features (as
defined in the Top1000 tampering) results in loss of
about 2 in F-Score (ENG 92.02, HEBErr 90.30,
HEBGold 91.67), while tampering Top1000 yields
a result very close to the best obtained (93.56, 92.41
or 93.3F).
This observation leads us to an interesting conclu-
sion about the real role of lexical features in SVM
based chunking: rare events (features) are used to
memorize hard examples. Intuitively, by giving a
heavy weight to rare events, the classifier learns spe-
cific rules such as ?if the word at position -2 is X and
the PoS at position 2 is Y, then the current word is
Inside a noun-phrase?. Most of these rules are acci-
dental ? there is no real relation between the partic-
ular word-pos combination and the class of the cur-
rent word, it just happens to be this way in the train-
ing samples. Marking the rare occurrences helps the
learner achieve better generalization on the other,
more common cases, which are similar to the outlier
on most features, except the ?irrelevant ones?. As
the events are rare, such rules usually have no effect
on chunking accuracy: they simply never occur in
the test data. This observation refines the common
conception that SVM chunking does not suffer from
irrelevant features: in chunking, SVM indeed gener-
alizes well for the common cases but also over-fits
the model on outliers.
Model tampering helps us design a model in two
ways: (1) it is a way to ?open the black box? ob-
tained when training an SVM and to analyze the re-
spective importance of features. In our case, this
analysis allowed us to identify the importance of
punctuation and prepositions and improve the model
by defining more focused features (improving over-
all result by ?1.0 F-point). (2) The analysis also led
us to the conclusion that ?feature selection? is com-
plex in the case of SVM ? irrelevant features help
prevent over-generalization by forcing over-fitting
on outliers.
We have also confirmed that the model learned re-
mains robust in the presence of noise in the PoS tags
and relies on only few lexical features. This veri-
fication is critical in the context of languages with
few computational resources, as we expect the size
of corpora and the quality of taggers to keep lagging
behind that achieved in English.
4 Anchored Learning
We pursue the observation of how SVM deals
with outliers by developing the Anchored Learning
method. The idea behind Anchored Learning is to
add a unique feature ai (an anchor) to each training
sample (we add as many new features to the model
as there are training samples). These new features
make our data linearly separable. The SVM learner
can then use these anchors (which will never occur
on the test data) to memorize the hard cases, de-
creasing this burden from ?real? features.
We present two uses for Anchored Learning. The
first is the identification of hard cases and corpus er-
rors, and the second is a preliminary feature selec-
tion approach for SVM to improve chunking accu-
racy.
4.1 Mining for Errors and Hard Cases
Following the intuition that SVM gives more weight
to anchor features of hard-to-classify cases, we can
228
actively look for such cases by training an SVM
chunker on anchored data (as the anchored data is
guaranteed to be linearly separable, we can set a very
high value to the C parameter, preventing any mis-
classification), and then investigating either the an-
chors whose weights5 are above some threshold t or
the top N heaviest anchors, and their corresponding
corpus locations. These locations are those that
the learner considers hard to classify. They can
be either corpus errors, or genuinely hard cases.
This method is similar to the corpus error detec-
tion method presented by Nakagawa and Matsumoto
(2002). They constructed an SVM model for PoS
tagging, and considered Support Vectors with high
? values to be indicative of suspicious corpus loca-
tions. These locations can be either outliers, or cor-
rectly labeled locations similar to an outlier. They
then looked for similar corpus locations with a dif-
ferent label, to point out right-wrong pairs with high
precision.
Using anchors improves their method in three as-
pects: (1) without anchors, similar examples are of-
ten indistinguishable to the SVM learner, and in case
they have conflicting labels both examples will be
given high weights. That is, both the regular case
and the hard case will be considered as hard exam-
ples. Moreover, similar corpus errors might result
in only one support vector that cover all the group of
similar errors. Anchors mitigate these effects, result-
ing in better precision and recall. (2) The more er-
rors there are in the corpus, the less linearly separa-
ble it is. Un-anchored learning on erroneous corpus
can take unreasonable amount of time. (3) Anchors
allow learning while removing some of the impor-
tant features but still allow the process to converge
in reasonable time. This lets us analyze which cases
become hard to learn if we don?t use certain features,
or in other words: what problematic cases are solved
by specific features.
The hard cases analysis achieved by anchored
learning is different from the usual error analysis
carried out on observed classification errors. The
traditional methods give us intuitions about where
the classifier fails to generalize, while the method
we present here gives us intuition about what the
classifier considers hard to learn, based on the
training examples alone.
5As each anchor appear in only one support vector, we can
treat the vector?s ? value as the anchor weight
The intuition that ?hard to learn? examples are
suspect corpus errors is not new, and appears also
in Abney et al (1999) , who consider the ?heaviest?
samples in the final distribution of the AdaBoost al-
gorithm to be the hardest to classify and thus likely
corpus errors. While AdaBoost models are easy to
interpret, this is not the case with SVM. Anchored
learning allows us to extract the hard to learn cases
from an SVM model. Interestingly, while both Ad-
aBoost and SVM are ?large margin? based classi-
fiers, there is less than 50% overlap in the hard cases
for the two methods (in terms of mistakes on the test
data, there were 234 mistakes shared by AdaBoost
and SVM, 69 errors unique to SVM and 126 errors
unique to AdaBoost)6. Analyzing the difference in
what the two classifiers consider hard is interesting,
and we will address it in future work. In the current
work, we note that for finding corpus errors the two
methods are complementary.
Experiment 1 ? Locating Hard Cases
A linear SVM model (Mfull) was trained on
the training subset of the anchored, punctuation-
normalized, HEBGold corpus, with the same fea-
tures as in the previous experiments, and a C value
of 9,999. Corpus locations corresponding to anchors
with weights >1 were inspected. There were about
120 such locations out of 4,500 sentences used in the
training set. Decreasing the threshold t would result
in more cases. We analyzed these locations into 3
categories: corpus errors, cases that challenge the
SimpleNP definition, and cases where the chunking
decision is genuinely difficult to make in the absence
of global syntactic context or world knowledge.
Corpus Errors: The analysis revealed the fol-
lowing corpus errors: we identified 29 hard cases
related to conjunction and apposition (is the comma,
colon or slash inside an NP or separating two distinct
NPs). 14 of these hard cases were indeed mistakes
in the corpus. This was anticipated, as we distin-
guished appositions and conjunctive commas using
heuristics, since the Treebank marking of conjunc-
tions is somewhat inconsistent.
In order to build the Chunk NP corpus, the syn-
tactic trees of the Treebank were processed to derive
chunks according to the SimpleNP definition. The
hard cases analysis identified 18 instances where this
6These numbers are for pairwise Linear SVM and AdaBoost
classifiers trained on the same features.
229
transformation results in erroneous chunks. For ex-
ample, null elements result in improper chunks, such
as chunks containing only adverbs or only adjec-
tives.
We also found 3 invalid sentences, 6 inconsisten-
cies in the tagging of interrogatives with respect to
chunk boundaries, as well as 34 other specific mis-
takes. Overall, more than half of the locations iden-
tified by the anchors were corpus errors. Looking for
cases similar to the errors identified by anchors, we
found 99 more locations, 77 of which were errors.
Refining the SimpleNP Definition: The hard
cases analysis identified examples that challenge
the SimpleNP definition proposed in Goldberg
et al (2006). The most notable cases are:
The ?et? marker : ?et? is a syntactic marker of defi-
nite direct objects in Hebrew. It was regarded as a
part of SimpleNPs in their definition. In some cases,
this forces the resulting SimpleNP to be too inclu-
sive:
[???????? ????? ??? ????? ,?????? ??]
[?et? (the government, the parliament and the media)]
Because in the Treebank the conjunction depends on
?et? as a single constituent, it is fully embedded in
the chunk. Such a conjunction should not be consid-
ered simple.
The ?? preposition (?of?) marks generalized posses-
sion and was considered unambiguous and included
in SimpleNPs. We found cases where ???? causes
PP attachment ambiguity:
[??????] ?? [?????] ? [???? ??? ????]
[president-cons house-cons the-law] for [discipline] of [the
police] / The Police Disciplinary Court President
Because 2 prepositions are involved in this NP, ????
(of) and ??? (for), the ???? part cannot be attached
unambiguously to its head (?court?). It is unclear
whether the ??? preposition should be given special
treatment to allow it to enter simple NPs in certain
contexts, or whether the inconsistent handling of
the ???? that results from the ??? inter-position is
preferable.
Complex determiners and quantifiers: In many
cases, complex determiners in Hebrew are multi-
word expressions that include nouns. The inclusion
of such determiners inside the SimpleNPs is not
consistent.
Genuinely hard cases were also identified.
These include prepositions, conjunctions and multi-
word idioms (most of them are adjectives and prepo-
sitions which are made up of nouns and determin-
ers, e.g., as the word unanimously is expressed in
Hebrew as the multi-word expression ?one mouth?).
Also, some adverbials and adjectives are impossible
to distinguish using only local context.
The anchors analysis helped us improve the
chunking method on two accounts: (1) it identified
corpus errors with high precision; (2) it made us fo-
cus on hard cases that challenge the linguistic defi-
nition of chunks we have adopted. Following these
findings, we intend to refine the Hebrew SimpleNP
definition, and create a new version of the Hebrew
chunking corpus.
Experiment 2 ? determining the role of
contextual lexical features
The intent of this experiment is to understand the
role of the contextual lexical features (wi, i 6= 0).
This is done by training 2 additional anchored lin-
ear SVM models, Mno?cont and Mnear. These are
the same as Mfull except for the lexical features
used during training. Mno?cont uses only w0, while
Mnear uses w0,w?1,w+1.
Anchors are again used to locate the hard exam-
ples for each classifier, and the differences are ex-
amined. The examples that are hard for Mnear but
not for Mfull are those solved by w?2,w+2. Sim-
ilarly, the examples that are hard for Mno?cont but
not for Mnear are those solved by w?1,w+1. Table 4
indicates the number of hard cases identified by the
anchor method for each model. One way to inter-
pret these figures, is that the introduction of features
w?1,+1 solves 5 times more hard cases than w?2,+2.
Model Number of hard
cases (t = 1)
Hard cases for
classifier B-I
Mfull 120 2
Mnear 320 (+ 200) 12
Mno?cont 1360 (+ 1040) 164
Table 4: Number of hard cases per model type.
Qualitative analysis of the hard cases solved by
the contextual lexical features shows that they con-
tribute mostly to the identification of chunk bound-
aries in cases of conjunction, apposition, attachment
of adverbs and adjectives, and some multi-word ex-
pressions.
The number of hard cases specific to the B-I clas-
sifier indicates how the features contribute to the de-
cision of splitting or continuing back-to-back NPs.
Back-to-back NPs amount to 6% of the NPs in
HEBGold and 8% of the NPs in ENG. However,
230
while in English most of these cases are easily re-
solved, Hebrew phenomena such as null-equatives
and free word order make them harder. To quantify
the difference: 79% of the first words of the second
NP in English belong to one of the closed classes
POS, DT, WDT, PRP, WP ? categories which mostly
cannot appear in the middle of base NPs. In con-
trast, in Hebrew, 59% are Nouns, Numbers or Proper
Names. Moreover, in English the ratio of unique first
words to number of adjacent NPs is 0.068, while in
Hebrew it is 0.47. That is, in Hebrew, almost every
second such NP starts with a different word.
These figures explain why surrounding lexical in-
formation is needed by the learner in order to clas-
sify such cases. They also suggest that this learning
is mostly superficial, that is, the learner just mem-
orizes some examples, but these will not generalize
well on test data. Indeed, the most common class of
errors reported in Goldberg et al , 2006 are of the
split/merge type. These are followed by conjunction
related errors, which suffer from the same problem.
Morphological features of smixut and agreement can
help to some extent, but this is still a limited solu-
tion. It seems that deciding the [NP][NP] case is
beyond the capabilities of chunking with local con-
text features alone, and more global features should
be sought.
4.2 Facilitating Better Learning
This section presents preliminary results using An-
chored Learning for better NP chunking. We present
a setting (English Base NP chunking) in which
selected features coupled together with anchored
learning show an improvement over previous results.
Section 3.6 hinted that SVM based chunking
might be hurt by using too many lexical features.
Specifically, the features w?2,w+2 were shown to
cause the chunker to overfit in English chunking.
Learning without these features, however, yields
lower results. This can be overcome by introduc-
ing anchors as a substitute. Anchors play the same
role as rare features when learning, while lowering
the chance of misleading the classifier on test data.
The results of the experiment using 5-fold cross
validation on ENG indicate that the F-score im-
proves on average from 93.95 to 94.10 when using
anchors instead of w?2 (+0.15), while just ignoring
the w?2 features drops the F-score by 0.10. The im-
provement is minor but consistent. Its implication
is that anchors can substitute for ?irrelevant? lexical
features for better learning results. In future work,
we will experiment with better informed sets of lex-
ical features mixed with anchors.
5 Conclusion
We have introduced two novel methods to under-
stand the inner structure of SVM-learned models.
We have applied these techniques to Hebrew NP
chunking, and demonstrated that the learned model
is robust in the presence of noise in the PoS tags, and
relies on only a few lexical features. We have iden-
tified corpus errors, better understood the nature of
the task in Hebrew ? and compared it quantitatively
to the task in English.
The methods provide general insight in the way
SVM classification works for chunking.
References
S. Abney, R. Schapire, and Y. Singer. 1999. Boosting
applied to tagging and PP attachment. EMNLP-1999.
M. Adler and M. Elhadad. 2006. An unsupervised
morpheme-based hmm for hebrew morphological dis-
ambiguation. In COLING/ACL2006.
C. Cardie and D. Pierce. 1998. Error-driven pruning of
treebank grammars for base noun phrase identification.
In ACL-1998.
Y. Goldberg, M. Adler, and M. Elhadad. 2006. Noun
phrase chunking in hebrew: Influence of lexical and
morphological features. In COLING/ACL2006.
T. Kudo and Y. Matsumoto. 2000. Use of support vector
learning for chunk identification. In CoNLL-2000.
T. Kudo and Y. Matsumoto. 2003. Fast methods for
kernel-based text analysis. In ACL-2003.
M. Marcus and L. Ramshaw. 1995. Text Chunking Us-
ing Transformation-Based Learning. In Proc. of the
3rd ACL Workshop on Very Large Corpora.
T. Nakagawa and Y. Matsumoto. 2002. Detecting er-
rors in corpora using support vector machines. In
COLING-2002.
Erik F. Tjong Kim Sang and S. Buchholz. 2000. Intro-
duction to the conll-2000 shared task: chunking. In
CoNLL-2000.
K. Sima?an, A. Itai, Y. Winter, A. Altman, and N. Nativ.
2001. Building a tree-bank of modern hebrew text.
Traitement Automatique des Langues, 42(2).
V. Vapnik. 1995. The nature of statistical learning the-
ory. Springer-Verlag New York, Inc.
231
Proceedings of ACL-08: HLT, pages 371?379,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Single Generative Model
for Joint Morphological Segmentation and Syntactic Parsing
Yoav Goldberg
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
yoavg@cs.bgu.ac.il
Reut Tsarfaty
Institute for Logic Language and Computation
University of Amsterdam
Plantage Muidergracht 24, Amsterdam, NL
rtsarfat@science.uva.nl
Abstract
Morphological processes in Semitic languages
deliver space-delimited words which intro-
duce multiple, distinct, syntactic units into the
structure of the input sentence. These words
are in turn highly ambiguous, breaking the
assumption underlying most parsers that the
yield of a tree for a given sentence is known in
advance. Here we propose a single joint model
for performing both morphological segmenta-
tion and syntactic disambiguation which by-
passes the associated circularity. Using a tree-
bank grammar, a data-driven lexicon, and a
linguistically motivated unknown-tokens han-
dling technique our model outperforms previ-
ous pipelined, integrated or factorized systems
for Hebrew morphological and syntactic pro-
cessing, yielding an error reduction of 12%
over the best published results so far.
1 Introduction
Current state-of-the-art broad-coverage parsers as-
sume a direct correspondence between the lexical
items ingrained in the proposed syntactic analyses
(the yields of syntactic parse-trees) and the space-
delimited tokens (henceforth, ?tokens?) that consti-
tute the unanalyzed surface forms (utterances). In
Semitic languages the situation is very different.
In Modern Hebrew (Hebrew), a Semitic language
with very rich morphology, particles marking con-
junctions, prepositions, complementizers and rela-
tivizers are bound elements prefixed to the word
(Glinert, 1989). The Hebrew token ?bcl?1, for ex-
ample, stands for the complete prepositional phrase
1We adopt here the transliteration of (Sima?an et al, 2001).
?in the shadow?. This token may further embed
into a larger utterance, e.g., ?bcl hneim? (literally
?in-the-shadow the-pleasant?, meaning roughly ?in
the pleasant shadow?) in which the dominated Noun
is modified by a proceeding space-delimited adjec-
tive. It should be clear from the onset that the parti-
cle b (?in?) in ?bcl? may then attach higher than the
bare noun cl (?shadow?). This leads to word- and
constituent-boundaries discrepancy, which breaks
the assumptions underlying current state-of-the-art
statistical parsers.
One way to approach this discrepancy is to as-
sume a preceding phase of morphological segmen-
tation for extracting the different lexical items that
exist at the token level (as is done, to the best of
our knowledge, in all parsing related work on Arabic
and its dialects (Chiang et al, 2006)). The input for
the segmentation task is however highly ambiguous
for Semitic languages, and surface forms (tokens)
may admit multiple possible analyses as in (Bar-
Haim et al, 2007; Adler and Elhadad, 2006). The
aforementioned surface form bcl, for example, may
also stand for the lexical item ?onion?, a Noun. The
implication of this ambiguity for a parser is that the
yield of syntactic trees no longer consists of space-
delimited tokens, and the expected number of leaves
in the syntactic analysis in not known in advance.
Tsarfaty (2006) argues that for Semitic languages
determining the correct morphological segmentation
is dependent on syntactic context and shows that in-
creasing information sharing between the morpho-
logical and the syntactic components leads to im-
proved performance on the joint task. Cohen and
Smith (2007) followed up on these results and pro-
371
posed a system for joint inference of morphological
and syntactic structures using factored models each
designed and trained on its own.
Here we push the single-framework conjecture
across the board and present a single model that
performs morphological segmentation and syntac-
tic disambiguation in a fully generative framework.
We claim that no particular morphological segmen-
tation is a-priory more likely for surface forms be-
fore exploring the compositional nature of syntac-
tic structures, including manifestations of various
long-distance dependencies. Morphological seg-
mentation decisions in our model are delegated to a
lexeme-based PCFG and we show that using a sim-
ple treebank grammar, a data-driven lexicon, and
a linguistically motivated unknown-tokens handling
our model outperforms (Tsarfaty, 2006) and (Co-
hen and Smith, 2007) on the joint task and achieves
state-of-the-art results on a par with current respec-
tive standalone models.2
2 Modern Hebrew Structure
Segmental morphology Hebrew consists of
seven particles m(?from?) f (?when?/?who?/?that?)
h(?the?) w(?and?) k(?like?) l(?to?) and b(?in?).
which may never appear in isolation and must
always attach as prefixes to the following open-class
category item we refer to as stem. Several such
particles may be prefixed onto a single stem, in
which case the affixation is subject to strict linear
precedence constraints. Co-occurrences among the
particles themselves are subject to further syntactic
and lexical constraints relative to the stem.
While the linear precedence of segmental mor-
phemes within a token is subject to constraints, the
dominance relations among their mother and sister
constituents is rather free. The relativizer f(?that?)
for example, may attach to an arbitrarily long rela-
tive clause that goes beyond token boundaries. The
attachment in such cases encompasses a long dis-
tance dependency that cannot be captured by Marko-
vian processes that are typically used for morpho-
logical disambiguation. The same argument holds
for resolving PP attachment of a prefixed preposition
or marking conjunction of elements of any kind.
A less canonical representation of segmental mor-
2Standalone parsing models assume a segmentation Oracle.
phology is triggered by a morpho-phonological pro-
cess of omitting the definite article h when occur-
ring after the particles b or l. This process triggers
ambiguity as for the definiteness status of Nouns
following these particles.We refer to such cases
in which the concatenation of elements does not
strictly correspond to the original surface form as
super-segmental morphology. An additional case of
super-segmental morphology is the case of Pronom-
inal Clitics. Inflectional features marking pronom-
inal elements may be attached to different kinds of
categories marking their pronominal complements.
The additional morphological material in such cases
appears after the stem and realizes the extended
meaning. The current work treats both segmental
and super-segmental phenomena, yet we note that
there may be more adequate ways to treat super-
segmental phenomena assuming Word-Based mor-
phology as we explore in (Tsarfaty and Goldberg,
2008).
Lexical and Morphological Ambiguity The rich
morphological processes for deriving Hebrew stems
give rise to a high degree of ambiguity for Hebrew
space-delimited tokens. The form fmnh, for exam-
ple, can be understood as the verb ?lubricated?, the
possessed noun ?her oil?, the adjective ?fat? or the
verb ?got fat?. Furthermore, the systematic way in
which particles are prefixed to one another and onto
an open-class category gives rise to a distinct sort
of morphological ambiguity: space-delimited tokens
may be ambiguous between several different seg-
mentation possibilities. The same form fmnh can be
segmented as f-mnh, f (?that?) functioning as a rele-
tivizer with the form mnh. The form mnh itself can
be read as at least three different verbs (?counted?,
?appointed?, ?was appointed?), a noun (?a portion?),
and a possessed noun (?her kind?).
Such ambiguities cause discrepancies between
token boundaries (indexed as white spaces) and
constituent boundaries (imposed by syntactic cate-
gories) with respect to a surface form. Such discrep-
ancies can be aligned via an intermediate level of
PoS tags. PoS tags impose a unique morphological
segmentation on surface tokens and present a unique
valid yield for syntactic trees. The correct ambigu-
ity resolution of the syntactic level therefore helps to
resolve the morphological one, and vice versa.
372
3 Previous Work on Hebrew Processing
Morphological analyzers for Hebrew that analyze a
surface form in isolation have been proposed by Se-
gal (2000), Yona and Wintner (2005), and recently
by the knowledge center for processing Hebrew (Itai
et al, 2006). Such analyzers propose multiple seg-
mentation possibilities and their corresponding anal-
yses for a token in isolation but have no means to
determine the most likely ones. Morphological dis-
ambiguators that consider a token in context (an ut-
terance) and propose the most likely morphologi-
cal analysis of an utterance (including segmentation)
were presented by Bar-Haim et al (2005), Adler
and Elhadad (2006), Shacham and Wintner (2007),
and achieved good results (the best segmentation re-
sult so far is around 98%).
The development of the very first Hebrew Tree-
bank (Sima?an et al, 2001) called for the exploration
of general statistical parsing methods, but the appli-
cation was at first limited. Sima?an et al (2001) pre-
sented parsing results for a DOP tree-gram model
using a small data set (500 sentences) and semi-
automatic morphological disambiguation. Tsarfaty
(2006) was the first to demonstrate that fully auto-
matic Hebrew parsing is feasible using the newly
available 5000 sentences treebank. Tsarfaty and
Sima?an (2007) have reported state-of-the-art results
on Hebrew unlexicalized parsing (74.41%) albeit as-
suming oracle morphological segmentation.
The joint morphological and syntactic hypothesis
was first discussed in (Tsarfaty, 2006; Tsarfaty and
Sima?an, 2004) and empirically explored in (Tsar-
faty, 2006). Tsarfaty (2006) used a morphological
analyzer (Segal, 2000), a PoS tagger (Bar-Haim et
al., 2005), and a general purpose parser (Schmid,
2000) in an integrated framework in which morpho-
logical and syntactic components interact to share
information, leading to improved performance on
the joint task. Cohen and Smith (2007) later on
based a system for joint inference on factored, inde-
pendent, morphological and syntactic components
of which scores are combined to cater for the joint
inference task. Both (Tsarfaty, 2006; Cohen and
Smith, 2007) have shown that a single integrated
framework outperforms a completely streamlined
implementation, yet neither has shown a single gen-
erative model which handles both tasks.
4 Model Preliminaries
4.1 The Status Space-Delimited Tokens
A Hebrew surface token may have several readings,
each of which corresponding to a sequence of seg-
ments and their corresponding PoS tags. We refer
to different readings as different analyses whereby
the segments are deterministic given the sequence of
PoS tags. We refer to a segment and its assigned PoS
tag as a lexeme, and so analyses are in fact sequences
of lexemes. For brevity we omit the segments from
the analysis, and so analysis of the form ?fmnh? as
f/REL mnh/VB is represented simply as REL VB.
Such tag sequences are often treated as ?complex
tags? (e.g. REL+VB) (cf. (Bar-Haim et al, 2007;
Habash and Rambow, 2005)) and probabilities are
assigned to different analyses in accordance with
the likelihood of their tags (e.g., ?fmnh is 30%
likely to be tagged NN and 70% likely to be tagged
REL+VB?). Here we do not submit to this view.
When a token fmnh is to be interpreted as the lex-
eme sequence f /REL mnh/VB, the analysis intro-
duces two distinct entities, the relativizer f (?that?)
and the verb mnh (?counted?), and not as the com-
plex entity ?that counted?. When the same token
is to be interpreted as a single lexeme fmnh, it may
function as a single adjective ?fat?. There is no re-
lation between these two interpretations other then
the fact that their surface forms coincide, and we ar-
gue that the only reason to prefer one analysis over
the other is compositional. A possible probabilistic
model for assigning probabilities to complex analy-
ses of a surface form may be
P (REL,VB|fmnh, context) =
P (REL|f)P (VB|mnh,REL)P (REL,VB| context)
and indeed recent sequential disambiguation models
for Hebrew (Adler and Elhadad, 2006) and Arabic
(Smith et al, 2005) present similar models.
We suggest that in unlexicalized PCFGs the syn-
tactic context may be explicitly modeled in the
derivation probabilities. Hence, we take the prob-
ability of the event fmnh analyzed as REL VB to be
P (REL? f|REL) ? P (VB? mnh|VB)
This means that we generate f and mnh indepen-
dently depending on their corresponding PoS tags,
373
and the context (as well as the syntactic relation be-
tween the two) is modeled via the derivation result-
ing in a sequence REL VB spanning the form fmnh.
4.2 Lattice Representation
We represent all morphological analyses of a given
utterance using a lattice structure. Each lattice arc
corresponds to a segment and its corresponding PoS
tag, and a path through the lattice corresponds to
a specific morphological segmentation of the utter-
ance. This is by now a fairly standard representa-
tion for multiple morphological segmentation of He-
brew utterances (Adler, 2001; Bar-Haim et al, 2005;
Smith et al, 2005; Cohen and Smith, 2007; Adler,
2007). Figure 1 depicts the lattice for a 2-words
sentence bclm hneim. We use double-circles to in-
dicate the space-delimited token boundaries. Note
that in our construction arcs can never cross token
boundaries. Every token is independent of the oth-
ers, and the sentence lattice is in fact a concatena-
tion of smaller lattices, one for each token. Fur-
thermore, some of the arcs represent lexemes not
present in the input tokens (e.g. h/DT, fl/POS), how-
ever these are parts of valid analyses of the token (cf.
super-segmental morphology section 2). Segments
with the same surface form but different PoS tags
are treated as different lexemes, and are represented
as separate arcs (e.g. the two arcs labeled neim from
node 6 to 7).
0
5
bclm/NNP
1b/IN
2
bcl/NN
7
hneim/VB
6
h/DT
clm/NN
clm/VB
cl/NN
3
h/DT
4
fl/POS
clm/NN
hm/PRP
neim/VB
neim/JJ
Figure 1: The Lattice for the Hebrew Phrase bclm hneim
A similar structure is used in speech recognition.
There, a lattice is used to represent the possible sen-
tences resulting from an interpretation of an acoustic
model. In speech recognition the arcs of the lattice
are typically weighted in order to indicate the prob-
ability of specific transitions. Given that weights on
all outgoing arcs sum up to one, weights induce a
probability distribution on the lattice paths. In se-
quential tagging models such as (Adler and Elhadad,
2006; Bar-Haim et al, 2007; Smith et al, 2005)
weights are assigned according to a language model
based on linear context. In our model, however, all
lattice paths are taken to be a-priori equally likely.
5 A Generative PCFG Model
The input for the joint task is a sequence W =
w1, . . . , wn of space-delimited tokens. Each tokenmay admit multiple analyses, each of which a se-
quence of one or more lexemes (we use li to denotea lexeme) belonging a presupposed Hebrew lexicon
LEX . The entries in such a lexicon may be thought
of as meaningful surface segments paired up with
their PoS tags li = ?si, pi?, but note that a surfacesegment s need not be a space-delimited token.
The Input The set of analyses for a token is thus
represented as a lattice in which every arc corre-
sponds to a specific lexeme l, as shown in Figure
1. A morphological analyzer M : W ? L is a
function mapping sentences in Hebrew (W ? W)
to their corresponding lattices (M(W ) = L ? L).
We define the lattice L to be the concatenation of the
lattices Li corresponding to the input words wi (s.t.
M(wi) = Li). Each connected path ?l1 . . . lk? ?
L corresponds to one morphological segmentation
possibility of W .
The Parser Given a sequence of input tokens
W = w1 . . . wn and a morphological analyzer, welook for the most probable parse tree pi s.t.
p?i = arg max
pi
P (pi|W,M)
Since the lattice L for a given sentence W is deter-
mined by the morphological analyzer M we have
p?i = arg max
pi
P (pi|W,M,L)
Hence, our parser searches for a parse tree pi over
lexemes ?l1 . . . lk? s.t. li = ?si, pi? ? LEX ,
?l1 . . . lk? ? L and M(W ) = L. So we remain with
p?i = arg max
pi
P (pi|L)
which is precisely the formula corresponding to the
so-called lattice parsing familiar from speech recog-
nition. Every parse pi selects a specific morphologi-
cal segmentation ?l1...lk? (a path through the lattice).This is akin to PoS tags sequences induced by dif-
ferent parses in the setup familiar from English and
explored in e.g. (Charniak et al, 1996).
374
Our use of an unweighted lattice reflects our be-
lief that all the segmentations of the given input sen-
tence are a-priori equally likely; the only reason to
prefer one segmentation over the another is due to
the overall syntactic context which is modeled via
the PCFG derivations. A compatible view is pre-
sented by Charniak et al (1996) who consider the
kind of probabilities a generative parser should get
from a PoS tagger, and concludes that these should
be P (w|t) ?and nothing fancier?.3 In our setting,
therefore, the Lattice is not used to induce a proba-
bility distribution on a linear context, but rather, it is
used as a common-denominator of state-indexation
of all segmentations possibilities of a surface form.
This is a unique object for which we are able to de-
fine a proper probability model. Thus our proposed
model is a proper model assigning probability mass
to all ?pi,L? pairs, where pi is a parse tree and L is
the one and only lattice that a sequence of characters
(and spaces) W over our alpha-beth gives rise to.
?
pi,L
P (pi,L) = 1; L uniquely index W
The Grammar Our parser looks for the most
likely tree spanning a single path through the lat-
tice of which the yield is a sequence of lexemes.
This is done using a simple PCFG which is lexeme-
based. This means that the rules in our grammar
are of two kinds: (a) syntactic rules relating non-
terminals to a sequence of non-terminals and/or PoS
tags, and (b) lexical rules relating PoS tags to lattice
arcs (lexemes). The possible analyses of a surface
token pose constraints on the analyses of specific
segments. In order to pass these constraints onto the
parser, the lexical rules in the grammar are of the
form pi ? ?si, pi?
Parameter Estimation The grammar probabili-
ties are estimated from the corpus using simple rela-
tive frequency estimates. Lexical rules are estimated
in a similar manner. We smooth Prf (p ? ?s, p?) forrare and OOV segments (s ? l, l ? L, s unseen) us-
ing a ?per-tag? probability distribution over rare seg-
ments which we estimate using relative frequency
estimates for once-occurring segments.
3An English sentence with ambiguous PoS assignment can
be trivially represented as a lattice similar to our own, where
every pair of consecutive nodes correspond to a word, and every
possible PoS assignment for this word is a connecting arc.
Handling Unknown tokens When handling un-
known tokens in a language such as Hebrew various
important aspects have to be borne in mind. Firstly,
Hebrew unknown tokens are doubly unknown: each
unknown token may correspond to several segmen-
tation possibilities, and each segment in such se-
quences may be able to admit multiple PoS tags.
Secondly, some segments in a proposed segment se-
quence may in fact be seen lexical events, i.e., for
some p tag Prf (p ? ?s, p?) > 0, while other seg-ments have never been observed as a lexical event
before. The latter arcs correspond to OOV words
in English. Finally, the assignments of PoS tags to
OOV segments is subject to language specific con-
straints relative to the token it was originated from.
Our smoothing procedure takes into account all
the aforementioned aspects and works as follows.
We first make use of our morphological analyzer to
find all segmentation possibilities by chopping off
all prefix sequence possibilities (including the empty
prefix) and construct a lattice off of them. The re-
maining arcs are marked OOV. At this stage the lat-
tice path corresponds to segments only, with no PoS
assigned to them. In turn we use two sorts of heuris-
tics, orthogonal to one another, to prune segmenta-
tion possibilities based on lexical and grammatical
constraints. We simulate lexical constraints by using
an external lexical resource against which we verify
whether OOV segments are in fact valid Hebrew lex-
emes. This heuristics is used to prune all segmenta-
tion possibilities involving ?lexically improper? seg-
ments. For the remaining arcs, if the segment is in
fact a known lexeme it is tagged as usual, but for the
OOV arcs which are valid Hebrew entries lacking
tags assignment, we assign all possible tags and then
simulate a grammatical constraint. Here, all token-
internal collocations of tags unseen in our training
data are pruned away. From now on all lattice arcs
are tagged segments and the assignment of probabil-
ity P (p ? ?s, p?) to lattice arcs proceeds as usual.4
A rather pathological case is when our lexical
heuristics prune away all segmentation possibilities
and we remain with an empty lattice. In such cases
we use the non-pruned lattice including all (possibly
ungrammatical) segmentation, and let the statistics
(including OOV) decide. We empirically control for
4Our heuristics may slightly alter Ppi,L P (pi, L) ? 1
375
the effect of our heuristics to make sure our pruning
does not undermine the objectives of our joint task.
6 Experimental Setup
Previous work on morphological and syntactic dis-
ambiguation in Hebrew used different sets of data,
different splits, differing annotation schemes, and
different evaluation measures. Our experimental
setup therefore is designed to serve two goals. Our
primary goal is to exploit the resources that are most
appropriate for the task at hand, and our secondary
goal is to allow for comparison of our models? per-
formance against previously reported results. When
a comparison against previous results requires addi-
tional pre-processing, we state it explicitly to allow
for the reader to replicate the reported results.
Data We use the Hebrew Treebank, (Sima?an
et al, 2001), provided by the knowledge center
for processing Hebrew, in which sentences from
the daily newspaper ?Ha?aretz? are morphologically
segmented and syntactically annotated. The tree-
bank has two versions, v1.0 and v2.0, containing
5001 and 6501 sentences respectively. We use v1.0
mainly because previous studies on joint inference
reported results w.r.t. v1.0 only.5 We expect that
using the same setup on v2.0 will allow a cross-
treebank comparison.6 We used the first 500 sen-
tences as our dev set and the rest 4500 for training
and report our main results on this split. To facili-
tate the comparison of our results to those reported
by (Cohen and Smith, 2007) we use their data set in
which 177 empty and ?malformed?7 were removed.
The first 3770 trees of the resulting set then were
used for training, and the last 418 are used testing.
(we ignored the 419 trees in their development set.)
Morphological Analyzer Ideally, we would use
an of-the-shelf morphological analyzer for mapping
each input token to its possible analyses. Such re-
sources exist for Hebrew (Itai et al, 2006), but un-
fortunately use a tagging scheme which is incom-
5The comparison to performance on version 2.0 is meaning-
less not only because of the change in size, but also conceptual
changes in the annotation scheme
6Unfortunatley running our setup on the v2.0 data set is cur-
rently not possible due to missing tokens-morphemes alignment
in the v2.0 treebank.
7We thank Shay Cohen for providing us with their data set
and evaluation Software.
patible with the one of the Hebrew Treebank.8 For
this reason, we use a data-driven morphological an-
alyzer derived from the training data similar to (Co-
hen and Smith, 2007). We construct a mapping from
all the space-delimited tokens seen in the training
sentences to their corresponding analyses.
Lexicon and OOV Handling Our data-driven
morphological-analyzer proposes analyses for un-
known tokens as described in Section 5. We use the
HSPELL9 (Har?el and Kenigsberg, 2004) wordlist
as a lexeme-based lexicon for pruning segmenta-
tions involving invalid segments. Models that em-
ploy this strategy are denoted hsp. To control for
the effect of the HSPELL-based pruning, we also ex-
perimented with a morphological analyzer that does
not perform this pruning. For these models we limit
the options provided for OOV words by not consid-
ering the entire token as a valid segmentation in case
at least some prefix segmentation exists. This ana-
lyzer setting is similar to that of (Cohen and Smith,
2007), and models using it are denoted nohsp,
Parser and Grammar We used BitPar (Schmid,
2004), an efficient general purpose parser,10 together
with various treebank grammars to parse the in-
put sentences and propose compatible morpholog-
ical segmentation and syntactic analysis.
We experimented with increasingly rich gram-
mars read off of the treebank. Our first model is
GTplain, a PCFG learned from the treebank afterremoving all functional features from the syntactic
categories. In our second model GTvpi we alsodistinguished finite and non-finite verbs and VPs as
8Mapping between the two schemes involves non-
deterministic many-to-many mappings, and in some cases re-
quire a change in the syntactic trees.
9An open-source Hebrew spell-checker.
10Lattice parsing can be performed by special initialization
of the chart in a CKY parser (Chappelier et al, 1999). We
currently simulate this by crafting a WCFG and feeding it to
BitPar. Given a PCFG grammar G and a lattice L with nodes
n1 . . . nk , we construct the weighted grammar GL as follows:for every arc (lexeme) l ? L from node ni to node nj , we add
to GL the rule [l ? tni , tni+1 , . . . , tnj?1 ] with a probability of1 (this indicates the lexeme l spans from node ni to node nj).
GL is then used to parse the string tn1 . . . tnk?1 , where tni isa terminal corresponding to the lattice span between node ni
and ni+1. Removing the leaves from the resulting tree yields a
parse for L under G, with the desired probabilities. We use a
patched version of BitPar allowing for direct input of probabili-
ties instead of counts. We thank Felix Hageloh (Hageloh, 2006)
for providing us with this version.
376
proposed in (Tsarfaty, 2006). In our third model
GTppp we also add the distinction between gen-eral PPs and possessive PPs following Goldberg and
Elhadad (2007). In our forth model GTnph weadd the definiteness status of constituents follow-
ing Tsarfaty and Sima?an (2007). Finally, model
GTv = 2 includes parent annotation on top of thevarious state-splits, as is done also in (Tsarfaty and
Sima?an, 2007; Cohen and Smith, 2007). For all
grammars, we use fine-grained PoS tags indicating
various morphological features annotated therein.
Evaluation We use 8 different measures to eval-
uate the performance of our system on the joint dis-
ambiguation task. To evaluate the performance on
the segmentation task, we report SEG, the stan-
dard harmonic means for segmentation Precision
and Recall F1 (as defined in Bar-Haim et al (2005);Tsarfaty (2006)) as well as the segmentation ac-
curacy SEGTok measure indicating the percentageof input tokens assigned the correct exact segmen-
tation (as reported by Cohen and Smith (2007)).
SEGTok(noH) is the segmentation accuracy ignor-ing mistakes involving the implicit definite article
h.11 To evaluate our performance on the tagging
task we report CPOS and FPOS corresponding
to coarse- and fine-grained PoS tagging results (F1)measure. Evaluating parsing results in our joint
framework, as argued by Tsarfaty (2006), is not triv-
ial under the joint disambiguation task, as the hy-
pothesized yield need not coincide with the correct
one. Our parsing performance measures (SY N )
thus report the PARSEVAL extension proposed in
Tsarfaty (2006). We further report SY NCS , the
parsing metric of Cohen and Smith (2007), to fa-
cilitate the comparison. We report the F1 value ofboth measures. Finally, our U (unparsed) measure
is used to report the number of sentences to which
our system could not propose a joint analysis.
7 Results and Analysis
The accuracy results for segmentation, tagging and
parsing using our different models and our standard
data split are summarized in Table 1. In addition
we report for each model its performance on gold-
segmented input (GS) to indicate the upper bound
11Overt definiteness errors may be seen as a wrong feature
rather than as wrong constituent and it is by now an accepted
standard to report accuracy with and without such errors.
for the grammars? performance on the parsing task.
The table makes clear that enriching our grammar
improves the syntactic performance as well as mor-
phological disambiguation (segmentation and POS
tagging) accuracy. This supports our main thesis that
decisions taken by single, improved, grammar are
beneficial for both tasks. When using the segmen-
tation pruning (using HSPELL) for unseen tokens,
performance improves for all tasks as well. Yet we
note that the better grammars without pruning out-
perform the poorer grammars using this technique,
indicating that the syntactic context aids, to some
extent, the disambiguation of unknown tokens.
Table 2 compares the performance of our system
on the setup of Cohen and Smith (2007) to the best
results reported by them for the same tasks.
Model SEGTok CPOS FPOS SY NCS
GTnohsp/pln 89.50 81.00 77.65 62.22
GTnohsp/???+nph 89.58 81.26 77.82 64.30
CSpln 91.10 80.40 75.60 64.00
CSv=2 90.90 80.50 75.40 64.40
GThsp/pln 93.13 83.12 79.12 64.46
GTnohsp/???+v=2 89.66 82.85 78.92 66.31Oracle CSpln 91.80 83.20 79.10 66.50Oracle CSv=2 91.70 83.00 78.70 67.40
GThsp/???+v=2 93.38 85.08 80.11 69.11
Table 2: Segmentation, Parsing and Tagging Results us-
ing the Setup of (Cohen and Smith, 2007) (sentence
length ? 40). The Models? are Ordered by Performance.
We first note that the accuracy results of our
system are overall higher on their setup, on all
measures, indicating that theirs may be an easier
dataset. Secondly, for all our models we provide
better fine- and coarse-grained POS-tagging accu-
racy, and all pruned models outperform the Ora-
cle results reported by them.12 In terms of syn-
tactic disambiguation, even the simplest grammar
pruned with HSPELL outperforms their non-Oracle
results. Without HSPELL-pruning, our simpler
grammars are somewhat lagging behind, but as the
grammars improve the gap is bridged. The addi-
tion of vertical markovization enables non-pruned
models to outperform all previously reported re-
12Cohen and Smith (2007) make use of a parameter (?)
which is tuned separately for each of the tasks. This essentially
means that their model does not result in a true joint inference,
as executions for different tasks involve tuning a parameter sep-
arately. In our model there are no such hyper-parameters, and
the performance is the result of truly joint disambiguation.
377
Model U SEGTok / no H SEGF CPOS FPOS SY N / SY NCS GS SY N
GTnohsp/pln 7 89.77 / 93.18 91.80 80.36 76.77 60.41 / 61.66 65.00
???+vpi 7 89.80 / 93.18 91.84 80.37 76.74 61.16 / 62.41 66.70
???+ppp 7 89.79 / 93.20 91.86 80.43 76.79 61.47 / 62.86 67.22
???+nph 7 89.78 / 93.20 91.86 80.43 76.87 61.85 / 63.06 68.23
???+v=2 9 89.12 / 92.45 91.77 82.02 77.86 64.53 / 66.02 70.82
GThsp/pln 11 92.00 / 94.81 94.52 82.35 78.11 62.10 / 64.17 65.00
???+vpi 11 92.03 / 94.82 94.58 82.39 78.23 63.00 / 65.06 66.70
???+ppp 11 92.02 / 94.85 94.58 82.48 78.33 63.26 / 65.42 67.22
???+nph 11 92.14 / 94.91 94.73 82.58 78.47 63.98 / 65.98 68.23
???+v=2 13 91.42 / 94.10 94.67 84.23 79.25 66.60 / 68.79 70.82
Table 1: Segmentation, tagging and parsing results on the Standard dev/train Split, for all Sentences
sults. Furthermore, the combination of pruning and
vertical markovization of the grammar outperforms
the Oracle results reported by Cohen and Smith.
This essentially means that a better grammar tunes
the joint model for optimized syntactic disambigua-
tion at least in as much as their hyper parameters
do. An interesting observation is that while vertical
markovization benefits all our models, its effect is
less evident in Cohen and Smith.
On the surface, our model may seem as a special
case of Cohen and Smith in which ? = 0. How-
ever, there is a crucial difference: the morphological
probabilities in their model come from discrimina-
tive models based on linear context. Many morpho-
logical decisions are based on long distance depen-
dencies, and when the global syntactic evidence dis-
agrees with evidence based on local linear context,
the two models compete with one another, despite
the fact that the PCFG takes also local context into
account. In addition, as the CRF and PCFG look at
similar sorts of information from within two inher-
ently different models, they are far from independent
and optimizing their product is meaningless. Cohen
and Smith approach this by introducing the ? hy-
perparameter, which performs best when optimized
independently for each sentence (cf. Oracle results).
In contrast, our morphological probabilities are
based on a unigram, lexeme-based model, and all
other (local and non-local) contextual considerations
are delegated to the PCFG. This fully generative
model caters for real interaction between the syn-
tactic and morphological levels as a part of a single
coherent process.
8 Discussion and Conclusion
Employing a PCFG-based generative framework to
make both syntactic and morphological disambigua-
tion decisions is not only theoretically clean and
linguistically justified and but also probabilistically
apropriate and empirically sound. The overall per-
formance of our joint framework demonstrates that
a probability distribution obtained over mere syn-
tactic contexts using a Treebank grammar and a
data-driven lexicon outperforms upper bounds pro-
posed by previous joint disambiguation systems and
achieves segmentation and parsing results on a par
with state-of-the-art standalone applications results.
Better grammars are shown here to improve per-
formance on both morphological and syntactic tasks,
providing support for the advantage of a joint frame-
work over pipelined or factorized ones. We conjec-
ture that this trend may continue by incorporating
additional information, e.g., three-dimensional mod-
els as proposed by Tsarfaty and Sima?an (2007). In
the current work morphological analyses and lexi-
cal probabilities are derived from a small Treebank,
which is by no means the best way to go. Using
a wide-coverage morphological analyzer based on
(Itai et al, 2006) should cater for a better cover-
age, and incorporating lexical probabilities learned
from a big (unannotated) corpus (cf. (Levinger et
al., 1995; Goldberg et al, ; Adler et al, 2008)) will
make the parser more robust and suitable for use in
more realistic scenarios.
Acknowledgments We thank Meni Adler and
Michael Elhadad (BGU) for helpful comments and
discussion. We further thank Khalil Simaan (ILLC-
UvA) for his careful advise concerning the formal
details of the proposal. The work of the first au-
thor was supported by the Lynn and William Frankel
Center for Computer Sciences. The work of the sec-
ond author as well as collaboration visits to Israel
was financed by NWO, grant number 017.001.271.
378
References
Meni Adler and Michael Elhadad. 2006. An Unsuper-
vised Morpheme-Based HMM for Hebrew Morpho-
logical Disambiguation. In Proceeding of COLING-
ACL-06, Sydney, Australia.
Meni Adler, Yoav Goldberg, David Gabay, and Michael
Elhadad. 2008. Unsupervised Lexicon-Based Reso-
lution of Unknown Words for Full Morpholological
Analysis. In Proceedings of ACL-08.
Meni Adler. 2001. Hidden Markov Model for Hebrew
Part-of-Speech Tagging. Master?s thesis, Ben-Gurion
University of the Negev.
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev, Beer-Sheva, Israel.
Roy Bar-Haim, Khalil Sima?an, and Yoad Winter. 2005.
Choosing an optimal architecture for segmentation and
pos- tagging of modern Hebrew. In Proceedings of
ACL-05 Workshop on Computational Approaches to
Semitic Languages.
Roy Bar-Haim, Khalil Sima?an, and Yoad Winter. 2007.
Part-of-speech tagging of Modern Hebrew text. Natu-
ral Language Engineering, 14(02):223?251.
J. Chappelier, M. Rajman, R. Aragues, and A. Rozen-
knop. 1999. Lattice Parsing for Speech Recognition.
Eugene Charniak, Glenn Carroll, John Adcock, An-
thony R. Cassandra, Yoshihiko Gotoh, Jeremy Katz,
Michael L. Littman, and John McCann. 1996. Tag-
gers for Parsers. AI, 85(1-2):45?57.
David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic Di-
alects. In Proceedings of EACL-06.
Shay B. Cohen and Noah A. Smith. 2007. Joint morpho-
logical and syntactic disambiguation. In Proceedings
of EMNLP-CoNLL-07, pages 208?217.
Lewis Glinert. 1989. The Grammar of Modern Hebrew.
Cambridge University Press.
Yoav Goldberg and Michael Elhadad. 2007. SVM Model
Tampering and Anchored Learning: A Case Study
in Hebrew NP Chunking. In Proceeding of ACL-07,
Prague, Czech Republic.
Yoav Goldberg, Meni Adler, and Michael Elhadad. EM
Can Find Pretty G]ood HMM POS-Taggers (When
Given a Good Start), booktitle = Proceedings of ACL-
08, year = 2008,.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceeding of
ACL-05.
Felix Hageloh. 2006. Parsing Using Transforms over
Treebanks. Master?s thesis, University of Amsterdam.
Nadav Har?el and Dan Kenigsberg. 2004. HSpell - the
free Hebrew Spell Checker and Morphological Ana-
lyzer. Israeli Seminar on Computational Linguistics.
Alon Itai, Shuly Wintner, and Shlomo Yona. 2006. A
Computational Lexicon of Contemporary Hebrew. In
Proceedings of LREC-06.
Moshe Levinger, Uzi Ornan, and Alon Itai. 1995. Learn-
ing Morpholexical Probabilities from an Untagged
Corpus with an Application to Hebrew. Computa-
tional Linguistics, 21:383?404.
Helmut Schmid, 2000. LoPar: Design and Implementa-
tion. Institute for Computational Linguistics, Univer-
sity of Stuttgart.
Helmut Schmid. 2004. Efficient Parsing of Highly Am-
biguous Context-Free Grammars with Bit Vector. In
Proceedings of COLING-04.
Erel Segal. 2000. Hebrew Morphological Analyzer for
Hebrew Undotted Texts. Master?s thesis, Technion,
Haifa, Israel.
Danny Shacham and Shuly Wintner. 2007. Morpho-
logical Disambiguation of Hebrew: A Case Study in
Classifier Combination. In Proceedings of EMNLP-
CoNLL-07, pages 439?447.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,
and Noa Nativ. 2001. Building a Tree-Bank for
Modern Hebrew Text. In Traitement Automatique des
Langues, volume 42.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambiguation
with random fields. In Proceedings of HLT-05, pages
475?482, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Reut Tsarfaty and Yoav Goldberg. 2008. Word-Based or
Morpheme-Based? Annotation Strategies for Modern
Hebrew Clitics. In Proceedings of LREC-08.
Reut Tsarfaty and Khalil Sima?an. 2004. An Integrated
Model for Morphological and Syntactic Disambigua-
tion in Modern Hebrew. MOZAIEK detailed proposal,
NWO Mozaiek scheme.
Reut Tsarfaty and Khalil Sima?an. 2007. Three-
Dimensional Parametrization for Parsing Morphologi-
cally Rich Languages. In Proceedings of IWPT-07.
Reut Tsarfaty. 2006. Integrated Morphological and Syn-
tactic Disambiguation for Modern Hebrew. In Pro-
ceedings of ACL-SRW-06.
Shlomo Yona and Shuly Wintner. 2005. A Finite-
state Morphological Grammar of Hebrew. In Proceed-
ings of the ACL-05 Workshop on Computational Ap-
proaches to Semitic Languages.
379
Proceedings of ACL-08: HLT, pages 728?736,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Unsupervised Lexicon-Based Resolution of Unknown Words for Full
Morphological Analysis
Meni Adler and Yoav Goldberg and David Gabay and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science?
POB 653 Be?er Sheva, 84105, Israel
{adlerm,goldberg,gabayd,elhadad}@cs.bgu.ac.il
Abstract
Morphological disambiguation proceeds in 2
stages: (1) an analyzer provides all possible
analyses for a given token and (2) a stochastic
disambiguation module picks the most likely
analysis in context. When the analyzer does
not recognize a given token, we hit the prob-
lem of unknowns. In large scale corpora, un-
knowns appear at a rate of 5 to 10% (depend-
ing on the genre and the maturity of the lexi-
con).
We address the task of computing the distribu-
tion p(t|w) for unknown words for full mor-
phological disambiguation in Hebrew. We in-
troduce a novel algorithm that is language in-
dependent: it exploits a maximum entropy let-
ters model trained over the known words ob-
served in the corpus and the distribution of
the unknown words in known tag contexts,
through iterative approximation. The algo-
rithm achieves 30% error reduction on dis-
ambiguation of unknown words over a com-
petitive baseline (to a level of 70% accurate
full disambiguation of unknown words). We
have also verified that taking advantage of a
strong language-specific model of morpholog-
ical patterns provides the same level of disam-
biguation. The algorithm we have developed
exploits distributional information latent in a
wide-coverage lexicon and large quantities of
unlabeled data.
?This work is supported in part by the Lynn and William
Frankel Center for Computer Science.
1 Introduction
The term unknowns denotes tokens in a text that can-
not be resolved in a given lexicon. For the task of
full morphological analysis, the lexicon must pro-
vide all possible morphological analyses for any
given token. In this case, unknown tokens can be
categorized into two classes of missing informa-
tion: unknown tokens are not recognized at all by
the lexicon, and unknown analyses, where the set
of analyses for a lexeme does not contain the cor-
rect analysis for a given token. Despite efforts on
improving the underlying lexicon, unknowns typi-
cally represent 5% to 10% of the number of tokens
in large-scale corpora. The alternative to continu-
ously investing manual effort in improving the lex-
icon is to design methods to learn possible analy-
ses for unknowns from observable features: their
letter structure and their context. In this paper, we
investigate the characteristics of Hebrew unknowns
for full morphological analysis, and propose a new
method for handling such unavoidable lack of in-
formation. Our method generates a distribution of
possible analyses for unknowns. In our evaluation,
these learned distributions include the correct anal-
ysis for unknown words in 85% of the cases, con-
tributing an error reduction of over 30% over a com-
petitive baseline for the overall task of full morpho-
logical analysis in Hebrew.
The task of a morphological analyzer is to pro-
duce all possible analyses for a given token. In
Hebrew, the analysis for each token is of the form
lexeme-and-features1: lemma, affixes, lexical cate-
1In contrast to the prefix-stem-suffix analysis format of
728
gory (POS), and a set of inflection properties (ac-
cording to the POS) ? gender, number, person, sta-
tus and tense. In this work, we refer to the mor-
phological analyzer of MILA ? the Knowledge Cen-
ter for Processing Hebrew2 (hereafter KC analyzer).
It is a synthetic analyzer, composed of two data re-
sources ? a lexicon of about 2,400 lexemes, and a
set of generation rules (see (Adler, 2007, Section
4.2)). In addition, we use an unlabeled text cor-
pus, composed of stories taken from three Hebrew
daily news papers (Aruts 7, Haaretz, The Marker),
of 42M tokens. We observed 3,561 different com-
posite tags (e.g., noun-sing-fem-prepPrefix:be) over
this corpus. These 3,561 tags form the large tagset
over which we train our learner. On the one hand,
this tagset is much larger than the largest tagset used
in English (from 17 tags in most unsupervised POS
tagging experiments, to the 46 tags of the WSJ cor-
pus and the about 150 tags of the LOB corpus). On
the other hand, our tagset is intrinsically factored as
a set of dependent sub-features, which we explicitly
represent.
The task we address in this paper is morphologi-
cal disambiguation: given a sentence, obtain the list
of all possible analyses for each word from the an-
alyzer, and disambiguate each word in context. On
average, each token in the 42M corpus is given 2.7
possible analyses by the analyzer (much higher than
the average 1.41 POS tag ambiguity reported in En-
glish (Dermatas and Kokkinakis, 1995)). In previ-
ous work, we report disambiguation rates of 89%
for full morphological disambiguation (using an un-
supervised EM-HMM model) and 92.5% for part of
speech and segmentation (without assigning all the
inflectional features of the words).
In order to estimate the importance of unknowns
in Hebrew, we analyze tokens in several aspects: (1)
the number of unknown tokens, as observed on the
corpus of 42M tokens; (2) a manual classification
of a sample of 10K unknown token types out of the
200K unknown types identified in the corpus; (3) the
number of unknown analyses, based on an annotated
corpus of 200K tokens, and their classification.
About 4.5% of the 42M token instances in the
Buckwalter?s Arabic analyzer (2004), which looks for any le-
gal combination of prefix-stem-suffix, but does not provide full
morphological features such as gender, number, case etc.
2http://mila.cs.technion.ac.il.html
training corpus were unknown tokens (45% of the
450K token types). For less edited text, such as ran-
dom text sampled from the Web, the percentage is
much higher ? about 7.5%. In order to classify these
unknown tokens, we sampled 10K unknown token
types and examined them manually. The classifica-
tion of these tokens with their distribution is shown
in Table 13. As can be seen, there are two main
classes of unknown token types: Neologisms (32%)
and Proper nouns (48%), which cover about 80%
of the unknown token instances. The POS distribu-
tion of the unknown tokens of our annotated corpus
is shown in Table 2. As expected, most unknowns
are open class words: proper names, nouns or adjec-
tives.
Regarding unknown analyses, in our annotated
corpus, we found 3% of the 100K token instances
were missing the correct analysis in the lexicon
(3.65% of the token types). The POS distribution of
the unknown analyses is listed in Table 2. The high
rate of unknown analyses for prepositions at about
3% is a specific phenomenon in Hebrew, where
prepositions are often prefixes agglutinated to the
first word of the noun phrase they head. We observe
the very low rate of unknown verbs (2%) ? which are
well marked morphologically in Hebrew, and where
the rate of neologism introduction seems quite low.
This evidence illustrates the need for resolution
of unknowns: The naive policy of selecting ?proper
name? for all unknowns will cover only half of the
errors caused by unknown tokens, i.e., 30% of the
whole unknown tokens and analyses. The other 70%
of the unknowns ( 5.3% of the words in the text in
our experiments) will be assigned a wrong tag.
As a result of this observation, our strategy is to
focus on full morphological analysis for unknown
tokens and apply a proper name classifier for un-
known analyses and unknown tokens. In this paper,
we investigate various methods for achieving full
morphological analysis distribution for unknown to-
kens. The methods are not based on an annotated
corpus, nor on hand-crafted rules, but instead ex-
ploit the distribution of words in an available lexicon
and the letter similarity of the unknown words with
known words.
3Transcription according to Ornan (2002)
729
Category Examples DistributionTypes Instances
Proper names ?asulin (family name) oileq`
?a?udi (Audi) ice`` 40% 48%
Neologisms ?agabi (incidental) iab`
tizmur (orchestration) xenfz 30% 32%
Abbreviation mz?p (DIFS) t"fnkb?t (security officer) h"aw 2.4% 7.8%
Foreign
presentacyah (presentation) divhpfxt
?a?ut (out) he``
right
3.8% 5.8%
Wrong spelling
?abibba?ah
.
ronah (springatlast) dpexg`aaia`
?idiqacyot (idication) zeivwici`
ryus?alaim (Rejusalem) milyeix
1.2% 4%
Alternative spelling ?opyynim (typical) mipiite`priwwilegyah (privilege ) diblieeixt 3.5% 3%
Tokenization ha?sap (the?threshold) sq"d
?al/17 (on/17) 71/lr 8% 2%
Table 1: Unknown Hebrew token categories and distribution.
Part of Speech Unknown Tokens Unknown Analyses Total
Proper name 31.8% 24.4% 56.2%
Noun 12.6% 1.6% 14.2%
Adjective 7.1% 1.7% 8.8%
Junk 3.0% 1.3% 4.3%
Numeral 1.1% 2.3% 3.4%
Preposition 0.3% 2.8% 3.1%
Verb 1.8% 0.4% 2.2%
Adverb 0.9% 0.9% 1.8%
Participle 0.4% 0.8% 1.2%
Copula / 0.8% 0.8%
Quantifier 0.3% 0.4% 0.7%
Modal 0.3% 0.4% 0.7%
Conjunction 0.1% 0.5% 0.6%
Negation / 0.6% 0.6%
Foreign 0.2% 0.4% 0.6%
Interrogative 0.1% 0.4% 0.5%
Prefix 0.3% 0.2% 0.5%
Pronoun / 0.5% 0.5%
Total 60% 40% 100%
Table 2: Unknowns Hebrew POS Distribution.
730
2 Previous Work
Most of the work that dealt with unknowns in the last
decade focused on unknown tokens (OOV). A naive
approach would assign all possible analyses for each
unknown token with uniform distribution, and con-
tinue disambiguation on the basis of a learned model
with this initial distribution. The performance of a
tagger with such a policy is actually poor: there are
dozens of tags in the tagset (3,561 in the case of He-
brew full morphological disambiguation) and only
a few of them may match a given token. Several
heuristics were developed to reduce the possibility
space and to assign a distribution for the remaining
analyses.
Weischedel et al (1993) combine several heuris-
tics in order to estimate the token generation prob-
ability according to various types of information ?
such as the characteristics of particular tags with
respect to unknown tokens (basically the distribu-
tion shown in Table 2), and simple spelling fea-
tures: capitalization, presence of hyphens and spe-
cific suffixes. An accuracy of 85% in resolving un-
known tokens was reported. Dermatas and Kokki-
nakis (1995) suggested a method for guessing un-
known tokens based on the distribution of the ha-
pax legomenon, and reported an accuracy of 66% for
English. Mikheev (1997) suggested a guessing-rule
technique, based on prefix morphological rules, suf-
fix morphological rules, and ending-guessing rules.
These rules are learned automatically from raw text.
They reported a tagging accuracy of about 88%.
Thede and Harper (1999) extended a second-order
HMM model with a C = ck,i matrix, in order to en-
code the probability of a token with a suffix sk to
be generated by a tag ti. An accuracy of about 85%
was reported.
Nakagawa (2004) combine word-level and
character-level information for Chinese and
Japanese word segmentation. At the word level, a
segmented word is attached to a POS, where the
character model is based on the observed characters
and their classification: Begin of word, In the
middle of a word, End of word, the character is a
word itself S. They apply Baum-Welch training over
a segmented corpus, where the segmentation of each
word and its character classification is observed, and
the POS tagging is ambiguous. The segmentation
(of all words in a given sentence) and the POS
tagging (of the known words) is based on a Viterbi
search over a lattice composed of all possible word
segmentations and the possible classifications of
all observed characters. Their experimental results
show that the method achieves high accuracy over
state-of-the-art methods for Chinese and Japanese
word segmentation. Hebrew also suffers from
ambiguous segmentation of agglutinated tokens into
significant words, but word formation rules seem to
be quite different from Chinese and Japanese. We
also could not rely on the existence of an annotated
corpus of segmented word forms.
Habash and Rambow (2006) used the
root+pattern+features representation of Arabic
tokens for morphological analysis and generation
of Arabic dialects, which have no lexicon. They
report high recall (95%?98%) but low precision
(37%?63%) for token types and token instances,
against gold-standard morphological analysis. We
also exploit the morphological patterns characteris-
tic of semitic morphology, but extend the guessing
of morphological features by using contextual
features. We also propose a method that relies
exclusively on learned character-level features and
contextual features, and eventually reaches the same
performance as the patterns-based approach.
Mansour et al (2007) combine a lexicon-based
tagger (such as MorphTagger (Bar-Haim et al,
2005)), and a character-based tagger (such as the
data-driven ArabicSVM (Diab et al, 2004)), which
includes character features as part of its classifica-
tion model, in order to extend the set of analyses
suggested by the analyzer. For a given sentence, the
lexicon-based tagger is applied, selecting one tag for
a token. In case the ranking of the tagged sentence is
lower than a threshold, the character-based tagger is
applied, in order to produce new possible analyses.
They report a very slight improvement on Hebrew
and Arabic supervised POS taggers.
Resolution of Hebrew unknown tokens, over a
large number of tags in the tagset (3,561) requires
a much richer model than the the heuristics used
for English (for example, the capitalization feature
which is dominant in English does not exist in He-
brew). Unlike Nakagawa, our model does not use
any segmented text, and, on the other hand, it aims
to select full morphological analysis for each token,
731
including unknowns.
3 Method
Our objective is: given an unknown word, provide
a distribution of possible tags that can serve as the
analysis of the unknown word. This unknown anal-
ysis step is performed at training and testing time.
We do not attempt to disambiguate the word ? but
only to provide a distribution of tags that will be dis-
ambiguated by the regular EM-HMM mechanism.
We examined three models to construct the distri-
bution of tags for unknown words, that is, whenever
the KC analyzer does not return any candidate anal-
ysis, we apply these models to produce possible tags
for the token p(t|w):
Letters A maximum entropy model is built for
all unknown tokens in order to estimate their tag
distribution. The model is trained on the known
tokens that appear in the corpus. For each anal-
ysis of a known token, the following features are
extracted: (1) unigram, bigram, and trigram letters
of the base-word (for each analysis, the base-word
is the token without prefixes), together with their
index relative to the start and end of the word. For
example, the n-gram features extracted for the word
abc are { a:1 b:2 c:3 a:-3 b:-2 c:-1
ab:1 bc:2 ab:-2 bc:-1 abc:1 abc:-1
} ; (2) the prefixes of the base-word (as a single
feature); (3) the length of the base-word. The class
assigned to this set of features, is the analysis of the
base-word. The model is trained on all the known
tokens of the corpus, each token is observed with its
possible POS-tags once for each of its occurrences.
When an unknown token is found, the model
is applied as follows: all the possible linguistic
prefixes are extracted from the token (one of the 76
prefix sequences that can occur in Hebrew); if more
than one such prefix is found, the token is analyzed
for each possible prefix. For each possible such
segmentation, the full feature vector is constructed,
and submitted to the Maximum Entropy model.
We hypothesize a uniform distribution among the
possible segmentations and aggregate a distribution
of possible tags for the analysis. If the proposed
tag of the base-word is never found in the corpus
preceded by the identified prefix, we remove this
possible analysis. The eventual outcome of the
model application is a set of possible full morpho-
logical analyses for the token ? in exactly the same
format as the morphological analyzer provides.
Patterns Word formation in Hebrew is based on
root+pattern and affixation. Patterns can be used to
identify the lexical category of unknowns, as well
as other inflectional properties. Nir (1993) investi-
gated word-formation in Modern Hebrew with a spe-
cial focus on neologisms; the most common word-
formation patterns he identified are summarized in
Table 3. A naive approach for unknown resolution
would add all analyses that fit any of these patterns,
for any given unknown token. As recently shown by
Habash and Rambow (2006), the precision of such
a strategy can be pretty low. To address this lack of
precision, we learn a maximum entropy model on
the basis of the following binary features: one fea-
ture for each pattern listed in column Formation of
Table 3 (40 distinct patterns) and one feature for ?no
pattern?.
Pattern-Letters This maximum entropy model is
learned by combining the features of the letters
model and the patterns model.
Linear-Context-based p(t|c) approximation
The three models above are context free. The
linear-context model exploits information about the
lexical context of the unknown words: to estimate
the probability for a tag t given a context c ? p(t|c)
? based on all the words in which a context occurs,
the algorithm works on the known words in the
corpus, by starting with an initial tag-word estimate
p(t|w) (such as the morpho-lexical approximation,
suggested by Levinger et al (1995)), and iteratively
re-estimating:
p?(t|c) =
?
w?W p(t|w)p(w|c)
Z
p?(t|w) =
?
c?C p(t|c)p(c|w)allow(t, w)
Z
where Z is a normalization factor, W is the set of
all words in the corpus, C is the set of contexts.
allow(t, w) is a binary function indicating whether t
is a valid tag for w. p(c|w) and p(w|c) are estimated
via raw corpus counts.
Loosely speaking, the probability of a tag given a
context is the average probability of a tag given any
732
Category Formation Example
Verb Template
?iCCeC ?ibh
.
en (diagnosed) oga`
miCCeC mih
.
zer (recycled) xfgn
CiCCen timren (manipulated) oxnz
CiCCet tiknet (programmed) zpkz
tiCCeC ti?arek (dated) jx`z
Participle Template
meCuCaca ms?wh
.
zar (reconstructed) xfgeyn
muCCaC muqlat
.
(recorded) hlwen
maCCiC malbin (whitening) oialn
Noun
Suffixation
ut h
.
aluciyut (pioneership) zeivelg
ay yomanay (duty officer) i`pnei
an ?egropan (boxer) otexb`
on pah
.
on (shack) oegt
iya marakiyah (soup tureen) diiwxn
it t
.
iyulit (open touring vehicle) zileih
a lomdah (courseware) dcnel
Template
maCCeC mas?neq (choke) wpyn
maCCeCa madgera (incubator) dxbcn
miCCaC mis?ap (branching) srqn
miCCaCa mignana (defensive fighting) dppbn
CeCeCa pelet
.
(output) hlt
tiCCoCet tiproset (distribution) zqextz
taCCiC tah
.
rit
.
(engraving) hixgz
taCCuCa tabru?ah (sanitation) d`exaz
miCCeCet micrepet (leotard) ztxvn
CCiC crir (dissonance) xixv
CaCCan bals?an (linguist) oyla
CaCeCet s?ah
.
emet (cirrhosis) zngy
CiCul t
.
ibu? (ringing) reaih
haCCaCa hanpas?a (animation) dytpd
heCCeC het?em (agreement) m`zd
Adjective
Suffixationb
i nora?i (awful) i`xep
ani yeh
.
idani (individual) ipcigi
oni t
.
elewizyonic (televisional) ipeifieelh
a?i yed
.
ida?i (unique) i`cigi
ali st
.
udentiali (student) il`ihpcehq
Template C1C2aC3C2aC3
d metaqtaq (sweetish) wzwzn
CaCuC rapus (flaccid ) qetx
Adverb Suffixation
ot qcarot (briefly) zexvw
it miyadit (immediately) zicin
Prefixation b bekeip (with fun) sika
aCoCeC variation: wzer ?wyeq (a copy).
bThe feminine form is made by the t and iya suffixes: ipcigi yeh
.
idanit (individual), dixvep nwcriya (Christian).
cIn the feminine form, the last h of the original noun is omitted.
dC1C2aC3C2oC3 variation: oehphw qt.ant.wn (tiny).
Table 3: Common Hebrew Neologism Formations.
733
Model Analysis Set MorphologicalDisambiguationCoverage Ambiguity Probability
Baseline 50.8% 1.5 0.48 57.3%
Pattern 82.8% 20.4 0.10 66.8%
Letter 76.7% 5.9 0.32 69.1%
Pattern-Letter 84.1% 10.4 0.25 69.8%
WordContext-Pattern 84.4% 21.7 0.12 66.5%
TagContext-Pattern 85.3% 23.5 0.19 64.9%
WordContext-Letter 80.7% 7.94 0.30 69.7%
TagContext-Letter 83.1% 7.8 0.22 66.9%
WordContext-Pattern-Letter 85.2% 12.0 0.24 68.8%
TagContext-Pattern-Letter 86.1% 14.3 0.18 62.1%
Table 4: Evaluation of unknown token full morphological analysis.
of the words appearing in that context, and similarly
the probability of a tag given a word is the averaged
probability of that tag in all the (reliable) contexts
in which the word appears. We use the function
allow(t, w) to control the tags (ambiguity class) al-
lowed for each word, as given by the lexicon.
For a given word wi in a sentence, we examine
two types of contexts: word context wi?1, wi+1,
and tag context ti?1, ti+1. For the case of word con-
text, the estimation of p(w|c) and p(c|w) is simply
the relative frequency over all the events w1, w2, w3
occurring at least 10 times in the corpus. Since the
corpus is not tagged, the relative frequency of the
tag contexts is not observed, instead, we use the
context-free approximation of each word-tag, in or-
der to determine the frequency weight of each tag
context event. For example, given the sequence
icnl ziznerl daebz tgubah l?umatit lmadai (a quite
oppositional response), and the analyses set pro-
duced by the context-free approximation: tgubah
[NN 1.0] l?umatit [] lmadai [RB 0.8, P1-NN 0.2].
The frequency weight of the context {NN RB} is
1 ? 0.8 = 0.8 and the frequency weight of the con-
text {NN P1-NN} is 1 ? 0.2 = 0.2.
4 Evaluation
For testing, we manually tagged the text which is
used in the Hebrew Treebank (consisting of about
90K tokens), according to our tagging guideline (?).
We measured the effectiveness of the three mod-
els with respect to the tags that were assigned to the
unknown tokens in our test corpus (the ?correct tag?),
according to three parameters: (1) The coverage of
the model, i.e., we count cases where p(t|w) con-
tains the correct tag with a probability larger than
0.01; (2) the ambiguity level of the model, i.e., the
average number of analyses suggested for each to-
ken; (3) the average probability of the ?correct tag?,
according to the predicted p(t|w). In addition, for
each experiment, we run the full morphology dis-
ambiguation system where unknowns are analyzed
according by the model.
Our baseline proposes the most frequent tag
(proper name) for all possible segmentations of the
token, in a uniform distribution. We compare the
following models: the 3 context free models (pat-
terns, letters and the combined patterns and letters)
and the same models combined with the word and
tag context models. Note that the context models
have low coverage (about 40% for the word context
and 80% for the tag context models), and therefore,
the context models cannot be used on their own. The
highest coverage is obtained for the combined model
(tag context, pattern, letter) at 86.1%.
We first show the results for full morphological
disambiguation, over 3,561 distinct tags in Table 4.
The highest coverage is obtained for the model com-
bining the tag context, patterns and letters models.
The tag context model is more effective because
it covers 80% of the unknown words, whereas the
word context model only covers 40%. As expected,
our simple baseline has the highest precision, since
the most frequent proper name tag covers over 50%
of the unknown words. The eventual effectiveness of
734
Model Analysis Set POS TaggingCoverage Ambiguity Probability
Baseline 52.9% 1.5 0.52 60.6%
Pattern 87.4% 8.7 0.19 76.0%
Letter 80% 4.0 0.39 77.6%
Pattern-Letter 86.7% 6.2 0.32 78.5%
WordContext-Pattern 88.7% 8.8 0.21 75.8%
TagContext-Pattern 89.5% 9.1 0.14 73.8%
WordContext-Letter 83.8% 4.5 0.37 78.2%
TagContext-Letter 87.1% 5.7 0.28 75.2%
WordContext-Pattern-Letter 87.8 6.5 0.32 77.5%
TagContext-Pattern-Letter 89.0% 7.2 0.25 74%
Table 5: Evaluation of unknown token POS tagging.
the method is measured by its impact on the eventual
disambiguation of the unknown words. For full mor-
phological disambiguation, our method achieves an
error reduction of 30% (57% to 70%). Overall, with
the level of 4.5% of unknown words observed in our
corpus, the algorithm we have developed contributes
to an error reduction of 5.5% for full morphological
disambiguation.
The best result is obtained for the model com-
bining pattern and letter features. However, the
model combining the word context and letter fea-
tures achieves almost identical results. This is an
interesting result, as the pattern features encapsulate
significant linguistic knowledge, which apparently
can be approximated by a purely distributional ap-
proximation.
While the disambiguation level of 70% is lower
than the rate of 85% achieved in English, it must
be noted that the task of full morphological disam-
biguation in Hebrew is much harder ? we manage
to select one tag out of 3,561 for unknown words as
opposed to one out of 46 in English. Table 5 shows
the result of the disambiguation when we only take
into account the POS tag of the unknown tokens.
The same models reach the best results in this case
as well (Pattern+Letters and WordContext+Letters).
The best disambiguation result is 78.5% ? still much
lower than the 85% achieved in English. The main
reason for this lower level is that the task in He-
brew includes segmentation of prefixes and suffixes
in addition to POS classification. We are currently
investigating models that will take into account the
specific nature of prefixes in Hebrew (which encode
conjunctions, definite articles and prepositions) to
better predict the segmentation of unknown words.
5 Conclusion
We have addressed the task of computing the distri-
bution p(t|w) for unknown words for full morpho-
logical disambiguation in Hebrew. The algorithm
we have proposed is language independent: it ex-
ploits a maximum entropy letters model trained over
the known words observed in the corpus and the dis-
tribution of the unknown words in known tag con-
texts, through iterative approximation. The algo-
rithm achieves 30% error reduction on disambigua-
tion of unknown words over a competitive baseline
(to a level of 70% accurate full disambiguation of
unknown words). We have also verified that tak-
ing advantage of a strong language-specific model
of morphological patterns provides the same level
of disambiguation. The algorithm we have devel-
oped exploits distributional information latent in a
wide-coverage lexicon and large quantities of unla-
beled data.
We observe that the task of analyzing unknown to-
kens for POS in Hebrew remains challenging when
compared with English (78% vs. 85%). We hy-
pothesize this is due to the highly ambiguous pattern
of prefixation that occurs widely in Hebrew and are
currently investigating syntagmatic models that ex-
ploit the specific nature of agglutinated prefixes in
Hebrew.
735
References
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev, Beer-Sheva, Israel.
Roy Bar-Haim, Khalil Sima?an, and Yoad Winter. 2005.
Choosing an optimal architecture for segmentation and
pos-tagging of modern Hebrew. In Proceedings of
ACL-05 Workshop on Computational Approaches to
Semitic Languages.
Tim Buckwalter. 2004. Buckwalter Arabic morphologi-
cal analyzer, version 2.0.
Evangelos Dermatas and George Kokkinakis. 1995. Au-
tomatic stochastic tagging of natural language texts.
Computational Linguistics, 21(2):137?163.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic tagging of Arabic text: From raw text to
base phrase chunks. In Proceeding of HLT-NAACL-
04.
Michael Elhadad, Yael Netzer, David Gabay, and Meni
Adler. 2005. Hebrew morphological tagging guide-
lines. Technical report, Ben-Gurion University, Dept.
of Computer Science.
Nizar Habash and Owen Rambow. 2006. Magead: A
morphological analyzer and generator for the arabic
dialects. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 681?688, Sydney, Australia, July. Associa-
tion for Computational Linguistics.
Moshe Levinger, Uzi Ornan, and Alon Itai. 1995. Learn-
ing morpholexical probabilities from an untagged cor-
pus with an application to Hebrew. Computational
Linguistics, 21:383?404.
Saib Mansour, Khalil Sima?an, and Yoad Winter. 2007.
Smoothing a lexicon-based pos tagger for Arabic and
Hebrew. In ACL07 Workshop on Computational Ap-
proaches to Semitic Languages, Prague, Czech Repub-
lic.
Andrei Mikheev. 1997. Automatic rule induction for
unknown-word guessing. Computational Linguistics,
23(3):405?423.
Tetsuji Nakagawa. 2004. Chinese and Japanese word
segmentation using word-level and character-level in-
formation. In Proceedings of the 20th international
conference on Computational Linguistics, Geneva.
Raphael Nir. 1993. Word-Formation in Modern Hebrew.
The Open University of Israel, Tel-Aviv, Israel.
Uzi Ornan. 2002. Hebrew in Latin script. Le?s?one?nu,
LXIV:137?151. (in Hebrew).
Scott M. Thede and Mary P. Harper. 1999. A second-
order hidden Markov model for part-of-speech tag-
ging. In Proceeding of ACL-99.
R. Weischedel, R. Schwartz, J. Palmucci, M. Meteer, and
L. Ramshaw. 1993. Coping with ambiguity and un-
known words through probabilistic models. Computa-
tional Linguistics, 19:359?382.
736
Proceedings of ACL-08: HLT, pages 746?754,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
EM Can Find Pretty Good HMM POS-Taggers
(When Given a Good Start)?
Yoav Goldberg and Meni Adler and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
{yoavg,adlerm,elhadad}@cs.bgu.ac.il
Abstract
We address the task of unsupervised POS tag-
ging. We demonstrate that good results can be
obtained using the robust EM-HMM learner
when provided with good initial conditions,
even with incomplete dictionaries. We present
a family of algorithms to compute effective
initial estimations p(t|w). We test the method
on the task of full morphological disambigua-
tion in Hebrew achieving an error reduction of
25% over a strong uniform distribution base-
line. We also test the same method on the stan-
dard WSJ unsupervised POS tagging task and
obtain results competitive with recent state-of-
the-art methods, while using simple and effi-
cient learning methods.
1 Introduction
The task of unsupervised (or semi-supervised) part-
of-speech (POS) tagging is the following: given a
dictionary mapping words in a language to their pos-
sible POS, and large quantities of unlabeled text
data, learn to predict the correct part of speech for
a given word in context. The only supervision given
to the learning process is the dictionary, which in
a realistic scenario, contains only part of the word
types observed in the corpus to be tagged.
Unsupervised POS tagging has been traditionally
approached with relative success (Merialdo, 1994;
Kupiec, 1992) by HMM-based generative mod-
els, employing EM parameters estimation using the
Baum-Welch algorithm. However, as recently noted
?This work is supported in part by the Lynn and William
Frankel Center for Computer Science.
by Banko and Moore (2004), these works made use
of filtered dictionaries: dictionaries in which only
relatively probable analyses of a given word are pre-
served. This kind of filtering requires serious su-
pervision: in theory, an expert is needed to go over
the dictionary elements and filter out unlikely anal-
yses. In practice, counts from an annotated corpus
have been traditionally used to perform the filtering.
Furthermore, these methods require rather compre-
hensive dictionaries in order to perform well.
In recent work, researchers try to address these
deficiencies by using dictionaries with unfiltered
POS-tags, and testing the methods on ?diluted dic-
tionaries? ? in which many of the lexical entries are
missing (Smith and Eisner, 2005) (SE), (Goldwater
and Griffiths, 2007) (GG), (Toutanova and Johnson,
2008) (TJ).
All the work mentioned above focuses on unsu-
pervised English POS tagging. The dictionaries are
all derived from tagged English corpora (all recent
work uses the WSJ corpus). As such, the setting of
the research is artificial: there is no reason to per-
form unsupervised learning when an annotated cor-
pus is available. The problem is rather approached
as a workbench for exploring new learning methods.
The result is a series of creative algorithms, that have
steadily improved results on the same dataset: unsu-
pervised CRF training using contrastive estimation
(SE), a fully-bayesian HMM model that jointly per-
forms clustering and sequence learning (GG), and
a Bayesian LDA-based model using only observed
context features to predict tag words (TJ). These so-
phisticated learning algorithms all outperform the
traditional baseline of EM-HMM based methods,
746
while relying on similar knowledge: the lexical con-
text of the words to be tagged and their letter struc-
ture (e.g., presence of suffixes, capitalization and
hyphenation).1
Our motivation for tackling unsupervised POS
tagging is different: we are interested in develop-
ing a Hebrew POS tagger. We have access to a good
Hebrew lexicon (and a morphological analyzer), and
a fair amount of unlabeled training data, but hardly
any annotated corpora. We actually report results
on full morphological disambiguation for Hebrew, a
task similar but more challenging than POS tagging:
we deal with a tagset much larger than English (over
3,561 distinct tags) and an ambiguity level of about
2.7 per token as opposed to 1.4 for English. Instead
of inventing a new learning framework, we go back
to the traditional EM trained HMMs. We argue that
the key challenge to learning an effective model is
to define good enough initial conditions. Given suf-
ficiently good initial conditions, EM trained models
can yield highly competitive results. Such models
have other benefits as well: they are simple, robust,
and computationally more attractive.
In this paper, we concentrate on methods for de-
riving sufficiently good initial conditions for EM-
HMM learning. Our method for learning initial con-
ditions for the p(t|w) distributions relies on a mix-
ture of language specific models: a paradigmatic
model of similar words (where similar words are
words with similar inflection patterns), simple syn-
tagmatic constraints (e.g., the sequence V-V is ex-
tremely rare in English). These are complemented
by a linear lexical context model. Such models are
simple to build and test.
We present results for unsupervised PoS tagging
of Hebrew text and for the common WSJ English
test sets. We show that our method achieves state-of-
the-art results for the English setting, even with a rel-
atively small dictionary. Furthermore, while recent
work report results on a reduced English tagset of
17 PoS tags, we also present results for the complete
45 tags tagset of the WSJ corpus. This considerably
raises the bar of the EM-HMM baseline. We also
report state-of-the-art results for Hebrew full mor-
1Another notable work, though within a slightly differ-
ent framework, is the prototype-driven method proposed by
(Haghighi and Klein, 2006), in which the dictionary is replaced
with a very small seed of prototypical examples.
phological disambiguation.
Our primary conclusion is that the problem of
learning effective stochastic classifiers remains pri-
marily a search task. Initial conditions play a domi-
nant role in solving this task and can rely on linguis-
tically motivated approximations. A robust learn-
ing method (EM-HMM) combined with good initial
conditions based on a robust feature set can go a
long way (as opposed to a more complex learning
method). It seems that computing initial conditions
is also the right place to capture complex linguistic
intuition without fear that over-generalization could
lead a learner to diverge.
2 Previous Work
The tagging accuracy of supervised stochastic tag-
gers is around 96%?97% (Manning and Schutze,
1999). Merialdo (1994) reports an accuracy
of 86.6% for an unsupervised token-based EM-
estimated HMM, trained on a corpus of about 1M
words, over a tagset of 159 tags. Elworthy (1994), in
contrast, reports accuracy of 75.49%, 80.87%, and
79.12% for unsupervised word-based HMM trained
on parts of the LOB corpora, with a tagset of 134
tags. With (artificially created) good initial condi-
tions, such as a good approximation of the tag distri-
bution for each word, Elworthy reports an improve-
ment to 94.6%, 92.27%, and 94.51% on the same
data sets. Merialdo, on the other hand, reports an im-
provement to 92.6% and 94.4% for the case where
100 and 2,000 sentences of the training corpus are
manually tagged. Later, Banko and Moore (2004)
observed that earlier unsupervised HMM-EM re-
sults were artificially high due to use of Optimized
Lexicons, in which only frequent-enough analyses
of each word were kept. Brill (1995b) proposed
an unsupervised tagger based on transformation-
based learning (Brill, 1995a), achieving accuracies
of above 95%. This unsupervised tagger relied on
an initial step in which the most probable tag for
each word is chosen. Optimized lexicons and Brill?s
most-probable-tag Oracle are not available in realis-
tic unsupervised settings, yet, they show that good
initial conditions greatly facilitate learning.
Recent work on unsupervised POS tagging for
English has significantly improved the results on this
task: GG, SE and most recently TJ report the best re-
747
sults so far on the task of unsupervised POS tagging
of the WSJ with diluted dictionaries. With dictionar-
ies as small as 1249 lexical entries the LDA-based
method with a strong ambiguity-class model reaches
POS accuracy as high as 89.7% on a reduced tagset
of 17 tags.
While these 3 methods rely on the same feature
set (lexical context, spelling features) for the learn-
ing stage, the LDA approach bases its predictions
entirely on observable features, and excludes the tra-
ditional hidden states sequence.
In Hebrew, Levinger et al (1995) introduced the
similar-words algorithm for estimating p(t|w) from
unlabeled data, which we describe below. Our
method uses this algorithm as a first step, and refines
the approximation by introducing additional linguis-
tic constraints and an iterative refinement step.
3 Initial Conditions For EM-HMM
The most common model for unsupervised learning
of stochastic processes is Hidden Markov Models
(HMM). For the case of tagging, the states corre-
spond to the tags ti, and words wi are emitted each
time a state is visited. The parameters of the model
can be estimated by applying the Baum-Welch EM
algorithm (Baum, 1972), on a large-scale corpus of
unlabeled text. The estimated parameters are then
used in conjunction with Viterbi search, to find the
most probable sequence of tags for a given sentence.
In this work, we follow Adler (2007) and use a vari-
ation of second-order HMM in which the probability
of a tag is conditioned by the tag that precedes it and
by the one that follows it, and the probability of an
emitted word is conditioned by its tag and the tag
that follows it2. In all experiments, we use the back-
off smoothing method of (Thede and Harper, 1999),
with additive smoothing (Chen, 1996) for the lexical
probabilities.
We investigate methods to approximate the initial
parameters of the p(t|w) distribution, from which
we obtain p(w|t) by marginalization and Bayesian
inversion. We also experiment with constraining the
p(t|t?1, t+1) distribution.
2Technically this is not Markov Model but a Dependency
Net. However, bidirectional conditioning seem more suitable
for language tasks, and in practice the learning and inference
methods are mostly unaffected. See (Toutanova et al, 2003).
General syntagmatic constraints We set linguis-
tically motivated constraints on the p(t|t?1, t+1)
distribution. In our setting, these are used to force
the probability of some events to 0 (e.g., ?Hebrew
verbs can not be followed by the of preposition?).
Morphology-based p(t|w) approximation
Levinger et al (1995) developed a context-free
method for acquiring morpho-lexical probabilities
(p(t|w)) from an untagged corpus. The method is
based on language-specific rules for constructing a
similar words (SW) set for each analysis of a word.
This set is composed of morphological variations
of the word under the given analysis. For example,
the Hebrew token ??? can be analyzed as either a
noun (boy) or a verb (gave birth). The noun SW set
for this token is composed of the definiteness and
number inflections ????,?????,?????? (the boy, boys,
the boys), while the verb SW set is composed
of gender and tense inflections ????,???? (she/they
gave birth). The approximated probability of each
analysis is based on the corpus frequency of its SW
set. For the complete details, refer to the original
paper. Cucerzan and Yarowsky (2000) proposed
a similar method for the unsupervised estimation
of p(t|w) in English, relying on simple spelling
features to characterize similar word classes.
Linear-Context-based p(t|w) approximation
The method of Levinger et al makes use of Hebrew
inflection patterns in order to estimate context free
approximation of p(t|w) by relating a word to its
different inflections. However, the context in which
a word occurs can also be very informative with
respect to its POS-analysis (Schu?tze, 1995). We
propose a novel algorithm for estimating p(t|w)
based on the contexts in which a word occurs.3
The algorithm starts with an initial p(t|w) esti-
mate, and iteratively re-estimates:
p?(t|c) =
?
w?W p(t|w)p(w|c)
Z
p?(t|w) =
?
c?RELC p(t|c)p(c|w)allow(t, w)
Z
3While we rely on the same intuition, our use of context
differs from earlier works on distributional POS-tagging like
(Schu?tze, 1995), in which the purpose is to directly assign the
possible POS for an unknown word. In contrast, our algorithm
aims to improve the estimate for the whole distribution p(t|w),
to be further disambiguated by the EM-HMM learner.
748
where Z is a normalization factor, W is the set of
all words in the corpus, C is the set of all contexts,
andRELC ? C is a set of reliable contexts, defined
below. allow(t, w) is a binary function indicating
whether t is a valid tag for w. p(c|w) and p(w|c) are
estimated via raw corpus counts.
Intuitively, we estimate the probability of a tag
given a context as the average probability of a tag
given any of the words appearing in that context, and
similarly the probability of a tag given a word is the
averaged probability of that tag in all the (reliable)
contexts in which the word appears. At each round,
we define RELC , the set of reliable contexts, to be
the set of all contexts in which p(t|c) > 0 for at most
X different ts.
The method is general, and can be applied to dif-
ferent languages. The parameters to specify for each
language are: the initial estimation p(t|w), the esti-
mation of the allow relation for known and OOV
words, and the types of contexts to consider.
4 Application to Hebrew
In Hebrew, several words combine into a single to-
ken in both agglutinative and fusional ways. This
results in a potentially high number of tags for each
token. On average, in our corpus, the number of pos-
sible analyses per known word reached 2.7, with the
ambiguity level of the extended POS tagset in cor-
pus for English (1.41) (Dermatas and Kokkinakis,
1995).
In this work, we use the morphological analyzer
of MILA ? Knowledge Center for Processing He-
brew (KC analyzer). In contrast to English tagsets,
the number of tags for Hebrew, based on all com-
binations of the morphological attributes, can grow
theoretically to about 300,000 tags. In practice, we
found ?only? about 3,560 tags in a corpus of 40M
tokens training corpus taken from Hebrew news ma-
terial and Knesset transcripts. For testing, we man-
ually tagged the text which is used in the Hebrew
Treebank (Sima?an et al, 2001) (about 90K tokens),
according to our tagging guidelines.
4.1 Initial Conditions
General syntagmatic constraints We define 4
syntagmatic constraints over p(t|t?1, t+1): (1) a
construct state form cannot be followed by a verb,
preposition, punctuation, existential, modal, or cop-
ula; (2) a verb cannot be followed by the preposition
?? s?el (of), (3) copula and existential cannot be fol-
lowed by a verb, and (4) a verb cannot be followed
by another verb, unless one of them has a prefix, or
the second verb is an infinitive, or the first verb is
imperative and the second verb is in future tense.4
Morphology-Based p(t|w) approximation We
extended the set of rules used in Levinger et al , in
order to support the wider tagset used by the KC an-
alyzer: (1) The SW set for adjectives, copulas, exis-
tentials, personal pronouns, verbs and participles, is
composed of all gender-number inflections; (2) The
SW set for common nouns is composed of all num-
ber inflections, with definite article variation for ab-
solute noun; (3) Prefix variations for proper nouns;
(4) Gender variation for numerals; and (5) Gender-
number variation for all suffixes (possessive, nomi-
native and accusative).
Linear-Context-based p(t|w) approximation
For the initial p(t|w) we use either a uniform distri-
bution based on the tags allowed in the dictionary,
or the estimate obtained by using the modified
Levinger et al algorithm. We use contexts of the
form LR=w?1, w+1 (the neighbouring words). We
estimate p(w|c) and p(c|w) via relative frequency
over all the events w1, w2, w3 occurring at least
10 times in the corpus. allow(t, w) follows the
dictionary. Because of the wide coverage of the
Hebrew lexicon, we take RELC to be C (all
available contexts).
4.2 Evaluation
We run a series of experiments with 8 distinct ini-
tial conditions, as shown in Table 1: our baseline
(Uniform) is the uniform distribution over all tags
provided by the KC analyzer for each word. The
Syntagmatic initial conditions add the p(t|t?1, t+1)
constraints described above to the uniform base-
line. The Morphology-Based and Linear-Context
initial conditions are computed as described above,
while the Morph+Linear is the result of applying
the linear-context algorithm over initial values com-
puted by the Morphology-based method. We repeat
4This rule was taken from Shacham and Wintner(2007).
749
Initial Condition Dist Context-Free EM-HMMFull Seg+Pos Full Seg+Pos
Uniform 60 63.8 71.9 85.5 89.8
Syntagmatic Pair Constraints 60 / / 85.8 89.8Init-Trans 60 / / 87.9 91
Morpho-Lexical
Morph-Based 76.8 76.4 83.1 87.7 91.6
Linear-Context 70.1 75.4 82.6 85.3 89.6
Morph+Linear 79.8 79.0 85.5 88 92
PairConst+Morph
Morph-Based / / / 87.6 91.4
Linear-Context / / / 84.5 89.0
Morph+Linear / / / 87.1 91.5
InitTrans+Morph
Morph-Based / / / 89.2 92.3
Linear-Context / / / 87.7 90.9
Morph+Linear / / / 89.4 92.4
Table 1: Accuracy (%) of Hebrew Morphological
Disambiguation and POS Tagging over various initial
conditions
these last 3 models with the addition of the syntag-
matic constraints (Synt+Morph).
For each of these, we first compare the computed
p(t|w) against a gold standard distribution, taken
from the test corpus (90K tokens), according to the
measure used by (Levinger et al, 1995) (Dist). On
this measure, we confirm that our improved morpho-
lexical approximation improves the results reported
by Levinger et al from 74% to about 80% on a
richer tagset, and on a much larger test set (90K vs.
3,400 tokens).
We then report on the effectiveness of p(t|w) as
a context-free tagger that assigns to each word the
most likely tag, both for full morphological analy-
sis (3,561 tags) (Full) and for the simpler task of
token segmentation and POS tag selection (36 tags)
(Seg+Pos). The best results on this task are 80.8%
and 87.5% resp. achieved on the Morph+Linear ini-
tial conditions.
Finally, we test effectiveness of the initial con-
ditions with EM-HMM learning. We reach 88%
accuracy on full morphological and 92% accuracy
for POS tagging and word segmentation, for the
Morph+Linear initial conditions.
As expected, EM-HMM improves results (from
80% to 88%). Strikingly, EM-HMM improves the
uniform initial conditions from 64% to above 85%.
However, better initial conditions bring us much
over this particular local maximum ? with an error
reduction of 20%. In all cases, the main improve-
ment over the uniform baseline is brought by the
morphology-based initial conditions. When applied
on its own, the linear context brings modest im-
provement. But the combination of the paradigmatic
morphology-based method with the linear context
improves all measures.
A most interesting observation is the detrimental
contribution of the syntagmatic constraints we in-
troduced. We found that 113,453 sentences of the
corpus (about 5%) contradict these basic and ap-
parently simple constraints. As an alternative to
these common-sense constraints, we tried to use a
small seed of randomly selected sentences (10K an-
notated tokens) in order to skew the initial uniform
distribution of the state transitions. We initialize the
p(t|t?1, t+1) distribution with smoothed ML esti-
mates based on tag trigram and bigram counts (ig-
noring the tag-word annotations). This small seed
initialization (InitTrans) has a great impact on ac-
curacy. Overall, we reach 89.4% accuracy on full
morphological and 92.4% accuracy for POS tagging
and word segmentation, for the Morph+Linear con-
ditions ? an error reduction of more than 25% from
the uniform distribution baseline.
5 Application to English
We now apply the same technique to English semi-
supervised POS tagging. Recent investigations of
this task use dictionaries derived from the Penn WSJ
corpus, with a reduced tag set of 17 tags5 instead of
the original 45-tags tagset. They experiment with
full dictionaries (containing complete POS informa-
tion for all the words in the text) as well as ?diluted?
dictionaries, from which large portions of the vo-
cabulary are missing. These settings are very dif-
ferent from those used for Hebrew: the tagset is
much smaller (17 vs. ?3,560) and the dictionaries
are either complete or extremely crippled. However,
for the sake of comparison, we have reproduced the
same experimental settings.
We derive dictionaries from the complete WSJ
corpus6, and the exact same diluted dictionaries used
in SE, TJ and GG.
5ADJ ADV CONJ DET ENDPUNC INPUNC LPUNC
RPUNC N POS PRT PREP PRT TO V VBG VBN WH
6The dictionary derived from the WSJ data is very noisy:
many of the stop words get wrong analyses stemming from tag-
ging mistakes (for instance, the word the has 6 possible analyses
in the data-derived dictionary, which we checked manually and
found all but DT erroneous). Such noise is not expected in a real
world dictionary, and our algorithm is not designed to accomo-
date it. We corrected the entries for the 20 most frequent words
in the corpus. This step could probably be done automatically,
but we consider it to be a non-issue in any realistic setting.
750
Syntagmatic Constraints We indirectly incor-
porated syntagmatic constraints through a small
change to the tagset. The 17-tags English tagset
allows for V-V transitions. Such a construction is
generally unlikely in English. By separating modals
from the rest of the verbs, and creating an addi-
tional class for the 5 be verbs (am,is,are,was,were),
we made such transition much less probable. The
new 19-tags tagset reflects the ?verb can not follow
a verb? constraint.
Morphology-Based p(t|w) approximation En-
glish morphology is much simpler compared to that
of Hebrew, making direct use of the Levinger con-
text free approximation impossible. However, some
morphological cues exist in English as well, in par-
ticular common suffixation patterns. We imple-
mented our morphology-based context-free p(t|w)
approximation for English as a special case of the
linear context-based algorithm described in Sect.3.
Instead of generating contexts based on neighboring
words, we generate them using the following 5 mor-
phological templates:
suff=S The word has suffix S (suff=ing).
L+suff=W,S The word appears just after word W ,
with suffix S (L+suff=have,ed).
R+suff=S,W The word appears just before wordW ,
with suffix S (R+suff=ing,to)
wsuf=S1,S2 The word suffix is S1, the same stem is
seen with suffix S2 (wsuf=,s).
suffs=SG The word stem appears with the SG group
of suffixes (suffs=ed,ing,s).
We consider a word to have a suffix only if the
word stem appears with a different suffix somewhere
in the text. We implemented a primitive stemmer
for extracting the suffixes while preserving a us-
able stem by taking care of few English orthogra-
phy rules (handling, e.g., , bigger ? big er, nicer
? nice er, happily ? happy ly, picnicking ? pic-
nic ing). For the immediate context W in the tem-
plates L+suff,R+suff, we consider only the 20 most
frequent tokens in the corpus.
Linear-Context-based p(t|w) approximation
We expect the context based approximation to be
particularly useful in English. We use the following
3 context templates: LL=w?2,w?1, LR=w?1,w+1
and RR=w+1,w+2. We estimate p(w|c) and p(c|w)
by relative frequency over word triplets occurring at
least twice in the unannotated training corpus.
Combined p(t|w) approximation This approx-
imation combines the morphological and linear
context approximations by using all the above-
mentioned context templates together in the iterative
process.
For all three p(t|w) approximations, we take
RELC to be contexts containing at most 4 tags.
allow(t, w) follows the dictionary for known words,
and is the set of all open-class POS for unknown
words. We take the initial p(t|w) for each w to be
uniform over all the dictionary specified tags for w.
Accordingly, the initial p(t|w) = 0 for w not in the
dictionary. We run the process for 8 iterations.7
Diluted Dictionaries and Unknown Words
Some of the missing dictionary elements are as-
signed a set of possible POS-tags and corresponding
probabilities in the p(t|w) estimation process. Other
unknown tokens remain with no analysis at the
end of the initial process computation. For these
missing elements, we assign an ambiguity class by
a simple ambiguity-class guesser, and set p(t|w)
to be uniform over all the tags in the ambiguity
class. Our ambiguity-class guesser assigns for each
word the set of all open-class tags that appeared
with the word suffix in the dictionary. The word
suffix is the longest (up to 3 characters) suffix of the
word that also appears in the top-100 suffixes in the
dictionary.
Taggers We test the resulting p(t|w) approxima-
tion by training 2 taggers: CF-Tag, a context-free
tagger assigning for each word its most probable
POS according to p(t|w), with a fallback to the most
probable tag in case the word does not appear in
the dictionary or if ?t, p(t|w) = 0. EM-HMM,
a second-order EM-HMM initialized with the esti-
mated p(t|w).
Baselines As baseline, we use two EM-trained
HMM taggers, initialized with a uniform p(t|w) for
every word, based on the allowed tags in the dic-
tionary. For words not in the dictionary, we take
the allowed tags to be either all the open-class POS
7This is the first value we tried, and it seems to work fine.
We haven?t experimented with other values. The same applies
for the choice of 4 as the RELC threshold.
751
(uniform(oc)) or the allowed tags according to our
simple ambiguity-class guesser (uniform(suf)).
All the p(t|w) estimates and HMM models are
trained on the entire WSJ corpus. We use the same
24K word test-set as used in SE, TJ and GG, as well
as the same diluted dictionaries. We report the re-
sults on the same reduced tagsets for comparison,
but also include the results on the full 46 tags tagset.
5.1 Results
Table 2 summarizes the results of our experiments.
Uniform initialization based on the simple suffix-
based ambiguity class guesser yields big improve-
ments over the uniform all-open-class initialization.
However, our refined initial conditions always im-
prove the results (by as much as 40% error re-
duction). As expected, the linear context is much
more effective than the morphological one, espe-
cially with richer dictionaries. This seem to indi-
cate that in English the linear context is better at re-
fining the estimations when the ambiguity classes
are known, while the morphological context is in
charge of adding possible tags when the ambigu-
ity classes are not known. Furthermore, the bene-
fit of the morphology-context is bigger for the com-
plete tagset setting, indicating that, while the coarse-
grained POS-tags are indicated by word distribu-
tion, the finer distinctions are indicated by inflec-
tions and orthography. The combination of linear
and morphology contexts is always beneficial. Syn-
tagmatic constraints (e.g., separating be verbs and
modals from the rest of the verbs) constantly im-
prove results by about 1%. Note that the context-free
tagger based on our p(t|w) estimates is quite accu-
rate. As with the EM trained models, combining lin-
ear and morphological contexts is always beneficial.
To put these numbers in context, Table 3 lists
current state-of-the art results for the same task.
CE+spl is the Contrastive-Estimation CRF method
of SE. BHMM is the completely Bayesian-HMM
of GG. PLSA+AC, LDA, LDA+AC are the mod-
els presented in TJ, LDA+AC is a Bayesian model
with a strong ambiguity class (AC) component, and
is the current state-of-the-art of this task. The other
models are variations excluding the Bayesian com-
ponents (PLSA+AC) or the ambiguity class.
While our models are trained on the unannotated
text of the entire WSJ Treebank, CE and BHMM use
much less training data (only the 24k words of the
test-set). However, as noted by TJ, there is no reason
one should limit the amount of unlabeled data used,
and in addition other results reported in GG,SE show
that accuracy does not seem to improve as more un-
labeled data are used with the models. We also re-
port results for training our EM-HMM tagger on the
smaller dataset (the p(t|w) estimation is still based
on the entire unlabeled WSJ).
All the abovementioned models follow the as-
sumption that all 17 tags are valid for the unknown
words. In contrast, we restrict the set of allowed
tags for an unknown word to open-class tags. Closed
class words are expected to be included in a dictio-
nary, even a small one. The practice of allowing only
open-class tags for unknown words goes back a long
way (Weischedel et al, 1993), and proved highly
beneficial also in our case.
Notice that even our simplest models, in which
the initial p(t|w) distribution for each w is uniform,
already outperform most of the other models, and,
in the case of the diluted dictionaries, by a wide
margin. Similarly, given the p(t|w) estimate, EM-
HMM training on the smaller dataset (24k) is still
very competitive (yet results improve with more un-
labeled data). When we use our refined p(t|w) dis-
tribution as the basis of EM-HMM training, we get
the best results for the complete dictionary case.
With the diluted dictionaries, we are outperformed
only by LDA+AC. As we outperform this model in
the complete dictionary case, it seems that the ad-
vantage of this model is due to its much stronger
ambiguity class model, and not its Bayesian com-
ponents. Also note that while we outperform this
model when using the 19-tags tagset, it is slightly
better in the original 17-tags setting. It could be that
the reliance of the LDA models on observed surface
features instead of hidden state features is beneficial
avoiding the misleading V-V transitions.
We also list the performance of our best mod-
els with a slightly more realistic dictionary setting:
we take our dictionary to include information for all
words occurring in section 0-18 of the WSJ corpus
(43208 words). We then train on the entire unanno-
tated corpus, and test on sections 22-24 ? the stan-
dard train/test split for supervised English POS tag-
ging. We achieve accuracy of 92.85% for the 19-
tags set, and 91.3% for the complete 46-tags tagset.
752
Initial Conditions Full dict ? 2 dict ? 3 dict
(49206 words) (2141 words) (1249 words)
CF-Tag EM-HMM CF-Tag EM-HMM CF-Tag EM-HMM
Uniform(oc) 81.7 88.7 68.4 81.9 62.5 79.6
Uniform(suf) NA NA 76.8 83.4 76.9 81.6
17tags Morph-Cont 82.2 88.6 73.3 83.9 69.1 81.7
Linear-Cont 90.1 92.9 81.1 87.8 78.3 85.8
Combined-Cont 89.9 93.3 83.1 88.5 81.1 86.4
Uniform(oc) 79.9 91.0 66.6 83.4 60.7 84.7
Uniform(suf) NA NA 75.1 86.5 73.1 86.7
19tags Morph-Cont 80.5 89.2 71.5 86.5 67.5 87.1
Linear-Cont 88.4 93.7 78.9 89.0 76.3 86.9
Combined-Cont 88.0 93.8 81.1 89.4 79.2 87.4
Uniform(oc) 76.7 88.3 61.2 * 55.7 *
Uniform(suf) NA NA 64.2 81.9 60.3 79.8
46tags Morph-Cont 74.8 88.8 65.6 83.0 61.9 80.3
Linear-Cont 85.5 91.2 74.5 84.0 70.1 82.2
Combined-Cont 85.9 91.4 75.4 85.5 72.4 83.3
Table 2: Accuracy (%) of English POS Tagging over various initial conditions
Dict InitEM-HMM (24k) LDA LDA+AC PLSA+AC CE+spl BHMM
Full 93.8 (91.1) 93.4 93.4 89.7 88.7 87.3
? 2 89.4 (87.9) 87.4 91.2 87.8 79.5 79.6
? 3 87.4 (85.9) 85 89.7 85.9 78.4 71
Table 3: Comparison of English Unsupervised POS Tagging Methods
6 Conclusion
We have demonstrated that unsupervised POS tag-
ging can reach good results using the robust EM-
HMM learner when provided with good initial con-
ditions, even with incomplete dictionaries. We pre-
sented a general family of algorithms to compute ef-
fective initial conditions: estimation of p(t|w) rely-
ing on an iterative process shifting probabilities be-
tween words and their contexts. The parameters of
this process (definition of the contexts and initial es-
timations of p(t|w) can safely encapsulate rich lin-
guistic intuitions.
While recent work, such as GG, aim to use the
Bayesian framework and incorporate ?linguistically
motivated priors?, in practice such priors currently
only account for the fact that language related dis-
tributions are sparse - a very general kind of knowl-
edge. In contrast, our method allow the incorpora-
tion of much more fine-grained intuitions.
We tested the method on the challenging task
of full morphological disambiguation in Hebrew
(which was our original motivation) and on the stan-
dard WSJ unsupervised POS tagging task.
In Hebrew, our model includes an improved ver-
sion of the similar words algorithm of (Levinger et
al., 1995), a model of lexical context, and a small
set of tag ngrams. The combination of these knowl-
edge sources in the initial conditions brings an error
reduction of more than 25% over a strong uniform
distribution baseline. In English, our model is com-
petitive with recent state-of-the-art results, while us-
ing simple and efficient learning methods.
The comparison with other algorithms indicates
directions of potential improvement: (1) our initial-
conditions method might benefit the other, more so-
phisticated learning algorithms as well. (2) Our
models were designed under the assumption of a
relatively complete dictionary. As such, they are
not very good at assigning ambiguity-classes to
OOV tokens when starting with a very small dic-
tionary. While we demonstrate competitive results
using a simple suffix-based ambiguity-class guesser
which ignores capitalization and hyphenation infor-
mation, we believe there is much room for improve-
ment in this respect. In particular, (Haghighi and
Klein, 2006) presents very strong results using a
distributional-similarity module and achieve impres-
sive tagging accuracy while starting with a mere
116 prototypical words. Experimenting with com-
bining similar models (as well as TJ?s ambiguity
class model) with our p(t|w) distribution estimation
method is an interesting research direction.
753
References
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev, Beer-Sheva, Israel.
Michele Banko and Robert C. Moore. 2004. Part-of-
speech tagging in context. In Proceedings of Coling
2004, pages 556?561, Geneva, Switzerland, Aug 23?
Aug 27. COLING.
Leonard E. Baum. 1972. An inequality and associ-
ated maximization technique in statistical estimation
for probabilistic functions of a Markov process. In-
equalities, 3:1?8.
Eric Brill. 1995a. Transformation-based error-driven
learning and natural languge processing: A case study
in part-of-speech tagging. Computational Linguistics,
21:543?565.
Eric Brill. 1995b. Unsupervised learning of disam-
biguation rules for part of speech tagging. In David
Yarovsky and Kenneth Church, editors, Proceedings
of the Third Workshop on Very Large Corpora, pages
1?13, Somerset, New Jersey. Association for Compu-
tational Linguistics.
Stanley F. Chen. 1996. Building Probabilistic Models for
Natural Language. Ph.D. thesis, Harvard University,
Cambridge, MA.
Silviu Cucerzan and David Yarowsky. 2000. Language
independent, minimally supervised induction of lex-
ical probabilities. In ACL ?00: Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 270?277, Morristown, NJ,
USA. Association for Computational Linguistics.
Evangelos Dermatas and George Kokkinakis. 1995. Au-
tomatic stochastic tagging of natural language texts.
Computational Linguistics, 21(2):137?163.
David Elworthy. 1994. Does Baum-Welch re-estimation
help taggers? In Proceeding of ANLP-94.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully bayesian approach to unsupervised part-of-
speech tagging. In Proceeding of ACL 2007, Prague,
Czech Republic.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the main conference on Human Language Technol-
ogy Conference of the North American Chapter of the
Association of Computational Linguistics, pages 320?
327, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
J. Kupiec. 1992. Robust part-of-speech tagging using
hidden Markov model. Computer Speech and Lan-
guage, 6:225?242.
Moshe Levinger, Uzi Ornan, and Alon Itai. 1995. Learn-
ing morpholexical probabilities from an untagged cor-
pus with an application to Hebrew. Computational
Linguistics, 21:383?404.
Christopher D. Manning and Hinrich Schutze. 1999.
Foundation of Statistical Language Processing. MIT
Press.
Bernard Merialdo. 1994. Tagging English text
with probabilistic model. Computational Linguistics,
20:155?171.
Hinrich Schu?tze. 1995. Distributional part-of-speech
tagging. In Proceedings of the seventh conference
on European chapter of the Association for Computa-
tional Linguistics, pages 141?148, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Danny Shacham and Shuly Wintner. 2007. Morpho-
logical disambiguation of hebrew: A case study in
classifier combination. In Proceeding of EMNLP-07,
Prague, Czech.
Khalil Sima?an, Alon Itai, Alon Altman Yoad Winter,
and Noa Nativ. 2001. Building a tree-bank of mod-
ern Hebrew text. Journal Traitement Automatique des
Langues (t.a.l.). Special Issue on NLP and Corpus
Linguistics.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
354?362, Ann Arbor, Michigan, June.
Scott M. Thede and Mary P. Harper. 1999. A second-
order hidden Markov model for part-of-speech tag-
ging. In Proceeding of ACL-99.
Kristina Toutanova and Mark Johnson. 2008. A bayesian
lda-based model for semi-supervised part-of-speech
tagging. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Information
Processing Systems 20. MIT Press, Cambridge, MA.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In HLT-
NAACL.
R. Weischedel, R. Schwartz, J. Palmucci, M. Meteer, and
L. Ramshaw. 1993. Coping with ambiguity and un-
known words through probabilistic models. Computa-
tional Linguistics, 19:359?382.
754
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 237?240,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
splitSVM: Fast, Space-Efficient, non-Heuristic, Polynomial Kernel
Computation for NLP Applications
Yoav Goldberg and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
{yoavg,elhadad}@cs.bgu.ac.il
Abstract
We present a fast, space efficient and non-
heuristic method for calculating the decision
function of polynomial kernel classifiers for
NLP applications. We apply the method to
the MaltParser system, resulting in a Java
parser that parses over 50 sentences per sec-
ond on modest hardware without loss of accu-
racy (a 30 time speedup over existing meth-
ods). The method implementation is available
as the open-source splitSVM Java library.
1 Introduction
Over the last decade, many natural language pro-
cessing tasks are being cast as classification prob-
lems. These are then solved by of-the-shelf
machine-learning algorithms, resulting in state-of-
the-art results. Support Vector Machines (SVMs)
have gained popularity as they constantly outper-
form other learning algorithms for many NLP tasks.
Unfortunately, once a model is trained, the de-
cision function for kernel-based classifiers such as
SVM is expensive to compute, and can grow lin-
early with the size of the training data. In contrast,
the computational complexity for the decisions func-
tions of most non-kernel based classifiers does not
depend on the size of the training data, making them
orders of magnitude faster to compute. For this rea-
son, research effort was directed at speeding up the
classification process of polynomial-kernel SVMs
(Isozaki and Kazawa, 2002; Kudo and Matsumoto,
2003; Wu et al, 2007). Existing accelerated SVM
solutions, however, either require large amounts of
memory, or resort to heuristics ? computing only an
approximation to the real decision function.
This work aims at speeding up the decision func-
tion computation for low-degree polynomial ker-
nel classifiers while using only a modest amount of
memory and still computing the exact function. This
is achieved by taking into account the Zipfian nature
of natural language data, and structuring the compu-
tation accordingly. On a sample application (replac-
ing the libsvm classifier used by MaltParser (Nivre
et al, 2006) with our own), we observe a speedup
factor of 30 in parsing time.
2 Background and Previous Work
In classification based NLP algorithms, a word and
its context is considered a learning sample, and en-
coded as Feature Vectors. Usually, context data in-
cludes the word being classified (w0), its part-of-
speech (PoS) tag (p0), word forms and PoS tags of
neighbouring words (w?2, . . . , w+2, p?2, . . . , p+2,
etc.). Computed features such as the length of a
word or its suffix may also be added. A feature vec-
tor (F ) is encoded as an indexed list of all the fea-
tures present in the training corpus. A feature fi of
the form w+1 = dog means that the word follow-
ing the one being classified is ?dog?. Every learning
sample is represented by an n = |F | dimensional
binary vector x. xi = 1 iff the feature fi is active
in the given sample, 0 otherwise. n is the number
of different features being considered. This encod-
ing leads to vectors with extremely high dimensions,
mainly because of lexical features wi.
SVM is a supervised binary classifier. The re-
sult of the learning process is the set SV of Sup-
237
port Vectors, associated weights ?i, and a constant
b. The Support Vectors are a subset of the training
feature vectors, and together with the weights and b
they define a hyperplane that optimally separates the
training samples. The basic SVM formulation is of a
linear classifier, but by introducing a kernel function
K that non-linearly transforms the data fromRn into
a space of higher dimension, SVM can be used to
perform non-linear classification. SVM?s decision
function is:
y(x) = sgn
(?
j?SV yj?jK(xj , x) + b
)
where x is an n dimensional feature vector to
be classified. The kernel function we consider
in this paper is a polynomial kernel of degree d:
K(xi, xj) = (?xi ? xj + c)d. When using binary
valued features (with ? = 1 and c = 1), this kernel
function essentially implies that the classifier con-
siders not only the explicitly specified features, but
also all available sets of size d of features. For
d = 2, this means considering all feature pairs,
while for d = 3 all feature triplets. In practice, a
polynomial kernel with d = 2 usually yields the
best results in NLP tasks, while higher degree ker-
nels tend to overfit the data.
2.1 Decision Function Computation
Note that the decision function involves a summa-
tion over all support vectors xj in SV . In natu-
ral language applications, the size |SV | tends to be
very large (Isozaki and Kazawa, 2002), often above
10,000. In particular, the size of the support vectors
set can grow linearly with the number of training ex-
amples, of which there are usually at least tens of
thousands. As a consequence, the computation of
the decision function is computationally expensive.
Several approaches have been designed to speed up
the decision function computation.
Classifier Splitting is a common, application
specific heuristic, which is used to speed up the
training as well as the testing stages (Nivre et al,
2006). The training data is split into several datasets
according to an application specific heuristic. A sep-
arate classifier is then trained for each dataset. For
example, it might be known in advance that nouns
usually behave differently than verbs. In such a
case, one can train one classifier on noun instances,
and a different classifier on verb instances. When
testing, only one of the classifiers will be applied,
depending on the PoS of the word. This technique
reduces the number of support vectors in each clas-
sifier (because each classifier was trained on only a
portion of the data). However, it relies on human in-
tuition on the way the data should be split, and usu-
ally results in a degradation in performance relative
to a single classifier trained on all the data points.
PKI ? Inverted Indexing (Kudo and Matsumoto,
2003), stores for each feature the support vectors in
which it appears. When classifying a new sample,
only the set of vectors relevant to features actually
appearing in the sample are considered. This ap-
proach is non-heuristic and intuitively appealing, but
in practice brings only modest improvements.
Kernel Expansion (Isozaki and Kazawa, 2002)
is used to transform the d-degree polynomial kernel
based classifier into a linear one, with a modified
decision function y(x) = sgn(w ? xd + b). w is a
very high dimensional weight vector, which is cal-
culated beforehand from the set of support vectors
and their corresponding ?i values. (the calculation
details appear in (Isozaki and Kazawa, 2002; Kudo
and Matsumoto, 2003)). This speeds up the decision
computation time considerably, as only |x|d weights
need to be considered, |x| being the number of ac-
tive features in the sample to be classified, which
is usually a very small number. However, even the
sparse-representation version of w tends to be very
large: (Isozaki and Kazawa, 2002) report that some
of their second degree expanded NER models were
more than 80 times slower to load than the original
models (and 224 times faster to classify).1 This ap-
proach obviously does not scale well, both to tasks
with more features and to larger degree kernels.
PKE ? Heuristic Kernel Expansion, was intro-
duced by (Kudo and Matsumoto, 2003). This heuris-
tic method addresses the deficiency of the Kernel
Expansion method by using a basket-mining algo-
rithm in order to greatly reduce the number of non-
zero elements in the calculated w. A parameter is
used to control the number of non-zero elements in
w. The smaller the number, the smaller the memory
requirement, but setting this number too low hurts
classification performance, as only an approxima-
1Using a combination of 33 classifiers, the overall loading
time is about 31 times slower, and classification time is about
21 times faster, than the non-expanded classifiers.
238
tion of the real decision function is calculated.
?Semi Polynomial Kernel? was introduced by
(Wu et al, 2007). The intuition behind this opti-
mization is to ?extend the linear kernel SVM toward
polynomial?. It does not train a polynomial kernel
classifier, but a regular linear SVM. A basket-mining
based feature selection algorithm is used to select
?useful? pairs and triplets of features prior to the
training stage, and a linear classifier is then trained
using these features. Training (and testing) are faster
then in the polynomial kernel case, but the result suf-
fer quite a big loss in accuracy as well.2.
3 Fast, Non-Heuristic Computation
We now turn to present our fast, space efficient and
non-heuristic approach for computing the Polyno-
mial Kernel decision function.3 Our approach is a
combination of the PKI and the Kernel Expansion
methods. While previous works considered kernels
of the form K(x, y) = (x ? y + 1)d, we consider
the more general form of the polynomial kernel:
K(x, y) = (?x ? y + c)d.
Our key observation is that in NLP classifica-
tion tasks, few of the features (e.g., PoS is X,
or prev word is the) are very frequent, while
most others are extremely rare (e.g., next word
is polynomial). The common features are ac-
tive in many of the support-vectors, while the rare
features are active only in few support vectors. This
is true for most language related tasks: the Zipfian
nature of language phenomena is reflected in the dis-
tribution of features in the support vectors.
It is because of common features that the PKI re-
verse indexing method does not yield great improve-
ments: if at least one of the features of the current
instance is active in a support vector, this vector is
taken into account in the sum calculation, and the
common features are active in many support vectors.
On the other hand, the long tail of rare features
is the reason the Kernel Expansion methods requires
2This loss of accuracy in comparison to the PKE approach
is to be expected, as (Goldberg and Elhadad, 2007) showed that
the effect of removing features prior to the learning stage is
much more severe than removing them after the learning stage.
3Our presentation is for the case where d = 2, as this is by
far the most useful kernel. However, the method can be easily
adapted to higher degree kernels as well. For completeness, our
toolkit provides code for d = 3 as well as 2.
so much space: every rare feature adds many possi-
ble feature pairs.
We propose a combined method. We first split
common from rare features. We then use Kernel
Expansion on the few common features, and PKI
for the remaining rare features. This ensures small
memory footprint for the expanded kernel vector,
while at the same time keeping a low number of vec-
tors from the reverse index.
3.1 Formal Details
The polynomial kernel of degree 2 is: K(x, y) =
(?x ? y + c)2, where x and y are binary feature vec-
tors. x ?y is the dot product between the vectors, and
in the case of binary feature vectors it corresponds
to the count of shared features among the vectors. F
is the set of all possible features.
We define FR and FC to be the sets of rare and
common features. FR?FC = ?, FR?FC = F . The
mapping function ?R(x) zeros out all the elements
of x not belonging to FR, while ?C(x) zeroes out
all the elements of x not in FC . Thus, for every x:
?R(x)+?C(x) = x, ?R(x)??C(x) = 0. For brevity,
denote ?C(x) = xC , ?R(x) = xR.
We now rewrite the kernel function:
K(x, y) = K(xR + xC , yR + yC) =
= (?(xR + xC) ? (yR + yC) + c)
2
= (?xR ? yR + ?xC ? yC + c)
2
= (?xR ? yR)
2
+ 2?2(xR ? yR)(xC ? yC)
+ 2c?(xR ? yR)
+ (?(xC ? yC) + c)
2
The first 3 terms are non-zero only when at
least one rare feature exists. We denote their sum
KR(x, y). The last term involves only common fea-
tures. We denote it KC(x, y). Note that KC(x, y) is
the polynomial kernel of degree 2 over feature vec-
tors of only common features.
We can now write the SVM decision function as:
?
j?SV
yj?jKR(xj , xR) +
?
j?SV
yj?jKC(xj , xC) + b
We calculate the first sum via PKI, taking into ac-
count only support-vectors which share at least one
feature with xR. The second sum is calculated via
kernel expansion while taking into account only the
239
common features. Thus, only pairs of common fea-
tures appear in the resulting weight vector using the
same expansion as in (Kudo and Matsumoto, 2003;
Isozaki and Kazawa, 2002). In our case, however,
the expansion is memory efficient, because we con-
sider only features in FC , which is small.
Our approach is similar to the PKE approach
(Kudo and Matsumoto, 2003), which used a basket
mining approach to prune many features from the
expansion. In contrast, we use a simpler approach to
choose which features to include in the expansion,
and we also compensate for the feature we did not
include by the PKI method. Thus, our method gen-
erates smaller expansions while computing the exact
decision function and not an approximation of it.
We take every feature occurring in less than s sup-
port vectors to be rare, and the other features to be
common. By changing s we get a trade-of between
space and time complexity: smaller s indicate more
common features (bigger memory requirement) but
also less rare features (less support vectors to in-
clude in the summation), and vice-versa. In con-
trast to other methods, changing s is guaranteed not
to change the classification accuracy, as it does not
change the computed decision function.
4 Toolkit and Evaluation
Using this method, one can accelerate SVM-based
NLP application by just changing the classification
function, keeping the rest of the logic intact. We
implemented an open-source software toolkit, freely
available at http://www.cs.bgu.ac.il/?nlpproj/. Our
toolkit reads models created by popular SVM pack-
ages (libsvm, SVMLight, TinySVM and Yamcha)
and transforms them into our format. The trans-
formed models can then be used by our efficient Java
implementation of the method described in this pa-
per. We supply wrappers for the interfaces of lib-
svm and the Java bindings of SVMLight. Changing
existing Java code to accommodate our fast SVM
classifier is done by loading a different model, and
changing a single function call.
4.1 Evaluation: Speeding up MaltParser
We evaluate our method by using it as the classi-
fication engine for the Java version of MaltParser,
an SVM-based state of the art dependency parser
(Nivre et al, 2006). MaltParser uses the libsvm
classification engine. We used the pre-trained En-
glish models (based on sections 0-22 of the Penn
WSJ) supplied with MaltParser. MaltParser already
uses an effective Classifiers Splitting heuristic when
training these models, setting a high baseline for our
method. The pre-trained parser consists of hundreds
of different classifiers, some very small. We report
here on actual memory requirement and parsing time
for sections 23-24, considering the classifier combi-
nation. We took rare features to be those appear-
ing in less than 0.5% of the support vectors, which
leaves us with less than 300 common features in
each of the ?big? classifiers. The results are summa-
rized in Table 1. As can be seen, our method parses
Method Mem. Parsing Time Sents/Sec
Libsvm 240MB 2166 (sec) 1.73
ThisPaper 750MB 70 (sec) 53
Table 1: Parsing Time for WSJ Sections 23-24 (3762
sentences), on Pentium M, 1.73GHz
about 30 times faster, while using only 3 times as
much memory. MaltParser coupled with our fast
classifier parses above 3200 sentences per minute.
5 Conclusions
We presented a method for fast, accurate and mem-
ory efficient calculation for polynomial kernels de-
cisions functions in NLP application. While the
method is applied to SVMs, it generalizes to other
polynomial kernel based classifiers. We demon-
strated the method on the MaltParser dependency
parser with a 30-time speedup factor on overall pars-
ing time, with low memory overhead.
References
Y. Goldberg and M. Elhadad. 2007. SVM model tamper-
ing and anchored learning: A case study in hebrew. np
chunking. In Proc. of ACL2007.
H. Isozaki and H. Kazawa. 2002. Efficient support vector
classifiers for named entity recognition. In Proc. of
COLING2002.
T. Kudo and Y. Matsumoto. 2003. Fast methods for
kernel-based text analysis. In ACL-2003.
J. Nivre, J. Hall, and J. Nillson. 2006. Maltparser: A
data-driven parser-generator for dependency parsing.
In Proc. of LREC2006.
Y. Wu, J. Yang, and Y. Lee. 2007. An approximate ap-
proach for training polynomial kernel svms in linear
time. In Proc. of ACL2007 (short-paper).
240
Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 32?39,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Gaiku : Generating Haiku with Word Associations Norms
Yael Netzer? and David Gabay and Yoav Goldberg? and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
{yaeln,gabayd,yoavg,elhadad}@cs.bgu.ac.il
Abstract
creativeness / a pleasing field / of bloom
Word associations are an important element
of linguistic creativity. Traditional lexical
knowledge bases such as WordNet formalize
a limited set of systematic relations among
words, such as synonymy, polysemy and hy-
pernymy. Such relations maintain their sys-
tematicity when composed into lexical chains.
We claim that such relations cannot explain
the type of lexical associations common in
poetic text. We explore in this paper the
usage of Word Association Norms (WANs)
as an alternative lexical knowledge source
to analyze linguistic computational creativity.
We specifically investigate the Haiku poetic
genre, which is characterized by heavy re-
liance on lexical associations. We first com-
pare the density of WAN-based word asso-
ciations in a corpus of English Haiku po-
ems to that of WordNet-based associations as
well as in other non-poetic genres. These
experiments confirm our hypothesis that the
non-systematic lexical associations captured
in WANs play an important role in poetic text.
We then present Gaiku, a system to automat-
ically generate Haikus from a seed word and
using WAN-associations. Human evaluation
indicate that generated Haikus are of lesser
quality than human Haikus, but a high propor-
tion of generated Haikus can confuse human
readers, and a few of them trigger intriguing
reactions.
? Supported by Deutsche Telekom Laboratories at Ben-
Gurion University of the Negev.
? Supported by the Lynn and William Frankel Center for
Computer Sciences.
1 Introduction
Traditional lexical knowledge bases such as Word-
Net formalize a limited set of systematic relations
that exist between words, such as synonymy, pol-
ysemy, hypernymy. When such relations are com-
posed, they maintain their systematicity, and do not
create surprising, unexpected word associations.
The human mind is not limited to such system-
atic relations, and people tend to associate words to
each other with a rich set of relations, such as non
systematic paradigmatic (doctor-nurse) and syntag-
matic relations (mash-potato) as identified by Saus-
sure (1949). Such associations rely on cultural
(mash-television), emotional (math - yuck) and per-
sonal experience (autumn - Canada).
In linguistic creativity, such as prose or poetry
writing, word associations play an important role
and the ability to connect words into new, unex-
pected relations is one of the key mechanisms that
triggers the reader involvement.
We explore in this paper the usage of Word As-
sociation Norms (WANs) as an alternative lexical
knowledge source to analyze linguistic computa-
tional creativity. WANs have been developed in psy-
chological research in the past 40 years. They record
typical word associations evoked by people when
they are submitted a trigger word. Such associations
(e.g., table to chair or cloth) are non-systematic, yet
highly stable across people, time (over a period of 30
years) and languages. WANs have been compiled in
various languages, and provide an interesting source
to analyze word associations in creative writing.
We specifically investigate the Haiku poetic
32
genre, which is characterized by heavy reliance on
lexical associations. The hypothesis we investigate
is that WANs play a role in computational creativ-
ity, and better explain the type of word associations
observed in creative writing than the systematic re-
lations found in thesauri such as WordNet.
In the rest of the paper, we refine our hypothe-
sis and present observations on a dataset of English
Haikus we collected. We find that the density of
WAN-based word associations in Haikus is much
higher than in other genres, and also much higher
than the density of WordNet-based associations. We
then present Gaiku, a system we developed to auto-
matically generate Haikus from a seed word using
word association norms. Evaluation we performed
with a group of 60 human readers indicates that the
generated Haikus exhibit interesting creative charac-
teristics and sometimes receive intriguing acclaim.
2 Background and Previous Work
2.1 Computational Creativity
Computational creativity in general and linguistic in
particular, is a fascinating task. On the one hand, lin-
guistic creativity goes beyond the general NLP tasks
and requires understanding and modelling knowl-
edge which, almost by definition, cannot be formal-
ized (i.e., terms like beautiful, touching, funny or in-
triguing). On the other hand, this vagueness itself
may enable a less restrictive formalization and allow
a variety of quality judgments. Such vague formal-
izations are naturally more useful when a computa-
tional creativity system does not attempt to model
the creativity process itself, but instead focuses on
?creative products? such as poetry (see Section 2.3),
prose and narrative (Montfort, 2006), cryptic cross-
word clues (Hardcastle, 2007) and many others.
Some research focus on the creative process itself
(see (Ritchie, 2006) for a comprehensive review of
the field). We discuss in this paper what Boden
(1998) calls P-Creativity (Psychological Creativity)
which is defined relative to the initial state of knowl-
edge, and H-Creativity (Historical Creativity) which
is relative to a specific reference culture. Boden
claims that, while hard to reproduce, exploratory
creativity is most successful in computer models of
creativity. This is because the other kinds of creativ-
ity are even more elusive due to the difficulty of ap-
proaching the richness of human associative mem-
ory, and the difficulty of identifying our values and
of expressing them in computational form.
We investigate in our work one way of addressing
this difficulty: we propose to use associative data as
a knowledge source as a first approximation of hu-
man associative capabilities. While we do not ex-
plain such associations, we attempt to use them in
a constructive manner as part of a simple combina-
tional model of creativity in poetry.
2.2 Word Associations and Creativity
Associations and creativity are long known to be
strongly connected. Mendick (Mendick, 1969) de-
fines creative thinking as ?the forming of associative
elements into new combinations which either meet
specified requirements or are in some way useful.?
The usefulness criterion distinguishes original think-
ing from creative thinking. A creative solution is
reached through three main paths: serendipity (ran-
dom stimuli evoke associative elements), similar-
ity (stimuli and solution are found similar through
an association) and mediation (both ?problem? and
?solution? can be associated to similar elements).
In our work, we hypothesize that interesting Haiku
poems exhibit creative word associations. We rely
on this hypothesis to first generate candidate word
associations starting from a seed word and follow-
ing random walks through WANs, but also to rank
candidate Haiku poems by measuring the density of
WAN-based associations they exhibit.
2.3 Poetry Generation
Although several automatic and semi-automatic po-
etry generation systems were developed over the
years, most of them did not rise above the level of
?party tricks? (Manurung et al, 2000). In his the-
sis, (Manurung, 2003), defined a poem to be a text
that meets three properties: meaningfulness, gram-
maticality and poeticness. Two of the few systems
that attempt to explicitly represent all three prop-
erties are reported in (Gervas, 2001) and (D??az-
Agudo et al, 2002). Both systems take as input a
prose message provided by the user, and translate it
into formal Spanish poetry. The system proposed
in (Manurung et al, 2000) is similar in that it fo-
cuses on the syntactic and phonetic patterns of the
poem, putting less stress on the semantics. The sys-
33
tem starts with a simple seed and gradually devel-
ops a poem, by making small syntactic and semantic
changes at every step.
Specifically in the subfield of Haiku generation,
the Haiku generator presented in (Wong and Chun,
2008) produces candidate poems by combining lines
taken from blogs. The system then ranks the can-
didates according to semantic similarity, which is
computed using the results returned by a search en-
gine when querying for words in each line. Hitch-
Haiku (Tosa et al, 2008), another Haiku generation
system, starts from two seed words given by the user.
It retrieves two phrases containing these words from
a corpus, and then adds a third phrase that connects
both input words, using lexical resources.
In our work, we induce a statistical language
model of the structure of Haikus from an analysis
of a corpus of English Haikus, and explore ways to
combine chains of lexical associations into the ex-
pected Haiku syntactic structure. The key issues we
investigate are the importance of WAN-based asso-
ciations in the Haiku generation process, and how a
chain of words, linked through WAN-based associa-
tions, can be composed into a Haiku-like structure.
2.4 Haiku
Haiku is a form of poetry originated in Japan in
the sixteenth century. The genre was adopted in
Western languages in the 20th Century. The origi-
nal form of a poem is of three lines of five, seven
and five syllables (although this constraint is loos-
ened in non-Japanese versions of Haiku (Gilbert and
Yoneoka, 2000)). Haiku, by its nature, aims to re-
flect or evoke emotion using an extremely economi-
cal linguistic form; most Haiku use present tense and
use no judgmental words; in addition, functional or
syntactic words may be dropped. Traditional Haiku
involve reference to nature and seasons, but modern
and western Haiku are not restricted to this theme1.
We adopt the less ?constraining? definition of the
author Jack Kerouac (2004) for a Haiku ?I propose
that the ?Western Haiku? simply say a lot in three
short lines in any Western language. Above all, a
Haiku must be very simple and free of all poetic
1Senryu poetry, similar in form to Haiku, is the Japanese
genre of poems that relate to human and relationships, and may
be humorous. Hereafter, we use Haiku for both the original
definition and the Senryu as well.
trickery and make a little picture and yet be as airy
and graceful as a Vivaldi Pastorella.? (pp. x-xi). In
addition, we are guided by the saying ? The best
haiku should leave the reader wondering ? (Quoted
in (Blasko and Merski, 1998))
2.5 Word Association Norms
The interest in word associations is common to
many fields. Idiosyncrasy of associations was used
as a diagnostic tool at the beginning of the 20th cen-
tury, but nowadays the majority of approaches deal
less with particular associations and more with gen-
eral patterns in order to study the structure of the
mental lexicon and of semantic memory (Rubinsten
et al, 2005).
Word Association Norms (WAN) are a collection
of cue words and the set of free associations that
were given as responses to the cue, accompanied
with quantitative and statistical measures. Subjects
are given a word and asked to respond immediately
with the first word that comes to their mind. The
largest WAN we know for English is the University
of South Florida Free Association Norms (Nelson et
al., 1998).
Word Association Norms and Thesauri in NLP
Sinopalnikova and Smrz (2004) have shown that
when building and extending semantic networks,
WANs have advantages over corpus-based meth-
ods. They found that WANs cover semantic rela-
tions that are difficult to acquire from a corpus: 42%
of the non-idiosyncratic cue-target pairs in an En-
glish WAN never co-appeared in a 10 words win-
dow in a large balanced text corpus. From the point
of view of computational creativity, this is encourag-
ing, since it suggests that association-based content
generation can lead to texts that are both sensible
and novel. (Duch and Pilichowski, 2007)?s work,
from a neuro-cognitive perspective, generates neol-
ogisms based, among other data, on word associa-
tion. (Duch and Pilichowski, 2007) sums ?creativity
requires prior knowledge, imagination and filtering
of the results.?
3 WordNet vs. Associations
Word association norms add an insight on language
that is not found in WordNet or are hard to acquire
from corpora, and therefore can be used as an ad-
ditional tool in NLP applications and computational
34
creativity.
We choose the Haiku generation task using word
associations, since this genre of poetry encapsulates
meaning in a special way. Haiku tend to use words
which are connected through associative or phono-
logical connections (very often ambiguous).
We hypothesize that word-associations are good
catalyzers for creativity, and use them as a building
block in the creative process of Haiku generation.
We first test this hypothesis by analyzing a corpus of
existing Haiku poems.
3.1 Analyzing existing text
Can the creativity of text as reflected in word as-
sociations be quantified? Are Haiku poems indeed
more associative than newswire text or prose? If
this is the case, we expect Haiku to have more asso-
ciative relations, which cannot be easily recovered
by WordNet than other type of text. We view the
WAN as an undirected graph in which the nodes
are stemmed words, and two nodes are connected
iff one of them is a cue for the other. We take the
associative distance between two words to be the
number of edges in the shortest path between the
words in the associations-graph. Interestingly, al-
most any word pair in the association graph is con-
nected with a path of at most 3 edges. Thus, we
take two words to be associatively related if their
associative distance is 1 or 2. Similarly, we define
the WordNet distance between two stemmed words
to be the number of edges in the shortest path be-
tween any synset of one word to any synset of the
other word2. Two words are WordNet-related if their
WordNet distance is less than 4 (this is consistent
with works on lexical-cohesion, (Morris and Hirst,
1991)).
We take the associativity of a piece of text to be
the number of associated word pairs in the text, nor-
malized by the number of word pairs in the text of
which both words are in the WAN.3 We take the
WordNet-relations level of a piece of text to be the
number of WordNet-related word pairs in the text.
2This is the inverse of the path-similarity measure of (Ped-
ersen et al, 2004).
3This normalization is performed to account for the limited
lexical coverage of the WAN. We don?t want words that appear
in a text, but are not covered by the WAN, to affect the associa-
tivity level of the text.
SOURCE AVG. ASSOC AVG. WORDNETRELATIONS (<3) RELATIONS (<4)
News 0.26 2.02
Prose 0.22 1.4
Haiku 0.32 1.38
Table 1: Associative and WordNet relations in various
text genres
We measure the average associativity and Word-
Net levels of 200 of the Haiku in our Haiku Cor-
pus (Section 4.1), as well as of random 12-word
sequences from Project Gutenberg and from the
NANC newswire corpus.
The results are presented in Table 1.
Perhaps surprisingly, the numbers for the Guten-
berg texts are lower on all measures. This is at-
tributed to the fact that Gutenberg texts have many
more pronouns and non-content words than the
Haiku and newswire text. Haiku text appears to
be more associative than newswire text. Moreover,
newswire documents have many more WordNet-
relations than the Haiku poems ? whenever words
are related in Haiku, this relatedness tends to be cap-
tured via the association network rather than via the
WordNet relations. The same trend is apparent also
when considering the Gutenberg numbers: they have
about 15% less associations than newswire text, but
about 30% less WordNet-relations. This supports
the claim that associative information which is not
readily available in WordNet is a good indicator of
creative content.
3.2 Generating creative content
We now investigate how word-associations can help
in the process of generating Haikus. We define
a 5 stage generative process: theme selection in
which the general theme of the Haiku is decided,
syntactic planning, which sets the Haiku form and
syntactic constraints, content selection / semantic
planning which combines syntactic and aesthetic
constraints with the theme selected in the previous
stages to form good building blocks, filtered over-
generation of many Haiku based on these selected
building blocks, and finally re-ranking of the gen-
erated Haiku based on external criteria.
The details of the generation algorithm are pre-
sented in Section 4.2. Here we focus on the creative
aspect of this process ? theme selection. Our main
claim is that WANs are a good source for interest-
35
ing themes. Specifically, interesting themes can be
obtained by performing a short random walk on the
association graph induced by the WAN network.
Table 2 presents the results of several random
walks of 3 steps starting from the seed words ?Dog?,
?Winter?, ?Nature? and ?Obsession?. For compar-
ison, we also present the results of random walks
over WordNet glosses for the same seeds.
We observe that the association network is bet-
ter for our needs than WordNet. Random walks in
WordNet are more likely to stay too close to the seed
word, limiting the poetic options, or to get too far
and produce almost random connections.
4 Algorithm for generating Haiku
4.1 Dataset
We used the Word Association Norms (WAN) of the
University of South Florida 4 (Nelson et al, 1998)
for discovering associations of words. The dataset
(Appendix A, there) includes 5,019 cue words and
10,469 additional target that were collected with
more than 6,000 participants since 1973.
We have compiled a Haiku Corpus, which in-
cludes approximately 3,577 Haiku in English of var-
ious sources (amateurish sites, children?s writings,
translations of classic Japanese Haiku of Bashu and
others, and ?official? sites of Haiku Associations
(e.g., Haiku Path - Haiku Society of America).
For the content selection part of the algorithms,
we experimented with two data sources: a corpus of
1TB web-based N-grams supplied by Google, and
the complete text of Project Gutenberg. The Guten-
berg data has the advantage of being easier to POS-
tag and contains less restricted-content, while the
Google Web data is somewhat more diverse.
4.2 Algorithm Details
Our Haiku generation algorithm includes 5 stages:
theme selection, syntactic planning, content selec-
tion, filtered over generation, and ranking.
The Theme Selection stage is in charge of dictat-
ing the overall theme of our Haiku. We start with
a user-supplied seed word (e.g. WINTER). We then
consult the Association database in order to enrich
the seed word with various associations. Ideally, we
would like these associations to be close enough to
4http://w3.usf.edu/FreeAssociation/
the seed word to be understandable, yet far enough
away from it as to be interesting. After some ex-
perimenting, we came up with the following heuris-
tic, which we found to provide adequate results. We
start with the seed word, and conduct a short random
walk on the associations graph. Each random step
is comprised of choosing a random direction (either
?Cue? or ?Target?) using a uniform distribution, and
then a random neighbor according to its relative fre-
quency. We conduct several (8) such walks, each
with 3 steps, and keep all the resulting words. This
gives us mostly close, probable associations, as well
as some less probable, further away from the seed.
The syntactic planning stage determines the
form of the generated Haiku, setting syntactic and
aesthetic constraints for the generative process. This
is done in a data-driven way by considering common
line patterns from our Haiku corpus. In a training
stage, we POS-tagged each of the Haiku, and then
extracted a pattern from each of the Haiku lines. A
line-pattern is a sequence of POS-tags, in which the
most common words are lexicalized to include the
word-form in addition to the POS-tag. An example
for such a line pattern might be DT the JJ NN.
We kept the top-40 frequent patterns for each of the
Haiku lines, overall 120 patterns. When generating a
new Haiku, we choose a random pattern for the first
line, then choose the second line pattern conditioned
on the first, and the third line pattern conditioned
on the second. The line patterns are chosen with a
probability proportional to their relative frequencies
in the training corpus. For the second and third lines
we use the conditional probabilities of a pattern ap-
pearing after the previous line pattern. The result
of this stage is a 3-line Haiku skeleton, dictating the
number of words on each line, their POS-tags, and
the placement of specific function words.
In the Content Selection stage, we look for pos-
sible Haiku lines, based on our selected theme and
syntactic structure. We go over our candidate lines5,
and extract lines which match the syntactic patterns
and contain a stemmed appearance of one of the
stemmed theme words. In our current implemen-
tation, we require the first line to contain the seed
word, and the second and third line to contain any of
5These are POS-tagged n-grams extracted from a large text
corpora: the Google T1 dataset or Project Gutenberg
36
SEED WAN WORDNET
Dog puppy adorable cute heel villain villainess
Dog cat curious george hound scoundrel villainess
Winter summer heat microwave wintertime solstice equinox
Winter chill cold alergy midwinter wintertime season
Nature animals instinct animals world body crotch
Nature natural environment surrounding complexion archaism octoroon
Obsession cologne perfume smell fixation preoccupation thought
Obsession compulsion feeling symptom compulsion onomatomania compulsion
Table 2: Some random walks on the WordNet and WAN induced graphs
the theme words. Other variations, such as choos-
ing a different word set for each line, are of course
possible.
The over generation stage involves creating
many possible Haiku candidates by randomly
matching lines collected in the content selection
stage. We filter away Haiku candidates which have
an undesired properties, such as repeating the same
content-word in two different lines.
All of the generated Haiku obey the syntactic and
semantic constraints, but not all of them are interest-
ing. Thus, we rank the Haiku in order to weed out
the better ones. The top-ranking Haiku is the output
of our system. Our current heuristic prefers highly
associative Haikus. This is done by counting the
number of 1st and 2nd degree associations in each
Haiku, while giving more weight to 2nd degree as-
sociations in order to encourage ?surprises?. While
all the candidate Haiku were generated based on a
common theme of intended associative connections,
the content selection and adherence to syntactic con-
straints introduce additional content words and with
them some new, unintended associative connections.
Our re-ranking approach tries to maximize the num-
ber of such connections.6
5 Evaluation
The ultimate goal of a poetry generation system is to
produce poems that will be considered good if writ-
ten by a human poet. It is difficult to evaluate to what
extent a poetry generation system can meet this goal
(Ritchie, 2001; Manurung et al, 2000). Difficulties
arise from two major sources: first, since a creative
6While this heuristic works well, it leaves a lot to be desired.
It considers only the quantity of the associations, and not their
quality. Indeed, when looking at the Haiku candidates produced
in the generation stage, one can find many interesting pieces,
where some of the lower ranking ones are far better than the top
ranking.
work should be novel, it cannot be directly evaluated
by comparison to some gold standard. Second, it is
hard for people to objectively evaluate the quality of
poetry. Even determining whether a text is a poem
or not is not an easy task, as readers expect poetry
to require creative reading, and tolerate, to some ex-
tent, ungrammatical structures or cryptic meaning.
5.1 ?Turing Test? Experiment
To evaluate the quality of Gaiku, we asked a group
of volunteers to read a set of Haiku, indicate how
much they liked each one (on a scale of 1-5), and
classify each Haiku as written by a human or by a
computer.
We compiled two sets of Haiku. The first set
(AUTO) contained 25 Haiku. 10 Haiku chosen at
random from our Haiku corpus, and 15 computer
generated ones. The computer generated Haiku
were created by identifying the main word in the first
line of each human-written Haiku, and passing it as
a seed word to the Haiku generation algorithm (in
case a first line in human-written Haiku contained
two main words, two Haiku were generated). We in-
cluded the top-ranking Haiku returning from a single
run of the system for each seed word. The only hu-
man judgement in compiling this set was in the iden-
tification of the main words of the human Haiku.
The second set (SEL) was compiled of 9 haiku po-
ems that won awards7, and 17 computer Haiku that
were selected by us, after several runs of the auto-
matic process. (Again, each poem in the automatic
poems set shared at least one word with some poem
in the human Haiku set).
The subjects were not given any information
about the number of computer-generated poems in
the sets.
7Gerald Brady Memorial Award Collection http://www.hsa-
haiku.org/bradyawards/brady.htm 2006-2007
37
The AUTO questionnaire was answered by 40
subjects and the SEL one by 22. (Altogether, 52 dif-
ferent people took part in the experiment, as some
subjects answered both versions). The subjects were
all adults (age 18 to 74), some were native English
speakers and others were fully fluent in English. Ex-
cept a few, they did not have academic background
in literature.
5.2 Results and Discussion
Results are presented in Table 3 and Figure 1.
Overall, subjects were correct in 66.7% of their
judgements in AUTO and 61.4% in SEL. The aver-
age grade that a poem - human or machine-made -
received correlates with the percentage of subjects
who classified it as human. The average grade and
rate of acceptance as written by human were signifi-
cantly higher for the Haiku written by people. How-
ever, some computer Haiku rivaled the average hu-
man poem in both measures. This is true even for
AUTO, in which both the generation and the selec-
tion processes were completely automatic. The best
computer Haiku of SEL scored better than most hu-
man Haiku in both measures.
The best computer poem in SEL was:
early dew / the water contains / teaspoons of honey
which got an average grade of 3.09 and was classi-
fied as human by 77.2% of the subjects.
At the other extreme, the computer poem (SEL):
space journey / musical instruments mythology /
of similar drugs
was classified as human by only 9% of the subjects,
and got an average grade of 2.04.
The best Haiku in the AUTO set was:
cherry tree / poisonous flowers lie / blooming
which was classified as human by 72.2% of the sub-
jects and got an average grade of 2.75.
The second human-like computer generated
Haiku in each set were:
spring bloom / showing / the sun?s pyre
(AUTO, 63.8% human) and:
blind snakes / on the wet grass / tombstoned terror
(SEL, 77.2% human).
There were, expectedly, lots of disagreements.
Poetry reading and evaluation is subjective and by
Human Poems Gaiku
AUTO avg. % classified as Human 72.5% 37.2%
avg. grade 2.86 2.11
SEL avg. % classified as Human 71.7% 44.1%
avg. grade 2.84 2.32
Table 3: Turing-test experiment results
itself (in particular for Haiku) a creative task. In ad-
dition, people have very different ideas in mind as to
a computer?s ability to do things. (One subject said,
for example, that the computer generated
holy cow / a carton of milk / seeking a church
is too stupid to be written by a computer; how-
ever, content is very strongly connected and does
not seem random). On the other end, subjects often
remarked that some of the human-authored Haiku
contained metaphors which were too obvious to be
written by a human.
Every subject was wrong at least 3 times (at least
once in every direction); every poem was wrongly-
classified at least once. Some really bad auto-poems
got a good grade here and there, while even the most
popular human poems got a low grade sometimes.
6 Discussion and Future Work
Word association norms were shown to be a useful
tool for a computational creativity task, aiding in the
creation of an automatic Haiku-generation software,
which is able to produce ?human-like? Haiku. How-
ever, associations can be used for many other tasks.
In the last decade, lexical chains are often used in
various NLP tasks such as text summarization or text
categorization; WordNet is the main resource for
detecting the cohesive relationships between words
and their relevance to a given chain (Morris and
Hirst, 1991). We believe that using word association
norms can enrich the information found in WordNet
and enable the detection of more relevant words.
Another possible application is for assisting
word-finding problem of children with specific lan-
guage impairments (SLI). A useful tactic practiced
as an assistance to retrieve a forgotten word is by
saying all words that come to mind. The NLP task,
therefore, is for a set of a given associations, recon-
struct the targeted word.
38
0 20 40 60 80 100
1.5
2
2.5
3
3.5
4
% of subjects who classified the poem as written by a human
Av
ar
eg
e 
gr
ad
e
 
 
Gaiku poems
Human poems
0 20 40 60 80 100
1.5
2
2.5
3
3.5
4
% of subjects who classified the poem as written by a human
Av
ar
eg
e 
gr
ad
e
 
 
Gaiku poems
Human poems
Figure 1: Average grades and percentages of subjects who classified poems as written by humans, for AUTO (left)
and SEL. Circles represent Haiku written by people, and stars represent machine-made Haiku
References
D.G. Blasko and D.W. Merski. 1998. Haiku poetry
and metaphorical thought: An invention to interdisci-
plinary study. Creativity Research Journal, 11.
M.A. Boden. 1998. Creativity and artificial intelligence.
Artificial Intelligence, 103(1?2).
F. de Saussure, C. Bally, A. Riedlinger, and
A. Sechehaye. 1949. Cours de linguistique gen-
erale. Payot, Paris.
B. D??az-Agudo, P. Gerva?s, and P. A. Gonza?lez-Calero.
2002. Poetry generation in COLIBRI. In Proc. of EC-
CBR.
W. Duch and M. Pilichowski. 2007. Experiments with
computational creativity. Neural Information Process-
ing, Letters and Reviews, 11(3).
P. Gervas. 2001. An expert system for the composition of
formal Spanish poetry. Journal of Knowledge-Based
Systems, 14.
R. Gilbert and J. Yoneoka. 2000. From 5-7-5 to 8-8-8:
An investigation of Japanese Haiku metrics and impli-
cations for English Haiku. Language Issues: Journal
of the Foreign Language Education Center.
D. Hardcastle. 2007. Cryptic crossword clues: Generat-
ing text with a hidden meaning BBKCS-07-04. Tech-
nical report, Birkbeck College, London.
J. Kerouac. 2004. Book of Haikus. Enitharmon Press.
H.M. Manurung, G. Ritchie, and H. Thompson. 2000.
Towards a computational model of poetry generation.
In Proc. of the AISB?00.
H.M. Manurung. 2003. An evolutionary algorithm ap-
proach to poetry generation. Ph.D. thesis, University
of Edinburgh.
S.A. Mendick. 1969. The associative basis of the cre-
ative process. Psychological Review.
N. Montfort. 2006. Natural language generation and nar-
rative variation in interactive fiction. In Proc. of Com-
putational Aesthetics Workshop at AAAI 2006, Boston.
J. Morris and G. Hirst. 1991. Lexical cohesion computed
by thesaural relations as an indicator of the structure of
text. Computational Linguistics, 17.
D.L. Nelson, C.L. Mcevoy, and T.A. Schreiber.
1998. The University of South Florida Word
Association, Rhyme, and Word Fragment Norms.
http://www.usf.edu/FreeAssociation/.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
Wordnet::similarity - measuring the relatedness of
concepts. In HLT-NAACL 2004: Demonstrations.
G. Ritchie. 2001. Assessing creativity. In Proc. of
AISB?01 Symposium.
G. Ritchie. 2006. The transformational creativity hy-
pothesis. New Generation Computing, 24.
O. Rubinsten, D. Anaki, A. Henik, S. Drori, and Y. Faran.
2005. Free association norms in the Hebrew language.
Word Norms in Hebrew. (In Hebrew).
A. Sinopalnikova and P. Smrz. 2004. Word association
thesaurus as a resource for extending semantic net-
works. In Communications in Computing.
N. Tosa, H. Obara, and M. Minoh. 2008. Hitch haiku:
An interactive supporting system for composing haiku
poem. In Proc. of the 7th International Conference on
Entertainment Computing.
M. Tsan Wong and A. Hon Wai Chun. 2008. Automatic
Haiku generation using vsm. In Proc. of ACACOS?08,
April.
39
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 129?133,
Paris, October 2009. c?2009 Association for Computational Linguistics
Hebrew Dependency Parsing: Initial Results
Yoav Goldberg and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
{yoavg,elhadad}@cs.bgu.ac.il
Abstract
We describe a newly available Hebrew
Dependency Treebank, which is extracted
from the Hebrew (constituency) Tree-
bank. We establish some baseline un-
labeled dependency parsing performance
on Hebrew, based on two state-of-the-art
parsers, MST-parser and MaltParser. The
evaluation is performed both in an artifi-
cial setting, in which the data is assumed
to be properly morphologically segmented
and POS-tagged, and in a real-world set-
ting, in which the parsing is performed on
automatically segmented and POS-tagged
text. We present an evaluation measure
that takes into account the possibility of
incompatible token segmentation between
the gold standard and the parsed data.
Results indicate that (a) MST-parser per-
forms better on Hebrew data than Malt-
Parser, and (b) both parsers do not make
good use of morphological information
when parsing Hebrew.
1 Introduction
Hebrew is a Semitic language with rich morpho-
logical structure and free constituent order.
Previous computational work addressed unsu-
pervised Hebrew POS tagging and unknown word
resolution (Adler, 2007), Hebrew NP-chunking
(Goldberg et al, 2006), and Hebrew constituency
parsing (Tsarfaty, 2006; Golderg et al, 2009).
Here, we focus on Hebrew dependency parsing.
Dependency-parsing got a lot of research at-
tention lately, in part due to two CoNLL shared
tasks focusing on multilingual dependency parsing
(Buchholz and Erwin, 2006; Nivre et al, 2007).
These tasks include relatively many parsing re-
sults for Arabic, a Semitic language similar to He-
brew. However, parsing accuracies for Arabic usu-
ally lag behind non-semitic languages. Moreover,
while there are many published results, we could
not find any error analysis or even discussion of
the results of Arabic dependency parsing models,
or the specific properties of Arabic making it easy
or hard to parse in comparison to other languages.
Our aim is to evaluate current state-of-the-art
dependency parsers and approaches on Hebrew
dependency parsing, to understand some of the
difficulties in parsing a Semitic language, and to
establish a strong baseline for future work.
We present the first published results on Depen-
dency Parsing of Hebrew.
Some aspects that make Hebrew challenging
from a parsing perspective are:
Affixation Common prepositions, conjunctions
and articles are prefixed to the following word,
and pronominal elements often appear as suffixes.
The segmentation of prefixes and suffixes is of-
ten ambiguous and must be determined in a spe-
cific context only. In term of dependency pars-
ing, this means that the dependency relations oc-
cur not between space-delimited tokens, but in-
stead between sub-token elements which we?ll re-
fer to as segments. Furthermore, any mistakes in
the underlying token segmentations are sure to be
reflected in the parsing accuracy.
Relatively free constituent order The ordering
of constituents inside a phrase is relatively free.
This is most notably apparent in the verbal phrases
and sentential levels. In particular, while most sen-
tences follow an SVO order, OVS and VSO con-
figurations are also possible. Verbal arguments
can appear before or after the verb, and in many
ordering. For example, the message ?went from
Israel to Thailand? can be expressed as ?went to
Thailand from Israel?, ?to Thailand went from Is-
rael?, ?from Israel went to Thailand?, ?from Israel
to Thailand went? and ?to Thailand from Israel
went?. This results in long and flat VP and S struc-
tures and a fair amount of sparsity, which suggests
129
that a dependency representations might be more
suitable to Hebrew than a constituency one.
Rich templatic morphology Hebrew has a
very productive morphological structure, which
is based on a root+template system. The pro-
ductive morphology results in many distinct word
forms and a high out-of-vocabulary rate, which
makes it hard to reliably estimate lexical param-
eters from annotated corpora. The root+template
system (combined with the unvocalized writing
system) makes it hard to guess the morphological
analyses of an unknown word based on its prefix
and suffix, as usually done in other languages.
Unvocalized writing system Most vowels are
not marked in everyday Hebrew text, which re-
sults in a very high level of lexical and morpho-
logical ambiguity. Some tokens can admit as many
as 15 distinct readings, and the average number of
possible morphological analyses per token in He-
brew text is 2.7, compared to 1.4 in English (Adler,
2007). This means that on average, every token is
ambiguous with respect to its POS and morpho-
logical features.
Agreement Hebrew grammar forces morpho-
logical agreement between Adjectives and Nouns
(which should agree in Gender and Number and
definiteness), and between Subjects and Verbs
(which should agree in Gender and Number).
2 Hebrew Dependency Treebank
Our experiments are based on the Hebrew De-
pendency Treebank (henceforth DepTB), which
we derived from Version 2 of the Hebrew
Constituency Treebank (Guthmann et al, 2009)
(henceforth TBv2). We briefly discuss the conver-
sion process and the resulting Treebank:
Parent-child dependencies TBv2 marks sev-
eral kinds of dependencies, indicating the mother-
daughter percolation of features such as number,
gender, definiteness and accusativity. See (Guth-
mann et al, 2009) for the details. We follow
TBv2?s HEAD, MAJOR and MULTIPLE depen-
dency marking in our-head finding rules. When
these markings are not available we use head find-
ing rules in the spirit of Collins. The head-finding
rules were developed by Reut Tsarfaty and used
in (Tsarfaty and Sima?an, 2008). We slightly ex-
tended them to handle previously unhandled cases.
Some conventions in TBv2 annotations resulted in
bad dependency structures. We identified these
constructions and transformed the tree structure,
Figure 1: Coordinated Verbs
Figure 2: Coordinated Sentence
either manually or automatically, prior to the de-
pendency extraction process.
The conversion process revealed some errors
and inconsistencies in TBv2, which we fixed.
We take relativizers as the head S and SBAR,
and prepositions as the heads of PPs. In the case
the parent of a word X is an empty element, we
take the parent of the empty element as the par-
ent of X instead. While this may result in non-
projective structures, in practice all but 34 of the
resulting trees are projective.
We take conjunctions to be the head of a coordi-
nated structure, resulting in dependency structures
such as the one in Figures 1 and 2. Notice how
in Figure 1 the parent of the subject ????/He? is
the coordinator ??/and?, and not one of the verbs.
While this makes things harder for the parser, we
find this representation to be much cleaner and
more expressive than the usual approach in which
the first coordinated element is taken as the head
of the coordinated structure.1
Dependency labels TBv2 marks 3 kinds of
functional relations: Subject, Object and Comple-
mentizer. We use these in our conversion pro-
cess, and label dependencies as being SBJ, OBJ
or CMP, as indicated in TBv2. We also trivially
mark the ROOT dependency, and introduce the re-
lations INF PREP, AT INF POS INF RB INF be-
tween a base word and its suffix for the cases of
suffix-inflected prepositions, accusative suffixes,
possessive suffixes and inflected-adverbs, respec-
tively. Still, most dependency relations remain un-
labeled. We are currently seeking a method of re-
liably labeling the remaining edges with a rich set
1A possible alternative would be to allow multiple par-
ents, as done in (de Marneffe et al, 2006), but current parsing
algorithms require the output to be tree structured.
130
of relations. However, in the current work we fo-
cus on the unlabeled dependency structure.
POS tags The Hebrew Treebank follows a syn-
tactic tagging scheme, while other Hebrew re-
sources prefer a more morphological/dictionary-
based scheme. For a discussion of these two tag-
ging schemes in the context of parsing, see (Gold-
erg et al, 2009). In DepTB, we kept the two
tagsets, and each token has two POS tags asso-
ciated with it. However, as current dependency
parsers rely on an external POS tagger, we per-
formed all of our experiments only with the mor-
phological tagset, which is what our tagger pro-
duces.
3 The Parsing Models
To establish some baseline results for Hebrew de-
pendency parsing, we experiment with two pars-
ing models, the graph-based MST-parser (Mc-
Donald, 2006) and the transition-based MaltParser
(Nivre et al, 2006). These two parsers repre-
sent the current mainstream approaches for de-
pendency parsing, and each was shown to pro-
vide state-of-the-art results on many languages
(CoNLL Shared Task 2006, 2007).
Briefly, a graph-based parsing model works by
assigning a score to every possible attachment be-
tween a pair (or a triple, for a second-order model)
of words, and then inferring a global tree struc-
ture that maximizes the sum of these local scores.
Transition-based models work by building the de-
pendency graph in a sequence of steps, where each
step is dependent on the next input word(s), the
previous decisions, and the current state of the
parser. For more details about these parsing mod-
els as well as a discussion on the relative benefits
of each model, see (McDonald and Nivre, 2007).
Contrary to constituency-based parsers, depen-
dency parsing models expect a morphologically
segmented and POS tagged text as input.
4 Experiments
Data We follow the train-test-dev split estab-
lished in (Tsarfaty and Sima?an, 2008). Specifi-
cally, we use Sections 2-12 (sentences 484-5724)
of the Hebrew Dependency Treebank as our train-
ing set, and report results on parsing the develop-
ment set, Section 1 (sentences 0-483). We do not
evaluate on the test set in this work.
The data in the Treebank is segmented and
POS-tagged. All of the models were trained on the
gold-standard segmented and tagged data. When
evaluating the parsing models, we perform two
sets of evaluations. The first one is an oracle ex-
periment, assuming gold segmentation and tag-
ging is available. The second one is a real-world
experiment, in which we segment and POS-tag the
test-set sentences using the morphological disam-
biguator described in (Adler, 2007; Goldberg et
al., 2008) prior to parsing.
Parsers and parsing models We use the freely
available implementation of MaltParser2 and
MSTParser3, with default settings for each of the
parsers.
For MaltParser, we experiment both with the de-
fault feature representation (MALT) and the fea-
ture representation used for parsing Arabic in
CoNLL 2006 and 2007 multilingual dependency
parsing shared tasks (MALT-ARA).
For MST parser, we experimented with first-
order (MST1) and second-order (MST2) models.
We varied the amount of lexical information
available to the parser. Each of the parsers was
trained on 3 datasets: LEXFULL, in which all the
lexical items are available, LEX20, in which lexi-
cal items appearing less than 20 times in the train-
ing data were replaced by an OOV token, and
LEX100 in which we kept only lexical items ap-
pearing more than 100 times in training.
We also wanted to control the effect of the rich
morphological information available in Hebrew
(gender and number marking, person, and so on).
To this end, we trained and tested each model ei-
ther with all the available morphological informa-
tion (+MORPH) or without any morphological in-
formation (-MORPH).
Evaluation Measure We evaluate the resulting
parses in terms of unlabeled accuracy ? the percent
of correctly identified (child,parent) pairs4. To be
precise, we calculate:
number of correctly identified pairs
number of pairs in gold parse
For the oracle case in which the gold-standard
token segmentation is available for the parser, this
is the same as the traditional unlabeled-accuracy
evaluation metric. However, in the real-word set-
ting in which the token segmentation is done auto-
matically, the yields of the gold-standard and the
2http://w3.msi.vxu.se/?jha/maltparser/
3http://sourceforge.net/projects/mstparser/
4All the results are macro averaged. The micro-averaged
numbers are about 2 percents higher for all cases.
131
Features MST1 MST2 MALT MALT-ARA
-M
OR
PH Full Lex 83.60 84.31 80.77 80.32Lex 20 82.99 84.52 79.69 79.40
Lex 100 82.56 83.12 78.66 78.56
+M
OR
PH Full Lex 83.60 84.39 80.77 80.73Lex 20 83.60 84.77 79.69 79.84
Lex 100 83.23 83.80 78.66 78.56
Table 1: Unlabeled dependency accuracy with
oracle token segmentation and POS-tagging.
Features MST1 MST2 MALT MALT-ARA
-M
OR
PH Full Lex 75.64 76.38 73.03 72.94Lex 20 75.48 76.41 72.04 71.88
Lex 100 74.97 75.49 70.93 70.73
+M
OR
PH Full Lex 73.90 74.62 73.03 73.43Lex 20 73.56 74.41 72.04 72.30
Lex 100 72.90 73.78 70.93 70.97
Table 2: Unlabeled dependency accuracy with
automatic token segmentation and POS-tagging.
automatic parse may differ, and one needs to de-
cide how to handle the cases in which one or more
elements in the identified (child,parent) pair are
not present in the gold-standard parse. Our evalua-
tion metric penalizes these cases by regarding any
such case as a mistake.
5 Results and Analysis
Results are presented in Tables 1 and 2.
It seems that the graph-based parsers perform
better than the transitions-based ones. We at-
tribute this to 2 factors: first, our representa-
tion of coordinated structure is hard to capture
with a greedy local search as performed by a
transition-based parser, because we need to de-
fer many attachment decisions until the final co-
ordinator is revealed. The global inference of the
graph-based parser is much more robust to these
kinds of structure. Indeed, when evaluating the
gold-morphology, fully-lexicalized models on a
subset of the test-set (314 sentences) which does
not have coordinated structures, the accuracy of
MALT improves in 3.98% absolute (from 80.77 to
84.75), while MST improves only in 2.66% abso-
lute (from 83.60 to 86.26). Coordination is hard
for both parsing models, but more so to the transi-
tion based MALT.
Second, it might be hard for a transition-based
parser to handle the free constituent order of He-
brew, as it has no means of generalizing from the
training set to various possible constituent order-
ing. The graph-based parser?s features and infer-
ence method do not take constituent order into ac-
count, making it more suitable for free constituent
order language.
As expected, the Second-order graph based
models perform better than the first-order ones.
Surprisingly, the Arabic-optimized feature-set do
not perform better than the English one for the
transition-based parsers. Overall, morphological
information seems to contribute very little (if at
all) to any of the parsers in the gold-morphology
(oracle) setting. MALTARA gets some benefit
from the morphological information in the fully-
lexicalized case, while the MST variants benefit
from morphology in the lexically-pruned models.
Overall, full lexicalization is not needed. In-
deed, less lexicalized LEX20 2nd-order graph-
based models perform better than the fully lexi-
calized ones. This strengthens our intuition that
robust lexical statistics are hard to acquire from
small annotated corpora, even more so for a lan-
guage with productive morphology such as He-
brew.
Moving from the oracle morphological disam-
biguation to an automatic one greatly hurts the per-
formance of all the models. This is in line with re-
sults for Hebrew constituency parsing, where go-
ing from gold segmentation to a parser derived one
caused a similar drop in accuracy (Golderg et al,
2009). This suggests that we should either strive
to improve the tagging accuracy, or perform joint
inference for parsing and morphological disam-
biguation. We believe the later would be a better
way to go, but it is currently unsupported in state-
of-the-art dependency parsing algorithms.
Interestingly, in the automatic morphological
disambiguation setting MALTARA benefits a little
from the addition of morpological features, while
the MST models perform better without these fea-
tures.
6 Conclusions
We presented the first results for unlabeled de-
pendency parsing of Hebrew, with two state-of-
the-art dependency parsing models of different
families. We experimented both with gold mor-
phological information, and with an automatically
derived one. It seems that graph-based models
have a slight edge in parsing Hebrew over current
transition-based ones. Both model families are not
currently making good use of morphological infor-
mation.
132
References
Meni Adler. 2007. Hebrew Morphological Disam-
biguation: An Unsupervised Stochastic Word-based
Approach. Ph.D. thesis, Ben-Gurion University of
the Negev, Beer-Sheva, Israel.
Sabine Buchholz and Marsi Erwin. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proc. of CoNLL.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proc. of LREC.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2006. Noun phrase chunking in hebrew: Influence
of lexical and morphological features. In Proc. of
COLING/ACL.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. EM can find pretty good HMM POS-Taggers
(when given a good start). In Proc. of ACL.
Yoav Golderg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing
performance using a wide coverage lexicon, fuzzy
tag-set mapping, and EM-HMM-based lexical prob-
abilities. In Proc of EACL.
Noemie Guthmann, Yuval Krymolowski, Adi Milea,
and Yoad Winter. 2009. Automatic annotation of
morpho-syntactic dependencies in a modern hebrew
treebank. In Proc of TLT.
Ryan McDonald and Joakim Nivre. 2007. Character-
izing the errors of data-driven dependency parsing
models. In Proc. of EMNLP.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Joakim Nivre, Johan Hall, and Jens Nillson. 2006.
MaltParser: A data-driven parser-generator for de-
pendency parsing. In Proc. of LREC.
Joakim Nivre, Johan Hall, Sandra Kubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proc. of the EMNLP-CoNLL.
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
realizational parsing. In Proc. of CoLING, August.
Reut Tsarfaty. 2006. Integrated morphological and
syntactic disambiguation for modern hebrew. In
Proceedings of ACL-SRW.
133
Word Segmentation, Unknown-word
Resolution, and Morphological Agreement
in a Hebrew Parsing System
Yoav Goldberg?
Ben Gurion University of the Negev
Michael Elhadad??
Ben Gurion University of the Negev
We present a constituency parsing system for Modern Hebrew. The system is based on the
PCFG-LA parsing method of Petrov et al (2006), which is extended in various ways in order
to accommodate the specificities of Hebrew as a morphologically rich language with a small
treebank. We show that parsing performance can be enhanced by utilizing a language resource
external to the treebank, specifically, a lexicon-based morphological analyzer. We present a
computational model of interfacing the external lexicon and a treebank-based parser, also in the
common case where the lexicon and the treebank follow different annotation schemes. We show
that Hebrew word-segmentation and constituency-parsing can be performed jointly using CKY
lattice parsing. Performing the tasks jointly is effective, and substantially outperforms a pipeline-
based model. We suggest modeling grammatical agreement in a constituency-based parser as a
filter mechanism that is orthogonal to the grammar, and present a concrete implementation of
the method. Although the constituency parser does not make many agreement mistakes to begin
with, the filter mechanism is effective in fixing the agreement mistakes that the parser does make.
These contributions extend outside of the scope of Hebrew processing, and are of general
applicability to the NLP community. Hebrew is a specific case of a morphologically rich language,
and ideas presented in this work are useful also for processing other languages, including
English. The lattice-based parsing methodology is useful in any case where the input is uncertain.
Extending the lexical coverage of a treebank-derived parser using an external lexicon is relevant
for any language with a small treebank.
1. Introduction
Different languages have different syntactic properties. In English, word order is rela-
tively fixed, whereas in other languages word order is much more flexible (in Hebrew,
the subject may appear either before or after a verb). In languages with a flexible word
order, the meaning of the sentence is realized using other structural elements, like word
? Computer Science Department, Ben Gurion University of the Negev, Israel.
E-mail: yoav.goldberg@gmail.com.
?? Computer Science Department, Ben Gurion University of the Negev, Israel.
E-mail: elhadad@cs.bgu.ac.il.
Submission received: 30 September 2011; revised submission received: 19 May 2012; accepted for publication:
3 August 2012.
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 1
inflections or markers, which are referred to as morphology (in Hebrew, the marker !??
is used to mark definite objects, distinguishing them from subjects in the same position.
In addition, verbs and nouns are marked for gender and number, and subject and verb
must share the same gender and number). A limited form of morphology also exists in
English: the -s and -ed suffixes are examples of Englishmorphological markings. In other
languages, morphological processes may be much more involved. The lexical units
(words) in English are always separated by white space. In Chinese, such separation
is not available. In Hebrew (and Arabic), most words are separated by white space,
but many of the function words (determiners like the, conjunctions such as and, and
prepositions like in or of ) do not stand on their own but are instead attached to the
following words.
A large part of the parsing literature is devoted to automatic parsing of English, a
language with a relatively simple morphology, relatively fixed word order, and a large
treebank. Data-driven English parsing is now at the state where naturally occurring text
in the news domain can be automatically parsed with accuracies of around 90% (accord-
ing to standard parsing evaluation measures). When moving from English to languages
with richer morphologies and less-rigid word orders, however, the parsing algorithms
developed for English exhibit a large drop in accuracy. In addition, whereas English has
a large treebank, containing over one million annotated words, many other languages
have much smaller treebanks, which also contribute to the drop in the accuracies of
the data-driven parsers. A similar drop in parsing accuracy is also exhibited in English
when moving from the news domain, on which parsers have traditionally been trained,
to other genres such as prose, blogs, poetry, product reviews, or biomedical texts, which
use different vocabularies and, to some extent, different syntactic rules.
This work focuses on constituency parsing of Modern Hebrew, a Semitic language
with a rich and productive morphology, relatively free word order,1 and a small tree-
bank. Several natural questions arise: Can the small size of the treebank be compensated
for using other available resources or sources of information? How should the word
segmentation issue (that function words do not appear in isolation but attach to the next
word, forming ambiguous letter patterns) be handled? Can morphological information
be used effectively in order to improve parsing accuracy?
We present a system which is based on a state-of-the-art model for constituency
parsing, namely, the probabilistic context-free grammar (PCFG) with latent annotations
(PCFG-LA) model of Petrov et al (2006), as implemented in the BerkeleyParser. After
evaluating the out-of-the-box performance of the BerkeleyParser on the Hebrew tree-
bank, we discuss some of its limitations and then go on to extend the PCFG-LA parsing
model in several directions, making it more suitable for parsing Hebrew and related
languages. Our extensions are based on the following themes.
Separation of lexical and syntactic knowledge. There are two kinds of knowledge inherent
in a parsing system. One of them is syntactic knowledge governing the way in which
words can be combined to form structures, which, in turn, can be combined to form
ever larger structures. The other is lexical knowledge about the identities of individual
words, the word classes they belong to, and the kinds of syntactic structures they can
participate in. We argue that the amount of syntactic knowledge needed for a parsing
system is relatively limited, and that sufficiently large parts of it can be captured also
1 To be more precise, in Hebrew the order of constituents is relatively free, whereas the order of the words
within certain constituents is relatively fixed.
122
Goldberg and Elhadad Parsing System for Hebrew
based on a relatively small treebank. Lexical knowledge, on the other hand, is much
more vast, and we should not rely on a treebank (small or large) to provide adequate
lexical coverage. Instead, we should aim to find ways of integrating lexical knowledge,
which is external to the treebank, into the parsing process.
We extend the lexical coverage of a treebank-based parser using a dictionary-based
morphological analyzer. We present a way of integrating the two resources also for the
common case where their annotations schemes diverge. This method is very effective in
improving parsing accuracy.
Encoding input uncertainty using a lattice-based representation. Sometimes, the language
signal (the input to the parser) may be uncertain. This happens in Hebrew when a
space-delimited token such as !???? can represent either a single word (?[an] onion?) or a
sequence of two words or three words (?in shadow? and ?in the shadow,? respectively).
When computationally feasible, it is best to let the uncertainty be resolved by the parser
rather than in a separate preprocessing step.
We propose encoding the input-uncertainty in a word lattice, and use lattice parsing
(Chappelier et al 1999; Hall 2005) to perform joint word segmentation and syntactic
disambiguation (Cohen and Smith 2007; Goldberg and Tsarfaty 2008). Performing the
tasks jointly is effective, and substantially outperforms a pipeline-based model.
Using morphological information to improve parsing accuracy. Morphology provides useful
hints for resolving syntactic ambiguity, and the parsing model should have a way of
utilizing these hints. There is a range of morphological hints than can be utilized: from
functional marking elements (such as the !?? marker indicating a definite direct object);
to elements marking syntactic properties such as definiteness (such as the Hebrew !?
marker); to agreement patterns requiring a compatibility in properties such as gender,
number, and person between syntactic constituents (such as a verb and its subject or
an adjective and the noun it modifies).
We suggest modeling agreement as a filtering process that is orthogonal to the
grammar. Although the constituency parser does not make many agreement mistakes
to begin with, the filter mechanism is effective in fixing the agreement mistakes that the
parser does make, without introducing new mistakes.
Aspects of the work presented in this article are discussed in earlier publica-
tions. Goldberg and Tsarfaty (2008) suggest the lattice-parsing mechanism, Goldberg
et al (2009) discuss ways of interfacing a treebank-derived PCFG-parser with an exter-
nal lexicon, and Goldberg and Elhadad (2011) present experiments using the PCFG-LA
BerkeleyParser. Here we provide a cohesive presentation of the entire system, as well as
a more detailed description and an expanded evaluation. We also extend the previous
work in several dimensions: We introduce a new method of interfacing the parser and
the external lexicon, which contributes to an improved parsing accuracy, and suggest
incorporating agreement information as a filter.
The methodologies we suggest extend outside the scope of Hebrew processing,
and are of general applicability to the NLP community. Hebrew is a specific case of
a morphologically rich language, and ideas presented in this work are useful also for
processing other languages, including English. The lattice-based parsing methodology
is useful in any case where the input is uncertain. Indeed, we have used it to solve
the problem of parsing while recovering null elements in both English and Chinese
(Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation
and parsing of Arabic (Green and Manning 2010). Extending the lexical coverage of
a treebank-derived parser using an external lexicon is relevant for any language with
123
Computational Linguistics Volume 39, Number 1
a small treebank, and also for domain adaptation scenarios for English. Finally, the
agreement-as-filter methodology is applicable to any morphologically rich language,
and although its contribution to the parsing task may be limited, it is of wide applica-
bility to syntactic generation tasks, such as target-side-syntax machine translation in a
morphologically rich language.
2. Modern Hebrew
2.1 Lexical and Syntactic Properties
Some relevant lexical and syntactic properties of Modern Hebrew are highlighted in this
section.
2.1.1 Unvocalized Orthography. Most vowels are not marked in everyday Hebrew
text, which results in a very high level of lexical and morphological ambiguity. Some
tokens can admit as many as 15 distinct readings, and the average number of pos-
sible morphological analyses per token in Hebrew text is 2.7, compared with 1.4 in
English (Adler 2007). The word !??????? can be read in at least eight different ways
(?spoons,? ?square cotton headkerchiefs,? ?coercions,? ?as mouths,? ?as spouts,? ?as fairies,?
?ungratefulness,? ?fun/adjectivefeminine,plural?), the word !???? in at least six ways (?a
journalist,? ?writing,? ?script,? ?wrote,? ?added someone as a recipient,? ?was added as
a recipient?) and the word !?? can be read as a very common case-marker (appearing
before definite direct objects), a very common pronoun (?you/feminine?), and a noun
(?shovel?).
2.1.2 Affixation. Eight common prepositions, conjunctions, and articles may never
appear in isolation and must always be attached as prefixes to the following word.2
These include the function words !?? (?from?), !? (?which?/?who?/?that?), !??? (?when?),
!? (?the?), !? (?and?), !?? (?like?), !? (?to?), and !? (?in?),. Several such elements may attach
together, producing forms such as !????????? ( !?-?-??-?-???? ?and-that-from-the-sun?). Notice
that when it appears by itself, the last part of the token, the noun !???? (?sun?), can also
be interpreted as the sequence !?-??? (?who moved?). The linear order of such elements
within a token is fixed (disallowing the reading !?-?-??-?-?-??? in the previous example).
The syntactic relations of these elements with respect to the rest of the sentence
are rather free, however. The relativizer !? (?that?), for example, may attach to an
arbitrarily long relative clause that goes beyond token boundaries. The attachment in
such cases encompasses a long-distance dependency that cannot be captured by local-
context (or Markovian) sequential processes that are typically used for morphological
disambiguation. The same argument holds for resolving PP attachment of a prefixed
preposition or marking conjunction of elements of any kind.
To further complicate matters, the definite article !? (?the?) is not realized in writing
when following the particles !? (?in?), !?? (?like?), and !? (?to?). Thus, the form !???? can be
interpreted as either !?-??? (?in house?) or !?-?-??? (?in the house?).3
2 In what follows, we indicate the correct segmentations of the different forms. Naturally occurring
Hebrew text does not have such indications.
3 This overt element is in fact indicated by vocalization, but is not realized in standard written text.
124
Goldberg and Elhadad Parsing System for Hebrew
In addition, pronominal elements (clitics) may attach to nouns, verbs, adverbs,
prepositions, and others as suffixes (e.g., !????? [ !????-??, ?brought-them?], !????? [ !-?????,?on
them?]).
These affixations result in highly ambiguous token segmentations: !??????? (?[they]
assigned numbers?) vs. !??????-? (?his number? or ?the one who cuts his hair?) vs. !-???-????
(?from his book? or ?from his barber?), !?????? (?putting together?) vs. !?-????? (?the train?),
and !???? (?an onion?) vs. !?-??? (?in the shadow?) are only a few examples of ambiguities
that may arise. Quantitatively, 99,896 out of 567,483 forms (17%) in a wide-coverage
lexicon of Hebrew can admit both segmented and unsegmented analyses.
In many cases the correct segmentation cannot be determined from local context
alone, but can be disambiguated by more global syntactic constraints (in ????? ?????
!???????, the middle token is ambiguous between !????? [?sky?] and !?-???? [?that/rel water?],
and the sequence can be interpreted as either ?I saw blue skies? or ?I saw that blue water.?
On the other hand, !???? ??? ?????? ??????? ????? ????? is unambiguous because the past
verb !?????? requires the relativizer !?, allowing only the segmented !?-???? reading ?I saw
that blue water broke from the well?. In the other direction, ??????? ??????? ????? ?????
!????? is also unambiguous, allowing only the unsegmented reading ?I saw blue skies
and went to sleep?.)
2.1.3 Rich Templatic Morphology. Hebrew words follow a complex morphological struc-
ture, which is based on a root + template system, with both derivational and inflectional
elements. Word forms can encode gender, number, person, and tense, and in addition
noun-compounding is also morphologically marked (see Section 2.1.7). Although the
exact details of the system are irrelevant (but see Adler [2007] and Glinert [1989] for a
good overview), we note that this word formation mechanism results in a very high
number of possible word forms, and that it is hard to guess the part-of-speech of words
based on prefixes and suffixes alone, a method frequently used in other languages.
2.1.4 The Participle Form. The Hebrew participle form ( !????????, literally the ?middle form?
of verbs) is a form that shares morphological and syntactic properties of nouns, verbs,
and adjectives. This form causes many disagreements between human annotators, and
large disagreement is found also between major Hebrew dictionaries regarding many
word forms (see Adler et al [2008b] for a discussion from tag set design and annotation
guidelines, including many syntactic, semantic, and lexical considerations). For the
purpose of this work, this form is of interest as it highlights the inherent ambiguity
between adjectival, nominal, and verbal readings of many words, which are hard to
disambiguate even in context.
2.1.5 Relatively Free Constituent Order. The ordering of constituents inside a phrase is
relatively free. This is most notably apparent in verbal phrases and sentential levels. In
particular, whereas most sentences follow a subject-verb-object order (SVO), OVS and
VSO configurations are also possible (counting in the Hebrew Treebank reveals 5,720
SV cases and 2,613 VS cases, compared with 81,135 SV and 3,018 VS constructions in
the English WSJ Treebank). In addition, verbal arguments can appear before or after
the verb, and in many orders. Such variations in constituent order are easy to capture
using ?flat? S structures putting the verbs and all of its arguments on the same clausal
level, and this is the annotation approach adopted by the Hebrew Treebank (as well
as by treebanks of other languages, such as French [Abeille?, Cle?ment, and Toussenel
2003]). These flat structures result in the grammar having more and longer rules and the
treebank having fewer instances of each rule type, however, causing a data sparseness
125
Computational Linguistics Volume 39, Number 1
problem for statistical estimation methods based on treebank counts, and making it
more difficult to reliably estimate the grammar parameters.
2.1.6 Verbless Constructions. Several constructions in which the verb is not realized are
common in Hebrew. These include the possessive constructions such as ????????? ?????
!???? (?to-Ido toys many? meaning ?ido has many toys?), which also feature a flexible
constituent order !????? ???? ????????? (?toys many to-Ido?, ?ido has many toys?), and
copular constructions such as !????? ???? (?the-boy cute? ?the boy is cute?) and !?????? ????
(?the-boy crazy? ?the boy is crazy?).
2.1.7 NP Structure and Construct-State. Although constituent order may vary, NP
internal structure is rigid. A special morphological marker (construct state, or !????????)
is used to mark noun-compounds as well as similar phenomena (this is similar to the
idafa construction in Arabic).4 Noun compounding in Modern Hebrew is productive
and very frequent?about a quarter of the noun tokens in the Hebrew Treebank are in
the construct state. Construct-state nouns can be highly ambiguous with non-construct-
state nouns. Some forms are morphologically marked but the marking is not present in
unvocalized text ( !?????/banot vs. !?????/bnot), and some forms are not marked at all ( !????).
The construct-state marker, although ambiguous, is essential for analyzing NP internal
structure. Where regular nouns are marked for definiteness using the definite marker
!?, construct-nouns acquire the definite status of the noun-phrase they compound to.
Construct constructions may be nested, as in !???????? ?????? ?????? ???? ???? (?shadeconst
colorconst lidconst boxconst the apples,? meaning ?the shade of the color of the lid of the box
of the apples?).
2.1.8 Definiteness. Definiteness is spread across many elements in the NP. All elements
in a definite NP, except for construct-nouns and proper-names, are explicitly marked
using the functional element !? that is prefixed to the token. Proper-names are inher-
ently definite and cannot take the definite marker, and construct-nouns acquire their
definiteness status from the NP they dominate (definiteness is not explicitly marked on
construct-nouns).
2.1.9 Case Marking. Definite direct objects are marked. The case marker in this case is
the function word !?? appearing before the direct object. Subjects, indirect objects, and
non-definite direct objects are not marked.
2.1.10 Agreement. Hebrew grammar forces morphological agreement between adjec-
tives and nominals (adjectives appear after the noun, and agree in gender, number, and
definiteness), and between subjects and verbs (including the verbless copular construc-
tions), which agree in gender, number, and person. Agreement in the predicative case
is a bit complex: When the verb is overt and the predicative-complement is a noun,
as in !????? ??? ??????? (?the-tripfem isfem an-excusemasc?), gender and number agreement
are required between the subject and the verb (but not the predicative-complement),
but in the verbless case, the subject and the predicate-complement noun must agree
( !????? ???????* ?the-tripfem an-excusemasc?). When the predicate-complement is an adjec-
tive, gender and number agreement between the subject and the predicate-complement
4 The construct state is not restricted to nouns, and can also appear on numbers (e.g., !????? !?????/?tens-of
kids?) and adjectives ( !???????? !????/?biggest-of authors?).
126
Goldberg and Elhadad Parsing System for Hebrew
is required regardless of the realization of the verb/copular element: !???? ????, ?????
!????*, !???? ??? ????, !???? ??? ?????* (?the-boy tallmasc?, ?*the-boy tallfem?, ?the-boy ismasc
tallmasc?, ?*the-girl isfem tallmasc?).
2.2 Implications for Parsing
After surveying some lexical and syntactic properties of Modern Hebrew, we turn
to highlight some aspects in which Modern Hebrew differs from English from the
perspective of parsing system design.
2.2.1 Small Amount of Annotated Data. Whereas the English Treebank is relatively large
(49,208 sentences, or 1,173,766 words), the Hebrew Treebank (Guthmann et al 2009) is
much smaller, containing only 6,220 sentences, or 115,661 tokens (156,316 words5).
The small size of the Hebrew Treebank implies a smaller training set for learning-
algorithms used to construct the parser.
2.2.2 Ambiguous Word Segmentation. Syntactic parsing systems treat the input sentence
as observed data?the leaves (in constituency parsing) of the tree are known in advance,
and the parser is expected to build a parse tree around them. This is not the case in
Hebrew, where many function words are not separated by white space but instead are
prefixed to the next word and appear within the same token. This makes the word
sequence unobserved to the parser, which has to infer both the syntactic-structure and
the token segmentation.6
One possible solution to the unobserved word-sequence problem is a pipeline
system in which an initial model is in charge of token-segmentation, and the output of
the initial model is fed as the input to a second stage parser. This is a popular approach
in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and
Manning 2010). As discussed in Section 2.1.2 (as well as in Tsarfaty [2006a], Goldberg
and Tsarfaty [2008], and Cohen and Smith [2007]), however, the token-segmentation and
syntactic-parsing tasks are closely intertwined and are better performed jointly instead
of in a pipeline fashion, which is the approach we explore in this work.
2.2.3 Morphological Variation and High Out-of-Vocabulary Rate. The intrinsic deficiency
caused by the small amount of training data is made even more severe due to Hebrew?s
rich morphological inflection patterns. The high amount of morphological variation
means that many word forms will not be observed in the training data, making it harder
to reliably estimate lexical probabilities based on the annotated resources alone.
Unlike English, where parts-of-speech for words are relatively easy to guess based
on simple orthographic features (words starting with capital letters are proper nouns,
words ending in -ed are usually verbs, etc.), this is not the case for Hebrew. Among
the 773 words appearing in English test data but not in the training data, 269 start
with a capital letter, 58 end with -ed, and 49 end with -ing. Together, these three simple
heuristics cover almost half of the unobserved tokens. Such heuristics are not available
for Hebrew in the common case of unvocalized text: Proper names are not marked
5 Because of agglutination, a Hebrew token may consist of several words, for example the token !????
comprises the two words !?(?in?) and !???(?house?).
6 Token segmentation is sometimes (erroneously) referred to as morphological segmentation.
127
Computational Linguistics Volume 39, Number 1
in writing, and word prefixes and suffixes are not indicative of the part-of-speech
tags.7 Thus, the out-of-vocabulary (OOV) problem is much harder in Hebrew than in
English and other European languages: On the one hand many words are unobserved
in training, and on the other, it is more difficult to guess the analysis of such unknown
words.
A system for handling automatic processing of Hebrew text cannot rely solely on
manually annotated corpora, as such corpora cannot provide adequate lexical coverage.
Systems that attempt to perform disambiguation on the lexical level (such as sequence-
based morphological disambiguators, or syntactic parsers that perform morphological
disambiguation as part of the parsing process) should be designed to incorporate lexical
knowledge from sources external to the annotated corpora. We discuss methods of en-
hancing the system?s performance based on a resource that is external to the treebank: A
lexicon-based broad-coverage morphological analyzer enhanced with semi-supervised
probability estimates based on expectation maximization (EM) training of a hidden
Markov model (HMM) tagger on a large amount of unannotated text.
2.2.4 Morphological Agreement. The rich morphological system also means that words
carry large amounts of extra information: definiteness, gender, number, tense, and
person. Some of this information interacts with syntax through agreement constraints.
Specifically, nouns and adjectives should agree in gender and number, and subjects
and verbs should agree in gender, number, and person. Agreement constraints can
provide useful hints for disambiguating the syntactic structure. Consider for example
the sentence !?????? ?? ?????? ???? ??? (?wife of the man who ate the apple?). The
English sentence is ambiguous with respect to the entity who ate the apple, but the
Hebrew version is not?the verb !????? (?ate?) is in feminine form, indicating that it was
the wife who did the eating. Can a parsing system make use of such information? This
issue is investigated further in Section 8.2.
2.3 Existing Resources for Hebrew Text Processing
Several linguistic resources are available for Hebrew, and are used as building blocks
for the parsing systems described in this work.
2.3.1 The Hebrew Constituency-Treebank. A constituency treebank of Modern Hebrew,
incrementally developed at the Technion over the course of more than eight years
(Sima?an et al 2001; Guthmann et al 2009), is maintained by MILA, the knowledge
center for processing Hebrew.8 The current version of the treebank (Version 2) contains
6,220 sentences taken from the Israeli daily newspaper !???? (Ha?aretz). The sentences
are manually annotated on both the lexical and the syntactic levels. Each token9 is
segmented into words, and eachword is assigned a part of speech tag that also captures,
7 Although the suffixes are good indicators of gender and number ( !?? is usually plural masculine, !? is
usually singular feminine), they are not good at indicating the core part-of-speech ( !? is a suffix can
appear in adjectives !????, verbs !?????, nouns !?????, and similarly for !?? ( !?????, !?????????, !?????????). Furthermore,
due to the root+template system, in most cases the first and last letters of the word are part of the root
and not of the pattern !??????!,?????, making the suffixes even less indicative.
8 http://www.mila.cs.technion.ac.il/mila/eng/index.html.
9 As discussed in Section 2.1.2, Hebrew tokens (entities separated by white space and punctuation symbols)
do not necessarily correspond to Hebrew words. A single token may contain several words.
128
Goldberg and Elhadad Parsing System for Hebrew
where applicable, the morphological properties of the word such as number, gender,
and person. Then a constituency tree is built on top of the segmented words. The
annotation of NPs is relatively nested, and the sentence level structures are relatively
flat (the verb and all of its arguments reside on one level under S). The treebank has
115,661 tokens and 156,764 words.
The POS tagging scheme in the treebank is highly syntactic in nature: A part-of-
speech is chosen to reflect the syntactic function of the given word in context. For exam-
ple, demonstrative pronouns are tagged in the treebank as adjectives when appearing
in an adjectival position ( !?? !???, ?this/JJ child/NN?), and a special MOD tag is used to
mark non-adverbial clausal level modification (that is, modifications that can be treated
as adverbial, but that are used to modify something other than a verb). For a more
detailed description of the Constituency Treebank see Sima?an et al (2001), Guthmann
et al (2009), and Tsarfaty (2010, pages 199?216), as well as the annotation guidelines.10
2.3.2 Train/dev/test Splits. Throughout the article, we follow the established train/
dev/test split for the treebank, namely, sentences 1?483 are used for development,
sentences 484?5,740 are used for training the parser, and sentences 5,741 to 6,220 are
used as the final test set.
2.3.3 The MILA Broad-Coverage Lexicon. Aside from the Constituency Treebank, Hebrew
has a wide-coverage, lexicon-based morphological analyzer which can assign morpho-
logical analyses (prefixes, suffixes, core POS, gender, number, person, etc.) to Hebrew
tokens. The lexicon (henceforth the KC Analyzer) is developed and maintained by
the Knowledge Center for Processing Hebrew (Itai and Wintner 2008). It is based on a
lexicon of roughly 25,000word lemmas and their inflection patterns. From these, 562,439
unique word forms are derived. These are then prefixed (subject to constraints) by 73
prepositional prefixes. Even with this seemingly large vocabulary, the KC Analyzer?s
coverage is not perfect. In Adler et al (2008a), we present a machine-learning method
that is trained on the basis of the analyzer and that can guess possible analyses for
words unknown to the analyzer with reasonable accuracies. Using this extension, the
analyzer has perfect coverage (even though the quality is obviously better for words
that are present in the analyzer?s database).
The tag set used by the lexicon/analyzer is lexicographic in nature, and is discussed
in depth in BGU Computational Linguistics Group (2008).
Creating a resource such as the morphological analyzer for a morphologically rich
language is a worthwhile and cost-effective effort: After establishing the tag set, it is
relatively straightforward to add lemmas to the lexicon, and the automatic inflection
process guarantees good coverage of all the possible inflections. This is much more
efficient than annotating enough text to obtain a similar coverage.
2.3.4 Hebrew Morphological Disambiguator. The morphological analyzer provides the
possible set of analyses for each token, but does not disambiguate the correct analy-
sis in context. A morphological disambiguator (henceforth ?the Hebrew tagger? or
?tagger?) was developed by Meni Adler at Ben-Gurion University of the Negev
(Adler and Elhadad 2006; Adler 2007; Goldberg, Adler, and Elhadad 2008). After the
(extended) morphological analyzer assigns the possible analyses for each token in an
10 http://www.mila.cs.technion.ac.il/mila/files/treebank/Decisions-Corpus1-5001.v1.pdf.
129
Computational Linguistics Volume 39, Number 1
input sentence, the tagger takes the output of the analyzer as input and chooses the sin-
gle best analysis for the entire sentence (performing both token segmentation of words
and part-of-speech assignment for each word). The tagger is an HMM-based sequential
model that is trained in a semi-supervised fashion using EM based on the output of the
morphological analyzer on a large amount (about 70M words) of unannotated Hebrew
text. The tagger is described in Adler and Elhadad (2006) and Adler (2007).
The tagger is relatively accurate: It achieves 93% accuracy in predicting segmen-
tation and tagging when measured on the POS accuracy, and 90% accuracy when
measured on the complete tag set, which includes the complete set of morphological
features. Because the tagger is not trained on a particular annotated training set but
instead on a very large corpus of text spanning multiple genres, its performance is
robust across domains.
The tagger?s success is due in part to a smart initialization procedure to the EM
training process. This initialization procedure takes the output of the analyzer and
assigns a conditional probability distribution P(tag|word) for each word. In other words,
it assigns an a priori, context-free likelihood for each analysis of a word (although the
word broke can be either a verb in the past tense or an adjective, it is more likely to be the
former; such preferences can be modeled as probability distributions, and the initializa-
tion procedure attempts to learn the values of these distributions automatically from
raw data). This initialization procedure is described in Goldberg, Adler, and Elhadad
(2008).
A side effect of the EM?HMM training of the tagger is pseudo-counts for ?word, tag?
events, which are based on patterns observed in the unannotated training data. We use
these counts in order to improve the lexical-disambiguation capacity of the parser.
2.3.5 A Resource Incompatibility Issue. Unfortunately, the KC Analyzer adopted a dif-
ferent tag set than the one used in the treebank, and analyses produced by the KC
Analyzer (and hence by the morphological disambiguator) are incompatible with the
Hebrew Treebank. These are not mere technical differences, but derive from different
perspectives on the data. The Hebrew Treebank (TB) tag set is syntactic in nature (?if
the word in this particular position functions as an adverb, tag it as an adverb, even
though it is listed in the dictionary only as a noun?), whereas the KC tag set (Adler
2007; Netzer et al 2007; Adler et al 2008b) takes a lexical approach to POS tagging
(?a word can assume only POS tags that would be assigned to it in a dictionary?). The
lexical approach does not accommodate generic modification POS tags such as MOD,
nor does it allow listing of demonstrative pronouns as adjectives.
These divergent perspectives are reflected in different guidelines to human taggers,
different principles underlying tag definitions, and different verification procedures.
This difference in perspective yields different performances for parsers induced from
tagged data, and a simple mapping between the two schemes is impossible to define.
Some Hebrew forms, particularly the present participle and modal forms, are in-
herently hard to define, and the wide disagreement about their status is reflected in
practically all Hebrew dictionaries. This kind of disagreement naturally appears also
between the KC and TB. See Adler et al (2008b) and Netzer et al (2007) for further
discussion on these two interesting cases.
Bridging the discrepancy between the two resources is an important aspect in the
creation of a successful parsing system. On the one hand the syntactic annotations in the
treebank are needed in order to train the parser, and on the other hand the information
provided by the morphological analyzer is needed in order to provide a good lexical
coverage. We discuss an approach to bridging this discrepancy in Section 6.
130
Goldberg and Elhadad Parsing System for Hebrew
2.4 Section Summary
To summarize, the Hebrew language and its analysis poses several challenges to parser
design: The amount of annotated material is relatively small, precluding the possibility
of learning robust lexical parameters from the annotated corpora. The productive
nature of the morphology results in many word forms, adding another obstacle to
estimating lexical parameters from annotated data. The nature of the word-formation
mechanism in Hebrew makes it hard to guess the morphological analysis of a word
based on its prefix and suffix alone as is done in other languages, requiring the use of a
more complex system for handling unknown words. Many function words in Hebrew
are not separated by white space but are instead attached to the next token, making
the observed word sequence ambiguous. Word segmentation needs to be performed
in addition to syntactic disambiguation. Successful word segmentation may rely on
syntactic disambiguation, suggesting that it is better to perform the segmentation
and syntactic-disambiguation tasks jointly. Finally, Hebrew grammar requires various
forms of morphological agreement, a fact which hopefully can help disambiguate
otherwise ambiguous syntactic structures. The syntactic parser should be able to make
use of agreement information.
In terms of existing resources, Hebrew has a small treebank annotated with con-
stituency structure and a broad-coverage, manually constructed, lexicon-based mor-
phological analyzer. The morphological analyzer is capable of providing the possible
morphological analyses for many lexical forms, and it is extended using a machine-
learning technique to also provide possible analyses for word-forms not covered by
the lexicon. The extended lexicon provides a good lexical coverage of Hebrew. Also
available is a morphological disambiguator that is capable of associating probabilities to
the possible analyses of the lexical forms in the lexicon, and disambiguating the analyses
of a sequence of lexical items in context based on a sequential model. The constituency
treebank can be used to learn the parameters of a syntactic-model of Hebrew, and
the morphological analyzer can be used to provide broad-coverage lexical knowledge.
Unfortunately, the treebank and the lexicon/disambiguator follow different annotation
schemes, and are therefore incompatible with each other. The annotation gap between
the two resources must be bridged before they can be used together.
We now turn to survey the components of our Hebrew parsing system.
3. Latent-Annotation State-Split Grammars (PCFG-LA)
Klein and Manning (2003) demonstrated that linguistically informed splitting of non-
terminal symbols in treebank-derived grammars can result in accurate grammars. Their
work triggered investigations in automatic grammar refinement and state-splitting
(Matsuzaki, Miyao, and Tsujii 2005; Prescher 2005), which was then perfected in work
by Petrov and colleagues (Petrov et al 2006; Petrov and Klein 2007; Petrov 2009).
State-split models assume that each non-terminal label has a latent annotation that
should be recovered. Instead of a single NP symbol, these models hypothesize that there
are many different NP symbols, NP1, . . . ,NPk, and each is used in a different context.
The labels are hidden, however, and we can only observe the core category label (NP).
The job of the training process is to come up with the hidden set of label assignments
to non-terminals, such that the resulting grammar assigns a high probability to the
observed treebank data. Such models are called PCFG with latent annotations (PCFG-
LA) and are shown empirically to produce very accurate parsing results.
131
Computational Linguistics Volume 39, Number 1
The model of Petrov et al (2006) and its publicly available implementation, the
BerkeleyParser,11 learns the latent annotations by starting with a bare-bones treebank-
derived grammar and automatically refining it in split-merge-smooth cycles, setting the
parameters using EM. We provide a brief description of the model and learning process
(refer to Petrov et al 2006; Petrov and Klein 2007; Petrov 2009 for the full details).
The learning works by following an iterative split-merge-smooth cycle, in which
the following steps are performed repetitively:
Splitting each non-terminal category in two All of the grammar symbols are split. In
the first round, NP is split into NP1 and NP2. In the second round these are
split into NP11, NP12, NP21, NP22, and so forth. Each splitting round results in
new grammar in which a rule of the form A ? BC is replaced by eight rules, the
result of splitting each A, B, and C in two. An EM procedure is then used to set
the probabilities of each of the split rules. The EM training is constrained by the
grammar on the one hand and by the annotated tree structures on the other.
Merging back non-effective splits Not all of the splits are useful. For example, the
punctuation POS tag will always result in punctuation, and there is no reason
to split it into two punctuation POS tags. Having a grammar with too many states
is difficult to manage in terms of memory, storage, and parsing time, and is also
prone to overfitting the data. Thus, the model aims to undo splits if they are not
useful. The splits are evaluated based on an information gain criteria, and splits
that are not useful are merged back into their parent symbol, resulting in a smaller
grammar (if the symbols B1 and B2 are merged back into B, the rules A ? B1 C
and A ? B2 C are merged into A ? B C). The merging step is also followed by an
EM procedure for setting the rule probabilities for the resulting grammar.
Smoothing the split non-terminals toward their shared ancestor Finally, split sym-
bols may still share some information (although an NP in subject position and
an NP in object position behave differently, they also retain some common prop-
erties). The smoothing procedure joggles the probability mass of the grammar
and moves some probability from the split symbol to its parent. This step is also
followed by parameter re-estimation using EM.
Performing five or six such split-merge-smooth cycles results in accurate grammars,
with annotations that capture many latent syntactic interactions. Six cycles mean that
symbols can have as many as 64 different substates.
At inference time, the latent annotations are (approximately) marginalized out,
resulting in the (approximate) most probable unannotated tree according to the refined
grammar (the score of the unsplit rule A ? B C is taken to be
?
x
?
y
?
zAx ? By Cz).
The grammar learning process is applied to binarized parse trees, with first-order
vertical and zeroth-order horizontal markovization (Klein and Manning 2003). This
means that in the initial grammar, each of the non-terminal symbols is effectively
conditioned on its parent alone, and is independent of its sisters. For example, the rule
S ? NP VP NP PP is binarized as:
S ? NP @S
@S ? VP @S
@S ? NP @S
@S ? PP
11 http://code.google.com/p/berkeleyparser/.
132
Goldberg and Elhadad Parsing System for Hebrew
indicating that S rules start with an NP, can be followed by a sequence of zero or
more NPs and VPs, and end with a PP. Such an extreme markovization suggests a
very strong independence assumption, and is too permissive on its own. It allows the
resulting refined grammar to encode its own set of dependencies between a node and
its sisters, however, as well as ordering preferences in long, flat rules. For example,
the binarized grammar allows the production S ? NP NP PP, which may be incorrect.
However, by annotating the symbols as follows:
S ? NP @S1
@S1 ? VP @S2
@S2 ? NP @S2
@S2 ? PP
the grammar now forces the VP to be produced before the NP, but still allows the NP to
be dropped. Similarly, by annotating the symbols as:
S ? NP @S1
@S1 ? VP @S2
@S2 ? NP @S3
@S3 ? PP
the grammar effectively allows only the original rule to be produced.
Initial experiments on Hebrew confirm that moving to higher order horizontal
markovization (encoding more context in the initial binarized rules) degrades parsing
performance, while producing much larger grammars.
The PCFG-LA parsing methodology is very robust, producing state-of-the-art accu-
racies for English, as well as many other languages including German (Petrov and Klein
2008), French (Candito, Crabbe?, and Seddah 2009), and Chinese (Huang and Harper
2009).
4. Baseline Experiments
The baseline system is an ?out-of-the-box? PCFG-LA parser, as described in Petrov
et al (2006) and Petrov and Klein (2007) and implemented in the BerkeleyParser.12
The parser is trained on the Modern Hebrew Treebank (see Section 9 for the exact
experimental settings) after stripping all the functional and morphological information
from the non-terminals.
We evaluate the resulting models on the development set, and consider three
settings:
Seg+POS Oracle: The parser has access to the gold segmentation and POS tags.
Seg Oracle: The parser has access to the gold segmentation, but not the POS tags.
Pipeline: A POS-tagger is used to perform word segmentation, which is then used as
parser input.
A better tag set. Glossing over the parses revealed that the parser failed to learn
the distinction between finite and non-finite verbs. The importance of this linguistic
12 http://code.google.com/p/berkeleyparser/.
133
Computational Linguistics Volume 39, Number 1
Table 1
Baseline: Out-of-the-box BerkeleyParser performance on the dev-set.
Setting Tag set F1 (4 cycles) F1 (5 cycles)
Seg+POS Oracle Core 89.7 89.5
Seg Oracle Core 82.6 83.6
Pipeline Core 76.3 77.2
Seg+POS Oracle Core+Verbs 89.9 90.9
Seg Oracle Core+Verbs 83.3 83.6
Pipeline Core+Verbs 77.1 77.3
distinction for parsing is obvious, and was also noted in Klein and Manning (2003) for
English and in our previous work on parsing Hebrew (Goldberg and Tsarfaty 2008).
Finite and non-finite verbs are easily distinguishable from each other based on surface
form alone. Although finiteness is clearly annotated in the treebank, it is not on the
?core? part of the POS tags and was removed prior to training the parser. In a second
set of experiments the core tag set of the parser was modified to distinguish finite verbs,
infinitives, and modals.13 The original core?tag set aleady includes some important
distinctions, such as construct from non-construct nouns.
Results and discussion. Table 1 presents the parsing results on the development set. With
gold POS tags and segmentation, the results are very high. Accuracy drops considerably
when the parser is not given access to the gold tags (from about 90 to less than 84 F1),
indicating that the POS tags are both informative and ambiguous. Results drop even
further (from 84 to 77) in the pipeline case where the gold segmentation is not available,
indicating that correct segmentation also provides valuable information to the parser
and that segmentation mistakes are costly.
Enriching the tag set to distinguish modals and finite and infinite verbs proved
useful, with an increase of about 1 F1 points (absolute) after four split-merge-smooth
cycles, and a smaller increase after five cycles. This stresses the importance of the core
representation: The automatic learning procedure goes a long way, but it can be aided
by linguistically motivated manual interventions in some cases.
4.1 Analyzing the Learned PCFG-LA Grammar
4.1.1 Terminal-Level (Lexical) Splits. We begin by inspecting the splits at the part-of-
speech level. Table 2 displays the number of splits learned for each of the parts-of-speech
symbols. Prepositions are the most heavily split, followed closely by the somewhat-
generic MOD tag and the nouns.
Nouns and adjectives. The noun and adjective splits are somewhat hard to decipher.
Some of the groups are obvious (things appearing after numbers, last names, parts-of-dates,
time related, places, etc.). Others are are much harder to interpret.
13 Unlike previous work, the distinction is retained only at the POS tag level and not propagated to the
phrase level. The tag-level information is sufficient for the parser to learn the phrase-level distinctions on
its own. Similar observations regarding the usefulness and sufficiency of linguistically motivated manual
state-splitting of preterminals (as opposed to tree-internal nodes) prior to training a latent-variable
grammar were also made by Crabbe? and Candito (2008).
134
Goldberg and Elhadad Parsing System for Hebrew
Table 2
Number of learned splits per POS category after five split-merge cycles.
Tag # Splits Tag # Splits
H 1 CDT 6
HAM 1 CC 7
POS 1 DT 7
REL 1 JJ 7
VB 1 VB-INF 7
AT 2 PRP 8
COM 2 CD 10
JJT 2 RB 13
QW 2 NN 16
RBR 2 NNP 17
VBMD 2 NNT 22
WDT 2 MOD 24
AGR 4 IN 26
AUX 6
MOD. For the general-modification POS tags, most categories clearly single out one
or two words with very specific usage patterns, such as !?? (?no?), !?? (?also?), !?? (?only?),
!?????? (?even?), !????? (?former?), and so forth. The other categories are harder to interpret.
Verbs. Finite-verbs are not split at all, even though they form an open-class category.
Modal verbs are split into two groups: One of them is dominated by nine modals
( !?????, !??, !?????, !???, !???, !?????, !?????, !????, !??????, roughly corresponding to the English could,
should, seem/appear, hard, shouldn?t, possible, appear/seem, important, fitting/required); and
the second contains all the others. This is an interesting distinction, as the nine singled-
out modals never take a subject, whereas the modals in the other group do.14 Infinitive
verbs are split into seven categories, six of which are dominated by one or two words
each, and the last is a catch-all category.
Coordination and question-words. Coordination words are heavily split, each of the
categories dominated by one or two words, indicating different usage patterns. The
question words !??? (?what?) and !??? (?who?) are singled out from the rest.
Gender/number agreement. The verbs are not split at all, indicating that the learned
grammar cannot model subject?verb agreement. Pronouns are split by type (personals,
demonstrative, and subtypes of demonstratives), but not by gender and number. Noun
and adjective splits are sometimes hard to decipher, but they do not exhibit any group-
ing based on gender or number properties, indicating that the grammar cannot model
adjective?noun agreement. Category splits for the AGR tag do show a clear division
that follows gender and number, but it is unclear what is captured by this division as
the information cannot interact with nouns, adjectives, verbs, or pronouns.
14 In fact, the nine modals are very similar in characterization to the words identified in Netzer et al (2007)
as modals, whereas many of the modals in the other group are not necessarily considered as modal
outside of the treebank guidelines.
135
Computational Linguistics Volume 39, Number 1
Table 3
Number of learned splits per NT-category after five split-merge cycles.
Tag # Splits Tag # Splits
FRAGQ 1 ADVP 16
INTJ 6 S 16
FRAG 7 PP 22
SQ 7 VP 22
PRN 8 PREDP 25
ADJP 14 NP 32
SBAR 14
4.1.2 Grammar-Level Splits. Table 3 shows the number of splits learned for each gram-
mar non-terminal. The NP category is the most heavily split, followed by predicative
phrases, verb phrases, and PPs. With the exception of the FRAGQ category, all symbols
are split into at least six substates. What information is encapsulated in the state splits?
As noted by Petrov et al (2006), the latent state-splits learned for the grammar symbols
are harder to analyze.
One way of shedding some light on the meanings of the split-states is by using the
grammar in generationmode and by samplingword sequences from each of the states.15
By looking at the resulting strings, one can sometimes infer the kind of information
encoded in the grammar.
NP. The split-NPs encode phrase length (some splits result in very long NPs, some
in very short, some in very specific one- or two-word patterns). They also encode the
definiteness rules (either an NP is definite or not), the interaction between definiteness
and the AT marker, and a limited interaction between definiteness and construct nouns.
Other NP splits are dedicated to pronouns or to question words, or encode proper
names, monetary units, and numbers.
SBAR. The split-SBARs are split according to the word introducing the SBAR. In
addition, some split-SBARs encode quoted and parenthetical items.
S. The split-Ss differ by length. In addition, some S splits seem to be modeling verb-less
sentences, variations in word order, and sentence-level coordination.
4.2 Limitation of PCFG-LA Parsing of Modern Hebrew
The PCFG-LA baseline is a strong one, and is substantially higher than all previous
reported results for Hebrew parsing in each of the setups (Seg+POS oracle, Seg Oracle,
and no Oracle). We also identify some of its limitations, namely:
Missed splits. The learning procedure is not perfect, and fails to capture some linguis-
tically meaningful state-splits. When such splits are manually supplied (i.e., the trivial
split of verbal types) accuracy improves.
15 Sampling a word sequence is performed by starting at a given state (a split grammar symbol), randomly
choosing a right-hand-side based on the PCFG-induced distribution, expanding the state into the chosen
right-hand side, and continuing recursively until we are left with only strings.
136
Goldberg and Elhadad Parsing System for Hebrew
Sensitivity to non-gold POS. The substantial drop in accuracy when the POS tags are
unobserved and need to be predicted is staggering, which suggests that it is difficult
for the parser to assign part-of-speech tags. Of the 698 part-of-speech errors, 314 are on
words not seen in training.
Sensitivity to non-gold segmentation. The accuracy drops even further when the parser
is presented with predicted segmentation. Segmentation errors are detrimental to the
parser.
Not encoding grammatical agreement. Finally, the learned grammar does not encode
grammatical agreement. Whereas the majority of the parser mistakes are due to the
flexible constituent order or ?standard? ambiguities such as coordination and PP
attachment, a handful of them could be resolved using agreement information.
In what follows, we address these four limitations, and substantially increase the
parser accuracy for the realistic case where gold segmentation and POS tags are not
available.
5. Manual State-Splits
We experimented with several linguistically motivated state-splits which were added as
tree-annotations prior to running the parser. Most of them did not help on their own and
slightly degraded parser performance when combined with other splits. These include
splits which were proven useful in previous work, such as marking of definite NPs, and
distinguishing possessive from other PPs. We also experimented with splits based on
morphological agreement features, which are discussed in Section 8.1.
Overall, the learning procedure is capable of producing good splits on its own. We
did, however, manage to improve upon it with the following annotation (the annota-
tions were removed prior to evaluation).
Subject NPs. Hebrew phrase order is rather flexible, and the subject can appear before
or after the verb. Identifying the subject can thus help in grounding the overall structure
of the sentence. The subject is also dependent on agreement constraints with the verb.
Following Johnson (1998), Klein and Manning (2003) implicitly annotate subject-NPs
in English using parent annotation (distinguishing NPs under S from other NPs), with
good results. When applied to English, the PCFG-LA also learns to model subject NPs
well. Hebrew?s non-configurationality, however, put both Subjects and Objects directly
under S, making it much harder to learn the distinction automatically.
Explicit marking of subject NPs contributes slightly to the accuracy of the parser.
Perhapsmore important than the small increase in accuracy is the fact that the parser can
identify subjects relatively well. In contrast, marking of object NPs did not help by itself
and slightly degraded the parsing accuracy when combined with other annotations.
Note, however, that Hebrew definite objects are already clearly marked using the !??
marker, making them an easy target for the parser.
6. Better Lexical Coverage with an External Lexicon
The drop in parsing accuracy when gold core POS tags are not available and need to be
inferred by the parser is huge (from above 90 to less than 84 F1).
137
Computational Linguistics Volume 39, Number 1
The large number of possible word forms make it very difficult for manually annotated
corpora to provide adequate lexical coverage. The problem is even more severe with
the case of the Hebrew Treebank, which is especially small. Although it is big enough to
learn meaningful syntactic generalizations (as demonstrated by the high performance
of the baseline system) it is far too small to learn a good lexical model (as evidenced by
the drop in accuracy when gold tags are not available).
We suggest increasing the lexical coverage of the parser using an external resource,
namely, a lexicon-based morphological analyzer. We further extend the utility of the
analyzer with lexical tagging probabilities learned from an unannotated corpus.
6.1 A Unified Lexical Probability Model
We would like to use the KC Analyzer (Section 2.3.3) to increase the lexical coverage
of the treebank-trained parser. That is, we would like to improve the lexical model
P(T ? W) of the generative parser. As discussed in Section 2.3.5, however, the tag sets
used by the two resources differ. How can this difference be reconciled?
One possibility is to re-tag the treebank with the KC tag set and then train on this
unified resource. In Goldberg et al (2009), we show that this procedure degrades parser
performance. Instead, Goldberg et al suggest a layered generative approach that retains
the benefits of the treebank tagging for frequent words and resorts to the KC tag set only
for rare and unseen words. Under this approach, frequent words are generated from
treebank POS tags as usual, but rare words follow a generative process in which first
the treebank tag generates a KC tag, and then the KC-tag generates the word. A sample
derivation using this layered representation is presented in Figure 1.
The Treebank-to-KC tag generation probabilities represent a fuzzy, probabilistic
mapping between the two resources. In Goldberg et al (2009), the estimation of these
probabilities was done based on a re-tagging of the treebank to use the KC tag set.
The re-tagging process was far from trivial, and many tagging cases required extensive
debates between human annotators.
Here, we present a new procedure which does not require the treebank to be re-
tagged with a new tag set. It still uses the layered representation, but instead of forcing
one unique KC analysis for each location, it embraces the uncertainty and allows all of
them. This is done by treating the KC-tag assignments as hidden variables, learning
the TB-KC mapping probabilities as part of the grammar training EM process, and
marginalizing the KC tags out for the final tree. The procedure is based on the following
assumptions:
r We have access to trees in which the POS tags ttb are taken from a given tag
set TTB.
...
JJTB
PRP-M-S-3-DEMExt
!??
Figure 1
A layered POS tag representation.
138
Goldberg and Elhadad Parsing System for Hebrew
.
.
.
NNTB
NN-M-SExt NNT-M-SExt VB-M-S-3-PastExt VB-M-S-3-ImpExt
!???
Figure 2
A latent layered POS tag representation.
r We have additional access to an external resource (lexicon) mapping
words to tags text from a different tag set TExt.
r Probabilities involving words which are frequent in the treebank can and
should be based on treebank counts.
r Probabilities involving less frequent words should be smoothed in with
information from the external lexicon.
r Smoothing should have a greater effect on less-frequent words.
r Probabilities for unseen words should be based solely on the external
lexicon.
Figure 2 illustrates the representation used for words which are rare or unseen in the
treebank training data. The treebank tag NNTB (upper level) generates the word-form
!??? (lower level) by considering all the possible KC POS tags allowed for the word in
the morphological analyzer (the middle level). The probabilities related to generating
the KC POS tags are summed, and all the other probabilities are multiplied. The exact
equations are detailed in the following.
Although the needed quantity is the emission probability P(TTB ? W) = P(W|TTB),
it is more convenient (for a reason which will be discussed later) to work with the
tagging probability P(TTB|W). Once the tagging probabilites P(TTB|W) are available,
they can easily be converted to emission probabilities using Bayesian inversion, based
on the relative-frequency estimates of P(W) and P(TTB) which are calculated from the
treebank:16
P(ttb|w)P(w)
P(ttb)
= P(w|ttb) = P(ttb ? w) (1)
16 Our notation uses capital letters to denote random variables, and lower-case letters to denote specific
events. Thus, P(T|W) refers the distributions in which a tag ? T is condition on a word ? W, P(T|w) refers
to the conditional distribution of tags t ? T given a specific word w, and P(t|w) refers to the probability
mass of the specific tag t given word w.
139
Computational Linguistics Volume 39, Number 1
Let us now focus on estimating the tagging probabilities P(TTB|W) for the cases of
frequent, rare, and OOV words.
For frequent words that are seen more than K times in the treebank, we simply use
treebank-based relative-frequency estimates:17
Ptb(ttb|w) =
c(w, ttb)
c(w) (2)
where c(?) is a counting function.
For OOV words that are not seen in the treebank, the tagging probability is estimated
using:
Poov(ttb|w) =
?
text?TExt
P(text|w)P(ttb|text) (3)
where P(TExt|W) is a tagging probability using the external tag set, and P(TTB|TExt) is
a transfer probability relating the tags from the two tag sets (the estimation of these
two probabilities is discussed subsequently). What this does is assume a process in
which the word is tagged by first choosing a tag according to the external lexicon, and
then choosing a tag from the TB tag set based on the external one. The external tag
assignments are then treated as latent variables, and are marginalized out.
Finally, for rare words that are seen only a few times in the treebank, we interpolate the
two quantities, weighted by the word?s frequency in the treebank:
Prare(ttb|w) =
c(w)Ptb(ttb|w)+ Poov(ttb|w)
1+ c(w) (4)
We now turn to describing the estimation of the external tagging probability P(TExt|W)
and the tag transfer probability P(TTB|TExt).
Estimating P(TExt|W). The tagging probability follows the morphological analyzer.
The analyzer provides the possible analyses, but does not provide probabilities for
them. One simple option would be to assign each possible analysis (tag) a uniform
probability, and assign 0 probability for tags not allowed by the lexicon for the given
word. This method is referred to as Punif(TExt|W). We know that not all the possible
analyses for a given word are equally likely, however, and in practice, the actual
tagging distribution is usually biased toward one or two of the tags. These tagging
preferences can be learned in an unsupervised manner given the lexicon and a large
corpus of unannotated text, using EM training of an HMM tagging model. Adler and
Elhadad (2006) suggest such a model for accurate tagging of Hebrew, and Adler (2007)
and Goldberg, Adler, and Elhadad (2008) extend it to provide state-of-the-art tagging
accuracies for Hebrew using a smart initialization. Here, we use the pseudo-counts from
17 In practice, a small amount of smoothing is added to allow tagging a word with open-class tags if it
wasn?t seen within the treebank: Ptb(ttb|w) = (c(w, ttb )+ 0.0001 ? P(ttb ))/(c(w)+ 0.0001).
140
Goldberg and Elhadad Parsing System for Hebrew
the final round of EM training in this tagging model in order to compute Pem(TExt|W).
We show in Section 9 that this unsupervised lexical probabilities estimation does
indeed provide better parsing results.
Estimating P(TTB|TExt). The tagset-transfer probabilities capture the patterns of transfer
between the syntactic tagging scheme of the treebank and the other tagging scheme
of the external resource. They are estimated using treebank counts and the tagging
distribution P(TExt|W):
P(ttb|text) =
c(ttb, text)
c(text)
=
?
w c(ttb,w)P(text|w)
?
w c(w)P(text|w)
(5)
Integration into the PCFG-LA model. The estimation procedure is incorporated into the
training process of the PCFG-LA model. Note that in the PCFG-LA model the treebank
tag set TTB is gradually split, and each tag takes the form ?tag, substate?, where substate
is a latent variable indicating a specific split of the given tag. This means that the
treebank tagging probability and the tag set?transfer probabilities are also defined over
these split tags. Whereas the external tagging probabilities P(TExt|W) are fixed prior to
PCFG-LA training, the other distributions (P(TTBsubstate |W) and P(TTBsubstate |TExt)) are re-
estimated in the EM process following each of the split, merge, and smooth stages. This
is done by replacing the corpus counts c(?) in Equations (2) and (5) with pseudo-counts
(expectations, marginal scores) of the same events in the E step of the EM procedure.
The main reason for using the Bayesian inversion (Equation (1)) instead of working
with the emission probability P(W|T) directly is that the emission probability is
highly dependent on the vocabulary size. The treebank estimates are based on a small
vocabulary, the external lexicon estimates are based on a very large vocabulary, and
a proper combination of the two emission probabilities is not trivial. In contrast, the
tagging probabilities do not depend on the vocabulary size, allowing a very simple
combination. We can then base the counts for the emission probability on the treebank
vocabulary alone, and estimate P(W) for words unseen in training as if they were seen
once.
7. Joint Segmentation and Parsing
When applied to real text (for which the gold word-segmentation is not available), the
baseline PCFG-LA parser is supplied with word segmentation produced by a separate
tagging process.18 This seriously degrades parsing performance. A major reason for the
performance drop is that the word-segmentation task and the syntactic-disambiguation
task are highly related. Segmentation mistakes drive the parser toward wrong syntactic
structures, and many segmentation decisions require long-distance information that is
not available to a sequential process (Tsarfaty 2006a). For these reasons, we claim that
parsing and segmentation should be performed jointly.
18 Although the tagger also produces POS tag assignments, we ignore them and use only the word
segmentation. This is done for two reasons: first, the tag set of the tagger is the one used by the
morphological analyzer, and is not compatible with the treebank. Second, we believe it is better for
the parser to produce its own tag assignments.
141
Computational Linguistics Volume 39, Number 1
Figure 3
The lattice for the Hebrew sequence !?????? ????? (see footnote 19).
Joint segmentation and parsing can be achieved using lattice parsing. Instead of
parsing over a fixed input string, the parser operates on a lattice?a structure encoding
all the possible segmentations.
7.1 Lattice Representation
Formally, a lattice is a directed acyclic graph in which all paths lead from the initial state
to the end state.
For the Hebrew segmentation task, all word segmentations of a given sentence are
represented using a lattice structure. Each lattice arc corresponds to a word and its
corresponding POS tag, and a path through the lattice corresponds to a specific word-
segmentation and POS tagging of the sentence. This is by now a fairly standard repre-
sentation for multiple morphological segmentations of Hebrew utterances (Adler 2001;
Bar-Haim, Sima?an, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg,
Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011). It
is also used for Arabic (Green and Manning 2010) and other languages (Smith, Smith,
and Tromble 2005).
Figure 3 depicts the lattice for the two-words sentence !?????? 19.????? Double-circles
indicate the space-delimited token boundaries. Note that in this construction arcs can
never cross token boundaries. Every token is independent of the others, and the sen-
tence lattice is in fact a concatenation of smaller lattices, one for each token. Further-
more, some of the arcs represent lexemes not present in the input tokens (e.g., !?/DT,
!??/POS), although these are parts of valid analyses of the token. Segments with the same
surface form but different POS tags are treated as different lexemes, and are represented
as separate arcs (e.g., the two arcs labeled !????? from node 6 to 7).
A similar structure is used in speech recognition. There, a lattice is used to represent
the possible sentences resulting from an interpretation of an acoustic model. In speech
recognition the arcs of the lattice are typically weighted in order to indicate the probabil-
ity of specific transitions. Given that weights on all outgoing arcs sum up to one, weights
induce a probability distribution on the lattice paths. In sequential tagging models such
as Smith, Smith, and Tromble (2005), Adler and Elhadad (2006), and Bar-Haim, Sima?an,
and Winter (2008) weights are assigned according to a tagging model based on linear
context. For the case of parsing, context-free weighting of lattice arcs is used: each arc
19 Whereas Hebrew is written right-to-left, the lattice is to be read left-to-right. The words on each arc
follow the Hebrew writing directions, and are written right-to-left.
142
Goldberg and Elhadad Parsing System for Hebrew
corresponds to a ?tag,word? pair, and is weighted according to the emission distribution
P(tag ? word).20
7.2 Lattice Parsing
The CKY parsing algorithm can be extended to accept a lattice, instead of a predefined
list of tokens, as its input (Chappelier et al 1999). The CKY search then finds a tree
spanning from the start-state to the end-state of the lattice, where the leaves of the tree
are lattice arcs. The lattice extension of the CKY algorithm is performed by indexing
lexical items according to their start- and end-states in the lattice instead of by their
sentence position, and changing the initialization procedure of CKY to allow terminal
and preterminal symbols of spans of sizes > 1. It is then relatively straightforward to
modify the parsing mechanism to support this change: not giving special treatments
for spans of size 1, and distinguishing lexical items from non-terminals by a specified
marking instead of by their position in the chart.
Figure 4 shows the CKY chart for the lattice in Figure 3, together with an (incorrect)
parse over the lattice. The chart is initialized with parts of speech corresponding to
the lattice arcs. Phrase-structures are then built on top of the POS tags (in blue). The
proposed structure must span the entire chart, and correspond to a path through the
lattice from the initial state (0) to the last one (7).
At training time the correct segmentation is fully observed, and the generative
parser is trained as usual over the treebank. At inference (test) time, the correct seg-
mentation is unknown, and the decoding is applied to the segmentation lattice. The best
derivation returned by the parser forces a specific segmentation. The returned parse tree
is the most probable ?segmentation, tree? pair according to the grammar.21 We modified
the PCFG-LA BerkeleyParser to accept lattice input at inference time.
Lattice parsing allows us to preserve the segmentation ambiguity and present it
to the parser, instead of committing to a specific segmentation prior to parsing. This
way segmentation decisions are performed in the parser as part of the global search
for the most probable structure, and can be affected by global syntactic considera-
tions. We show in Section 9 that this methodology is indeed superior to the pipeline
approach.
Early descriptions of algorithms for parsing over word lattices can be found in
Lang (1974, 1988) and Billott and Lang (1989). Lattice parsing was explored in the
context of parsing of speech signals by Chappelier et al (1999), Sima?an (1999), and
Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation
in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning
(2010).
20 Lattice parsing for Hebrew is explored also in Cohen and Smith (2007). There, lattice arc weights
are assigned based on aggregate quantities (forward-backward tagging marginals) derived from a
discriminative CRF tagging model. This approach is not ideal from a modeling perspective, as it makes
each POS tag be accounted for twice: once by the syntactic model, and once by the sequential one.
In this work, a sequential tagging model is not used at all. If the use of a sequential model is desired,
an alternative method for integrating a sequence model and a syntactic model is making the models
?negotiate? an agreed upon structure that maximizes the score under both models, using optimization
techniques such as dual decomposition (Dantzig and Wolfe 1960), which was recently introduced into
natural language processing (Rush et al 2010).
21 Note that finding the most probable segmentation requires summing over all the trees resulting in each
segmentation?a much harder task, proven to be NP-complete in Sima?an (1996).
143
Computational Linguistics Volume 39, Number 1
Figure 4
Lattice initialization of the CKY chart.
8. Incorporating Morphological Agreement
Inspecting the learned grammars reveal that they do not encode any knowledge of
morphological agreement: The split categories for nouns, verbs, and adjectives do not
group words according to any relevant morphological property such as gender or
number, making it impossible for the grammar to model agreement patterns. At the
same time, inspecting some of the bad parses reveals several clear cases of agreement
mistakes. Can morphological agreement be incorporated in the parsing model?
8.1 Forcing Morphologically Motivated Splits
Our initial attempts focused on making the PCFG-LA learning procedure pick up on
agreement-relevant state-splits. When neither the core tag set nor the non-terminals
encode gender and number information, it is very hard for the parser to pick up on
agreement patterns.22
We attempted to train the parser on trees which mark the agreement features (either
the gender, the number, or both) either on the POS tags, the relevant constituents, or
22 In the external lexicon case, the external lexicon tags do encode the morphological features, making
it possible in principle for the parser to learn to map certain substates to certain agreement features.
This did not happen in practice, arguably because other structural factors were more powerful than
the agreement ones.
144
Goldberg and Elhadad Parsing System for Hebrew
both. Annotating agreement features on the POS tag?level made the parsing much
slower, but did make the parser assign certain split categories to certain gender?number
combinations, and sampling utterances from the learned grammar did indicate a notion
of grammatical agreement. This did not improve parsing accuracy, however?and even
slightly degraded it.
When propagating the agreement features and annotating them on the constituent
level, parsing accuracy dropped considerably. When inspecting the learned grammar
we observe that most of the agreement-annotated constituents (e.g., NPMasc,Plural) were
still fully split, indicating that the parser picked on patterns which were orthogonal to
the agreement mechanism. The pre-splitting according to agreement-features properties
caused data sparseness, aided over-fitting, and hurt parsing performance: The smooth-
ing procedure of the BerkeleyParser shares some probability-mass between various
splits of the same symbol, but was not applied in our case (no information flowed
between, for example, NPMasc,Plural and NPMasc,Singular). We attempted to counter this
effect by changing the smoothingmechanism of the BerkeleyParser to share information
also between the manually split symbols. This brought parsing accuracy back to the
initial level, but also caused the parser to, again, not model agreement very well. The
reason for this is clear in hindsight: Morphological agreement is an absolute concept,
not a fuzzy one (things can either agree or not). Smoothing the probabilities between
the different morphology-based split-licensed grammar rules that allow morphological
disagreement, and made the grammar lose its discrimination power. This was then
reinforced by the training process, which picked on other syntactic factors instead, and
further phased out the agreement knowledge.
A note on product-grammars. In recent work, Petrov (2010) showed that a committee of
latent-variable grammars encoding different grammatical preferences can be combined
into a product-grammar that is better than the individual ensemble members. Petrov
created the ensemble by training several PCFG-LA parsers on the same data, but using
different random seeds when initializing the EM starting point. We attempted to cre-
ate a similar ensemble by providing the learning process with different linguistically
motivated tree annotations (with and without encoding agreement features, with and
without encoding definiteness, etc.). The combined parser did increase the performance
level over that of the individual parsers, but an ensemble with the same number of
components that was produced using the random-seeds approach produced far su-
perior results. This reinforces the findings of Petrov (2010) who also reports that the
ensemble creation using random initialization is exceptionally strong and outperforms
other methods of ensemble creation.23
8.2 Agreement as Filter
We now turn to suggest an approach to modeling agreement, which rests on the follow-
ing principles:
r Agreement can be modeled as a set of hard (not probabilistic) constraints.
r Agreement is completely orthogonal to the other aspects of the grammar.
23 The product grammar approach with random seeds works well and is effective for improving the
accuracy of Hebrew parsing. As it is completely orthogonal to the approaches presented in this article,
however, we chose not to discuss it further other than commenting on its applicability.
145
Computational Linguistics Volume 39, Number 1
Based on these principles, we suggest treating agreement as a filter, a device that can
rule out illegal parses. Under the agreement-as-filter framework, we want the parser to
produce the most probable parse according to its grammar and subject to hard agreement
constraints. This approach completely decouples the grammar from the agreement ver-
ification mechanism. The agreement information is not modeled in the grammar and
is not used to guide the search for the best parse. Instead, it is a separate process that
imposes hard constraints on the search space and rules out parts of it completely. That
is, agreement is a part of the parser and not of the grammar. This is similar in spirit to
ideas from constraint-based grammars such as LFG (Falk 2001) and HPSG (Pollard and
Sag 1994), which also model aspects of the syntax as Boolean constraints.
Grammatical agreement is a relation between constituents. The relevant morpho-
logical features are propagated from one of the leaves up to the constituent level.
When constituents are combined to form a larger constituent, their morphological
features are assigned to the newly created constituent according to language-specific
rules (it is possible that different morphological features will be assigned by different
constituents). An agreement violation occurs when two or more constituents assign
conflicting features to their parent.
Implementation. In the implementation, an agreement-verification mechanism is man-
ually constructed (not learned) based on a set of simple, language-dependent rules.
First, we provide a set of rules to propagate the morphological agreement features from
the leaves to the constituents. Then, we specify an additional set of rules to inspect
local tree configuration and identify agreement violations (the Hebrew set of rules is
described later, along with a concrete example). The feature-propagation mechanism
works bottom?up and the agreement verification rules are very local, making it possible
to integrate the filtering mechanism into a bottom?up CKY parsing algorithm (refusing
to complete a constituent if it violates an agreement constraint). We did not pursue this
route for the experiments in this work, however. Instead, we opted for an approximation
in which we take the 100-best trees for each sentence, and choose the first tree that
does not have an agreement violation (this is an approximation because the 100-best
trees may not contain a valid tree, in which case we accept the agreement violation and
choose the first-best tree). The specific details of the Hebrew agreement filter are given
in the appendix.
Verifying the hard-constraint property. We verified that the hard constraint assumption
works and that the agreement verification mechanism is valid by applying the proce-
dure to the gold-standard trees in the training-set and checking that (1) the propagated
features agree with the manually marked ones, and (2) none of the training-set trees
were filtered due to agreement violation. We did find a few cases in which the prop-
agated features disagreed with the manually marked ones, and a few gold-standard
trees that the mechanism marked as containing an agreement violation. All of these
cases were due to mistakes in the manual annotation.
Connections to parse-reranking. Our implementation is similar to parse-reranking
(Charniak and Johnson 2005; Collins and Koo 2005). Indeed, if we were to model
agreement as soft constraints, we could have incorporated this information as features
in a reranking model. The filter approach differs in that it poses hard constraints and
not soft ones, pruning away parts of the search space entirely. Thus, the use of k-best
list is merely a technical detail in our implementation?the agreement information is
146
Goldberg and Elhadad Parsing System for Hebrew
easily decomposable and the hard constraints can be efficiently incorporated into the
CKY search procedure.
9. Evaluation and Results
Data set. For all the experiments we use Version 2 of the Hebrew Treebank (Guthmann
et al 2009), with the established test-train-dev splits: Sentences 484?5,740 are used for
training, sentences 1?483 are the development set, and sentences 5,741?6,220 are used
for the final test set.
Evaluation Measure. In the cases where the gold segmentation is given, we use the well-
known evalb F1 score. Namely, each tree is treated as a set of labeled constituents.24
Each constituent is represented as a 3-tuple ?i, j,L?, in which i and j are the indices of
the first and the last words in the constituent, respectively, and L is the constituency
label. For example, (2, 4,NP) indicates an NP spanning from word 2 to word 4. The
performance of a parser is evaluated based on the amount of constituents it recovered
correctly. Let G denote the set of constituents in a gold-standard constituency tree, and
P denote the set of constituents in a predicted tree. Precision (P), recall (R), and F1 are
defined as:
precision = |G ? P||P| recall =
|G ? P|
|G|
F1 = 21
precision +
1
recall
F1 ranges from 0 to 1, and it is 1 iff both precision and recall are 1, indicating the trees
are identical. We report numbers in precentages rather than fractions.
When measuring the performance of models in which the token-segmentation is
predicted and can contradict the gold-standard, a generalization of these measures is
used. Instead of representing a constituent by a triplet ?i, j,L?, each constituent is repre-
sented by a pair containing the concatenation of the words at its yield, and its label L.
This measure was suggested by Tsarfaty (2006a) and used in subsequent work (Tsarfaty
2006b; Goldberg and Tsarfaty 2008; Goldberg et al 2009; Goldberg and Elhadad 2011).
This is equivalent to reassigning the i and j indices to represent character positions
instead of word numbers. When the yields of the gold standard and the predicted trees
are the same, this is equivalent to the standard evaluation measure using the ?i, j,L?
triplets of word indices and a label, and it will produce the same precision, recall, and
F1 as above.
Effect of external lexicon. We start by evaluating the effect of extending the parser?s lexical
model with an external lexicon, as described in Section 6.1. The rare-word threshold
is set to 100. We use the morphological analyzer described in Section 2.3.3. We test
two conditions: UNIFORM, in which the P(Text|w) distribution is uniform over all the
24 This assumes unary-chains do not contain cycles.
147
Computational Linguistics Volume 39, Number 1
Table 4
Dev-set results when incorporating an external lexicon.
Setting Ext-Lexicon/Probs F1 (4 cycles) F1 (5 cycles)
Seg Oracle NONE 83.13 83.39
Pipeline NONE 75.98 76.65
Seg Oracle UNIFORM 84.92 84.56
Pipeline UNIFORM 77.53 77.35
Seg Oracle HMM-BASED 86.17 85.79
Pipeline HMM-BASED 78.75 78.78
analyses suggested by the morphological analyzer for the word, and HMM-BASED in
which the P(Text|w) distribution is based on pseudo-counts from the final round of EM?
HMM training of the semi-supervised POS tagger described in Section 2.3.4. Results are
presented in Table 4.
Incorporating the external lexicon helps both in the case where the correct segmen-
tation is assumed to be known, as well as in the pipeline case where the segmentation is
automatically induced by a sequential tagger. Incorporating the semi-supervised lexical
probabilities learned over large unannotated corpora (HMM-BASED) further improves
the results, up to 86.1 F1 for the gold-segmentation case and 78.7 F1 for the pipeline
case. The pipeline model still lags behind the gold-segmentation case, indicating that
the correct segmentation is very informative for the parser.
Joint segmentation and parsing. Having established that the external lexicon can be effec-
tively incorporated into the parser, we turn to evaluate the method for joint segmenta-
tion and parsing. We follow the same conditions as before (UNIFORM and HMM-BASED
lexical probabilities), but in this set of experiments the parser is allowed to choose its
preferred segmentation using the lattice-parsing methodology presented in Section 7.2.
The lattice is constructed according to the analyses licensed by the morphological
analyzer. Table 5 lists the results. Lattice parsing is effective, leading to an improvement
of about 2?3 F1 points over the pipeline model.
Agreement filter. We now turn to add the agreement filtering on top of the lexicon-
enhanced models. In this setting, the model outputs its 100-best trees for each sentence,
agreement features are propagated, and agreement violations are checked as described
Table 5
Dev-set results when using lattice parsing on top of an external lexicon/analyzer.
Setting Ext-Lexicon/Probs F1 (4 cycles) F1 (5 cycles)
Pipeline UNIFORM 77.53 77.35
Lattice (Joint) UNIFORM 80.35 80.31
Pipeline HMM-BASED 78.75 78.78
Lattice (Joint) HMM-BASED 80.91 80.46
148
Goldberg and Elhadad Parsing System for Hebrew
Table 6
Dev-set results of using the agreement-filter on top of the lexicon-enhanced parser (starting from
gold segmentation).
Setting Ext-Lexicon/Probs F1 (4 cycles) F1 (5 cycles)
No Agreement UNIFORM 84.92 84.56
Agreement as Filter UNIFORM 85.30 84.52
No Agreement HMM-BASED 86.17 85.79
Agreement as Filter HMM-BASED 86.55 86.25
in Section 12, and the first tree that does not contain any agreement violation is returned
as the final parse for the sentence (or the first-best tree in case that all of the output
trees contain an agreement violation). Table 6 lists the results when agreement filtering
is performed on top of parses based on gold segmentation, and Table 7 lists the results
when agreement filtering is performed on top of a lattice-based parsing model that does
not assume gold segmentation is available.
Discussion of agreement filter results. Although the agreement filter does not hurt
the parser performance, the benefits from it are very small. To understand why that
is the case, we analyzed the 1-best parses produced by the 5-cycles-trained grammar on
the gold-segmented development set (these conditions corresponds to the last column
of the third row in Table 6). The analysis revealed the following reasons for the low
impact of the agreement filter: (1) The grammar is strong enough to produce fairly
accurate structures, which have very few agreement mistakes to begin with, and (2)
fixing an agreement mistake does not necessarily mean fixing the entire parse?in some
cases it is very easy for the parser to fix the agreement mistake and still produce an
incorrect parse for other parts of the structure.
The 1-best trees of the 480 sentences of the development set contain 22,500 parse-
tree nodes. Of these 22,500 nodes, 2,368 nodes triggered a gender-agreement check:
about 10% of the parsing decisions could benefit from gender agreement. Of the 2,368
relevant nodes, however, 130 nodes involved conjunctions or possessives, and were
outside of the scope of our agreement verification rules. Of the remaining 2,238 parse-
tree nodes, 2,204 passed the agreement check, and only 34 nodes (1.5% of the relevant
nodes, and 0.15% of the total number of nodes) were flagged as gender-agreement
violations. Similarly for number agreement, 2,244 nodes triggered an agreement check,
of which 2,131 nodes could be handled by our system. Of these relevant nodes, 2,109
nodes passed the gender-agreement check, and only 23 nodes (1.07% of relevant nodes,
Table 7
Dev-set results of using the agreement-filter on top of the lexicon-enhanced lattice parser (parser
does both segmentation and parsing).
Setting Ext-Lexicon/Probs F1 (4 cycles) F1 (5 cycles)
No Agreement UNIFORM 80.35 80.31
Agreement as Filter UNIFORM 80.55 80.74
No Agreement HMM-BASED 80.91 80.46
Agreement as Filter HMM-BASED 81.04 80.72
149
Computational Linguistics Volume 39, Number 1
Table 8
Numbers of parse-tree nodes in the 1-best parses of the development set that triggered gender or
number agreement checks, and the results of these checks.
Gender Agreement Number Agreement
Triggered agreement check 2,368 2,244
Could be handled by the system 2,238 2,131
No agreement violation 2,204 2,109
Agreement violation 34 23
and 0.1% of the total nodes) were flagged as agreement violations. The numbers are
summarized in Table 8. It is clear that the vast majority of the parser decisions are
compatible with the agreement constraints.
Turning to inspect the cases in which the agreement filter caught an agreement
violation, we note that the agreement filter marked 51 of the 480 development sentences
as having an agreement violation in the 1-best parse?about 10% of the sentences could
potentially benefit from the agreement filter. For 38 of the 51 agreement violations, the
agreement violation was fixed in the tree suggested in the 100-best list. We manually
inspected these 51 parse trees, and highlight some the trends we observed. In the
13 cases in which the 100-best list did not contain a fix to the agreement violation,
the cause was usually that the 1-best parse had many mistakes that were not related
to the agreement violation, and diversity in the 100-best list reflected fixes to these
mistakes without affecting the agreement violation. Another cause of error was an erro-
neous agreement mistake due to an omission in the lexicon. Of the 38 fixable agreement
violations, 25 were local to a noun-phrase, 10 were cases of subject?verb agreement,
and the remaining three were either corner-cases or harder to categorize. The subject?
verb agreement violations were handled almost exclusively by keeping the structure
mostly intact and changing the NPSUBJ label to some other closely related label that does
not require verb agreement, usually NP. This is a good strategy for fixing subject-less
sentences (about half of the cases), but it is only a partial fix in case the subject should
be assigned to a different NP (which does not happen in practice) or in case a more
drastic structural change to the parse-structure is needed. In one of the 10 cases, the
subject?verb agreement mistake indeed resulted in a structural change that improved
the overall parse quality. The NP internal agreement violations include many cases of
noun-compound attachments, and some cases involving coordination. The corrections
to the agreement violation were mostly local, and usually resulted in correct structure,
but sometimes introduced new errors. Figure 5 presents some examples of the different
cases. Our overall impression is that for NP internal mistakes the agreement-filtering
method was mostly doing the right thing.
To conclude, the agreement filter is useful in overcoming some errors and providing
better parses, especially with respect to noun-compound construct-state constructions.
Due to the limited number of parsing mistakes involving agreement violations, how-
ever, and because of the local nature of the agreement-violation mistakes, the total effect
of the agreement filter on the final parsing score is small.
10. The Final Model
Finally, we evaluate the best performing model on the test set. Table 9 presents the
results of parsing the test set while incorporating the external lexicon and using the
150
Goldberg and Elhadad Parsing System for Hebrew
Figure 5
NP agreement violations that were caught by the agreement filter system. (a) Noun-compound
case that was correctly handled. (b) Case involving conjunction that was correctly handled.
(c) A case where fixing the agreement violation introduces a PP-attachment mistake.
Table 9
Test-set results of the best-performing models.
Setting Model F1 (4 cycles)
Gold Segmentation HMM-Based External Lexicon 85.67
+ Agreement 85.70
Lattice-parsing HMM-Based External Lexicon 76.87
+ Agreement 76.95
151
Computational Linguistics Volume 39, Number 1
HMM-based probabilities, for a grammar trained for four split-merge iterations. This
grammar is applied both to the gold-segmentation case and to the realistic case where
segmentation and parsing are performed jointly using lattice-parsing. We also test the
effectiveness of the agreement-filter in both situations.
Agreement information does not hurt performance, but contributes very little to the
final accuracy?additionally on the test sentences, the parser makes very few agreement
mistakes to begin with.
Consistent with previous reports (Tsarfaty 2010), the test set is somewhat harder
than the development set. With gold-segmentation, the models achieve accuracies of
85.70% F1. In the realistic scenario in which the segmentation is induced by the parser,
the accuracies are around 76.9% F1. We verified that the HMM-based lexical probabili-
ties also outperform the Uniform probabilities on the test set (the F1 scores when using
uniform lexical probabilities are 84.06 and 76.30 for the gold and induced segmenta-
tions, respectively). These are the best reported results for parsing the test-set of the
Hebrew Treebank.
11. Related Work in Parsing of Morphologically Rich Languages
Coping with unknown words. Several papers show that the handling of unknown words
is a major component to be considered when adapting a parser to a new language.
For example, the work in Attia et al (2010) uses language-specific unknown-word
signatures for several languages based on various indicative prefixes and suffixes, and
Huang and Harper (2009) suggest a Chinese-specific model based on the geometric
average of the emission probabilities of the individual characters in the rare or unknown
word. Another method of coping with lexical sparsity is word clustering. In Candito
and Crabbe? (2009), the authors demonstrate that replacing words by a combination of
a morphological signature and a word-cluster (based on the linear context of a word in
a large unannotated corpus) improves parsing performance for French. The technique
provides more reliable estimates for in-vocabulary words (a given cluster appears more
frequently than the actual word form), and it also increases the known vocabulary:
Unknown words may share a cluster with known words.
Arabic. Arabic is similar to Hebrew in the challenges it presents for automatic pars-
ing. Most early work on constituency parsing of Arabic focused on straightforward
adaptations of Bikel?s parser to Arabic, with little empirical success. Attia et al (2010)
show that parsing accuracies of around 81% F1 can be achieved for Arabic (assuming
gold word segmentation) by using a PCFG-LA parser with Arabic-specific unknown-
word signatures. Recently, Green and Manning (2010) report on an extensive set of
experiments with several kinds of tree annotations and refinements, and report pars-
ing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA
BerkeleyParser, both when assuming gold word segmentation. The work of Green and
Manning also explored the use of lattice-parsing as suggested in Section 7 of this article,
as well as earlier in Goldberg and Tsarfaty (2008) and Cohen and Smith (2007), and
report promising results for joint segmentation and parsing of Arabic (an F1 score of
76% for sentences of up to 70 words). The best reported results for parsing Arabic
when the gold word segmentation is not known, however, are obtained using a pipeline
model in which a tagger and word-segmenter is applied prior to a manually state-split
constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words)
(Green and Manning 2010).
152
Goldberg and Elhadad Parsing System for Hebrew
Hebrew and relational-realizational parsing. Some related work deals directly with con-
stituency parsing of Modern Hebrew. The work of Tsarfaty and Sima?an (2007) experi-
ments with grammar refinement for Hebrew, and shows that annotating definiteness
and accusativity of constituents, together with parent annotation, improves parsing
accuracy when gold word segmentation is available.
The Relational Realizational (RR) line of work presented in Tsarfaty et al (Tsarfaty
and Sima?an 2008; Tsarfaty, Sima?an, and Scha 2009; Tsarfaty and Sima?an 2010; Tsarfaty
2010) handles the constituent-order variation in Hebrew by presenting a separation
between the form and function aspects of the grammar. Briefly, whereas plain treebank-
derived grammars have rules such as S ? NP VP PP NP PP that are applied in a
single step, the RR approach suggests a generative model in which the generation of
flat clausal structures is decomposed into three distinct steps. First, in the projection step,
a non-terminal generates the kinds of its children without specifying their form or the
order between them, using rules of the form S ? {OBJ,SBJ,PRED,COM,Adjunct}@S.
Second, in the configuration step, an order is chosen based on a separate ordering
distribution, using rules of the form
{OBJ,SBJ,PRED,COM,Adjunct}@S ? SBJ@S PRED@S Adj@S OBJ@S COM@S.
Third, in the realization step, each functional element receives a specific form, using rules
of the form SBJ@S ? NP or Adj@S ? PP. The realization rules can encode syntactic
properties that are required by the grammar for the given function?for example, a
rule such as OBJ@S ? NPdef,acc captures the requirement that definite objects in Hebrew
must be marked for accusativity using the !?? marker, and the rest if the generative
process will generate the object NP according to this specified constraint. This kind of
linguistically motivated separation of form and function is shown to produce models
with fewer parameters and result in better parsing accuracies than plain (or head-
driven) PCFGs derived from the same trees.
The relational-realizational model can accommodate agreement information. It is
shown in Tsarfaty and Sima?an (2010) that, given gold-standard POS tags that include
the gender and number information for individual words, RR models enriched with
gender and number agreement information can provide Modern Hebrew parsing ac-
curacies of 84% F1 for sentences of up to 40 words, the highest reported number for
Modern Hebrew parsing based on gold POS tags and word-segmentation by the time
of its publication.
Although the RR framework is well motivated linguistically and appealing aesthet-
ically, in the current work we chose to rely on the extreme markovization employed by
the PCFG-LA BerkeleyParser in order to cope with the constituent order variation, and
tomodel agreement as an external filter that is orthogonal to the grammar. The approach
taken in this article provides state-of-the-art results for Hebrew constituency parsing.
We leave the question of integrating the RR approach with the approach presented here
to future work.
12. Conclusions
We presented experiments on Hebrew Constituency Parsing based on the PCFG-LA
methodology of Petrov et al (2006). The PCFG-LA model performs well out-of-the-box,
especially when the gold POS tags are available to the parser. It is possible to improve
the learned grammar, however, by specifying some manual state-splits, specifically
153
Computational Linguistics Volume 39, Number 1
distinguishing between modal, finite, and infinitive verbs, and explicit marking of
subject-NPs.
Parsing accuracies drop considerably when the gold POS tags are not available, and
drop even further when using non-gold segmentation. A large part of the drop when
the gold POS tags are not available is due to the large percentage of lexical events that
are unseen or seen only a few times in the training set. This drop can be mitigated
by extending the lexical coverage of the parser using an external lexical resource such
as a wide-coverage morphological analyzer for mapping lexical items to their possible
POS tags. The POS-tagging schemes assumed by the treebank and the morphological
analyzer need not be compatible with each other: We present a method for bridging
the POS tags differences between the two resources. The morphological analyzer does
not provide lexical probabilities. Parsing accuracies can be further improved by using
lexical probabilities which are derived in a semi-supervised fashion based on the mor-
phological analyzer and a large corpus of unannotated text.
The correct token-segmentation is very important for achieving high-quality parses,
and when the gold segmentation is not available, parsing results drop considerably.
It is better to let the parser induce its preferred segmentation in interaction with the
parsing process rather than to use a segmentation based on an external sequence model
in a pipeline fashion. The joint induction of both the syntactic structure and the token-
segmentation can be performed by representing the possible segmentations in lattice
structure, and using lattice parsing. Joint parsing and segmentation is shown to outper-
form the pipeline approach. The parsing accuracies with non-gold segmentation are still
far below the accuracies when the gold-segmentation is assumed to be known, however,
and accurate parsing with non-gold segmentation remains a challenging open research
problem.
The learned PCFG-LA grammar is not capable of modeling agreement information.
We considered methods of using morphological agreement information to improve
parsing accuracy. We propose modeling agreement information as a filtering process
that is orthogonal to the grammar used for parsing. The approach works in the sense
that, in contrast to other methods of using agreement information, it does not degrade
parsing accuracy and even improves it slightly. The benefit from the agreement filtering
is small, however: With the strong grammar induced by the PCFG-LA training pro-
cedure, the parser makes very few agreement mistakes to begin with. Modeling mor-
phological agreement is probably more useful in syntactic generation than in syntactic
parsing. We expect the filtering approach we propose to be proven useful for tasks
involving syntactic generation, such as target-side-syntax machine translation into a
morphologically rich language.
Overall, we presented four enhancements to the PCFG-LA mechanism in order
to adapt it to parsing Hebrew: the introduction of manual, linguistically motivated
state-splits; extending the lexical coverage of the parser using an external morpho-
logical analyzer; performing segmentation and parsing jointly using a lattice parser;
and incorporating agreement information in a filtering framework. Together, these
enhancements result in the best published results for Hebrew Constituency Parsing to
date.
Appendix A: The Hebrew Agreement Filter
Hebrew syntax requires agreement in gender, number, and person. The implementation
considers only the gender and number features, which are the most common. Each of
154
Goldberg and Elhadad Parsing System for Hebrew
the features can take one of five values Masculine, Feminine, Both, Unknown, and NA for
Gender, and Singular, Plural, Both, Unknown and NA for Number. Masculine, Feminine,
Singular, and Plural are self-explanatory, and are assigned when the feature value is
obvious. NA means that the feature is irrelevant for the given constituent (adverbs
and PPs do not carry gender or number features). Both and Unknown are assigned
when we are uncertain about the corresponding feature value. Both and Unknown are
identical in the sense that they leave the feature value unspecified, and have the same
effect on the filtering process. From a practical perspective they could be collapsed
into the same category. We chose to maintain the distinction between the two cases
because they have slightly different semantics. Both indicates that both options are
possible (for example, the form !????? is ambiguous between the plural girls and the
singular childhood, and the titular !??, Dr. can refer both to males and females), whereas
Unknown means that the feature value could not be computed due to a limitation
of the model (for example, there is no clear rule as to the gender of a conjunction
which coordinates masculine and feminine NPs, and we are currently unable to accu-
rately infer the gender and number associated with certain complex quantifiers such
as !??? (most). Compare: !????? ?????? ???, !??????? ????? ???, !????? ??? ?????? (?most of
the classfem stayedmasc, most of the cakefem was eatenfem, most of the cakefem was
eatenmasc?).
Feature values are said to agree if they are compatible with each other. Feminine is
compatible with NA, Both, and Unknown but not with Masculine. Similarly, Singular is
compatible with NA, Both, and Unknown, but not with Plural.
Agreement cases. The system is designed to handle the following cases of morphological
agreement:
NP level agreement between nouns and adjectives. !?????? ?????? ??????? ???? (?box-ofSg
applesPl greenPl bigPl?) , !???? ?????? ??????? ???? (?box-ofSg applesPl greenPl bigSg?)
S level agreement between subject and verbs. !??? ?????? ??? (?[one-of the-kids]Sg
walkedSg?)
Predicative agreement between the subject, ADJP, and copular element. !???? ??? (?he
[is] smartmasc?), !???????? ???? ??? (?she was amazing/fem?), but not with nouns ????
!???? ??? (?she was a-symbolmasc?).
Agreement between the Verb in a relativized SBAR and the realization of the Null-
subject in the external NP.
!?????? ???? ? ?????? (?the-committeefem which [*] discussedfem the-matter?)
Morphological feature propagation. The first step of determining agreement is propagating
the relevant features from the leaves up to the constituent level.
The procedure begins by assigning each leaf gender and number features. These
are assigned based either on the TB tag assigned for the word if training on gold
POS tags, or on the morphological analyzer entries for the given word (in most cases
the number and gender features are easy to predict, even in cases where the core
POS is not clear. In the relatively rare cases where the analyzer contains both a fem-
inine and masculine (alt. singular and plural) analyses, feature value is marked as
Both).
155
Computational Linguistics Volume 39, Number 1
Table A.1
Gender and number percolation rules. FC = first child with non-NA gender/number. Rules for
each constituent type are applied in order, until a condition holds. Rules for gender and number
are applied independently of each other.
Constituent Condition Feature Values
SBAR has REL and S children S.features
SBAR otherwise NA
PREDP has ADJP child ADJP.features
PREDP has AGR child and no NP child AGR.features
PREDP otherwise NA
S has VP child and no NP-Subj child VP.features
S has VB child and no NP-Subj child VB.features
S otherwise NA
NNPG always U
NP has NNT child NNT.features
NP has CDT and NP children CDT.number NP.gender
NP is a conjunction gender=U number=Plural
NP has a ? child U
NP first child is NP, second is POS NP.features
NP has IN child FC.gender number=U
NP has child with non-NA gen/num FC.gender FC.number
NP otherwise NA
ADJP has JJT child JJT.features
ADJP has child with non-NA gen/num FC.gender FC.number
ADJP otherwise NA
VP has VB child VB.features
VP has VB-Modal child VB-Modal.features
VP has VP child VP.features
VP otherwise NA
other always NA
After each leaf is assigned feature values, the features are propagated up the tree
according to a set of rules such as the following (the complete set of rules is given in
Table A.1):
r If the constituent is an NP and has a Construct-noun child, it is assigned
the gender of the Construct-noun.
r If the constituent is a coordinated NP (has a CC child), set its number
feature to plural.
r If the constituent is an S and it has VP child but no NP-Subject child, take
the gender from the VP.
Agreement rules. Once the features are propagated from the leaves to a constituent,
agreement is verified at the constituent level according to the following rules:
NP agreement rules:
r Agreement for coordinated NPs and Possessive NPs is not checked.
156
Goldberg and Elhadad Parsing System for Hebrew
r If NP has an SBAR child, all the children up to the SBAR whose type is
nominal or adjectival must agree in gender and number.
r If NP has an ADJP child, all the children up to the ADJP whose type is
nominal or adjectival must agree in gender and number.
(a)
NP
NP
NNTFem,Sg
!??????
NP
NNMs,Pl
!???????
JJMs,Pl
!??????
JJFem,Sg
!?????
(b)
NP
NP
NNTFem,Sg
!??????
NP
NNMs,Pl
!???????
JJMs,Pl
!??????
JJFem,Sg
!?????
(c)
NPFem,Sg
NPFem,Sg
NNTFem,Sg
!??????
NPMs,Pl
NNMs,Pl
!???????
JJMs,Pl
!??????
JJFem,Sg
!?????
(d)
NPFem,Sg
NPFem,Sg
NNTFem,Sg
!??????
NPMs,Pl
NNMs,Pl
!???????
JJMs,Pl
!??????
JJFem,Sg
!?????
Figure A.1
Agreement annotation and validation example: correct tree. The sentence words translate to
box-of apples green big, literally, a big box of green apples.
(a)
NP
NNTFem,Sg
!??????
NP
NP
NNMs,Pl
!???????
JJMs,Pl
!??????
JJFem,Sg
!?????
(b)
NP
NNTFem,Sg
!??????
NP
NP
NNMs,Pl
!???????
JJMs,Pl
!??????
JJFem,Sg
!?????
(c)
NPFem,Sg
NNTFem,Sg
!??????
NPMs,Pl
NPMs,Pl
NNMs,Pl
!???????
JJMs,Pl
!??????
JJFem,Sg
!?????
(d)
NPFem,Sg
NNTFem,Sg
!??????
NPMs,Pl
NPMs,Pl
NNMs,Pl
!???????
JJMs,Pl
!??????
JJFem,Sg
!?????
Figure A.2
Agreement annotation and validation example: incorrect tree, agreement violation. box-of apples
green big, literally, a big box of green apples, though the parse tree suggests the interpretation a box
of big green apples.
157
Computational Linguistics Volume 39, Number 1
S agreement rule:
r All children of S with type in {NP-Subject, VP, VB, AUX, PREDP} must
agree in their gender and number features.
ADJP agreement rule:
r All children of ADJP with type in {NP, NP-Subject, NN, JJ, ADJP} must
agree in their gender and number features.
An example. Consider the tree in Figure A.1a. In the first stage (Figure A.1b), agreement
features are propagated according to the rules in Table A.1, resulting in the annotated
tree in Figure A.1c. Agreement is then validated in Figure A.1d (nodes in which an
agreement rule applied and passed are marked in green). In contrast, the tree in Fig-
ure A.2a has an agreement mistake. As before, the agreement features are propagated
according to the rules (Figure A.2b) resulting in Figure A.2c. Agreement validation
fails at Figure A.2d (the node in which agreement validation was applied and failed
is marked in red).
References
Abeille?, Anne, Lionel Cle?ment, and Franc?ois
Toussenel. 2003. Building a treebank for
French. In A. Abeille?, editor. Treebanks:
Building and Using Parsed Corpora.
Springer, Berlin, pages 165?188.
Adler, Meni. 2001. Hidden Markov model for
Hebrew part-of-speech tagging. Master?s
thesis, Ben-Gurion University of the
Negev.
Adler, Meni. 2007. Hebrew Morphological
Disambiguation: An Unsupervised Stochastic
Word-based Approach. Ph.D. thesis,
Ben-Gurion University of the Negev.
Adler, Meni and Michael Elhadad. 2006.
An unsupervised morpheme-based
HMM for Hebrew morphological
disambiguation. In Proceedings of the 21st
International Conference on Computational
Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics,
pages 665?672, Sydney.
Adler, Meni, Yoav Goldberg, David
Gabay, and Michael Elhadad. 2008a.
Unsupervised lexicon-based resolution of
unknown words for full morphological
analysis. In Proceedings of ACL-08: HLT,
pages 728?736, Columbus, OH.
Adler, Meni, Yael Netzer, David Gabay,
Yoav Goldberg, and Michael Elhadad.
2008b. Tagging a Hebrew corpus: The case
of participles. In Proceedings of LREC 2008,
pages 3167?3174, Marrakech.
Attia, Mohammed, Jennifer Foster, Deirdre
Hogan, Joseph Le Roux, Lamia Tounsi,
and Josef van Genabith. 2010. Handling
unknown words in statistical
latent-variable parsing models for Arabic,
English and French. In Proceedings of the
NAACL HLT 2010 First Workshop on
Statistical Parsing of Morphologically-Rich
Languages, pages 67?75, Los Angeles, CA.
Bar-Haim, Roy, Khalil Sima?an, and Yoad
Winter. 2005. Choosing an optimal
architecture for segmentation and
POS-tagging of Modern Hebrew. In
Proceedings of the ACL Workshop on
Computational Approaches to Semitic
Languages, pages 39?46, Ann Arbor, MI.
Bar-Haim, Roy, Khalil Sima?an, and Yoad
Winter. 2008. Part-of-speech tagging of
Modern Hebrew text. Natural Language
Engineering, 14(2):223?251.
Billott, Sylvie and Bernard Lang. 1989. The
structure of shared forests in ambiguous
parsing. In Proceedings of the 27th Annual
Meeting of the Association for Computational
Linguistics, pages 143?151, Vancouver.
BGU Computational Linguistics Group.
2008. Hebrew morphological tagging
guidelines. Technical report, Ben Gurion
University of the Negev.
Cai, Shu, David Chiang, and Yoav Goldberg.
2011. Language-independent parsing with
empty elements. In Proceedings of the
49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies, pages 212?216, Portland, OR.
Candito, Marie and Beno??t Crabbe?. 2009.
Improving generative statistical parsing
with semi-supervised word clustering.
In Proceedings of the 11th International
158
Goldberg and Elhadad Parsing System for Hebrew
Conference on Parsing Technologies
(IWPT?09), pages 138?141, Paris.
Candito, Marie, Beno??t Crabbe?, and
Djame? Seddah. 2009. On statistical
parsing of French with supervised
and semi-supervised strategies.
In EACL 2009 Workshop Grammatical
Inference for Computational Linguistics,
pages 49?57, Athens.
Chappelier, J., M. Rajman, R. Aragues, and
A. Rozenknop. 1999. Lattice parsing for
speech recognition. In Sixth Conference
sur le Traitement Automatique du Langage
Naturel (TANL?99), pages 95?104, Carge?se.
Charniak, Eugene and Mark Johnson.
2005. Coarse-to-fine n-best parsing and
maxent discriminative reranking.
In Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL?05), pages 173?180,
Ann Arbor, MI.
Cohen, Shay B. and Noah A. Smith. 2007.
Joint morphological and syntactic
disambiguation. In Proceedings of the
2007 Joint Conference on Empirical Methods
in Natural Language Processing and
Computational Natural Language Learning
(EMNLP-CoNLL), pages 208?217, Prague.
Collins, Michael and Terry Koo. 2005.
Discriminative reranking for natural
language parsing. Computational
Linguistics, 31(1):25?69.
Crabbe?, Beno??t and Marie Candito. 2008.
Expe?riences d?analyses syntaxique
statistique du franc?ais. In Proceedings
of TALN, pages 45?54, Avignon.
Dantzig, G. B. and P. Wolfe. 1960.
Decomposition principle for linear
programs. Operations Research, 8:101?111.
Falk, Yehuda N. 2001. Lexical-Functional
Grammar: An Introduction to Parallel
Constraint-Based Syntax. CSLI Publications,
Stanford, CA.
Glinert, Lewis. 1989. The Grammar of Modern
Hebrew. Cambridge University Press.
Goldberg, Yoav, Meni Adler, and Michael
Elhadad. 2008. EM can find pretty good
HMM POS-taggers (when given a good
start). In Proceedings of ACL-08: HLT,
pages 746?754, Columbus, OH.
Goldberg, Yoav and Michael Elhadad.
2011. Joint Hebrew segmentation and
parsing using a PCFGLA lattice parser. In
Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics:
Human Language Technologies,
pages 704?709, Portland, OR.
Goldberg, Yoav and Reut Tsarfaty. 2008.
A single generative model for joint
morphological segmentation and syntactic
parsing. In Proceedings of ACL-08: HLT,
pages 371?379, Columbus, OH.
Goldberg, Yoav, Reut Tsarfaty, Meni Adler,
and Michael Elhadad. 2009. Enhancing
unlexicalized parsing performance using
a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical
probabilities. In Proceedings of the 12th
Conference of the European Chapter of the
ACL (EACL 2009), pages 327?335,
Athens.
Green, Spence and Christopher D. Manning.
2010. Better Arabic parsing: Baselines,
evaluations, and analysis. In Proceedings
of the 23rd International Conference on
Computational Linguistics (Coling 2010),
pages 394?402, Beijing.
Guthmann, Noemie, Yuval Krymolowski,
Adi Milea, and Yoad Winter. 2009.
Automatic annotation of morpho-syntactic
dependencies in a Modern Hebrew
Treebank. In Proceedings of the 1st Workshop
on Treebanks and Linguistic Theories (TLT),
pages 1?12, Groningen.
Hall, Keith. 2005. Best-first Word-lattice
Parsing: Techniques for Integrated Syntactic
Language Modeling. Ph.D. thesis, Brown
University.
Huang, Zhongqiang and Mary Harper.
2009. Self-training PCFG grammars with
latent annotations across languages.
In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing, pages 832?841, Singapore.
Itai, Alon and Shuly Wintner. 2008. Language
resources for Hebrew. Language Resources
and Evaluation, 42(1):75?98.
Jiang, Wenbin, Liang Huang, and Qun Liu.
2009. Automatic adaptation of annotation
standards: Chinese word segmentation
and POS tagging?a case study. In
Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th
International Joint Conference on Natural
Language Processing of the AFNLP,
pages 522?530, Suntec.
Johnson, Mark. 1998. PCFG models of
linguistic tree representations.
Computational Linguistics, 24:613?632.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics,
pages 423?430, Sapporo.
Lang, Bernard. 1974. Deterministic
techniques for efficient non-deterministic
parsers. In J. Loeckx, editor, Automata,
Languages and Programming, volume 14 of
159
Computational Linguistics Volume 39, Number 1
Lecture Notes in Computer Science. Springer,
Berlin Heidelberg, pages 255?269.
Lang, Bernard. 1988. Parsing incomplete
sentences. In Proceedings of COLING,
pages 365?371, Budapest.
Matsuzaki, Takuya, Yusuke Miyao, and
Jun?ichi Tsujii. 2005. Probabilistic CFG
with latent annotations. In Proceedings of
the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL?05),
pages 75?82, Ann Arbor, MI.
Netzer, Yael, Meni Adler, David Gabay, and
Michael Elhadad. 2007. Can you tag the
modal? You should! In Proceedings of the
2007 Workshop on Computational Approaches
to Semitic Languages: Common Issues and
Resources, pages 57?64, Prague.
Petrov, Slav. 2009. Coarse-to-Fine Natural
Language Processing. Ph.D. thesis,
University of California at Berkeley.
Petrov, Slav. 2010. Products of random latent
variable grammars. In Proceedings of
NAACL, pages 19?27, Los Angeles, CA.
Petrov, Slav, Leon Barrett, Romain Thibaux,
and Dan Klein. 2006. Learning accurate,
compact, and interpretable tree
annotation. In Proceedings of the 21st
International Conference on Computational
Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics,
pages 433?440, Sydney.
Petrov, Slav and Dan Klein. 2007. Improved
inference for unlexicalized parsing.
In Human Language Technologies 2007:
The Conference of the North American
Chapter of the Association for Computational
Linguistics; Proceedings of the Main
Conference, pages 404?411, Rochester, NY.
Petrov, Slav and Dan Klein. 2008. Parsing
German with latent variable grammars.
In Proceedings of the Workshop on Parsing
German, pages 33?39, Columbus, OH.
Pollard, Carl and Ivan A. Sag. 1994.
Head-driven phrase structure grammar.
University of Chicago Press.
Prescher, Detlef. 2005. Inducing head-driven
PCFGs with latent heads: Refining a tree-
bank grammar for parsing. In Proceedings
of the European Conference on Machine
Learning (ECML), pages 292?304, Porto.
Rush, Alexander M, David Sontag, Michael
Collins, and Tommi Jaakkola. 2010.
On dual decomposition and linear
programming relaxations for natural
language processing. In Proceedings of
EMNLP, pages 1?11, Cambridge, MA.
Sima?an, Khalil. 1996. Computational
complexity of probabilistic disambiguation
by means of tree grammars. In Proceedings
of COLING, pages 1175?1180, Copenhagen.
Sima?an, Khalil. 1999. Learning Efficient
Disambiguation. Ph.D. thesis, ILLC
Dissertation Series, University of
Amsterdam.
Sima?an, Khalil, Alon Itai, Yoad Winter,
Alon Altman, and Noa Nativ. 2001.
Building a tree-bank of Modern Hebrew
text. Traitement Automatique des Langues,
42(2):1?32.
Smith, Noah A., David A. Smith, and
Roy W. Tromble. 2005. Context-based
morphological disambiguation with
random fields. In Proceedings of EMNLP,
pages 475?482, Vancouver.
Tsarfaty, Reut. 2006a. Integrated
morphological and syntactic
disambiguation for Modern Hebrew.
In Proceedings of the COLING/ACL 2006
Student Research Workshop, pages 49?54,
Sydney.
Tsarfaty, Reut. 2006b. The Interplay of Syntax
and Morphology in Building Parsing
Models for Modern Hebrew. In Proceedings
of ESSLI Student Session, pages 263?274,
Malaga.
Tsarfaty, Reut. 2010. Relational-Realizational
Parsing. Ph.D. thesis, ILLC Dissertation
Series, University of Amsterdam.
Tsarfaty, Reut and Khalil Sima?an. 2007.
Three-dimensional parametrization for
parsing morphologically rich languages.
In Proceedings of the Tenth International
Conference on Parsing Technologies,
pages 156?167, Prague.
Tsarfaty, Reut and Khalil Sima?an. 2008.
Relational-realizational parsing. In
Proceedings of CoLING, pages 889?896,
Manchester.
Tsarfaty, Reut and Khalil Sima?an. 2010.
Modeling morphosyntactic agreement
in constituency-based parsing of
Modern Hebrew. In Proceedings of the
NAACL HLT 2010 First Workshop on
Statistical Parsing of Morphologically-Rich
Languages, pages 40?48, Los Angeles, CA.
Tsarfaty, Reut, Khalil Sima?an, and Remko
Scha. 2009. An alternative to head-driven
approaches for parsing a (relatively) free
word-order language. In Proceedings
of the 2009 Conference on Empirical
Methods in Natural Language Processing,
pages 842?851, Singapore.
160
Constrained Arc-Eager Dependency Parsing
Joakim Nivre?
Uppsala University
Yoav Goldberg??
Bar-Ilan University
Ryan McDonald?
Google
Arc-eager dependency parsers process sentences in a single left-to-right pass over the input
and have linear time complexity with greedy decoding or beam search. We show how such
parsers can be constrained to respect two different types of conditions on the output dependency
graph: span constraints, which require certain spans to correspond to subtrees of the graph,
and arc constraints, which require certain arcs to be present in the graph. The constraints are
incorporated into the arc-eager transition system as a set of preconditions for each transition and
preserve the linear time complexity of the parser.
1. Introduction
Data-driven dependency parsers in general achieve high parsing accuracy without re-
lying on hard constraints to rule out (or prescribe) certain syntactic structures (Yamada
and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; McDonald, Crammer, and Pereira
2005; Zhang and Clark 2008; Koo and Collins 2010). Nevertheless, there are situations
where additional information sources, not available at the time of training the parser,
may be used to derive hard constraints at parsing time. For example, Figure 1 shows
the parse of a greedy arc-eager dependency parser trained on the Wall Street Journal
section of the Penn Treebank before (left) and after (right) being constrained to build a
single subtree over the span corresponding to the named entity ?Cat on a Hot Tin Roof,?
which does not occur in the training set but can easily be found in on-line databases. In
this case, adding the span constraint fixes both prepositional phrase attachment errors.
Similar constraints can also be derived from dates, times, or other measurements that
can often be identified with high precision using regular expressions (Karttunen et al.
1996), but are under-represented in treebanks.
? Uppsala University, Department of Linguistics and Philology, Box 635, SE-75126, Uppsala, Sweden.
E-mail: joakim.nivre@lingfil.uu.se.
?? Bar-Ilan University, Department of Computer Science, Ramat-Gan, 5290002, Israel.
E-mail: yoav.goldberg@gmail.com.
? Google, 76 Buckingham Palace Road, London SW1W9TQ, United Kingdom.
E-mail: ryanmcd@google.com.
Submission received: 26 June 2013; accepted for publication: 10 October 2013.
doi:10.1162/COLI a 00184
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 2
Figure 1
Span constraint derived from a title assisting parsing. Left: unconstrained. Right: constrained.
In this article, we examine the problem of constraining transition-based dependency
parsers based on the arc-eager transition system (Nivre 2003, 2008), which perform a
single left-to-right pass over the input, eagerly adding dependency arcs at the earliest
possible opportunity, resulting in linear time parsing. We consider two types of con-
straints: span constraints, exemplified earlier, require the output graph to have a single
subtree over one or more (non-overlapping) spans of the input; arc constraints instead
require specific arcs to be present in the output dependency graph. The main contri-
bution of the article is to show that both span and arc constraints can be implemented
as efficiently computed preconditions on parser transitions, thus maintaining the linear
runtime complexity of the parser.1
Demonstrating accuracy improvements due to hard constraints is challenging, be-
cause the phenomena we wish to integrate as hard constraints are by definition not
available in the parser?s training and test data. Moreover, adding hard constraints may
be desirable even if it does not improve parsing accuracy. For example, many organi-
zations have domain-specific gazetteers and want the parser output to be consistent
with these even if the output disagrees with gold treebank annotations, sometimes
because of expectations of downstream modules in a pipeline. In this article, we con-
centrate on the theoretical side of constrained parsing, but we nevertheless provide
some experimental evidence illustrating how hard constraints can improve parsing
accuracy.
2. Preliminaries and Notation
Dependency Graphs. Given a set L of dependency labels, we define a dependency graph
for a sentence x = w1, . . . , wn as a labeled directed graph G = (Vx, A), consisting of a
set of nodes Vx = {1, . . . , n}, where each node i corresponds to the linear position of a
word wi in the sentence, and a set of labeled arcs A ? Vx ? L ? Vx, where each arc (i, l, j)
represents a dependency with head wi, dependent wj, and label l. We assume that the
final word wn is always a dummy word ROOT and that the corresponding node n is a
designated root node.
Given a dependency graph G for sentence x, we say that a subgraph G[i,j] =
(V[i,j], A[i,j]) of G is a projective spanning tree over the interval [i, j] (1 ? i ? j ? n) iff
(i) G[i,j] contains all nodes corresponding to words between wi and wj inclusive, (ii) G[i,j]
is a directed tree, and (iii) it holds for every arc (i, l, j) ? G[i,j] that there is a directed path
1 Although span and arc constraints can easily be added to other dependency parsing frameworks, this
often affects parsing complexity. For example, in graph-based parsing (McDonald, Crammer, and Pereira
2005) arc constraints can be enforced within the O(n3) Eisner algorithm (Eisner 1996) by pruning out
inconsistent chart cells, but span constraints require the parser to keep track of full subtree end points,
which would necessitate the use of O(n4) algorithms (Eisner and Satta 1999).
250
Nivre, Goldberg, and McDonald Constrained Arc-Eager Dependency Parsing
from i to every node k such that min(i, j) < k < max(i, j) (projectivity). We now define
two constraints on a dependency graph G for a sentence x:
r G is a projective dependency tree (PDT) if and only if it is a projective
spanning tree over the interval [1, n] rooted at node n.
r G is a projective dependency graph (PDG) if and only if it can be extended
to a projective dependency tree simply by adding arcs.
It is clear from the definitions that every PDT is also a PDG, but not the other way around.
Every PDG can be created by starting with a PDT and removing some arcs.
Arc-Eager Transition-Based Parsing. In the arc-eager transition system of Nivre (2003), a
parser configuration is a triple c = (?|i, j|?, A) such that ? and B are disjoint sublists of
the nodes Vx of some sentence x, and A is a set of dependency arcs over Vx (and some
label set L). Following Ballesteros and Nivre (2013), we take the initial configuration
for a sentence x = w1, . . . , wn to be cs(x) = ([ ], [1, . . . , n], { }), where n is the designated
root node, and we take a terminal configuration to be any configuration of the form
c = ([ ], [n], A) (for any arc set A). We will refer to the list ? as the stack and the list B
as the buffer, and we will use the variables ? and ? for arbitrary sublists of ? and B,
respectively. For reasons of perspicuity, we will write ? with its head (top) to the right
and B with its head to the left. Thus, c = (?|i, j|?, A) is a configuration with the node i on
top of the stack ? and the node j as the first node in the buffer B.
There are four types of transitions for going from one configuration to the next,
defined formally in Figure 2 (disregarding for now the Added Preconditions column):
r LEFT-ARCl adds the arc (j, l, i) to A, where i is the node on top of the stack
and j is the first node in the buffer, and pops the stack. It has as a
precondition that the token i does not already have a head.
r RIGHT-ARCl adds the arc (i, l, j) to A, where i is the node on top of the stack
and j is the first node in the buffer, and pushes j onto the stack. It has as a
precondition that j 6= n.
r REDUCE pops the stack and requires that the top token has a head.
r SHIFT removes the first node in the buffer and pushes it onto the stack,
with the precondition that j 6= n.
A transition sequence for a sentence x is a sequence C0,m = (c0, c1, . . . , cm) of configu-
rations, such that c0 is the initial configuration cs(x), cm is a terminal configuration, and
there is a legal transition t such that ci = t(ci?1) for every i, 1 ? i ? m. The dependency
graph derived by C0,m is Gcm = (Vx, Acm ), where Acm is the set of arcs in cm.
Complexity and Correctness. For a sentence of length n, the number of transitions in
the arc-eager system is bounded by 2n (Nivre 2008). This means that a parser using
greedy inference (or constant width beam search) will run in O(n) time provided that
transitions plus required precondition checks can be performed in O(1) time. This holds
for the arc-eager system and, as we will demonstrate, its constrained variants as well.
The arc-eager transition system as presented here is sound and complete for the set
of PDTs (Nivre 2008). For a specific sentence x = w1, . . . , wn, this means that any transi-
tion sequence for x produces a PDT (soundness), and that any PDT for x is generated by
251
Computational Linguistics Volume 40, Number 2
Transition Added Preconditions
LEFT-ARCl (?|i, j|?, A) ? (?, j|?, A?{(j, l, i)}) ARC CONSTRAINTS
??a ? AC : da = i ? [ha ? ? ? la 6= l]
??a ? A : da = i ??a ? AC : ha = i ? da ? j|?
SPAN CONSTRAINTS
?[IN-SPAN(i) ? s(i) = s(j) ? i = r(s(i))]
?[IN-SPAN(i) ? s(i) 6= s(j) ? i 6= r(s(i))]
?[NONE? IN-SPAN(j) ? s(i) 6= s(j)]
?[ROOT ? IN-SPAN(j) ? s(i) 6= s(j) ? j 6= r(s(j))]
RIGHT-ARCl (?|i, j|?, A) ? (?|i|j,?, A?{(i, l, j)}) ARC CONSTRAINTS
??a ? AC : da = j ? [ha ? ? ? la 6= l]
j 6= n ??a ? AC : ha = j ? da ? i|?
SPAN CONSTRAINTS
?[ENDS-SPAN(j) ? #CC > 1]
?[IN-SPAN(j) ? s(i) = s(j) ? j = r(s(j))]
?[IN-SPAN(j) ? s(i) 6= s(j) ? j 6= r(s(j))]
?[NONE? IN-SPAN(i) ? s(i) 6= s(j)]
?[ROOT ? IN-SPAN(i) ? s(i) 6= s(j) ? i 6= r(s(i))]
REDUCE (?|i, j|?, A) ? (?, j|?, A) ARC CONSTRAINTS
??a ? AC : ha = i ? da ? j|?
?a ? A : da = i SPAN CONSTRAINTS
?[IN-SPAN(i) ? s(i) = s(j) ? i = r(s(i))]
SHIFT (?, i|?, A) ? (?|i,?, A) ARC CONSTRAINTS
??a ? AC : da = j ? ha ? i|?
i 6= n ??a ? AC : ha = j ? da ? i|?
SPAN CONSTRAINTS
?[ENDS-SPAN(j) ? #CC > 0]
Figure 2
Transitions for the arc-eager transition system with preconditions for different constraints. The
symbols ha, la, and da are used to denote the head node, label, and dependent node, respectively,
of an arc a (that is, a = (ha, la, da )); IN-SPAN(i) is true if i is contained in a span in SC; END-SPAN(i)
is true if i is the last word in a span in SC; s(i) denotes the span containing i (with a dummy span
for all words that are not contained in any span); r(s) denotes the designated root of span s
(if any); #CC records the number of connected components in the current span up to and
including the last word that was pushed onto the stack; NONE and ROOT are true if we allow no
outgoing arcs from spans and if we allow outgoing arcs only from the span root, respectively.
some transition sequence (completeness).2 In constrained parsing, we want to restrict
the system so that, when applied to a sentence x, it is sound and complete for the subset
of PDTs that satisfy all constraints.
3. Parsing with Arc Constraints
Arc Constraints. Given a sentence x = w1, . . . , wn and a label set L, an arc constraint
set is a set AC of dependency arcs (i, l, j) (1 ? i, j ? n, i 6= j 6= n, l ? L), where each arc
is required to be included in the parser output. Because the arc-eager system can only
derive PDTs, the arc constraint set has to be such that the constraint graph GC = (Vx, AC)
can be extended to a PDT, which is equivalent to requiring that GC is a PDG. Thus, the
task of arc-constrained parsing can be defined as the task of deriving a PDT G such
2 Although the transition system in Nivre (2008) is complete but not sound, it is trivial to show that the
system as presented here (with the root node at the end of the buffer) is both sound and complete.
252
Nivre, Goldberg, and McDonald Constrained Arc-Eager Dependency Parsing
that GC ? G. An arc-constrained transition system is sound if it only derives proper
extensions of the constraint graph and complete if it derives all such extensions.
Added Preconditions. We know that the unconstrained arc-eager system can derive any
PDT for the input sentence x, which means that any arc in Vx ? L ? Vx is reachable
from the initial configuration, including any arc in the arc constraint set AC. Hence, in
order to make the parser respect the arc constraints, we only need to add preconditions
that block transitions that would make an arc in AC unreachable.3 We achieve this
through the following preconditions, defined formally in Figure 2 under the heading
ARC CONSTRAINTS for each transition:
r LEFT-ARCl in a configuration (?|i, j|?, A) adds the arc (j, l, i) and makes
unreachable any arc that involves i and a node in the buffer (other than
(j, l, i)). Hence, we permit LEFT-ARCl only if no such arc is in AC.
r RIGHT-ARCl in a configuration (?|i, j|?, A) adds the arc (i, l, j) and makes
unreachable any arc that involves j and a node on the stack (other than
(i, l, j)). Hence, we permit RIGHT-ARCl only if no such arc is in AC.
r REDUCE in a configuration (?|i, j|?, A) pops i from the stack and makes
unreachable any arc that involves i and a node in the buffer. Hence, we
permit REDUCE only if no such arc is in AC.
r SHIFT in a configuration (?, i|?, A) moves i to the stack and makes
unreachable any arc that involves j and a node on the stack. Hence,
we permit SHIFTl only if no such arc is in AC.
Complexity and Correctness. Because the transitions remain the same, the arc-constrained
parser will terminate after at most 2n transitions, just like the unconstrained system.
However, in order to guarantee termination, we must also show that at least one
transition is applicable in every non-terminal configuration. This is trivial in the un-
constrained system, where the SHIFT transition can apply to any configuration that
has a non-empty buffer. In the arc-constrained system, SHIFT will be blocked if there
is an arc a ? AC involving the node i to be shifted and some node on the stack, and
we need to show that one of the three remaining transitions is then permissible. If a
involves i and the node on top of the stack, then either LEFT-ARCl and RIGHT-ARCl
is permissible (in fact, required). Otherwise, either LEFT-ARCl or REDUCE must be
permissible, because their preconditions are implied by the fact that AC is a PDG.
In order to obtain linear parsing complexity, we must also be able to check all pre-
conditions in constant time. This can be achieved by preprocessing the sentence x and
arc constraint set AC and recording for each node i ? Vx its constrained head (if any),
its leftmost constrained dependent (if any), and its rightmost constrained dependent (if
any), so that we can evaluate the preconditions in each configuration without having
to scan the stack and buffer linearly. Because there are at most O(n) arcs in the arc
constraint set, the preprocessing will not take more than O(n) time but guarantees that
all permissibility checks can be performed in O(1) time.
Finally, we note that the arc-constrained system is sound and complete in the sense
that it derives all and only PDTs compatible with a given arc constraint set AC for a sen-
tence x. Soundness follows from the fact that, for every arc (i, l, j) ? AC, the preconditions
3 For further discussion of reachability in the arc-eager system, see Goldberg and Nivre (2012, 2013).
253
Computational Linguistics Volume 40, Number 2
force the system to reach a configuration of the form (?|min(i, j),max(i, j)|?, A) in which
either LEFT-ARCl (i > j) or RIGHT-ARCl (i < j) will be the only permissible transition.
Completeness follows from the observation that every PDT G compatible with AC is also
a PDG and can therefore be viewed as a larger constraint set for which every transition
sequence (given soundness) derives G exactly.
Empirical Case Study: Imperatives. Consider the problem of parsing commands to
personal assistants such as Siri or Google Now. In this setting, the distribution of
utterances is highly skewed towards imperatives making them easy to identify.
Unfortunately, parsers trained on treebanks like the Penn Treebank (PTB) typically
do a poor job of parsing such utterances (Hara et al. 2011). However, we know that
if the first word of a command is a verb, it is likely the root of the sentence. If we
take an arc-eager beam search parser (Zhang and Nivre 2011) trained on the PTB, it
gets 82.14 labeled attachment score on a set of commands.4 However, if we constrain
the same parser so that the first word of the sentence must be the root, accuracy
jumps dramatically to 85.56. This is independent of simply knowing that the first
word of the sentence is a verb, as both parsers in this experiment had access to gold
part-of-speech tags.
4. Parsing with Span Constraints
Span Constraints. Given a sentence x = w1, . . . , wn, we take a span constraint set to be
a set SC of non-overlapping spans [i, j] (1 ? i < j ? n). The task of span-constrained
parsing can then be defined as the task of deriving a PDT G such that, for every span
[i, j] ? SC, G[i,j] is a (projective) spanning tree over the interval [i, j]. A span-constrained
transition system is sound if it only derives dependency graphs compatible with the
span constraint set and complete if it derives all such graphs. In addition, we may add
the requirement that no word inside a span may have dependents outside the span
(NONE), or that only the root of the span may have such dependents (ROOT).
Added Preconditions. Unlike the case of arc constraints, parsing with span constraints
cannot be reduced to simply enforcing (and blocking) specific dependency arcs. In
this sense, span constraints are more global than arc constraints as they require en-
tire subgraphs of the dependency graph to have a certain property. Nevertheless,
we can use the same basic technique as before and enforce span constraints by
adding new preconditions to transitions, but these preconditions need to refer to vari-
ables that are updated dynamically during parsing. We need to keep track of two
things:
r Which word is the designated root of a span? A word becomes the
designated root r(s) of its span s if it acquires a head outside the span or
if it acquires a dependent outside the span under the ROOT condition.
r How many connected components are in the subgraph over the current
span up to and including the last word pushed onto the stack? A variable
#CC is set to 1 when the first span word enters the stack, incremented by
1 for every SHIFT and decremented by 1 for every LEFT-ARCl.
4 Data and splits from the Web Treebank of Petrov and McDonald (2012). Commands used for evaluation
were sentences from the test set that had a sentence initial verb root.
254
Nivre, Goldberg, and McDonald Constrained Arc-Eager Dependency Parsing
Given this information, we need to add preconditions that guarantee the following:
r The designated root must not acquire a head inside the span.
r No word except the designated root may acquire a head outside the span.
r The designated root must not be popped from the stack before the last
word of the span has been pushed onto the stack.
r The last word of a span must not be pushed onto the stack in a
RIGHT-ARCl transition if #CC > 1.
r The last word of a span must not be pushed onto the stack in a SHIFT
transition if #CC > 0.
In addition, we must block outside dependents of all words in a span under the NONE
condition, and of all words in a span other than the designated root under the ROOT
condition. All the necessary preconditions are given in Figure 2 under the heading
SPAN CONSTRAINTS.
Complexity and Correctness. To show that the span-constrained parser always terminates
after at most 2n transitions, it is again sufficient to show that there is at least one
permissible transition for every non-terminal configuration. Here, SHIFT is blocked if
the word i to be shifted is the last word of a span and #CC > 0. But in this case, one of the
other three transitions must be permissible. If #CC = 1, then RIGHT-ARCl is permissible;
if #CC > 1 and the word on top of the stack does not have a head, then LEFT-ARCl is
permissible; and if #CC > 1 and the word on top of the stack already has a head, then
REDUCE is permissible (as #CC > 1 rules out the possibility that the word on top of the
stack has its head outside the span). In order to obtain linear parsing complexity, all
preconditions should be verifiable in constant time. This can be achieved during initial
sentence construction by recording the span s(i) for every word i (with a dummy span
for words that are not inside a span) and by updating r(s) (for every span s) and #CC as
described herein.
Finally, we note that the span-constrained system is sound and complete in the
sense that it derives all and only PDTs compatible with a given span constraint set SC for
a sentence x. Soundness follows from the observation that failure to have a connected
subgraph G[i,j] for some span [i, j] ? SC can only arise from pushing j onto the stack in
a SHIFT with #CC > 0 or a RIGHT-ARCl with #CC > 1, which is explicitly ruled out by
the added preconditions. Completeness can be established by showing that a transition
sequence that derives a PDT G compatible with SC in the unconstrained system cannot
violate any of the added preconditions, which is straightforward but tedious.
Empirical Case Study: Korean Parsing. In Korean, white-space-separated tokens corre-
spond to phrasal units (similar to Japanese bunsetsus) and not to basic syntactic cat-
egories like nouns, adjectives, or verbs. For this reason, a further segmentation step is
needed in order to transform the space-delimited tokens to units that are a suitable input
for a parser and that will appear as the leaves of a syntactic tree. Here, the white-space
boundaries are good candidates for posing hard constraints on the allowed sentence
structure, as only a single dependency link is allowed between different phrasal units,
and all the other links are phrase-internal. An illustration of the process is given in
Figure 3. Experiments on the Korean Treebank from McDonald et al. (2013) show that
adding span constraints based on white space indeed improves parsing accuracy for
an arc-eager beam search parser (Zhang and Nivre 2011). Unlabeled attachment score
255
Computational Linguistics Volume 40, Number 2
Figure 3
Parsing a Korean sentence (the man writes the policy decisions) using span constraints derived from
original white space cues indicating phrasal chunks.
increases from an already high 94.10 without constraints to 94.92, and labeled attach-
ment score increases from 89.91 to 90.75.
Combining Constraints. What happens if we want to add arc constraints on top of
the span constraints? In principle, we can simply take the conjunction of the added
preconditions from the arc constraint case and the span constraint case, but some
care is required to enforce correctness. First of all, we have to check that the arc
constraints are consistent with the span constraints and do not require, for example,
that there are two words with outside heads inside the the same span. In addition, we
need to update the variables r(s) already in the preprocessing phase in case the arc
constraints by themselves fix the designated root because they require a word inside
the span to have an outside head or (under the ROOT condition) to have an outside
dependent.
5. Conclusion
We have shown how the arc-eager transition system for dependency parsing can
be modified to take into account both arc constraints and span constraints, without
affecting the linear runtime and while preserving natural notions of soundness and
completeness. Besides the practical applications discussed in the introduction and case
studies, constraints can also be used as partial oracles for parser training.
References
Ballesteros, Miguel and Joakim Nivre.
2013. Getting to the roots of dependency
parsing. Computational Linguistics,
39:5?13.
Eisner, Jason and Giorgio Satta. 1999.
Efficient parsing for bilexical context-free
grammars and head automaton grammars.
In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics,
pages 457?464, Santa Cruz, CA.
Eisner, Jason M. 1996. Three new
probabilistic models for dependency
parsing: An exploration. In Proceedings
of the 16th International Conference on
Computational Linguistics (COLING),
pages 340?345, Copenhagen.
Goldberg, Yoav and Joakim Nivre.
2012. A dynamic oracle for arc-eager
dependency parsing. In Proceedings
of the 24th International Conference on
Computational Linguistics, pages 959?976,
Shanghai.
Goldberg, Yoav and Joakim Nivre. 2013.
Training deterministic parsers with
non-deterministic oracles. Transactions
of the Association for Computational
Linguistics, 1:403?414.
Hara, Tadayoshi, Takuya Matsuzaki, Yusuke
Miyao, and Jun?ichi Tsujii. 2011. Exploring
difficulties in parsing imperatives and
questions. In Proceedings of the 5th
International Joint Conference on Natural
Language Processing, pages 749?757,
Chiang Mai.
Karttunen, Lauri, Jean-Pierre Chanod,
Gregory Grefenstette, and Anne Schiller.
1996. Regular expressions for language
engineering. Natural Language Engineering,
2(4):305?328.
256
Nivre, Goldberg, and McDonald Constrained Arc-Eager Dependency Parsing
Koo, Terry and Michael Collins. 2010.
Efficient third-order dependency parsers.
In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics,
pages 1?11, Uppsala.
McDonald, Ryan, Koby Crammer, and
Fernando Pereira. 2005. Online
large-margin training of dependency
parsers. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, pages 91?98, Ann Arbor, MI.
McDonald, Ryan, Joakim Nivre, Yvonne
Quirmbach-Brundage, Yoav Goldberg,
Dipanjan Das, Kuzman Ganchev, Keith
Hall, Slav Petrov, Hao Zhang, Oscar
Ta?ckstro?m, Claudia Bedini, Nu?ria
Bertomeu Castello?, and Jungmee Lee.
2013. Universal dependency annotation
for multilingual parsing. In Proceedings of
the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2:
Short Papers), pages 92?97, Sofia.
Nivre, Joakim. 2003. An efficient algorithm
for projective dependency parsing. In
Proceedings of the 8th International Workshop
on Parsing Technologies, pages 149?160,
Nancy.
Nivre, Joakim. 2008. Algorithms for
deterministic incremental dependency
parsing. Computational Linguistics,
34:513?553.
Nivre, Joakim, Johan Hall, and Jens Nilsson.
2004. Memory-based dependency parsing.
In Proceedings of the 8th Conference on
Computational Natural Language Learning,
pages 49?56, Boston, MA.
Petrov, Slav and Ryan McDonald. 2012.
Overview of the 2012 shared task on
parsing the web. In Notes of the First
Workshop on Syntactic Analysis of
Non-Canonical Language (SANCL),
Montreal.
Yamada, Hiroyasu and Yuji Matsumoto.
2003. Statistical dependency analysis
with support vector machines.
In Proceedings of the 8th International
Workshop on Parsing Technologies,
pages 195?206, Nancy.
Zhang, Yue and Stephen Clark. 2008.
A tale of two parsers: Investigating
and combining graph-based and
transition-based dependency parsing.
In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 562?571,
Honolulu, HI.
Zhang, Yue and Joakim Nivre. 2011.
Transition-based parsing with rich
non-local features. In Proceedings of the
49th Annual Meeting of the Association for
Computational Linguistics, pages 188?193,
Portland, OR.
257

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 742?750,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing
Yoav Goldberg? and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
{yoavg|elhadad}@cs.bgu.ac.il
Abstract
We present a novel deterministic dependency pars-
ing algorithm that attempts to create the easiest arcs
in the dependency structure first in a non-directional
manner. Traditional deterministic parsing algorithms
are based on a shift-reduce framework: they traverse
the sentence from left-to-right and, at each step, per-
form one of a possible set of actions, until a complete
tree is built. A drawback of this approach is that
it is extremely local: while decisions can be based
on complex structures on the left, they can look only
at a few words to the right. In contrast, our algo-
rithm builds a dependency tree by iteratively select-
ing the best pair of neighbours to connect at each
parsing step. This allows incorporation of features
from already built structures both to the left and to the
right of the attachment point. The parser learns both
the attachment preferences and the order in which
they should be performed. The result is a determin-
istic, best-first, O(nlogn) parser, which is signifi-
cantly more accurate than best-first transition based
parsers, and nears the performance of globally opti-
mized parsing models.
1 Introduction
Dependency parsing has been a topic of active re-
search in natural language processing in the last sev-
eral years. An important part of this research effort
are the CoNLL 2006 and 2007 shared tasks (Buch-
holz and Marsi, 2006; Nivre et al, 2007), which al-
lowed for a comparison of many algorithms and ap-
proaches for this task on many languages.
?Supported by the Lynn and William Frankel Center for
Computer Sciences, Ben Gurion University
Current dependency parsers can be categorized
into three families: local-and-greedy transition-
based parsers (e.g., MALTPARSER (Nivre et al,
2006)), globally optimized graph-based parsers
(e.g., MSTPARSER (McDonald et al, 2005)), and
hybrid systems (e.g., (Sagae and Lavie, 2006b;
Nivre and McDonald, 2008)), which combine the
output of various parsers into a new and improved
parse, and which are orthogonal to our approach.
Transition-based parsers scan the input from left
to right, are fast (O(n)), and can make use of rich
feature sets, which are based on all the previously
derived structures. However, all of their decisions
are very local, and the strict left-to-right order im-
plies that, while the feature set can use rich struc-
tural information from the left of the current attach-
ment point, it is also very restricted in information
to the right of the attachment point: traditionally,
only the next two or three input tokens are avail-
able to the parser. This limited look-ahead window
leads to error propagation and worse performance on
root and long distant dependencies relative to graph-
based parsers (McDonald and Nivre, 2007).
Graph-based parsers, on the other hand, are glob-
ally optimized. They perform an exhaustive search
over all possible parse trees for a sentence, and find
the highest scoring tree. In order to make the search
tractable, the feature set needs to be restricted to fea-
tures over single edges (first-order models) or edges
pairs (higher-order models, e.g. (McDonald and
Pereira, 2006; Carreras, 2007)). There are several
attempts at incorporating arbitrary tree-based fea-
tures but these involve either solving an ILP prob-
lem (Riedel and Clarke, 2006) or using computa-
742
(1) ATTACHRIGHT(2)
a brown fox jumped with joy
-157
-27
-68
403
-197
-47
-152
-243
231
3
(2) ATTACHRIGHT(1)
a fox
brown
jumped with joy
-52
314
-159
0
-176
-146
246
12
(3) ATTACHRIGHT(1)
fox
a brown
jumped with joy
-133
270
-149
-154
246
10
(4) ATTACHLEFT(2)
jumped
fox
a brown
with joy
-161
-435
186
-2
(5) ATTACHLEFT(1)
jumped
fox
a brown
with
joy
430
-232
(6)
jumped
fox
a brown
with
joy
Figure 1: Parsing the sentence ?a brown fox jumped with joy?. Rounded arcs represent possible actions.
tionally intensive sampling-based methods (Naka-
gawa, 2007). As a result, these models, while accu-
rate, are slow (O(n3) for projective, first-order mod-
els, higher polynomials for higher-order models, and
worse for richer tree-feature models).
We propose a new category of dependency pars-
ing algorithms, inspired by (Shen et al, 2007): non-
directional easy-first parsing. This is a greedy, de-
terministic parsing approach, which relaxes the left-
to-right processing order of transition-based pars-
ing algorithms. By doing so, we allow the ex-
plicit incorporation of rich structural features de-
rived from both sides of the attachment point, and
implicitly take into account the entire previously de-
rived structure of the whole sentence. This exten-
sion allows the incorporation of much richer features
than those available to transition- and especially to
graph-based parsers, and greatly reduces the local-
ity of transition-based algorithm decisions. On the
other hand, it is still a greedy, best-first algorithm
leading to an efficient implementation.
We present a concrete O(nlogn) parsing algo-
rithm, which significantly outperforms state-of-the-
art transition-based parsers, while closing the gap to
graph-based parsers.
2 Easy-first parsing
When humans comprehend a natural language sen-
tence, they arguably do it in an incremental, left-to-
right manner. However, when humans consciously
annotate a sentence with syntactic structure, they
hardly ever work in fixed left-to-right order. Rather,
they start by building several isolated constituents
by making easy and local attachment decisions and
only then combine these constituents into bigger
constituents, jumping back-and-forth over the sen-
tence and proceeding from easy to harder phenom-
ena to analyze. When getting to the harder decisions
a lot of structure is already in place, and this struc-
ture can be used in deciding a correct attachment.
Our parser follows a similar kind of annotation
process: starting from easy attachment decisions,
and proceeding to harder and harder ones. When
making later decisions, the parser has access to the
entire structure built in earlier stages. During the
training process, the parser learns its own notion of
easy and hard, and learns to defer specific kinds of
decisions until more structure is available.
3 Parsing algorithm
Our (projective) parsing algorithm builds the parse
tree bottom up, using two kinds of actions: AT-
TACHLEFT(i) and ATTACHRIGHT(i) . These
actions are applied to a list of partial structures
p1, . . . , pk, called pending, which is initialized with
the n words of the sentence w1, . . . , wn. Each ac-
743
tion connects the heads of two neighbouring struc-
tures, making one of them the parent of the other,
and removing the daughter from the list of partial
structures. ATTACHLEFT(i) adds a dependency
edge (pi, pi+1) and removes pi+1 from the list. AT-
TACHRIGHT(i) adds a dependency edge (pi+1, pi)
and removes pi from the list. Each action shortens
the list of partial structures by 1, and after n?1 such
actions, the list contains the root of a connected pro-
jective tree over the sentence.
Figure 1 shows an example of parsing the sen-
tence ?a brown fox jumped with joy?. The pseu-
docode of the algorithm is given in Algorithm 1.
Algorithm 1: Non-directional Parsing
Input: a sentence= w1 . . . wn
Output: a set of dependency arcs over the
sentence (Arcs)
Acts = {ATTACHLEFT, ATTACHRIGHT}1
Arcs? {}2
pending = p1 . . . pn ? w1 . . . wn3
while length(pending) > 1 do4
best? arg max
act?Acts
1?i?len(pending)
score(act(i))
5
(parent, child)? edgeFor(best)6
Arcs.add( (parent, child) )7
pending.remove(child)8
end9
return Arcs10
edgeFor(act(i)) =
{
(pi, pi+1) ATTACHLEFT(i)
(pi+1, pi) ATTACHRIGHT(i)
At each step the algorithm chooses a spe-
cific action/location pair using a function
score(ACTION(i)), which assign scores to ac-
tion/location pairs based on the partially built
structures headed by pi and pi+1, as well as neigh-
bouring structures. The score() function is learned
from data. This scoring function reflects not only
the correctness of an attachment, but also the order
in which attachments should be made. For example,
consider the attachments (brown,fox) and (joy,with)
in Figure (1.1). While both are correct, the scoring
function prefers the (adjective,noun) attachment
over the (prep,noun) attachment. Moreover, the
attachment (jumped,with), while correct, receives
a negative score for the bare preposition ?with?
(Fig. (1.1) - (1.4) ), and a high score once the verb
has its subject and the PP ?with joy? is built (Fig.
(1.5) ). Ideally, we would like to score easy and
reliable attachments higher than harder less likely
attachments, thus performing attachments in order
of confidence. This strategy allows us both to limit
the extent of error propagation, and to make use of
richer contextual information in the later, harder
attachments. Unfortunately, this kind of ordering
information is not directly encoded in the data. We
must, therefore, learn how to order the decisions.
We first describe the learning algorithm (Section
4) and a feature representation (Section 5) which en-
ables us to learn an effective scoring function.
4 Learning Algorithm
We use a linear model score(x) = ~w ? ?(x), where
?(x) is a feature representation and ~w is a weight
vector. We write ?act(i) to denote the feature repre-
sentation extracted for action act at location i. The
model is trained using a variant of the structured per-
ceptron (Collins, 2002), similar to the algorithm of
(Shen et al, 2007; Shen and Joshi, 2008). As usual,
we use parameter averaging to prevent the percep-
tron from overfitting.
The training algorithm is initialized with a zero
parameter vector ~w. The algorithm makes several
passes over the data. At each pass, we apply the
training procedure given in Algorithm 2 to every
sentence in the training set.
At training time, each sentence is parsed using the
parsing algorithm and the current ~w. Whenever an
invalid action is chosen by the parsing algorithm, it
is not performed (line 6). Instead, we update the pa-
rameter vector ~w by decreasing the weights of the
features associated with the invalid action, and in-
creasing the weights for the currently highest scor-
ing valid action.1 We then proceed to parse the sen-
tence with the updated values. The process repeats
until a valid action is chosen.
Note that each single update does not guarantee
that the next chosen action is valid, or even different
than the previously selected action. Yet, this is still
an aggressive update procedure: we do not leave a
sentence until our parameters vector parses it cor-
1We considered 3 variants of this scheme: (1) using the high-
est scoring valid action, (2) using the leftmost valid action, and
(3) using a random valid action. The 3 variants achieved nearly
identical accuracy, while (1) converged somewhat faster than
the other two.
744
rectly, and we do not proceed from one partial parse
to the next until ~w predicts a correct location/action
pair. However, as the best ordering, and hence the
best attachment point is not known to us, we do not
perform a single aggressive update step. Instead, our
aggressive update is performed incrementally in a
series of smaller steps, each pushing ~w away from
invalid attachments and toward valid ones. This way
we integrate the search of confident attachments into
the learning process.
Algorithm 2: Structured perceptron training
for direction-less parser, over one sentence.
Input: sentence,gold arcs,current ~w,feature
representation ?
Output: weight vector ~w
Arcs? {}1
pending ? sent2
while length(pending) > 1 do3
allowed? {act(i)|isV alid(act(i), Gold,Arcs)}4
choice? arg max
act?Acts
1?i?len(pending)
~w ? ?act(i)
5
if choice ? allowed then6
(parent, child)? edgeFor(choice)7
Arcs.add( (parent, child) )8
pending.remove(child)9
else10
good? arg max
act(j)?allowed
~w ? ?act(j)
11
~w ? ~w + ?good ? ?choice12
end13
return ~w14
Function isValid(action,Gold,Arcs)
(p, c)? edgeFor(action)1
if (?c? : (c, c?) ? Gold ? (c, c?) 6? Arcs)2
? (p, c) 6? Gold then
return false3
return true4
The function isV alid(act(i), gold, arcs) (line 4)
is used to decide if the chosen action/location pair
is valid. It returns True if two conditions apply: (a)
(pi, pj) is present in gold, (b) all edges (2, pj) in
gold are also in arcs. In words, the function verifies
that the proposed edge is indeed present in the gold
parse and that the suggested daughter already found
all its own daughters.2
2This is in line with the Arc-Standard parsing strategy of
shift-reduce dependency parsers (Nivre, 2004). We are cur-
rently experimenting also with an Arc-Eager variant of the non-
5 Feature Representation
The feature representation for an action can take
into account the original sentence, as well as
the entire parse history: ?act(i) above is actually
?(act(i), sentence,Arcs, pending).
We use binary valued features, and each feature is
conjoined with the type of action.
When designing the feature representation, we
keep in mind that our features should not only di-
rect the parser toward desired actions and away from
undesired actions, but also provide the parser with
means of choosing between several desired actions.
We want the parser to be able to defer some desired
actions until more structure is available and a more
informed prediction can be made. This desire is re-
flected in our choice of features: some of our fea-
tures are designed to signal to the parser the pres-
ence of possibly ?incomplete? structures, such as an
incomplete phrase, a coordinator without conjuncts,
and so on.
When considering an action ACTION(i), we limit
ourselves to features of partial structures around the
attachment point: pi?2, pi?1, pi, pi+1, pi+2, pi+3,
that is the two structures which are to be attached by
the action (pi and pi+1), and the two neighbouring
structures on each side3.
While these features encode local context, it is lo-
cal in terms of syntactic structure, and not purely in
terms of sentence surface form. This let us capture
some, though not all, long-distance relations.
For a partial structure p, we use wp to refer to
the head word form, tp to the head word POS tag,
and lcp and rcp to the POS tags of the left-most and
right-most child of p respectively.
All our prepositions (IN) and coordinators (CC)
are lexicalized: for them, tp is in fact wptp.
We define structural, unigram, bigram and pp-
attachment features.
The structural features are: the length of the
structures (lenp), whether the structure is a word
(contains no children: ncp), and the surface distance
between structure heads (?pipj ). The unigram and
bigram features are adapted from the feature set for
left-to-right Arc-Standard dependency parsing de-
directional algorithm.
3Our sentences are padded from each side with sentence de-
limiter tokens.
745
Structural
for p in pi?2, pi?1, pi, pi+1, pi+2, pi+3 lenp , ncp
for p,q in (pi?2, pi?1),(pi?1, pi),(pi, pi+1),(pi+1, pi+ 2),(pi+2, pi+3) ?qp , ?qptptq
Unigram
for p in pi?2, pi?1, pi, pi+1, pi+2, pi+3 tp , wp , tplcp , tprcp , tprcplcp
Bigram
for p,q in (pi, pi+1),(pi, pi+2),(pi?1, pi),(pi?1, pi+2),(pi+1, pi+2) tptq , wpwq , tpwq , wptq
tptqlcplcq , tptqrcplcq
tptqlcprcq , tptqrcprcq
PP-Attachment
if pi is a preposition wpi?1wpircpi , tpi?1wpircwpi
if pi+1 is a preposition wpi?1wpi+1rcpi+1 , tpi?1wpi+1rcwpi+1
wpiwpi+1rcpi+1 , tpiwpi+1rcwpi+1
if pi+2 is a preposition wpi+1wpi+2rcpi+2 , tpi+1wpi+2rcwpi+2
wpiwpi+2rcpi+2 , tpiwpi+2rcwpi+2
Figure 2: Feature Templates
scribed in (Huang et al, 2009). We extended that
feature set to include the structure on both sides of
the proposed attachment point.
In the case of unigram features, we added features
that specify the POS of a word and its left-most and
right-most children. These features provide the non-
directional model with means to prefer some attach-
ment points over others based on the types of struc-
tures already built. In English, the left- and right-
most POS-tags are good indicators of constituency.
The pp-attachment features are similar to the bi-
gram features, but fire only when one of the struc-
tures is headed by a preposition (IN). These features
are more lexicalized than the regular bigram fea-
tures, and include also the word-form of the right-
most child of the PP (rcwp). This should help the
model learn lexicalized attachment preferences such
as (hit, with-bat).
Figure 2 enumerate the feature templates we use.
6 Computational Complexity and Efficient
Implementation
The parsing algorithm (Algorithm 1) begins with
n+1 disjoint structures (the words of the sentence +
ROOT symbol), and terminates with one connected
structure. Each iteration of the main loop connects
two structures and removes one of them, and so the
loop repeats for exactly n times.
The argmax in line 5 selects the maximal scoring
action/location pair. At iteration i, there are n ? i
locations to choose from, and a naive computation of
the argmax isO(n), resulting in anO(n2) algorithm.
Each performed action changes the partial struc-
tures and with it the extracted features and the com-
puted scores. However, these changes are limited
to a fixed local context around the attachment point
of the action. Thus, we observe that the feature ex-
traction and score calculation can be performed once
for each action/location pair in a given sentence, and
reused throughout all the iterations. After each iter-
ation we need to update the extracted features and
calculated scores for only k locations, where k is a
fixed number depending on the window size used in
the feature extraction, and usually k  n.
Using this technique, we perform only (k + 1)n
feature extractions and score calculations for each
sentence, that is O(n) feature-extraction operations
per sentence.
Given the scores for each location, the argmax can
then be computed in O(logn) time using a heap,
resulting in an O(nlogn) algorithm: n iterations,
where the first iteration involves n feature extrac-
tion operations and n heap insertions, and each sub-
sequent iteration involves k feature extractions and
heap updates.
We note that the dominating factor in polynomial-
time discriminative parsers, is by far the feature-
extraction and score calculation. It makes sense to
compare parser complexity in terms of these opera-
tions only.4 Table 1 compares the complexity of our
4Indeed, in our implementation we do not use a heap, and
opt instead to find the argmax using a simple O(n) max oper-
ation. This O(n2) algorithm is faster in practice than the heap
based one, as both are dominated by the O(n) feature extrac-
tion, while the cost of the O(n) max calculationis negligible
compared to the constants involved in heap maintenance.
746
parser to other dependency parsing frameworks.
Parser Runtime Features / Scoring
MALT O(n) O(n)
MST O(n3) O(n2)
MST2 O(n3) O(n3)
BEAM O(n ? beam) O(n ? beam)
NONDIR (This Work) O(nlogn) O(n)
Table 1: Complexity of different parsing frameworks.
MST: first order MST parser, MST2: second order MST
parser, MALT: shift-reduce left-to-right parsing. BEAM:
beam search parser, as in (Zhang and Clark, 2008)
In terms of feature extraction and score calcula-
tion operations, our algorithm has the same cost as
traditional shift-reduce (MALT) parsers, and is an
order of magnitude more efficient than graph-based
(MST) parsers. Beam-search decoding for left-to-
right parsers (Zhang and Clark, 2008) is also linear,
but has an additional linear dependence on the beam-
size. The reported results in (Zhang and Clark,
2008) use a beam size of 64, compared to our con-
stant of k = 6.
Our Python-based implementation5 (the percep-
tron is implemented in a C extension module) parses
about 40 tagged sentences per second on an Intel
based MacBook laptop.
7 Experiments and Results
We evaluate the parser using the WSJ Treebank. The
trees were converted to dependency structures with
the Penn2Malt conversion program,6 using the head-
finding rules from (Yamada and Matsumoto, 2003).7
We use Sections 2-21 for training, Section 22 for
development, and Section 23 as the final test set.
The text is automatically POS tagged using a trigram
HMM based POS tagger prior to training and pars-
ing. Each section is tagged after training the tagger
on all other sections. The tagging accuracy of the
tagger is 96.5 for the training set and 96.8 for the
test set. While better taggers exist, we believe that
the simpler HMM tagger overfits less, and is more
5http://www.cs.bgu.ac.il/?yoavg/software/
6http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
7While other and better conversions exist (see, e.g., (Johans-
son and Nugues, 2007; Sangati and Mazza, 2009)), this con-
version heuristic is still the most widely used. Using the same
conversion facilitates comparison with previous works.
representative of the tagging performance on non-
WSJ corpus texts.
Parsers We evaluate our parser against the
transition-based MALT parser and the graph-based
MST parser. We use version 1.2 of MALT parser8,
with the settings used for parsing English in the
CoNLL 2007 shared task. For the MST parser9,
we use the default first-order, projective parser set-
tings, which provide state-of-the-art results for En-
glish. All parsers are trained and tested on the same
data. Our parser is trained for 20 iterations.
Evaluation Measures We evaluate the parsers using
three common measures:
(unlabeled) Accuracy: percentage of tokens which
got assigned their correct parent.
Root: The percentage of sentences in which the
ROOT attachment is correct.
Complete: the percentage of sentences in which all
tokens were assigned their correct parent.
Unlike most previous work on English dependency
parsing, we do not exclude punctuation marks from
the evaluation.
Results are presented in Table 2. Our non-
directional easy-first parser significantly outper-
forms the left-to-right greedy MALT parser in terms
of accuracy and root prediction, and significantly
outperforms both parsers in terms of exact match.
The globally optimized MST parser is better in root-
prediction, and slightly better in terms of accuracy.
We evaluated the parsers also on the English
dataset from the CoNLL 2007 shared task. While
this dataset is also derived from the WSJ Treebank, it
differs from the previous dataset in two important as-
pects: it is much smaller in size, and it is created us-
ing a different conversion procedure, which is more
linguistically adequate. For these experiments, we
use the dataset POS tags, and the same parameters as
in the previous set of experiments: we train the non-
directional parser for 20 iterations, with the same
feature set. The CoNLL dataset contains some non-
projective constructions. MALT and MST deal with
non-projectivity. For the non-directional parser, we
projectivize the training set prior to training using
the procedure described in (Carreras, 2007).
Results are presented in Table 3.
8http://maltparser.org/dist/1.2/malt-1.2.tar.gz
9http://sourceforge.net/projects/mstparser/
747
Parser Accuracy Root Complete
MALT 88.36 87.04 34.14
MST 90.05 93.95 34.64
NONDIR (this work) 89.70 91.50 37.50
Table 2: Unlabeled dependency accuracy on PTB Section
23, automatic POS-tags, including punctuation.
Parser Accuracy Root Complete
MALT 85.82 87.85 24.76
MST 89.08 93.45 24.76
NONDIR (this work) 88.34 91.12 29.43
Table 3: Unlabeled dependency accuracy on CoNLL
2007 English test set, including punctuation.
While all models suffer from the move to the
smaller dataset and the more challenging annotation
scheme, the overall story remains the same: the non-
directional parser is better than MALT but not as
good as MST in terms of parent-accuracy and root
prediction, and is better than both MALT and MST
in terms of producing complete correct parses.
That the non-directional parser has lower accu-
racy but more exact matches than the MST parser
can be explained by it being a deterministic parser,
and hence still vulnerable to error propagation: once
it erred once, it is likely to do so again, result-
ing in low accuracies for some sentences. How-
ever, due to the easy-first policy, it manages to parse
many sentences without a single error, which lead
to higher exact-match scores. The non-directional
parser avoids error propagation by not making the
initial error. On average, the non-directional parser
manages to assign correct heads to over 60% of the
tokens before making its first error.
The MST parser would have ranked 5th in the
shared task, and NONDIR would have ranked 7th.
The better ranking systems in the shared task
are either higher-order global models, beam-search
based systems, or ensemble-based systems, all of
which are more complex and less efficient than the
NONDIR parser.
Parse Diversity The parses produced by the non-
directional parser are different than the parses pro-
duced by the graph-based and left-to-right parsers.
To demonstrate this difference, we performed an Or-
acle experiment, in which we combine the output of
several parsers by choosing, for each sentence, the
parse with the highest score. Results are presented
Combination Accuracy Complete
Penn2Malt, Train 2-21, Test 23
MALT+MST 92.29 44.03
NONDIR+MALT 92.19 45.48
NONDIR+MST 92.53 44.41
NONDIR+MST+MALT 93.54 49.79
CoNLL 2007
MALT+MST 91.50 33.64
NONDIR+MALT 91.02 34.11
NONDIR+MST 91.90 34.11
NONDIR+MST+MALT 92.70 38.31
Table 4: Parser combination with Oracle, choosing the
highest scoring parse for each sentence of the test-set.
in Table 4.
A non-oracle blending of MALT+MST+NONDIR
using Sagae and Lavie?s (2006) simplest combina-
tion method assigning each component the same
weight, yield an accuracy of 90.8 on the CoNLL
2007 English dataset, making it the highest scoring
system among the participants.
7.1 Error Analysis / Limitations
When we investigate the POS category of mistaken
instances, we see that for all parsers, nodes with
structures of depth 2 and more which are assigned
an incorrect head are predominantly PPs (headed
by ?IN?), followed by NPs (headed by ?NN?). All
parsers have a hard time dealing with PP attachment,
but MST parser is better at it than NONDIR, and both
are better than MALT.
Looking further at the mistaken instances, we no-
tice a tendency of the PP mistakes of the NONDIR
parser to involve, before the PP, an NP embedded
in a relative clause. This reveals a limitation of our
parser: recall that for an edge to be built, the child
must first acquire all its own children. This means
that in case of relative clauses such as ?I saw the
boy [who ate the pizza] with my eyes?, the parser
must decide if the PP ?with my eyes? should be at-
tached to ?the pizza? or not before it is allowed to
build parts of the outer NP (?the boy who. . . ?). In
this case, the verb ?saw? and the noun ?boy? are
both outside of the sight of the parser when decid-
ing on the PP attachment, and it is forced to make a
decision in ignorance, which, in many cases, leads
to mistakes. The globally optimized MST does not
suffer as much from such cases. We plan to address
this deficiency in future work.
748
8 Related Work
Deterministic shift-reduce parsers are restricted by a
strict left-to-right processing order. Such parsers can
rely on rich syntactic information on the left, but not
on the right, of the decision point. They are forced
to commit early, and suffer from error propagation.
Our non-directional parser addresses these deficien-
cies by discarding the strict left-to-right processing
order, and attempting to make easier decisions be-
fore harder ones. Other methods of dealing with
these deficiencies were proposed over the years:
Several Passes Yamada and Matsumoto?s (2003)
pioneering work introduces a shift-reduce parser
which makes several left-to-right passes over a sen-
tence. Each pass adds structure, which can then be
used in subsequent passes. Sagae and Lavie (2006b)
extend this model to alternate between left-to-right
and right-to-left passes. This model is similar to
ours, in that it attempts to defer harder decisions to
later passes over the sentence, and allows late deci-
sions to make use of rich syntactic information (built
in earlier passes) on both sides of the decision point.
However, the model is not explicitly trained to op-
timize attachment ordering, has an O(n2) runtime
complexity, and produces results which are inferior
to current single-pass shift-reduce parsers.
Beam Search Several researchers dealt with the
early-commitment and error propagation of deter-
ministic parsers by extending the greedy decisions
with various flavors of beam-search (Sagae and
Lavie, 2006a; Zhang and Clark, 2008; Titov and
Henderson, 2007). This approach works well and
produces highly competitive results. Beam search
can be incorporated into our parser as well. We leave
this investigation to future work.
Strict left-to-right ordering is also prevalent in se-
quence tagging. Indeed, one major influence on
our work is Shen et.al.?s bi-directional POS-tagging
algorithm (Shen et al, 2007), which combines a
perceptron learning procedure similar to our own
with beam search to produce a state-of-the-art POS-
tagger, which does not rely on left-to-right process-
ing. Shen and Joshi (2008) extends the bidirectional
tagging algorithm to LTAG parsing, with good re-
sults. We build on top of that work and present a
concrete and efficient greedy non-directional depen-
dency parsing algorithm.
Structure Restrictions Eisner and Smith (2005)
propose to improve the efficiency of a globally op-
timized parser by posing hard constraints on the
lengths of arcs it can produce. Such constraints
pose an explicit upper bound on parser accuracy.10
Our parsing model does not pose such restrictions.
Shorter edges are arguably easier to predict, and our
parses builds them early in time. However, it is
also capable of producing long dependencies at later
stages in the parsing process. Indeed, the distribu-
tion of arc lengths produced by our parser is similar
to those produced by the MALT and MST parsers.
9 Discussion
We presented a non-directional deterministic depen-
dency parsing algorithm, which is not restricted by
the left-to-right parsing order of other deterministic
parsers. Instead, it works in an easy-first order. This
strategy allows using more context at each decision.
The parser learns both what and when to connect.
We show that this parsing algorithm significantly
outperforms a left-to-right deterministic algorithm.
While it still lags behind globally optimized pars-
ing algorithms in terms of accuracy and root pre-
diction, it is much better in terms of exact match,
and much faster. As our parsing framework can eas-
ily and efficiently utilize more structural information
than globally optimized parsers, we believe that with
some enhancements and better features, it can out-
perform globally optimized algorithms, especially
when more structural information is needed, such as
for morphologically rich languages.
Moreover, we show that our parser produces
different structures than those produced by both
left-to-right and globally optimized parsers, mak-
ing it a good candidate for inclusion in an ensem-
ble system. Indeed, a simple combination scheme
of graph-based, left-to-right and non-directional
parsers yields state-of-the-art results on English de-
pendency parsing on the CoNLL 2007 dataset.
We hope that further work on this non-directional
parsing framework will pave the way to better under-
standing of an interesting cognitive question: which
kinds of parsing decisions are hard to make, and
which linguistic constructs are hard to analyze?
10In (Dreyer et al, 2006), constraints are chosen ?to be the
minimum value that will allow recovery of 90% of the left
(right) dependencies in the training corpus?.
749
References
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proc. of CoNLL.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. of CoNLL
Shared Task, EMNLP-CoNLL.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In Proc of EMNLP.
Markus Dreyer, David A. Smith, and Noah A. Smith.
2006. Vine parsing and minimum risk reranking for
speed and precision. In Proc. of CoNLL, pages 201?
205, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Jason Eisner and Noah A. Smith. 2005. arsing with soft
and hard constraints on dependency length. In Proc.
of IWPT.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proc of EMNLP.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
Proc of NODALIDA.
Ryan McDonald and Joakim Nivre. 2007. Characteriz-
ing the errors of data-driven dependency parsing mod-
els. In Proc. of EMNLP.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proc of EACL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proc of ACL.
Tetsuji Nakagawa. 2007. Multilingual dependency pars-
ing using global features. In Proc. of EMNLP-CoNLL.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proc. of ACL, pages 950?958, Columbus, Ohio,
June. Association for Computational Linguistics.
Joakim Nivre, Johan Hall, and Jens Nillson. 2006. Malt-
Parser: A data-driven parser-generator for dependency
parsing. In Proc. of LREC.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mcdon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proc. of EMNLP-CoNLL.
Joakim Nivre. 2004. Incrementality in deterministic de-
pendency parsing. In Incremental Parsing: Bringing
Engineering and Cognition Together, ACL-Workshop.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective depen-
dency parsing. In Proc. of EMNLP 2006, July.
Kenji Sagae and Alon Lavie. 2006a. A best-first proba-
bilistic shift-reduce parser. In Proc of ACL.
Kenji Sagae and Alon Lavie. 2006b. Parser combination
by reparsing. In Proc of NAACL.
Federico Sangati and Chiara Mazza. 2009. An english
dependency treebank a` la tesnie`re. In Proc of TLT8.
Libin Shen and Aravind K. Joshi. 2008. Ltag depen-
dency parsing with bidirectional incremental construc-
tion. In Proc of EMNLP.
Libin Shen, Giorgio Satta, and Aravind K. Joshi. 2007.
Guided learning for bidirectional sequence classifica-
tion. In Proc of ACL.
Ivan Titov and James Henderson. 2007. Fast and robust
multilingual dependency parsing with a generative la-
tent variable model. In Proc. of EMNLP-CoNLL.
Yamada and Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In Proc. of
IWPT.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-based
and transition-based dependency parsing using beam-
search. In Proc of EMNLP.
750
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 212?216,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Language-Independent Parsing with Empty Elements
Shu Cai and David Chiang
USC Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
{shucai,chiang}@isi.edu
Yoav Goldberg
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
yoavg@cs.bgu.ac.il
Abstract
We  present  a  simple, language-independent
method for integrating recovery of empty ele-
ments into syntactic parsing. This method out-
performs  the  best  published  method  we  are
aware of on English and a recently published
method on Chinese.
1 Introduction
Empty elements in the syntactic analysis of a sen-
tence are markers that show where a word or phrase
might otherwise be expected to appear, but does not.
They play an important role in understanding the
grammatical relations in the sentence. For example,
in the tree of Figure 2a, the first empty element (*)
marks where John would be if believed were in the
active voice (someone believed. . .), and the second
empty element (*T*) marks where the manwould be
ifwhowere not fronted (John was believed to admire
who?).
Empty elements exist in many languages and serve
different purposes. In languages such as Chinese and
Korean, where subjects and objects can be dropped
to avoid duplication, empty elements are particularly
important, as they indicate the position of dropped
arguments. Figure 1 gives an example of a Chinese
parse tree with empty elements. The first empty el-
ement (*pro*) marks the subject of the whole sen-
tence, a pronoun inferable from context. The second
empty element (*PRO*) marks the subject of the de-
pendent VP (sh?sh? f?l? ti?ow?n).
The Penn Treebanks (Marcus et  al., 1993; Xue
et al, 2005) contain detailed annotations of empty
elements. Yet  most  parsing  work  based  on  these
resources has ignored empty elements, with some
.IP
. .VP
. .VP
. .IP
. .VP
. .NP
. .NN
.??
ti?ow?n
clause
.
NN
.??
f?l?
law
.
VV
.??
sh?sh?
implement
.
NP
.-NONE-
.*PRO*
.
VV
.??
zh?ngzh?
suspend
.
ADVP
.AD
.??
z?nsh?
for now
.
NP
.-NONE-
.*pro*
Figure 1: Chinese parse tree with empty elements marked.
The meaning of the sentence is, ?Implementation of the
law is temporarily suspended.?
notable exceptions. Johnson (2002) studied empty-
element  recovery in English, followed by several
others (Dienes and Dubey, 2003; Campbell, 2004;
Gabbard et al, 2006); the best results we are aware of
are due to Schmid (2006). Recently, empty-element
recovery for Chinese has begun to receive attention:
Yang and Xue (2010) treat it as classification prob-
lem, while Chung and Gildea (2010) pursue several
approaches for both Korean and Chinese, and ex-
plore applications to machine translation.
Our intuition motivating this work is that empty
elements are an integral part of syntactic structure,
and should be constructed jointly with it, not added
in afterwards. Moreover, we expect empty-element
recovery to improve as the parsing quality improves.
Our method makes use of a strong syntactic model,
the PCFGs with latent annotation of Petrov et al
(2006), which  we  extend  to  predict  empty  cate-
212
gories  by the  use  of lattice  parsing. The method
is language-independent and performs very well on
both languages we tested it on: for English, it out-
performs the best published method we are aware of
(Schmid, 2006), and for Chinese, it outperforms the
method of Yang and Xue (2010).
1
2 Method
Our method is fairly simple. We take a state-of-the-
art parsing model, the Berkeley parser (Petrov et al,
2006), train it on data with explicit empty elements,
and test it on word lattices that can nondeterminis-
tically insert empty elements anywhere. The idea is
that the state-splitting of the parsing model will en-
able it to learn where to expect empty elements to be
inserted into the test sentences.
Tree transformations Prior to training, we alter
the annotation of empty elements so that the termi-
nal label is a consistent symbol (?), the preterminal
label is the type of the empty element, and -NONE-
is deleted (see Figure 2b). This simplifies the lat-
tices because there is only one empty symbol, and
helps the parsing model to learn dependencies be-
tween nonterminal labels and empty-category types
because there is no intervening -NONE-.
Then, following Schmid (2006), if a constituent
contains an empty element that is linked to another
node with label X, then we append /X to its label.
If there is more than one empty element, we pro-
cess them bottom-up (see Figure 2b). This helps the
parser learn to expect where to find empty elements.
In our experiments, we did this only for elements of
type *T*. Finally, we train the Berkeley parser on the
preprocessed training data.
Lattice parsing Unlike the training data, the test
data does not mark any empty elements. We allow
the parser to produce empty elements by means of
lattice-parsing (Chappelier et al, 1999), a general-
ization of CKY parsing allowing it to parse a word-
lattice instead of a predetermined list of terminals.
Lattice parsing adds a layer of flexibility to exist-
ing parsing technology, and allows parsing in sit-
uations where the yield of  the tree  is  not  known
in advance. Lattice parsing originated in the speech
1
Unfortunately, not  enough  information  was  available  to
carry out comparison with the method of Chung and Gildea
(2010).
processing community  (Hall, 2005; Chappelier  et
al., 1999), and  was  recently  applied  to  the  task
of joint clitic-segmentation and syntactic-parsing in
Hebrew  (Goldberg  and  Tsarfaty, 2008; Goldberg
and Elhadad, 2011) and Arabic (Green and Man-
ning, 2010). Here, we use lattice parsing for empty-
element recovery.
We use a modified version of the Berkeley parser
which allows handling lattices as input.
2
The modifi-
cation is fairly straightforward: Each lattice arc cor-
respond to a lexical item. Lexical items are now in-
dexed by their start and end states rather than by
their sentence position, and the initialization proce-
dure of the CKY chart is changed to allow lexical
items of spans greater than 1. We then make the nec-
essary adjustments to the parsing algorithm to sup-
port this change: trying rules involving preterminals
even when the span is greater than 1, and not relying
on span size for identifying lexical items.
At test time, we first construct a lattice for each
test sentence that allows 0, 1, or 2 empty symbols
(?) between each pair of words or at the start/end of
the sentence. Then we feed these lattices through our
lattice parser to produce trees with empty elements.
Finally, we reverse the transformations that had been
applied to the training data.
3 Evaluation Measures
Evaluation metrics for empty-element recovery are
not well established, and previous studies use a vari-
ety of metrics. We review several of these here and
additionally propose a unified evaluation of parsing
and empty-element recovery.
3
If A and B are multisets, let A(x) be the number
of occurrences of x in A, let |A| = ?x A(x), and
let A ? B be the multiset such that (A ? B)(x) =
min(A(x), B(x)). If T is the multiset of ?items? in the
trees being tested andG is the multiset of ?items? in
the gold-standard trees, then
precision =
|G ? T |
|T | recall =
|G ? T |
|G|
F1 =
2
1
precision
+
1
recall
2
The modified parser is available at http://www.cs.bgu.
ac.il/~yoavg/software/blatt/
3
We provide a scoring script which supports all of these eval-
uation metrics. The code is available at http://www.isi.edu/
~chiang/software/eevalb.py .
213
.SBARQ
. .SQ
. .VP
. .S
. .VP
. .VP
. .NP
.-NONE-
.*T*
.
VB
.admire
.
TO
.to
.
NP
.-NONE-
.*
.
VBN
.believed
.
.NP
.NNP
.John
.
VBZ
.is
.
WHNP
.WP
.who
.SBARQ
. .SQ/WHNP
. .VP/WHNP/NP
. .S/WHNP/NP
. .VP/WHNP
. .VP/WHNP
. .NP/WHNP
.*T*
.?
.
VB
.admire
.
TO
.to
.
NP
.*
.?
.
VBN
.believed
.
.NP
.NNP
.John
.
VBZ
.is
.
WHNP
.WP
.who
(a) (b)
Figure 2: English parse tree with empty elements marked. (a) As annotated in the Penn Treebank. (b) With empty
elements reconfigured and slash categories added.
where ?items? are defined differently for each met-
ric, as  follows. Define  a nonterminal node, for
present purposes, to be a node which is neither a ter-
minal nor preterminal node.
The  standard  PARSEVAL metric  (Black  et  al.,
1991) counts labeled nonempty brackets: items are
(X, i, j) for each nonempty nonterminal node, where
X is its label and i, j are the start and end positions
of its span.
Yang  and  Xue  (2010)  simply  count unlabeled
empty elements: items are (i, i) for each empty ele-
ment, where i is its position. If multiple empty ele-
ments occur at the same position, they only count the
last one.
The metric originally proposed by Johnson (2002)
counts labeled empty brackets: items are (X/t, i, i) for
each empty nonterminal node, where X is its label
and t is the type of the empty element it dominates,
but also (t, i, i) for each empty element not domi-
nated by an empty nonterminal node.
4
The following
structure has an empty nonterminal dominating two
empty elements:
.SBAR
. .S
.-NONE-
.*T*
.
-NONE-
.0
Johnson  counts  this  as (SBAR, i, i), (S/*T*, i, i);
Schmid  (2006)  counts  it  as  a  single
4
This happens in the Penn Treebank for types *U* and 0, but
never in the Penn Chinese Treebank except by mistake.
(SBAR-S/*T*, i, i).5 We  tried  to  follow  Schmid
in a generic way: we collapse any vertical chain of
empty nonterminals into a single nonterminal.
In order to avoid problems associated with cases
like this, we suggest a pair of simpler metrics. The
first is to count labeled empty elements, i.e., items
are (t, i, i) for each empty element, and the second,
similar in spirit to SParseval (Roark et al, 2006), is
to count all labeled brackets, i.e., items are (X, i, j)
for  each nonterminal  node (whether  nonempty or
empty). These two metrics, together with part-of-
speech accuracy, cover all possible nodes in the tree.
4 Experiments and Results
English As is standard, we trained the parser on
sections 02?21 of  the Penn Treebank Wall  Street
Journal corpus, used section 00 for development, and
section 23 for testing. We ran 6 cycles of training;
then, because we were unable to complete the 7th
split-merge cycle with the default setting of merg-
ing 50% of splits, we tried increasing merges to 75%
and ran 7 cycles of training. Table 1 presents our
results. We chose the parser settings that gave the
best labeled empty elements F1 on the dev set, and
used these settings for the test set. We outperform the
state of the art at recovering empty elements, as well
as achieving state of the art accuracy at recovering
phrase structure.
5
This difference is not small; scores using Schmid?s metric
are lower by roughly 1%. There are other minor differences in
Schmid?s metric which we do not detail here.
214
Labeled Labeled All Labeled
Empty Brackets Empty Elements Brackets
Section System P R F1 P R F1 P R F1
00 Schmid (2006) 88.3 82.9 85.5 89.4 83.8 86.5 87.1 85.6 86.3
split 5? merge 50% 91.0 79.8 85.0 93.1 81.8 87.1 90.4 88.7 89.5
split 6? merge 50% 91.9 81.1 86.1 93.6 82.4 87.6 90.4 89.1 89.7
split 6? merge 75% 92.7 80.7 86.3 94.6 82.0 87.9 90.3 88.5 89.3
split 7? merge 75% 91.0 80.4 85.4 93.2 82.1 87.3 90.5 88.9 89.7
23 Schmid (2006) 86.1 81.7 83.8 87.9 83.0 85.4 86.8 85.9 86.4
split 6? merge 75% 90.1 79.5 84.5 92.3 80.9 86.2 90.1 88.5 89.3
Table 1: Results on Penn (English) Treebank, Wall Street Journal, sentences with 100 words or fewer.
Unlabeled Labeled All Labeled
Empty Elements Empty Elements Brackets
Task System P R F1 P R F1 P R F1
Dev split 5? merge 50% 82.5 58.0 68.1 72.6 51.8 60.5 84.6 80.7 82.6
split 6? merge 50% 76.4 60.5 67.5 68.2 55.1 60.9 83.2 81.3 82.2
split 7? merge 50% 74.9 58.7 65.8 65.9 52.5 58.5 82.7 81.1 81.9
Test Yang and Xue (2010) 80.3 57.9 63.2
split 6? merge 50% 74.0 61.3 67.0 66.0 54.5 58.6 82.7 80.8 81.7
Table 2: Results on Penn (Chinese) Treebank.
Chinese We  also  experimented  on  a  subset  of
the  Penn  Chinese  Treebank  6.0. For  comparabil-
ity  with  previous  work  (Yang  and  Xue, 2010),
we trained the parser on sections 0081?0900, used
sections 0041?0080 for development, and sections
0001?0040 and 0901?0931 for testing. The results
are shown in Table 2.We selected the 6th split-merge
cycle based on the labeled empty elements F1 mea-
sure. The unlabeled empty elements column shows
that our system outperforms the baseline system of
Yang and Xue (2010). We also analyzed the empty-
element recall by type (Table 3). Our system outper-
formed that of Yang and Xue (2010) especially on
*pro*, used for dropped arguments, and *T*, used
for relative clauses and topicalization.
5 Discussion and Future Work
The  empty-element  recovery  method  we  have
presented  is  simple, highly  effective, and  fully
integrated with  state  of  the  art  parsing. We hope
to  exploit  cross-lingual  information  about  empty
elements  in  machine  translation. Chung  and
Gildea (2010)  have  shown that  such  information
indeed helps translation, and we plan to extend this
work  by  handling  more  empty  categories  (rather
Total Correct Recall
Type Gold YX Ours YX Ours
*pro* 290 125 159 43.1 54.8
*PRO* 299 196 199 65.6 66.6
*T* 578 338 388 58.5 67.1
*RNR* 32 20 15 62.5 46.9
*OP* 134 20 65 14.9 48.5
* 19 5 3 26.3 15.8
Table 3: Recall on different types of empty categories.
YX = (Yang and Xue, 2010), Ours = split 6?.
than just *pro* and *PRO*), and to incorporate them
into a syntax-based translation model instead of a
phrase-based model.
We also plan to extend our work here to recover
coindexation information (links between a moved el-
ement and the trace which marks the position it was
moved from). As a step towards shallow semantic
analysis, this may further benefit other natural lan-
guage processing tasks such as machine translation
and summary generation.
Acknowledgements
We would like to thank Slav Petrov for his help in
running the Berkeley parser, and Yaqin Yang, Bert
215
Xue, Tagyoung Chung, and Dan Gildea for their an-
swering our  many questions. We would also like
to  thank  our  colleagues  in  the  Natural  Language
Group  at  ISI for  meaningful  discussions  and  the
anonymous reviewers for their thoughtful sugges-
tions. This work was supported in part by DARPA
under contracts HR0011-06-C-0022 (subcontract to
BBN Technologies) and DOI-NBC N10AP20031,
and by NSF under contract IIS-0908532.
References
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage of
English grammars. In Proc. DARPA Speech and Natu-
ral Language Workshop.
Richard Campbell. 2004. Using linguistic principles to
recover empty categories. In Proc. ACL.
J.-C. Chappelier, M. Rajman, R. Aragu?es, and A. Rozen-
knop. 1999. Lattice parsing for speech recognition.
In Proc. Traitement Automatique du Langage Naturel
(TALN).
Tagyoung Chung and Daniel  Gildea. 2010. Effects
of empty categories on machine translation. In Proc.
EMNLP.
Pe?ter Dienes and Amit Dubey. 2003. Antecedent recov-
ery: Experiments with a trace tagger. In Proc. EMNLP.
Ryan Gabbard, Seth Kulick, and Mitchell Marcus. 2006.
Fully parsing the Penn Treebank. In Proc. NAACL
HLT.
Yoav Goldberg and Michael Elhadad. 2011. Joint He-
brew segmentation and parsing using a PCFG-LA lat-
tice parser. In Proc. of ACL.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and
syntactic parsing. In Proc. of ACL.
Spence Green and Christopher D. Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis. In
Proc of COLING-2010.
Keith B. Hall. 2005. Best-first word-lattice parsing:
techniques for integrated syntactic language modeling.
Ph.D. thesis, Brown University, Providence, RI, USA.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm  for  recovering  empty  nodes  and  their  an-
tecedents. In Proc. ACL.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19:313?330.
Slav  Petrov, Leon Barrett, Romain  Thibaux, and  Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. COLING-ACL.
Brian  Roark, Mary  Harper, Eugene  Charniak, Bonnie
Dorr, Mark Johnson, Jeremy G. Kahn, Yang Liu, Mari
Ostendorf, John Hale, Anna Krasnyanskaya, Matthew
Lease, Izhak Shafran, Matthew Snover, Robin Stewart,
and Lisa Yung. 2006. SParseval: Evaluation metrics
for parsing speech. In Proc. LREC.
Helmut Schmid. 2006. Trace prediction and recovery
with unlexicalized PCFGs and slash features. In Proc.
COLING-ACL.
Nianwen  Xue, Fei  Xia, Fu-dong  Chiou, and  Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Yaqin Yang and Nianwen Xue. 2010. Chasing the ghost:
recovering empty categories in the Chinese Treebank.
In Proc. COLING.
216
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 704?709,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Joint Hebrew Segmentation and Parsing
using a PCFG-LA Lattice Parser
Yoav Goldberg and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
{yoavg|elhadad}@cs.bgu.ac.il
Abstract
We experiment with extending a lattice pars-
ing methodology for parsing Hebrew (Gold-
berg and Tsarfaty, 2008; Golderg et al, 2009)
to make use of a stronger syntactic model: the
PCFG-LA Berkeley Parser. We show that the
methodology is very effective: using a small
training set of about 5500 trees, we construct
a parser which parses and segments unseg-
mented Hebrew text with an F-score of almost
80%, an error reduction of over 20% over the
best previous result for this task. This result
indicates that lattice parsing with the Berkeley
parser is an effective methodology for parsing
over uncertain inputs.
1 Introduction
Most work on parsing assumes that the lexical items
in the yield of a parse tree are fully observed, and
correspond to space delimited tokens, perhaps af-
ter a deterministic preprocessing step of tokeniza-
tion. While this is mostly the case for English, the
situation is different in languages such as Chinese,
in which word boundaries are not marked, and the
Semitic languages of Hebrew and Arabic, in which
various particles corresponding to function words
are agglutinated as affixes to content bearing words,
sharing the same space-delimited token. For exam-
ple, the Hebrew token bcl1 can be interpreted as
the single noun meaning ?onion?, or as a sequence
of a preposition and a noun b-cl meaning ?in (the)
shadow?. In such languages, the sequence of lexical
1We adopt here the transliteration scheme of (Sima?an et al,
2001)
items corresponding to an input string is ambiguous,
and cannot be determined using a deterministic pro-
cedure. In this work, we focus on constituency pars-
ing of Modern Hebrew (henceforth Hebrew) from
raw unsegmented text.
A common method of approaching the discrep-
ancy between input strings and space delimited to-
kens is using a pipeline process, in which the in-
put string is pre-segmented prior to handing it to a
parser. The shortcoming of this method, as noted
by (Tsarfaty, 2006), is that many segmentation de-
cisions cannot be resolved based on local context
alone. Rather, they may depend on long distance re-
lations and interact closely with the syntactic struc-
ture of the sentence. Thus, segmentation deci-
sions should be integrated into the parsing process
and not performed as an independent preprocess-
ing step. Goldberg and Tsarfaty (2008) demon-
strated the effectiveness of lattice parsing for jointly
performing segmentation and parsing of Hebrew
text. They experimented with various manual re-
finements of unlexicalized, treebank-derived gram-
mars, and showed that better grammars contribute
to better segmentation accuracies. Goldberg et al
(2009) showed that segmentation and parsing ac-
curacies can be further improved by extending the
lexical coverage of a lattice-parser using an exter-
nal resource. Recently, Green and Manning (2010)
demonstrated the effectiveness of lattice-parsing for
parsing Arabic.
Here, we report the results of experiments cou-
pling lattice parsing together with the currently best
grammar learning method: the Berkeley PCFG-LA
parser (Petrov et al, 2006).
704
2 Aspects of Modern Hebrew
Some aspects that make Hebrew challenging from a
language-processing perspective are:
Affixation Common function words are prefixed
to the following word. These include: m(?from?)
f (?who?/?that?) h(?the?) w(?and?) k(?like?) l(?to?)
and b(?in?). Several such elements may attach to-
gether, producing forms such as wfmhfmf (w-f-m-h-
fmf ?and-that-from-the-sun?). Notice that the last
part of the token, the noun fmf (?sun?), when ap-
pearing in isolation, can be also interpreted as the
sequence f-mf (?who moved?). The linear order
of such segmental elements within a token is fixed
(disallowing the reading w-f-m-h-f-mf in the previ-
ous example). However, the syntactic relations of
these elements with respect to the rest of the sen-
tence is rather free. The relativizer f (?that?) for
example may attach to an arbitrarily long relative
clause that goes beyond token boundaries. To fur-
ther complicate matters, the definite article h(?the?)
is not realized in writing when following the par-
ticles b(?in?),k(?like?) and l(?to?). Thus, the form
bbit can be interpreted as either b-bit (?in house?) or
b-h-bit (?in the house?). In addition, pronominal el-
ements may attach to nouns, verbs, adverbs, preposi-
tions and others as suffixes (e.g. lqxn(lqx-hn, ?took-
them?), elihm(eli-hm,?on them?)). These affixations
result in highly ambiguous token segmentations.
Relatively free constituent order The ordering of
constituents inside a phrase is relatively free. This
is most notably apparent in the verbal phrases and
sentential levels. In particular, while most sentences
follow an SVO order, OVS and VSO configurations
are also possible. Verbal arguments can appear be-
fore or after the verb, and in many ordering. This
results in long and flat VP and S structures and a fair
amount of sparsity.
Rich templatic morphology Hebrew has a very
productive morphological structure, which is based
on a root+template system. The productive mor-
phology results in many distinct word forms and a
high out-of-vocabulary rate which makes it hard to
reliably estimate lexical parameters from annotated
corpora. The root+template system (combined with
the unvocalized writing system and rich affixation)
makes it hard to guess the morphological analyses
of an unknown word based on its prefix and suffix,
as usually done in other languages.
Unvocalized writing system Most vowels are not
marked in everyday Hebrew text, which results in a
very high level of lexical and morphological ambi-
guity. Some tokens can admit as many as 15 distinct
readings.
Agreement Hebrew grammar forces morpholog-
ical agreement between Adjectives and Nouns
(which should agree on Gender and Number and
definiteness), and between Subjects and Verbs
(which should agree on Gender and Number).
3 PCFG-LA Grammar Estimation
Klein and Manning (2003) demonstrated that lin-
guistically informed splitting of non-terminal sym-
bols in treebank-derived grammars can result in ac-
curate grammars. Their work triggered investiga-
tions in automatic grammar refinement and state-
splitting (Matsuzaki et al, 2005; Prescher, 2005),
which was then perfected by (Petrov et al, 2006;
Petrov, 2009). The model of (Petrov et al, 2006) and
its publicly available implementation, the Berke-
ley parser2, works by starting with a bare-bones
treebank derived grammar and automatically refin-
ing it in split-merge-smooth cycles. The learning
works by iteratively (1) splitting each non-terminal
category in two, (2) merging back non-effective
splits and (3) smoothing the split non-terminals to-
ward their shared ancestor. Each of the steps is
followed by an EM-based parameter re-estimation.
This process allows learning tree annotations which
capture many latent syntactic interactions. At in-
ference time, the latent annotations are (approxi-
mately) marginalized out, resulting in the (approx-
imate) most probable unannotated tree according to
the refined grammar. This parsing methodology is
very robust, producing state of the art accuracies for
English, as well as many other languages including
German (Petrov and Klein, 2008), French (Candito
et al, 2009) and Chinese (Huang and Harper, 2009)
among others.
The grammar learning process is applied to bi-
narized parse trees, with 1st-order vertical and 0th-
order horizontal markovization. This means that in
2http://code.google.com/p/berkeleyparser/
705
Figure 1: Lattice representation of the sentence bclm hneim. Double-circles denote token boundaries. Lattice arcs correspond
to different segments of the token, each lattice path encodes a possible reading of the sentence. Notice how the token bclm have
analyses which include segments which are not directly present in the unsegmented form, such as the definite article h (1-3) and the
pronominal suffix which is expanded to the sequence fl hm (?of them?, 2-4, 4-5).
the initial grammar, each of the non-terminal sym-
bols is effectively conditioned on its parent alone,
and is independent of its sisters. This is a very
strong independence assumption. However, it al-
lows the resulting refined grammar to encode its own
set of dependencies between a node and its sisters, as
well as ordering preferences in long, flat rules. Our
initial experiments on Hebrew confirm that moving
to higher order horizontal markovization degrades
parsing performance, while producing much larger
grammars.
4 Lattice Representation and Parsing
Following (Goldberg and Tsarfaty, 2008) we deal
with the ambiguous affixation patterns in Hebrew by
encoding the input sentence as a segmentation lat-
tice. Each token is encoded as a lattice representing
its possible analyses, and the token-lattices are then
concatenated to form the sentence-lattice. Figure 1
presents the lattice for the two token sentence ?bclm
hneim?. Each lattice arc correspond to a lexical item.
Lattice Parsing The CKY parsing algorithm can
be extended to accept a lattice as its input (Chap-
pelier et al, 1999). This works by indexing lexi-
cal items by their start and end states in the lattice
instead of by their sentence position, and changing
the initialization procedure of CKY to allow termi-
nal and preterminal sybols of spans of sizes > 1. It is
then relatively straightforward to modify the parsing
mechanism to support this change: not giving spe-
cial treatments for spans of size 1, and distinguish-
ing lexical items from non-terminals by a specified
marking instead of by their position in the chart. We
modified the PCFG-LA Berkeley parser to accept
lattice input at inference time (training is performed
as usual on fully observed treebank trees).
Lattice Construction We construct the token lat-
tices using MILA, a lexicon-based morphological
analyzer which provides a set of possible analyses
for each token (Itai and Wintner, 2008). While being
a high-coverage lexicon, its coverage is not perfect.
For the future, we consider using unknown handling
techniques such as those proposed in (Adler et al,
2008). Still, the use of the lexicon for lattice con-
struction rather than relying on forms seen in the
treebank is essential to achieve parsing accuracy.
Lexical Probabilities Estimation Lexical p(t ?
w) probabilities are defined over individual seg-
ments rather than for complete tokens. It is the role
of the syntactic model to assign probabilities to con-
texts which are larger than a single segment. We
use the default lexical probability estimation of the
Berkeley parser.3
Goldberg et al (2009) suggest to estimate lexi-
cal probabilities for rare and unseen segments using
emission probabilities of an HMM tagger trained us-
ing EM on large corpora. Our preliminary exper-
iments with this method with the Berkeley parser
3Probabilities for robust segments (lexical items observed
100 times or more in training) are based on the MLE estimates
resulting from the EM procedure. Other segments are assigned
smoothed probabilities which combine the p(w|t) MLE esti-
mate with unigram tag probabilities. Segments which were not
seen in training are assigned a probability based on a single
distribution of tags for rare words. Crucially, we restrict each
segment to appear only with tags which are licensed by a mor-
phological analyzer, as encoded in the lattice.
706
showed mixed results. Parsing performance on the
test set dropped slightly.When analyzing the parsing
results on out-of-treebank text, we observed cases
where this estimation method indeed fixed mistakes,
and others where it hurt. We are still uncertain if the
slight drop in performance over the test set is due to
overfitting of the treebank vocabulary, or the inade-
quacy of the method in general.
5 Experiments and Results
Data In all the experiments we use Ver.2 of the
Hebrew treebank (Guthmann et al, 2009), which
was converted to use the tagset of the MILA mor-
phological analyzer (Golderg et al, 2009). We use
the same splits as in previous work, with a train-
ing set of 5240 sentences (484-5724) and a test set
of 483 sentences (1-483). During development, we
evaluated on a random subset of 100 sentences from
the training set. Unless otherwise noted, we used the
basic non-terminal categories, without any extended
information available in them.
Gold Segmentation and Tagging To assess the
adequacy of the Berkeley parser for Hebrew, we per-
formed baseline experiments in which either gold
segmentation and tagging or just gold segmenta-
tion were available to the parser. The numbers are
very high: an F-measure of about 88.8% for the
gold segmentation and tagging, and about 82.8% for
gold segmentation only. This shows the adequacy
of the PCFG-LA methodology for parsing the He-
brew treebank, but also goes to show the highly am-
biguous nature of the tagging. Our baseline lattice
parsing experiment (without the lexicon) results in
an F-score of around 76%.4
Segmentation ? Parsing pipeline As another
baseline, we experimented with a pipeline system
in which the input text is automatically segmented
and tagged using a state-of-the-art HMM pos-tagger
(Goldberg et al, 2008). We then ignore the pro-
duced tagging, and pass the resulting segmented text
as input to the PCFG-LA parsing model as a deter-
ministic input (here the lattice representation is used
while tagging, but the parser sees a deterministic,
4For all the joint segmentation and parsing experiments, we
use a generalization of parseval that takes segmentation into ac-
count. See (Tsarfaty, 2006) for the exact details.
segmented input).5 In the pipeline setting, we either
allow the parser to assign all possible POS-tags, or
restrict it to POS-tags licensed by the lexicon.
Lattice Parsing Experiments Our initial lattice
parsing experiments with the Berkeley parser were
disappointing. The lattice seemed too permissive,
allowing the parser to chose weird analyses. Error
analysis suggested the parser failed to distinguish
among the various kinds of VPs: finite, non-finite
and modals. Once we annotate the treebank verbs
into finite, non-finite and modals6, results improve
a lot. Further improvement was gained by specifi-
cally marking the subject-NPs.7 The parser was not
able to correctly learn these splits on its own, but
once they were manually provided it did a very good
job utilizing this information.8 Marking object NPs
did not help on their own, and slightly degraded the
performance when both subjects and objects were
marked. It appears that the learning procedure man-
aged to learn the structure of objects without our
help. In all the experiments, the use of the morpho-
logical analyzer in producing the lattice was crucial
for parsing accuracy.
Results Our final configuration (marking verbal
forms and subject-NPs, using the analyzer to con-
struct the lattice and training the parser for 5 itera-
tions) produces remarkable parsing accuracy when
parsing from unsegmented text: an F-score of
79.9% (prec: 82.3 rec: 77.6) and seg+tagging F of
93.8%. The pipeline systems with the same gram-
mar achieve substantially lower F-scores of 75.2%
(without the lexicon) and 77.3 (with the lexicon).
For comparison, the previous best results for pars-
ing Hebrew are 84.1%F assuming gold segmenta-
tion and tagging (Tsarfaty and Sima?an, 2010)9, and
73.7%F starting from unsegmented text (Golderg et
5The segmentation+tagging accuracy of the HMM tagger on
the Treebank data is 91.3%F.
6This information is available in both the treebank and the
morphological analyzer, but we removed it at first. Note that the
verb-type distinction is specified only on the pre-terminal level,
and not on the phrase-level.
7Such markings were removed prior to evaluation.
8Candito et al (2009) also report improvements in accu-
racy when providing the PCFG-LA parser with few manually-
devised linguistically-motivated state-splits.
9The 84.1 figure is for sentences of length ? 40, and thus
not strictly comparable with all the other numbers in this paper,
which are based on the entire test-set.
707
System Oracle OOV Handling Prec Rec F1
Tsarfaty and Sima?an 2010 Gold Seg+Tag ? - - 84.1
Goldberg et al 2009 None Lexicon 73.4 74.0 73.8
Seg ? PCFG-LA Pipeline None Treebank 75.6 74.8 75.2
Seg ? PCFG-LA Pipeline None Lexicon 79.5 75.2 77.3
PCFG-LA + Lattice (Joint) None Lexicon 82.3 77.6 79.9
Table 1: Parsing scores of the various systems
al., 2009). The numbers are summarized in Table 1.
While the pipeline system already improves over the
previous best results, the lattice-based joint-model
improves results even further. Overall, the PCFG-
LA+Lattice parser improve results by 6 F-points ab-
solute, an error reduction of about 20%. Tagging
accuracies are also remarkable, and constitute state-
of-the-art tagging for Hebrew.
The strengths of the system can be attributed to
three factors: (1) performing segmentation, tagging
and parsing jointly using lattice parsing, (2) relying
on an external resource (lexicon / morphological an-
alyzer) instead of on the Treebank to provide lexical
coverage and (3) using a strong syntactic model.
Running time The lattice representation effec-
tively results in longer inputs to the parser. It is
informative to quantify the effect of the lattice rep-
resentation on the parsing time, which is cubic in
sentence length. The pipeline parser parsed the
483 pre-segmented input sentences in 151 seconds
(3.2 sentences/second) not including segmentation
time, while the lattice parser took 175 seconds (2.7
sents/second) including lattice construction. Parsing
with the lattice representation is slower than in the
pipeline setup, but not prohibitively so.
Analysis and Limitations When analyzing the
learned grammar, we see that it learned to distin-
guish short from long constituents, models conjunc-
tion parallelism fairly well, and picked up a lot
of information regarding the structure of quantities,
dates, named and other kinds of NPs. It also learned
to reasonably model definiteness, and that S ele-
ments have at most one Subject. However, the state-
split model exhibits no notion of syntactic agree-
ment on gender and number. This is troubling, as
we encountered a fair amount of parsing mistakes
which would have been solved if the parser were to
use agreement information.
6 Conclusions and Future Work
We demonstrated that the combination of lattice
parsing with the PCFG-LA Berkeley parser is highly
effective. Lattice parsing allows much needed flexi-
bility in providing input to a parser when the yield of
the tree is not known in advance, and the grammar
refinement and estimation techniques of the Berke-
ley parser provide a strong disambiguation compo-
nent. In this work, we applied the Berkeley+Lattice
parser to the challenging task of joint segmentation
and parsing of Hebrew text. The result is the first
constituency parser which can parse naturally occur-
ring unsegmented Hebrew text with an acceptable
accuracy (an F1 score of 80%).
Many other uses of lattice parsing are possible.
These include joint segmentation and parsing of
Chinese, empty element prediction (see (Cai et al,
2011) for a successful application), and a princi-
pled handling of multiword-expressions, idioms and
named-entities. The code of the lattice extension to
the Berkeley parser is publicly available.10
Despite its strong performance, we observed that
the Berkeley parser did not learn morphological
agreement patterns. Agreement information could
be very useful for disambiguating various construc-
tions in Hebrew and other morphologically rich lan-
guages. We plan to address this point in future work.
Acknowledgments
We thank Slav Petrov for making available and an-
swering questions about the code of his parser, Fed-
erico Sangati for pointing out some important details
regarding the evaluation, and the three anonymous
reviewers for their helpful comments. The work is
supported by the Lynn and William Frankel Center
for Computer Sciences, Ben-Gurion University.
10http://www.cs.bgu.ac.il/?yoavg/software/blatt/
708
References
Meni Adler, Yoav Goldberg, David Gabay, and Michael
Elhadad. 2008. Unsupervised lexicon-based resolu-
tion of unknown words for full morphological analy-
sis. In Proc. of ACL.
Shu Cai, David Chiang, and Yoav Goldberg. 2011.
Language-independent parsing with empty elements.
In Proc. of ACL (short-paper).
Marie Candito, Benoit Crabbe?, and Djame? Seddah. 2009.
On statistical parsing of French with supervised and
semi-supervised strategies. In EACL 2009 Workshop
Grammatical inference for Computational Linguistics,
Athens, Greece.
J. Chappelier, M. Rajman, R. Aragues, and A. Rozen-
knop. 1999. Lattice Parsing for Speech Recognition.
In In Sixth Conference sur le Traitement Automatique
du Langage Naturel (TANL99), pages 95?104.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and
syntactic parsing. In Proc. of ACL.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM Can find pretty good HMM POS-Taggers (when
given a good start). In Proc. of ACL.
Yoav Golderg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and em-hmm-based lexical probabilities. In
Proc. of EACL.
Spence Green and Christopher Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis.
In Proc. of COLING.
Noemie Guthmann, Yuval Krymolowski, Adi Milea, and
Yoad Winter. 2009. Automatic annotation of morpho-
syntactic dependencies in a Modern Hebrew Treebank.
In Proc. of TLT.
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
across languages. In Proc. of the EMNLP, pages 832?
841. Association for Computational Linguistics.
Alon Itai and Shuly Wintner. 2008. Language resources
for Hebrew. Language Resources and Evaluation,
42(1):75?98, March.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proc. of ACL, Sapporo,
Japan, July. Association for Computational Linguis-
tics.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proc of ACL.
Slav Petrov and Dan Klein. 2008. Parsing German with
latent variable grammars. In Proceedings of the ACL
Workshop on Parsing German.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proc. of ACL, Sydney,
Australia.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley, Berkeley, CA, USA.
Detlef Prescher. 2005. Inducing head-driven PCFGs
with latent heads: Refining a tree-bank grammar for
parsing. In Proc. of ECML.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,
and Noa Nativ. 2001. Building a Tree-Bank of
Modern Hebrew text. Traitement Automatique des
Langues, 42(2).
Reut Tsarfaty and Khalil Sima?an. 2010. Model-
ing morphosyntactic agreement in constituency-based
parsing of Modern Hebrew. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Reut Tsarfaty. 2006. Integrated Morphological and Syn-
tactic Disambiguation for Modern Hebrew. In Proc. of
ACL-SRW.
709
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92?97,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Universal Dependency Annotation for Multilingual Parsing
Ryan McDonald? Joakim Nivre?? Yvonne Quirmbach-Brundage? Yoav Goldberg??
Dipanjan Das? Kuzman Ganchev? Keith Hall? Slav Petrov? Hao Zhang?
Oscar Ta?ckstro?m?? Claudia Bedini? Nu?ria Bertomeu Castello?? Jungmee Lee?
Google, Inc.? Uppsala University? Appen-Butler-Hill? Bar-Ilan University?
Contact: ryanmcd@google.com
Abstract
We present a new collection of treebanks
with homogeneous syntactic dependency
annotation for six languages: German,
English, Swedish, Spanish, French and
Korean. To show the usefulness of such a
resource, we present a case study of cross-
lingual transfer parsing with more reliable
evaluation than has been possible before.
This ?universal? treebank is made freely
available in order to facilitate research on
multilingual dependency parsing.1
1 Introduction
In recent years, syntactic representations based
on head-modifier dependency relations between
words have attracted a lot of interest (Ku?bler et
al., 2009). Research in dependency parsing ? com-
putational methods to predict such representations
? has increased dramatically, due in large part to
the availability of dependency treebanks in a num-
ber of languages. In particular, the CoNLL shared
tasks on dependency parsing have provided over
twenty data sets in a standardized format (Buch-
holz and Marsi, 2006; Nivre et al, 2007).
While these data sets are standardized in terms
of their formal representation, they are still hetero-
geneous treebanks. That is to say, despite them
all being dependency treebanks, which annotate
each sentence with a dependency tree, they sub-
scribe to different annotation schemes. This can
include superficial differences, such as the renam-
ing of common relations, as well as true diver-
gences concerning the analysis of linguistic con-
structions. Common divergences are found in the
1Downloadable at https://code.google.com/p/uni-dep-tb/.
analysis of coordination, verb groups, subordinate
clauses, and multi-word expressions (Nilsson et
al., 2007; Ku?bler et al, 2009; Zeman et al, 2012).
These data sets can be sufficient if one?s goal
is to build monolingual parsers and evaluate their
quality without reference to other languages, as
in the original CoNLL shared tasks, but there are
many cases where heterogenous treebanks are less
than adequate. First, a homogeneous represen-
tation is critical for multilingual language tech-
nologies that require consistent cross-lingual anal-
ysis for downstream components. Second, consis-
tent syntactic representations are desirable in the
evaluation of unsupervised (Klein and Manning,
2004) or cross-lingual syntactic parsers (Hwa et
al., 2005). In the cross-lingual study of McDonald
et al (2011), where delexicalized parsing models
from a number of source languages were evalu-
ated on a set of target languages, it was observed
that the best target language was frequently not the
closest typologically to the source. In one stun-
ning example, Danish was the worst source lan-
guage when parsing Swedish, solely due to greatly
divergent annotation schemes.
In order to overcome these difficulties, some
cross-lingual studies have resorted to heuristics to
homogenize treebanks (Hwa et al, 2005; Smith
and Eisner, 2009; Ganchev et al, 2009), but we
are only aware of a few systematic attempts to
create homogenous syntactic dependency anno-
tation in multiple languages. In terms of auto-
matic construction, Zeman et al (2012) attempt
to harmonize a large number of dependency tree-
banks by mapping their annotation to a version of
the Prague Dependency Treebank scheme (Hajic?
et al, 2001; Bo?hmova? et al, 2003). Addition-
ally, there have been efforts to manually or semi-
manually construct resources with common syn-
92
tactic analyses across multiple languages using al-
ternate syntactic theories as the basis for the repre-
sentation (Butt et al, 2002; Helmreich et al, 2004;
Hovy et al, 2006; Erjavec, 2012).
In order to facilitate research on multilingual
syntactic analysis, we present a collection of data
sets with uniformly analyzed sentences for six lan-
guages: German, English, French, Korean, Span-
ish and Swedish. This resource is freely avail-
able and we plan to extend it to include more data
and languages. In the context of part-of-speech
tagging, universal representations, such as that of
Petrov et al (2012), have already spurred numer-
ous examples of improved empirical cross-lingual
systems (Zhang et al, 2012; Gelling et al, 2012;
Ta?ckstro?m et al, 2013). We aim to do the same for
syntactic dependencies and present cross-lingual
parsing experiments to highlight some of the bene-
fits of cross-lingually consistent annotation. First,
results largely conform to our expectations of
which target languages should be useful for which
source languages, unlike in the study of McDon-
ald et al (2011). Second, the evaluation scores
in general are significantly higher than previous
cross-lingual studies, suggesting that most of these
studies underestimate true accuracy. Finally, un-
like all previous cross-lingual studies, we can re-
port full labeled accuracies and not just unlabeled
structural accuracies.
2 Towards A Universal Treebank
The Stanford typed dependencies for English
(De Marneffe et al, 2006; de Marneffe and Man-
ning, 2008) serve as the point of departure for our
?universal? dependency representation, together
with the tag set of Petrov et al (2012) as the under-
lying part-of-speech representation. The Stanford
scheme, partly inspired by the LFG framework,
has emerged as a de facto standard for depen-
dency annotation in English and has recently been
adapted to several languages representing different
(and typologically diverse) language groups, such
as Chinese (Sino-Tibetan) (Chang et al, 2009),
Finnish (Finno-Ugric) (Haverinen et al, 2010),
Persian (Indo-Iranian) (Seraji et al, 2012), and
Modern Hebrew (Semitic) (Tsarfaty, 2013). Its
widespread use and proven adaptability makes it a
natural choice for our endeavor, even though ad-
ditional modifications will be needed to capture
the full variety of grammatical structures in the
world?s languages.
Alexandre re?side avec sa famille a` Tinqueux .
NOUN VERB ADP DET NOUN ADP NOUN P
NSUBJ ADPMOD
ADPOBJ
POSS
ADPMOD
ADPOBJ
P
Figure 1: A sample French sentence.
We use the so-called basic dependencies (with
punctuation included), where every dependency
structure is a tree spanning all the input tokens,
because this is the kind of representation that most
available dependency parsers require. A sample
dependency tree from the French data set is shown
in Figure 1. We take two approaches to generat-
ing data. The first is traditional manual annotation,
as previously used by Helmreich et al (2004) for
multilingual syntactic treebank construction. The
second, used only for English and Swedish, is to
automatically convert existing treebanks, as in Ze-
man et al (2012).
2.1 Automatic Conversion
Since the Stanford dependencies for English are
taken as the starting point for our universal annota-
tion scheme, we begin by describing the data sets
produced by automatic conversion. For English,
we used the Stanford parser (v1.6.8) (Klein and
Manning, 2003) to convert the Wall Street Jour-
nal section of the Penn Treebank (Marcus et al,
1993) to basic dependency trees, including punc-
tuation and with the copula verb as head in cop-
ula constructions. For Swedish, we developed a
set of deterministic rules for converting the Tal-
banken part of the Swedish Treebank (Nivre and
Megyesi, 2007) to a representation as close as pos-
sible to the Stanford dependencies for English.
This mainly consisted in relabeling dependency
relations and, due to the fine-grained label set used
in the Swedish Treebank (Teleman, 1974), this
could be done with high precision. In addition,
a small number of constructions required struc-
tural conversion, notably coordination, which in
the Swedish Treebank is given a Prague style anal-
ysis (Nilsson et al, 2007). For both English and
Swedish, we mapped the language-specific part-
of-speech tags to universal tags using the map-
pings of Petrov et al (2012).
2.2 Manual Annotation
For the remaining four languages, annotators were
given three resources: 1) the English Stanford
93
guidelines; 2) a set of English sentences with Stan-
ford dependencies and universal tags (as above);
and 3) a large collection of unlabeled sentences
randomly drawn from newswire, weblogs and/or
consumer reviews, automatically tokenized with a
rule-based system. For German, French and Span-
ish, contractions were split, except in the case of
clitics. For Korean, tokenization was more coarse
and included particles within token units. Annota-
tors could correct this automatic tokenization.
The annotators were then tasked with producing
language-specific annotation guidelines with the
expressed goal of keeping the label and construc-
tion set as close as possible to the original English
set, only adding labels for phenomena that do not
exist in English. Making fine-grained label dis-
tinctions was discouraged. Once these guidelines
were fixed, annotators selected roughly an equal
amount of sentences to be annotated from each do-
main in the unlabeled data. As the sentences were
already randomly selected from a larger corpus,
annotators were told to view the sentences in or-
der and to discard a sentence only if it was 1) frag-
mented because of a sentence splitting error; 2) not
from the language of interest; 3) incomprehensible
to a native speaker; or 4) shorter than three words.
The selected sentences were pre-processed using
cross-lingual taggers (Das and Petrov, 2011) and
parsers (McDonald et al, 2011).
The annotators modified the pre-parsed trees us-
ing the TrEd2 tool. At the beginning of the annota-
tion process, double-blind annotation, followed by
manual arbitration and consensus, was used itera-
tively for small batches of data until the guidelines
were finalized. Most of the data was annotated
using single-annotation and full review: one an-
notator annotating the data and another reviewing
it, making changes in close collaboration with the
original annotator. As a final step, all annotated
data was semi-automatically checked for annota-
tion consistency.
2.3 Harmonization
After producing the two converted and four an-
notated data sets, we performed a harmonization
step, where the goal was to maximize consistency
of annotation across languages. In particular, we
wanted to eliminate cases where the same label
was used for different linguistic relations in dif-
ferent languages and, conversely, where one and
2Available at http://ufal.mff.cuni.cz/tred/.
the same relation was annotated with different la-
bels, both of which could happen accidentally be-
cause annotators were allowed to add new labels
for the language they were working on. Moreover,
we wanted to avoid, as far as possible, labels that
were only used in one or two languages.
In order to satisfy these requirements, a number
of language-specific labels were merged into more
general labels. For example, in analogy with the
nn label for (element of a) noun-noun compound,
the annotators of German added aa for compound
adjectives, and the annotators of Korean added vv
for compound verbs. In the harmonization step,
these three labels were merged into a single label
compmod for modifier in compound.
In addition to harmonizing language-specific la-
bels, we also renamed a small number of relations,
where the name would be misleading in the uni-
versal context (although quite appropriate for En-
glish). For example, the label prep (for a mod-
ifier headed by a preposition) was renamed adp-
mod, to make clear the relation to other modifier
labels and to allow postpositions as well as prepo-
sitions.3 We also eliminated a few distinctions in
the original Stanford scheme that were not anno-
tated consistently across languages (e.g., merging
complm with mark, number with num, and purpcl
with advcl).
The final set of labels is listed with explanations
in Table 1. Note that relative to the universal part-
of-speech tagset of Petrov et al (2012) our final
label set is quite rich (40 versus 12). This is due
mainly to the fact that the the former is based on
deterministic mappings from a large set of annota-
tion schemes and therefore reduced to the granu-
larity of the greatest common denominator. Such a
reduction may ultimately be necessary also in the
case of dependency relations, but since most of our
data sets were created through manual annotation,
we could afford to retain a fine-grained analysis,
knowing that it is always possible to map from
finer to coarser distinctions, but not vice versa.4
2.4 Final Data Sets
Table 2 presents the final data statistics. The num-
ber of sentences, tokens and tokens/sentence vary
3Consequently, pobj and pcomp were changed to adpobj
and adpcomp.
4The only two data sets that were created through con-
version in our case were English, for which the Stanford de-
pendencies were originally defined, and Swedish, where the
native annotation happens to have a fine-grained label set.
94
Label Description
acomp adjectival complement
adp adposition
adpcomp complement of adposition
adpmod adpositional modifier
adpobj object of adposition
advcl adverbial clause modifier
advmod adverbial modifier
amod adjectival modifier
appos appositive
attr attribute
aux auxiliary
auxpass passive auxiliary
cc conjunction
ccomp clausal complement
Label Description
compmod compound modifier
conj conjunct
cop copula
csubj clausal subject
csubjpass passive clausal subject
dep generic
det determiner
dobj direct object
expl expletive
infmod infinitival modifier
iobj indirect object
mark marker
mwe multi-word expression
neg negation
Label Description
nmod noun modifier
nsubj nominal subject
nsubjpass passive nominal subject
num numeric modifier
p punctuation
parataxis parataxis
partmod participial modifier
poss possessive
prt verb particle
rcmod relative clause modifier
rel relative
xcomp open clausal complement
Table 1: Harmonized label set based on Stanford dependencies (De Marneffe et al, 2006).
source(s) # sentences # tokens
DE N, R 4,000 59,014
EN PTB? 43,948 1,046,829
SV STB? 6,159 96,319
ES N, B, R 4,015 112,718
FR N, B, R 3,978 90,000
KO N, B 6,194 71,840
Table 2: Data set statistics. ?Automatically con-
verted WSJ section of the PTB. The data release
includes scripts to generate this data, not the data
itself. ?Automatically converted Talbanken sec-
tion of the Swedish Treebank. N=News, B=Blogs,
R=Consumer Reviews.
due to the source and tokenization. For example,
Korean has 50% more sentences than Spanish, but
?40k less tokens due to a more coarse-grained to-
kenization. In addition to the data itself, anno-
tation guidelines and harmonization rules are in-
cluded so that the data can be regenerated.
3 Experiments
One of the motivating factors in creating such a
data set was improved cross-lingual transfer eval-
uation. To test this, we use a cross-lingual transfer
parser similar to that of McDonald et al (2011).
In particular, it is a perceptron-trained shift-reduce
parser with a beam of size 8. We use the features
of Zhang and Nivre (2011), except that all lexical
identities are dropped from the templates during
training and testing, hence inducing a ?delexical-
ized? model that employs only ?universal? proper-
ties from source-side treebanks, such as part-of-
speech tags, labels, head-modifier distance, etc.
We ran a number of experiments, which can be
seen in Table 3. For these experiments we ran-
domly split each data set into training, develop-
ment and testing sets.5 The one exception is En-
glish, where we used the standard splits. Each
row in Table 3 represents a source training lan-
guage and each column a target evaluation lan-
guage. We report both unlabeled attachment score
(UAS) and labeled attachment score (LAS) (Buch-
holz and Marsi, 2006). This is likely the first re-
liable cross-lingual parsing evaluation. In partic-
ular, previous studies could not even report LAS
due to differences in treebank annotations.
We can make several interesting observations.
Most notably, for the Germanic and Romance tar-
get languages, the best source language is from
the same language group. This is in stark contrast
to the results of McDonald et al (2011), who ob-
serve that this is rarely the case with the heteroge-
nous CoNLL treebanks. Among the Germanic
languages, it is interesting to note that Swedish
is the best source language for both German and
English, which makes sense from a typological
point of view, because Swedish is intermediate be-
tween German and English in terms of word or-
der properties. For Romance languages, the cross-
lingual parser is approaching the accuracy of the
supervised setting, confirming that for these lan-
guages much of the divergence is lexical and not
structural, which is not true for the Germanic lan-
guages. Finally, Korean emerges as a very clear
outlier (both as a source and as a target language),
which again is supported by typological consider-
ations as well as by the difference in tokenization.
With respect to evaluation, it is interesting to
compare the absolute numbers to those reported
in McDonald et al (2011) for the languages com-
5These splits are included in the release of the data.
95
Source
Training
Language
Target Test Language
Unlabeled Attachment Score (UAS) Labeled Attachment Score (LAS)
Germanic Romance Germanic Romance
DE EN SV ES FR KO DE EN SV ES FR KO
DE 74.86 55.05 65.89 60.65 62.18 40.59 64.84 47.09 53.57 48.14 49.59 27.73
EN 58.50 83.33 70.56 68.07 70.14 42.37 48.11 78.54 57.04 56.86 58.20 26.65
SV 61.25 61.20 80.01 67.50 67.69 36.95 52.19 49.71 70.90 54.72 54.96 19.64
ES 55.39 58.56 66.84 78.46 75.12 30.25 45.52 47.87 53.09 70.29 63.65 16.54
FR 55.05 59.02 65.05 72.30 81.44 35.79 45.96 47.41 52.25 62.56 73.37 20.84
KO 33.04 32.20 27.62 26.91 29.35 71.22 26.36 21.81 18.12 18.63 19.52 55.85
Table 3: Cross-lingual transfer parsing results. Bolded are the best per target cross-lingual result.
mon to both studies (DE, EN, SV and ES). In that
study, UAS was in the 38?68% range, as compared
to 55?75% here. For Swedish, we can even mea-
sure the difference exactly, because the test sets
are the same, and we see an increase from 58.3%
to 70.6%. This suggests that most cross-lingual
parsing studies have underestimated accuracies.
4 Conclusion
We have released data sets for six languages with
consistent dependency annotation. After the ini-
tial release, we will continue to annotate data in
more languages as well as investigate further au-
tomatic treebank conversions. This may also lead
to modifications of the annotation scheme, which
should be regarded as preliminary at this point.
Specifically, with more typologically and morpho-
logically diverse languages being added to the col-
lection, it may be advisable to consistently en-
force the principle that content words take func-
tion words as dependents, which is currently vi-
olated in the analysis of adpositional and copula
constructions. This will ensure a consistent analy-
sis of functional elements that in some languages
are not realized as free words or are not obliga-
tory, such as adpositions which are often absent
due to case inflections in languages like Finnish. It
will also allow the inclusion of language-specific
functional or morphological markers (case mark-
ers, topic markers, classifiers, etc.) at the leaves of
the tree, where they can easily be ignored in appli-
cations that require a uniform cross-lingual repre-
sentation. Finally, this data is available on an open
source repository in the hope that the community
will commit new data and make corrections to ex-
isting annotations.
Acknowledgments
Many people played critical roles in the pro-
cess of creating the resource. At Google, Fer-
nando Pereira, Alfred Spector, Kannan Pashu-
pathy, Michael Riley and Corinna Cortes sup-
ported the project and made sure it had the re-
quired resources. Jennifer Bahk and Dave Orr
helped coordinate the necessary contracts. Andrea
Held, Supreet Chinnan, Elizabeth Hewitt, Tu Tsao
and Leigha Weinberg made the release process
smooth. Michael Ringgaard, Andy Golding, Terry
Koo, Alexander Rush and many others provided
technical advice. Hans Uszkoreit gave us per-
mission to use a subsample of sentences from the
Tiger Treebank (Brants et al, 2002), the source of
the news domain for our German data set. Anno-
tations were additionally provided by Sulki Kim,
Patrick McCrae, Laurent Alamarguy and He?ctor
Ferna?ndez Alcalde.
References
Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and Barbora
Hladka?. 2003. The Prague Dependency Treebank:
A three-level annotation scenario. In Anne Abeille?,
editor, Treebanks: Building and Using Parsed Cor-
pora, pages 103?127. Kluwer.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
Treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Miriam Butt, Helge Dyvik, Tracy Holloway King,
Hiroshi Masuichi, and Christian Rohrer. 2002.
The parallel grammar project. In Proceedings of
the 2002 workshop on Grammar engineering and
evaluation-Volume 15.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative
reordering with Chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation (SSST-3)
at NAACL HLT 2009.
96
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of ACL-HLT.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation.
Marie-Catherine De Marneffe, Bill MacCartney, and
Chris D. Manning. 2006. Generating typed depen-
dency parses from phrase structure parses. In Pro-
ceedings of LREC.
Tomaz Erjavec. 2012. MULTEXT-East: Morphosyn-
tactic resources for Central and Eastern European
languages. Language Resources and Evaluation,
46:131?142.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction
via bitext projection constraints. In Proceedings of
ACL-IJCNLP.
Douwe Gelling, Trevor Cohn, Phil Blunsom, and Joao
Grac?a. 2012. The pascal challenge on grammar in-
duction. In Proceedings of the NAACL-HLT Work-
shop on the Induction of Linguistic Structure.
Jan Hajic?, Barbora Vidova Hladka, Jarmila Panevova?,
Eva Hajic?ova?, Petr Sgall, and Petr Pajas. 2001.
Prague Dependency Treebank 1.0. LDC, 2001T10.
Katri Haverinen, Timo Viljanen, Veronika Laippala,
Samuel Kohonen, Filip Ginter, and Tapio Salakoski.
2010. Treebanking finnish. In Proceedings of
The Ninth International Workshop on Treebanks and
Linguistic Theories (TLT9).
Stephen Helmreich, David Farwell, Bonnie Dorr, Nizar
Habash, Lori Levin, Teruko Mitamura, Florence
Reeder, Keith Miller, Eduard Hovy, Owen Rambow,
and Advaith Siddharthan. 2004. Interlingual anno-
tation of multilingual text corpora. In Proceedings
of the HLT-EACL Workshop on Frontiers in Corpus
Annotation.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of NAACL.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(03):311?325.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL.
Dan Klein and Chris D. Manning. 2004. Corpus-based
induction of syntactic structure: models of depen-
dency and constituency. In Proceedings of ACL.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Morgan and Claypool.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP.
Jens Nilsson, Joakim Nivre, and Johan Hall. 2007.
Generalizing tree transformations for inductive de-
pendency parsing. In Proceedings of ACL.
Joakim Nivre and Bea?ta Megyesi. 2007. Bootstrap-
ping a Swedish treebank using cross-corpus harmo-
nization and annotation projection. In Proceedings
of the 6th International Workshop on Treebanks and
Linguistic Theories.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on
dependency parsing. In Proceedings of EMNLP-
CoNLL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC.
Mojgan Seraji, Bea?ta Megyesi, and Nivre Joakim.
2012. Bootstrapping a Persian dependency tree-
bank. Linguistic Issues in Language Technology,
7(18):1?10.
David A. Smith and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of EMNLP.
Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the ACL.
Ulf Teleman. 1974. Manual fo?r grammatisk beskrivn-
ing av talad och skriven svenska. Studentlitteratur.
Reut Tsarfaty. 2013. A unified morpho-syntactic
scheme of stanford dependencies. Proceedings of
ACL.
Daniel Zeman, David Marecek, Martin Popel,
Loganathan Ramasamy, Jan S?tepa?nek, Zdene?k
Z?abokrtsky`, and Jan Hajic. 2012. Hamledt: To
parse or not to parse. In Proceedings of LREC.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL-HLT.
Yuan Zhang, Roi Reichart, Regina Barzilay, and Amir
Globerson. 2012. Learning to map into a universal
pos tagset. In Proceedings of EMNLP.
97
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 628?633,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Efficient Implementation of Beam-Search Incremental Parsers?
Yoav Goldberg
Dept. of Computer Science
Bar-Ilan University
Ramat Gan, Tel Aviv, 5290002 Israel
yoav.goldberg@gmail.com
Kai Zhao Liang Huang
Graduate Center and Queens College
City University of New York
{kzhao@gc, lhuang@cs.qc}.cuny.edu
{kzhao.hf, liang.huang.sh}.gmail.com
Abstract
Beam search incremental parsers are ac-
curate, but not as fast as they could be.
We demonstrate that, contrary to popu-
lar belief, most current implementations
of beam parsers in fact run in O(n2),
rather than linear time, because each state-
transition is actually implemented as an
O(n) operation. We present an improved
implementation, based on Tree Structured
Stack (TSS), in which a transition is per-
formed in O(1), resulting in a real linear-
time algorithm, which is verified empiri-
cally. We further improve parsing speed
by sharing feature-extraction and dot-
product across beam items. Practically,
our methods combined offer a speedup of
?2x over strong baselines on Penn Tree-
bank sentences, and are orders of magni-
tude faster on much longer sentences.
1 Introduction
Beam search incremental parsers (Roark, 2001;
Collins and Roark, 2004; Zhang and Clark, 2008;
Huang et al, 2009; Huang and Sagae, 2010;
Zhang and Nivre, 2011; Zhang and Clark, 2011)
provide very competitive parsing accuracies for
various grammar formalisms (CFG, CCG, and de-
pendency grammars). In terms of purning strate-
gies, they can be broadly divided into two cat-
egories: the first group (Roark, 2001; Collins
and Roark, 2004) uses soft (aka probabilistic)
beams borrowed from bottom-up parsers (Char-
niak, 2000; Collins, 1999) which has no control
of complexity, while the second group (the rest
and many more recent ones) employs hard beams
borrowed from machine translation (Koehn, 2004)
which guarantee (as they claim) a linear runtime
O(kn) where k is the beam width. However, we
will demonstrate below that, contrary to popular
?Supported in part by DARPA FA8750-13-2-0041 (DEFT).
belief, in most standard implementations their ac-
tual runtime is in fact O(kn2) rather than linear.
Although this argument in general also applies to
dynamic programming (DP) parsers,1 in this pa-
per we only focus on the standard, non-dynamic
programming approach since it is arguably still the
dominant practice (e.g. it is easier with the popular
arc-eager parser with a rich feature set (Kuhlmann
et al, 2011; Zhang and Nivre, 2011)) and it bene-
fits more from our improved algorithms.
The dependence on the beam-size k is because
one needs to do k-times the number of basic opera-
tions (feature-extractions, dot-products, and state-
transitions) relative to a greedy parser (Nivre and
Scholz, 2004; Goldberg and Elhadad, 2010). Note
that in a beam setting, the same state can expand
to several new states in the next step, which is usu-
ally achieved by copying the state prior to making
a transition, whereas greedy search only stores one
state which is modified in-place.
Copying amounts to a large fraction of the
slowdown of beam-based with respect to greedy
parsers. Copying is expensive, because the state
keeps track of (a) a stack and (b) the set of
dependency-arcs added so far. Both the arc-set and
the stack can grow to O(n) size in the worst-case,
making the state-copy (and hence state-transition)
an O(n) operation. Thus, beam search imple-
mentations that copy the entire state are in fact
quadratic O(kn2) and not linear, with a slowdown
factor of O(kn) with respect to greedy parsers,
which is confirmed empirically in Figure 4.
We present a way of decreasing the O(n) tran-
sition cost to O(1) achieving strictly linear-time
parsing, using a data structure of Tree-Structured
Stack (TSS) that is inspired by but simpler than
the graph-structured stack (GSS) of Tomita (1985)
used in dynamic programming (Huang and Sagae,
2010).2 On average Treebank sentences, the TSS
1The Huang-Sagae DP parser (http://acl.cs.qc.edu)
does run in O(kn), which inspired this paper when we ex-
perimented with simulating non-DP beam search using GSS.
2Our notion of TSS is crucially different from the data
628
input: w0 . . . wn?1
axiom 0 : ?0, ?: ?
SHIFT
` : ?j, S? : A
`+ 1 : ?j + 1, S|wj? : A
j < n
REDUCEL
` : ?j, S|s1|s0? : A
`+ 1 : ?j, S|s0? : A ? {s1xs0}
REDUCER
` : ?j, S|s1|s0? : A
`+ 1 : ?j, S|s1? : A ? {s1ys0}
goal 2n? 1 : ?n, s0?: A
Figure 1: An abstraction of the arc-standard de-
ductive system Nivre (2008). The stack S is a list
of heads, j is the index of the token at the front of
the buffer, and ` is the step number (beam index).
A is the arc-set of dependency arcs accumulated
so far, which we will get rid of in Section 4.1.
version, being linear time, leads to a speedup of
2x?2.7x over the naive implementation, and about
1.3x?1.7x over the optimized baseline presented
in Section 5.
Having achieved efficient state-transitions, we
turn to feature extraction and dot products (Sec-
tion 6). We present a simple scheme of sharing
repeated scoring operations across different beam
items, resulting in an additional 7 to 25% speed in-
crease. On Treebank sentences, the methods com-
bined lead to a speedup of ?2x over strong base-
lines (?10x over naive ones), and on longer sen-
tences they are orders of magnitude faster.
2 Beam Search Incremental Parsing
We assume familiarity with transition-based de-
pendency parsing. The unfamiliar reader is re-
ferred to Nivre (2008). We briefly describe a
standard shift-reduce dependency parser (which is
called ?arc-standard? by Nivre) to establish nota-
tion. Parser states (sometimes called configura-
tions) are composed of a stack, a buffer, and an
arc-set. Parsing transitions are applied to states,
and result in new states. The arc-standard system
has three kinds of transitions: SHIFT, REDUCEL,
structure with the same name in an earlier work of Tomita
(1985). In fact, Tomita?s TSS merges the top portion of the
stacks (more like GSS) while ours merges the bottom por-
tion. We thank Yue Zhang for informing us that TSS was
already implemented for the CCG parser in zpar (http://
sourceforge.net/projects/zpar/) though it was not men-
tioned in his paper (Zhang and Clark, 2011).
and REDUCER, which are summarized in the de-
ductive system in Figure 1. The SHIFT transition
removes the first word from the buffer and pushes
it to the stack, and the REDUCEL and REDUCER
actions each add a dependency relation between
the two words on the top of the stack (which is
achieved by adding the arc s1xs0 or s1ys0 to the
arc-set A), and pops the new dependent from the
stack. When reaching the goal state the parser re-
turns a tree composed of the arcs in the arc-set.
At parsing time, transitions are chosen based on
a trained scoring model which looks at features
of the state. In a beam parser, k items (hypothe-
ses) are maintained. Items are composed of a state
and a score. At step i, each of the k items is ex-
tended by applying all possible transitions to the
given state, resulting in k ? a items, a being the
number of possible transitions. Of these, the top
scoring k items are kept and used in step i+1. Fi-
nally, the tree associated with the highest-scoring
item is returned.
3 The Common Implementation of State
The stack is usually represented as a list or an array
of token indices, and the arc-set as an array heads
of length n mapping the word at position m to the
index of its parent. In order to allow for fast fea-
ture extraction, additional arrays are used to map
each token to its left-most and right-most modi-
fier, which are used in most incremental parsers,
e.g. (Huang and Sagae, 2010; Zhang and Nivre,
2011). The buffer is usually implemented as a
pointer to a shared sentence object, and an index j
to the current front of the buffer. Finally, it is com-
mon to keep an additional array holding the tran-
sition sequence leading to the current state, which
can be represented compactly as a pointer to the
previous state and the current action. The state
structure is summarized below:
class state
stack[n] of token_ids
array[n] heads
array[n] leftmost_modifiers
array[n] rightmost_modifiers
int j
int last_action
state previous
In a greedy parser, state transition is performed in-
place. However, in a beam parser the states cannot
be modified in place, and a state transition oper-
ation needs to result in a new, independent state
object. The common practice is to copy the cur-
rent state, and then update the needed fields in the
copy. Copying a stack and arrays of size n is an
629
O(n) operation. In what follows, we present a way
to perform transitions in O(1).
4 Efficient State Transitions
4.1 Distributed Representation of Trees
The state needs to keep track of the set of arcs
added to the tree so far for two reasons:
(a) In order to return the complete tree at the end.
(b) In order to compute features when parsing.
Observe that we do not in fact need to store any
arc in order to achieve (a) ? we could reconstruct
the entire set by backtracking once we reach the
final configuration. Hence, the arc-set in Figure 1
is only needed for computing features. Instead of
storing the entire arc-set, we could keep only the
information needed for feature computation. In
the feature set we use (Huang and Sagae, 2010),
we need access to (1) items on the buffer, (2)
the 3 top-most elements of the stack, and (3) the
current left-most and right-most modifiers of the
two topmost stack elements. The left-most and
right-most modifiers are already kept in the state
representation, but store more information than
needed: we only need to keep track of the mod-
ifiers of current stack items. Once a token is re-
moved from the stack it will never return, and we
will not need access to its modifiers again. We
can therefore remove the left/rightmost modifier
arrays, and instead have the stack store triplets
(token, leftmost_mod, rightmost_mod). The
heads array is no longer needed. Our new state
representation becomes:
class state
stack[n] of (tok, left, right)
int j
int last_action
state previous
4.2 Tree Structured Stack: TSS
We now turn to handle the stack. Notice that the
buffer, which is also of size O(n), is represented
as a pointer to an immutable shared object, and is
therefore very efficient to copy. We would like to
treat the stack in a similar fashion.
An immutable stack can be implemented func-
tionally as a cons list, where the head is the top
of the stack and the tail is the rest of the stack.
Pushing an item to the stack amounts to adding a
new head link to the list and returning it. Popping
an item from the stack amounts to returning the
tail of the list. Notice that, crucially, a pop opera-
tion does not change the underlying list at all, and
a push operation only adds to the front of a list.
Thus, the stack operations are non-destructive, in
the sense that once you hold a reference to a stack,
the view of the stack through this reference does
not change regardless of future operations that are
applied to the stack. Moreover, push and pop op-
erations are very efficient. This stack implementa-
tion is an example of a persistent data structure ? a
data structure inspired by functional programming
which keeps the old versions of itself intact when
modified (Okasaki, 1999).
While each client sees the stack as a list, the un-
derlying representation is a tree, and clients hold
pointers to nodes in the tree. A push operation
adds a branch to the tree and returns the new
pointer, while a pop operation returns the pointer
of the parent, see Figure 3 for an example. We call
this representation a tree-structured stack (TSS).
Using this stack representation, we can replace
the O(n) stack by an integer holding the item at
the top of the stack (s0), and a pointer to the tail of
the stack (tail). As discussed above, in addition
to the top of the stack we also keep its leftmost and
rightmost modifiers s0L and s0R. The simplified
state representation becomes:
class state
int s0, s0L, s0R
state tail
int j
int last_action
state previous
State is now reduced to seven integers, and the
transitions can be implemented very efficiently as
we show in Figure 2. The parser state is trans-
formed into a compact object, and state transitions
are O(1) operations involving only a few pointer
lookups and integer assignments.
4.3 TSS vs. GSS; Space Complexity
TSS is inspired by the graph-structured stack
(GSS) used in the dynamic-programming parser of
Huang and Sagae (2010), but without reentrancy
(see also Footnote 2). More importantly, the state
signature in TSS is much slimmer than that in
GSS. Using the notation of Huang and Sagae, in-
stead of maintaining the full DP signature of
f?DP(j, S) = (j, fd(sd), . . . , f0(s0))
where sd denotes the dth tree on stack, in non-DP
TSS we only need to store the features f0(s0) for
the final tree on the stack,
f?noDP(j, S) = (j, f0(s0)),
630
def Shift(state)
newstate.s0 = state.j
newstate.s0L = None
newstate.s0R = None
newstate.tail = state
newstate.j = state.j + 1
return newstate
def ReduceL(state)
newstate.s0 = state.s0
newstate.s0L = state.tail.s0
newstate.s0R = state.s0R
newstate.tail = state.tail.tail
newstate.j = j
return newstate
def ReduceR(state)
newstate.s0 = state.tail.s0
newstate.s0L = state.tail.s0L
newstate.s0R = state.s0
newstate.tail = state.tail.tail
newstate.j = j
return newstate
Figure 2: State transitions implementation in the TSS representation (see Fig. 3 for the tail pointers).
The two lines on s0L and s0R are specific to feature set design, and can be expanded for richer feature
sets. To conserve space, we do not show the obvious assignments to last_action and previous.
b
1 2
 c
3
a
a d
4
b
c
0
b
c
c
L
R
L
R
sh sh sh sh
sh
sh
Figure 3: Example of tree-structured stack. The
forward arrows denote state transitions, and the
dotted backward arrows are the tail pointers to
the stack tail. The boxes denote the top-of-stack at
each state. Notice that for b = shift(a) we perform
a single push operation getting b.tail = a, while
for b = reduce(a) transition we perform two pops
and a push, resulting in b.tail = a.tail.tail.
thanks to the uniqueness of tail pointers (?left-
pointers? in Huang and Sagae).
In terms of space complexity, each state is re-
duced from O(n) in size to O(d) with GSS and
to O(1) with TSS,3 making it possible to store the
entire beam in O(kn) space. Moreover, the con-
stant state-size makes memory management easier
and reduces fragmentation, by making it possible
to pre-allocate the entire beam upfront. We did
not explore its empirical implications in this work,
as our implementation language, Python, does not
support low-level memory management.
4.4 Generality of the Approach
We presented a concrete implementation for the
arc-standard system with a relatively simple (yet
state-of-the-art) feature set. As in Kuhlmann et
al. (2011), our approach is also applicable to
other transitions systems and richer feature-sets
with some additional book-keeping. A well-
3For example, a GSS state in Huang and Sagae?s experi-
ments also stores s1, s1L, s1R, s2 besides the f0(s0) fea-
tures (s0, s0L, s0R) needed by TSS. d is treated as a con-
stant by Huang and Sagae but actually it could be a variable.
documented Python implementation for the la-
beled arc-eager system with the rich feature set
of Zhang and Nivre (2011) is available on the first
author?s homepage.
5 Fewer Transitions: Lazy Expansion
Another way of decreasing state-transition costs
is making less transitions to begin with: instead
of performing all possible transitions from each
beam item and then keeping only k of the re-
sulting states, we could perform only transitions
that are sure to end up on the next step in the
beam. This is done by first computing transition
scores from each beam item, then keeping the top
k highest scoring (state, action) pairs, perform-
ing only those k transitions. This technique is
especially important when the number of possi-
ble transitions is large, such as in labeled parsing.
The technique, though never mentioned in the lit-
erature, was employed in some implementations
(e.g., Yue Zhang?s zpar). We mention it here for
completeness since it?s not well-known yet.
6 (Partial) Feature Sharing
After making the state-transition efficient, we turn
to deal with the other major expensive operation:
feature-extractions and dot-products. While we
can?t speed up the process, we observe that some
computations are repeated in different parts of the
beam, and propose to share these computations.
Notice that relatively few token indices from a
state can determine the values of many features.
For example, knowing the buffer index j deter-
mines the words and tags of items after location
j on the buffer, as well as features composed of
combinations of these values.
Based on this observation we propose the no-
tion of a state signature, which is a set of token
indices. An example of a state signature would
be sig(state) = (s0, s0L, s1, s1L), indicating the
indices of the two tokens at the top of the stack to-
gether with their leftmost modifiers. Given a sig-
631
Figure 4: Non-linearity of the standard beam
search compared to the linearity of our TSS beam
search for labeled arc-eager and unlabeled arc-
standard parsers on long sentences (running times
vs. sentence length). All parsers use beam size 8.
nature, we decompose the feature function ?(x)
into two parts ?(x) = ?s(sig(x)) + ?o(x), where
?s(sig(x)) extracts all features that depend exclu-
sively on signature items, and ?o(x) extracts all
other features.4 The scoring function w ? ?(x) de-
composes into w ? ?s(sig(x)) + w ? ?o(x). Dur-
ing beam decoding, we maintain a cache map-
ping seen signatures sig(state) to (partial) tran-
sition scores w ? ?s(sig(state)). We now need
to calculate w ? ?o(x) for each beam item, but
w ? ?s(sig(x)) only for one of the items sharing
the signature. Defining the signature involves a
natural balance between signatures that repeat of-
ten and signatures that cover many features. In the
experiments in this paper, we chose the signature
function for the arc-standard parser to contain all
core elements participating in feature extraction5,
and for the arc-eager parser a signature containing
only a partial subset.6
7 Experiments
We implemented beam-based parsers using the
traditional approach as well as with our proposed
extension and compared their runtime.
The first experiment highlights the non-linear
behavior of the standard implementation, com-
pared to the linear behavior of the TSS method.
4One could extend the approach further to use several sig-
natures and further decompose the feature function. We did
not pursue this idea in this work.
5s0,s0L,s0R,s1,s1L,s1R,s2,j.
6s0, s0L, s0R,s0h,b0L,j, where s0h is the parent of
s0, and b0L is the leftmost modifier of j.
system plain plain plain plain +TSS+lazy
+TSS +lazy +TSS +feat-share
(sec 3) (sec 4) (sec 5) +lazy (sec 6)
ArcS-U 20.8 38.6 24.3 41.1 47.4
ArcE-U 25.4 48.3 38.2 58.2 72.3
ArcE-L 1.8 4.9 11.1 14.5 17.3
Table 1: Parsing speeds for the different tech-
niques measured in sentences/sec (beam size 8).
All parsers are implemented in Python, with dot-
products in C. ArcS/ArcE denotes arc-standard
vs. arc-eager, L/U labeled (stanford deps, 49 la-
bels) vs. unlabeled parsing. ArcS use feature set
of Huang and Sagae (2010) (50 templates), and ArcE
that of Zhang and Nivre (2011) (72 templates).
As parsing time is dominated by score computa-
tion, the effect is too small to be measured on
natural language sentences, but it is noticeable
for longer sentences. Figure 4 plots the runtime
for synthetic examples with lengths ranging from
50 to 1000 tokens, which are generated by con-
catenating sentences from Sections 22?24 of Penn
Treebank (PTB), and demonstrates the non-linear
behavior (dataset included). We argue parsing
longer sentences is by itself an interesting and
potentially important problem (e.g. for other lan-
guages such as Arabic and Chinese where word
or sentence boundaries are vague, and for pars-
ing beyond sentence-level, e.g. discourse parsing
or parsing with inter-sentence dependencies).
Our next set of experiments compares the actual
speedup observed on English sentences. Table 1
shows the speed of the parsers (sentences/sec-
ond) with the various proposed optimization tech-
niques. We first train our parsers on Sections 02?
21 of PTB, using Section 22 as the test set. The
accuracies of all our parsers are at the state-of-
the-art level. The final speedups are up to 10x
against naive baselines and ?2x against the lazy-
transitions baselines.
8 Conclusions
We demonstrated in both theory and experiments
that the standard implementation of beam search
parsers run in O(n2) time, and have presented im-
proved algorithms which run in O(n) time. Com-
bined with other techniques, our method offers
significant speedups (?2x) over strong baselines,
or 10x over naive ones, and is orders of magnitude
faster on much longer sentences. We have demon-
strated that our approach is general and we believe
it will benefit many other incremental parsers.
632
References
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of ACL.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Yoav Goldberg and Michael Elhadad. 2010. An ef-
ficient algorithm for easy-first non-directional de-
pendency parsing. In Proceedings of HLT-NAACL,
pages 742?750.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of ACL 2010.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. In Proceedings of AMTA, pages 115?
124.
Marco Kuhlmann, Carlos Gmez-Rodrguez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
for transition-based dependency parsers. In Pro-
ceedings of ACL.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of english text. In Proceedings
of COLING, Geneva.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513?553.
Chris Okasaki. 1999. Purely functional data struc-
tures. Cambridge University Press.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Masaru Tomita. 1985. An efficient context-free pars-
ing algorithm for natural languages. In Proceedings
of the 9th international joint conference on Artificial
intelligence - Volume 2, pages 756?764.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of EMNLP.
Yue Zhang and Stephen Clark. 2011. Shift-reduce ccg
parsing. In Proceedings of ACL.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL, pages 188?193.
633
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 289?295,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Automatic Detection of Machine Translated Text and Translation Quality
Estimation
Roee Aharoni
Dept. of Computer Science
Bar Ilan University
Ramat-Gan, Israel 52900
roee.aharoni@gmail.com
Moshe Koppel
Dept. of Computer Science
Bar Ilan University
Ramat-Gan, Israel 52900
moishk@gmail.com
Yoav Goldberg
Dept. of Computer Science
Bar Ilan University
Ramat-Gan, Israel 52900
yoav.goldberg@gmail.com
Abstract
We show that it is possible to automati-
cally detect machine translated text at sen-
tence level from monolingual corpora, us-
ing text classification methods. We show
further that the accuracy with which a
learned classifier can detect text as ma-
chine translated is strongly correlated with
the translation quality of the machine
translation system that generated it. Fi-
nally, we offer a generic machine transla-
tion quality estimation technique based on
this approach, which does not require ref-
erence sentences.
1 Introduction
The recent success and proliferation of statistical
machine translation (MT) systems raise a number
of important questions. Prominent among these
are how to evaluate the quality of such a system
efficiently and how to detect the output of such
systems (for example, to avoid using it circularly
as input for refining MT systems).
In this paper, we will answer both these ques-
tions. First, we will show that using style-related
linguistic features, such as frequencies of parts-
of-speech n-grams and function words, it is pos-
sible to learn classifiers that distinguish machine-
translated text from human-translated or native
English text. While this is a straightforward and
not entirely novel result, our main contribution is
to relativize the result. We will see that the suc-
cess of such classifiers are strongly correlated with
the quality of the underlying machine translation
system. Specifically, given a corpus consisting of
both machine-translated English text (English be-
ing the target language) and native English text
(not necessarily the reference translation of the
machine-translated text), we measure the accuracy
of the system in classifying the sentences in the
corpus as machine-translated or not. This accu-
racy will be shown to decrease as the quality of
the underlying MT system increases. In fact, the
correlation is strong enough that we propose that
this accuracy measure itself can be used as a mea-
sure of MT system quality, obviating the need for
a reference corpus, as for example is necessary for
BLEU (Papineni et al, 2001).
The paper is structured as follows: In the next
section, we review previous related work. In the
third section, we describe experiments regarding
the detection of machine translation and in the
fourth section we discuss the use of detection tech-
niques as a machine translation quality estimation
method. In the final section we offer conclusions
and suggestions for future work.
2 Previous Work
2.1 Translationese
The special features of translated texts have been
studied widely for many years. Attempts to de-
fine their characteristics, often called ?Translation
Universals?, include (Toury, 1980; Blum-Kulka
and Levenston, 1983; Baker, 1993; Gellerstam,
1986). The differences between native and trans-
lated texts found there go well beyond systematic
translation errors and point to a distinct ?Transla-
tionese? dialect.
Using automatic text classification methods in
the field of translation studies had many use cases
in recent years, mainly as an empirical method
of measuring, proving or contradicting translation
universals. Several works (Baroni and Bernar-
dini, 2006; Kurokawa et al, 2009; Ilisei et al,
2010) used text classification techniques in order
to distinguish human translated text from native
language text at document or paragraph level, us-
ing features like word and POS n-grams, propor-
tion of grammatical words in the text, nouns, fi-
nite verbs, auxiliary verbs, adjectives, adverbs, nu-
289
merals, pronouns, prepositions, determiners, con-
junctions etc. Koppel and Ordan (2011) classi-
fied texts to original or translated, using a list
of 300 function words taken from LIWC (Pen-
nebaker et al, 2001) as features. Volanski et
al. (2013) also tested various hypotheses regarding
?Translationese?, using 32 different linguistically-
informed features, to assess the degree to which
different sets of features can distinguish between
translated and original texts.
2.2 Machine Translation Detection
Regarding the detection of machine translated
text, Carter and Inkpen (2012) translated the
Hansards of the 36th Parliament of Canada us-
ing the Microsoft Bing MT web service, and
conducted three detection experiments at docu-
ment level, using unigrams, average token length,
and type-token ratio as features. Arase and
Zhou (2013) trained a sentence-level classifier to
distinguish machine translated text from human
generated text on English and Japanese web-page
corpora, translated by Google Translate, Bing and
an in-house SMT system. They achieved very high
detection accuracy using application-specific fea-
ture sets for this purpose, including indicators of
the ?Phrase Salad? (Lopez, 2008) phenomenon or
?Gappy-Phrases? (Bansal et al, 2011).
While Arase and Zhou (2013) considered MT
detection at sentence level, as we do in this pa-
per, they did not study the correlation between the
translation quality of the machine translated text
and the ability to detect it. We show below that
such detection is possible with very high accuracy
only on low-quality translations. We examine this
detection accuracy vs. quality correlation, with
various MT systems, such as rule-based and sta-
tistical MT, both commercial and in-house, using
various feature sets.
3 Detection Experiments
3.1 Features
We wish to distinguish machine translated En-
glish sentences from either human-translated sen-
tences or native English sentences. Due to the
sparseness of the data at the sentence level, we
use common content-independent linguistic fea-
tures for the classification task. Our features are
binary, denoting the presence or absence of each
of a set of part-of-speech n-grams acquired using
the Stanford POS tagger (Toutanova et al, 2003),
as well as the presence or absence of each of 467
function words taken from LIWC (Pennebaker et
al., 2001). We consider only those entries that ap-
pear at least ten times in the entire corpus, in order
to reduce sparsity in the data. As our learning al-
gorithm we use SVM with sequential minimal op-
timization (SMO), taken from the WEKA machine
learning toolkit (Hall et al, 2009).
3.2 Detecting Different MT Systems
In the first experiment set, we explore the ability
to detect outputs of machine translated text from
different MT systems, in an environment contain-
ing both human generated and machine translated
text. For this task, we use a portion of the Cana-
dian Hansard corpus (Germann, 2001), containing
48,914 parallel sentences from French to English.
We translate the French portion of the corpus using
several MT systems, respectively: Google Trans-
late, Systran, and five other commercial MT sys-
tems available at the http://itranslate4.eu website,
which enables to query example MT systems built
by several european MT companies. After trans-
lating the sentences, we take 20,000 sentences
from each engine output and conduct the detection
experiment by labeling those sentences as MT sen-
tences, and another 20,000 sentences, which are
the human reference translations, labeled as ref-
erence sentences. We conduct a 10-fold cross-
validation experiment on the entire 40,000 sen-
tence corpus. We also conduct the same exper-
iment using 20,000 random, non-reference sen-
tences from the same corpus, instead of the ref-
erence sentences. Using simple linear regression,
we also obtain an R
2
value (coefficient of deter-
mination) over the measurements of detection ac-
curacy and BLEU score, for each of three feature
set combinations (function words, POS tags and
mixed) and the two data combinations (MT vs.
reference and MT vs. non reference sentences).
The detection and R
2
results are shown in Table 1.
As can be seen, best detection results are ob-
tained using the full combined feature set. It can
also be seen that, as might be expected, it is easier
to distinguish machine-translated sentences from
a non-reference set than from the reference set. In
Figure 1, we show the relationship of the observed
detection accuracy for each system with the BLEU
score of that system. As is evident, regardless
of the feature set or non-MT sentences used, the
correlation between detection accuracy and BLEU
290
10 20 30
60
70
80
90
BLEU
d
e
t
e
c
t
i
o
n
a
c
c
u
r
a
c
y
(
%
)
mix-nr
mix-r
fw-nr
fw-r
pos-nr
pos-r
Figure 1: Correlation between detection accu-
racy and BLEU score on commercial MT systems,
using POS, function words and mixed features
against reference and non-reference sentences.
score is very high, as we can also see from the R
2
values in Table 1.
3.3 In-House SMT Systems
Parallel Monolingual BLEU
SMT-1 2000k 2000k 28.54
SMT-2 1000k 1000k 27.76
SMT-3 500k 500k 29.18
SMT-4 100k 100k 23.83
SMT-5 50k 50k 24.34
SMT-6 25k 25k 22.46
SMT-7 10k 10k 20.72
Table 3: Details for Moses based SMT systems
In the second experiment set, we test our de-
tection method on SMT systems we created, in
which we have control over the training data and
the expected overall relative translation quality. In
order to do so, we use the Moses statistical ma-
chine translation toolkit (Koehn et al, 2007). To
train the systems, we take a portion of the Europarl
corpus (Koehn, 2005), creating 7 different SMT
systems, each using a different amount of train-
ing data, for both the translation model and lan-
guage model. We do this in order to create dif-
ferent quality translation systems, details of which
are described in Table 3. For purposes of classifi-
cation, we use the same content independent fea-
tures as in the previous experiment, based on func-
20 22 24 26 28 30
72
73
74
75
76
BLEU
d
e
t
e
c
t
i
o
n
a
c
c
u
r
a
c
y
(
%
)
R
2
= 0.789
Figure 2: Correlation between detection accu-
racy and BLEU score on in-house Moses-based
SMT systems against non-reference sentences us-
ing content independent features.
tion words and POS tags, again with SMO-based
SVM as the classifier. For data, we use 20,000 ran-
dom, non reference sentences from the Hansard
corpus, against 20,000 sentences from one MT
system per experiment, again resulting in 40,000
sentence instances per experiment. The relation-
ship between the detection results for each MT
system and the BLEU score for that system, re-
sulting in R
2
= 0.774, is shown in Figure 2.
4 Machine Translation Evaluation
4.1 Human Evaluation Experiments
As can be seen in the above experiments, there is
a strong correlation between the BLEU score and
the MT detection accuracy of our method. In fact,
results are linearly and negatively correlated with
BLEU, as can be seen both on commercial systems
and our in-house SMT systems. We also wish to
consider the relationship between detection accu-
racy and a human quality estimation score. To
do this, we use the French-English data from the
8th Workshop on Statistical Machine Translation
- WMT13? (Bojar et al, 2013), containing out-
puts from 13 different MT systems and their hu-
man evaluations. We conduct the same classifi-
cation experiment as above, with features based
on function words and POS tags, and SMO-based
SVM as the classifier. We first use 3000 refer-
291
Features Data Google Moses Systran ProMT Linguatec Skycode Trident R
2
mixed MT/non-ref 63.34 72.02 72.36 78.2 79.57 80.9 89.36 0.946
mixed MT/ref 59.51 69.47 69.77 75.86 78.11 79.24 88.85 0.944
func. w. MT/non-ref 60.43 69.17 69.87 69.78 71.38 75.46 84.97 0.798
func. w. MT/ref 57.27 66.05 67.48 67.06 68.58 73.37 84.79 0.779
POS MT/non-ref 60.32 64.39 66.61 73 73.9 74.33 79.6 0.978
POS MT/ref 57.21 65.55 64.12 70.29 73.06 73.04 78.84 0.948
Table 1: Classifier performance, including the R
2
coefficient describing the correlation with BLEU.
MT Engine Example
Google Translate ?These days, all but one were subject to a vote,
and all had a direct link to the post September 11th.?
Moses ?these days , except one were the subject of a vote ,
and all had a direct link with the after 11 September .?
Systran ?From these days, all except one were the object of a vote,
and all were connected a direct link with after September 11th.?
Linguatec ?Of these days, all except one were making the object of a vote
and all had a straightforward tie with after September 11.?
ProMT ?These days, very safe one all made object a vote,
and had a direct link with after September 11th.?
Trident ?From these all days, except one operated object voting,
and all had a direct rope with after 11 septembre.?
Skycode ?In these days, all safe one made the object in a vote
and all had a direct connection with him after 11 of September.?
Table 2: Outputs from several MT systems for the same source sentence (function words marked in bold)
0.3 0.4 0.5 0.6
58
60
62
64
human evaluation score
d
e
t
e
c
t
i
o
n
a
c
c
u
r
a
c
y
(
%
)
R
2
= 0.774
Figure 3: Correlation between detection accuracy
and human evaluation scores on systems from
WMT13? against reference sentences.
0.3 0.4 0.5 0.6
73
74
75
76
77
human evaluation score
d
e
t
e
c
t
i
o
n
a
c
c
u
r
a
c
y
(
%
)
R
2
= 0.556
Figure 4: Correlation between detection accu-
racy and human evaluation scores on systems from
WMT 13? against non-reference sentences.
292
0.3 0.4 0.5 0.6
62
64
66
human evaluation score
d
e
t
e
c
t
i
o
n
a
c
c
u
r
a
c
y
(
%
)
R
2
= 0.829
Figure 5: Correlation between detection accu-
racy and human evaluation scores on systems from
WMT 13? against non-reference sentences, using
the syntactic CFG features described in section 4.2
ence sentences from the WMT13? English refer-
ence translations, against the matching 3000 out-
put sentences from one MT system at a time, re-
sulting in 6000 sentence instances per experiment.
As can be seen in Figure 3, the detection accuracy
is strongly correlated with the evaluations scores,
yielding R
2
= 0.774. To provide another mea-
sure of correlation, we compared every pair of
data points in the experiment to get the proportion
of pairs ordered identically by the human evalu-
ators and our method, with a result of 0.846 (66
of 78). In the second experiment, we use 3000
random, non reference sentences from the new-
stest 2011-2012 corpora published in WMT12?
(Callison-Burch et al, 2012) against 3000 output
sentences from one MT system at a time, again re-
sulting in 6000 sentence instances per experiment.
While applying the same classification method as
with the reference sentences, the detection accu-
racy rises, while the correlation with the transla-
tion quality yields R
2
= 0.556, as can be seen in
Figure 4. Here, the proportion of identically or-
dered pairs is 0.782 (61 of 78).
4.2 Syntactic Features
We note that the second leftmost point in Figures
3, 4 is an outlier: that is, our method has a hard
time detecting sentences produced by this system
although it is not highly rated by human evalu-
ators. This point represents the Joshua (Post et
al., 2013) SMT system. This system is syntax-
based, which apparently confound our POS and
FW-based classifier, despite it?s low human evalu-
ation score. We hypothesize that the use of syntax-
based features might improve results. To ver-
ify this intuition, we create parse trees using the
Berkeley parser (Petrov and Klein, 2007) and ex-
tract the one-level CFG rules as features. Again,
we represent each sentence as a boolean vector,
in which each entry represents the presence or ab-
sence of the CFG rule in the parse-tree of the sen-
tence. Using these features alone, without the FW
and POS tag based features presented above, we
obtain an R
2
= 0.829 with a proportion of iden-
tically ordered pairs at 0.923 (72 of 78), as shown
in Figure 5.
5 Discussion and Future Work
We have shown that it is possible to detect ma-
chine translation from monolingual corpora con-
taining both machine translated text and human
generated text, at sentence level. There is a strong
correlation between the detection accuracy that
can be obtained and the BLEU score or the human
evaluation score of the machine translation itself.
This correlation holds whether or not a reference
set is used. This suggests that our method might be
used as an unsupervised quality estimation method
when no reference sentences are available, such
as for resource-poor source languages. Further
work might include applying our methods to other
language pairs and domains, acquiring word-level
quality estimation or integrating our method in
a machine translation system. Furthermore, ad-
ditional features and feature selection techniques
can be applied, both for improving detection ac-
curacy and for strengthening the correlation with
human quality estimation.
Acknowledgments
We would like to thank Noam Ordan and Shuly
Wintner for their help and feedback on the early
stages of this work. This research was funded in
part by the Intel Collaborative Research Institute
for Computational Intelligence.
293
References
Yuki Arase and Ming Zhou. 2013. Machine transla-
tion detection from monolingual web-text. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1597?1607, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Mona Baker. 1993. Corpus linguistics and transla-
tion studies: Implications and applications. Text and
technology: in honour of John Sinclair, 233:250.
Mohit Bansal, Chris Quirk, and Robert C. Moore.
2011. Gappy phrasal alignment by agreement. In
Dekang Lin, Yuji Matsumoto, and Rada Mihalcea,
editors, ACL, pages 1308?1317. The Association for
Computer Linguistics.
Marco Baroni and Silvia Bernardini. 2006. A new
approach to the study of translationese: Machine-
learning the difference between original and trans-
lated text. LLC, 21(3):259?274.
Shoshana Blum-Kulka and Eddie A. Levenston. 1983.
Universals of lexical simplification. Strategies in In-
terlanguage Communication, pages 119?139.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montr?eal, Canada, June. Association for
Computational Linguistics.
Dave Carter and Diana Inkpen. 2012. Searching
for poor quality machine translated text: Learning
the difference between human writing and machine
translations. In Leila Kosseim and Diana Inkpen,
editors, Canadian Conference on AI, volume 7310
of Lecture Notes in Computer Science, pages 49?60.
Springer.
Martin Gellerstam. 1986. Translationese in swedish
novels translated from english. In Lars Wollin
and Hans Lindquist, editors, Translation Studies in
Scandinavia, pages 88?95.
Ulrich Germann. 2001. Aligned hansards of the 36th
parliament of canada release 2001-1a.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
Iustina Ilisei, Diana Inkpen, Gloria Corpas Pastor, and
Ruslan Mitkov. 2010. Identification of transla-
tionese: A machine learning approach. In Alexan-
der F. Gelbukh, editor, CICLing, volume 6008 of
Lecture Notes in Computer Science, pages 503?511.
Springer.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL. The Association for Computer Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Sum-
mit, pages 79?86, Phuket, Thailand. AAMT, AAMT.
Moshe Koppel and Noam Ordan. 2011. Translationese
and its dialects. In Dekang Lin, Yuji Matsumoto,
and Rada Mihalcea, editors, ACL, pages 1318?1326.
The Association for Computer Linguistics.
David Kurokawa, Cyril Goutte, and Pierre Isabelle.
2009. Automatic Detection of Translated Text and
its Impact on Machine Translation. In Conference
Proceedings: the twelvth Machine Translation Sum-
mit.
Adam Lopez. 2008. Statistical machine translation.
ACM Computing Surveys (CSUR), 40(3):8.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Technical report,
IBM Research Report.
J.W. Pennebaker, M.E. Francis, and R.J. Booth. 2001.
Linguistic inquiry and word count: Liwc 2001.
Mahway: Lawrence Erlbaum Associates.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Proceedings of the Main Confer-
ence, pages 404?411.
Marius Popescu. 2011. Studying translationese at
the character level. In Galia Angelova, Kalina
Bontcheva, Ruslan Mitkov, and Nicolas Nicolov, ed-
itors, RANLP, pages 634?639. RANLP 2011 Organ-
ising Committee.
Matt Post, Juri Ganitkevitch, Luke Orland, Jonathan
Weese, Yuan Cao, and Chris Callison-Burch. 2013.
Joshua 5.0: Sparser, better, faster, server. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, August 8-9, 2013., pages 206?
212. Association for Computational Linguistics.
Gideon Toury. 1980. In Search of a Theory of Transla-
tion.
294
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In IN PROCEEDINGS OF HLT-NAACL, pages 252?
259.
Hans van Halteren. 2008. Source language markers
in europarl translations. In Donia Scott and Hans
Uszkoreit, editors, COLING, pages 937?944.
Vered Volansky, Noam Ordan, and Shuly Wintner.
2013. On the features of translationese. Literary
and Linguistic Computing.
295
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 302?308,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Dependency-Based Word Embeddings
Omer Levy
?
and Yoav Goldberg
Computer Science Department
Bar-Ilan University
Ramat-Gan, Israel
{omerlevy,yoav.goldberg}@gmail.com
Abstract
While continuous word embeddings are
gaining popularity, current models are
based solely on linear contexts. In this
work, we generalize the skip-gram model
with negative sampling introduced by
Mikolov et al to include arbitrary con-
texts. In particular, we perform exper-
iments with dependency-based contexts,
and show that they produce markedly
different embeddings. The dependency-
based embeddings are less topical and ex-
hibit more functional similarity than the
original skip-gram embeddings.
1 Introduction
Word representation is central to natural language
processing. The default approach of represent-
ing words as discrete and distinct symbols is in-
sufficient for many tasks, and suffers from poor
generalization. For example, the symbolic repre-
sentation of the words ?pizza? and ?hamburger?
are completely unrelated: even if we know that
the word ?pizza? is a good argument for the verb
?eat?, we cannot infer that ?hamburger? is also
a good argument. We thus seek a representation
that captures semantic and syntactic similarities
between words. A very common paradigm for ac-
quiring such representations is based on the distri-
butional hypothesis of Harris (1954), stating that
words in similar contexts have similar meanings.
Based on the distributional hypothesis, many
methods of deriving word representations were ex-
plored in the NLP community. On one end of the
spectrum, words are grouped into clusters based
on their contexts (Brown et al, 1992; Uszkor-
eit and Brants, 2008). On the other end, words
?
Supported by the European Community?s Seventh
Framework Programme (FP7/2007-2013) under grant agree-
ment no. 287923 (EXCITEMENT).
are represented as a very high dimensional but
sparse vectors in which each entry is a measure
of the association between the word and a particu-
lar context (see (Turney and Pantel, 2010; Baroni
and Lenci, 2010) for a comprehensive survey).
In some works, the dimensionality of the sparse
word-context vectors is reduced, using techniques
such as SVD (Bullinaria and Levy, 2007) or LDA
(Ritter et al, 2010; S?eaghdha, 2010; Cohen et
al., 2012). Most recently, it has been proposed
to represent words as dense vectors that are de-
rived by various training methods inspired from
neural-network language modeling (Bengio et al,
2003; Collobert and Weston, 2008; Mnih and
Hinton, 2008; Mikolov et al, 2011; Mikolov et
al., 2013b). These representations, referred to as
?neural embeddings? or ?word embeddings?, have
been shown to perform well across a variety of
tasks (Turian et al, 2010; Collobert et al, 2011;
Socher et al, 2011; Al-Rfou et al, 2013).
Word embeddings are easy to work with be-
cause they enable efficient computation of word
similarities through low-dimensional matrix op-
erations. Among the state-of-the-art word-
embedding methods is the skip-gram with nega-
tive sampling model (SKIPGRAM), introduced by
Mikolov et al (2013b) and implemented in the
word2vec software.
1
Not only does it produce
useful word representations, but it is also very ef-
ficient to train, works in an online fashion, and
scales well to huge copora (billions of words) as
well as very large word and context vocabularies.
Previous work on neural word embeddings take
the contexts of a word to be its linear context ?
words that precede and follow the target word, typ-
ically in a window of k tokens to each side. How-
ever, other types of contexts can be explored too.
In this work, we generalize the SKIP-
GRAM model, and move from linear bag-of-words
contexts to arbitrary word contexts. Specifically,
1
code.google.com/p/word2vec/
302
following work in sparse vector-space models
(Lin, 1998; Pad?o and Lapata, 2007; Baroni and
Lenci, 2010), we experiment with syntactic con-
texts that are derived from automatically produced
dependency parse-trees.
The different kinds of contexts produce no-
ticeably different embeddings, and induce differ-
ent word similarities. In particular, the bag-of-
words nature of the contexts in the ?original?
SKIPGRAM model yield broad topical similari-
ties, while the dependency-based contexts yield
more functional similarities of a cohyponym na-
ture. This effect is demonstrated using both quali-
tative and quantitative analysis (Section 4).
The neural word-embeddings are considered
opaque, in the sense that it is hard to assign mean-
ings to the dimensions of the induced represen-
tation. In Section 5 we show that the SKIP-
GRAM model does allow for some introspection
by querying it for contexts that are ?activated by? a
target word. This allows us to peek into the learned
representation and explore the contexts that are
found by the learning process to be most discrim-
inative of particular words (or groups of words).
To the best of our knowledge, this is the first work
to suggest such an analysis of discriminatively-
trained word-embedding models.
2 The Skip-Gram Model
Our departure point is the skip-gram neural em-
bedding model introduced in (Mikolov et al,
2013a) trained using the negative-sampling pro-
cedure presented in (Mikolov et al, 2013b). In
this section we summarize the model and train-
ing objective following the derivation presented by
Goldberg and Levy (2014), and highlight the ease
of incorporating arbitrary contexts in the model.
In the skip-gram model, each word w ? W is
associated with a vector v
w
? R
d
and similarly
each context c ? C is represented as a vector
v
c
? R
d
, where W is the words vocabulary, C
is the contexts vocabulary, and d is the embed-
ding dimensionality. The entries in the vectors
are latent, and treated as parameters to be learned.
Loosely speaking, we seek parameter values (that
is, vector representations for both words and con-
texts) such that the dot product v
w
? v
c
associated
with ?good? word-context pairs is maximized.
More specifically, the negative-sampling objec-
tive assumes a dataset D of observed (w, c) pairs
of words w and the contexts c, which appeared in
a large body of text. Consider a word-context pair
(w, c). Did this pair come from the data? We de-
note by p(D = 1|w, c) the probability that (w, c)
came from the data, and by p(D = 0|w, c) =
1 ? p(D = 1|w, c) the probability that (w, c) did
not. The distribution is modeled as:
p(D = 1|w, c) =
1
1+e
?v
w
?v
c
where v
w
and v
c
(each a d-dimensional vector) are
the model parameters to be learned. We seek to
maximize the log-probability of the observed pairs
belonging to the data, leading to the objective:
argmax
v
w
,v
c
?
(w,c)?D
log
1
1+e
?v
c
?v
w
This objective admits a trivial solution in which
p(D = 1|w, c) = 1 for every pair (w, c). This can
be easily achieved by setting v
c
= v
w
and v
c
?v
w
=
K for all c, w, where K is large enough number.
In order to prevent the trivial solution, the ob-
jective is extended with (w, c) pairs for which
p(D = 1|w, c) must be low, i.e. pairs which are
not in the data, by generating the set D
?
of ran-
dom (w, c) pairs (assuming they are all incorrect),
yielding the negative-sampling training objective:
argmax
v
w
,v
c
(
?
(w,c)?D
p(D = 1|c, w)
?
(w,c)?D
?
p(D = 0|c, w)
)
which can be rewritten as:
argmax
v
w
,v
c
(
?
(w,c)?D
log ?(v
c
? v
w
) +
?
(w,c)?D
?
log ?(?v
c
? v
w
)
)
where ?(x) = 1/(1+e
x
). The objective is trained
in an online fashion using stochastic-gradient up-
dates over the corpus D ?D
?
.
The negative samples D
?
can be constructed in
various ways. We follow the method proposed by
Mikolov et al: for each (w, c) ? D we construct
n samples (w, c
1
), . . . , (w, c
n
), where n is a hy-
perparameter and each c
j
is drawn according to its
unigram distribution raised to the 3/4 power.
Optimizing this objective makes observed
word-context pairs have similar embeddings,
while scattering unobserved pairs. Intuitively,
words that appear in similar contexts should have
similar embeddings, though we have not yet found
a formal proof that SKIPGRAM does indeed max-
imize the dot product of similar words.
3 Embedding with Arbitrary Contexts
In the SKIPGRAM embedding algorithm, the con-
texts of a word w are the words surrounding it
303
in the text. The context vocabulary C is thus
identical to the word vocabulary W . However,
this restriction is not required by the model; con-
texts need not correspond to words, and the num-
ber of context-types can be substantially larger
than the number of word-types. We generalize
SKIPGRAM by replacing the bag-of-words con-
texts with arbitrary contexts.
In this paper we experiment with dependency-
based syntactic contexts. Syntactic contexts cap-
ture different information than bag-of-word con-
texts, as we demonstrate using the sentence ?Aus-
tralian scientist discovers star with telescope?.
Linear Bag-of-Words Contexts This is the
context used by word2vec and many other neu-
ral embeddings. Using a window of size k around
the target word w, 2k contexts are produced: the
k words before and the k words after w. For
k = 2, the contexts of the target word w are
w
?2
, w
?1
, w
+1
, w
+2
. In our example, the contexts
of discovers are Australian, scientist, star, with.
2
Note that a context window of size 2 may miss
some important contexts (telescope is not a con-
text of discovers), while including some acciden-
tal ones (Australian is a context discovers). More-
over, the contexts are unmarked, resulting in dis-
covers being a context of both stars and scientist,
which may result in stars and scientists ending
up as neighbours in the embedded space. A win-
dow size of 5 is commonly used to capture broad
topical content, whereas smaller windows contain
more focused information about the target word.
Dependency-Based Contexts An alternative to
the bag-of-words approach is to derive contexts
based on the syntactic relations the word partic-
ipates in. This is facilitated by recent advances
in parsing technology (Goldberg and Nivre, 2012;
Goldberg and Nivre, 2013) that allow parsing to
syntactic dependencies with very high speed and
near state-of-the-art accuracy.
After parsing each sentence, we derive word
contexts as follows: for a target word w with
modifiers m
1
, . . . ,m
k
and a head h, we consider
the contexts (m
1
, lbl
1
), . . . , (m
k
, lbl
k
), (h, lbl
?1
h
),
2
word2vec?s implementation is slightly more compli-
cated. The software defaults to prune rare words based on
their frequency, and has an option for sub-sampling the fre-
quent words. These pruning and sub-sampling happen before
the context extraction, leading to a dynamic window size. In
addition, the window size is not fixed to k but is sampled
uniformly in the range [1, k] for each word.
Australian scientist discovers star with telescope
amod
nsubj
dobj
prep
pobj
Australian scientist discovers star telescope
amod
nsubj
dobj
prep with
WORD CONTEXTS
australian scientist/amod
?1
scientist australian/amod, discovers/nsubj
?1
discovers scientist/nsubj, star/dobj, telescope/prep with
star discovers/dobj
?1
telescope discovers/prep with
?1
Figure 1: Dependency-based context extraction example.
Top: preposition relations are collapsed into single arcs,
making telescope a direct modifier of discovers. Bottom: the
contexts extracted for each word in the sentence.
where lbl is the type of the dependency relation be-
tween the head and the modifier (e.g. nsubj, dobj,
prep with, amod) and lbl
?1
is used to mark the
inverse-relation. Relations that include a preposi-
tion are ?collapsed? prior to context extraction, by
directly connecting the head and the object of the
preposition, and subsuming the preposition itself
into the dependency label. An example of the de-
pendency context extraction is given in Figure 1.
Notice that syntactic dependencies are both
more inclusive and more focused than bag-of-
words. They capture relations to words that are
far apart and thus ?out-of-reach? with small win-
dow bag-of-words (e.g. the instrument of discover
is telescope/prep with), and also filter out ?coinci-
dental? contexts which are within the window but
not directly related to the target word (e.g. Aus-
tralian is not used as the context for discovers). In
addition, the contexts are typed, indicating, for ex-
ample, that stars are objects of discovery and sci-
entists are subjects. We thus expect the syntactic
contexts to yield more focused embeddings, cap-
turing more functional and less topical similarity.
4 Experiments and Evaluation
We experiment with 3 training conditions: BOW5
(bag-of-words contexts with k = 5), BOW2
(same, with k = 2) and DEPS (dependency-based
syntactic contexts). We modified word2vec to
support arbitrary contexts, and to output the con-
text embeddings in addition to the word embed-
dings. For bag-of-words contexts we used the
original word2vec implementation, and for syn-
tactic contexts, we used our modified version. The
negative-sampling parameter (how many negative
contexts to sample for every correct one) was 15.
304
All embeddings were trained on English
Wikipedia. For DEPS, the corpus was tagged
with parts-of-speech using the Stanford tagger
(Toutanova et al, 2003) and parsed into labeled
Stanford dependencies (de Marneffe and Man-
ning, 2008) using an implementation of the parser
described in (Goldberg and Nivre, 2012). All to-
kens were converted to lowercase, and words and
contexts that appeared less than 100 times were
filtered. This resulted in a vocabulary of about
175,000 words, with over 900,000 distinct syntac-
tic contexts. We report results for 300 dimension
embeddings, though similar trends were also ob-
served with 600 dimensions.
4.1 Qualitative Evaluation
Our first evaluation is qualitative: we manually in-
spect the 5 most similar words (by cosine similar-
ity) to a given set of target words (Table 1).
The first target word, Batman, results in similar
sets across the different setups. This is the case for
many target words. However, other target words
show clear differences between embeddings.
In Hogwarts - the school of magic from the
fictional Harry Potter series - it is evident that
BOW contexts reflect the domain aspect, whereas
DEPS yield a list of famous schools, capturing
the semantic type of the target word. This ob-
servation holds for Turing
3
and many other nouns
as well; BOW find words that associate with w,
while DEPS find words that behave like w. Turney
(2012) described this distinction as domain simi-
larity versus functional similarity.
The Florida example presents an ontologi-
cal difference; bag-of-words contexts generate
meronyms (counties or cities within Florida),
while dependency-based contexts provide cohy-
ponyms (other US states). We observed the same
behavior with other geographical locations, partic-
ularly with countries (though not all of them).
The next two examples demonstrate that simi-
larities induced from DEPS share a syntactic func-
tion (adjectives and gerunds), while similarities
based on BOW are more diverse. Finally, we ob-
serve that while both BOW5 and BOW2 yield top-
ical similarities, the larger window size result in
more topicality, as expected.
3
DEPS generated a list of scientists whose name ends with
?ing?. This is may be a result of occasional POS-tagging
errors. Still, the embedding does a remarkable job and re-
trieves scientists, despite the noisy POS. The list contains
more mathematicians without ?ing? further down.
Target Word BOW5 BOW2 DEPS
batman
nightwing superman superman
aquaman superboy superboy
catwoman aquaman supergirl
superman catwoman catwoman
manhunter batgirl aquaman
hogwarts
dumbledore evernight sunnydale
hallows sunnydale collinwood
half-blood garderobe calarts
malfoy blandings greendale
snape collinwood millfield
turing
nondeterministic non-deterministic pauling
non-deterministic finite-state hotelling
computability nondeterministic heting
deterministic buchi lessing
finite-state primality hamming
florida
gainesville fla texas
fla alabama louisiana
jacksonville gainesville georgia
tampa tallahassee california
lauderdale texas carolina
object-oriented
aspect-oriented aspect-oriented event-driven
smalltalk event-driven domain-specific
event-driven objective-c rule-based
prolog dataflow data-driven
domain-specific 4gl human-centered
dancing
singing singing singing
dance dance rapping
dances dances breakdancing
dancers breakdancing miming
tap-dancing clowning busking
Table 1: Target words and their 5 most similar words, as in-
duced by different embeddings.
We also tried using the subsampling option
(Mikolov et al, 2013b) with BOW contexts (not
shown). Since word2vec removes the subsam-
pled words from the corpus before creating the
window contexts, this option effectively increases
the window size, resulting in greater topicality.
4.2 Quantitative Evaluation
We supplement the examples in Table 1 with
quantitative evaluation to show that the qualita-
tive differences pointed out in the previous sec-
tion are indeed widespread. To that end, we use
the WordSim353 dataset (Finkelstein et al, 2002;
Agirre et al, 2009). This dataset contains pairs of
similar words that reflect either relatedness (top-
ical similarity) or similarity (functional similar-
ity) relations.
4
We use the embeddings in a re-
trieval/ranking setup, where the task is to rank the
similar pairs in the dataset above the related ones.
The pairs are ranked according to cosine sim-
ilarities between the embedded words. We then
draw a recall-precision curve that describes the
embedding?s affinity towards one subset (?sim-
ilarity?) over another (?relatedness?). We ex-
pect DEPS?s curve to be higher than BOW2?s
curve, which in turn is expected to be higher than
4
Some word pairs are judged to exhibit both types of sim-
ilarity, and were ignored in this experiment.
305
Figure 2: Recall-precision curve when attempting to rank the
similar words above the related ones. (a) is based on the
WordSim353 dataset, and (b) on the Chiarello et al dataset.
BOW5?s. The graph in Figure 2a shows this is in-
deed the case. We repeated the experiment with a
different dataset (Chiarello et al, 1990) that was
used by Turney (2012) to distinguish between do-
main and functional similarities. The results show
a similar trend (Figure 2b). When reversing the
task such that the goal is to rank the related terms
above the similar ones, the results are reversed, as
expected (not shown).
5
5 Model Introspection
Neural word embeddings are often considered
opaque and uninterpretable, unlike sparse vec-
tor space representations in which each dimen-
sion corresponds to a particular known context, or
LDA models where dimensions correspond to la-
tent topics. While this is true to a large extent, we
observe that SKIPGRAM does allow a non-trivial
amount of introspection. Although we cannot as-
sign a meaning to any particular dimension, we
can indeed get a glimpse at the kind of informa-
tion being captured by the model, by examining
which contexts are ?activated? by a target word.
Recall that the learning procedure is attempting
to maximize the dot product v
c
?v
w
for good (w, c)
pairs and minimize it for bad ones. If we keep the
context embeddings, we can query the model for
the contexts that are most activated by (have the
highest dot product with) a given target word. By
doing so, we can see what the model learned to be
a good discriminative context for the word.
To demonstrate, we list the 5 most activated
contexts for our example words with DEPS em-
beddings in Table 2. Interestingly, the most dis-
criminative syntactic contexts in these cases are
5
Additional experiments (not presented in this paper) re-
inforce our conclusion. In particular, we found that DEPS
perform dramatically worse than BOW contexts on analogy
tasks as in (Mikolov et al, 2013c; Levy and Goldberg, 2014).
batman hogwarts turing
superman/conj
?1
students/prep at
?1
machine/nn
?1
spider-man/conj
?1
educated/prep at
?1
test/nn
?1
superman/conj student/prep at
?1
theorem/poss
?1
spider-man/conj stay/prep at
?1
machines/nn
?1
robin/conj learned/prep at
?1
tests/nn
?1
florida object-oriented dancing
marlins/nn
?1
programming/amod
?1
dancing/conj
beach/appos
?1
language/amod
?1
dancing/conj
?1
jacksonville/appos
?1
framework/amod
?1
singing/conj
?1
tampa/appos
?1
interface/amod
?1
singing/conj
florida/conj
?1
software/amod
?1
ballroom/nn
Table 2: Words and their top syntactic contexts.
not associated with subjects or objects of verbs
(or their inverse), but rather with conjunctions, ap-
positions, noun-compounds and adjectivial modi-
fiers. Additionally, the collapsed preposition rela-
tion is very useful (e.g. for capturing the school
aspect of hogwarts). The presence of many con-
junction contexts, such as superman/conj for
batman and singing/conj for dancing, may
explain the functional similarity observed in Sec-
tion 4; conjunctions in natural language tend to en-
force their conjuncts to share the same semantic
types and inflections.
In the future, we hope that insights from such
model introspection will allow us to develop better
contexts, by focusing on conjunctions and prepo-
sitions for example, or by trying to figure out why
the subject and object relations are absent and
finding ways of increasing their contributions.
6 Conclusions
We presented a generalization of the SKIP-
GRAM embedding model in which the linear bag-
of-words contexts are replaced with arbitrary ones,
and experimented with dependency-based con-
texts, showing that they produce markedly differ-
ent kinds of similarities. These results are ex-
pected, and follow similar findings in the distri-
butional semantics literature. We also demon-
strated how the resulting embedding model can be
queried for the discriminative contexts for a given
word, and observed that the learning procedure
seems to favor relatively local syntactic contexts,
as well as conjunctions and objects of preposition.
We hope these insights will facilitate further re-
search into improved context modeling and better,
possibly task-specific, embedded representations.
Our software, allowing for experimentation with
arbitrary contexts, together with the embeddings
described in this paper, are available for download
at the authors? websites.
306
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 19?27, Boulder, Colorado, June. Association
for Computational Linguistics.
Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representations
for multilingual nlp. In Proc. of CoNLL 2013.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673?721.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
Peter F Brown, Robert L Mercer, Vincent J
Della Pietra, and Jenifer C Lai. 1992. Class-based
n-gram models of natural. Computational Linguis-
tics, 18(4).
John A Bullinaria and Joseph P Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39(3):510?526.
Christine Chiarello, Curt Burgess, Lorie Richards, and
Alma Pollock. 1990. Semantic and associative
priming in the cerebral hemispheres: Some words
do, some words don?t... sometimes, some places.
Brain and Language, 38(1):75?104.
Raphael Cohen, Yoav Goldberg, and Michael Elhadad.
2012. Domain adaptation of a dependency parser
with a class-class selectional preference model. In
Proceedings of ACL 2012 Student Research Work-
shop, pages 43?48, Jeju Island, Korea, July. Associ-
ation for Computational Linguistics.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning, pages 160?167.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, pages 1?8, Manchester, UK, Au-
gust. Coling 2008 Organizing Committee.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116?131.
Yoav Goldberg and Omer Levy. 2014. word2vec
explained: deriving mikolov et al?s negative-
sampling word-embedding method. arXiv preprint
arXiv:1402.3722.
Yoav Goldberg and Joakim Nivre. 2012. A dynamic
oracle for the arc-eager system. In Proc. of COLING
2012.
Yoav Goldberg and Joakim Nivre. 2013. Training
deterministic parsers with non-deterministic oracles.
Transactions of the association for Computational
Linguistics, 1.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Omer Levy and Yoav Goldberg. 2014. Linguistic
regularities in sparse and explicit word representa-
tions. In Proceedings of the Eighteenth Conference
on Computational Natural Language Learning, Bal-
timore, Maryland, USA, June. Association for Com-
putational Linguistics.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics - Volume 2, ACL ?98, pages
768?774, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Tomas Mikolov, Stefan Kombrink, Lukas Burget,
JH Cernocky, and Sanjeev Khudanpur. 2011.
Extensions of recurrent neural network language
model. In Acoustics, Speech and Signal Processing
(ICASSP), 2011 IEEE International Conference on,
pages 5528?5531. IEEE.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their com-
positionality. In Advances in Neural Information
Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. Pro-
ceedings of a meeting held December 5-8, 2013,
Lake Tahoe, Nevada, United States, pages 3111?
3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746?751, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
307
Andriy Mnih and Geoffrey E Hinton. 2008. A scal-
able hierarchical distributed language model. In Ad-
vances in Neural Information Processing Systems,
pages 1081?1088.
Sebastian Pad?o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent dirichlet alocation method for selectional pref-
erences. In ACL, pages 424?434.
Diarmuid
?
O S?eaghdha. 2010. Latent variable models
of selectional preference. In ACL, pages 435?444.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151?161. Association for
Computational Linguistics.
Kristina Toutanova, Dan Klein, Chris Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of NAACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
P.D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37(1):141?
188.
Peter D. Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533?
585.
Jakob Uszkoreit and Thorsten Brants. 2008. Dis-
tributed word clustering for large scale class-based
language modeling in machine translation. In Proc.
of ACL, pages 755?762.
308
A Tabular Method for Dynamic Oracles in Transition-Based Parsing
Yoav Goldberg
Department of
Computer Science
Bar Ilan University, Israel
yoav.goldberg@gmail.com
Francesco Sartorio
Department of
Information Engineering
University of Padua, Italy
sartorio@dei.unipd.it
Giorgio Satta
Department of
Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Abstract
We develop parsing oracles for two trans-
ition-based dependency parsers, including the
arc-standard parser, solving a problem that
was left open in (Goldberg and Nivre, 2013).
We experimentally show that using these or-
acles during training yields superior parsing
accuracies on many languages.
1 Introduction
Greedy transition-based dependency parsers (Nivre,
2008) incrementally process an input sentence from
left to right. These parsers are very fast and
provide competitive parsing accuracies (Nivre et al.,
2007). However, greedy transition-based parsers
still fall behind search-based parsers (Zhang and
Clark, 2008; Huang and Sagae, 2010) with respect
to accuracy.
The training of transition-based parsers relies on
a component called the parsing oracle, which maps
parser configurations to optimal transitions with re-
spect to a gold tree. A discriminative model is then
trained to simulate the oracle?s behavior. A parsing
oracle is deterministic if it returns a single canon-
ical transition. Furthermore, an oracle is partial if it
is defined only for configurations that can reach the
gold tree, that is, configurations representing pars-
ing histories with no mistake. Oracles that are both
deterministic and partial are called static. Tradition-
ally, only static oracles have been exploited in train-
ing of transition-based parsers.
Recently, Goldberg and Nivre (2012; 2013)
showed that the accuracy of greedy parsers can be
substantially improved without affecting their pars-
ing speed. This improvement relies on the intro-
duction of novel oracles that are nondeterministic
and complete. An oracle is nondeterministic if it re-
turns the set of all transitions that are optimal with
respect to the gold tree, and it is complete if it is
well-defined and correct for every configuration that
is reachable by the parser. Oracles that are both non-
deterministic and complete are called dynamic.
Goldberg and Nivre (2013) develop dynamic or-
acles for several transition-based parsers. The con-
struction of these oracles is based on a property of
transition-based parsers that they call arc decompos-
ition. They also prove that the popular arc-standard
system (Nivre, 2004) is not arc-decomposable, and
they leave as an open research question the construc-
tion of a dynamic oracle for the arc-standard system.
In this article, we develop one such oracle (?4) and
prove its correctness (?5).
An extension to the arc-standard parser was
presented by Sartorio et al. (2013), which relaxes
the bottom-up construction order and allows mixing
of bottom-up and top-down strategies. This parser,
called here the LR-spine parser, achieves state-of-
the-art results for greedy parsing. Like the arc-stand-
ard system, the LR-spine parser is not arc-decom-
posable, and a dynamic oracle for this system was
not known. We extend our oracle for the arc-stand-
ard system to work for the LR-spine system as well
(?6).
The dynamic oracles developed by Goldberg and
Nivre (2013) for arc-decomposable systems are
based on local properties of computations. In con-
trast, our novel dynamic oracle algorithms rely on
arguably more complex structural properties of com-
putations, which are computed through dynamic
programming. This leaves open the question of
whether a machine-learning model can learn to ef-
fectively simulate such complex processes: will the
119
Transactions of the Association for Computational Linguistics, 2 (2014) 119?130. Action Editor: Ryan McDonald.
Submitted 11/2013; Revised 2/2014; Published 4/2014. c?2014 Association for Computational Linguistics.
benefit of training with the dynamic oracle carry
over to the arc-standard and LR-spine systems? We
show experimentally that this is indeed the case (?8),
and that using the training-with-exploration method
of (Goldberg and Nivre, 2013) with our dynamic
programming based oracles yields superior parsing
accuracies on many languages.
2 Arc-Standard Parser
In this section we introduce the arc-standard parser
of Nivre (2004), which is the model that we use in
this article. To keep the notation at a simple level,
we only discuss the unlabeled version of the parser;
however, a labeled extension is used in ?8 for our
experiments.
2.1 Preliminaries and Notation
The set of non-negative integers is denoted as N0.
For i, j ? N0 with i ? j, we write [i, j] to denote
the set {i, i + 1, . . . , j}. When i > j, [i, j] denotes
the empty set.
We represent an input sentence as a string w =
w0 ? ? ?wn, n ? N0, where token w0 is a special
root symbol, and each wi with i ? [1, n] is a lex-
ical token. For i, j ? [0, n] with i ? j, we write
w[i, j] to denote the substring wiwi+1 ? ? ?wj of w.
We write i ? j to denote a grammatical de-
pendency of some unspecified type between lexical
tokens wi and wj , where wi is the head and wj is the
dependent. A dependency tree for w is a directed,
ordered tree t = (Vw, A), such that Vw = [0, n] is
the set of nodes, A ? Vw?Vw is the set of arcs, and
node 0 is the root. Arc (i, j) encodes a dependency
i ? j, and we will often use the latter notation to
denote arcs.
2.2 Transition-Based Dependency Parsing
We assume the reader is familiar with the formal
framework of transition-based dependency parsing
originally introduced by Nivre (2003); see Nivre
(2008) for an introduction. We only summarize here
our notation.
Transition-based dependency parsers use a stack
data structure, where each stack element is associ-
ated with a tree spanning (generating) some sub-
string of the input w. The parser processes the in-
put string incrementally, from left to right, applying
at each step a transition that updates the stack and/or
consumes one token from the input. Transitions may
also construct new dependencies, which are added to
the current configuration of the parser.
We represent the stack data structure as an
ordered sequence ? = [?d, . . . , ?1], d ? N0, of
nodes ?i ? Vw, with the topmost element placed
at the right. When d = 0, we have the empty stack
? = []. Sometimes we use the vertical bar to denote
the append operator for ?, and write ? = ??|?1 to
indicate that ?1 is the topmost element of ?.
The parser also uses a buffer to store the portion
of the input string still to be processed. We represent
the buffer as an ordered sequence ? = [i, . . . , n] of
nodes from Vw, with i the first element of the buf-
fer. In this way ? always encodes a (non-necessarily
proper) suffix of w. We denote the empty buffer as
? = []. Sometimes we use the vertical bar to denote
the append operator for ?, and write ? = i|?? to in-
dicate that i is the first token of ?; consequently, we
have ?? = [i+ 1, . . . , n].
When processing w, the parser reaches several
states, technically called configurations. A config-
uration of the parser relative to w is a triple c =
(?, ?,A), where ? and ? are a stack and a buffer,
respectively, and A ? Vw ? Vw is a set of arcs. The
initial configuration for w is ([], [0, . . . , n], ?). For
the purpose of this article, a configuration is final
if it has the form ([0], [], A), and in a final config-
uration arc set A always defines a dependency tree
for w.
The core of a transition-based parser is the set of
its transitions, which are specific to each family of
parsers. A transition is a binary relation defined
over the set of configurations of the parser. We use
symbol ` to denote the union of all transition rela-
tions of a parser.
A computation of the parser on w is a sequence
c0, . . . , cm, m ? N0, of configurations (defined rel-
ative to w) such that ci?1 ` ci for each i ? [1,m].
We also use the reflexive and transitive closure rela-
tion `? to represent computations. A computation is
called complete whenever c0 is initial and cm is fi-
nal. In this way, a complete computation is uniquely
associated with a dependency tree for w.
2.3 Arc-Standard Parser
The arc-standard model uses the three types of trans-
itions formally specified in Figure 1
120
(?, i|?,A) `sh (?|i, ?, A)
(?|i|j, ?,A) `la (?|j, ?,A ? {j ? i})
(?|i|j, ?,A) `ra (?|i, ?, A ? {i? j})
Figure 1: Transitions in the arc-standard model.
? Shift (sh) removes the first node in the buffer
and pushes it into the stack;
? Left-Arc (la) creates a new arc with the topmost
node on the stack as the head and the second-
topmost node as the dependent, and removes
the second-topmost node from the stack;
? Right-Arc (ra) is symmetric to la in that it cre-
ates an arc with the second-topmost node as the
head and the topmost node as the dependent,
and removes the topmost node.
Notation We sometimes use the functional nota-
tion for a transition ? ? {sh, la, ra}, and write
?(c) = c? in place of c `? c?. Naturally, sh applies
only when the buffer is not empty, and la,ra require
two elements on the stack. We denote by valid(c)
the set of valid transitions in a given configuration.
2.4 Arc Decomposition
Goldberg and Nivre (2013) show how to derive dy-
namic oracles for any transition-based parser which
has the arc decomposition property, defined below.
They also show that the arc-standard parser is not
arc-decomposable.
For a configuration c, we write Ac to denote the
associated set of arcs. A transition-based parser is
arc-decomposable if, for every configuration c and
for every set of arcs A that can be extended to a pro-
jective tree, we have
?(i? j) ? A,?c?[c `? c? ? (i? j) ? Ac? ]
? ?c??[c `? c?? ?A ? Ac?? ] .
In words, if each arc in A is individually derivable
from c, then the set A in its entirety can be derived
from c as well. The arc decomposition property
is useful for deriving dynamic oracles because it is
relatively easy to investigate derivability for single
arcs and then, using this property, draw conclusions
about the number of gold-arcs that are simultan-
eously derivable from the given configuration.
Unfortunately, the arc-standard parser is not arc-
decomposable. To see why, consider a configura-
tion with stack ? = [i, j, k]. Consider also arc set
A = {(i, j), (i, k)}. The arc (i, j) can be derived
through the transition sequence ra, ra, and the arc
(i, k) can be derived through the alternative trans-
ition sequence la, ra. Yet, it is easy to see that a con-
figuration containing both arcs cannot be reached.
As we cannot rely on the arc decomposition prop-
erty, in order to derive a dynamic oracle for the arc-
standard model we need to develop more sophistic-
ated techniques which take into account the interac-
tion among the applied transitions.
3 Configuration Loss and Dynamic Oracles
We aim to derive a dynamic oracle for the arc-stand-
ard (and related) system. This is a function that takes
a configuration c and a gold tree tG and returns a set
of transitions that are ?optimal? for c with respect
to tG. As already mentioned in the introduction, a
dynamic oracle can be used to improve training of
greedy transition-based parsers. In this section we
provide a formal definition for a dynamic oracle.
Let t1 and t2 be two dependency trees over the
same stringw, with arc setsA1 andA2, respectively.
We define the loss of t1 with respect to t2 as
L(t1, t2) = |A1 \A2| . (1)
Note that L(t1, t2) = L(t2, t1), since |A1| =
|A2|. Furthermore L(t1, t2) = 0 if and only if t1
and t2 are the same tree.
Let c be a configuration of our parser relative to
input string w. We write D(c) to denote the set of
all dependency trees that can be obtained in a com-
putation of the form c `? cf , where cf is some final
configuration. We extend the loss function in (1) to
configurations by letting
L(c, t2) = min
t1?D(c)
L(t1, t2) . (2)
Assume some reference (desired) dependency
tree tG for w, which we call the gold tree. Quantity
L(c, tG) can be used to compute a dynamic oracle
relating a parser configuration c to a set of optimal
actions by setting
oracle(c, tG) =
{? | L(?(c), tG)? L(c, tG) = 0} . (3)
121
We therefore need to develop an algorithm for com-
puting (2). We will do this first for the arc-standard
parser, and then for an extension of this model.
Notation We also apply the loss function L(t, tG)
in (1) when t is a dependency tree for a substring
of w. In this case the nodes of t are a subset of
the nodes of tG, and L(t, tG) provides a count of
the nodes of t that are assigned a wrong head node,
when tG is considered as the reference tree.
4 Main Algorithm
Throughout this section we assume an arc-standard
parser. Our algorithm takes as input a projective
gold tree tG and a configuration c = (?L, ?, A). We
call ?L the left stack, in contrast with a right stack
whose construction is specified below.
4.1 Basic Idea
The algorithm consists of two steps. Informally, in
the first step we compute the largest subtrees, called
here tree fragments, of the gold tree tG that have
their span entirely included in the buffer ?. The
root nodes of these tree fragments are then arranged
into a stack data structure, according to the order in
which they appear in ? and with the leftmost root in
? being the topmost element of the stack. We call
this structure the right stack ?R. Intuitively, ?R can
be viewed as the result of pre-computing ? by ap-
plying all sequences of transitions that match tG and
that can be performed independently of the stack in
the input configuration c, that is, ?L.
In the second step of the algorithm we use dy-
namic programming techniques to simulate all com-
putations of the arc-standard parser starting in a con-
figuration with stack ?L and with a buffer consisting
of ?R, with the topmost token of ?R being the first
token of the buffer. As we will see later, the search
space defined by these computations includes the de-
pendency trees for w that are reachable from the in-
put configuration c and that have minimum loss. We
then perform a Viterbi search to pick up such value.
The second step is very similar to standard imple-
mentations of the CKY parser for context-free gram-
mars (Hopcroft and Ullman, 1979), running on an
input string obtained as the concatenation of ?L and
?R. The main difference is that we restrict ourselves
to parse only those constituents in ?L?R that dom-
inate the topmost element of ?L (the rightmost ele-
ment, if ?L is viewed as a string). In this way, we ac-
count for the additional constraint that we visit only
those configurations of the arc-standard parser that
can be reached from the input configuration c. For
instance, this excludes the reduction of two nodes in
?L that are not at the two topmost positions. This
would also exclude the reduction of two nodes in
?R: this is correct, since the associated tree frag-
ments have been chosen as the largest such frag-
ments in ?.
The above intuitive explanation will be made
mathematically precise in ?5, where the notion of
linear dependency tree is introduced.
4.2 Construction of the Right Stack
In the first step we process ? and construct a stack
?R, which we call the right stack associated with c
and tG. Each node of ?R is the root of a tree t which
satisfies the following properties
? t is a tree fragment of the gold tree tG having
span entirely included in the buffer ?;
? t is bottom-up complete for tG, meaning that
for each node i of t different from t?s root, the
dependents of i in tG cannot be in ?L;
? t is maximal for tG, meaning that every super-
tree of t in tG violates the above conditions.
The stack ?R is incrementally constructed by pro-
cessig ? from left to right. Each node i is copied into
?R if it satisfies any of the following conditions
? the parent node of i in tG is not in ?;
? some dependent of i in tG is in ?L or has
already been inserted in ?R.
It is not difficult to see that the nodes in ?R are the
roots of tree fragments of tG that satisfy the condi-
tion of bottom-up completeness and the condition of
maximality defined above.
4.3 Computation of Configuration Loss
We start with some notation. Let `L = |?L| and
`R = |?R|. We write ?L[i] to denote the i-th ele-
ment of ?L and t(?L[i]) to denote the correspond-
ing tree fragment; ?R[i] and t(?R[i]) have a similar
meaning. In order to simplify the specification of
the algorithm, we assume below that ?L[1] = ?R[1].
122
Algorithm 1 Computation of the loss function for the arc-standard parser
1: T [1, 1](?L[1])? L(t(?L[1]), tG)
2: for d? 1 to `L + `R ? 1 do . d is the index of a sub-anti-diagonal
3: for j ? max{1, d? `L + 1} to min{d, `R} do . j is the column index
4: i? d? j + 1 . i is the row index
5: if i < `L then . expand to the left
6: for each h ? ?i,j do
7: T [i+ 1, j](h)? min{T [i+ 1, j](h), T [i, j](h) + ?G(h? ?L[i+ 1])}
8: T [i+ 1, j](?L[i+ 1])? min{T [i+ 1, j](?L[i+ 1]), T [i, j](h) + ?G(?L[i+ 1]? h)}
9: if j < `R then . expand to the right
10: for each h ? ?i,j do
11: T [i, j + 1](h)? min{T [i, j + 1](h), T [i, j](h) + ?G(h? ?R[j + 1])}
12: T [i, j+1](?R[j + 1])? min{T [i, j+1](?R[j + 1]), T [i, j](h)+?G(?R[j + 1]? h)}
13: return T [`L, `R](0) +?i?[1,`L] L(t(?L[i]), tG)
Therefore the elements of ?R which have been con-
structed in ?4.2 are ?R[i], i ? [2, `R].
Algorithm 1 uses a two-dimensional array T of
size `L ? `R, where each entry T [i, j] is an as-
sociation list from integers to integers. An entry
T [i, j](h) stores the minimum loss among depend-
ency trees rooted at h that can be obtained by run-
ning the parser on the first i elements of stack ?L and
the first j elements of buffer ?R. More precisely, let
?i,j = {?L[k] | k ? [1, i]} ?
{?R[k] | k ? [1, j]} . (4)
For each h ? ?i,j , the entry T [i, j](h) is the
minimum loss among all dependency trees defined
as above and with root h. We also assume that
T [i, j](h) is initialized to +? (not reported in the
algorithm).
Algorithm 1 starts at the top-left corner of T , vis-
iting each individual sub-anti-diagonal of T in as-
cending order, and eventually reaching the bottom-
right corner of the array. For each entry T [i, j], the
left expansion is considered (lines 5 to 8) by com-
bining with tree fragment ?L[i+ 1], through a left
or a right arc reduction. This results in the update
of T [i + 1, j](h), for each h ? ?i+1,j , whenever a
smaller value of the loss is achieved for a tree with
root h. The Kronecker-like function used at line 8
provides the contribution of each single arc to the
loss of the current tree. Denoting with AG the set of
arcs of tG, such a function is defined as
?G(i? j) =
{
0, if (i? j) ? AG;
1, otherwise. (5)
A symmetrical process is implemented for the
right expansion of T [i, j] through tree fragment
?R[j + 1] (lines 9 to 12).
As we will see in the next section, quantity
T [`L, `R](0) is the minimal loss of a tree composed
only by arcs that connect nodes in ?L and ?R. By
summing the loss of all tree fragments t(?L[i]) to
the loss in T [`L, `R](0), at line 13, we obtain the
desired result, since the loss of each tree fragment
t(?R[j]) is zero.
5 Formal Properties
Throughout this section we let w, tG, ?L, ?R and
c = (?L, ?, A) be defined as in ?4, but we no longer
assume that ?L[1] = ?R[1]. To simplify the present-
ation, we sometimes identify the tokens in w with
the associated nodes in a dependency tree for w.
5.1 Linear Trees
Algorithm 1 explores all dependency trees that can
be reached by an arc-standard parser from configur-
ation c, under the condition that (i) the nodes in the
buffer ? are pre-computed into tree fragments and
collapsed into their root nodes in the right stack ?R,
and (ii) nodes in ?R cannot be combined together
prior to their combination with other nodes in the
left stack ?L. This set of dependency trees is char-
123
j4
i6 i5 i3 j5
i4 i1 j3
i2 j1 j2
?R?L
Figure 2: A possible linear tree for string pair (?L, ?R),
where ?L = i6i5i4i3i2i1 and ?R = j1j2j3j4j5. The
spine of the tree consists of nodes j4, i3 and i1.
acterized here using the notion of linear tree, to be
used later in the correctness proof.
Consider two nodes ?L[i] and ?L[j] with j >
i > 1. An arc-standard parser can construct an arc
between ?L[i] and ?L[j], in any direction, only after
reaching a configuration in which ?L[i] is at the top
of the stack and ?L[j] is at the second topmost posi-
tion. In such configuration we have that ?L[i] dom-
inates ?L[1]. Furthermore, consider nodes ?R[i] and
?R[j] with j > i ? 1. Since we are assuming that
tree fragments t(?R[i]) and t(?R[j]) are bottom-up
complete and maximal, as defined in ?4.2, we allow
the construction of an arc between ?R[i] and ?R[j],
in any direction, only after reaching a configuration
in which ?R[i] dominates node ?L[1].
The dependency trees satisfying the restrictions
above are captured by the following definition. A
linear tree over (?L, ?R) is a projective dependency
tree t for string ?L?R satisfying both of the addi-
tional conditions reported below. The path from t?s
root to node ?L[1] is called the spine of t.
? Every node of t not in the spine is a dependent
of some node in the spine.
? For each arc i ? j in t with j in the spine, no
dependent of i can be placed in between i and
j within string ?L?R.
An example of a linear tree is depicted in Figure 2.
Observe that the second condition above forbids the
reduction of two nodes i and j, in case none of these
dominates node ?L[1]. For instance, the ra reduc-
tion of nodes i3 and i2 would result in arc i3 ? i2
replacing arc i1 ? i2 in Figure 2. The new depend-
ency tree is not linear, because of a violation of the
second condition above. Similarly, the la reduction
of nodes j3 and j4 would result in arc j4 ? j3 re-
placing arc i3 ? j3 in Figure 2, again a violation of
the second condition above.
Lemma 1 Any tree t ? D(c) can be decomposed
into trees t(?L[i]), i ? [1, `L], trees tj , j ? [1, q] and
q ? 1, and a linear tree tl over (?L, ?R,t), where
?R,t = r1 ? ? ? rq and each rj is the root node of tj . 2
PROOF (SKETCH) Trees t(?L[i]) are common to
every tree in D(c), since the arc-standard model can
not undo the arcs already built in the current con-
figuration c. Similar to the construction in ?4.2 of
the right stack ?R from tG, we let tj , j ? [1, q], be
tree fragments of t that cover only nodes associated
with the tokens in the buffer ? and that are bottom-
up complete and maximal for t. These trees are in-
dexed according to their left to right order in ?. Fi-
nally, tl is implicitly defined by all arcs of t that are
not in trees t(?L[i]) and tj . It is not difficult to see
that tl has a spine ending with node ?L[1] and is a
linear tree over (?L, ?R,t). 
5.2 Correctness
Our proof of correctness for Algorithm 1 is based on
a specific dependency tree t? for w, which we define
below. Let SL = {?L[i] | i ? [1, `L]} and letDL be
the set of nodes that are descendants of some node
in SL. Similarly, let SR = {?R[i] | i ? [1, `R]}
and let DR be the set of descendants of nodes in
SR. Note that sets SL, SR, DL and DR provide a
partition of Vw.
We choose any linear tree t?l over (?L, ?R) having
root 0, such that L(t?l , tG) = mint L(t, tG), where
t ranges over all possible linear trees over (?L, ?R)
with root 0. Tree t? consists of the set of nodes Vw
and the set of arcs obtained as the union of the set
of arcs of t?l and the set of arcs of all trees t(?L[i]),
i ? [1, `L], and t(?R[j]), j ? [1, `R].
Lemma 2 t? ? D(c). 2
PROOF (SKETCH) All tree fragments t(?L[i]) have
already been parsed and are available in the stack
associated with c. Each tree fragment t(?R[j]) can
later be constructed in the computation, when a con-
figuration c? is reached with the relevant segment of
w at the start of the buffer. Note also that parsing of
t(?R[j]) can be done in a way that does not depend
on the content of the stack in c?.
124
Finally, the parsing of the tree fragments t(?R[j])
is interleaved with the construction of the arcs from
the linear tree t?l , which are all of the form (i ? j)
with i, j ? (SL ? SR). More precisely, if (i ? j)
is an arc from t?l , at some point in the computation
nodes i and j will become available at the two top-
most positions in the stack. This follows from the
second condition in the definition of linear tree. 
We now show that tree t? is ?optimal? within the
set D(c) and with respect to tG.
Lemma 3 L(t?, tG) = L(c, tG). 2
PROOF Consider an arbitrary tree t ? D(c). As-
sume the decomposition of t defined in the proof of
Lemma 1, through trees t(?L[i]), i ? [1, `L], trees
tj , j ? [1, q], and linear tree tl over (?L, ?R,t).
Recall that an arc i ? j denotes an ordered pair
(i, j). Let us consider the following partition for the
set of arcs of any dependency tree for w
A1 = (SL ?DL)?DL ,
A2 = (SR ?DR)?DR ,
A3 = (Vw ? Vw) \ (A1 ?A2) .
In what follows, we compare the losses L(t, tG) and
L(t?, tG) by separately looking into the contribution
to such quantities due to the arcs in A1, A2 and A3.
Note that the arcs of trees t(?L[i]) are all in A1,
the arcs of trees t(?R[j]) are all in A2, and the arcs
of tree t?l are all in A3. Since t and t? share trees
t(?L[i]), when restricted to arcs in A1 quantities
L(t, tG) and L(t?, tG) are the same. When restric-
ted to arcs in A2, quantity L(t?, tG) is zero, by con-
struction of the trees t(?R[j]). Thus L(t, tG) can not
be smaller thanL(t?, tG) for these arcs. The difficult
part is the comparison of the contribution to L(t, tG)
and L(t?, tG) due to the arcs in A3. We deal with
this below.
LetAS,G be the set of all arcs from tG that are also
in set (SL ? SR) ? (SR ? SL). In words, AS,G rep-
resents gold arcs connecting nodes in SL and nodes
in SR, in any direction. Within tree t, these arcs can
only be found in the tl component, since nodes in
SL are all placed within the spine of tl, or else at the
left of that spine.
Let us consider an arc (j ? i) ? AS,G with j ?
SL and i ? SR, and let us assume that (j ? i) is in
t?l . If token ai does not occur in ?R,t, node i is not
in tl and (j ? i) can not be an arc of t. We then
have that (j ? i) contributes one unit to L(t, tG)
but does not contribute to L(t?, tG). Similarly, let
(i ? j) ? AS,G be such that i ? SR and j ? SL,
and assume that (i? j) is in t?l . If token ai does not
occur in ?R,t, arc (i ? j) can not be in t. We then
have that (i ? j) contributes one unit to L(t, tG)
but does not contribute to L(t?, tG).
Intuitively, the above observations mean that the
winning strategy for trees in D(c) is to move nodes
from SR as much as possible into the linear tree
component tl, in order to make it possible for these
nodes to connect to nodes in SL, in any direction. In
this case, arcs fromA3 will also move into the linear
tree component of a tree inD(c), as it happens in the
case of t?. We thus conclude that, when restricted to
the set of arcs in A3, quantity L(t, tG) is not smal-
ler than L(t?, tG), because stack ?R has at least as
many tokens corresponding to nodes in SR as stack
?R,t, and because t?l has the minimum loss among
all the linear trees over (?L, ?R).
Putting all of the above observations together,
we conclude that L(t, tG) can not be smaller than
L(t?, tG). This concludes the proof, since t has been
arbitrarily chosen in D(c). 
Theorem 1 Algorithm 1 computes L(c, tG). 2
PROOF (SKETCH) Algorithm 1 implements a Vi-
terbi search for trees with smallest loss among all
linear trees over (?L, ?R). Thus T [`L, `R](0) =
L(t?l , tG). The loss of the tree fragments t(?R[j])
is 0 and the loss of the tree fragments t(?L[i]) is ad-
ded at line 13 in the algorithm. Thus the algorithm
returns L(t?, tG), and the statement follows from
Lemma 2 and Lemma 3. 
5.3 Computational Analysis
Following ?4.2, the right stack ?R can be easily
constructed in time O(n), n the length of the in-
put string. We now analyze Algorithm 1. For each
entry T [i, j] and for each h ? ?i,j , we update
T [i, j](h) a number of times bounded by a con-
stant which does not depend on the input. Each up-
dating can be computed in constant time as well.
We thus conclude that Algorithm 1 runs in time
O(`L ? `R ? (`L + `R)). Quantity `L+`R is bounded
by n, but in practice the former is significantly smal-
ler. When measured over the sentences in the Penn
125
Treebank, the average value of `L+`Rn is 0.29. Interms of runtime, training is 2.3 times slower when
using our oracle instead of a static oracle.
6 Extension to the LR-Spine Parser
In this section we consider the transition-based
parser proposed by Sartorio et al. (2013), called
here the LR-spine parser. This parser is not arc-
decomposable: the same example reported in ?2.4
can be used to show this fact. We therefore extend to
the LR-spine parser the algorithm developed in ?4.
6.1 The LR-Spine Parser
Let t be a dependency tree. The left spine of t is
an ordered sequence ?i1, . . . , ip?, p ? 1, consisting
of all nodes in a descending path from the root of
t taking the leftmost child node at each step. The
right spine of t is defined symmetrically. We use ?
to denote sequence concatenation.
In the LR-spine parser each stack element ?[i] de-
notes a partially built subtree t(?[i]) and is represen-
ted by a pair (lsi, rsi), with lsi and rsi the left and the
right spine, respectively, of t(?[i]). We write lsi[k]
(rsi[k]) to represent the k-th element of lsi (rsi, re-
spectively). We also write r(?[i]) to denote the root
of t(?[i]), so that r(?[i]) = lsi[1] = rsi[1].
Informally, the LR-spine parser uses the same
transition typologies as the arc-standard parser.
However, an arc (h ? d) can now be created with
the head node h chosen from any node in the spine
of the involved tree. The transition types of the LR-
spine parser are defined as follows.
? Shift (sh) removes the first node from the buf-
fer and pushes into the stack a new element,
consisting of the left and right spines of the as-
sociated tree
(?, i|?,A) `sh (?|(?i?, ?i?), ?, A) .
? Left-Arc k (lak) creates a new arc h ? d from
the k-th node in the left spine of the topmost
tree in the stack to the head of the second ele-
ment in the stack. Furthermore, the two top-
most stack elements are replaced by a new ele-
ment associated with the resulting tree
(??|?[2]|?[1], ?, A) `lak (??|?lak , ?, A ? {h? d})
where we have set h = ls1[k], d = r(?[2]) and
?lak = (?ls1[1], . . . , ls1[k]? ? ls2, rs1).
? Right-Arc k (rak for short) is defined symmet-
rically with respect to lak
(??|?[2]|?[1], ?, A) `rak (??|?rak , ?, A ? {h? d})
where we have set h = rs2[k], d = r(?[1]) and
?rak = (ls2, ?rs2[1], . . . , rs2[k]? ? rs1).
Note that, at each configuration in the LR-spine
parser, there are |ls1| possible lak transitions, one for
each choice of a node in the left spine of t(?[1]);
similarly, there are |rs2| possible rak transitions,
one for each choice of a node in the right spine of
t(?[2]).
6.2 Configuration Loss
We only provide an informal description of the ex-
tended algorithm here, since it is very similar to the
algorithm in ?4.
In the first phase we use the procedure of ?4.2 for
the construction of the right stack ?R, considering
only the roots of elements in ?L and ignoring the
rest of the spines. The only difference is that each
element ?R[j] is now a pair of spines (lsR,j , rsR,j).
Since tree fragment t(?R[j]) is bottom-up complete
(see ?4.1), we now restrict the search space in such
a way that only the root node r(?R[j]) can take de-
pendents. This is done by setting lsR,j = rsR,j =
?r(?R[j])? for each j ? [1, `R]. In order to simplify
the presentation we also assume ?R[1] = ?L[1], as
done in ?4.3.
In the second phase we compute the loss of an in-
put configuration using a two-dimensional array T ,
defined as in ?4.3. However, because of the way
transitions are defined in the LR-spine parser, we
now need to distinguish tree fragments not only on
the basis of their roots, but also on the basis of their
left and right spines. Accordingly, we define each
entry T [i, j] as an association list with keys of the
form (ls, rs). More specifically, T [i, j](ls, rs) is the
minimum loss of a tree with left and right spines ls
and rs, respectively, that can be obtained by running
the parser on the first i elements of stack ?L and the
first j elements of buffer ?R.
We follow the main idea of Algorithm 1 and ex-
pand each tree in T [i, j] at its left side, by combin-
ing with tree fragment t(?L[i+ 1]), and at its right
side, by combining with tree fragment t(?R[j + 1]).
126
Tree combination deserves some more detailed dis-
cussion, reported below.
We consider the combination of a tree ta from
T [i, j] and tree t(?L[i+ 1]) by means of a left-arc
transition. All other cases are treated symmetric-
ally. Let (lsa, rsa) be the spine pair of ta, so that
the loss of ta is stored in T [i, j](lsa, rsa). Let also
(lsb, rsb) be the spine pair of t(?L[i+ 1]). In case
there exists a gold arc in tG connecting a node from
lsa to r(?L[i+ 1]), we choose the transition lak,
k ? [1, |lsa|], that creates such arc. In case such gold
arc does not exists, we choose the transition lak with
the maximum possible value of k, that is, k = |lsa|.
We therefore explore only one of the several pos-
sible ways of combining these two trees by means
of a left-arc transition.
We remark that the above strategy is safe. In fact,
in case the gold arc exists, no other gold arc can ever
involve the nodes of lsa eliminated by lak (see the
definition in ?6.1), because arcs can not cross each
other. In case the gold arc does not exist, our choice
of k = |lsa| guarantees that we do not eliminate any
element from lsa.
Once a transition lak is chosen, as described
above, the reduction is performed and the spine
pair (ls, rs) for the resulting tree is computed from
(lsa, rsa) and (lsb, rsb), as defined in ?6.1. At the
same time, the loss of the resulting tree is com-
puted, on the basis of the loss T [i, j](lsa, rsa), the
loss of tree t(?L[i+ 1]), and a Kronecker-like func-
tion defined below. This loss is then used to update
T [i+ 1, j](ls, rs).
Let ta and tb be two trees that must be combined
in such a way that tb becomes the dependent of
some node in one of the two spines of ta. Let also
pa = (lsa, rsa) and pb = (lsb, rsb) be spine pairs for
ta and tb, respectively. Recall that AG is the set of
arcs of tG. The new Kronecker-like function for the
computation of the loss is defined as
?G(pa, pb) =
?
?????
?????
0, if r(ta) < r(tb)?
?k[(rska ? r(tb)) ? AG];
0, if r(ta) > r(tb)?
?k[(lska ? r(tb)) ? AG];
1, otherwise.
6.3 Efficiency Improvement
The algorithm in ?6.2 has an exponential behaviour.
To see why, consider trees in T [i, j]. These trees are
produced by the combination of trees in T [i ? 1, j]
with tree t(?L[i]), or by the combination of trees in
T [i, j ? 1] with tree t(?R[j]). Since each combin-
ation involves either a left-arc or a right-arc trans-
ition, we obtain a recursive relation that resolves into
a number of trees in T [i, j] bounded by 4i+j?2.
We introduce now two restrictions to the search
space of our extended algorithm that result in a huge
computational saving. For a spine s, we write N (s)
to denote the set of all nodes in s. We also let ?i,j be
the set of all pairs (ls, rs) such that T [i, j](ls, rs) 6=
+?.
? Every time a new pair (ls, rs) is created in
?[i, j], we remove from ls all nodes different
from the root that do not have gold dependents
in {r(?L[k]) | k < i}, and we remove from
rs all nodes different from the root that do not
have gold dependents in {r(?R[k]) | k > j}.
? A pair pa = (lsa, rsa) is removed from
?[i, j] if there exists a pair pb = (lsb, rsb)
in ?[i, j] with the same root node as pa and
with (lsa, rsa) 6= (lsb, rsb), such that N (lsa) ?
N (lsb), N (rsa) ? N (rsb), and T [i, j](pa) ?
T [i, j](pb).
The first restriction above reduces the size of a spine
by eliminating a node if it is irrelevant for the com-
putation of the loss of the associated tree. The
second restriction eliminates a tree ta if there is a
tree tb with smaller loss than ta, such that in the
computations of the parser tb provides exactly the
same context as ta. It is not difficult to see that
the above restrictions do not affect the correctness of
the algorithm, since they always leave in our search
space some tree that has optimal loss.
A mathematical analysis of the computational
complexity of the extended algorithm is quite in-
volved. In Figure 3, we plot the worst case size
of T [i, j] for each value of j + i ? 1, computed
over all configurations visited in the training phase
(see ?7). We see that |T [i, j]| grows linearly with
j + i? 1, leading to the same space requirements of
Algorithm 1. Empirically, training with the dynamic
127
0 10 20 30 40 500
10
20
30
40
50
i+ j ? 1
ma
xn
um
be
ro
fe
lem
en
ts
Figure 3: Empirical worst case size of T [i, j] for each
value of i + j ? 1 as measured on the Penn Treebank
corpus.
Algorithm 2 Online training for greedy transition-
based parsers
1: w? 0
2: for k iterations do
3: shuffle(corpus)
4: for sentencew and gold tree tG in corpus do
5: c? INITIAL(w)
6: while not FINAL(c) do
7: ?p ? argmax??valid(c)w ? ?(c, ?)
8: ?o ? argmax??oracle(c,tG)w??(c, ?)
9: if ?p 6? oracle(c, tG) then
10: w? w + ?(c, ?o)? ?(c, ?p)
11: ? ?
{
?p if EXPLORE
?o otherwise
12: c? ?(c)
return averaged(w)
oracle is only about 8 times slower than training with
the oracle of Sartorio et al. (2013) without exploring
incorrect configurations.
7 Training
We follow the training procedure suggested by
Goldberg and Nivre (2013), as described in Al-
gorithm 2. The algorithm performs online learning
using the averaged perceptron algorithm. A weight
vector w (initialized to 0) is used to score the valid
transitions in each configuration based on a feature
representation ?, and the highest scoring transition
?p is predicted. If the predicted transition is not
optimal according to the oracle, the weights w are
updated away from the predicted transition and to-
wards the highest scoring oracle transition ?o. The
parser then moves to the next configuration, by tak-
ing either the predicted or the oracle transition. In
the ?error exploration? mode (EXPLORE is true), the
parser follows the predicted transition, and other-
wise the parser follows the oracle transition. Note
that the error exploration mode requires the com-
pleteness property of a dynamic oracle.
We consider three training conditions: static, in
which the oracle is deterministic (returning a single
canonical transition for each configuration) and no
error exploration is performed; nondet, in which we
use a nondeterministic partial oracle (Sartorio et al.,
2013), but do not perform error exploration; and ex-
plore in which we use the dynamic oracle and per-
form error exploration. The static setup mirrors the
way greedy parsers are traditionally trained. The
nondet setup allows the training procedure to choose
which transition to take in case of spurious ambigu-
ities. The explore setup increases the configuration
space explored by the parser during training, by ex-
posing the training procedure to non-optimal con-
figurations that are likely to occur during parsing,
together with the optimal transitions to take in these
configurations. It was shown by Goldberg and Nivre
(2012; 2013) that the nondet setup outperforms the
static setup, and that the explore setup outperforms
the nondet setup.
8 Experimental Evaluation
Datasets Performance evaluation is carried out on
CoNLL 2007 multilingual dataset, as well as on the
Penn Treebank (PTB) (Marcus et al., 1993) conver-
ted to Stanford basic dependencies (De Marneffe
et al., 2006). For the CoNLL datasets we use gold
part-of-speech tags, while for the PTB we use auto-
matically assigned tags. As usual, the PTB parser is
trained on sections 2-21 and tested on section 23.
Setup We train labeled versions of the arc-stand-
ard (std) and LR-spine (lrs) parsers under the static,
nondet and explore setups, as defined in ?7. In
the nondet setup we use a nondeterministic partial
oracle and in the explore setup we use the non-
deterministic complete oracles we present in this pa-
per. In the static setup we resolve oracle ambiguities
and choose a canonic transition sequence by attach-
ing arcs as soon as possible. In the explore setup,
128
parser:train Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian Turkish PTB
UAS
std:static 81.39 75.37 90.32 85.17 78.90 85.69 79.90 77.67 82.98 77.04 89.86
std:nondet 81.33 74.82 90.75 84.80 79.92 86.89 81.19 77.51 84.15 76.85 90.56
std:explore 82.56 74.39 90.95 85.65 81.01 87.70 81.85 78.72 84.37 77.21 90.92
lrs:static 81.67 76.07 91.47 84.24 77.93 86.36 79.43 76.56 84.64 77.00 90.33
lrs:nondet 83.14 75.53 91.31 84.98 80.03 88.38 81.12 76.98 85.29 77.63 91.18
lrs:explore 84.54 75.82 91.92 86.72 81.19 89.37 81.78 77.48 85.38 78.61 91.77
LAS
std:static 71.93 65.64 84.90 80.35 71.39 84.60 72.25 67.66 78.77 65.90 87.56
std:nondet 71.09 65.28 85.36 80.06 71.47 85.91 73.40 67.77 80.06 65.81 88.30
std:explore 72.89 65.27 85.82 81.28 72.92 86.79 74.22 69.57 80.25 66.71 88.72
lrs:static 72.24 66.21 86.02 79.36 70.48 85.38 72.36 66.79 80.38 66.02 88.07
lrs:nondet 72.94 65.66 86.03 80.47 71.32 87.45 73.09 67.70 81.32 67.02 88.96
lrs:explore 74.54 66.91 86.83 82.38 72.72 88.44 74.04 68.76 81.50 68.06 89.53
Table 1: Scores on the CoNLL 2007 dataset (including punctuation, gold parts of speech) and on Penn Tree Bank
(excluding punctuation, predicted parts of speech). Label ?std? refers to the arc-standard parser, and ?lrs? refers to the
LR-spine parser. Each number is an average over 5 runs with different randomization seeds.
from the first round of training onward, we always
follow the predicted transition (EXPLORE is true).
For all languages, we deal with non-projectivity by
skipping the non-projective sentences during train-
ing but not during test. For each parsing system,
we use the same feature templates across all lan-
guages.1 The arc-standard models are trained for 15
iterations and the LR-spine models for 30 iterations,
after which all the models seem to have converged.
Results In Table 1 we report the labeled (LAS)
and unlabeled (UAS) attachment scores. As expec-
ted, the LR-spine parsers outperform the arc-stand-
ard parsers trained under the same setup. Training
with the dynamic oracles is also beneficial: despite
the arguable complexity of our proposed oracles, the
trends are consistent with those reported by Gold-
berg and Nivre (2012; 2013). For the arc-standard
model we observe that the move from a static to
a nondeterministic oracle during training improves
the accuracy for most of languages. Making use of
the completeness of the dynamic oracle and moving
to the error exploring setup further improve results.
The only exceptions are Basque, that has a small
dataset with more than 20% of non-projective sen-
tences, and Chinese. For Chinese we observe a re-
duction of accuracy in the nondet setup, but an in-
crease in the explore setup.
For the LR-spine parser we observe a practically
constant increase of performance by moving from
1Our complete code, together with the description of the fea-
ture templates, is available on the second author?s homepage.
the static to the nondeterministic and then to the er-
ror exploring setups.
9 Conclusions
We presented dynamic oracles, based on dynamic
programming, for the arc-standard and the LR-
spine parsers. Empirical evaluation on 10 languages
showed that, despite the apparent complexity of the
oracle calculation procedure, the oracles are still
learnable, in the sense that using these oracles in
the error exploration training algorithm presented in
(Goldberg and Nivre, 2012) considerably improves
the accuracy of the trained parsers.
Our algorithm computes a dynamic oracle using
dynamic programming to explore a forest of depend-
ency trees that can be reached from a given parser
configuration. For the arc-standard parser, the com-
putation takes cubic time in the size of the largest of
the left and right input stacks. Dynamic program-
ming for the simulation of arc-standard parsers have
been proposed by Kuhlmann et al. (2011). That al-
gorithm could be adapted to compute minimum loss
for a given configuration, but the running time is
O(n4), n the size of the input string: besides being
asymptotically slower by one order of magnitude, in
practice n is also larger than the stack size above.
Acknowledgments We wish to thank the anonym-
ous reviewers. In particular, we are indebted to one
of them for two important technical remarks. The
third author has been partially supported by MIUR
under project PRIN No. 2010LYA9RH 006.
129
References
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation (LREC), volume 6,
pages 449?454.
Yoav Goldberg and Joakim Nivre. 2012. A dynamic or-
acle for arc-eager dependency parsing. In Proc. of the
24th COLING, Mumbai, India.
Yoav Goldberg and Joakim Nivre. 2013. Training
deterministic parsers with non-deterministic oracles.
Transactions of the association for Computational
Linguistics, 1.
John E. Hopcroft and Jeffrey D. Ullman. 1979. Intro-
duction to Automata Theory, Languages and Compu-
tation. Addison-Wesley, Reading, MA.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, July.
Marco Kuhlmann, Carlos Go?mez-Rodr??guez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
for transition-based dependency parsers. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Techno-
logies, pages 673?682, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of EMNLP-CoNLL.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
Eighth International Workshop on Parsing Technolo-
gies (IWPT), pages 149?160, Nancy, France.
Joakim Nivre. 2004. Incrementality in deterministic de-
pendency parsing. In Workshop on Incremental Pars-
ing: Bringing Engineering and Cognition Together,
pages 50?57, Barcelona, Spain.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguist-
ics, 34(4):513?553.
Francesco Sartorio, Giorgio Satta, and Joakim Nivre.
2013. A transition-based dependency parser using a
dynamic parsing strategy. In Proceedings of the 51st
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 135?144,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proceedings
of EMNLP.
130
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 241?247, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
A Dataset of Syntactic-Ngrams over Time
from a Very Large Corpus of English Books
Yoav Goldberg
Bar Ilan University?
yoav.goldberg@gmail.com
Jon Orwant
Google Inc.
orwant@google.com
Abstract
We created a dataset of syntactic-ngrams
(counted dependency-tree fragments) based
on a corpus of 3.5 million English books. The
dataset includes over 10 billion distinct items
covering a wide range of syntactic configura-
tions. It also includes temporal information,
facilitating new kinds of research into lexical
semantics over time. This paper describes the
dataset, the syntactic representation, and the
kinds of information provided.
1 Introduction
The distributional hypothesis of Harris (1954) states
that properties of words can be captured based on
their contexts. The consequences of this hypoth-
esis have been leveraged to a great effect by the
NLP community, resulting in algorithms for in-
ferring syntactic as well as semantic properties of
words (see e.g. (Turney and Pantel, 2010; Baroni
and Lenci, 2010) and the references therein).
In this paper, we describe a very large dataset
of syntactic-ngrams, that is, structures in which the
contexts of words are based on their respective po-
sition in a syntactic parse tree, and not on their se-
quential order in the sentence: the different words in
the ngram may be far apart from each other in the
sentence, yet close to each other syntactically. See
Figure 1 for an example of a syntactic-ngram.
The utility of syntactic contexts of words for con-
structing vector-space models of word meanings is
well established (Lin, 1998; Lin and Pantel, 2001;
Pado? and Lapata, 2007; Baroni and Lenci, 2010).
Syntactic relations are successfully used for mod-
eling selectional preferences (Erk and Pado?, 2008;
?Work performed while at Google.
Erk et al, 2010; Ritter et al, 2010; Se?aghdha,
2010), and dependency paths are also used to in-
fer binary relations between words (Lin and Pantel,
2001; Wu and Weld, 2010). The use of syntactic-
ngrams holds promise also for improving the accu-
racy of core NLP tasks such as syntactic language-
modeling (Shen et al, 2008) and syntactic-parsing
(Chen et al, 2009; Sagae and Gordon, 2009; Co-
hen et al, 2012), though most successful attempts
to improve syntactic parsing by using counts from
large corpora are based on sequential rather than
syntactic information (Koo et al, 2008; Bansal and
Klein, 2011; Pitler, 2012), we believe this is be-
cause large-scale datasets of syntactic counts are not
readily available. Unfortunately, most work utiliz-
ing counts from large textual corpora does not use a
standardized corpora for constructing their models,
making it very hard to reproduce results and chal-
lenging to compare results across different studies.
Our aim in this work is not to present new meth-
ods or results, but rather to provide a new kind of a
large-scale (based on corpora about 100 times larger
than previous efforts) high-quality and standard re-
source for researchers to build upon. Instead of fo-
cusing on a specific task, we aim to provide a flexi-
ble resource that could be adapted to many possible
tasks.
Specifically, the contribution of this work is in
creating a dataset of syntactic-ngrams which is:
? Derived from a very large (345 billion words)
corpus spanning a long time period.
? Covers a wide range of syntactic phenomena
and is adaptable to many use cases.
? Based on state-of-the-art syntactic processing
in a modern syntactic representation.
? Broken down by year of occurrence, as well
241
Figure 1: A syntactic ngram appearing 112 times in
the extended-biarcs set, which include structures contain-
ing three content words (see Section 4). Grayed items
are non-content words and are not included in the word
count. The dashed auxiliary ?have? is a functional marker
(see Section 3), appearing only in the extended-* sets.
as some coarse-grained regional and genre dis-
tinctions (British, American, Fiction).
? Freely available for non-commercial use. 1
After describing the underlying syntactic represen-
tation, we will present our definition of a syntactic-
ngram, and detail the kinds of syntactic-ngrams we
chose to include in the dataset. Then, we present de-
tails of the corpus and the syntactic processing we
performed.
With respect to previous efforts, the dataset has the
following distinguishing characteristics:
Temporal Dimension A unique aspect of our
dataset is the temporal dimension, allowing inspec-
tion of how the contexts of different words vary
over time. For example, one could examine how the
meaning of a word evolves over time by looking at
the contexts it appears in within different time peri-
ods. Figure 2 shows the cosine similarity between
the word ?rock? and the words ?stone? and ?jazz?
from year 1930 to 2000, showing that rock acquired
a new meaning around 1968.
Large syntactic contexts Previous efforts of provid-
ing syntactic counts from large scale corpora (Ba-
roni and Lenci, 2010) focus on relations between
two content words. Our dataset include structures
covering much larger tree fragments, some of them
including 5 or more content words. By including
such structures we hope to encourage research ex-
ploring higher orders of interactions, for example
modeling the relation between adjectives of two con-
joined nouns, the interactions between subjects and
objects of verbs, or fine-grained selectional prefer-
ences of verbs and nouns.
1The dataset is made publicly available under the Cre-
ative Commons Attribution-Non Commercial ShareAlike 3.0
Unported License: http://creativecommons.org/licenses/by-nc-
sa/3.0/legalcode.
Figure 2: Word-similarity over time: The word ?rock? starts
to become similar to ?jazz? around 1968. The plot shows the
cosine similarity between the immediate syntactic contexts of
of the word ?rock? in each year, to the immediate syntactic con-
texts of the words ?jazz? (in red) and ?stone? (in blue) aggre-
gated over all years.
A closely related effort to add syntactic anno-
tation to the books corpus is described in Lin et
al. (2012). That effort emphasize an interactive
query interface covering several languages, in which
the underlying syntactic representations are linear-
ngrams enriched with universal part-of-speech tags,
as well as first order unlabeled dependencies. In
contrast, our emphasis is not on an easy-to-use query
interface but instead a useful and flexible resource
for computational-minded researchers. We focus
on English and use finer-grained English-specific
POS-tags. The syntactic analysis is done using a
more accurate parser, and we provide counts over la-
beled tree fragments, covering a diverse set of tree-
fragments many of which include more than two
content words.
Counted Fragments instead of complete trees
While some efforts provide complete parse trees
from large corpora (Charniak, 2000; Baroni et al,
2009; Napoles et al, 2012), we instead provide
counted tree fragments. We believe that our form of
aggregate information is of more immediate use than
the raw parse trees. While access to the parse trees
may allow for somewhat greater flexibility in the
kinds of questions one could ask, it also comes with
a very hefty price tag in terms of the required com-
putational resources: while counting seems trivial,
it is, in fact, quite demanding computationally when
done on such a scale, and requires a massive infras-
tructure. By lifting this burden of NLP researchers,
we hope to free them to tackle interesting research
questions.
242
2 Underlying Syntactic Representation
We assume the part-of-speech tagset of the Penn
Treebank (Marcus et al, 1993). The syntactic rep-
resentation we work with is based on dependency-
grammar. Specifically, we use labeled dependency
trees following the ?basic? variant of the Stanford-
dependencies scheme (de Marneffe and Manning,
2008b; de Marneffe and Manning, 2008a).
Dependency grammar is a natural choice, as it
emphasizes individual words and explicitly mod-
els the connections between them. Stanford de-
pendencies are appealing because they model rela-
tions between content words directly, without in-
tervening functional markers (so in a construction
such as ?wanted to know? there is a direct rela-
tion (wanted, know) instead of two relation
(wanted, to) and (to, know). This facil-
itates focusing on meaning-bearing content words
and including the maximal amount of information
in an ngram.
3 Syntactic-ngrams
We define a syntactic-ngram to be a rooted con-
nected dependency tree over k words, which is a
subtree of a dependency tree over an entire sentence.
For each of the k words in the ngram, we provide in-
formation about the word-form, its part-of-speech,
and its dependency relation to its head. The ngram
also indicates the relative ordering between the dif-
ferent words (the order of the words in the syntactic-
ngram is the same as the order in which the words
appear in the underlying sentence) but not the dis-
tance between them, nor an indication whether there
is a missing material between the nodes. Examples
of syntactic-ngrams are provided in Figures 1 and 3.
Content-words and Functional-markers We dis-
tinguish between content-words which are mean-
ing bearing elements and functional-markers, which
serve to add polarity, modality or definiteness in-
formation to the meaning bearing elements, but do
not carry semantic meaning of their own, such as
the auxiliary verb ?have? in Figure 1. Specifi-
cally, we treat words with a dependency-label of
det, poss, neg, aux, auxpass, ps, mark,
complm and prt as functional-markers. With the
exception of poss, these are all closed-class cat-
egories. All other words except for prepositions
and conjunctions are treated as content-words. A
syntactic-ngram of order n includes exactly n con-
tent words. It may optionally include all of the
functional-markers that modify the content-words.
Conjunctions and Prepositions Conjunctions and
Prepositions receive a special treatment. When a co-
ordinating word (?and?, ?or?, ?but?) appears as part
of a conjunctive structure (e.g. ?X, Y, and Z?), it
is treated as a non-content word. Instead, it is al-
ways included in the syntactic-ngrams that include
the conjunctive relation it is a part of, allowing to
differentiate between the various kinds of conjunc-
tions. An example is seen in Figure 3d, in which
the relation conj(efficient, effective)
is enriched with the coordinating word ?or?. When
a coordinating word does not explicitly take part in
a conjunction relation (e.g. ?But, . . . ?) it is treated
as a content word.
When a preposition is part of a prepositional mod-
ification (i.e. in the middle of the pair (prep,
pcomp) or (prep, pobj)), such as the word
?of? in Figures 1 and 3h and the word ?as? in Figure
3e, it is treated as a non-content word, and is always
included in a syntactic-ngram whenever the words it
connects are included. In cases of ellipsis or other
cases where there is no overt pobj or pcomp (?he
is hard to deal with?) the preposition is treated as a
content word.2
Multiword Expressions Some multiword expres-
sions are recognized by the parser. Whenever a con-
tent word in an ngram has modifiers with the mwe
relation, they are included in the ngram.
4 The Provided Ngram Types
We aimed to include a diverse set of relations, with
maximal emphasis on relations between content-
bearing words, while still retaining access to defi-
2This treatment of prepositions and conjunction is similar to
the ?collapsed? variant of Stanford Dependencies (de Marneffe
and Manning, 2008a), in which preposition- and conjunction-
words do not appear as nodes in the tree but are instead anno-
tated on the dependency label between the content words they
connect, e.g. prep with(saw, telescope). However,
we chose to represent the preposition or conjunction as a node
in the tree rather than moving it to the dependency label as it
retains the information about the location of the function word
with respect to the other words in the structure, is consistent
with cases in which one of the content words is not present, and
does not blow up the label-set size.
243
niteness, modality and polarity if they are desired.
The dataset includes the following types of syntactic
structures:
nodes (47M items) consist of a single content word,
and capture the syntactic role of that word (as in Fig-
ure 3a). For example, we can learn that the pro-
noun ?he? is predominantly used as a subject, and
that ?help? as a noun is over 4 times more likely to
appear in object than in subject position.
arcs (919M items) consist of two content words, and
capture direct dependency relations such as ?subject
of?, ?adverbial modifier of? and so on (see Figure
3c,3d for examples). These correspond to ?depen-
dency triplets? as used in Lin (1998) and most other
work on syntax-based semantic similarity.
biarcs (1.78B items) consist of three content words
(either a word and its two daughters, or a child-
parent-grandparent chain) and capture relations such
as ?subject verb object?, ?a noun and two adjectivial
modifiers?, ?verb, object and adjectivial modifier of
the object? and many others.
triarcs (1.87B items) consist of four content words
(example in Figure 3f). The locality of the depen-
dency representation causes this set of three-arcs
structures to be large, sparse and noisy ? many of
the relations may appear random because some arcs
are in many cases almost independent given the oth-
ers. However, some of the relations are known to be
of interest, and we hope more of them will prove to
be of interest in the future. Some of the interesting
relations include:
- modifiers of the head noun of the subject or object
in an SVO construction: ((small,boy), ate, cookies),
(boy, ate, (tasty, cookies)), and with abstraction: ad-
jectives that a boy likes to eat: (boy, ate, (tasty, *) )
- arguments of an embeded verb (said, (boy, ate,
cookie) ), (said, ((small, boy), ate) )
- modifiers of conjoined elements ( (small, boy)
(young, girl) ) , ( (small, *) (young, *) )
- relative clause constructions ( boy, (girl, with-
cookies, saw) )
quadarcs (187M items) consist of 5 content words
(example in Figure 3h). In contrast to the previous
datasets, this set includes only a subset of the pos-
sible relations involving 5 content words. We chose
to focus on relations which are attested in the liter-
ature (Pado? and Lapata 2007; Appendix A), namely
structures consisting of two chains of length 2 with a
single head, e.g. ( (small, boy), ate, (tasty, cookie) ).
extended-nodes, extended-arcs, extended-biarcs,
extended-triarcs, extended-quadarcs (80M,
1.08B, 1.62B, 1.71B, and 180M items) Like the
above, but the functional markers of each content
words are included as well (see examples in Figures
3b, 3e, 3g). These structures retain information
regarding aspects such as modality, polarity and
definiteness, distinguishing, e.g. ?his red car? from
?her red car?, ?will go? from ?should go? and ?a
best time? from ?the best time?.
verbargs (130M items) This set of ngrams consist
of verbs with all their immediate arguments, and
can be used to study interactions between modi-
fiers of a verb, as well as subcategorization frames.
These structures are also useful for syntactic lan-
guage modeling, as all the daughters of a verb are
guaranteed to be present.
nounargs (275M items) This set of ngrams consist
of nouns with all their immediate arguments.
verbargs-unlex, nounargs-unlex (114M, 195M
items) Like the above, but only the head word and
the top-1000 occurring words in the English-1M
subcorpus are lexicalized ? other words are replaced
with a *W* symbol. By abstracting away from non-
frequent words, we include many of the larger syn-
tactic configurations that will otherwise be pruned
away by our frequency threshold. These could be
useful for inspecting fine-grained syntactic subcate-
gorization frames.
5 Corpora and Syntactic Processing
The dataset is based on the English Google Books
corpus. This is the same corpus used to derive the
Google Books Ngrams, and is described in detail in
Michel et al (2011). The corpus consists of the text
of 3,473,595 English books which were published
between 1520 and 2008, with the majority of the
content published after 1800. We provide counts
based on the entire corpus, as well as on several sub-
sets of it:
English 1M Uniformly sampled 1 million books.
Fiction Works of Fiction.
American English Books published in the US.
British English Books published in Britain.
The sizes of the different corpora are detailed in Ta-
ble 1.
244
Figure 3: Syntactic-ngram examples. Non-content words are
grayed, functional markers appearing only in the extended-*
collections are dashed. (a) node (b) extended-node (c) arcs (d)
arcs, including the coordinating word (e) extended-arcs, includ-
ing a preposition (f) triarcs (g) extended-triarcs (h) quadarcs,
including a preposition.
Counts Each syntactic ngram in each of the sub-
corpora is coupled with a corpus-level count as well
as counts from each individual year. To keep the
Corpus # Books # Pages # Sentences # Tokens
All 3.5M 925.7M 17.6B 345.1B
1M 1M 291.1M 5.1B 101.3B
Fiction 817K 231.3M 4.7B 86.1B
American 1.4M 387.6M 7.9B 146.2B
British 423K 124.9M 2.4B 46.1B
Table 1: Corpora sizes.
data manageable, we employ a frequency threshold
of 10 on the corpus-level count.
Data Processing We ignored pages with over 600
white-spaces (which are indicative of OCR errors or
non-textual content), as well as sentences of over 60
tokens. Table 1 details the sizes of the various cor-
pora.
After OCR, sentence splitting and tokenization,
the corpus went through several stages of syntactic
processing: part-of-speech tagging, syntactic pars-
ing, and syntactic-ngrams extraction.
Part-of-speech tagging was performed using a
first order CRF tagger, which was trained on a union
of the Penn WSJ Corpus (Marcus et al, 1993), the
Brown corpus (Kucera and Francis, 1967) and the
Questions Treebank (Judge et al, 2006). In addition
to the diverse training material, the tagger makes use
of features based on word-clusters derived from tri-
grams of the Books corpus. These cluster-features
make the tagger more robust on the books domain.
For further details regarding the tagger, see Lin et al
(2012).
Syntactic parsing was performed using a re-
implementation of a beam-search shift-reduce de-
pendency parser (Zhang and Clark, 2008) with a
beam of size 8 and the feature-set described in
Zhang and Nivre (2011). The parser was trained
on the same training data as the tagger after 4-way
jack-knifing so that the parser is trained on data with
predicted part-of-speech tags. The parser provides
state-of-the-art syntactic annotations for English.3
3Evaluating the quality of syntactic annotation on such a var-
ied dataset is a challenging task on its own right ? the underly-
ing corpus includes many different genres spanning different
time periods, as well as varying levels of digitization and OCR
quality. It is extremely difficult to choose a representative sam-
ple to manually annotate and evaluate on, and we believe no
single number will do justice to describing the annotation qual-
ity across the entire dataset. On top of that, we then aggregate
fragments and filter based on counts, further changing the data
distribution. We feel that it is better not to provide any numbers
than to provide inaccurate, misleading or uninformative num-
245
6 Conclusion
We created a dataset of syntactic-ngrams based on
a very large literary corpus. The dataset contains
over 10 billion unique items covering a wide range
of syntactic structures, and includes a temporal di-
mension.
The dataset is available for download at
http://storage.googleapis.com/
books/syntactic-ngrams/index.html
Acknowledgments
We would like to thank the members of Google?s
extended syntactic-parsing team (Ryan McDonald,
Keith Hall, Slav Petrov, Dipanjan Das, Hao Zhang,
Kuzman Ganchev, Terry Koo, Michael Ringgaard
and, at the time, Joakim Nivre) for many discus-
sions, support, and of course the creation and main-
tenance of an extremely robust parsing infrastruc-
ture. We further thank Fernando Pereira for sup-
porting the project, and Andrea Held and Supreet
Chinnan for their hard work in making this possible.
Sebastian Pado?, Marco Baroni, Alessandro Lenci,
Jonathan Berant and Dan Klein provided valuable
input that helped shape the final form of this re-
source.
References
Mohit Bansal and Dan Klein. 2011. Web-scale features
for full-scale parsing. In ACL, pages 693?702.
Marco Baroni and Alessandro Lenci. 2010. Distribu-
tional memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):673?
721.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Eugene Charniak. 2000. Bllip 1987-89 wsj corpus re-
lease 1. In Linguistic Data Consortium, Philadelphia.
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving depen-
dency parsing with subtrees from auto-parsed data. In
EMNLP, pages 570?579.
bers. We therefore chose not to provide a numeric estimation
of syntactic-annotation quality, but note that we used a state-
of-the-art parser, and believe most of its output to be correct,
although we do expect a fair share of annotation errors as well.
Raphael Cohen, Yoav Goldberg, and Michael Elhadad.
2012. Domain adaptation of a dependency parser with
a class-class selectional preference model. In Pro-
ceedings of ACL 2012 Student Research Workshop,
pages 43?48, Jeju Island, Korea, July. Association for
Computational Linguistics.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008a. Stanford dependencies manual. Techni-
cal report, Stanford University.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008b. The stanford typed dependencies repre-
sentation. In Coling 2008: Proceedings of the work-
shop on Cross-Framework and Cross-Domain Parser
Evaluation, CrossParser ?08, pages 1?8, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Katrin Erk and Sebastian Pado?. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of EMNLP, Honolulu, HI. To appear.
Katrin Erk, Sebastian Pado?, and Ulrike Pado?. 2010. A
flexible, corpus-driven model of regular and inverse
selectional preferences. Computational Linguistics,
36(4):723?763.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
John Judge, Aoife Cahill, and Josef van Genabith. 2006.
Questionbank: Creating a corpus of parse-annotated
questions. In Proc. of ACL, pages 497?504. Associa-
tion for Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Proc.
of ACL, pages 595?603.
Henry Kucera and W. Nelson Francis. 1967. Compu-
tational Analysis of Present-Day American English.
Brown University Press.
Dekang Lin and Patrick Pantel. 2001. Dirt: discovery of
inference rules from text. In KDD, pages 323?328.
Yuri Lin, Jean-Baptiste Michel, Erez Aiden Lieberman,
Jon Orwant, Will Brockman, and Slav Petrov. 2012.
Syntactic annotations for the google books ngram cor-
pus. In ACL (System Demonstrations), pages 169?
174.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics - Volume 2, ACL ?98, pages 768?
774, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19:313?330.
246
Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser
Aiden, Adrian Veres, Matthew K. Gray, The
Google Books Team, Joseph P. Pickett, Dale Hoiberg,
Dan Clancy, Peter Norvig, Jon Orwant, Steven Pinker,
Martin A. Nowak, and Erez Lieberman Aiden. 2011.
Quantitative analysis of culture using millions of digi-
tized books. Science, 331(6014):176?182.
Courtney Napoles, Matthew Gormley, and Benjamin Van
Durme. 2012. Annotated gigaword. In AKBC-
WEKEX Workshop at NAACL 2012, June.
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2):161?199.
Emily Pitler. 2012. Attacking parsing bottlenecks with
unlabeled data and relevant factorizations. In ACL,
pages 768?776.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet alocation method for selectional preferences.
In ACL, pages 424?434.
Kenji Sagae and Andrew S. Gordon. 2009. Clustering
words by syntactic similarity improves dependency
parsing of predicate-argument structures. In IWPT,
pages 192?201.
Diarmuid O? Se?aghdha. 2010. Latent variable models of
selectional preference. In ACL, pages 435?444.
Libin Shen, Jinxi Xu, and Ralph M. Weischedel. 2008.
A new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
ACL, pages 577?585.
P.D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37(1):141?188.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using wikipedia. In ACL, pages 118?127.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proc. of
EMNLP, pages 562?571.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188?193.
247
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 1?12,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Statistical Parsing of Morphologically Rich Languages (SPMRL)
What, How and Whither
Reut Tsarfaty
Uppsala Universitet
Djame? Seddah
Alpage (Inria/Univ. Paris-Sorbonne)
Yoav Goldberg
Ben Gurion University
Sandra Ku?bler
Indiana University
Marie Candito
Alpage (Inria/Univ. Paris 7)
Jennifer Foster
NCLT, Dublin City University
Yannick Versley
Universita?t Tu?bingen
Ines Rehbein
Universita?t Saarbru?cken
Lamia Tounsi
NCLT, Dublin City University
Abstract
The term Morphologically Rich Languages
(MRLs) refers to languages in which signif-
icant information concerning syntactic units
and relations is expressed at word-level. There
is ample evidence that the application of read-
ily available statistical parsing models to such
languages is susceptible to serious perfor-
mance degradation. The first workshop on sta-
tistical parsing of MRLs hosts a variety of con-
tributions which show that despite language-
specific idiosyncrasies, the problems associ-
ated with parsing MRLs cut across languages
and parsing frameworks. In this paper we re-
view the current state-of-affairs with respect
to parsing MRLs and point out central chal-
lenges. We synthesize the contributions of re-
searchers working on parsing Arabic, Basque,
French, German, Hebrew, Hindi and Korean
to point out shared solutions across languages.
The overarching analysis suggests itself as a
source of directions for future investigations.
1 Introduction
The availability of large syntactically annotated cor-
pora led to an explosion of interest in automati-
cally inducing models for syntactic analysis and dis-
ambiguation called statistical parsers. The devel-
opment of successful statistical parsing models for
English focused on the Wall Street Journal Penn
Treebank (PTB, (Marcus et al, 1993)) as the pri-
mary, and sometimes only, resource. Since the ini-
tial release of the Penn Treebank (PTB Marcus et
al. (1993)), many different constituent-based parsing
models have been developed in the context of pars-
ing English (e.g. (Magerman, 1995; Collins, 1997;
Charniak, 2000; Chiang, 2000; Bod, 2003; Char-
niak and Johnson, 2005; Petrov et al, 2006; Huang,
2008; Finkel et al, 2008; Carreras et al, 2008)).
At their time, each of these models improved the
state-of-the-art, bringing parsing performance on the
standard test set of the Wall-Street-Journal to a per-
formance ceiling of 92% F1-score using the PARS-
EVAL evaluation metrics (Black et al, 1991). Some
of these parsers have been adapted to other lan-
guage/treebank pairs, but many of these adaptations
have been shown to be considerably less successful.
Among the arguments that have been proposed
to explain this performance gap are the impact of
small data sets, differences in treebanks? annotation
schemes, and inadequacy of the widely used PARS-
EVAL evaluation metrics. None of these aspects in
isolation can account for the systematic performance
deterioration, but observed from a wider, cross-
linguistic perspective, a picture begins to emerge ?
that the morphologically rich nature of some of the
languages makes them inherently more susceptible
to such performance degradation. Linguistic factors
associated with MRLs, such as a large inventory of
word-forms, higher degrees of word order freedom,
and the use of morphological information in indi-
cating syntactic relations, makes them substantially
harder to parse with models and techniques that have
been developed with English data in mind.
1
In addition to these technical and linguistic fac-
tors, the prominence of English parsing in the litera-
ture reduces the visibility of research aiming to solve
problems particular to MRLs. The lack of stream-
lined communication among researchers working
on different MRLs often leads to a reinventing the
wheel syndrome. To circumvent this, the first work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL 2010) offers a platform for
this growing community to share their views of the
different problems and oftentimes similar solutions.
We identify three main types of challenges, each
of which raises many questions. Many of the ques-
tions are yet to be conclusively answered. The first
type of challenges has to do with the architectural
setup of parsing MRLs: What is the nature of the in-
put? Can words be represented abstractly to reflect
shared morphological aspects? How can we cope
with morphological segmentation errors propagated
through the pipeline? The second type concerns the
representation of morphological information inside
the articulated syntactic model: Should morpholog-
ical information be encoded at the level of PoS tags?
On dependency relations? On top of non-terminals
symbols? How should the integrated representations
be learned and used? A final genuine challenge
has to do with sound estimation for lexical probabil-
ities: Given the finite, and often rather small, set of
data, and the large number of morphological analy-
ses licensed by rich inflectional systems, how can we
analyze words unseen in the training data?
Many of the challenges reported here are mostly
irrelevant when parsing Section 23 of the PTB but
they are of primordial importance in other tasks, in-
cluding out-of-domain parsing, statistical machine
translation, and parsing resource-poor languages.
By synthesizing the contributions to the workshop
and bringing it to the forefront, we hope to advance
the state of the art of statistical parsing in general.
In this paper we therefore take the opportunity
to analyze the knowledge that has been acquired in
the different investigations for the purpose of iden-
tifying main bottlenecks and pointing out promising
research directions. In section 2, we define MRLs
and identify syntactic characteristics associated with
them. We then discuss work on parsing MRLs in
both the dependency-based and constituency-based
setup. In section 3, we review the types of chal-
lenges associated with parsing MRLs across frame-
works. In section 4, we focus on the contributions to
the SPMRL workshop and identify recurring trends
in the empirical results and conceptual solutions. In
section 5, we analyze the emerging picture from a
bird?s eye view, and conclude that many challenges
could be more faithfully addressed in the context of
parsing morphologically ambiguous input.
2 Background
2.1 What are MRLs?
The term Morphologically Rich Languages (MRLs)
is used in the CL/NLP literature to refer to languages
in which substantial grammatical information, i.e.,
information concerning the arrangement of words
into syntactic units or cues to syntactic relations, is
expressed at word level.
The common linguistic and typological wisdom is
that ?morphology competes with syntax? (Bresnan,
2001). In effect, this means that rich morphology
goes hand in hand with a host of nonconfigurational
syntactic phenomena of the kind discussed by Hale
(1983). Because information about the relations be-
tween syntactic elements is indicated in the form of
words, these words can freely change their positions
in the sentence. This is referred to as free word or-
der (Mithun, 1992). Information about the group-
ing of elements together can further be expressed by
reference to their morphological form. Such logical
groupings of disparate elements are often called dis-
continuous constituents. In dependency structures,
such discontinuities impose nonprojectivity. Finally,
rich morphological information is found in abun-
dance in conjunction with so-called pro-drop or zero
anaphora. In such cases, rich morphological infor-
mation in the head (or co-head) of the clause of-
ten makes it possible to omit an overt subject which
would be semantically impoverished.
English, the most heavily studied language within
the CL/NLP community, is not an MRL. Even
though a handful of syntactic features (such as per-
son and number) are reflected in the form of words,
morphological information is often secondary to
other syntactic factors, such as the position of words
and their arrangement into phrases. German, an
Indo-European language closely related to English,
already exhibits some of the properties that make
2
parsing MRLs problematic. The Semitic languages
Arabic and Hebrew show an even more extreme case
in terms of the richness of their morphological forms
and the flexibility in their syntactic ordering.
2.2 Parsing MRLs
Pushing the envelope of constituency parsing:
The Head-Driven models of the type proposed
by Collins (1997) have been ported to parsing
many MRLs, often via the implementation of Bikel
(2002). For Czech, the adaptation by Collins et al
(1999) culminated in an 80 F1-score.
German has become almost an archetype of the
problems caused by MRLs; even though German
has a moderately rich morphology and a moder-
ately free word order, parsing results are far from
those for English (see (Ku?bler, 2008) and references
therein). Dubey (2005) showed that, for German
parsing, adding case and morphology information
together with smoothed markovization and an ade-
quate unknown-word model is more important than
lexicalization (Dubey and Keller, 2003).
For Modern Hebrew, Tsarfaty and Sima?an (2007)
show that a simple treebank PCFG augmented with
parent annotation and morphological information as
state-splits significantly outperforms Head-Driven
markovized models of the kind made popular by
Klein and Manning (2003). Results for parsing
Modern Standard Arabic using Bikel?s implemen-
tation on gold-standard tagging and segmentation
have not improved substantially since the initial re-
lease of the treebank (Maamouri et al, 2004; Kulick
et al, 2006; Maamouri et al, 2008).
For Italian, Corazza et al (2004) used the Stan-
ford parser and Bikel?s parser emulation of Collins?
model 2 (Collins, 1997) on the ISST treebank, and
obtained significantly lower results compared to En-
glish. It is notable that these models were ap-
plied without adding morphological signatures, us-
ing gold lemmas instead. Corazza et al (2004) fur-
ther tried different refinements including parent an-
notation and horizontal markovization, but none of
them obtained the desired improvement.
For French, Crabbe? and Candito (2008) and Sed-
dah et al (2010) show that, given a corpus compara-
ble in size and properties (i.e. the number of tokens
and grammar size), the performance level, both for
Charniak?s parser (Charniak, 2000) and the Berke-
ley parser (Petrov et al, 2006) was higher for pars-
ing the PTB than it was for French. The split-merge-
smooth implementation of (Petrov et al, 2006) con-
sistently outperform various lexicalized and unlexi-
calized models for French (Seddah et al, 2009) and
for many other languages (Petrov and Klein, 2007).
In this respect, (Petrov et al, 2006) is considered
MRL-friendly, due to its language agnostic design.
The rise of dependency parsing: It is commonly
assumed that dependency structures are better suited
for representing the syntactic structures of free word
order, morphologically rich, languages, because this
representation format does not rely crucially on the
position of words and the internal grouping of sur-
face chunks (Mel?c?uk, 1988). It is an entirely differ-
ent question, however, whether dependency parsers
are in fact better suited for parsing such languages.
The CoNLL shared tasks on multilingual depen-
dency parsing in 2006 and 2007 (Buchholz and
Marsi, 2006; Nivre et al, 2007a) demonstrated that
dependency parsing for MRLs is quite challenging.
While dependency parsers are adaptable to many
languages, as reflected in the multiplicity of the lan-
guages covered,1 the analysis by Nivre et al (2007b)
shows that the best result was obtained for English,
followed by Catalan, and that the most difficult lan-
guages to parse were Arabic, Basque, and Greek.
Nivre et al (2007a) drew a somewhat typological
conclusion, that languages with rich morphology
and free word order are the hardest to parse. This
was shown to be the case for both MaltParser (Nivre
et al, 2007c) and MST (McDonald et al, 2005), two
of the best performing parsers on the whole.
Annotation and evaluation matter: An emerg-
ing question is therefore whether models that have
been so successful in parsing English are necessar-
ily appropriate for parsing MRLs ? but associated
with this question are important questions concern-
ing the annotation scheme of the related treebanks.
Obviously, when annotating structures for languages
with characteristics different than English one has to
face different annotation decisions, and it comes as
no surprise that the annotated structures for MRLs
often differ from those employed in the PTB.
1The shared tasks involved 18 languages, including many
MRLs such as Arabic, Basque, Czech, Hungarian, and Turkish.
3
For Spanish and French, it was shown by Cowan
and Collins (2005) and in (Arun and Keller, 2005;
Schluter and van Genabith, 2007), that restructuring
the treebanks? native annotation scheme to match
the PTB annotation style led to a significant gain in
parsing performance of Head-Driven models of the
kind proposed in (Collins, 1997). For German, a
language with four different treebanks and two sub-
stantially different annotation schemes, it has been
shown that a PCFG parser is sensitive to the kind of
representation employed in the treebank.
Dubey and Keller (2003), for example, showed
that a simple PCFG parser outperformed an emula-
tion of Collins? model 1 on NEGRA. They showed
that using sister-head dependencies instead of head-
head dependencies improved parsing performance,
and hypothesized that it is due to the flatness of
phrasal annotation. Ku?bler et al (2006) showed con-
siderably lower PARSEVAL scores on NEGRA (Skut
et al, 1998) relative to the more hierarchically struc-
tured Tu?Ba-D/Z (Hinrichs et al, 2005), again, hy-
pothesizing that this is due to annotation differences.
Related to such comparisons is the question of the
relevance of the PARSEVAL metrics for evaluating
parsing results across languages and treebanks. Re-
hbein and van Genabith (2007) showed that PARS-
EVAL measures are sensitive to annotation scheme
particularities (e.g. the internal node ratio). It was
further shown that different metrics (i.e. the Leaf-
ancestor path (Sampson and Babarczy, 2003) and
dependency based ones in (Lin, 1995)) can lead to
different performance ranking. This was confirmed
also for French by Seddah et al (2009).
The questions of how to annotate treebanks for
MRLs and how to evaluate the performance of the
different parsers on these different treebanks is cru-
cial. For the MRL parsing community to be able to
assess the difficulty of improving parsing results for
French, German, Arabic, Korean, Basque, Hindi or
Hebrew, we ought to first address fundamental ques-
tions including: Is the treebank sufficiently large
to allow for proper grammar induction? Does the
annotation scheme fit the language characteristics?
Does the use of PTB annotation variants for other
languages influence parsing results? Does the space-
delimited tokenization allow for phrase boundary
detection? Do the results for a specific approach
generalize to more than one language?
3 Primary Research Questions
It is firmly established in theoretical linguistics that
morphology and syntax closely interact through pat-
terns of case marking, agreement, clitics and various
types of compounds. Because of such close interac-
tions, we expect morphological cues to help parsing
performance. But in practice, when trying to incor-
porate morphological information into parsing mod-
els, three types of challenges present themselves:
Architecture and Setup: When attempting to
parse complex word-forms that encapsulate both
lexical and functional information, important archi-
tectural questions emerge, namely, what is the na-
ture of the input that is given to the parsing system?
Does the system attempt to parse sequences of words
or does it aim to assign structures to sequences of
morphological segments? If the former is the case,
how can we represent words abstractly so as to re-
flect shared morphological aspects between them?
If the latter is the case, how can we arrive at a good
enough morphological segmentation for the purpose
of statistical parsing, given raw input texts?
When working with morphologically rich lan-
guages such as Hebrew or Arabic, affixes may have
syntactically independent functions. Many parsing
models assume segmentation of the syntactically in-
dependent parts, such as prepositions or pronominal
clitics, prior to parsing. But morphological segmen-
tation requires disambiguation which is non-trivial,
due to case syncretism and high morphological am-
biguity exhibited by rich inflectional systems. The
question is then when should we disambiguate the
morphological analyses of input forms? Should we
do that prior to parsing or perhaps jointly with it?2
Representation and Modeling: Assuming that
the input to our system reflects morphological infor-
mation, one way or another, which types of morpho-
2Most studies on parsing MRLs nowadays assume the gold
standard segmentation and disambiguated morphological infor-
mation as input. This is the case, for instance, for the Arabic
parsing at CoNLL 2007 (Nivre et al, 2007a). This practice de-
ludes the community as to the validity of the parsing results
reported for MRLs in shared tasks. Goldberg et al (2009), for
instance, show a gap of up to 6pt F1-score between performance
on gold standard segmentation vs. raw text. One way to over-
come this is to devise joint morphological and syntactic disam-
biguation frameworks (cf. (Goldberg and Tsarfaty, 2008)).
4
logical information should we include in the parsing
model? Inflectional and/or derivational? Case infor-
mation and/or agreement features? How can valency
requirements reflected in derivational morphology
affect the overall syntactic structure? In tandem with
the decision concerning the morphological informa-
tion to include, we face genuine challenges concern-
ing how to represent such information in the syntac-
tic model, be it constituency-based or dependency-
based. Should we encode morphological informa-
tion at the level of PoS tags and/or on top of syn-
tactic elements? Should we decorate non-terminals
nodes and/or dependency arcs or both?
Incorporating morphology in the statistical model
is often even more challenging than the sum of
these bare decisions, because of the nonconfigu-
rational structures (free word order, discontinuous
constituents) for rich markings are crucial (Hale,
1983). The parsing models designed for English of-
ten focus on learning rigid word order, and they do
not take morphological information into account (cf.
developing parsers for German (Dubey and Keller,
2003; Ku?bler et al, 2006)). The more complex ques-
tion is therefore: what type of parsing model should
we use for parsing MRLs? shall we use a general
purpose implementation and attempt to amend it?
how? or perhaps we should devise a new model from
first principles, to address nonconfigurational phe-
nomena effectively? using what form of representa-
tion? is it possible to find a single model that can
effectively cope with different kinds of languages?
Estimation and Smoothing: Compared to En-
glish, MRLs tend to have a greater number of word
forms and higher out-of-vocabulary (OOV) rates,
due to the many feature combinations licensed by
the inflectional system. A typical problem associ-
ated with parsing MRLs is substantial lexical data
sparseness due to high morphological variation in
surface forms. The question is therefore, given our
finite, and often fairly small, annotated sets of data,
how can we guess the morphological analyses, in-
cluding the PoS tag assignment and various features,
of an OOV word? How can we learn the probabil-
ities of such assignments? In a more general setup,
this problem is akin to handling out-of-vocabulary
or rare words for robust statistical parsing, and tech-
niques for domain adaptation via lexicon enhance-
Constituency-Based Dependency-Based
Arabic (Attia et al, 2010) (Marton et al, 2010)?
Basque - (Bengoetxea and Gojenola, 2010)
English (Attia et al, 2010) -
French (Attia et al, 2010)
(Seddah et al, 2010)
(Candito and Seddah, 2010)? -
German (Maier, 2010) -
Hebrew (Tsarfaty and Sima?an, 2010) (Goldberg and Elhadad, 2010)?
Hindi - (Ambati et al, 2010a)?
(Ambati et al, 2010b)
Korean (Chung et al, 2010) -
Table 1: An overview of SPMRL contributions. (? report
results also for non-gold standard input)
ment (also explored for English and other morpho-
logically impoverished languages).
So, in fact, incorporating morphological informa-
tion inside the syntactic model for the purpose of
statistical parsing is anything but trivial. In the next
section we review the various approaches taken in
the individual contributions of the SPMRL work-
shop for addressing such challenges.
4 Parsing MRLs: Recurring Trends
The first workshop on parsing MRLs features 11
contributions for a variety of languages with a
range of different parsing frameworks. Table 1 lists
the individual contributions within a cross-language
cross-framework grid. In this section, we focus on
trends that occur among the different contributions.
This may be a biased view since some of the prob-
lems that exist for parsing MRLs may have not been
at all present, but it is a synopsis of where we stand
with respect to problems that are being addressed.
4.1 Architecture and Setup: Gold vs. Predicted
Morphological Information
While morphological information can be very infor-
mative for syntactic analysis, morphological anal-
ysis of surface forms is ambiguous in many ways.
In German, for instance, case syncretism (i.e. a sin-
gle surface form corresponding to different cases) is
pervasive, and in Hebrew and Arabic, the lack of vo-
calization patterns in written texts leads to multiple
morphological analyses for each space-delimited to-
ken. In real world situations, gold morphological in-
formation is not available prior to parsing. Can pars-
ing systems make effective use of morphology even
when gold morphological information is absent?
5
Several papers address this challenge by present-
ing results for both the gold and the automatically
predicted PoS and morphological information (Am-
bati et al, 2010a; Marton et al, 2010; Goldberg and
Elhadad, 2010; Seddah et al, 2010). Not very sur-
prisingly, all evaluated systems show a drop in pars-
ing accuracy in the non-gold settings.
An interesting trend is that in many cases, us-
ing noisy morphological information is worse than
not using any at all. For Arabic Dependency pars-
ing, using predicted CASE causes a substantial drop
in accuracy while it greatly improves performance
in the gold setting (Marton et al, 2010). For
Hindi Dependency Parsing, using chunk-internal
cues (i.e. marking non-recursive phrases) is benefi-
cial when gold chunk-boundaries are available, but
suboptimal when they are automatically predicted
(Ambati et al, 2010a). For Hebrew Dependency
Parsing with the MST parser, using gold morpholog-
ical features shows no benefit over not using them,
while using automatically predicted morphological
features causes a big drop in accuracy compared to
not using them (Goldberg and Elhadad, 2010). For
French Constituency Parsing, Seddah et al (2010)
and Candito and Seddah (2010) show that while
gold information for the part-of-speech and lemma
of each word form results in a significant improve-
ment, the gain is low when switching to predicted
information. Reassuringly, Ambati et al (2010a),
Marton et al (2010), and Goldberg and Elhadad
(2010) demonstrate that some morphological infor-
mation can indeed be beneficial for parsing even in
the automatic setting. Ensuring that this is indeed
so, appears to be in turn linked to the question of
how morphology is represented and incorporated in
the parsing model.
The same effect in a different guise appears in
the contribution of Chung et al (2010) concerning
parsing Korean. Chung et al (2010) show a sig-
nificant improvement in parsing accuracy when in-
cluding traces of null anaphors (a.k.a. pro-drop) in
the input to the parser. Just like overt morphology,
traces and null elements encapsulate functional in-
formation about relational entities in the sentence
(the subject, the object, etc.), and including them at
the input level provides helpful disambiguating cues
for the overall structure that represents such rela-
tions. However, assuming that such traces are given
prior to parsing is, for all practical purposes, infeasi-
ble. This leads to an interesting question: will iden-
tifying such functional elements (marked as traces,
overt morphology, etc) during parsing, while com-
plicating that task itself, be on the whole justified?
Closely linked to the inclusion of morphological
information in the input is the choice of PoS tag set
to use. The generally accepted view is that fine-
grained PoS tags are morphologically more informa-
tive but may be harder to statistically learn and parse
with, in particular in the non-gold scenario. Mar-
ton et al (2010) demonstrate that a fine-grained tag
set provides the best results for Arabic dependency
parsing when gold tags are known, while a much
smaller tag set is preferred in the automatic setting.
4.2 Representation and Modeling:
Incorporating Morphological Information
Many of the studies presented here explore the use
of feature representation of morphological informa-
tion for the purpose of syntactic parsing (Ambati et
al., 2010a; Ambati et al, 2010b; Bengoetxea and
Gojenola, 2010; Goldberg and Elhadad, 2010; Mar-
ton et al, 2010; Tsarfaty and Sima?an, 2010). Clear
trends among the contributions emerge concerning
the kind of morphological information that helps sta-
tistical parsing. Morphological CASE is shown to be
beneficial across the board. It is shown to help for
parsing Basque, Hebrew, Hindi and to some extent
Arabic.3 Morphological DEFINITENESS and STATE
are beneficial for Hebrew and Arabic when explic-
itly represented in the model. STATE, ASPECT and
MOOD are beneficial for Hindi, but only marginally
beneficial for Arabic. CASE and SUBORDINATION-
TYPE are the most beneficial features for Basque
transition-based dependency parsing.
A closer view into the results mentioned in the
previous paragraph suggests that, beyond the kind
of information that is being used, the way in which
morphological information is represented and used
by the model has substantial ramification as to
whether or not it leads to performance improve-
ments. The so-called ?agreement features? GEN-
DER, NUMBER, PERSON, provide for an interesting
case study in this respect. When included directly as
3For Arabic, CASE is useful when gold morphology infor-
mation is available, but substantially hurt results when it is not.
6
machine learning features, agreement features ben-
efit dependency parsing for Arabic (Marton et al,
2010), but not Hindi (dependency) (Ambati et al,
2010a; Ambati et al, 2010b) or Hebrew (Goldberg
and Elhadad, 2010). When represented as simple
splits of non-terminal symbols, agreement informa-
tion does not help constituency-based parsing per-
formance for Hebrew (Tsarfaty and Sima?an, 2010).
However, when agreement patterns are directly rep-
resented on dependency arcs, they contribute an im-
provement for Hebrew dependency parsing (Gold-
berg and Elhadad, 2010). When agreement is en-
coded at the realization level inside a Relational-
Realizational model (Tsarfaty and Sima?an, 2008),
agreement features improve the state-of-the-art for
Hebrew parsing (Tsarfaty and Sima?an, 2010).
One of the advantages of the latter study is that
morphological information which is expressed at the
level of words gets interpreted elsewhere, on func-
tional elements higher up the constituency tree. In
dependency parsing, similar cases may arise, that
is, morphological information might not be as use-
ful on the form on which it is expressed, but would
be more useful at a different position where it could
influence the correct attachment of the main verb
to other elements. Interesting patterns of that sort
occur in Basque, where the SUBORDINATIONTYPE
morpheme attaches to the auxiliary verb, though it
mainly influences attachments to the main verb.
Bengoetxea and Gojenola (2010) attempted two
different ways to address this, one using a trans-
formation segmenting the relevant morpheme and
attaching it to the main verb instead, and another
by propagating the morpheme along arcs, through
a ?stacking? process, to where it is relevant. Both
ways led to performance improvements. The idea of
a segmentation transformation imposes non-trivial
pre-processing, but it may be that automatically
learning the propagation of morphological features
is a promising direction for future investigation.
Another, albeit indirect, way to include morpho-
logical information in the parsing model is using
so-called latent information or some mechanism
of clustering. The general idea is the following:
when morphological information is added to stan-
dard terminal or non-terminal symbols, it imposes
restrictions on the distribution of these no-longer-
equivalent elements. Learning latent informa-
tion does not represent morphological information
directly, but presumably, the distributional restric-
tions can be automatically learned along with the
splits of labels symbols in models such as (Petrov
et al, 2006). For Korean (Chung et al, 2010),
latent information contributes significant improve-
ments. One can further do the opposite, namely,
merging terminals symbols for the purpose of ob-
taining an abstraction over morphological features.
When such clustering uses a morphological signa-
ture of some sort, it is shown to significantly im-
prove constituency-based parsing for French (Can-
dito and Seddah, 2010).
4.3 Representation and Modeling: Free Word
Order and Flexible Constituency Structure
Off-the-shelf parsing tools are found in abundance
for English. One problematic aspect of using them
to parse MRLs lies in the fact that these tools fo-
cus on the statistical modeling of configurational
information. These models often condition on the
position of words relative to one another (e.g. in
transition-based dependency parsing) or on the dis-
tance between words inside constituents (e.g. in
Head-Driven parsing). Many of the contributions to
the workshop show that working around existing im-
plementations may be insufficient, and we may have
to come up with more radical solutions.
Several studies present results that support the
conjecture that when free word-order is explicitly
taken into account, morphological information is
more likely to contribute to parsing accuracy. The
Relational-Realizational model used in (Tsarfaty
and Sima?an, 2010) allows for reordering of con-
stituents at a configuration layer, which is indepen-
dent of the realization patterns learned from the data
(vis-a`-vis case marking and agreement). The easy-
first algorithm of (Goldberg and Elhadad, 2010)
which allows for significant flexibility in the order of
attachment, allows the model to benefit from agree-
ment patterns over dependency arcs that are easier
to detect and attach first. The use of larger subtrees
in (Chung et al, 2010) for parsing Korean, within a
Bayesian framework, allows the model to learn dis-
tributions that take more elements into account, and
thus learn the different distributions associated with
morphologically marked elements in constituency
structures, to improve performance.
7
In addition to free word order, MRLs show higher
degree of freedom in extraposition. Both of these
phenomena can result in discontinuous structures.
In constituency-based treebanks, this is either an-
notated as additional information which has to be
recovered somehow (traces in the case of the PTB,
complex edge labels in the German Tu?Ba-D/Z), or
as discontinuous phrase structures, which cannot be
handled with current PCFG models. Maier (2010)
suggests the use of Linear Context-Free Rewriting
Systems (LCFRSs) in order to make discontinuous
structure transparent to the parsing process and yet
preserve familiar notions from constituency.
Dependency representation uses non-projective
dependencies to reflect discontinuities, which is
problematic to parse with models that assume pro-
jectivity. Different ways have been proposed to deal
with non-projectivity (Nivre and Nilsson, 2005; Mc-
Donald et al, 2005; McDonald and Pereira, 2006;
Nivre, 2009). Bengoetxea and Gojenola (2010)
discuss non-projective dependencies in Basque and
show that the pseudo-projective transformation of
(Nivre and Nilsson, 2005) improves accuracy for de-
pendency parsing of Basque. Moreover, they show
that in combination with other transformations, it
improves the utility of these other ones, too.
4.4 Estimation and Smoothing: Coping with
Lexical Sparsity
Morphological word form variation augments the
vocabulary size and thus worsens the problem of lex-
ical data sparseness. Words occurring with medium-
frequency receive less reliable estimates, and the
number of rare/unknown words is increased. One
way to cope with the one of both aspects of this
problem is through clustering, that is, providing an
abstract representation over word forms that reflects
their shared morphological and morphosyntactic as-
pects. This was done, for instance, in previous work
on parsing German. Versley and Rehbein (2009)
cluster words according to linear context features.
These clusters include valency information added to
verbs and morphological features such as case and
number added to pre-terminal nodes. The clusters
are then integrated as features in a discriminative
parsing model to cope with unknown words. Their
discriminative model thus obtains state-of-the-art re-
sults on parsing German.
Several contribution address similar challenges.
For constituency-based generative parsers, the sim-
ple technique of replacing word forms with more
abstract symbols is investigated by (Seddah et al,
2010; Candito and Seddah, 2010). For French, re-
placing each word form by its predicted part-of-
speech and lemma pair results in a slight perfor-
mance improvement (Seddah et al, 2010). When
words are clustered, even according to a very local
linear-context similarity measure, measured over a
large raw corpus, and when word clusters are used in
place of word forms, the gain in performance is even
higher (Candito and Seddah, 2010). In both cases,
the technique provides more reliable estimates for
in-vocabulary words, since a given lemma or cluster
appear more frequently. It also increases the known
vocabulary. For instance, if a plural form is un-
seen in the training set but the corresponding singu-
lar form is known, then in a setting of using lemmas
in terminal symbols, both forms are known.
For dependency parsing, Marton et al (2010) in-
vestigates the use of morphological features that in-
volve some semantic abstraction over Arabic forms.
The use of undiacritized lemmas is shown to im-
prove performance. Attia et al (2010) specifically
address the handling of unknown words in the latent-
variable parsing model. Here again, the technique
that is investigated is to project unknown words to
more general symbols using morphological clues. A
study on three languages, English, French and Ara-
bic, shows that this method helps in all cases, but
that the greatest improvement is obtained for Arabic,
which has the richest morphology among three.
5 Where we?re at
It is clear from the present overview that we are
yet to obtain a complete understanding concerning
which models effectively parse MRLs, how to an-
notate treebanks for MRLs and, importantly, how
to evaluate parsing performance across types of lan-
guages and treebanks. These foundational issues are
crucial for deriving more conclusive recommenda-
tions as to the kind of models and morphological
features that can lead to advancing the state-of-the-
art for parsing MRLs. One way to target such an
understanding would be to encourage the investiga-
tion of particular tasks, individually or in the context
8
of shared tasks, that are tailored to treat those prob-
lematic aspects of MRLs that we surveyed here.
So far, constituency-based parsers have been as-
sessed based on their performance on the PTB (and
to some extent, across German treebanks (Ku?bler,
2008)) whereas comparison across languages was
rendered opaque due to data set differences and
representation idiosyncrasies. It would be interest-
ing to investigate such a cross-linguistic compari-
son of parsers in the context of a shared task on
constituency-based statistical parsing, in additional
to dependency-based ones as reported in (Nivre et
al., 2007a). Standardizing data sets for a large
number of languages with different characteristics,
would require us, as a community, to aim for
constituency-representation guidelines that can rep-
resent the shared aspects of structures in different
languages, while at the same time allowing differ-
ences between them to be reflected in the model.
Furthermore, it would be a good idea to intro-
duce parsing tasks, for either constituent-based or
dependency-based setups, which consider raw text
as input, rather than morphologically segmented
and analyzed text. Addressing the parsing prob-
lem while facing the morphological disambiguation
challenge in its full-blown complexity would be il-
luminating and educating for at least two reasons:
firstly, it would give us a better idea of what is the
state-of-the-art for parsing MRLs in realistic scenar-
ios. Secondly, it might lead to profound insights
about the potentially successful ways to use mor-
phology inside a parser, which may differ from the
insights concerning the use of morphology in the
less realistic parsing scenarios, where gold morpho-
logical information is given.
Finally, to be able to perceive where we stand
with respect to parsing MRLs and how models fare
against one another across languages, it would be
crucial to arrive at evaluation metrics that capture
information that is shared among the different repre-
sentations, for instance, functional information con-
cerning predicate-argument relations. Using the dif-
ferent kinds of measures in the context of cross-
framework tasks will help us understand the util-
ity of the different evaluation metrics that have been
proposed and to arrive at a clearer picture of what it
is that we wish to compare, and how we can faith-
fully do so across models, languages and treebanks.
6 Conclusion
This paper presents the synthesis of 11 contributions
to the first workshop on statistical parsing for mor-
phologically rich languages. We have shown that
architectural, representational, and estimation issues
associated with parsing MRLs are found to be chal-
lenging across languages and parsing frameworks.
The use of morphological information in the non
gold-tagged input scenario is found to cause sub-
stantial differences in parsing performance, and in
the kind of morphological features that lead to per-
formance improvements.
Whether or not morphological features help pars-
ing also depends on the kind of model in which
they are embedded, and the different ways they are
treated within. Furthermore, sound statistical esti-
mation methods for morphologically rich, complex
lexica, turn out to be crucial for obtaining good pars-
ing accuracy when using general-purpose models
and algorithms. In the future we hope to gain better
understanding of the common pitfalls in, and novel
solutions for, parsing morphologically ambiguous
input, and to arrive at principled guidelines for se-
lecting the model and features to include when pars-
ing different kinds of languages. Such insights may
be gained, among other things, in the context of
more morphologically-aware shared parsing tasks.
Acknowledgements
The program committee would like to thank
NAACL for hosting the workshop and SIGPARSE
for their sponsorship. We further thank INRIA Al-
page team for their generous sponsorship. We are
finally grateful to our reviewers and authors for their
dedicated work and individual contributions.
References
Bharat Ram Ambati, Samar Husain, Sambhav Jain,
Dipti Misra Sharma, and Rajeev Sangal. 2010a. Two
methods to incorporate local morphosyntactic features
in Hindi dependency parsing. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010b. On the role of morphosyntactic
features in Hindi dependency parsing. In Proceedings
9
of the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
French. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
306?313, Ann Arbor, MI.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Kepa Bengoetxea and Koldo Gojenola. 2010. Applica-
tion of different techniques to dependency parsing of
Basque. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Daniel M. Bikel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing engine. In Pro-
ceedings of the Second International Conference on
Human Language Technology Research, pages 178?
182. Morgan Kaufmann Publishers Inc. San Francisco,
CA, USA.
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage
of English grammars. In Proceedings of the DARPA
Speech and Natural Language Workshop, pages 306?
311, San Mateo (CA). Morgan Kaufman.
Rens Bod. 2003. An efficient implementation of a new
DOP model. In Proceedings of the tenth conference
on European chapter of the Association for Computa-
tional Linguistics, pages 19?26, Budapest, Hungary.
Joan Bresnan. 2001. Lexical-Functional Syntax. Black-
well, Oxford.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Language Learning (CoNLL), pages 149?164,
New York, NY.
Marie Candito and Djame? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for
efficient, feature-rich parsing. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 9?16, Manchester,
UK.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
ACL, pages 173?180, Barcelona, Spain, June.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Annual Meeting of the
North American Chapter of the ACL (NAACL), Seattle.
David Chiang. 2000. Statistical parsing with an
automatically-extracted Tree Adjoining Grammar. In
Proceedings of the 38th Annual Meeting on Associ-
ation for Computational Linguistics, pages 456?463,
Hong Kong. Association for Computational Linguis-
tics Morristown, NJ, USA.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of Korean parsing. In
Proceedings of the NAACL/HLT Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), Los Angeles, CA.
Michael Collins, Jan Hajic?, Lance Ramshaw, and
Christoph Tillmann. 1999. A statistical parser for
Czech. In Proceedings of the 37th Annual Meeting
of the ACL, volume 37, pages 505?512, College Park,
MD.
Michael Collins. 1997. Three Generative, Lexicalized
Models for Statistical Parsing. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics, pages 16?23, Madrid, Spain.
Anna Corazza, Alberto Lavelli, Giogio Satta, and
Roberto Zanoli. 2004. Analyzing an Italian treebank
with state-of-the-art statistical parsers. In Proceedings
of the Third Third Workshop on Treebanks and Lin-
guistic Theories (TLT 2004), Tu?bingen, Germany.
Brooke Cowan and Michael Collins. 2005. Morphology
and reranking for the statistical parsing of Spanish. In
in Proceedins of EMNLP.
Benoit Crabbe? and Marie Candito. 2008. Expe?riences
d?analyse syntaxique statistique du franc?ais. In Actes
de la 15e`me Confe?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
Amit Dubey and Frank Keller. 2003. Probabilistic pars-
ing for German using sister-head dependencies. In In
Proceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 96?103,
Ann Arbor, MI.
Amit Dubey. 2005. What to do when lexicalization fails:
parsing German with suffix analysis and smoothing.
In 43rd Annual Meeting of the Association for Compu-
tational Linguistics.
10
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL.
Yoav Goldberg and Michael Elhadad. 2010. Easy-
first dependency parsing of Modern Hebrew. In Pro-
ceedings of the NAACL/HLT Workshop on Statistical
Parsing of Morphologically Rich Languages (SPMRL
2010), Los Angeles, CA.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syntac-
tic parsing. In Proceedings of the 46nd Annual Meet-
ing of the Association for Computational Linguistics.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and em-hmm-based lexical probabilities. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 327?335.
Kenneth L. Hale. 1983. Warlpiri and the grammar of
non-configurational languages. Natural Language and
Linguistic Theory, 1(1).
Erhard W. Hinrichs, Sandra Ku?bler, and Karin Naumann.
2005. A unified representation for morphological,
syntactic, semantic, and referential annotations. In
Proceedings of the ACL Workshop on Frontiers in Cor-
pus Annotation II: Pie in the Sky, pages 13?20, Ann
Arbor, MI.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430.
Sandra Ku?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it really that difficult to parse German?
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 111?
119, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Sandra Ku?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63. Association for Com-
putational Linguistics.
Seth Kulick, Ryan Gabbard, and Mitchell Marcus. 2006.
Parsing the Arabic treebank: Analysis and improve-
ments. In Proceedings of TLT.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In International
Joint Conference on Artificial Intelligence, pages
1420?1425, Montreal.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic treebank:
Building a large-scale annotated Arabic corpus. In
Proceedings of NEMLAR International Conference on
Arabic Language Resources and Tools.
Mohamed Maamouri, Ann Bies, and Seth Kulick. 2008.
Enhanced annotation and parsing of the Arabic tree-
bank. In Proceedings of INFOS.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of the 33rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 276?283, Cambridge, MA.
Wolfgang Maier. 2010. Direct parsing of discontin-
uous constituents in german. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Yuval Marton, Nizar Habash, and Owen Rambow. 2010.
Improving Arabic dependency parsing with lexical and
inflectional morphological features. In Proceedings of
the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Ryan T. McDonald and Fernando C. N. Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In Proc. of EACL?06.
Ryan T. McDonald, Koby Crammer, and Fernando C. N.
Pereira. 2005. Online large-margin training of depen-
dency parsers. In Proc. of ACL?05, Ann Arbor, USA.
Igor Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
Marianne Mithun. 1992. Is basic word order universal?
In Doris L. Payne, editor, Pragmatics of Word Order
Flexibility. John Benjamins, Amsterdam.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL), Ann Arbor, MI.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007a. The CoNLL 2007 shared task on depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, pages 915?932,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007b. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 915?932.
11
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?ls?en Eryig?it, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007c. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Joakim Nivre. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Ines Rehbein and Josef van Genabith. 2007. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), Prague, Czech Republic.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Djame? Seddah, Marie Candito, and Benoit Crabbe?. 2009.
Cross parser evaluation and tagset variation: A French
Treebank study. In Proceedings of the 11th Interna-
tion Conference on Parsing Technologies (IWPT?09),
pages 150?161, Paris, France, October. Association
for Computational Linguistics.
Djame? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Wojciech Skut, Thorsten Brants, Brigitte Krenn, and
Hans Uszkoreit. 1998. A linguistically interpreted
corpus of German newspaper texts. In ESSLLI
Workshop on Recent Advances in Corpus Annotation,
Saarbru?cken, Germany.
Reut Tsarfaty and Khalil Sima?an. 2007. Three-
dimensional parametrization for parsing morphologi-
cally rich languages. In Proceedings of the 10th Inter-
national Conference on Parsing Technologies (IWPT),
pages 156?167.
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
Realizational parsing. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics,
pages 889?896.
Reut Tsarfaty and Khalil Sima?an. 2010. Model-
ing morphosyntactic agreement in constituency-based
parsing of Modern Hebrew. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Yannick Versley and Ines Rehbein. 2009. Scalable dis-
criminative parsing for german. In Proceedings of the
11th International Conference on Parsing Technolo-
gies (IWPT?09), pages 134?137, Paris, France, Octo-
ber. Association for Computational Linguistics.
12
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 103?107,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Easy First Dependency Parsing of Modern Hebrew
Yoav Goldberg? and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
{yoavg|elhadad}@cs.bgu.ac.il
Abstract
We investigate the performance of an easy-
first, non-directional dependency parser on the
Hebrew Dependency treebank. We show that
with a basic feature set the greedy parser?s ac-
curacy is on a par with that of a first-order
globally optimized MST parser. The addition
of morphological-agreement feature improves
the parsing accuracy, making it on-par with a
second-order globally optimized MST parser.
The improvement due to the morphological
agreement information is persistent both when
gold-standard and automatically-induced mor-
phological information is used.
1 Introduction
Data-driven Dependency Parsing algorithms are
broadly categorized into two approaches (Ku?bler et
al., 2009). Transition based parsers traverse the
sentence from left to right1 using greedy, local in-
ference. Graph based parsers use global inference
and seek a tree structure maximizing some scoring
function defined over trees. This scoring function
is usually decomposed over tree edges, or pairs of
such edges. In recent work (Goldberg and Elhadad,
2010), we proposed another dependency parsing ap-
proach: Easy First, Non-Directional dependency
?Supported by the Lynn and William Frankel Center for
Computer Sciences, Ben Gurion University
1Strictly speaking, the traversal order is from start to end.
This distinction is important when discussing Hebrew parsing,
as the Hebrew language is written from right-to-left. We keep
the left-to-right terminology throughout this paper, as this is the
common terminology. However, ?left? and ?right? should be
interpreted as ?start? and ?end? respectively. Similarly, ?a token
to the left? should be interpreted as ?the previous token?.
parsing. Like transition based methods, the easy-
first method adopts a local, greedy policy. How-
ever, it abandons the strict left-to-right processing
order, replacing it with an alternative order, which
attempts to make easier attachments decisions prior
to harder ones. The model was applied to English
dependency parsing. It was shown to be more accu-
rate than MALTPARSER, a state-of-the-art transition
based parser (Nivre et al, 2006), and near the perfor-
mance of the first-order MSTPARSER, a graph based
parser which decomposes its score over tree edges
(McDonald et al, 2005), while being more efficient.
The easy-first parser works by making easier de-
cisions before harder ones. Each decision can be
conditioned by structures created by previous deci-
sions, allowing harder decisions to be based on rel-
atively rich syntactic structure. This is in contrast to
the globally optimized parsers, which cannot utilize
such rich syntactic structures. It was hypothesized
in (Goldberg and Elhadad, 2010) that this rich con-
ditioning can be especially beneficial in situations
where informative structural information is avail-
able, such as in morphologically rich languages.
In this paper, we investigate the non-directional
easy-first parser performance on Modern Hebrew, a
semitic language with rich morphology, relatively
free constituent order, and a small treebank com-
pared to English. We are interested in two main
questions: (a) how well does the non-directional
parser perform on Hebrew data? and (b) can the
parser make effective use of morphological features,
such as agreement?
In (Goldberg and Elhadad, 2009), we describe
a newly created Hebrew dependency treebank, and
103
report results on parsing this corpus with both
MALTPARSER and first- and second- order vari-
ants of MSTPARSER. We find that the second-
order MSTPARSER outperforms the first order MST-
PARSER, which in turn outperforms the transition
based MALTPARSER. In addition, adding mor-
phological information to the default configurations
of these parsers does not improve parsing accu-
racy. Interestingly, when using automatically in-
duced (rather than gold-standard) morphological in-
formation, the transition based MALTPARSER?s ac-
curacy improves with the addition of the morpho-
logical information, while the scores of both glob-
ally optimized parsers drop with the addition of the
morphological information.
Our experiments in this paper show that the ac-
curacy of the non-directional parser on the same
dataset outperforms the first-order MSTPARSER.
With the addition of morphological agreement fea-
tures, the parser accuracy improves even further, and
is on-par with the performance of the second-order
MSTPARSER. The improvement due to the morpho-
logical information persists also when automatically
induced morphological information is used.
2 Modern Hebrew
Some aspects that make Hebrew challenging from a
language-processing perspective are:
Affixation Common prepositions, conjunctions
and articles are prefixed to the following word, and
pronominal elements often appear as suffixes. The
segmentation of prefixes and suffixes is often am-
biguous and must be determined in a specific context
only. In term of dependency parsing, this means that
the dependency relations occur not between space-
delimited tokens, but instead between sub-token el-
ements which we?ll refer to as segments. Further-
more, mistakes in the underlying token segmenta-
tions are sure to be reflected in the parsing accuracy.
Relatively free constituent order The ordering of
constituents inside a phrase is relatively free. This
is most notably apparent in the verbal phrases and
sentential levels. In particular, while most sentences
follow an SVO order, OVS and VSO configurations
are also possible. Verbal arguments can appear be-
fore or after the verb, and in many ordering. For
example, the message ?went from Israel to Thai-
land? can be expressed as ?went to Thailand from
Israel?, ?to Thailand went from Israel?, ?from Israel
went to Thailand?, ?from Israel to Thailand went?
and ?to Thailand from Israel went?. This results in
long and flat VP and S structures and a fair amount
of sparsity, which suggests that a dependency repre-
sentations might be more suitable to Hebrew than a
constituency one.
NP Structure and Construct State While con-
stituents order may vary, NP internal structure is
rigid. A special morphological marker (Construct
State) is used to mark noun compounds as well as
similar phenomena. This marker, while ambiguous,
is essential for analyzing NP internal structure.
Case Marking definite direct objects are marked.
The case marker in this case is the function word z`
appearing before the direct object.2
Rich templatic morphology Hebrew has a very
productive morphological structure, which is based
on a root+template system. The productive mor-
phology results in many distinct word forms and
a high out-of-vocabulary rate which makes it hard
to reliably estimate lexical parameters from anno-
tated corpora. The root+template system (combined
with the unvocalized writing system) makes it hard
to guess the morphological analyses of an unknown
word based on its prefix and suffix, as usually done
in other languages.
Unvocalized writing system Most vowels are not
marked in everyday Hebrew text, which results in a
very high level of lexical and morphological ambi-
guity. Some tokens can admit as many as 15 distinct
readings, and the average number of possible mor-
phological analyses per token in Hebrew text is 2.7,
compared to 1.4 in English (Adler, 2007).
Agreement Hebrew grammar forces morpholog-
ical agreement between Adjectives and Nouns
(which should agree in Gender and Number and def-
initeness), and between Subjects and Verbs (which
should agree in Gender and Number).
2The orthographic form z` is ambiguous. It can also stand
for the noun ?shovel? and the pronoun ?you?(2nd,fem,sing).
104
3 Easy First Non Directional Parsing
Easy-First Non Directional parsing is a greedy
search procedure. It works with a list of partial
structures, pi, . . . , pk, which is initialized with the
n words of the sentence. Each structure is a head
token which is not yet assigned a parent, but may
have dependants attached to it. At each stage of the
parsing algorithm, two neighbouring partial struc-
tures, (pi, pi+1) are chosen, and one of them be-
comes the parent of the other. The new dependant is
then removed from the list of partial structures. Pars-
ing proceeds until only one partial structure, corre-
sponding to the root of the sentence, is left. The
choice of which neighbouring structures to attach is
based on a scoring function. This scoring function
is learned from data, and attempts to attach more
confident attachments before less confident ones.
The scoring function makes use of features. These
features can be extracted from any pre-built struc-
tures. In practice, the features are defined on pre-
built structures which are around the proposed at-
tachment point. For complete details about training,
features and implementation, refer to (Goldberg and
Elhadad, 2010).
4 Experiments
We follow the setup of (Goldberg and Elhadad,
2009).
Data We use the Hebrew dependency treebank de-
scribed in (Goldberg and Elhadad, 2009). We use
Sections 2-12 (sentences 484-5724) as our training
set, and report results on parsing the development
set, Section 1 (sentences 0-483). As in (Goldberg
and Elhadad, 2009), we do not evaluate on the test
set in this work.
The data in the treebank is segmented and POS-
tagged. Both the parsing models were trained on
the gold-standard segmented and tagged data. When
evaluating the parsing models, we perform two sets
of evaluations. The first one is an oracle experi-
ment, assuming gold segmentation and tagging is
available. The second one is a real-world experi-
ment, in which we segment and POS-tag the test-
set sentences using the morphological disambigua-
tor described in (Adler, 2007; Goldberg et al, 2008)
prior to parsing.
Parsers and parsing models We use our freely
available implementation3 of the non-directional
parser.
Evaluation Measure We evaluate the resulting
parses in terms of unlabeled accuracy ? the percent
of correctly identified (child,parent) pairs4. To be
precise, we calculate:
number of correctly identified pairs
number of pairs in gold parse
For the oracle case in which the gold-standard to-
ken segmentation is available for the parser, this is
the same as the traditional unlabeled-accuracy eval-
uation metric. However, in the real-word setting in
which the token segmentation is done automatically,
the yields of the gold-standard and the automatic
parse may differ, and one needs to decide how to
handle the cases in which one or more elements in
the identified (child,parent) pair are not present in
the gold-standard parse. Our evaluation metric pe-
nalizes these cases by regarding them as mistakes.
5 Results
Base Feature Set On the first set of experiments,
we used the English feature set which was used in
(Goldberg and Elhadad, 2010). Our only modifica-
tion to the feature set for Hebrew was not to lex-
icalize prepositions (we found it to work somewhat
better due to the smaller treebank size, and Hebrew?s
rather productive preposition system).
Results of parsing the development set are sum-
marized in Table 1. For comparison, we list the per-
formance of the MALT and MST parsers on the same
data, as reported in (Goldberg and Elhadad, 2009).
The case marker z`, as well as the morpholog-
ically marked construct nouns, are covered by all
feature models. z` is a distinct lexical element in a
predictable position, and all four parsers utilize such
function word information. Construct nouns are dif-
ferentiated from non-construct nouns already at the
POS tagset level.
All models suffer from the absence of gold
POS/morphological information. The easy-first
non-directional parser with the basic feature set
3http://www.cs.bgu.ac.il/?yoavg/software/nondirparser/
4All the results are macro averaged.
105
(NONDIR) outperforms the transition based MALT-
PARSER in all cases. It also outperforms the first or-
der MST1 model when gold POS/morphology infor-
mation is available, and has nearly identical perfor-
mance to MST1 when automatically induced POS/-
morphology information is used.
Additional Morphology Features Error inspec-
tion reveals that most errors are semantic in nature,
and involve coordination, PP-attachment or main-
verb hierarchy. However, some small class of er-
rors reflected morphological disagreement between
nouns and adjectives. These errors were either in-
side a simple NP, or, in some cases, could affect rel-
ative clause attachments. We were thus motivated to
add specific features encoding morphological agree-
ment to try and avoid this class of errors.
Our features are targeted specifically at capturing
noun-adjective morphological agreement.5 When
attempting to score the attachment of two neigh-
bouring structures in the list, pi and pi+1, we in-
spect the pairs (pi, pi+1), (pi, pi+2), (pi?1, pi+1),
(pi?2, pi), (pi+1, pi+2). For each such pair, in case
it is made of a noun and an adjective, we add two
features: a binary feature indicating presence or ab-
sence of gender agreement, and another binary fea-
ture for number agreement.
The last row in Table 1 (NONDIR+MORPH)
presents the parser accuracy with the addition of
these agreement features. Agreement contributes
to the accuracy of the parser, making it as accu-
rate as the second-order MST2. Interestingly, the
non-directional model benefits from the agreement
features also when automatically induced POS/mor-
phology information is used (going from 75.5% to
76.2%). This is in contrast to the MST parsers,
where the morphological features hurt the parser
when non-gold morphology is used (75.6 to 73.9
for MST1 and 76.4 to 74.6 for MST2). This can
be attributed to either the agreement specific na-
ture of the morphological features added to the non-
directional parser, or to the easy-first order of the
non-directional parser, and to the fact the morpho-
logical features are defined only over structurally
close heads at each stage of the parsing process.
5This is in contrast to the morphological features used in
out-of-the-box MST and MALT parsers, which are much more
general.
Gold Morph/POS Auto Morph/POS
MALT 80.3 72.9
MALT+MORPH 80.7 73.4
MST1 83.6 75.6
MST1+MORPH 83.6 73.9
MST2 84.3 76.4
MST2+MORPH 84.4 74.6
NONDIR 83.8 75.5
NONDIR+MORPH 84.2 76.2
Table 1: Unlabeled dependency accuracy of the various
parsing models.
6 Discussion
We have verified that easy-first, non-directional de-
pendency parsing methodology of (Goldberg and El-
hadad, 2010) is successful for parsing Hebrew, a
semitic language with rich morphology and a small
treebank. We further verified that the model can
make effective use of morphological agreement fea-
tures, both when gold-standard and automatically in-
duced morphological information is provided. With
the addition of the morphological agreement fea-
tures, the non-directional model is as effective as a
second-order globally optimized MST model, while
being much more efficient, and easier to extend with
additional structural features.
While we get adequate parsing results for Hebrew
when gold-standard POS/morphology/segmentation
information is used, the parsing performance in
the realistic setting, in which gold POS/morpholo-
gy/segmentation information is not available, is still
low. We strongly believe that parsing and morpho-
logical disambiguation should be done jointly, or
at least interact with each other. This is the main
future direction for dependency parsing of Modern
Hebrew.
References
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev, Beer-Sheva, Israel.
Yoav Goldberg and Michael Elhadad. 2009. Hebrew De-
pendency Parsing: Initial Results. In Proc. of IWPT.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Proc. of NAACL.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
106
EM Can find pretty good HMM POS-Taggers (when
given a good start). In Proc. of ACL.
Sandra Ku?bler, Ryan T. McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures on
Human Language Technologies. Morgan & Claypool
Publishers.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proc of ACL.
Joakim Nivre, Johan Hall, and Jens Nillson. 2006. Malt-
Parser: A data-driven parser-generator for dependency
parsing. In Proc. of LREC.
107
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 234?242,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Inspecting the Structural Biases of Dependency Parsing Algorithms ?
Yoav Goldberg and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
yoavg|elhadad@cs.bgu.ac.il
Abstract
We propose the notion of a structural bias
inherent in a parsing system with respect
to the language it is aiming to parse. This
structural bias characterizes the behaviour
of a parsing system in terms of structures
it tends to under- and over- produce. We
propose a Boosting-based method for un-
covering some of the structural bias inher-
ent in parsing systems. We then apply
our method to four English dependency
parsers (an Arc-Eager and Arc-Standard
transition-based parsers, and first- and
second-order graph-based parsers). We
show that all four parsers are biased with
respect to the kind of annotation they are
trained to parse. We present a detailed
analysis of the biases that highlights spe-
cific differences and commonalities be-
tween the parsing systems, and improves
our understanding of their strengths and
weaknesses.
1 Introduction
Dependency Parsing, the task of inferring a depen-
dency structure over an input sentence, has gained
a lot of research attention in the last couple of
years, due in part to to the two CoNLL shared
tasks (Nivre et al, 2007; Buchholz and Marsi,
2006) in which various dependency parsing algo-
rithms were compared on various data sets. As a
result of this research effort, we have a choice of
several robust, efficient and accurate parsing algo-
rithms.
?We would like to thank Reut Tsarfaty for comments and
discussions that helped us improve this paper. This work is
supported in part by the Lynn and William Frankel Center for
Computer Science.
These different parsing systems achieve com-
parable scores, yet produce qualitatively different
parses. Sagae and Lavie (2006) demonstrated that
a simple combination scheme of the outputs of dif-
ferent parsers can obtain substantially improved
accuracies. Nivre and McDonald (2008) explore
a parser stacking approach in which the output of
one parser is fed as an input to a different kind of
parser. The stacking approach also produces more
accurate parses.
However, while we know how to produce accu-
rate parsers and how to blend and stack their out-
puts, little effort was directed toward understand-
ing the behavior of different parsing systems in
terms of structures they produce and errors they
make. Question such as which linguistic phenom-
ena are hard for parser Y? and what kinds of er-
rors are common for parser Z?, as well as the more
ambitious which parsing approach is most suitable
to parse language X?, remain largely unanswered.
The current work aims to fill this gap by propos-
ing a methodology to identify systematic biases in
various parsing models and proposing and initial
analysis of such biases.
McDonald and Nivre (2007) analyze the dif-
ference between graph-based and transition-based
parsers (specifically the MALT and MST parsers)
by comparing the different kinds of errors made by
both parsers. They focus on single edge errors, and
learn that MST is better for longer dependency
arcs while MALT is better on short dependency
arcs, that MALT is better than MST in predict-
ing edges further from the root and vice-versa, that
MALT has a slight advantage when predicting the
parents of nouns and pronouns, and that MST is
better at all other word categories. They also con-
clude that the greedy MALT Parser suffer from er-
ror propagation more than the globally optimized
234
MST Parser.
In what follows, we complement their work by
suggesting a different methodology of analysis of
parsers behaviour. Our methodology is based on
the notion of structural bias of parsers, further ex-
plained in Section 2. Instead of comparing two
parsing systems in terms of the errors they pro-
duce, our analysis compares the output of a pars-
ing system with a collection of gold-parsed trees,
and searches for common structures which are pre-
dicted by the parser more often than they appear in
the gold-trees or vice-versa. These kinds of struc-
tures represent the bias of the parsing systems, and
by analyzing them we can gain important insights
into the strengths, weaknesses and inner working
of the parser.
In Section 2.2 we propose a Boosting-based
algorithm for uncovering these structural biases.
Then, in Section 3 we go on to apply our analysis
methodology to four parsing systems for English:
two transition-based systems and two graph-based
systems (Sections 4 and 5). The analysis shows
that the different parsing systems indeed possess
different biases. Furthermore, the analysis high-
lights the differences and commonalities among
the different parsers, and sheds some more light
on the specific behaviours of each system.
Recent work by Dickinson (2010), published
concurrently with this one, aims to identify depen-
dency errors in automatically parsed corpora by
inspecting grammatical rules which appear in the
automatically parsed corpora and do not fit well
with the grammar learned from a manually anno-
tated treebank. While Dickinson?s main concern is
with automatic identification of errors rather than
characterizing parsers behaviour, we feel that his
work shares many intuitions with this one: auto-
matic parsers fail in predictable ways, those ways
can be analyzed, and this analysis should be car-
ried out on structures which are larger than single
edges, and by inspecting trends rather than indi-
vidual decisions.
2 Structural Bias
Language is a highly structured phenomena, and
sentences exhibit structure on many levels. For
example, in English sentences adjectives appear
before nouns, subjects tend to appear before their
verb, and syntactic trees show a tendency toward
right-branching structures.1
1As noted by (Owen Rambow, 2010), there is little sense
in talking about the structure of a language without referring
Different combinations of languages and anno-
tation strategies exhibit different structural prefer-
ences: under a specific combination of language
and annotation strategy some structures are more
frequent than others, some structures are illegal
and some are very rare.
We argue that parsers also exhibit such struc-
tural preferences in the parses they produce. These
preferences stem from various parser design deci-
sions. Some of the preferences, such as projectiv-
ity, are due to explicit design decisions and lie at
the core of some parsing algorithms. Other pref-
erences are more implicit, and are due to specific
interactions between the parsing mechanism, the
feature function, the statistical mechanism and the
training data.
Ideally, we would like the structural preferences
of a parser trained on a given sample to reflect the
general preferences of the language. However, as
we demonstrate in Section 3, that it is usually not
the case.
We propose the notion of structural bias for
quantifying the differences in structural prefer-
ences between a parsing system and the language
it is aiming to parse. The structural bias of a
parser with respect to a language is composed of
the structures that tend to occur more often in the
parser?s output than in the language, and vice-
versa.
Structural biases are related to but different than
common errors. Parser X makes many PP at-
tachment errors is a claim about a common error.
Parser X tends to produce low attachment for PPs
while the language tends to have high attachment
is a claim about structural bias, which is related to
parser errors. Parser X can never produce struc-
ture Y is a claim about a structural preference of
a parser, which may or may not be related to its
error patterns.
Structural bias is a vast and vague concept. In
order to give a more concrete definition, we pose
the following question:
Assuming we are given two parses of the same
sentence. Can we tell, by looking at the parses and
without knowing the correct parse, which parser
produced which parse?
Any predictor which can help in answering this
question is an indicator of a structural bias.
to a specific annotation scheme. In what follow, we assume a
fixed annotation strategy is chosen.
235
Definition: structural bias between sets of trees
Given two sets of parse trees, A and B, over the
same sentences, a structural bias between these
sets is the collection of all predictors which can
help us decide, for a tree t, whether it belongs to
A or to B.
The structural bias between a parsing system
and an annotated corpus is then the structural bias
between the corpus and the output of the parser
on the sentences in the corpus. Note that this
definition adheres to the error vs. bias distinction
given above.
Under this task-based definition, uncovering
structural biases between two sets of trees amounts
to finding good predictors for discriminating be-
tween parses coming from these two sets of trees.
In what follows, we present a rich class of struc-
tural predictors, and an algorithm for efficiently
searching this predictor class for good predictors.
2.1 Representing Structure
A dependency representation of sentences in-
cludes words and dependency relations between
them (one word is the ROOT of the sentence, and
each other word has a single word as its parent).
Whenever possible, we would like to equate words
with their part-of-speech tags, to facilitate gener-
alization. However, in some cases the exact iden-
tity of the word may be of interest. When ana-
lyzing a language with a relatively fixed word or-
der, such as English, we are also interested in the
linear order between words. This includes the di-
rection between a parent and its dependent (does
the parent appear before or after the dependent in
the sentence?), as well as the order among several
dependents of the same parent. The length of a de-
pendency relation (distance in words between the
parent and dependent) may also be structurally in-
teresting.2
In order to capture this kind of information, we
take a structural element of a dependency tree to
be any connected subtree, coupled with informa-
tion about the incoming edge to the root of the
subtree. Examples of such structural elements are
given in Figure 1. This class of predictors is not
complete ? it does not directly encode, for in-
stance, information about the number of siblings
2Relations can also be labeled, and labeling fit naturally
in our representation. However, we find the commonly used
set of edge labels for English to be lacking, and didn?t include
edge labels in the current analysis.
(a) JJ
3
(b) NN VB IN/with
2
Figure 1: Structural Elements Examples. (a) is an adjective
with a parent 3 words to its right. (b) is a verb whose parent
is on the left, it has a noun dependent on its left, and a prepo-
sition dependent 2 words to its right. The lexical item of the
preposition is with. The lexical items and distance to parent
are optional, while all other information is required. There
is also no information about other dependents a given word
may have.
a node has or the location of the structure relative
to the root of the tree. However, we feel it does
capture a good deal of linguistic phenomena, and
provide a fine balance between expressiveness and
tractability.
The class of predictors we consider is the set of
all structural elements. We seek to find structural
elements which appear in many trees of set A but
in few trees of set B, or vice versa.
2.2 Boosting Algorithm with Subtree
Features
The number of possible predictors is exponential
in the size of each tree, and an exhaustive search is
impractical. Instead, we solve the search problem
using a Boosting algorithm for tree classification
using subtree features. The details of the algo-
rithm and its efficient implementation are given in
(Kudo and Matsumoto, 2004). We briefly describe
the main idea behind the algorithm.
The Boosting algorithm with subtree features
gets as input two parse sets with labeled, ordered
trees. The output of the algorithm is a set of sub-
trees ti and their weights wi. These weighted sub-
trees define a linear classifier over trees f(T ) =
?
ti?T
wi, where f(T ) > 0 for trees in set A and
f(T ) < 0 for trees in set B.
The algorithm works in rounds. Initially, all
input trees are given a uniform weight. At each
round, the algorithm seeks a subtree t with a max-
imum gain, that is the subtree that classifies cor-
rectly the subset of trees with the highest cumu-
lative weight. Then, it re-weights the input trees,
so that misclassified trees get higher weights. It
continues to repeatedly seek maximum gain sub-
trees, taking into account the tree weights in the
gain calculation, and re-weighting the trees after
each iteration. The same subtree can be selected
in different iterations.
Kudo and Matsumoto (2004) present an effec-
236
(a) JJ?
d:3
(b) VB?
NN? IN?
w:with d:2
Figure 2: Encoding Structural Elements as Ordered Trees.
These are the tree encodings of the structural elements in Fig-
ure 1. Direction to parent is encoded in the node name, while
the optional lexical item and distance to parent are encoded
as daughters.
tive branch-and-bound technique for efficiently
searching for the maximum gain tree at each
round. The reader is referred to their paper for the
details.
Structural elements as subtrees The boosting
algorithm works on labeled, ordered trees. Such
trees are different than dependency trees in that
they contain information about nodes, but not
about edges. We use a simple transformation to
encode dependency trees and structural elements
as labeled, ordered trees. The transformation
works by concatenating the edge-to-parent infor-
mation to the node?s label for mandatory informa-
tion, and adding edge-to-parent information as a
special child node for optional information. Figure
2 presents the tree-encoded versions of the struc-
tural elements in Figure 1. We treat the direction-
to-parent and POS tag as required information,
while the distance to parent and lexical item are
optional.
2.3 Structural Bias Predictors
The output of the boosting algorithm is a set of
weighted subtrees. These subtrees are good can-
didates for structural bias predictors. However,
some of the subtrees may be a result of over-fitting
the training data, while the weights are tuned to
be used as part of a linear classifier. In our ap-
plication, we disregard the boosting weights, and
instead rank the predictors based on their number
of occurrences in a validation set. We seek predic-
tors which appear many times in one tree-set but
few times in the other tree-set on both the train-
ing and the validation sets. Manual inspection of
these predictors highlights the structural bias be-
tween the two sets. We demonstrate such an anal-
ysis for several English dependency parsers below.
In addition, the precision of the learned Boost-
ing classifier on the validation set can serve as a
metric for measuring the amount of structural bias
between two sets of parses. A high classification
accuracy means more structural bias between the
two sets, while an accuracy of 50% or lower means
that, at least under our class of predictors, the sets
are structurally indistinguishable.
3 Biases in Dependency Parsers
3.1 Experimental Setup
In what follows, we analyze and compare the
structural biases of 4 parsers, with respect to a de-
pendency representation of English.
Syntactic representation The dependency tree-
bank we use is a conversion of the English WSJ
treebank (Marcus et al, 1993) to dependency
structure using the procedure described in (Jo-
hansson and Nugues, 2007). We use the Mel?c?uk
encoding of coordination structure, in which the
first conjunct is the head of the coordination struc-
ture, the coordinating conjunction depends on the
head, and the second conjunct depend on the coor-
dinating conjunction (Johansson, 2008).
Data Sections 15-18 were used for training the
parsers3. The first 4,000 sentences from sections
10-11 were used to train the Boosting algorithm
and find structural predictors candidates. Sec-
tions 4-7 were used as a validation set for ranking
the structural predictors. In all experiments, we
used the gold-standard POS tags. We binned the
distance-to-parent values to 1,2,3,4-5,6-8 and 9+.
Parsers For graph-based parsers, we used
the projective first-order (MST1) and second-
order (MST2) variants of the freely available
MST parser4 (McDonald et al, 2005; McDon-
ald and Pereira, 2006). For the transition-based
parsers, we used the arc-eager (ARCE) variant of
the freely available MALT parser5 (Nivre et al,
2006), and our own implementation of an arc-
standard parser (ARCS) as described in (Huang et
al., 2009). The unlabeled attachment accuracies of
the four parsers are presented in Table 1.
Procedure For each parser, we train a boosting
classifier to distinguish between the gold-standard
trees and the parses produced for them by the
3Most work on parsing English uses a much larger train-
ing set. We chose to use a smaller set for convenience. Train-
ing the parsers is much faster, and we can get ample test data
without resorting to jackknifing techniques. As can be seen
in Table 1, the resulting parsers are still accurate.
4http://sourceforge.net/projects/mstparser/
5http://maltparser.org/
237
MST1 MST2 ARCE ARCS
88.8 89.8 87.6 87.4
Table 1: Unlabeled accuracies of the analyzed parsers
Parser Train Accuracy Val Accuracy
MST1 65.4 57.8
MST2 62.8 56.6
ARCE 69.2 65.3
ARCS 65.1 60.1
Table 2: Distinguishing parser output from gold-trees based
on structural information
parser. We remove from the training and valida-
tion sets all the sentences which the parser got
100% correct. We then apply the models to the
validation set. We rank the learned predictors
based on their appearances in gold- and parser-
produced trees in the train and validation sets, and
inspect the highest ranking predictors.
Training the boosting algorithm was done us-
ing the bact6 toolkit. We ran 400 iterations of
boosting, resulting in between 100 and 250 dis-
tinct subtrees in each model. Of these, the top 40
to 60 ranked subtrees in each model were good in-
dicators of structural bias. Our wrapping code is
available online7 in order to ease the application
of the method to other parsers and languages.
3.2 Quantitative Analysis
We begin by comparing the accuracies of the
boosting models trained to distinguish the pars-
ing results of the various parsers from the English
treebank. Table 2 lists the accuracies on both the
training and validation sets.
The boosting method is effective in finding
structural predictors. All parsers output is dis-
tinguishable from English trees based on struc-
tural information alone. The ArcEager variant of
MALT is the most biased with respect to English.
The transition-based parsers are more structurally
biased than the graph-based ones.
We now turn to analyze the specific structural
biases of the parsing systems. For each system
we present some prominent structures which are
under-produced by the system (these structures
appear in the language more often than they are
produce by the parser) and some structures which
are over-produced by the system (these structures
6http://chasen.org/?taku/software/bact/
7http://www.cs.bgu.ac.il/?yoavg/software/
are produced by the parser more often than they
appear in the language).8 Specifically, we manu-
ally inspected the predictors where the ratio be-
tween language and parser was high, ranked by
absolute number of occurrences.
4 Transition-based Parsers
We analyze two transition-based parsers (Nivre,
2008). The parsers differ in the transition sys-
tems they adopt. The ARCE system makes
use of a transition system with four transitions:
LEFT,RIGHT,SHIFT,REDUCE. The semantics of
this transition system is described in (Nivre,
2004). The ARCS system adopts an alterna-
tive transition system, with three transitions: AT-
TACHL,ATTACHR,SHIFT. The semantics of the
system is described in (Huang et al, 2009). The
main difference between the systems is that the
ARCE system makes attachments as early as pos-
sible, while the ARCS system should not attach a
parent to its dependent until the dependent has ac-
quired all its own dependents.
4.1 Biases of the Arc-Eager System
Over-produced structures The over-produced
structures of ARCE with respect to English are
overwhelmingly dominated by spurious ROOT at-
tachments.
The structures ROOT?? , ROOT?DT,
ROOT?WP are produced almost 300 times by
the parser, yet never appear in the language. The
structures ROOT?? , ROOT?WRB , ROOT?JJ
appear 14 times in the language and are produced
hundreds of time by the parser. Another interest-
ing case is ROOT ??9+ NN , produced 180 times by
the parser and appearing 7 times in the language.
As indicated by the distance marking (9+), nouns
are allowed to be heads of sentences, but then they
usually appear close to the beginning, a fact which
is not captured by the parsing system. Other, less
clear-cut cases, are ROOT as the parent of IN,
NN, NNS or NNP. Such structures do appear in
the language, but are 2-5 times more common in
the parser.
A different ROOT attachment bias is captured
by
ROOT VBZ VBD and ROOT VBD VBD ,
8One can think of over- and under- produced structures
in terms of the precision and recall metrics: over-produced
structures have low precision, while under-produced struc-
tures have low recall.
238
appearing 3 times in the language and produced
over a 100 times by the parser.
It is well known that the ROOT attachment ac-
curacies of transition-based systems is lower than
that of graph-based system. Now we can refine
this observation: the ARCE parsing system fails
to capture the fact that some categories are more
likely to be attached to ROOT than others. It also
fails to capture the constraint that sentences usu-
ally have only one main verb.
Another related class of biases are captured by
the structures?VBD ??9+ VBD,?VBD ??5?7 VBD
and ROOT?VBZ?VBZ which are produced by
the parser twice as many times as they appear
in the language. When confronted with embed-
ded sentences, the parser has a strong tendency of
marking the first verb as the head of the second
one.
The pattern ??+9 IN suggests that the parser
prefers high attachment for PPs. The pattern
DT?NN
9+
captures the bias of the parser
toward associating NPs with the preceding verb
rather than the next one, even if this preceding verb
is far away.
Under-produced structures We now turn to
ARCE?s under-produced structures. These include
the structures IN/that? , MD? , VBD? (each 4
times more frequent in the language than in the
parser) and VBP? (twice more frequent in the
language). MD and that usually have their par-
ents to the left. However, in some constructions
this is not the case, and the parser has a hard time
learning these constructions.
The structure ?$?RB appearing 20 times in
the language and 4 times in the parser, reflects a
very specific construction (?$ 1.5 up from $ 1.2?).
These constructions pop up as under-produced by
all the parsers we analyze.
The structures ??1 RB?IN and ?RB?JJ ap-
pear twice as often in the language. These
stem from constructions such as ?not/RB unex-
pected/JJ?, ?backed away/RB from/IN?, ?pushed
back/RB in/IN?, and are hard for the parser.
Lastly, the structure JJ?NN?NNS?, deviates
from the the ?standard? NP construction, and is
somewhat hard for the parser (39 times parser, 67
in language). However, we will see below that this
same construction is even harder for other parsers.
4.2 Biases of the Arc-Standard System
Over-produced structures The over-produced
structures of ARCS do not show the spurious
ROOT attachment ambiguity of ARCE. They do
include ROOT?IN, appearing twice as often in
the parser output than in the language.
The patterns ROOT?VBZ??9+, , ?VBP??9+,
, ?VBD??9+VBD and ?VB?VBD all reflect
the parser?s tendency for right-branching struc-
ture, and its inability to capture the verb-hierarchy
in the sentence correctly, with a clear preference
for earlier verbs as parents of later verbs.
Similarly, ??9+NNP and ??9+NNS indicate a ten-
dency to attach NPs to a parent on their left (as an
object) rather than to their right (as a subject) even
when the left candidate-parent is far away.
Finally, WRB MD VB , produced
48 times by the parser and twice by the language,
is the projective parser?s way of annotating the
correct non-projective structure in which the wh-
adverb is dependent on the verb.
Under-produced structures of ARCS in-
clude two structures WRB VBN and
WRB VB , which are usually part of
non-projective structures, and are thus almost
never produced by the projective parser.
Other under-produced structures include appos-
itive NPs:
? IN NN ? ?
(e.g., ?by Merill , the nation?s largest firm , ?), and
the structure NN DT NN , which can
stand for apposition (?a journalist, the first jour-
nalist to . . . ?) or phrases such as ?30 %/NN a
month?.
TO usually has its parent on its left. When this
is not the case (when it is a part of a quantifier,
such as ?x to y %?, or due to fronting: ?Due to
X, we did Y?), the parser is having a hard time to
adapt and is under-producing this structure.
Similar to the other parsers, ARCS also under-
produces NPs with the structure JJ?NN??1 NN,
and the structure?$?RB.
Finally, the parser under-produces the con-
junctive structures ?NN?CC?NN?IN and
?IN?CC?IN.
239
5 Graph-based Parsers
We analyze the behaviour of two graph-based
parsers (McDonald, 2006). Both parsers perform
exhaustive search over all projective parse trees,
using a dynamic programming algorithm. They
differ in the factorizations they employ to make
the search tractable. The first-order model em-
ploys a single-edge factorization, in which each
edge is scored independently of all other edges.
The second-order model employs a two-edge fac-
torization, in which scores are assigned to pairs
of adjacent edges rather than to a single edge at a
time.
5.1 Biases of First-order MST Parser
Over-produced structures of MST1 include:
? IN NN NN ? IN NNP NN
? IN NNP NNS ? IN NN VBZ/D
where the parsers fails to capture the fact
that prepositions only have one dependent.
Similarly, in the pattern: ?CC NN NNS
the parser fails to capture that only one phrase
should attach to the coordinator, and the patterns
NN NN VBZ NNS NNS VBP
highlight the parser?s failing to capture that
verbs have only one object.
In the structure ROOT WRB VBD , pro-
duced by the parser 15 times more than it appears
in the language, the parser fails to capture the fact
that verbs modified by wh-adverbs are not likely
to head a sentence.
All of these over-produced structures are fine
examples of cases where MST1 fails due to its
edge-factorization assumption.
We now turn to analyzing the structures under-
produced by MST1.
Under-produced structures The non-
projective structures
WRB VBN
1
and WRB VB
1
clearly cannot be produced by the projective
parser, yet they appear over 100 times in the
language.
The structure WRB?VBD?VBD which is
represented in the language five times more than
in the parser, complements the over-produced case
in which a verb modified by a wh-adverb heads the
sentence.
IN/that?, which was under-produced by
ARCE is under-produced here also, but less so
than in ARCE. ?$?RB is also under-produced
by the parser.
The structure CC??1 , usually due to conjunc-
tions such as either, nor, but is produced 29 times
by the parser and appear 54 times in the language.
An interesting under-produced structure is
?NN IN CC NN . This structure reflects
the fact that the parser is having a hard time coor-
dinating ?heavy? NPs, where the head nouns are
modified by PPs. This bias is probably a result
of the ?in-between pos-tag? feature, which lists
all the pos-tags between the head and dependent.
This feature was shown to be important to the
parser?s overall performance, but probably fails it
in this case.
The construction ??6?8JJ, where the adjective
functions as an adverb (e.g., ?he survived X
unscathed? or ?to impose Y corporate-wise?)
is also under-produced by the parser, as well
as IN NN in which the preposition
functions as a determiner/quantifier (?at least?,
?between?, ?more than?).
Finally, MST1 is under-producing NPs with
somewhat ?irregular? structures: JJ?NN?NNS
or JJ?NN?NNS (?common stock purchase war-
rants?, ?cardiac bypass patients?), or JJ?JJ? (?a
good many short-sellers?, ?West German insur-
ance giant?)
5.2 Biases of Second-order MST Parser
Over-produced structures by MST2 are differ-
ent than those of MST1. The less-extreme edge
factorization of the second-order parser success-
fully prevents the structures where a verb has two
objects or a preposition has two dependents.
One over-produced structure,
NNS JJ NNP ? ?
, produced
10 times by the parser and never in the language,
is due to one very specific construction, ?bonds
due Nov 30 , 1992 ,? where the second comma
should attach higher up the tree.
240
Another over-produced structure involves
the internal structure of proper names:
NNP NNP NNP NNP (the ?correct? analysis
more often makes the last NNP head of all the
others).
More interesting are: ??1 CC?VBD and
??1 CC?NN?IN . These capture the parser?s in-
ability to capture the symmetry of coordinating
conjunctions.
Under-produced structures of MST2 are over-
all very similar to the under-produced structures of
MST1.
The structure CC??1 which is under-produced by
MST1 is no longer under-produced by MST2. All
the other under-produced structures of MST1 reap-
pear here as well.
In addition, MST2 under-produces the struc-
tures ROOT?NNP?. (it tends not to trust NNPs
as the head of sentences) and??6?8TO??1 V B (where
the parser is having trouble attaching TO correctly
to its parent when they are separated by a lot of
sentential material).
6 Discussion
We showed that each of the four parsing systems
is structurally biased with respect to the English
training corpus in a noticeable way: we were able
to learn a classifier that can tell, based on structural
evidence, if a parse came from a parsing system
or from the training corpus, with various success
rates. More importantly, the classifier?s models are
interpretable. By analyzing the predictors induced
by the classifier for each parsing system, we un-
covered some of the biases of these systems.
Some of these biases (e.g., that transition-based
system have lower ROOT-attachment accuracies)
were already known. Yet, our analysis refines this
knowledge and demonstrates that in the Arc-Eager
system a large part of this inaccuracy is not due
to finding the incorrect root among valid ambigu-
ous candidates, but rather due to many illegal root
attachments, or due to illegal structures where a
sentence is analyzed to have two main verbs. In
contrast, the Arc-Standard system does not share
this spurious root attachment behaviour, and its
low root accuracies are due to incorrectly choos-
ing among the valid candidates. A related bias of
the Arc-Standard system is its tendency to choose
earlier appearing verbs as parents of later occur-
ring verbs.
Some constructions were hard for all the parsing
models. For example, While not discussed in the
analysis above, all parsers had biased structures
containing discourse level punctuation elements
(some commas, quotes and dashes) ? we strongly
believe parsing systems could benefit from special
treatment of such markers.
The NP construction (JJ?NN?NNS?) ap-
peared in the analyses of all the parsers, yet were
easier for the transition-based parsers than for the
graph-based ones. Other NP constructions (dis-
cussed above) were hard only for the graph-based
parsers.
One specific construction involving the dollar
sign and an adverb appeared in all the parsers,
and may deserve a special treatment. Simi-
larly, different parsers have different ?soft spots?
(e.g., ?backed away from?, ?not unexpected? for
ARCE, ?at least? for MST1, TO? for ARCS, etc.)
which may also benefit from special treatments.
It is well known that the first-order edge-
factorization of the MST1 parser is too strong.
Our analysis reveals some specific cases where
this assumptions indeed breaks down. These
cases do not appear in the second-order factoriza-
tion. Yet we show that the second-order model
under-produces the same structures as the first-
order model, and that both models have specific
problems in dealing with coordination structures,
specifically coordination of NPs containing PPs.
We hypothesize that this bias is due to the ?pos-in-
between? features used in the MST Parser.
Regarding coordination, the analysis reveals
that different parsers show different biases with re-
spect to coordination structures.
7 Conclusions and Future Work
We presented the notion of structural bias ? spe-
cific structures that are systematically over- or
under- represented in one set of parse trees relative
to another set of parse trees ? and argue that differ-
ent parsing systems exhibit different structural bi-
ases in the parses they produced due to various ex-
plicit and implicit decisions in parser design. We
presented a method for uncovering some of this
structural bias, and effectively used it to demon-
strate that parsers are indeed biased with respect
to the corpus they are trained on, and that differ-
ent parsers show different biases. We then ana-
lyzed the biases of four dependency parsing sys-
tems with respect to an English treebank. We ar-
241
gue that by studying the structural biases of pars-
ing systems we can gain a better understanding on
where dependency parsers fail, and how they dif-
fer from each other. This understanding can in turn
lead us toward designing better parsing systems.
We feel that the current study is just the tip of
the iceberg with respect to the analysis of struc-
tural bias. Any parsing system for any language
and annotation scheme can benefit from such anal-
ysis.
References
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proc. of CoNLL.
Markus Dickinson. 2010. Detecting errors in
automatically-parsed dependency relations. In Proc.
of ACL.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proc of EMNLP.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for en-
glish. In Proc of NODALIDA.
Richard Johansson. 2008. Dependency-based Seman-
tic Analysis of Natural-language Text. Ph.D. thesis,
Lund University.
Taku Kudo and Yuji Matsumoto. 2004. A Boost-
ing Algorithm for Classification of Semi-Structured
Text. In Proceedings of EMNLP.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marchinkiewicz. 1993. Building a large annotated
corpus of English: The penn treebank. Computa-
tional Linguistics, 19:313?330.
Ryan McDonald and Joakim Nivre. 2007. Character-
izing the errors of data-driven dependency parsing
models. In Proc. of EMNLP.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proc of EACL.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proc of ACL.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of ACL, pages 950?958.
Joakim Nivre, Johan Hall, and Jens Nillson. 2006.
MaltParser: A data-driven parser-generator for de-
pendency parsing. In Proc. of LREC.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proc. of EMNLP-CoNLL.
Joakim Nivre. 2004. Incrementality in determinis-
tic dependency parsing. In Incremental Parsing:
Bringing Engineering and Cognition Together, ACL-
Workshop.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4), December.
Owen Rambow. 2010. The Simple Truth about De-
pendency and Phrase Structure Representations: An
Opinion Piece. In Proceedings of NAACL.
Kenji Sagae and Alon Lavie. 2006. Parser combina-
tion by reparsing. In Proceedings of HLT-NAACL,
pages 129?133.
242
Proceedings of the 2012 Student Research Workshop, pages 43?48,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Domain Adaptation of a Dependency Parser with a Class-Class Selectional Preference Model  
 Abstract When porting parsers to a new domain, many of the errors are related to wrong attachment of out-of-vocabulary words. Since there is no available annotated data to learn the attachment preferences of the target domain words, we attack this problem using a model of selectional preferences based on domain-specific word classes. Our method uses Latent Dirichlet Allocations (LDA) to learn a domain-specific Selectional Preference model in the target domain using un-annotated data. The model provides features that model the affinities among pairs of words in the domain.  To incorporate these new features in the parsing model, we adopt the co-training approach and retrain the parser with the selectional preferences features. We apply this method for adapting Easy First, a fast non-directional parser trained on WSJ, to the biomedical domain (Genia Treebank). The Selectional Preference features reduce error by 4.5% over the co-training baseline. 1 Introduction Dependency parsing captures a useful representation of syntactic structure for information extraction. For example, the Stanford Dependency representation has been used extensively in domain-specific relation extraction tasks such as BioNLP09 (Kim, Ohta et al 2009) and BioNLP11 (Pyysalo, Ohta et al 2011). One obstacle to widespread adoption of such syntactic representations is that parsers are generally trained on a specific domain (typically WSJ news data) and it has often been observed that the accuracy of dependency parsers drops significantly when used in a domain other than the training domain.  
Domain adaptation for dependency parsing has been explored extensively in the CoNLL 2007 Shared Task (Nivre, Hall et al 2007). The objective in this task is to adapt an existing parser from a source domain in order to achieve high parsing accuracy on a target domain in which no annotated data is available. Common approaches include self-training (McClosky, Charniak et al 2006), using word distribution features (Koo, Carreras et al 2008) and co-training (Sagae and Tsujii 2007) . Dredze et al (Dredze, Blitzer et al 2007) explored a variety of methods for domain adaptation, which consistently showed little improvement and concluded that domain adaptation for dependency parsing is indeed a hard task. Typically, parsing accuracy drops from 90+% in-domain to 80-84% in the target domain. When porting parsers to the target domain, many of the errors are related to wrong attachment of out-of-vocabulary words, i.e., words which were not observed when training on the source domain. Since there is not sufficient annotated data to learn the attachment preferences of the target domain words, we attack this problem using a model of selectional preferences based on domain-specific word classes.  Selectional preferences (SP) describe the relative affinity of arguments and head of a syntactic relation. For example, in the sentence: ?D3 activates receptors in blood cells from patients?, the preposition ?from? may be attached to either ?cells? or ?receptors?. However, the head word ?cells? has greater affinity to ?patients? than the candidate ?receptors? would have towards "patients". Note that this preference is highly context-specific. Several methods for learning SP (not in the context of domain adaptation) have been proposed. Commonly, these methods rely on learning semantic classes for arguments and learning the preference of a predicate to a semantic class. These semantic classes may be derived from manual knowledge bases such as WordNet or FrameNet, or semantic classes learned from large corpora. Recently, Ritter et al (2010) and 
Raphael Cohen* Yoav Goldberg** Michael Elhadad Ben Gurion University of the Negev Department of Computer Science POB 653 Be?er Sheva, 84105, Israel {cohenrap,yoavg,elhadad}@cs.bgu.ac.il 
?Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University **Current affiliation: Google Inc. 
43
S?aghdha (2010) both present induction methods of SP of verb-arguments using LDA (Blei, Ng et al 2003). Hartung and Frank (2011) extended the LDA-based approach to learning preference for adjective-noun phrases.  In this work, we tackle the task of domain adaptation by developing a domain-specific SP model. Our initial observation is that parsers fail on the target domain when trying to attach domain-specific words not seen during training. We observe as many as 15% of the words are unknown when applying a WSJ-trained parser on Genia and PennBioIE data, compared to only 2.5% in-domain. Parsers trained on the source domain cannot learn attachment preferences for such words. Our motivation is, therefore, to attempt to learn attachment preferences for domain specific words using un-annotated data. Specifically, we focus on acquiring a domain-specific SP model.  Our approach consists of using the low-accuracy source-domain parser on large quantities of in-domain sentences. We extract from the resulting parse trees a collection of syntactically related pairs of words. We then train an LDA model over these pairs of words and derive a domain-specific model of lexical affinities between pairs of words.  We finally re-train a parser model to exploit this domain-specific data.  To this end, we use the approach of co-training, which consists of identifying reliable parse trees in the target domain in an unsupervised manner using an ensemble of two distinct parsers, and extending the annotated training set with these reliable parse trees. Co-training alone significantly reduces the proportion of unknown words in the re-trained parser ? in the extended co-training dataset, we observe that the unknown words rate drops from 15% to 4.5%. Data sparseness, however, remains an issue: 1/3 of the domain-specific words added to the model by co-training appear only once in the extended training set, and we observe that many of the attachment errors are concentrated in a few syntactic configurations (e.g., head(V or N)-prep-pobj, N-N or head(N)-Adj).  We extend co-training by introducing our SP model, which is class-based and specific to these difficult syntactic configurations. Our method reduces error in the Genia Treebank (Tateisi, Yakushiji et al 2005) by 3.5% over co-training. Introducing additional distributional lexical features (Brown clusters learned in-domain), further reduces error to a total 4.5% reduction. Overall, our parser achieves an accuracy of 83.6% UAS on the Genia domain without annotated data in this domain. 
2 Our Approach To understand the difficulty of domain adaptation, we applied our parser trained on the WSJ news domain to the Genia and measured observed errors.  Most of the errors were found in a small set of syntactic configurations: verb-prep-noun, noun-adjective, noun-noun (together these relations make up 32 % of the errors).  For example: in ?nuclear factor-kappa-B DNA-binding activity? the parser chooses ?factor-kappa-B? as the head of ?nuclear? instead of ?activity?. We observe that these errors involve domain-specific vocabulary, and are difficult to disambiguate for non-expert humans as well. Accordingly, we try to acquire a domain-specific model of word-pairs affinities.  Our parsing model (EasyFirst) allows us to use such bi-lexical features in an efficient manner.  Because of data sparseness, however, we aim to acquire class-based features, and decide to model these lexical preferences using the LDA approach. Our method proceeds in two stages: 1. Learn selectional preferences from an automatically parsed corpus using LDA on selected syntactic configurations 2. Integrate the preferences into the parsing model as new features using co-training. 2.1 Learning Selectional Preferences Following (Ritter, Mausam et al 2010) and (S?aghdha 2010), we model lexical affinity between words in specific syntactic configurations using LDA.  Traditionally, LDA learns a set of "topics" from observed documents, based on observed word co-occurrences. In our case, we form artificial documents, which we call syntactic contexts, by collecting head-daughter pairs from parse trees. A syntactic context is constructed for each head word, which contains the related words to which it was found attached.  In the collection process, we identify two syntactic configurations that yield high error rates: head-prep-noun and noun-adj. We collect two types of syntactic contexts: the preposition contexts contain the set of nouns related to the head through any preposition and the adjective contexts contain the set of adjectives directly related to the head noun. We then learn an LDA model on each of these contexts collections.  We use Mallet (McCallum 2002) to learn topic models with hyper-parameter optimization(Wallach, Mimno et al 2009). The optimal number of topics is selected empirically based on model fit to held-out data. 
44
The resulting topics represent latent semantic classes of the daughter words. We define a measure of shared affinity between a head word h and a candidate daughter word d (in a given configuration) s: ??????? ?, ? =  ? ?(?|?) ? ?(?|?) ? ??? ? ???  where P(c|h) is the predicted probability of topic c given the syntactic context associated to head word h. That is, when we apply the LDA model on the syntactic context of h, we assign topics to each of the associated daughter words and count their proportion. Note that this affinity measure may predict a non-zero affinity to a pair (h, d) even though this word pair has never been observed. The result is a class-class SP model with reduced dimensionality compared to word-word models based for example on PMI.  Table 1 lists examples of learned topics. Note that these topics are high-quality semantic clusters that reflect domain semantics, with marked differences between the news and bio-medical domains. 2.2  Co-training to exploit domain features At this stage, we have acquired a domain-specific model of word affinity that exploits semantic classes and depends on specific syntactic configurations (head-prep-obj and noun-adj).  We now attempt to exploit this model to adapt our source parser to the target domain.  To this end, we want to re-train the parser using new features based on the SP model in addition to the original features.  We use the framework of co-training to achieve this goal (Sagae 
and Tsujii 2007): we use two different parsers: Easy-First (Goldberg and Elhadad 2010) and MALT (Nivre, Hall et al 2006) trained on the same WSJ source domain. We apply these two parsers on a large set of target-domain sentences. We select those sentences where the 2 parsers agree (produce identical trees) and add them to the original source-domain training set. We thus obtain an extended training set with many in-domain samples. We can now re-train the parser using the new SP features. 2.3 SP as features for the Easy First parser We use the deterministic non-directional Easy-First parser for re-training. This parser incrementally adds edges between words starting with the easier decisions before continuing to difficult ones. Simple structures are first created and their information is available when deciding how to connect complex ones. Easy-First operates in ?(?????) time compared to ?(??) of graph-based parsers such as MST (McDonald, Pereira et al 2005). As a baseline we use the features provided in the Easy-First distribution. We extend these features with pair-wise affinity measures based on our SP model. The affinity measure ranges from 0 to 1. We bin this measure into (low, medium, high, very-high) binary features. When attaching a preposition to its parent, we add one more feature: the affinity of the head candidate with the preposition's daughter (the pobj).  In addition to these pair-wise features, we also 
Source Relation Type Semantic Class Arguments Predicates BLLIP Arg?? Prep ?? Predicate Show Business   actors clips soundtrack genre taping characters roles immortalized starred costumes premise screening featured performances poster trumpeted star retrospective clip script  
film show movie films movies shows television series stage theater program production version music hollywood broadway 
BLLIP Arg?? Prep ?? Predicate Sports quarterbacks starters pitcher pitchers quarterback coaching receiver linebackers cornerback outfielder baseman fullback  team game league teams games time field players years baseball year rules nfl seasons level player leagues nba club history school state BLLIP Arg?? Prep ?? Predicate Work Position jockeying groom groomed relegate relieved unwinding jockeyed selected selecting appointing disqualify named  job post position draft positions candidate team one jobs which role posts successor Genia Arg?? Prep ?? Predicate Cell-cycle process stages stage process steps committed block regulator acquire switch points needed directs determinant il-21 proceeds arrest regulators relate d3  
differentiation development activation  maturation cycle hematopoiesis infection commitment lymphopoiesis stage lineage selection erythropoiesis cascade Genia Arg?? Prep ?? Predicate Cells and growing conditions supernatants co-culture co-cultured replication medium surface chemotaxis supernatant beta migration cocultured cultures hyporesponsiveness  
cell monocyte lymphocyte pbmc macrophage line blood neutrophil cd dc leukocyte t eosinophil fibroblast platelet keratinocyte Genia Adjective?? Noun Protein activity and regulation factor-induced tnfalpha-induced agonist-induced thrombin-induced il-2-induced factor-alpha-induced il-1beta-induced cd40-induced rankl-induced augmented il-4-induced  
expression activation production phosphorylation response proliferation activity binding secretion apoptosis differentiation translocation release signaling adhesion synthesis generation Table 1 High affinity classes in the Class-Class Selectional Preferences model extracted with LDA. Classes 1-5 are from preposition head/object pairs (e.g ?groomed for position? fits the third topic) and class 6 are adjective modifier pairs. Classes 1-3 are from Bllip (un-annotated WSJ corpus) (Charniak, Blaheta et al 2000) while classes 4-6 are from a corpus composed of Medline abstracts from the Genia (see section 5.1). Class 4 contains arguments and predicates concerning cell-cycle process. In class 5 arguments are cell growing conditions and predicates are types of cells. 
45
introduce features that correspond to the latent topic class of the words according to each of the 2 acquired LDA models (this introduces one binary feature for each topic).  These latent semantic class features are similar in nature to distributional lexical features as used in (Koo, Carreras et al 2008). The EasyFirst parser combines partial trees bottom-up. When deciding whether to attach the partial tree "from patients" to either "cells" or "receptors", we compute the affinities of "cells/patients" and "receptors/patients". Our model produces features indicating medium affinity for ?receptors from patients? and a high affinity for cells from patients?. 3 Experiments and Evaluation 3.1 Genia Treebank The Genia Treebank (Tateisi, Yakushiji et al 2005) contains 18K sentences from the biomedical domain, transformed into dependency trees 1  using (De Marneffe, MacCartney et al 2006) 2 . The corpus contains 2.3K sentences longer than 40 tokens that were excluded from the evaluation. The treebank was divided into test and development sets of equal size.  We created an un-annotated corpus of 200K sentences by querying Medline with the same query terms used to create Genia. We used the Genia POS Tagger on this dataset (Tsuruoka, Tateishi et al 2005). The corpus was parsed with Easy-First and MALT (arc-eager, polynomial) to create co-training data, yielding 21K sentences with 100% agreement. The parsed corpus of 200K sentences was used to produce selectional preference models for adjective-nouns, with 200 topics, and for head-prep-object with 300 topics. We used word lemmas for each pair when preparing syntactic contexts for LDA training (see Table 2).  Relation #  Pairs # Daughter # Heads preposition 360,041 1,727 2,391 adjective 384,347 1,570 2,003  Table 2. Statistics for the training data of the SP model. 3.2 Coverage Many of the features learned in training a parser are lexicalized; this is an important factor in the drop in accuracy when parsing in a new domain.  To understand the nature of the contribution of the features learned by our SP model, we calculated the coverage of the features acquired in two unsupervised methods: Brown clustering and our SP classes. We                                                       1 We use the PTB version of Genia created by Illes Solt. 2 We convert using the Stanford Parser bundle. 
count the number of tokens in the Treebank which gain a feature at training time (we ignore punctuation, coordination and preposition tokens). Our SP model covers 53% of the tokens in the test set. Brown clusters calculated with the implementation of Liang (2005) achieve coverage of 73%.  Brown clusters features are also class-based distributional features based on n-gram language models, but do not take into account syntactic configurations. 3.3 Adaptation Evaluation We use a number of baselines for the adaptation task. Three parsers were evaluated on the target domain: Easy-First, MST second order and MALT arc-eager with a polynomial kernel. We report UAS scores of trees of length < 40 without punctuation. The first baseline setting for each parser is the model trained on WSJ sections 2-21.  The second baseline we report is co-training using WSJ 2-21 combined with the 21K full agreement parse trees extracted from Medline, but without new features. Parser Training Data Features UAS (Exact Match)  MST WSJ 2-21  79.6 (10)  MALT WSJ 2-21  81.1 (16.6)  Easy-First WSJ 2-21  80.5 (12.3)  MST Co-Training  81.3 (14.1)  MALT Co-Training  82.1 (16.5)  Easy-First Co-Training  82.8 (16.2)  Easy-First Co-Training +Brown Clusters 83.1 (17) +0.3 Easy-First Co-Training +SP-Lexicalized 83.0 (16.9) +0.2 Easy-First Co-Training +SP-Lexicalized +SP-Classes 83.4 (16.6) +0.6 Easy-First Co-Training +SP-Lexicalized +SP-Classes +Brown Clusters 83.6 (17.2) +0.8 Easy-First GeniaTB Dev  89.8 (28.6)  Table 3. Accuracy for different parser settings on Genia test set.  The best performing adapted model trains with co-training data and combines SP and Brown clusters as features.  In Table 3, we see that the combined SP-Features improved the co-training baseline by 0.6%, a significant error reduction of 3.5% (p-value < 0.01).  We list improvement when introducing only pair-wise SP features, and when adding SP-based semantic classes. The effect is also additive with the Brown clusters features, producing an improvement of 0.8% when combined (error reduction of 4.5%). To evaluate the model adapted for Genia on the general biomedical domain, we used the PennBioIE Treebank . This dataset contains 6K sentences from different biomedical domains. We compared 3 models (see Table 4):  1. Easy-First, MALT and MST trained on WSJ. 2. Easy-First with co-training on Genia. 
46
3. Easy-First with co-training on Genia with Selectional Preference features. Domain adaptation to Genia carried over to the closely related PennBioIE dataset, demonstrating the generalization capability of the method. Parser Training Data Features UAS  MALT WSJ 2-21  78.8  MST WSJ 2-21  81.4  Easy-First WSJ 2-21  79.8  Easy-First Co-Training  81.9  Easy-First Co-Training +SP-Lexicalized +SP-Classes +Brown Clusters 82.2 +0.3 Table 4. Accuracy of parsers on PennBioIE Treebank.  3.4 Error Analysis We compare the parser using the SP pair-wise features for preposition attachment to the co-trained baseline on Genia. The overall accuracy of the parser is improved by 0.2%. However, the two models agree only on 90% of the edges, indicating the new SP features play a very active role when parsing. For ?E3330 inhibited this induced promoter activity in a dose-dependent manner?, the co-trained parser chose ?activity? as the head of ?in? instead of ?inhibited?. The affinity feature in our model for (?inhibited?, ?manner?) shows affinity of high (40-60%) compared to low (5-20%) for the wrong pair ("activity", "manner"). The same change occurs for ?LysoPC attenuates activation during inflammation and athero-sclerosis?, where the improved model prefers the pair (?attenuates?, ?inflammation?) to the pair (?activation?, ?inflammation?) which was chosen by the co-trained model. The modest overall improvement is due to errors introduced by the new model. In ?Tissue obtained from ectopic pregnancies may identify the mechanism of trophoblast invasion in ectopic pregnancies?, the correct governor of ?in? is ?invasion?. However, the SP model ranks the affinity of (?invasion?, ?pregnancies?) lower than that of (?mechanism?, ?pregnancies?). Most of the improvement of the full SP model (+0.6%) comes from an improvement in the N-N relation from 83% to 84.9% (11% error reduction), this improvement is due to semantic classes features learned on the relations of noun-adjective and head-prep-pobj. 3.5 Effect on NER Since most of the improvement comes from the N-N relation, we expect improvement for downstream applications such as Named Entity Recognition, a basic task frequently used in the biomedical domain. 
We use the portion of the Genia Treebank covered by the Genia NER corpus (Kim, Ohta et al 2004). We expect the inner tokens of a named entity to be connected by relation of N-N or N-Adj. We evaluate the accuracy of these two relations for NE tokens. The Easy-First with co-training baseline produces accuracy of 82.9% on this specific set of relations, improved by the SP model to 84.4%, a reduction in error of 8.7%. 4 Related Work 4.1 Learning of Selectional Preference Preference of predicate-argument pairs has been studied in depth with a number of approaches. Resnik (1993) suggested a class-based model for preference of predicates combining WordNet classes with mutual information techniques for associating an argument with a predicate class from WordNet.  Another approach models words in a corpus as context vectors (Erk and Pado 2008; Turney and Pantel 2010) for discovering predicate or argument classes using large corpora or the Web. Recently, semantic classes were successfully induced using LDA topic modeling. These methods have shown success in modeling verb argument relationship to a single predicate (Ritter, Mausam et al 2010) or a predicate pair (S?aghdha 2010), as well as for adjective-noun preference (Hartung and Frank 2011).  4.2 Learning SP for improving dependency parsing  The argument-predicate choice learned in SP is directly related to the decision of creating an edge between them in a parse tree. Van Noord (2007) modeled verb-noun preferences using pointwise mutual information (PMI) using an automatically parsed corpus in Dutch. Association scores of pairs were added as features improving the accuracy significantly from 87.4% to 87.9%.  Nakov and Hearst (Nakov and Hearst 2005) focused on resolving PP attachments and coordination. They used co-occurrence counts from web queries in order to estimate selectional restrictions. Zhou et al (2011) used N-gram counts from Google search and Google V1 to deduce word-word attachment preferences. They used these counts in a pair-wise mutual information (PMI) scheme as features for improving parsing in the News domain (WSJ) and adaptation for biomedical domain. Their evaluation showed improvement of 1% on WSJ 
47
section 23 over the vanilla MST parser and a significant increase in the domain adaptation problem.  4.3 Domain adaptation of dependency parsing Domain adaptation for dependency parsing has been studied mostly in regard to the CoNLL 2007 shared task (Nivre, Hall et al 2007). Both of the leading methods included learning from a parser ensemble. Attardi et al?s (2007) used a weak parser in order to identify common parsing errors and overcome those in the training of a stronger parser. Sagae and Tsujii (2007) used two different parsers to parse un-annotated in-domain data and used the trees where the two parsers agreed to augment the training corpus.  Dredze et al (2007) approached the ?closed? problem, i.e., without using additional un-annotated data. They used the PennBioIE Treebank and applied a number of adaptation techniques: (1) features concerning NPs such as chunking information and frequency; (2) word distribution features; (3) features encoding information from diverse parsers; (4) target focused learning ? giving greater weight in training to sentences which are more likely in a target domain language model.  These methods have not improved accuracy over the baseline of the MST parser (McDonald, Pereira et al 2005) trained on WSJ.  5 Conclusion Learning class-class selectional preferences from a large in-domain corpus assists dependency parsing significantly. We have suggested a method for learning selectional preference classes for a specific domain using an existing parser and a standard implementation of LDA topic modeling. The SP model can be used for estimating the affinity between a pair of tokens or simply as a feature of semantic class association. This approach is faster when querying the model for the affinity of a pair of words than a PMI model suggested by Zhou et al(2011). While covering fewer tokens in the target test set than Brown clusters, the method achieved a higher improvement of parsing performance. Furthermore, some of the improvement was additive and reduced UAS error by 4.5% compared to a strong co-training baseline. 6 References  Attardi, G., F. Dell?Orletta, et al (2007). Multilingual dependency parsing and domain adaptation using DeSR. ACL. Blei, D. M., A. Y. Ng, et al (2003). "Latent dirichlet alocation." JMLR 3: 993-1022. Charniak, E., D. Blaheta, et al (2000). "Bllip 1987-89 wsj corpus release 1." LDC. 
De Marneffe, M. C., B. MacCartney, et al (2006). Generating typed dependency parses from phrase structure parses. LREC. Dredze, M., J. Blitzer, et al (2007). Frustratingly hard domain adaptation for dependency parsing. CoNLL 2007. Erk, K. and S. Pado (2008). A structured vector space model for word meaning in context. EMNLP 2008: 897-906. Goldberg, Y. and M. Elhadad (2010). An efficient algorithm for easy-first non-directional dependency parsing. NAACL 2010: 742-750. Hartung, M. and A. Frank (2011). Exploring Supervised LDA Models for Assigning Attributes to Adjective-Noun Phrases. ACL. Kim, J.-D., T. Ohta, et al (2009). Overview of BioNLP'09 shared task on event extraction. Current Trends in Biomedical NLP, ACL: 1-9. Kim, J.-D., T. Ohta, et al (2004). Introduction to the bio-entity recognition task at JNLPBA. Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications. Geneva, Switzerland, Association for Computational Linguistics: 70-75. Koo, T., X. Carreras, et al (2008). Simple semi-supervised dependency parsing. ACL 2008: 595-603. Liang, P. (2005). Semi-supervised learning for natural language, Massachusetts Institute of Technology. McCallum, A. K. (2002). "Mallet: A machine learning for language toolkit." McClosky, D., E. Charniak, et al (2006). Effective self-training for parsing, ACL. McDonald, R., F. Pereira, et al (2005). Non-projective dependency parsing using spanning tree algorithms. EMNLP: 523-530. Nakov, P. and M. Hearst (2005). Using the web as an implicit training set: application to structural ambiguity resolution. EMNLP, Association for Computational Linguistics: 835-842. Nivre, J., J. Hall, et al (2007). The CoNLL 2007 Shared Task on Dependency Parsing, CoNLL 2007. s. 915-932. Nivre, J., J. Hall, et al (2006). Maltparser: A data-driven parser-generator for dependency parsing. Noord, G. v. (2007). Using self-trained bilexical preferences to improve disambiguation accuracy. 10th International Conference on Parsing Technologies, ACL: 1-10. Pyysalo, S., T. Ohta, et al (2011). "Overview of the Entity Relations (REL) supporting task of BioNLP 2011." ACL HLT 2011 1(480): 83. Ritter, A., Mausam, et al (2010). A latent dirichlet alocation method for selectional preferences. ACL 2010: 424-434. Sagae, K. and J.-i. Tsujii (2007). Dependency parsing and domain adaptation with LR models and parser ensembles. EMNLP-CoNLL 2007: 1044-1050. S?aghdha, D. (2010). Latent variable models of selectional preference. ACL 2010: 435-444. Tateisi, Y., A. Yakushiji, et al (2005). Syntax Annotation for the GENIA corpus. ACL. Tsuruoka, Y., Y. Tateishi, et al (2005). "Developing a robust part-of-speech tagger for biomedical text." AII: 382-392. Turney, P. D. and P. Pantel (2010). "From frequency to meaning: Vector space models of semantics." JAIR 37(1): 141-188. Wallach, H., D. Mimno, et al (2009). "Rethinking LDA: Why priors matter." NIPS 22: 1973?1981. Zhou, G., J. Zhao, et al (2011). Exploiting web-derived selectional preference to improve statistical dependency parsing. ACL. 
48
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 163?172,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
A Non-Monotonic Arc-Eager Transition System for Dependency Parsing
Matthew Honnibal
Department of Computing
Macquarie University
Sydney, Australia
matthew.honnibal@mq.edu.edu.au
Yoav Goldberg
Department of Computer Science
Bar Ilan University
Ramat Gan, Israel
yoav.goldberg@gmail.com
Mark Johnson
Department of Computing
Macquarie University
Sydney, Australia
mark.johnson@mq.edu.edu.au
Abstract
Previous incremental parsers have used
monotonic state transitions. However,
transitions can be made to revise previous
decisions quite naturally, based on further
information.
We show that a simple adjustment to the
Arc-Eager transition system to relax its
monotonicity constraints can improve ac-
curacy, so long as the training data in-
cludes examples of mistakes for the non-
monotonic transitions to repair. We eval-
uate the change in the context of a state-
of-the-art system, and obtain a statistically
significant improvement (p < 0.001) on
the English evaluation and 5/10 of the
CoNLL languages.
1 Introduction
Historically, monotonicity has played an im-
portant role in transition-based parsing systems.
Non-monotonic systems, including the one pre-
sented here, typically redundantly generate multi-
ple derivations for each syntactic analysis, leading
to spurious ambiguity (Steedman, 2000). Early,
pre-statistical work on transition-based parsing
such as Abney and Johnson (1991) implicitly as-
sumed that the parser searches the entire space
of possible derivations. The presence of spuri-
ous ambiguity causes this search space to be a di-
rected graph rather than a tree, which considerably
complicates the search, so spurious ambiguity was
avoided whenever possible.
However, we claim that non-monotonicity and
spurious ambiguity are not disadvantages in a
modern statistical parsing system such as ours.
Modern statistical models have much larger search
spaces because almost all possible analyses are al-
lowed, and a numerical score (say, a probability
distribution) is used to distinguish better analy-
ses from worse ones. These search spaces are so
large that we cannot exhaustively search them, so
instead we use the scores associated with partial
analyses to guide a search that explores only a mi-
nuscule fraction of the space (In our case we use
greedy decoding, but even a beam search only ex-
plores a small fraction of the exponentially-many
possible analyses).
In fact, as we show here the additional redun-
dant pathways between search states that non-
monotonicity generates can be advantageous be-
cause they allow the parser to ?correct? an ear-
lier parsing move and provide an opportunity to
recover from formerly ?fatal? mistakes. Infor-
mally, non-monotonicity provides ?many paths up
the mountain? in the hope of making it easier to
find at least one.
We demonstrate this by modifying the Arc-
Eager transition system (Nivre, 2003; Nivre et al,
2004) to allow a limited capability for non-
monotonic transitions. The system normally em-
ploys two deterministic constraints that limit the
parser to actions consistent with the previous his-
tory. We remove these, and update the transitions
so that conflicts are resolved in favour of the latest
prediction.
The non-monotonic behaviour provides an im-
provement of up to 0.2% accuracy over the cur-
rent state-of-the-art in greedy parsing. It is pos-
sible to implement the greedy parser we de-
scribe very efficiently: our implementation, which
can be found at http://www.github.com/
syllog1sm/redshift, parses over 500 sen-
tences a second on commodity hardware.
163
2 The Arc-Eager Transition System
In transition-based parsing, a parser consists of a
state (or a configuration) which is manipulated by
a set of actions. An action is applied to a state
and results in a new state. The parsing process
concludes when the parser reaches a final state, at
which the parse tree is read from the state. A par-
ticular set of states and actions yield a transition-
system. Our starting point in this paper is the pop-
ular Arc-Eager transition system, described in de-
tail by Nivre (2008).
The state of the arc-eager system is composed
of a stack, a buffer and a set of arcs. The stack and
the buffer hold the words of a sentence, and the set
of arcs represent derived dependency relations.
We use a notation in which the stack items are
indicated by Si, with S0 being the top of the stack,
S1 the item previous to it and so on. Similarly,
buffer items are indicated as Bi, with B0 being
the first item on the buffer. The arcs are of the
form (h, l,m), indicating a dependency in which
the word m modifies the word h with label l.
In the initial configuration the stack is empty,
and the buffer contains the words of the sentence
followed by an artificial ROOT token, as sug-
gested by Ballesteros and Nivre (2013). In the fi-
nal configuration the buffer is empty and the stack
contains the ROOT token.
There are four parsing actions (Shift, Left-Arc,
Right-Arc and Reduce, abbreviated as S,L,R,D re-
spectively) that manipulate stack and buffer items.
The Shift action pops the first item from the buffer
and pushes it on the stack (the Shift action has a
natural precondition that the buffer is not empty,
as well as a precondition that ROOT can only be
pushed to an empty stack). The Right-Arc action
is similar to the Shift action, but it also adds a
dependency arc (S0, B0), with the current top of
the stack as the head of the newly pushed item
(the Right action has an additional precondition
that the stack is not empty).1 The Left-Arc action
adds a dependency arc (B0, S0) with the first item
in the buffer as the head of the top of the stack,
and pops the stack (with a precondition that the
stack and buffer are not empty, and that S0 is not
assigned a head yet). Finally, the Reduce action
pops the stack, with a precondition that the stack
is not empty and that S0 is already assigned a head.
1For labelled dependency parsing, the Right-Arc and
Left-Arc actions are parameterized by a label L such that the
action RightL adds an arc (S0, L,B0), similarly for LeftL.
2.1 Monotonicty
The preconditions of the Left-Arc and Reduce ac-
tions ensure that every word is assigned exactly
one head, resulting in a well-formed parse tree.
The single head constraint is enforced by ensur-
ing that once an action has been performed, sub-
sequent actions must be consistent with it. We re-
fer to this consistency as the monotonicity of the
system.
Due to monotonicity, there is a natural pair-
ing between the Right-Arc and Reduce actions
and the Shift and Left-Arc actions: a word which
is pushed into the stack by Right-Arc must be
popped using Reduce, and a word which is pushed
by Shift action must be popped using Left-Arc. As
a consequence of this pairing, a Right-Arc move
determines that the head of the pushed token must
be to its left, while a Shift moves determines a
head to its right. Crucially, the decision whether
to Right-Arc or Shift is often taken in a state of
missing information regarding the continuation of
the sentence, forcing an incorrect head assignment
on a subsequent move.
Consider a sentence pair such as (a)?I saw Jack
and Jill? / (b)?I saw Jack and Jill fall?. In (a), ?Jack
and Jill? is the NP object of ?saw?, while in (b) it is
a subject of the embedded verb ?fall?. The mono-
tonic arc-eager parser has to decide on an analysis
as soon as it sees ?saw? on the top of the stack and
?Jack? at the front of the buffer, without access to
the disambiguating verb ?fall?.
In what follows, we suggest a non-monotonic
variant of the Arc-Eager transition system, allow-
ing the parser to recover from the incorrect head
assignments which are forced by an incorrect res-
olution of a Shift/Right-Arc ambiguity.
3 The Non-Monotonic Arc-Eager System
The Arc-Eager transition system (Nivre et al,
2004) has four moves. Two of them create depen-
dencies, two push a word from the buffer to the
stack, and two remove an item from the stack:
Push Pop
Adds dependency Right-Arc Left-Arc
No new dependency Shift Reduce
Every word in the sentence is pushed once and
popped once; and every word must have exactly
one head. This creates two pairings, along the di-
agonals: (S, L) and (R, D). Either the push move
adds the head or the pop move does, but not both
and not neither.
164
I saw Jack and Jill fall R
S L S R R D R D L R D L
1 2 3 4 5 6 7 8 9 10 11 12
I saw Jack and Jill fall R
2 4 5
7
2 5
7
9
Figure 1: State before and after a non-monotonic Left-
Arc. At move 9, fall is the first word of the buffer (marked
with an arrow), and saw and Jack are on the stack (circled).
The arc created at move 4 was incorrect (in red). Arcs are
labelled with the move that created them. After move 9 (the
lower state), the non-monotonic Left-Arc move has replaced
the incorrect dependency with a correct Left-Arc (in green).
Thus in the Arc-Eager system the first move de-
termines the corresponding second move. In our
non-monotonic system the second move can over-
write an attachment made by the first. This change
makes the transition system non-monotonic, be-
cause if the model decides on an incongruent pair-
ing we will have to either undo or add a depen-
dency, depending on whether we correct a prior
Right-Arc, or a prior Shift.
3.1 Non-monotonic Left-Arc
Figure 1 shows a before-and-after view of a non-
monotonic transition. The sequence below the
words shows the transition history. The words that
are circled in the upper and lower line are on the
stack before and after the transition, respectively.
The arrow shows the start of the buffer, and arcs
are labelled with the move that added them.
The parser began correctly by Shifting I and
Left-Arcing it to saw, which was then also Shifted.
The mistake, made at Move 4, was to Right-Arc
Jack instead of Shifting it.
The difficulty of this kind of a decision for an
incremental parser is fundamental. The leftward
context does not constrain the decision, and an ar-
bitrary amount of text could separate Jack from
fall. Eye-tracking experiments show that humans
often perform a saccade while reading such exam-
ples (Frazier and Rayner, 1982).
In moves 5-8 the parser correctly builds the rest
of the NP, and arrives at fall. The monotonicity
constraints would force an incorrect analysis, hav-
ing fall modify Jack or saw, or having saw modify
fall as an embedded verb with no arguments.
I saw Jack and Jill R
S L S S R D R D R D D L
1 2 3 4 5 6 7 8 9 10 11 12
I saw Jack and Jill R
2 5
7
2 5
7
9
Figure 2: State before and after a non-monotonic Reduce.
After making the wrong push move at 4, at move 11 the parser
has Jack on the stack (circled), with only the dummy ROOT
token left in the buffer. A monotonic parser must determinis-
tically Left-Arc Jack here to preserve the previous decision,
despite the current state. We remove this constraint, and in-
stead assume that when the model selects Reduce for a head-
less item, it is reversing the previous Shift/Right decision. We
add the appropriate arc, assigning the label that scored high-
est when the Shift/Right decision was made.
We allow Left-Arcs to ?clobber? edges set by
Right-Arcs if the model recommends it. The pre-
vious edge is deleted, and the Left-Arc proceeds
as normal. The effect of this is exactly as if the
model had correctly chosen Shift at move 4. We
simply give the model a second chance to make
the correct choice.
3.2 Non-monotonic Reduce
The upper arcs in Figure 2 show a state resulting
from the opposite error. The parser has Shifted
Jack instead of Right-Arcing it. After building the
NP the buffer is exhausted, except for the ROOT
token, which is used to wrongly Left-Arc Jack as
the sentence?s head word.
Instead of letting the previous choice lock us in
to the pair (Shift, Left-Arc), we let the later deci-
sion reverse it to (Right-Arc, Reduce), if the parser
has predicted Reduce in spite of the signal from its
previous decision. In the context shown in Figure
2, the correctness of the Reduce move is quite pre-
dictable, once the choice is made available.
When the Shift/Right-Arc decision is reversed,
we add an arc between the top of the stack (S0)
and the word preceding it (S1). This is the arc that
would have been created had the parser chosen to
Right-Arc when it chose to Shift. Since our idea is
to reverse this mistake, we select the Right-Arc la-
bel that the model scored most highly at that time.2
2An alternative approach to label assignment is to parame-
terize the Reduce action with a label, similar to the Right-Arc
and Left-Arc actions, and let that label override the previ-
ously predicted label. This would allow the parser to con-
165
To summarize, our Non-Monotnonic Arc-
Eager system differs from the monotonic
Arc-Eager system by:
? Changing the Left-Arc action by removing
the precondition that S0 does not have a head,
and updating the dependency arcs such previ-
ously derived arcs having S0 as a dependent
are removed from the arcs set.
? Changing the Reduce action by removing the
precondition that S0 has a head, and updating
the dependency arcs such that if S0 does not
have a head, S1 is assigned as the head of S0.
4 Why have two push moves?
We have argued above that it is better to trust the
second decision that the model makes, rather than
using the first decision to determine the second.
If this is the case, is the first decision entirely re-
dundant? Instead of defining how pop moves can
correct Shift/Right-Arc mistakes, we could instead
eliminate the ambiguity. There are two possibili-
ties: Shift every token, and create all Right-Arcs
via Reduce; or Right-Arc every token, and replace
them with Left-Arcs where necessary.
Preliminary experiments on the development
data revealed a problem with these approaches. In
many cases the decision whether to Shift or Right-
Arc is quite clear, and its result provides useful
conditioning context to later decisions. The in-
formation that determined those decisions is never
lost, but saving all of the difficulty for later is not
a very good structured prediction strategy.
As an example of the problem, if the Shift move
is eliminated, about half of the Right-Arcs created
will be spurious. All of these arcs will be assigned
labels making important features uselessly noisy.
In the other approach, we avoid creating spurious
arcs, but the model does not predict whether S0 is
attached to S1, or what the label would be, and we
miss useful features.
The non-monotonic transition system we pro-
pose does not have these problems. The model
learns to make Shift vs. Right-Arc decisions as
normal, and conditions on them ? but without
committing to them.
dition its label decision on the new context, which was suf-
ficiently surprising to change its move prediction. For effi-
ciency and simplicity reasons, we chose instead to trust the
label the model proposed when the reduced token was ini-
tially pushed into the stack. This requires an extra vector of
labels to be stored during parsing.
5 Dynamic Oracles
An essential component when training a
transition-based parser is an oracle which,
given a gold-standard tree, dictates the sequence
of moves a parser should make in order to derive
it. Traditionally, these oracles are defined as func-
tions from trees to sequences, mapping a gold tree
to a single sequence of actions deriving it, even
if more than one sequence of actions derives the
gold tree. We call such oracles static. Recently,
Goldberg and Nivre (2012) introduced the concept
of a dynamic oracle, and presented a concrete ora-
cle for the arc-eager system. Instead of mapping
a gold tree to a sequence of actions, the dynamic
oracle maps a ?configuration, gold tree? pair to a
set of optimal transitions. More concretely, the
dynamic oracle presented in Goldberg and Nivre
(2012) maps ?action, configuration, tree? tuples
to an integer, indicating the number of gold arcs
in tree that can be derived from configuration
by some sequence of actions, but could not be
derived after applying action to the configuration.
There are two advantages to this. First, the
ability to label any configuration, rather than only
those along a single path to the gold-standard
derivation, allows much better training data to be
generated. States come with realistic histories, in-
cluding errors ? a critical point for the current
work. Second, the oracle accounts for spurious
ambiguity correctly, as it will label multiple ac-
tions as correct if the optimal parses resulting from
them are equally accurate.
In preliminary experiments in which we trained
the parser using the static oracle but allowed the
non-monotonic repair operations during parsing,
we found that the the repair moves yielded no im-
provement. This is because the static oracle does
not generate any examples of the repair moves dur-
ing training, causing the parser to rarely predict
them in test time. We will first describe the Arc-
Eager dynamic oracle, and then define dynamic
oracles for the non-monotonic transition systems
we present.
5.1 Monotonic Arc-Eager Dynamic Oracle
We now briefly describe the dynamic oracle for the
arc-eager system. For more details, see Goldberg
and Nivre (2012). The oracle is computed by rea-
soning about the arcs which are reachable from a
given state, and counting the number of gold arcs
which will no longer be reachable after applying a
166
given transition at a given state. 3
The reasoning is based on the observations that
in the arc-eager system, new arcs (h, l,m) can be
derived iff the following conditions hold:
(a) There is no existing arc (h?, l?,m) such that
h? 6= h, and (b) Either both h and m are on the
buffer, or one of them is on the buffer and the other
is on the stack. In other words:
(a) once a word acquires a head (in a Left-Arc or
Right-Arc transition) it loses the ability to acquire
any other head.
(b) once a word is moved from the buffer to the
stack (Shift or Right-Arc) it loses the ability to ac-
quire heads that are currently on the stack, as well
as dependents that are currently on the stack and
are not yet assigned a head.4
(c) once a word is removed from the stack (Left-
Arc or Reduce) it loses the ability to acquire any
dependents on the buffer.
Based on these observations, Goldberg and Nivre
(2012) present an oracle C(a, c, t) for the mono-
tonic arc-eager system, computing the number of
arcs in the gold tree t that are reachable from a
parser?s configuration c and are no longer reach-
able from the configuration a(c) resulting from the
application of action a to configuration c.
5.2 Non-monotonic Dynamic Oracles
Given the oracle C(a, c, t) for the monotonic sys-
tem, we adapt it to a non-monotonic variant by
considering the changes from the monotonic to the
non-monotonic system, and adding ? terms ac-
cordingly. We define three novel oracles: CNML,
CNMD and CNML+D for systems with a non-
monotonic Left-Arc, Reduce or both.
CNML(a, c, t) = C(a, c, t) +?NML(a, c, t)
CNMD(a, c, t) = C(a, c, t) +?NMD(a, c, t)
CNML+D(a, c, t) = C(a, c, t) +?NML(a, c, t)
+?NMD(a, c, t)
The terms ?NML and ?NMD reflect the score
adjustments that need to be done to the arc-eager
oracle due to the changes of the Left-Arc and Re-
duce actions, respectively, and are detailed below.
3The correctness of the oracle is based on a property of
the arc-eager system, stating that if a set of arcs which can be
extended to a projective tree can be individually derived from
a given configuration, then a projective tree containing all of
the arcs in the set is also derivable from the same configura-
tion. This same property holds also for the non-monotonic
variants we propose.
4The condition that the words on the stack are not yet as-
signed a head is missing from (Goldberg and Nivre, 2012)
Changes due to non-monotonic Left-Arc:
? ?NML(RIGHTARC, c, t): The cost of Right-
Arc is decreased by 1 if the gold head of B0 is
on the buffer (because B0 can still acquire its
correct head later with a Left-Arc action). It
is increased by 1 for any word w on the stack
such that B0 is the gold parent of w and w
is assigned a head already (in the monotonic
oracle, this cost was taken care of when the
word was assigned an incorrect head. In the
non-monotonic variant, this cost is delayed).
? ?NML(REDUCE, c, t): The cost of Reduce is
increased by 1 if the gold head of S0 is on the
buffer, because removing S0 from the stack
precludes it from acquiring its correct head
later on with a Left-Arc action. (This cost is
paid for in the monotonic version when S0
acquired its incorrect head).
? ?NML(LEFTARC, c, t): The cost of Left-
Arc is increased by 1 if S0 is already assigned
to its gold parent. (This situation is blocked
by a precondition in the monotonic case).
The cost is also increased if S0 is assigned
to a non-gold parent, and the gold parent is
in the buffer, but not B0. (As a future non-
monotonic Left-Arc is prevented from setting
the correct head.)
? ?NML(SHIFT, c, gold): The cost of Shift is
increased by 1 for any word w on the stack
such that B0 is the gold parent of w and w is
assigned a head already. (As in Right-Arc, in
the monotonic oracle, this cost was taken care
of when w was assigned an incorrect head.)
Changes due to non-monotonic Reduce:
? ?NMD(SHIFT, c, gold): The cost of Shift is
decreased by 1 if the gold head of B0 is S0
(Because this arc can be added later on with
a non-monotonic Reduce action).
? ?NMD(LEFTARC, c, gold): The cost of
Left-Arc is increased by 1 if S0 is not as-
signed a head, and the gold head of S0 is
S1 (Because this precludes adding the correct
arc with a Reduce of S0 later).
? ?NMD(REDUCE, c, gold) = 0. While it may
seem that a change to the cost of a Reduce ac-
tion is required, in fact the costs of the mono-
tonic system hold here, as the head of S0 is
167
predetermined to be S1. The needed adjust-
ments are taken care of in Left-Arc and Shift
actions.5
? ?NMD(RIGHTARC, c, gold) = 0
6 Applying the Oracles in Training
Once the dynamic-oracles for the non-monotonic
system are defined, we could in principle just plug
them in the perceptron-based training procedure
described in Goldberg and Nivre (2012). How-
ever, a tacit assumption of the dynamic-oracles is
that all paths to recovering a given arc are treated
equally. This assumption may be sub-optimal
for the purpose of training a parser for a non-
monotonic system.
In Section 4, we explained why removing the
ambiguity between Shift and Right-Arcs alto-
gether was an inferior strategy. Failing to discrim-
inate between arcs reachable by monotonic and
non-monotonic paths does just that, so this oracle
did not perform well in preliminary experiments
on the development data.
Instead, we want to learn a model that will offer
its best prediction of Shift vs. Right-Arc, which
we expect to usually be correct. However, in those
cases where the model does make the wrong de-
cision, it should have the ability to later over-turn
that decision, by having an unconstrained choice
of Reduce vs. Left-Arc.
In order to correct for that, we don?t use the
non-monotonic oracles directly when training the
parser, but instead train the parser using both the
monotonic and non-monotonic oracles simultane-
ously by combining their judgements: while we
always prefer zero-cost non-monotonic actions to
monotonic-actions with non-zero cost, if the non-
monotonic oracle assigns several actions a zero-
cost, we prefer to follow those actions that are also
assigned a zero-cost by the monotonic oracle, as
these actions lead to the best outcome without re-
lying on a non-monotonic (repair) operation down
the road.
7 Experiments
We base our experiments on the parser described
by Goldberg and Nivre (2012). We began by im-
plementing their baseline system, a standard Arc-
Eager parser using an averaged Perceptron learner
and the extended feature set described by Zhang
5If using a labeled reduce transition, the label assignment
costs should be handled here.
Stanford MALT
W S W S
Unlabelled Attachment
Baseline (G&N-12) 91.2 42.0 90.9 39.7
NM L 91.4 43.1 91.0 40.1
NM D 91.4 42.8 91.1 41.2
NM L+D 91.6 43.3 91.3 41.5
Labelled Attachment
Baseline (G&N-12) 88.7 31.8 89.7 36.6
NM L 89.0 32.5 89.8 36.9
NM D 88.9 32.3 89.9 37.7
NM L+D 89.1 32.7 90.0 37.9
Table 1: Development results on WSJ 22. Both non-
monotonic transitions bring small improvements in per-token
(W) and whole sentence (S) accuracy, and the improvements
are additive.
and Nivre (2011). We follow Goldberg and Nivre
(2012) in training all models for 15 iterations, and
shuffling the sentences before each iteration.
Because the sentence ordering affects the
model?s accuracy, all results are averaged from
scores produced using 20 different random seeds.
The seed determines how the sentences are shuf-
fled before each iteration, as well as when to fol-
low an optimal action and when to follow a non-
optimal action during training. The Wilcoxon
signed-rank test was used for significance testing.
A train/dev/test split of 02-21/22/23 of the Penn
Treebank WSJ (Marcus et al, 1993) was used for
all models. The data was converted into Stan-
ford dependencies (de Marneffe et al, 2006) with
copula-as-head and the original PTB noun-phrase
bracketing. We also evaluate our models on de-
pendencies created by the PENN2MALT tool, to
assist comparison with previous results. Automat-
ically assigned POS tags were used during training,
to match the test data more closely. 6 We also eval-
uate the non-monotonic transitions on the CoNLL
2007 multi-lingual data.
8 Results and analysis
Table 1 shows the effect of the non-monotonic
transitions on labelled and unlabelled attachment
score on the development data. All results are av-
erages from 20 models trained with different ran-
dom seeds, as the ordering of the sentences at each
iteration of the Perceptron algorithm has an effect
on the system?s accuracy. The two non-monotonic
transitions each bring small but statistically signif-
icant improvements that are additive when com-
bined in the NM L+D system. The result is stable
6We thank Yue Zhang for supplying the POS-tagged files
used in the Zhang and Nivre (2011) experiments.
168
across both dependency encoding schemes.
Frequency analysis. Recall that there are two pop
moves available: Left-Arc and Reduce. The Left-
Arc is considered non-monotonic if the top of the
stack has a head specified, and the Reduce move
is considered non-monotonic if it does not. How
often does the parser select monotonic and non-
monotonic pop moves, and how often is its deci-
sion correct?
In Table 2, the True Positive column shows how
often non-monotonic transitions were used to add
gold standard dependencies. The False Positive
column shows how often they were used incor-
rectly. The False Negative column shows how
often the parser missed a correct non-monotonic
transition, and the True Negative column shows
how often the monotonic alternative was correctly
preferred (e.g. the parser correctly chose mono-
tonic Reduce in place of non-monotonic Left-
Arc). Punctuation dependencies were excluded.
The current system has high precision but low
recall for repair operations, as they are relatively
rare in the gold-standard. While we already
see improvements in accuracy, the upper bound
achievable by the non-monotonic operations is
higher, and we hope to approach it in the future
using improved learning techniques.
Linguistic analysis. To examine what construc-
tions were being corrected, we looked at the fre-
quencies of the labels being introduced by the
non-monotonic moves. We found that there were
two constructions being commonly repaired, and
a long-tail of miscellaneous cases.
The most frequent repair involved the mark la-
bel. This is assigned to conjunctions introducing
subordinate clauses. For instance, in the sentence
Results were released after the market closed, the
Stanford scheme attaches after to closed. The
parser is misled into greedily attaching after to re-
leased here, as that would be correct if after were a
preposition, as in Results were released after mid-
night. This construction was repaired 33 times, 13
where the initial decision was mark, and 21 times
the other way around. The other commonly re-
paired construction involved greedily attaching an
object that was actually the subject of a comple-
ment clause, e.g. NCNB corp. reported net income
doubled. These were repaired 19 times.
WSJ evaluation. Table 3 shows the final test
results. While still lagging behind search based
parsers, we push the boundaries of what can be
TP FP TN FN
Left-Arc 60 14 18,466 285
Reduce 52 26 14,066 250
Total 112 40 32,532 535
Table 2: True/False positive/negative rates for the predic-
tion of the non-monotonic transitions. The non-monotonic
transitions add correct dependencies 112 times, and produce
worse parses 40 times. 535 opportunities for non-monotonic
transitions were missed.
System O Stanford Penn2Malt
LAS UAS LAS UAS
K&C 10 n3 ? ? ? 93.00
Z&N 11 nk 91.9 93.5 91.8 92.9
G&N 12 n 88.72 90.96 ? ?
Baseline(G&N-12) n 88.7 90.9 88.7 90.6
NM L+D n 88.9 91.1 88.9 91.0
Table 3: WSJ 23 test results, with comparison against the
state-of-the-art systems from the literature of different run-
times. K&C 10=Koo and Collins (2010); Z&N 11=Zhang
and Nivre (2011); G&N 12=Goldberg and Nivre (2012).
achieved with a purely greedy system, with a sta-
tistically significant improvement over G&N 12.
CoNLL 2007 evaluation. Table 4 shows the ef-
fect of the non-monotonic transitions across the
ten languages in the CoNLL 2007 data sets. Statis-
tically significant improvements in accuracy were
observed for five of the ten languages. The accu-
racy improvement on Hungarian and Arabic did
not meet our significance threshold. The non-
monotonic transitions did not decrease accuracy
significantly on any of the languages.
9 Related Work
One can view our non-monotonic parsing system
as adding ?repair? operations to a greedy, deter-
ministic parser, allowing it to undo previous de-
cisions and thus mitigating the effect of incorrect
parsing decisions due to uncertain future, which
is inherent in greedy left-to-right transition-based
parsers. Several approaches have been taken to ad-
dress this problem, including:
Post-processing Repairs (Attardi and Ciaramita,
2007; Hall and Nova?k, 2005; Inokuchi and Ya-
maoka, 2012) Closely related to stacking, this line
of work attempts to train classifiers to repair at-
tachment mistakes after a parse is proposed by
a parser by changing head attachment decisions.
The present work differs from these by incorporat-
ing the repair process into the transition system.
Stacking (Nivre and McDonald, 2008; Martins
et al, 2008), in which a second-stage parser runs
over the sentence using the predictions of the first
parser as features. In contrast our parser works in
169
System AR BASQ CAT CHI CZ ENG GR HUN ITA TUR
Baseline 83.4 76.2 91.5 82.3 78.8 87.9 81.2 77.6 83.8 78.0
NM L+D 83.6 76.1 91.5 82.7 80.1 88.4 81.8 77.9 84.1 78.0
Table 4: Multi-lingual evaluation. Accuracy improved on Chinese, Czech, English, Greek and Italian (p < 0.001), trended
upward on Arabic and Hungarian (p < 0.005), and was unchanged on Basque, Catalan and Turkish (p > 0.4).
a single, left-to-right pass over the sentence.
Non-directional Parsing The EasyFirst parser
of Goldberg and Elhadad (2010) tackles similar
forms of ambiguities by dropping the Shift action
altogether, and processing the sentence in an easy-
to-hard bottom-up order instead of left-to-right,
resulting in a greedy but non-directional parser.
The indeterminate processing order increases the
parser?s runtime from O(n) to O(n log n). In con-
trast, our parser processes the sentence incremen-
tally, and runs in a linear time.
Beam Search An obvious approach to tackling
ambiguities is to forgo the greedy nature of the
parser and instead to adopt a beam search (Zhang
and Clark, 2008; Zhang and Nivre, 2011) or a
dynamic programming (Huang and Sagae, 2010;
Kuhlmann et al, 2011) approach. While these ap-
proaches are very successful in producing high-
accuracy parsers, we here explore what can be
achieved in a strictly deterministic system, which
results in much faster and incremental parsing al-
gorithms. The use of non-monotonic transitions in
beam-search parser is an interesting topic for fu-
ture work.
10 Conclusion and future work
We began this paper with the observation that
because the Arc-Eager transition system (Nivre
et al, 2004) attaches a word to its governor ei-
ther when the word is pushed onto the stack or
when it is popped off the stack, monotonicity (plus
the ?tree constraint? that a word has exactly one
governor) implies that a word?s push-move de-
termines its associated pop-move. In this paper
we suggest relaxing the monotonicity constraint
to permit the pop-move to alter existing attach-
ments if appropriate, thus breaking the 1-to-1 cor-
respondence between push-moves and pop-moves.
This permits the parser to correct some early in-
correct attachment decisions later in the parsing
process. Adding additional transitions means that
in general there are multiple transition sequences
that generate any given syntactic analysis, i.e., our
non-monotonic transition system generates spuri-
ous ambiguities (note that the Arc-Eager transition
system on its own generates spurious ambiguities).
As we explained in the paper, with the greedy de-
coding used here additional spurious ambiguity is
not necessarily a draw-back.
The conventional training procedure for
transition-based parsers uses a ?static? oracle
based on ?gold? parses that never predicts a
non-monotonic transition, so it is clearly not
appropriate here. Instead, we use the incremental
error-based training procedure involving a ?dy-
namic? oracle proposed by Goldberg and Nivre
(2012), where the parser is trained to predict the
transition that will produce the best-possible anal-
ysis from its current configuration. We explained
how to modify the Goldberg and Nivre oracle so
it predicts the optimal moves, either monotonic or
non-monotonic, from any configuration, and use
this to train an averaged perceptron model.
When evaluated on the standard WSJ training
and test sets we obtained a UAS of 91.1%, which
is a 0.2% improvement over the already state-of-
the-art baseline of 90.9% that is obtained with the
error-based training procedure of Goldberg and
Nivre (2012). On the CoNLL 2007 datasets, ac-
curacy improved significantly on 5/10 languages,
and did not decline significantly on any of them.
Looking to the future, we believe that it would
be interesting to investigate whether adding non-
monotonic transitions is beneficial in other parsing
systems as well, including systems that target for-
malisms other than dependency grammars. As we
observed in the paper, the spurious ambiguity that
non-monotonic moves introduce may well be an
advantage in a statistical parser with an enormous
state-space because it provides multiple pathways
to the correct analysis (of which we hope at least
one is navigable).
We investigated a very simple kind of non-
monotonic transition here, but of course it?s pos-
sible to design transition systems with many more
transitions, including transitions that are explicitly
designed to ?repair? characteristic parser errors. It
might even be possible to automatically identify
the most useful repair transitions and incorporate
them into the parser.
170
Acknowledgments
The authors would like to thank the anony-
mous reviewers for their valuable comments.
This research was supported under the Aus-
tralian Research Council?s Discovery Projects
funding scheme (project numbers DP110102506
and DP110102593).
References
Stephen Abney and Mark Johnson. 1991. Mem-
ory requirements and local ambiguities of pars-
ing strategies. Journal of Psycholinguistic Re-
search, 20(3):233?250.
Giuseppe Attardi and Massimiliano Ciaramita.
2007. Tree revision learning for dependency
parsing. In Human Language Technologies
2007: The Conference of the North American
Chapter of the Association for Computational
Linguistics; Proceedings of the Main Confer-
ence, pages 388?395. Association for Compu-
tational Linguistics, Rochester, New York.
Miguel Ballesteros and Joakim Nivre. 2013. Go-
ing to the roots of dependency parsing. Compu-
tational Linguistics. 39:1.
Marie-Catherine de Marneffe, Bill MacCartney,
and Christopher D. Manning. 2006. Generating
typed dependency parses from phrase structure
parses. In Proceedings of the 5th International
Conference on Language Resources and Evalu-
ation (LREC).
Lyn Frazier and Keith Rayner. 1982. Making and
correcting errors during sentence comprehen-
sion: Eye movements in the analysis of struc-
turally ambiguous sentences. Cognitive Psy-
chology, 14(2):178?210.
Yoav Goldberg and Michael Elhadad. 2010. An
efficient algorithm for easy-first non-directional
dependency parsing. In Human Language Tech-
nologies: The 2010 Annual Conference of the
North American Chapter of the Association
for Computational Linguistics (NAACL HLT),
pages 742?750.
Yoav Goldberg and Joakim Nivre. 2012. A dy-
namic oracle for arc-eager dependency parsing.
In Proceedings of the 24th International Con-
ference on Computational Linguistics (Coling
2012). Association for Computational Linguis-
tics, Mumbai, India.
Keith Hall and Vaclav Nova?k. 2005. Corrective
modeling for non-projective dependency pars-
ing. In Proceedings of the 9th International
Workshop on Parsing Technologies (IWPT),
pages 42?52.
Liang Huang and Kenji Sagae. 2010. Dynamic
programming for linear-time incremental pars-
ing. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguis-
tics (ACL), pages 1077?1086.
Akihiro Inokuchi and Ayumu Yamaoka. 2012.
Mining rules for rewriting states in a transition-
based dependency parser for English. In Pro-
ceedings of COLING 2012, pages 1275?1290.
The COLING 2012 Organizing Committee,
Mumbai, India.
Terry Koo and Michael Collins. 2010. Efficient
third-order dependency parsers. In Proceedings
of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 1?
11.
Marco Kuhlmann, Carlos Go?mez-Rodr??guez, and
Giorgio Satta. 2011. Dynamic program-
ming algorithms for transition-based depen-
dency parsers. In Proceedings of the 49th An-
nual Meeting of the Association for Computa-
tional Linguistics: Human Language Technolo-
gies - Volume 1, HLT ?11, pages 673?682. Asso-
ciation for Computational Linguistics, Strouds-
burg, PA, USA.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19:313?
330.
Andre? Filipe Martins, Dipanjan Das, Noah A.
Smith, and Eric P. Xing. 2008. Stacking de-
pendency parsers. In Proceedings of the Con-
ference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 157?166.
Joakim Nivre. 2003. An efficient algorithm for
projective dependency parsing. In Proceedings
of the 8th International Workshop on Parsing
Technologies (IWPT), pages 149?160.
Joakim Nivre. 2008. Algorithms for determinis-
tic incremental dependency parsing. Computa-
tional Linguistics, 34:513?553.
Joakim Nivre, Johan Hall, and Jens Nilsson.
2004. Memory-based dependency parsing. In
171
Hwee Tou Ng and Ellen Riloff, editors, HLT-
NAACL 2004 Workshop: Eighth Conference
on Computational Natural Language Learn-
ing (CoNLL-2004), pages 49?56. Association
for Computational Linguistics, Boston, Mas-
sachusetts, USA.
Joakim Nivre and Ryan McDonald. 2008. In-
tegrating graph-based and transition-based de-
pendency parsers. In Proceedings of the 46th
Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 950?958.
Mark Steedman. 2000. The Syntactic Process.
MIT Press, Cambridge, MA.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-
based and transition-based dependency parsing.
In Proceedings of the 2008 Conference on Em-
pirical Methods in Natural Language Process-
ing, pages 562?571. Association for Computa-
tional Linguistics, Honolulu, Hawaii.
Yue Zhang and Joakim Nivre. 2011. Transition-
based dependency parsing with rich non-local
features. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies,
pages 188?193. Association for Computational
Linguistics, Portland, Oregon, USA.
172
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146?182,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Overview of the SPMRL 2013 Shared Task:
Cross-Framework Evaluation of Parsing Morphologically Rich Languages?
Djam? Seddaha, Reut Tsarfatyb, Sandra K?blerc,
Marie Canditod, Jinho D. Choie, Rich?rd Farkasf , Jennifer Fosterg, Iakes Goenagah,
Koldo Gojenolai, Yoav Goldbergj , Spence Greenk, Nizar Habashl, Marco Kuhlmannm,
Wolfgang Maiern, Joakim Nivreo, Adam Przepi?rkowskip, Ryan Rothq, Wolfgang Seekerr,
Yannick Versleys, Veronika Vinczet, Marcin Wolin?skiu,
Alina Wr?blewskav, Eric Villemonte de la Cl?rgeriew
aU. Paris-Sorbonne/INRIA, bWeizman Institute, cIndiana U., dU. Paris-Diderot/INRIA, eIPsoft Inc., f,tU. of Szeged,
gDublin City U., h,iU. of the Basque Country, jBar Ilan U., kStanford U., l,qColumbia U., m,oUppsala U., nD?sseldorf U.,
p,u,vPolish Academy of Sciences, rStuttgart U., sHeidelberg U., wINRIA
Abstract
This paper reports on the first shared task on
statistical parsing of morphologically rich lan-
guages (MRLs). The task features data sets
from nine languages, each available both in
constituency and dependency annotation. We
report on the preparation of the data sets, on
the proposed parsing scenarios, and on the eval-
uation metrics for parsing MRLs given dif-
ferent representation types. We present and
analyze parsing results obtained by the task
participants, and then provide an analysis and
comparison of the parsers across languages and
frameworks, reported for gold input as well as
more realistic parsing scenarios.
1 Introduction
Syntactic parsing consists of automatically assigning
to a natural language sentence a representation of
its grammatical structure. Data-driven approaches
to this problem, both for constituency-based and
dependency-based parsing, have seen a surge of inter-
est in the last two decades. These data-driven parsing
approaches obtain state-of-the-art results on the de
facto standard Wall Street Journal data set (Marcus et
al., 1993) of English (Charniak, 2000; Collins, 2003;
Charniak and Johnson, 2005; McDonald et al, 2005;
McClosky et al, 2006; Petrov et al, 2006; Nivre et
al., 2007b; Carreras et al, 2008; Finkel et al, 2008;
?Contact authors: djame.seddah@paris-sorbonne.fr,
reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu
Huang, 2008; Huang et al, 2010; Zhang and Nivre,
2011; Bohnet and Nivre, 2012; Shindo et al, 2012),
and provide a foundation on which many tasks oper-
ating on semantic structure (e.g., recognizing textual
entailments) or even discourse structure (coreference,
summarization) crucially depend.
While progress on parsing English ? the main
language of focus for the ACL community ? has in-
spired some advances on other languages, it has not,
by itself, yielded high-quality parsing for other lan-
guages and domains. This holds in particular for mor-
phologically rich languages (MRLs), where impor-
tant information concerning the predicate-argument
structure of sentences is expressed through word for-
mation, rather than constituent-order patterns as is the
case in English and other configurational languages.
MRLs express information concerning the grammati-
cal function of a word and its grammatical relation to
other words at the word level, via phenomena such
as inflectional affixes, pronominal clitics, and so on
(Tsarfaty et al, 2012c).
The non-rigid tree structures and morphological
ambiguity of input words contribute to the challenges
of parsing MRLs. In addition, insufficient language
resources were shown to also contribute to parsing
difficulty (Tsarfaty et al, 2010; Tsarfaty et al, 2012c,
and references therein). These challenges have ini-
tially been addressed by native-speaking experts us-
ing strong in-domain knowledge of the linguistic
phenomena and annotation idiosyncrasies to improve
the accuracy and efficiency of parsing models. More
146
recently, advances in PCFG-LA parsing (Petrov et al,
2006) and language-agnostic data-driven dependency
parsing (McDonald et al, 2005; Nivre et al, 2007b)
have made it possible to reach high accuracy with
classical feature engineering techniques in addition
to, or instead of, language-specific knowledge. With
these recent advances, the time has come for estab-
lishing the state of the art, and assessing strengths
and weaknesses of parsers across different MRLs.
This paper reports on the first shared task on sta-
tistical parsing of morphologically rich languages
(the SPMRL Shared Task), organized in collabora-
tion with the 4th SPMRL meeting and co-located
with the conference on Empirical Methods in Natural
Language Processing (EMNLP). In defining and exe-
cuting this shared task, we pursue several goals. First,
we wish to provide standard training and test sets for
MRLs in different representation types and parsing
scenarios, so that researchers can exploit them for
testing existing parsers across different MRLs. Sec-
ond, we wish to standardize the evaluation protocol
and metrics on morphologically ambiguous input,
an under-studied challenge, which is also present in
English when parsing speech data or web-based non-
standard texts. Finally, we aim to raise the awareness
of the community to the challenges of parsing MRLs
and to provide a set of strong baseline results for
further improvement.
The task features data from nine, typologically di-
verse, languages. Unlike previous shared tasks on
parsing, we include data in both dependency-based
and constituency-based formats, and in addition to
the full data setup (complete training data), we pro-
vide a small setup (a training subset of 5,000 sen-
tences). We provide three parsing scenarios: one in
which gold segmentation, POS tags, and morphologi-
cal features are provided, one in which segmentation,
POS tags, and features are automatically predicted
by an external resource, and one in which we provide
a lattice of multiple possible morphological analyses
and allow for joint disambiguation of the morpholog-
ical analysis and syntactic structure. These scenarios
allow us to obtain the performance upper bound of
the systems in lab settings using gold input, as well
as the expected level of performance in realistic pars-
ing scenarios ? where the parser follows a morpho-
logical analyzer and is a part of a full-fledged NLP
pipeline.
The remainder of this paper is organized as follows.
We first survey previous work on parsing MRLs (?2)
and provide a detailed description of the present task,
parsing scenarios, and evaluation metrics (?3). We
then describe the data sets for the nine languages
(?4), present the different systems (?5), and empiri-
cal results (?6). Then, we compare the systems along
different axes (?7) in order to analyze their strengths
and weaknesses. Finally, we summarize and con-
clude with challenges to address in future shared
tasks (?8).
2 Background
2.1 A Brief History of the SPMRL Field
Statistical parsing saw initial success upon the avail-
ability of the Penn Treebank (PTB, Marcus et al,
1994). With that large set of syntactically annotated
sentences at their disposal, researchers could apply
advanced statistical modeling and machine learning
techniques in order to obtain high quality structure
prediction. The first statistical parsing models were
generative and based on treebank grammars (Char-
niak, 1997; Johnson, 1998; Klein and Manning, 2003;
Collins, 2003; Petrov et al, 2006; McClosky et al,
2006), leading to high phrase-structure accuracy.
Encouraged by the success of phrase-structure
parsers for English, treebank grammars for additional
languages have been developed, starting with Czech
(Hajic? et al, 2000) then with treebanks of Chinese
(Levy and Manning, 2003), Arabic (Maamouri et
al., 2004b), German (K?bler et al, 2006), French
(Abeill? et al, 2003), Hebrew (Sima?an et al, 2001),
Italian (Corazza et al, 2004), Spanish (Moreno et al,
2000), and more. It quickly became apparent that
applying the phrase-based treebank grammar tech-
niques is sensitive to language and annotation prop-
erties, and that these models are not easily portable
across languages and schemes. An exception to that
is the approach by Petrov (2009), who trained latent-
annotation treebank grammars and reported good
accuracy on a range of languages.
The CoNLL shared tasks on dependency parsing
(Buchholz and Marsi, 2006; Nivre et al, 2007a) high-
lighted the usefulness of an alternative linguistic for-
malism for the development of competitive parsing
models. Dependency relations are marked between
input tokens directly, and allow the annotation of
147
non-projective dependencies that are parseable effi-
ciently. Dependency syntax was applied to the de-
scription of different types of languages (Tesni?re,
1959; Mel?c?uk, 2001), which raised the hope that in
these settings, parsing MRLs will further improve.
However, the 2007 shared task organizers (Nivre
et al, 2007a) concluded that: "[Performance] classes
are more easily definable via language characteris-
tics than via characteristics of the data sets. The
split goes across training set size, original data for-
mat [...], sentence length, percentage of unknown
words, number of dependency labels, and ratio of
(C)POSTAGS and dependency labels. The class
with the highest top scores contains languages with
a rather impoverished morphology." The problems
with parsing MRLs have thus not been solved by de-
pendency parsing, but rather, the challenge has been
magnified.
The first event to focus on the particular challenges
of parsing MRLs was a dedicated panel discussion
co-located with IWPT 2009.1 Work presented on
Hebrew, Arabic, French, and German made it clear
that researchers working on non-English parsing face
the same overarching challenges: poor lexical cover-
age (due to high level of inflection), poor syntactic
coverage (due to more flexible word ordering), and,
more generally, issues of data sparseness (due to
the lack of large-scale resources). Additionally, new
questions emerged as to the evaluation of parsers in
such languages ? are the word-based metrics used
for English well-equipped to capture performance
across frameworks, or performance in the face of
morphological complexity? This event provoked ac-
tive discussions and led to the establishment of a
series of SPMRL events for the discussion of shared
challenges and cross-fertilization among researchers
working on parsing MRLs.
The body of work on MRLs that was accumulated
through the SPMRL workshops2 and hosting ACL
venues contains new results for Arabic (Attia et al,
2010; Marton et al, 2013a), Basque (Bengoetxea
and Gojenola, 2010), Croatian (Agic et al, 2013),
French (Seddah et al, 2010; Candito and Seddah,
2010; Sigogne et al, 2011), German (Rehbein, 2011),
Hebrew (Tsarfaty and Sima?an, 2010; Goldberg and
1http://alpage.inria.fr/iwpt09/panel.en.
html
2See http://www.spmrl.org/ and related workshops.
Elhadad, 2010a), Hindi (Ambati et al, 2010), Ko-
rean (Chung et al, 2010; Choi and Palmer, 2011) and
Spanish (Le Roux et al, 2012), Tamil (Green et al,
2012), amongst others. The awareness of the model-
ing challenges gave rise to new lines of work on top-
ics such as joint morpho-syntactic processing (Gold-
berg and Tsarfaty, 2008), Relational-Realizational
Parsing (Tsarfaty, 2010), EasyFirst Parsing (Gold-
berg, 2011), PLCFRS parsing (Kallmeyer and Maier,
2013), the use of factored lexica (Green et al, 2013),
the use of bilingual data (Fraser et al, 2013), and
more developments that are currently under way.
With new models and data, and with lingering in-
terest in parsing non-standard English data, questions
begin to emerge, such as: What is the realistic per-
formance of parsing MRLs using today?s methods?
How do the different models compare with one an-
other? How do different representation types deal
with parsing one particular language? Does the suc-
cess of a parsing model on a language correlate with
its representation type and learning method? How to
parse effectively in the face of resource scarcity? The
first step to answering all of these questions is pro-
viding standard sets of comparable size, streamlined
parsing scenarios, and evaluation metrics, which are
our main goals in this SPMRL shared task.
2.2 Where We Are At: The Need for
Cross-Framework, Realistic, Evaluation
Procedures
The present task serves as the first attempt to stan-
dardize the data sets, parsing scenarios, and evalu-
ation metrics for MRL parsing, for the purpose of
gaining insights into parsers? performance across lan-
guages. Ours is not the first cross-linguistic task on
statistical parsing. As mentioned earlier, two previ-
ous CoNLL shared tasks focused on cross-linguistic
dependency parsing and covered thirteen different
languages (Buchholz and Marsi, 2006; Nivre et al,
2007a). However, the settings of these tasks, e.g.,
in terms of data set sizes or parsing scenarios, made
it difficult to draw conclusions about strengths and
weaknesses of different systems on parsing MRLs.
A key aspect to consider is the relation between
input tokens and tree terminals. In the standard sta-
tistical parsing setup, every input token is assumed
to be a terminal node in the syntactic parse tree (after
deterministic tokenization of punctuation). In MRLs,
148
morphological processes may have conjoined several
words into a single token. Such tokens need to be seg-
mented and their analyses need to be disambiguated
in order to identify the nodes in the parse tree. In
previous shared tasks on statistical parsing, morpho-
logical information was assumed to be known in ad-
vance in order to make the setup comparable to that
of parsing English. In realistic scenarios, however,
morphological analyses are initially unknown and are
potentially highly ambiguous, so external resources
are used to predict them. Incorrect morphological
disambiguation sets a strict ceiling on the expected
performance of parsers in real-world scenarios. Re-
sults reported for MRLs using gold morphological
information are then, at best, optimistic.
One reason for adopting this less-than-realistic
evaluation scenario in previous tasks has been the
lack of sound metrics for the more realistic scenario.
Standard evaluation metrics assume that the number
of terminals in the parse hypothesis equals the num-
ber of terminals in the gold tree. When the predicted
morphological segmentation leads to a different num-
ber of terminals in the gold and parse trees, standard
metrics such as ParsEval (Black et al, 1991) or At-
tachment Scores (Buchholz and Marsi, 2006) fail
to produce a score. In this task, we use TedEval
(Tsarfaty et al, 2012b), a metric recently suggested
for joint morpho-syntactic evaluation, in which nor-
malized tree-edit distance (Bille, 2005) on morpho-
syntactic trees allows us to quantify the success on
the joint task in realistic parsing scenarios.
Finally, the previous tasks focused on dependency
parsing. When providing both constituency-based
and dependency-based tracks, it is interesting to com-
pare results across these frameworks so as to better
understand the differences in performance between
parsers of different types. We are now faced with
an additional question: how can we compare pars-
ing results across different frameworks? Adopting
standard metrics will not suffice as we would be com-
paring apples and oranges. In contrast, TedEval is
defined for both phrase structures and dependency
structures through the use of an intermediate repre-
sentation called function trees (Tsarfaty et al, 2011;
Tsarfaty et al, 2012a). Using TedEval thus allows us
to explore both dependency and constituency parsing
frameworks and meaningfully compare the perfor-
mance of parsers of different types.
3 Defining the Shared-Task
3.1 Input and Output
We define a parser as a structure prediction function
that maps sequences of space-delimited input tokens
(henceforth, tokens) in a language to a set of parse
trees that capture valid morpho-syntactic structures
in that language. In the case of constituency parsing,
the output structures are phrase-structure trees. In de-
pendency parsing, the output consists of dependency
trees. We use the term tree terminals to refer to the
leaves of a phrase-structure tree in the former case
and to the nodes of a dependency tree in the latter.
We assume that input sentences are represented
as sequences of tokens. In general, there may be a
many-to-many relation between input tokens and tree
terminals. Tokens may be identical to the terminals,
as is often the case in English. A token may be
mapped to multiple terminals assigned their own POS
tags (consider, e.g., the token ?isn?t?), as is the case
in some MRLs. Several tokens may be grouped into
a single (virtual) node, as is the case with multiword
expressions (MWEs) (consider ?pomme de terre? for
?potatoe?). This task covers all these cases.
In the standard setup, all tokens are tree terminals.
Here, the task of a parser is to predict a syntactic
analysis in which the tree terminals coincide with the
tokens. Disambiguating the morphological analyses
that are required for parsing corresponds to selecting
the correct POS tag and possibly a set of morpho-
logical features for each terminal. For the languages
Basque, French, German, Hungarian, Korean, Polish,
and Swedish, we assume this standard setup.
In the morphologically complex setup, every token
may be composed of multiple terminals. In this case,
the task of the parser is to predict the sequence of tree
terminals, their POS tags, and a correct tree associ-
ated with this sequence of terminals. Disambiguating
the morphological analysis therefore requires split-
ting the tokens into segments that define the terminals.
For the Semitic languages Arabic and Hebrew, we
assume this morphologically complex setup.
In the multiword expression (MWEs) setup, pro-
vided here for French only, groupings of terminals
are identified as MWEs (non-terminal nodes in con-
stituency trees, marked heads in dependency trees).
Here, the parser is required to predict how terminals
are grouped into MWEs on top of predicting the tree.
149
3.2 Data Sets
The task features nine languages from six language
families, from Germanic languages (Swedish and
German) and Romance (French) to Slavic (Polish),
Koreanic (Korean), Semitic (Arabic, Hebrew), Uralic
(Hungarian), and the language isolate Basque.
These languages cover a wide range of morpho-
logical richness, with Arabic, Basque, and Hebrew
exhibiting a high degree of inflectional and deriva-
tional morphology. The Germanic languages, Ger-
man and Swedish, have greater degrees of phrasal
ordering freedom than English. While French is not
standardly classified as an MRL, it shares MRLs char-
acteristics which pose challenges for parsing, such as
a richer inflectional system than English.
For each contributing language, we provide two
sets of annotated sentences: one annotated with la-
beled phrase-structure trees, and one annotated with
labeled dependency trees. The sentences in the two
representations are aligned at token and POS levels.
Both representations reflect the predicate-argument
structure of the same sentence, but this information
is expressed using different formal terms and thus
results in different tree structures.
Since some of our native data sets are larger than
others, we provide the training set in two sizes: Full
containing all sentences in the standard training set
of the language, and 5k containing the number of
sentences that is equivalent in size to our smallest
training set (5k sentences). For all languages, the data
has been split into sentences, and the sentences are
parsed and evaluated independently of one another.
3.3 Parsing Scenarios
In the shared task, we consider three parsing scenar-
ios, depending on how much of the morphological
information is provided. The scenarios are listed
below, in increasing order of difficulty.
? Gold: In this scenario, the parser is provided
with unambiguous gold morphological segmen-
tation, POS tags, and morphological features for
each input token.
? Predicted: In this scenario, the parser is pro-
vided with disambiguated morphological seg-
mentation. However, the POS tags and mor-
phological features for each input segment are
unknown.
Scenario Segmentation PoS+Feat. Tree
Gold X X ?
Predicted X 1-best ?
Raw (1-best) 1-best 1-best ?
Raw (all) ? ? ?
Table 1: A summary of the parsing and evaluation sce-
narios. X depicts gold information, ? depicts unknown
information, to be predicted by the system.
? Raw: In this scenario, the parser is provided
with morphologically ambiguous input. The
morphological segmentation, POS tags, and
morphological features for each input token are
unknown.
The Predicted and Raw scenarios require predict-
ing morphological analyses. This may be done using
a language-specific morphological analyzer, or it may
be done jointly with parsing. We provide inputs that
support these different scenarios:
? Predicted: Gold treebank segmentation is given
to the parser. The POS tags assignment and mor-
phological features are automatically predicted
by the parser or by an external resource.
? Raw (1-best): The 1st-best segmentation and
POS tags assignment is predicted by an external
resource and given to the parser.
? Raw (all): All possible segmentations and POS
tags are specified by an external resource. The
parser selects jointly a segmentation and a tree.
An overview of all shown in table 1. For languages
in which terminals equal tokens, only Gold and Pre-
dicted scenarios are considered. For Semitic lan-
guages we further provide input for both Raw (1-
best) and Raw (all) scenarios. 3
3.4 Evaluation Metrics
This task features nine languages, two different repre-
sentation types and three different evaluation scenar-
ios. In order to evaluate the quality of the predicted
structures in the different tracks, we use a combina-
tion of evaluation metrics that allow us to compare
the systems along different axes.
3The raw Arabic lattices were made available later than the
other data. They are now included in the shared task release.
150
In this section, we formally define the different
evaluation metrics and discuss how they support sys-
tem comparison. Throughout this paper, we will be
referring to different evaluation dimensions:
? Cross-Parser Evaluation in Gold/Predicted
Scenarios. Here, we evaluate the results of dif-
ferent parsers on a single data set in the Gold
or Predicted setting. We use standard evalu-
ation metrics for the different types of anal-
yses, that is, ParsEval (Black et al, 1991)
on phrase-structure trees, and Labeled At-
tachment Scores (LAS) (Buchholz and Marsi,
2006) for dependency trees. Since ParsEval is
known to be sensitive to the size and depth of
trees (Rehbein and van Genabith, 2007b), we
also provide the Leaf-Ancestor metric (Samp-
son and Babarczy, 2003), which is less sensitive
to the depth of the phrase-structure hierarchy. In
both scenarios we also provide metrics to evalu-
ate the prediction of MultiWord Expressions.
? Cross-Parser Evaluation in Raw Scenarios.
Here, we evaluate the results of different parsers
on a single data set in scenarios where morpho-
logical segmentation is not known in advance.
When a hypothesized segmentation is not iden-
tical to the gold segmentation, standard evalua-
tion metrics such as ParsEval and Attachment
Scores break down. Therefore, we use TedEval
(Tsarfaty et al, 2012b), which jointly assesses
the quality of the morphological and syntactic
analysis in morphologically-complex scenarios.
? Cross-Framework Evaluation. Here, we com-
pare the results obtained by a dependency parser
and a constituency parser on the same set of sen-
tences. In order to avoid comparing apples and
oranges, we use the unlabeled TedEval metric,
which converts all representation types inter-
nally into the same kind of structures, called
function trees. Here we use TedEval?s cross-
framework protocol (Tsarfaty et al, 2012a),
which accomodates annotation idiosyncrasies.
? Cross-Language Evaluation. Here, we com-
pare parsers for the same representation type
across different languages. Conducting a com-
plete and faithful evaluation across languages
would require a harmonized universal annota-
tion scheme (possibly along the lines of (de
Marneffe and Manning, 2008; McDonald et al,
2013; Tsarfaty, 2013)) or task based evaluation.
As an approximation we use unlabeled TedEval.
Since it is unlabeled, it is not sensitive to label
set size. Since it internally uses function-trees,
it is less sensitive to annotation idiosyncrasies
(e.g., head choice) (Tsarfaty et al, 2011).
The former two dimensions are evaluated on the full
sets. The latter two are evaluated on smaller, compa-
rable, test sets. For completeness, we provide below
the formal definitions and essential modifications of
the evaluation software that we used.
3.4.1 Evaluation Metrics for Phrase Structures
ParsEval The ParsEval metrics (Black et al, 1991)
are evaluation metrics for phrase-structure trees. De-
spite various shortcomings, they are the de-facto stan-
dard for system comparison on phrase-structure pars-
ing, used in many campaigns and shared tasks (e.g.,
(K?bler, 2008; Petrov and McDonald, 2012)). As-
sume that G and H are phrase-structure gold and
hypothesized trees respectively, each of which is rep-
resented by a set of tuples (i, A, j) where A is a
labeled constituent spanning from i to j. Assume
that g is the same as G except that it discards the
root, preterminal, and terminal nodes, likewise for h
and H . The ParsEval scores define the accuracy of
the hypothesis in terms of the normalized size of the
intersection of the constituent sets.
Precision(g, h) = |g?h||h|
Recall(g, h) = |g?h||g|
F1(g, h) = 2?P?RP+R
We evaluate accuracy on phrase-labels ignoring any
further decoration, as it is in standard practices.
Evalb, the standard software that implements Par-
sEval,4 takes a parameter file and ignores the labels
specified therein. As usual, we ignore root and POS
labels. Contrary to the standard practice, we do take
punctuation into account. Note that, as opposed to the
official version, we used the SANCL?2012 version5
modified to actually penalize non-parsed trees.
4http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Evalb
5Modified by Petrov and McDonald (2012) to be less sensi-
tive to punctuation errors.
151
Leaf-Ancestor The Leaf-Ancestor metric (Samp-
son and Babarczy, 2003) measures the similarity be-
tween the path from each terminal node to the root
node in the output tree and the corresponding path
in the gold tree. The path consists of a sequence of
node labels between the terminal node and the root
node, and the similarity of two paths is calculated
by using the Levenshtein distance. This distance is
normalized by path length, and the score of the tree
is an aggregated score of the values for all terminals
in the tree (xt is the leaf-ancestor path of t in tree x).
LA(h, g) =
?
t?yield(g) Lv(ht,gt)/(len(ht)+len(gt))
|yield(g)|
This metric was shown to be less sensitive to dif-
ferences between annotation schemes in (K?bler et
al., 2008), and was shown by Rehbein and van Gen-
abith (2007a) to evaluate trees more faithfully than
ParsEval in the face of certain annotation decisions.
We used the implementation of Wagner (2012).6
3.4.2 Evaluation Metrics for Dependency
Structures
Attachment Scores Labeled and Unlabeled At-
tachment scores have been proposed as evaluation
metrics for dependency parsing in the CoNLL shared
tasks (Buchholz and Marsi, 2006; Nivre et al, 2007a)
and have since assumed the role of standard metrics
in multiple shared tasks and independent studies. As-
sume that g, h are gold and hypothesized dependency
trees respectively, each of which is represented by
a set of arcs (i, A, j) where A is a labeled arc from
terminal i to terminal j. Recall that in the gold and
predicted settings, |g| = |h| (because the number of
terminals determines the number of arcs and hence it
is fixed). So Labeled Attachment Score equals preci-
sion and recall, and it is calculated as a normalized
size of the intersection between the sets of gold and
parsed arcs.7
Precision(g, h) = |g?h||g|
Recall(g, h) = |g?h||h|
LAS(g, h) = |g?h||g| =
|g?h|
|h|
6The original version is available at
http://www.grsampson.net/Resources.
html, ours at http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Leaf.
7http://ilk.uvt.nl/conll/software.html.
3.4.3 Evaluation Metrics for Morpho-Syntactic
Structures
TedEval The TedEval metrics and protocols have
been developed by Tsarfaty et al (2011), Tsarfaty
et al (2012a) and Tsarfaty et al (2012b) for coping
with non-trivial evaluation scenarios, e.g., comparing
parsing results across different frameworks, across
representation theories, and across different morpho-
logical segmentation hypotheses.8 Contrary to the
previous metrics, which view accuracy as a normal-
ized intersection over sets, TedEval computes the ac-
curacy of a parse tree based on the tree-edit distance
between complete trees. Assume a finite set of (pos-
sibly parameterized) edit operations A = {a1....an},
and a cost function c : A ? 1. An edit script is the
cost of a sequence of edit operations, and the edit dis-
tance of g, h is the minimal cost edit script that turns
g into h (and vice versa). The normalized distance
subtracted from 1 provides the level of accuracy on
the task. Formally, the TedEval score on g, h is de-
fined as follows, where ted is the tree-edit distance,
and the |x| (size in nodes) discards terminals and root
nodes.
TedEval(g, h) = 1?
ted(g, h)
|g|+ |h|
In the gold scenario, we are not allowed to manipu-
late terminal nodes, only non-terminals. In the raw
scenarios, we can add and delete both terminals and
non-terminals so as to match both the morphological
and syntactic hypotheses.
3.4.4 Evaluation Metrics for
Multiword-Expression Identification
As pointed out in section 3.1, the French data set is
provided with tree structures encoding both syntactic
information and groupings of terminals into MWEs.
A given MWE is defined as a continuous sequence of
terminals, plus a POS tag. In the constituency trees,
the POS tag of the MWE is an internal node of the
tree, dominating the sequence of pre-terminals, each
dominating a terminal. In the dependency trees, there
is no specific node for the MWE as such (the nodes
are the terminals). So, the first token of a MWE is
taken as the head of the other tokens of the same
MWE, with the same label (see section 4.4).
8http://www.tsarfaty.com/unipar/
download.html.
152
To evaluate performance on MWEs, we use the
following metrics.
? R_MWE, P_MWE, and F_MWE are recall, pre-
cision, and F-score over full MWEs, in which
a predicted MWE counts as correct if it has the
correct span (same group as in the gold data).
? R_MWE +POS, R_MWE +POS, and F_MWE
+POS are defined in the same fashion, except
that a predicted MWE counts as correct if it has
both correct span and correct POS tag.
? R_COMP, R_COMP, and F_COMP are recall,
precision and F-score over non-head compo-
nents of MWEs: a non-head component of MWE
counts as correct if it is attached to the head of
the MWE, with the specific label that indicates
that it is part of an MWE.
4 The SPMRL 2013 Data Sets
4.1 The Treebanks
We provide data from nine different languages anno-
tated with two representation types: phrase-structure
trees and dependency trees.9 Statistics about size,
average length, label set size, and other character-
istics of the treebanks and schemes are provided in
Table 2. Phrase structures are provided in an ex-
tended bracketed style, that is, Penn Treebank brack-
eted style where every labeled node may be extended
with morphological features expressed. Dependency
structures are provided in the CoNLL-X format.10
For any given language, the dependency and con-
stituency treebanks are aligned at the token and ter-
minal levels and share the same POS tagset and mor-
phological features. That is, any form in the CoNLL
format is a terminal of the respective bracketed tree.
Any CPOS label in the CoNLL format is the pre-
terminal dominating the terminal in the bracketed
tree. The FEATS in the CoNLL format are repre-
sented as dash-features decorated on the respective
pre-terminal node in the bracketed tree. See Fig-
ure 1(a)?1(b) for an illustration of this alignment.
9Additionally, we provided the data in TigerXML format
(Brants et al, 2002) for phrase structure trees containing cross-
ing branches. This allows the use of more powerful parsing
formalisms. Unfortunately, we received no submissions for this
data, hence we discard them in the rest of this overview.
10See http://ilk.uvt.nl/conll/.
For ambiguous morphological analyses, we pro-
vide the mapping of tokens to different segmentation
possibilities through lattice files. See Figure 1(c) for
an illustration, where lattice indices mark the start
and end positions of terminals.
For each of the treebanks, we provide a three-way
dev/train/set split and another train set containing the
first 5k sentences of train (5k). This section provides
the details of the original treebanks and their anno-
tations, our data-set preparation, including prepro-
cessing and data splits, cross-framework alignment,
and the prediction of morphological information in
non-gold scenarios.
4.2 The Arabic Treebanks
Arabic is a morphologically complex language which
has rich inflectional and derivational morphology. It
exhibits a high degree of morphological ambiguity
due to the absence of the diacritics and inconsistent
spelling of letters, such as Alif and Ya. As a conse-
quence, the Buckwalter Standard Arabic Morpholog-
ical Analyzer (Buckwalter, 2004; Graff et al, 2009)
produces an average of 12 analyses per word.
Data Sets The Arabic data set contains two tree-
banks derived from the LDC Penn Arabic Treebanks
(PATB) (Maamouri et al, 2004b):11 the Columbia
Arabic Treebank (CATiB) (Habash and Roth, 2009),
a dependency treebank, and the Stanford version
of the PATB (Green and Manning, 2010), a phrase-
structure treebank. We preprocessed the treebanks
to obtain strict token matching between the treebanks
and the morphological analyses. This required non-
trivial synchronization at the tree token level between
the PATB treebank, the CATiB treebank and the mor-
phologically predicted data, using the PATB source
tokens and CATiB feature word form as a dual syn-
chronized pivot.
The Columbia Arabic Treebank The Columbia
Arabic Treebank (CATiB) uses a dependency repre-
sentation that is based on traditional Arabic grammar
and that emphasizes syntactic case relations (Habash
and Roth, 2009; Habash et al, 2007). The CATiB
treebank uses the word tokenization of the PATB
11The LDC kindly provided their latest version of the Arabic
Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al,
2005), PATB 2 v3.1 (Maamouri et al, 2004a) and PATB 3 v3.3.
(Maamouri et al, 2009)
153
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
train:
#Sents 15,762 7,577 14,759 40,472 8,146 23,010 6,578
#Tokens 589,220 96,368 443,113 719,532 170,141 351,184 68,424
Lex. Size 36,906 25,136 27,470 77,222 40,782 11,1540 22,911
Avg. Length 37.38 12.71 30.02 17.77 20.88 15.26 10.40
Ratio #NT/#Tokens 0.19 0.82 0.34 0.60 0.59 0.60 0.94
Ratio #NT/#Sents 7.40 10.50 10.33 10.70 12.38 9.27 9.84
#Non Terminals 22 12 32 25 16 8 34
#POS tags 35 25 29 54 16 1,975 29
#total NTs 116,769 79,588 152,463 433,215 100,885 213,370 64,792
Dep. Label Set Size 9 31 25 43 417 22 27
train5k:
#Sents 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000
#Tokens 224,907 61,905 150,984 87,841 128,046 109,987 68,336 52,123 76,357
Lex. Size 19,433 18,405 15,480 17,421 15,975 29,009 29,715 18,632 14,110
Avg. Length 44.98 12.38 30.19 17.56 25.60 21.99 13.66 10.42 15.27
Ratio #NT/#Tokens 0.15 0.83 0.34 0.60 0.42 0.57 0.68 0.94 0.58
Ratio #NT/#Sents 7.18 10.33 10.32 10.58 10.97 12.57 9.29 9.87 8.96
#Non Terminals 22 12 29 23 60 16 8 34 8
#POS Tags 35 25 29 51 50 16 972 29 25
#total NTs 35,909 5,1691 51,627 52,945 54,856 62,889 46,484 49,381 44,845
Dep. Label Set Size 9 31 25 42 43 349 20 27 61
dev:
#Sents 1,985 948 1,235 5,000 500 1,051 2,066 821 494
#Tokens 73,932 13,851 38,820 76,704 11,301 29,989 30,480 8,600 9,341
Lex. Size 12,342 5,551 6,695 15,852 3,175 10,673 15,826 4,467 2,690
Avg. Length 37.24 14.61 31.43 15.34 22.60 28.53 14.75 10.47 18.90
Ratio #NT/#Tokens 0.19 0.74 0.33 0.63 0.47 047 0.63 0.94 0.48
Ratio #NT/#Sents 7.28 10.92 10.48 9.71 10.67 13.66 9.33 9.90 9.10
#Non Terminals 21 11 27 24 55 16 8 31 8
#POS Tags 32 23 29 50 47 16 760 29 24
#total NTs 14,452 10,356 12,951 48,560 5,338 14,366 19,283 8,132 4,496
Dep. Label Set Size 9 31 25 41 42 210 22 26 59
test:
#Sents 1959 946 2541 5000 716 1009 2287 822 666
#Tokens 73878 11457 75216 92004 16998 19908 33766 8545 10690
Lex. Size 12254 4685 10048 20149 4305 7856 16475 4336 3112
Avg. Length 37.71 12.11 29.60 18.40 23.74 19.73 14.76 10.39 16.05
Ratio #NT/#Tokens 0.19 0.83 0.34 0.60 0.47 0.62 0.61 0.95 0.57
Ratio #NT/#Sents 7.45 10.08 10.09 11.07 11.17 12.26 9.02 9.94 9.18
#Non Terminals 22 12 30 23 54 15 8 31 8
#POS Tags 33 22 30 52 46 16 809 27 25
#total NTs 14,610 9,537 25,657 55,398 8,001 12,377 20,640 8,175 6,118
Dep. Label Set Size 9 31 26 42 41 183 22 27 56
Table 2: Overview of participating languages and treebank properties. ?Sents? = number of sentences, ?Tokens? =
number of raw surface forms. ?Lex. size? and ?Avg. Length? are computed in terms of tagged terminals. ?NT? = non-
terminals in constituency treebanks, ?Dep Labels? = dependency labels on the arcs of dependency treebanks. ? A more
comprehensive table is available at http://www.spmrl.org/spmrl2013-sharedtask.html/#Prop.
154
(a) Constituency Tree
% % every line is a single tree in a bracketed Penn Treebank format
(ROOT (S (NP ( NNP-#pers=3|num=sing# John))(VP ( VB-#pers=3|num=sing# likes)(NP ( NNP-#pers=3|num=sing# Mary)))))
(b) Dependency Tree
%% every line describes a terminal: terminal-id form lemma CPOS FPOS FEATS Head Rel PHead PRel
1 John John NNP NNP pers=3|num=sing 2 sbj _ _
2 likes like VB VB pers=3|num=sing 0 root _ _
3 Mary Mary NNP NNP pers=3|num=sing 2 obj _ _
Input Lattice
0 1 2 3 4 5 6
1:AIF/NN
1:AIF/VB
1:AIF/NNT
2:LA/RB
3:NISH/VB
3:NISH/NN
4:L/PREP
4:LHSTIR/VB
4:HSTIR/VB
5:ZAT/PRP
%% every line describes a terminal: start-id end-id form lemma CPOS FPOS FEATS token-id
0 1 AIF AIF NN NN _ 1
0 1 AIF AIF NNT NNT _ 1
0 1 AIF AIF VB VB _ 1
1 2 LA LA RB RB _ 2
2 3 NISH NISH VB VB _ 3
2 3 NISH NISH NN NN _ 3
3 5 LHSTIR HSTIR VB VB _ 4
3 4 L L PREP PREP _ 4
4 5 HSTIR HSTIR VB VB _ 4
5 6 ZAT ZAT PRP PRP _ 5
Figure 1: File formats. Trees (a) and (b) are aligned constituency and dependency trees for a mockup English example.
Boxed labels are shared across the treebanks. Figure (c) shows an ambiguous lattice. The red part represents the yield
of the gold tree. For brevity, we use empty feature columns, but of course lattice arcs may carry any morphological
features, in the FEATS CoNLL format.
and employs a reduced POS tagset consisting of six
tags only: NOM (non-proper nominals including
nouns, pronouns, adjectives and adverbs), PROP
(proper nouns), VRB (active-voice verbs), VRB-
PASS (passive-voice verbs), PRT (particles such as
prepositions or conjunctions) and PNX (punctuation).
(This stands in extreme contrast with the Buckwalter
Arabic tagset (PATB official tagset) which is almost
500 tags.) To obtain these dependency trees, we used
the constituent-to-dependency tool (Habash and Roth,
2009). Additional CATiB trees were annotated di-
rectly, but we only use the portions that are converted
from phrase-structure representation, to ensure that
the constituent and dependency yields can be aligned.
The Stanford Arabic Phrase Structure Treebank
In order to stay compatible with the state of the art,
we provide the constituency data set with most of the
pre-processing steps of Green and Manning (2010),
as they were shown to improve baseline performance
on the PATB parsing considerably.12
To convert the original PATB to preprocessed
phrase-structure trees ? la Stanford, we first discard
all trees dominated by X, which indicates errors and
non-linguistic text. At the phrasal level, we collapse
unary chains with identical categories like NP? NP.
We finally remove all traces, but, unlike Green and
Manning (2010), we keep all function tags.
In the original Stanford instance, the pre-terminal
morphological analyses were mapped to the short-
ened Bies tag set provided with the treebank (where
Determiner markers, ?DT?, were added to definite
noun and adjectives, resulting in 32 POS tags). Here
we use the Kulick tagset (Kulick et al, 2006) for
12Both the corpus split and pre-processing code are available
with the Stanford parser at http://nlp.stanford.edu/
projects/arabic.shtml.
155
pre-terminal categories in the phrase-structure trees,
where the Bies tag set is included as a morphological
feature (stanpos) in our PATB instance.
Adapting the Data to the Shared Task We con-
verted the CATiB representation to the CoNLL rep-
resentation and added a ?split-from-previous? and
?split-from-next? markers as in LDC?s tree-terminal
fields.
A major difference between the CATiB treebank
and the Stanford treebank lies in the way they han-
dle paragraph annotations. The original PATB con-
tains sequences of annotated trees that belong to a
same discourse unit (e.g., paragraph). While the
CATiB conversion tool considers each sequence a
single parsing unit, the Stanford pre-processor treats
each such tree structure rooted at S, NP or Frag as
a tree spanning a single sentence. To be compati-
ble with the predicted morphology data which was
bootstrapped and trained on the CATiB interpretation,
we deterministically modified the original PATB by
adding pseudo XP root nodes, so that the Stanford
pre-proprecessor will generate the same tree yields
as the CATiB treebank.
Another important aspect of preprocessing (often-
delegated as a technicality in the Arabic parsing lit-
erature) is the normalization of token forms. Most
Arabic parsing work used transliterated text based on
the schemes proposed by Buckwalter (2002). The
transliteration schemes exhibit some small differ-
ences, but enough to increase the out-of-vocabulary
rate by a significant margin (on top of strictly un-
known morphemes). This phenomenon is evident in
the morphological analysis lattices (in the predicted
dev set there is a 6% OOV rate without normalization,
and half a point reduction after normalization is ap-
plied, see (Habash et al, 2009b; Green and Manning,
2010)). This rate is much lower for gold tokenized
predicted data (with an OOV rate of only 3.66%,
similar to French for example). In our data set, all
tokens are minimally normalized: no diacritics, no
normalization.13
Data Splits For the Arabic treebanks, we use the
data split recommended by the Columbia Arabic and
Dialect Modeling (CADiM) group (Diab et al, 2013).
13Except for the minimal normalization present in MADA?s
back-end tools. This script was provided to the participants.
The data of the LDC first three annotated Arabic Tree-
banks (ATB1, ATB2 and ATB3) were divided into
roughly a 10/80/10% dev/train/test split by word vol-
ume. When dividing the corpora, document bound-
aries were maintained. The train5k files are simply
the first 5,000 sentences of the training files.
POS Tagsets Given the richness of Arabic mor-
phology, there are multiple POS tag sets and tokeniza-
tion schemes that have been used by researchers, (see,
e.g., Marton et al (2013a)). In the shared task, we fol-
low the standard PATB tokenization which splits off
several categories of orthographic clitics, but not the
definite article Al+. On top of that, we consider three
different POS tag sets with different degrees of gran-
ularity: the Buckwalter tag set (Buckwalter, 2004),
the Kulick Reduced Tag set (Kulick et al, 2006), and
the CATiB tag set (Habash et al, 2009a), considering
that granularity of the morphological analyses may
affect syntactic processing. For more information see
Habash (2010).
Predicted Morphology To prepare input for the
Raw scenarios (?3.3), we used the MADA+TOKAN
system (Habash et al, 2009b). MADA is a system
for morphological analysis and disambiguation of
Arabic. It can predict the 1-best tokenization, POS
tags, lemmas and diacritization in one fell swoop.
The MADA output was also used to generate the
lattice files for the Raw-all scenario.
To generate input for the gold token / predicted
tag input scenario, we used Morfette (Chrupa?a et al,
2008), a joint lemmatization and POS tagging model
based on an averaged perceptron. We generated two
tagging models, one trained with the Buckwalter tag
set, and the other with the Kulick tag set. Both were
mapped back to the CATiB POS tag set such that all
predicted tags are contained in the feature field.14
4.3 The Basque Treebank
Basque is an agglutinative language with a high ca-
pacity to generate inflected wordforms, with free
constituent order of sentence elements with respect
to the main verb. Contrary to many other treebanks,
the Basque treebank was originally annotated with
dependency trees, which were later on converted to
constituency trees.
14A conversion script from the rich Buckwalter tagset to
CoNLL-like features was provided to the participants.
156
The Basque Dependency Treebank (BDT) is a
dependency treebank in its original design, due to
syntactic characteristics of Basque such as its free
word order. Before the syntactic annotation, mor-
phological analysis was performed, using the Basque
morphological analyzer of Aduriz et al (2000). In
Basque each lemma can generate thousands of word-
forms ? differing in morphological properties such
as case, number, tense, or different types of subordi-
nation for verbs. If only POS category ambiguity is
resolved, the analyses remain highly ambiguous.
For the main POS category, there is an average of
1.55 interpretations per wordform, which rises to 2.65
for the full morpho-syntactic information, resulting
in an overall 64% of ambiguous wordforms. The
correct analysis was then manually chosen.
The syntactic trees were manually assigned. Each
word contains its lemma, main POS category, POS
subcategory, morphological features, and the la-
beled dependency relation. Each form indicates mor-
phosyntactic features such as case, number and type
of subordination, which are relevant for parsing.
The first version of the Basque Dependency Tree-
bank, consisting of 3,700 sentences (Aduriz et al,
2003), was used in the CoNLL 2007 Shared Task on
Dependency Parsing (Nivre et al, 2007a). The cur-
rent shared task uses the second version of the BDT,
which is the result of an extension and redesign of the
original requirements, containing 11,225 sentences
(150,000 tokens).
The Basque Constituency Treebank (BCT) was
created as part of the CESS-ECE project, where the
main aim was to obtain syntactically annotated con-
stituency treebanks for Catalan, Spanish and Basque
using a common set of syntactic categories. BCT
was semi-automatically derived from the dependency
version (Aldezabal et al, 2008). The conversion pro-
duced complete constituency trees for 80% of the
sentences. The main bottlenecks have been sentence
connectors and non-projective dependencies which
could not be straightforwardly converted into projec-
tive tree structures, requiring a mechanism similar to
traces in the Penn English Treebank.
Adapting the Data to the Shared Task As the
BCT did not contain all of the original non-projective
dependency trees, we selected the set of 8,000 match-
ing sentences in both treebanks for the shared task.15
This implies that around 2k trees could not be gen-
erated and therefore were discarded. Furthermore,
the BCT annotation scheme does not contain attach-
ment for most of the punctuation marks, so those
were inserted into the BCT using a simple lower-left
attachment heuristic. The same goes for some con-
nectors that could not be aligned in the first phase.
Predicted Morphology In order to obtain pre-
dicted tags for the non-gold scenarios, we used the
following pipeline. First, morphological analysis as
described above was performed, followed by a dis-
ambiguation step. At that point, it is hard to obtain a
single interpretation for each wordform, as determin-
ing the correct interpretation for each wordform may
require knowledge of long-distance elements on top
of the free constituency order of the main phrasal el-
ements in Basque. The disambiguation is performed
by the module by Ezeiza et al (1998), which uses
a combination of knowledge-based disambiguation,
by means of Constraint Grammar (Karlsson et al,
1995; Aduriz et al, 1997), and a posterior statistical
disambiguation module, using an HMM.16
For the shared task data, we chose a setting that
disambiguates most word forms, and retains ? 97%
of the correct interpretations, leaving an ambiguity
level of 1.3 interpretations. For the remaining cases
of ambiguity, we chose the first interpretation, which
corresponds to the most frequent option. This leaves
open the investigation of more complex approaches
for selecting the most appropriate reading.17
4.4 The French Treebank
French is not a morphologically rich language per se,
though its inflectional system is richer than that of
English, and it also exhibits a limited amount of word
order variation occurring at different syntactic levels
including the word level (e.g. pre- or post-nominal
15We generated a 80/10/10 split, ? train/dev/test ? The first 5k
sentences of the train set were used as a basis for the train5k.
16Note that the statistical module can be parametrized accord-
ing to the level of disambiguation to trade off precision and
recall. For example, disambiguation based on the main cate-
gories (abstracting over morpho-syntactic features) maintains
most of the correct interpretations but still gives an output with
several interpretations per wordform.
17This is not an easy task. The ambiguity left is the hardest to
solve given that the knowledge-based and statistical disambigua-
tion processes have not been able to pick out a single reading.
157
adjective, pre- or post-verbal adverbs) and the phrase
level (e.g. possible alternations between post verbal
NPs and PPs). It also has a high degree of multi-
word expressions, that are often ambiguous with a
literal reading as a sequence of simple words. The
syntactic and MWE analysis shows the same kind of
interaction (though to a lesser extent) as morphologi-
cal and syntactic interaction in Semitic languages ?
MWEs help parsing, and syntactic information may
be required to disambiguate MWE identification.
The Data Set The French data sets were gener-
ated from the French Treebank (Abeill? et al, 2003),
which consists of sentences from the newspaper Le
Monde, manually annotated with phrase structures
and morphological information. Part of the treebank
trees are also annotated with grammatical function
tags for dependents of verbs. In the SPMRL shared
task release, we used only this part, consisting of
18,535 sentences,18 split into 14,759 sentences for
training, 1,235 sentences for development, and 2,541
sentences for the final evaluation.19
Adapting the Data to the Shared Task The con-
stituency trees are provided in an extended PTB
bracketed format, with morphological features at the
pre-terminal level only. They contain slight, auto-
matically performed, modifications with respect to
the original trees of the French treebank. The syntag-
matic projection of prepositions and complementiz-
ers was normalized, in order to have prepositions and
complementizers as heads in the dependency trees
(Candito et al, 2010).
The dependency representations are projective de-
pendency trees, obtained through automatic conver-
sion from the constituency trees. The conversion pro-
cedure is an enhanced version of the one described
by Candito et al (2010).
Both the constituency and the dependency repre-
sentations make use of coarse- and fine-grained POS
tags (CPOS and FPOS respectively). The CPOS are
the categories from the original treebank. The FPOS
18The process of functional annotation is still ongoing, the
objective of the FTB providers being to have all the 20000 sen-
tences annotated with functional tags.
19The first 9,981 training sentences correspond to the canoni-
cal 2007 training set. The development set is the same and the
last 1235 sentences of the test set are those of the canonical test
set.
are merged using the CPOS and specific morphologi-
cal information such as verbal mood, proper/common
noun distinction (Crabb? and Candito, 2008).
Multi-Word Expressions The main difference
with respect to previous releases of the bracketed
or dependency versions of the French treebank
lies in the representation of multi-word expressions
(MWEs). The MWEs appear in an extended format:
each MWE bears an FPOS20 and consists of a se-
quence of terminals (hereafter the ?components? of
the MWE), each having their proper CPOS, FPOS,
lemma and morphological features. Note though that
in the original treebank the only gold information
provided for a MWE component is its CPOS. Since
leaving this information blank for MWE components
would have provided a strong cue for MWE recog-
nition, we made sure to provide the same kind of
information for every terminal, whether MWE com-
ponent or not, by providing predicted morphological
features, lemma, and FPOS for MWE components
(even in the ?gold? section of the data set). This infor-
mation was predicted by the Morfette tool (Chrupa?a
et al, 2008), adapted to French (Seddah et al, 2010).
In the constituency trees, each MWE corresponds
to an internal node whose label is the MWE?s FPOS
suffixed by a +, and which dominates the component
pre-terminal nodes.
In the dependency trees, there is no ?node? for a
MWE as a whole, but one node (a terminal in the
CoNLL format) per MWE component. The first com-
ponent of a MWE is taken as the head of the MWE.
All subsequent components of the MWE depend on
the first one, with the special label dep_cpd. Further-
more, the first MWE component bears a feature mwe-
head equal to the FPOS of the MWE. For instance,
the MWE la veille (the day before) is an adverb, con-
taining a determiner component and a common noun
component. Its bracketed representation is (ADV+
(DET la) (NC veille)), and in the dependency repre-
sentation, the noun veille depends on the determiner
la, which bears the feature mwehead=ADV+.
Predicted Morphology For the predicted mor-
phology scenario, we provide data in which the
mwehead has been removed and with predicted
20In the current data, we did not carry along the lemma and
morphological features pertaining to the MWE itself, though this
information is present in the original trees.
158
FPOS, CPOS, lemma, and morphological features,
obtained by training Morfette on the whole train set.
4.5 The German Treebank
German is a fusional language with moderately free
word order, in which verbal elements are fixed in
place and non-verbal elements can be ordered freely
as long as they fulfill the ordering requirements of
the clause (H?hle, 1986).
The Data Set The German constituency data set
is based on the TiGer treebank release 2.2.21 The
original annotation scheme represents discontinuous
constituents such that all arguments of a predicate
are always grouped under a single node regardless of
whether there is intervening material between them
or not (Brants et al, 2002). Furthermore, punctua-
tion and several other elements, such as parentheses,
are not attached to the tree. In order to make the
constituency treebank usable for PCFG parsing, we
adapted this treebank as described shortly.
The conversion of TiGer into dependencies is a
variant of the one by Seeker and Kuhn (2012), which
does not contain empty nodes. It is based on the same
TiGer release as the one used for the constituency
data. Punctuation was attached as high as possible,
without creating any new non-projective edges.
Adapting the Data to the Shared Task For
the constituency version, punctuation and other
unattached elements were first attached to the tree.
As attachment target, we used roughly the respec-
tive least common ancestor node of the right and
left terminal neighbor of the unattached element (see
Maier et al (2012) for details), and subsequently, the
crossing branches were resolved.
This was done in three steps. In the first step, the
head daughters of all nodes were marked using a
simple heuristic. In case there was a daughter with
the edge label HD, this daughter was marked, i.e.,
existing head markings were honored. Otherwise, if
existing, the rightmost daughter with edge label NK
(noun kernel) was marked. Otherwise, as default, the
leftmost daughter was marked. In a second step, for
each continuous part of a discontinuous constituent,
a separate node was introduced. This corresponds
21This version is available from http://www.ims.
uni-stuttgart.de/forschung/ressourcen/
korpora/tiger.html
to the "raising" algorithm described by Boyd (2007).
In a third steps, all those newly introduced nodes
that did not cover the head daughter of the original
discontinuous node were deleted. For the second
and the third step, we used the same script as for the
Swedish constituency data.
Predicted Morphology For the predicted scenario,
a single sequence of POS tags and morphologi-
cal features has been assigned using the MATE
toolchain via a model trained on the train set via cross-
validation on the training set. The MATE toolchain
was used to provide predicted annotation for lem-
mas, POS tags, morphology, and syntax. In order to
achieve the best results for each annotation level, a
10-fold jackknifing was performed to provide realis-
tic features for the higher annotation levels. The pre-
dicted annotation of the 5k training set were copied
from the full data set.22
4.6 The Hebrew Treebank
Modern Hebrew is a Semitic language, characterized
by inflectional and derivational (templatic) morphol-
ogy and relatively free word order. The function
words for from/to/like/and/when/that/the are prefixed
to the next token, causing severe segmentation ambi-
guity for many tokens. In addition, Hebrew orthogra-
phy does not indicate vowels in modern texts, leading
to a very high level of word-form ambiguity.
The Data Set Both the constituency and the de-
pendency data sets are derived from the Hebrew
Treebank V2 (Sima?an et al, 2001; Guthmann et
al., 2009). The treebank is based on just over 6000
sentences from the daily newspaper ?Ha?aretz?, man-
ually annotated with morphological information and
phrase-structure trees and extended with head infor-
mation as described in Tsarfaty (2010, ch. 5). The
unlabeled dependency version was produced by con-
version from the constituency treebank as described
in Goldberg (2011). Both the constituency and depen-
dency trees were annotated with a set grammatical
function labels conforming to Unified Stanford De-
pendencies by Tsarfaty (2013).
22We also provided a predicted-all scenario, in which we
provided morphological analysis lattices with POS and mor-
phological information derived from the analyses of the SMOR
derivational morphology (Schmid et al, 2004). These lattices
were not used by any of the participants.
159
Adapting the Data to the Shared Task While
based on the same trees, the dependency and con-
stituency treebanks differ in their POS tag sets, as
well as in some of the morphological segmentation
decisions. The main effort towards the shared task
was unifying the two resources such that the two tree-
banks share the same lexical yields, and the same
pre-terminal labels. To this end, we took the layering
approach of Goldberg et al (2009), and included two
levels of POS tags in the constituency trees. The
lower level is lexical, conforming to the lexical re-
source used to build the lattices, and is shared by
the two treebanks. The higher level is syntactic, and
follows the tag set and annotation decisions of the
original constituency treebank.23 In addition, we uni-
fied the representation of morphological features, and
fixed inconsistencies and mistakes in the treebanks.
Data Split The Hebrew treebank is one of the
smallest in our language set, and hence it is provided
in only the small (5k) setting. For the sake of com-
parability with the 5k set of the other treebanks, we
created a comparable size of dev/test sets containing
the first and last 500 sentences respectively, where
the rest serve as the 5k training.24
Predicted Morphology The lattices encoding the
morphological ambiguity for the Raw (all) scenario
were produced by looking up the possible analyses
of each input token in the wide-coverage morpholog-
ical analyzer (lexicon) of the Knowledge Center for
Processing Hebrew (Itai and Wintner, 2008; MILA,
2008), with a simple heuristic for dealing with un-
known tokens. A small lattice encoding the possible
analyses of each token was produced separately, and
these token-lattices were concatenated to produce the
sentence lattice. The lattice for a given sentence may
not include the gold analysis in cases of incomplete
lexicon coverage.
The morphologically disambiguated input files for
the Raw (1-best) scenario were produced by run-
ning the raw text through the morphological disam-
23Note that this additional layer in the constituency treebank
adds a relatively easy set of nodes to the trees, thus ?inflating?
the evaluation scores compared to previously reported results.
To compensate, a stricter protocol than is used in this task would
strip one of the two POS layers prior to evaluation.
24This split is slightly different than the split in previous stud-
ies.
biguator (tagger) described in Adler and Elhadad
(2006; Goldberg et al (2008),Adler (2007). The
disambiguator is based on the same lexicon that is
used to produce the lattice files, but utilizes an extra
module for dealing with unknown tokens Adler et al
(2008). The core of the disambiguator is an HMM
tagger trained on about 70M unannotated tokens us-
ing EM, and being supervised by the lexicon.
As in the case of Arabic, we also provided data
for the Predicted (gold token / predicted morphol-
ogy) scenario. We used the same sequence labeler,
Morfette (Chrupa?a et al, 2008), trained on the con-
catenation of POS and morphological gold features,
leading to a model with respectable accuracy.25
4.7 The Hungarian Treebank
Hungarian is an agglutinative language, thus a lemma
can have hundreds of word forms due to derivational
or inflectional affixation (nominal declination and
verbal conjugation). Grammatical information is typ-
ically indicated by suffixes: case suffixes mark the
syntactic relationship between the head and its argu-
ments (subject, object, dative, etc.) whereas verbs
are inflected for tense, mood, person, number, and
the definiteness of the object. Hungarian is also char-
acterized by vowel harmony.26 In addition, there are
several other linguistic phenomena such as causa-
tion and modality that are syntactically expressed in
English but encoded morphologically in Hungarian.
The Data Set The Hungarian data set used in
the shared task is based on the Szeged Treebank,
the largest morpho-syntactic and syntactic corpus
manually annotated for Hungarian. This treebank
is based on newspaper texts and is available in
both constituent-based (Csendes et al, 2005) and
dependency-based (Vincze et al, 2010) versions.
Around 10k sentences of news domain texts were
made available to the shared task.27 Each word is
manually assigned all its possible morpho-syntactic
25POS+morphology prediction accuracy is 91.95% overall
(59.54% for unseen tokens). POS only prediction accuracy is
93.20% overall (71.38% for unseen tokens).
26When vowel harmony applies, most suffixes exist in two
versions ? one with a front vowel and another one with a back
vowel ? and it is the vowels within the stem that determine which
form of the suffix is selected.
27The original treebank contains 82,000 sentences, 1.2 million
words and 250,000 punctuation marks from six domains.
160
tags and lemmas and the appropriate one is selected
according to the context. Sentences were manu-
ally assigned a constituency-based syntactic struc-
ture, which includes information on phrase structure,
grammatical functions (such as subject, object, etc.),
and subcategorization information (i.e., a given NP
is subcategorized by a verb or an infinitive). The
constituency trees were later automatically converted
into dependency structures, and all sentences were
then manually corrected. Note that there exist some
differences in the grammatical functions applied to
the constituency and dependency versions of the tree-
bank, since some morpho-syntactic information was
coded both as a morphological feature and as dec-
oration on top of the grammatical function in the
constituency trees.
Adapting the Data to the Shared Task Origi-
nally, the Szeged Dependency Treebank contained
virtual nodes for elided material (ELL) and phonolog-
ically covert copulas (VAN). In the current version,
they have been deleted, their daughters have been
attached to the parent of the virtual node, and have
been given complex labels, e.g. COORD-VAN-SUBJ,
where VAN is the type of the virtual node deleted,
COORD is the label of the virtual node and SUBJ is
the label of the daughter itself. When the virtual node
was originally the root of the sentence, its daughter
with a predicative (PRED) label has been selected as
the new root of the sentence (with the label ROOT-
VAN-PRED) and all the other daughters of the deleted
virtual node have been attached to it.
Predicted Morphology In order to provide the
same POS tag set for the constituent and dependency
treebanks, we used the dependency POS tagset for
both treebank instances. Both versions of the tree-
bank are available with gold standard and automatic
morphological annotation. The automatic POS tag-
ging was carried out by a 10-fold cross-validation
on the shared task data set by magyarlanc, a natu-
ral language toolkit for processing Hungarian texts
(segmentation, morphological analysis, POS tagging,
and dependency parsing). The annotation provides
POS tags and deep morphological features for each
input token (Zsibrita et al, 2013).28
28The full data sets of both the constituency and de-
pendency versions of the Szeged Treebank are available at
4.8 The Korean Treebank
The Treebank The Korean corpus is generated by
collecting constituent trees from the KAIST Tree-
bank (Choi et al, 1994), then converting the con-
stituent trees to dependency trees using head-finding
rules and heuristics. The KAIST Treebank consists
of about 31K manually annotated constituent trees
from 97 different sources (e.g., newspapers, novels,
textbooks). After filtering out trees containing an-
notation errors, a total of 27,363 trees with 350,090
tokens are collected.
The constituent trees in the KAIST Treebank29 also
come with manually inspected morphological analy-
sis based on ?eojeol?. An eojeol contains root-forms
of word tokens agglutinated with grammatical affixes
(e.g., case particles, ending markers). An eojeol can
consist of more than one word token; for instance, a
compound noun ?bus stop? is often represented as
one eojeol in Korean, ????????????, which can be
broken into two word tokens,???? (bus) and????????
(stop). Each eojeol in the KAIST Treebank is sepa-
rated by white spaces regardless of punctuation. Fol-
lowing the Penn Korean Treebank guidelines (Han
et al, 2002), punctuation is separated as individual
tokens, and parenthetical notations surrounded by
round brackets are grouped into individual phrases
with a function tag (PRN in our corpus).
All dependency trees are automatically converted
from the constituent trees. Unlike English, which
requires complicated head-finding rules to find the
head of each phrase (Choi and Palmer, 2012), Ko-
rean is a head final language such that the rightmost
constituent in each phrase becomes the head of that
phrase. Moreover, the rightmost conjunct becomes
the head of all other conjuncts and conjunctions in
a coordination phrase, which aligns well with our
head-final strategy.
The constituent trees in the KAIST Treebank do
not consist of function tags indicating syntactic or
semantic roles, which makes it difficult to generate
dependency labels. However, it is possible to gener-
ate meaningful labels by using the rich morphology
in Korean. For instance, case particles give good
the following website: www.inf.u-szeged.hu/rgai/
SzegedTreebank, and magyarlanc is downloadable from:
www.inf.u-szeged.hu/rgai/magyarlanc.
29See Lee et al (1997) for more details about the bracketing
guidelines of the KAIST Treebank.
161
indications of what syntactic roles eojeols with such
particles should take. Given this information, 21
dependency labels were generated according to the
annotation scheme proposed by Choi (2013).
Adapting the Data to the Shared Task All details
concerning the adaptation of the KAIST treebank
to the shared task specifications are found in Choi
(2013). Importantly, the rich KAIST treebank tag set
of 1975 POS tag types has been converted to a list of
CoNLL-like feature-attribute values refining coarse
grained POS categories.
Predicted Morphology Two sets of automatic
morphological analyses are provided for this task.
One is generated by the HanNanum morphological
analyzer.30 The HanNanum morphological ana-
lyzer gives the same morphemes and POS tags as the
KAIST Treebank. The other is generated by the Se-
jong morphological analyzer.31 The Sejong morpho-
logical analyzer gives a different set of morphemes
and POS tags as described in Choi and Palmer (2011).
4.9 The Polish Treebank
The Data Set Sk?adnica is a constituency treebank
of Polish (Wolin?ski et al, 2011; S?widzin?ski and
Wolin?ski, 2010). The trees were generated with
a non-probabilistic DCG parser S?wigra and then
disambiguated and validated manually. The ana-
lyzed texts come from the one-million-token sub-
corpus of the National Corpus of Polish (NKJP,
(Przepi?rkowski et al, 2012)) manually annotated
with morpho-syntactic tags.
The dependency version of Sk?adnica is a re-
sult of an automatic conversion of manually disam-
biguated constituent trees into dependency structures
(Wr?blewska, 2012). The conversion was an entirely
automatic process. Conversion rules were based
on morpho-syntactic information, phrasal categories,
and types of phrase-structure rules encoded within
constituent trees. It was possible to extract dependen-
cies because the constituent trees contain information
about the head of the majority of constituents. For
other constituents, heuristics were defined in order to
select their heads.
30http://kldp.net/projects/hannanum
31http://www.sejong.or.kr
The version of Sk?adnica used in the shared task
comprises parse trees for 8,227 sentences.32
Predicted Morphology For the shared task Pre-
dicted scenario, an automatic morphological an-
notation was generated by the PANTERA tagger
(Acedan?ski, 2010).
4.10 The Swedish Treebank
Swedish is moderately rich in inflections, including
a case system. Word order obeys the verb second
constraint in main clauses but is SVO in subordinate
clauses. Main clause order is freer than in English
but not as free as in some other Germanic languages,
such as German. Also, subject agreement with re-
spect to person and number has been dropped in
modern Swedish.
The Data Set The Swedish data sets are taken
from the Talbanken section of the Swedish Treebank
(Nivre and Megyesi, 2007). Talbanken is a syntacti-
cally annotated corpus developed in the 1970s, orig-
inally annotated according to the MAMBA scheme
(Teleman, 1974) with a syntactic layer consisting
of flat phrase structure and grammatical functions.
The syntactic annotation was later automatically con-
verted to full phrase structure with grammatical func-
tions and from that to dependency structure, as de-
scribed by Nivre et al (2006).
Both the phrase structure and the dependency
version use the functional labels from the original
MAMBA scheme, which provides a fine-grained clas-
sification of syntactic functions with 65 different la-
bels, while the phrase structure annotation (which
had to be inferred automatically) uses a coarse set
of only 8 labels. For the release of the Swedish tree-
bank, the POS level was re-annotated to conform to
the current de facto standard for Swedish, which is
the Stockholm-Ume? tagset (Ejerhed et al, 1992)
with 25 base tags and 25 morpho-syntactic features,
which together produce over 150 complex tags.
For the shared task, we used version 1.2 of the
treebank, where a number of conversion errors in
the dependency version have been corrected. The
phrase structure version was enriched by propagating
morpho-syntactic features from preterminals (POS
32Sk?adnica is available from http://zil.ipipan.waw.
pl/Sklicense.
162
tags) to higher non-terminal nodes using a standard
head percolation table, and a version without crossing
branches was derived using the lifting strategy (Boyd,
2007).
Adapting the Data to the Shared Task Explicit
attribute names were added to the feature field and the
split was changed to match the shared task minimal
training set size.
Predicted Morphology POS tags and morpho-
syntactic features were produced using the Hun-
PoS tagger (Hal?csy et al, 2007) trained on the
Stockholm-Ume? Corpus (Ejerhed and K?llgren,
1997).
5 Overview of the Participating Systems
With 7 teams participating, more than 14 systems for
French and 10 for Arabic and German, this shared
task is on par with the latest large-scale parsing evalu-
ation campaign SANCL 2012 (Petrov and McDonald,
2012). The present shared task was extremely de-
manding on our participants. From 30 individuals or
teams who registered and obtained the data sets, we
present results for the seven teams that accomplished
successful executions on these data in the relevant
scenarios in the given the time frame.
5.1 Dependency Track
Seven teams participated in the dependency track.
Two participating systems are based on MaltParser:
MALTOPTIMIZER (Ballesteros, 2013) and AI:KU
(Cirik and S?ensoy, 2013). MALTOPTIMIZER uses
a variant of MaltOptimizer (Ballesteros and Nivre,
2012) to explore features relevant for the processing
of morphological information. AI:KU uses a combi-
nation of MaltParser and the original MaltOptimizer.
Their system development has focused on the inte-
gration of an unsupervised word clustering method
using contextual and morphological properties of the
words, to help combat sparseness.
Similarly to MaltParser ALPAGE:DYALOG
(De La Clergerie, 2013) also uses a shift-reduce
transition-based parser but its training and decoding
algorithms are based on beam search. This parser is
implemented on top of the tabular logic programming
system DyALog. To the best of our knowledge, this
is the first dependency parser capable of handling
word lattice input.
Three participating teams use the MATE parser
(Bohnet, 2010) in their systems: the BASQUETEAM
(Goenaga et al, 2013), IGM:ALPAGE (Constant et
al., 2013) and IMS:SZEGED:CIS (Bj?rkelund et al,
2013). The BASQUETEAM uses the MATE parser in
combination with MaltParser (Nivre et al, 2007b).
The system combines the parser outputs via Malt-
Blender (Hall et al, 2007). IGM:ALPAGE also uses
MATE and MaltParser, once in a pipeline architec-
ture and once in a joint model. The models are com-
bined via a re-parsing strategy based on (Sagae and
Lavie, 2006). This system mainly focuses on MWEs
in French and uses a CRF tagger in combination
with several large-scale dictionaries to handle MWEs,
which then serve as input for the two parsers.
The IMS:SZEGED:CIS team participated in both
tracks, with an ensemble system. For the depen-
dency track, the ensemble includes the MATE parser
(Bohnet, 2010), a best-first variant of the easy-first
parser by Goldberg and Elhadad (2010b), and turbo
parser (Martins et al, 2010), in combination with
a ranker that has the particularity of using features
from the constituent parsed trees. CADIM (Marton et
al., 2013b) uses their variant of the easy-first parser
combined with a feature-rich ensemble of lexical and
syntactic resources.
Four of the participating teams use exter-
nal resources in addition to the parser. The
IMS:SZEGED:CIS team uses external morpholog-
ical analyzers. CADIM uses SAMA (Graff et al,
2009) for Arabic morphology. ALPAGE:DYALOG
and IGM:ALPAGE use external lexicons for French.
IGM:ALPAGE additionally uses Morfette (Chrupa?a
et al, 2008) for morphological analysis and POS
tagging. Finally, as already mentioned, AI:KU clus-
ters words and POS tags in an unsupervised fashion
exploiting additional, un-annotated data.
5.2 Constituency Track
A single team participated in the constituency parsing
task, the IMS:SZEGED:CIS team (Bj?rkelund et al,
2013). Their phrase-structure parsing system uses a
combination of 8 PCFG-LA parsers, trained using a
product-of-grammars procedure (Petrov, 2010). The
50-best parses of this combination are then reranked
by a model based on the reranker by Charniak and
163
Johnson (2005).33
5.3 Baselines
We additionally provide the results of two baseline
systems for the nine languages, one for constituency
parsing and one for dependency parsing.
For the dependency track, our baseline system is
MaltParser in its default configuration (the arc-eager
algorithm and liblinear for training). Results marked
as BASE:MALT in the next two sections report the
results of this baseline system in different scenarios.
The constituency parsing baseline is based on the
most recent version of the PCFG-LA model of Petrov
et al (2006), used with its default settings and five
split/merge cycles, for all languages.34 We use this
parser in two configurations: a ?1-best? configura-
tion where all POS tags are provided to the parser
(predicted or gold, depending on the scenario), and
another configuration in which the parser performs
its own POS tagging. These baselines are referred to
as BASE:BKY+POS and BASE:BKY+RAW respec-
tively in the following results sections. Note that
even when BASE:BKY+POS is given gold POS tags,
the Berkeley parser sometimes fails to reach a perfect
POS accuracy. In cases when the parser cannot find a
parse with the provided POS, it falls back on its own
POS tagging for all tokens.
6 Results
The high number of submitted system variants and
evaluation scenarios in the task resulted in a large
number of evaluation scores. In the following evalu-
ation, we focus on the best run for each participant,
and we aim to provide key points on the different
dimensions of analysis resulting from our evaluation
protocol. We invite our interested readers to browse
the comprehensive representation of our results on
the official shared-task results webpages.35
33Note that a slight but necessary change in the configuration
of one of our metrics, which occurred after the system submis-
sion deadline, resulted in the IMS:SZEGED:CIS team to submit
suboptimal systems for 4 languages. Their final scores are ac-
tually slightly higher and can be found in (Bj?rkelund et al,
2013).
34For Semitic languages, we used the lattice based PCFG-LA
extension by Goldberg (2011).
35http://www.spmrl.org/
spmrl2013-sharedtask-results.html.
6.1 Gold Scenarios
This section presents the parsing results in gold sce-
narios, where the systems are evaluated on gold seg-
mented and tagged input. This means that the se-
quence of terminals, POS tags, and morphological
features are provided based on the treebank anno-
tations. This scenario was used in most previous
shared tasks on data-driven parsing (Buchholz and
Marsi, 2006; Nivre et al, 2007a; K?bler, 2008). Note
that this scenario was not mandatory. We thank our
participants for providing their results nonetheless.
We start by reviewing dependency-based parsing
results, both on the trees and on multi-word expres-
sion, and continue with the different metrics for
constituency-based parsing.
6.1.1 Dependency Parsing
Full Training Set The results for the gold parsing
scenario of dependency parsing are shown in the top
block of table 3.
Among the six systems, IMS:SZEGED:CIS
reaches the highest LAS scores, not only on aver-
age, but for every single language. This shows that
their approach of combining parsers with (re)ranking
provides robust parsing results across languages with
different morphological characteristics. The second
best system is ALPAGE:DYALOG, the third best sys-
tem is MALTOPTIMIZER. The fact that AI:KU is
ranked below the Malt baseline is due to their sub-
mission of results for 6 out of the 9 languages. Simi-
larly, CADIM only submitted results for Arabic and
ranked in the third place for this language, after the
two IMS:SZEGED:CIS runs. IGM:ALPAGE and
BASQUETEAM did not submit results for this setting.
Comparing LAS results across languages is prob-
lematic due to the differences between languages,
treebank size and annotation schemes (see section 3),
so the following discussion is necessarily tentative. If
we consider results across languages, we see that the
lowest results (around 83% for the best performing
system) are reached for Hebrew and Swedish, the
languages with the smallest data sets. The next low-
est result, around 86%, is reached for Basque. Other
languages reach similar LAS scores, around 88-92%.
German, with the largest training set, reaches the
highest LAS, 91.83%.
Interstingly, all systems have high LAS scores
on the Korean Treebank given a training set size
164
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 89.83 86.68 90.29 91.83 83.87 88.06 89.59 89.58 83.97 88.19
ALPAGE:DYALOG 85.87 80.39 87.69 88.25 80.70 79.60 88.23 86.00 79.80 84.06
MALTOPTIMIZER 87.03 82.07 85.71 86.96 80.03 83.14 89.39 80.49 77.67 83.61
BASE:MALT 82.28 69.19 79.86 79.98 76.61 72.34 88.43 77.70 75.73 78.01
AI:KU 86.39 86.98 79.42 83.67 85.16 78.87 55.61
CADIM 85.56 9.51
2) gold setting / 5k training set
IMS:SZEGED:CIS 87.35 85.69 88.73 87.70 83.87 87.21 83.38 89.16 83.97 86.34
ALPAGE:DYALOG 83.25 79.11 85.66 83.88 80.70 78.42 81.91 85.67 79.80 82.04
MALTOPTIMIZER 85.30 81.40 84.93 83.59 80.03 82.37 83.74 79.79 77.67 82.09
BASE:MALT 80.36 67.13 78.16 76.64 76.61 71.27 81.93 76.64 75.73 76.05
AI:KU 84.98 83.47 79.42 82.84 84.37 78.87 54.88
CADIM 82.67 9.19
3) predicted setting / full training set
IMS:SZEGED:CIS 86.21 85.14 85.24 89.65 80.89 86.13 86.62 87.07 82.13 85.45
ALPAGE:DYALOG 81.20 77.55 82.06 84.80 73.63 75.58 81.02 82.56 77.54 79.55
MALTOPTIMIZER 81.90 78.58 79.00 82.75 73.01 79.63 82.65 79.89 75.82 79.25
BASE:MALT 80.36 70.11 77.98 77.81 69.97 70.15 82.06 75.63 73.21 75.25
AI:KU 72.57 82.32 69.01 78.92 81.86 76.35 51.23
BASQUETEAM 84.25 84.51 88.66 84.97 80.88 47.03
IGM:ALPAGE 85.86 9.54
CADIM 83.20 9.24
4) predicted setting / 5k training set
IMS:SZEGED:CIS 83.66 83.84 83.45 85.08 80.89 85.24 80.80 86.69 82.13 83.53
MALTOPTIMIZER 79.64 77.59 77.56 79.22 73.01 79.00 75.90 79.50 75.82 77.47
ALPAGE:DYALOG 78.65 76.06 80.11 73.07 73.63 74.48 73.79 82.04 77.54 76.60
BASE:MALT 78.48 68.12 76.54 74.81 69.97 69.08 74.87 75.29 73.21 73.37
AI:KU 71.23 79.16 69.01 78.04 81.30 76.35 50.57
BASQUETEAM 83.19 82.65 84.70 84.01 80.88 46.16
IGM:ALPAGE 83.60 9.29
CADIM 80.51 8.95
Table 3: Dependency parsing: LAS scores for full and 5k training sets and for gold and predicted input. Results in bold
show the best results per language and setting.
of approximately 23,000 sentences, which is a little
over half of the German treebank. For German, on
the other hand, only the IMS:SZEGED:CIS system
reaches higher LAS scores than for Korean. This
final observation indicates that more than treebank
size is important for comparing system performance
across treebanks. This is the reason for introducing
the reduced set scenario, in which we can see how the
participating system perform on a common ground,
albeit small.
5k Training Set The results for the gold setting
on the 5k train set are shown in the second block
of Table 3. Compared with the full training, we
see that there is a drop of around 2 points in this
setting. Some parser/language pairs are more sensi-
tive to data sparseness than others. CADIM, for in-
stance, exhibit a larger drop than MALTOPTIMIZER
on Arabic, and MALTOPTIMIZER shows a smaller
drop than IMS:SZEGED:CIS on French. On average,
among all systems that covered all languages, MALT-
OPTIMIZER has the smallest drop when moving to
5k training, possibly since the automatic feature opti-
mization may differ for different data set sizes.
Since all languages have the same number of sen-
tences in the train set, these results can give us limited
insight into the parsing complexity of the different
treebanks. Here, French, Arabic, Polish, and Korean
reach the highest LAS scores while Swedish reaches
165
Team F_MWE F_COMP F_MWE+POS
1) gold setting / full training set
AI:KU 99.39 99.53 99.34
IMS:SZEGED:CIS 99.26 99.39 99.21
MALTOPTIMIZER 98.95 98.99 0
ALPAGE:DYALOG 98.32 98.81 0
BASE:MALT 68.7 72.55 68.7
2) predicted setting / full training set
IGM:ALPAGE 80.81 81.18 77.37
IMS:SZEGED:CIS 79.45 80.79 70.48
ALPAGE:DYALOG 77.91 79.25 0
BASQUE-TEAM 77.19 79.81 0
MALTOPTIMIZER 70.29 74.25 0
BASE:MALT 67.49 71.01 0
AI:KU 0 0 0
3) predicted setting / 5k training set
IGM:ALPAGE 77.66 78.68 74.04
IMS:SZEGED:CIS 77.28 78.92 70.42
ALPAGE:DYALOG 75.17 76.82 0
BASQUETEAM 73.07 76.58 0
MALTOPTIMIZER 65.76 70.42 0
BASE:MALT 62.05 66.8 0
AI:KU 0 0 0
Table 4: Dependency Parsing: MWE results
the lowest one. Treebank variance depends not only
on the language but also on annotation decisions,
such as label set (Swedish, interestingly, has a rela-
tively rich one). A more careful comparison would
then take into account the correlation of data size,
label set size and parsing accuracy. We investigate
these correlations further in section 7.1.
6.1.2 Multiword Expressions
MWE results on the gold setting are found at
the top of Table 4. All systems, with the excep-
tion of BASE:MALT, perform exceedingly well in
identifying the spans and non-head components of
MWEs given gold morphology.36 These almost per-
fect scores are the consequence of the presence of
two gold MWE features, namely MWEHEAD and
PRED=Y, which respectively indicate the node span
of the whole MWE and its dependents, which do not
have a gold feature field. The interesting scenario is,
of course, the predicted one, where these features are
not provided to the parser, as in any realistic applica-
tion.
36Note that for the labeled measure F_MWE+POS, both
MALTOPTIMIZER and ALPAGE:DYALOG have an F-score of
zero, since they do not attempt to predict the MWE label at all.
6.1.3 Constituency Parsing
In this part, we provide accuracy results for phrase-
structure trees in terms of ParsEval F-scores. Since
ParsEval is sensitive to the non-terminals-per-word
ratio in the data set (Rehbein and van Genabith,
2007a; Rehbein and van Genabith, 2007b), and given
the fact that this ratio varies greatly within our data
set (as shown in Table 2), it must be kept in mind that
ParsEval should only be used for comparing parsing
performance over treebank instances sharing the ex-
act same properties in term of annotation schemes,
sentence length and so on. When comparing F-Scores
across different treebanks and languages, it can only
provide a rough estimate of the relative difficulty or
ease of parsing these kinds of data.
Full Training Set The F-score results for the gold
scenario are provided in the first block of Table 5.
Among the two baselines, BASE:BKY+POS fares
better than BASE:BKY+RAW since the latter selects
its own POS tags and thus cannot benefit from the
gold information. The IMS:SZEGED:CIS system
clearly outperforms both baselines, with Hebrew as
an outlier.37
As in the dependency case, the results are not
strictly comparable across languages, yet we can
draw some insights from them. We see consider-
able differences between the languages, with Basque,
Hebrew, and Hungarian reaching F-scores in the low
90s for the IMS:SZEGED:CIS system, Korean and
Polish reaching above-average F-scores, and Ara-
bic, French, German, and Swedish reaching F-scores
below the average, but still in the low 80s. The per-
formance is, again, not correlated with data set sizes.
Parsing Hebrew, with one of the smallest training
sets, obtains higher accuracy many other languages,
including Swedish, which has the same training set
size as Hebrew. It may well be that gold morphologi-
cal information is more useful for combatting sparse-
ness in languages with richer morphology (though
Arabic here would be an outlier for this conjecture),
or it may be that certain treebanks and schemes are
inherently harder to parser than others, as we investi-
gate in section 7.
For German, the language with the largest training
37It might be that the easy layer of syntactic tags benefits from
the gold POS tags provided. See section 4 for further discussion
of this layer.
166
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 82.20 90.04 83.98 82.07 91.64 92.60 86.50 88.57 85.09 86.97
BASE:BKY+POS 80.76 76.24 81.76 80.34 92.20 87.64 82.95 88.13 82.89 83.66
BASE:BKY+RAW 79.14 69.78 80.38 78.99 87.32 81.44 73.28 79.51 78.94 78.75
2) gold setting / 5k training set
IMS:SZEGED:CIS 79.47 88.45 82.25 74.78 91.64 91.87 80.10 88.18 85.09 84.65
BASE:BKY+POS 77.54 74.06 78.07 71.37 92.20 86.74 72.85 87.91 82.89 80.40
BASE:BKY+RAW 75.22 67.16 75.91 68.94 87.32 79.34 60.40 78.30 78.94 74.61
3) predicted setting / full training set
IMS:SZEGED:CIS 81.32 87.86 81.83 81.27 89.46 91.85 84.27 87.55 83.99 85.49
BASE:BKY+POS 78.66 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 80.89
BASE:BKY+RAW 79.19 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.18 78.53
4) predicted setting / 5k training set
IMS:SZEGED:CIS 78.85 86.65 79.83 73.61 89.46 90.53 78.47 87.46 83.99 83.21
BASE:BKY+POS 74.84 72.35 76.19 69.40 85.42 83.82 67.97 87.17 80.64 77.53
BASE:BKY+RAW 74.57 66.75 75.76 68.68 86.96 79.35 58.49 78.38 79.18 74.24
Table 5: Constituent Parsing: ParsEval F-scores for full and 5k training sets and for gold and predicted input. Results in
bold show the best results per language and setting.
set and the highest scores in dependency parsing,
the F-scores are at the lower end. These low scores,
which are obtained despite the larger treebank and
only moderately free word-order, are surprising. This
may be due to case syncretism; gold morphological
information exhibits its own ambiguity and thus may
not be fully utilized.
5k Training Set Parsing results on smaller com-
parable test sets are presented in the second block
of Table 5. On average, IMS:SZEGED:CIS is less
sensitive than BASE:BKY+POS to the reduced size.
Systems are not equally sensitive to reduced training
sets, and the gaps range from 0.4% to 3%, with Ger-
man and Korean as outliers (Korean suffering a 6.4%
drop in F-score and German 7.3%). These languages
have the largest treebanks in the full setting, so it is
not surprising that they suffer the most. But this in
itself does not fully explain the cross-treebank trends.
Since ParsEval scores are known to be sensitive to
the label set sizes and the depth of trees, we provide
LeafAncestor scores in the following section.
6.1.4 Leaf-Ancestor Results
The variation across results in the previous subsec-
tion may have been due to differences across annota-
tion schemes. One way to neutralize this difference
(to some extent) is to use a different metric. We
evaluated the constituency parsing results using the
Leaf-Ancestor (LA) metric, which is less sensitive
to the number of nodes in a tree (Rehbein and van
Genabith, 2007b; K?bler et al, 2008). As shown in
Table 6, these results are on a different (higher) scale
than ParsEval, and the average gap between the full
and 5k setting is lower.
Full Training Set The LA results in gold setting
for full training sets are shown in the first block of Ta-
ble 6. The trends are similar to the ParsEval F-scores.
German and Arabic present the lowest LA scores
(in contrast to the corresponding F-scores, Arabic is
a full point below German for IMS:SZEGED:CIS).
Basque and Hungarian have the highest LA scores.
Hebrew, which had a higher F-score than Basque,
has a lower LA than Basque and is closer to French.
Korean also ranks worse in the LA analysis. The
choice of evaluation metrics thus clearly impacts sys-
tem rankings ? F-scores rank some languages suspi-
ciously high (e.g., Hebrew) due to deeper trees, and
another metric may alleviate that.
5k Training Set The results for the leaf-ancestor
(LA) scores in the gold setting for the 5k training set
are shown in the second block of Table 6. Across
167
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 88.61 94.90 92.51 89.63 92.84 95.01 91.30 94.52 91.46 92.31
BASE:BKY+POS 87.85 91.55 91.74 88.47 92.69 92.52 90.82 92.81 90.76 91.02
BASE:BKY+RAW 87.05 89.71 91.22 87.77 91.29 90.62 87.11 90.58 88.97 89.37
2) gold setting / 5k training set
IMS:SZEGED:CIS 86.68 94.21 91.56 85.74 92.84 94.79 88.87 94.17 91.46 91.15
BASE:BKY+POS 86.26 90.72 89.71 84.11 92.69 92.11 86.75 92.91 90.76 89.56
BASE:BKY+RAW 84.97 88.68 88.74 83.08 91.29 89.94 81.82 90.31 88.97 87.53
3) predicted setting / full training set
IMS:SZEGED:CIS 88.45 94.50 91.79 89.32 91.95 94.90 90.13 94.11 91.05 91.80
BASE:BKY+POS 86.60 90.90 90.96 87.46 89.66 91.72 89.10 92.56 89.51 89.83
BASE:BKY+RAW 86.97 89.91 91.11 87.46 90.77 90.50 86.68 90.48 89.16 89.23
4) predicted setting / 5k training set
IMS:SZEGED:CIS 86.69 93.85 90.76 85.20 91.95 94.05 87.99 93.99 91.05 90.61
BASE:BKY+POS 84.76 89.83 89.18 83.05 89.66 91.24 84.87 92.74 89.51 88.32
BASE:BKY+RAW 84.63 88.50 89.00 82.69 90.77 89.93 81.50 90.08 89.16 87.36
Table 6: Constituent Parsing: Leaf-Ancestor scores for full and 5k training sets and for gold and predicted input.
parsers, IMS:SZEGED:CIS again has a smaller drop
than BASE:BKY+POS on the reduced size. German
suffers the most from the reduction of the training
set, with a loss of approximately 4 points. Korean,
however, which was also severely affected in terms
of F-scores, only loses 1.17 points in the LA score.
On average, the LA seem to reflect a smaller drop
when reducing the training set ? this underscores
again the impact of the choice of metrics on system
evaluation.
6.2 Predicted Scenarios
Gold scenarios are relatively easy since syntactically
relevant morphological information is disambiguated
in advance and is provided as input. Predicted scenar-
ios are more difficult: POS tags and morphological
features have to be automatically predicted, by the
parser or by external resources.
6.2.1 Dependency Parsing
Eight participating teams submitted dependency
results for this scenario. Two teams submitted for a
single language. Four teams covered all languages.
Full Training Set The results for the predicted
scenario in full settings are shown in the third
block of Table 3. Across the board, the re-
sults are considerably lower than the gold sce-
nario. Again, IMS:SZEGED:CIS is the best per-
forming system, followed by ALPAGE:DYALOG and
MALTOPTIMIZER. The only language for which
IMS:SZEGED:CIS is outperformed is French, for
which IGM:ALPAGE reaches higher results (85.86%
vs. 85.24%). This is due to the specialized treatment
of French MWEs in the IGM:ALPAGE system, which
is thereby shown to be beneficial for parsing in the
predicted setting.
If we compare the results for the predicted set-
ting and the gold one, given the full training set,
the IMS:SZEGED:CIS system shows small differ-
ences between 1.5 and 2 percent. The only ex-
ception is French, for which the LAS drops from
90.29% to 85.24% in the predicted setting. The
other systems show somewhat larger differences than
IMS:SZEGED:CIS, with the highest drops for Ara-
bic and Korean. The AI:KU system shows a similar
problem as IMS:SZEGED:CIS for French.
5k Training Set When we consider the predicted
setting for the 5k training set, in the last block of
Table 3, we see the same trends as comparing with
the full training set or when comparing to the gold
setting. Systems suffer from not having gold stan-
dard data, and they suffer from the small training set.
Interestingly, the loss between the different training
set sizes in the predicted setting is larger than in the
168
gold setting, but only marginally so, with a differ-
ence < 0.5. In other words, the predicted setting
adds a challenge to parsing, but it only minimally
compounds data sparsity.
6.2.2 Multiword Expressions Evaluation
In the predicted setting, shown in the second
block of table 4 for the full training set and in the
third block of the same table for the 5k training set,
we see that only two systems, IGM:ALPAGE and
IMS:SZEGED:CIS can predict the MWE label when
it is not present in the training set. IGM:ALPAGE?s
approach of using a separate classifier in combination
with external dictionaries is very successful, reach-
ing an F_MWE+POS score of 77.37. This is com-
pared to the score of 70.48 by IMS:SZEGED:CIS,
which predicts this node label as a side effect of
their constituent feature enriched dependency model
(Bj?rkelund et al, 2013). AI:KU has a zero score
for all predicted settings, which results from an erro-
neous training on the gold data rather than the pre-
dicted data.38
6.2.3 Constituency Parsing
Full Training Set The results for the predicted set-
ting with the full training set are shown in the third
block of table 5. A comparison with the gold setting
shows that all systems have a lower performance in
the predicted scenario, and the differences are in the
range of 0.88 for Arabic and 2.54 for Basque. It is
interesting to see that the losses are generally smaller
than in the dependency framework: on average, the
loss across languages is 2.74 for dependencies and
1.48 for constituents. A possible explanation can be
found in the two-dimensional structure of the con-
stituent trees, where only a subset of all nodes is
affected by the quality of morphology and POS tags.
The exception to this trend is Basque, for which the
loss in constituents is a full point higher than for de-
pendencies. Another possible explanation is that all
of our constituent parsers select their own POS tags
in one way or another. Most dependency parsers ac-
cept predicted tags from an external resource, which
puts an upper-bound on their potential performance.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the bottom
38Unofficial updated results are to to be found in (Cirik and
S?ensoy, 2013)
block of table 5. They show the same trends as the
dependency ones: The results are slightly lower than
the results obtained in gold setting and the ones uti-
lizing the full training set.
6.2.4 Leaf Ancestor Metrics
Full Training Set The results for the predicted sce-
nario with a full training set are shown in the third
block of table 6. In the LA evaluation, the loss
in moving from gold morphology are considerably
smaller than in F-scores. For most languages, the
loss is less than 0.5 points. Exceptions are French
with a loss of 0.72, Hebrew with 0.89, and Korean
with 1.17. Basque, which had the highest loss in
F-scores, only shows a minor loss of 0.4 points. Also,
the average loss of 0.41 points is much smaller than
the one in the ParsEval score, 1.48.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the last
block of table 6. These results, though considerably
lower (around 3 points), exhibit the exact same trends
as observed in the gold setting.
6.3 Realistic Raw Scenarios
The previous scenarios assume that input surface to-
kens are identical to tree terminals. For languages
such as Arabic and Hebrew, this is not always the
case. In this scenario, we evaluate the capacity of a
system to predict both morphological segmentation
and syntactic parse trees given raw, unsegmented
input tokens. This may be done via a pipeline as-
suming a 1-st best morphological analysis, or jointly
with parsing, assuming an ambiguous morpholog-
ical analysis lattice as input. In this task, both of
these scenarios are possible (see section 3). Thus,
this section presents a realistic evaluation of the par-
ticipating systems, using TedEval, which takes into
account complete morpho-syntactic parses.
Tables 7 and 8 present labeled and unlabeled
TedEval results for both constituency and depen-
dency parsers, calculated only for sentence of length
<= 70.39 We firstly observe that labeled TedEval
scores are considerably lower than unlabeled Ted-
Eval scores, as expected, since unlabeled scores eval-
uate only structural differences. In the labeled setup,
39TedEval builds on algorithms for calculating edit distance
on complete trees (Bille, 2005). In these algorithms, longer
sentences take considerably longer to evaluate.
169
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 83.34 1.63 82.54 0.67 56.47 0.67 69.51 69.51
IMS:SZEGED:CIS 89.12 8.37 87.82 5.56 86.08 8.27 86.95 86.95
CADIM 87.81 6.63 86.43 4.21 - - 43.22 86.43
MALTOPTIMIZER 86.74 5.39 85.63 3.03 83.05 5.33 84.34 84.34
ALPAGE:DYALOG 86.60 5.34 85.71 3.54 82.96 6.17 41.48 82.96
ALPAGE:DYALOG (RAW) - - - - 82.82 4.35 41.41 82.82
AI:KU - - - - 78.57 3.37 39.29 78.57
Table 7: Realistic Scenario: Tedeval Labeled Accuracy and Exact Match for the Raw scenario.
The upper part refers to constituency results, the lower part refers to dependency results
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 92.06 9.49 91.29 7.13 89.30 13.60 90.30 90.30
IMS:SZEGED:CIS 91.74 9.83 90.85 7.30 89.47 16.97 90.16 90.16
ALPAGE:DYALOG 89.99 7.98 89.46 5.67 88.33 12.20 88.90 88.90
MALTOPTIMIZER 90.09 7.08 89.47 5.56 87.99 11.64 88.73 88.73
CADIM 90.75 8.48 89.89 5.67 - - 44.95 89.89
ALPAGE:DYALOG (RAW) - - - - 87.61 10.24 43.81 87.61
AI:KU - - - - 86.70 8.98 43.35 86.70
Table 8: Realistic Scenario: Tedeval Unlabeled Accuracy and Exact Match for the Raw scenario.
Top upper part refers to constituency results, the lower part refers to dependency results.
the IMS:SZEGED:CIS dependency parser are the
best for both languages and data set sizes. Table 8
shows that their unlabeled constituency results reach
a higher accuracy than the next best system, their
own dependency results. However, a quick look at
the exact match metric reveals lower scores than for
its dependency counterparts.
For the dependency-based joint scenarios, there
is obviously an upper bound on parser performance
given inaccurate segmentation. The transition-based
systems, ALPAGE:DYALOG & MALTOPTIMIZER,
perform comparably on Arabic and Hebrew, with
ALPAGE:DYALOG being slightly better on both lan-
guages. Note that ALPAGE:DYALOG reaches close
results on the 1-best and the lattice-based input set-
tings, with a slight advantage for the former. This is
partly due to the insufficient coverage of the lexical
resource we use: many lattices do not contain the
gold path, so the joint prediction can only as be high
as the lattice predicted path allows.
7 Towards In-Depth Cross-Treebank
Evaluation
Section 6 reported evaluation scores across systems
for different scenarios. However, as noted, these re-
sults are not comparable across languages, represen-
tation types and parsing scenarios due to differences
in the data size, label set size, length of sentences and
also differences in evaluation metrics.
Our following discussion in the first part of this
section highlights the kind of impact that data set
properties have on the standard metrics (label set size
on LAS, non-terminal nodes per sentence on F-score).
Then, in the second part of this section we use the
TedEval cross-experiment protocols for comparative
evaluation that is less sensitive to representation types
and annotation idiosyncrasies.
7.1 Parsing Across Languages and Treebanks
To quantify the impact of treebank characteristics on
parsing parsing accuracy we looked at correlations
of treebank properties with parsing results. The most
highly correlated combinations we have found are
shown in Figures 2, 3, and 4 for the dependency track
and the constituency track (F-score and LeafAnces-
170
21/09/13 03:00SPMRL charts
Page 3 sur 3http://pauillac.inria.fr/~seddah/updated_official.spmrl_results.html
Correlation between label set size, treebank size, and mean LAS
FrP
FrP
GeP
GeP
HuP
HuP
SwP
ArP
ArP
ArG
ArG
BaP
BaP
FrG
FrG
GeG
GeG
HeP
HeG
HuG
HuG
PoP
PoP
PoG
PoG
SwG
BaG
BaG
KoP
KoP
KoG
KoG
10 50 100 500 1 000
72
74
76
78
80
82
84
86
88
90
treebank size / #labels
L
A
S
 
(
%
)
Figure 2: The correlation between treebank size, label set size, and LAS scores. x: treebank size / #labels ; y: LAS (%)
01/10/13 00:43SPMRL charts: all sent.
Page 1 sur 5file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-S?/SPMRL_FINAL/RESULTS/OFFICIAL/official_ptb-all.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, all sent.)
(13/10/01 00:34:34
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean F1
Arabic
Basque
French
German
Hebrew
Hungarian
Korean
Polish
Swedish
ArP
ArG
BaP
BaG
FrP
FrG
GeP
GeG
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
8 9 10
72
74
76
78
80
82
84
86
88
90
92
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 3: The correlation between the non terminals per sentence ratio and F-scores. x: #non terminal/ #sentence ; y:
F1 (%)
171
tor) respectively.
Figure 2 presents the LAS against the average num-
ber of tokens relative to the number of labels. The
numbers are averaged per language over all partici-
pating systems, and the size of the ?bubbles? is pro-
portional to the number of participants for a given
language setting. We provide ?bubbles? for all lan-
guages in the predicted (-P) and gold (-G) setting,
for both training set sizes. The lower dot in terms
of parsing scores always corresponds to the reduced
training set size.
Figure 2 shows a clear correlation between data-
set complexity and parsing accuracy. The simpler
the data set is (where ?simple" here translates into
large data size with a small set of labels), the higher
the results of the participating systems. The bubbles
reflects a diagonal that indicates correlation between
these dimensions. Beyond that, we see two interest-
ing points off of the diagonal. The Korean treebank
(pink) in the gold setting and full training set can be
parsed with a high LAS relative to its size and label
set. It is also clear that the Hebrew treebank (purple)
in the predicted version is the most difficult one to
parse, relative to our expectation about its complexity.
Since the Hebrew gold scenario is a lot closer to the
diagonal again, it may be that this outlier is due to the
coverage and quality of the predicted morphology.
Figure 340 shows the correlation of data complex-
ity in terms of the average number of non-terminals
per sentence, and parsing accuracy (ParsEval F-
score). Parsing accuracy is again averaged over all
participating systems for a given language. In this
figure, we see a diagonal similar to the one in figure 2,
where Arabic (dark blue) has high complexity of the
data (here interpreted as flat trees, low number of
non terminals per sentence) and low F-scores accord-
ingly. Korean (pink), Swedish (burgundy), Polish
(light green), and Hungarian (light blue) follow, and
then Hebrew (purple) is a positive outlier, possibly
due to an additional layer of ?easy" syntactic POS
nodes which increases tree size and inflates F-scores.
French (orange), Basque (red), and German (dark
green) are negative outliers, falling off the diago-
nal. German has the lowest F-score with respect to
40This figure was created from the IMS:SZEGED:CIS
(Const.) and our own PCFG-LA baseline in POS Tagged mode
(BASE:BKY+POS) so as to avoid the noise introduced by the
parser?s own tagging step (BASE:BKY+RAW).
what would be expected for the non-terminals per
sentence ratio, which is in contrast to the LAS fig-
ure where German occurs among the less complex
data set to parse. A possible explanation may be
the crossing branches in the original treebank which
were re-attached. This creates flat and variable edges
which might be hard predict accurately.
Figure 441 presents the correlation between parsing
accuracy in terms the LeafAncestor metrics (macro
averaged) and treebank complexity in terms of the
average number of non-terminals per sentence. As
in the correlation figures, the parsing accuracy is
averaged over the participanting systems for any lan-
guage. The LeafAncestor accuracy is calculated over
phrase structure trees, and we see a similar diago-
nal to the one in Figure 3 showing that flatter tree-
banks are harder (that is, are correlated with lower
averaged scores) But, its slope is less steep than for
the F-score, which confirms the observation that the
LeafAncestor metric is less sensitive than F-score to
the non-terminals-per-sentence ratio.
Similarly to Figure 3, German is a negative outlier,
which means that this treebank is harder to parse ? it
obtains lower scores on average than we would ex-
pect. As for Hebrew, it is much closer to the diagonal.
As it turns out, the "easy" POS layer that inflates the
scores does not affect the LA ratings as much.
7.2 Evaluation Across Scenarios, Languages
and Treebanks
In this section we analyze the results in cross-
scenario, cross-annotation, and cross-framework set-
tings using the evaluation protocols discussed in
(Tsarfaty et al, 2012b; Tsarfaty et al, 2011; Tsarfaty
et al, 2012a).
As a starting point, we select comparable sections
of the parsed data, based on system runs trained on
the small train set (train5k). For those, we selected
subsets containing the first 5,000 tree terminals (re-
specting sentence boundaries) of the test set. We only
used TedEval on sentences up to 70 terminals long,
and projectivized non-projective sentences in all sets.
We use the TedEval metrics to calculate scores on
both constituency and dependency structures in all
languages and all scenarios. Since the metric de-
fines one scale for all of these different cases, we can
41This figure was created under the same condition as the
F-score correlation in figure (Figure 3).
172
04/10/13 23:05SPMRL charts:
Page 1 sur 6file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-SHAREDTASK/SPMRL_FINAL/RESULTS/TESTLEAF.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, )
(13/10/04 23:05:31
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean Leaf Accuracy
ArP
ArG
BaP
BaG
FrP
FrG
GeP
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
GeG
8 9 10
74
76
78
80
82
84
86
88
90
92
94
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 4: The correlation between the non terminals per sentence ratio and Leaf Accuracy (macro) scores. x: #non
terminal/ #sentence ; y: Acc.(%)
compare the performance across annotation schemes,
assuming that those subsets are representative of their
original source.42
Ideally, we would be using labeled TedEval scores,
as the labeled parsing task is more difficult, and la-
beled parses are far more informative than unlabeled
ones. However, most constituency-based parsers do
not provide function labels as part of the output, to
be compared with the dependency arcs. Furthermore,
as mentioned earlier, we observed a huge difference
between label set sizes for the dependency runs. Con-
sequently, labeled scores will not be as informative
across treebanks and representation types. We will
therefore only use labels across scenarios for the
same language and representation type.
42We choose this sample scheme for replicability. We first
tried sampling sentences, aiming at the same average sentence
length (20), but that seemed to create artificially difficult test sets
for languages as Polish and overly simplistic ones for French or
Arabic.
7.2.1 Cross-Scenario Evaluation: raw vs. gold
One novel aspect of this shared task is the evalu-
ation on non-gold segmentation in addition to gold
morphology. One drawback is that the scenarios are
currently not using the same metrics ? the metrics
generally applied for gold and predicted scenrios can-
not apply for raw. To assess how well state of the art
parsers perform in raw scenarios compared to gold
scenarios, we present here TedEval results comparing
raw and gold systems using the evaluation protocol
of Tsarfaty et al (2012b).
Table 9 presents the labeled and unlabeled results
for Arabic and Hebrew (in Full and 5k training set-
tings), and Table 10 presents unlabeled TedEval re-
sults (for all languages) in the gold settings. The
unlabeled TedEval results for the raw settings are
substantially lower then TedEval results on the gold
settings for both languages.
When comparing the unlabeled TedEval results for
Arabic and Hebrew on the participating systems, we
see a loss of 3-4 points between Table 9 (raw) and Ta-
ble 10 (gold). In particular we see that for the best per-
173
forming systems on Arabic (IMS:SZEGED:CIS for
both constituency and dependency), the gap between
gold and realistic scenarios is 3.4 and 4.3 points,
for the constituency and the dependency parser re-
spectively. These results are on a par with results
by Tsarfaty et al (2012b), who showed for different
settings, constituency and dependency based, that
raw scenarios are considerably more difficult to parse
than gold ones on the standard split of the Modern
Hebrew treebank.
For Hebrew, the performance gap between unla-
beled TedEval in raw (Table 9) and gold (Table 10)
is even more salient, with around 7 and 8 points of
difference between the scenarios. We can only specu-
late that such a difference may be due to the difficulty
of resolving Hebrew morpho-syntactic ambiguities
without sufficient syntactic information. Since He-
brew and Arabic now have standardized morpholog-
ically and syntactically analyzed data sets available
through this task, it will be possible to investigate
further how cross-linguistic differences in morpho-
logical ambiguity affect full-parsing accuracy in raw
scenarios.
This section compared the raw and gold parsing
results only on unlabeled TedEval metrics. Accord-
ing to what we have seen so far is expected that
for labeled TedEval metrics using the same protocol,
the gap between gold and raw scenario will be even
greater.
7.2.2 Cross-Framework Evaluation:
Dependency vs. Constituency
In this section, our focus is on comparing parsing
results across constituency and dependency parsers
based on the protocol of Tsarfaty et al (2012a) We
have only one submission from IMS:SZEGED:CIS
in the constituency track, and. from the same group,
a submission on the dependency track. We only com-
pare the IMS:SZEGED:CIS results on constituency
and dependency parsing with the two baselines we
provided. The results of the cross-framework evalua-
tion protocol are shown in Table 11.
The results comparing the two variants of the
IMS:SZEGED:CIS systems show that they are very
close for all languages, with differences ranging from
0.03 for German to 0.8 for Polish in the gold setting.
It has often been argued that dependency parsers
perform better than a constituency parser, but we
notice that when using a cross framework protocol,
such as TedEval, and assuming that our test set sam-
ple is representative, the difference between the in-
terpretation of both representation?s performance is
alleviated. Of course, here the metric is unlabeled, so
it simply tells us that both kind of parsing models are
equally able to provide similar tree structures. Said
differently, the gaps in the quality of predicting the
same underlying structure across representations for
MRLs is not as large as is sometimes assumed.
For most languages, the baseline constituency
parser performs better than the dependency base-
line one, with Basque and Korean as an exception,
and at the same time, the dependency version of
IMS:SZEGED:CIS performs slightly better than their
constituent parser for most languages, with the excep-
tion of Hebrew and Hungarian. It goes to show that,
as far as these present MRL results go, there is no
clear preference for a dependency over a constituency
parsing representation, just preferences among par-
ticular models.
More generally, we can say that even if the linguis-
tic coverage of one theory is shown to be better than
another one, it does not necessarily mean that the
statistical version of the formal theory will perform
better for structure prediction. System performance
is more tightly related to the efficacy of the learning
and search algorithms, and feature engineering on
top of the selected formalism.
7.2.3 Cross-Language Evaluation: All
Languages
We conclude with an overall outlook of the Ted-
Eval scores across all languages. The results on the
gold scenario, for the small training set and the 5k
test set are presented in Table 10. We concentrate
on gold scenarios (to avoid the variation in cover-
age of external morphological analyzers) and choose
unlabeled metrics as they are not sensitive to label
set sizes. We emphasize in bold, for each parsing
system (row in the table), the top two languages that
most accurately parsed by it (boldface) and the two
languages it performed the worse on (italics).
We see that the European languages German
and Hungarian are parsed most accurately in the
constituency-based setup, with Polish and Swedish
having an advantage in dependency parsing. Across
all systems, Korean is the hardest to parse, with Ara-
174
Arabic Hebrew AVG1 SOFT AVG Arabic Hebrew AVG2 SOFT AVG2
1) Constituency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS (Bky) 83.59 56.43 70.01 70.01 92.18 88.02 90.1 90.1
2) Dependency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS 88.61 84.74 86.68 86.68 91.41 88.58 90 90
ALPAGE:DYALOG 87.20 81.65 40.83 81.65 90.74 87.44 89.09 89.09
CADIM 87.99 - 44 87.99 91.22 - 45.61 91.22
MALTOPTIMIZER 86.62 81.74 43.31 86.62 90.26 87.00 45.13 90.26
ALPAGE:DYALOG (RAW) - 82.82 41.41 82.82 - 87.43 43.72 87.43
AI:KU - 77.8 38.9 77.8 - 85.87 42.94 85.87
Table 9: Labeled and Unlabeled TedEval Results for raw Scenarios, Trained on 5k sentences and tested on 5k terminals.
The upper part refers to constituency parsing and the lower part refers to dependency parsing.
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) Constituency Evaluation
IMS:SZEGED:CIS (Bky) 95.35 96.91 95.98 97.12 96.22 97.92 92.91 97.19 96.65
BASE:BKY+POS 95.11 94.69 95.08 97.01 95.85 97.08 90.55 96.99 96.38
BASE:BKY+RAW 94.58 94.32 94.72 96.74 95.64 96.15 87.08 95.93 95.90
2) Dependency Evaluation
IMS:SZEGED:CIS 95.76 97.63 96.59 96.88 96.29 97.56 94.62 98.01 97.22
ALPAGE:DYALOG 93.76 95.72 95.75 96.4 95.34 95.63 94.56 96.80 96.55
BASE:MALT 94.16 95.08 94.21 94.55 94.98 95.25 94.27 95.83 95.33
AI:KU - - 95.46 96.34 95.07 96.53 - 96.88 95.87
MALTOPTIMIZER 94.91 96.82 95.23 96.32 95.46 96.30 94.69 96.06 95.90
CADIM 94.66 - - - - - - - -
Table 10: Cross-Language Evaluation: Unlabeled TedEval Results in gold input scenario, On a 5k-sentences set set and
a 5k-terminals test set. The upper part refers to constituency parsing and the lower part refers to dependency parsing.
For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics.
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) gold setting
IMS:SZEGED:CIS (Bky) 95.82 97.30 96.15 97.43 96.37 98.25 94.07 97.22 96.89
IMS:SZEGED:CIS 95.87 98.06 96.61 97.46 96.31 97.93 94.62 98.04 97.24
BASE:BKY+POS 95.61 95.25 95.48 97.31 96.03 97.53 92.15 96.97 96.66
BASE:MALT 94.26 95.76 94.23 95.53 95.00 96.09 94.27 95.90 95.35
2) predicted setting
IMS:SZEGED:CIS (Bky) 95.74 97.07 96.21 97.31 96.10 98.03 94.05 96.92 96.90
IMS:SZEGED:CIS 95.18 97.67 96.15 97.09 96.22 97.63 94.43 97.50 97.02
BASE:BKY+POS 95.03 95.35 97.12 95.36 97.20 91.34 96.92 96.25
BASE:MALT 95.49 93.84 95.39 94.41 95.72 93.74 96.04 95.09
Table 11: Cross Framework Evaluation: Unlabeled TedEval on generalized gold trees in gold scenario, trained on 5k
sentences and tested on 5k terminals.
bic, Hebrew and to some extent French following. It
appears that on a typological scale, Semitic and Asian
languages are still harder to parse than a range of Eu-
ropean languages in terms of structural difficulty and
complex morpho-syntactic interaction. That said,
note that we cannot tell why certain treebanks appear
more challenging to parse then others, and it is still
unclear whether the difficulty is inherent on the lan-
guage, in the currently available models, or because
of the annotation scheme and treebank consistency.43
43The latter was shown to be an important factor orthogonal
to the morphologically-rich nature of the treebank?s language
175
8 Conclusion
This paper presents an overview of the first shared
task on parsing morphologically rich languages. The
task features nine languages, exhibiting different lin-
guistic phenomena and varied morphological com-
plexity. The shared task saw submissions from seven
teams, and results produced by more than 14 different
systems. The parsing results were obtained in dif-
ferent input scenarios (gold, predicted, and raw) and
evaluated using different protocols (cross-framework,
cross-scenario, and cross-language). In particular,
this is the first time an evaluation campaign reports
on the execution of parsers in realistic, morphologi-
cally ambiguous, setting.
The best performing systems were mostly ensem-
ble systems combining multiple parser outputs from
different frameworks or training runs, or integrat-
ing a state-of-the-art morphological analyzer on top
of a carefully designed feature set. This is con-
sistent with previous shared tasks such as ConLL
2007 or SANCL?2012. However, dealing with am-
biguous morphology is still difficult for all systems,
and a promising approach, as demonstrated by AL-
PAGE:DYALOG, is to deal with parsing and morphol-
ogy jointly by allowing lattice input to the parser. A
promising generalization of this approach would be
the full integration of all levels of analysis that are
mutually informative into a joint model.
The information to be gathered from the results of
this shared task is vast, and we only scratched the
surface with our preliminary analyses. We uncov-
ered and documented insights of strategies that make
parsing systems successful: parser combination is
empirically proven to reach a robust performance
across languages, though language-specific strategies
are still a sound avenue for obtaining high quality
parsers for that individual language. The integration
of morphological analysis into the parsing needs to
be investigated thoroughly, and new approaches that
are morphologically aware need to be developed.
Our cross-parser, cross-scenario, and cross-
framework evaluation protocols have shown that, as
expected, more data is better, and that performance
on gold morphological input is significantly higher
than that in more realistic scenarios. We have shown
that gold morphological information is more help-
(Schluter and van Genabith, 2007)
ful to some languages and parsers than others, and
that it may also interact with successful identification
of multiword expressions. We have shown that dif-
ferences between dependency and constituency are
smaller than previously assumed and that properties
of the learning model and granularity of the output
labels are more influential. Finally, we observed
that languages which are typologically farthest from
English, such as Semitic and Asian languages, are
still amongst the hardest to parse, regardless of the
parsing method used.
Our cross-treebank, in-depth analysis is still pre-
liminary, owing to the limited time between the end
of the shared task and the deadline for publication
of this overview. but we nonetheless feel that our
findings may benefit researchers who aim to develop
parsers for diverse treebanks.44
A shared task is an inspection of the state of the
art, but it may also accelerate research in an area
by providing a stable data basis as well as a set of
strong baselines. The results produced in this task
give a rich picture of the issues associated with pars-
ing MRLs and initial cues towards their resolution.
This set of results needs to be further analyzed to be
fully understood, which will in turn contribute to new
insights. We hope that this shared task will provide
inspiration for the design and evaluation of future
parsing systems for these languages.
Acknowledgments
We heartily thank Miguel Ballesteros and Corentin
Ribeire for running the dependency and constituency
baselines. We warmly thank the Linguistic Data Con-
sortium: Ilya Ahtaridis, Ann Bies, Denise DiPersio,
Seth Kulick and Mohamed Maamouri for releasing
the Arabic Penn Treebank for this shared task and
for their support all along the process. We thank
Alon Itai and MILA, the knowledge center for pro-
cessing Hebrew, for kindly making the Hebrew tree-
bank and morphological analyzer available for us,
Anne Abeill? for allowing us to use the French tree-
bank, and Key-Sun Choi for the Kaist Korean Tree-
bank. We thank Grzegorz Chrupa?a for providing
the morphological analyzer Morfette, and Joachim
44The data set will be made available as soon as possible under
the license distribution of the shared-task, with the exception
of the Arabic data, which will continue to be distributed by the
LDC.
176
Wagner for his LeafAncestor implementation. We
finally thank ?zlem ?etinog?lu, Yuval Marton, Benoit
Crabb? and Benoit Sagot who have been nothing but
supportive during all that time.
At the end of this shared task (though watch out
for further updates and analyses), what remains to be
mentioned is our deep gratitude to all people involved,
either data providers or participants. Without all of
you, this shared task would not have been possible.
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.
2003. Building a treebank for French. In Anne Abeill?,
editor, Treebanks. Kluwer, Dordrecht.
Szymon Acedan?ski. 2010. A Morphosyntactic Brill Tag-
ger for Inflectional Languages. In Advances in Natural
Language Processing, volume 6233 of Lecture Notes
in Computer Science, pages 3?14. Springer-Verlag.
Meni Adler and Michael Elhadad. 2006. An unsupervised
morpheme-based HMM for Hebrew morphological dis-
ambiguation. In Proceedings COLING-ACL, pages
665?672, Sydney, Australia.
Meni Adler, Yoav Goldberg, David Gabay, and Michael
Elhadad. 2008. Unsupervised lexicon-based resolution
of unknown words for full morphological analysis. In
Proceedings of ACL-08: HLT, pages 728?736, Colum-
bus, OH.
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev.
Itziar Aduriz, Jos? Mar?a Arriola, Xabier Artola, A D?az
de Ilarraza, et al 1997. Morphosyntactic disambigua-
tion for Basque based on the constraint grammar for-
malism. In Proceedings of RANLP, Tzigov Chark, Bul-
garia.
Itziar Aduriz, Eneko Agirre, Izaskun Aldezabal, I?aki
Alegria, Xabier Arregi, Jose Maria Arriola, Xabier Ar-
tola, Koldo Gojenola, Aitor Maritxalar, Kepa Sarasola,
et al 2000. A word-grammar based morphological
analyzer for agglutinative languages. In Proceedings
of COLING, pages 1?7, Saarbr?cken, Germany.
Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria Arriola,
Aitziber Atutxa, A Diaz de Ilarraza, Aitzpea Garmen-
dia, and Maite Oronoz. 2003. Construction of a
Basque dependency treebank. In Proceedings of the
2nd Workshop on Treebanks and Linguistic Theories
(TLT), pages 201?204, V?xj?, Sweden.
Zeljko Agic, Danijela Merkler, and Dasa Berovic. 2013.
Parsing Croatian and Serbian by using Croatian depen-
dency treebanks. In Proceedings of the Fourth Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Seattle, WA.
I. Aldezabal, M.J. Aranzabe, A. Diaz de Ilarraza, and
K. Fern?ndez. 2008. From dependencies to con-
stituents in the reference corpus for the processing of
Basque. In Procesamiento del Lenguaje Natural, no
41 (2008), pages 147?154. XXIV edici?n del Congreso
Anual de la Sociedad Espa?ola para el Procesamiento
del Lenguaje Natural (SEPLN).
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010. On the role of morphosyntactic
features in Hindi dependency parsing. In Proceedings
of the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL), Los Angeles, CA.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOpti-
mizer: An optimization tool for MaltParser. In Pro-
ceedings of EACL, pages 58?62, Avignon, France.
Miguel Ballesteros. 2013. Effective morphological fea-
ture selection with MaltOptimizer at the SPMRL 2013
shared task. In Proceedings of the Fourth Workshop on
Statistical Parsing of Morphologically-Rich Languages,
pages 53?60, Seattle, WA.
Kepa Bengoetxea and Koldo Gojenola. 2010. Appli-
cation of different techniques to dependency parsing
of Basque. In Proceedings of the NAACL/HLT Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL 2010), Los Angeles, CA.
Philip Bille. 2005. A survey on tree edit distance and re-
lated problems. Theoretical Computer Science, 337(1?
3):217?239, 6.
Anders Bj?rkelund, Ozlem Cetinoglu, Rich?rd Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(Re)ranking meets morphosyntax: State-of-the-art re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing
of Morphologically-Rich Languages, pages 134?144,
Seattle, WA.
Ezra Black, Steven Abney, Dan Flickinger, Claudia
Gdaniec, Ralph Grishman, Philip Harrison, Donald
Hindle, Robert Ingria, Frederick Jelinek, Judith Kla-
vans, Mark Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek Strzalkowski. 1991. A
procedure for quantitatively comparing the syntactic
coverage of English grammars. In Proceedings of the
DARPA Speech and Natural Language Workshop 1991,
pages 306?311, Pacific Grove, CA.
177
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proceed-
ings of the EMNLP-CoNLL, pages 1455?1465, Jeju,
Korea.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of COL-
ING, pages 89?97, Beijing, China.
Adriane Boyd. 2007. Discontinuity revisited: An im-
proved conversion to context-free representations. In
Proceedings of the Linguistic Annotation Workshop,
Prague, Czech Republic.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the First Workshop on Treebanks
and Linguistic Theories (TLT), pages 24?41, Sozopol,
Bulgaria.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164, New York, NY.
Tim Buckwalter. 2002. Arabic morphological analyzer
version 1.0. Linguistic Data Consortium.
Tim Buckwalter. 2004. Arabic morphological analyzer
version 2.0. Linguistic Data Consortium.
Marie Candito and Djam? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Marie Candito, Benoit Crabb?, and Pascal Denis. 2010.
Statistical French dependency parsing: Treebank con-
version and first results. In Proceedings of LREC, Val-
letta, Malta.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of CoNLL,
pages 9?16, Manchester, UK.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180, Barcelona,
Spain.
Eugene Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In AAAI/IAAI, pages
598?603.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of NAACL, pages 132?139, Seat-
tle, WA.
Jinho D. Choi and Martha Palmer. 2011. Statistical de-
pendency parsing in Korean: From corpus generation
to automatic parsing. In Proceedings of Second Work-
shop on Statistical Parsing of Morphologically Rich
Languages, pages 1?11, Dublin, Ireland.
Jinho D. Choi and Martha Palmer. 2012. Guidelines
for the Clear Style Constituent to Dependency Conver-
sion. Technical Report 01-12, University of Colorado
at Boulder.
Key-sun Choi, Young S. Han, Young G. Han, and Oh W.
Kwon. 1994. KAIST Tree Bank Project for Korean:
Present and Future Development. In In Proceedings
of the International Workshop on Sharable Natural
Language Resources, pages 7?14, Nara, Japan.
Jinho D. Choi. 2013. Preparing Korean data for the
shared task on parsing morphologically rich languages.
arXiv:1309.1649.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with Morfette. In
Proceedings of LREC, Marrakech, Morocco.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of Korean parsing. In
Proceedings of the NAACL/HLT Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), Los Angeles, CA.
Volkan Cirik and H?sn? S?ensoy. 2013. The AI-KU
system at the SPMRL 2013 shared task: Unsuper-
vised features for dependency parsing. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 68?75, Seat-
tle, WA.
Michael Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
29(4):589?637.
Matthieu Constant, Marie Candito, and Djam? Seddah.
2013. The LIGM-Alpage architecture for the SPMRL
2013 shared task: Multiword expression analysis and
dependency parsing. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 46?52, Seattle, WA.
Anna Corazza, Alberto Lavelli, Giogio Satta, and Roberto
Zanoli. 2004. Analyzing an Italian treebank with
state-of-the-art statistical parsers. In Proceedings of
the Third Workshop on Treebanks and Linguistic Theo-
ries (TLT), T?bingen, Germany.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In Actes
de la 15?me Conf?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
D?ra Csendes, J?nos Csirik, Tibor Gyim?thy, and Andr?s
Kocsor. 2005. The Szeged treebank. In Proceedings of
the 8th International Conference on Text, Speech and
Dialogue (TSD), Lecture Notes in Computer Science,
pages 123?132, Berlin / Heidelberg. Springer.
Eric De La Clergerie. 2013. Exploring beam-based
shift-reduce dependency parsing with DyALog: Re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
178
Morphologically-Rich Languages, pages 81?89, Seat-
tle, WA.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
Mona Diab, Nizar Habash, Owen Rambow, and Ryan
Roth. 2013. LDC Arabic treebanks and associated cor-
pora: Data divisions manual. Technical Report CCLS-
13-02, Center for Computational Learning Systems,
Columbia University.
Eva Ejerhed and Gunnel K?llgren. 1997. Stockholm
Ume? Corpus. Version 1.0. Department of Linguis-
tics, Ume? University and Department of Linguistics,
Stockholm University.
Eva Ejerhed, Gunnel K?llgren, Ola Wennstedt, and Mag-
nus ?str?m. 1992. The linguistic annotation system
of the Stockholm?Ume? Corpus project. Technical
Report 33, University of Ume?: Department of Linguis-
tics.
Nerea Ezeiza, I?aki Alegria, Jos? Mar?a Arriola, Rub?n
Urizar, and Itziar Aduriz. 1998. Combining stochastic
and rule-based methods for disambiguation in aggluti-
native languages. In Proceedings of COLING, pages
380?384, Montr?al, Canada.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL, pages
959?967, Columbus, OH.
Alexander Fraser, Helmut Schmid, Rich?rd Farkas, Ren-
jing Wang, and Hinrich Sch?tze. 2013. Knowledge
sources for constituent parsing of German, a morpho-
logically rich and less-configurational language. Com-
putational Linguistics, 39(1):57?85.
Iakes Goenaga, Koldo Gojenola, and Nerea Ezeiza. 2013.
Exploiting the contribution of morphological informa-
tion to parsing: the BASQUE TEAM system in the
SPRML?2013 shared task. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 61?67, Seattle, WA.
Yoav Goldberg and Michael Elhadad. 2010a. Easy-first
dependency parsing of Modern Hebrew. In Proceed-
ings of the NAACL/HLT Workshop on Statistical Pars-
ing of Morphologically Rich Languages (SPMRL 2010),
Los Angeles, CA.
Yoav Goldberg and Michael Elhadad. 2010b. An ef-
ficient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of HLT: NAACL, pages
742?750, Los Angeles, CA.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syntac-
tic parsing. In Proceedings of ACL, Columbus, OH.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proc. of ACL, Columbus, OH.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities. In
Proceedings of EALC, pages 327?335, Athens, Greece.
Yoav Goldberg. 2011. Automatic syntactic processing of
Modern Hebrew. Ph.D. thesis, Ben Gurion University
of the Negev.
David Graff, Mohamed Maamouri, Basma Bouziri, Son-
dos Krouna, Seth Kulick, and Tim Buckwalter. 2009.
Standard Arabic Morphological Analyzer (SAMA) ver-
sion 3.1. Linguistic Data Consortium LDC2009E73.
Spence Green and Christopher D. Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis.
In Proceedings of COLING, pages 394?402, Beijing,
China.
Nathan Green, Loganathan Ramasamy, and Zden?k
?abokrtsk?. 2012. Using an SVM ensemble system for
improved Tamil dependency parsing. In Proceedings
of the ACL 2012 Joint Workshop on Statistical Pars-
ing and Semantic Processing of Morphologically Rich
Languages, pages 72?77, Jeju, Korea.
Spence Green, Marie-Catherine de Marneffe, and Christo-
pher D. Manning. 2013. Parsing models for identify-
ing multiword expressions. Computational Linguistics,
39(1):195?227.
Noemie Guthmann, Yuval Krymolowski, Adi Milea, and
Yoad Winter. 2009. Automatic annotation of morpho-
syntactic dependencies in a Modern Hebrew Treebank.
In Proceedings of the Eighth International Workshop on
Treebanks and Linguistic Theories (TLT), Groningen,
The Netherlands.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proceedings of ACL-
IJCNLP, pages 221?224, Suntec, Singapore.
Nizar Habash, Ryan Gabbard, Owen Rambow, Seth
Kulick, and Mitch Marcus. 2007. Determining case in
Arabic: Learning complex linguistic behavior requires
complex linguistic features. In Proceedings of EMNLP-
CoNLL, pages 1084?1092, Prague, Czech Republic.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009a. Syn-
tactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on
Arabic Language Resources and Tools, Cairo, Egypt.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009b.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS tag-
ging, stemming and lemmatization. In Proceedings of
the Second International Conference on Arabic Lan-
guage Resources and Tools. Cairo, Egypt.
179
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publishers.
Jan Hajic?, Alena B?hmov?, Eva Hajic?ov?, and Barbora
Vidov?-Hladk?. 2000. The Prague Dependency Tree-
bank: A three-level annotation scenario. In Anne
Abeill?, editor, Treebanks: Building and Using Parsed
Corpora. Kluwer Academic Publishers.
P?ter Hal?csy, Andr?s Kornai, and Csaba Oravecz. 2007.
HunPos ? an open source trigram tagger. In Proceed-
ings of ACL, pages 209?212, Prague, Czech Republic.
Johan Hall, Jens Nilsson, Joakim Nivre, G?ls?en Eryig?it,
Be?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? A study in multilingual
parser optimization. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
933?939, Prague, Czech Republic.
Chung-hye Han, Na-Rae Han, Eon-Suk Ko, Martha
Palmer, and Heejong Yi. 2002. Penn Korean Treebank:
Development and evaluation. In Proceedings of the
16th Pacific Asia Conference on Language, Information
and Computation, Jeju, Korea.
Tilman H?hle. 1986. Der Begriff "Mittelfeld", Anmerkun-
gen ?ber die Theorie der topologischen Felder. In Ak-
ten des Siebten Internationalen Germanistenkongresses
1985, pages 329?340, G?ttingen, Germany.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with products of latent variable grammars.
In Proceedings of EMNLP, pages 12?22, Cambridge,
MA.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of ACL,
pages 586?594, Columbus, OH.
Alon Itai and Shuly Wintner. 2008. Language resources
for Hebrew. Language Resources and Evaluation,
42(1):75?98, March.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24(4):613?
632.
Laura Kallmeyer and Wolfgang Maier. 2013. Data-driven
parsing using probabilistic linear context-free rewriting
systems. Computational Linguistics, 39(1).
Fred Karlsson, Atro Voutilainen, Juha Heikkilae, and Arto
Anttila. 1995. Constraint Grammar: a language-
independent system for parsing unrestricted text. Wal-
ter de Gruyter.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430, Sapporo, Japan.
Sandra K?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it really that difficult to parse German? In Pro-
ceedings of EMNLP, pages 111?119, Sydney, Australia,
July.
Sandra K?bler, Wolfgang Maier, Ines Rehbein, and Yan-
nick Versley. 2008. How to compare treebanks. In
Proceedings of LREC, pages 2322?2329, Marrakech,
Morocco.
Sandra K?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63, Columbus, OH.
Seth Kulick, Ryan Gabbard, and Mitch Marcus. 2006.
Parsing the Arabic Treebank: Analysis and Improve-
ments. In Proceedings of the Treebanks and Linguistic
Theories Conference, pages 31?42, Prague, Czech Re-
public.
Joseph Le Roux, Benoit Sagot, and Djam? Seddah. 2012.
Statistical parsing of Spanish and data driven lemmati-
zation. In Proceedings of the Joint Workshop on Statis-
tical Parsing and Semantic Processing of Morphologi-
cally Rich Languages, pages 55?61, Jeju, Korea.
Kong Joo Lee, Byung-Gyu Chang, and Gil Chang Kim.
1997. Bracketing Guidelines for Korean Syntactic Tree
Tagged Corpus. Technical Report CS/TR-97-112, De-
partment of Computer Science, KAIST.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese treebank? In
Proceedings of ACL, Sapporo, Japan.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2004a. Arabic Treebank: Part 2 v 2.0.
LDC catalog number LDC2004T02.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004b. The Penn Arabic Treebank:
Building a large-scale annotated Arabic corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102?109, Cairo, Egypt.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2005. Arabic Treebank: Part 1 v 3.0. LDC
catalog number LDC2005T02.
Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma Gad-
deche, Wigdan Mekki, Sondos Krouna, and Basma
Bouziri. 2009. The Penn Arabic Treebank part 3 ver-
sion 3.1. Linguistic Data Consortium LDC2008E22.
Wolfgang Maier, Miriam Kaeshammer, and Laura
Kallmeyer. 2012. Data-driven PLCFRS parsing re-
visited: Restricting the fan-out to two. In Proceedings
of the Eleventh International Conference on Tree Ad-
joining Grammars and Related Formalisms (TAG+11),
Paris, France.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn TreeBank. Computational
Linguistics, 19(2):313?330.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference. In
Proceedings of EMNLP, pages 34?44, Cambridge, MA.
180
Yuval Marton, Nizar Habash, and Owen Rambow. 2013a.
Dependency parsing of Modern Standard Arabic with
lexical and inflectional features. Computational Lin-
guistics, 39(1):161?194.
Yuval Marton, Nizar Habash, Owen Rambow, and Sarah
Alkhulani. 2013b. SPMRL?13 shared task system:
The CADIM Arabic dependency parser. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 76?80, Seat-
tle, WA.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of HLT:NAACL, pages 152?159, New York, NY.
Ryan T. McDonald, Koby Crammer, and Fernando C. N.
Pereira. 2005. Online large-margin training of depen-
dency parsers. In Proceedings of ACL, pages 91?98,
Ann Arbor, MI.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar
Tackstrom, Claudia Bedini, Nuria Bertomeu Castello,
and Jungmee Lee. 2013. Universal dependency anno-
tation for multilingual parsing. In Proceedings of ACL,
Sofia, Bulgaria.
Igor Mel?c?uk. 2001. Communicative Organization in Nat-
ural Language: The Semantic-Communicative Struc-
ture of Sentences. J. Benjamins.
Knowledge Center for Processing Hebrew
MILA. 2008. Hebrew morphological analyzer.
http://mila.cs.technion.ac.il.
Antonio Moreno, Ralph Grishman, Susana Lopez, Fer-
nando Sanchez, and Satoshi Sekine. 2000. A treebank
of Spanish and its application to parsing. In Proceed-
ings of LREC, Athens, Greece.
Joakim Nivre and Be?ta Megyesi. 2007. Bootstrapping a
Swedish treeebank using cross-corpus harmonization
and annotation projection. In Proceedings of the 6th
International Workshop on Treebanks and Linguistic
Theories, pages 97?102, Bergen, Norway.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase structure
and dependency annotation. In Proceedings of LREC,
pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007a. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task Ses-
sion of EMNLP-CoNLL 2007, pages 915?932, Prague,
Czech Republic.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
G?ls?en Eryig?it, Sandra K?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 Shared Task on Parsing the Web. In Proceedings
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL), a NAACL-HLT 2012
workshop, Montreal, Canada.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL, Sydney, Australia.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley, Berkeley, CA.
Slav Petrov. 2010. Products of random latent variable
grammars. In Proceedings of HLT: NAACL, pages 19?
27, Los Angeles, CA.
Adam Przepi?rkowski, Miros?aw Ban?ko, Rafa? L. G?rski,
and Barbara Lewandowska-Tomaszczyk, editors. 2012.
Narodowy Korpus Jkezyka Polskiego. Wydawnictwo
Naukowe PWN, Warsaw.
Ines Rehbein and Josef van Genabith. 2007a. Eval-
uating Evaluation Measures. In Proceedings of the
16th Nordic Conference of Computational Linguistics
NODALIDA-2007, Tartu, Estonia.
Ines Rehbein and Josef van Genabith. 2007b. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of EMNLP-CoNLL, Prague, Czech Re-
public.
Ines Rehbein. 2011. Data point selection for self-training.
In Proceedings of the Second Workshop on Statistical
Parsing of Morphologically Rich Languages, pages 62?
67, Dublin, Ireland.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of HLT-NAACL, pages
129?132, New York, NY.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology covering
derivation, composition and inflection. In Proceedings
of LREC, Lisbon, Portugal.
Djam? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
First Workshop on Statistical Parsing of Morphologi-
cally Rich Languages (SPMRL), Los Angeles, CA.
Wolfgang Seeker and Jonas Kuhn. 2012. Making el-
lipses explicit in dependency conversion for a German
181
treebank. In Proceedings of LREC, pages 3132?3139,
Istanbul, Turkey.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined tree
substitution grammars for syntactic parsing. In Pro-
ceedings of ACL, pages 440?448, Jeju, Korea.
Anthony Sigogne, Matthieu Constant, and Eric Laporte.
2011. French parsing enhanced with a word clustering
method based on a syntactic lexicon. In Proceedings
of the Second Workshop on Statistical Parsing of Mor-
phologically Rich Languages, pages 22?27, Dublin,
Ireland.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altmann,
and Noa Nativ. 2001. Building a tree-bank of Modern
Hebrew text. Traitement Automatique des Langues,
42:347?380.
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards
a bank of constituent parse trees for Polish. In Pro-
ceedings of Text, Speech and Dialogue, pages 197?204,
Brno, Czech Republic.
Ulf Teleman. 1974. Manual f?r grammatisk beskrivning
av talad och skriven svenska. Studentlitteratur.
Lucien Tesni?re. 1959. ?l?ments De Syntaxe Structurale.
Klincksieck, Paris.
Reut Tsarfaty and Khalil Sima?an. 2010. Modeling mor-
phosyntactic agreement in constituency-based parsing
of Modern Hebrew. In Proceedings of the First Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Djame Seddah, Yoav Goldberg, Sandra
K?bler, Marie Candito, Jennifer Foster, Yannick Vers-
ley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical
parsing for morphologically rich language (SPMRL):
What, how and whither. In Proceedings of the First
workshop on Statistical Parsing of Morphologically
Rich Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-framework evaluation. In Pro-
ceedings of EMNLP, Edinburgh, UK.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012a. Cross-framework evaluation for statistical pars-
ing. In Proceeding of EACL, Avignon, France.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012b. Joint evaluation for segmentation and parsing.
In Proceedings of ACL, Jeju, Korea.
Reut Tsarfaty, Djam? Seddah, Sandra K?bler, and Joakim
Nivre. 2012c. Parsing morphologically rich languages:
Introduction to the special issue. Computational Lin-
guistics, 39(1):15?22.
Reut Tsarfaty. 2010. Relational-Realizational Parsing.
Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A unified morpho-syntactic scheme
of Stanford dependencies. In Proceedings of ACL,
Sofia, Bulgaria.
Veronika Vincze, D?ra Szauter, Attila Alm?si, Gy?rgy
M?ra, Zolt?n Alexin, and J?nos Csirik. 2010. Hungar-
ian Dependency Treebank. In Proceedings of LREC,
Valletta, Malta.
Joachim Wagner. 2012. Detecting Grammatical Errors
with Treebank-Induced Probabilistic Parsers. Ph.D.
thesis, Dublin City University.
Marcin Wolin?ski, Katarzyna G?owin?ska, and Marek
S?widzin?ski. 2011. A preliminary version of
Sk?adnica?a treebank of Polish. In Proceedings of
the 5th Language & Technology Conference, pages
299?303, Poznan?, Poland.
Alina Wr?blewska. 2012. Polish Dependency Bank. Lin-
guistic Issues in Language Technology, 7(1):1?15.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL:HLT, pages 188?193, Portland,
OR.
J?nos Zsibrita, Veronika Vincze, and Rich?rd Farkas.
2013. magyarlanc: A toolkit for morphological and
dependency parsing of Hungarian. In Proceedings of
RANLP, pages 763?771, Hissar, Bulgaria.
182
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 171?180,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Linguistic Regularities in Sparse and Explicit Word Representations
Omer Levy
?
and Yoav Goldberg
Computer Science Department
Bar-Ilan University
Ramat-Gan, Israel
{omerlevy,yoav.goldberg}@gmail.com
Abstract
Recent work has shown that neural-
embedded word representations capture
many relational similarities, which can be
recovered by means of vector arithmetic
in the embedded space. We show that
Mikolov et al.?s method of first adding
and subtracting word vectors, and then
searching for a word similar to the re-
sult, is equivalent to searching for a word
that maximizes a linear combination of
three pairwise word similarities. Based on
this observation, we suggest an improved
method of recovering relational similar-
ities, improving the state-of-the-art re-
sults on two recent word-analogy datasets.
Moreover, we demonstrate that analogy
recovery is not restricted to neural word
embeddings, and that a similar amount
of relational similarities can be recovered
from traditional distributional word repre-
sentations.
1 Introduction
Deep learning methods for language processing
owe much of their success to neural network lan-
guage models, in which words are represented as
dense real-valued vectors in R
d
. Such representa-
tions are referred to as distributed word represen-
tations or word embeddings, as they embed an en-
tire vocabulary into a relatively low-dimensional
linear space, whose dimensions are latent contin-
uous features. The embedded word vectors are
trained over large collections of text using vari-
ants of neural networks (Bengio et al., 2003; Col-
lobert and Weston, 2008; Mnih and Hinton, 2008;
Mikolov et al., 2011; Mikolov et al., 2013b). The
?
Supported by the European Community?s Seventh
Framework Programme (FP7/2007-2013) under grant agree-
ment no. 287923 (EXCITEMENT).
word embeddings are designed to capture what
Turney (2006) calls attributional similarities be-
tween vocabulary items: words that appear in sim-
ilar contexts will be close to each other in the
projected space. The effect is grouping of words
that share semantic (?dog cat cow?, ?eat devour?)
or syntactic (?cars hats days?, ?emptied carried
danced?) properties, and are shown to be effective
as features for various NLP tasks (Turian et al.,
2010; Collobert et al., 2011; Socher et al., 2011;
Al-Rfou et al., 2013). We refer to such word rep-
resentations as neural embeddings or just embed-
dings.
Recently, Mikolov et al. (2013c) demonstrated
that the embeddings created by a recursive neu-
ral network (RNN) encode not only attributional
similarities between words, but also similarities
between pairs of words. Such similarities are
referred to as linguistic regularities by Mikolov
et al. and as relational similarities by Turney
(2006). They capture, for example, the gen-
der relation exhibited by the pairs ?man:woman?,
?king:queen?, the language-spoken-in relation in
?france:french?, ?mexico:spanish? and the past-
tense relation in ?capture:captured?, ?go:went?.
Remarkably, Mikolov et al. showed that such rela-
tions are reflected in vector offsets between word
pairs (apples ? apple ? cars ? car), and
that by using simple vector arithmetic one could
apply the relation and solve analogy questions of
the form ?a is to a
?
as b is to ?? in which the
nature of the relation is hidden. Perhaps the most
famous example is that the embedded representa-
tion of the word queen can be roughly recovered
from the representations of king, man and woman:
queen ? king ?man+ woman
The recovery of relational similarities using vector
arithmetic on RNN-embedded vectors was evalu-
ated on many relations, achieving state-of-the-art
results in relational similarity identification tasks
171
(Mikolov et al., 2013c; Zhila et al., 2013). It was
later demonstrated that relational similarities can
be recovered in a similar fashion also from embed-
dings trained with different architectures (Mikolov
et al., 2013a; Mikolov et al., 2013b).
This fascinating result raises a question: to what
extent are the relational semantic properties a re-
sult of the embedding process? Experiments in
(Mikolov et al., 2013c) show that the RNN-based
embeddings are superior to other dense represen-
tations, but how crucial is it for a representation to
be dense and low-dimensional at all?
An alternative approach to representing words
as vectors is the distributional similarity repre-
sentation, or bag of contexts. In this representa-
tion, each word is associated with a very high-
dimensional but sparse vector capturing the con-
texts in which the word occurs. We call such vec-
tor representations explicit, as each dimension di-
rectly corresponds to a particular context. These
explicit vector-space representations have been
extensively studied in the NLP literature (see (Tur-
ney and Pantel, 2010; Baroni and Lenci, 2010) and
the references therein), and are known to exhibit
a large extent of attributional similarity (Pereira
et al., 1993; Lin, 1998; Lin and Pantel, 2001;
Sahlgren, 2006; Kotlerman et al., 2010).
In this study, we show that similarly to the
neural embedding space, the explicit vector space
also encodes a vast amount of relational similar-
ity which can be recovered in a similar fashion,
suggesting the explicit vector space representation
as a competitive baseline for further work on neu-
ral embeddings. Moreover, this result implies that
the neural embedding process is not discovering
novel patterns, but rather is doing a remarkable
job at preserving the patterns inherent in the word-
context co-occurrence matrix.
A key insight of this work is that the vector
arithmetic method can be decomposed into a linear
combination of three pairwise similarities (Section
3). While mathematically equivalent, we find that
thinking about the method in terms of the decom-
posed formulation is much less puzzling, and pro-
vides a better intuition on why we would expect
the method to perform well on the analogy re-
covery task. Furthermore, the decomposed form
leads us to suggest a modified optimization objec-
tive (Section 6), which outperforms the state-of-
the-art at recovering relational similarities under
both representations.
2 Explicit Vector Space Representation
We adopt the traditional word representation used
in the distributional similarity literature (Turney
and Pantel, 2010). Each word is associated with
a sparse vector capturing the contexts in which it
occurs. We call this representation explicit, as each
dimension corresponds to a particular context.
For a vocabulary V and a set of contexts C,
the result is a |V |?|C| sparse matrix S in which
S
ij
corresponds to the strength of the association
between word i and context j. The association
strength between a word w ? V and a context
c ? C can take many forms. We chose to use
the popular positive pointwise mutual information
(PPMI) metric:
S
ij
= PPMI(w
i
, c
j
)
PPMI(w, c) =
{
0 PMI(w, c) < 0
PMI(w, c) otherwise
PMI(w, c) = log
P (w,c)
P (w)P (c)
= log
freq(w,c)|corpus|
freq(w)freq(c)
where |corpus| is the number of items in the cor-
pus, freq(w, c) is the number of times word w
appeared in context c in the corpus, and freq(w),
freq(c) are the corpus frequencies of the word
and the context respectively.
The use of PMI in distributional similarity mod-
els was introduced by Church and Hanks (1990)
and widely adopted (Dagan et al., 1994; Turney,
2001). The PPMI variant dates back to at least
(Niwa and Nitta, 1994), and was demonstrated to
perform very well in Bullinaria and Levy (2007).
In this work, we take the linear contexts in
which words appear. We consider each word sur-
rounding the target word w in a window of 2 to
each side as a context, distinguishing between dif-
ferent sequential positions. For example, in the
sentence a b c d e the contexts of the word c
are a
?2
, b
?1
, d
+1
and e
+2
. Each vector?s dimen-
stion is thus |C| ? 4 |V |. Empirically, the num-
ber of non-zero dimensions for vocabulary items
in our corpus ranges between 3 (for some rare to-
kens) and 474,234 (for the word ?and?), with a
mean of 1595 and a median of 415.
Another popular choice of context is the syntac-
tic relations the word participates in (Lin, 1998;
Pad?o and Lapata, 2007; Levy and Goldberg,
2014). In this paper, we chose the sequential
context as it is compatible with the information
available to the state-of-the-art neural embedding
method we are comparing against.
172
3 Analogies and Vector Arithmetic
Mikolov et al. demonstrated that vector space rep-
resentations encode various relational similarities,
which can be recovered using vector arithmetic
and used to solve word-analogy tasks.
3.1 Analogy Questions
In a word-analogy task we are given two pairs of
words that share a relation (e.g. ?man:woman?,
?king:queen?). The identity of the fourth word
(?queen?) is hidden, and we need to infer it based
on the other three (e.g. answering the question:
?man is to woman as king is to ? ??). In the rest
of this paper, we will refer to the four words as
a:a
?
, b:b
?
. Note that the type of the relation is
not explicitly provided in the question, and solv-
ing the question correctly (by a human) involves
first inferring the relation, and then applying it to
the third word (b).
3.2 Vector Arithmetic
Mikolov et al. showed that relations between
words are reflected to a large extent in the
offsets between their vector embeddings
(queen ? king ? woman ? man),
and thus the vector of the hidden word b
?
will be
similar to the vector b ? a + a
?
, suggesting that
the analogy question can be solved by optimizing:
argmax
b
?
?V
(sim (b
?
, b? a+ a
?
))
where V is the vocabulary excluding the question
words b, a and a
?
, and sim is a similarity mea-
sure. Specifically, they used the cosine similarity
measure, defined as:
cos (u, v) =
u ? v
?u??v?
resulting in:
argmax
b
?
?V
(cos (b
?
, b? a+ a
?
)) (1)
Since cosine is inverse to the angle, high cosine
similarity (close to 1) means that the vectors share
a very similar direction. Note that this metric nor-
malizes (and thus ignores) the vectors? lengths,
unlike the Euclidean distance between them. For
reasons that will be clear later, we refer to (1) as
the 3COSADD method.
An alternative to 3COSADD is to require that
the direction of transformation be conserved:
argmax
b
?
?V
(cos (b
?
? b, a
?
? a)) (2)
This basically means that b
?
? b shares the same
direction with a
?
? a, ignoring the distances. We
refer to this method as PAIRDIRECTION. Though
it was not mentioned in the paper, Mikolov
et al. (2013c) used PAIRDIRECTION for solving
the semantic analogies of the SemEval task, and
3COSADD for solving the syntactic analogies.
1
3.3 Reinterpreting Vector Arithmetic
In Mikolov et al.?s experiments, all word-vectors
were normalized to unit length. Under such nor-
malization, the argmax in (1) is mathematically
equivalent to (derived using basic algebra):
argmax
b
?
?V
(cos (b
?
, b)? cos (b
?
, a) + cos (b
?
, a
?
))
(3)
This means that solving analogy questions with
vector arithmetic is mathematically equivalent to
seeking a word (b
?
) which is similar to b and a
?
but is different from a. Relational similarity is
thus expressed as a sum of attributional similari-
ties. While (1) and (3) are equal, we find the intu-
ition as to why (3) ought to find analogies clearer.
4 Empirical Setup
We derive explicit and neural-embedded vec-
tor representations, and compare their capacities
to recover relational similarities using objectives
3COSADD (eq. 3) and PAIRDIRECTION (eq. 2).
Underlying Corpus and Preprocessing Previ-
ous reported results on the word analogy tasks us-
ing vector arithmetics were obtained using propri-
etary corpora. To make our experiments repro-
ducible, we selected an open and widely accessi-
ble corpus ? the English Wikipedia. We extracted
all sentences from article bodies (excluding ti-
tles, infoboxes, captions, etc) and filtered non-
alphanumeric tokens, allowing mid-token symbols
as apostrophes, hyphens, commas, and periods.
All the text was lowercased. Duplicates and sen-
tences with less than 5 tokens were then removed.
Overall, we retained a corpus of about 1.5 billion
tokens, in 77.5 million sentences.
Word Representations To create contexts for
both embedding and sparse representation, we
used a window of two tokens to each side (5-
grams, in total), ignoring words that appeared less
1
This was confirmed both by our independent trials and
by corresponding with the authors.
173
than 100 times in the corpus. The filtered vocabu-
lary contained 189,533 terms.
2
The explicit vector representations were created
as described in Section 2. The neural embeddings
were created using the word2vec software
3
ac-
companying (Mikolov et al., 2013b). We embed-
ded the vocabulary into a 600 dimensional space,
using the state-of-the-art skip-gram architecture,
the negative-training approach with 15 negative
samples (NEG-15), and sub-sampling of frequent
words with a parameter of 10
?5
. The parameter
settings follow (Mikolov et al., 2013b).
4.1 Evaluation Conditions
We evaluate the different word representations us-
ing the three datasets used in previous work. Two
of them (MSR and GOOGLE) contain analogy
questions, while the third (SEMEVAL) requires
ranking of candidate word pairs according to their
relational similarity to a set of supplied word pairs.
Open Vocabulary The open vocabulary
datasets (MSR and GOOGLE) present questions
of the form ?a is to a
?
as b is to b
?
?, where b
?
is hidden, and must be guessed from the entire
vocabulary. Performance on these datasets is
measured by micro-averaged accuracy.
The MSR dataset
4
(Mikolov et al., 2013c) con-
tains 8000 analogy questions. The relations por-
trayed by these questions are morpho-syntactic,
and can be categorized according to parts of
speech ? adjectives, nouns and verbs. Adjec-
tive relations include comparative and superlative
(good is to best as smart is to smartest). Noun
relations include single and plural, possessive and
non-possessive (dog is to dog?s as cat is to cat?s).
Verb relations are tense modifications (work is to
worked as accept is to accepted).
The GOOGLE dataset
5
(Mikolov et al., 2013a)
contains 19544 questions. It covers 14 relation
types, 7 of which are semantic in nature and 7
are morpho-syntactic (enumerated in Section 8).
The dataset was created by manually constructing
example word-pairs of each relation, and provid-
ing all the pairs of word-pairs (within each relation
type) as analogy questions.
2
Initial experiments with different window-sizes and cut-
offs showed similar trends.
3
http://code.google.com/p/word2vec
4
research.microsoft.com/en-us/
projects/rnn/
5
code.google.com/p/word2vec/source/
browse/trunk/questions-words.txt
Out-of-vocabulary words
6
were removed from
both test sets.
Closed Vocabulary The SEMEVAL dataset con-
tains the collection of 79 semantic relations that
appeared in SemEval 2012 Task 2: Measuring Re-
lation Similarity (Jurgens et al., 2012). Each rela-
tion is exemplified by a few (usually 3) character-
istic word-pairs. Given a set of several dozen tar-
get word pairs, which supposedly have the same
relation, the task is to rank the target pairs ac-
cording to the degree in which this relation holds.
This can be cast as an analogy question in the
following manner: For example, take the Recipi-
ent:Instrument relation with the prototypical word
pairs king:crown and police:badge. To measure
the degree that a target word pair wife:ring has the
same relation, we form the two analogy questions
?king is to crown as wife is to ring? and ?police is
to badge as wife is to ring?. We calculate the score
of each analogy, and average the results. Note that
as opposed to the first two test sets, this one does
not require searching the entire vocabulary for the
most suitable word in the corpus, but rather to rank
a list of existing word pairs.
Following previous work, performance on SE-
MEVAL was measured using accuracy, macro-
averaged across all the relations.
5 Preliminary Results
Our first experiment uses 3COSADD (method (3)
in Section 3) to measure the prevalence of linguis-
tic regularities within each representation.
Representation MSR GOOGLE SEMEVAL
Embedding 53.98% 62.70% 38.49%
Explicit 29.04% 45.05% 38.54%
Table 1: Performance of 3COSADD on different tasks with
the explicit and neural embedding representations.
The results in Table 1 show that a large amount
of relational similarities can be recovered with
both representations. In fact, both representations
achieve the same accuracy on the SEMEVAL task.
However, there is a large performance gap in favor
of the neural embedding in the open-vocabulary
MSR and GOOGLE tasks.
Next, we run the same experiment with
PAIRDIRECTION (method (2) in Section 3).
6
i.e. words that appeared in English Wikipedia less
than 100 times. This removed 882 instances from the
MSR dataset and 286 instances from GOOGLE.
174
Representation MSR GOOGLE SEMEVAL
Embedding 9.26% 14.51% 44.77%
Explicit 0.66% 0.75% 45.19%
Table 2: Performance of PAIRDIRECTION on different tasks
with the explicit and neural embedding representations.
The results in Table 2 show that the PAIRDI-
RECTION method is better than 3COSADD on
the restricted-vocabulary SEMEVAL task (accu-
racy jumps from 38% to 45%), but fails at the
open-vocabulary questions in GOOGLE and MSR.
When the method does work, the numbers for the
explicit and embedded representations are again
comparable to one another.
Why is PAIRDIRECTION performing so well
on the SEMEVAL task, yet so poorly on the oth-
ers? Recall that the PAIRDIRECTION objective
focuses on the similarity of b
?
? b and a
?
? a,
but does not take into account the spatial distances
between the individual vectors. Relying on di-
rection alone, while ignoring spatial distance, is
problematic when considering the entire vocabu-
lary as candidates (as is required in the MSR and
GOOGLE tasks). We are likely to find candidates
b
?
that have the same relation to b as reflected by
a ? a
?
but are not necessarily similar to b. As a
concrete example, in man:woman, king:?, we are
likely to recover feminine entities, but not neces-
sarily royal ones. The SEMEVAL test set, on the
other hand, already provides related (and therefore
geometrically close) candidates, leaving mainly
the direction to reason about.
6 Refining the Objective Function
The 3COSADD objective, as expressed in (3), re-
veals a ?balancing act? between two attractors and
one repeller, i.e. two terms that we wish to maxi-
mize and one that needs to be minimized:
argmax
b
?
?V
(cos (b
?
, b)? cos (b
?
, a) + cos (b
?
, a
?
))
A known property of such linear objectives is that
they exhibit a ?soft-or? behavior and allow one
sufficiently large term to dominate the expression.
This behavior is problematic in our setup, because
each term reflects a different aspect of similarity,
and the different aspects have different scales. For
example, king is more royal than it is masculine,
and will therefore overshadow the gender aspect
of the analogy. It is especially true in the case of
explicit vector representations, as each aspect of
the similarity is manifested by a different set of
features with varying sizes and weights.
A case in point is the analogy question ?London
is to England as Baghdad is to ? ??, which we
answer using:
argmax
x?V
(cos (x, en)? cos (x, lo) + cos (x, ba))
We seek a word (Iraq) which is similar to Eng-
land (both are countries), is similar to Baghdad
(similar geography/culture) and is dissimilar to
London (different geography/culture). Maximiz-
ing the sum yields an incorrect answer (under both
representations): Mosul, a large Iraqi city. Look-
ing at the computed similarities in the explicit vec-
tor representation, we see that both Mosul and Iraq
are very close to Baghdad, and are quite far from
England and London:
(EXP) ? England ? London ? Baghdad Sum
Mosul 0.031 0.031 0.244 0.244
Iraq 0.049 0.038 0.206 0.217
The same trends appear in the neural embedding
vectors, though with different similarity scores:
(EMB) ? England ? London ? Baghdad Sum
Mosul 0.130 0.141 0.755 0.748
Iraq 0.153 0.130 0.631 0.655
While Iraq is much more similar to England than
Mosul is (both being countries), both similarities
(0.049 and 0.031 in explicit, 0.130 and 0.153 in
embedded) are small and the sums are dominated
by the geographic and cultural aspect of the anal-
ogy: Mosul and Iraq?s similarity to Baghdad (0.24
and 0.20 in explicit, 0.75 and 0.63 in embedded).
To achieve better balance among the different
aspects of similarity, we propose switching from
an additive to a multiplicative combination:
argmax
b
?
?V
cos (b
?
, b) cos (b
?
, a
?
)
cos (b
?
, a) + ?
(4)
(? = 0.001 is used to prevent division by zero)
This is equivalent to taking the logarithm of each
term before summation, thus amplifying the dif-
ferences between small quantities and reducing
the differences between larger ones. Using this ob-
jective, Iraq is scored higher than Mosul (0.259 vs
0.236, 0.736 vs 0.691). We refer to objective (4)
as 3COSMUL.
7
7
3COSMUL requires that all similarities be non-negative,
which trivially holds for explicit representations. With em-
beddings, we transform cosine similarities to [0, 1] using
(x+ 1)/2 before calculating (4).
175
7 Main Results
We repeated the experiments, this time using the
3COSMUL method. Table 3 presents the results,
showing that the multiplicative objective recov-
ers more relational similarities in both representa-
tions. The improvements achieved in the explicit
representation are especially dramatic, with an ab-
solute increase of over 20% correctly identified re-
lations in the MSR and GOOGLE datasets.
Objective Representation MSR GOOGLE
3COSADD
Embedding 53.98% 62.70%
Explicit 29.04% 45.05%
3COSMUL
Embedding 59.09% 66.72%
Explicit 56.83% 68.24%
Table 3: Comparison of 3COSADD and 3COSMUL.
3COSMUL outperforms the state-of-the-art
(3COSADD) on these two datasets. Moreover, the
results illustrate that a comparable amount of rela-
tional similarities can be recovered with both rep-
resentations. This suggests that the linguistic reg-
ularities apparent in neural embeddings are not a
consequence of the embedding process, but rather
are well preserved by it.
On SEMEVAL, 3COSMUL preformed on par
with 3COSADD , recovering a similar amount of
analogies with both explicit and neural representa-
tions (38.37% and 38.67%, respectively).
8 Error Analysis
With 3COSMUL, both the explicit vectors and
the neural embeddings recover similar amounts of
analogies, but are these the same patterns, or per-
haps different types of relational similarities?
8.1 Agreement between Representations
Considering the open-vocabulary tasks (MSR and
GOOGLE), we count the number of times both rep-
resentations guessed correctly, both guessed in-
correctly, and when one representations leads to
the right answer while the other does not (Ta-
ble 4). While there is a large amount of agreement
between the representations, there is also a non-
negligible amount of cases in which they comple-
ment each other. If we were to run in an ora-
cle setup, in which an answer is considered cor-
rect if it is correct in either representation, we
would have achieved an accuracy of 71.9% on the
MSR dataset and 77.8% on GOOGLE.
Both Both Embedding Explicit
Correct Wrong Correct Correct
MSR 43.97% 28.06% 15.12% 12.85%
GOOGLE 57.12% 22.17% 9.59% 11.12%
ALL 53.58% 23.76% 11.08% 11.59%
Table 4: Agreement between the representations on open-
vocabulary tasks.
Relation Embedding Explicit
G
O
O
G
L
E
capital-common-countries 90.51% 99.41%
capital-world 77.61% 92.73%
city-in-state 56.95% 64.69%
currency 14.55% 10.53%
family (gender inflections) 76.48% 60.08%
gram1-adjective-to-adverb 24.29% 14.01%
gram2-opposite 37.07% 28.94%
gram3-comparative 86.11% 77.85%
gram4-superlative 56.72% 63.45%
gram5-present-participle 63.35% 65.06%
gram6-nationality-adjective 89.37% 90.56%
gram7-past-tense 65.83% 48.85%
gram8-plural (nouns) 72.15% 76.05%
gram9-plural-verbs 71.15% 55.75%
M
S
R
adjectives 45.88% 56.46%
nouns 56.96% 63.07%
verbs 69.90% 52.97%
Table 5: Breakdown of relational similarities in each repre-
sentation by relation type, using 3COSMUL.
8.2 Breakdown by Relation Type
Table 5 presents the amount of analogies dis-
covered in each representation, broken down by
relation type. Some trends emerge: the ex-
plicit representation is superior in some of the
more semantic tasks, especially geography re-
lated ones, as well as the ones superlatives and
nouns. The neural embedding, however, has the
upper hand on most verb inflections, compara-
tives, and family (gender) relations. Some rela-
tions (currency, adjectives-to-adverbs, opposites)
pose a challenge to both representations, though
are somewhat better handled by the embedded
representations. Finally, the nationality-adjectives
and present-participles are equally handled by
both representations.
8.3 Default-Behavior Errors
The most common error pattern under both repre-
sentations is that of a ?default behavior?, in which
one central representative word is provided as an
answer to many questions of the same type. For
example, the word ?Fresno? is returned 82 times
as an incorrect answer in the city-in-state rela-
tion in the embedded representation, and the word
?daughter? is returned 47 times as an incorrect an-
swer in the family relation in the explicit represen-
176
RELATION WORD EMB EXP
gram7-past-tense who 0 138
city-in-state fresno 82 24
gram6-nationality-adjective slovak 39 39
gram6-nationality-adjective argentine 37 39
gram6-nationality-adjective belarusian 37 39
gram8-plural (nouns) colour 36 35
gram3-comparative higher 34 35
city-in-state smith 1 61
gram7-past-tense and 0 49
gram1-adjective-to-adverb be 0 47
family (gender inflections) daughter 8 47
city-in-state illinois 3 40
currency currency 5 40
gram1-adjective-to-adverb and 0 39
gram7-past-tense enhance 39 20
Table 6: Common default-behavior errors under both repre-
sentations. EMB / EXP: the number of time the word was
returned as an incorrect answer for the given relation under
the embedded or explicit representation.
tation. Loosely, ?Fresno? is identified by the em-
bedded representation as a prototypical location,
while ?daughter? is identified by the explicit rep-
resentation as a prototypical female. Under a def-
inition in which a default behavior error is one in
which the same incorrect answer is returned for a
particular relation 10 or more times, such errors
account for 49% of the errors in the explicit repre-
sentation, and for 39% of the errors in the embed-
ded representation.
Table 6 lists the 15 most common default er-
rors under both representations. In most default er-
rors the category of the default word is closely re-
lated to the analogy question, sharing the category
of either the correct answer, or (as in the case of
?Fresno?) the question word. Notable exceptions
are the words ?who?, ?and?, ?be? and ?smith? that
are returned as default answers in the explicit rep-
resentation, and which are very far from the in-
tended relation. It seems that in the explicit repre-
sentation, some very frequent function words act
as ?hubs? and confuse the model. In fact, the
performance gap between the representations in
the past-tense and plural-verb relations can be at-
tributed specifically to such function-word errors:
23.4% of the mistakes in past-tense relation are
due to the explicit representation?s default answer
of ?who? or ?and?, while 19% of the mistakes in
the plural-verb relations are due to default answers
of ?is/and/that/who?.
8.4 Verb-inflection Errors
A correct solution to the morphological anal-
ogy task requires recovering both the correct in-
flection (requiring syntactic similarity) and the
correct base word (requiring semantic similar-
ity). We observe that linguistically, the mor-
phological distinctions and similarities tend to
rely on a few common word forms (for exam-
ple, the ?walk:walking? relation is characterized
by modals such as ?will? appearing before ?walk?
and never before ?walking?, and be verbs ap-
pearing before walking and never before ?walk?),
while the support for the semantic relations is
spread out over many more items. We hypothe-
size that the morphological distinctions in verbs
are much harder to capture than the semantics. In-
deed, under both representations, errors in which
the selected word has a correct form with an incor-
rect inflection are over ten times more likely than
errors in which the selected word has the correct
inflection but an incorrect base form.
9 Interpreting Relational Similarities
The ability to capture relational similarities by
performing vector (or similarity) arithmetic is re-
markable. In this section, we try and provide intu-
ition as to why it works.
Consider the word ?king?; it has several aspects,
high-level properties that it implies, such as roy-
alty or (male) gender, and its attributional simi-
larity with another word is based on a mixture of
those aspects; e.g. king is related to queen on the
royalty and the human axes, and shares the gender
and the human aspect with man. Relational simi-
larities can be viewed as a composition of attribu-
tional similarities, each one reflecting a different
aspect. In ?man is to woman as king is to queen?,
the two main aspects are gender and royalty. Solv-
ing the analogy question involves identifying the
relevant aspects, and trying to change one of them
while preserving the other.
How are concepts such as gender, royalty, or
?cityness? represented in the vector space? While
the neural embeddings are mostly opaque, one of
the appealing properties of explicit vector repre-
sentations is our ability to read and understand the
vectors? features. For example, king is represented
in our explicit vector space by 51,409 contexts, of
which the top 3 are tut
+1
, jeongjo
+1
, adulyadej
+2
? all names of monarchs. The explicit representa-
tion allows us to glimpse at the way different as-
pects are represented. To do so, we choose a repre-
sentative pair of words that share an aspect, inter-
sect their vectors, and inspect the highest scoring
177
Aspect Examples Top Features
Female woman queen estrid
+1
ketevan
+1
adeliza
+1
nzinga
+1
gunnhild
+1
impregnate
?2
hippolyta
+1
Royalty queen king savang
+1
uncrowned
?1
pmare
+1
sisowath
+1
nzinga
+1
tupou
+1
uvea
+2
majesty
?1
Currency yen ruble devalue
?2
banknote
+1
denominated
+1
billion
?1
banknotes
+1
pegged
+2
coin
+1
Country germany  australia emigrates
?2
1943-45
+2
pentathletes
?2
emigrated
?2
emigrate
?2
hong-kong
?1
Capital berlin canberra hotshots
?1
embassy
?2
1925-26
+2
consulate-general
+2
meetups
?2
nunciature
?2
Superlative sweetest tallest freshest
+2
asia?s
?1
cleveland?s
?2
smartest
+1
world?s
?1
city?s
?1
america?s
?1
Height taller  tallest regnans
?2
skyscraper
+1
skyscrapers
+1
6?4
+2
windsor?s
?1
smokestacks
+1
burj
+2
Table 7: The top features of each aspect, recovered by pointwise multiplication of words that share that aspect. The result of
pointwise multiplication is an ?aspect vector? in which the features common to both words, characterizing the relation, receive
the highest scores. The feature scores (not shown) correspond to the weight the feature contributes to the cosine similarity
between the vectors. The superscript marks the position of the feature relative to the target word.
features in the intersection. Table 7 presents the
top (most influential) features of each aspect.
Many of these features are names of people or
places, which appear rarely in our corpus (e.g.
Adeliza, a historical queen, and Nzinga, a royal
family) but are nonetheless highly indicative of
the shared concept. The prevalence of rare words
stems from PMI, which gives them more weight,
and from the fact that words like woman and queen
are closely related (a queen is a woman), and thus
have many features in common. Ordering the fea-
tures of woman  queen by prevalence reveals
female pronouns (?she?, ?her?) and a long list of
common feminine names, reflecting the expected
aspect shared by woman and queen. Word pairs
that share more specific aspects, such as capital
cities or countries, show features that are charac-
teristic of their shared aspect (e.g. capital cities
have embassies and meetups, while immigration
is associated with countries). It is also interesting
to observe how the relatively syntactic ?superlativ-
ity? aspect is captured with many regional posses-
sives (?america?s?, ?asia?s?, ?world?s?).
10 Related Work
Relational similarity (and answering analogy
questions) was previously tackled using explicit
representations. Previous approaches use task-
specific information, by either relying on a
(word-pair, connectives) matrix rather than the
standard (word, context) matrix (Turney and
Littman, 2005; Turney, 2006), or by treating anal-
ogy detection as a supervised learning task (Ba-
roni and Lenci, 2009; Jurgens et al., 2012; Turney,
2013). In contrast, the vector arithmetic approach
followed here is unsupervised, and works on a
generic single-word representation. Even though
the training process is oblivious to the task of anal-
ogy detection, the resulting representation is able
to detect them quite accurately. Turney (2012) as-
sumes a similar setting but with two types of word
similarities, and combines them with products and
ratios (similar to 3COSMUL) to recover a variety
of semantic relations, including analogies.
Arithmetic combination of explicit word vec-
tors is extensively studied in the context of com-
positional semantics (Mitchell and Lapata, 2010),
where a phrase composed of two or more words
is represented by a single vector, computed by a
function of its component word vectors. Blacoe
and Lapata (2012) compare different arithmetic
functions across multiple representations (includ-
ing embeddings) on a range of compositionality
benchmarks. To the best of our knowledge such
methods of word vector arithmetic have not been
explored for recovering relational similarities in
explicit representations.
11 Discussion
Mikolov et al. showed how an unsupervised neural
network can represent words in a space that ?nat-
urally? encodes relational similarities in the form
of vector offsets. This study shows that finding
analogies through vector arithmetic is actually a
form of balancing word similarities, and that, con-
trary to the recent findings of Baroni et al. (2014),
under certain conditions traditional word similar-
ities induced by explicit representations can per-
form just as well as neural embeddings on this
task.
Learning to represent words is a fascinating and
important challenge with implications to most cur-
rent NLP efforts, and neural embeddings in par-
ticular are a promising research direction. We
believe that to improve these representations we
should understand how they work, and hope that
the methods and insights provided in this work
will help to deepen our grasp of current and future
investigations of word representations.
178
References
Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representations
for multilingual nlp. In Proc. of CoNLL 2013.
Marco Baroni and Alessandro Lenci. 2009. One dis-
tributional memory, many semantic spaces. In Pro-
ceedings of the Workshop on Geometrical Models
of Natural Language Semantics, pages 1?8, Athens,
Greece, March. Association for Computational Lin-
guistics.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673?721.
Marco Baroni, Georgiana Dinu, and Germ?an
Kruszewski. 2014. Dont count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers), Baltimore, Maryland, USA, June. Association
for Computational Linguistics.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 546?556, Jeju Island, Korea,
July. Association for Computational Linguistics.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
havior Research Methods, 39(3):510?526.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22?29.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning, pages 160?167.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Ido Dagan, Fernando Pereira, and Lillian Lee. 1994.
Similarity-based estimation of word cooccurrence
probabilities. In Proceedings of the 32nd annual
meeting on Association for Computational Linguis-
tics, pages 272?278. Association for Computational
Linguistics.
David A Jurgens, Peter D Turney, Saif M Mohammad,
and Keith J Holyoak. 2012. Semeval-2012 task 2:
Measuring degrees of relational similarity. In Pro-
ceedings of the First Joint Conference on Lexical
and Computational Semantics, pages 356?364. As-
sociation for Computational Linguistics.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359?389.
Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), Balti-
more, Maryland, USA, June. Association for Com-
putational Linguistics.
Dekang Lin and Patrick Pantel. 2001. Dirt: discovery
of inference rules from text. In KDD, pages 323?
328.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics - Volume 2, ACL ?98, pages
768?774, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Tomas Mikolov, Stefan Kombrink, Lukas Burget,
JH Cernocky, and Sanjeev Khudanpur. 2011.
Extensions of recurrent neural network language
model. In Acoustics, Speech and Signal Processing
(ICASSP), 2011 IEEE International Conference on,
pages 5528?5531. IEEE.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their com-
positionality. In Advances in Neural Information
Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. Pro-
ceedings of a meeting held December 5-8, 2013,
Lake Tahoe, Nevada, United States, pages 3111?
3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746?751, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1439.
179
Andriy Mnih and Geoffrey E Hinton. 2008. A scal-
able hierarchical distributed language model. In Ad-
vances in Neural Information Processing Systems,
pages 1081?1088.
Yoshiki Niwa and Yoshihiko Nitta. 1994. Co-
occurrence vectors from corpora vs. distance vec-
tors from dictionaries. In Proceedings of the 15th
conference on Computational linguistics-Volume 1,
pages 304?309. Association for Computational Lin-
guistics.
Sebastian Pad?o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of english words. In
Proceedings of the 31st annual meeting on Associa-
tion for Computational Linguistics, pages 183?190.
Association for Computational Linguistics.
Magnus Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Stock-
holm.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151?161. Association for
Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Peter D. Turney and Michael L. Littman. 2005.
Corpus-based learning of analogies and semantic re-
lations. Machine Learning, 60(1-3):251?278.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1):141?188.
Peter D. Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. In Proceedings of the 12th
European Conference on Machine Learning, pages
491?502. Springer-Verlag.
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Computational Linguistics, 32(3):379?416.
Peter D. Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533?
585.
Peter D. Turney. 2013. Distributional semantics be-
yond words: Supervised learning of analogy and
paraphrase. CoRR, abs/1310.5042.
Alisa Zhila, Wen-tau Yih, Christopher Meek, Geof-
frey Zweig, and Tomas Mikolov. 2013. Combining
heterogeneous models for measuring relational sim-
ilarity. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1000?1009, Atlanta, Georgia, June.
Association for Computational Linguistics.
180
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 66?70,
Baltimore, Maryland USA, June 26 2014.
c
?2014 Association for Computational Linguistics
Intermediary Semantic Representation
through Proposition Structures
Gabriel Stanovsky
?
, Jessica Ficler
?
, Ido Dagan, Yoav Goldberg
Computer Science Department, Bar-Ilan University
?Both authors equally contributed to this paper
{gabriel.satanovsky,jessica.ficler,yoav.goldberg}@gmail.com
dagan@cs.biu.ac.il
Abstract
We propose an intermediary-level seman-
tic representation, providing a higher level
of abstraction than syntactic parse trees,
while not committing to decisions in cases
such as quantification, grounding or verb-
specific roles assignments. The proposal
is centered around the proposition struc-
ture of the text, and includes also im-
plicit propositions which can be inferred
from the syntax but are not transparent in
parse trees, such as copular relations intro-
duced by appositive constructions. Other
benefits over dependency-trees are ex-
plicit marking of logical relations between
propositions, explicit marking of multi-
word predicate such as light-verbs, and a
consistent representation for syntactically-
different but semantically-similar struc-
tures. The representation is meant to
serve as a useful input layer for semantic-
oriented applications, as well as to provide
a better starting point for further levels of
semantic analysis such as semantic-role-
labeling and semantic-parsing.
1 Introduction
Parsers for semantic formalisms (such as Neo-
davidsonian (Artzi and Zettlemoyer, 2013) and
DRT (Kamp, 1988)) take unstructured natural lan-
guage text as input, and output a complete seman-
tic representation, aiming to capture the mean-
ing conveyed by the text. We suggest that this
task may be effectively separated into a sequential
combination of two different tasks. The first of
these tasks is syntactic abstraction over phenom-
ena such as expression of tense, negation, modal-
ity, and passive versus active voice, which are all
either expressed or implied from syntactic struc-
ture. The second task is semantic interpretation
over the syntactic abstraction, deriving quantifi-
cation, grounding, etc. Current semantic parsers
(such as Boxer (Bos, 2008)) tackle these tasks si-
multaneously, mixing syntactic and semantic is-
sues in a single framework. We believe that sepa-
rating semantic parsing into two well defined tasks
will help to better target and identify challenges
in syntactic and semantic domains. Challenges
which are often hidden due to the one-step archi-
tecture of current parsers.
Many of today?s semantic parsers, and semantic
applications in general, leverage dependency pars-
ing (De Marneffe and Manning, 2008a) as an ab-
straction layer, since it directly represents syntac-
tic dependency relations between predicates and
arguments. Some systems exploit Semantic Role
Labeling (SRL) (Carreras and M?arquez, 2005),
where predicate-argument relationships are cap-
tured at a thematic (rather than syntactic) level,
though current SRL technology is less robust and
accurate for open domains than syntactic pars-
ing. While dependency structures and semantic
roles capture much of the proposition structure of
sentences, there are substantial aspects which are
not covered by these representations and therefore
need to be handled by semantic applications on
their own (or they end up being ignored).
Such aspects, as detailed in Section 3, include
propositions which are not expressed directly as
such but are rather implied by syntactic struc-
ture, like nominalizations, appositions and pre-
modifying adjectives. Further, the same proposi-
tion structure may be expressed in many differ-
ent ways by the syntactic structure, forcing sys-
tems to recognize this variability and making the
task of recognizing semantic roles harder. Other
aspects not addressed by common representations
include explicit marking of links between propo-
sitions within a sentence, which affect their asser-
tion or truth status, and the recognition of multi-
word predicates (e.g., considering ?take a deci-
66
Figure 1: Proposed representation for the sentence: ?If you
leave the park, you will find the Peak Tram terminal?
sion? as a single predicate, rather than considering
decision as an argument).
In this position paper we propose an intermedi-
ary representation level for the first syntactic ab-
straction phase described above, intended to re-
place syntactic parsing as a more abstract repre-
sentation layer. It is designed to capture the full
proposition structure which is expressed, either
explicitly or implicitly, by the syntactic structure
of sentences. Thus, we aim to both extract im-
plicit propositions as well as to abstract away syn-
tactic variations which yield the same proposition
structure. At the same time, we aim to remain at
a representation level that corresponds to syntac-
tic properties and relationships, while avoiding se-
mantic interpretations, to be targeted by systems
implementing the further step of semantic inter-
pretation, as discussed above.
In addition, we suggest our representation as a
useful input for semantic applications which need
to recognize the proposition structure of sentences
in order to identify targeted information, such as
Question Answering(QA), Information Extraction
(IE) and multidocument summarization. We ex-
pect that our representation may be more useful
in comparison with current popular use of depen-
dency parsing, in such applications.
2 Representation Scheme
Our representation is centered around proposi-
tions, where a proposition is a statement for which
a truth-value can be assigned. We propose to rep-
resent sentences as a set of inter-linked proposi-
tions. Each proposition is composed of one pred-
icate and a set of arguments. An example rep-
resentation can be seen in Figure 1. Predicates
are usually centered around verbs, and we con-
sider multi-word verbs (e.g., ?take apart?) as sin-
gle predicates. Both the predicates and arguments
are represented as sets of feature-value pairs. Each
argument is marked with a relation to its predicate,
and the same argument can appear in different
propositions. The relation-set we use is syntactic
in nature, including relations such as Subject,
Object, and Preposition with, in contrast
to semantic relations such as instrument.
Canonical Representation The same proposi-
tion can be realized syntactically in many forms.
An important goal of our proposal is abstracting
over idiosyncrasies in the syntactic structure and
presenting unified structures when possible. We
canonicalize on two levels:
? We canonicalize each predicate and argument
by representing each predicate as its main
lemma, and indicating other aspects of the
predication (e.g., tense, negation and time) as
features; Similarly, we mark arguments with
features such as definiteness and plurality.
? We canonicalize the argument structure by
abstracting away over word order and phe-
nomena such as topicalization and pas-
sive/active voice, and present a unified rep-
resentation in terms of the argument roles (so
that, for example, in the sentence ?the door
was opened? the argument ?door? will re-
ceive the object role, with the passive be-
ing indicated as a feature of the predicate).
Relations Between Propositions Some propo-
sitions must be interpreted taking into account
their relations to other propositions. These in-
clude conditionals (?if congress does nothing,
President Bush will have won? (wsj 0112));
temporal relations (?UAL?s announcement came
after the market closed yesterday?(wsj 0112));
and conjunctions (?They operate ships and
banks.?(wsj 0083)).
We model such relations as typed links between
extracted propositions. Figure 1 presents an exam-
ple of handling a conditional relation: the depen-
dence between the propositions is made explicit by
the Cond(if) relation.
3 Implicit Propositions
Crucially, our proposal aims to capture not only
explicit but also implicit propositions ? proposi-
tions that can be inferred from the syntactic struc-
67
ture but which are not explicitly marked in syn-
tactic dependency trees, as we elaborate below.
Some of these phenomena are relatively easy to
address by post-processing over syntactic parsers,
and could thus be included in a first implemen-
tation that produces our proposed representations.
Other phenomena are more subtle and would re-
quire further research, yet they seem important
while not being addressed by current techniques.
The syntactic structures giving rise to implicit
propositions include:
Copular sentences such as ?This is not a triv-
ial issue.? (wsj 0108) introduces a proposition by
linking between a non-verbal predicate and its ar-
gument. We represent this by making ?not a triv-
ial issue? a predicate, and ?this? an argument of
type Predication.
Appositions, we distinguish between co-reference
and predicative appositions. In Co-reference in-
dication appositions (?The company, Random
House, doesn?t report its earnings.? (adaption of
wsj 0111)) we produce a proposition to indicate
the co-reference between two lexical items. Other
propositions relating to the entity use the main
clause as the referent for this entity. In this ex-
ample, we will produce:
1. Random House == the company.
2. The company doesn?t report its earnings.
In Predicative appositions (?Pierre Vinken, 61
years old, will join the board as a nonexecutive di-
rector Nov. 29.? (wsj 0001)) an apposition is used
in order to convey knowledge about an entity. In
our representation this will produce:
1. Pierre Vinken is 61 years old (which is canoni-
calized to the representation of copular sentences)
2. Pierre Vinken will join the board as a nonexec-
utive director Nov. 29.
Adjectives, as in the sentence ?you emphasized
the high prevalence of mental illness? (wsj 0105).
Here an adjective is used to describe a definite sub-
ject and introduces another proposition, namely
the high prevalence of mental illness.
Nominalizations, for instance in the sentence
?Googles acquisition of Waze occurred yester-
day?, introduce the implicit proposition that
?Google acquired Waze?. Such propositions were
studied and annotated in the NOMLEX (Macleod
et al., 1998) and NOMBANK (Meyers et al., 2004)
resources. It remains an open issue how to repre-
sent or distinguish cases in which nominalization
introduce an underspecified proposition. For ex-
ample, consider ?dancing? in ?I read a book about
dancing?.
Possessives, such as ?John?s book? introduce the
proposition that John has a book. Similarly, ex-
amples such as ?John?s Failure? combine a pos-
sessive construction with nominalization and in-
troduce the proposition that John has failed.
Conjunctions - for example in ?They operate
ships and banks.? (wsj 0083), introduce several
propositions in one sentence:
1. They operate ships
2. They operate banks
We mark that they co-refer to the same lexical unit
in the original sentence. Such cases are already
represented explicitly in the ?collapsed? version
of Stanford-dependencies (De Marneffe and Man-
ning, 2008a).
1
Implicit future tense indication, for instance
in ?I?m going to vote for it? (wsj 0098) and
?The economy is about to slip into recession.?
(wsj 0036), verbs like ?going to? and ?about to?
are used as future-tense markers of the proposi-
tion following them, rather than predicates on their
own. We represent these as a single predicate
(?vote?) in which the tense is marked as a fea-
ture.
2
Other phenomena, omitted for lack of space,
include propositional modifiers (e.g., relative
clause modifiers), propositional arguments (such
as ?John asserted that he will go home?), condi-
tionals, and the canonicalization of passive and
active voice.
4 Relation to Other Representations
Our proposed representation is intended to serve
as a bridging layer between purely syntactic rep-
resentations such as dependency trees, and seman-
tic oriented applications. In particular, we explic-
itly represent many semantic relations expressed
in a sentence that are not captured by contempo-
rary proposition-directed semantic representations
(Baker et al., 1998; Kingsbury and Palmer, 2003;
Meyers et al., 2004; Carreras and M`arquez, 2005).
Compared to dependency-based representations
such as Stanford-dependency trees (De Marneffe
1
A case of conjunctions requiring special treatment is in-
troduced by reciprocals, in which the entities roles are ex-
changeable. For example: ?John and Mary bet against each
other on future rates? (adaption of wsj 0117).
2
Care needs to be taken to distinguish from cases such as
?going to Italy? in which ?going to? is not followed by a
verbal predicate.
68
and Manning, 2008b), we abstract away over
many syntactic details (e.g., the myriad of ways
of expressing tense, negation and modality, or the
difference between passive and active) which are
not necessary for semantic interpretation and mark
them instead using a unified set of features and ar-
gument types. We make explicit many relations
that can be inferred from the syntax but which
are not directly encoded in dependency relations.
We directly connect predicates with all of their ar-
guments in e.g., conjunctions and embedded con-
structions, and we do not commit to a tree struc-
ture. We also explicitly mark predicate and argu-
ment boundaries, and explicitly mark multi-word
predicates such as light-verb constructions.
Compared to proposition-based semantic rep-
resentations, we do not attempt to assign frame-
specific thematic roles, nor do we attempt to dis-
ambiguate or interpret word meanings. We restrict
ourselves to representing predicates by their (lem-
matized) surface forms, and labeling arguments
based on a ?syntactic? role inventory, similar to the
label-sets available in dependency representations.
This design choice makes our representation much
easier to assign automatically to naturally occur-
ring text (perhaps pre-annotated using a syntactic
parser) than it is to assign semantic roles. At the
same time, as described in Section 3, we capture
many relations that are currently not annotated in
resources such as FrameNet, and provide a com-
prehensive set of propositions present in the sen-
tence (either explicitly or implicitly) as well as the
relations between them ? an objective which is not
trivial even when presented with full semantic rep-
resentation.
Compared to more fine-grained semantic repre-
sentations used in semantic-parsers (i.e. lambda-
calculus (Zettlemoyer and Collins, 2005), neo-
davidsonian semantics (Artzi and Zettlemoyer,
2013), DRT (Kamp, 1988) or the DCS represen-
tation of Liang (2011)), we do not attempt to
tackle quantification, nor to ground the arguments
and predicates to a concrete domain-model or on-
tology. These important tasks are orthogonal to
our representation, and we believe that semantic-
parsers can benefit from our proposal by using it
as input in addition to or instead of the raw sen-
tence text ? quantification, binding and grounding
are hard enough without needing to deal with the
subtleties of syntax or the identification of implicit
propositions.
5 Conclusion and Future Work
We proposed an intermediate semantic repre-
sentation through proposition extraction, which
captures both explicit and implicit propositions,
while staying relatively close to the syntactic
level. We believe that this kind of representation
will serve not only as an advantageous input for
semantically-centered applications, such as ques-
tion answering, summarization and information
extraction, but also serve as a rich representation
layer that can be used as input for systems aiming
to provide a finer level of semantic analysis, such
as semantic-parsers.
We are currently at the beginning of our in-
vestigation. In the near future we plan to semi-
automatically annotate the Penn Tree Bank (Mar-
cus et al., 1993) with these structures, as well as
to provide software for deriving (some of) the im-
plicit and explicit annotations from automatically
produced parse-trees. We believe such resources
will be of immediate use to semantic-oriented ap-
plications. In the longer term, we plan to inves-
tigate dedicated algorithms for automatically pro-
ducing such representation from raw text.
The architecture we describe can easily accom-
modate additional layers of abstraction, by en-
coding these layers as features of propositions,
predicates or arguments. Such layers can include
the marking of named entities, the truth status of
propositions and author commitment.
In the current version infinitive constructions
are treated as nested propositions, similar to their
representation in syntactic parse trees. Providing
a consistent, useful and transparent representation
for infinitive constructions is a challenging direc-
tion for future research.
Other extensions of the proposed representa-
tion are also possible. One appealing direction
is going beyond the sentence level and represent-
ing discourse level relations, including implied
propositions and predicate - argument relation-
ships expressed by discourse (Stern and Dagan,
2014; Ruppenhofer et al., 2010; Gerber and Chai,
2012). Such an extension may prove useful as an
intermediary representation for parsers of seman-
tic formalisms targeted at the discourse level (such
as DRT).
6 Acknowledgments
This work was partially supported by the Eu-
ropean Community?s Seventh Framework Pro-
69
gramme (FP7/2007-2013) under grant agreement
no. 287923 (EXCITEMENT).
References
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised
learning of semantic parsers for mapping instructions to
actions. Transactions of the Association for Computa-
tional Linguistics, 1(1):49?62.
Collin F Baker, Charles J Fillmore, and John B Lowe. 1998.
The berkeley framenet project. In Proceedings of ACL,
pages 86?90. Association for Computational Linguistics.
Johan Bos. 2008. Wide-coverage semantic analysis with
boxer. In Proceedings of the 2008 Conference on Seman-
tics in Text Processing, pages 277?286. Association for
Computational Linguistics.
Xavier Carreras and Llu??s M`arquez. 2005. Introduction to
the conll-2005 shared task: Semantic role labeling. In
Proceedings of CONLL, pages 152?164.
Marie-Catherine De Marneffe and Christopher D Manning.
2008a. Stanford typed dependencies manual. Technical
report, Stanford University.
Marie-Catherine De Marneffe and Christopher D Manning.
2008b. The stanford typed dependencies representation.
In Coling 2008: Proceedings of the workshop on Cross-
Framework and Cross-Domain Parser Evaluation, pages
1?8.
Matthew Gerber and Joyce Y Chai. 2012. Semantic role la-
beling of implicit arguments for nominal predicates. Com-
putational Linguistics, 38(4):755?798.
Hans Kamp. 1988. Discourse representation theory. In Nat-
ural Language at the computer, pages 84?111. Springer.
Paul Kingsbury and Martha Palmer. 2003. Propbank: the
next level of treebank. In Proceedings of Treebanks and
lexical Theories, volume 3.
P. Liang, M. I. Jordan, and D. Klein. 2011. Learning
dependency-based compositional semantics. In Proceed-
ings of ACL, pages 590?599.
Catherine Macleod, Ralph Grishman, Adam Meyers, Leslie
Barrett, and Ruth Reeves. 1998. Nomlex: A lexicon
of nominalizations. In Proceedings of EURALEX, vol-
ume 98, pages 187?193.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus of
english: The penn treebank. Computational linguistics,
19(2):313?330.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph Gr-
ishman. 2004. The nombank project: An interim report.
In HLT-NAACL 2004 workshop: Frontiers in corpus an-
notation, pages 24?31.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2010. Semeval-2010
task 10: Linking events and their participants in discourse.
In Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 45?50. Association for Compu-
tational Linguistics.
Asher Stern and Ido Dagan. 2014. Recognizing implied
predicate-argument relationships in textual inference. In
Proceedings of ACL. Association for Computational Lin-
guistics.
Luke S. Zettlemoyer and Michael Collins. 2005. Learning
to map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In UAI, pages
658?666. AUAI Press.
70
