Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 204?215,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Augmenting String-to-Tree Translation Models with Fuzzy Use of 
Source-side Syntax 
Jiajun Zhang, Feifei Zhai and Chengqing Zong 
Institute of Automation, Chinese Academy of Sciences 
Beijing, China 
{jjzhang, ffzhai, cqzong}@nlpr.ia.ac.cn 
 
 
 
 
 
Abstract 
Due to its explicit modeling of the 
grammaticality of the output via target-side 
syntax, the string-to-tree model has been 
shown to be one of the most successful 
syntax-based translation models. However, 
a major limitation of this model is that it 
does not utilize any useful syntactic 
information on the source side. In this 
paper, we analyze the difficulties of 
incorporating source syntax in a string-to-
tree model. We then propose a new way to 
use the source syntax in a fuzzy manner, 
both in source syntactic annotation and in 
rule matching. We further explore three 
algorithms in rule matching: 0-1 matching, 
likelihood matching, and deep similarity 
matching. Our method not only guarantees 
grammatical output with an explicit target 
tree, but also enables the system to choose 
the proper translation rules via fuzzy use of 
the source syntax. Our extensive 
experiments have shown significant 
improvements over the state-of-the-art 
string-to-tree system. 
1 Introduction 
In recent years, statistical translation models based 
upon linguistic syntax have shown promising 
progress in improving translation quality. It 
appears that encoding syntactic annotations on 
either side or both sides in translation rules can 
increase the expressiveness of rules and can 
produce more accurate translations with improved 
reordering.  
One of the most successful syntax-based models 
is the string-to-tree model (Galley et al, 2006; 
Marcu et al, 2006; Shen et al, 2008; Chiang et al, 
2009). Since it explicitly models the 
grammaticality of the output via target-side syntax, 
the string-to-tree model (Xiao et al, 2010) 
significantly outperforms both the state-of-the-art 
phrase-based system Moses (Koehn et al, 2007) 
and the formal syntax-based system Hiero (Chiang, 
2007). However, there is a major limitation in the 
string-to-tree model: it does not utilize any useful 
source-side syntactic information, and thus to some 
extent lacks the ability to distinguish good 
translation rules from bad ones. 
The source syntax is well-known to be helpful in 
improving translation accuracy, as shown 
especially by tree-to-string systems (Quirk et al, 
2005; Liu et al, 2006; Huang et al, 2006; Mi et al, 
2008; Zhang et al, 2009).  The tree-to-string 
systems are simple and efficient, but they also have 
a major limitation: they cannot guarantee the 
grammaticality of the translation output because 
they lack target-side syntactic constraints.  
Thus a promising solution is to combine the 
advantages of the tree-to-string and string-to-tree 
approaches. A natural idea is the tree-to-tree model 
(Ding and Palmer, 2005; Cowan et al, 2006; Liu et 
al., 2009). However, as discussed by Chiang 
(2010), while tree-to-tree translation is indeed 
promising in theory, in practice it usually ends up 
over-constrained. Alternatively, Mi and Liu (2010) 
proposed to enhance the tree-to-string model with 
target dependency structures (as a language model). 
In this paper, we explore in the other direction: 
based on the strong string-to-tree model which 
builds an explicit target syntactic tree during 
decoding rather than apply only a syntactic 
language model, we aim to find a useful way to 
incorporate the source-side syntax. 
204
First, we give a motivating example to show the 
importance of the source syntax for a string-to-tree 
model. Then we discuss the difficulties of 
integrating the source syntax into the string-to-tree 
model. Finally, we propose our solutions. 
Figure 1 depicts a standard process that 
transforms a Chinese string into an English tree 
using several string-to-tree translation rules. The 
tree with solid lines is produced by the baseline 
string-to-tree system. Although the yield is 
grammatical, the translation is not correct since the 
system mistakenly applies rule r2, thus translating 
the Chinese preposition ? (h? ) in the example 
sentence into the English conjunction and. As a 
result, the Chinese prepositional phrase ?? ?? 
??  ?? (?with terrorist networks?) is wrongly 
translated as a part of the relevant noun phrase 
(?[Hussein] and terrorists networks?). Why does 
this happen? We find that r2 occurs 103316 times 
in our training data, while r3 occurs only 1021 
times. Thus, without source syntactic clues, the 
Chinese word ? (h ? ) is converted into the 
conjunction and in most cases. In general, this 
conversion is correct when the word?(h?) is used 
as a conjunction. But?(h?) is a preposition in the 
source sentence. If we are given this source 
syntactic clue, rule r3 will be preferred. This 
example motivates us to provide a moderate 
amount of source-side syntactic information so as 
to obtain the correct English tree with dotted lines 
(as our proposed system does). 
A natural question may arise that is it easy to 
incorporate source syntax in the string-to-tree 
model? To the best of our knowledge, no one has 
studied this approach before. In fact, it is not a 
trivial question if we look into the string-to-tree 
model. We find that the difficulties lie in at least 
three problems: 1) For a string-to-tree rule such as 
r6 in figure 1, how should we syntactically annotate 
its source string? 2) Given the source-annotated 
string-to-tree rules, how should we match these 
rules according to the test source tree during 
decoding? 3) How should we binarize the source-
annotated string-to-tree rules for efficient decoding? 
For the first problem, one may require the 
source side of a string-to-tree rule to be a 
constituent. However, such excessive constraints 
will exclude many good string-to-tree rules whose 
source strings are not constituents. Inspired by 
Chiang (2010), we adopt a fuzzy way to label 
every source string with the complex syntactic 
categories of SAMT (Zollmann and Venugopal, 
2006). This method leads to a one-to-one 
correspondence between the new rules and the 
string-to-tree rules. We will detail our fuzzy 
labeling method in Section 2. 
For the second problem, it appears simple and 
intuitive to match rules by requiring a rule?s source 
syntactic category to be the same as the category of 
the test string. However, this hard constraint will 
greatly narrow the search space during decoding. 
Continuing to pursue the fuzzy methodology, we 
adopt a fuzzy matching procedure to enable 
matching of all the rules whose source strings 
match the test string, and then determine the 
degree of matching between the test source tree 
and each rule. We will discuss three fuzzy 
matching algorithms, from simple to complex, in 
Section 3. 
The third question is a technical problem, and 
we will give our solution in Section 4. 
Our method not only guarantees the 
grammaticality of the output via the target tree 
structure, but also enables the system to choose 
appropriate translation rules during decoding 
through source syntactic fuzzy labeling and fuzzy 
matching.  
The main contributions of this paper are as 
follows: 
1) We propose a fuzzy method for both source 
syntax annotation and rule matching for 
augmenting string-to-tree models. 
2) We design and investigate three fuzzy rule 
matching algorithms: 0-1 matching, 
likelihood matching, and deep similarity 
matching. 
We hope that this paper will demonstrate how to 
effectively incorporate both source and target 
syntax into a translation model with promising 
results. 
2 Rule Extraction 
Since we annotate the source side of each string-to-
tree rule with source parse tree information in a 
fuzzy way, we will henceforward denote the 
source-syntax-decorated string-to-tree rule as a 
fuzzy-tree to exact-tree rule. We first briefly 
review issues of string-to-tree rule extraction; then 
we discuss how to augment the string-to-tree rules 
to yield fuzzy-tree to exact-tree rules. 
205
 Figure 1:  Two alternative derivations for a sample string-to-tree translation. The rules used are listed on the right. 
The target yield of the tree with solid lines is hussein and terrorist networks established relations. The target yield 
of the tree with dotted lines is hussein established relations with terrorist networks. 
 
2.1 String-to-Tree Rule Extraction 
Galley et al (2004) proposed the GHKM algorithm 
for extracting (minimal) string-to-tree translation 
rules from a triple (f, et, a), where f is the source-
language sentence, et is a target-language parse tree 
whose yield e is the translation of f, and a is the set 
of word alignments between e and f. The basic idea 
of GHKM is to obtain the set of minimally-sized 
translation rules which can explain the mappings 
between source string and target parse tree. The 
minimal string-to-tree rules are extracted in three 
steps: (1) frontier set computation; (2) 
fragmentation; and (3) extraction. 
  The frontier set (FS) is the set of potential points 
at which to cut the graph G constructed by the 
triple (f, et, a) into fragments. A node satisfying the 
word alignment is a frontier. Bold italic nodes in 
the English parse tree in Figure 2 are all frontiers. 
   Given the frontier set, a well-formed 
fragmentation of G is generated by restricting each 
fragment to take only nodes in FS as the root and 
leaf nodes. 
   With fragmentation completed, the rules are 
extracted through a depth-first traversal of te : for 
each frontier being visited, a rule is extracted. 
These extracted rules are called minimal rules 
(Galley et al, 2004). For example, rules r ra i? in 
Figure 2 are part of the total of 13 minimal rules.  
To improve the rule coverage, SPMT models 
can be employed to obtain phrasal rules (Marcu et 
at., 2006). In addition, the minimal rules which 
share the adjacent tree fragments can be connected 
together to form composed rules (Galley et al, 
2006). In Figure 2, jr  is a rule composed by 
combining cr and gr . 
2.2 Fuzzy-tree to Exact-tree Rule Extraction 
Our fuzzy-tree to exact-tree rule extraction works 
on word-aligned tree-to-tree data (Figure 2 
illustrates a Chinese-English tree pair).  Basically, 
the extraction algorithm includes two parts: 
(1) String-to-tree rule extraction (without 
considering the source parse tree); 
(2) Decoration of the source side of the string-to-
tree rules with syntactic annotations. 
We use the same algorithm introduced in the 
previous section for extracting the base string-to-
tree rules. The source-side syntactic decoration is 
much more complicated. 
The simplest way to decorate, as mentioned in 
the Introduction, is to annotate the source-side of a 
string-to-tree rule with the syntactic tag that 
exactly covers the source string. This is what the 
exact tree-to-tree procedure does (Liu et al, 2009). 
However, many useful string-to-tree rules will 
become invalid if we impose such a tight 
restriction. For example, in Figure 2, the English 
phrase discuss ? them is a VP, but its Chinese 
counterpart is not a constituent. Thus we will miss 
the rule rh although it is a useful reordering rule. 
According to the analysis of our training data, the 
rules with rigid source-side syntactic constraints 
account for only about 74.5% of the base string-to-
tree rules. In this paper, we desire more general 
applicability. 
206
IP
NP VP
ADJP PP VP
AD P NP VP NP
PN VV NN
PN
?
?? ?
?? ?? ??
i
the
happy
to
discuss
matter
am
with them
NPIN
PP
NNDT
NPVB
VPTO
 VPJJ
  ADJPVBP
VP
S
NP
FW
rb: ?? JJ(happy)
String-to-Tree rules:
ra: ? FW(i)
rm: ?{P} IN(with)
rd: ?? NP(them)re: ?? VB(discuss)rf: ?? NP(DT(the) NN(matter))rg: x0 x1 PP(x0:IN x1:NP)rh: x2 x0 x1 VP(x0:VB x1:NP x2:PP)ri: x0 VP(TO(to) x0:VP)
rj: ? x0 PP(IN(with) x0:VP)
Fuzzy-tree to exact-tree rules:
rk: ?{PN} FW(i)
rl: ??{AD} JJ(happy)
rc: ? IN(with)
rn: x2 x0 x1{PP*VP} VP(x0:VB x1:NP x2:PP)
ro: x0{PP*VP} VP(TO(to) x0:VP)
...
...
 Figure 2:  A sample Chinese-English tree pair for rule extraction. The bold italic nodes in the target English tree are 
frontiers. Note that string-to-tree rules are extracted without considering source-side syntax (upper-right). The new 
fuzzy-tree to exact-tree rules are extracted with both-side parse trees (bottom-right). 
 
Inspired by (Zollmann and Venugopal, 2006; 
Chiang, 2010), we resort to SAMT-style syntactic 
categories in the style of categorial grammar (Bar-
Hillel, 1953). The annotation of the source side of 
string-to-tree rules is processed in three steps: (1) 
If the source-side string corresponds to a syntactic 
category C in the source parse tree, we label the 
source string with C. (2) Otherwise, we check if 
there exists an extended category of the forms 
C1*C2, C1/C2 or C2\C11, indicating respectively that 
the source string spans two adjacent syntactic 
categories, a partial syntactic category C1 missing a 
C2 on the right, or a partial C1 missing a C2 on the 
left. (3) If the second step fails, we check if there is 
an extended category of the forms C1*C2*C3 or 
C1..C2, showing that the source string spans three 
adjacent syntactic categories or a partial category 
with C1 and C2 on each side. In the worst case, 
C1..C2 can denote every source string, thus all of 
the decorations in our training data can be 
explained within the above three steps. Using the 
SAMT-style grammar, each source string can be 
associated with a syntactic category. Thus our 
fuzzy-tree to exact-tree extraction does not lose 
                                                          
1 The kinds of categories are checked in order. This means that 
if C1*C2, C1/C2 can both describe the same source string, we will choose C1*C2. 
any rules as compared with string-to-tree 
extraction. For example, rule ro in Figure 2 uses the 
product category *PP VP  on the source side. 
A problem may arise: How should we handle the 
situation where several rules are observed which 
only differ in their source-side syntactic categories? 
For example, besides the rule rm in Figure 2, we 
encountered rules like ? ? ? ?CC IN with??  in the 
training data. Which source tag should we retain? 
We do not make a partial choice in the rule 
extraction phase. Instead, we simply make a union 
of the relevant rules and retain the respective tag 
counts. Applying this strategy, the rule takes the 
form of ? ? ? ?: 6, : 4P CC IN with?? 2, indicating that 
the source-side preposition tag appears six times 
while the conjunction occurs four times. Note that 
the final rule format used in translation depends on 
the specific fuzzy rule matching algorithm adopted. 
3 Fuzzy Rule Matching Algorithms 
The extracted rules will ultimately be applied to 
derive translations during decoding. One way to 
apply the fuzzy-tree to exact-tree rules is to narrow 
the rule search space. Given a test source sentence 
                                                          
2 6 and 4 are not real counts. They are used for illustration 
only. 
207
with its parse tree, we can according to this 
strategy choose only the rules whose source syntax 
matches the test source tree.  However, this 
restriction will rule out many potentially correct 
rules. In this study, we keep the rule search space 
identical to that of the string-to-tree setting, and 
postpone the use of source-side syntax until the 
derivation stage. During derivation, a fuzzy 
matching algorithm will be adopted to compute a 
score to measure the compatibility between the 
rule and the test source syntax. The translation 
model will learn to distinguish good rules from bad 
ones via the compatibility scores. 
   In this section, three fuzzy matching algorithms, 
from simple to complex, are investigated in order. 
3.1 0-1 Matching 
0-1 matching is a straightforward approach that 
rewards rules whose source syntactic category 
exactly matches the syntactic category of the test 
string and punishes mismatches. It has mainly been 
employed in hierarchical phrase-based models for 
integrating source or both-side syntax (Marton and 
Resnik, 2008; Chiang et al, 2009; Chiang, 2010). 
Since it is verified to be very effective in 
hierarchical models, we borrow this idea in our 
source-syntax-augmented string-to-tree translation.  
In 0-1 matching, the rule?s source side must 
contain only one syntactic category, but a rule may 
have been decorated with more than one syntactic 
category on the source side. Thus we have to 
choose the most reliable category and discard the 
others. Here, we select the one with the highest 
frequency. For example, the tag P in the rule 
? ? ? ?: 6, : 4P CC IN with??  appears more frequently, 
so the final rule used in 0-1 matching will be 
? ? ? ?P IN with?? . Accordingly, we design two 
features: 
1. match_count calculates in a derivation the 
number of rules whose source-side syntactic 
category matches the syntactic category of the 
test string. 
2. unmatch_count counts the number of 
mismatches. 
For example, in the derivations of Figure 1, we 
know the Chinese word?(h?)  is a preposition in 
this sentence (and thus can be written as P(?)), 
therefore, match_count += 1 if the above rule 
? ? ? ?P IN with?? is employed. 
These two features are integrated into the log-
linear translation model and the corresponding 
feature weights will be tuned along with other 
model features to learn which rules are preferred. 
3.2 Likelihood Matching 
It appears intuitively that the 0-1 matching 
algorithm does not make full use of the source-side 
syntax because it keeps only the most-frequent 
syntactic label and discards some potentially useful 
information. Therefore, it runs the risk of treating 
all the discarded source syntactic categories of the 
rule as equally likely. For example, there is an 
extracted rule as follows: 
? ? ? ?:11233, :11073, : 65DEC DEG DEV IN of??  
 0-1 matching converts it into ? ? ? ?DEC IN of?? . 
The use of this rule will be penalized if the 
syntactic category of the test string ?(d?) is parsed 
as DEG or DEV. On one hand, the frequency of the 
tag DEG is just slightly less than that of DEC, but 
the 0-1 matching punishes the former while 
rewarding the latter. On the other hand, the 
frequency of DEG is much more than that of DEV, 
but they are penalized equally. It is obvious that 
the syntactic categories are not finely distinguished. 
   Considering this situation, we propose the 
likelihood matching algorithm. First, we compute 
the likelihood of the rule?s source syntactic 
categories. Since we need to deal with the potential 
problem that the rule is hit by the test string but the 
syntactic category of the test string is not in the 
category set of the rule?s source side, we apply the 
m-estimate of probability (Mitchell, 1997) to 
calculate a smoothed likelihood 
c
c
n mplikelihood n m
?? ?                     (1) 
in which nc is the count of each syntactic category 
c in a specific rule, n denotes the total count of the 
rule, m is a constant called the equivalent sample 
size, and p is the prior probability of the category c. 
In our work, we set the constant m=1 and the prior 
p to 1/12599 where 12599 is the total number of 
source-side syntactic categories in our training data.  
For example, the rule ? ? ? ?: 6, : 4P CC IN with??  
becomes ? ? ? ?: 0.545, : 0.364, 7.2 -6P CC e IN with? ?  
after likelihood computation. Then, if we apply 
likelihood matching in the derivations in Figure 1 
where the test string is? and its syntax is P(?), 
208
the matching score with the above rule will be 
0.545. When the test Chinese word ? is parsed as 
a category other than P or CC, the matching score 
with the above rule will be 7.2e-6. 
   Similar to 0-1 matching, likelihood matching will 
serve as an additional model feature representing 
the compatibility between categories and rules. 
3.3 Deep Similarity Matching 
Considering the two algorithms above, we can see 
that the purpose of fuzzy matching is in fact to 
calculate a similarity. 0-1 matching assigns 
similarity 1 for exact matches and 0 for mismatch, 
while likelihood matching directly utilizes the 
likelihood to measure the similarity. Going one 
step further, we adopt a measure of deep similarity, 
computed using latent distributions of syntactic 
categories. Huang et al (2010) proposed this 
method to compute the similarity between two 
syntactic tag sequences, used to impose soft 
syntactic constraints in hierarchical phrase-based 
models. Analogously, we borrow this idea to 
calculate the similarity between two SAMT-style 
syntactic categories, and then apply it to calculate 
the degree of matching between a translation rule 
and the syntactic category of a test source string 
for purposes of fuzzy matching. We call this 
procedure deep similarity matching. 
Instead of directly using the SAMT-style 
syntactic categories, we represent each category by 
a real-valued feature vector. Suppose there is a set 
of n latent syntactic categories ? ?1, , nV v v? ?  (n=16 
in our experiments). For each SAMT-style 
syntactic category, we compute its distribution of 
latent syntactic categories ? ? ? ? ? ?? ?1 , ,c c c nP V P v P v?? ? .  
For example, ? ? ? ?* 0.4, 0.2, 0.3, 0.1VP NPP V ??  means that 
the latent syntactic categories v1, v2, v3, v4 are 
distributed as p(v1)=0.4, p(v2)=0.2, p(v3)=0.3 and 
p(v4)=0.1 for the SAMT-style syntactic category 
VP*NP. Then we further transform the distribution 
to a normalized feature vector 
? ? ? ? ? ?c cF c P V P V?? ? ?  to represent the SAMT-style 
syntactic category c. 
With the real-valued vector representation for 
each SAMT-style syntactic category, the degree of 
similarity between two syntactic categories can be 
simply computed as a dot-product of their feature 
vectors: 
? ? ? ? ? ? ? ?
1
' 'i i
i n
F c F c f c f c
? ?
? ? ??? ??                   (2) 
This computation yields a similarity score ranging 
from 0 (totally different syntactically) to 1 (totally 
identical syntactically). 
Since we can now compute the similarity of any 
syntactic category pair, we are currently ready to 
compute the matching degree between the 
syntactic category of a test source string and a 
fuzzy-tree to exact-tree rule. To do this, we first 
convert the original fuzzy-tree to exact-tree rule to 
the rule of likelihood format without any 
smoothing. For example, the rule 
? ? ? ?: 6, : 4P CC IN with? ? becomes 
? ? ? ?: 0.6, : 0.4P CC IN with?? after conversion. We 
then denote the syntax of a rule?s source-side RS 
by weighting all the SAMT-style categories in RS 
? ? ? ? ? ?RS
c RS
F RS P c F c
?
? ?? ?                     (3) 
where ? ?RSP c  is the likelihood of the category c. 
Finally, the deep similarity between a SAMT-style 
syntactic category tc of a test source string and a 
fuzzy-tree to exact-tree rule is computed as follows: 
? ? ? ? ? ?,DeepSim tc RS F tc F RS? ?? ?                   (4) 
This deep similarity score will serve as a useful 
feature in the string-to-tree model which will 
enable the model to learn how to take account of 
the source-side syntax during translation. 
We have ignored the details of latent syntactic 
category induction in this paper. In brief, the set of 
latent syntactic categories is automatically induced 
from a source-side parsed, word-aligned parallel 
corpus. The EM algorithm is employed to induce 
the parameters. We simply follow the algorithm of 
(Huang et al, 2010), except that we replace the tag 
sequence with SAMT-style syntactic categories.  
4 Rule Binarization 
In the baseline string-to-tree model, the rules are 
not in Chomsky Normal Form. There are several 
ways to ensure cubic-time decoding. One way is to 
prune the extracted rules using a scope-3 grammar 
and do SCFG decoding without binarization 
(Hopkins and Lengmead, 2010). The other, and 
most popular way is to binarize the translation 
rules (Zhang et al, 2006). We adopt the latter 
approach for efficient decoding with integrated n-
gram language models since this binarization 
technique has been well studied in string-to-tree 
209
translation. However, when the rules? source string 
is decorated with syntax (fuzzy-tree to exact-tree 
rules), how should we binarize these rules? 
    We use the rule rn in Figure 2 for illustration: ? ? ? ?2 0 1 0 1 2: * : : :nr x x x PP VP VP x VB x NP x PP? . 
Without regarding the source-side syntax, we 
obtain the following two binarized rules: ? ?
? ?
0 1
0 1
2 0*1 0*1 * 2
0 1 * 0 1
1: : :
2 : : :
x x
x x
B x x VP x V x PP
B x x V x VB x NP
?
?
 
Since the source-side syntax PP*VP in rule rn 
only accounts for the entire source side, it is 
unclear how to annotate the source side of a partial 
rule such as the second binary rule B2.  
Analyzing the derivation process, we observe 
that a partial rule such as binary rule B2 never 
appears in the final derivation unless the rooted 
binary rule B1 also appears in the derivation. 
Based on this observation, we design a heuristic3 
strategy: we simply attach the syntax PP*VP in the 
rooted binary rule B1, and do not decorate other 
binary rules with source syntax. Thus rule rn will 
be binarized as: 
? ? ? ? ? ?
? ? ? ?
0 1
0 1
2 0*1 0*1 * 2
0 1 * 0 1
1 * : :
2 : :
x x
x x
x x PP VP VP x V x PP
x x V x VB x NP
?
?
 
5 Translation Model and Decoding 
The proposed translation system is an 
augmentation of the string-to-tree model. In the 
baseline string-to-tree model, the decoder searches 
for the optimal derivation *d  that parses a source 
string f into a target tree et from all possible 
derivations D: 
? ?? ? ? ?
? ?
*
1 2
3
arg max log
|
LMd D
d p d d
d R d f
? ? ? ?
?
?
? ?
? ?
                  (5) 
where the first element is a language model score 
in which ? ?d?  is the target yield of derivation d ; 
the second element is the translation length penalty; 
the third element is used to control the derivation 
length; and the last element is a translation score 
that includes six features: 
                                                          
3 We call it heuristic because there may be other syntactic 
annotation strategies for the binarized rules. It should be noted 
that our strategy makes the annotated binarized rules 
equivalent to the original rule. 
? ? ? ? ? ?
? ? ? ?
? ? ? ?
4 5
6 7
8 9
| log | ( ) log | ( )
log | ( ) log ( ) | ( )
log ( ) | ( ) _
r d
lex
lex
R d f p r root r p r lhs r
p r rhs r p lhs r rhs r
p rhs r lhs r is comp
? ?
? ?
? ? ?
?
? ?
? ?
? ?
?
(6) 
In equation (6), the first three elements denote the 
conditional probability of the rule given the root, 
the source-hand side, and the target-hand side. The 
next two elements are bidirectional lexical 
translation probabilities. The last element is the 
preferred binary feature for learning: either the 
composed rule or the minimal rule. 
    In our source-syntax-augmented model, the 
decoder also searches for the best derivation. With 
the help of the source syntactic information, the 
derivation rules in our new model are much more 
distinguishable than that in the string-to-tree model: 
? ?? ? ? ?
? ?
*
1 2
3
arg max log
|
LMd D
d p d d
d R d f
? ? ? ?
?
?
? ?
? ?
            (7) 
Here, all elements except the last one are the same 
as in the string-to-tree model. The last item is: 
? ? ? ?
? ? ? ?? ?
? ? ? ?? ?
? ? ? ? ? ?? ?
10
11
12 13
| |
log ,
log ,
01
r d
R d f R d f
DeepSim DeepSim tag r
likelihood likelihood tag r
match unmatch
? ?
? ?
? ? ? ? ?
?
?
?
?
? ?
?      (8) 
The 0-1 matching4 is triggered only when we set 
? ?01 1? ? . The other two fuzzy matching algorithms 
are triggered in a similar way. 
During decoding, we use a CKY-style parser 
with beam search and cube-pruning (Huang and 
Chiang, 2007) to decode the new source sentences. 
6 Experiments 
6.1 Experimental Setup 
The experiments are conducted on Chinese-to-
English translation, with training data consisting of 
about 19 million English words and 17 million 
Chinese words5. We performed bidirectional word 
alignment using GIZA++, and employed the grow-
diag-final balancing strategy to generate the final 
                                                          
4  In theory, the features unmatch_count, match_count and 
derivation_length are linearly dependent, so the 
unmatch_count is redundant. In practice, since the derivation 
may include glue rules which are not scored by fuzzy 
matching. Thus, "unmatch_count + match_count + 
glue_rule_number = derivation_length". 
5  LDC catalog number: LDC2002E18, LDC2003E14, 
LDC2003E07, LDC2004T07 and LDC2005T06. 
210
symmetric word alignment. We parsed both sides 
of the parallel text with the Berkeley parser (Petrov 
et al, 2006) and trained a 5-gram language model 
with the target part of the bilingual data and the 
Xinhua portion of the English Gigaword corpus. 
    For tuning and testing, we use NIST MT 
evaluation data for Chinese-to-English from 2003 
to 2006 (MT03 to MT06). The development data 
set comes from MT06 in which sentences with 
more than 20 words are removed to speed up 
MERT6 (Och, 2003). The test set includes MT03 
to MT05. 
   We implemented the baseline string-to-tree 
system ourselves according to (Galley et al, 2006; 
Marcu et al, 2006). We extracted minimal GHKM 
rules and the rules of SPMT Model 1 with source 
language phrases up to length L=4. We further 
extracted composed rules by composing two or 
three minimal GHKM rules. We also ran the state-
of-the-art hierarchical phrase-based system Joshua 
(Li et al, 2009) for comparison. In all systems, we 
set the beam size to 200. The final translation 
quality is evaluated in terms of case-insensitive 
BLEU-4 with shortest length penalty. The 
statistical significance test is performed using the 
re-sampling approach (Koehn, 2004). 
6.2 Results 
Table 1 shows the translation results on 
development and test sets. First, we investigate the 
performance of the strong baseline string-to-tree 
model (s2t for short). As the table shows, s2t 
outperforms the hierarchical phrase-based system 
Joshua by more than 1.0 BLEU point in all 
translation tasks. This result verifies the superiority 
of the baseline string-to-tree model. 
   With the s2t system providing a baseline, we 
further study the effectiveness of our source-
syntax-augmented string-to-tree system with 
fuzzy-tree to exact-tree rules (we use FT2ET to 
denote our proposed system). The last three lines 
in Table 1 show that, for each fuzzy matching 
algorithm, our new system TF2ET performs 
significantly better than the baseline s2t system, 
with an improvement of more than 0.5 absolute 
BLEU points in all tasks. This result demonstrates 
the success of our new method of incorporating 
source-side syntax in a string-to-tree model. 
                                                          
6 The average decoding speed is about 50 words per minute in 
the baseline string-to-tree system and our proposed systems. 
System MT06
(dev)
MT03 MT04 MT05
Joshua 29.42 28.62 31.52 31.39 
s2t 30.84 29.75 32.68 32.41 
0-1 31.61** 30.60** 33.45** 33.37**
LH 31.35* 30.34* 33.21* 33.05*
 
FT2ET
DeepSim 31.77** 30.82** 33.69** 33.50**
Table 1: Results (in BLEU scores) of different 
translation models in multiple tasks. LH=likelihood. 
*or**=significantly better than s2t system (p<0.05 or 
0.01 respectively). 
 
 Very similar 
? ? ? ?'F c F c?? ? >0.9 
Very dissimilar 
? ? ? ?'F c F c?? ? <0.1
ADJP JJ;  AD\ADJP VP;  ADVP\NP 
NP DT*NN;  LCP*P*NP CP;  BA*CP 
Table 2: Example of similar and dissimilar categories. 
 
Specifically, the FT2ET system with deep 
similarity matching obtains the best translation 
quality in all tasks and surpasses the baseline s2t 
system by 0.93 BLEU points in development data 
and by more than 1.0 BLEU point in test sets. The 
0-1 matching algorithm is simple but effective, and 
it yields quite good performance (line 3). The 
contribution of 0-1 matching as reflected in our 
experiments is consistent with the conclusions of 
(Marton and Resnik, 2008; Chiang, 2010). By 
contrast, the system with likelihood matching does 
not perform as well as the other two algorithms, 
although it also significantly improves the baseline 
s2t in all tasks. 
6.3 Analysis and Discussion 
We are a bit surprised at the large improvement 
gained by the 0-1 matching algorithm. This 
algorithm has several advantages: it is simple and 
easy to implement, and enhances the translation 
model by enabling its rules to take account of the 
source-side syntax to some degree. However, a 
major deficiency of this algorithm is that it does 
not make full use of the source side syntax, since it 
retains only the most frequent SAMT-style 
syntactic category to describe the rule?s source 
syntax. Thus this algorithm penalizes all the other 
categories equally, although some may be more 
frequent than others, as in the case of DEG and 
DEV in the rule 
? ? ? ?:11233, :11073, : 65DEC DEG DEV IN of?? .  
To some extent, the likelihood matching 
algorithm solves the main problem of 0-1 matching. 
211
Instead of rewarding or penalizing, this algorithm 
uses the likelihood of the syntactic category to 
approximate the degree of matching between the 
test source syntactic category and the rule. For a 
category not in the rule?s source syntactic category 
set, the likelihood algorithm computes a smoothed 
likelihood. However, the likelihood algorithm does 
not in fact lead to very promising improvement. 
We conjecture that this disappointing performance 
is due to the simple smoothing method we 
employed. Future work will investigate more fully. 
Compared with the above two matching 
algorithms, the deep similarity matching algorithm 
based on latent syntactic distribution is much more 
beautiful in theory. This algorithm can successfully 
measure the similarity between any two SAMT-
style syntactic categories (Table 2 gives some 
examples of similar and dissimilar category pairs).  
Then it can accurately compute the degree of 
matching between a test source syntactic category 
and a fuzzy-tree to exact-tree rule. Thus this 
algorithm obtains the best translation quality. 
However, the deep similarity matching algorithm 
has two practical shortcomings. First, it is not easy 
to determine the number of latent categories. We 
have to conduct multiple experiments to arrive at a 
number which can yield a tradeoff between 
translation quality and model complexity. In our 
work, we have tried the numbers n=4, 8, 16, 32, 
and have found n=16 to give the best tradeoff. The 
second shortcoming is that the induction of latent 
syntactic categories has been very time consuming, 
since we have applied the EM algorithm to the 
entire source-parsed parallel corpus. Even with 
n=8, it took more than a week to induce the latent 
syntactic categories on our middle-scale training 
data when using a Xeon four-core computer 
( 2.5 2 16GHz CPU GB? ? memory). When the training 
data contains tens of millions of sentence pairs, the 
computation time may no longer be tolerable. 
Table 3 shows some translation examples for 
comparison. In the first example, the Chinese 
preposition word ? is mistakenly translated into 
English conjunction word and in Joshua and 
baseline string-to-tree system s2t, however, our 
source-syntax-augmented system FT2ET-DeepSim 
correctly converts the Chinese word ?  into 
English preposition with and finally yield the right 
translation. In the second example, our proposed 
system moves the prepositional phrase at an early 
date after the sibling verb phrase. It is more 
reasonable compared with the baseline system s2t. 
In the third example, the proposed system FT2ET-
DeepSim successfully recognizes the Chinese long 
prepositional phrase ? ? ?? ?? ??? ?? ?
? ? ?? ??? ?? ? and short verb phrase ?, 
and obtains the correct phrase reordering at last. 
7 Related Work 
Several studies have tried to incorporate source or 
target syntax into translation models in a fuzzy 
manner. 
Zollmann and Venugopal (2006) augment the 
hierarchical string-to-string rules (Chiang, 2005) 
with target-side syntax. They annotate the target 
side of each string-to-string rule using SAMT-style 
syntactic categories and aim to generate the output 
more syntactically. Zhang et al (2010) base their 
approach on tree-to-string models, and generate 
grammatical output more reliably with the help of 
tree-to-tree sequence rules. Neither of them builds 
target syntactic trees using target syntax, however. 
Thus they can be viewed as integrating target 
syntax in a fuzzy manner. By contrast, we base our 
approach on a string-to-tree model which does 
construct target syntactic trees during decoding. 
(Marton and Resnik, 2008; Chiang et al, 2009 
and Huang et al, 2010) apply fuzzy techniques for 
integrating source syntax into hierarchical phrase-
based systems (Chiang, 2005, 2007). The first two 
studies employ 0-1 matching and the last tries deep 
similarity matching between two tag sequences. By 
contrast, we incorporate source syntax into a 
string-to-tree model. Furthermore, we apply fuzzy 
syntactic annotation on each rule?s source string 
and design three fuzzy rule matching algorithms. 
Chiang (2010) proposes a method for learning to 
translate with both source and target syntax in the 
framework of a hierarchical phrase-based system. 
He not only executes 0-1 matching on both sides of 
rules, but also designs numerous features such as 
. 'X Xroot  which counts the number of rules whose 
source-side root label is X  and target-side root 
label is 'X .  This fuzzy use of source and target 
syntax enables the translation system to learn 
which tree labels are similar enough to be 
compatible, which ones are harmful to combine, 
and which ones can be ignored. The differences 
between us are twofold: 1) his work applies fuzzy 
syntax in both sides, while ours bases on the string- 
212
Source sentence ?? ? [? ?? ???] ?? ? ?? 
Reference hussein also established ties with terrorist networks 
Joshua hussein also and terrorist networks established relations 
s2t hussein also and terrorist networks established relations 
 
 
1 
FT2ET- DeepSim hussein also established relations with terrorist networks 
Source sentence ? [? ?] [??] [??] [? ? ?? ?? ? ?? ??] 
Reference .. to end years of bloody conflict between israel and palestine as soon as possible 
.. to end at an early date years of bloody conflict between israel and palestine 
Joshua ? in the early period to end years of blood conflict between israel and palestine 
s2t ? at an early date to end years of blood conflict between israel and palestine 
 
 
 
2 
FT2ET- DeepSim ? to end years of blood conflict between israel and palestine at an early date 
Source sentence ?? [? ? ?? ?? ??? ?? ?? ? ?? ??? ?? ?] [?] ? 
 
Reference 
the europen union said in a joint statement issued after its summit meeting with china ?s 
premier wen jiabao ? 
in a joint statement released after the summit with chinese premier wen jiabao , the 
europen union said ? 
Joshua the europen union with chinese premier wen jiabao in a joint statement issued after the 
summit meeting said ? 
s2t the europen union in a joint statement issued after the summit meeting with chinese 
premier wen jiabao said ? 
 
 
 
 
 
3 
FT2ET- DeepSim the europen union said in a joint statement issued after the summit meeting with chinese 
premier wen jiabao ? 
 
Table 3: Some translation examples produced by Joshua, string-to-tree system s2t and source-syntax-augmented 
string-to-tree system FT2ET with deep similarity matching algorithm 
 
to-tree model and applies fuzzy syntax on source 
side; and 2) we not only adopt the 0-1 fuzzy rule 
matching algorithm, but also investigate likelihood 
matching and deep similarity matching algorithms. 
8 Conclusion and Future Work 
In this paper, we have proposed a new method for 
augmenting string-to-tree translation models with 
fuzzy use of the source syntax. We first applied a 
fuzzy annotation method which labels the source 
side of each string-to-tree rule with SAMT-style 
syntactic categories. Then we designed and 
explored three fuzzy rule matching algorithms: 0-1 
matching, likelihood matching, and deep similarity 
matching. The experiments show that our new 
system significantly outperforms the strong 
baseline string-to-tree system. This substantial 
improvement verifies that our fuzzy use of source 
syntax is effective and can enhance the ability to 
choose proper translation rules during decoding 
while guaranteeing grammatical output with 
explicit target trees. We believe that our work may 
demonstrate effective ways of incorporating both-
side syntax in a translation model to yield 
promising results. 
   Next, we plan to further study the likelihood 
fuzzy matching and deep similarity matching 
algorithms in order to fully exploit their potential. 
For example, we will combine the merits of 0-1 
matching and likelihood matching so as to avoid 
the setting of parameter m in likelihood matching. 
We also plan to explore another direction: we will 
annotate the source side of each string-to-tree rule 
with subtrees or subtree sequences. We can then 
apply tree-kernel methods to compute a degree of 
matching between a rule and a test source subtree 
or subtree sequence. 
Acknowledgments 
The research work has been funded by the Natural 
Science Foundation of China under Grant No. 
60975053, 61003160 and 60736014 and supported 
by the External Cooperation Program of the 
Chinese Academy of Sciences. We would also like 
to thank Mark Seligman and Yu Zhou for revising 
the early draft, and anonymous reviewers for their 
valuable suggestions.  
 
 
213
References  
Yehoshua Bar-Hillel, 1953. A quasi-arithmetical 
notation for syntactic description. Language, 29 (1). 
pages 47-58. 
David Chiang, 2005. A hiearchical phrase-based model 
for statistical machine translation. In Proc. of ACL 
2005, pages 263-270. 
David Chiang, 2007. Hierarchical phrase-based 
translation. Computational Linguistics, 33 (2). 
pages 201-228. 
David Chiang, 2010. Learning to translate with source 
and target syntax. In Proc. of ACL 2010, pages 
1443-1452. 
David Chiang, Kevin Knight and Wei Wang, 2009. 
11,001 new features for statistical machine 
translation. In Proc. of NAACL 2009, pages 218-
226. 
Brooke Cowan, Ivona Kucerova and Michael Collins, 
2006. A discriminative model for tree-to-tree 
translation. In Proc. of EMNLP, pages 232-241. 
Yuan Ding and Martha Palmer, 2005. Machine 
translation using probabilistic synchronous 
dependency insertion grammars. In Proc. of ACL 
2005, pages 541-548. 
Michel Galley, Mark Hopkins, Kevin Knight and Daniel 
Marcu, 2004. What?s in a translation rule. In Proc. 
of HLT-NAACL 2004, pages 273?280. 
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel 
Marcu, Steve DeNeefe, Wei Wang and Ignacio 
Thayer, 2006. Scalable inference and training of 
context-rich syntactic translation models. In Proc. 
of ACL-COLING 2006. 
Mark Hopkins and Greg Langmead, 2010. SCFG 
decoding without binarization. In Proc. of EMNLP 
2010, pages 646-655. 
Liang Huang and David Chiang, 2007. Forest rescoring: 
Faster decoding with integrated language models. 
In Proc. of ACL 2007, pages 144-151. 
Liang Huang, Kevin Knight and Aravind Joshi, 2006. A 
syntax-directed translator with extended domain of 
locality. In Proc. of AMTA 2006, pages 65-73. 
Zhongqiang Huang, Martin Cmejrek and Bowen Zhou, 
2010. Soft syntactic constraints for hierarchical 
phrase-based translation using latent syntactic 
distributions. In Proc. of EMNLP 2010, pages 138-
147. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer, Ondrej Bojar, 
Alexandra Constantin and Evan Herbst, 2007. 
Moses: Open source toolkit for statistical machine 
translation. In Proc. of ACL 2007, pages 177-180. 
Philipp Koehn, 2004. Statistical significance tests for 
machine translation evaluation. In Proc. of EMNLP 
2004, pages 388?395. 
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri 
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, 
Wren N.G. Thornton, Jonathan Weese and Omar F. 
Zaidan, 2009. Joshua: An open source toolkit for 
parsing-based machine translation. In Proc. of ACL 
2009, pages 135-139. 
Yang Liu, Qun Liu and Shouxun Lin, 2006. Tree-to-
string alignment template for statistical machine 
translation. In Proc. of ACL-COLING 2006, pages 
609-616. 
Yang Liu, Yajuan Lv and Qun Liu, 2009. Improving 
tree-to-tree translation with packed forests. In Proc. 
of ACL-IJCNLP 2009, pages 558-566. 
Daniel Marcu, Wei Wang, Abdessamad Echihabi and 
Kevin Knight, 2006. SPMT: Statistical machine 
translation with syntactified target language 
phrases. In Proc. of EMNLP 2006, pages 44-52. 
Yuval Marton and Philip Resnik, 2008. Soft syntactic 
constraints for hierarchical phrased-based 
translation. In Proc. of ACL-08: HLT. pages 1003?
1011. 
Haitao Mi, Liang Huang and Qun Liu, 2008. Forest-
based translation. In Proc. of ACL-08: HLT. pages 
192?199. 
Haitao Mi and Qun Liu, 2010. Constituency to 
dependency translation with forests. In Proc. of 
ACL 2010, pages 1433-1442. 
Tom M. Mitchell, 1997. Machine learning. Mac Graw 
Hill. 
Franz Josef Och, 2003. Minimum error rate training in 
statistical machine translation. In Proc. of ACL 
2003, pages 160-167. 
Slav Petrov, Leon Barrett, Romain Thibaux and Dan 
Klein, 2006. Learning accurate, compact, and 
interpretable tree annotation. In Proc. of COLING-
ACL 2006, pages 433-440. 
Chris Quirk, Arul Menezes and Colin Cherry, 2005. 
Dependency treelet translation: Syntactically 
informed phrasal SMT. In Proc. of ACL 2005, 
pages 271-279. 
Libin Shen, Jinxi Xu and Ralph Weischedel, 2008. A 
new string-to-dependency machine translation 
algorithm with a target dependency language 
model. In Proc. of ACL-08: HLT, pages 577-585. 
Tong Xiao, Jingbo Zhu, Muhua Zhu and and Huizhen 
Wang, 2010. Boosting-based System Combination 
for Machine Translation. In Proc. of ACL 2010, 
pages 739-748. 
Hao Zhang, Liang Huang, Daniel Gildea and Kevin 
Knight, 2006. Synchronous binarization for 
machine translation. In Proc. of HLT-NAACL 2006, 
pages 256-263. 
214
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, Chew 
Lim Tan, 2009. Forest-based tree sequence to 
string translation model. In Proc. of ACL-IJCNLP 
2009, pages 172-180. 
Hui Zhang, Min Zhang, Haizhou Li and Chng Eng 
Siong, 2010. Non-isomorphic forest pair 
translation. In Proc. of EMNLP 2010, pages 440-
450. 
Andreas Zollmann and Ashish Venugopal, 2006. Syntax 
augmented machine translation via chart parsing. 
In Proc. of Workshop on Statistical Machine 
Translation 2006, pages 138-141. 
 
 
215
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1127?1136,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Handling Ambiguities of Bilingual Predicate-Argument Structures for 
Statistical Machine Translation 
 
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing Zong 
National Laboratory of Pattern Recognition, Institute of Automation, 
Chinese Academy of Sciences, Beijing, China 
{ffzhai,jjzhang,yzhou,cqzong}@nlpr.ia.ac.cn 
 
 
 
Abstract 
Predicate-argument structure (PAS) has been 
demonstrated to be very effective in improving 
SMT performance. However, since a source-
side PAS might correspond to multiple differ-
ent target-side PASs, there usually exist many 
PAS ambiguities during translation. In this pa-
per, we group PAS ambiguities into two types: 
role ambiguity and gap ambiguity. Then we 
propose two novel methods to handle the two 
PAS ambiguities for SMT accordingly: 1) in-
side context integration; 2) a novel maximum 
entropy PAS disambiguation (MEPD) model. 
In this way, we incorporate rich context in-
formation of PAS for disambiguation. Then 
we integrate the two methods into a PAS-
based translation framework. Experiments 
show that our approach helps to achieve sig-
nificant improvements on translation quality. 
1 Introduction 
Predicate-argument structure (PAS) depicts the 
relationship between a predicate and its associat-
ed arguments, which indicates the skeleton struc-
ture of a sentence on semantic level. Basically, 
PAS agrees much better between two languages 
than syntax structure (Fung et al, 2006; Wu and 
Fung, 2009b). Considering that current syntax-
based translation models are always impaired by 
cross-lingual structure divergence (Eisner, 2003; 
Zhang et al, 2010), PAS is really a better repre-
sentation of a sentence pair to model the bilin-
gual structure mapping. 
However, since a source-side PAS might 
correspond to multiple different target-side PASs, 
there usually exist many PAS ambiguities during 
translation. For example, in Figure 1, (a) and (b) 
carry the same source-side PAS <[A0]1 
[Pred(?)]2 [A1]3> for Chinese predicate ???. 
However, in Figure 1(a), the corresponding 
target-side-like PAS is <[X1] [X2] [X3]>, while in 
Figure 1(b), the counterpart target-side-like PAS1 
is <[X2] [X3] [X1]>. This is because the two 
PASs play different roles in their corresponding 
sentences. Actually, Figure 1(a) is an independ-
ent PAS, while Figure 1(b) is a modifier of the 
noun phrase ??? ? ????. We call this kind 
of PAS ambiguity role ambiguity. 
??  ?  ??? ?? ???
[           A0         ]1 [     A1    ]3[Pred]2
?
being , should  ?two major countries
[           X3            ][X2]
China and Russia
[          X1           ]
? ?
?? ?? ? ???
[ A0 ]1 [          A1         ]3[Pred]2
flood  prevention is the  primary  mission
[           X1          ] [ X2 ] [              X3              ]
??? ? ?? ? ??? ? ? ? ?
[      A0      ]1 [    A1   ]3[Pred]2
the location of the olympic village for athletesis the best
[     X3    ][X2][                    X1                     ]
(a)
(c)
(b)
 
Figure 1. An example of ambiguous PASs. 
Meanwhile, Figure 1 also depicts another kind 
of PAS ambiguity. From Figure 1, we can see 
that (a) and (c) get the same source-side PAS and 
target-side-like PAS. However, they are different 
because in Figure 1(c), there is a gap string ?? 
???? between [A0] and [Pred]. Generally, the 
gap strings are due to the low recall of automatic 
semantic role labeling (SRL) or complex sen-
tence structures. For example, in Figure 1(c), the 
gap string ?? ???? is actually an argument 
?AM-PRP? of the PAS, but the SRL system has 
                                                 
1We use target-side-like PAS to refer to a list of general 
non-terminals in target language order, where a non-
terminal aligns to a source argument. 
1127
ignored it. We call this kind of PAS ambiguity 
gap ambiguity. 
During translation, these PAS ambiguities will 
greatly affect the PAS-based translation models. 
Therefore, in order to incorporate the bilingual 
PAS into machine translation effectively, we 
need to decide which target-side-like PAS should 
be chosen for a specific source-side PAS. We 
call this task PAS disambiguation. 
In this paper, we propose two novel methods 
to incorporate rich context information to handle 
PAS ambiguities. Towards the gap ambiguity, 
we adopt a method called inside context 
integration to extend PAS to IC-PAS. In terms of 
IC-PAS, the gap strings are combined effectively 
to deal with the gap ambiguities. As to the role 
ambiguity, we design a novel maximum entropy 
PAS disambiguation (MEPD) model to combine 
various context features, such as context words 
of PAS. For each ambiguous source-side PAS, 
we build a specific MEPD model to select 
appropriate target-side-like PAS for translation. 
We will detail the two methods in Section 3 and 
4 respectively. 
Finally, we integrate the above two methods 
into a PAS-based translation framework (Zhai et 
al. 2012). Experiments show that the two PAS 
disambiguation methods significantly improve 
the baseline translation system. The main 
contribution of this work can be concluded as 
follows: 
1) We define two kinds of PAS ambiguities: 
role ambiguity and gap ambiguity. To our 
best knowledge, we are the first to handle 
these PAS ambiguities for SMT. 
2) Towards the two different ambiguities, we 
design two specific methods for PAS 
disambiguation: inside context integration 
and the novel MEPD model.  
2 PAS-based Translation Framework 
PAS-based translation framework is to perform 
translation based on PAS transformation (Zhai et 
al., 2012). In the framework, a source-side PAS 
is first converted into target-side-like PASs by 
PAS transformation rules, and then perform 
translation based on the obtained target-side-like 
PASs. 
2.1 PAS Transformation Rules 
PAS transformation rules (PASTR) are used to 
convert a source-side PAS into a target one. 
Formally, a PASTR is a triple <Pred, SP, TP>: 
? Pred means the predicate where the rule is 
extracted. 
? SP denotes the list of source elements in 
source language order. 
? TP refers to the target-side-like PAS, i.e., a 
list of general non-terminals in target 
language order. 
For example, Figure 2 shows the PASTR 
extracted from Figure 1(a). In this PASTR, Pred 
is Chinese verb ???, SP is the source element 
list <[A0]1 [Pred]2 [A1]3>, and TP is the list of 
non-terminals <X1 X2 X3>. The same subscript in 
SP and TP means a one-to-one mapping between 
a source element and a target non-terminal. Here, 
we utilize the source element to refer to the 
predicate or argument of the source-side PAS. 
[X3] [X2] [A0]1 [Pred]2 [A1]3 [X1] 
source-side PAS(?) target-side-like PAS
 
Figure 2. An example PASTR. 
2.2 PAS Decoding 
The PAS decoding process is divided into 3 steps: 
(1) PAS acquisition: perform semantic role 
labeling (SRL) on the input sentences to achieve 
their PASs, i.e., source-side PASs; 
(2) Transformation: use the PASTR to match 
the source-side PAS i.e., the predicate Pred and 
the source element list SP. Then by the matching 
PASTRs, transform source-side PASs to target-
side-like PASs. 
(3) Translation: in this step, the decoder first 
translates each source element respectively, and 
then a CKY-style decoding algorithm is adopted 
to combine the translation of each element and 
get the final translation of the PAS.  
2.3 Sentence Decoding with the PAS-based 
translation framework 
Sometimes, the source sentence cannot be fully 
covered by the PAS, especially when there are 
several predicates. Thus to translate the whole 
sentence, Zhai et al (2012) further designed an 
algorithm to decode the entire sentence.  
In the algorithm, they organized the space of 
translation candidates into a hypergraph. For the 
span covered by PAS (PAS span), a multiple-
branch hyperedge is employed to connect it to 
the PAS?s elements. For the span not covered by 
PAS (non-PAS span), the decoder considers all 
the possible binary segmentations of it and uti-
lizes binary hyperedges to link them. 
1128
During translation, the decoder fills the spans 
with translation candidates in a bottom-up man-
ner. For the PAS span, the PAS-based translation 
framework is adopted. Otherwise, the BTG sys-
tem (Xiong et al, 2006) is used. When the span 
covers the whole sentence, we get the final trans-
lation result. 
 
Obviously, PAS ambiguities are not 
considered in this framework at all. The target-
side-like PAS is selected only according to the 
language model and translation probabilities, 
without considering any context information of 
PAS. Consequently, it would be difficult for the 
decoder to distinguish the source-side PAS from 
different context. This harms the translation 
quality. Thus to overcome this problem, we de-
sign two novel methods to cope with the PAS 
ambiguities: inside-context integration and a 
maximum entropy PAS disambiguation (MEPD) 
model. They will be detailed in the next two sec-
tions. 
3 Inside Context Integration 
In this section, we integrate the inside context of 
the PAS into PASTRs to do PAS disambiguation. 
Basically, a PAS consists of several elements (a 
predicate and several arguments), which are ac-
tually a series of continuous spans. For a specific 
PAS <E1,?, En>, such as the source-side PAS 
<[A0][Pred][A1]> in Figure 2, its controlled range 
is defined as: 
( ) { ( ), [1, ]}irange PAS s E i n= ? ?  
where s(Ei) denotes the span of element Ei. Fur-
ther, we define the closure range of a PAS. It 
refers to the shortest continuous span covered by 
the entire PAS: 
0( ) ( )
_ min , max
nj s E j s E
closure range j j
? ?
? ?= ? ?? ?
 
Here, E0 and En are the leftmost and rightmost 
element of the PAS respectively. The closure 
range is introduced here because adjacent source 
elements in a PAS are usually separated by gap 
strings in the sentence. We call these gap strings 
the inside context (IC) of the PAS, which satisfy: 
_ ( ) ( ( ) ( ) )closure range PAS IC PAS range PAS= ? ?  
The operator ?  takes a list of neighboring spans 
as input2, and returns their combined continuous 
span. As an example, towards the PAS ?<[A0] 
[Pred][A1]>? (the one for Chinese predicate ??
(shi)?) in Figure 3, its controlled range is 
{[3,5],[8,8],[9,11]} and its closure range is [3,11]. 
The IC of the PAS is thus {[6,7]}. 
To consider the PAS?s IC during PAS trans-
formation process, we incorporate its IC into the 
extracted PASTR. For each gap string in IC, we 
abstract it by the sequence of highest node cate-
gories (named as s-tag sequence). The s-tag se-
quence dominates the corresponding syntactic 
tree fragments in the parse tree. For example, in 
Figure 3, the s-tag sequence for span [6,8] is ?PP 
VC?. Thus, the sequence for the IC (span [6,7]) 
in Figure 3 is ?PP?. We combine the s-tag se-
quences with elements of the PAS in order. The 
resulting PAS is called IC-PAS, just like the left 
side of Figure 4(b) shows. 
[           A0           ] [        PP        ]
???3 ???7 ?8 ?10
de wei-zhiao-yun-cun
??5?4 ?6
dui yun-dong-yuan shi
?9 ?11
zui hao de
NN DEC NN
NP
P NN
PP
VC AD VA DEC
CP
VP
IP
??1
VV
biao-shi
VP
,2
PU
?0
PN
ta
?
PU
IP
DNP
[Pred] [      A1     ]  
Figure 3. The illustration of inside context (IC). The 
subscript in each word refers to its position in sen-
tence. 
Differently, Zhai et al (2012) attached the IC 
to its neighboring elements based on parse trees. 
For example, in Figure 3, they would attach the 
gap string ??(dui) ???(yun-dong-yuan)? to the 
PAS?s element ?Pred?, and then the span of 
?Pred? would become [6,8]. Consequently, the 
span [6,8] will be translated as a whole source 
element in the decoder. This results in a bad 
translation because the gap string ??(dui) ???
(yun-dong-yuan)? and predicate ??(shi)? should 
be translated separately, just as Figure 4(a) 
shows. Therefore, we can see that the attachment 
decision in (Zhai et al, 2012) is sometimes un-
reasonable and the IC also cannot be used for 
PAS disambiguation at all. In contrast, our meth-
                                                 
2 Here, two spans are neighboring means that the beginning 
of the latter span is the former span?s subsequent word in 
the sentence. For example, span [3,6] and [7,10] are neigh-
boring spans. 
1129
od of inside context integration is much flexible 
and beneficial for PAS disambiguation. 
(a)
(b)
[X1] [X2] [X4] [A0]1 [PP]2 [Pred]3 [A1]4 [X3] 
source-side PAS(?) target-side-like PAS
??? ??? ? ?
[            A0            ]1 [      A1     ]4[Pred]3
[the location of the olympic village]1 [for athletes]2[is]3 [the best]4
[         PP         ]2
de wei-zhiao-yun-cun
??? ?
dui yun-dong-yuan shi
? ?
zui hao de
 
Figure 4. Example of IC-PASTR. (a) The aligned 
span of each element of the PAS in Figure 3; (b) The 
extracted IC-PASTR from (a). 
Using the IC-PASs, we look for the aligned 
target span for each element of the IC-PAS. We 
demand that every element and its corresponding 
target span must be consistent with word align-
ment. Otherwise, we discard the IC-PAS. After-
wards, we can easily extract a rule for PAS trans-
formation, which we call IC-PASTR. As an ex-
ample, Figure 4(b) is the extracted IC-PASTR 
from Figure 4(a). 
Note that we only apply the source-side PAS 
and word alignment for IC-PASTR extraction. 
By contrast, Zhai et al (2012) utilized the result 
of bilingual SRL (Zhuang and Zong, 2010b). 
Generally, bilingual SRL could give a better 
alignment between bilingual elements. However, 
bilingual SRL usually achieves a really low re-
call on PASs, about 226,968 entries in our train-
ing set while it is 882,702 by using monolingual 
SRL system. Thus to get a high recall for PASs, 
we only utilize word alignment instead of captur-
ing the relation between bilingual elements. In 
addition, to guarantee the accuracy of IC-
PASTRs, we only retain rules with more than 5 
occurrences. 
4 Maximum Entropy PAS Disambigua-
tion (MEPD) Model 
In order to handle the role ambiguities, in this 
section, we concentrate on utilizing a maximum 
entropy model to incorporate the context infor-
mation for PAS disambiguation. Actually, the 
disambiguation problem can be considered as a 
multi-class classification task. That is to say, for 
a source-side PAS, every corresponding target-
side-like PAS can be considered as a label. For 
example, in Figure 1, for the source-side PAS 
?[A0]1[Pred]2[A1]3?, the target-side-like PAS 
?[X1] [X2] [X3]? in Figure 1(a) is thus a label and 
?[X2] [X3] [X1]? in Figure 1(b) is another label of 
this classification problem. 
The maximum entropy model is the classical 
way to handle this problem: 
exp( ( , , ( ), ( )))
( | , ( ), ( ))
exp( ( , , ( ), ( )))
i i i
tp i i i
h sp tp c sp c tp
P tp sp c sp c tp
h sp tp c sp c tp?
?
??
= ?
? ?
 
where sp and tp refer to the source-side PAS (not 
including the predicate) and the target-side-like 
PAS respectively. c(sp) and c(tp) denote the sur-
rounding context of sp and tp. hi is a binary fea-
ture function and ?i is the weight of hi. 
We train a maximum entropy classifier for 
each sp via the off-the-shelf MaxEnt toolkit 3 . 
Note that to avoid sparseness, sp does not in-
clude predicate of the PAS. Practically, the pred-
icate serves as a feature of the MEPD model. As 
an example, for the rule illustrated in Figure 4(b), 
we build a MEPD model for its source element 
list sp <[A0] [PP] [Pred] [A1]>, and integrate the 
predicate ??(shi)? into the MEPD model as a 
feature. 
In detail, we design a list of features for each 
pair <sp, tp> as follows: 
?   Lexical Features. These features include 
the words immediately to the left and right of sp, 
represented as w-1 and w+1. Moreover, the head 
word of each argument also serves as a lexical 
feature, named as hw(Ei). For example, Figure 3 
shows the context of the IC-PASTR in Figure 
4(b), and the extracted lexical features of the in-
stance are: w-1=? , w+1=? , hw([A0]1)=??
(wei-zhi), hw([A1]4)=?(hao). 
?   POS Features. These features are defined 
as the POS tags of the lexical features, p-1, p+1 
and phw(Ei) respectively. Thus, the correspond-
ing POS features of Figure 4 (b) are: p-1=PU, 
p+1=PU, phw([A0]1)=NN, phw([A1]4)=VA. 
?   Predicate Feature. It is the pair of source 
predicate and its corresponding target predicate. 
For example, in Figure 4(b), the source and tar-
get predicate are ??(shi)? and ?is? respectively. 
The predicate feature is thus ?PredF=?(shi)+is?. 
The target predicate is determined by: 
_ ( )
- arg max ( | - )j
j t range PAS
t pred p t s pred
?
=  
where s-pred is the source predicate and t-pred 
is the corresponding target predicate. 
                                                 
3http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.htm
l 
1130
t_range(PAS) refers to the target range covering 
all the words that are reachable from the PAS via 
word alignment.  tj refers to the jth word in 
t_range(PAS). The utilized lexical translation 
probabilities are from the toolkit in Moses 
(Koehn et al, 2007). 
?   Syntax Features. These features include 
st(Ei), i.e., the highest syntax tag for each argu-
ment, and fst(PAS) which is the lowest father 
node of sp in the parse tree. For example, for the 
rule shown in Figure 4(b), syntax features are 
st([A0]1)=NP, st([A1]4)=CP, and fst(PAS)=IP 
respectively.  
Using these features, we can train the MEPD 
model. We set the Gaussian prior to 1.0 and per-
form 100 iterations of the L-BFGS algorithm for 
each MEPD model. At last, we build 160 and 
215 different MEPD classifiers, respectively, for 
the PASTRs and IC-PASTRs. Note that since the 
training procedure of maximum entropy classifi-
er is really fast, it does not take much time to 
train these classifiers. 
5 Integrating into the PAS-based Trans-
lation Framework 
In this section, we integrate our method of PAS 
disambiguation into the PAS-based translation 
framework when translating each test sentence. 
For inside context integration, since the format 
of IC-PASTR is the same to PASTR4, we can 
use the IC-PASTR to substitute PASTR for 
building a PAS-based translation system directly. 
We use ?IC-PASTR? to denote this system. In 
addition, since our method of rule extraction is 
different from (Zhai et al, 2012), we also use 
PASTR to construct a translation system as the 
baseline system, which we call ?PASTR?. 
On the basis of PASTR and IC-PASTR, we 
further integrate our MEPD model into transla-
tion. Specifically, we take the score of the MEPD 
model as another informative feature for the de-
coder to distinguish good target-side-like PASs 
from bad ones. The weights of the MEPD feature 
can be tuned by MERT (Och, 2003) together 
with other translation features, such as language 
model. 
6 Related Work 
The method of PAS disambiguation for SMT is 
relevant to the previous work on context depend-
                                                 
4 The only difference between IC-PASTR and PASTR is 
that there are many syntactic labels in IC-PASTRs.  
ent translation. 
Carpuat and Wu (2007a, 2007b) and Chan et 
al. (2007) have integrated word sense disambig-
uation (WSD) and phrase sense disambiguation 
(PSD) into SMT systems. They combine rich 
context information to do disambiguation for 
words or phrases, and achieve improved transla-
tion performance. 
Differently, He et al (2008), Liu et al (2008) 
and Cui et al (2010) designed maximum entropy 
(ME) classifiers to do better rule section for hier-
archical phrase-based model and tree-to-string 
model respectively. By incorporating the rich 
context information as features, they chose better 
rules for translation and yielded stable improve-
ments on translation quality. 
Our work differs from the above work in the 
following two aspects: 1) in our work, we focus 
on the problem of disambiguates on PAS; 2) we 
define two kinds of PAS ambiguities: role 
ambiguity and gap ambiguity. 3) towards the two 
different ambiguities, we design two specific 
methods for PAS disambiguation: inside context 
integration and the novel MEPD model. 
In addition, Xiong et al (2012) proposed an 
argument reordering model to predict the relative 
position between predicates and arguments. They 
also combine the context information in the 
model. But they only focus on the relation be-
tween the predicate and a specific argument, ra-
ther than the entire PAS. Different from their 
work, we incorporate the context information to 
do PAS disambiguation based on the entire PAS. 
This is very beneficial for global reordering dur-
ing translation (Zhai et al, 2012). 
7 Experiment 
7.1 Experimental Setup  
We perform Chinese-to-English translation to 
demonstrate the effectiveness of our PAS disam-
biguation method. The training data contains 
about 260K sentence pairs5. To get accurate SRL 
results, we ensure that the length of each sen-
tence in the training data is among 10 and 30 
words. We run GIZA++ and then employ the 
grow-diag-final-and (gdfa) strategy to produce 
symmetric word alignments. The development 
set and test set come from the NIST evaluation 
test data (from 2003 to 2005). Similar to the 
training set, we also only retain the sentences 
                                                 
5 It is extracted from the LDC corpus. The LDC category 
number : LDC2000T50, LDC2002E18, LDC2003E07, 
LDC2004T07, LDC2005T06, LDC2002L27, LDC2005T10 
and LDC2005T34. 
1131
whose lengths are among 10 and 30 words. Fi-
nally, the development set includes 595 sentenc-
es from NIST MT03 and the test set contains 
1,786 sentences from NIST MT04 and MT05. 
We train a 5-gram language model with the 
Xinhua portion of English Gigaword corpus and 
target part of the training data. The translation 
quality is evaluated by case-insensitive BLEU-4 
with shortest length penalty. The statistical sig-
nificance test is performed by the re-sampling 
approach (Koehn, 2004). 
We perform SRL on the source part of the 
training set, development set and test set by the 
Chinese SRL system used in (Zhuang and Zong, 
2010b). To relieve the negative effect of SRL 
errors, we get the multiple SRL results by 
providing the SRL system with 3-best parse trees 
of Berkeley parser (Petrov and Klein, 2007), 1-
best parse tree of Bikel parser (Bikel, 2004) and 
Stanford parser (Klein and Manning, 2003). 
Therefore, at last, we can get 5 SRL result for 
each sentence. For the training set, we use these 
SRL results to do rule extraction respectively. 
We combine the obtained rules together to get a 
combined rule set. We discard the rules with 
fewer than 5 appearances. Using this set, we can 
train our MEPD model directly. 
As to translation, we match the 5 SRL results 
with transformation rules respectively, and then 
apply the resulting target-side-like PASs for de-
coding. As we mentioned in section 2.3, we use 
the state-of-the-art BTG system to translate the 
non-PAS spans. 
source-side PAS counts number of classes 
[A0] [Pred(?)] [A1] 245 6 
[A0] [Pred(?)] [A1] 148 6 
[A0] [AM-ADV] [Pred(?)] [A1] 68 20 
[A0] [Pred(??)] [A1] 66 6 
[A0] [Pred(?)] [A1] 42 6 
[A0] [Pred(??)] [A1] 32 4 
[A0] [AM-ADV] [Pred(?)] [A1] 32 19 
[A0] [Pred(??)] [A1] 29 4 
[AM-ADV] [Pred(?)] [A1] 26 6 
[A2] [Pred(?)] [A1] 16 5 
Table 1. The top 10 frequent source-side PASs in the 
dev and test set. 
7.2 Ambiguities in Source-side PASs 
We first give Table 1 to show some examples of 
role ambiguity. In the table, for instance, the se-
cond line denotes that the source-side PAS ?[A0] 
[Pred(?)] [A1]? appears 148 times in the devel-
opment and test set al together, and it corre-
sponds to 6 different target-side-like PASs in the 
training set. 
As we can see from Table 1, all the top 10 
PASs correspond to several different target-side-
like PASs. Moreover, according to our statistics, 
among all PASs appearing in the development 
set and test set, 56.7% of them carry gap strings. 
These statistics demonstrate the importance of 
handling the role ambiguity and gap ambiguity in 
the PAS-based translation framework. Therefore, 
we believe that our PAS disambiguation method 
would be helpful for translation. 
7.3 Translation Result  
We compare the translation result using PASTR, 
IC-PASTR and our MEPD model in this section. 
The final translation results are shown in Table 2. 
As we can see, after employing PAS for transla-
tion, all systems outperform the baseline BTG 
system significantly. This comparison verifies 
the conclusion of (Zhai et al, 2012) and thus also 
demonstrates the effectiveness of PAS. 
MT system Test set 
n-gram precision 
1 2 3 4 
BTG 32.75 74.39 41.91 24.75 14.91 
PASTR 33.24* 75.28 42.62 25.18 15.10 
PASTR+MEPD 33.78* 75.32 43.08 25.75 15.58 
IC-PASTR 33.95*# 75.62 43.36 25.92 15.58 
IC-PASTR+MEPD 34.19*# 75.66 43.40 26.15 15.92 
Table 2. Result of baseline system and the MT sys-
tems using our PAS-based disambiguation method. 
The ?*? and ?#? denote that the result is significantly 
better than BTG and PASTR respectively (p<0.01).  
Specifically, after integrating the inside con-
text information of PAS into transformation, we 
can see that system IC-PASTR significantly out-
performs system PASTR by 0.71 BLEU points. 
Moreover, after we import the MEPD model into 
system PASTR, we get a significant improve-
ment over PASTR (by 0.54 BLEU points). These 
comparisons indicate that both the inside context 
integration and our MEPD model are beneficial 
for the decoder to choose better target-side-like 
PAS for translation. 
On the basis of IC-PASTR, we further add our 
MEPD model into translation and get system IC-
PASTR+MEPD. We can see that this system 
further achieves a remarkable improvement over 
system PASTR (0.95 BLEU points).  
However, from Table 2, we find that system 
IC-PASTR+MEPD only outperforms system IC-
PASTR slightly (0.24 BLEU points). The result 
seems to show that our MEPD model is not such 
1132
useful after using IC-PASTR. We will explore 
the reason in section 7.5. 
7.4 Effectiveness of Inside Context Integra-
tion 
The method of inside context integration is used 
to combine the inside context (gap strings) into 
PAS for translation, i.e., extend the PASTR to 
IC-PASTR. In order to demonstrate the effec-
tiveness of inside context integration, we first 
give Table 3, which illustrates statistics on the 
matching PASs. The statistics are conducted on 
the combination of development set and test set. 
Transformation 
Rules 
Matching PAS 
None Gap PAS Gap PAS Total 
PASTR 1702 1539 3241 
IC-PASTR 1546 832 2378 
Table 3. Statistics on the matching PAS. 
In Table 3, for example, the line for PASTR 
means that if we use PASTR for the combined 
set, 3241 PASs (column ?Total?) can match 
PASTRs in total. Among these matching PASs, 
1539 ones (column ?Gap PAS?) carry gap strings, 
while 1702 ones do not (column ?None Gap 
PAS?). Consequently, since PASTR does not 
consider the inside context during translation, the 
Gap PASs, which account for 47% (1539/3241) 
of all matching PASs, might be handled inappro-
priately in the PAS-based translation framework. 
Therefore, integrating the inside context into 
PASTRs, i.e., using the proposed IC-PASTRs, 
would be helpful for translation. The translation 
result shown in Table 2 also demonstrates this 
conclusion. 
(a) reference
(c) translation result using IC-PASTR
[for economic recovery , especially of investment confidence is]
[  A0  ] [                              PP                               ] [Pred] [      A1      ]
? ? ? ? ???? ?? ?? ? ??? ?? ?? ??
[ a good sign ] [ for economic recovery , especially of investment confidence ]this is
? ? ? ? ??? ?? ?? ? ??? ?? ?? ??  ? 
[a good sign]this
(b) translation result using PASTR
[  A0  ] [                              PP                               ] [Pred] [      A1      ]
? ? ? ? ???? ?? ?? ? ??? ?? ?? ??
[a good sign]this is [for economic recovery and the restoration of investors ' confidence]
[  A0  ] [                            Pred                             ] [      A1      ]
 
Figure 5. Translation examples to verify the effec-
tiveness of inside context.  
From Table 3, we can also find that the num-
ber of matching PASs decreases after using IC-
PASTR. This is because IC-PASTR is more spe-
cific than PASTR. Therefore, for a PAS with 
specific inside context (gap strings), even if the 
matched PASTR is available, the matched IC-
PASTR might not. This indicates that comparing 
with PASTR, IC-PASTR is more capable of dis-
tinguishing different PASs. Therefore, based on 
this advantage, although the number of matching 
PASs decreases, IC-PASTR still improves the 
translation system using PASTR significantly. Of 
course, we believe that it is also possible to inte-
grate the inside context without decreasing the 
number of matching PASs and we plan this as 
our future work. 
We further give a translation example in Fig-
ure 5 to illustrate the effectiveness of our inside 
context integration method. In the example, the  
automatic SRL system ignores the long preposi-
tion phrase ?? ???? ???? ?? ???
?? for the PAS. Thus, the system using PASTRs 
can only attach the long phrase to the predicate 
??? according to the parse tree, and meanwhile, 
make use of a transformation rule as follows: 
[X3] [X2] [A0]1 [Pred]2 [A1]3 [X1] 
source-side PAS(?) target-side-like PAS
 
In this way, the translation result is very bad, just 
as Figure 5(b) shows. The long preposition 
phrases are wrongly positioned in the translation. 
In contrast, after inside context integration, we 
match the inside context during PAS transfor-
mation. As Figure 5(c) shows, the inside context 
helps to selects a right transformation rule as fol-
lows and gets a good translation result finally. 
[X1] [X2] [X4] [A0]1 [PP]2 [Pred]3 [A1]4 [X3] 
source-side PAS(?) target-side-like PAS
 
7.5 Effectiveness of the MEPD Model 
The MEPD model incorporates various context 
features to select better target-side-like PAS for 
translation. On the basis of PASTR and IC-
PASTR, we build 160 and 215 different MEPD 
classifies, respectively, for the frequent source-
side PASs. 
In Table 2, we have found that our MEPD 
model improves system IC-PASTR slightly. We 
conjecture that this phenomenon is due to two 
possible reasons. On one hand, sometimes, many 
PAS ambiguities might be resolved by both in-
side context and the MEPD model. Therefore, 
the improvement would not be such significant 
1133
when we combine these two methods together. 
On the other hand, as Table 3 shows, the number 
of matching PASs decreases after using IC-
PASTR. Since the MEPD model works on PASs, 
its effectiveness would also weaken to some ex-
tent. Future work will explore this phenomenon 
more thoroughly. 
PASTR
Ref
PASTR 
+ MEPD
...  ,  [??]A0    [?]Pred    [? ?? ??]A1  ?
...  [the hague]     [is]      [the last leg]  .
...  ,  [??]    [?]    [? ?? ??]  ?
...  [the hague]   [is]   [his last stop]  .
...  ,  [??]A0    [?]Pred    [? ?? ??]A1  ?
...   [is]    [his last leg of]    [the hague] .
 
Figure 6. Translation examples to demonstrate the 
effectiveness of our MEPD model. 
Now, we give Figure 6 to demonstrate the ef-
fectiveness of our MEPD model. From the Fig-
ure, we can see that the system using PASTRs 
selects an inappropriate transformation rule for 
translation: 
[X1] [X3] [A0]1 [Pred]2 [A1]3 [X2] 
source-side PAS(?) target-side-like PAS
 
This rule wrongly moves the subject ???
(Hague)? to the end of the translation. We do not 
give the translation result of the BTG system 
here because it makes the same mistake. 
Conversely, by considering the context infor-
mation, the PASTR+MEPD system chooses a 
correct rule for translation: 
[X3] [X2] [A0]1 [Pred]2 [A1]3 [X1] 
source-side PAS(?) target-side-like PAS
 
As we can see, the used rule helps to keep the 
SVO structure unchanged, and gets the correct 
translation. 
8 Conclusion and Future Work 
In this paper, we focus on the problem of ambi-
guities for PASs. We first propose two ambigui-
ties: gap ambiguity and role ambiguity. Accord-
ingly, we design two novel methods to do effi-
cient PAS disambiguation: inside-context inte-
gration and a novel MEPD model. For inside 
context integration, we abstract the inside con-
text and combine them into the PASTRs for PAS 
transformation. Towards the MEPD model, we 
design a maximum entropy model for each ambi-
tious source-side PASs. The two methods suc-
cessfully incorporate the rich context information 
into the translation process. Experiments show 
that our PAS disambiguation methods help to 
improve the translation performance significantly.  
In the next step, we will conduct experiments 
on other language pairs to demonstrate the effec-
tiveness of our PAS disambiguation method. In 
addition, we also will try to explore more useful 
and representative features for our MEPD model. 
Acknowledgments 
The research work has been funded by the Hi-
Tech Research and Development Program (?863? 
Program) of China under Grant No. 
2011AA01A207, 2012AA011101, and 
2012AA011102 and also supported by the Key 
Project of Knowledge Innovation Program of 
Chinese Academy of Sciences under Grant 
No.KGZD-EW-501. We thank the anonymous 
reviewers for their valuable comments and sug-
gestions. 
References  
Wilker Aziz, Miguel Rios, and Lucia Specia. (2011). 
Shallow semantic trees for smt. In Proceedings of 
the Sixth Workshop on Statistical Machine Trans-
lation, pages 316?322, Edinburgh, Scotland, July. 
Daniel Bikel. (2004). Intricacies of Collins parsing 
model. Computational Linguistics, 30(4):480-511. 
David Chiang, (2007). Hierarchical phrase-based 
translation. Computational Linguistics, 33 (2):201?
228. 
Marine Carpuat and Dekai Wu. 2007a. How phrase-
sense disambiguation outperforms word sense dis-
ambiguation for statistical machine translation. In 
11th Conference on Theoretical and Methodologi-
cal Issues in Machine Translation, pages 43?52. 
Marine Carpuat and Dekai Wu. 2007b. Improving 
statistical machine translation using word sense 
disambiguation. In Proceedings of EMNLP-CoNLL 
2007, pages 61?72. 
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 
2007. Word sense disambiguation improves statis-
tical machine translation. In Proc. ACL 2007, pag-
es 33?40. 
Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou and 
Tiejun Zhao. A Joint Rule Selection Model for 
Hierarchical Phrase-Based Translation. In Proc. 
of ACL 2010. 
1134
Jason Eisner. (2003). Learning non-isomorphic tree 
mappings for machine translation. In Proc. of ACL 
2003. 
Pascale Fung, Wu Zhaojun, Yang Yongsheng, and 
Dekai Wu. (2006). Automatic learning of chinese 
english semantic structure mapping. In IEEE/ACL 
2006 Workshop on Spoken Language Technology 
(SLT 2006), Aruba, December. 
Pascale Fung, Zhaojun Wu, Yongsheng Yang and 
Dekai Wu. (2007). Learning bilingual semantic 
frames: shallow semantic sarsing vs. semantic sole 
projection. In Proceedings of the 11th Conference 
on Theoretical and Methodological Issues in Ma-
chine Translation, pages 75-84. 
Qin Gao and Stephan Vogel. (2011). Utilizing target-
side semantic role labels to assist hierarchical 
phrase-based machine translation. In Proceedings 
of Fifth Workshop on Syntax, Semantics and Struc-
ture in Statistical Translation, pages 107?115, 
Portland, Oregon, USA, June 2011. Association for 
Computational Linguistics 
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexi-
calized rule selection. In Proc. of Coling 2008, 
pages 321?328. 
Franz Josef Och. (2003). Minimum error rate training 
in statistical machine translation. In Proc. of ACL 
2003, pages 160?167. 
Franz Josef Och and Hermann Ney. (2004). The 
alignment template approach to statistical machine 
translation. Computational Linguistics, 30:417?449. 
Dan Klein and Christopher D. Manning. (2003). Ac-
curate unlexicalized parsing. In Proc. of ACL-2003, 
pages 423-430. 
Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 
(2003). Statistical phrase-based translation. In Pro-
ceedings of NAACL 2003, pages 58?54, Edmonton, 
Canada, May-June. 
Philipp Koehn. (2004). Statistical significance tests 
for machine translation evaluation. In Proceedings 
of EMNLP 2004, pages 388?395, Barcelona, Spain, 
July. 
P Koehn, H Hoang, A Birch, C Callison-Burch, M 
Federico, N Bertoldi, B Cowan, W Shen, C Moran 
and R Zens, (2007). Moses: Open source toolkit for 
statistical machine translation. In Proc. of ACL 
2007. pages 177?180, Prague, Czech Republic, 
June. Association for Computational Linguistics. 
Mamoru Komachi and Yuji Matsumoto. (2006). 
Phrase reordering for statistical machine translation 
based on predicate-argument structure. In Proceed-
ings of the International Workshop on Spoken 
Language Translation: Evaluation Campaign on 
Spoken Language Translation, pages 77?82. 
Ding Liu and Daniel Gildea. (2008). Improved tree-
to-string transducer for machine Translation. In 
Proceedings of the Third Workshop on Statistical 
Machine Translation, pages 62?69, Columbus, 
Ohio, USA, June 2008. 
Ding Liu and Daniel Gildea. (2010). Semantic role 
features for machine translation. In Proc. of Coling 
2010, pages 716?724, Beijing, China, August. 
Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin. 
Maximum Entropy based Rule Selection Model for 
Syntax-based Statistical Machine Translation. In 
Proc. of EMNLP 2008.  
Yang Liu, Qun Liu and Shouxun Lin. (2006). Tree-to-
string alignment template for statistical machine 
translation. In Proc. of ACL-COLING 2006. 
Daniel Marcu, Wei Wang, Abdessamad Echihabi and 
Kevin Knight. (2006). SPMT: Statistical machine 
translation with syntactified target language 
phrases. In Proc. of EMNLP 2006, pages 44-52. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. (2002). Bleu: a method for automat-
ic evaluation of machine translation. In Proc. ACL 
2002, pages 311?318, Philadelphia, Pennsylvania, 
USA, July. 
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan 
Klein. (2006). Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the 
21st International Conference on Computational 
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 433?
440, Sydney, Australia, July. Association for Com-
putational Linguistics. 
Andreas Stolcke. (2002). Srilm ? an extensible lan-
guage modelling toolkit. In Proceedings of the 7th 
International Conference on Spoken Language 
Processing, pages 901?904, Denver, Colorado, 
USA, September. 
Dekai Wu and Pascale Fung. (2009a). Can semantic 
role labelling improve smt. In Proceedings of the 
13th Annual Conference of the EAMT, pages 218?
225, Barcelona, May. 
Dekai Wu and Pascale Fung. (2009b). Semantic roles 
for smt: A hybrid two-pass model. In Proc. NAACL 
2009, pages 13?16, Boulder, Colorado, June. 
ShuminWu and Martha Palmer. (2011). Semantic 
mapping using automatic word alignment and se-
mantic role labelling. In Proceedings of Fifth 
Workshop on Syntax, Semantics and Structure in 
Statistical Translation, pages 21?30, Portland, Or-
egon, USA, June 2011. 
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime 
Tsukada, and Masaaki Nagata. (2011). Extracting 
preordering rules from predicate-argument struc-
tures. In Proc. IJCNLP 2011, pages 29?37, Chiang 
Mai, Thailand, November.  
1135
Deyi Xiong, Qun Liu, and Shouxun Lin. (2006). Max-
imum entropy based phrase reordering model for 
statistical machine translation. In Proceedings of 
the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the 
Association for Computational Linguistics, pages 
521?528, Sydney, Australia, July. 
Deyi Xiong, Min Zhang, and Haizhou Li. (2012). 
Modelling the translation of predicate-argument 
structure for smt. In Proc. of ACL 2012, pages 
902?911, Jeju, Republic of Korea, 8-14 July 2012. 
Nianwen Xue. (2008). Labelling chinese predicates 
with semantic roles. Computational Linguistics, 
34(2): 225-255. 
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing 
Zong. Machine Translation by Modeling Predicate- 
Argument Structure Transformation. In Proc. of 
COLING 2012. 
Hui Zhang, Min  Zhang, Haizhou Li and Eng Siong 
Chng. (2010). Non-isomorphic Forest Pair Transla-
tion. In Proceedings of EMNLP 2010, pages 440-
450, Massachusetts, USA, 9-11 October 2010.  
Tao Zhuang, and Chengqing Zong. (2010a). A mini-
mum error weighting combination strategy for chi-
nese semantic role labelling. In Proceedings of 
COLING-2010, pages 1362-1370. 
Tao Zhuang and Chengqing Zong. (2010b). Joint in-
ference for bilingual semantic role labelling. In 
Proceedings of EMNLP 2010, pages 304?314, 
Massachusetts, USA, 9-11 October 2010.  
1136
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 779?784,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
RNN-based Derivation Structure Prediction for SMT
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing Zong
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
{ffzhai, jjzhang, yzhou, cqzong}@nlpr.ia.ac.cn
Abstract
In this paper, we propose a novel deriva-
tion structure prediction (DSP) model
for SMT using recursive neural network
(RNN). Within the model, two steps are
involved: (1) phrase-pair vector represen-
tation, to learn vector representations for
phrase pairs; (2) derivation structure pre-
diction, to generate a bilingual RNN that
aims to distinguish good derivation struc-
tures from bad ones. Final experimental
results show that our DSP model can sig-
nificantly improve the translation quality.
1 Introduction
Derivation structure is important for SMT decod-
ing, especially for the translation model based
on nested structures of languages, such as BTG
(bracket transduction grammar) model (Wu, 1997;
Xiong et al, 2006), hierarchical phrase-based
model (Chiang, 2007), and syntax-based model
(Galley et al, 2006; Marcu et al, 2006; Liu et
al., 2006; Huang et al, 2006; Zhang et al, 2008;
Zhang et al, 2011; Zhai et al, 2013). In general,
derivation structure refers to the tuple that records
the used translation rules and their compositions
during decoding, just as Figure 1 shows.
Intuitively, a good derivation structure usually
yields a good translation, while bad derivations al-
ways result in bad translations. For example in
Figure 1, (a) and (b) are two different derivations
for Chinese sentence ??? ? ?9 ?1 
 ?
!?. Comparing the two derivations, (a) is more
reasonable and yields a better translation. How-
ever, (b) wrongly translates phrase ?? ?9? to
?and Sharon? and combines it with [??;Bush]
incorrectly, leading to a bad translation.
To explore the derivation structure?s potential
on yielding good translations, in this paper, we
propose a novel derivation structure prediction
(DSP) model for SMT decoding.
(a) (b)
??
Bush
??? ??
held a talk
? ??
with Sharon
?? ? ??
held a talk
? ??
with Sharon
??
Bush
?? ? ??
held a talk
? ??with Sharon
??
Bush
Bush and
? ??
Sharon
??
??
Bush
??? ??
held a talk
? ??
and Sharon
? ??
and Sharon
?? ? ??
held a talk
Figure 1: Two different derivation structures of
BTG translation model. In the structure, leaf
nodes denote the used translation rules. For each
node, the first line is the source string, while the
second line is its corresponding translation.
The proposed DSP model is built on recur-
sive neural network (RNN). Within the model,
two steps are involved: (1) phrase-pair vector
representation, to learn vector representations for
phrase pairs; (2) derivation structure prediction,
to build a bilingual RNN that aims to distinguish
good derivation structures from bad ones. Ex-
tensive experiments show that the proposed DSP
model significantly improves the translation qual-
ity, and thus verify the effectiveness of derivation
structure on indicating good translations.
We make the following contributions in this
work:
? We propose a novel RNN-based model to do
derivation structure prediction for SMT de-
coding. To our best knowledge, this is the
first work on this issue in SMT community;
? In current work, RNN has only been verified
to be useful on monolingual structure learn-
ing (Socher et al, 2011a; Socher et al, 2013).
We go a step further, and design a bilingual
RNN to represent the derivation structure;
? To train the RNN-based DSP model, we pro-
pose a max-margin objective that prefers gold
derivations yielded by forced decoding to
n-best derivations generated by the conven-
tional BTG translation model.
779
2 The DSP Model
The basic idea of DSP model is to represent the
derivation structure by RNN (Figure 2). Here, we
build the DSP model for BTG translation model,
which is naturally compatible with RNN. We be-
lieve that the DSP model is also beneficial to other
translation models. We leave them as our future
work.
2.1 Phrase-Pair Vector Representation
Phrase pairs, i.e., the used translation rules, are the
leaf nodes of derivation structure. Hence, to repre-
sent the derivation structure by RNN, we need first
to represent the phrase pairs. To do this, we use
two unsupervised recursive autoencoders (RAE)
(Socher et al, 2011b), one for the source phrase
and the other for the target phrase. We call the unit
of the two RAEs the Leaf Node Network (LNN).
Using n-dimension word embedding, RAE can
learn a n-dimension vector for any phrase. Mean-
while, RAE will build a binary tree for the phrase,
as Figure 2 (in box) shows, and compute a re-
construction error to evaluate the vector. We use
E(T
ph
) to denote the reconstruction error given by
RAE, where ph is the phrase and T
ph
is the corre-
sponding binary tree. In RAE, higher error corre-
sponds to worse vector. More details can be found
in (Socher et al, 2011b).
Given a phrase pair (sp, tp), we can use LNN
to generate two n-dimension vectors, representing
sp and tp respectively. Then, we concatenate the
two vectors directly, and get a vector r ? R
2n
to
represent phrase pair (sp, tp) (shown in Figure
2). The vector r is evaluated by combining the
reconstruction error on both sides:
E(T
sp
, T
tp
) =
1
2
[E(T
sp
) + E(T
tp
) ?
N
s
N
t
]
(1)
where T
sp
and T
tp
are the binary trees for sp and
tp. N
s
and N
t
denote the number of nodes in T
sp
and T
tp
. Note that in order to unify the errors on
the two sides, we use ratio N
s
/N
t
to eliminate the
influence of phrase length.
Then, according to Equation (1), we compute
an LNN score to evaluate the vector of all phrase
pairs, i.e., leaf nodes, in derivation d:
LNN(d) = ?
?
(sp,tp)
E(T
sp
, T
tp
)
(2)
where (sp, tp) is the used phrase pair in derivation
d. Obviously, the derivation with better phrase-
pair representations will get a higher LNN score.
??
? ?? with Sharon
?? ? ?? held a talk
Bush
Figure 2: Illustration of DSP model, based on the
derivation structure in Figure 1(a).
The LNN score will serve as part of the DSP
model for predicting good derivation structures.
2.2 Derivation Structure Prediction
Using the vector representations of phrase pairs,
we then build a Derivation Structure Network
(DSN) for prediction (Figure 2).
In DSN, the derivation structure is repre-
sented by repeatedly applying unit neural net-
work (UNN, Figure 3) at each non-leaf node. The
UNN receives two node vectors r
1
? R
2n
and
r
2
? R
2n
as input, and induces a vector p ? R
2n
to represent the parent node.
r1 r2
p
score
Figure 3: The unit neural network used in DSN.
For example, in Figure 2, node [? ?9; with
Sharon] serves as the first child with vector r
1
,
and node [?1
?!; held a talk] as the second
child with vector r
2
. The parent node vector p,
representing [? ?9 ?1 
 ?!; held a talk
with Sharon], is computed by merging r
1
and r
2
:
p = f(W
UNN
[r
1
; r
2
] + b
UNN
) (3)
where [r
1
; r
2
] ? R
4n?1
is the concatenation of r
1
and r
2
, W
UNN
? R
2n?4n
and b
UNN
? R
2n?1
are
the network?s parameter weight matrix and bias
term respectively. We use tanh(?) as function f .
Then, we compute a local score using a simple
inner product with a row vector W
score
UNN
? R
1?2n
:
s(p) = W
score
UNN
? p (4)
The score measures how well the two child nodes
r
1
and r
2
are merged into the parent node p.
As we all know, in BTG derivations, we have
two different ways to merge translation candi-
dates, monotone or inverted, meaning that we
780
merge two candidates in a monotone or inverted
order. We believe that different merging or-
der (monotone or inverted) needs different UNN.
Hence, we keep two different ones in DSN, one for
monotone order (with parameter W
mono
, b
mono
,
and W
score
mono
), and the other for inverted (with pa-
rameter W
inv
, b
inv
, and W
score
inv
). The idea is that
the merging order of the two candidates will de-
termine which UNN will be used to generate their
parent?s vector and compute the score in Equa-
tion (4). Using a set of gold derivations, we can
train the network so that correct order will receive
a high score by Equation (4) and incorrect one will
receive a low score.
Thus, when we merge the candidates of two ad-
jacent spans during BTG-based decoding, the lo-
cal score in Equation (4) is useful in two aspects:
(1) for the same merging order, it evaluates how
well the two candidates are merged; (2) for the dif-
ferent order, it compares the candidates generated
by monotone order and inverted order.
Further, to assess the entire derivation structure,
we apply UNN to each node recursively, until the
root node. The final score utilized for derivation
structure prediction is the sum of all local scores:
DSN(d) =
?
p
s(p) (5)
where d denotes the derivation structure and p is
the non-leaf node in d. Obviously, by this score,
we can easily assess different derivations. Good
derivations will get higher scores while bad ones
will get lower scores.
Li et al (2013) presented a network to predict
how to merge translation candidates, in monotone
or inverted order. Our DSN differs from Li?s work
in two points. For one thing, DSN can not only
predict how to merge candidates, but also evaluate
whether two candidates should be merged. For an-
other, DSN focuses on the entire derivation struc-
ture, rather than only the two candidates for merg-
ing. Therefore, the translation decoder will pursue
good derivation structures via DSN. Actually, Li?s
work can be easily integrated into our work. We
leave it as our future work.
3 Training
In this section, we present the method of training
the DSP model. The parameters involved in this
process include: word embedding, parameters of
the two unsupervised RAEs in LNN, and parame-
ters in DSN.
3.1 Max-Margin Framework
In DSP model, our goal is to assign higher scores
to gold derivations, and lower scores to bad ones.
To reach this goal, we adopt a max-margin frame-
work (Socher et al, 2010; Socher et al, 2011a;
Socher et al, 2013) for training.
Specifically, suppose we have a training data
like (u
i
,G(u
i
),A(u
i
)), where u
i
is the input
source sentence, G(u
i
) is the gold derivation set
containing all gold derivations of u
i
1
, and A(u
i
)
is the possible derivation set that contains all
possible derivations of u
i
. We want to minimize
the following regularized risk function:
J(?) =
1
N
N
?
i=1
R
i
(?) +
?
2
? ? ?
2
, where
R
i
(?) = max
?
d?A(u
i
)
(
s
(
?, u
i
,
?
d
)
+ ?
(
?
d,G(u
i
)
)
)
? max
d?G(u
i
)
(
s
(
?, u
i
, d
)
)
(6)
Here, ? is the model parameter. s(?, u
i
, d) is the
DSP score for sentence u
i
?s derivation d. It is
computed by summing LNN score (Equation (2))
and DSN score (Equation (5)):
s(?, u, d) = LNN
?
(d) +DSN
?
(d) (7)
?(
?
d,G(u
i
)) is the structure loss margin, which
penalizes derivation
?
d more if it deviates more
from gold derivations. It is formulated as:
?
(
?
d,G(u
i
)
)
=
?
pi?
?
d
?
s
?{pi 6? G(u
i
)}+ ?
t
Dist(y(
?
d), ref)
(8)
The margin includes two parts. For the first part,
pi is the source span in derivation
?
d, ? {?} is an
indicator function. We use the first part to count
the number of source spans in derivation
?
d, but
not in gold derivations. The second part is for
target side. Dist(y(
?
d), ref) computes the edit-
distance between the translation result y(
?
d) de-
fined by derivation
?
d and the reference translation
ref . Obviously, this margin can effectively esti-
mate the difference between derivation
?
d and gold
derivations, both on source side and target side.
Note that ?
s
and ?
t
are only two hyperparameters
for scaling. They are independent of each other,
and we set ?
s
= 0.1 and ?
t
= 0.1 respectively.
1
We investigate the general case here and suppose that
one sentence could have several different gold derivations.In
the experiment, we only use one gold derivation for simple
implementation.
781
3.2 Learning
As the risk function, Equation (6) is not differ-
entiable. We train the model via the subgradient
method (Ratliff et al, 2007; Socher et al, 2013).
For parameter ?, the subgriadient of J(?) is:
?J
??
=
1
N
?
i
?s(?, u
i
,
?
d
m
)
??
?
?s(?, u
i
, d
m
)
??
+??
where
?
d
m
is the derivation with the highest DSP
score, and d
m
denotes the gold derivation with the
highest DSP score. We adopt the diagonal vari-
ant of AdaGrad (Duchi et al, 2011; Socher et al,
2013) to minimize the risk function for training.
3.3 Training Instances Collection
In order to train the model, we need to collect the
gold derivation set G(u
i
) and possible derivation
set A(u
i
) for input sentence u
i
.
For G(u
i
) , we define it by force decoding
derivation (FDD). Basically, FDD refers to the
derivation that produces the exact reference trans-
lation (single reference in our training data). For
example, since ?Bush held a talk with Sharon? is
the reference of test sentence ??? ? ?9 ?
1
?!?, then Figure 1(a) is one of the FDDs.
As FDD can produce reference translation, we be-
lieve that FDD is of high quality, and take them as
gold derivations for training.
For A(u
i
), it should contain all possible deriva-
tions of u
i
. However, it is too difficult to obtain
all derivations. Thus, we use n-best derivations of
SMT decoding to simulate the complete derivation
space, and take them as the derivations in A(u
i
).
4 Integrating the DSP Model into SMT
To integrate the DSP model into decoding, we take
it (named DSP feature) as one of the features in the
log-linear framework of SMT. During decoding,
the DSP feature is distributed to each node in the
derivation structure. For the leaf node, the score
in Equation (2), i.e., LNN score, serves as the fea-
ture. For the non-leaf node, Equation (4) plays
the role. In order to give positive feature value to
the log-linear framework (for logarithm), we nor-
malize the DSP scores to [0,1] during decoding.
Due to the length limit, we ignore the specific nor-
malization methods here. We just preform some
simple transformations (such as adding a constant,
computing reciprocal), and convert the scores pro-
portionally to [0,1] at last.
5 Experiments
5.1 Experimental Setup
To verify the effectiveness of our DSP model, we
perform experiments on Chinese-to-English trans-
lation. The training data contains about 2.1M sen-
tence pairs with about 27.7M Chinese words and
31.9M English words
2
. We train a 5-gram lan-
guage model by the Xinhua portion of Gigaword
corpus and the English part of the training data.
We obtain word alignment by GIZA++, and adopt
the grow-diag-final-and strategy to generate the
symmetric alignment. We use NIST MT 2003 data
as the development set, and NIST MT04-08
3
as
the test set. We use MERT (Och, 2004) to tune pa-
rameters. The translation quality is evaluated by
case-insensitive BLEU-4 (Papineni et al, 2002).
The statistical significance test is performed by
the re-sampling approach (Koehn, 2004). The
baseline system is our in-house BTG system (Wu,
1997; Xiong et al, 2006; Zhang and Zong, 2009).
To train the DSP model, we first use Word2Vec
4
toolkit to pre-train the word embedding on large-
scale monolingual data. The used monolingual
data contains about 1.06B words for Chinese and
1.12B words for English. The dimensionality of
our vectors is 50. The detiled training process is
as follows:
(1) Using the BTG system to perform force de-
coding on FBIS part of the bilingual training data
5
,
and collect the sentences succeeded in force de-
coding (86,902 sentences in total)
6
. We then col-
lect the corresponding force decoding derivations
as gold derivations. Here, we only use the best
force decoding derivation for simple implementa-
tion. In future, we will try to use multiple force
decoding derivations for training.
(2) Collecting the bilingual phrases in the leaf
nodes of gold derivations. We train LNN by these
phrases via L-BFGS algorithm. Finally, we get
351,448 source phrases to train the source side
RAE and 370,948 target phrases to train the tar-
get side RAE.
2
LDC category number : LDC2000T50, LDC2002E18,
LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27,
LDC2005T10 and LDC2005T34.
3
For MT06 and MT08, we only use the part of news data.
4
https://code.google.com/p/word2vec/
5
Here we only use the high quality corpus FBIS to guar-
antee the quality of force decoding derivation.
6
Many sentence pairs fail in forced decoding due to many
reasons, such as reordering limit, noisy alignment, and phrase
length limit (Yu et al, 2013).
782
(3) Decoding the 86902 sentences by the BTG
system to get n-best translations and correspond-
ing derivations. The n-best derivations are used to
simulate the entire derivation space. We retain at
most 200-best derivations for each sentence.
(4) Leveraging force decoding derivations and
n-best derivations to train the DSP model. Note
that all parameters, including word embedding and
parameters in LNN and DSN, are tuned together in
this step. It takes about 15 hours to train the entire
network using a 16-core, 2.9 GHz Xeon machine.
5.2 Experimental Results
We compare baseline BTG system and the DSP-
augmented BTG system in this section. The final
translation results are shown in Table 1.
After integrating the DSP model into BTG sys-
tem, we get significant improvement on all test
sets, about 1.0 BLEU points over BTG system on
average. This comparison strongly demonstrates
that our DSP model is useful and will be a good
complement to current translation models.
Systems
BLEU(%)
MT04 MT05 MT06 MT08 Aver
BTG 36.91 34.69 33.83 27.17 33.15
BTG+DSP 37.41 35.77 35.08 28.42 34.17
Table 1: Final translation results. Bold numbers
denote that the result is significantly better than
baseline BTG system (p < 0.05). Column ?Aver?
gives the average BLEU points of the 4 test sets.
To have a better intuition for the effectiveness
of our DSP model, we give a case study in Figure
4. It depicts two derivations built by BTG system
and BTG+DSP system respectively.
From Figure 4(b), we can see that BTG system
yields a bad translation due to the bad derivation
structure. In the figure, BTG system makes three
mistakes. It attaches candidates [??; achieve-
ments], [? ? ; has reached] and [#\?;
singapore] to the big candidate [?UTransactions of the Association for Computational Linguistics, 1 (2013) 243?254. Action Editor: Philipp Koehn.
Submitted 12/2012; Revised 3/2013; Published 5/2013. c?2013 Association for Computational Linguistics.
Unsupervised Tree Induction for Tree-based Translation 
 
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing Zong 
National Laboratory of Pattern Recognition, Institute of Automation,  
Chinese Academy of Sciences, Beijing, China 
{ffzhai,jjzhang,yzhou,cqzong}@nlpr.ia.ac.cn 
  
 
 
 
 
Abstract 
In current research, most tree-based translation 
models are built directly from parse trees. In 
this study, we go in another direction and build 
a translation model with an unsupervised tree 
structure derived from a novel non-parametric 
Bayesian model. In the model, we utilize 
synchronous tree substitution grammars (STSG) 
to capture the bilingual mapping between 
language pairs. To train the model efficiently, 
we develop a Gibbs sampler with three novel 
Gibbs operators. The sampler is capable of 
exploring the infinite space of tree structures by 
performing local changes on the tree nodes. 
Experimental results show that the string-to-
tree translation system using our Bayesian tree 
structures significantly outperforms the strong 
baseline string-to-tree system using parse trees. 
1 Introduction 
In recent years, tree-based translation models1 are 
drawing more and more attention in the 
community of statistical machine translation 
(SMT). Due to their remarkable ability to 
incorporate context structure information and long 
distance reordering into the translation process, 
tree-based translation models have shown 
promising progress in improving translation 
quality (Liu et al, 2006, 2009; Quirk et al, 2005; 
Galley et al, 2004, 2006; Marcu et al, 2006; Shen 
et al, 2008; Zhang et al, 2011b). 
However, tree-based translation models always 
suffer from two major challenges: 1) They are 
usually built directly from parse trees, which are 
generated by supervised linguistic parsers. 
                                                          
1 A tree-based translation model is defined as a model 
using tree structures on one side or both sides. 
However, for many language pairs, it is difficult to 
acquire such corresponding linguistic parsers due 
to the lack of Tree-bank resources for training. 2) 
Parse trees are actually only used to model and 
explain the monolingual structure, rather than the 
bilingual mapping between language pairs. This 
indicates that parse trees are usually not the 
optimal choice for training tree-based translation 
models (Wang et al, 2010). 
Based on the above analysis, we can conclude 
that the tree structure that is independent from 
Tree-bank resources and simultaneously considers 
the bilingual mapping inside the bilingual sentence 
pairs would be a good choice for building tree-
based translation models. 
Therefore, complying with the above conditions, 
we propose an unsupervised tree structure for tree-
based translation models in this study. In the 
structures, tree nodes are labeled by combining the 
word classes of their boundary words rather than 
by syntactic labels, such as NP, VP. Furthermore, 
using these node labels, we design a generative 
Bayesian model to infer the final tree structure 
based on synchronous tree substitution grammars 
(STSG) 2 . STSG is derived from the word 
alignments and thus can grasp the bilingual 
mapping effectively. 
Training the Bayesian model is difficult due to 
the exponential space of possible tree structures for 
each training instance. We therefore develop an 
efficient Gibbs sampler with three novel Gibbs 
operators for training. The sampler is capable of 
exploring the infinite space of tree structures by 
performing local changes on the tree nodes. 
                                                          
2 We believe it is possible to design a model to infer the 
node label and tree structure jointly. We plan this as 
future work, and here, we focus only on inferring the 
tree structure in terms of the node labels derived from 
word classes. 
243
The tree structure formed in this way is 
independent from the Tree-bank resources and 
simultaneously exploits the bilingual mapping 
effectively. Experiments show that the proposed 
unsupervised tree (U-tree) is more effective and 
reasonable for tree-based translation than the parse 
tree. 
The main contributions of this study are as 
follows: 
1) Instead of the parse tree, we propose a 
Bayesian model to induce a U-tree for tree-
based translation. The U-tree exploits the 
bilingual mapping effectively and does not 
rely on any Tree-bank resources. 
2) We design a Gibbs sampler with three novel 
Gibbs operators to train the Bayesian model 
efficiently. 
The remainder of the paper is organized as 
follows. Section 2 introduces the related work. 
Section 3 describes the STSG generation process, 
and Section 4 depicts the adopted Bayesian model. 
Section 5 describes the Gibbs sampling algorithm 
and Gibbs operators. In Section 6, we analyze the 
achieved U-trees and evaluate their effectiveness. 
Finally, we conclude the paper in Section 7. 
2 Related Work 
In this study, we move in a new direction to build a 
tree-based translation model with effective 
unsupervised U-tree structures. 
For unsupervised tree structure induction, 
DeNero and Uszkoreit (2011) adopted a parallel 
parsing model to induce unlabeled trees of source 
sentences for syntactic pre-reordering. Our 
previous work (Zhai et al, 2012) designed an EM-
based method to construct unsupervised trees for 
tree-based translation models. This work differs 
from the above work in that we design a novel 
Bayesian model to induce unsupervised U-trees, 
and prior knowledge can be encoded into the 
model more freely and effectively. 
Blunsom et al (2008, 2009, 2010) utilized 
Bayesian methods to learn synchronous context 
free grammars (SCFG) from a parallel corpus. The 
obtained SCFG is further used in a phrase-based 
and hierarchical phrase-based system (Chiang, 
2007). Levenberg et al (2012) employed a 
Bayesian method to learn discontinuous SCFG 
rules. This study differs from their work because 
we concentrate on constructing tree structures for 
tree-based translation models. Our U-trees are 
learned based on STSG, which is more appropriate 
for tree-based translation models than SCFG. 
Burkett and Klein (2008) and Burkett et al 
(2010) focused on joint parsing and alignment. 
They utilized the bilingual Tree-bank to train a 
joint model for both parsing and word alignment. 
Cohn and Blunsom (2009) adopted a Bayesian 
method to infer an STSG by exploring the space of 
alignments based on parse trees. Liu et al (2012) 
re-trained the linguistic parsers bilingually based 
on word alignment. Burkett and Klein (2012) 
utilized a transformation-based method to learn a 
sequence of monolingual tree transformations for 
translation. Compared to their work, we do not rely 
on any Tree-bank resources and focus on 
generating effective unsupervised tree structures 
for tree-based translation models. 
Zollmann and Venugopal (2006) substituted the 
non-terminal X in hierarchical phrase-based model 
by extended syntactic categories. Zollmann and 
Vogel (2011) further labeled the SCFG rules with 
POS tags and unsupervised word classes. Our work 
differs from theirs in that we present a Bayesian 
model to learn effective STSG translation rules and 
U-tree structures for tree-based translation models, 
rather than designing a labeling strategy for 
translation rules. 
3 The STSG Generation Process 
In this work, we induce effective U-trees for the 
string-to-tree translation model, which is based on 
a synchronous tree substitution grammar (STSG) 
between source strings and target tree fragments. 
We take STSG as the generation grammar to match 
the translation model. Typically, such an STSG3 is 
a 5-tuple as follows: 
( , , , , )s t t tG N S P ? ?  
where: 
i s?  and t?  represent the set of source and 
target words, respectively, 
i tN  is the set of target non-terminals, 
i t tS N?  is the start root non-terminal, and 
i P  is the production rule set. 
                                                          
3 Generally, an STSG involves tree fragments on both 
sides. Here we only consider the special case where the 
source side is actually a string. 
244
Apart from the start non-terminal tS , we define 
all the other non-terminals in tN  by word classes. 
Inspired by (Zollmann and Vogel, 2011), we 
divide these non-terminals into three categories: 
one-word, two-word and multi-word non-terminals. 
The one-word non-terminal is a word class, such as 
C, meaning that it dominates a word whose word 
class is C. Two-word non-terminals are used to 
stand for two word strings. They are labeled in the 
form of C1+C2, where C1 and C2 are the word 
classes of the two words separately. Accordingly, 
multi-word non-terminals represent the strings 
containing more than two words. They are labeled 
as C1?Cn, demanding that the word classes of the 
leftmost word and the rightmost word are C1 and 
Cn, respectively. 
We use POS tag to play the role of word class4. 
For example, the head node of the rule in Figure 1 
is a multi-word non-terminal PRP?RB. It requires 
that the POS tags of the leftmost and rightmost 
word must be PRP and RB, respectively. Xiong et 
al. (2006) showed that the boundary word is an 
effective indicator for phrase reordering. Thus, we 
believe that combining the word class of boundary 
words can denote the whole phrase well. 
PRP...RB
we
PRP
VBP:x0 RB:x1
VBP+RB
??   x1   x0
wo-men
 
Figure 1. An example of an STSG production rule. 
Each production rule in P  consists of a source 
string and a target tree fragment. In the target tree 
fragment, each internal node is labeled with a non-
terminal in tN , and each leaf node is labeled with 
either a target word in t?  or a non-terminal in tN . 
The source string in a production rule comprises 
source words and variables. Each variable 
corresponds to a leaf non-terminal in the target tree 
fragment. In the STSG, the production rule is used 
to rewrite the root node into a string and a tree 
fragment. For example, in Figure 1, the rule 
rewrites the head node PRP?RB into the 
corresponding string and fragment. 
An STSG derivation refers to the process of 
generating a specific source string and target tree 
                                                          
4 The demand of a POS tagger impairs the independence 
from manual resources to some extent. In future, we 
plan to design a method to learn effective unsupervised 
labels for the non-terminals. 
structure by production rules. This process begins 
with the start non-terminal tS  and an empty source 
string. We repeatedly choose production rules to 
rewrite the leaf non-terminals and expand the 
string until no leaf non-terminal is left. Finally, we 
acquire a source string and a target tree structure 
defined by the derivation. The probability of a 
derivation is given as follows: 
  
1
( ) ( | )
n
i i
i
p d p r N
 
 ?  (1) 
where the derivation comprises a sequence of rules 
d=(r1,?,rn), and Ni represents the root node of rule 
ri. Hence, for a specific bilingual sentence pair, we 
can generate the best target-side tree structure 
based on the STSG, independent from the Tree-
bank resources. The STSG used in the above 
process is learned by the Bayesian model that is 
detailed in the next section. 
Actually, SCFG can also be used to build the U-
trees. We do not use SCFG because most of the 
tree-based models are based on STSG. In our 
Bayesian model, the U-trees are optimized through 
selecting a set of STSG rules. These STSG rules 
are consistent with the translation rules used in the 
tree-based models. 
Another reason is that STSG has a stronger 
expressive power on tree construction than SCFG. 
In a STSG-based U-tree or a STSG rule, although 
not linguistically informed, the nodes labeled by 
POS tags are also effective on distinguishing 
different ones. However, with SCFG, we have to 
discard all the internal nodes (i.e., flattening the U-
trees or rules) to express the same sequence, 
leading to a poor ability of distinguishing different 
U-trees and production rules. Thus, using STSG, 
we can build more specific U-trees for translation.  
In addition, we find that the Bayesian SCFG 
grammar cannot even significantly outperform the 
heuristic SCFG grammar (Blunsom et al 2009)5. 
This would indicate that the SCFG-based 
derivation tree as by-product is also not such good 
for tree-based translation models. Considering the 
above reasons, we believe that the STSG-based 
learning procedure would result in a better 
translation grammar for tree-based models. 
                                                          
5 In (Blunsom et al, 2009), for Chinese-to-English 
translation, the Bayesian SCFG grammar only 
outperform the heuristic SCFG grammar by 0.1 BLEU 
points on NIST MT 2004 and 0.6 BLEU points on NIST 
MT 2005 in the NEWS domain. 
245
4 Bayesian Model 
In this section, we present a Bayesian model to 
learn STSG defined in section 3. In the model, we 
use ?N to denote the probability distribution 
( | )p r N  in Equation (1). ?N follows a multinomial 
distribution and we impose a Dirichlet prior (DP) 
on it: 
  
0 0
| ~ ( )
| , ~ ( , ( | ) )
N
N N N
r N Multi
P DP P N
T
T D D <  (2) 
where 0 ( | )P N<  (base distribution) is used to assign 
prior probabilities to the STSG production rules. ?N 
controls the model?s tendency to either reuse 
existing rules or create new ones using the base 
distribution 0 ( | )P N< . 
Instead of denoting the multinomial distribution 
explicitly with a specific ?N, we integrate over all 
possible values of ?N to achieve the probabilities of 
rules. This integration results in the following 
conditional probability for rule ri given the 
previously observed rules r-i = r1 ,?, ri-1: 
 
0
0
( | )
( | , , , ) i
i
r N ii
i N i
N N
n P r N
p r r N P
n
DD D




   (3) 
Where n-i ri  denotes the number of ri in ir , and n
-i 
N  
represents the total count of rules rewriting non-
terminal N in ir . Thanks to the exchangeability of 
the model, all permutations of the rules are actually 
equiprobable. This means that we can compute the 
probability of each rule based on the previous and 
subsequent rules (i.e. consider each rule as the last 
one). This characteristic allows us to design an 
efficient Gibbs sampling algorithm to train the 
Bayesian model. 
4.1  Base Distribution 
The base distribution 0 ( | )P r N  is designed to 
assign prior probabilities to the STSG production 
rules. Because each rule r consists of a target tree 
fragment frag and a source string str in the model, 
we follow Cohn and Blunsom (2009) and 
decompose the prior probability 0 ( | )P r N  into two 
factors as follows: 
  0 ( | ) ( | ) ( | )P r N P frag N P str frag ?  (4) 
where ( | )P frag N  is the probability of 
producing the target tree fragment frag. To 
generate frag, Cohn and Blunsom (2009) used a 
geometric prior to decide how many child nodes to 
assign each node. Differently, we require that each 
multi-word non-terminal node must have two child 
nodes. This is because the binary structure has 
been verified to be very effective for tree-based 
translation (Wang et al, 2007; Zhang et al, 2011a).  
The generation process starts at root node N. At 
first, root node N is expanded into two child nodes. 
Then, each newly generated node will be checked 
to expand into two new child nodes with 
probability pexpand. This process repeats until all the 
new non-terminal nodes are checked. Obviously, 
pexpand controls the scale of tree fragments, where a 
large pexpand corresponds to large fragments
6. The 
new terminal nodes (words) are drawn uniformly 
from the target-side vocabulary, and the non-
terminal nodes are created by asking two questions 
as follows: 
1) What type is the node, one-word, two-
word or multi-word non-terminal? 
2) What tag is used to label the node? 
The answer to question 1) is chosen from a 
uniform distribution, i.e., the probability is 1/3 for 
each type of non-terminal. The entire generation 
process is in a top-down manner, i.e., generating a 
parent node first and then its children. 
With respect to question 2), because the father 
node has determined the POS tags of boundary 
words, we only need one POS tag to generate the 
label of the current node. For example, in Figure 1, 
as the father node PRP?RB demands that the POS 
tag of the rightmost word is RB, the right child of 
PRP?RB must also satisfy this condition. 
Therefore, we choose a POS tag VBP and obtain 
the label VBP+RB. The POS tag is drawn 
uniformly from the POS tag set. If the current node 
is a one-word non-terminal, question 2) is 
unnecessary. Similarly, with respect to the two-
word non-terminal node, questions 1) and 2) are 
both unnecessary for its two child nodes because 
they have already been defined by their father node. 
As an example of the generative process, the 
tree fragment in Figure 1 is created as follows: 
a. Determine that the left child of PRP?RB is 
a one-word non-terminal (labeled with PRP); 
b. Expand PRP and generate the word ?we? for 
PRP; 
                                                          
6 In our experiment, we set pexpand to 1/3 to encourage 
small tree fragments.  
246
c. Determine that the right child of PRP?RB is 
a two-word non-terminal; 
d. Utilize the predetermined RB and a POS tag 
VBP to form the tag of the two-word non-
terminal: VBP+RB; 
e. Expand VBP+RB (to VBP and RB); 
f. Do not expand VBP and RB. 
( | )P str frag  in Equation (4) is the probability of 
generating the source string, which contains 
several source words and variables. Inspired by 
(Blunsom et al, 2009) and (Cohn and Blunsom, 
2009), we define ( | )P str frag  as follows: 
 
var
1
1 1
( | ) ( ;1)
| |
poisson sw
sw
sw
c
c
s i
P str frag P c
c i 
 u u? ?  (5) 
where csw is the number of words in the source 
string. ?s means the source vocabulary set. Further, 
cvar denotes the number of variables, which is 
determined by the tree fragment frag. 
As shown in Equation(5), we first determine 
how many source words to generate using a 
Poisson distribution Ppoisson(csw;1), which imposes a 
stable preference for short source strings. Then, we 
draw each source word from a uniform distribution 
over ?s. Afterwards, we insert the variables into 
the string. The variables are inserted one at a time 
using a uniform distribution over the possible 
positions. This factor discourages more variables.  
For the example rule in Figure 1, the generative 
process of the source string is: 
a. Decide to generate one source word;  
b. Generate the source word ??? (wo-men) ?;  
c. Insert the first variable after the word;  
d. Insert the second variable between the word 
and the first variable. 
Intuitively, a good translation grammar should 
carry both small translation rules with enough 
generality and large rules with enough context 
information. DeNero and Klein (2007) proposed 
this statement, and Cohn and Blunsom (2009) has 
verified it in their experiments with parse trees. 
Our base distribution is also designed based on 
this intuition. Considering the two factors in our 
base distribution, we penalize both large target tree 
fragments with many nodes and long source strings 
with many words and variables. The Bayesian 
model tends to select both small and frequent 
STSG production rules to construct the U-trees. 
With these types of trees, we can extract small 
rules with good generality and simultaneously 
obtain large rules with enough context information 
by composition. We will show the effectiveness of 
our U-trees in the verification experiment. 
5 Model Training by Gibbs Sampling 
In this section, we introduce a collapsed Gibbs 
sampler, which enables us to train the Bayesian 
model efficiently. 
5.1 Initialization State 
At first, we use random binary trees to initialize the 
sampler. To get the initial U-trees, we recursively 
and randomly segment a sentence into two parts 
and simultaneously create a tree node to dominate 
each part. The created tree nodes are labeled by the 
non-terminals described in section 3. 
Using the initial target U-trees, source sentences 
and word alignment, we extract minimal GHKM 
translation rules7 in terms of frontier nodes (Galley 
et al, 2004). Frontier nodes are the tree nodes that 
can map onto contiguous substrings on the source 
side via word alignment. For example, the bold 
italic nodes with shadows in Figure 2 are frontier 
nodes. In addition, it should be noted that the word 
alignment is fixed8, and we only explore the entire 
space of tree structures in our sampler. Differently, 
Cohn and Blunsom (2009) designed a sampler to 
infer an STSG by fixing the tree structure and 
exploring the space of alignment. We believe that 
it is possible to investigate the space of both tree 
structure and alignment simultaneously. This 
subject will be one of our future work topics. 
For each training instance (a pair of source 
sentence and target U-tree structure), the extracted 
GHKM minimal translation rules compose a 
unique STSG derivation9. Moreover, all the rules 
developed from the training data constitute an 
initial STSG for the Gibbs sampler. 
                                                          
7 We attach the unaligned word to the lowest frontier 
node that can cover it in terms of word alignment. 
8 The sampler might reinforce the frequent alignment 
errors (AE), which would harm the translation model 
(TM). Actually, the frequent AEs also greatly impair the 
conventional TM. Besides, our sampler encourages the 
correct alignments and simultaneously discourages the 
infrequent AEs. Thus, compared with the conventional 
TMs, we believe that our final TM would not be worse 
due to AEs. Our final experiments verify this point and 
we will conduct a much detailed analysis in future. 
9 We only use the minimal GHKM rules (Galley et al, 
2004) here to reduce the complexity of the sampler. 
247
jin-tian jian-mianwo-men zai-ci
PRP+VBP
today
NN
we
PRP
meet
VBP
again
RB
?? ?? ?? ??
PRP...RB
NN...RB
 
Figure 2. Illustration of an initial U-tree structure. The 
bold italic nodes with shadows are frontier nodes. 
Under this initial STSG, the sampler modifies 
the initial U-trees (initial sample) to create a series 
of new ones (new samples) by the Gibbs operators. 
Consequently, new STSGs are created based on the 
new U-trees simultaneously and used for the next 
sampling operation. Repeatedly and after a number 
of iterations, we can obtain the final U-trees for 
building translation models. 
5.2 The Gibbs Operators 
In this section, we develop three novel Gibbs 
operators for the sampler. They explore the entire 
space of the U-tree structures by performing local 
changes on the tree nodes. 
For a U-tree of a given sentence, we define s-
node as the non-root node covering at least two 
words. Thus, the set of s-node contains all the tree 
nodes except the root node, the pre-terminal nodes 
and leaf nodes, which we call non-s-node. For 
example, in Figure 2, PRB?RB and PRP+VBP are 
s-nodes, while NN and NN?RB are non-s-nodes. 
Since the POS tag sequence of the sentence is 
fixed, all non-s-nodes would stay unchanged in all 
possible U-trees of the sentence. Based on this fact, 
our Gibbs operators work only on s-nodes. 
Further, we assign 3 descendant candidates (DC) 
for each s-node: its left child, right child and its 
sibling. For example, in Figure 3, the 3 DCs for the 
s-node are node PRP, VBP and RB respectively. 
According to the different DCs it governs, every s-
node might be in one of the two different states: 
1) Left state: as Figure 3(a) shows, the s-node 
governs the left two DCs, PRP and VBP, 
and is labeled PRP+VBP. 
2) Right state: as Figure 3(b) shows, the s-node 
governs the right two DCs, VBP and RB, and 
is labeled VBP+RB. 
For a specific U-tree, the states of s-nodes are fixed. 
Thus, by changing an s-node?s state, we can easily 
transform this U-tree to another one, i.e., from the 
current sample to a new one. 
To formulate the U-tree transformation process, 
we associate a binary variable ??{0,1} with each 
s-node, indicating whether the s-node is in the left 
?  or right state ?  Then we can change 
the U-tree by changing value of the ? parameters. 
Our first Gibbs operator, Rotate, just works by 
sampling value of the ?parameters, one at a time, 
and changing the U-tree accordingly. For example, 
in Figure 3(a), the s-node is currently in the left 
VWDWH? :HVDPSOHWKH?RIWKLVQRGHDQGLI
WKHVDPSOHGYDOXHRI?LVZHNHHSWKHVWUXFWXUH
unchanged, i.e., in the left state. Otherwise, we 
change its state to the right state ? , and 
transform the U-tree to Figure 3(b) accordingly. 
jian-mianwo-men zai-ci
s-node
we
PRP
meet
VBP
again
RB
?? ?? ??
PRP...RB
PRP+VBP
jian-mianwo-men zai-ci
s-node
we
PRP
meet
VBP
again
RB
?? ?? ??
PRP...RB
VBP+RB
(b) ?=1(a) ?=0
Rotate
 
Figure 3. Illustration of the Rotate operator. In the 
figure, (a) and (b) denote the s-node?s left state and right 
state respectively. The bold italic nodes with shadows in 
the figure are frontier nodes. 
Obviously, towards an s-node for sampling, the 
two values of ? would define two different U-trees. 
Using the GHKM algorithm (Galley et al 2004), 
we can get two different STSG derivations from 
the two U-trees based on the fixed word alignment. 
Each derivation carries a set of STSG rules (i.e., 
minimal GHKM translation rules) of its own. In 
the two derivations, the STSG rules defined by the 
two states include the one rooted at the s-node?s 
lowest ancestor frontier node, and the one rooted at 
the s-node if it is a frontier node. For instance, in 
Figure 3(a), as the s-node is not a frontier node, the 
left state (? ) defines only one rule: 
 
0 2 1
0 1 2
:
... ( ( : : ) : )
leftr x x x
PRP RB PRP VBP x PRP x VBP x RB
o
  
Differently, in Figure 3(b), the s-node is a 
frontier node and thus the right state (? 1) defines 
two rules: 
248
 
0 0 1 0 1
1 1 0 0 1
: ... ( : : )
: ( : : )
right
right
r x x PRP RB x PRP x VBP RB
r x x VBP RB x VBP x RB


o 
o   
Using these STSG rules, the two derivations are 
evaluated as follows (We use the value of ? to 
denote the corresponding STSG derivation): 
0 1
0 1 0
( 0) ( | )
( 1) ( , | )
( | ) ( | , )
left
right right
right right right
p p r r
p p r r r
p r r p r r r


 
 
  
<  v
<  v
 
 
Where r  refers to the conditional context, i.e., the 
set of all other rules in the training data. All the 
probabilities in the above formulas are computed 
by Equation(3). We then normalize the two scores 
and sample a value of ? based on them. With the 
Bayesian model described in section 4, the sampler 
ZLOOSUHIHUWKH?WKDWSURGXFHVVPDOODQGIUHTXHQW
STSG rules. This tendency results in more frontier 
nodes in the U-tree (i.e., the s-node tends to be in 
the state that is a frontier node), which will factor 
the training instance into more small STSG rules. 
In this way, the overall likelihood of the bilingual 
data is improved by the sampler. 
Theoretically, the Rotate operator is capable of 
arriving at any possible U-tree from the initial U-
tree. This is because we can first convert the initial 
U-tree to a left branch tree by the Rotate operator, 
and then transform it to any other U-tree. However, 
it may take a long time to do so. Thus, to speed up 
the structure transformation process, we employ a 
Two-level-Rotate operator, which takes a pair of s-
nodes in a parent-child relationship as a unit for 
sampling. Similar to the Rotate operator, we also 
assign a binary variable ??{0,1} to each unit and 
update the U-tree by sampling the value of ?. The 
method of sampling ? is similar to the one used for 
?. Figure 4 shows an example of the operator. As 
shown in Figure 4(a), the unit NN?VBP and 
PRP+VBP is in the left state (?=0), and governs 
the left three descendants: NN, PRP, and VBP. By 
the Two-level-Rotate operator, we can convert the 
unit to Figure 4(b), i.e., the ULJKWVWDWH?=1). Just as 
Figure 4(b) shows, the governed descendants of the 
unit are turned to PRP, VBP, and RB. 
It may be confusing when choosing the parent-
child s-node pair for sampling because the parent 
node always faces two choices: combining the left 
child or right child for sampling. To avoid 
confusion, we split the Two-level-Rotate operator 
into two operators: Two-level-left-Rotate operator, 
which works with the parent node and its left child, 
and Two-level-right-Rotate operator, which only 
considers the parent node and its right child 10 . 
Therefore, the operator used in Figure 4 is a Two-
level-right-Rotate operator. 
jin-tian jian-mianwo-men zai-ci
PRP+VBP
Today
NN
we
PRP
meet
VBP
again
RB
?? ?? ?? ??
NN...VBP
NN...RB
jin-tian jian-mianwo-men zai-ci
VBP+RB
Today
NN
we
PRP
meet
VBP
again
RB
?? ?? ?? ??
PRP...RB
NN...RB
(a) ?=0 (b) ?=1
Two-level-right-Rotate
 
Figure 4. Illustration of the Two-level-Rotate operator. 
The bold italic nodes with shadows in the Figure are 
frontier nodes. 
During sampling, for each training instance, the 
sampler first applies the Two-level-left-Rotate 
operator to all candidate pairs of s-nodes (parent s-
node and its left child s-node) in the U-tree. After 
that, the Two-level-right-Rotate operator is applied 
to all the candidate pairs of s-nodes (parent s-node 
and its right child s-node). Then, we use the Rotate 
operator on every s-node in the U-tree. By utilizing 
the operators separately, we can guarantee that our 
sampler satisfies detailed balance. We visit all the 
training instances in a random order (one iteration). 
After a number of iterations, we can obtain the 
final U-tree structures and build the tree-based 
translation model accordingly. 
6 Experiments 
6.1 Experimental Setup 
The experiments are conducted on Chinese-to-
English translation. The training data are the FBIS 
corpus with approximately 7.1 million Chinese 
words and 9.2 million English words. We obtain 
the bidirectional word alignment with GIZA++, 
and then adopt the grow-diag-final-and strategy to 
obtain the final symmetric alignment. We train a 5-
gram language model on the Xinhua portion of the 
English Gigaword corpus and the English part of 
                                                          
10 We can also take more nodes as a unit for sampling, 
but this would make the algorithm much more complex. 
249
the training data. For tuning and testing, we use the 
NIST MT 2003 evaluation data as the development 
set, and use the NIST MT04 and MT05 data as the 
test set. We use MERT (Och, 2004) to tune 
parameters. Since MERT is prone to search errors, 
we run MERT 5 times and select the best tuning 
parameters in the tuning set. The translation quality 
is evaluated by case-insensitive BLEU-4 with the 
shortest length penalty. The statistical significance 
test is performed by the re-sampling approach 
(Koehn, 2004). 
To create the baseline system, we use the open-
source Joshua 4.0 system (Ganitkevitch et al, 2012) 
to build a hierarchical phrase-based (HPB) system, 
and a syntax-augmented MT (SAMT) 11  system 
(Zollmann and Venugopal, 2006) respectively. 
The translation system used for testing the 
effectiveness of our U-trees is our in-house string-
to-tree system (abbreviated as s2t). The system is 
implemented based on (Galley et al, 2006) and 
(Marcu et al 2006). In the system, we extract both 
the minimal GHKM rules (Galley et al, 2004), and 
the rules of SPMT Model 1 (Galley et al, 2006) 
with phrases up to length L=5 on the source side. 
We then obtain the composed rules by composing 
two or three adjacent minimal rules. 
To build the above s2t system, we first use the 
parse tree, which is generated by parsing the 
English side of the bilingual data with the Berkeley 
parser (Petrov et al, 2006). Then, we binarize the 
English parse trees using the head binarization 
approach (Wang et al, 2007) and use the resulting 
binary parse trees to build another s2t system. 
For the U-trees, we run the Gibbs sampler for 
1000 iterations on the whole corpus. The sampler 
uses 1,087s per iteration, on average, using a single 
core, 2.3 GHz Intel Xeon machine. For the 
hyperparameters, we set ? to 0.1 and pexpand = 1/3 
to give a preference to the rules with small 
fragments. We built an s2t translation system with 
the achieved U-trees after the 1000th iteration. We 
only use one sample to extract the translation 
grammar because multiple samples would result in 
a grammar that would be too large. 
                                                          
11 From (Zollmann and Vogel, 2011), we find that the 
performance of SAMT system is similar with the 
method of labeling SCFG rules with POS tags. Thus, to 
be convenient, we only conduct experiments with the 
SAMT system. 
6.2 Analysis of The Gibbs Sampler 
To evaluate the effectiveness of the Gibbs sampler, 
we explore the change of the training data?s 
likelihood with increasing sampling iterations. 
1.239E+08
1.243E+08
1.247E+08
1.251E+08
1.255E+08
1.259E+08
100 200 300 400 500 600 700 800 900 1000
Number of Sampling Iterations 
N
e
g
a
ti
v
e
-L
o
g
 L
ik
e
li
h
o
o
d random 1
random 2
random 3
 
Figure 5. Histograms of the training data?s likelihood vs. 
the number of sampling iterations. In the figure, random 
1 to 3 refers to three independent runs of the sampler 
with different initial U-trees as initialization states. 
Figure 5 depicts the negative-log likelihood of 
the training data after several sampling iterations. 
The results show that the overall likelihood of the 
training data is improved by the sampler. Moreover, 
comparing the three independent runs, we see that 
although the sampler begins with different initial 
U-trees, the training data?s likelihood is always 
similar during sampling. This demonstrates that 
our sampler is not sensitive to the random initial 
U-trees and can always arrive at a good final state 
beginning from different initialization states. Thus, 
we only utilize the U-trees from random 1 for 
further analysis hereafter. 
1.035E+07
1.040E+07
1.045E+07
1.050E+07
1.055E+07
1.060E+07
100 200 300 400 500 600 700 800 900 1000
Number of Sampling Iterations 
T
o
ta
l 
N
u
m
b
e
r 
o
f 
F
ro
n
ti
e
r 
N
o
d
e
s
random 1
random 2
random 3
 
Figure 6. The total number of frontier nodes for the 
three independent runs. 
6.3 Analysis of the U-tree Structure 
Acquiring better U-trees for translation is our final 
purpose. However, are the U-trees achieved by the 
250
Gibbs sampler appropriate for the tree-based 
translation model? 
To answer this question, we first analyze the 
effect of the sampler on the U-trees. Figure 6 
shows the total number of frontier nodes in the 
training data during sampling. The results show 
that the number of frontier nodes increases with 
increased sampling. This tendency indicates that 
our sampler prefers the tree structure with more 
frontier nodes. Consequently, the final U-tree 
structures can always be factored into many small 
minimal translation rules. Just as we have argued 
in section 4.1, this is beneficial for a good 
translation grammar. 
To demonstrate the above analysis, Figure 7 
shows a visual comparison between our U-tree 
(from random 1) and the binary parse tree (found 
by head binarization). Because the traditional parse 
tree is not binarized, we do not consider it for this 
analysis. Figure 7 shows that whether it is the 
target tree fragment or the source string of the rule, 
our U-trees always tend to obtain the smaller 
ones12. This comparison verifies that our Bayesian 
tree induction model is effective in shifting the tree 
structures away from complex minimal rules, 
which tend to negatively affect translation. 
0
200k
400k
600k
800k
1000k
2 3 4 5 6 7 8 9 10 >=11
U-Tree
binary parse tree
Number of Nodes in the Target Tree Fragment
N
um
be
r 
of
 R
ul
es
Number of Words and Variables in the Source String
0
300k
600k
900k
1200k
1 2 3 4 5 6 7
N
um
be
r 
of
 R
ul
es
 
Figure 7. Histograms over minimal translation rule 
statistics comparing our U-trees and binary parse trees. 
                                                          
12 Binary parse trees get more tree fragments with two 
nodes than U-trees. This is because there are many 
unary edges in the binary parse trees, while no unary 
edge exists in our U-trees. 
Specifically, we show an example of a binary 
parse tree and our U-tree in Figure 8. The example 
U-tree is more conducive to extracting effective 
translation rules. For example, to translate the 
Chinese phrase ?? ??, we can extract a rule (R2 
in Figure 9) directly from the U-tree because the 
phrase ?? ?? is governed by a frontier node, i.e., 
node ?VBD+RB?. However, because no node 
governs ?? ?? in the binary parse tree, we can 
only obtain a rule (R1 in Figure 9) with many extra 
nodes and edges, such as node CD in R1. Due to 
these extra things, R1 is too large to show good 
generality. 
was
QP
dollarsUS1500only
VBD NNSNNPCDRB
NP
NP
? ???????
NP-COMP
(a) binary parse tree
(b) U-tree
was dollarsUS1500only
VBD NNSNNPCDRB
? ???????
VBD+RB NNP+NNS
CD...NNS
VBD...NNS
 
Figure 8. Example of different tree structures. The node 
NP-COMP is achieved by head binarization. The bold 
italic nodes with shadows denote frontier nodes. 
was QP
only
VBD
CD:x0RB
NP
NP
NP-COMP:x1
was only
VBD RB
? ?VBD+RB
? x1x0?
R1:
R2:
 
Figure 9. Example rules to translate the Chinese phrase 
??  ? .? R1 is extracted from Figure 8(a), i.e., the 
binary parse tree. R2 is from Figure 8(b), i.e., the U-tree. 
251
Based on the above analysis, we can conclude 
that our proposed U-tree structures are conducive 
to extracting small, minimal translation rules. This 
indicates that the U-trees are more consistent with 
the word alignment and are good at capturing 
bilingual mapping information. Therefore, because 
parse trees are always constrained by cross-lingual 
structure divergence, we believe that the proposed 
U-trees would result in a better translation 
grammar. We demonstrate this conclusion in the 
next sub-section. 
6.4 Final Translation Results 
The final translation results are shown in Table 1. 
In the table, lines 3-6 refer to the string-to-tree 
systems built with different types of tree structures. 
Table 1 shows that all our s2t systems 
outperform the Joshua (HPB) and Joshua (SAMT) 
system significantly. This comparison verifies the 
superiority of our in-house s2t system. Moreover, 
the results shown in Table 1 also demonstrate the 
effectiveness of head binarization, which helps to 
improve the s2t system using parse trees in all 
translation tasks. 
To test the effectiveness of our U-trees, we give 
the s2t translation system using the U-trees (from 
random 1). The results show that the system using 
U-trees achieves the best translation result from all 
of the systems. It surpasses the s2t system using 
parse trees by 1.47 BLEU points on MT04 and 
1.44 BLEU points on MT05. Moreover, even using 
the binary parse trees, the achieved s2t system is 
still lower than our U-tree-based s2t system by 
0.97 BLEU points on the combined test set. From 
the translation results, we can validate our former 
analysis that the U-trees generated by our Bayesian 
tree induction model are more appropriate for 
string-to-tree translation than parse trees. 
System MT04 MT05 All 
Joshua (HPB) 31.73 28.82 30.64 
Joshua (SAMT) 32.48 29.77 31.56 
s2t (parse-tree) 33.73* 30.25* 32.75* 
s2t (binary-parse-tree) 34.09* 30.99*# 32.92* 
s2t (U-tree) 35.20*# 31.69*# 33.89*# 
Table 1. Results (in case-insensitive BLEU-4 scores) of 
s2t systems using different types of trees. The ?*? and 
?#? denote that the results are significantly better than 
the Joshua (SAMT) system and the s2t system using 
parse trees (p<0.01). 
6.5 Large Data 
We also conduct an experiment on a larger 
bilingual training data from the LDC corpus13. The 
training corpus contains 2.1M sentence pairs with 
approximately 27.7M Chinese words and 31.9M 
English words. Similarly, we train a 5-gram 
language model using the Xinhua portion of the 
English Gigaword corpus and the English part of 
the training corpus. With the same settings as 
before, we run the Gibbs sampler for 1000 
iterations and utilize the final U-tree structure to 
build a string-to-tree translation system. 
The final BLEU score results are shown in Table 
2. In the scenario with a large data, the string-to-
tree system using our U-trees still significantly 
outperforms the system using parse trees. 
System MT04 MT05 All 
Joshua (HPB) 34.55 33.11 34.01 
Joshua (SAMT) 34.76 33.72 34.37 
s2t (parse-tree) 36.40* 34.53* 35.70* 
s2t (binary-parse-tree) 37.38*# 35.14*# 36.54*# 
s2t (U-tree) 38.02*# 36.12*# 37.34*# 
Table 2. Results (in case-insensitive BLEU-4 scores) for 
the large training data. The meaning of ?*? and ?#? are 
similar to Table 1. 
7 Conclusion and Future Work 
In this paper, we explored a new direction to build 
a tree-based model based on unsupervised 
Bayesian trees rather than supervised parse trees. 
To achieve this purpose, we have made two major 
efforts in this paper: 
(1) We have proposed a novel generative 
Bayesian model to induce effective U-trees for 
tree-based translation. We utilized STSG in the 
model to grasp bilingual mapping information. We 
further imposed a reasonable hierarchical prior on 
the tree structures, encouraging small and frequent 
minimal rules for translation. 
(2) To train the Bayesian tree induction 
model efficiently, we developed a Gibbs sampler 
with three novel Gibbs operators. The operators are 
designed specifically to explore the infinite space 
of tree structures by performing local changes on 
the tree structure. 
                                                          
13 LDC category number : LDC2000T50, LDC2002E18, 
LDC2003E07, LDC2004T07, LDC2005T06, 
LDC2002L27, LDC2005T10 and LDC2005T34. 
252
Experiments on the string-to-tree translation 
model demonstrated that our U-trees are better 
than the parse trees. The translation results verify 
that the well-designed unsupervised trees are 
actually more appropriate for tree-based translation 
than parse trees. Therefore, we believe that the 
unsupervised tree structure would be a promising 
research direction for tree-based translation. 
In future, we plan to testify our sampler with 
various initial trees, such as the tree structure 
formed by (Zhang et al, 2008). We also plan to 
perform a detailed empirical comparison between 
STST and SCFG under our settings. Moreover, we 
will further conduct experiments to compare our 
methods with other relevant works, such as (Cohn 
and Blunsom, 2009) and (Burkett and Klein, 2012). 
Acknowledgments 
We would like to thank Philipp Koehn and three 
anonymous reviewers for their valuable comments 
and suggestions. The research work has been 
funded by the Hi-Tech Research and Development 
Program (?863? Program) of China under Grant 
No. 2011AA01A207, 2012AA011101, and 
2012AA011102. 
References 
Phil Blunsom, Trevor Cohn, Miles Osborne. 2008. 
Bayesian synchronous grammar induction. In 
Advances in Neural Information Processing Systems, 
volume 21, pages 161-168. 
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles 
Osborne. 2009. A gibbs sampler for phrasal 
synchronous grammar induction. In Proc. of ACL 
2009, pages 782-790. 
Phil Blunsom and Trevor Cohn. 2010. Inducing 
synchronous grammars with slice sampling. In Proc. 
of NAACL 2010, pages 238-241. 
David Burkett and Dan Klein. 2008. Two languages are 
better than one (for syntactic Parsing). In Proc. of 
EMNLP 2008, pages 877-886. 
David Burkett, John Blitzer, and Dan Klein. 2010. Joint 
parsing and alignment with weakly synchronized 
grammars. In Proc. of NAACL 2010, pages 127-135.  
David Burkett and Dan Klein. 2012. Transforming trees 
to improve syntactic convergence. In Proc. of 
EMNLP 2012, pages 863-872. 
David Chiang. 2007. Hierarchical phrase-based 
translation. Computational Linguistics, 33 (2). pages 
201-228. 
Dekai Wu. 1996. A polynomial-time algorithm for 
statistical machine translation. In Proc. of ACL 1996, 
pages 152-158. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics, 23:377-404. 
Trevor Cohn and Phil Blunsom. 2009. A bayesian 
model of syntax-directed tree to string grammar 
induction. In Proc. of EMNLP 2009, pages 352-361. 
Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 
2010. Inducing tree-substitution grammars. Journal 
of Machine Learning Research, pages 3053-3096. 
Brooke Cowan, Ivona Kucerova and Michael Collins. 
2006. A discriminative model for tree-to-tree 
translation. In Proc. of EMNLP 2006, pages 232-241. 
John DeNero and Dan Klein. 2007. Tailoring word 
alignments to syntactic machine translation. In Proc. 
of ACL 2007, pages 17-24. 
John DeNero and Jakob Uszkoreit. 2011. Inducing 
sentence structure from parallel corpora for 
reordering. In Proc. of EMNLP 2011, pages 193-203. 
Chris Dyer. 2010. Two monolingual parses are better 
than one (synchronous parse). In Proc. of NAACL 
2010, pages 263-266. 
Jason Eisner. 2003. Learning non-isomorphic tree 
mappings for machine translation. In Proc. of ACL 
2003, pages 205-208. 
Michel Galley, Mark Hopkins, Kevin Knight and Daniel 
Marcu. 2004. What?s in a translation rule. In Proc. of 
HLT-NAACL 2004, pages 273?280. 
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel 
Marcu, Steve DeNeefe, Wei Wang and Ignacio 
Thayer. 2006. Scalable inference and training of 
context-rich syntactic translation models. In Proc. of 
ACL-COLING 2006, pages 961-968. 
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post and Adam Lopez. 2011. Joshua 3.0: 
syntax-based machine translation with the thrax 
Grammar Extractor. In Proc of WMT11, pages 478-
484. 
Liang Huang, Kevin Knight and Aravind Joshi. 2006. A 
syntax-directed translator with extended domain of 
locality. In Proc. of AMTA 2006, pages 65-73. 
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.  
Statistical phrase-based translation, In Proc. of 
HLT/NAACL 2003, pages 48-54.  
253
Philipp Koehn. 2004. Statistical significance tests for 
machine translation evaluation. In Proc. of EMNLP 
2004, pages 388?395. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, 
RichDUG =HQV &KULV '\HU DQG 2QG?HM %RMDU. 2007. 
Moses: open source toolkit for statistical machine 
translation. In Proc. of ACL 2007, pages 177-180. 
Abby Levenberg, Chris Dyer and Phil Blunsom. 2012. 
A bayesian model for learning SCFGs with 
discontiguous Rules. In Proc. of EMNLP 2012, pages 
223-232. 
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri 
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, 
Wren N.G. Thornton, Jonathan Weese and Omar F. 
Zaidan. 2009. Joshua: An open source toolkit for 
parsing-based machine translation. In Proc. of ACL 
2009, pages 135-139. 
Shujie Liu, Chi-Ho Li, Mu Li, Ming Zhou. 2012. Re-
training monolingual parser bilingually for syntactic 
SMT. In Proc. of EMNLP 2012, pages 854-862. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine 
translation. In Proc. of ACL-COLING 2006, pages 
609-616. 
Yang Liu, Yajuan Lv and Qun Liu. 2009. Improving 
tree-to-tree translation with packed forests. In Proc. 
of ACL-IJCNLP 2009, pages 558-566. 
Daniel Marcu, Wei Wang, Abdessamad Echihabi and 
Kevin Knight. 2006. SPMT: Statistical machine 
translation with syntactified target language phrases. 
In Proc. of EMNLP 2006, pages 44-52. 
Franz Och, 2003. Minimum error rate training in 
statistical machine translation. In Proc. of ACL 2003, 
pages 160-167. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic 
evaluation of machine translation. In Proc. of ACL 
2002, pages 311-318. 
Slav Petrov, Leon Barrett, Romain Thibaux and Dan 
Klein. 2006. Learning accurate, compact, and 
interpretable tree annotation. In Proc. of COLING-
ACL 2006, pages 433-440. 
Chris Quirk, Arul Menezes and Colin Cherry. 2005. 
Dependency treelet translation: syntactically 
informed phrasal SMT. In Proc. of ACL 2005, pages 
271-279. 
Libin Shen, Jinxi Xu and Ralph Weischedel. 2008. A 
new string-to-dependency machine translation 
algorithm with a target dependency language model. 
In Proc. of ACL-08, pages 577-585. 
Wei Wang, Kevin Knight, and Daniel Marcu. 2007. 
Binarizing syntax trees to improve syntax-based 
machine translation accuracy. In Proc. of EMNLP 
2007, pages 746-754. 
Wei Wang, Jonathan May, Kevin Knight, and Daniel 
Marcu. 2010. Re-structuring, re-labeling, and re-
aligning for syntax-based machine translation. 
Computational Linguistics, 36(2):247?277. 
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing 
Zong. 2012. Tree-based translation without using 
parse trees. In Proc. of COLING 2012, pages 3037-
3054. 
Hao Zhang, Liang Huang, Daniel Gildea and Kevin 
Knight. 2006. Synchronous binarization for machine 
translation. In Proc. of HLT-NAACL 2006, pages 
256-263. 
Hao Zhang, Daniel Gildea, and David Chiang. 2008. 
Extracting synchronous grammars rules from word 
level alignments in linear time. In Proc. of COLING 
2008, pages 1081-1088. 
Hao Zhang, Licheng Fang, Peng Xu, Xiaoyun Wu.  
2011a. Binarized forest to string translation. In Proc. 
of ACL 2011, pages 835-845. 
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, Chew 
Lim Tan. 2009. Forest-based tree sequence to string 
translation model. In Proc. of ACL-IJCNLP 2009, 
pages 172-180. 
Jiajun Zhang, Feifei Zhai and Chengqing Zong. 2011b. 
Augmenting string-to-tree translation models with 
fuzzy use of source-side syntax. In Proc. of EMNLP 
2011, pages 204-215. 
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Chew 
Lim Tan and Sheng Li. 2007. A tree-to-tree 
alignment-based model for statistical Machine 
translation. MT-Summit-07. pages 535-542 
Min Zhang, Hongfei Jiang, Ai ti Aw, Haizhou Li, Chew 
Lim Tan and Sheng Li. 2008. A tree sequence 
alignment-based tree-to-tree translation model. In 
Proc. of ACL 2008, pages 559-567. 
Andreas Zollmann and Ashish Venugopal. 2006. Syntax 
augmented machine translation via chart parsing. In 
Proc. of Workshop on Statistical Machine 
Translation 2006, pages 138-141. 
Andreas Zollmann and Stephan Vogel. 2011. A word-
class approach to labeling PSCFG rules for machine 
translation. In Proc. of ACL 2011, pages 1-11. 
254
