Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 109?113,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Joint Syntactic and Semantic Dependency Parsing System based on
Maximum Entropy Models
Buzhou Tang1 Lu Li2 Xinxin Li1 Xuan Wang2 Xiaolong Wang2
Shenzhen Graduate School
Harbin Institute of Technology
Shenzhen,518055, China
1{tangbuzhou,lixxin2}@gmail.com
2{lli,wangxuan,wangxl}@insun.hit.edu.cn
Abstract
A joint syntactic and semantic dependency
parsing system submitted to the CoNLL-2009
shared task is presented in this paper. The
system is composed of three components: a
syntactic dependency parser, a predicate clas-
sifier and a semantic parser. The first-order
MSTParser is used as our syntactic depen-
dency pasrser. Projective and non-projective
MSTParsers are compared with each other on
seven languages. Predicate classification and
semantic parsing are both recognized as clas-
sification problem, and the Maximum Entropy
Models are used for them in our system. For
semantic parsing and predicate classifying, we
focus on finding optimized features on multi-
ple languages. The average Macro F1 Score
of our system is 73.97 for joint task in closed
challenge.
1 Introduction
The task for CoNLL-2009 is an extension of the
CoNLL-2008 shared task to multiple languages: En-
glish (Surdeanu et al, 2008), Catalan plus Span-
ish (Mariona Taule? et al, 2008), Chinese (Martha
Palmer et al, 2009), Czech (Jan Hajic? et al,
2006), German (Aljoscha Burchardt et al, 2006) and
Japanese (Daisuke Kawahara et al, 2002). Com-
pared to the CoNLL-2008 shared task, the predi-
cates are given for us in semantic dependencies task.
Therefore, we have only need to label the semantic
roles of nouns and verbs, and the frames of predi-
cates.
In this paper, a joint syntactic and semantic de-
pendency parsing system submitted to the CoNLL-
2009 shared task is presented. The system is com-
posed of three components: a syntactic dependency
parser, a predicate classifier and a semantic parser.
The first-order MSTParser is used as our syntactic
dependency parser. Projective and non-projective
MSTParsers are compared with each other on seven
languages. The predicate classifier labeling the
frames of predicates and the semantic parser label-
ing the semantic roles of nouns and verbs for each
predicate are both recognized as classification prob-
lem, and the Maximum Entropy Models (MEs) are
used for them in our system. Among three com-
ponents, we mainly focus on the predicate classifier
and the semantic parser.
For semantic parsing and predicate classifying,
features of different types are selected to our sys-
tem. The effect of them on multiple languages will
be described in the following sections in detail.
2 System Description
Generally Speaking, a syntactic and semantic de-
pendency parsing system is usually divided into four
separate subtasks: syntactic parsing, predicate iden-
tification, predicate classification, and semantic role
labeling. In the CoNLL-2009 shared task, the pred-
icate identification is not required, since the pred-
icates are given for us. Therefore, the system we
present is only composed of three components: a
syntactic dependency parser, a predicate classifier
and a semantic parser. The syntactic dependencies
are processed with the MSTParser 0.4.3b. The pred-
icates identification and semantic role label are pro-
cessed with MEs-based classifier respectively. Un-
like conventional systems, the predicates identifica-
109
tion and the semantic parser are independent with
each other. Figure 1 is the architecture of our sys-
tem.
Figure 1: System Architecture
In our system, we firstly select an appropriate
mode (projective or non-projective) of Graph-based
Parser (MSTParser) for each language, then con-
struct the MEs-based predicates classification and
the MEs-based semantic parser with syntactic de-
pendency relationships and predicate classification
respectively.
2.1 Syntactic Dependency Parsing
MSTParser (McDonald, 2008) is used as our syn-
tactic dependency parser. It is a state-of-the-art de-
pendency parser that searches for maximum span-
ning trees (MST) over directed graph. Both of pro-
jective and non-projective are supported by MST-
Parser. Our system employs the first-order frame-
work with projective and non-projective modes on
seven given languages.
2.2 Predicate Classification
In this phase, we label the sense of each predicate
and the MEs are adopted for classification. Features
of different types are extracted for each predicate,
and an optimized combination of them is adopted in
our final system. Table 1 lists all features. 1-20 are
the features used in Li?s system (Lu Li et al, 2008),
No Features No Features
1 w0 20 Lemma
2 p0 21 DEPREL
3 p?1 22 CHD POS
4 p1 23 CHD POS U
5 p?1p0 24 CHD REL
6 p0p1 25 CHD REL U
7 p?2p0 26 SIB REL
8 p0p2 27 SIB REL U
9 p?3p0 28 SIB POS
10 p0p3 29 SIB POS U
11 p?1p0p1 30 VERB V
12 w0p0 31 4+11
13 w0p?1p0 32 Indegree
14 w0p0p1 33 Outdegree
15 w0p?2p0 34 Degree
16 w0p0p2 35 ARG IN
17 w0p?3p0 36 ARG OUT
18 w0p0p3 37 ARG Degree
19 w0p?1p0p1 38 Span
Table 1: Features for Predicate Classification.
and 21-31 are a part of the optimized features pre-
sented in Che?s system (Wanxiang Che et al, 2008)
In Table 1, ?w? denotes the word and ?p? de-
notes POS of the words. Features in the form of
part1 part2 denote the part2 of the part1, while fea-
tures in the form of part1+part2 denote the combi-
nation of the part1 and part2. ?CHD? and ?SIB? de-
note a sequence of the child and the sibling words
respectively, ?REL? denotes the type of relations,
?U? denotes the result after reducing the adjacent
duplicate tags to one, ?V? denotes whether the part
is a voice, ?In? and ?OUT? denote the in degree and
out degree, which denotes how many dependency
relations coming into this word and going away from
this word,and ?ARG? denotes the semantic roles of
the predicate. The ?Span? denotes the maximum
length between the predicate and its arguments. The
final optimized feature combination is :1-31 and 33-
37.
2.3 Semantic Role Labeling
The semantic role labeling usually contains two sub-
tasks: argument identification and argument classi-
fication. In our system, we perform them in a single
110
stage through one classifier, which specifies a par-
ticular role label to the argument candidates directly
and assigns ?NONE? label to the argument candi-
dates with no role. MEs are also adopted for classifi-
cation. For each word in a sentence, MEs gives each
candidate label (including semantic role labels and
none label) a probability for the predicate. The fea-
tures except for the feature (lemma plus sense num-
ber of the predicate in (Lu Li et al, 2008)) and the
features 32-38 in Table 1 are selected in our system.
3 Experiments and Results
We train the first-order MSTParser 1 with projective
and non-projective modes in terms of default param-
eters respectively. Our maximum entropy classifiers
are implemented with the Maximum Entropy Mod-
eling Toolkit 2 . The default classifier parameters are
used in our system except for iterations. All mod-
els are trained using all training data, and tested on
the whole development data and test data, with 64-
bit 3.00GHz Intel(R) Pentium(R) D CPU and 4.0G
memory.
3.1 Syntactic Dependency Parsing
Table 2 is a performance comparison between pro-
jective parser and non-projective parser on the devel-
opment data of seven languages. In Table 2, ?LAS?,
?ULAS? and ?LCS? denote as Labeled attachment
score, Unlabeled attachment score and Label accu-
racy score respectively.
The experiments show that Catalan, Chinese and
Spanish have projective property and others have
non-projective property.
3.2 Predicate Classification
To get the optimized system, three group features are
used for comparison.
? group 1: features 1-20 in Table 1.
? group 2: features 1-31 in Table 1.
? group 3: all features in Table 1.
The performance of predicate classification on the
development data of the six languages, which con-
tain this subtask, are given in Table 3. The results
1http://sourceforge.net/projects/mstparser.
2http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.
LAS(%) ULAS(%) LCS(%)
Catalan 84.18 88.18 91.76
83.69 87.74 91.59
Chinese 72.58 77.06 82.07
62.85 69.47 73.00
Czech 72.79 81.40 80.93
73.18 81.86 81.30
English 86.89 90.29 91.50
86.88 90.34 91.58
German 83.43 86.89 90.24
84.00 87.40 90.61
Japanese 92.23 93.16 98.38
92.23 93.14 98.45
Spanish 83.88 87.93 91.36
83.46 87.46 91.37
Table 2: Performance of Syntactic Dependency
Parsing with different modes. The above line is the
performance of projective mode, while the below
one is the performance of non-projective mode for
each language.
group 1 group 2 group 3
Catalan 75.51 80.90 82.23
Chinese 93.79 94.99 94.75
Czech 91.83 91.77 91.86
English 92.12 92.48 93.20
German 74.49 74.14 75.85
Spanish 74.01 76.22 76.53
Table 3: Performance of predicate classification (F1
scores) for different group features on the develop-
ment data of the six languages.
show that Che?s features and the degrees of the pred-
icate and its arguments are useful for all languages,
the former improves the labeled F1 measure by 0.3%
to 5.4%, and the latter by 0.3% to 1.7%.
3.3 Semantic Role Labeling
In this phase, feature selection and performance lose
caused by P-columns are studied. Firstly, we com-
pare the following two group features:
? group 1: The features except for the lemma
plus sense number of the predicate in (Lu Li
et al, 2008).
111
LF1 ULF1 PF1
Catalan 73.25 92.69 38.41
72.71 91.93 35.22
83.23 100.00 61.88
Chinese 69.60 82.15 28.35
71.49 81.71 29.41
85.44 95.21 58.20
Czech 80.62 92.49 70.04
79.10 91.44 68.34
85.42 96.93 77.78
English 73.91 87.26 33.16
76.10 88.58 36.28
79.35 91.74 43.32
German 64.85 88.05 27.21
65.36 88.63 26.70
72.78 94.54 41.50
Japanese 69.43 82.79 29.27
69.87 83.31 29.69
72.80 87.13 34.96
Spanish 73.49 93.15 39.64
78.18 91.68 33.57
81.96 99.98 59.20
Table 4: Performance of Semantic Role Labeling
(F1 score) with different features.
? group 2: group1+the degrees of the predicate
and its arguments presented in the last section.
Secondly, features extracted from golden-columns
and P-columns are both used for testing.
The performance of them are given in Table 4,
where ?LF1?, ?ULF1? and ?PF1? denote as Labeled
F1 score, Unlabeled F1 score and Proposition F1
score respectively. The above line is the F1 scores of
Semantic Role Labeling with different features. The
uppermost line is the result of group1 features, the
middle line is the result of group2 features extracted
from P-columns, and the downmost one is the result
of group2 features extracted from golden-columns
for each language.
The results show that the features of degree also
improves the labeled F1 measure by 3.4% to 15.8%,
the different labeled F1 between golden-columns
and P-columns is about 2.9%?13.9%.
LAS LF1 M LF1
Catalan 84.18 72.71 81.46
75.68 66.95 71.32
Chinese 72.58 71.49 72.20
63.95 67.06 65.53
Czech 73.18 79.10 76.37
72.60 79.08 75.85
Czech-ood 69.81 79.80 74.81
English 86.88 76.10 82.89
86.61 77.17 81.92
English-ood 80.09 67.21 73.69
German 84.00 65.36 83.06
79.85 61.98 70.93
German-ood 71.86 61.83 66.86
Japanese 92.23 69.87 83.77
91.26 69.58 80.49
Spanish 83.88 71.18 80.74
77.21 66.23 71.72
Table 5: Overall performance of our final joint sys-
tem.
3.4 Overall Performance
In the final system, we select the optimized feature
subset discussed in the former sections. The overall
performance of the system on the development data ,
test data and Out-of-domain data are shown in Table
5 (all features are extracted from P-columns). The
average Macro F1 Scores of our system are 73.97
on test data and 71.79 on Out-of-domain data.
In Table 5, ?LAS?, ?LF1? and ?M LF1? denote
as Labeled accuracy score for Syntactic Dependency
Parsing, Labeled F1 score for Semantic Role Label-
ing, and Overall Macro Labeled F1 score respec-
tively. The topmost line is the result on the devel-
opment data, the middle one is the result on the test
data for each language and the downmost one is the
result on the Out-of-domain data if the data exist.
4 Conclusion and Discussion
We present a joint syntactic and semantic depen-
dency parsing system for CoNLL2009 Shared Task,
which composed of three components: a syntac-
tic dependency parser, a predicate classifier and a
semantic parser. All of them are built with some
state-of-the-art methods. For the predicate classifier
and the semantic parser, a new kind of features?
112
degrees, which reflect the activeness of the words
in a sentence improves their performance. In order
to improve the performance further, we will study
new machine learning methods for semantic depen-
dency parsing, especially the joint learning methods,
which can avoid the information loss problem of our
system.
Acknowledgments
We would like to thank McDonald for providing
the MSTParser program, to Zhang Le for provid-
ing the Maxent program. This research has been
partially supported by the National Natural Science
Foundation of China(No.60703015) and the Na-
tional 863 Program of China (No.2006AA01Z197,
No.2007AA01Z194).
References
Jan Hajic? and Massimiliano Ciaramita and Richard Jo-
hansson and Daisuke Kawahara and Maria Anto`nia
Mart?? and Llu??s Ma`rquez and Adam Meyers and
Joakim Nivre and Sebastian Pado? and Jan S?te?pa?nek
and Pavel Stran?a?k and Miahi Surdeanu and Nianwen
Xue and Yi Zhang. 2009. The CoNLL-2009 Shared
Task: Syntactic and Semantic Dependencies in Multi-
ple Languages. Proceedings of the 13th Conference on
Computational Natural Language Learning (CoNLL-
2009), June 4-5. Boulder, Colorado, USA.
Mariona Taule? and Maria Anto`nia Mart?? and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Cor-
pora for Catalan and Spanish. Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008). Marrakesh, Morroco.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1),pages 143?172.
Jan Hajic? and Jarmila Panevova? and Eva Hajic?ova? and
Petr Sgall and Petr Pajas and Jan S?te?pa?nek and Jir???
Havelka and Marie Mikulova? and Zdene?k Z?abokrtsky?.
2006. Prague Dependency Treebank 2.0. CD-ROM,
Cat. No. LDC2006T01, ISBN 1-58563-370-4. Lin-
guistic Data Consortium, Philadelphia, Pennsylvania,
USA. URL: http://ldc.upenn.edu.
Surdeanu, Mihai and Johansson, Richard and Meyers,
Adam and Ma`rquez, Llu??s and Nivre, Joakim. 2008.
The CoNLL-2008 Shared Task on Joint Parsing of
Syntactic and Semantic Dependencies. Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning(CoNLL-2008).
Aljoscha Burchardt and Katrin Erk and Anette Frank and
Andrea Kowalski and Sebastian Pado? and Manfred
Pinkal. 2006. The SALSA corpus: a German corpus
resource for lexical semantics. Proceedings of the 5rd
International Conference on Language Resources and
Evaluation (LREC-2006), pages 2008?2013. Genoa,
Italy.
Daisuke Kawahara and Sadao Kurohashi and Ko?iti
Hasida. 2002. Construction of a Japanese Relevance-
tagged Corpus. Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013. Las Palmas, Canary
Islands.
McDonald and Ryan. 2006. Discriminative Learning
and Spanning Tree Algorithms for Dependency Pars-
ing, Ph.D. thesis. University of Pennsylvania.
Lu Li, Shixi Fan, Xuan Wang, XiaolongWang. 2008.
Discriminative Learning of Syntactic and Semantic
Dependencies. CoNLL 2008: Proceedings of the
12th Conference on Computational Natural Language
Learning, pages 218?222. Manchester.
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li,
Bing Qin, Ting Liu, Sheng Li. 2008. A Cascaded
Syntactic and Semantic Dependency Parsing System.
CoNLL 2008: Proceedings of the 12th Conference
on Computational Natural Language Learning, pages
238?242. Manchester.
113
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 802?806,
Dublin, Ireland, August 23-24, 2014.
UTH_CCB: A Report for SemEval 2014 ? Task 7 Analysis of Clinical Text  
   Yaoyun Zhang1   Jingqi Wang1   Buzhou Tang2   Yonghui Wu1   Min Jiang1              Yukun Chen3  Hua Xu1* 1University of Texas School of Biomedical Informatics at Houston Houston, TX, 77030, USA 
2Harbin Institute of Technology Shenzhen Graduate School Shenzhen, 518055, China 
3Vanderbilt University Department of Biomedical Informatics Nashville, TN, 37240, USA {Yaoyun.Zhang, Yonghui.Wu, Min.Jiang, Hua.Xu} @uth.tmc.edu tangbuzhou@gmail.com yukun.chen@Vanderbilt.Edu      Abstract This work describes the participation of the University of Texas Health Science Center at Houston (UTHealth) team on the SemEval 2014 ? Task 7 analysis of clinical text challenge. The task consisted of two subtasks: (1) disorder entity recognition,  recognizing mentions of disorder concepts; (2) disorder entity encoding, mapping each mention to a unique Concept Unique Identifier (CUI) defined in Unified Medical Language System (UMLS). We developed three ensemble learning approaches for recognizing disorder entities and a Vector Space Model based method for encoding. Our approaches achieved top rank in both subtasks, with the best F measure of 0.813 for entity recognition and the best accuracy of 74.1% for encoding, indicating the proposed approaches are promising.  1 Introduction  In recent years, clinical natural language processing (NLP) has received great attention for its critical role in unlocking information embedded in clinical documents. Leveraging such information can facilitate the secondary1 use of electronic health record (EHR) data to                                                      This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details:http://creativecommons.org/licenses/by/4.0/ 
promote clinical and translational research.  Clinical entity recognition, which recognizes mentions of clinically relevant concepts (e.g., disorders, procedures, drugs etc.) in narratives,   and clinical entity encoding, which maps the recognized entities to concepts in standard vocabularies (e.g., UMLS CUI (Bodenreider, 2004)), are among the fundamental tasks in clinical NLP research. Many systems have been developed to extract clinical concepts from various types of clinical notes in last two decades, ranging from early symbolic NLP systems heavily dependent on domain knowledge to machine learning algorithm based systems driven by increasingly available annotated clinical corpora. The representative systems include MedLEE (Friedman et al., 1994), MetaMap (Aronson and Lang, 2010), KnowledgeMap (Denny et al., 2003), cTAKES (Savova et al., 2010), etc. Clinical NLP challenges organized by the Center for Informatics for Integrating Biology & the Beside (i2b2) have promoted research using machine learning algorithms to recognize clinical entities (Uzuner et al., 2010; Uzuner et al., 2011).  Unlike the previous i2b2 challenges, the ShARe/CLEF challenge of clinical disorder extraction and encoding held in 2013 took the initiative to recognize disjoint entities, in addition to entities made up of consecutive words (Chapman et al., 2013). ShARe/CLEF challenge also required encoding of the disorder entities to Systematized Nomenclature Of Medicine Clinical Terms (SNOMED-CT) (using UMLS CUIs).  
802
In this paper, we describe our system for Task 7 of SemEval 2014, which followed the requirements of 2013 ShARe/CLEF challenge. Our system employed ensemble learning based approaches for disorder entity recognition and a Vector Space Model (VSM) based method for mapping extracted entities to CUIs of SNOMED-CT concepts. Our system was top-ranked among all participating teams according to evaluation by the organizer. 2 Method Our end-to-end system for Task 7 of SemEval 2014 consists of two components: disorder entity recognition and encoding. The raw clinical notes first went through the pre-processing modules for rule-based sentence boundary detection and tokenization. Extracted features were then used to train two machine learning algorithm-based entity recognition models, Conditional Random Fields (CRFs) (Lafferty et al., 2001) and Structural Support Vector Machines (SSVMs) (Tsochantaridis et al., 2005), respectively. These two models were ensembled with MetaMap, a symbolic biomedical NLP system, by three different approaches. Recognized entities were mapped to SNOWMED-CT CUIs in the encoding component. Detailed information of the components are presented in the following sections.  2.1 Dataset The training and test sets of 2013 ShARe/CLEF challenge were used as the training and development sets respectively for system development in SemEval 2014 Task 7. The training set consists of 199 notes and the development set has 99 notes, both of which were collected from four types of clinical notes including discharge summaries (DIS), radiology reports (RAD), and ECG/ECHO reports. Based on a pre-defined guideline, disorder entities were annotated for each note and then mapped to UMLS CUIs of SNOMED-CT concepts. Disorder entities not found in SNOMED-CT were marked as ?CUI-less?. The training set contained 5811 disorder entities which were mapped to 1007 unique CUIs or CUI-less. The development set contained 5340 disorder entities mapped to 795 CUIs or CUI-less. The test set contained 133 notes, all of which were discharge summaries. As the gold-standard annotation of the test set is not released by the organizer, the detailed annotation information of the test set is 
not available. Table 1 shows the total counts of notes, entities and CUIs in the three datasets.   Dataset Type Note Entity CUI CUI-less Train ALL 199 5816 4177 1639 ECHO 42 828 662 166 RAD 42 555 392 163 DIS 61 3589 2646 943 ECG 54 193 103 90 Dev ALL 99 5340 3619 1721 ECHO 12 338 241 97 RAD 12 162 126 36 DIS 75 4840 3252 1588 ECG 0 0 0  Test ALL 133 - -  DIS 133 - -   Table 1. Statistics of the dataset.  2.2 Disorder entity recognition The disorder entity recognition component consists of two modules: 1) the machine learning (e.g., CRF and SSVM) based named entity recognition (NER) module and 2) the   ensemble learning module. For the challenge of this year, we mainly focused on the second ensemble learning module. Machine learning based NER Module. This module was built based on our previous challenge participation in the 2013 ShARe/CLEF challenge (Tang et al., 2013). Annotated data were typically converted into a BIO format in machine learning-based NER systems. Each word was assigned one of the three labels: B for beginning of an entity, I for inside an entity, and O for outside of an entity. A unique challenge of this task is the high frequency (>10%) of disjoint disorders. For example, in the sentence ?the left atrium is not moderately dilated?, the discontinuous phrase ?left atrium?dilated? is defined as a disjoint disorder. Such entities could not be directly represented using the traditional BIO approach. Therefore, in addition to traditional BIO tags used for labeling words in the consecutive disorder entities, two sets of tags were created for disjoint entities: (1) D{B, I} was used to label disjoint entity words that are not shared by multiple concepts; and (2) H{B, I} was used to label head words that belonged to more than two disjoint concepts. Ultimately, we assigned one of the seven labels {B, I, O, DB, DI, HB, HI} to each word. A few simple rules were then defined to convert labeled words to entities (Tang et al., 2013).  
803
We exploited two state-of-the-art machine learning algorithms for disorder entity recognition, namely CRF (Lafferty et al., 2001) and SSVM (Tsochantaridis et al., 2005). CRFsuite and SVMhmm were used to implement CRF and SSVM respectively. For features, we used bag-of-word, part-of-speech from Stanford tagger, type of notes, section information, word representation from Brown clustering (Brown et al., 1992), random indexing (Lund and Burgess, 1996) and semantic categories of words based on UMLS lookup, MetaMap, and cTAKES outputs. More detailed information of this module can be found in our paper for 2013 ShARe/CLEF challenge (Tang et al., 2013).  One thing to note is that for word representation features like Brown clustering and random indexing, we only use the combination of traning and development and test datasets for feature extraction. The non-annotated corpus provided by the SemEval organizers was not employed currently. We do plan to pre-generate word clusters and random indexing using the provided corpus in the near future. Ensemble Learning Module. Three approaches were employed to consolidate the CRF-model, SSVM-model and the MetaMap outputs, namely machine learning classifier based ensemble (ensembleML), majortiy voting based ensemble (ensembleMV) and direct merging of the entity recognition results from the three models (ensembleDM). In the ensembleML approach, a binary classifier was trained to determine if the entities recognized by the CRF-model, SSVM-model and MetaMap were true positives. A new set of features were then extracted for each candidate entity, that included the specific models recognizing the entity, the entity itself, n-gram and word shape features of the first/last word of the entity. A sliding window based feature was extracted to check whether there was any recognized entity within 20 characters before the first and after the last word. Some features extracted from the first module were also employed. We used the open source toolkit Liblinear (Fan et al., 2008), to build the binary classifier for ensembleML.  2.3 Disorder Entity Encoding We developed a Vector Space Model (VSM) based approach to find the most suitable CUI for a given disorder entity. The disorder entity was 
used as query and all the UMLS terms were treated as documents. We used the cosine-similarity score to rank the candidate terms. For post-processing, if the top-ranked CUI was not a disorder CUI, it was replaced with ?CUI-less?.  ?CUI-less? was also assigned to entities without any retrieved candidate CUI. 2.4 Experiments and Evaluation Our system was developed and trained using the enlarged training set by merging the 199 notes in the training set and the 99 notes in the development set. All parameters of CRF, SSVM and Liblinear were optimized by 10-fold cross-validation on the enlarged training dataset. The performance of disorder entity recognition was evaluated by precision, recall and F-measure, which were measured in both ?strict? and ?re-laxed? modes. The ?strict? mode was defined as follows: a concept is correctly recognized if and only if it can be matched exactly to a disorder mention in the gold standard, and the ?relaxed? mode means that a disorder mention is correctly recognized if it overlaps with any disorder men-tion in the gold standard. For entity encoding, all participating systems were evaluated using accu-racy, in ?strict? and ?relaxed? modes, as defined in (Suominen et al., 2013). 3 Results Table 2 and Table 3 show the best performance of our systems in the SemEval 2014 Task 7 as reported by the organizers, where ?P?, ?R?, ?F? denote precision, recall and F-measure respectively. For disorder entity recognition, the ensembleML based system outperformed the other two ensemble approaches, achieving the best F-measure of 0.813 under ?strict? criterion and was ranked first in the challenge. For encoding, our system achieved an accuracy of 0.741 by ensembleDM under ?strict? criterion and was again ranked first in the challenge.   Strict Relaxed P R F P R F ensembleML 84.3 78.6 81.3 93.6 86.6 90.0 Table 2. The disorder recognition performance of our system for the SemEval 2014 task 7 (%).   Accuracy Strict Relaxed ensembleDM 0.741 0.873 Table 3. The SNOMED encoding performance of our system for the SemEval 2014 task 7.  
804
4 Discussion In this study, we developed an ensemble learning-based approach to recognize disorder entities and a vector space model-based method to encode disorders to UMLS CUIs.  Our system was top-ranked among all participating teams. However, there are still expectations for further improvement.  For disorder entity recognition, directly merging the entity recognition results of the three models (ensembleDM) achieved the highest encoding accuracy of 0.741. This shows the great potential of performance enhancement by combining different models. However, the precision of ensembleDM was much lower than the current machine learning-based ensemble approach ensembleML. ensembleML improved the precision to 84.3%, with the lowest recall of 78.6% among the three ensemble approaches. Further investigations for balancing and enhancing both precision and recall simultaneously by combining different models will be pursued in the follow-up studies. For encoding, when a disorder entity can be labelled with multiple CUIs in different contexts, a more effective disambiguation model could be exploited. Further, query expansion techniques may be helpful and worth investigating. The above methods should be potentially helpful to address the problems caused by synonyms or spelling variants.  5 Conclusion We developed a clinical disorder recognition and encoding system that consists of a ensemble learning-based approach to recognize disorder entities and a vector space model-based method to encode the identified disorders to UMLS CUIs of SNOMED-CT concepts. The performance of our system was top-ranked in the SemEval 2014 Task 7, indicating that our approaches are promising. However, further improvements are needed in order to enhance performance on concept extraction and encoding in clinical text. Acknowledgments This study is supported in part by grants from NLM R01LM010681, NCI 1R01CA141307, NIGMS 1R01GM102282 and CPRIT R1307 (H.X).    
Reference Aronson, A. R., & Lang, F.-M. (2010). An overview of MetaMap: historical perspective and recent advances. Journal of the American Medical Informatics Association: JAMIA, 17(3), 229?236.  Bodenreider, O. (2004). The unified medical language system (UMLS): integrating biomedical terminology. Nucleic Acids Research, 32(suppl 1), 267?270. Brown, P. F., deSouza, P. V., Mercer, R. L., Pietra, V. J. D., & Lai, J. C. (1992). Class-Based n-gram Models of Natural Language. Computational Linguistics, 18, 467?479. Denny, J. C., Irani, P. R., Wehbe, F. H., Smithers, J. D., & Spickard, A. (2003). The KnowledgeMap Project: Development of a Concept-Based Medical School Curriculum Database. AMIA Annual Symposium Proceedings, 2003, 195?199. Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., & Lin, C.-J. (2008). LIBLINEAR: A library for large linear classification. The Journal of Machine Learning Research, 9, 1871?1874. Friedman, C., Alderson, P. O., Austin, J. H., Cimino, J. J., & Johnson, S. B. (1994). A general natural-language text processor for clinical radiology. Journal of the American Medical Informatics Association, 1(2), 161?174. Lafferty, J., McCallum, A., & Pereira, F. C. N. (2001). Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. Departmental Papers (CIS).  Lund, K., & Burgess, C. (1996). Producing high-dimensional semantic spaces from lexical co-occurrence. Behavior Research Methods, Instruments, & Computers, 28(2), 203?208.  Savova, G. K., Masanz, J. J., Ogren, P. V., Zheng, J., Sohn, S., Kipper-Schuler, K. C., & Chute, C. G. (2010). Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications. Journal of the American Medical Informatics Association: JAMIA, 17(5), 507?513.  Suominen, H., Salanter?, S., Velupillai, S., Chapman, W. W., Savova, G., & Elhadad, N. (2013). Overview of the ShARe/CLEF eHealth Evaluation Lab 2013. Information Access Evaluation. Multilinguality, Multimodality, and Visualization, 2013, 212?231. Tang, B., Cao, H., Wu, Y., Jiang, M., & Xu, H. (2013). Recognizing and Encoding Discorder Concepts in Clinical Text using Machine Learning and Vector Space Model. Workshop of ShARe/CLEF eHealth Evaluation Lab 2013.  
805
Tsochantaridis, I., Joachims, T., Hofmann, T., & Altun, Y. (2005). Large margin methods for structured and interdependent output variables. Journal of Machine Learning Research, 6, 1453?1484. Uzuner, ?., Solti, I., & Cadag, E. (2010). Extracting medication information from clinical text. Journal of the American Medical Informatics Association, 17(5), 514?518.  Uzuner, ?., South, B. R., Shen, S., & DuVall, S. L. (2011). 2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text. Journal of the American Medical Informatics Association: JAMIA, 18(5), 552?556.    
806
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 13?17,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A Cascade Method for Detecting Hedges and their Scope in Natural
Language Text
Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan, Shixi Fan
Key Laboratory of Network Oriented Intelligent Computation
Harbin Institute of Technology Shenzhen Graduate School
Shenzhen, Guangdong, China
{tangbuzhou,yuanbo.hitsz}@gmail.com
{wangxl,wangxuan,fanshixi}@insun.hit.edu.cn
Abstract
Detecting hedges and their scope in nat-
ural language text is very important for
information inference. In this paper,
we present a system based on a cascade
method for the CoNLL-2010 shared task.
The system composes of two components:
one for detecting hedges and another one
for detecting their scope. For detecting
hedges, we build a cascade subsystem.
Firstly, a conditional random field (CRF)
model and a large margin-based model are
trained respectively. Then, we train an-
other CRF model using the result of the
first phase. For detecting the scope of
hedges, a CRF model is trained according
to the result of the first subtask. The ex-
periments show that our system achieves
86.36% F-measure on biological corpus
and 55.05% F-measure on Wikipedia cor-
pus for hedge detection, and 49.95% F-
measure on biological corpus for hedge
scope detection. Among them, 86.36%
is the best result on biological corpus for
hedge detection.
1 Introduction
Hedge cues are very common in natural language
text. Vincze et al (2008) report that 17.70% of
the sentences in the abstract section and 19.94% of
sentences in the full paper section contain hedges
on BioScope corpus. As Vincze et al (2008)
suggest that information that falls in the scope
of hedges can not be presented as factual in-
formation. Detecting hedges and their scope in
natural language text is very important for in-
formation inference. Recently, relative research
has received considerable interest in the biomed-
ical NLP community, including detecting hedges
and their in-sentence scope in biomedical texts
(Morante and Daelemans, 2009). The CoNLL-
2010 has launched a shared task for exploiting the
hedge scope annotated in the BioScope (Vincze et
al., 2008) and publicly available Wikipedia (Gan-
ter and Strube, 2009) weasel annotations. The
shared task contains two subtasks (Farkas et al,
2010): 1. learning to detect hedges in sentences on
BioScope and Wikipedia; 2. learning to detect the
in-sentence scope of these hedges on BioScope.
In this paper, we present a system based on a
cascade method for the CoNLL-2010 shared task.
The system composes of two components: one
for detecting hedges and another one for detect-
ing their scope. For detecting hedges, we build
a cascade subsystem. Firstly, conditional ran-
dom field (CRF) model and a large margin-based
model are trained respectively. Then, we train
another CRF model using the result of the first
phase. For detecting the scope of hedges, a CRF
model is trained according to the result of the first
subtask. The experiments show that our system
achieves 86.36% F-measure on biological corpus
and 55.05% F-measure on Wikipedia corpus for
hedge detection, and 49.95% F-measure on bio-
logical corpus for hedge scope detection. Among
them, 86.36% is the best result on biological cor-
pus for hedge detection.
2 System Description
As there are two subtasks, we present a system
based on a cascade supervised machine learning
methods for the CoNLL-2010 shared task. The ar-
chitecture of our system is shown in Figure 1.
The system composes of two subsystems for
two subtasks respectively, and the first subsystem
is a two-layer cascaded classifier.
2.1 Hedge Detection
The hedges are represented by indicating whether
a token is in a hedge and its position in the
CoNLL-2010 shared task. Three tags are used for
13
Figure 1: System architecture
this scheme, where O cue indicates a token out-
side of a hedge, B cue indicates a token at the
beginning of a hedge and I cue indicates a to-
ken inside of a hedge. In this subsystem, we do
preprocessing by GENIA Tagger (version 3.0.1)1
at first, which does lemma extraction, part-of-
speech (POS), chunking and named entity recog-
nition (NER) for feature extraction. For the out-
put of GENIA Tagger, we convert the first char
of a lemma into lower case and BIO chunk tag
into BIOS chunk tag, where S indicates a token
is a chunk, B indicates a token at the beginning
of a chunk, I indicates a token inside of a chunk,
and O indicates a token outside of a chunk. Then
a two-layer cascaded classifier is built for pre-
diction. There are a CRF classifier and a large
margin-based classifier in the first layer and a CRF
classifier in the second layer.
In the first layer, the following features are used
in our system:
? Word andWord Shape of the lemma: we used
the similar scheme as shown in (Tsai et al,
2005).
1http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/
? Prefix and Suffix with length 3-5.
? Context of the lemma, POS and the chunk in
the window [-2,2].
? Combined features including L0C0, LiP0
and LiC0, where ?1 ? i ? 1 L denotes the
lemma of a word, P denotes a POS and C
denotes a chunk tag.
? The type of a chunk; the lemma and POS se-
quences of it.
? Whether a token is a part of the pairs ?neither
... nor? and ?either ... or? as both tokens of a
pair are always labeled with the same tag.
? Whether a token can possibly be classified
into B cue, I cue or O cue; its lemma, POS
and chunk tag for each possible case: these
features are extracted according to a dictio-
nary extracted from training corpus, which
lists all possible hedge tag for each word in
the training corpus.
In the second layer, we used some features
about the result of the last layer besides those men-
tioned above. They are listed as follow:
? The lemma and POS sequences of the hedge
predicted by each classifier.
? The times of a token classified into B cue,
I cue and O cue by the first two classifiers.
? Whether a token is the last token of the hedge
predicted by each classifier.
2.2 Hedge Scope Detection
We follow the way of Morante and Daelemans
(2009) to represent the scope of a hedge, where
F scope indicates a token at the beginning of a
scope sequence, L scope indicates a token at the
last of a scope sequence, and NONE indicates
others. In this phase, we do preprocessing by
GDep Tagger (version beta1)2 at first, which does
lemma extraction, part-of-speech (POS), chunk-
ing, named entity recognition (NER) and depen-
dency parse for feature extraction. For the out-
put of GDep Tagger, we deal with the lemma and
chunk tag using the same way mentioned in the
last section. Then, a CRF classifier is built for pre-
diction, which uses the following features:
2http://www.cs.cmu.edu/ sagae/parser/gdep
14
? Word.
? Context of the lemma, POS, the chunk, the
hedge and the dependency relation in the
window [-2,2].
? Combined features including L0C0,
L0H0, L0D0, LiP0, PiC0,PiH0, CiH0,
PiD0,CiD0, where ?1 ? i ? 1 L denotes
the lemma of a word, P denotes a POS, C
denotes a chunk tag, H denotes a hedge tag
and D denotes a dependency relation tag.
? The type of a chunk; the lemma and POS se-
quences of it.
? The type of a hedge; the lemma, POS and
chunk sequences of it.
? The lemma, POS, chunk, hedge and depen-
dency relation sequences of 1st and 2nd de-
pendency relation edges; the lemma, POS,
chunk, hedge and dependency relation se-
quences of the path from a token to the root.
? Whether there are hedges in the 1st, 2nd de-
pendency relation edges or path from a token
to the root.
? The location of a token relative to the nega-
tion signal: previous the first hedge, in the
first hedge, between two hedge cues, in the
last hedge, post the last hedge.
At last, we provided a postprocessing system
for the output of the classifier to build the com-
plete sequence of tokens that constitute the scope.
We applied the following postprocessing:
? If a hedge is bracketed by a F scope and a
L scope, its scope is formed by the tokens be-
tween them.
? If a hedge is only bracketed by a F scope, and
there is no L scope in the sentence, we search
the first possible word from the end of the
sentence according to a dictionary, which ex-
tracted from the training corpus, and assign it
as L scope. The scope of the hedge is formed
by the tokens between them.
? If a hedge is only bracketed by a F scope, and
there are at least one L scope in the sentence,
we think the last L scope is the L scope of the
hedge, and its scope is formed by the tokens
between them.
? If a hedge is only bracketed by a L scope,
and there is no F scope in the sentence, we
search the first possible word from the begin-
ning of the sentence to the hedge according to
the dictionary, and assign it as F scope. The
scope of the hedge is formed by the tokens
between them.
? If a hedge is only bracketed by a L scope,
and there are at least one F scope in the sen-
tence, we search the first possible word from
the hedge to the beginning of the sentence ac-
cording to the dictionary, and think it as the
F scope of the hedge. The scope of the hedge
is formed by the tokens between them.
? If a hedge is bracketed by neither of them, we
remove it.
3 Experiments and Results
Two annotated corpus: BioScope and Wikipedia
are supplied for the CoNLL-2010 shared task. The
BioScope corpus consists of two parts: biological
paper abstracts and biological full papers, and it
is used for two subtasks. The Wikipedia corpus is
only used for hedge detection. The detailed infor-
mation of these two corpora is shown in Table 1
and Table 2, respectively.
Abstracts Papers Test
#Documents 1273 9 15
#Sentences 11871 2670 5003
%Hedge sent. 17.70 19.44 15.75
#Hedges 2694 682 1043
#AvL. of sent. 30.43 27.95 31.30
#AvL. of scopes 17.27 14.17 17.51
Table 1: The detailed information of BioScope
corpus. ?AvL.? stands for average length.
Train Test
#Documents 2186 2737
#Sentences 11111 9634
%Hedge sentences 22.36 23.19
#Hedges 3133 3143
#AvL. of sentences 23.07 20.82
Table 2: The detail information of Wikipedia cor-
pus. ?AvL.? stands for average length.
In our experiments, CRF++-0.533 implemen-
3http://crfpp.sourceforge.net/
15
tation is employed to CRF, and svm hmm 3.104
implementation is employed to the large margin
method. All parameters are default except C
(the trade-off between training error and margin,
C=8000, for selecting C, the training corpus is par-
titioned into three parts, two of them are used for
training and the left one is used as a development
dataset) in svm hmm. Both of them are state-of-
the-art toolkits for the sequence labeling problem.
3.1 Hedge Detection
We first compare the performance of each single
classifier with the cascaded system on two corpora
in domain, respectively. Each model is trained by
whole corpus, and the performance of them was
evaluated by the official tool of the CoNLL-2010
shared task. There were two kinds of measure:
one for sentence-level performance and another
one for cue-match performance. Here, we only
focused on the first one, and the results shown in
Table 3.
Corpus System Prec. Recall F1
CRF 87.12 86.46 86.79
BioScope LM 85.24 87.72 86.46
CAS 85.03 87.72 86.36
CRF 86.10 35.77 50.54
Wikipedia LM 82.28 41.36 55.05
CAS 82.28 41.36 55.05
Table 3: In-sentence performance of the hedge
detection subsystem for in-domain test. ?Prec.?
stands for precision, ?LM? stands for large mar-
gin, and ?CAS? stands for cascaded system.
From Table 3, we can see that the cascaded sys-
tem is not better than other two single classifiers
and the single CRF classifier achieves the best per-
formance with F-measure 86.79%. The reason for
selecting this cascaded system for our final sub-
mission is that the cascaded system achieved the
best performance on the two training corpus when
we partition each one into three parts: two of them
are used for training and the left one is used for
testing.
For cross-domain test, we train a cascaded clas-
sifier using BioScope+Wikipedia cropus. Table 4
shows the results.
As shown in Table 5, the performance of cross-
domain test is worse than that of in-domain test.
4http://www.cs.cornell.edu/People/tj/svm light/svm-
hmm.html
Corpus Precision Recall F1
BioScope 89.91 73.29 80.75
Wikipedia 81.56 40.20 53.85
Table 4: Results of the hedge detection for cross-
domain test. ?LM? stands for large margin, and
?CAS? stands for cascaded system.
3.2 Hedge Scope Detection
For test the affect of postprocessing for hedge
scope detection, we test our system using two eval-
uation tools: one for scope tag and the other one
for sentence-level scope (the official tool). In or-
der to evaluate our system comprehensively, four
results are used for comparison. The ?gold? is the
performance using golden hedge tags for test, the
?CRF? is the performance using the hedge tags
prediction of single CRF for test, the ?LM? is the
performance using the hedge tag prediction of sin-
gle large margin for test, and ?CAS? is the per-
formance of using the hedge tag prediction of cas-
caded subsystem for test. The results of scope tag
and scope sentence-level are listed in Table 5 and
Table 6, respectively. Here, we should notice that
the result listed here is different with that submit-
ted to the CoNLL-2010 shared task because some
errors for feature extraction in the previous system
are revised here.
HD tag Precision Recall F1
F scope 92.06 78.83 84.94
gold L scope 80.56 68.67 74.14
NONE 99.68 99.86 99.77
F scope 78.83 66.89 72.37
CRF L scope 72.52 60.50 65.97
NONE 99.56 99.75 99.65
F scope 77.25 67.57 72.09
LM L scope 72.33 61.41 66.42
NONE 99.56 99.73 99.31
F scope 77.32 67.86 72.29
CAS L scope 72.00 61.29 66.22
NONE 99.57 99.73 99.65
Table 5: Results of the hedge scope tag. ?HD?
stands for hedge detection subsystem we used,
?LM? stands for large margin, and ?CAS? stands
for cascaded system.
As shown in Table 5, the performance of
L scope is much lower than that of F scope.
Therefore, the first problem we should solve is
16
HD subsystem Precision Recall F1
gold 57.92 55.95 56.92
CRF 52.36 48.40 50.30
LM 51.06 48.89 49.95
CAS 50.96 48.98 49.95
Table 6: Results of the hedge scope in-sentence.
?HD? stands for hedge detection subsystem we
used, ?LM? stands for large margin, and ?CAS?
stands for cascaded system.
how to improve the prediction performance of
L scope. Moreover, compared the performance
shown in Table 5 and 6, about 15% (F1 of L scope
in Table 5 - F1 in Table 6) scope labels are mis-
matched. An efficient postprocessing is needed to
do F-L scope pair match.
As ?CRF? hedge detection subsystem is bet-
ter than the other two subsystems, our system
achieves the best performance with F-measure
50.30% when using the ?CRF? subsystem.
4 Conclusions
This paper presents a cascaded system for the
CoNLL-2010 shared task, which contains two
subsystems: one for detecting hedges and an-
other one for detecting their scope. Although
the best performance of hedge detection subsys-
tem achieves F-measure 86.79%, the best per-
formance of the whole system only achieves F-
measure 50.30%. How to improve it, we think
some complex features such as context free gram-
mar may be effective for detecting hedge scope.
In addition, the postprocessing can be further im-
proved.
Acknowledgments
We wish to thank the organizers of the CoNLL-
2010 shared task for preparing the datasets
and organizing the challenge shared tasks.
We also wish to thank all authors supply-
ing the toolkits used in this paper. This
research has been partially supported by the
National Natural Science Foundation of China
(No.60435020 and No.90612005), National 863
Program of China (No.2007AA01Z194) and the
Goal-oriented Lessons from the National 863 Pro-
gram of China (No.2006AA01Z197).
References
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, Gyo?rgy Szarvas. 2010. The CoNLL-2010
Shared Task: Learning to Detect Hedges and their
Scope in Natural Language Text. In Proceedings of
the Fourteenth Conference on Computational Nat-
ural Language Learning (CoNLL-2010): Shared
Task, pages 1?12, Uppsala, Sweden, July. Associ-
ation for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: Hedge detection using
wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 173?176, Suntec, Singapore,
August. Association for Computational Linguistics.
Roser Morante andWalter Daelemans. 2009. Learning
the scope of hedge cues in biomedical texts. In Pro-
ceedings of the BioNLP 2009 Workshop, pages 28?
36, Boulder, Colorado, June. Association for Com-
putational Linguistics.
Tzong-Han Tsai, Chia-Wei Wu, and Wen-Lian Hsu.
2005. Using Maximum Entropy to Extract Biomed-
ical Named Entities without Dictionaries. In Sec-
ond International Joint Conference on Natural Lan-
guage Processing, pages 268?273.
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(Suppl 11):S9.
17
