Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 47?50,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Automated Suggestions for Miscollocations 
 
Anne Li-E Liu David Wible Nai-Lung Tsao 
Research Centre for English 
and Applied Linguistics 
Graduate Institute of Learning and 
Instruction 
Graduate Institute of Learning and  
Instruction 
University of Cambridge National Central University National Central University 
Cambridge, CB3 9DP, 
United Kingdom 
Jhongli City, Taoyuan County 
32001, Taiwan 
Jhongli City, Taoyuan County 
32001, Taiwan 
lel29@cam.ac.uk wible45@yahoo.com beaktsao@gmail.com 
 
 
Abstract 
One of the most common and persistent error 
types in second language writing is colloca-
tion errors, such as learn knowledge instead of 
gain or acquire knowledge, or make damage 
rather than cause damage.  In this work-in-
progress report, we propose a probabilistic 
model for suggesting corrections to lexical 
collocation errors. The probabilistic model in-
corporates three features: word association 
strength (MI), semantic similarity (via Word-
Net) and the notion of shared collocations (or 
intercollocability).  The results suggest that 
the combination of all three features outper-
forms any single feature or any combination 
of two features.  
1 Collocation in Language Learning  
The importance and difficulty of collocations for 
second language users has been widely acknowl-
edged and various sources of the difficulty put 
forth (Granger 1998, Nesselhauf 2004, Howarth 
1998, Liu 2002, inter alia). Liu?s study of a 4-
million-word learner corpus reveals that verb-noun 
(VN) miscollocations make up the bulk of the lexi-
cal collocation errors in learners? essays. Our study 
focuses, therefore, on VN miscollocation correc-
tion. 
2 Error Detection and Correction in NLP 
Error detection and correction have been two 
major issues in NLP research in the past decade. 
Projects involving learner corpora in analyzing and 
categorizing learner errors include NICT Japanese 
Learners of English (JLE), the Chinese Learners of 
English Corpus (Gamon et al, 2008) and English 
Taiwan Learner Corpus (or TLC) (Wible et al, 
2003). Studies that focus on providing automatic 
correction, however, mainly deal with errors that 
derive from closed-class words, such as articles 
(Han et al, 2004) and prepositions (Chodorow et 
al., 2007). One goal of this work-in-progress is to 
address the less studied issue of open class lexical 
errors, specifically lexical collocation errors. 
3 The Present Study 
We focus on providing correct collocation sug-
gestions for lexical miscollocations. Three features 
are employed to identify the correct collocation 
substitute for a miscollocation: word association 
measurement, semantic similarity between the cor-
rection candidate and the misused word to be re-
placed, and intercollocability (i.e., the concept of 
shared collocates in collocation clusters proposed 
by Cowie and Howarth, 1995). NLP research on 
learner errors includes work on error detection and 
error correction. While we are working on both, 
here we report specifically on our work on lexical 
miscollocation correction.  
4 Method  
We incorporate both linguistic and computa-
tional perspectives in our approach. 84 VN miscol-
locations from Liu?s (2002) study were employed 
as the training and the testing data in that each 
comprised 42 randomly chosen miscollocations. 
Two experienced English teachers1 manually went 
through the 84 miscollocations and provided a list 
of correction suggestions. Only when the system 
output matches to any of the suggestions offered 
                                                          
1 One native speaker and one experienced non-native English teacher. 
47
by the two annotators would the data be included 
in the result. The two main knowledge resources 
that we incorporated are British National Corpus2 
and WordNet (Miller, 1990). BNC was utilized to 
measure word association strength and to extract 
shared collocates while WordNet was used in de-
termining semantic similarity. Our probabilistic 
model that combines the features is described in 
sub-section 4.4. Note that all the 84 VN miscollo-
cations are combination of incorrect verbs and fo-
cal nouns, our approach is therefore aimed to find 
the correct verb replacements.  
4.1 Word Association Measurement 
The role of word association in miscollocation 
suggestions are twofold: 1. all suggested correct 
collocations in any case have to be identified as 
collocations; thus, we assume candidate replace-
ments for the miscollocate verbs must exceed a 
threshold word association strength with the focal 
noun; 2. we examine the possibility that the higher 
the word association score the more likely it is to 
be a correct substitute for the wrong collocate. We 
adopt Mutual Information (Church et al 1991) as 
our association measurement. 
4.2 Semantic Similarity 
Both Gitsaki et al (2000) and Liu (2002) sug-
gest a semantic relation holds between a miscollo-
cate and its correct counterpart. Following this, we 
assume that in the 84 miscollocations, the miscol-
locates should stand in more or less a semantic re-
lation with the corrections. For example, say in an 
attested learner miscollocation say story is found to 
be a synonym of the correct verb tell in WordNet. 
Based on this assumption, words that show some 
degree of semantic similarity with the miscollocate 
are considered possible candidates for replacing it. 
To measure similarity we take the synsets of 
WordNet to be nodes in a graph. We quantify the 
semantic similarity of the incorrect verb in a mis-
collocation with other possible substitute verbs by 
measuring graph-theoretic distance between the 
synset containing the miscollocate verb and the 
synset containing candidate substitutes. In cases of 
polysemy, we take the closest synsets for the dis-
tance measure. If the miscollocate and the candi-
                                                          
2 The British National Corpus, version 3 (BNC XML Edition). 2007. 
URL: http://www.natcorp.ox.ac.uk/ 
date substitute occur in the same synset, then the 
distance between them is zero.   
The similarity measurement function is as fol-
lows (Tsao et al, 2003): 
)
),max(2
),(
1(max),(
)(),(21 21
ji
ji ss
ji
wsynsetswsynsets LL
ssdis
wwsim ??= ??
 
,where  means the node path length be-
tween the synset  and  in WordNet hy-
per/hypo tree.   means the level number of s in 
hyper/hypo tree and the level of top node is 1. 
Multiplying  by 2 ensures the simi-
larity is less than 1. If   and  are synonymous, 
the similarity will be 1. 
),( ji ssdis
is js
sL
),max(
ji ss
LL
is js
4.3 Shared Collocates in Collocation Clusters 
Futagi et al(2008) review several studies which 
adopt computational approaches in tackling collo-
cation errors; yet none of them, including Futagi et 
al., include the notion of collocation cluster. We 
borrow the cluster idea from Cowie & Howarth 
(1995) who propose ?overlapping cluster? to denote 
sets of collocations that carry similar meaning and 
shared collocates. Figure 1 represents a collocation 
cluster that expresses the concept of ?bringing 
something into actuality.? The key here is that not 
all VN combinations in Figure 1 are acceptable. 
While fulfill and achieve collocate with the four 
nouns on the right, realize does not collocate with 
purpose, as indicated by the dotted line. Cowie and 
Howarth?s point is that collocations that can be 
clustered via overlapping collocates can be the 
source of collocation errors for language learners. 
That both fulfill and reach collocate with goal and 
the further collocability of fulfill with ambition and 
purpose plausibly lead learners to assume that 
reach shares this collocability as well, leading by 
overgeneralization to the miscollocations reach an 
ambition or reach a purpose.  
 
Figure 1. Collocation cluster of ?bringing something 
into actuality? 
48
We employ the ideas of ?collocation cluster? and 
?shared collocates? in identifying correct counter-
parts to the miscollocations. Specifically, taking 
the miscollocation reach their purpose as a starting 
point, our system generates a collocation cluster by 
finding the verbs that collocate with purpose and 
nouns that reach collocates with. We consider this 
formed cluster the source that contains the possible 
correct replacement for reach in reach their pur-
pose. By finding verbs that not only collocate with 
purpose but also share the most other collocating 
nouns with the wrong verb reach, successfully, we 
identified candidate substitutes fulfill and achieve 
for the incorrect verb reach. 
4.4 Our Probabilistic Model 
The three features we described above are inte-
grated into a probabilistic model. Each feature is 
used to look up the correct collocation suggestion 
for a miscollocation. For instance, cause damage, 
one of the possible suggestions for the miscolloca-
tion make damage, is found to be ranked the 5th 
correction candidate by using word association 
measurement merely, the 2nd by semantic similarity 
and the 14th by using shared collocates. If we com-
bine the three features, however, cause damage is 
ranked first.  
The conditional probability of the case where 
the candidate is a correct one can be presented as: 
)( ,mcFverbcorrectaiscP   
where c means a candidate for a specific miscollo-
cation and Fc, m means the features values between 
m (misused words) and c (candidates). According 
to Bayes theorem and Bayes assumption, which 
assume that these features are independent, the 
probability can be computed by: 
( ) ( ) ( )( )
( ) ( )
( )?
?
?
??=
mc
mc
Ff
c
Ff
c
mc
ccmc
mcc fP
SPSfP
FP
SPSFP
FSP
,
,
,
,
,
 
where  means the situation ?c is a correct verb?, 
as described above and f is one of the three particu-
lar features. We use probability values to choose 
and rank the K-best suggestions. 
cS
5 Experimental Results  
Any found VN combination via our probabilistic 
approach was compared to the suggestions made 
by the two human experts. A match would be 
counted as a true positive. A discrete probability 
distribution is produced for each feature. We di-
vided feature value into five levels and obtained 
prior predicting value for each level of the three 
features. For example, we divided MI value to five 
levels (<1.5, 1.5~3.0, 3.0~4.5, 4.5~6, >6).  The five 
ranks for semantic similarity and normalized 
shared collocates number are 0.0~0.2, 0.2~0.4, 
0.4~0.6, 0.6~0.8 and 0.8 ~1.0. For every feature, 
we obtain a predicting value for each level after the 
training process. The predicting value is shown 
as ( )( )fP SfP c . In line with that, P(MI>6)  means the 
probability of all VN collocations retrieved from 
BNC in which the MI value is higher than 6 
whereas P(MI>6| ) shows the probability of  all 
correct VN collocations with the MI value higher 
than 6.  
cS
Different combinations of the three features are 
made on the basis of the probabilistic model de-
scribed in Section 4.4. Seven models derive from 
such combinations (See Table 1). Table 2 shows 
the precision of k-best suggestions for each model.  
 
Models Feature(s) considered 
M 1 MI (Mutual Information) 
M 2 SS (Semantic Similarity) 
M 3 SC (Shared Collocates) 
M 4 MI + SS 
M 5 MI + SC 
M 6 SS + SC 
M 7 MI + SS + SC 
Table 1.  Models of feature combinations.  
 
K-Best M1 M2 M3 M4 M5 M6 M7
1 16.67 40.48 22.62 48.81 29.76 55.95 53.57
2 36.90 53.57 38.10 60.71 44.05 63.1 67.86
3 47.62 64.29 50.00 71.43 59.52 77.38 78.57
4 52.38 67.86 63.10 77.38 72.62 80.95 82.14
5 64.29 75.00 72.62 83.33 78.57 83.33 85.71
6 65.48 77.38 75.00 85.71 83.33 84.52 88.10
7 67.86 80.95 77.38 86.90 86.90 86.9 89.29
8 70.24 83.33 82.14 86.90 89.29 88.1 91.67
9 72.62 86.90 85.71 88.10 92.86 90.48 92.86
10 76.19 86.90 88.10 88.10 94.05 90.48 94.05
Table 2. The precision rate of Model 1- 7. 
 
K-Best M2 M6 M7 
1 aim *obtain *acquire 
2 generate share share 
3 draw *develop *obtain 
4 *obtain generate *develop 
5 *develop *acquire *gain 
Table 3. The K-Best suggestions for get 
knowledge. 
49
 
Table 2 shows that, considering the results for 
each feature run separately (M1-M3), the feature 
?semantic similarity? (M2) outperforms the other 
two. Among combined feature models (M4-M7), 
M7 (MI + SS+ SC), provides the highest propor-
tion of true positives at every value of k except k = 
1. The full hybrid of all three features (M7) outper-
forms any single feature. The best results are 
achieved when taking into account both statistical 
and semantic features. This is illustrated with re-
sults for the example get knowledge in Table 3 (the 
asterisks (*) indicate the true positives.) 
6 Conclusion 
In this report of work in progress, we present a 
probabilistic model that adopts word association 
measurement, semantic similarity and shared col-
locates in looking for corrections for learners? mis-
collocations. Although only VN miscollocations 
are examined, the model is designed to be applica-
ble to other types of miscollocations. Applying 
such mechanisms to other types of miscollocations 
as well as detecting miscollocations will be the 
next steps of this research. Further, a larger amount 
of miscollocations should be included in order to 
verify our approach and to address the issue of the 
small drop of the full-hybrid M7 at k=1.  
Acknowledgments 
The work reported in this paper was partially 
supported by the grants from the National Science 
Council, Taiwan (Project Nos. 96-2524-S-008-
003- and 98-2511-S-008-002-MY2) 
References 
Anne. Li-E Liu 2002. A Corpus-based Lexical Semantic 
Investigation of VN Miscollocations in Taiwan 
Learners? English. Master Thesis, Tamkang Univer-
sity, Taiwan. 
Anthony P Cowie and Peter Howarth. 1995. Phrase-
ological Competence and Written Proficiency, Paper 
Presented at the British Association of Applied Lin-
guistics Conference (BAAL), Southampton, England, 
September. 
Christina Gitsaki, Nagoya Shoka Daigaku, and Richard 
P. Taylor. 2000. English Collocations and Their 
Place in the EFL Classroom, Available at: 
http://www.hum.nagoya-cu.ac.jp/ 
~taylor/publications/collocations.html. 
Claudia Leacock and Martin Chodorow. 2003. Auto-
mated Grammatical Error Detection, In MD Shermis 
& JC Burstein (Eds.), Automated Essay Scoring: A 
Cross-disciplinary, Mahwah, NJ: Lawrence Erlbaum 
Associates. 
David Wible, Chin-Hwa Kuo, Nai-Lung Tsao, Anne Li-
E Liu, and Hsiu-Lin Lin. 2003. Bootstrapping in a 
Language Learning Environment. Journal of Com-
puter Assisted Learning, 19, 90-102. 
George Miller. 1990. WordNet: An On-line Lexical 
Database, International Journal of Lexicography. 
Kenji Kita and Hiroaki Ogata. 1997. Collocations in 
Language Learning: Corpus-based Automatic compi-
lation of Collocations and Bilingual Collocation 
Concordancer, Computer Assisted Language Learn-
ing. Vol.10, No. 3, 229-238. 
Kenneth Church, William Gale, Patrick Hanks and 
Donald Hindle. 1991. Using Statistics in Lexical 
Analysis, in Zernik (ed), Lexical Acquisition: Using 
On-line Resources to Build a Lexicon, Lawrence Erl-
baum, pp. 115-164. 
Martin Chodorow, Joel R. Tetreault and Na-Rae Han. 
2007. Detection of Grammatical Errors Involving 
Prepositions, Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics, 
Special Interest Group on Semantics, Workshop on 
Prepositions, 25-30. 
Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-
dre Klementiev, William B. Dolan, Dmitriy Belenko, 
Lucy Vanderwende. 2008. Using Contextual Speller 
Techniques and Language Modeling for ESL Error 
Correction, Proceedings of The Third International 
Joint Conference on Natural Language Processing, 
Hyderabad, India. 
Na-Rae Han, Martin Chodorow and Claudia Leacock. 
2004. Detecting Errors in English Article Usage with 
a Maximum Entropy Classifier Trained on a Large, 
Diverse Corpus, Proceedings of the 4th International 
Conference on Language Resources and Evaluation, 
Lisbon, Portugal. 
Nai-Lung Tsao, David Wible and Chin-Hwa Kuo. 2003. 
Feature Expansion for Word Sense Disambiguation, 
Proceedings of the International Conference on 
Natural Language Processing and Knowledge Engi-
neering, 126-131. 
Peter Howarth. 1998. Phraseology and Second Lan-
guage Acquisition. Applied Linguistics. 19/1, 24-44.  
Yoko Futagi, Paul Deane, Martin Chodorow & Joel 
Tetreault. 2008. A computational approach to detecting 
collocation errors in the writing of non-native speakers of 
English. Computer Assisted Language Learn-
ing,21:4,353 ? 367 
 
50
Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 51?54,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Method for Unsupervised Broad-Coverage  
Lexical Error Detection and Correction 
 
Nai-Lung Tsao David Wible 
Graduate Institute of Learning and Instruction Graduate Institute of Learning and Instruction 
National Central University National Central University 
Jhongli City, Taoyuan County 32001, Taiwan Jhongli City, Taoyuan County 32001, Taiwan 
beaktsao@gmail.com Wible45@yahoo.com 
 
 
Abstract 
We describe and motivate an unsupervised 
lexical error detection and correction algo-
rithm and its application in a tool called Lex-
bar appearing as a query box on the Web 
browser toolbar or as a search engine inter-
face. Lexbar accepts as user input candidate 
strings of English to be checked for accept-
ability and, where errors are detected, offers 
corrections. We introduce the notion of hy-
brid n-gram and extract these from BNC as 
the knowledgebase against which to compare 
user input. An extended notion of edit dis-
tance is used to identify most likely candi-
dates for correcting detected errors. Results 
are illustrated with four types of errors. 
1 Introduction 
We describe and motivate an unsupervised lexical 
error detection and correction algorithm and its 
application in a tool called Lexbar appearing as a 
query box in a web-based corpus search engine or 
on the Web browser toolbar. The tool is intended 
as a proxy for search engines in the common prac-
tice where users put search engines to use as error 
checkers. A problem with this use of search en-
gines like Google is that such searches commonly 
provide false positives, hits for strings that contain 
errors. Lexbar accepts as user input candidate 
strings of English to be checked for acceptability 
and, where errors are detected, offers corrections.  
2 Related Work 
Among the many works on error detection, re-
cently unsupervised error detection approaches 
have been proposed, such as [Chodorow and Lea-
cock, 2000] and [Quixal and Badia 2008]. These 
use contextual features and statistical word asso-
ciation measurement to decide if the detected bi-
gram or trigram is an error or not. To our 
knowledge, such unsupervised methods have not 
been applied in error correction. [Gamon et  al 
2008] and [Felice and Pulman 2008] propose un-
supervised approaches to build a probabilistic 
model for detecting errors (prepositions and arti-
cles) and providing correct answers. They also 
typically focus on a particular type of error, usu-
ally limited to a specific word class such as prepo-
sition errors, often in a pre-determined 
paradigmatic slot. Our approach reported here is 
unsupervised in both detection and correction and 
is not tailored to a specific target error subtype or 
targeted to a specific position in a string. More 
generally the family of error types suitable for this 
approach are lexical or lexico-grammatical errors 
since detection and correction are based on pat-
terns of word use detected statistically. At the core 
of our approach is a bank of what we call ?hybrid 
n-grams? extracted from BNC to serve as the tar-
get knowledge against which learner input is 
compared for detection and correction. We illus-
trate the single algorithm with results on four dif-
ferent categories of errors. 
3 Overview of the Algorithm 
The Lexbar application consists of two main 
components: (1) the target language knowledge-
base of hybrid n-grams that serves as the standard 
against which learner production is examined for 
errors, and (2) the error detection and correction 
algorithm that uses this knowledgebase to evalu-
51
ate learner production through matching and edit 
distance. Relatively broad coverage is achieved 
from one algorithm since no specific error type is 
targeted but violations of word behaviors patterns. 
Typically, n-grams are contiguous sequences of 
lemmas or specific word forms. Using traditional 
n-grams and string matching against them as a 
means of error detection leads to weak precision 
since the absence of a specific n-gram in a stan-
dard corpus does not render it an error. To address 
this limitation, we extend the notion of n-gram to 
include in the string not only lemmas or word 
forms but parts-of-speech as well. For example, 
the chunk point of view can be part of a longer 
string from my point of view. Here, the preposition 
from is non-substitutable whereas the possessive 
pronoun my can be replaced by others of the same 
POS (his/her/your/etc.). Hence, replacing the one 
results in an error (*in my point of view1) while 
replacing the other is fine (from her/his/their/our 
point of view). The purpose of hybrid n-grams is 
to introduce the flexibility to capture the appropri-
ate level of abstraction for each slot in a lexical 
chunk. Hybrid n-grams permit any combination of 
word forms, lemmas, POSs in a string (see details 
below). Thus the hybrid n-gram for from my point 
of view is from [dps] point of view2.  
For a string of input submitted for error check-
ing, the algorithm first does a matching operation 
between the input string and the hybrid n-gram 
bank. The second step for input is finding hybrid 
n-grams which nearly match the input, using edit 
distance to measure nearness or similarity. Hybrid 
n-grams with a distance of 1 or less from the input 
string are candidates as correction suggestions and 
are ranked, least distant from the input string 
ranked as top correction suggestion. 
4 The Knowledgebase: Hybrid N-grams 
As mentioned in Section 3, a hybrid n-gram bank 
will be needed. In our model, each slot has four 
levels of representation to choose from: word 
form (enjoys but not enjoy or enjoying, etc); 
lemma (representing all word forms of that lex-
eme, e.g., enjoy, enjoys, and enjoyed, etc); de-
tailed POS (CLAWS5 with 46 different POSs); 
                                                 
1
 We use * to represent the error part in n-gram string. 
2
 We use [] to represent POS categories. [dps] is the 
CLAWS5 tag for possessive pronoun.  
rough POS (9 different POSs)3. The main chal-
lenge is to extract hybrid n-grams which are the 
optimum combination of representations for each 
slot to represent a lexical chunk or pattern. One 
key to this is a pruning method (described below). 
Clearly, compared with traditional n-gram extrac-
tion, the size of our hybrid n-gram bank size will 
be extremely large if we save all the combinations 
that can be generated for each n-gram. Consider-
ing the example from my point of view and setting 
point as the target word, if we only extract hybrid 
5-gram strings for it, we will get 2*44=512 (two 
forms of noun point and four forms of others) dif-
ferent hybrid 5-grams. This entails many disad-
vantages, for example in storage space and 
processing time. Therefore, we apply several 
pruning approaches to keep only useful hybrid n-
grams in the bank. Another motivation for pruning 
the bank is to reach optimum recall and precision. 
The choice of which hybrid n-grams to retain in or 
discard from the bank directly determines which 
input strings would be judged as errors and what 
candidate corrections would be generated for er-
rors. We illustrate the effects of pruning below.  
The first criterion for pruning is frequency. 
Only hybrid n-grams with a frequency greater 
than the threshold are saved. The second criterion 
is called subset pruning. There will be overlap 
among different hybrid n-grams. For example, the 
chunk from my point of view could be represented 
by dozens of hybrid n-grams. Two of them are: (1) 
from [dps] point of view, and (2) from my point of 
view. Notice an input string from her point of view 
would match (1) but not (2). Here the optimum n-
gram is (1) because it includes all cases covered 
by (2) but other acceptable ones as well. Crucially, 
it is not the case that the more general hybrid n-
gram will always yield the more optimum results, 
however. This must be determined case by case. 
Consider the first slot in the same chunk from my 
point of view. The following two versions could 
represent that chunk: (3) from [dps] point of view 
and (4) [prp] [dps] point of view4. Notice here, 
however, that it will be the more specific rather 
than the more inclusive version that is to be pre-
ferred. (3) specifies the exact preposition for the 
chunk whereas (4) would accept any preposition 
                                                 
3
 Rough POS includes verb, noun, adj, adv, conj, interj, prep, 
pron, vm0. 
4
 [prp] is the CLAWS5 tag for preposition. 
52
(or [prp]) occurring in the first slot. But indeed 
from is not freely substitutable in this chunk (cf 
*in my point of view). Thus in each slot in each 
chunk, pruning checks each potential hybrid n-
gram against the target corpus to determine statis-
tically the n-grams that capture the optimum de-
gree of substitutability or frozenness for each slot.  
This creates an extremely flexible means of 
representing the knowledgebase. Consider verb 
complement selection. In examples such as They 
enjoy swimming, the level of generalization is dif-
ferent for the governing verb slot (enjoy) on the 
one hand and the complement (swimming) on the 
other. The right generalization for the complement 
is a specific verb form but not specific to any one 
verb. This slot is captured under the CLAWS5 
POS [vvg] 5 , thus permitting enjoy swim-
ming/reading/sleeping, but not enjoy to 
swim/swam and so on. Unlike the complement, 
the governing verb slot here is a specific lexeme 
(enjoy swimming but not hope swimming; cf hope 
to swim) and moreover, it permits that lexeme in 
any of its word forms (enjoy/enjoying/enjoyed 
swimming). A hybrid n-gram representation has 
the power to capture these different levels of gen-
eralization and restriction in one representation. 
Here is how pruning is done. First, we set a fil-
ter factor ?, where 0<?<1. Assume x and y are 
two hybrid n-grams and len(x)=len(y). If x ? y and 
yx  ? ?6, we will eliminate y from bank. For 
example, for the two 5-grams x=from [dps] point 
of view and y=[prp] [dps] point of view, obviously 
x ? y because from is a kind of [prp] (preposition). 
If we set the filter factor ?=80% and yx >?, y 
will be not included in the hybrid n-gram bank. 
For example from 100M-word BNC, before prun-
ing, there are 110K hybrid n-grams containing 
target lemma point. After pruning, there are only 
5K useful hybrid n-grams left.   
5 The Edit Distance Algorithm for Error 
Detection and Correction 
5.1 Error Detection 
We apply a simple edit distance for error detection 
by comparing user input n-grams and standard 
                                                 
5
 [vvg] is the CLAWS5 tag for gerund. 
6
 x  means the frequency of x in BNC. 
hybrid n-gram in the bank. The approaches are 
briefly summarized and short examples given in 
the following: 
Step 1: POS tag the user input string and get al 
hybrid n-grams that can represent that string. For 
example, a user inputs in my point of view and 
then [prp] my point of view, [prp] [dps] point of 
view, in [dps] point of view, in my point of 
[nn1]? etc. will be generated. Let C denote the 
entire set of hybrid n-grams generated from an 
instance of user input. 
Step 2: Search all hybrid n-grams in the target 
knowledgebase containing point or view, which 
are the content words in user input. Let S denote 
all of the target hybrid n-grams containing point 
or view.   
Step 3: Compute the edit distance d between 
every element in C and S. If ? d=0 in (C, S), we 
assume the user input n-gram is correct. If ? d>1 
in (C, S), our system will ignore this case and pro-
vide nothing. If ? d=1, we assume the user input 
might be wrong and the system will enter the error 
correction procedure. 
For efficiency?s sake in Step 2, the hybrid n-
grams are indexed by content words. We use 
Levenshtein?s edit distance algorithm [Leven-
shtein 1996] in Step 3. It indicates the difference 
between user input and standard n-grams in three 
ways: ?substitute relation,? i.e., two n-grams are 
the same length and identical except for one slot. 
?Delete relation? and ?insert relation? hold be-
tween two different length n-grams. In this paper 
we consider only the ?substitute relation,? such as 
in my point of view and from my point of view. 
This limits edit distance computing to pairs of n-
grams of the same length (e.g. 5-gram to 5-gram).  
5.2 Error Correction 
The system identifies correction candidates from S 
as those with edit distance d=1 from some mem-
ber(s) in C. Once the system gets several correc-
tion candidates for an input string whose edit 
distances from user input are 1, we have to decide 
the ranking of the correct candidates by a value 
called weighted edit distance. Weighted edit dis-
tance can identify more appropriate correct n-
grams for the user. Imagine a case where an n-
gram from C and an n-gram from S show a substi-
tution relation. Assume u is the differing element 
in the C n-gram and v is its counterpart in the S n-
53
gram. Weighted edit distance between these two is 
computed by the following rules: 
Rule 1: If u and v are both word-forms and are 
different word-forms of the same lemma (for ex-
ample enjoyed and enjoying), given distance ?. 
Rule 2: If u and v are both members of 
CLAWS5 POS and their rough POS are the same, 
given distance ?7. 
Rule 3: If u and v are both function words, give 
distance ?. 
Rule 4: If u and v are both content word, give 
distance ?. 
We set ?<? and ?<?. Correct candidate with 
lower weighted distance makes itself more appro-
priate for suggestion. For example, before weight-
ing, the error string pay attention on gets two 
distance 1 correct candidates pay attention to and 
focus attention on. Weighting will give pay atten-
tion to a lower weighted distance because on and 
to are function words whereas focus and pay are 
content words.  
6 Experimental Result 
Four types of errors shown in Table 1 are exam-
ined for our detection and correction algorithm.   
Error string Algorithm result Correction sug-
gested to user 
Preposition 
have a look *of have a look at have a look at 
I am interested 
*of 
[pnp] be interested in I am interested in 
*in my point of 
view 
from [dps] point of view from my point of 
view 
pay attention 
*on 
pay attention to 
pay attention to 
pay attention to 
We can discuss 
*about. 
we [vm0] discuss it 
we [vm0] discuss 
[noun] 
we [vm0] discuss [av0] 
we can discuss it 
we can discuss 
[noun] 
we can discuss 
{adv} 
Adjectival participles 
He is 
*confusing with 
[pnp] be confused [prp] He is confused 
with 
I am 
*interesting in 
[pnp] be interested in I am interested in 
I am *exciting 
about 
[pnp] be excited [prp] I am excited about 
Verb form 
He wants 
*reading. 
he wants [vvt] 
he want [vvt] 
He wants to read 
 I enjoy *to 
read. 
i enjoy [vvg] 
i enjoy [vvg] 
I enjoy reading 
                                                 
7
 Recall we use two levels of POS tagging in our hybrid n-
grams: 1. The detailed one is CLAWS5 with 46 tags. 2. The 
rough or simple tag set of 9 tags. 
let them *to 
stay. 
let them [vvi] 
let them [vvi] 
let them stay 
make him *to 
leave 
make him [vvi] 
make him [vvi] 
make him leave 
must let them 
*to stay 
[vm0] let them [vvi] must let them stay 
spend time to 
understand 
spend time [vvg] spend time under-
standing 
will make him 
*to leave 
will make [pnp] [vvi] will make him 
leave 
Missing be 
I* afraid of be afraid of 
[adv] afraid of 
[av0] afraid of 
be afraid of 
[adv]afraid of 
[adj] afraid of 
They* aware of be aware of 
[av0] aware of 
be aware of 
[adv]aware of 
Table 1: Four error types and their examples with cor-
rect suggestions.  
7 Conclusion 
We propose an algorithm for unsupervised lexical 
error detection and correction and apply it to a 
user tool called Lexbar. This is a work-in-progress 
report, and we have not yet run full testing with a 
large data set, such as a learner corpus. However 
the early stage experimental results show promise, 
especially its broad coverage over different error 
types compared to error-specific approaches.  
Acknowledgments 
The work described in this paper was partially 
supported by the grants from the National Science 
Council, Taiwan (Project Nos. 96-2524-S-008-
003- and 98-2511-S-008-002-MY2) 
Reference 
Martin Chodorow and Claudia Leacock 2000. An un-
supervised method for detecting grammatical errors. 
Proceedings of the 1st conference of NAACL, pages 
140?147. 
Rachele De Felice and Stephen G. Pulman 2008. 
Automatic detection of preposition errors in learner 
writing. CALICO AALL Workshop. 
M. Gamon, J. Gao, C. Brockett, A. Klementiev, W. B. 
Dolan, D. Belenko, and L. Vanderwende 2008. Us-
ing Contextual Speller Techniques and Language 
Modeling for ESL Error Correction. Proceedings of 
IJCNLP. 
V. I. Levenshtein 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Soviet 
Physics Doklady, 10:707?710. 
Mart? Quixal and Toni Badia 2008. Exploiting unsu-
pervised techniques to predict EFL learner errors. 
CALICO AALL Workshop. 
54
Proceedings of the NAACL HLT Workshop on Extracting and Using Constructions in Computational Linguistics, pages 25?31,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
StringNet as a Computational Resource for Discovering 
and Investigating Linguistic Constructions 
David Wible Nai-Lung Tsao 
National Central University 
No.300, Jhongda Rd. 
Jhongli City, Taoyuan County 32001, Taiwan 
wible@stringnet.org beaktsao@stringnet.org 
 
Abstract 
We describe and motivate the design of a 
lexico-grammatical knowledgebase called 
StringNet and illustrate its significance for re-
search into constructional phenomena in Eng-
lish. StringNet consists of a massive archive 
of what we call hybrid n-grams. Unlike tradi-
tional n-grams, hybrid n-grams can consist of 
any co-occurring combination of POS tags, 
lexemes, and specific word forms. Further, we 
detect and represent superordinate and subor-
dinate relations among hybrid n-grams by 
cross-indexing, allowing the navigation of 
StringNet through these hierarchies, from spe-
cific fixed expressions (?It?s the thought that 
counts?) up to their hosting proto-
constructions (e.g. the It Cleft construction: 
?it?s the [noun] that [verb]?). StringNet sup-
ports discovery of grammatical dependencies 
(e.g., subject-verb agreement) in non-
canonical configurations as well as lexical de-
pendencies (e.g., adjective/noun collocations 
specific to families of constructions). 
1 Introduction 
Constructions have posed persistent challenges to 
the field of computational linguistics (Baldwin et 
al 2004; Sag et al2002; Zhang et al2006). Chal-
lenges to both statistical and symbolic approaches 
arise, for example, from the meager degree of pro-
ductivity and non-canonical structures of many 
constructions and, as a loosely defined family of 
linguistic phenomena, their varied mix of regular-
ity and idiomicity (Fillmore, Kay, and O?Connor 
1988). It has been argued for decades that con-
structions are central rather than peripheral to any 
adequate account of linguistic knowledge and that 
they pose substantial challenges to mainstream 
accounts of language (Bolinger, 1977, 1985; Fill-
more, Kay, and O?Connor, 1988; Goldberg, 1995; 
inter alia). But the recent attention they have been 
receiving in computational research is perhaps due 
more to their status as troublemakers (or a ?pain in 
the neck?, Sag et al2002). Baldwin et al(2004) 
found, for example, that 39% of parse failures on 
clean data (BNC) occurred on constructions. (See 
Zhang et al(2006) for other such findings.) Thus, 
it is becoming urgent to ?deal with? constructions 
for the sake of NLP. In this paper, however, we 
would like to shift perspective a bit to explore in-
stead the application of computational resources 
for the sake of constructions. Our longer term aim 
is to broaden and deepen research on constructions 
in order to support the learning and teaching of 
constructions in second language education. Two 
basic challenges we address are: (1) the varied mix 
of regularity and idiomicity to be found within the 
wide range of constructions in a language (Fill-
more, Kay, and O?Connor, 1988; Jackendoff, 2008 
inter alia), and (2) the inheritance-like hierarchical 
relations holding between and among different 
constructions as instances of more general con-
structions or proto-constructions subsuming other 
constructions as sub-cases (Goldberg 1995 inter 
alia). To address these, we introduce a lexico-
grammatical knowledgebase called StringNet and 
describe some ways that it can support the investi-
gation of constructions. 
Within the broad range of definitions for con-
structions, one widely shared premise is that the 
traditional division between lexical knowledge on 
the one hand and grammatical rules on the other is 
an artificial one. There are huge tracts of linguistic 
territory lying between the lexical and the gram-
matical which usage-attuned linguists have seen as 
not simply a residue of undiscovered deeper gen-
eral principles but as the actual lay of the linguistic 
land (Bolinger 1977). We have taken this lexico-
grammatical territory as a core target of the work 
we report here. StringNet has been designed to 
25
provide traction on some of this intermediate ter-
rain.  
The paper is organized as follows. Section 2 de-
scribes and motivates the basic approach we have 
taken in designing StringNet. Section 3 describes 
the design of StringNet itself. In Section 4, we il-
lustrate the significance of StringNet for construc-
tion research with some extended examples. 
Section 5 is the conclusion. 
2 Background and Approach  
The specific approach we take to designing 
StringNet is motivated by the varied mixture of 
idiomicity and regularity exhibited by construc-
tions mentioned above and the problems this poses 
both for symbolic and statistical approaches in 
computational linguistics. To frame the properties 
of constructions that we hope StringNet can help 
address, we make use of Fillmore, Kay, and 
O?Connor?s distinction between substantive and 
formal idioms (1988), the latter of which they 
categorize eventually under ?grammatical con-
structions? (p. 506). Substantive (or ?lexically 
filled?) idioms are those fixed at the lexical level, 
that is, lexical strings relatively frozen except per-
haps for inflectional variation. Among examples 
they site are pull a fast one, all of a sudden, kick 
the bucket. Others, extracted by StringNet, would 
include as a matter of fact, at a moment?s notice, 
just to be on the safe side, and a massive inventory 
of other fixed strings. In contrast to substantive 
idioms, formal (or ?lexically open?) idioms ??are 
syntactic patterns dedicated to semantic and prag-
matic purposes not knowable from their form 
alone? (p. 505) These would include such expres-
sions detected with StringNet as ?bring [pnp1] to 
[dps] senses,? ?stop [pnp] in [dps] tracks,? ?It is 
safe to [verb] that? (e.g., It is safe to as-
sume/say/predict that),? ?There is a tendency for 
[noun] to [verb],? ?[verb][dps] socks off? (e.g., 
knock your socks off). As mentioned above, on 
Fillmore et als analysis, it is the latter type, the 
formal idioms, which are eventually ?absorbed into 
the category of grammatical constructions? (p. 
506). Crucially for us, however, they point out the 
potential significance of substantive (lexically 
                                                           
1 The glosses for the POS tags appearing in the paper, taken 
from CLAWS 5 tagset is are follows: pnp = pronoun, dps = 
possessive determiner, nn1 = singular noun, nn2 = plural noun, 
vvz = present 3rd person singular verb; vm0 = modal verb. 
filled) idioms for construction research. A substan-
tive or frozen idiom may be a sub-case of a formal 
or lexically open idiom. Our example of this is the 
lexically filled idiom ?It?s the thought that counts? 
with its idiosyncratic interpretation that must be 
learned as a listeme; it presupposes something sub-
standard about a gift or an effort as well as for-
giveness of this in light of the good intentions of 
the giver. Yet much of its meaning derives from its 
status as an instance of the more general ?It cleft? 
construction; the focus slot hosts one member of a 
contrasting pair or set, and that member is assumed 
to be new information, etc.).  
Considering the challenges of extracting and 
representing these two sorts of expressions, sub-
stantive idioms have been the far more tractable of 
the two. Specifically, substantive, lexically filled 
idioms are readily susceptible to detection and rep-
resentation by traditional n-grams. It is formal 
(lexically open) idioms, however, which have been 
identified more closely with constructions, yet they 
have proven much more resistant to extraction by 
computational means; for example, approaches 
using n-grams have so far shown little progress in 
handling this category of expression. And parsers 
famously have difficulties with their non-canonical 
structures (Baldwin et al2004; Zhang et al2006; 
inter alia).  
The design of StringNet is aimed at addressing 
three long-recognized qualities of constructions: 
(1) the non-canonical structures of many of them; 
(2) their syntagmatic mixing of fixed and substitut-
able slots, making them resistant to representation 
by traditional n-grams; and (3) the hierarchical re-
lations holding among them, as, for example, ?it?s 
the thought that counts? instantiates the general It 
Cleft construction while each arguably warrants 
independent status as a construction. 
3 Design and Construction of StringNet  
3.1 Overview 
In this section we describe the design of StringNet. 
In light of the well-documented problems that con-
structions pose for parsers, we eschew parsing at 
this stage to see what we can achieve without it 
first.2 StringNet is a corpus-derived knowledge-
                                                           
2 StringNet will provide some natural spaces where shallow 
parsing could play a well-motivated role, but we leave that for 
future work. 
26
base, automatically extracted from the British Na-
tional Corpus (BNC). The structure of StringNet 
can be described in two parts: (1) a special type of 
n-grams that we refer to as hybrid n-grams, consti-
tuting the core content of StringNet and (2) the 
inter-relations among these hybrid n-grams, repre-
sented by cross-indexing. We describe and moti-
vate these two aspects in turn.    
3.2 Hybrid n-grams 
Unlike traditional n-grams, hybrid n-grams can 
consist of co-occurring grams from different levels 
of analysis, more specifically, a combination of 
lexemes, word forms, and parts of speech (POSs) 
potentially side by side within the same string. For 
example, ?from my point of view? is a traditional 
n-gram attested in BNC, where the grams are all 
lexical. However, our hybrid n-gram extraction, in 
addition, detects the substitutability of the second 
slot in this string and indicates this substitutability 
by a POS in that position: ?from [dps] point of 
view?. By including POS categories, hybrid n-
grams can encode the paradigmatic dimension in 
addition to the syntagmatic one represented by tra-
ditional n-grams. 
The hybrid n-grams that constitute StringNet?s 
content are derived from BNC. Specifically, we 
include any contiguous combination of gram types 
ranging from bi-grams to 8-grams. Two criteria 
must be met for each hybrid n-gram. (1) It must 
include at least one lexical gram in the string (that 
is, either a lexeme or a specific word form). This 
means that all of the hybrid n-grams are ?lexically 
anchored? to some extent. And (2) it must be at-
tested in BNC at a minimum frequency of five to-
kens.  
There are four categories of grams that can oc-
cur in the hybrid n-grams of StringNet. From spe-
cific to general, these categories are: (1) word form 
(thus, ran, run, and running are three distinct word 
forms); (2) lexeme (run, including all its different 
inflectional forms: run, ran, running); these are 
indicated in bold to distinguish them from word 
forms; (3) detailed POS category, taken from the 
large CLAWS set of 46 tags ([nn1] for singular 
noun); these are marked off in brackets; (4) rough 
POS category, taken from abbreviated tagset of 12 
POS tags ([noun], including plural and singular 
nouns); indicated with brackets as well to avoid 
flooding users with too many distinctions in the 
representations. Further, each hybrid n-gram is 
indexed to all tokens instantiating it in BNC. Thus, 
every token of ?saw the light? occurring in BNC is 
indexed to all hybrid n-grams that it satisfies, for 
example, indexed to ?[verb] the light?, ?see [det] 
light?, ?[verb] [det] light?, ?saw the [noun]?, and 
so on. As mentioned above, only hybrid n-grams 
attested by at least five tokens occurring in BNC 
are kept in StringNet. 
3.3 Structure of StringNet: Cross-indexing of 
Hybrid n-grams  
Since the inventory of gram types consists of four 
categories and these can stand in subordinate and 
superordinate relation to each other, it becomes 
possible to find relations of inclusion or subsump-
tion between hybrid n-grams. For the sake of sim-
plicity in the user interface, we label these as 
parent/child relations.  
Take the tri-gram ?paying attention to? as an ex-
ample. As a string of word forms, this hybrid n-
gram can be considered a child of the hybrid n-
grams: pay attention to (where pay indicates the 
lexeme and includes forms pay, paid, paying). 
Non-monotonically, then, ?paying attention to? can 
(and does) have more than one parent, for exam-
ple: pay [noun] to; pay attention [prep]; among 
several others. StringNet exhaustively cross-
indexes all of these thus-related hybrid n-grams. 
(Note that hybrid n-grams can have more compli-
cated relations with each other, but these are not 
indexed in the current StringNet.) As a massive 
inventory of hybrid n-grams and the cross-indexing 
among them, StringNet is very large. For compari-
son, the size of our POS-tagged BNC is 4.4 GB. 
StringNet, which we extracted from BNC, is over a 
terabyte (over 1,000 GB), about 250 times the size 
of BNC. 
The hybrid n-grams making up StringNet were 
extracted from BNC on the simple criterion of fre-
quency (minimum frequency of 5 tokens in BNC), 
making no use of statistical techniques such as 
word association measures in the extraction proc-
ess. However, to support queries of StringNet we 
must have some criteria for ranking the hybrid n-
grams returned in a query result. For this, we use 
MI as our default hybrid n-gram association meas-
urement. The MI equation is as follows: 
 
 
27
                 
 
,where  
 
This equation is well-known as an association 
measure for collocations consisting of word pairs. 
However it is not appropriate directly used in 
measuring hybrid n-grams or n-grams in Lex-
Checker because it cannot compare n-grams of 
different length, i.e with different values of n. It 
would typically be biased toward longer n-grams. 
Therefore we use a version which normalizes, as 
follows: 
 
 
 
,where hn is the target hybrid n-gram, q is user 
query, MI( ) is the traditional MI equation men-
tioned above and maxMIn is the maximum MI 
score achieved among all of the n-grams of any 
given length n and retrieved for query q. 
For example, a hybrid tri-gram T=?pay attention 
to? and a hybrid 4-gram Q=?pay attention to the? 
will be shown in the results of the query 
q=?attention?. Assume MI(T)=5, MI(Q)=7, max-
MI3 (?attention?) =15 and maxMI4  (?attention?) = 
20. Then the Normalized MI(T,q) = 5/15 = 0.334 
and Normalized MI(Q,q) = 7/20 = 0.35. So we can 
rank Q higher than T. MI(hn) will never be greater 
than maxMIn(q) because by stipulation, maxMIn(q) 
represents the highest MI score of all n-grams at a 
given value of n and a query q. So Normalized MI 
will always fall between 0 and 1. This creates a 
common specified range within which MI scores 
for hybrid n-grams of different lengths can be 
ranked. It is important to note that this ranking 
measure is not incorporated into StringNet itself 
(e.g., as a criterion for hybrid n-grams to be in-
cluded in StringNet). Rather it is a post hoc means 
of ranking search results. StringNet is compatible 
with other methods of ranking and contains all sta-
tistical information needed to run such alternative 
measures. 
3.4 Pruning 
As we mention above, hybrid n-grams in StringNet 
consist of all possible combinations of word form, 
lexeme and two types of POS in strings from 2 to 8 
grams in length. Thus for every single traditional 
n-gram consisting of a string of word forms, there 
are numerous hybrid n-grams that also describe 
that same string. For a traditional 8-gram, for ex-
ample, we create 47!2=32768 different hybrid n-
grams (taking into account our criterion that at 
least one token has to be a word form or lexeme). 
Such a large amount of information will cause low 
performance of the StringNet applications.  In or-
der to decrease the search space while still keeping 
most of the useful information, we introduce prun-
ing. Specifically, pruning is intended to eliminate 
redundant hybrid n-grams from searches or appli-
cations of StringNet. There are two types of prun-
ing we use in StringNet currently: Vertical pruning 
and Horizontal pruning. 
 
Vertical pruning:  
Vertical pruning considers pairs of hybrid n-grams 
that are identical in length and differ in the identity 
of some gram in the sequence. Consider the fol-
lowing such pair.  
 
a. hybrid n-gram 1: my point [prep] view  
b. hybrid n-gram 2: my point  of  view 
 
These 4-grams are identical except for the third 
gram; moreover, the counterpart grams occupying 
that third slot (?of? and [prep]) stand in an inclu-
sion relation, ?of? being a member of the POS 
category [prep]. Recalling our cross-indexing, this 
parenthood relation between such hybrid n-grams 
can be readily detected. Pruning of the parent oc-
curs in cases where a threshold proportion of the 
instances attested in BNC of that parent are also 
instances of the child. Consider (a) and (b) above. 
Here the parent (a) ?my point [prep] view? would 
be pruned since all cases of [prep] in this pattern in 
BNC are indeed cases of the preposition ?of?. 
Consider now (c), another parent hybrid n-gram of 
(b) that, in contrast, would not be pruned.  
 
c. hybrid n-gram 3: [dps] point of view 
 
This parent is retained because ?my? accounts 
for fewer than 80% of the instances of the [dps] in 
this pattern. The retention of ?[dps] point of view? 
indicates that more than one possessive pronoun is 
attested in the [dps] slot of this string in a threshold 
proportion of its cases and thus the slot shows sub-
28
stitutability. In a word, vertical pruning eliminates 
hybrid n-grams containing POS grams which do 
not represent attested substitutability. Currently, 
for our StringNet search interface (LexChecker) 
we prune parents with children that represent over 
80% of the BNC tokens also described by that par-
ent.  
 
Horizontal pruning: 
The main idea of Horizontal pruning is the same as 
Vertical pruning. The only difference is the axis of 
comparison: For horizontal pruning, two hybrid n-
grams for comparison differ only by value of n (ie., 
by length). For example, comparing the hybrid n-
gram ?[dps] point of? and ?[dps] point of view,? 
the shorter one is parent and is pruned if a thresh-
old proportion of its instantiations in BNC are also 
instances of the longer child ?[dps] point of view.? 
In horizontal pruning, the shorter of the two com-
pared hybrid n-grams is the potentially redundant 
one and thus the candidate for pruning. As with our 
MI measure, both vertical and horizontal pruning 
rate are set post hoc, applied by post-processing, 
and so are adjustable. 
4 Illustrating with Examples 
Although StringNet can support a wide range of 
applications (such as error detection and correction 
(Tsao and Wible 2009); document similarity meas-
urement, etc.), for ease of exposition in what fol-
lows, we take a search query as our access point to 
illustrate StringNet content. Taking eye as our 
query term, StringNet yields a ranked list of 3,765 
hybrid n-grams containing either this lexeme or 
one of its inflected forms. The following are sam-
ples from the top 50 (i.e., the first page of results): 
 
visible [prep] the naked eye 
turning a blind eye to 
out of the corner of [dps] eye 
[dps] eyes filled with tears 
keeping an eye on the [noun]  
[adv] see eye to eye     
look [pers prn] straight in the eye 
cast a [adj] eye [prep] (e.g., cast a critical eye 
over, cast a cold eye on) 
 
Each hybrid n-gram listed in a search result is 
accompanied by links to examples and parent and 
child icons that link to its parent and children hy-
brid n-grams. (See Fig 1 and 2.) Consider one of 
the hybrid n-grams listed in the results for eye: 
?keep a close eye on.? Recalling Fillmore et als 
distinction between substantive and formal idioms, 
in the case of ?keep a close eye on? we are at the 
level of the formal (lexically filled) idiom. Note 
that since it is a string of lexical items, as are all 
substantive idioms by definition, this sort can just 
as easily be extracted and represented using tradi-
tional flat n-grams. StringNet?s hybrid n-grams and 
their cross-indexing, however, allow us to see 
whether this is a one-off lexically filled idiom or 
an instance of a lexically open formal idiom (i.e., 
of a construction). Without hybrid n-grams, the 
next step up in abstraction to determine this would 
be pure POS n-grams (strings of POS categories 
only) used in the literature (Feldman et al2009; 
Florian et al2003; Gamon et al2009). In the case 
of ?keep a close eye on? the corresponding POS n-
gram would be ?[verb][det][adj][noun][prep].? 
This, however, could describe strings as far afield 
as ?buy a new car with? or ?sequester the entire 
jury until.? Our hybrid n-grams are intended to 
address this Goldilocks problem where construc-
tional phenomena fall between these two sorts of 
traditional n-gram representations evading detec-
tion by both. 
 
 
Figure 1: StringNet search interface: 
?keep a [adj] eye on? 
 
 
 
Figure 2: Children of ?keep a [adj] eye on? 
 
 
29
Navigating from ?keep a close eye on? upward 
through the pruned StringNet network using the 
parent and child links, we find the parent ?keep a 
[adj] eye on? instantiated by attested examples 
?keep a close/watchful/wary/keen eye on.? An-
other parent of ?keep a close eye on? is ?keep a 
close [noun] on?.  
Tellingly there are only two nouns attested more 
than once in the noun slot in this frame: ?keep a 
close eye/watch on.? Both of these parents in turn 
share the common parent ?keep a [adj][nn1] on.? 
This parent is attested by 268 tokens in BNC. 
Among these, there are 80 distinct [adj][nn1] pair-
ings filling those two POS slots in this hybrid n-
gram (e.g., close eye, firm grip, tight rein, close 
watch, etc.). StringNet alows the extraction of this 
set of 80 [adj][nn1] pairs and indexes this set to 
this specific hybrid n-gram. This enables a range of 
investigations. One direction from here is to ex-
plore this particular set of 80 [adj][noun] pairs. For 
example, we could take this set of pairs as a poten-
tial identifying feature set of this construction and 
search StringNet for other hybrid n-grams with the 
substring [adj][noun] to identify those that show a 
large overlap with the 80 pairs from ?keep a 
[adj][noun] on.? This would constitute an approach 
to detecting similar constructions or family resem-
blances between and among constructions. Another 
direction is to see whether ?keep? is an anchoring 
lexical element of this construction or substitutable 
much like the [adj] and [noun] slots. This could be 
investigated in a number of ways in StringNet. For 
example, by comparing ?keep a [adj][noun] on? 
with minimally distinct hybrid n-grams with verbs 
other than ?keep,? conditional or relative probabil-
ity measures could indicate whether that set of 80 
[adj][noun] pairs from ?keep a [adj][noun] on? is 
conditioned by ?keep? or independent of the par-
ticular verb in this string.  
 
It?s the thought that counts: 
For this example, we query StringNet for ?count? 
and get 436 distinct, unpruned hybrid n-grams for 
the verb. The eight listed below include the top-
ranked 5 with 3 others sampled from the top 12, 
rank order retained: 
 
stand up and be counted 
count the number of [nn2] 
count [dps] blessings 
it be the [noun] that count 
[vm0][adv] be counted as 
[pnp] [vm0] not count on 
what counts as [nn1] 
count [pronoun reflx] lucky 
 
Ranked 4th among these is ?it be the [noun] that 
count,? attested with 21 tokens in BNC. In 9 of 
these tokens, the [noun] is thought, so of course, 
navigating down we find ?it?s the thought that 
counts? as a descendant hybrid n-gram. Numerous 
aspects suggest themselves. First is the relation 
between lexically filled substantive idioms and 
more abstract formal idioms that host them. Start-
ing with the lexically filled ?it?s the thought that 
counts? and navigating upward we note that count 
remains specified but can host a range of nouns in 
the focus position, as indicated by our 4th ranked 
?it be the [noun] that count.? The nouns attested in 
this slot are: hunt, perception, topic, message, fu-
ture, critic, change, books, feelings, character, 
voter, sport. Upward from here to a proto ancestor, 
we reach ?it be the [noun] that [verb],? a bare-
bones frame of the It Cleft construction and host to 
the generations of instantiations below it. 
 
Dependency Discovery 
In addition to relations among constructions that 
StringNet encodes, it also yields up internal de-
pendencies between co-occurring grams within a 
construction. A grand-daughter of the proto ?It 
Cleft? string is telling in this respect: ?it be the 
[nn1] that [vvz]?. In other words, StringNet here 
indicates morphological agreement in the ?It Cleft? 
construction. Statistical work on the tokens of 
these hybrid n-grams can detect such dependencies 
automatically. Crucially, StringNet provides trac-
tion on the grammatical features of quirky aspects 
of constructions, that terrain between regularity 
and idiomicity that poses such persistent problems 
for NLP. 
5 Conclusion 
StringNet has been created as a resource for inves-
tigating constructions and a range of multiple word 
expressions and for supporting NLP applications 
that traffic in constructions. While StringNet has 
been extracted from BNC, we hope that in turn 
StringNet can provide a richer setting for investi-
gating a range of linguistic phenomena. For exam-
ple, while computational techniques for extracting 
collocations have been run on traditional corpora, 
30
deeper and more finely nuanced collocation 
knowledge can be discovered when the larger con-
text of a framing construction is taken into ac-
count. Thus not just extracting [adj][noun] 
collocations, but ones particular to a framing con-
struction or family of constructions. StringNet alo 
renders up grammatical dependencies otherwise 
hard to detect since they are within the non-
canonical structures of constructions. It is hoped 
that further cross-indexings of StringNet in the fu-
ture can support increasingly nuanced research on 
constructions. 
Acknowledgments 
The work described in this paper was partially 
supported by the grants from the National Science 
Council, Taiwan (Project Nos. 96-2524-S-008-
003- and 98-2511-S-008-002-MY2). 
References  
Timothy Baldwin, Emily M. Bender, Dan Flickinger, 
Ara Kim and Stephan Oepen. 2004. Road-testing the 
English Resource Grammar over the British National 
Corpus. In Proceedings of the Fourth International 
Conference on Language Resources and Evaluation 
(LREC 2004), Lisbon, Portugal, pp. 2047-2050. 
Dwight Bolinger. 1977. Idioms Have Relations. Forum 
Linguisticum 2:157-69. 
Dwight Bolinger. 1985. Defining the Indefinable. In 
Robert Ilson (ed.) Dictionaries, Lexicography, and 
Language Learning, ELT Documents 120. Oxford: 
Pergamon Press, pp. 69-73. 
Gosse Bouma and Begona Villada.2002. Corpus-based 
acquisition of collocational prepositional phrases. In 
Proceedings of Computational Linguistics in the 
Netherlands (CLIN) 2001, University of Twente. 
Sergey Feldman, Marius Marin, Julie Medero and Mari 
Ostendorf. 2009. Classifying Factored Genres with 
Part-of-Speech Histograms. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the 
Association for Computational Linguistics, Boulder, 
Colorado, pp. 173-176. 
Charles J. Fillmore, Paul Kay, and Mary Katherine 
O?Connor. 1988. Regularity and Idiomaticity in 
Grammatical Constructions: the Case of Let Alone. 
Language 64: 501?538. 
Radu Florian, Abe Ittycheriah, Hongyan Jing and Tong 
Zhang. 2003.  Named Entity Recognition through 
Classifier Combination. In Proceedings of the Sev-
enth Conference on Natural Language Learning at 
HLT-NAACL 2003. 
Michael Gamon, Claudia Leacock, Chris Brockett, Wil-
liam B. Dolan, Jianfeng Gao, Dmitriy Belenko, and 
Alexandre Klementiev. 2009. Using Statistical Tech-
niques and Web Search to Correct ESL Errors. 
CALICO Journal, 26(3), pp 491-511. 
Adele Goldberg, 1995. Constructions: A Construction 
Grammar Approach to Argument Structure. Chicago: 
University of Chicago Press. 
Ray Jackendoff 1997. The Boundaries of the Lexicon. 
in M. Everaert, E.-J. van der Linden, A. Schenk, and 
R. Schreuder, eds., Idioms: Structural and Psycho-
logical Perspectives, 133-165. Hillsdale, NJ: Erl-
baum. 
Ivan Sag, Timothy Baldwin, Francis Bond, Ann 
Copestake, Dan Flickinger. 2002. Multiword expres-
sions: A pain in the neck for NLP. In Proceedings of 
the 3rd International Conference on Intelligent Text 
Processing and Computational Linguistics (CICLing-
2002), Mexico City, Mexico, pp. 1-15. 
Nai-Lung Tsao and David Wible. 2009. A Method for 
Unsupervised Lexical Error Detection and Correction. 
The NAACL Workshop on Innovative Use of NLP for 
Building Educational Applications, Boulder, Colo-
rado, pp. 51-54. 
Yi Zhang, Valia Kordoni, Aline Villavicencio, Marco 
Idiart. 2006. Automated Multiword Expression Pre-
diction for Grammar Engineering. In Proceedings of 
the Workshop on Multiword Expressions: Identifying 
and Exploiting Underlying Properties. COLING-ACL 
2006. Sydney. Australia. 
31
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 128?130,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
The StringNet Lexico-Grammatical  
Knowledgebase and its Applications 
 
David Wible Nai-Lung Tsao 
National Central University 
No.300, Jhongda Rd. 
Jhongli City, Taoyuan County 32001, Taiwan 
wible@stringnet.org beaktsao@stringnet.org 
 
Abstract 
This demo introduces a suite of web-based 
English lexical knowledge resources, called 
StringNet and StringNet Navigator 
(http://nav.stringnet.org), designed to 
provide access to the immense territory of 
multiword expressions that falls between 
what the lexical entries encode in lexicons 
on the one hand and what productive 
grammar rules cover on the other. 
StringNet?s content consists of 1.6 billion 
hybrid n-grams, strings in which word 
forms and parts of speech grams can co-
occur. Subordinate and super-ordinate 
relations among hybrid n-grams are 
indexed, making StringNet a navigable 
web rather than a list. Applications include 
error detection and correction tools and 
web browser-based tools that detect 
patterns in the webpages that a user 
browses. 
1 Introduction and Background 
This demo introduces a suite of web-based English 
lexical knowledge resources, called StringNet and 
StringNet Navigator (http://nav.stringnet.org), 
which have been designed to give lexicographers,  
translators, language teachers and language 
learners direct access to the immense territory of 
multiword expressions, more specifically to the 
lexical patterning that falls in the gap between 
dictionaries and grammar books.  
MWEs are widely recognized in two different 
research communities as posing persistent 
problems, specifically in the fields of 
computational linguistics and human language 
learning and pedagogy.  
In computational linguistics, MWEs are 
notorious as a ?pain in the neck? (Sag et al2002; 
Baldwin et al2004; Villavicencio et al2005; inter 
alia). The high proportion of MWEs with non-
canonical structures lead to parse failures and their 
non-compositional or only partially compositional 
semantics raise difficult choices between which 
ones to store whole and which ones to construct as 
needed. Perhaps above all, this massive family of 
expressions resists any unified treatment since they 
constitute a heterogeneous mix of regularity and 
idiomicity (Fillmore et al1988).  
The other area where they famously cause 
difficulties is in human language learning and 
teaching, and largely for reasons parallel to those 
that make them hard for NLP. They resist 
understanding or production by general rules or 
composition, and they constitute an unpredictable 
mix of productivity and idiomicity. 
The StringNet lexico-grammatical knowledge-
base has been designed to capture this 
heterogeneity of MWEs by virtue of its unique 
content and structure. These we describe in turn 
below. 
2 StringNet Content: Hybrid N-grams 
The content of StringNet consists of a special 
breed of n-grams which we call hybrid n-grams 
(Tsao and Wible 2009; Wible and Tsao 2010). 
Unlike traditional n-grams, there are four different 
categories of gram type. From specific to general 
(or abstract) these four are: specific word forms 
(enjoyed and enjoys would be two distinct word 
forms); lexemes (enjoy, including all its 
inflectional variations, enjoyed, enjoys, etc); rough 
POS categories (V, N, etc); and fine-grained POS 
categories (verbs are distinguished as VVn, VVd, 
VVt, etc.). A hybrid n-gram can consist of any 
sequence from any of these four categories with 
128
our stipulation that one of the grams must be a 
word form or lexeme (to insure that all hybrid n-
grams are lexically anchored). A traditional bi-
gram such as enjoyed hiking can be described by 
16 distinct hybrid n-grams, such as enjoyed VVg, 
enjoy VVg, enjoy hike, and so on. A traditional 5-
gram, such as kept a close eye on has 1024 hybrid 
n-gram variants (45), e.g., keep a close eye on; kept 
a [Adj] eye on; keep a close [N][Prep]; and so on. 
We have extracted all hybrid n-grams ranging in 
length from bigrams to 8-grams that are attested at 
least five times in BNC. StringNet?s content thus 
consists of 1.6 billion hybrid n-grams (including 
traditional n-grams), each indexed to its attested 
instances in BNC. 
3 Structure and Navigation 
Rather than a list of hybrid n-grams, StringNet is a 
structured net. Hybrid n-grams can stand in sub-
ordinate or super-ordinate relation to each other 
(we refer to these as parent/child relations). For 
example, the hybrid tri-gram consider yourselves 
lucky has among its many parents the more 
inclusive consider [prn rflx] lucky; which in turn 
has among its parents the even more general 
consider [prn rflx] [Adj] and [V] [prn rflx] lucky 
and so on. We index all of these relations within 
the entire set of hybrid n-grams. 
StringNet Navigator is the Web interface 
(shown in Figure 1) for navigating this massive, 
structured lexico-grammatical knowledgebase of 
English MWEs. Queries are as simple as 
submitting a Google query. A query of the noun 
trouble immediately shows users (say, language 
learners) subtle but important patterns such as take 
the trouble [to-V] and go to the trouble of [VVg] 
(shown in Figure 2). Submitting mistake yields 
make the mistake of [VVg] and it would be a 
mistake [to-V]. StringNet Navigator also accepts 
multiword queries, returning all hybrid n-grams 
where the submitted words or the submitted words 
and POSs co-occur. For all queries, clicking on any 
pattern given in the results will display all the 
attested example sentences with that pattern from 
BNC. Each listed pattern for a query also gives 
links to that pattern?s parents and children or to its 
expansion (longer version) or contraction (shorter 
version) (See Figure 2). 
4 Some Applications 
Among the many sorts of knowledge that 
StringNet renders tractable is the degree of 
frozenness or substitutability available for any 
MWE. Thus, not only does a query of the noun eye 
yield the string keep a close eye on. Navigating 
upward reveals that close and eye in this string can 
be replaced (keep a close watch on; keep a careful 
eye on; keep a tight grip on; keep a firm hold on, 
etc), but also that, in this same frame keep a 
[Adj][N] on, the verb slot occupied by keep is 
basically unsubstitutable, essentially serving as a 
lexical anchor to this expression. Thus, due to its 
structure as a net, StringNet makes it possible to 
glean the degree and location(s) of the frozenness 
or substitutability of an MWE. 
4.1 Error Checking 
Automatic error detection and correction is a 
rapidly growing area of application in 
computational linguistics (See Leacock et al2010 
for a recent book-length review). StringNet 
supports a novel approach to this area of work. The 
flexibility afforded by hybrid n-grams makes it 
possible to capture patterns that involve subtle 
combinations of lexical specificity or generality for 
different grams within the same string. For 
example, running StringNet on BNC data shows 
that ?enjoy hiking? is best captured as an instance 
of the lexeme enjoy followed by a verb in ?ing 
form: enjoy Vvg. For error checking this makes it 
possible to overcome sparseness. Thus, while BNC 
has no tokens of either ?enjoy spelunking? or 
?enjoy to spelunk,? we can distinguish between 
them nevertheless and detect that the former is 
correct and the latter is an error.  The wide range of 
error types that can be handled by a single 
algorithm run on StringNet will be shown in the 
demo. 
4.2 Browser-based Tools 
Other tools include a toolbar that can be installed 
on the user?s own web browser (Wible et al2011), 
from which the system can detect lexical patterns 
in the text of the web pages the user freely browses. 
A ?Query Doctor? on the toolbar detects errors in 
multiword queries (submitting ?in my point of 
view? triggers the suggestion: ?from my point of 
view?).
129
 
Figure 1: StringNet Navigator front page. 
 
 
Figure 2: Top 2 search results for ?trouble?  
 
5 Conclusion 
Future areas of application for StringNet include 
machine translation (e.g., detecting semi-
compositional constructions); detection of similar 
and confusable words for learners, document 
similarity using hybrid n-grams as features, and 
StringNet Builder for generating StringNets from 
corpora of languages other than English and from 
domain-specific corpora. 
Acknowledgments 
The work described in this paper was partially 
supported by the grants from the National Science 
Council, Taiwan (Project Nos. 99-2631-H-008-
004-  and 98-2511-S-008-002-MY2). 
References  
Timothy Baldwin, Emily M. Bender, Dan Flickinger, 
Ara Kim and Stephan Oepen. 2004. Road-testing the 
English Resource Grammar over the British National 
Corpus. In Proceedings of the Fourth International 
Conference on Language Resources and Evaluation 
(LREC 2004), Lisbon, Portugal, pp. 2047-2050. 
Charles J. Fillmore, Paul Kay, and Mary Katherine 
O?Connor. 1988. Regularity and Idiomaticity in 
Grammatical Constructions: the Case of Let Alone. 
Language 64: 501?538. 
Claudia Leacock, Martin Chodorow, Michael Gamon, 
and Joel Tetreault, 2010. Automated Grammatical 
Error Detection for Language Learners. Morgan and 
Claypool Publishers. 
Ivan Sag, Timothy Baldwin, Francis Bond, Ann 
Copestake, Dan Flickinger. 2002. Multiword 
expressions: A pain in the neck for NLP. In 
Proceedings of the 3rd International Conference on 
Intelligent Text Processing and Computational 
Linguistics (CICLing-2002), Mexico City, Mexico, 
pp. 1-15. 
Nai-Lung Tsao and David Wible. 2009. A Method for 
Unsupervised Broad-Coverage Lexical Error 
Detection and Correction.  The NAACL Workshop on 
Innovative Use of NLP for Building Educational 
Applications, Boulder, Colorado, June 2009. 
Aline Villavicencio, Francis Bond, Anna Korhonen,  
and Diana McCarthy. 2005. Introduction to the 
Special Issue on Multiword Expressions: Having a 
Crack at a Hard Nut. Computer Speech & Language 
19(4): 365-377. 
David Wible and Nai-Lung Tsao. 2010. StringNet as a 
Computational Resource for Discovering and 
Investigating Linguistic Constructions. The NAACL 
Workshop on Extracting and Using Constructions in 
Computational Linguistics, Los Angeles, June 2010. 
David Wible, Anne Li-E Liu and Nai-Lung Tsao. 2011. 
A Browser-based Approach to Incidental 
Individualization of Vocabulary Learning. Journal of 
Computer Assisted Learning, in press, early view.  
130
