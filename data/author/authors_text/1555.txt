Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 41?50,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
One-Class Clustering in the Text Domain
Ron Bekkerman
HP Laboratories
Palo Alto, CA 94304, USA
ron.bekkerman@hp.com
Koby Crammer
University of Pennsylvania
Philadelphia, PA 19104, USA
crammer@cis.upenn.edu
Abstract
Having seen a news title ?Alba denies wedding
reports?, how do we infer that it is primar-
ily about Jessica Alba, rather than about wed-
dings or reports? We probably realize that, in a
randomly driven sentence, the word ?Alba? is
less anticipated than ?wedding? or ?reports?,
which adds value to the word ?Alba? if used.
Such anticipation can be modeled as a ratio
between an empirical probability of the word
(in a given corpus) and its estimated proba-
bility in general English. Aggregated over all
words in a document, this ratio may be used
as a measure of the document?s topicality. As-
suming that the corpus consists of on-topic
and off-topic documents (we call them the
core and the noise), our goal is to determine
which documents belong to the core. We pro-
pose two unsupervised methods for doing this.
First, we assume that words are sampled i.i.d.,
and propose an information-theoretic frame-
work for determining the core. Second, we
relax the independence assumption and use
a simple graphical model to rank documents
according to their likelihood of belonging to
the core. We discuss theoretical guarantees of
the proposed methods and show their useful-
ness for Web Mining and Topic Detection and
Tracking (TDT).
1 Introduction
Many intelligent applications in the text domain aim
at determining whether a document (a sentence, a
snippet etc.) is on-topic or off-topic. In some appli-
cations, topics are explicitly given. In binary text
classification, for example, the topic is described
in terms of positively and negatively labeled docu-
ments. In information retrieval, the topic is imposed
by a query. In many other applications, the topic
is unspecified, however, its existence is assumed.
Examples of such applications are within text sum-
marization (extract the most topical sentences), text
clustering (group documents that are close topi-
cally), novelty detection (reason whether or not test
documents are on the same topic as training docu-
ments), spam filtering (reject incoming email mes-
sages that are too far topically from the content of a
personal email repository), etc.
Under the (standard) Bag-Of-Words (BOW) rep-
resentation of a document, words are the functional
units that bear the document?s topic. Since some
words are topical and some are not, the problem of
detecting on-topic documents has a dual formulation
of detecting topical words. This paper deals with the
following questions: (a) Which words can be con-
sidered topical? (b) How can topical words be de-
tected? (c) How can on-topic documents be detected
given a set of topical words?
The BOW formalism is usually translated into
the generative modeling terms by representing doc-
uments as multinomial word distributions. For the
on-topic/off-topic case, we assume that words in a
document are sampled from a mixture of two multi-
nomials: one over topical words and another one
over general English (i.e. the background). Obvi-
ously enough, the support of the ?topic? multinomial
is significantly smaller than the support of the back-
ground. A document?s topicality is then determined
by aggregating the topicality of its words (see below
for details). Note that by introducing the background
distribution we refrain from explicitly modeling the
class of off-topic documents?a document is sup-
posed to be off-topic if it is ?not topical enough?.
Such a formulation of topicality prescribes us-
ing the one-class modeling paradigm, as opposed
to sticking to the binary case. Besides being much
41
Figure 1: The problem of hyperspherical decision bound-
aries in one-class models for text, as projected on 2D:
(left) a too small portion of the core is captured; (right)
too much space around the core is captured.
less widely studied and therefore much more attrac-
tive from the scientific point of view, one-class mod-
els appear to be more adequate for many real-world
tasks, where negative examples are not straightfor-
wardly observable. One-class models separate the
desired class of data instances (the core) from other
data instances (the noise). Structure of noise is either
unknown, or too complex to be explicitly modeled.
One-class problems are traditionally approached
using vector-space methods, where a convex deci-
sion boundary is built around the data instances of
the desired class, separating it from the rest of the
universe. In the text domain, however, those vector-
space models are questionably applicable?unlike
effective binary vector-space models. In binary
models, decision boundaries are linear1, whereas in
(vector-space) one-class models, the boundaries are
usually hyperspherical. Intuitively, since core docu-
ments tend to lie on a lower-dimensional manifold
(Lebanon, 2005), inducing hyperspherical bound-
aries may be sub-optimal as they tend to either cap-
ture just a portion of the core, or capture too much
space around it (see illustration in Figure 1). Here
we propose alternative ways for detecting the core,
which work well in text.
One-class learning problems have been studied as
either outlier detection or identifying a small coher-
ent subset. In one-class outlier detection (Tax and
Duin, 2001; Scho?lkopf et al, 2001), the goal is to
identify a few outliers from the given set of exam-
ples, where the vast majority of the examples are
considered relevant. Alternatively, a complementary
goal is to distill a subset of relevant examples, in the
space with many outliers (Crammer and Chechik,
1As such, or after applying the kernel trick (Cristianini and
Shawe-Taylor, 2000)
2004; Gupta and Ghosh, 2005; Crammer et al,
2008). Most of the one-class approaches employ ge-
ometrical concepts to capture the notion of relevancy
(or irrelevancy) using either hyperplanes (Scho?lkopf
et al, 2001) or hyperspheres (Tax and Duin, 2001;
Crammer and Chechik, 2004; Gupta and Ghosh,
2005). In this paper we adopt the latter approach:
we formulate one-class clustering in text as an opti-
mization task of identifying the most coherent subset
(the core) of k documents drawn from a given pool
of n > k documents.2
Given a collection D of on-topic and off-topic
documents, we assume that on-topic documents
share a portion of their vocabulary that consists of
?relatively rare? words, i.e. words that are used in D
more often than they are used in general English. We
call them topical words. For example, if some doc-
uments in D share words such as ?Bayesian?, ?clas-
sifier?, ?reinforcement? and other machine learning
terms (infrequent in general English), whereas other
documents do not seem to share any subset of words
(besides stopwords), then we conclude that the ma-
chine learning documents compose the core of D,
while non-machine learning documents are noise.
We express the level of topicality of a word w
in terms of the ratio ?(w) = p(w)q(w) , where p(w) is
w?s empirical probability (in D), and q(w) is its es-
timated probability in general English. We discuss
an interesting characteristic of ?(w): if D is large
enough, then, with high probability, ?(w) values are
greater for topical words than for non-topical words.
Therefore, ?(w) can be used as a mean to measure
the topicality of w.
Obviously, the quality of this measure depends on
the quality of estimating q(w), i.e. the general En-
glish word distribution, which is usually estimated
over a large text collection. The larger the collec-
tion is, the better would be the estimation. Recently,
Google has released the Web 1T dataset3 that pro-
vides q(w) estimated on a text collection of one tril-
lion tokens. We use it in our experimentation.
We propose two methods that use the ? ratio to
2The parameter k is analogous to the number of clusters in
(multi-class) clustering, as well as to the number of outliers (Tax
and Duin, 2001) or the radius of Bregmanian ball (Crammer and
Chechik, 2004)?in other formulations of one-class clustering.
3http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2006T13
42
gz
r |d| nw gzy
r |d| nw
Figure 2: (left) A simple generative model; (right) Latent
Topic/Background model (Section 4).
solve the one-class clustering problem. First, we ex-
press documents? topicality in terms of aggregating
their words? ? ratios into an information-theoretic
?topicality measure?. The core is then composed
of k documents with the highest topicality measure.
We show that the proposed measure is optimal for
constructing the core cluster among documents of
equal length. However, our method is not useful
in a setup where some long documents have a top-
ical portion: such documents should be considered
on-topic, but their heavy tail of background words
overcomes the topical words? influence. We gener-
alize our method to non-equally-long documents by
first extracting words that are supposed to be topi-
cal and then projecting documents over those words.
Such projection preserves the optimality characteris-
tic and results in constructing a more accurate core
cluster in practice. We call such a method of choos-
ing both topical words and core documents One-
Class Co-Clustering (OCCC).
It turns out that our OCCC method?s performance
depends heavily on choosing the number of topical
words. We propose a heuristic for setting this num-
ber. As another alternative, we propose a method
that does not require tuning this parameter: we
use words? ? ratios to initialize an EM algorithm
that computes the likelihood of documents to be-
long to the core?we then choose k documents of
maximal likelihood. We call this model the Latent
Topic/Background (LTB) model. LTB outperforms
OCCC in most of our test cases.
Our one-class clustering models have interesting
cross-links with models applied to other Informa-
tion Retrieval tasks. For example, a model that
resembles our OCCC, is proposed by Zhou and
Croft (2007) for query performance prediction. Tao
and Zhai (2004) describe a pseudo-relevance feed-
back model that is similar to our LTB. These types
of cross-links are common for the models that are
Figure 3: (left) Words? p(w) values when sorted by their
q(w) values; (right) words? ?(w) values.
general enough and relatively simple. In this paper
we put particular emphasis on the simplicity of our
models, such that they are feasible for theoretical
analysis as well as for efficient implementation.
2 Motivation for using ? ratios
Recall that we use the ?(w) = p(w)q(w) ratios to express
the level of our ?surprise? of seeing the word w. A
high value of ?(w) means that w is used in the cor-
pus more frequently than in general English, which,
we assume, implies that w is topical. The more top-
ical words a document contains, the more ?topical?
it is?k most topical documents compose the core
Dk ? D.
An important question is whether or not the ? ra-
tios are sufficient to detecting the actually topical
words. To address this question, let us model the
corpus D using a simple graphical model (Figure 2
left). In this model, the word distribution p(w) is
represented as a mixture of two multinomial distri-
butions: pr over a set R of topical words, and pg
over all the words G ? R in D. For each word wij
in a document di, we toss a coin Zij , such that, if
Zij = 1, then wij is sampled from pr, otherwise it
is sampled from pg. Define pi , p(Zij = 1).
If |G| ? |R| ? 0, and if pi ? 0, then top-
ical words would tend to appear more often than
non-topical words. However, we cannot simply base
our conclusions on word counts, as some words are
naturally more frequent than others (in general En-
glish). Figure 3 (left) illustrates this observation: it
shows words? p(w) values sorted by their q(w) val-
ues. It is hard to fit a curve that would separate be-
tween R and G \R. We notice however, that we can
?flatten? this graph by drawing ?(w) values instead
(see Figure 3 right). Here, naturally frequent words
are penalized by the q factor, so we can assume that,
when re-normalized, ?(w) behaves as a mixture of
two discrete uniform distributions. A simple thresh-
old can then separate between R and G \ R.
43
Proposition 1 Under the uniformity assumption, it
is sufficient to have a log-linear size sample (in |G|)
in order to determine the setRwith high probability.
See Bekkerman (2008) for the proof. The proposi-
tion states that in corpora of practical size4 the set of
topical words can be almost perfectly detected, sim-
ply by taking words with the highest ? ratios. Con-
sequently, the core Dk will consist of k documents,
each of which contains more topical words than any
document from D \ Dk.
To illustrate this theoretical result, we followed
the generative process as described above, and con-
structed an artificial dataset with characteristics sim-
ilar to those of our WAD dataset (see Section 5.1).
In particular, we fixed the size of the artificial dataset
to be equal to the size of the WAD dataset (N =
330, 000). We set the ratio of topical words to 0.2
and assumed uniformity of the ? values. In this
setup, we were able to detect the set of topical words
with a 98.5% accuracy.
2.1 Max-KL Algorithm
In this section, we propose a simple information-
theoretic algorithm for identifying the core Dk, and
show that it is optimal under the uniformity assump-
tion. Given the ? ratios of words, the aggregated
topicality of the corpus D can be expressed in terms
of the KL-divergence:
KL(p||q) =
?
w?G
p(w) log p(w)q(w)
=
?
d?D,w?G
p(d,w) log p(w)q(w) .
A document d?s contribution to the aggregated topi-
cality measure will assess the topicality of d:
KLd(p||q) =
?
w?G
p(d,w) log p(w)q(w) . (1)
The core Dk will be composed of documents with
the highest topicality scores. A simple, greedy algo-
rithm for detecting Dk is then:
1. Sort documents according to their topicality
value (1), in decreasing order.
2. Select the first k documents.
4N = O(m logm), where N is the number of word tokens
in D, and m = |G| is the size of the vocabulary.
Since the algorithm chooses documents with high
values of the KL divergence we call it the Max-KL
algorithm. We now argue that it is optimal under
the uniformity assumption. Indeed, if the corpus
D is large enough, then according to Proposition 1
(with high probability) any topical word w has a
lower ? ratio than any non-topical word. Assume
that all documents are of the same length (|d| is con-
stant). The Max-KL algorithm chooses documents
that contain more topical words than any other doc-
ument in the corpus?which is exactly the definition
of the core, as presented in Section 1. We summarize
this observation in the following proposition:
Proposition 2 If the corpus D is large enough, and
all the documents are of the same length, then the
Max-KL algorithm is optimal for the one-class clus-
tering problem under the uniformity assumption.
In contrast to the (quite natural) uniformity assump-
tion, the all-the-same-length assumption is quite re-
strictive. Let us now propose an algorithm that over-
comes this issue.
3 One-Class Co-Clustering (OCCC)
As accepted in Information Retrieval, we decide that
a document is on-topic if it has a topical portion, no
matter how long its non-topical portion is. There-
fore, we decide about documents? topicality based
on topical words only?non-topical words can be
completely disregarded. This observation leads us to
proposing a one-class co-clustering (OCCC) algo-
rithm: we first detect the set R of topical words, rep-
resent documents over R, and then detect Dk based
on the new representation.5
We reexamine the document?s topicality score (1)
and omit non-topical words. The new score is then:
KLrd(p||q) =
?
w?R
p?(d,w) log p(w)q(w) , (2)
where p?(d,w) = p(d,w)/(?w?R p(d,w)) is a
joint distribution of documents and (only) topical
words. The OCCC algorithm first uses ?(w) to
5OCCC is the simplest, sequential co-clustering algorithm,
where words are clustered prior to clustering documents (see,
e.g., Slonim and Tishby (2000)). In OCCC, word clustering is
analogous to feature selection. More complex algorithms can
be considered, where this analogy is less obvious.
44
choose the most topical words, then it projects doc-
uments on these words and apply the Max-KL algo-
rithm, as summarized below:
1. Sort words according to their ? ratios, in de-
creasing order.
2. Select a subset R of the first mr words.
3. Represent documents as bags-of-words over R
(delete counts of words from G \ R).
4. Sort documents according to their topicality
score (2), in decreasing order.
5. Select a subset Dk of the first k documents.
Considerations analogous to those presented in Sec-
tion 2.1, lead us to the following result:
Proposition 3 If the corpus D is large enough, the
OCCC algorithm is optimal for one-class clustering
of documents, under the uniformity assumption.
Despite its simplicity, the OCCC algorithm shows
excellent results on real-world data (see Section 5).
OCCC?s time complexity is particularly appealing:
O(N), where N is the number of word tokens in D.
3.1 Choosing size mr of the word cluster
The choice of mr = |R| can be crucial. We propose
a useful heuristic for choosing it. We assume that
the distribution of ? ratios for w ? R is a Gaussian
with a mean ?r ? 1 and a variance ?2r , and that the
distribution of ? ratios for w ? G \ R is a Gaussian
with a mean ?nr = 1 and a variance ?2nr. We also
assume that all the words with ?(w) < 1 are non-
topical. Since Gaussians are symmetric, we further
assume that the number of non-topical words with
?(w) < 1 equals the number of non-topical words
with ?(w) ? 1. Thus, our estimate of |G\R| is twice
the number of words with ?(w) < 1, and then the
number of topical words can be estimated as mr =
|G| ? 2 ?#{words with ?(w) < 1}.
4 Latent Topic/Background (LTB) model
Instead of sharply thresholding topical and non-
topical words, we can have them all, weighted with a
probability of being topical. Also, we notice that our
original generative model (Figure 2 left) assumes
that words are i.i.d. sampled, which can be relaxed
by deciding on the document topicality first. In our
new generative model (Figure 2 right), for each doc-
ument di, Yi is a Bernoulli random variable where
Algorithm 1 EM algorithm for one-class clustering
using the LTB model.
Input:
D ? the dataset
?(wl) = p(wl)q(wl) ? ? scores for each word wl|
m
l=1
T ? number of EM iterations
Output: Posteriors p(Yi = 1|di,?T ) for each doc di|ni=1
Initialization:
for each document di initialize pi1i
for each word wl initialize p1r(wl) = ?r?(wl);
p1g(wl) = ?g?(wl) , s.t. ?r and ?g are normalization factors
Main loop:
for all t = 1, . . . , T do
E-step:
for each document di compute ?ti = p(Yi = 1|di,?t)
for each word token wij compute
?tij = p(Zij = 1|Yi = 1, wij ,?t)
M-step:
for each document di update pit+1 = 1|di|
?
j ?tij
for each word wl update
pt+1r (wl) =
?
i ?ti
?
j ?(wij = wl) ?tij?
i ?ti
?
j ?tij
pt+1g (wl) =
Nw ?
?
i ?ti
?
j ?(wij = wl) ?tij
N ??i ?ti
?
j ?tij
Yi = 1 corresponds to di being on-topic. As be-
fore, Zij decides on the topicality of a word token
wij , but now given Yi. Since not all words in a
core document are supposed to be topical, then for
each word of a core document we make a separate
decision (based on Zij) whether it is sampled from
pr(W ) or pg(W ). However, if a document does not
belong to the core (Yi = 0), each its word is sampled
from pg(W ), i.e. p(Zij = 0|Yi = 0) = 1.
Inspired by Huang and Mitchell (2006), we use
the Expectation-Maximization (EM) algorithm to
exactly estimate parameters of our model from the
dataset. We now describe the model parameters ?.
First, the probability of any document to belong to
the core is denoted by p(Yi = 1) = kn = pd (thisparameter is fixed and will not be learnt from data).
Second, for each document di, we maintain a proba-
bility of each its word to be topical given that the
document is on-topic, p(Zij = 1|Yi = 1) = pii
for i = 1, . . . , n. Third, for each word wl (for
k = 1...m), we let p(wl|Zl = 1) = pr(wl) and
p(wl|Zl = 0) = pg(wl). The overall number of pa-
45
rameters is n+ 2m+ 1, one of which (pd) is preset.
The dataset likelihood is then:
p(D) =
n?
i=1
[pd p(di|Yi = 1) + (1? pd)p(di|Yi = 0)]
=
n?
i=1
?
?pd
|di|?
j=1
[piipr(wij) + (1? pii)pg(wij)]
+(1? pd)
|di|?
j=1
pg(wij)
?
? .
At each iteration t of the EM algorithm, we first
perform the E-step, where we compute the poste-
rior distribution of hidden variables {Yi} and {Zij}
given the current parameter values ?t and the data
D. Then, at the M-step, we compute the new pa-
rameter values ?t+1 that maximize the model log-
likelihood given ?t,D and the posterior distribution.
The initialization step is crucial for the EM al-
gorithm. Our pilot experimentation showed that if
distributions pr(W ) and pg(W ) are initialized as
uniform, the EM performance is close to random.
Therefore, we decided to initialize word probabili-
ties using normalized ? scores. We do not propose
the optimal way to initialize pii parameters, however,
as we show later in Section 5, our LTB model ap-
pears to be quite robust to the choice of pii.
The EM procedure is presented in Algorithm 1.
For details, see Bekkerman (2008). After T itera-
tions, we sort the documents according to ?i in de-
creasing order and choose the first k documents to
be the core. The complexity of Algorithm 1 is lin-
ear: O(TN). To avoid overfitting, we set T to be a
small number: in our experiments we fix T = 5.
5 Experimentation
We evaluate our OCCC and LTB models on two ap-
plications: a Web Mining task (Section 5.1), and a
Topic Detection and Tracking (TDT) (Allan, 2002)
task (Section 5.2).
To define our evaluation criteria, let C be the con-
structed cluster and let Cr be its portion consisting
of documents that actually belong to the core. We
define precision as Prec = |Cr|/|C|, recall as Rec =
|Cr|/k and F-measure as (2 Prec Rec)/(Prec+Rec).
Unless stated otherwise, in our experiments we fix
|C| = k, such that precision equals recall and is then
called one-class clustering accuracy, or just accu-
racy.
We applied our one-class clustering methods in
four setups:
? OCCC with the heuristic to choose mr (from
Section 3.1).
? OCCC with optimal mr. We unfairly choose
the number mr of topical words such that the
resulting accuracy is maximal. This setup
can be considered as the upper limit of the
OCCC?s performance, which can be hypotheti-
cally achieved if a better heuristic for choosing
mr is proposed.
? LTB initialized with pii = 0.5 (for each i).
As we show in Section 5.1 below, the LTB
model demonstrates good performance with
this straightforward initialization.
? LTB initialized with pii = pd. Quite naturally,
the number of topical words in a dataset de-
pends on the number of core documents. For
example, if the core is only 10% of a dataset, it
is unrealistic to assume that 50% of all words
are topical. In this setup, we condition the ratio
of topical words on the ratio of core documents.
We compare our methods with two existing al-
gorithms: (a) One-Class SVM clustering6 (Tax and
Duin, 2001); (b) One-Class Rate Distortion (OC-
RD) (Crammer et al, 2008). The later is considered
a state-of-the-art in one-class clustering. Also, to es-
tablish the lowest baseline, we show the result of a
random assignment of documents to the core Dk.
The OC-RD algorithm is based on rate-distortion
theory and expresses the one-class problem as a
lossy coding of each instance into a few possible
instance-dependent codewords. Each document is
represented as a distribution over words, and the KL-
divergence is used as a distortion function (gener-
ally, it can be any Bregman function). The algo-
rithm also uses an ?inverse temperature? parameter
(denoted by ?) that represents the tradeoff between
compression and distortion. An annealing process
is employed, in which the algorithm is applied with
a sequence of increasing values of ?, when initial-
ized with the result obtained at the previous itera-
6We used Chih-Jen Lin?s LibSVM with the -s 2 parame-
ter. We provided the core size using the -n parameter.
46
Method WAD TW
Random assignment 38.7% 34.9? 3.1%
One-class SVM 46.3% 45.2? 3.2%
One-class rate distortion 48.8% 63.6? 3.5%
OCCC with the mr heuristic 80.2% 61.4? 4.5%
OCCC with optimal m 82.4% 68.3? 3.6%
LTB initialized with pii = 0.5 79.8% 65.3? 7.3%
LTB initialized with pii = pd 78.3% 68.0? 5.9%
Table 1: One-class clustering accuracy of our OCCC and
LTB models on the WAD and the TW detection tasks, as
compared to OC-SVM and OC-RD. For TW, the accura-
cies are macro-averaged over the 26 weekly chunks, with
the standard error of the mean presented after the ? sign.
tion. The outcome is a sequence of cores with de-
creasing sizes. The annealing process is stopped
once the largest core size is equal to k.
5.1 Web appearance disambiguation
Web appearance disambiguation (WAD) is proposed
by Bekkerman and McCallum (2005) as the problem
of reasoning whether a particular mention of a per-
son name in the Web refers to the person of interest
or to his or her unrelated namesake. The problem is
solved given a few names of people from one social
network, where the objective is to construct a cluster
of Web pages that mention names of related people,
while filtering out pages that mention their unrelated
namesakes.
WAD is a classic one-class clustering task, that
is tackled by Bekkerman and McCallum with simu-
lated one-class clustering: they use a sophisticated
agglomerative/conglomerative clustering method to
construct multiple clusters, out of which one cluster
is then selected. They also use a simple link struc-
ture (LS) analysis method that matches hyperlinks
of the Web pages in order to compose a cloud of
pages that are close to each other in the Web graph.
The authors suggest that the best performance can
be achieved by a hybrid of the two approaches.
We test our models on the WAD dataset,7 which
consists of 1085 Web pages that mention 12 people
names of AI researchers, such as Tom Mitchell and
Leslie Kaelbling. Out of the 1085 pages, 420 are
on-topic, so we apply our algorithms with k = 420.
At a preprocessing step, we binarize document vec-
tors and remove low frequent words (both in terms
7http://www.cs.umass.edu/?ronb/name_
disambiguation.html
# OCCC LTB
1 cheyer artificial
2 kachites learning
3 quickreview cs
4 adddoc intelligence
5 aaai98 machine
6 kaelbling edu
7 mviews algorithms
8 mlittman proceedings
9 hardts computational
10 meuleau reinforcement
11 dipasquo papers
12 shakshuki cmu
13 xevil aaai
14 sangkyu workshop
15 gorfu kaelbling
Table 2: Most highly ranked words by OCCC and LTB,
on the WAD dataset.
of p(w) and q(w)). The results are summarized in
the middle column of Table 1. We can see that both
OCCC and LTB dramatically outperform their com-
petitors, while showing practically indistinguishable
results compared to each other. Note that when the
size of the word cluster in OCCC is unfairly set to
its optimal value, mr = 2200, the OCCC method
is able to gain a 2% boost. However, for obvious
reasons, the optimal value of mr may not always be
obtained in practice.
Table 2 lists a few most topical words according
to the OCCC and LTB models. The OCCC algo-
rithm sorts words according to their ? scores, such
that words that often occur in the dataset but rarely in
the Web, are on the top of the list. These are mostly
last names or login names of researchers, venues etc.
The EM algorithm of LTB is the given ? scores as an
input to initialize p1r(w) and p1g(w), which are then
updated at each M-step. In the LTB columns, words
are sorted by p5r(w). High quality of the LTB list
is due to conditional dependencies in our generative
model (via the Yi nodes).
Solid lines in Figure 4 demonstrate the robustness
of our models to tuning their main parameters (mr
for OCCC, and the pii initialization for LTB). As can
be seen from the left panel, OCCC shows robust
performance: the accuracy above 80% is obtained
when the word cluster is of any size in the 1000?
3000 range. The heuristic from Section 3.1 suggests
a cluster size of 1000. The LTB is even more robust:
practically any value of pii (besides the very large
ones, pii ? 1) can be chosen.
47
0 2500 5000 7500 100000.660.7
0.740.78
0.820.86
size of word cluster
accura
cy of do
c cluster
OCCC method
 
 OCCCOCCC+link
0 0.2 0.4 0.6 0.8 10.660.7
0.740.78
0.820.86
pii parameter initialization
accura
cy of do
c cluster
LTB method
 
 LTBLTB+link
Figure 4: Web appearance disambiguation: (left)
OCCC accuracy as a function of the word cluster size;
(right) LTB accuracy over various initializations of pii pa-
rameters. The red dotted lines show the accuracy of each
method?s results combined with the Link Structure model
results. On the absolute scale, OCCC outperforms LTB,
however LTB shows more robust behavior than OCCC.
To perform a fair comparison of our results
with those obtained by Bekkerman and McCal-
lum (2005), we construct hybrids of their link struc-
ture (LS) analysis model with our OCCC and LTB,
as follows. First, we take their LS core cluster,
which consists of 360 documents. Second, we pass
over all the WAD documents in the order as they
were ranked by either OCCC or LTB, and enlarge
the LS core with 60 most highly ranked documents
that did not occur in the LS core. In either case, we
end up with a hybrid core of 420 documents.
Dotted lines in Figure 4 show accuracies of the
resulting models. As the F-measure of the hy-
brid model proposed by Bekkerman and McCal-
lum (2005) is 80.3%, we can see that it is signifi-
cantly inferior to the results of either OCCC+LS or
LTB+LS, when their parameters are set to a small
value (mr < 3000 for OCCC, pii < 0.06 for
LTB). Such a choice of parameter values can be
explained by the fact that we need only 60 docu-
ments to expand the LS core cluster to the required
size k = 420. When the values of mr and pii are
small, both OCCC and LTB are able to build very
small and very precise core clusters, which is exactly
what we need here. The OCCC+LS hybrid is par-
ticularly successful, because it uses non-canonical
words (see Table 2) to compose a clean core that al-
most does not overlap with the LS core. Remark-
ably, the OCCC+LS model obtains 86.4% accuracy
with mr = 100, which is the state-of-the-art result
on the WAD dataset.
200 400 600 800 1000
0.6
0.7
0.8
0.9
0.5 document cluster size
F?me
asure
 
 OCCCOCCLTB
Figure 5: Web appearance disambiguation: F-measure
as a function of document cluster size: a vertical line in-
dicates the point where precision equals recall (and there-
fore equals accuracy). ?OCC? refers to the OCCC model
where all the words are taken as the word cluster (i.e. no
word filtering is done).
To answer the question how much our models are
sensitive to the choice of the core size k, we com-
puted the F-measure of both OCCC and LTB as a
function of k (Figure 5). It turns out that our meth-
ods are quite robust to tuning k: choosing any value
in the 300?500 range leads to good results.
5.2 Detecting the topic of the week
Real-world data rarely consists of a clean core and
uniformly distributed noise. Usually, the noise has
some structure, namely, it may contain coherent
components. With this respect, one-class clustering
can be used to detect the largest coherent compo-
nent in a dataset, which is an integral part of many
applications. In this section, we solve the problem of
automatically detecting the Topic of the Week (TW)
in a newswire stream, i.e. detecting all articles in a
weekly news roundup that refer to the most broadly
discussed event.
We evaluate the TW detection task on the bench-
mark TDT-5 dataset8, which consists of 250 news
events spread over a time period of half a year, and
9,812 documents in English, Arabic and Chinese
(translated to English), annotated by their relation-
ship to those events.9 The largest event in TDT-5
dataset (#55106, titled ?Bombing in Riyadh, Saudi
Arabia?) has 1,144 documents, while 66 out of the
250 events have only one document each. We split
the dataset to 26 weekly chunks (to have 26 full
8http://projects.ldc.upenn.edu/TDT5/
9We take into account only labeled documents, while ignor-
ing unlabeled documents that can be found in the TDT-5 data.
48
1 2 3 4 5 6 7 8 9 10 11 12 130
0.5
1
week
acc
ura
cy
Performance of OCCC and LTB on the "topic of the week" task
14 15 16 17 18 19 20 21 22 23 24 25 260
0.5
1
week
acc
ura
cy
 
 
OCCC with the mr heuristicOCCC with the optimal mrLTB initialized with pii = 0.5LTB initialized with pii = pd
Figure 6: ?Topic of the week? detection task: Accuracies of two OCCC methods and two LTB methods.
weeks, we delete all the documents dated with the
last day in the dataset, which decreases the dataset?s
size to 9,781 documents). Each chunk contains from
138 to 1292 documents.
The one-class clustering accuracies, macro-
averaged over the 26 weekly chunks, are presented
in the right column of Table 1. As we can see, both
LTB models, as well as OCCC with the optimal mr,
outperform our baselines. Interestingly, even the op-
timal choice of mr does not lead OCCC to signif-
icantly superior results while compared with LTB.
The dataset-dependent initialization of LTB?s pii pa-
rameters (pii = pd) appears to be preferable over the
dataset-independent one (pii = 0.5).
Accuracies per week are shown in Figure 6. These
results reveal two interesting observations. First,
OCCC tends to outperform LTB only on data chunks
where the results are quite low in general (less than
60% accuracy). Specifically, on weeks 2, 4, 11,
and 16 the LTB models show extremely poor per-
formance. While investigating this phenomenon, we
discovered that in two of the four cases LTB was
able to construct very clean core clusters, however,
those clusters corresponded to the second largest
topic, while we evaluate our methods on the first
largest topic.10 Second, the (completely unsuper-
10For example, on the week-4 data, topic #55077 (?River
ferry sinks on Bangladeshi river?) was discovered by LTB as
the largest and most coherent one. However, in that dataset,
topic #55077 is represented by 20 documents, while topic
#55063 (?SARS Quarantined medics in Taiwan protest?) is
represented by 27 documents, such that topic #55077 is in fact
the second largest one.
vised) LTB model can obtain very good results on
some of the data chunks. For example, on weeks 5,
8, 19, 21, 23, 24, and 25 the LTB?s accuracy is above
90%, with a striking 100% on week-23.
6 Conclusion
We have developed the theory and proposed practi-
cal methods for one-class clustering in the text do-
main. The proposed algorithms are very simple,
very efficient and still surprisingly effective. More
sophisticated algorithms (e.g. an iterative11 version
of OCCC) are emerging.
7 Acknowledgements
We thank Erik Learned-Miller for the inspiration
on this project. We also thank Gunjan Gupta,
James Allan, and Fernando Diaz for fruitful dis-
cussions. This work was supported in part by the
Center for Intelligent Information Retrieval and in
part by the Defense Advanced Research Projects
Agency (DARPA) under contract number HR0011-
06-C-0023. Any opinions, findings and conclusions
or recommendations expressed in this material are
the authors? and do not necessarily reflect those of
the sponsor.
References
J. Allan, editor. 2002. Topic detection and tracking:
event-based information organization. Kluwer Aca-
demic Publishers.
11See, e.g., El-Yaniv and Souroujon (2001)
49
R. Bekkerman and A. McCallum. 2005. Disambiguat-
ing web appearances of people in a social network. In
Proceedings of WWW-05, the 14th International World
Wide Web Conference.
R. Bekkerman. 2008. Combinatorial Markov Random
Fields and their Applications to Information Organi-
zation. Ph.D. thesis, University of Massachusetts at
Amherst.
K. Crammer and G. Chechik. 2004. A needle in a
haystack: local one-class optimization. In Proceed-
ings of the 21st International Conference on Machine
Learning.
K. Crammer, P. Talukdar, and F. Pereira. 2008. A rate-
distortion one-class model and its applications to clus-
tering. In Proceedings of the 25st International Con-
ference on Machine Learning.
N. Cristianini and J. Shawe-Taylor. 2000. An In-
troduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Univer-
sity Press.
R. El-Yaniv and O. Souroujon. 2001. Iterative double
clustering for unsupervised and semi-supervised learn-
ing. In Advances in Neural Information Processing
Systems (NIPS-14).
G. Gupta and J. Ghosh. 2005. Robust one-class cluster-
ing using hybrid global and local search. In Proceed-
ings of the 22nd International Conference on Machine
Learning, pages 273?280.
Y. Huang and T. Mitchell. 2006. Text clustering with ex-
tended user feedback. In Proceedings of the 29th an-
nual international ACM SIGIR conference, pages 413?
420.
G. Lebanon. 2005. Riemannian Geometry and Statistical
Machine Learning. Ph.D. thesis, CMU.
B. Scho?lkopf, J. C. Platt, J. C. Shawe-Taylor, A. J. Smola,
and R. C. Williamson. 2001. Estimating the support
of a high-dimensional distribution. Neural Computa-
tion, 13(7):1443?1471.
N. Slonim and N. Tishby. 2000. Document cluster-
ing using word clusters via the information bottleneck
method. In Proceedings of the 23rd annual interna-
tional ACM SIGIR conference, pages 208?215.
T. Tao and C. Zhai. 2004. A two-stage mixture model for
pseudo feedback. In Proceedings of the 27th annual
international ACM SIGIR conference, pages 486?487.
D. M. J. Tax and R. P. W. Duin. 2001. Outliers and
data descriptions. In Proceedings of the 7th Annual
Conference of the Advanced School for Computing and
Imaging, pages 234?241.
Y. Zhou and W. B. Croft. 2007. Query performance pre-
diction in web search environments. In Proceedings
of the 30th Annual International ACM SIGIR Confer-
ence.
50
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 689?697,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Online Methods for Multi-Domain Learning and Adaptation
Mark Dredze and Koby Crammer
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104 USA
{mdredze,crammer}@cis.upenn.edu
Abstract
NLP tasks are often domain specific, yet sys-
tems can learn behaviors across multiple do-
mains. We develop a new multi-domain online
learning framework based on parameter com-
bination from multiple classifiers. Our algo-
rithms draw from multi-task learning and do-
main adaptation to adapt multiple source do-
main classifiers to a new target domain, learn
across multiple similar domains, and learn
across a large number of disparate domains.
We evaluate our algorithms on two popular
NLP domain adaptation tasks: sentiment clas-
sification and spam filtering.
1 Introduction
Statistical classifiers routinely process millions of
websites, emails, blogs and other text every day.
Variability across different data sources means that
training a single classifier obscures differences and
separate classifiers ignore similarities. Similarly,
adding new domains to existing systems requires
adapting existing classifiers.
We present new online algorithms for three multi-
domain learning scenarios: adapting existing classi-
fiers to new domains, learning across multiple simi-
lar domains and scaling systems to many disparate
domains. Multi-domain learning combines char-
acteristics of both multi-task learning and domain
adaptation and drawing from both areas, we de-
velop a multi-classifier parameter combination tech-
nique for confidence-weighted (CW) linear classi-
fiers (Dredze et al, 2008). We focus on online algo-
rithms that scale to large amounts of data.
Next, we describe multi-domain learning and re-
view the CW algorithm. We then consider our three
settings using multi-classifier parameter combina-
tion. We conclude with related work.
2 Multi-Domain Learning
In online multi-domain learning, each instance x is
drawn from a domain d specific distribution x ? Dd
over a vectors space RN and labeled with a domain
specific function fd with label y ? {?1,+1} (for
binary classification.) On round i the classifier re-
ceives instance xi and domain identifier di and pre-
dicts label y?i ? {?1,+1}. It then receives the true
label yi ? {?1,+1} and updates its prediction rule.
As an example, consider a multi-user spam fil-
ter, which must give high quality predictions for
new users (without new user data), learn on multi-
ple users simultaneously and scale to thousands of
accounts. While a single classifier trained on all
users would generalize across users and extend to
new users, it would fail to learn user-specific prefer-
ences. Alternatively, separate classifiers would cap-
ture user-specific behaviors but would not general-
ize across users. The approach we take to solv-
ing multi-domain problems is to combine domain-
specific classifiers. In the adaptation setting, we
combine source domain classifiers for a new tar-
get domain. For learning across domains, we com-
bine domain-specific classifiers and a shared classi-
fier learned across all domains. For learning across
disparate domains we learn which domain-specific
and shared classifiers to combine.
Multi-domain learning combines properties of
both multi-task learning and domain adaptation. As
689
in multi-task learning, we consider domains that are
labeled with different classification functions. For
example, one user may enjoy some emails that an-
other user considers spam: differing in their classifi-
cation function. The goal of multi-task learning is to
generalize across tasks/domains (Dekel et al, 2006;
Evgeniou and Pontil, 2004). Furthermore, as in do-
main adaptation, some examples are draw from dif-
ferent distributions. For example, one user may re-
ceive emails about engineering while another about
art, differing in their distribution over features. Do-
main adaptation deals with these feature distribution
changes (Blitzer et al, 2007; Jiang and Zhai, 2007).
Our work combines these two areas by learning both
across distributions and behaviors or functions.
3 Confidence-Weighted Linear Classifiers
Confidence-weighted (CW) linear classification
(Dredze et al, 2008), a new online algorithm, main-
tains a probabilistic measure of parameter confi-
dence, which may be useful in combining parame-
ters from different domain distributions. We sum-
marize CW learning to familiarize the reader.
Parameter confidence is formalized by a Gaussian
distribution over weight vectors with mean ? ? RN
and diagonal covariance ? ? RN?N . The values
?j and ?j,j represent knowledge of and confidence
in the parameter for feature j. The smaller ?j,j ,
the more confidence we have in the mean parameter
value ?j . In this work we consider diagonal covari-
ance matrices to scale to NLP data.
A model predicts the highest probability label,
arg max
y?{?1}
Prw?N (?,?) [yi(w ? xi) ? 0] .
The Gaussian distribution over parameter vectors w
induces a univariate Gaussian distribution over the
score Si = w ? xi parameterized by ?, ? and the
instance xi: Si ? N
(
?i, ?2i
)
, with mean ?i = ??xi
and variance ?2i = x
>
i ?xi.
The CW algorithm is inspired by the Passive Ag-
gressive (PA) update (Crammer et al, 2006) ?
which ensures a positive margin while minimizing
parameter change. CW replaces the Euclidean dis-
tance used in the PA update with the Kullback-
Leibler (KL) divergence over Gaussian distribu-
tions. It also replaces the minimal margin constraint
with a minimal probability constraint: with some
given probability ? ? (0.5, 1] a drawn classifier will
assign the correct label. This strategy yields the fol-
lowing objective solved on each round of learning:
min DKL (N (?,?) ?N (?i,?i))
s.t. Pr [yi (w ? xi) ? 0] ? ? ,
where (?i,?i) are the parameters on round i and
w ? N (?,?). The constraint ensures that the re-
sulting parameters
(
?i+1,?i+1
)
will correctly clas-
sify xi with probability at least ?. For convenience
we write ? = ??1 (?), where ? is the cumula-
tive function of the normal distribution. The opti-
mization problem above is not convex, but a closed
form approximation of its solution has the follow-
ing additive form: ?i+1 = ?i + ?iyi?ixi and
??1i+1 = ?
?1
i + 2?i?xix
>
i for,
?i=
?(1+2??i)+
?
(1+2??i)
2?8?
(
?i???2i
)
4??2i
.
Each update changes the feature weights ?, and in-
creases confidence (variance ? always decreases).
We employ CW classifiers since they provide con-
fidence estimates, which are useful for classifier
combination. Additionally, since we require per-
parameter confidence estimates, other confidence
based classifiers are not suitable for this setting.
4 Multi-Classifier Parameter Combination
The basis of our approach to multi-domain learning
is to combine the parameters of CW classifiers from
separate domains while respecting parameter confi-
dence. A combination method takes M CW classi-
fiers each parameterized by its own mean and vari-
ance parameters {(?m,?m)}Mm=1 and produces a
single combined classifier (?c,?c). A simple tech-
nique would be to average the parameters of classi-
fiers into a new classifier. However, this ignores the
difference in feature distributions. Consider for ex-
ample that the weight associated with some word in
a source classifier has a value of 0. This could either
mean that the word is very rare or that it is neutral
for prediction (like the work ?the?). The informa-
tion captured by the variance parameter allow us to
distinguish between the two cases: an high-variance
indicates a lack of confidence in the value of the
690
weight vectors because of small number of exam-
ples (first case), and vise-versa, small-variance indi-
cates that the value of the weight is based on plenty
of evidence. We favor combinations sensitive to this
distinction.
Since CW classifiers are Gaussian distributions,
we formalize classifier parameter combination as
finding a new distribution that minimizes the
weighted-divergence to a set of given distributions:
(?c,?c) = arg min
M?
m
D((?c,?c)||(?m,?m) ; bm) ,
where (since ? is diagonal),
D((?c,?c)||(?,?) ; b) =
?N
f bfD((?
c
f ,?
c
f,f )||(?f ,?f,f )) .
The (classifier specific) importance-weights bm ?
RN+ are used to weigh certain parameters of some
domains differently in the combination. When D is
the Euclidean distance (L2), we have,
D((?cf ,?
c
f,f )||(?f ,?f,f )) =
(?cf ? ?f )
2 + (?cf,f ? ?f,f )
2 .
and we obtain:
?cf =
1
?M
m b
m
f
M?
m
bmf ?
m
f ,
?cf,f =
1
?
m?M b
m
f
M?
m
bmf ?
m
f,f . (1)
Note that this is a (weighted) average of parameters.
The other case we consider is when D is a weighted
KL divergence we obtain a weighting of ? by ??1:
?cf =
(
M?
m
(?mf,f )
?1bmf
)?1 M?
m
(?mf,f )
?1?mf b
m
f
(?c)?1 =
(
M
M?
m
bmf
)?1 M?
m
(?mf )
?1bf
m . (2)
While each parameter is weighed by its variance in
the KL, we can also explicitly encode this behavior
as bmf = a ? ?
m
f,f ? 0, where a is the initializa-
tion value for ?mf,f . We call this weighting ?vari-
ance? as opposed to a uniform weighting of param-
eters (bmf = 1). We therefore have two combination
methods (L2 and KL) and two weighting methods
(uniform and variance).
5 Datasets
For evaluation we selected two domain adaptation
datasets: spam (Jiang and Zhai, 2007) and sentiment
(Blitzer et al, 2007). The spam data contains two
tasks, one with three users (task A) and one with 15
(task B). The goal is to classify an email (bag-of-
words) as either spam or ham (not-spam) and each
user may have slightly different preferences and fea-
tures. We used 700 and 100 training messages for
each user for task A and B respectively and 300 test
emails for each user.
The sentiment data contains product reviews from
Amazon for four product types: books, dvds, elec-
tronics and kitchen appliances and we extended this
with three additional domains: apparel, music and
videos. We follow Blitzer et. al. for feature ex-
traction. We created different datasets by modify-
ing the decision boundary using the ordinal rating
of each instance (1-5 stars) and excluding boundary
instances. We use four versions of this data:
? All - 7 domains, one per product type
? Books - 3 domains of books with the binary
decision boundary set to 2, 3 and 4 stars
? DVDs - Same as Books but with DVD reviews
? Books+DVDs - Combined Books and DVDs
The All dataset captures the typical domain adap-
tation scenario, where each domain has the same
decision function but different features. Books
and DVDs have the opposite problem: the same
features but different classification boundaries.
Books+DVDs combines both issues. Experiments
use 1500 training and 100 test instances per domain.
6 Multi-Domain Adaptation
We begin by examining the typical domain adapta-
tion scenario, but from an online perspective since
learning systems often must adapt to new users or
domains quickly and with no training data. For ex-
ample, a spam filter with separate classifiers trained
on each user must also classify mail for a new
user. Since other user?s training data may have been
deleted or be private, the existing classifiers must be
combined for the new user.
691
Train L2 KL
Target Domain All Src Target Best Src Avg Src Uniform Variance Uniform Variance
S
pa
m
user0 3.85 1.80 4.80 8.26 5.25 4.63 4.53 4.32
user1 3.57 3.17 4.28 6.91 4.53 3.80 4.23 3.83
user2 3.30 2.40 3.77 5.75 4.75 4.60 4.93 4.67
S
en
ti
m
en
t
apparel 12.32 12.02 14.12 21.15 14.03 13.18 13.50 13.48
books 16.85 18.95 22.95 25.76 19.58 18.63 19.53 19.05
dvd 13.65 17.40 17.30 21.89 15.53 13.73 14.48 14.15
kitchen 13.65 14.40 15.52 22.88 16.68 15.10 14.78 14.02
electronics 15.00 14.93 15.52 23.84 18.75 17.37 17.45 16.82
music 18.20 18.30 20.75 24.19 18.38 17.83 18.10 18.22
video 17.00 19.27 19.43 25.78 17.13 16.25 16.33 16.42
Table 1: Test error for multi-source adaptation on sentiment and spam data. Combining classifiers improves over
selecting a single classifier a priori (Avg Src).
We combine the existing user-specific classifiers
into a single new classifier for a new user. Since
nothing is known about the new user (their deci-
sion function), each source classifier may be useful.
However, feature similarity ? possibly measured us-
ing unlabeled data ? could be used to weigh source
domains. Specifically, we combine the parameters
of each classifier according to their confidence us-
ing the combination methods described above.
We evaluated the four combination strategies ? L2
vs. KL, uniform vs. variance ? on spam and sen-
timent data. For each evaluation, a single domain
was held out for testing while separate classifiers
were trained on each source domain, i.e. no target
training. Source classifiers are then combined and
the combined classifier is evaluated on the test data
(400 instances) of the target domain. Each classi-
fier was trained for 5 iterations over the training data
(to ensure convergence) and each experiment was
repeated using 10-fold cross validation. The CW
parameter ? was tuned on a single randomized run
for each experiment. We include several baselines:
training on target data to obtain an upper bound
on performance (Target), training on all source do-
mains together, a useful strategy if all source data is
maintained (All Src), selecting (with omniscience)
the best performing source classifier on target data
(Best Src), and the expected real world performance
of randomly selecting a source classifier (Avg Src).
While at least one source classifier achieved high
performance on the target domain (Best Src), the
correct source classifier cannot be selected without
target data and selecting a random source classifier
yields high error. In contrast, a combined classifier
almost always improved over the best source domain
classifier (table 1). That some of our results improve
over the best training scenario is likely caused by in-
creased training data from using multiple domains.
Increases over all available training data are very in-
teresting and may be due to a regularization effect of
training separate models.
The L2 methods performed best and KL improved
7 out of 10 combinations. Classifier parameter com-
bination can clearly yield good classifiers without
prior knowledge of the target domain.
7 Learning Across Domains
In addition to adapting to new domains, multi-
domain systems should learn common behaviors
across domains. Naively, we can assume that the
domains are either sufficiently similar to warrant
one classifier or different enough for separate clas-
sifiers. The reality is often more complex. Instead,
we maintain shared and domain-specific parameters
and combine them for learning and prediction.
Multi-task learning aims to learn common behav-
iors across related problems, a similar goal to multi-
domain learning. The primary difference is the na-
ture of the domains/tasks: in our setting each domain
is the same task but differs in the types of features in
addition to the decision function. A multi-task ap-
proach can be adapted to our setting by using our
classifier combination techniques.
692
Spam Sentiment
Method Task A Task B Books DVD Books+DVD All
Single 3.88 8.75 23.7 25.11 23.26 16.57
Separate 5.46 14.53 22.22 21.64 21.23 21.89
Feature Splitting 4.16 8.93 15.65 16.20 14.60 17.45
MDR 4.09 9.18 15.65 15.12 13.76 17.45
MDR+L2 4.27 8.61 12.70 14.95 12.73 17.16
MDR+L2-Var 3.75 7.52 12.90 14.21 12.52 17.37
MDR+KL 4.32 9.22 13.51 13.81 13.32 17.20
MDR+KL-Var 4.02 8.70 14.93 14.03 14.22 18.40
Table 2: Online training error for learning across domains.
Spam Sentiment
Method Task A Task B Books DVD Books+DVD All
Single 2.11 5.60 18.43 18.67 19.08 14.09
Separate 2.43 8.5 18.87 15.97 16.45 17.23
Feature Splitting 1.94 5.51 9.97 9.70 9.05 14.73
MDR 1.94 5.69 9.97 8.33 8.20 14.73
MDR+L2 1.87 5.16 6.63 7.97 7.62 14.20
MDR+L2-Var 1.90 4.78 6.40 7.83 7.30 14.33
MDR+KL 1.94 5.61 8.37 7.07 8.43 14.60
MDR+KL-Var 1.97 5.46 9.40 7.50 8.05 15.50
Table 3: Test data error: learning across domains (MDR) improves over the baselines and Daume? (2007).
We seek to learn domain specific parameters
guided by shared parameters. Dekel et al (2006)
followed this approach for an online multi-task algo-
rithm, although they did not have shared parameters
and assumed that a training round comprised an ex-
ample from each task. Evgeniou and Pontil (2004)
achieved a similar goal by using shared parameters
for multi-task regularization. Specifically, they as-
sumed that the weight vector for problem d could be
represented aswc = wd+ws, wherewd are task spe-
cific parameters and ws are shared across all tasks.
In this framework, all tasks are close to some under-
lying meanws and each one deviates from this mean
by wd. Their SVM style multi-task objective mini-
mizes the loss ofwc and the norm ofwd andws, with
a tradeoff parameter allowing for domain deviance
from the mean. The simple domain adaptation al-
gorithm of feature splitting used by Daume? (2007)
is a special case of this model where the norms are
equally weighted. An analogous CW objective is:
min
1
?1
DKL
(
N
(
?d,?d
)
?N
(
?di ,?
d
i
))
+
1
?2
DKL (N (?s,?s) ?N (?si ,?
s
i ))
s.t. Prw?N (?c,?c) [yi (w ? xi) ? 0] ? ? . (3)
(
?d,?d
)
are the parameters for domain d, (?s,?s)
for the shared classifier and (?c,?c) for the com-
bination of the domain and shared classifiers. The
parameters are combined via (2) with only two ele-
ments summed - one for the shared parameters s and
the other for the domain parameters d . This captures
the intuition of Evgeniou and Pontil: updates en-
force the learning condition on the combined param-
eters and minimize parameter change. For conve-
nience, we rewrite ?2 = 2? 2?1, where ?1 ? [0, 1].
If classifiers are combined using the sum of the indi-
vidual weight vectors and ?1 = 0.5, this is identical
to feature splitting (Daume?) for CW classifiers.
The domain specific and shared classifiers can be
693
updated using the closed form solution to (3) as:
?s = ?si + ?2?yi?
cxi
(?s)?1 = (?si )
?1 + 2?2??xixTi
?d = ?di + ?1?yi?
c
ixi
(?d)?1 = (?di )
?1 + 2?1??xixTi
(4)
We call this objective Multi-Domain Regulariza-
tion (MDR). As before, the combined parameters
are produced by one of the combination methods.
On each round, the algorithm receives instance xi
and domain di for which it creates a combined clas-
sifier (?c,?c) using the shared (?s,?s) and domain
specific parameters
(
?d,?d
)
. A prediction is is-
sued using the standard linear classifier prediction
rule sign(?c ? x) and updates follow (4). The ef-
fect is that features similar across domains quickly
converge in the shared classifier, sharing informa-
tion across domains. The combined classifier re-
flects shared and domain specific parameter confi-
dences: weights with low variance (i.e. greater con-
fidence) will contribute more.
We evaluate MDR on a single pass over a stream
of instances from multiple domains, simulating a
real world setting. Parameters ?1 and ? are iter-
atively optimized on a single randomized run for
each dataset. All experiments use 10-fold CV. In ad-
dition to evaluating the four combination methods
with MDR, we evaluate the performance of a sin-
gle classifier trained on all domains (Single), a sep-
arate classifier trained on each domain (Separate),
Feature Splitting (Daume?) and feature splitting with
optimized ?1 (MDR). Table 3 shows results on test
data and table 2 shows online training error.
In this setting, L2 combinations prove best on 5
of 6 datasets, with the variance weighted combina-
tion doing the best. MDR (optimizing ?1) slightly
improves over feature splitting, and the combination
methods improve in every case. Our best result is
statistically significant compared to Feature Split-
ting using McNemar?s test (p = .001) for Task B,
Books, DVD, Books+DVD. While a single or sepa-
rate classifiers have a different effect on each dataset,
MDR gives the best performance overall.
8 Learning in Many Domains
So far we have considered settings with a small
number of similar domains. While this is typical
of multi-task problems, real world settings present
many domains which do not all share the same be-
haviors. Online algorithms scale to numerous ex-
amples and we desire the same behavior for numer-
ous domains. Consider a spam filter used by a large
email provider, which filters billions of emails for
millions of users. Suppose that spammers control
many accounts and maliciously label spam as legiti-
mate. Alternatively, subsets of users may share pref-
erences. Since behaviors are not consistent across
domains, shared parameters cannot be learned. We
seek algorithms robust to this behavior.
Since subsets of users share behaviors, these can
be learned using our MDR framework. For example,
discovering spammer and legitimate mail accounts
would enable intra-group learning. The challenge is
the online discovery of these subsets while learning
model parameters. We augment the MDR frame-
work to additionally learn this mapping.
We begin by generalizing MDR to include k
shared classifiers instead of a single set of shared pa-
rameters. Each set of shared parameters represents
a different subset of domains. If the corresponding
shared parameters are known for a domain, we could
use the same objective (3) and update (4) as before.
If there are many fewer shared parameters than do-
mains (k  D), we can benefit from multi-domain
learning. Next, we augment the learning algorithm
to learn a mapping between the domains and shared
classifiers. Intuitively, a domain should be mapped
to shared parameters that correctly classify that do-
main. A common technique for learning such ex-
perts in the Weighted Majority algorithm (Little-
stone and Warmuth, 1994), which weighs a mixture
of experts (classifiers). However, since we require a
hard assignment ? pick a single shared parameter
set s ? rather than a mixture, the algorithm reduces
to picking the classifier s with the fewest mistakes
in predicting domain d. This requires tracking the
number of mistakes made by each shared classifier
on each domain once a label is revealed. For learn-
ing, the shared classifier with the fewest mistakes
for a domain is selected for an MDR update. Clas-
sifier ties are broken randomly. While we experi-
694
Figure 1: Learning across many domains - spam (left) and sentiment (right) - with MDR using k shared classifiers.
Figure 2: Learning across many domains - spam (left) and sentiment (right) - with no domain specific parameters.
mented with more complex techniques, this simple
method worked well in practice. When a new do-
main is added to the system, it takes fewer exam-
ples to learn which shared classifier to use instead of
learning a new model from scratch.
While this approach adds another free parameter
(k) that can be set using development data, we ob-
serve that k can instead be fixed to a large constant.
Since only a single shared classifier is updated each
round, the algorithm will favor selecting a previ-
ously used classifier as opposed to a new one, using
as many classifiers as needed but not scaling up to k.
This may not be optimal, but it is a simple.
To evaluate a larger number of domains, we cre-
ated many varying domains using spam and senti-
ment data. For spam, 6 email users were created by
splitting the 3 task A users into 2 users, and flipping
the label of one of these users (a malicious user),
yielding 400 train and 100 test emails per user. For
sentiment, the book domain was split into 3 groups
with binary boundaries at a rating of 2, 3 or 4. Each
of these groups was split into 8 groups of which half
had their labels flipped, creating 24 domains. The
same procedure was repeated for DVD reviews but
for a decision boundary of 3, 6 groups were created,
and for a boundary of 2 and 4, 3 groups were created
with 1 and 2 domains flipped respectively, resulting
in 12 DVD domains and 36 total domains with var-
ious decision boundaries, features, and inverted de-
cision functions. Each domain used 300 train and
100 test instances. 10-fold cross validation with one
training iteration was used to train models on these
695
two datasets. Parameters were optimized as before.
Experiments were repeated for various settings of
k. Since L2 performed well before, we evaluated
MDR+L2 and MDR+L2-Var.
The results are shown in figure 1. For both spam
and sentiment adding additional shared parameters
beyond the single shared classifier significantly re-
duces error, with further reductions as k increases.
This yields a 45% error reduction for spam and a
38% reduction for sentiment over the best baseline.
While each task has an optimal k (about 5 for spam,
2 for sentiment), larger values still achieve low error,
indicating the flexibility of using large k values.
While adding parameters clearly helps for many
domains, it may be impractical to keep domain-
specific classifiers for thousands or millions of do-
mains. In this case, we could eliminate the domain-
specific classifiers and rely on the k shared clas-
sifiers only, learning the domain to classifier map-
ping. We compare this approach using the best result
from MDR above, again varying k. Figure 2 shows
that losing domain-specific parameters hurts perfor-
mance, but is still an improvement over baseline
methods. Additionally, we can expect better perfor-
mance as the number of similar domains increases.
This may be an attractive alternative to keeping a
very large number of parameters.
9 Related Work
Multi-domain learning intersects two areas of re-
search: domain adaptation and multi-task learning.
In domain adaptation, a classifier trained for a source
domain is transfered to a target domain using either
unlabeled or a small amount of labeled target data.
Blitzer et al (2007) used structural correspondence
learning to train a classifier on source data with
new features induced from target unlabeled data. In
a complimentary approach, Jiang and Zhai (2007)
weighed training instances based on their similarity
to unlabeled target domain data. Several approaches
utilize source data for training on a limited number
of target labels, including feature splitting (Daume?,
2007) and adding the source classifier?s prediction
as a feature (Chelba and Acero, 2004). Others have
considered transfer learning, in which an existing
domain is used to improve learning in a new do-
main, such as constructing priors (Raina et al, 2006;
Marx et al, 2008) and learning parameter functions
for text classification from related data (Do and Ng,
2006). These methods largely require batch learn-
ing, unlabeled target data, or available source data
at adaptation. In contrast, our algorithms operate
purely online and can be applied when no target data
is available.
Multi-task algorithms, also known as inductive
transfer, learn a set of related problems simultane-
ously (Caruana, 1997). The most relevant approach
is that of Regularized Multi-Task Learning (Evge-
niou and Pontil, 2004), which we use to motivate
our online algorithm. Dekel et al (2006) gave a sim-
ilar online approach but did not use shared parame-
ters and assumed multiple instances for each round.
We generalize this work to both include an arbi-
trary classifier combination and many shared classi-
fiers. Some multi-task work has also considered the
grouping of tasks similar to our learning of domain
subgroups (Thrun and O?Sullivan, 1998; Bakker and
Heskes, 2003).
There are many techniques for combining the out-
put of multiple classifiers for ensemble learning or
mixture of experts. Kittler et al (Mar 1998) provide
a theoretical framework for combining classifiers.
Some empirical work has considered adding versus
multiplying classifier output (Tax et al, 2000), using
local accuracy estimates for combination (Woods et
al., 1997), and applications to NLP tasks (Florian et
al., 2003). However, these papers consider combin-
ing classifier output for prediction. In contrast, we
consider parameter combination for both prediction
and learning.
10 Conclusion
We have explored several multi-domain learning
settings using CW classifiers and a combination
method. Our approach creates a better classifier for
a new target domain than selecting a random source
classifier a prior, reduces learning error on multiple
domains compared to baseline approaches, can han-
dle many disparate domains by using many shared
classifiers, and scales to a very large number of do-
mains with a small performance reduction. These
scenarios are realistic for NLP systems in the wild.
This work also raises some questions about learning
on large numbers of disparate domains: can a hi-
696
erarchical online clustering yield a better represen-
tation than just selecting between k shared parame-
ters? Additionally, how can prior knowledge about
domain similarity be included into the combination
methods? We plan to explore these questions in fu-
ture work.
Acknowledgements This material is based upon
work supported by the Defense Advanced Re-
search Projects Agency (DARPA) under Contract
No. FA8750-07-D-0185.
References
B. Bakker and T. Heskes. 2003. Task clustering and gat-
ing for bayesian multi?task learning. Journal of Ma-
chine Learning Research, 4:83?99.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In As-
sociation for Computational Linguistics (ACL).
Rich Caruana. 1997. Multitask learning. Machine
Learning, 28:41?75.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
max- imum entropy classifier: Little data can help a
lot. In Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Hal Daume?. 2007. Frustratingly easy domain adaptation.
In Association for Computational Linguistics (ACL).
Ofer Dekel, Philip M. Long, and Yoram Singer. 2006.
Online multitask learning. In Conference on Learning
Theory (COLT).
Chuong B. Do and Andrew Ng. 2006. Transfer learning
for text classification. In Advances in Neural Informa-
tion Processing Systems (NIPS).
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification.
In International Conference on Machine Learning
(ICML).
Theodoros Evgeniou and Massimiliano Pontil. 2004.
Regularized multi-task learning. In Conference on
Knowledge Discovery and Data Mining (KDD).
Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong
Zhang. 2003. Named entity recognition through clas-
sifier combination. In Conference on Computational
Natural Language Learning (CONLL).
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In Association for
Computational Linguistics (ACL).
J. Kittler, M. Hatef, R.P.W. Duin, and J. Matas. Mar
1998. On combining classifiers. Pattern Analy-
sis and Machine Intelligence, IEEE Transactions on,
20(3):226?239.
N. Littlestone and M. K. Warmuth. 1994. The weighted
majority algorithm. Information and Computation,
108:212?261.
Zvika Marx, Michael T. Rosenstein, Thomas G. Diet-
terich, and Leslie Pack Kaelbling. 2008. Two algo-
rithms for transfer learning. In Inductive Transfer: 10
years later.
Rajat Raina, Andrew Ng, and Daphne Koller. 2006.
Constructing informative priors using transfer learn-
ing. In International Conference on Machine Learn-
ing (ICML).
David M. J. Tax, Martijn van Breukelen, Robert P. W.
Duina, and Josef Kittler. 2000. Combining multiple
classifiers by averaging or by multiplying? Pattern
Recognition, 33(9):1475?1485, September.
S. Thrun and J. O?Sullivan. 1998. Clustering learning
tasks and the selective cross?task transfer of knowl-
edge. In S. Thrun and L.Y. Pratt, editors, Learning To
Learn. Kluwer Academic Publishers.
Kevin Woods, W. Philip Kegelmeyer Jr., and Kevin
Bowyer. 1997. Combination of multiple classifiers
using local accuracy estimates. IEEE Transactions on
Pattern Analysis andMachine Intelligence, 19(4):405?
410.
697
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 496?504,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Multi-Class Confidence Weighted Algorithms
Koby Crammer
?
?
Department of Computer
and Information Science
University of Pennsylvania
Philadelphia, PA 19104
{crammer,kulesza}@cis.upenn.edu
Mark Dredze
?
Alex Kulesza
?
?
Human Language Technology
Center of Excellence
Johns Hopkins University
Baltimore, MD 21211
mdredze@cs.jhu.edu
Abstract
The recently introduced online
confidence-weighted (CW) learning
algorithm for binary classification per-
forms well on many binary NLP tasks.
However, for multi-class problems CW
learning updates and inference cannot
be computed analytically or solved as
convex optimization problems as they are
in the binary case. We derive learning
algorithms for the multi-class CW setting
and provide extensive evaluation using
nine NLP datasets, including three derived
from the recently released New York
Times corpus. Our best algorithm out-
performs state-of-the-art online and batch
methods on eight of the nine tasks. We
also show that the confidence information
maintained during learning yields useful
probabilistic information at test time.
1 Introduction
Online learning algorithms such as the Perceptron
process one example at a time, yielding simple and
fast updates. They generally make few statisti-
cal assumptions about the data and are often used
for natural language problems, where high dimen-
sional feature representations, e.g., bags-of-words,
demand efficiency. Most online algorithms, how-
ever, do not take into account the unique properties
of such data, where many features are extremely
rare and a few are very frequent.
Dredze, Crammer and Pereira (Dredze et al,
2008; Crammer et al, 2008) recently introduced
confidence weighted (CW) online learning for bi-
nary prediction problems. CW learning explicitly
models classifier weight uncertainty using a multi-
variate Gaussian distribution over weight vectors.
The learner makes online updates based on its con-
fidence in the current parameters, making larger
changes in the weights of infrequently observed
features. Empirical evaluation has demonstrated
the advantages of this approach for a number of bi-
nary natural language processing (NLP) problems.
In this work, we develop and test multi-class
confidence weighted online learning algorithms.
For binary problems, the update rule is a sim-
ple convex optimization problem and inference
is analytically computable. However, neither is
true in the multi-class setting. We discuss sev-
eral efficient online learning updates. These up-
date rules can involve one, some, or all of the
competing (incorrect) labels. We then perform an
extensive evaluation of our algorithms using nine
multi-class NLP classification problems, includ-
ing three derived from the recently released New
York Times corpus (Sandhaus, 2008). To the best
of our knowledge, this is the first learning evalua-
tion on these data. Our best algorithm outperforms
state-of-the-art online algorithms and batch algo-
rithms on eight of the nine datasets.
Surprisingly, we find that a simple algorithm in
which updates consider only a single competing
label often performs as well as or better than multi-
constraint variants if it makes multiple passes over
the data. This is especially promising for large
datasets, where the efficiency of the update can
be important. In the true online setting, where
only one iteration is possible, multi-constraint al-
gorithms yield better performance.
Finally, we demonstrate that the label distribu-
tions induced by the Gaussian parameter distribu-
tions resulting from our methods have interesting
properties, such as higher entropy, compared to
those from maximum entropy models. Improved
label distributions may be useful in a variety of
learning settings.
2 Problem Setting
In the multi-class setting, instances from an input
space X take labels from a finite set Y , |Y| = K.
496
We use a standard approach (Collins, 2002) for
generalizing binary classification and assume a
feature function f(x, y) ? R
d
mapping instances
x ? X and labels y ? Y into a common space.
We work in the online framework, where learn-
ing is performed in rounds. On each round the
learner receives an input x
i
, makes a prediction y?
i
according to its current rule, and then learns the
true label y
i
. The learner uses the new example
(x
i
, y
i
) to modify its prediction rule. Its goal is to
minimize the total number of rounds with incor-
rect predictions, |{i : y
i
6= y?
i
}|.
In this work we focus on linear models parame-
terized by weightsw and utilizing prediction func-
tions of the form h
w
(x) = arg max
z
w ? f(x, z).
Note that since we can choose f(x, y) to be the
vectorized Cartesian product of an input feature
function g(x) and y, this setup generalizes the use
of unique weight vectors for each element of Y .
3 Confidence Weighted Learning
Dredze, Crammer, and Pereira (2008) introduced
online confidence weighted (CW) learning for bi-
nary classification, where X = R
d
and Y =
{?1}. Rather than using a single parameter vec-
tor w, CW maintains a distribution over param-
eters N (?,?), where N (?,?) the multivariate
normal distribution with mean ? ? R
d
and co-
variance matrix ? ? R
d?d
. Given an input in-
stance x, a Gibbs classifier draws a weight vector
w from the distribution and then makes a predic-
tion according to the sign of w ? x.
This prediction rule is robust if the example
is classified correctly with high-probability, that
is, for some confidence parameter .5 ? ? < 1,
Pr
w
[y (w ? x) ? 0] ? ?. To learn a binary CW
classifier in the online framework, the robustness
property is enforced at each iteration while mak-
ing a minimal update to the parameter distribution
in the KL sense:
(?
i+1
,?
i+1
) =
arg min
?,?
D
KL
(N (?,?) ?N (?
i
,?
i
))
s.t. Pr
w
[y
i
(w ? x
i
) ? 0] ? ? (1)
Dredze et al (2008) showed that this optimization
can be solved in closed form, yielding the updates
?
i+1
= ?
i
+ ?
i
?
i
x
i
(2)
?
i+1
=
(
?
?1
i
+ ?
i
x
i
x
T
i
)
?1
(3)
for appropriate ?
i
and ?
i
.
For prediction, they use the Bayesian rule
y? = arg max
z?{?1}
Pr
w?N (?,?)
[z (x ?w) ? 0] ,
which for binary labels is equivalent to using the
mean parameters directly, y? = sign (? ? x).
4 Multi-Class Confidence Weighted
Learning
As in the binary case, we maintain a distribution
over weight vectors w ? N (?,?). Given an in-
put instance x, a Gibbs classifier draws a weight
vector w ? N (?,?) and then predicts the label
with the maximal score, arg max
z
(w ? f(x, z)).
As in the binary case, we use this prediction rule
to define a robustness condition and corresponding
learning updates.
We generalize the robustness condition used in
Crammer et al (2008). Following the update on
round i, we require that the ith instance is correctly
labeled with probability at least ? < 1. Among the
distributions that satisfy this condition, we choose
the one that has the minimal KL distance from the
current distribution. This yields the update
(?
i+1
,?
i+1
) = (4)
arg min
?,?
D
KL
(N (?,?) ?N (?
i
,?
i
))
s.t. Pr [y
i
|x
i
,?,?] ? ? ,
where
Pr [y |x,?,?] =
Pr
w?N (?,?)
[
y = arg max
z?Y
(w ? f(x, z))
]
.
Due to the max operator in the constraint, this op-
timization is not convex when K > 2, and it does
not permit a closed form solution. We therefore
develop approximations that can be solved effi-
ciently. We define the following set of events for a
general input x:
A
r,s
(x)
def
= {w : w ? f(x, r) ? w ? f(x, s)}
B
r
(x)
def
= {w : w ? f(x, r) ? w ? f(x, s) ?s}
=
?
s 6=r
A
r,s
(x)
We assume the probability that w ? f(x, r) =
w ? f(x, s) for some s 6= r is zero, which
497
holds for non-trivial distribution parameters and
feature vectors. We rewrite the prediction y? =
arg max
r
Pr [B
r
(x)], and the constraint from
Eq. (4) becomes
Pr [B
y
i
(x)] ? ? . (5)
We focus now on approximating the event B
y
i
(x)
in terms of events A
y
i
,r
. We rely on the fact that
the level sets of Pr [A
y
i
,r
] are convex in ? and
?. This leads to convex constraints of the form
Pr [A
y
i
,r
] ? ?.
Outer Bound: Since B
r
(x) ? A
r,s
(x), it holds
trivially that Pr [B
y
i
(x)] ? ? ? Pr [A
y
i
,r
] ?
?,?r 6= y
i
. Thus we can replace the constraint
Pr [B
y
i
(x)] ? ? with Pr [A
y
i
,r
] ? ? to achieve an
outer bound. We can simultaneously apply all of
the pairwise constraints to achieve a tighter bound:
Pr [A
y
i
,r
] ? ? ?r 6= y
i
This yields a convex approximation to Eq. (4) that
may improve the objective value at the cost of
violating the constraint. In the context of learn-
ing, this means that the new parameter distribu-
tion will be close to the previous one, but may not
achieve the desired confidence on the current ex-
ample. This makes the updates more conservative.
Inner Bound: We can also consider an inner
bound. Note that B
y
i
(x)
c
= (?
r
A
y
i
,r
(x))
c
=
?
r
A
y
i
,r
(x)
c
, thus the constraint Pr [B
y
i
(x)] ? ?
is equivalent to
Pr [?
r
A
y
i
,r
(x)
c
] ? 1? ? ,
and by the union bound, this follows whenever
?
r
Pr [A
y
i
,r
(x)
c
] ? 1? ? .
We can achieve this by choosing non-negative
?
r
? 0,
?
r
?
r
= 1, and constraining
Pr [A
y
i
,r
(x)] ? 1? (1? ?) ?
r
for r 6= y
i
.
This formulation yields an inner bound on the
original constraint, guaranteeing its satisfaction
while possibly increasing the objective. In the
context of learning, this is a more aggressive up-
date, ensuring that the current example is robustly
classified even if doing so requires a larger change
to the parameter distribution.
Algorithm 1 Multi-Class CW Online Algorithm
Input: Confidence parameter ?
Feature function f(x, y) ? R
d
Initialize: ?
1
= 0 , ?
1
= I
for i = 1, 2 . . . do
Receive x
i
? X
Predict ranking of labels y?
1
, y?
2
, . . .
Receive y
i
? Y
Set ?
i+1
,?
i+1
by approximately solving
Eq. (4) using one of the following:
Single-constraint update (Sec. 5.1)
Exact many-constraint update (Sec. 5.2)
Seq. many-constraint approx. (Sec. 5.2)
Parallel many-constraint approx. (Sec. 5.2)
end for
Output: Final ? and ?
Discussion: The two approximations are quite
similar in form. Both replace the constraint
Pr [B
y
i
(x)] ? ? with one or more constraints of
the form
Pr [A
y
i
,r
(x)] ? ?
r
. (6)
To achieve an outer bound we choose ?
r
= ? for
any set of r 6= y
i
. To achieve an inner bound we
use all K ? 1 possible constraints, setting ?
r
=
1 ? (1? ?) ?
r
for suitable ?
r
. A simple choice is
?
r
= 1/(K ? 1).
In practice, ? is a learning parameter whose
value will be optimized for each task. In this case,
the outer bound (when all constraints are included)
and inner bound (when ?
r
= 1/(K ? 1)) can be
seen as equivalent, since for any fixed value of
?
(in)
for the inner bound we can choose
?
(out)
= 1?
1? ?
(in)
K ? 1
,
for the outer bound and the resulting ?
r
will be
equal. By optimizing ? we automatically tune the
approximation to achieve the best compromise be-
tween the inner and outer bounds. In the follow-
ing, we will therefore assume ?
r
= ?.
5 Online Updates
Our algorithms are online and process examples
one at a time. Pseudo-code for our approach is
given in algorithm 1. We approximate the pre-
diction step by ranking each label y according
to the score given by the mean weight vector,
? ? f(x
i
, y). Although this approach is Bayes op-
timal for binary problems (Dredze et al, 2008),
498
it is an approximation in general. We note that
more accurate inference can be performed in the
multi-class case by sampling weight vectors from
the distribution N (?,?) or selecting labels sen-
sitive to the variance of prediction; however, in
our experiments this did not improve performance
and required significantly more computation. We
therefore proceed with this simple and effective
approximation.
The update rule is given by an approximation
of the type described in Sec. 4. All that remains
is to choose the constraint set and solve the opti-
mization efficiently. We discuss several schemes
for minimizing KL divergence subject to one or
more constraints of the form Pr [A
y
i
,r
(x)] ? ?.
We start with a single constraint.
5.1 Single-Constraint Updates
The simplest approach is to select the single con-
straint Pr [A
y
i
,r
(x)] ? ? corresponding to the
highest-ranking label r 6= y
i
. This ensures that,
following the update, the true label is more likely
to be predicted than the label that was its closest
competitor. We refer to this as the k = 1 update.
Whenever we have only a single constraint, we
can reduce the optimization to one of the closed-
form CW updates used for binary classification.
Several have been proposed, based on linear ap-
proximations (Dredze et al, 2008) and exact for-
mulations (Crammer et al, 2008). For simplicity,
we use the Variance method from Dredze et al
(2008), which did well in our initial evaluations.
This method leads to the following update rules.
Note that in practice ? is projected to a diagonal
matrix as part of the update; this is necessary due
to the large number of features that we use.
?
i+1
= ?
i
+ ?
i
?
i
g
i,y
i
,r
(7)
?
i+1
=
(
?
?1
i
+ 2?
i
?g
i,y
i
,r
g
>
i,y
i
,r
)
?1
(8)
g
i,y
i
,r
= f(x
i
, y
i
)? f (x
i
, r) ? = ?
?1
(?)
The scale ?
i
is given by max(?
i
, 0), where ?
i
is
equal to
?(1 + 2?m
i
) +
?
(1 + 2?m
i
)
2
? 8?(m
i
? ?v
i
)
4?v
i
and
m
i
= ?
i
? g
i,y
i
,r
v
i
= g
>
i,y
i
,r
?
i
g
i,y
i
,r
.
These rules derive directly from Dredze et al
(2008) or Figure 1 in Crammer et al (2008); we
simply substitute y
i
= 1 and x
i
= g
i,y
i
,r
.
5.2 Many-Constraints Updates
A more accurate approximation can be obtained
by selecting multiple constraints. Analogously, we
choose the k ? K?1 constraints corresponding to
the labels r
1
, . . . , r
k
6= y
i
that achieve the highest
predicted ranks. The resulting optimization is con-
vex and can be solved by a standard Hildreth-like
algorithm (Censor & Zenios, 1997). We refer to
this update as Exact. However, Exact is expen-
sive to compute, and tends to over-fit in practice
(Sec. 6.2). We propose several approximate alter-
natives.
Sequential Update: The Hildreth algorithm it-
erates over the constraints, updating with respect
to each until convergence is reached. We approxi-
mate this solution by making only a single pass:
? Set ?
i,0
= ?
i
and ?
i,0
= ?
i
.
? For j = 1, . . . , k, set (?
i,j
,?
i,j
) to the solu-
tion of the following optimization:
min
?,?
D
KL
(
N (?,?) ?N
(
?
i,j?1
,?
i,j?1
))
s.t. Pr
[
A
y
i
,r
j
(x)
]
? ?
? Set ?
i+1
= ?
i,k
and ?
i+1
= ?
i,k
.
Parallel Update: As an alternative to the Hil-
dreth algorithm, we consider the simultaneous al-
gorithm of Iusem and Pierro (1987), which finds
an exact solution by iterating over the constraints
in parallel. As above, we approximate the exact
solution by performing only one iteration. The
process is as follows.
? For j = 1, . . . , k, set (?
i,j
,?
i,j
) to the solu-
tion of the following optimization:
min
?,?
D
KL
(N (?,?) ?N (?
i
,?
i
))
s.t. Pr
[
A
y
i
,r
j
(x)
]
? ?
? Let ? be a vector, ?
j
?0 ,
?
j
?
j
=1.
? Set ?
i+1
=
?
j
?
j
?
i,j
, ?
?1
i+1
=
?
j
?
j
?
?1
i,j
.
In practice we set ?
j
= 1/k for all j.
6 Experiments
6.1 Datasets
Following the approach of Dredze et al (2008),
we evaluate using five natural language classifica-
tion tasks over nine datasets that vary in difficulty,
size, and label/feature counts. See Table 1 for an
overview. Brief descriptions follow.
499
Task Instances Features Labels Bal.
20 News 18,828 252,115 20 Y
Amazon 7 13,580 686,724 7 Y
Amazon 3 7,000 494,481 3 Y
Enron A 3,000 13,559 10 N
Enron B 3,000 18,065 10 N
NYTD 10,000 108,671 26 N
NYTO 10,000 108,671 34 N
NYTS 10,000 114,316 20 N
Reuters 4,000 23,699 4 N
Table 1: A summary of the nine datasets, includ-
ing the number of instances, features, and labels,
and whether the numbers of examples in each class
are balanced.
Amazon Amazon product reviews. Using the
data of Dredze et al (2008), we created two do-
main classification datasets from seven product
types (apparel, books, dvds, electronics, kitchen,
music, video). Amazon 7 includes all seven prod-
uct types and Amazon 3 includes books, dvds, and
music. Feature extraction follows Blitzer et al
(2007) (bigram features and counts).
20 Newsgroups Approximately 20,000 news-
group messages, partitioned across 20 different
newsgroups.
1
This dataset is a popular choice for
binary and multi-class text classification as well as
unsupervised clustering. We represent each mes-
sage as a binary bag-of-words.
Enron Automatic sorting of emails into fold-
ers.
2
We selected two users with many email
folders and messages: farmer-d (Enron A) and
kaminski-v (Enron B). We used the ten largest
folders for each user, excluding non-archival email
folders such as ?inbox,? ?deleted items,? and ?dis-
cussion threads.? Emails were represented as bi-
nary bags-of-words with stop-words removed.
NY Times To the best of our knowledge we are
the first to evaluate machine learning methods on
the New York Times corpus. The corpus con-
tains 1.8 million articles that appeared from 1987
to 2007 (Sandhaus, 2008). In addition to being
one of the largest collections of raw news text,
it is possibly the largest collection of publicly re-
leased annotated news text, and therefore an ideal
corpus for large scale NLP tasks. Among other
annotations, each article is labeled with the desk
that produced the story (Financial, Sports, etc.)
(NYTD), the online section to which the article was
1
http://people.csail.mit.edu/jrennie/20Newsgroups/
2
http://www.cs.cmu.edu/?enron/
Task Sequential Parallel Exact
20 News 92.16 91.41 88.08
Amazon 7 77.98 78.35 77.92
Amazon 3 93.54 93.81 93.00
Enron A 82.40 81.30 77.07
Enron B 71.80 72.13 68.00
NYTD 83.43 81.43 80.92
NYTO 82.02 78.67 80.60
NYTS 52.96 54.78 51.62
Reuters 93.60 93.97 93.47
Table 2: A comparison of k = ? updates. While
the two approximations (sequential and parallel)
are roughly the same, the exact solution over-fits.
posted (NYTO), and the section in which the arti-
cle was printed (NYTS). Articles were represented
as bags-of-words with feature counts (stop-words
removed).
Reuters Over 800,000 manually categorized
newswire stories (RCV1-v2/ LYRL2004). Each
article contains one or more labels describing its
general topic, industry, and region. We performed
topic classification with the four general topics:
corporate, economic, government, and markets.
Details on document preparation and feature ex-
traction are given by Lewis et al (2004).
6.2 Evaluations
We first set out to compare the three update ap-
proaches proposed in Sec. 5.2: an exact solution
and two approximations (sequential and parallel).
Results (Table 2) show that the two approxima-
tions perform similarly. For every experiment the
CW parameter ? and the number of iterations (up
to 10) were optimized using a single randomized
iteration. However, sequential converges faster,
needing an average of 4.33 iterations compared to
7.56 for parallel across all datasets. Therefore, we
select sequential for our subsequent experiments.
The exact method performs poorly, displaying
the lowest performance on almost every dataset.
This is unsurprising given similar results for bi-
nary CW learning Dredze et al (2008), where ex-
act updates were shown to over-fit but converged
after a single iteration of training. Similarly, our
exact implementation converges after an average
of 1.25 iterations, much faster than either of the
approximations. However, this rapid convergence
appears to come at the expense of accuracy. Fig. 1
shows the accuracy on Amazon 7 test data after
each training iteration. While both sequential and
parallel improve with several iterations, exact de-
500
1 2 3 4 5Training Iterations
77.0
77.5
78.0
78.5
Tes
t Ac
cura
cy
K=1Sequential K=5Sequential K=AllParallel K=AllExact K=All
Figure 1: Accuracy on test data after each iteration
on the Amazon 7 dataset.
grades after the first iteration, suggesting that it
may over-fit to the training data. The approxima-
tions appear to smooth learning and produce better
performance in the long run.
6.3 Relaxing Many-Constraints
While enforcing many constraints may seem op-
timal, there are advantages to pruning the con-
straints as well. It may be time consuming to en-
force dozens or hundreds of constraints for tasks
with many labels. Structured prediction tasks of-
ten involve exponentially many constraints, mak-
ing pruning mandatory. Furthermore, many real
world datasets, especially in NLP, are noisy, and
enforcing too many constraints can lead to over-
fitting. Therefore, we consider the impact of re-
ducing the constraint set in terms of both reducing
run-time and improving accuracy.
We compared using all constraints (k = ?)
with using 5 constraints (k = 5) for the sequential
update method (Table 3). First, we observe that
k = 5 performs better than k =? on nearly every
dataset: fewer constraints help avoid over-fitting
and once again, simpler is better. Additionally,
k = 5 converges faster than k = ? in an average
of 2.22 iterations compared with 4.33 iterations.
Therefore, reducing the number of constraints im-
proves both speed and accuracy. In comparing
k = 5 with the further reduced k = 1 results, we
observe the latter improves on seven of the nine
methods. This surprising result suggests that CW
learning can perform well even without consid-
ering more than a single constraint per example.
However, k = 1 exceeds the performance of mul-
tiple constraints only through repeated training it-
erations. k = 5 CW learning converges faster ?
2.22 iterations compared with 6.67 for k = 1 ? a
desirable property in many resource restricted set-
tings. (In the true online setting, only a single it-
eration may be possible.) Fig. 1 plots the perfor-
mance of k = 1 and k = 5 CW on test data after
each training iteration. While k = 1 does better
in the long run, it lags behind k = 5 for several
iterations. In fact, after a single training iteration,
k = 5 outperforms k = 1 on eight out of nine
datasets. Thus, there is again a tradeoff between
faster convergence (k = 5) and increased accuracy
(k = 1). While the k = 5 update takes longer per
iteration, the time required for the approximate so-
lutions grows only linearly in the number of con-
straints. The evaluation in Fig. 1 required 3 sec-
onds for the first iteration of k = 1, 10 seconds
for k = 5 and 11 seconds for one iteration of all
7 constraints. These differences are insignificant
compared to the cost of performing multiple itera-
tions over a large dataset. We note that, while both
approximate methods took about the same amount
of time, the exact solution took over 4 minutes for
its first iteration.
Finally, we compare CW methods with sev-
eral baselines in Table 3. Online baselines in-
clude Top-1 Perceptron (Collins, 2002), Top-1
Passive-Aggressive (PA), and k-best PA (Cram-
mer & Singer, 2003; McDonald et al, 2004).
Batch algorithms include Maximum Entropy (de-
fault configuration in McCallum (2002)) and sup-
port vector machines (LibSVM (Chang & Lin,
2001) for one-against-one classification and multi-
class (MC) (Crammer & Singer, 2001)). Classifier
parameters (C for PA/SVM and maxent?s Gaus-
sian prior) and number of iterations (up to 10) for
the online methods were optimized using a sin-
gle randomized iteration. On eight of the nine
datasets, CW improves over all baselines. In gen-
eral, CW provides faster and more accurate multi-
class predictions.
7 Error and Probabilistic Output
Our focus so far has been on accuracy and speed.
However, there are other important considerations
for selecting learning algorithms. Maximum en-
tropy and other probabilistic classification algo-
rithms are sometimes favored for their probabil-
ity scores, which can be useful for integration
with other learning systems. However, practition-
501
PA CW SVM
Task Perceptron K=1 K=5 K=1 K=5 K=? 1 vs. 1 MC Maxent
20 News 81.07 88.59 88.60 ??92.90 ??92.78 ??92.16 85.18 90.33 88.94
Amazon 7 74.93 76.55 76.72 ??78.70 ??78.04 ??77.98 75.11 76.60 76.40
Amazon 3 92.26 92.47 93.29 ?94.01 ??94.29 93.54 92.83 93.60 93.60
Enron A 74.23 79.27 80.77 ??83.83 ?82.23 ?82.40 80.23 82.60 82.80
Enron B 66.30 69.93 68.90 ??73.57 ??72.27 ??71.80 65.97 71.87 69.47
NYTD 80.67 83.12 81.31 ??84.57 ?83.94 83.43 82.95 82.00 83.54
NYTO 78.47 81.93 81.22 ?82.72 ?82.55 82.02 82.13 81.01 82.53
NYTS 50.80 56.19 55.04 54.67 54.26 52.96 55.81 56.74 53.82
Reuters 92.10 93.12 93.30 93.60 93.67 93.60 92.97 93.32 93.40
Table 3: A comparison of CW learning (k = 1, 5,? with sequential updates) with several baseline
algorithms. CW learning achieves the best performance eight out of nine times. Statistical significance
(McNemar) is measured against all baselines (? indicates 0.05 and ?? 0.001) or against online baselines
(? indicates 0.05 and ?? 0.001).
0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.7528
29
30
31
32
33
entropy
error
MC CWMaxEnt
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90
200400
600800
10001200
Bin lower threshold
Number
 of exam
ples per
 bin
MaxEntMC CW
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90
2
4
68
1012
Bin lower threshold
Test err
or in bin
MaxEntMC CW
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90
0.10.2
0.30.4
0.50.6
0.70.8
Bin lower threshold
Test err
or given
 bin
MaxEntMC CW
Figure 2: First panel: Error versus prediction entropy on Enron B. As CW converges (right to left) error
and entropy are reduced. Second panel: Number of test examples per prediction probability bin. The
red bars correspond to maxent and the blue bars to CW, with increasing numbers of epochs from left
to right. Third panel: The contribution of each bin to the total test error. Fourth panel: Test error
conditioned on prediction probability.
ers have observed that maxent probabilities can
have low entropy and be unreliable for estimating
prediction confidence (Malkin & Bilmes, 2008).
Since CW also produces label probabilities ? and
does so in a conceptually distinct way ? we in-
vestigate in this section some empirical properties
of the label distributions induced by CW?s param-
eter distributions and compare them with those of
maxent.
We trained maxent and CW k = 1 classi-
fiers on the Enron B dataset, optimizing parame-
ters as before (maxent?s Gaussian prior and CW?s
?). We estimated the label distributions from our
CW classifiers after each iteration and on every
test example x by Gibbs sampling weight vec-
tors w ? N (?,?), and for each label y count-
ing the fraction of weight vectors for which y =
arg max
z
w ? f(x, z). Normalizing these counts
yields the label distributions Pr [y|x]. We denote
by y? the predicted label for a given x, and refer to
Pr [y?|x] as the prediction probability.
The leftmost panel of Fig. 2 plots each
method?s prediction error against the nor-
malized entropy of the label distribution
?
(
1
m
?
i
?
z
Pr [z|x
i
] log (Pr [z|x
i
])
)
/ log(K).
Each CW iteration (moving from right to left in
the plot) reduces both error and entropy. From our
maxent results we make the common observation
that maxent distributions have (ironically) low
entropy. In contrast, while CW accuracy exceeds
maxent after its second iteration, normalized
entropy remains high. Higher entropy suggests
a distribution over labels that is less peaked and
potentially more informative than those from
maxent. We found that the average probability
assigned to a correct prediction was 0.75 for
CW versus 0.83 for maxent and for an incorrect
prediction was 0.44 for CW versus 0.56 for
maxent.
Next, we investigate how these probabilities
relate to label accuracy. In the remaining pan-
els, we binned examples according to their pre-
diction probabilities Pr [y?|x] = max
y
Pr [y|x].
The second panel of Fig. 2 shows the numbers
of test examples with Pr [y?|x] ? [?, ? + 0.1) for
? = 0.1, 0.2 . . . 0.9. (Note that since there are 10
502
classes in this problem, we must have Pr [y?|x] ?
0.1.) The red (leftmost) bar corresponds to the
maximum entropy classifier, and the blue bars cor-
respond, from left to right, to CW after each suc-
cessive training epoch.
From the plot we observe that the maxent classi-
fier assigns prediction probability greater than 0.9
to more than 1,200 test examples out of 3,000.
Only 50 examples predicted by maxent fall in the
lowest bin, and the rest of examples are distributed
nearly uniformly across the remaining bins. The
large number of examples with very high predic-
tion probability explains the low entropy observed
for the maximum entropy classifier.
In contrast, the CW classifier shows the oppo-
site behavior after one epoch of training (the left-
most blue bar), assigning low prediction probabil-
ity (less than 0.3) to more than 1,200 examples
and prediction probability of at least 0.9 to only
100 examples. As CW makes additional passes
over the training data, its prediction confidence
increases and shifts toward more peaked distribu-
tions. After seven epochs fewer than 100 examples
have low prediction probability and almost 1,000
have high prediction probability. Nonetheless, we
note that this distribution is still less skewed than
that of the maximum entropy classifier.
Given the frequency of high probability maxent
predictions, it seems likely that many of the high
probability maxent labels will be wrong. This is
demonstrated in the third panel, which shows the
contribution of each bin to the total test error. Each
bar reflects the number of mistakes per bin divided
by the size of the complete test set (3,000). Thus,
the sum of the heights of the corresponding bars
in each bin is proportional to test error. Much of
the error of the maxent classifier comes not only
from the low-probability bins, due to their inac-
curacy, but also from the highest bin, due to its
very high population. In contrast, the CW clas-
sifiers see very little error contribution from the
high-probability bins. As training progresses, we
see again that the CW classifiers move in the direc-
tion of the maxent classifier but remain essentially
unimodal.
Finally, the rightmost panel shows the condi-
tional test error given bin identity, or the fraction
of test examples from each bin where the predic-
tion was incorrect. This is the pointwise ratio be-
tween corresponding values of the previous two
histograms. For both methods, there is a monoton-
ically decreasing trend in error as prediction prob-
ability increases; that is, the higher the value of
the prediction probability, the more likely that the
prediction it provides is correct. As CW is trained,
we see an increase in the conditional test error, yet
the overall error decreases (not shown). This sug-
gests that as CW is trained and its overall accuracy
improves, there are more examples with high pre-
diction probability, and the cost for this is a rela-
tive increase in the conditional test error per bin.
The maxent classifier produces an extremely large
number of test examples with very high prediction
probabilities, which yields relatively high condi-
tional test error. In nearly all cases, the conditional
error values for the CW classifiers are smaller than
the corresponding values for maximum entropy.
These observations suggest that CW assigns prob-
abilities more conservatively than maxent does,
and that the (fewer) high confidence predictions it
makes are of a higher quality. This is a potentially
valuable property, e.g., for system combination.
8 Conclusion
We have proposed a series of approximations for
multi-class confidence weighted learning, where
the simple analytical solutions of binary CW
learning do not apply. Our best CW method out-
performs online and batch baselines on eight of
nine NLP tasks, and is highly scalable due to the
use of a single optimization constraint. Alterna-
tively, our multi-constraint algorithms provide im-
proved performance for systems that can afford
only a single pass through the training data, as in
the true online setting. This result stands in con-
trast to previously observed behaviors in non-CW
settings (McDonald et al, 2004). Additionally, we
found improvements in both label entropy and ac-
curacy as compared to a maximum entropy clas-
sifier. We plan to extend these ideas to structured
problems with exponentially many labels and de-
velop methods that efficiently model label correla-
tions. An implementation of CW multi-class algo-
rithms is available upon request from the authors.
References
Blitzer, J., Dredze, M., & Pereira, F. (2007).
Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment
classification. Association for Computational
Linguistics (ACL).
503
Censor, Y., & Zenios, S. (1997). Parallel opti-
mization: Theory, algorithms, and applications.
Oxford University Press, New York, NY, USA.
Chang, C.-C., & Lin, C.-J. (2001). LIBSVM: a
library for support vector machines. Software
available at http://www.csie.ntu.edu.
tw/
?
cjlin/libsvm.
Collins, M. (2002). Discriminative training meth-
ods for hidden markov models: Theory and ex-
periments with perceptron algorithms. Empir-
ical Methods in Natural Language Processing
(EMNLP).
Crammer, K., Dredze, M., & Pereira, F. (2008).
Exact confidence-weighted learning. Advances
in Neural Information Processing Systems 22.
Crammer, K., & Singer, Y. (2001). On the al-
gorithmic implementation of multiclass kernel-
based vector machines. Jornal of Machine
Learning Research, 2, 265?292.
Crammer, K., & Singer, Y. (2003). Ultraconserva-
tive online algorithms for multiclass problems.
Jornal of Machine Learning Research (JMLR),
3, 951?991.
Dredze, M., Crammer, K., & Pereira, F. (2008).
Confidence-weighted linear classification. In-
ternational Conference on Machine Learning
(ICML).
Iusem, A., & Pierro, A. D. (1987). A simultaneous
iterative method for computing projections on
polyhedra. SIAM J. Control and Optimization,
25.
Lewis, D. D., Yang, Y., Rose, T. G., & Li, F.
(2004). Rcv1: A new benchmark collection for
text categorization research. Journal of Machine
Learning Research (JMLR), 5, 361?397.
Malkin, J., & Bilmes, J. (2008). Ratio semi-
definite classifiers. IEEE Int. Conf. on Acous-
tics, Speech, and Signal Processing.
McCallum, A. (2002). MALLET: A machine
learning for language toolkit. http://
mallet.cs.umass.edu.
McDonald, R., Crammer, K., & Pereira, F. (2004).
Large margin online learning algorithms for
scalable structured classification. NIPS Work-
shop on Structured Outputs.
Sandhaus, E. (2008). The new york times an-
notated corpus. Linguistic Data Consortium,
Philadelphia.
504
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 987?994, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Flexible Text Segmentation with Structured Multilabel Classification
Ryan McDonald Koby Crammer Fernando Pereira
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104
{ryantm,crammer,pereira}@cis.upenn.edu
Abstract
Many language processing tasks can be re-
duced to breaking the text into segments
with prescribed properties. Such tasks
include sentence splitting, tokenization,
named-entity extraction, and chunking.
We present a new model of text segmenta-
tion based on ideas from multilabel clas-
sification. Using this model, we can natu-
rally represent segmentation problems in-
volving overlapping and non-contiguous
segments. We evaluate the model on en-
tity extraction and noun-phrase chunking
and show that it is more accurate for over-
lapping and non-contiguous segments, but
it still performs well on simpler data sets
for which sequential tagging has been the
best method.
1 Introduction
Text segmentation is a basic task in language pro-
cessing, with applications such as tokenization, sen-
tence splitting, named-entity extraction, and chunk-
ing. Many parsers, translation systems, and extrac-
tion systems rely on such segmentations to accu-
rately process the data. Depending on the applica-
tion, segments may be tokens, phrases, or sentences.
However, in this paper we primarily focus on seg-
menting sentences into tokens.
The most common approach to text segmenta-
tion is to use finite-state sequence tagging mod-
els, in which each atomic text element (character
or token) is labeled with a tag representing its role
in a segmentation. Models of that form include
hidden Markov models (Rabiner, 1989; Bikel et
al., 1999) as well as discriminative tagging mod-
els based on maximum entropy classification (Rat-
naparkhi, 1996; McCallum et al, 2000), conditional
random fields (Lafferty et al, 2001; Sha and Pereira,
2003), and large-margin techniques (Kudo and Mat-
sumoto, 2001; Taskar et al, 2003). Tagging mod-
els are the best previous methods for text segmen-
tation. However, their purely sequential form limits
their ability to naturally handle overlapping or non-
contiguous segments.
We present here an alternative view of segmenta-
tion as structured multilabel classification. In this
view, a segmentation of a text is a set of segments,
each of which is defined by the set of text positions
that belong to the segment. Thus, a particular seg-
ment may not be a set of consecutive positions in
the text, and segments may overlap. Given a text
x = x1 ? ? ? xn, the set of possible segments, which
corresponds to the set of possible classification la-
bels, is seg(x) = {O,I}n; for y ? seg(x), yi = I
iff xi belongs to the segment. Then, our segmen-
tation task is to determine which labels are correct
segments in a given text. We have thus a structured
multilabel classification problem: each instance, a
text, may have multiple structured labels, represent-
ing each of its segments. These labels are structured
in that they do not come from a predefined set, but
instead are built from sets of choices associated to
the elements of arbitrarily long instances.
More generally, we may be interested in typed
segments, e.g. segments naming different types of
987
entities. In that case, the set of segment labels is
seg(x) = T ? {O,I}n, where T is the set of seg-
ment types. Since the extension is straightforward,
we frame the discussion in terms of untyped seg-
ments, and only discuss segment types as needed.
At first sight, it might appear that we have made
the segmentation problem intractably harder by turn-
ing it into a classification problem with a number
of labels exponential on the length of the instance.
However, we can bound the number of labels under
consideration and take advantage of the structure of
labels to find the k most likely labels efficiently. This
will allow us to exploit recent advances in online dis-
criminative methods for multilabel classification and
ranking (Crammer and Singer, 2002).
Though multilabel classification has been well
studied (Schapire and Singer, 1999; Elisseeff and
Weston, 2001), as far as we are aware, this is the
first study involving structured labels.
2 Segmentation as Tagging
The standard approach to text segmentation is to use
tagging techniques with a BIO tag set. Elements in
the input text are tagged with one of B for the be-
ginning of a contiguous segment, I for the inside
of a contiguous segment, or O for outside a seg-
ment. Thus, segments must be contiguous and non-
overlapping. For instance, consider the sentence Es-
timated volume was a light 2.4 million ounces. Fig-
ure 1a shows how this sentence would be labeled
using the BIO tag set for the problem of identifying
base NPs in text. Given a particular tagging for a
sentence, it is trivial to find all the segments, those
whose tag sequences are longest matches for the reg-
ular expression BI?. For typed segments, the BIO
tag set is easily augmented to indicate not only seg-
ment boundaries, but also the type of each segment.
Figure 1b exemplifies the tags for the task of finding
people and organizations in text.
Sequential tagging with the BIO tag set has
proven quite accurate for shallow parsing and named
entity extraction tasks (Kudo and Matsumoto, 2001;
Sha and Pereira, 2003; Tjong Kim Sang and
De Meulder, 2003). However, this approach
can only identify non-overlapping, contiguous seg-
ments. This is sufficient for some applications, and
in any case, most training data sets are annotated
without concern for overlapping or non-contiguous
segments. However, there are instances in which se-
quential labeling techniques using the BIO label set
will encounter problems.
Figure 2 shows two simple examples of segmen-
tations involving overlapping, non-contiguous seg-
ments. In both cases, it is difficult to see how a
sequential tagger could extract the segments cor-
rectly. It would be possible to grow the tag set to
represent a bounded number of overlapping, non-
contiguous segments by representing all possible
combinations of segment membership over k over-
lapping segments, but this would require an arbitrary
upper bound on k and would lead to models that gen-
eralize poorly and are expensive to train.
Dickinson and Meurers (2005) point out that, as
language processing begins to tackle problems in
free-word order languages and discourse analysis,
annotating and extracting non-contiguous segmen-
tations of text will become increasingly important.
Though we focus primarily on entity extraction and
NP chunking in this paper, there is no reason why
ideas presented here could not be extended to man-
aging other non-contiguous phenomena.
3 Structured Multilabel Classification
As outlined in Section 1, we represent segmentation
as multilabel classification, assigning to each text
the set of segments it contains. Figure 3 shows the
segments for the examples of Figure 2. Each seg-
ment is given by a O/I assignment to its words, in-
dicating which words belong to the segment.
By representing the segmentation problems as
multilabel classification, we have fundamentally
changed the objective of our learning and inference
algorithms. The sequential tagging formulation is
aimed to learn and find the best possible tagging of
a text. In multilabel classification, we train model
parameters so that correct labels ? that is, correct
segments ? receive higher score than all incorrect
ones. Likewise, inference becomes the problem of
finding the set of correct labels for a text, that is, the
set of correct segments.
We now describe the learning problem using the
decision-theoretic multilabel classification and rank-
ing framework of Crammer and Singer (2002) and
Crammer (2005) as our starting point. In Sec-
988
a. Estimated volume was a light 2.4 million ounces .
B I O B I I I I O
b. Bill Clinton and Microsoft founder Bill Gates met today for 20 minutes .
B-PER I-PER O B-ORG O B-PER I-PER O O O O O O
Figure 1: Sequential labeling formulation of text segmentation using the BIO label set. a) NP-chunking
tasks. b) Named-entity extraction task.
a) Today, Bill and Hilary Clinton traveled to Canada.
- Person: Bill Clinton
- Person: Hilary Clinton
b) ... purified bovine P450 11 beta / 18 / 19 - hydroxylase was ...
- Enzyme: P450 11 beta-hydroxylase
- Enzyme: P450 18-hydroxylase
- Enzyme: P450 19-hydroxilase
Figure 2: Examples of overlapping and non-contiguous text segmentations.
tion 3.2, we describe a polynomial-time inference
algorithm for finding up to k correct segments.
3.1 Training Multilabel Classifiers
Our model is based on a linear score s(x,y; w) for
each segment y of text x, defined as
s(x,y; w) = w ? f(x,y)
where f(x,y) is a feature vector representation of
the sentence-segment pair, and w is a vector of
feature weights. For a given text x, act(x) ?
seg(x) denotes the set of correct segments for x, and
bestk(x; w) denotes the set of k segments with high-
est score relative to the weight vector w. For learn-
ing, we use a training set T = {(xt, act(xt))}|T |t=1 of
texts labeled with the correct segmentation.
We will discuss later the design of f(x,y) and an
efficient algorithm for finding the k highest scoring
segments (where k is sufficiently large to include
all correct segments). In this section, we present a
method for learning a weight vector w that seeks to
score correct segments above all incorrect segments.
Crammer and Singer (2002), extended by Cram-
mer (2005), provide online learning algorithms for
multilabel classification and ranking that take one
instance at a time, construct a set of scoring con-
straints for the instance, and adjust the weight vec-
tor to satisfy the constraints. The constraints en-
force a margin between the scores of correct labels
and those of incorrect labels. The benefits of large-
margin learning are best known from SVMs (Cris-
tianini and Shawe-Taylor, 2000; Scho?lkopf and
Training data: T = {(xt, act(xt))}|T |t=1
1. w(0) = 0; i = 0
2. for n : 1..N
3. for t : 1..|T |
4. w(i+1) = arg minw
?
?
?
w ? w(i)
?
?
?
2
s.t. s(xt, y; w) ? s(xt, y?; w) + 1
?y ? act(xt), ?y? ? bestk(xt; w(i)) ? act(xt)
6. i = i + 1
7. w = w(N?|T |)
Figure 4: A simplified version of the multilabel
learning algorithm of Crammer and Singer (2002).
Smola, 2002), and are analyzed in detail by Cram-
mer (2005) for online multilabel classification.
For segmentation, the number of possible labels
(segments) is exponential on the length of the text.
We make the problem tractable by including only the
margin constraints between correct segments and at
most k highest scoring incorrect segments. Figure 4
sketches an online learning algorithm for multilabel
classification based on the work of Crammer (2005).
In the algorithm, w(i+1) is the projection of w(i) onto
the set of weight vectors such that the scores of cor-
rect segments are separated by a margin of at least
1 from the scores of incorrect segments among the
k top-scoring segments. This update is conservative
in that there is no weight change if the constraint set
is already satisfied or empty; if some constraints are
not satisfied, we make the smallest weight change
that satisfies the constraints. Since, the objective is
quadratic in w and the constraints are linear, the op-
timization problem can be solved by Hildreth?s al-
989
a) Today , Bill and Hilary Clinton traveled to Canada .
O O I O O I O O O O
O O O O I I O O O O
b) ... purified bovine P450 11 beta / 18 / 19 - hydroxylase was ...
O O I I I O O O O I I O
O O I O O O I O O I I O
O O I O O O O O I I I O
Figure 3: Correct segments for two examples.
gorithm (Censor and Zenios, 1997).
Using standard arguments for linear classifiers
(add constant feature, rescale weights) and the fact
that all the correct scores in line 4 of Figure 4 are re-
quired to be above all the incorrect scores in the top
k, that line can be replaced by
w(i+1) = arg minw
?
?w ? w(i)
?
?
2
s.t. s(xt,y; w) ? 1 and s(xt,y?; w) ? ?1
?y ? act(xt),?y? ? bestk(xt; w(i)) ? act(xt)
If v is the number of correct segments for x,
this transformation replaces O(kv) constraints with
O(k + v) constraints: segment scores are compared
to a single positive or negative threshold rather then
to each other. At test time, we find the segments
with positive score by finding the k highest scoring
segments and discarding those with a negative score.
3.2 Inference
During learning and at test time we require a method
for finding the k highest scoring segments. At test
time, we predict as correct all the segments with pos-
itive score in the top k. In this section we give an
algorithm that calculates this precisely.
For inference, tagging models typically use the
Viterbi algorithm (Rabiner, 1989). The algorithm is
given by the following standard recurrences:
S[i, t] = maxt? s(t?, t, i) + S[i ? 1, t?]
B[i, t] = arg maxt? s(t?, t, i) + S[i ? 1, t?]
with appropriate initial conditions, where s(t?, t, i)
is the score for going from tag t? at i ? 1 to tag t
at i. The dynamic programming table S[i, t] stores
the score of the best tag sequence ending at posi-
tion i with tag t, and B[i, t] is a back-pointer to the
previous tag in the best sequence ending at i with
t, which allows us to reconstruct the best sequence.
The Viterbi algorithm has easy k-best extensions.
We could find the k highest scoring segments us-
ing Viterbi. However, for the case of non-contiguous
segments, we would like to represent higher-order
dependencies that are difficult to model in Viterbi. In
particular, in Figure 3b we definitely want a feature
bridging the gap between Bill and Clinton, which
could not be captured with a standard first-order
model. But moving to higher-order models would
require adding dimensions to the dynamic program-
ming tables S and B, with corresponding multipliers
to the complexity of inference.
To represent dependencies between non-
contiguous text positions, for any given segment
y = y1 ? ? ? yn, let i(y) = 0i1 ? ? ? im(n + 1) be the
increasing sequence of indices ij such that yij = I,
padded for convenience with the dummy first index
0 and last index n + 1. Also for convenience, set
x0 = -s- and xn+1 = -e- for fixed start and
end markers. Then, we restrict ourselves to feature
functions f(x,y) that factor relative to the input as
f(x,y) =
|i(y)|
?
j=1
g(i(y)j?1, i(y)j) (1)
where i(y)j is the jth integer in i(y) and g is a fea-
ture function depending on arbitrary properties of
the input relative to the indices i(y)j?1 and i(y)j .
Applying (1) to the segment Bill Clinton in Fig-
ure 3, its score would be
w ? [g(0, 3) + g(3, 6) + g(6, 11)]
This feature representation allows us to include de-
pendencies between non-contiguous segment posi-
tions, as well as dependencies on any properties of
the input, including properties of skipped positions.
We now define the following dynamic program
S[i] = maxj<i S[j] + w ? g(j, i)
B[i] = arg maxj<i S[j] + w ? g(j, i)
990
These recurrences compute the score S[i] of the best
partial segment ending at i as the sum of the max-
imum score of a partial segment ending at position
j < i, and the score of skipping from j to i. The
back-pointer table B allows us to reconstruct the se-
quence of positions included in the segment.
Clearly, this program requires O(n2) time for a
text of length n. Furthermore we can easily augment
this algorithm in the standard fashion to find the k
best segments, and multiple segment types, result-
ing in a runtime of O(n2kT ), where T is the number
of types. O(n2kT ) is not ideal, but is still practical
since in this work we are segmenting sentences. If
we can bound the largest gap in any non-contiguous
segment by a constant g  n, then the runtime can
be improved to O(ngkT ). This runtime does not
compare favorably to the standard Viterbi algorithm
that runs in O(nT 2), especially for large k. How-
ever, we found that for even large k we could still
train large models in a matter of hours and test on
unseen data in a few minutes.
3.2.1 Restrictions
Often a segmentation task or data set will restrict
particular kinds of segments. For instance, it may be
the case that a data set does not have any overlap-
ping or non-contiguous segments. Embedded seg-
mentations ? those in which one segment?s tokens
are a subset of another?s ? is also a phenomenon that
sometimes does not occur.
It is easy to restrict the inference algorithm to dis-
allow such segments if they are unnecessary. For ex-
ample, if two segments overlap or are embedded, the
inference algorithm can just return the highest scor-
ing one. Or it can simply ignore all non-contiguous
segments if it is known that they do not occur in the
data. In Section 4 we will augment the inference
algorithm accordingly for each data set.
3.3 Feature Representation
We now discuss the design of the feature function
for two consecutive segment positions g(j, i), where
j < i. We build individual binary-valued features
from predicates over the input, for instance, the iden-
tities of words in the sentence at particular posi-
tions relative to i and j. The selection of predicates
varies by task, and we provide specific predicate sets
in Section 4 for various data sets. In this section,
we use for illustration word-pair identity predicates
such as xj = Bill & xi = Clinton.
For sequential tagging models, predicates are
combined with the set of states (or tags) to create
a feature representation. For our model, we define
the following possible states:
start ? j = 0
end ? i = n + 1
next ? j = i ? 1
skip ? j < i ? 1
For example, the following features would be on for
g(0, 3)1 and g(3, 6), respectively, in Figure 3a:
xj = -s- & xi = Bill & start
xj = Bill & xi = Clinton & skip
These features indicate a predicate?s role in the seg-
ment: at the beginning, at the end, over contiguous
segment words or skipping over some words. All
features can be augmented to indicate specific seg-
ment types for multi-type segmentation tasks. No
matter what the task, we always add predicates that
represent ranges of the distance i?j, as well as what
words or part-of-speech tags occur between the two
words. For instance, g(3, 6) might contain
word-in-between= and & skip
These features are designed to identify common
characteristics of non-contiguous segments such
as the presence of conjunctions or punctuation in
skipped portions. Although we have considered only
binary features here, the model in principle allows
arbitrary real-valued feature.
3.4 Summary
We presented a method for text segmentation that
equates the problem to structured multilabel classi-
fication where each label corresponds to a segment.
We showed that learning and inference can be man-
aged tractably in the formulation by efficiently find-
ing the k highest scoring segments through a dy-
namic programming algorithm that factors the struc-
ture of each segment. The only concern is that k
must be large enough to include all correct segments,
1Note that ?skip? is not on for g(0, 3) even though j < i?1.
Start and end states override other states.
991
which we will discuss further in Section 4. This
method naturally models all possible segmentations
including those with overlapping or non-contiguous
segments. Out approach can be seen as multilabel
variant of the work of McDonald et al (2004), which
creates a set of constraints to separate the score of
the single correct output from the k highest scoring
outputs with an appropriate large margin.
4 Experiments
We now describe a set of experiments on named en-
tity and base NP segmentation. For these experi-
ments, we set k = n, where n is the length of the
sentence. This represents a reasonable upper bound
on the number of entities or chunks in a sentence and
results in a time complexity of O(n3T ).
We compare our methods with both the averaged
perceptron (Collins, 2002) and conditional random
fields (Lafferty et al, 2001) using identical predicate
sets. Though all systems use identical predicates, the
actual features of the systems are different due to
the fundamental differences between the multilabel
classification and sequential tagging models.
4.1 Standard data sets
Our first experiments are standard named entity and
base NP data sets with no overlapping, embedded or
non-contiguous segments. These experiments will
show that, for simple segmentations, our model is
competitive with sequential tagging models.
For the named entity experiments we used the
CoNLL 2003 (Tjong Kim Sang and De Meulder,
2003) data with people, organizations, locations and
miscellaneous entities. We used standard predicates
based on word, POS and orthographic information
over a previous to next word window. For the NP
chunking experiments we used the standard CoNLL
2000 data set (Kudo and Matsumoto, 2001; Sha and
Pereira, 2003) using the predicate set defined by Sha
and Pereira (2003).
The first three rows of Table 1 compare the mul-
tilabel classification approach to standard sequen-
tial classifiers. As one might expect, the perfor-
mance of the multilabel classification method is be-
low that of the sequential tagging methods. This is
because those methods model contiguous segments
well without the need for thresholds or k-best infer-
ence. In addition, the multilabel method shows sig-
nificantly higher precision then recall. One possible
reason for this is that during the course of learning,
the model will see many segments that are nearly
correct, e.g., segments that overlap correct segments
and differ by a single token. As a result, the model
learns to score all segments containing even a small
amount of negative evidence as invalid in order to
ensure that these nearly correct segments have a suf-
ficiently low score.
One way to alleviate this problem is to restrict the
inference algorithm to not return any overlapping,
non-contiguous or embedded segmentations as dis-
cussed in Section 3.2.1, since this data set does not
contain segments of this kind. This way, the learning
stage only updates the parameters when a nearly cor-
rect segment actually out scores the correct one. The
results of this system are shown in row 4 of Table 1.
We can see that this change did lead to a more bal-
anced precision/recall, however it is clear that more
investigation is required.
4.2 Chemical substance extraction
The second set of experiments involves extract-
ing chemical substance names from MEDLINE ab-
stracts that relevant to the inhibition of the enzyme
CYP450 (PennBioIE, 2005). We focus on abstracts
that have at least one overlapping or non-contiguous
annotation. This data set contains 6164 annotated
chemical substances, including 6% that are both
overlapping and non-contiguous. Figure 3b is an
example from the corpus. We use identical predi-
cates to the named entity experiments in Section 4.1.
Though the data does contain overlapping and non-
contiguous segments, it does not contain embedded
segments. Results are shown in Table 2 using 10-
fold cross validation. The sequential tagging models
were trained using only sentences with no overlap-
ping or non-contiguous entities. We found this pro-
vided the best performance. Row 4 of Table 2 shows
the multilabel approach with the inference algorithm
restricted to not allow embedded segments.
We can see that our method does significantly bet-
ter on this data set (up to a 26% reduction in er-
ror). It is also apparent that the model is picking up
some overlapping and non-contiguous entities (see
Table 2). However, the models performance on these
kinds of entities is lower than overall performance.
992
a. Named-Entity Extraction b. NP-chunking
Precision Recall F-measure Precision Recall F-measure
Avg. Perceptron 82.46 83.14 82.80 94.22 93.88 94.05
CRFs 83.36 83.57 83.47 94.57 94.00 94.29
Multilabel 92.47 74.19 82.33 94.65 92.28 93.45
Multilabel with Restrictions 91.08 76.68 83.26 94.10 93.70 93.90
Table 1: Results for named-entity extraction and NP-chunking on data sets with only non-overlapping and
contiguous segments annotated.
Chem Substance Extraction - A Chem Substance Extraction - B
Precision Recall F-measure Precision Recall F-measure
Avg. Perceptron 82.98 79.40 81.15 1.0 0.0 0.0
CRFs 85.85 79.06 82.31 1.0 0.0 0.0
Multilabel 88.24 80.84 84.38 62.56 33.67 43.78
Multilabel with Restrictions 88.55 84.59 86.53 72.58 45.92 56.25
Table 2: Results for chemical substance extraction. Table A is for all entities in the data set and Table B is
only for those entities that are overlapping and non-contiguous.
4.3 Tuning Precision and Recall
The learning algorithm in Section 3.1 seeks a sep-
arator through the origin, though, our experimental
results suggest that this tends to favor precision at
the expense of recall. However, at test time we can
use a separation threshold different from zero. This
parameter allows us to trade off precision against re-
call, and could be tuned on held-out data.
Figure 5 plots precision, recall and f-measure
against the threshold for the basic multilabel model
on the chemical substance, NP chunking and person
entity extraction data sets. These plots clearly show
what is expected: higher thresholds give higher pre-
cision, and lower thresholds give higher recall. In
these data sets at least, a zero threshold is almost
always near optimal, though sometimes we would
benefit from a slightly lower threshold.
5 Discussion
We have presented a method for text segmentation
that is base on discriminatively learning structured
multilabel classifications. The benefits include
? Competitive performance with sequential tag-
ging models.
? Flexible modeling of complex segmentations,
including overlapping, embedded and non-
contiguous segments.
? Adjustable precision-recall trade off.
However, there is a computation cost for our models.
For a text of length n, training and testing require
O(n3T ) time, where T is the number of segment
types. Fortunately, this still results in training times
on the order of hours.
Our approach is related to the work of Bockhorst
and Craven (2004). In this work, a conditional ran-
dom field model is trained to allow for overlapping
segments with an O(n2) inference algorithm. The
model is applied to biological sequence modeling
with promising results. However, our approaches
differ in two major respects. First, their model is
probabilistic, and trained to maximize segmenta-
tion likelihood, while our model is trained to max-
imize margin. Second, our method allows for non-
contiguous segments, at the cost of a slower O(n3)
inference algorithm.
In further work, the classification threshold
should also be learned to achieve the desired balance
between precision and recall. It would also be useful
to investigate methods for combining these models
with standard sequential tagging models to get top
performance on simple segmentations as well as on
overlapping or non-contiguous ones.
A broader area of investigation are other problems
in language processing that can benefit from struc-
tured multilabel classification, e.g., ambiguities in
language often result in multiple acceptable parses
for sentences. It may be possible to extend the al-
gorithms presented here to learn to distinguish all
acceptable parses from unacceptable ones instead of
just finding a single parse when many are valid.
993
?1 ?0.8 ?0.6 ?0.4 ?0.2 0 0.2 0.4 0.6 0.8 1
0.4
0.5
0.6
0.7
0.8
0.9
1
CHEM
?1 ?0.8 ?0.6 ?0.4 ?0.2 0 0.2 0.4 0.6 0.8 1
0.7
0.75
0.8
0.85
0.9
0.95
1
NP
?1 ?0.8 ?0.6 ?0.4 ?0.2 0 0.2 0.4 0.6 0.8 1
0.4
0.5
0.6
0.7
0.8
0.9
1
PER
Figure 5: Precision (squares), Recall (circles) and F-measure (line) plotted against threshold values. CHEM:
chemical substance extraction, NP: noun-phrase chunking, and PER: person name extraction.
Acknowledgments
We thank the members of the Penn BioIE project
for the development of the CYP450 corpus that we
used for our experiments. In particular, Seth Kulick
answered many questions about the data. This work
has been supported by the NSF ITR grant 0205448.
References
D.M. Bikel, R. Schwartz, and R.M. Weischedel. 1999.
An algorithm that learns what?s in a name. Machine
Learning Journal Special Issue on Natural Language
Learning, 34(1/3):221?231.
J. Bockhorst and M. Craven. 2004. Markov networks for
detecting overlapping elements in sequence data. In
Proc. NIPS.
Y. Censor and S.A. Zenios. 1997. Parallel optimization :
theory, algorithms, and applications. Oxford Univer-
sity Press.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In Proc. EMNLP.
K. Crammer and Y. Singer. 2002. A new family of online
algorithms for category ranking. In Proc SIGIR.
K. Crammer. 2005. Online Learning for Complex Cat-
egorial Problems. Ph.D. thesis, Hebrew University of
Jerusalem. to appear.
N. Cristianini and J. Shawe-Taylor. 2000. An Introduc-
tion to Support Vector Machines. Cambridge Univer-
sity Press.
M. Dickinson and W.D. Meurers. 2005. Detecting errors
in discontinuous structural annotation. In Proc. ACL.
A. Elisseeff and J. Weston. 2001. A kernel method for
multi-labeled classification. In Proc. NIPS.
T. Kudo and Y. Matsumoto. 2001. Chunking with sup-
port vector machines. In Proc. NAACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ICML.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy Markov models for information extrac-
tion and segmentation. In Proceedings of ICML.
R. McDonald, K. Crammer, and F. Pereira. 2004. Large
margin online learning algorithms for scalable struc-
tured classication. In NIPS Workshop on Structured
Outputs.
PennBioIE. 2005. Mining The Bibliome Project.
http://bioie.ldc.upenn.edu/.
L. R. Rabiner. 1989. A tutorial on hidden Markov mod-
els and selected applications in speech recognition.
Proceedings of the IEEE, 77(2):257?285, February.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. EMNLP.
R. E. Schapire and Y. Singer. 1999. Improved boosting
algorithms using confidence-rated predictions. Ma-
chine Learning, 37(3):1?40.
B. Scho?lkopf and A. J. Smola. 2002. Learning with Ker-
nels: Support Vector Machines, Regularization, Opti-
mization and Beyond. MIT Press.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proc. HLT-NAACL.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Proc. NIPS.
E. F. Tjong Kim Sang and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proceedings
of CoNLL-2003.
http://www.cnts.ua.ac.be/conll2003/ner.
994
Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 61?65,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Loss-Sensitive Discriminative Training of Machine Transliteration Models
Kedar Bellare
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA 01003, USA
kedarb@cs.umass.edu
Koby Crammer
Department of Computer Science
University of Pennsylvania
Philadelphia, PA 19104, USA
crammer@cis.upenn.edu
Dayne Freitag
SRI International
San Diego, CA 92130, USA
dayne.freitag@sri.com
Abstract
In machine transliteration we transcribe a
name across languages while maintaining its
phonetic information. In this paper, we
present a novel sequence transduction algo-
rithm for the problem of machine transliter-
ation. Our model is discriminatively trained
by the MIRA algorithm, which improves the
traditional Perceptron training in three ways:
(1) It allows us to consider k-best translitera-
tions instead of the best one. (2) It is trained
based on the ranking of these transliterations
according to user-specified loss function (Lev-
enshtein edit distance). (3) It enables the user
to tune a built-in parameter to cope with noisy
non-separable data during training. On an
Arabic-English name transliteration task, our
model achieves a relative error reduction of
2.2% over a perceptron-based model with sim-
ilar features, and an error reduction of 7.2%
over a statistical machine translation model
with more complex features.
1 Introduction and Related Work
Proper names and other technical terms are fre-
quently encountered in natural language text. Both
machine translation (Knight and Graehl, 1997) and
cross-language information retrieval (Jeong et al,
1999; Virga and Khudanpur, 2003; Abdul-Jaleel and
Larkey, 2003) can benefit by explicitly translating
such words from one language into another. This
approach is decidedly better than treating them uni-
formly as out-of-vocabulary tokens. The goal of ma-
chine transliteration is to translate words between
alphabets of different languages such that they are
phonetically equivalent.
Given a source language sequence f =
f1f2 . . . fm from an alphabet F , we want to produce
a target language sequence e = e1e2 . . . en in the al-
phabet E such that it maximizes some score function
s(e, f),
e = argmax
e?
s(e?, f).
Virga and Khudanpur (2003) model this scoring
function using a separate translation and language
model, that is, s(e, f) = Pr(f |e)Pr(e). In con-
strast, Al-Onaizan and Knight (2002) directly model
the translation probability Pr(e|f) using a log-linear
combination of several individually trained phrase
and character-based models. Others have treated
transliteration as a phrase-based transduction (Sherif
and Kondrak, 2007). All these approaches are adap-
tations of statistical models for machine transla-
tion (Brown et al, 1994). In general, the parame-
ters of the scoring function in such approaches are
trained generatively and do not utilize complex fea-
tures of the input sequence pairs.
Recently, there has been interest in applying
discriminatively-trained sequence alignment mod-
els to many real-world problems. McCallum et al
(2005) train a conditional random field model to
discriminate between matching and non-matching
string pairs treating alignments as latent. Learning
accurate alignments in this model requires finding
?close? non-match pairs which can be a challenge.
A similar conditional latent-variable model has been
applied to the task of lemmatization and genera-
tion of morphological forms (Dreyer et al, 2008).
61
Zelenko and Aone (2006) model transliteration as
a structured prediction problem where the letter ei
is predicted using local and global features derived
from e1e2 . . . ei?1 and f . Bergsma and Kondrak
(2007) address cognate identification by training a
SVM classification model using phrase-based fea-
tures obtained from a Levenshtein alignment. Both
these models do not learn alignments that is needed
to obtain high performance on transliteration tasks.
Freitag and Khadivi (2007) describe a discrimina-
tively trained sequence alignment model based on
averaged perceptron, which is closely related to the
method proposed in this paper.
Our approach improves over previous directions
in two ways. First, our system produces better k-best
transliterations than related approaches by training
on multiple hypotheses ranked according to a user-
specified loss function (Levenshtein edit distance).
Hence, our method achieves a 19.2% error reduction
in 5-best performance over a baseline only trained
with 1-best transliterations. This is especially help-
ful when machine transliteration is part of a larger
machine translation or information retrieval pipeline
since additional sentence context can be used to
choose the best among top-K transliterations. Sec-
ond, our training procedure accounts for noise and
non-separability in the data. Therefore, our translit-
eration system would work well in cases where per-
son names were misspelled or in cases in which a
single name had many reasonable translations in the
foreign language.
The training algorithm we propose in this pa-
per is based on the K-best MIRA algorithm which
has been used earlier in structured prediction prob-
lems (McDonald et al, 2005a; McDonald et al,
2005b). Our results demonstrate a significant im-
provement in accuracy of 7.2% over a statistical
machine translation (SMT) system (Zens et al,
2005) and of 2.2% over a perceptron-based edit
model (Freitag and Khadivi, 2007).
2 Sequence Alignment Model
Let e = e1e2 . . . en and f = f1f2 . . . fm be se-
quences from the target alhabet E and source al-
phabet F respectively. Let a = a1a2 . . . al be a se-
quence of alignment operations needed to convert f
into e. Each alignment operation either appends a
letter to the end of the source sequence, the target
sequence or both sequences. Hence, it is a member
of the cross-product ak ? E?{?}?F?{?}\{(?, ?)},
where ? is the null character symbol. Let ak1 =
a1a2 . . . ak denote the sequence of first k alignment
operations. Similarly ek1 and fk1 are prefixes of e and
f of length k.
We define the scoring function between a word
and its transliteration to be the a maximum over all
possible alignment sequences a,
s(e, f) = max
a
s(a, e, f) ,
where the score of a specific alignment a between
two words is given by a linear relation,
s(a, e, f) = w ? ?(a, e, f),
for a parameter vector w and a feature vec-
tor ?(a, e, f). Furthermore, let ?(a, e, f) =?l
k=1 ?(ak, e, i, f , j) be the sum of feature vec-
tors associated with individual alignment operations.
Here i, j are positions in sequences e, f after per-
forming operations ak1 . For fixed sequences e and f
the function s(e, f) can be efficiently computed us-
ing a dynamic programming algorithm,
s(ei1, f j1 ) =
max
?
??
??
s(ei?11 , f j1 ) + w ? ?(?ei, ??, e, i, f , j)
s(ei1, f j?11 ) + w ? ?(??, fj?, e, i, f , j)
s(ei?11 , f j?11 ) + w ? ?(?ei, fj?, e, i, f , j).
(1)
Given a source sequence f computing the best scor-
ing target sequence e = argmaxe? s(e?, f) among
all possible sequences E? requires a beam search
procedure (Freitag and Khadivi, 2007). This pro-
cedure can also be used to produce K-best target
sequences {e?1, e?2, . . . , e?K} such that s(e?1, f) ?
s(e?2, f) ? . . . ? s(e?K , f).
In this paper, we employ the same features as
those used by Freitag and Khadivi (2007). All lo-
cal feature functions ?(ak, e, i, f , j) are conjunc-
tions of the alignment operation ak and forward or
backward-looking character m-grams in sequences
e and f at positions i and j respectively. For
the source sequence f both forward and backward-
looking m-gram features are included. We restrict
the m-gram features in our target sequence e to only
62
be backward-looking since we do not have access to
forward-looking m-grams during beam-search. An
order M model is one that uses m-gram features
where m = 0, 1, . . . M .
Our training algorithm takes as input a data set
D of source-target transliteration pairs and outputs
a parameter vector u. The algorithm pseudo-code
appears in Fig. (1). In the algorithm, the function
L(e?, e) defines a loss incurred by predicting e? in-
stead of e. In most structured prediction problems,
the targets are of equal length and in such cases the
Hamming loss function can be used. However, in
our case the targets may differ in terms of length and
thus we use the Levenshtein edit distance (Leven-
shtein, 1966) with unit costs for insertions, deletions
and substitutions. Since the targets are both in the
same alphabet E this loss function is well-defined.
The user also supplies three paramters: (1) T - the
number of training iterations (2) K - the number
of best target hypotheses used (3) C - a complex-
ity parameter. A low C is useful if the data is non-
separable and noisy.
The final parameter vector u returned by the al-
gorithm is the average of the intermediate parameter
vectors produced during training. We find that av-
eraging helps to improve performance. At test time,
we use the beam search procedure to produce K-
best hypotheses using the parameter vector u.
3 Experimental Results
We apply our model to the real-world Arabic-
English name transliteration task on a data set of
10,084 Arabic names from the LDC. The data set
consists of Arabic names in an ASCII-based alpha-
bet and its English rendering. Table 1 shows a
few examples of Arabic-English pairs in our data
set. We use the same training/development/testing
(8084/1000/1000) set as the one used in a previ-
ous benchmark study (Freitag and Khadivi, 2007).
The development and testing data were obtained
by randomly removing entries from the training
data. The absence of short vowels (e.g. ?a? in
?NB?I, nab?i?), doubled consonants (e.g. ?ww?
in ?FWAL, fawwal?) and other diacritics in Arabic
make the transliteration a hard problem. Therefore,
it is hard to achieve perfect accuracy on this data set.
For training, we set K = 20 best hypotheses and
Input parameters
Training Data D
Complexity parameter C > 0
Number of epochs T
Initialize w0 = 0 (zero vector) ; ? = 0 ; u = 0
Repeat T times:
For Each (e, f) ? D :
1. a = argmaxa?w? ? ?(a?, e, f) (Find best scoring
alignment between e and f using dynamic program-
ming)
2. Generate a list of K-best target hypotheses
{e?1, e?2, . . . , e?K} given the current parameters w? .
Let the corresponding alignments for the targets be
{a?1,a?2, . . . ,a?K}.
3. Set w?+1 to be the solution of :
minw 12 ||w ?w? ||2 + C
?K
k=1 ?k
subject to (for k = 1 . . .K) :
w ? (?(a, e, f)? ?(a?k, e?k, f)) ? L(e, e?k)? ?k
?k ? 0
4. u? u+ w?+1
5. ? ? ? + 1
Output Scoring function s(a, e, f) = u ? ?(a, e, f)
Figure 1: The k-best MIRA algorithm for discriminative
learning of transliterations.
Arabic English
NB?I nab?i
HNBLI hanbali
FRIFI furayfi
MLKIAN malikian
BI;ANT bizant
FWAL fawwal
OALDAWI khalidawi
BUWUI battuti
H;? hazzah
Table 1: Examples of Arabic names in the ASCII alpha-
bet and their English transliterations.
C = 1.0 and run the algorithm for T = 10 epochs.
To evaluate our algorithm, we generate 1-best (or 5-
best) hypotheses using the beam search procedure
and measure accuracy as the percentage of instances
in which the target sequence e is one of the 1-best
(or 5-best) targets. The input features are based on
character m-grams for m = 1, 2, 3. Unlike previ-
63
ous generative transliteration models, no additional
language model feature is used.
We compare our model against a state-of-the-art
statistical machine translation (SMT) system (Zens
et al, 2005) and an averaged perceptron edit
model (PTEM) with identical features (Freitag and
Khadivi, 2007). The SMT system directly models
the posterior probability Pr(e|f) using a log-linear
combination of several sub-models: a character-
based phrase translation model, a character-based
lexicon model, a character penalty and a phrase
penalty. In the PTEM model, the update rule only
considers the best target sequence and modifies the
parameters w?+1 = w? + ?(a, e, f) ? ?(a?, e?, f)
if the score s(e?, f) ? s(e, f).
Model (train+dev) 1-best 5-best
SMT 0.528 0.824
PTEM 0.552 0.803
MIRA 0.562 0.841
Table 2: The 1-best and 5-best accuracy of differ-
ent models on the Arabic-English transliteration task.
At 95% confidence level, MIRA/PTEM outperform the
SMT model in 1-best accuracy and MIRA outperforms
PTEM/SMT in 5-best accuracy.
Table 2 shows the 1-best and 5-best accuracy of
each model trained on the combined train+dev data
set. All the models are evaluated on the same test
set. Both MIRA and PTEM algorithms outperform
the SMT model in terms of 1-best accuracy. The
differences in accuracy are significant at 95% con-
fidence level, using the bootstrapping method for
hypothesis testing. The difference in 1-best per-
formance of MIRA and PTEM is not significant.
At 5-best, the MIRA model outperforms both SMT
and PTEM model. We conjecture that using the
problem-specific Levenshtein loss function helps fil-
ter bad target sequences from the K-best outputs
during training.
In a second experiment we studied the effect
of changing C on the performance of the algo-
rithm. We ran the algorithm with the above set-
tings, except varying the value of the complexity
parameter to one of 7 values in the range C =
0.00001, 0.0001, . . . , 0.1, 1.0, training only using
the train set, and evaluating the resulting model on
Model (train) 1-best 5-best
C = 1.0 0.545? 0.832
C = 0.5 0.548? 0.83
C = 0.2 0.549? 0.834
C = 0.01 0.545 0.852?
C = 0.001 0.518 0.843
C = 0.0001 0.482 0.798
C = 0.00001 0.476 0.798
Table 3: The effect of varying model parameter C on 1,5-
best accuracy on the test set. All the models are trained
with Levenshtein loss and 20-best targets. The super-
script ? indicates the models that achieved the greatest
performance on the dev set for a particular column.
the test set. The results are summarized in Table 3.
The entry marked with a star ? indicates the model
that achieved the best performance on the dev set for
a particular choice of evaluation measure (1-best or
5-best). We find that changing C does have an effect
on model performance. As the value of C decreases,
the performance at lower ranks improves: C = 0.01
is good for 5-best accuracy and C = 0.001 for 20-
best accuracy (not in table). As C is further reduced,
a greater number of iterations are needed to con-
verge. In our model, where the alignments are not
observed but inferred during training, we find that
making small incremental updates makes our algo-
rithm more robust. Indeed, setting C = 0.01 and
training on the train+dev set improves 5-best per-
formance of our model from 0.841 to 0.861. Hence,
the choice of C is important.
4 Conclusions and Future Work
We have shown a significant improvement in accu-
racy over state-of-the-art transliteration models by
taking into consideration the ranking of multiple
hypotheses (top-K) by Levenshtein distance, and
making the training algorithm robust to noisy non-
separable data. Our model does consistently well
at high (K = 1) and low ranks (K = 5), and can
therefore be used in isolation or in a pipelined sys-
tem (e.g. machine translation or cross-language in-
formation retrieval) to achieve better performance.
In a pipeline system, more features of names around
proper nouns and previous mentions of the name can
be used to improve scoring of K-best outputs.
64
In our experiments, the Levenshtein loss function
uses only unit costs for edit operations and is not
specifically tuned towards our application. In fu-
ture work, we may imagine penalizing insertions
and deletions higher than substitutions and other
non-uniform schemes for better transliteration per-
formance. Our K-best framework can also be easily
extended to cases where one name has multiple for-
eign translations that are equally likely.
References
Nasreen Abdul-Jaleel and Leah S. Larkey. 2003. Statis-
tical transliteration for English-Arabic cross language
information retrieval. In CIKM ?03, pages 139?146,
New York, NY, USA. ACM.
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in arabic text. In Proceed-
ings of the ACL-02 Workshop on Computational Ap-
proaches to Semitic Languages, pages 1?13.
Shane Bergsma and Greg Kondrak. 2007. Alignment-
based discriminative string similarity. In ACL, pages
656?663, June.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1994. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Markus Dreyer, Jason Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions with
finite-state methods. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1080?1089, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
Dayne Freitag and Shahram Khadivi. 2007. A sequence
alignment model based on the averaged perceptron. In
EMNLP-CoNLL, pages 238?247.
K.S. Jeong, S. H. Myaeng, J.S. Lee, and K.-S.
Choi. 1999. Automatic identification and back-
transliteration of foreign words for information re-
trieval. Information Processing and Management,
35:523?540.
Kevin Knight and Jonathan Graehl. 1997. Machine
transliteration. In Philip R. Cohen and Wolfgang
Wahlster, editors, Proceedings of the Thirty-Fifth An-
nual Meeting of the Association for Computational
Linguistics and Eighth Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 128?135, Somerset, New Jersey. Associa-
tion for Computational Linguistics.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707?710.
Andrew McCallum, Kedar Bellare, and Fernando Pereira.
2005. A conditional random field for discriminatively-
trained finite-state string edit distance. In UAI.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005a. Flexible text segmentation with structured
multilabel classification. In HLT-EMNLP, pages 987?
994, Vancouver, BC, Canada, October. Association for
Computational Linguistics.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005b. Online large-margin training of dependency
parsers. In ACL, pages 91?98, Ann Arbor, Michigan,
June.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In ACL, pages 944?951, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Paola Virga and Sanjeev Khudanpur. 2003. Translit-
eration of proper names in cross-lingual information
retrieval. In Proceedings of the ACL 2003 workshop
on Multilingual and Mixed-language Named Entity
Recognition, pages 57?64, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Dmitry Zelenko and Chinatsu Aone. 2006. Discrimi-
native methods for transliteration. In EMNLP, pages
612?617, Sydney, Australia, July. Association for
Computational Linguistics.
R. Zens, O. Bender, S. Hasan, S. Khadivi, E. Matusov,
J. Xu, Y. Zhang, and H. Ney. 2005. The RWTH
Phrase-based Statistical Machine Translation System.
In Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), Pittsburgh, PA,
USA.
65
Proceedings of the 43rd Annual Meeting of the ACL, pages 91?98,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Online Large-Margin Training of Dependency Parsers
Ryan McDonald Koby Crammer Fernando Pereira
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA
{ryantm,crammer,pereira}@cis.upenn.edu
Abstract
We present an effective training al-
gorithm for linearly-scored dependency
parsers that implements online large-
margin multi-class training (Crammer and
Singer, 2003; Crammer et al, 2003) on
top of efficient parsing techniques for de-
pendency trees (Eisner, 1996). The trained
parsers achieve a competitive dependency
accuracy for both English and Czech with
no language specific enhancements.
1 Introduction
Research on training parsers from annotated data
has for the most part focused on models and train-
ing algorithms for phrase structure parsing. The
best phrase-structure parsing models represent gen-
eratively the joint probability P (x,y) of sentence
x having the structure y (Collins, 1999; Charniak,
2000). Generative parsing models are very conve-
nient because training consists of computing proba-
bility estimates from counts of parsing events in the
training set. However, generative models make com-
plicated and poorly justified independence assump-
tions and estimations, so we might expect better per-
formance from discriminatively trained models, as
has been shown for other tasks like document classi-
fication (Joachims, 2002) and shallow parsing (Sha
and Pereira, 2003). Ratnaparkhi?s conditional max-
imum entropy model (Ratnaparkhi, 1999), trained
to maximize conditional likelihood P (y|x) of the
training data, performed nearly as well as generative
models of the same vintage even though it scores
parsing decisions in isolation and thus may suffer
from the label bias problem (Lafferty et al, 2001).
Discriminatively trained parsers that score entire
trees for a given sentence have only recently been
investigated (Riezler et al, 2002; Clark and Curran,
2004; Collins and Roark, 2004; Taskar et al, 2004).
The most likely reason for this is that discrimina-
tive training requires repeatedly reparsing the train-
ing corpus with the current model to determine the
parameter updates that will improve the training cri-
terion. The reparsing cost is already quite high
for simple context-free models with O(n3) parsing
complexity, but it becomes prohibitive for lexical-
ized grammars with O(n5) parsing complexity.
Dependency trees are an alternative syntactic rep-
resentation with a long history (Hudson, 1984). De-
pendency trees capture important aspects of func-
tional relationships between words and have been
shown to be useful in many applications includ-
ing relation extraction (Culotta and Sorensen, 2004),
paraphrase acquisition (Shinyama et al, 2002) and
machine translation (Ding and Palmer, 2005). Yet,
they can be parsed in O(n3) time (Eisner, 1996).
Therefore, dependency parsing is a potential ?sweet
spot? that deserves investigation. We focus here on
projective dependency trees in which a word is the
parent of all of its arguments, and dependencies are
non-crossing with respect to word order (see Fig-
ure 1). However, there are cases where crossing
dependencies may occur, as is the case for Czech
(Hajic?, 1998). Edges in a dependency tree may be
typed (for instance to indicate grammatical func-
tion). Though we focus on the simpler non-typed
91
root John hit the ball with the bat
Figure 1: An example dependency tree.
case, all algorithms are easily extendible to typed
structures.
The following work on dependency parsing is
most relevant to our research. Eisner (1996) gave
a generative model with a cubic parsing algorithm
based on an edge factorization of trees. Yamada and
Matsumoto (2003) trained support vector machines
(SVM) to make parsing decisions in a shift-reduce
dependency parser. As in Ratnaparkhi?s parser, the
classifiers are trained on individual decisions rather
than on the overall quality of the parse. Nivre and
Scholz (2004) developed a history-based learning
model. Their parser uses a hybrid bottom-up/top-
down linear-time heuristic parser and the ability to
label edges with semantic types. The accuracy of
their parser is lower than that of Yamada and Mat-
sumoto (2003).
We present a new approach to training depen-
dency parsers, based on the online large-margin
learning algorithms of Crammer and Singer (2003)
and Crammer et al (2003). Unlike the SVM
parser of Yamada and Matsumoto (2003) and Ratna-
parkhi?s parser, our parsers are trained to maximize
the accuracy of the overall tree.
Our approach is related to those of Collins and
Roark (2004) and Taskar et al (2004) for phrase
structure parsing. Collins and Roark (2004) pre-
sented a linear parsing model trained with an aver-
aged perceptron algorithm. However, to use parse
features with sufficient history, their parsing algo-
rithm must prune heuristically most of the possible
parses. Taskar et al (2004) formulate the parsing
problem in the large-margin structured classification
setting (Taskar et al, 2003), but are limited to pars-
ing sentences of 15 words or less due to computation
time. Though these approaches represent good first
steps towards discriminatively-trained parsers, they
have not yet been able to display the benefits of dis-
criminative training that have been seen in named-
entity extraction and shallow parsing.
Besides simplicity, our method is efficient and ac-
curate, as we demonstrate experimentally on English
and Czech treebank data.
2 System Description
2.1 Definitions and Background
In what follows, the generic sentence is denoted by
x (possibly subscripted); the ith word of x is de-
noted by xi. The generic dependency tree is denoted
by y. If y is a dependency tree for sentence x, we
write (i, j) ? y to indicate that there is a directed
edge from word xi to word xj in the tree, that is, xi
is the parent of xj . T = {(xt,yt)}Tt=1 denotes the
training data.
We follow the edge based factorization method of
Eisner (1996) and define the score of a dependency
tree as the sum of the score of all edges in the tree,
s(x,y) =
?
(i,j)?y
s(i, j) =
?
(i,j)?y
w ? f(i, j)
where f(i, j) is a high-dimensional binary feature
representation of the edge from xi to xj . For exam-
ple, in the dependency tree of Figure 1, the following
feature would have a value of 1:
f(i, j) =
{
1 if xi=?hit? and xj=?ball?
0 otherwise.
In general, any real-valued feature may be used, but
we use binary features for simplicity. The feature
weights in the weight vector w are the parameters
that will be learned during training. Our training al-
gorithms are iterative. We denote by w(i) the weight
vector after the ith training iteration.
Finally we define dt(x) as the set of possi-
ble dependency trees for the input sentence x and
bestk(x; w) as the set of k dependency trees in dt(x)
that are given the highest scores by weight vector w,
with ties resolved by an arbitrary but fixed rule.
Three basic questions must be answered for mod-
els of this form: how to find the dependency tree y
with highest score for sentence x; how to learn an
appropriate weight vector w from the training data;
and finally, what feature representation f(i, j) should
be used. The following sections address each of
these questions.
2.2 Parsing Algorithm
Given a feature representation for edges and a
weight vector w, we seek the dependency tree or
92
h1 h1 h2 h2
?
s h1 h1 r r+1 h2 h2 t
h1
h1 h2 h2
?
s h1 h1 h2 h2 t
h1
h1
s h1 h1 t
Figure 2: O(n3) algorithm of Eisner (1996), needs to keep 3 indices at any given stage.
trees that maximize the score function, s(x,y). The
primary difficulty is that for a given sentence of
length n there are exponentially many possible de-
pendency trees. Using a slightly modified version of
a lexicalized CKY chart parsing algorithm, it is pos-
sible to generate and represent these sentences in a
forest that is O(n5) in size and takes O(n5) time to
create.
Eisner (1996) made the observation that if the
head of each chart item is on the left or right periph-
ery, then it is possible to parse in O(n3). The idea is
to parse the left and right dependents of a word inde-
pendently and combine them at a later stage. This re-
moves the need for the additional head indices of the
O(n5) algorithm and requires only two additional
binary variables that specify the direction of the item
(either gathering left dependents or gathering right
dependents) and whether an item is complete (avail-
able to gather more dependents). Figure 2 shows
the algorithm schematically. As with normal CKY
parsing, larger elements are created bottom-up from
pairs of smaller elements.
Eisner showed that his algorithm is sufficient for
both searching the space of dependency parses and,
with slight modification, finding the highest scoring
tree y for a given sentence x under the edge fac-
torization assumption. Eisner and Satta (1999) give
a cubic algorithm for lexicalized phrase structures.
However, it only works for a limited class of lan-
guages in which tree spines are regular. Further-
more, there is a large grammar constant, which is
typically in the thousands for treebank parsers.
2.3 Online Learning
Figure 3 gives pseudo-code for the generic online
learning setting. A single training instance is con-
sidered on each iteration, and parameters updated
by applying an algorithm-specific update rule to the
instance under consideration. The algorithm in Fig-
ure 3 returns an averaged weight vector: an auxil-
iary weight vector v is maintained that accumulates
Training data: T = {(xt, yt)}Tt=1
1. w0 = 0; v = 0; i = 0
2. for n : 1..N
3. for t : 1..T
4. w(i+1) = update w(i) according to instance (xt, yt)
5. v = v + w(i+1)
6. i = i + 1
7. w = v/(N ? T )
Figure 3: Generic online learning algorithm.
the values of w after each iteration, and the returned
weight vector is the average of all the weight vec-
tors throughout training. Averaging has been shown
to help reduce overfitting (Collins, 2002).
2.3.1 MIRA
Crammer and Singer (2001) developed a natural
method for large-margin multi-class classification,
which was later extended by Taskar et al (2003) to
structured classification:
min ?w?
s.t. s(x,y) ? s(x,y?) ? L(y,y?)
?(x,y) ? T , y? ? dt(x)
where L(y,y?) is a real-valued loss for the tree y?
relative to the correct tree y. We define the loss of
a dependency tree as the number of words that have
the incorrect parent. Thus, the largest loss a depen-
dency tree can have is the length of the sentence.
Informally, this update looks to create a margin
between the correct dependency tree and each incor-
rect dependency tree at least as large as the loss of
the incorrect tree. The more errors a tree has, the
farther away its score will be from the score of the
correct tree. In order to avoid a blow-up in the norm
of the weight vector we minimize it subject to con-
straints that enforce the desired margin between the
correct and incorrect trees1.
1The constraints may be unsatisfiable, in which case we can
relax them with slack variables as in SVM training.
93
The Margin Infused Relaxed Algorithm
(MIRA) (Crammer and Singer, 2003; Cram-
mer et al, 2003) employs this optimization directly
within the online framework. On each update,
MIRA attempts to keep the norm of the change to
the parameter vector as small as possible, subject to
correctly classifying the instance under considera-
tion with a margin at least as large as the loss of the
incorrect classifications. This can be formalized by
substituting the following update into line 4 of the
generic online algorithm,
min
?
?w(i+1) ? w(i)
?
?
s.t. s(xt,yt) ? s(xt,y?) ? L(yt,y?)
?y? ? dt(xt)
(1)
This is a standard quadratic programming prob-
lem that can be easily solved using Hildreth?s al-
gorithm (Censor and Zenios, 1997). Crammer and
Singer (2003) and Crammer et al (2003) provide
an analysis of both the online generalization error
and convergence properties of MIRA. In equation
(1), s(x,y) is calculated with respect to the weight
vector after optimization, w(i+1).
To apply MIRA to dependency parsing, we can
simply see parsing as a multi-class classification
problem in which each dependency tree is one of
many possible classes for a sentence. However, that
interpretation fails computationally because a gen-
eral sentence has exponentially many possible de-
pendency trees and thus exponentially many margin
constraints.
To circumvent this problem we make the assump-
tion that the constraints that matter for large margin
optimization are those involving the incorrect trees
y? with the highest scores s(x,y?). The resulting
optimization made by MIRA (see Figure 3, line 4)
would then be:
min
?
?w(i+1) ? w(i)
?
?
s.t. s(xt,yt) ? s(xt,y?) ? L(yt,y?)
?y? ? bestk(xt; w(i))
reducing the number of constraints to the constant k.
We tested various values of k on a development data
set and found that small values of k are sufficient to
achieve close to best performance, justifying our as-
sumption. In fact, as k grew we began to observe a
slight degradation of performance, indicating some
overfitting to the training data. All the experiments
presented here use k = 5. The Eisner (1996) algo-
rithm can be modified to find the k-best trees while
only adding an additional O(k log k) factor to the
runtime (Huang and Chiang, 2005).
A more common approach is to factor the struc-
ture of the output space to yield a polynomial set of
local constraints (Taskar et al, 2003; Taskar et al,
2004). One such factorization for dependency trees
is
min
?
?w(i+1) ? w(i)
?
?
s.t. s(l, j) ? s(k, j) ? 1
?(l, j) ? yt, (k, j) /? yt
It is trivial to show that if these O(n2) constraints
are satisfied, then so are those in (1). We imple-
mented this model, but found that the required train-
ing time was much larger than the k-best formu-
lation and typically did not improve performance.
Furthermore, the k-best formulation is more flexi-
ble with respect to the loss function since it does not
assume the loss function can be factored into a sum
of terms for each dependency.
2.4 Feature Set
Finally, we need a suitable feature representation
f(i, j) for each dependency. The basic features in
our model are outlined in Table 1a and b. All fea-
tures are conjoined with the direction of attachment
as well as the distance between the two words being
attached. These features represent a system of back-
off from very specific features over words and part-
of-speech tags to less sparse features over just part-
of-speech tags. These features are added for both the
entire words as well as the 5-gram prefix if the word
is longer than 5 characters.
Using just features over the parent-child node
pairs in the tree was not enough for high accuracy,
because all attachment decisions were made outside
of the context in which the words occurred. To solve
this problem, we added two other types of features,
which can be seen in Table 1c. Features of the first
type look at words that occur between a child and
its parent. These features take the form of a POS
trigram: the POS of the parent, of the child, and of
a word in between, for all words linearly between
the parent and the child. This feature was particu-
larly helpful for nouns identifying their parent, since
94
a)
Basic Uni-gram Features
p-word, p-pos
p-word
p-pos
c-word, c-pos
c-word
c-pos
b)
Basic Big-ram Features
p-word, p-pos, c-word, c-pos
p-pos, c-word, c-pos
p-word, c-word, c-pos
p-word, p-pos, c-pos
p-word, p-pos, c-word
p-word, c-word
p-pos, c-pos
c)
In Between POS Features
p-pos, b-pos, c-pos
Surrounding Word POS Features
p-pos, p-pos+1, c-pos-1, c-pos
p-pos-1, p-pos, c-pos-1, c-pos
p-pos, p-pos+1, c-pos, c-pos+1
p-pos-1, p-pos, c-pos, c-pos+1
Table 1: Features used by system. p-word: word of parent node in dependency tree. c-word: word of child
node. p-pos: POS of parent node. c-pos: POS of child node. p-pos+1: POS to the right of parent in sentence.
p-pos-1: POS to the left of parent. c-pos+1: POS to the right of child. c-pos-1: POS to the left of child.
b-pos: POS of a word in between parent and child nodes.
it would typically rule out situations when a noun
attached to another noun with a verb in between,
which is a very uncommon phenomenon.
The second type of feature provides the local con-
text of the attachment, that is, the words before and
after the parent-child pair. This feature took the form
of a POS 4-gram: The POS of the parent, child,
word before/after parent and word before/after child.
The system also used back-off features to various tri-
grams where one of the local context POS tags was
removed. Adding these two features resulted in a
large improvement in performance and brought the
system to state-of-the-art accuracy.
2.5 System Summary
Besides performance (see Section 3), the approach
to dependency parsing we described has several
other advantages. The system is very general and
contains no language specific enhancements. In fact,
the results we report for English and Czech use iden-
tical features, though are obviously trained on differ-
ent data. The online learning algorithms themselves
are intuitive and easy to implement.
The efficient O(n3) parsing algorithm of Eisner
allows the system to search the entire space of de-
pendency trees while parsing thousands of sentences
in a few minutes, which is crucial for discriminative
training. We compare the speed of our model to a
standard lexicalized phrase structure parser in Sec-
tion 3.1 and show a significant improvement in pars-
ing times on the testing data.
The major limiting factor of the system is its re-
striction to features over single dependency attach-
ments. Often, when determining the next depen-
dent for a word, it would be useful to know previ-
ous attachment decisions and incorporate these into
the features. It is fairly straightforward to modify
the parsing algorithm to store previous attachments.
However, any modification would result in an as-
ymptotic increase in parsing complexity.
3 Experiments
We tested our methods experimentally on the Eng-
lish Penn Treebank (Marcus et al, 1993) and on the
Czech Prague Dependency Treebank (Hajic?, 1998).
All experiments were run on a dual 64-bit AMD
Opteron 2.4GHz processor.
To create dependency structures from the Penn
Treebank, we used the extraction rules of Yamada
and Matsumoto (2003), which are an approximation
to the lexicalization rules of Collins (1999). We split
the data into three parts: sections 02-21 for train-
ing, section 22 for development and section 23 for
evaluation. Currently the system has 6, 998, 447 fea-
tures. Each instance only uses a tiny fraction of these
features making sparse vector calculations possible.
Our system assumes POS tags as input and uses the
tagger of Ratnaparkhi (1996) to provide tags for the
development and evaluation sets.
Table 2 shows the performance of the systems
that were compared. Y&M2003 is the SVM-shift-
reduce parsing model of Yamada and Matsumoto
(2003), N&S2004 is the memory-based learner of
Nivre and Scholz (2004) and MIRA is the the sys-
tem we have described. We also implemented an av-
eraged perceptron system (Collins, 2002) (another
online learning algorithm) for comparison. This ta-
ble compares only pure dependency parsers that do
95
English Czech
Accuracy Root Complete Accuracy Root Complete
Y&M2003 90.3 91.6 38.4 - - -
N&S2004 87.3 84.3 30.4 - - -
Avg. Perceptron 90.6 94.0 36.5 82.9 88.0 30.3
MIRA 90.9 94.2 37.5 83.3 88.6 31.3
Table 2: Dependency parsing results for English and Czech. Accuracy is the number of words that correctly
identified their parent in the tree. Root is the number of trees in which the root word was correctly identified.
For Czech this is f-measure since a sentence may have multiple roots. Complete is the number of sentences
for which the entire dependency tree was correct.
not exploit phrase structure. We ensured that the
gold standard dependencies of all systems compared
were identical.
Table 2 shows that the model described here per-
forms as well or better than previous comparable
systems, including that of Yamada and Matsumoto
(2003). Their method has the potential advantage
that SVM batch training takes into account all of
the constraints from all training instances in the op-
timization, whereas online training only considers
constraints from one instance at a time. However,
they are fundamentally limited by their approximate
search algorithm. In contrast, our system searches
the entire space of dependency trees and most likely
benefits greatly from this. This difference is am-
plified when looking at the percentage of trees that
correctly identify the root word. The models that
search the entire space will not suffer from bad ap-
proximations made early in the search and thus are
more likely to identify the correct root, whereas the
approximate algorithms are prone to error propaga-
tion, which culminates with attachment decisions at
the top of the tree. When comparing the two online
learning models, it can be seen that MIRA outper-
forms the averaged perceptron method. This differ-
ence is statistically significant, p < 0.005 (McNe-
mar test on head selection accuracy).
In our Czech experiments, we used the depen-
dency trees annotated in the Prague Treebank, and
the predefined training, development and evaluation
sections of this data. The number of sentences in
this data set is nearly twice that of the English tree-
bank, leading to a very large number of features ?
13, 450, 672. But again, each instance uses just a
handful of these features. For POS tags we used the
automatically generated tags in the data set. Though
we made no language specific model changes, we
did need to make some data specific changes. In par-
ticular, we used the method of Collins et al (1999) to
simplify part-of-speech tags since the rich tags used
by Czech would have led to a large but rarely seen
set of POS features.
The model based on MIRA also performs well on
Czech, again slightly outperforming averaged per-
ceptron. Unfortunately, we do not know of any other
parsing systems tested on the same data set. The
Czech parser of Collins et al (1999) was run on a
different data set and most other dependency parsers
are evaluated using English. Learning a model from
the Czech training data is somewhat problematic
since it contains some crossing dependencies which
cannot be parsed by the Eisner algorithm. One trick
is to rearrange the words in the training set so that
all trees are nested. This at least allows the train-
ing algorithm to obtain reasonably low error on the
training set. We found that this did improve perfor-
mance slightly to 83.6% accuracy.
3.1 Lexicalized Phrase Structure Parsers
It is well known that dependency trees extracted
from lexicalized phrase structure parsers (Collins,
1999; Charniak, 2000) typically are more accurate
than those produced by pure dependency parsers
(Yamada and Matsumoto, 2003). We compared
our system to the Bikel re-implementation of the
Collins parser (Bikel, 2004; Collins, 1999) trained
with the same head rules of our system. There are
two ways to extract dependencies from lexicalized
phrase structure. The first is to use the automatically
generated dependencies that are explicit in the lex-
icalization of the trees, we call this system Collins-
auto. The second is to take just the phrase structure
output of the parser and run the automatic head rules
over it to extract the dependencies, we call this sys-
96
English
Accuracy Root Complete Complexity Time
Collins-auto 88.2 92.3 36.1 O(n5) 98m 21s
Collins-rules 91.4 95.1 42.6 O(n5) 98m 21s
MIRA-Normal 90.9 94.2 37.5 O(n3) 5m 52s
MIRA-Collins 92.2 95.8 42.9 O(n5) 105m 08s
Table 3: Results comparing our system to those based on the Collins parser. Complexity represents the
computational complexity of each parser and Time the CPU time to parse sec. 23 of the Penn Treebank.
tem Collins-rules. Table 3 shows the results compar-
ing our system, MIRA-Normal, to the Collins parser
for English. All systems are implemented in Java
and run on the same machine.
Interestingly, the dependencies that are automati-
cally produced by the Collins parser are worse than
those extracted statically using the head rules. Ar-
guably, this displays the artificialness of English de-
pendency parsing using dependencies automatically
extracted from treebank phrase-structure trees. Our
system falls in-between, better than the automati-
cally generated dependency trees and worse than the
head-rule extracted trees.
Since the dependencies returned from our system
are better than those actually learnt by the Collins
parser, one could argue that our model is actu-
ally learning to parse dependencies more accurately.
However, phrase structure parsers are built to max-
imize the accuracy of the phrase structure and use
lexicalization as just an additional source of infor-
mation. Thus it is not too surprising that the de-
pendencies output by the Collins parser are not as
accurate as our system, which is trained and built to
maximize accuracy on dependency trees. In com-
plexity and run-time, our system is a huge improve-
ment over the Collins parser.
The final system in Table 3 takes the output of
Collins-rules and adds a feature to MIRA-Normal
that indicates for given edge, whether the Collins
parser believed this dependency actually exists, we
call this system MIRA-Collins. This is a well known
discriminative training trick ? using the sugges-
tions of a generative system to influence decisions.
This system can essentially be considered a correc-
tor of the Collins parser and represents a significant
improvement over it. However, there is an added
complexity with such a model as it requires the out-
put of the O(n5) Collins parser.
k=1 k=2 k=5 k=10 k=20
Accuracy 90.73 90.82 90.88 90.92 90.91
Train Time 183m 235m 627m 1372m 2491m
Table 4: Evaluation of k-best MIRA approximation.
3.2 k-best MIRA Approximation
One question that can be asked is how justifiable is
the k-best MIRA approximation. Table 4 indicates
the accuracy on testing and the time it took to train
models with k = 1, 2, 5, 10, 20 for the English data
set. Even though the parsing algorithm is propor-
tional to O(k log k), empirically, the training times
scale linearly with k. Peak performance is achieved
very early with a slight degradation around k=20.
The most likely reason for this phenomenon is that
the model is overfitting by ensuring that even un-
likely trees are separated from the correct tree pro-
portional to their loss.
4 Summary
We described a successful new method for training
dependency parsers. We use simple linear parsing
models trained with margin-sensitive online training
algorithms, achieving state-of-the-art performance
with relatively modest training times and no need
for pruning heuristics. We evaluated the system on
both English and Czech data to display state-of-the-
art performance without any language specific en-
hancements. Furthermore, the model can be aug-
mented to include features over lexicalized phrase
structure parsing decisions to increase dependency
accuracy over those parsers.
We plan on extending our parser in two ways.
First, we would add labels to dependencies to rep-
resent grammatical roles. Those labels are very im-
portant for using parser output in tasks like infor-
mation extraction or machine translation. Second,
97
we are looking at model extensions to allow non-
projective dependencies, which occur in languages
such as Czech, German and Dutch.
Acknowledgments: We thank Jan Hajic? for an-
swering queries on the Prague treebank, and Joakim
Nivre for providing the Yamada and Matsumoto
(2003) head rules for English that allowed for a di-
rect comparison with our systems. This work was
supported by NSF ITR grants 0205456, 0205448,
and 0428193.
References
D.M. Bikel. 2004. Intricacies of Collins parsing model.
Computational Linguistics.
Y. Censor and S.A. Zenios. 1997. Parallel optimization :
theory, algorithms, and applications. Oxford Univer-
sity Press.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. NAACL.
S. Clark and J.R. Curran. 2004. Parsing the WSJ using
CCG and log-linear models. In Proc. ACL.
M. Collins and B. Roark. 2004. Incremental parsing with
the perceptron algorithm. In Proc. ACL.
M. Collins, J. Hajic?, L. Ramshaw, and C. Tillmann. 1999.
A statistical parser for Czech. In Proc. ACL.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In Proc. EMNLP.
K. Crammer and Y. Singer. 2001. On the algorithmic
implementation of multiclass kernel based vector ma-
chines. JMLR.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. JMLR.
K. Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer.
2003. Online passive aggressive algorithms. In Proc.
NIPS.
A. Culotta and J. Sorensen. 2004. Dependency tree ker-
nels for relation extraction. In Proc. ACL.
Y. Ding and M. Palmer. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mars. In Proc. ACL.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexi-
cal context-free grammars and head-automaton gram-
mars. In Proc. ACL.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. COLING.
J. Hajic?. 1998. Building a syntactically annotated cor-
pus: The Prague dependency treebank. Issues of Va-
lency and Meaning.
L. Huang and D. Chiang. 2005. Better k-best parsing.
Technical Report MS-CIS-05-08, University of Penn-
sylvania.
Richard Hudson. 1984. Word Grammar. Blackwell.
T. Joachims. 2002. Learning to Classify Text using Sup-
port Vector Machines. Kluwer.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ICML.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of english: the penn
treebank. Computational Linguistics.
J. Nivre and M. Scholz. 2004. Deterministic dependency
parsing of english text. In Proc. COLING.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. EMNLP.
A. Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning.
S. Riezler, T. King, R. Kaplan, R. Crouch, J. Maxwell,
and M. Johnson. 2002. Parsing the Wall Street Journal
using a lexical-functional grammar and discriminative
estimation techniques. In Proc. ACL.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proc. HLT-NAACL.
Y. Shinyama, S. Sekine, K. Sudo, and R. Grishman.
2002. Automatic paraphrase acquisition from news ar-
ticles. In Proc. HLT.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Proc. NIPS.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proc. EMNLP.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
IWPT.
98
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 233?236,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Active Learning with Confidence
Mark Dredze and Koby Crammer
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104
{mdredze,crammer}@cis.upenn.edu
Abstract
Active learning is a machine learning ap-
proach to achieving high-accuracy with a
small amount of labels by letting the learn-
ing algorithm choose instances to be labeled.
Most of previous approaches based on dis-
criminative learning use the margin for choos-
ing instances. We present a method for in-
corporating confidence into the margin by us-
ing a newly introduced online learning algo-
rithm and show empirically that confidence
improves active learning.
1 Introduction
Successful applications of supervised machine
learning to natural language rely on quality labeled
training data, but annotation can be costly, slow and
difficult. One popular solution is Active Learning,
which maximizes learning accuracy while minimiz-
ing labeling efforts. In active learning, the learning
algorithm itself selects unlabeled examples for anno-
tation. A variety of techniques have been proposed
for selecting examples that maximize system perfor-
mance as compared to selecting instances randomly.
Two learning methodologies dominate NLP ap-
plications: probabilistic methods ? naive Bayes,
logistic regression ? and margin methods ? sup-
port vector machines and passive-aggressive. Active
learning for probabilistic methods often uses uncer-
tainty sampling: label the example with the lowest
probability prediction (the most ?uncertain?) (Lewis
and Gale, 1994). The equivalent technique for mar-
gin learning associates the margin with prediction
certainty: label the example with the lowest margin
(Tong and Koller, 2001). Common intuition equates
large margins with high prediction confidence.
However, confidence and margin are two distinct
properties. For example, an instance may receive
a large margin based on a single feature which has
been updated only a small number of times. Another
example may receive a small margin, but its features
have been learned from a large number of examples.
While the first example has a larger margin it has
low confidence compared to the second. Both the
margin value and confidence should be considered
in choosing which example to label.
We present active learning with confidence us-
ing a recently introduced online learning algo-
rithm called Confidence-Weighted linear classifica-
tion. The classifier assigns labels according to a
Gaussian distribution over margin values instead of
a single value, which arises from parameter confi-
dence (variance). The variance of this distribution
represents the confidence in the mean (margin). We
then employ this distribution for a new active learn-
ing criteria, which in turn could improve other mar-
gin based active learning techniques. Additionally,
we favor the use of an online method since online
methods have achieved good NLP performance and
are fast to train ? an important property for inter-
active learning. Experimental validation on a num-
ber of datasets shows that active learning with con-
fidence can improve standard methods.
2 Confidence-Weighted Linear Classifiers
Common online learning algorithms, popular in
many NLP tasks, are not designed to deal with
the particularities of natural language data. Fea-
233
ture representations have very high dimension and
most features are observed on a small fraction of in-
stances. Confidence-weighted (CW) linear classifi-
cation (Dredze et al, 2008), a new online algorithm,
maintains a probabilistic measure of parameter con-
fidence leading to a measure of prediction confi-
dence, potentially useful for active learning. We
summarize CW learning to familiarize the reader.
Parameter confidence is formalized with a distri-
bution over weight vectors, specifically a Gaussian
distribution with mean ? ? RN and diagonal co-
variance ? ? RN?N . The values ?j and ?j,j repre-
sent knowledge of and confidence in the parameter
for feature j. The smaller ?j,j , the more confidence
we have in the mean parameter value ?j .
A model predicts the label with the highest prob-
ability, maxy?{?1} Prw?N (?,?) [y(w ? x) ? 0] .
The Gaussian distribution over parameter vectors w
induces a univariate Gaussian distribution over the
unsigned-margin M = w ? x parameterized by ?,
? and the instance x: M ? N (M,V ), where the
mean is M = ? ? x and the variance V = x>?x.
CW is an online algorithm inspired by the Passive
Aggressive (PA) update (Crammer et al, 2006) ?
which ensures a positive margin while minimizing
parameter change. CW replaces the Euclidean dis-
tance used in the PA update with the KL divergence
over Gaussian distributions. It also replaces the min-
imal margin constraint with a minimal probability
constraint: with some given probability ? ? (0.5, 1]
a drawn classifier will assign the correct label. This
strategy yields the following objective solved on
each round of learning:
(?i+1,?i+1) = min DKL (N (?,?) ?N (?i,?i))
s.t. Pr [yi (? ? xi) ? 0] ? ? ,
where (?i,?i) are the parameters on round i and(
?i+1,?i+1
)
are the new parameters after update.
The constraint ensures that the resulting parameters
will correctly classify xi with probability at least ?.
For convenience we write ? = ??1 (?), where ? is
the cumulative function of the normal distribution.
The optimization problem above is not convex, but
a closed form approximation of its solution has the
following additive form: ?i+1 = ?i+?iyi?ixi and
??1i+1 = ?
?1
i + 2?i?xix
>
i for,
?i=
?(1+2?Mi)+
?
(1+2?Mi)
2?8? (Mi??Vi)
4?Vi
.
Each update changes the feature weights ?, and in-
creases confidence (variance ? always decreases).
3 Active Learning with Confidence
We consider pool based active learning. An active
learning algorithm is given a pool of unlabeled in-
stances U = {xi}ni=1, a learning algorithm A and a
set of labeled examples initially set to be L = ? . On
each round the active learner uses its selection crite-
ria to return a single instance xi to be labeled by an
annotator with yi ? {?1,+1} (for binary classifica-
tion). The instance and label are added to the labeled
set L ? L ? {(xi, yi)} and passed to the learning
algorithm A, which in turn generates a new model.
At the end of labeling the algorithm returns a classi-
fier trained on the final labeled set. Effective active
learning minimizes prediction error and the number
of labeled examples.
Most active learners for margin based algorithms
rely on the magnitude of the margin. Tong and
Koller (2001) motivate this approach by consider-
ing the half-space representation of the hypothesis
space for learning. They suggest three margin based
active learning methods: Simple margin, MaxMin
margin, and Ratio margin. In Simple margin, the al-
gorithm predicts an unsigned margin M for each in-
stance in U and returns for labeling the instance with
the smallest margin. The intuition is that instances
for which the classifier is uncertain (small margin)
provide the most information for learning. Active
learning based on PA algorithms runs in a similar
fashion but full SVM retraining on every round is
replaced with a single PA update using the new la-
beled example, greatly increasing learning speed.
Maintaining a distribution over prediction func-
tions makes the CW algorithm attractive for ac-
tive learning. Instead of using a geometrical
quantity (?margin?), it use a probabilistic quan-
tity and picks the example whose label is pre-
dicted with the lowest probability. Formally,
the margin criteria, x = argminz?U (w ? z),
is replaced with a probabilistic criteria x =
argminz?U |
(
Prw?N (?i,?i) [sign(w ? z) = 1]
)
? 12 | .
234
The selection criteria naturally captures the notion
that we should label the example with the highest
uncertainty. Interestingly, we can show (omitted due
to lack of space) that the probabilistic criteria can be
translated into a corrected geometrical criteria. In
practice, we can compute this normalized margin as
M? = M/
?
V . We call this selection criteria Active
Confident Learning (ACL).
4 Evaluation
To evaluate our active learning methods we used
a similar experimental setup to Tong and Koller
(2001). Each active learning algorithm was given
two labeled examples, one from each class, for ini-
tial training of a classifier, and remaining data as un-
labeled examples. On each round the algorithm se-
lected a single instance for which it was then given
the correct label. The algorithm updated the online
classifier and evaluated it on held out test data to
measure learning progress.
We selected four binary NLP datasets for evalu-
ation: 20 Newsgroups1 and Reuters (Lewis et al,
2004) (used by Tong and Koller) and sentiment clas-
sification (Blitzer et al, 2007) and spam (Bickel,
2006). For each dataset we extracted binary uni-
gram features and sentiment was prepared accord-
ing to Blitzer et al (2007). From 20 Newsgroups
we created 3 binary decision tasks to differentiate
between two similar labels from computers, sci-
ence and talk. We created 3 similar problems from
Reuters from insurance, business services and re-
tail distribution. Sentiment used 4 Amazon domains
(book, dvd, electronics, kitchen). Spam used the
three users from task A data. Each problem had
2000 instances except for 20 Newsgroups, which
used between 1850 and 1971 instances. This created
13 classification problems across four tasks.
Each active learning algorithm was evaluated us-
ing a PA (with slack variable c = 1) or CW classifier
(? = 1) using 10-fold cross validation. We eval-
uated several methods in the Simple margin frame-
work: PA Margin and CW Margin, which select ex-
amples with the smallest margin, and ACL. As a
baseline we included selecting a random instance.
We also evaluated CW and a PA classifier trained on
all training instances. Each method was evaluated by
1
http://people.csail.mit.edu/jrennie/20Newsgroups/
labeling up to 500 labels, about 25% of the training
data. The 10 runs on each dataset for each problem
appear in the left and middle panel of Fig. 1, which
show the test accuracy after each round of active
learning. Horizontal lines indicate CW (solid) and
PA (dashed) training on all instances. Legend num-
bers are accuracy after 500 labels. The left panel av-
erages results over 20 Newsgroups, and the middle
panel averages results over all 13 datasets.
To achieve 80% of the accuracy of training on all
data, a realistic goal for less than 100 labels, PA
Margin required 93% the number of labels of PA
Random, while CW Margin needed only 73% of
the labels of CW Random. By using fewer labels
compared to random selection baselines, CW Mar-
gin learns faster in the active learning setting as com-
pared with PA. Furthermore, adding confidence re-
duced labeling cost compared to margin alone. ACL
improved over CW Margin on every task and after
almost every round; it required 63% of the labels of
CW Random to reach the 80% mark.
We computed the fraction of labels CW Margin
and ACL required (compared to CW Random) to
achieve the 80% accuracy mark of training with all
data. The results are summarized in the right panel
of Fig. 1, where we plot one point per dataset. Points
above the diagonal-line demonstrate the superiority
of ACL over CW Margin. ACL required fewer la-
bels than CW margin twice as often as the opposite
occurred (8 vs 4). Note that CW Margin used more
labels than CW Random in three cases, while ACL
only once, and this one time only about a dozen la-
bels were needed. To conclude, not only does CW
Margin outperforms PA Margin for active-learning,
CW maintains additional valuable information (con-
fidence), which further improves performance.
5 Related Work
Active learning has been widely used for NLP tasks
such as part of speech tagging (Ringger et al, 2007),
parsing (Tang et al, 2002) and word sense disam-
biguation (Chan and Ng, 2007). Many methods rely
on entropy-based scores such as uncertainty sam-
pling (Lewis and Gale, 1994). Others use margin
based methods, such as Kim et al (2006), who com-
bined margin scores with corpus diversity, and Sas-
sano (2002), who considered SVM active learning
235
100 150 200 250 300 350 400 450 500Labels0.65
0.70
0.75
0.80
0.85
0.90
0.95
Test 
Accu
racy
20 Newsgroups
PA Random (82.53)CW Random (92.92)PA Margin (88.06)CW Margin (95.39)ACL (95.51) 100 150 200 250 300 350 400 450 500Labels
0.75
0.80
0.85
0.90
Test 
Accu
racy
All
PA Random (81.30)CW Random (86.67)PA Margin (83.99)CW Margin (88.61)ACL (88.79) 0.2 0.4 0.6 0.8 1.0 1.2 1.4ACL Labels0.2
0.4
0.6
0.8
1.0
1.2
1.4
CW M
argin
 Labe
ls
Reuters20 NewsgroupsSentimentSpam
Figure 1: Results averaged over 20 Newsgroups (left) and all datasets (center) showing test accuracy over active
learning rounds. The right panel shows the amount of labels needed by CW Margin and ACL to achieve 80% of the
accuracy of training on all data - each points refers to a different dataset.
for Japanese word segmentation. Our confidence
based approach can be used to improve these tasks.
Furthermore, margin methods can outperform prob-
abilistic methods; CW beats maximum entropy on
many NLP tasks (Dredze et al, 2008).
A theoretical analysis of margin based methods
selected labels that maximize the reduction of the
version space, the hypothesis set consistent with the
training data (Tong and Koller, 2001). Another ap-
proach selects instances that minimize the future er-
ror in probabilistic algorithms (Roy and McCallum,
2001). Since we consider an online learning algo-
rithm our techniques can be easily extended to on-
line active learning (Cesa-Bianchi et al, 2005; Das-
gupta et al, 2005; Sculley, 2007).
6 Conclusion
We have presented techniques for incorporating con-
fidence into the margin for active learning and have
shown that CW selects better examples than PA, a
popular online algorithm. This approach creates op-
portunities for new active learning frameworks that
depend on margin confidence.
References
S. Bickel. 2006. Ecml-pkdd discovery challenge
overview. In The Discovery Challenge Workshop.
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies,
bollywood, boom-boxes and blenders: Domain adap-
tation for sentiment classification. In ACL.
Nicolo` Cesa-Bianchi, Ga?bor Lugosi, and Gilles Stolt.
2005. Minimizing regret with label efficient predic-
tion. IEEE Tran. on Inf. Theory, 51(6), June.
Y. S. Chan and H. T. Ng. 2007. Domain adaptation with
active learning for word sense disambiguation. In As-
sociation for Computational Linguistics (ACL).
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. JMLR, 7:551?585.
S. Dasgupta, A.T. Kalai, and C. Monteleoni. 2005. Anal-
ysis of perceptron-based active learning. In COLT.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. In
ICML.
S. Kim, Yu S., K. Kim, J-W Cha, and G.G. Lee. 2006.
Mmr-based active machine learning for bio named en-
tity recognition. In NAACL/HLT.
D. D. Lewis and W. A. Gale. 1994. A sequential algo-
rithm for training text classifiers. In SIGIR.
D. D. Lewis, Y. Yand, T. Rose, and F. Li. 2004. Rcv1:
A new benchmark collection for text categorization re-
search. JMLR, 5:361?397.
E. Ringger, P. McClanahan, R. Haertel, G. Busby,
M. Carmen, J. Carroll, K. Seppi, and D. Lonsdale.
2007. Active learning for part-of-speech tagging: Ac-
celerating corpus annotation. In ACL Linguistic Anno-
tation Workshop.
N. Roy and A. McCallum. 2001. Toward optimal active
learning through sampling estimation of error reduc-
tion. In ICML.
Manabu Sassano. 2002. An empirical study of active
learning with support vector machines for japanese
word segmentation. In ACL.
D. Sculley. 2007. Online active learning methods for fast
label-efficient spam filtering. In CEAS.
M. Tang, X. Luo, and S. Roukos. 2002. Active learning
for statistical natural language parsing. In ACL.
S. Tong and D. Koller. 2001. Supprt vector machine
active learning with applications to text classification.
JMLR.
236
Tutorial Abstracts of ACL-08: HLT, page 4,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Advanced Online Learning for Natural Language Processing
Koby Crammer
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104
crammer@cis.upenn.edu
Introduction: Most research in machine learning
has been focused on binary classification, in which
the learned classifier outputs one of two possible
answers. Important fundamental questions can be
analyzed in terms of binary classification, but real-
world natural language processing problems often
involve richer output spaces. In this tutorial, we will
focus on classifiers with a large number of possi-
ble outputs with interesting structure. Notable ex-
amples include information retrieval, part-of-speech
tagging, NP chucking, parsing, entity extraction, and
phoneme recognition.
Our algorithmic framework will be that of on-
line learning, for several reasons. First, online algo-
rithms are in general conceptually simple and easy
to implement. In particular, online algorithms pro-
cess one example at a time and thus require little
working memory. Second, our example applications
have all been treated successfully using online al-
gorithms. Third, the analysis of online algorithms
uses simpler mathematical tools than other types of
algorithms. Fourth, the online learning framework
provides a very general setting which can be applied
to a broad setting of problems, where the only ma-
chinery assumed is the ability to perform exact in-
ference, which computes a maxima over some score
function.
Goals: (1) To provide the audience system-
atic methods to design, analyze and implement
efficiently learning algorithms for their specific
complex-output problems: from simple binary clas-
sification through multi-class categorization to in-
formation extraction, parsing and speech recog-
nition. (2) To introduce new online algorithms
which provide state-of-the-art performance in prac-
tice backed by interesting theoretical guarantees.
Content: The tutorial is divided into two parts. In
the first half we introduce online learning and de-
scribe the Perceptron algorithm (Rosenblatt, 1958)
and the passive-aggressive framework (Crammer et
al., 2006). We then discuss in detail an approach for
deriving algorithms for complex natural language
processing (Crammer, 2004). In the second half we
discuss is detail relevant applications including text
classification (Crammer and Singer, 2003), named
entity recognition (McDonald et al, 2005), pars-
ing (McDonald, 2006), and other tasks. We also
relate the online algorithms to their batch counter-
parts.
References
K. Crammer and Y. Singer. 2003. A new family of online
algorithms for category ranking. Jornal of Machine
Learning Research, 3:1025?1058.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. JMLR, 7:551?585.
K. Crammer. 2004. Online Learning of Complex Cate-
gorial Problems. Ph.D. thesis, Hebrew Universtiy.
R. McDonald, K. Crammer, and F. Pereira. 2005. Flex-
ible text segmentation with structured multilabel clas-
sification. In HLT/EMNLP.
R. McDonald. 2006. Discriminative Training and Span-
ning Tree Algorithms for Dependency Parsing. Ph.D.
thesis, University of Pennsylvania.
F. Rosenblatt. 1958. The perceptron: A probabilistic
model for information storage and organization in the
brain. Psychological Review, 65:386?407.
4
BioNLP 2007: Biological, translational, and clinical language processing, pages 129?136,
Prague, June 2007. c?2007 Association for Computational Linguistics
Automatic Code Assignment to Medical Text
Koby Crammer and Mark Dredze and Kuzman Ganchev and Partha Pratim Talukdar
Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA
{crammer|mdredze|kuzman|partha}@seas.upenn.edu
Steven Carroll
Division of Oncology, The Children?s Hospital of Philadelphia, Philadelphia, PA
carroll@genome.chop.edu
Abstract
Code assignment is important for handling
large amounts of electronic medical data in
the modern hospital. However, only expert
annotators with extensive training can as-
sign codes. We present a system for the
assignment of ICD-9-CM clinical codes to
free text radiology reports. Our system as-
signs a code configuration, predicting one or
more codes for each document. We com-
bine three coding systems into a single learn-
ing system for higher accuracy. We compare
our system on a real world medical dataset
with both human annotators and other auto-
mated systems, achieving nearly the maxi-
mum score on the Computational Medicine
Center?s challenge.
1 Introduction
The modern hospital generates tremendous amounts
of data: medical records, lab reports, doctor notes,
and numerous other sources of information. As hos-
pitals move towards fully electronic record keeping,
the volume of this data only increases. While many
medical systems encourage the use of structured in-
formation, including assigning standardized codes,
most medical data, and often times the most impor-
tant information, is stored as unstructured text.
This daunting amount of medical text creates
exciting opportunities for applications of learning
methods, such as search, document classification,
data mining, information extraction, and relation ex-
traction (Shortliffe and Cimino, 2006). These ap-
plications have the potential for considerable bene-
fit to the medical community as they can leverage
information collected by hospitals and provide in-
centives for electronic record storage. Much of the
data generated by medical personnel is unused past
the clinical visit, often times because there is no way
to simply and quickly apply the wealth of informa-
tion. Medical NLP holds the promise of both greater
care for individual patients and enhanced knowledge
about health care.
In this work we explore the assignment of ICD-9-
CM codes to clinical reports. We focus on this prac-
tical problem since it is representative of the type
of task faced by medical personnel on a daily ba-
sis. Many hospitals organize and code documents
for later retrieval using different coding standards.
Often times, these standards are extremely complex
and only trained expert coders can properly perform
the task, making the process of coding documents
both expensive and unreliable since a coder must se-
lect from thousands of codes a small number for a
given report. An accurate automated system would
reduce costs, simplify the task for coders, and create
a greater consensus and standardization of hospital
data.
This paper addresses some of the challenges asso-
ciated with ICD-9-CM code assignment to clinical
free text, as well as general issues facing applica-
tions of NLP to medical text. We present our auto-
mated system for code assignment developed for the
Computational Medicine Center?s challenge. Our
approach uses several classification systems, each
with the goal of predicting the exact code configu-
ration for a medical report. We then use a learning
129
system to combine our predictions for superior per-
formance.
This paper is organized as follows. First, we ex-
plain our task and difficulties in detail. Next we de-
scribe our three automated systems and features. We
combine the three approaches to create a single su-
perior system. We evaluate our system on clinical
reports and show accuracy approaching human per-
formance and the challenge?s best score.
2 Task Overview
The health care system employs a large number of
categorization and classification systems to assist
data management for a variety of tasks, including
patient care, record storage and retrieval, statistical
analysis, insurance, and billing. One of these sys-
tems is the International Classification of Diseases,
Ninth Revision, Clinical Modification (ICD-9-CM)
which is the official system of assigning codes to di-
agnoses and procedures associated with hospital uti-
lization in the United States. 1 The coding system
is based on World Health Organization guidelines.
An ICD-9-CM code indicates a classification of a
disease, symptom, procedure, injury, or information
from the personal history. Codes are organized hier-
archically, where top level entries are general group-
ings (e.g. ?diseases of the respiratory system?) and
bottom level codes indicate specific symptoms or
diseases and their location (e.g. ?pneumonia in as-
pergillosis?). Each specific, low-level code consists
of 4 or 5 digits, with a decimal after the third. Higher
level codes typically include only 3 digits. Overall,
there are thousands of codes that cover a broad range
of medical conditions.
Codes are assigned to medical reports by doc-
tors, nurses and other trained experts based on com-
plex coding guidelines (National Center for Health
Statistics, 2006). A particular medical report can be
assigned any number of relevant codes. For exam-
ple, if a patient exhibits a cough, fever and wheez-
ing, all three codes should be assigned. In addi-
tion to finding appropriate codes for each condition,
complex rules guide code assignment. For exam-
ple, a diagnosis code should always be assigned if a
diagnosis is reached, a diagnosis code should never
1http://www.cdc.gov/nchs/about/otheract/
icd9/abticd9.htm
be assigned when the diagnosis is unclear, a symp-
tom should never be assigned when a diagnosis is
present, and the most specific code is preferred. This
means that codes that seem appropriate to a report
should be omitted in specific cases. For example,
a patient with hallucinations should be coded 780.1
(hallucinations) but for visual hallucinations, the
correct code is 368.16. The large number of codes
and complexity of assignment rules make this a diffi-
cult problem for humans (inter-annotator agreement
is low). Therefore, an automated system that sug-
gested or assigned codes could make medical data
more consistent.
These complexities make the problem difficult
for NLP systems. Consider the task as multi-class,
multi-label. For a given document, many codes may
seem appropriate but it may not be clear to the algo-
rithm how many to assign. Furthermore, the codes
are not independent and different labels can inter-
act to either increase or decrease the likelihood of
the other. Consider a report that says, ?patient re-
ports cough and fever.? The presence of the words
cough and fever indicate codes 786.2 (cough) and
780.6 (fever). However, if the report continues to
state that ?patient has pneumonia? then these codes
are dropped in favor of 486 (pneumonia). Further-
more, if the report then says ?verify clinically?, then
the diagnosis is uncertain and only codes 786.2 and
780.6 apply. Clearly, this is a challenging problem,
especially for an automated system.
2.1 Corpus
We built and evaluated our system in accordance
with the Computational Medicine Center?s (CMC)
2007 Medical Natural Language Processing Chal-
lenge.2 Since release of medical data must strictly
follow HIPAA standards, the challenge corpus un-
derwent extensive treatment for disambiguation,
anonymization, and careful scrubbing. A detailed
description of data preparation is found in Compu-
tational Medicine Center (2007). We describe the
corpus here to provide context for our task.
The training corpus is comprised of 978 radiolog-
ical reports taken from real medical records. A test
corpus contains 976 unlabeled documents. Radiol-
ogy reports have two text fields, clinical history and
2www.computationalmedicine.org/challenge
130
impression. The physician ordering the x-ray writes
the clinical history, which contains patient informa-
tion for the radiologist, including history and current
symptoms. Sometimes a guess as to the diagnosis
appears (?evaluate for asthma?). The descriptions
are sometimes whole sentences and other times sin-
gle words (?cough?). The radiologist writes the im-
pression to summarize his or her findings. It con-
tains a short analysis and often times a best guess as
to the diagnosis. At times this field is terse, (?pneu-
monia? or ?normal kidneys?) and at others it con-
tains an entire paragraph of text. Together, these two
fields are used to assign ICD-9-CM codes, which
justify a certain procedure, possibly for reimburse-
ment by the insurance company.
Only a small percentage of ICD-9-CM codes ap-
pear in the challenge. In total, the reports include 45
different codes arranged in 94 configurations (com-
binations). Some of these codes appear frequently,
while others are rare, appearing only a single time.
The test set is restricted so that each configuration
appears at least once in the training set, although
there is no further guarantee as to the test set?s distri-
bution over codes. Therefore, in addition to a large
number of codes, there is variability in the amount
of data for each code. Four codes have over 100
examples each and 24 codes have 10 or fewer doc-
uments, with 10 of these codes having only a single
document.
Since code annotation is a difficult task, each doc-
ument in the corpus was evaluated by three expert
annotators. A gold annotation was created by tak-
ing the majority of the annotators; if two of the three
annotators provided a code, that code is used in the
gold configuration. This approach means that a doc-
ument?s configuration may be a construction of mul-
tiple annotators and may not match any of the three
annotators exactly. Both the individual and the ma-
jority annotations are included with the training cor-
pus.
While others have attempted ICD-9 code classi-
fication, our task differs in two respects (Section 7
provides an overview of previous work). First, pre-
vious work has used discharge reports, which are
typically longer with more text fields. Second, while
most systems are evaluated as a recommendation
system, offering the top k codes and then scoring
recall at k, our task is to provide the exact configu-
ration. The CMC challenge evaluated systems using
an F1 score, so we are penalized if we suggest any
label that does not appear in the majority annotation.
To estimate task difficulty we measured the inter-
annotator score for the training set using the three
annotations provided. We scored two annotations
with the micro average F1, which weighs each code
assignment equally (see Section 5 for details on
evaluation metrics). If an annotator omitted a code
and included an extra code, he or she is penalized
with a false positive (omitting a code) and a false
negative (adding an extra code). We measured anno-
tators against each other; the average f-measure was
74.85 (standard deviation of .06). These scores were
low since annotators chose from an unrestricted set
of codes, many of which were not included in the fi-
nal majority annotation. However, these scores still
indicate the human accuracy for this task using an
unrestricted label set. 3
3 Code Assignment System
We developed three automated systems guided by
our above analysis. First, we designed a learning
system that used natural language features from the
official code descriptions and the text of each re-
port. It is general purpose and labels all 45 codes
and 94 configurations (labels). Second, we built a
rule based system that assigned codes based on the
overlap between the reports and code descriptions,
similar to how an annotator may search code de-
scriptions for appropriate labels. Finally, a special-
ized system aimed at the most common codes imple-
mented a policy that mimics the guidelines a medical
staffer would use to assign these codes.
3.1 Learning System
We begin with some notational definitions. In what
follows, x denotes the generic input document (ra-
diology report), Y denotes the set of possible label-
ings (code configurations) of x, and y?(x) the cor-
rect labeling of x. For each pair of document x
and labeling y ? Y , we compute a vector-valued
feature representation f(x, y). A linear model is
3We also measured each annotator with the majority codes,
taking the average score (87.48), and the best annotator with
the majority label (92.8). However, these numbers are highly
biased since the annotator influences the majority labeling. We
observe that our final system still exceeds the average score.
131
given by a weight vector w. Given this weight vec-
tor w, the score w ? f(x, y) ranks possible labelings
of x, and we denote by Yk,w(x) the set of k top
scoring labelings for x. For some structured prob-
lems, a factorization of f(x, y) is required to enable
a dynamic program for inference. For our problem,
we know all the possible configurations in advance
(there are 94 of them) so we can pick the highest
scoring y ? Y by trying them all. For each docu-
ment x and possible labeling y, we compute a score
using w and the feature representation f(x, y). The
top scoring y is output as the correct label. Section
3.1.1 describes our feature function f(x, y) while
Section 3.1.2 describes how we find a good weight
vector w.
3.1.1 Features
Problem representation is one of the most impor-
tant aspects of a learning system. In our case, this
is defined by the set of features f(x, y). Ideally we
would like a linear combination of our features to ex-
actly specify the true labeling of all the instances, but
we want to have a small total number of features so
that we can accurately estimate their values. We sep-
arate our features into two classes: label specific fea-
tures and transfer features. For simplicity, we index
features by their name. Label specific features are
only present for a single label. For example, a simple
class of label specific features is the conjunction of a
word in the document with an ICD-9-CM code in the
label. Thus, for each word we create 94 features, i.e.
the word conjoined with every label. These features
tend to be very powerful, since weights for them can
encode very specific information about the way doc-
tors talk about a disease, such as the feature ?con-
tains word pneumonia and label contains code 486?.
Unfortunately, the cost of this power is that there are
a large number of these features, making parameter
estimation difficult for rare labels. In contrast, trans-
fer features can be present in multiple labels. An
example of a transfer feature might be ?the impres-
sion contains all the words in the code descriptions
of the codes in this label?. Transfer features allow us
to generalize from one label to another by learning
things like ?if all the words of the label description
occur in the impression, then this label is likely? but
have the drawback that we cannot learn specific de-
tails about common labels. For example, we cannot
learn that the word ?pneumonia? in the impression
is negatively correlated with the code cough. The
inclusion of both label specific and transfer features
allows us to learn specificity where we have a large
number of examples and generality for rare codes.
Before feature extraction we normalized the re-
ports? text by converting it to lower case and by
replacing all numbers (and digit sequences) with a
single token ?NUM?. We also prepared a synonym
dictionary for a subset of the tokens and n-grams
present in the training data. The synonym dictionary
was based onMeSH4, the Medical Subject Headings
vocabulary, in which synonyms are listed as terms
under the same concept. All ngrams and tokens
in the training data which had mappings defined in
the synonym dictionary were then replaced by their
normalized token; e.g. all mentions of ?nocturnal
enuresis? or ?nighttime urinary incontinence? were
replaced by the token ?bedwetting?. Additionally,
we constructed descriptions for each code automati-
cally from the official ICD-9-CM code descriptions
in National Center for Health Statistics (2006). We
also created a mapping between code and code type
(diagnosis or symptom) using the guidelines.
Our system used the following features. The de-
scriptions of particular features are in quotes, while
schemes for constructing features are not.
? ?this configuration contains a disease code?,
?this configuration contains a symptom code?,
?this configuration contains an ambiguous
code? and ?this configuration contains both dis-
ease and symptom codes?.5
? With the exception of stop-words, all words of
the impression and history conjoined with each
label in the configuration; pairs of words con-
joined with each label; words conjoined with
pairs of labels. For example, ?the impression
contains ?pneumonia? and the label contains
codes 786.2 and 780.6?.
? A feature indicating when the history or im-
pression contains a complete code description
4www.nlm.nih.gov/mesh
5We included a feature for configurations that had both dis-
ease and symptom codes because they appeared in the training
data, even though coding guidelines prohibit these configura-
tions.
132
for the label; one for a word in common with
the code description for one of the codes in the
label; a common word conjoined with the pres-
ence of a negation word nearby (?no?, ?not?,
etc.); a word in common with a code descrip-
tion not present in the label. We applied similar
features using negative words associated with
each code.
? A feature indicating when a soft negation word
appears in the text (?probable?, ?possible?,
?suspected?, etc.) conjoined with words that
follow; the token length of a text field (?im-
pression length=3?); a conjunction of a feature
indicating a short text field with the words in
the field (?impression length=1 and ?pneumo-
nia? ?)
? A feature indicating each n-gram sequence that
appears in both the impression and clinical his-
tory; the conjunction of certain terms where
one appears in the history and the other in the
impression (e.g. ?cough in history and pneu-
monia in impression?).
3.1.2 Learning Technique
Using these feature representations, we now learn
a weight vector w that scores the correct labelings
of the data higher than incorrect labelings. We used
a k-best version of the MIRA algorithm (Crammer,
2004; McDonald et al, 2005). MIRA is an online
learning algorithm that for each training document
x updates the weight vector w according to the rule:
wnew = argmin
w
?w ? wold?
s.t. ?y ? Yk,wold(x) :
w ? f(x, y?(x)) ? w ? f(x, y) ? L(y?(x), y)
where L(y?(x), y) is a measure of the loss of label-
ing y with respect to the correct labeling y?(x). For
our experiments, we set k to 30 and iterated over the
training data 10 times. Two standard modifications
to this approach also helped. First, rather than using
just the final weight vector, we average all weight
vectors. This has a smoothing effect that improves
performance on most problems. The second modifi-
cation is the introduction of slack variables:
wnew = argmin
w
?w ? wold? + ?
?
i
?i
s.t. ?y ? Yk,wold(x) :
w ? f(x, y?(x)) ? w ? f(x, y) ? L(y?(x), y) ? ?i
?i ? {1 . . . k} : ?i ? 0.
We used a ? of 10?3 in our experiments.
The most straightforward loss function is the 0/1
loss, which is one if y does not equal y?(x) and zero
otherwise. Since we are evaluated based on the num-
ber of false negative and false positive ICD-9-CM
codes assigned to all the documents, we used a loss
that is the sum of the number of false positive and the
number of false negative labels that y assigns with
respect to y?(x).
Finally, we only used features that were possi-
ble for some labeling of the test data by using only
the test data to construct our feature alphabet. This
forced the learner to focus on hypotheses that could
be used at test time and resulted in a 1% increase in
F-measure in our final system on the test data.
3.2 Rule Based System
Since some of the configurations appear a small
number of times in our corpus (some only once),
we built a rule based system that requires no train-
ing. The system uses a description of the ICD-9-CM
codes and their types, similar to the list used by our
learning system (Section 3.1.1). The code descrip-
tions include between one and four short descrip-
tions, such as ?reactive airway disease?, ?asthma?,
and ?chronic obstructive pulmonary disease?. We
treat each of these descriptions as a bag of words.
For a given report, the system parses both the clini-
cal history and impression into sentences, using ?.?
as a sentence divider. Each sentence is the checked
to see if all of the words in a code description appear
in the sentence. If a match is found, we set a flag
corresponding to the code. However, if the code is
a disease, we search for a negation word in the sen-
tence, removing the flag if a negation word is found.
Once all code descriptions have been evaluated, we
check if there are any flags set for disease codes. If
so, we remove all symptom code flags. We then emit
a code corresponding to each set flag. This simple
system does not enforce configuration restrictions;
133
we may predict a code configuration that does not
appear in our training data. Adding this restriction
improved precision but hurt recall, leading to a slight
decrease in F1 score. We therefore omitted the re-
striction from our system.
3.3 Automatic Coding Policies
As we described in Section 2, enforcing coding
guidelines can be a complex task. While a learning
system may have trouble coding a document, a hu-
man may be able to define a simple policy for cod-
ing. Since some of the most frequent codes in our
dataset have this property, we decided to implement
such an automatic coding policy. We selected two
related sets of codes to target with a rule based sys-
tem, a set of codes found in pneumonia reports and
a set for urinary tract infection/reflux reports.
Reports related to pneumonia are the most com-
mon in our dataset and include codes for pneumo-
nia, asthma, fever, cough and wheezing; we handle
them with a single policy. Our policy is as follows:
? Search for a small set of keywords (e.g.
?cough?, ?fever?) to determine if a code should
be applied.
? If ?pneumonia? appears unnegated in the im-
pression and the impression is short, or if it oc-
curs in the clinical history and is not preceded
by phrases such as ?evaluate for? or ?history
of?, apply pneumonia code and stop.
? Use the same rule to code asthma by looking
for ?asthma? or ?reactive airway disease?.
? If no diagnosis is found, code all non-negated
symptoms (cough, fever, wheezing).
We selected 80% of the training set to evaluate in the
construction of our rules. We then ran the finished
system on both this training set and the held out 20%
of the data. The system achieved F1 scores of 87%
on the training set and 84% on the held out data for
these five codes. The comparable scores indicates
that we did not over-fit the training data.
We designed a similar policy for two other related
codes, urinary tract infection and vesicoureteral re-
flux. We found these codes to be more complex as
they included a wide range of kidney disorders. On
these two codes, our system achieved 78% on the
train set and 76% on the held out data. Overall, au-
tomatically applying our two policies yielded high
confidence predictions for a significant subset of the
corpus.
4 Combined System
Since our three systems take complimentary ap-
proaches to the problem, we combined them to im-
prove performance. First, we took our automatic
policy and rule based systems and cascaded them; if
the automatic policy system does not apply a code,
the rule based system classifies the report. We used
a cascaded approach since the automatic policy sys-
tem was very accurate when it was able to assign
a code. Therefore, the rule based system defers to
the policy system when it is triggered. Next, we in-
cluded the prediction of the cascaded system as a
feature for our learning system. We used two fea-
ture rules: ?cascaded-system predicted exactly this
label? and ?cascaded-system predicted one of the
codes in this label?. As we show, this yielded our
most accurate system. While we could have used a
meta-classifier to combine the three systems, includ-
ing the rule based systems as features to the learning
system allowed it to learn the appropriate weights
for the rule based predictions.
5 Evaluation Metric
Evaluation metrics for this task are often based on
recommendation systems, where the system returns
a list of the top k codes for selection by the user. As
a result, typical metrics are ?recall at k? and aver-
age precision (Larkey and Croft, 1995). Instead, our
goal was to predict the exact configuration, returning
exactly the number of codes predicted to be on the
report. The competition used a micro-averaged F1
score to evaluate predictions. A contingency table
(confusion matrix) is computed by summing over
each predicted code for each document by predic-
tion type (true positive, false positive, false negative)
weighing each code assignment equally. F1 score
is computed based on the resultant table. If specific
codes or under-coding is favored, we can modify our
learning loss function as described in Section 3.1.2.
A detailed treatment of this evaluation metric can be
found in Computational Medicine Center (2007).
134
System Precision Recall F1
BL 61.86 72.58 66.79
RULE 81.9 82.0 82.0
CASCADE 86.04 84.56 85.3
LEARN 85.5 83.6 84.6
CASCADE+LEARN 87.1 85.9 86.5
Table 1: Performance of our systems on the provided
labeled training data (F1 score). The learning sys-
tems (CASCADE+LEARN and LEARN ) were eval-
uated on ten random split of the data while RULE
was evaluated on all of the training data. We include
a simple rule based system (BL ) as a baseline.
6 Results
We evaluated our systems on the labeled training
data of 978 radiology reports. For each report, each
system predicted an exact configuration of codes
(i.e. one of 94 possible labels). We score each sys-
tem using a micro-averaged F1 score. Since we only
had labels for the training data, we divided the data
using an 80/20 training test split and averaged results
over 10 runs for our learning systems. We evaluated
the following systems:
? RULE : The rule based system based on ICD-
9-CM code descriptions (Section 3.2).
? CASCADE : The automatic code policy system
(Section 3.3) cascaded with RULE (Section 4).
? LEARN : The learning system with both label
specific and transfer features (Section 3.1).
? CASCADE+LEARN : Our combined system
that incorporates CASCADE predictions as a
feature to LEARN (Section 4).
For a baseline, we built a simple system that ap-
plies the official ICD-9-CM code descriptions to find
the correct labels (BL ). For each code in the train-
ing set, the system generates text-segments related to
it. During testing, for each new document, the sys-
tem checks if any text-segment (as discovered dur-
ing training) appears in the document. If so, the cor-
responding code is predicted. The results from our
four systems and baseline are shown in Table 1.
System Train Test
CASCADE 85.3 84
CASCADE+LEARN 86.5 87.60
Average - 76.6
Best - 89.08
Table 2: Performance of two systems on the train
and test data. Results obtained from the web sub-
mission interface were rounded. Average and Best
are the average and best f-measures of the 44 sub-
mitted systems (standard deviation 13.40).
Each of our systems easily beats the baseline, and
the average inter-annotator score for this task. Ad-
ditionally, we were able to evaluate two of our sys-
tems on the test data using a web interface as pro-
vided by the competition. The test set contains 976
documents (about the same as the training set) and
is drawn the from same distribution as the training
data. Our test results were comparable to perfor-
mance on the training data, showing that we did
not over-fit to the training data (Table 2). Addi-
tionally, our combined system (CASCADE+LEARN
) achieved a score of 87.60%, beating our training
data performance and exceeding the average inter-
annotator score. Out of 44 submitted systems, the
average score on test data was 76.7% (standard devi-
ation of 13.40) and the maximum score was 89.08%.
Our system scored 4th overall and was less than
1.5% behind the best system. Overall, in comparison
with our baselines and over 40 systems, we perform
very well on this task.
7 Related Work
There have been several attempts at ICD-9-CM
code classification and related problems for med-
ical records. The specific problem of ICD-9-CM
code assignment was studied by Lussier et al (2000)
through an exploratory study. Larkey and Croft
(1995) designed classifiers for the automatic assign-
ment of ICD-9 codes to discharge summaries. Dis-
charge summaries tend to be considerably longer
than our data and contain multiple text fields. Ad-
ditionally, the number of codes per document has
a larger range, varying between 1 and 15 codes.
Larkey and Croft use three classifiers: K-nearest
neighbors, relevance feedback, and bayesian inde-
135
pendence. Similar to our approach, they tag items
as negated and try to identify diagnosis and symp-
tom terms. Additionally, their final system combines
all three models. A direct comparison is not possi-
ble due to the difference in data and evaluation met-
rics; they use average precision and recall at k. On
a comparable metric, ?principal code is top candi-
date?, their best system achieves 59.9% accuracy. de
Lima et al (1998) rely on the hierarchical nature of
medical codes to design a hierarchical classification
scheme. This approach is likely to help on our task
as well but we were unable to test this since the lim-
ited number of codes removes any hierarchy. Other
approaches have used a variety of NLP techniques
(Satomura and Amaral, 1992).
Others have used natural language systems for the
analysis of medical records (Zweigenbaum, 1994).
Chapman and Haug (1999) studied radiology re-
ports looking for cases of pneumonia, a goal sim-
ilar to that of our automatic coding policy system.
Meystre and Haug (2005) processed medical records
to harvest potential entries for a medical problem
list, an important part of electronic medical records.
Chuang et al (2002) studied Charlson comorbidi-
ties derived from processing discharge reports and
chest x-ray reports and compared them with admin-
istrative data. Additionally, Friedman et al (1994)
applies NLP techniques to radiology reports.
8 Conclusion
We have presented a learning system that processes
radiology reports and assigns ICD-9-CM codes.
Each of our systems achieves results comparable
with an inter-annotator baseline for our training data.
A combined system improves over each individ-
ual system. Finally, we show that on test data un-
available during system development, our final sys-
tem continues to perform well, exceeding the inter-
annotator baseline and achieving the 4th best score
out of 44 systems entered in the CMC challenge.
9 Acknowledgements
We thank Andrew Lippa for his extensive medical
wisdom. Dredze is supported by an NDSEG fel-
lowship; Ganchev and Talukdar by NSF ITR EIA-
0205448; and Crammer by DARPA under Contract
No. NBCHD03001. Any opinions, findings, and
conclusions or recommendations expressed in this
material are those of the author(s) and do not nec-
essarily reflect the views of the DARPA or the De-
partment of Interior-National Business Center (DOI-
NBC).
References
W.W. Chapman and P.J. Haug. 1999. Comparing expert sys-
tems for identifying chest x-ray reports that support pneu-
monia. In AMIA Symposium, pages 216?20.
JH Chuang, C Friedman, and G Hripcsak. 2002. A com-
parison of the charlson comorbidities derived from medical
language processing and administrative data. AMIA Sympo-
sium, pages 160?4.
Computational Medicine Center. 2007. The
computational medicine center?s 2007 med-
ical natural language processing challenge.
http://computationalmedicine.org/challenge/index.php.
Koby Crammer. 2004. Online Learning of Complex Categorial
Problems. Ph.D. thesis, Hebrew Univeristy of Jerusalem.
Luciano R. S. de Lima, Alberto H. F. Laender, and Berthier A.
Ribeiro-Neto. 1998. A hierarchical approach to the auto-
matic categorization of medical documents. In CIKM.
C Friedman, PO Alderson, JH Austin, JJ Cimino, and SB John-
son. 1994. A general natural-language text processor for
clinical radiology. Journal of the American Medical Infor-
matics Association, 1:161?74.
Leah S. Larkey and W. Bruce Croft. 1995. Automatic assign-
ment of icd9 codes to discharge summaries. Technical re-
port, University of Massachusetts at Amherst, Amherst, MA.
YA Lussier, C Friedman, L Shagina, and P Eng. 2000. Au-
tomating icd-9-cm encoding using medical language pro-
cessing: A feasibility study.
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005.
Flexible text segmentation with structured multilabel classi-
fication. In HLT/EMNLP.
Stephane Meystre and Peter J Haug. 2005. Automation of a
problem list using natural language processing. BMC Medi-
cal Informatics and Decision Making.
National Center for Health Statistics. 2006. Icd-
9-cm official guidelines for coding and reporting.
http://www.cdc.gov/nchs/datawh/ftpserv/ftpicd9/ftpicd9.htm.
Y Satomura and MB Amaral. 1992. Automated diagnostic in-
dexing by natural language processing. Medical Informat-
ics, 17:149?163.
Edward H. Shortliffe and James J. Cimino, editors. 2006.
Biomedical Informatics: Computer Applications in Health
Care and Biomedicine. Springer.
P. Zweigenbaum. 1994. Menelas: an access system for medical
records using natural language. Comput Methods Programs
Biomed, 45:117?20.
136
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 971?981,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Confidence in Structured-Prediction using Confidence-Weighted Models
Avihai Mejer
Department of Computer Science
Technion-Israel Institute of Technology
Haifa 32000, Israel
amejer@tx.technion.ac.il
Koby Crammer
Department of Electrical Engineering
Technion-Israel Institute of Technology
Haifa 32000, Israel
koby@ee.technion.ac.il
Abstract
Confidence-Weighted linear classifiers (CW)
and its successors were shown to perform
well on binary and multiclass NLP prob-
lems. In this paper we extend the CW ap-
proach for sequence learning and show that it
achieves state-of-the-art performance on four
noun phrase chucking and named entity recog-
nition tasks. We then derive few algorith-
mic approaches to estimate the prediction?s
correctness of each label in the output se-
quence. We show that our approach provides
a reliable relative correctness information as
it outperforms other alternatives in ranking
label-predictions according to their error. We
also show empirically that our methods output
close to absolute estimation of error. Finally,
we show how to use this information to im-
prove active learning.
1 Introduction
In the past decade structured classification has seen
much interest by the machine learning community.
After the introduction of conditional random fields
(CRFs) (Lafferty et al, 2001), and maximum mar-
gin Markov networks (Taskar et al, 2003), which
are batch algorithms, new online method were in-
troduced. For example the passive-aggressive algo-
rithm was adapted to chunking (Shimizu and Haas,
2006), parsing (McDonald et al, 2005b), learning
preferences (Wick et al, 2009) and text segmenta-
tion (McDonald et al, 2005a). These new online
algorithms are fast to train and simple to implement,
yet they generate models that output merely a pre-
diction with no additional information, as opposed
to probabilistic models like CRFs or HMMs.
In this work we fill this gap proposing few al-
ternatives to compute confidence in the output of
discriminative non-probabilistic algorithms. As be-
fore, our algorithms output the highest-scoring la-
beling. However, they also compute additional la-
belings, that are used to compute the per word con-
fidence in its labelings. We build on the recently
introduced confidence-weighted learning (Dredze et
al., 2008; Crammer et al, 2009b) and induce a dis-
tribution over labelings from the distribution main-
tained over weight-vectors.
We show how to compute confidence estimates
in the label predicted per word, such that the con-
fidence reflects the probability that the label is not
correct. We then use this confidence information
to rank all labeled words (in all sentences). This
can be thought of as a retrieval of the erroneous
words, which can than be passed to human anno-
tator for an examination, either to correct these mis-
takes or as a quality control component. Next, we
show how to apply our techniques to active learning
over sequences. We evaluate our methods on four
NP chunking and NER datasets and demonstrate the
usefulness of our methods. Finally, we report the
performance of obtained by CW-like adapted to se-
quence prediction, which are comparable with cur-
rent state-of-the-art algorithms.
2 Confidence-Weighted Learning
Consider the following online binary classification
problem that proceeds in rounds. On the ith round
the online algorithm receives an input xi ? Rd and
971
applies its current rule to make a prediction y?i ? Y ,
for the binary set Y = {?1,+1}. It then receives
the correct label yi ? Y and suffers a loss `(yi, y?i).
At this point, the algorithm updates its prediction
rule with the pair (xi, yi) and proceeds to the next
round. A summary of online algorithms can be
found in (Cesa-Bianchi and Lugosi, 2006).
Online confidence-weighted (CW) learning
(Dredze et al, 2008; Crammer et al, 2008),
generalized the passive-aggressive (PA) update
principle to multivariate Gaussian distributions
over the weight vectors - N (?,?) - for binary
classification. The mean ? ? Rd contains the
current estimate for the best weight vector, whereas
the Gaussian covariance matrix ? ? Rd?d captures
the confidence in this estimate. More precisely,
the diagonal elements ?p,p, capture the confidence
in the value of the corresponding weight ?p ; the
smaller the value of ?p,p, is, the more confident
is the model in the value of ?p. The off-diagonal
elements ?p,q for p 6= q capture the correlation
between the values of ?p and ?q. When the data
is of large dimension, such as in natural language
processing, a model that maintains a full covariance
matrix is not feasible and we back-off to diagonal
covariance matrices.
CW classifiers are trained according to a PA rule
that is modified to track differences in Gaussian dis-
tributions. At each round, the new mean and co-
variance of the weight vector distribution is chosen
to be the solucion of an optimization problem (see
(Crammer et al, 2008) for details). This particu-
lar CW rule may over-fit by construction. A more
recent alternative scheme called AROW (adaptive
regularization of weight-vectors) (Crammer et al,
2009b) replaces the guaranteed prediction at each
round with the a more relaxed objective (see (Cram-
mer et al, 2009b)). AROW has been shown to
perform well in practice, especially for noisy data
where CW severely overfits.
The solution for the updates of CW and AROW
share the same general form,
?i+1 =?i+?i?iyixi ; ?
?1
i+1 =?
?1
i+1+?ixix
>
i , (1)
where the difference between CW and AROW is the
specific instance-dependent rule used to set the val-
ues of ?i and ?i.
Algorithm 1 Sequence Labeling CW/AROW
Input: Joint feature mapping ?(x,y) ? Rd
Initial variance a > 0
Tradeoff Parameter r > 0 (AROW)
or Confidence parameter ? (CW)
Initialize: ?0 = 0 , ?0 = aI
for i = 1, 2 . . . do
Get xi ? X
Predict best labeling
y?i = arg maxz ?i?1 ??(xi, z)
Get correct labeling yi ? Y
|xi|
Define ?i,y,y? = ?(x,yi)??(x, y?i)
Compute ?i and ?i (Eq. (3) for CW ;
Eqs. (4),?i = 1/r) for AROW)
Set ?i = ?i?1 + ?i?i?1?i,y,y?
Set ??1i = ?
?1
i?1 + ?i?i,y,y??
>
i,y,y?
end for
3 Sequence Labeling
In the sequence labeling setting, instances x be-
long to a general input space X and conceptually are
composed of a finite number n of components, such
as words of a sentence. The number of components
n = |x| varies between instances. Each part of an
instance is labelled from a finite set Y , |Y| = K.
That is, a labeling of an entire instance belongs to
the product set y ? Y ? Y . . .Y (n times).
We employ a general approach (Collins, 2002;
Crammer et al, 2009a) to generalize binary clas-
sification and use a joined feature mapping of an
instance x and a labeling y into a common vector
space, ?(x,y) ? Rd.
Given an input instance x and a model ? ? Rd
we predict the labeling with the highest score, y? =
arg maxz ? ??(x, z). A brute-force approach eval-
uates the value of the score ? ??(x, z) for each pos-
sible labeling z ? Yn, which is not feasible for large
values of n. Instead, we follow standard factoriza-
tion and restrict the joint mapping to be of the form,
?(x,y) =
?n
p=1 ?(x, yp)+
?n
q=2 ?(x, yq, yq?1).
That is, the mapping is a sum of mappings, each tak-
ing into consideration only a label of a single part, or
two consecutive parts. The time required to compute
the max operator is linear in n and quadratic in K
using the dynamic-programming Viterbi algorithm.
After the algorithm made a prediction, it uses
972
the current labeled instance (xi,yi) to update the
model. We now define the update rule both for a
version of CW and for AROW for strucutred learn-
ing, staring with CW. Given the input parameter ?
of CW we denote by ?? = 1 + ?2/2, ??? = 1 + ?2.
We follow a similar argument as in the single up-
date of (Crammer et al, 2009a, sec. 5.1) to se-
quence labeling by a reduction to binary classifica-
tion. We first define the difference between the fea-
ture vector associated with the current labeling yi
and the feature vector associated with some label-
ing z to be, ?i,y,z = ?(x,yi) ? ?(x, z) , and
in particular, when we use the prediction y?i we get,
?i,y,y? = ?(x,yi)??(x, y?i) . The CW update is,
?i = ?i?1 + ?i?i?1?i,y,y?
??1i = ?
?1
i?1 + ?i?i,y,y??
>
i,y,y? , (2)
where the two scalars ?i and ?i are set using the
update rule defined by (Crammer et al, 2008) for
binary classification,
vi = ?
>
i,y,y??i?1?i,y,y? , mi = ?i?1 ??i,y,y? (3)
?i = max
{
0,
1
vi???
(
?mi?
? +
?
m2i
?4
4
+ vi?2???
)}
?i =
?i?
?
v+i
, v+i =
1
4
(
??ivi?+
?
?2i v
2
i ?
2 + 4vi
)2
We turn our attention and describe a mod-
ification of AROW for sequence prediction.
Replacing the binary-hinge loss in (Crammer
et al, 2009b, Eqs. (1,2)) the first one with the
corresponding multi-class hinge loss for structured
problems we obtain, 12 (?i??)
>??1i (?i??) +
1
2r (max {0,maxz 6=y {d(y, z)? ? ? (?i,y,z)}})
2,
where, d(y, z) =
?|x|
q=1 1yq 6=zq , is the hamming
distance between the two label sequences y and z.
The last equation is hard to optimize since the max
operator is enumerating over exponential number of
possible labellings z. We thus approximate the enu-
meration over all possible z with the predicted label
sequence y?i and get, 12 (?i??)
>??1i (?i??) +
1
2r
(
max
{
0, d(yi, y?i)? ? ?
(
?i,y,y?
)})2
. Com-
puting the optimal value of the last equation we get
an update of the form of the first equation of Eq. (2)
where
?i =
max
{
0, d(yi, y?i)? ?i?1 ?
(
?i,y,y?
)}
r + ?>i,y,y??i?1?i,y,y?
. (4)
Dataset Sentences Words Features
NP chunking 11K 259K 1.35M
NER English 17.5K 250K 1.76M
NER Spanish 10.2K 317.6K 1.85M
NER Dutch 21K 271.5K 1.76M
Table 1: Properties of datasets.
AROW CW 5-best PA Perceptron
NP chunking 0.946 0.947 0.946 **0.944
NER English 0.878 0.877 * 0.870 * 0.862
NER Dutch 0.791 0.787 0.784 * 0.761
NER Spanish 0.775 0.774 0.773 * 0.756
Table 2: Averaged F-measure of methods. Statistical sig-
nificance (t-test) are with respect to AROW, where * in-
dicates 0.001 and ** indicates 0.01
We proceed with the confidence paramters in
(Crammer et al, 2009b, Eqs. (1,2)), which takes into
considiration the change of confidence due to the up-
date. The effective features vector that is used to
update the mean parameters is ?i,y,y?, and thus the
structured update is, 12 log
(
det ?i
det ?
)
+12Tr
(
??1i?1?
)
+
1
2r?
>
i,y,y???i,y,y? . Solving the above equation we
get an update of the form of the second term of
Eq. (2) where ?i = 1r . The pseudo-code of CW and
AROW for sequence problems appears in Alg. 1.
4 Evaluation
For the experiments described in this paper we used
four large sequential classification datasets taken
from the CoNLL-2000, 2002 and 2003 shared tasks:
noun-phrase (NP) chunking (Kim et al, 2000),
and named-entity recognition (NER) in Spanish,
Dutch (Tjong and Sang, 2002) and English (Tjong
et al, 2003). The properties of the four datasets
are summarized in Table 1. We followed the feature
generation process of (Sha and Pereira, 2003).
Although our primary goal is estimating confi-
dence in prediction and not the actual performance
itself, we first report the results of using AROW and
CW for sequence learning. We compared the perfor-
mance CW and AROW of Alg. 1 with two standard
online baseline algorithms: Averaged-Perceptron al-
gorithm and 5-best PA (the value of five was shown
to be optimal for various tasks (Crammer et al,
2005)). The update rule described in Alg. 1 assumes
a full covariance matrix, which is not feasible in our
973
0.934 0.936 0.938 0.94 0.942 0.9440.936
0.9380.94
0.9420.944
0.9460.948
Recall
Precisio
n
 
 
PerceptronPACWAROW
(a) NP Chunking
0.81 0.82 0.83 0.84 0.85 0.860.82
0.830.84
0.850.86
0.870.88
0.89
Recall
Precisio
n
 
 
PerceptronPACWAROW
(b) NER English
0.68 0.7 0.72 0.74 0.760.7
0.720.74
0.760.78
0.8
Recall
Precisio
n
 
 
PerceptronPACWAROW
(c) NER Dutch
0.7 0.72 0.74 0.760.7
0.72
0.74
0.76
0.78
Recall
Precisio
n
 
 
PerceptronPACWAROW
(d) NER Spanish
Figure 1: Precision and Recall on four datasets (four panels). Each connected set of ten points corresponds to the performance of
a specific algorithm after each of the 10 iterations, increasing from bottom-left to top-right.
Prec Recall F-meas % Err
CW 0.945 0.942 0.943 2.34%
NP chunking
CRF 0.938 0.934 0.936 2.66%
CW 0.838 0.826 0.832 3.38%
NER English
CRF 0.823 0.820 0.822 3.53%
CW 0.803 0.755 0.778 2.05%
NER Dutch
CRF 0.775 0.753 0.764 2.09%
CW 0.738 0.720 0.729 4.09%
NER Spanish
CRF 0.751 0.730 0.740 2.05%
Table 3: Precision, Recall, F-measure and percentage of
mislabeled words results of CW vs. CRF
setting. Three options are possible: compute a full ?
and then take its diagonal elements; compute a full
inverse ?, take its diagonal elements and then com-
pute its inverse; assume that ? is diagonal and com-
pute the optimal update for this choice. We found
the first method to work best, and thus employ it
from now on.
The hyper parameters (r for AROW, ? for CW, C
for PA) were tuned for each task by a single run over
a random split of the data into a three-fourths train-
ing set and a one-fourth test set. We used parameter
averaging with all methods.
For each of the four datasets we used 10-fold
cross validation. All algorithms (Perceptron, PA,
CW and AROW) are online, and as mentioned above
work in rounds. For each of the ten folds, each of the
four algorithm performed ten (10) iterations over the
training set and the performance (Recall, Precision
and F-measure) was evaluated on the test set after
each iteration.
The F-measure of the four algorithms after 10 it-
erations over the four datasets is summarized in Ta-
ble 2. The general trend is that AROW slightly out-
performs CW, which is better than PA that is bet-
ter than the Perceptron. The difference between
AROW and the Perceptron is significant, and be-
tween AROW and PA is significant in two datasets.
The difference between AROW and CW is not sig-
nificant although it is consistent.
We further investigate the convergence properties
of the algorithms in Fig. 1. The figure shows the re-
call and precision results after each training round
averaged across the 10 folds. Each panel summa-
rizes the results on a single dataset, and in each panel
a single set of connected points corresponds to one
algorithm. Points in the left-bottom of the plot cor-
respond to early iterations and points in the right-top
correspond to later iterations. Long segments indi-
cate a big improvement in performance between two
consecutive iterations.
Few points are in order. First, high (in the y-axis)
values indicate better precision and right (in the x-
axis) values indicate better recall. Second, the per-
formance of all algorithms is converging in about 10
iterations as indicated by the fact the points in the
top-right of the plot are close to each other. Third,
the long segments in the bottom-left for the Percep-
tron algorithm indicate that this algorithm benefits
more from more than one pass compared with the
other. Fourth, on the three NER datasets after 10 it-
erations AROW gets slightly higher precision values
than CW, while CW gets slightly higher recall val-
ues than AROW. This is indicated by the fact that
the top-right red square is left and above to the top-
right blue circle. Finally, in two datasets, PA get
slightly better recall than CW and AROW, but pay-
ing in terms of precision and overall F-measure per-
formance.
In addition to online algorithms we also com-
pared the performance of CW with the CRF algo-
974
NP chunking NER English NER Spanish NER Dutch00.05
0.10.15
0.20.25
0.30.35
0.40.45
0.50.55
 
 
CRFKD?Fixed (K=50)KD?PC (K=50)DeltaWKBV (K=30)KBV (K=30)Random
(a) AvgP CW & CRF
NP chunking NER English NER Spanish NER Dutch0
0.02
0.04
0.06
0.08
0.1
Root Me
an Squa
red Error
 in Confi
dence
 
 
CRFKD?Fixed (K=50)KD?PC (K=50)WKBV (K=30)
(b) RMSE CW & CRF
NP chunking NER English NER Spanish NER Dutch00.05
0.10.15
0.20.25
0.30.35
0.40.45
0.50.55
 
 
KD?Fixed (K=50)DeltaWKBV (K=30)KBV (K=30)Random
(c) AvgP PA
NP chunking NER English NER Spanish NER Dutch0
0.02
0.04
0.06
0.08
0.1
Root Me
an Squa
red Error
 in Confid
ence
 
 
KD?Fixed (K=50)WKBV (K=30)
(d) RMSE PA
Figure 2: Two left panels: average precision of rankings of
the words of the test-set according to confidence in the predic-
tion of seven methods (left to right bars in each group): CRF,
KD-Fixed, KD-PC, Delta, WKBV, KBV and random ordering,
when training with the CW algorithm (top) and the PA algo-
rithm (bottom). Two right panels: The root-mean-squared-error
of four methods that output absolute valued confidence: CRF,
KD-Fixed, KD-PC and WKBV.
rithm which is a batch algorithm. We used Mal-
let toolkit (McCallum, 2002) for CRF implementa-
tion. For feature generation we used a combination
of standard methods provided with Mallet toolkit
(called pipes). We chose a combination yielding a
feature set that is close as possible to the feature
set we used in our system but it was not a perfect
match, CRF generated about 20% fewer features in
all datasets. Nevertheless, any other combination of
pipes we tried only hurt CRF performance. The pre-
cision, recall, F-measure and percentage of misla-
beled words of CW algorithm compared with CRF
measured over a single split of the data into a three-
fourths training set and a one-fourth test set is sum-
marized in Table 3. We see that in three of the four
datasets CW outperforms CRF and in one dataset
CRF performs better. Some of the performance dif-
ferences may be due to the differences in features.
5 Confidence in the Prediction
Most large-margin-based training algorithms output
models that their prediction is a single labeling of
the input, with no additional confidence information
about the correctness of that prediction. This situ-
ation is acceptable when the output of the system
is used anyway, irrespectively of its quality. This
situation is not acceptable when the output of the
system is used as an input of another system that is
sensitive to correctness of the specific prediction or
that integrates various input sources. In such cases,
additional confidence information about the correct-
ness of these feeds for specific input can be used
to improve the total output quality. Another case
where such information is useful, is when there is
additional agent that is validating the output of the
system. The confidence information can be used
to direct the check into small number of suspected
predictions as opposed to random check, which may
miss errors if their rate is small.
Some methods only provide relative confidence
information. This information can be used to rank
all predictions according to their confidence score,
which can be used to direct a quality control com-
ponent to detect errors in the prediction. Note,
the confidence score is meaningless by itself and
in fact, any monotonic transformation of the con-
fidence scores yield equivalent confidence informa-
tion. Other methods are providing confidence in the
predicted output as an absolute information, that is,
the probability of a prediction to be correct. We re-
fer to these probabilistic outputs in a frequentists ap-
proach. When taking a large set of events (predic-
tions) with similar probability confidence value ? of
being correct, we expect that about ? fraction of the
predictions in the group will be correct.
Algorithms: All of our methods to evaluate confi-
dence, except two (Delta and CRF below), share the
same conceptual approach and work in two stages.
First, a method generates a set of K possible label-
ings for the input sentence (instead of a single pre-
diction). Then, the confidence in a predicted label-
ing for a specific word is defined to be the proportion
of labelings which are consistent with the predicted
label. Formally, let z(i) for i = 1 . . .K be the K
labelings for some input x, and let y? be the actual
prediction for the input. (We do not assume that
y? = z(i) for some i). The confidence in the label
y?p of word p = 1 . . . |x| is defined to be
?p = |{i : y?p = z
(i)
p }|/K . (5)
975
1000 2000 3000 4000 50000
200400
600800
1000
Word Index
No. of W
ords Clas
sified Inc
orrectly
 
 CRFKBV (K=30)WKBV (K=30)KD?PC (K=50)KD?Fixed (K=50)DeltaRandom
(a) NP Chunking
1000 2000 3000 4000 50000200
400600
8001000
12001400
1600
Word Index
No. of W
ords Clas
sified Inc
orrectly
 
 CRFKBV (K=30)WKBV (K=30)KD?PC (K=50)KD?Fixed (K=50)DeltaRandom
(b) NER English
1000 2000 3000 4000 50000200
400600
8001000
12001400
Word Index
No. of W
ords Clas
sified Inc
orrectly
 
 CRFKBV (K=30)WKBV (K=30)KD?PC (K=50)KD?Fixed (K=50)DeltaRandom
(c) NER Dutch
1000 2000 3000 4000 50000
500
1000
1500
Word Index
No. of W
ords Clas
sified Inc
orrectly
 
 CRFKBV (K=30)WKBV (K=30)KD?PC (K=50)KD?Fixed (K=50)DeltaRandom
(d) NER Spanish
1000 2000 3000 4000 5000?100?50
050
100150
200250
Word Index
No. of W
ords Clas
sified Inc
orrectly
 
 
CRFKBV (K=30)WKBV (K=30)KD?PC (K=50)KD?Fixed (K=50)DeltaRandom
(e) NP Chunking
1000 2000 3000 4000 5000?100?50
050
100150
200250
300
Word Index
No. of W
ords Clas
sified Inc
orrectly
 
 
CRFKBV (K=30)WKBV (K=30)KD?PC (K=50)KD?Fixed (K=50)DeltaRandom
(f) NER English
1000 2000 3000 4000 5000?100
?50
0
50
100
Word Index
No. of W
ords Clas
sified Inc
orrectly
 
 
CRFKBV (K=30)WKBV (K=30)KD?PC (K=50)KD?Fixed (K=50)DeltaRandom
(g) NER Dutch
1000 2000 3000 4000 5000?100?50
050
100150
200
Word Index
No. of W
ords Clas
sified Inc
orrectly
 
 
CRFKBV (K=30)WKBV (K=30)KD?PC (K=50)KD?Fixed (K=50)DeltaRandom
(h) NER Spanish
Figure 3: Total number of detected erroneous words vs. the number of ranked words (top panels), and relative to the Delta method
(bottom panels). In other words, the lines in the bottom panels are the number of additional erroneous words detected compared to
Delta method. All methods builds on the same weight-vector except CRF (see text).
We tried four approaches to generate the set of K
possible labelings. The first method is valid only
for methods that induce a probability distribution
over predicted labels. In this case, we draw K la-
belings from this distribution. Specifically, we ex-
ploit the Gaussian distribution over weight vectors
w ? N (?,?) maintained by AROW and CW, by
inducing a distribution over labelings given an in-
put. The algorithm samples K weight vectors ac-
cording to this Gaussian distribution and outputs the
best labeling with respect to each weight vector. For-
mally, we define the set Z = {z(i) : z(i) =
arg maxzw ??(x, z) where w ? N (?,?)}
The predictions of algorithms that use the mean
weight vector y? = arg maxz ? ??(x, z) are invari-
ant to the value of the input ? (as noted by (Cram-
mer et al, 2008)). However for the purpose of con-
fidence estimation the specific value of ? has a huge
affect. Small eigenvalue of ? yield that all the ele-
ments of Z will be the same, while large values yield
random elements in the set, ignoring the input.
One possible simple option is to run the algorithm
few times, with few possible initializations of ? and
choose one using the training set. However since the
actual predictions of all these versions is the same
(invariance to scaling, see (Crammer et al, 2008))
in practice we run the algorithm once initializing
? = I . Then, after the training is completed, we
try few scalings of the final covariance s? for some
positive scalar s, and choose the best value s using
the training set. We refer to this method as KD-PC
for K-Draws by Parameters Confidence.
The second method to estimate confidence fol-
lows the same conceptual steps, except that we used
an isotropic covariance matrix, ? = sI for some
positive scale information s. As before, the value
of s was tuned on the training set. We denote this
method KD-Fixed for K Draws by Fixed Stan-
dard Deviation. This method is especially appeal-
ing, since it can be used in combination with training
algorithms that do not maintain confidence informa-
tion, such as the Perceptron or PA.
Our third and fourth methods are deterministic
and do not involve a stochastic process. We mod-
ified the Viterbi algorithm to output the K distinct
labelings with highest score (computed using the
mean weight vector in case of CW or AROW). The
third method assigns uniform importance to each
of the K labelings ignoring the actual score val-
ues. We call this method KBV, for K-best Viterbi.
We thus propose the fourth method in which we de-
fine an importance weight ?i to each labeling z(i)
and evaluate confidence using the weights, ?p =(?
i s.t. y?p=z
(i)
p
?i
)
/ (
?
i ?i) , where we set the
weights to be their score value clipped at zero from
below ?i = max{0,? ? ?(x, z(i))}. (In practice,
976
top score was always positive.) We call this method
WKBV for weighted K-best Viterbi.
In addition to these four methods we propose a
fifth method that is based on the margin and does
not share the same conceptual structure of the previ-
ous methods. This method provide confidence score
that is only relative and not absolute, namely its out-
put can be used to compare the confidence in two
labelings, yet there is no semantics defined over the
scores. Given an input sentence to be labeledx and a
model we define the confidence in the prediction as-
sociated with the pthword to be the difference in the
highest score and the closest score, where we set the
label of that word to anything but the label with the
highest score. Formally, as before we define the best
labeling y? = arg maxz ? ? ?(x, z), then the score
of word p is defined to be, ???(x, y?)?maxu6=y?p ??
?(x, z|zp=u) , where we define the labeling z|zp=u
to be the labeling that agrees with z on all words,
except the pth word, where we define its label to
be u. We refer to this method as Delta where the
confidence information is a difference, aka as delta,
between two score values.
Finally, as an additional baseline, we used a sixth
method based on the confidence values for single
words produced by CRF model. We considered the
marginal probability of the word p to be assigned the
predicted label y?p to be the confide value, this prob-
ability is calculated using the forward-backwards al-
gorithm. This method is close in spirit to the Delta
method as the later can be thought of computing
marginals (in score, rather than probability). It also
close to the K-Draws methods, as both CRF and K-
Draws induce a distribution over labels. For CRF we
can compute the marginals explicitly, while for the
Gaussian models generated by CW (or AROW) the
marginals can not be computed expliclity, and thus a
sample based estimation (K-Draws) is used.
Experimental Setting: We evaluate the above
methods as follows. We trained a classifier using
the CW algorithm running for ten (10) iterations on
three-fourth of the data and applied it to the remain-
ing one-fourth to get a labeling of the test set. There
are between 49K ? 54K words to be labeled in
all tasks, except NER Dutch where there are about
74K words. The fraction of words for which the
trained model makes a mistake ranges between 2%
(for NER Dutch) to 4.1% for NER Spanish.
We set the value of the hyper parameter ? to its
optimal value obtained in the experiments reported
in the previous section. The size ofK of the number
of labelings used in the four first methods (KD-PC,
KD-Fixed, KBV, WKBV) and the weighting scalar
s used in KD-PC and KD-Fixed were tuned for each
dataset on a single evaluation on subset of the train-
ing set according to the best measured average pre-
cision. For the parameter s we tried about 20 values
in the range 0.01 to 1.0, and for the number of labels
K we tried the values in 10, 20 . . . 80. The optimal
values are K = 50 for KD-PC and KD-Fixed, and
K = 30 for KBV and WKBV. We noticed that KD-
PC and KD-Fixed were robust to larger values of K,
while the performance of KBV and WKBV was de-
graded significantly for large values of K.
We also trained CRF on the same training sets and
applied it to label and assign confidence values to
all the words in the test sets. The fraction of mis-
labeled words produced by the CRF model and the
CW model is summarized in Table 3.
Relative Confidence: For each of the datasets,
we first trained a model using the CW algorithm and
applied each of the confidence methods on the out-
put, ranking from low to high all the words of the
test set according to the confidence in the prediction
associated with them. Ideally, the top ranked words
are the ones for which the classifier made a mistake
on. This task can be thought of as a retrieval task of
the erroneous words.
The average precision is the average of the pre-
cision values computed at all ranks of erroneous
words. The average precision for ranking the words
of the test-set according the confidence in the predic-
tion of seven methods appears in the top-left panel of
Fig. 2. (left to right bars in each group : CRF, KD-
Fixed, KD-PC, Delta, WKBV, KBV and random or-
dering.) We see that when ordering the words ran-
domly, the average precision is about the frequency
of erroneous word, which is the lowest average pre-
cision. Next are the two methods based on the best
Viterbi labelings, where the weighted approach out-
performing the non-weighted version. Thus, taking
the actual score value into consideration improves
the ability to detect erroneous words. Next in per-
formance is Delta, the margin-induced method. The
977
0 0.2 0.4 0.6 0.8 10
0.2
0.4
0.6
0.8
1
Expected Accuracy (bin center)
Actual A
ccuracy
 
 
WKBV(K?30)KD?PC (K=50)KD?Fixed (K=50)CRF
(a) NP Chunking
0 0.2 0.4 0.6 0.8 10
0.2
0.4
0.6
0.8
1
Expected Accuracy (bin center)
Actual A
ccuracy
 
 
WKBV(K?30)KD?PC (K=50)KD?Fixed (K=50)CRF
(b) NER English
0 0.2 0.4 0.6 0.8 10
0.2
0.4
0.6
0.8
1
Expected Accuracy (bin center)
Actual A
ccuracy
 
 
WKBV(K?30)KD?PC (K=50)KD?Fixed (K=50)CRF
(c) NER Dutch
0 0.2 0.4 0.6 0.8 10
0.2
0.4
0.6
0.8
1
Expected Accuracy (bin center)
Actual A
ccuracy
 
 
WKBV(K?30)KD?PC (K=50)KD?Fixed (K=50)CRF
(d) NER Spanish
Figure 4: Predicted error in each bin vs. the actual frequency of mistakes in each bin. Best performance is obtained by methods
close to the line y = x (black line) for four tasks. Four methods are compared: weighted K-Viterbi (WKBV), K-draws PC
(KD-PC) and K-draws fixed covariance (KD-Fixed) and CRF.
two best performing among the CW based methods
are KD-Fixed and KD-PC, where the former is bet-
ter in three out of four datasets. When compared
to CRF we see that in two cases CRF outperforms
the K-Draws based methods and in the other two
cases it performs equally. We found the relative suc-
cess of KD-Fixed compared to KD-PC surprising,
as KD-Fixed does not take into consideration the ac-
tual uncertainty in the parameters learned by CW,
and in fact replaced it with a fixed value across all
features. Since this method does not need to as-
sume a confidence-based learning approach we re-
peated the experiment, training a model with the
passive-aggressive algorithm, rather than CW. All
confidence estimation methods can be used except
the KD-PC, which does take the confidence infor-
mation into consideration. The results appear in
the bottom-left panel of Fig. 2, and basically tell
the same story, KD-Fixed outperform the margin
based method (Delta), and the Viterbi based meth-
ods (KBV, WKBV).
To better understand the behavior of the various
methods we plot the total number of detected erro-
neous words vs. the number of ranked words (first
5, 000 ranked words) in the top panels of Fig. 3. The
bottom panels show the relative additional number
of words each methods detects on top of the margin-
based Delta method. Clearly, KD-Fixed and KD-
PC detect erroneous words better than the other CW
based methods, finding about 100 more words than
Delta (when ranking 5, 000 words) which is about
8% of the total number of erroneous words.
Regarding CRF, it outperforms the K-Draws
methods in NER English and NP chunking datasets,
finding about 150 more words, CRF performed
equally for NER Dutch, and performed worse for
NER Spanish finding about 80 less words. We em-
phasize that all methods except CRF were based on
the same exact weight vector, ranking the same pre-
dations, while CRF used an alternative weight vector
that yields different number of erroneous words.
In details, we observe some correlation between
the percentage or erroneous words in the entire set
and the number of erroneous words detected among
the first 5, 000 ranked words. For NP chunking
and NER English datasets, CRF has more erroneous
words compared to CW and it detects more erro-
neous words compared to K-Draws. For NER Dutch
dataset CRF and CW have almost same number of
erroneous words and almost same number of erro-
neous words detected, and finally in NER Spanish
dataset CRF has fewer erroneous words and it de-
tected less erroneous words. In other words, where
there are more erroneous words to find (e.g. CRF in
NP chunking), the task of ranking erroneous words
is easier, and vice-versa.
We hypothesize that part of the performance dif-
ferences we see between the K-Draws and CRF
methods is due to the difference in the number of
erroneous words in the ranked set.
This ranking view can be thought of marking sus-
pected words to be evaluated manually by a human
annotator. Although in general it may be hard for a
human to annotate a single word with no need to an-
notate its close neighbor, this is not the case here. As
the neighbor words are already labeled, and pretty
reliably, as mentioned above.
Absolute Confidence: Our next goal is to eval-
uate how reliable are the absolute confidence val-
ues output by the proposed methods. As before, the
confidence estimation methods (KD-PC, KD-Fixed,
978
KBV, WKBV and CRF) were applied on the entire
set of predicted labels. (Delta method is omitted as
the confidence score it produces is not in [0, 1]).
For each of the four datasets and the five algo-
rithms we grouped the words according to the value
of their confidence. Specifically, we used twenty
(20) bins dividing uniformly the confidence range
into intervals of size 0.05. For each bin, we com-
puted the fraction of words predicted correctly from
the words assigned to that bin. Ultimately, the value
of the computed frequency should be about the cen-
ter value of the interval of the bin. Formally, bin
indexed j contains words with confidence value in
the range [(j ? 1)/20, j/20) for j = 1 . . . 20. Let bj
be the center value of bin j, that is bj = j/20?1/40.
The frequency of correct words in bin j, denoted
by cj is the fraction of words with confidence ? ?
[(j?1)/20, j/20) that their assigned label is correct.
Ultimately, these two values should be the same,
bj = cj , meaning that the confidence information
is a good estimator of the frequency of correct la-
bels. Methods for which cj > bj are too pessimistic,
predicting too high frequency of erroneous labels,
while methods for which cj < bj are too optimistic,
predicting too low frequency of erroneous words.
The results are summarized in Fig 4, one panel
per dataset, where we plot the value of the center-
of-bin bj vs. the frequency of correct prediction cj ,
connecting the points associated with a single algo-
rithm. Four algorithms are shown: KD-PC, KD-
Fixed, WKBV and CRF. We omit the results of the
KBV approach - they were substantially inferior to
all other methods. Best performance is obtained
when the resulting line is close to the line y = x.
From the plots we observe that WKBV is too pes-
simistic as its corresponding line (blue square) is
above the line y = x. CRF method is too optimistic,
its corresponding line is below the line y = x.
The KD-Fixed method is too pessimistic on NER-
Dutch and too optimistic on NER-English. The best
method is KD-PC which, surprisingly, tracks the line
x = y pretty closely. We hypothesis that its superi-
ority is because it makes use of the uncertainty infor-
mation captured in the covariance matrix ? which is
part of the Gaussian distribution.
Finally, these bins plots does not reflect the fact
that different bins were not populated uniformly, the
bins with higher values were more heavily popu-
lated. We thus plot in the top-right of Fig. 2 the
root mean-square error in predicting the bin center
value given by
?(?
j nj(bj ? cj)
2
)
/
(?
j nj
)
,
where nj is the number of words in the jth bin.
We observed a similar trend to the one appeared in
the previous figure. WKBV is the least-performing
method, then KD-Fixed and CRF, and then KD-PC
which achieved lowest RMSE in all four datasets.
Similar plot but when using PA for training appear
in the bottom-right panel of Fig. 2. In this case we
also see that KD-Fixed is better than WKBV, even
though both methods were not trained with an algo-
rithm that takes uncertainty information into consid-
eration, like CW.
The success of KD-PC and KD-Fixed in evaluat-
ing confidence led us to experiment with using sim-
ilar techniques for inference. Given an input sen-
tence, the inference algorithm samplesK times from
the Gaussian distribution and output the best label-
ing according to each sampled weight vector. Then
the algorithm predicts for each word the most fre-
quent label. We found this method inferior to infer-
ence with the mean parameters. This approach dif-
fers from the one used by (Crammer et al, 2009a),
as they output the most frequent labeling in a set,
while the predicted label of our algorithm may not
even belong to the set of predictions.
6 Active Learning
Encouraged by the success of the KD-PC and KD-
Fixed algorithms in estimating the confidence in the
prediction we apply these methods to the task of ac-
tive learning. In active learning, the algorithm is
given a large set of unlabeled data and a small set
of labeled data and works in iterations. On each it-
eration, the overall labeled data at this point is used
to build a model, which is then used to choose new
subset of examples to be annotated.
In our setting, we have a large set of unlabeled
sentences and start with a small set of 50 annotated
sentences. The active learning algorithm is then us-
ing the CW algorithm to build a model, which in turn
is used to rank sentences. The new data items are
then annotated and accumulated to the set of labeled
data points, ready for the next round. Many active
learning algorithms are first computing a prediction
for each of the unlabeled-data examples, which is
979
then used to choose new examples to be labeled. In
our case the goal is to label sentences, which are
expensive to label. We thus applied the following
setting. First, we chose a subset of 9K sentences
as unlabeled training set, and another subset of size
3K for evaluation. After obtaining a model, the al-
gorithm labels random 1, 000 sentences and chose a
subset of 10 sentences using the active learning rule,
which we will define shortly. After repeating this
process 10 times we then evaluate the current model
using the test data and proceed to choose new un-
labeled examples to be labeled. Each method was
applied to pick 5, 000 sentences to be labeled.
In the previous section, we used the confidence
estimation algorithms to choose individual words to
be annotated by a human. This setting is realistic
since most words in each sentence were already clas-
sified (correctly). However, when moving to active
learning, the situation changes. Now, all the words
in a sentence are not labeled, thus a human may need
to label additional words than the one in target, in or-
der to label the target word. We thus experimented
with the following protocol. On each iteration, the
algorithm defines the score of an entire sentence to
be the score of the least confident word in the sen-
tence. Then the algorithm chooses the least confi-
dent sentence, breaking ties by favoring shorter sen-
tences (assuming they contain relatively more infor-
mative words to be labeled than long sentences).
We evaluated five methods, KD-PC and KD-
Fixed mentioned above. The method that ranks
a sentence by the difference in score between the
top- and second-best labeling, averaged over the
length of sentence, denoted by MinMargin (Tong
and Koller, 2001). A similar approach, motivated
by (Dredze and Crammer, 2008), normalizes Min-
Margin score using the confidence information ex-
tracted from the Gaussian covariance matrix, we call
this method MinConfMargin. Finally, We also eval-
uated an approach that picks random sentences to be
labeled, denoted by RandAvg (averaged 5 times).
The averaged cumulative F-measure vs. num-
ber of words labeled is presented in Figs. 5,6. We
can see that for short horizon (small number of sen-
tences) the MinMargin is worse (in three out of four
data sets), while MinConfMargin is worse in NP
Chunking. Then there is no clear winner, but the
KD-Fixed seems to be the best most of the time. The
2000 4000 6000 8000 100000.84
0.850.86
0.870.88
0.890.9
Total No. of Labeled Words
F?Meas
ure
 
 
KD?PC (50)KD?Fixed (K=50)MinMarginMinConfMarginRandAvg
(a) NP Chunking
2000 4000 6000 8000 100000.35
0.40.45
0.50.55
0.6
Total No. of Labeled Words
F?Meas
ure
 
 
KD?PC (50)KD?Fixed (K=50)MinMarginMinConfMarginRandAvg
(b) NER English
1040.9
0.9050.91
0.9150.92
0.9250.93
Total No. of Labeled Words
F?Meas
ure
 
 
KD?PC (50)KD?Fixed (K=50)MinMarginMinConfMarginRandAvg
(c) NP Chunking
1040.62
0.640.66
0.680.7
0.720.74
0.76
Total No. of Labeled Words
F?Meas
ure
 
 
KD?PC (50)KD?Fixed (K=50)MinMarginMinConfMarginRandAvg
(d) NER English
Figure 5: Averaged cumulative F-score vs. total number of
words labeled. The top panels show the results for up to 10, 000
labeled words, while the bottom panels show the results for
more than 10k labeled words.
bottom panels show the results for more than 10k
training words. Here, the random method perform-
ing the worst, while KD-PC and KD-Fixed are the
best, and as shown in (Dredze and Crammer, 2008),
MinConfMargin outperforming MinMargin.
Related Work: Most previous work has fo-
cused on confidence estimation for an entire exam-
ple or some fields of an entry (Culotta and McCal-
lum, 2004) using CRFs. (Kristjansson et al, 2004)
show the utility of confidence estimation is extracted
fields of an interactive information extraction system
by high-lighting low confidence fields for the user.
(Scheffer et al, 2001) estimate confidence of sin-
gle token label in HMM based information extrac-
tion system by a method similar to the Delta method
we used. (Ueffing and Ney, 2007) propose several
methods for word level confidence estimation for the
task of machine translation. One of the methods they
use is very similar to the weighted and non-weighted
K-best Viterbi methods we used with the proper ad-
justments to the machine translation task.
Acknowledgments
The resrach is supported in part by German-Israeli
Foundation grant GIF-2209-1912. KC is a Horev
Fellow, supported by the Taub Foundations. The re-
viewers thanked for their constructive comments.
980
2000 4000 6000 8000 100000.3
0.350.4
0.450.5
0.55
Total No. of Labeled Words
F?Meas
ure
 
 
KD?PC (50)KD?Fixed (K=50)MinMarginMinConfMarginRandAvg
(a) NER Dutch
2000 4000 6000 8000 100000.320.34
0.360.38
0.40.42
0.440.46
0.48
Total No. of Labeled Words
F?Meas
ure
 
 
KD?PC (50)KD?Fixed (K=50)MinMarginMinConfMarginRandAvg
(b) NER Spanish
1040.58
0.60.62
0.640.66
0.680.7
0.72
Total No. of Labeled Words
F?Meas
ure
 
 
KD?PC (50)KD?Fixed (K=50)MinMarginMinConfMarginRandAvg
(c) NER Dutch
104 1050.480.5
0.520.54
0.560.58
0.60.62
0.64
Total No. of Labeled Words
F?Meas
ure
 
 
KD?PC (50)KD?Fixed (K=50)MinMarginMinConfMarginRandAvg
(d) NER Spanish
Figure 6: See Fig. 5
References
[Cesa-Bianchi and Lugosi2006] N. Cesa-Bianchi and
G. Lugosi. 2006. Prediction, Learning, and Games.
Cambridge University Press, New York, NY, USA.
[Collins2002] M. Collins. 2002. Discriminative training
methods for hidden markov models: Theory and ex-
periments with perceptron algorithms. In EMNLP.
[Crammer et al2005] K. Crammer, R. Mcdonald, and
F. Pereira. 2005. Scalable large-margin online learn-
ing for structured classification. Tech. report, Dept. of
CIS, U. of Penn.
[Crammer et al2008] K. Crammer, M. Dredze, and
F. Pereira. 2008. Exact confidence-weighted learning.
In NIPS 22.
[Crammer et al2009a] K. Crammer, M. Dredze, and
A. Kulesza. 2009a. Multi-class confidence weighted
algorithms. In EMNLP.
[Crammer et al2009b] K. Crammer, A. Kulesza, and
M. Dredze. 2009b. Adaptive regularization of
weighted vectors. In NIPS 23.
[Culotta and McCallum2004] A. Culotta and A. McCal-
lum. 2004. Confidence estimation for information ex-
traction. In HLT-NAACL, pages 109?112.
[Dredze and Crammer2008] M. Dredze and K. Crammer.
2008. Active learning with confidence. In ACL.
[Dredze et al2008] M. Dredze, K. Crammer, and
F. Pereira. 2008. Confidence-weighted linear
classification. In ICML.
[Kim et al2000] E.F. Tjong Kim, S. Buchholz, and
K. Sang. 2000. Introduction to the conll-2000 shared
task: Chunking.
[Kristjansson et al2004] T. Kristjansson, A. Culotta,
P. Viola, and A. McCallum. 2004. Interactive infor-
mation extraction with constrained conditional random
fields. In AAAI, pages 412?418.
[Lafferty et al2001] J. Lafferty, A. McCallum, and
F. Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data.
[McCallum2002] Andrew McCallum. 2002. MALLET:
A machine learning for language toolkit. http://
mallet.cs.umass.edu.
[McDonald et al2005a] R.T. McDonald, K. Crammer,
and F. Pereira. 2005a. Flexible text segmentation with
structured multilabel classification. In HLT/EMNLP.
[McDonald et al2005b] Ryan T. McDonald, Koby Cram-
mer, and Fernando C. N. Pereira. 2005b. Online large-
margin training of dependency parsers. In ACL.
[Scheffer et al2001] Tobias Scheffer, Christian Deco-
main, and Stefan Wrobel. 2001. Active hidden
markov models for information extraction. In IDA,
pages 309?318, London, UK. Springer-Verlag.
[Sha and Pereira2003] Fei Sha and Fernando Pereira.
2003. Shallow parsing with conditional random fields.
In Proc. of HLT-NAACL, pages 213?220.
[Shimizu and Haas2006] N. Shimizu and A. Haas. 2006.
Exact decoding for jointly labeling and chunking se-
quences. In COLING/ACL, pages 763?770.
[Taskar et al2003] B. Taskar, C. Guestrin, and D. Koller.
2003. Max-margin markov networks. In nips.
[Tjong and Sang2002] Erik F. Tjong and K. Sang. 2002.
Introduction to the conll-2002 shared task: Language-
independent named entity recognition. In CoNLL.
[Tjong et al2003] E.F. Tjong, K. Sang, and F. De Meul-
der. 2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
CoNLL, pages 142?147.
[Tong and Koller2001] S. Tong and D. Koller. 2001.
Support vector machine active learning with applica-
tions to text classification. In JMLR, pages 999?1006.
[Ueffing and Ney2007] Nicola Ueffing and Hermann Ney.
2007. Word-level confidence estimation for machine
translation. Comput. Linguist., 33(1):9?40.
[Wick et al2009] M. Wick, K. Rohanimanesh, A. Cu-
lotta, and A. McCallum. 2009. Samplerank: Learning
preferences from atomic gradients. In NIPS Workshop
on Advances in Ranking.
981
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 488?497,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Training Dependency Parser Using Light Feedback
Avihai Mejer
Department of Electrical Engineering
Technion-Israel Institute of Technology
Haifa 32000, Israel
amejer@tx.technion.ac.il
Koby Crammer
Department of Electrical Engineering
Technion-Israel Institute of Technology
Haifa 32000, Israel
koby@ee.technion.ac.il
Abstract
We introduce lightly supervised learning for
dependency parsing. In this paradigm, the al-
gorithm is initiated with a parser, such as one
that was built based on a very limited amount
of fully annotated training data. Then, the al-
gorithm iterates over unlabeled sentences and
asks only for a single bit of feedback, rather
than a full parse tree. Specifically, given an
example the algorithm outputs two possible
parse trees and receives only a single bit indi-
cating which of the two alternatives has more
correct edges. There is no direct information
about the correctness of any edge. We show
on dependency parsing tasks in 14 languages
that with only 1% of fully labeled data, and
light-feedback on the remaining 99% of the
training data, our algorithm achieves, on av-
erage, only 5% lower performance than when
training with fully annotated training set. We
also evaluate the algorithm in different feed-
back settings and show its robustness to noise.
1 Introduction
Supervised learning is a dominant paradigm in ma-
chine learning in which a prediction model is built
based on examples, each of which is composed of in-
puts and a corresponding full annotation. In the task
of parsing, examples are composed of sentences in
some language and associated with full parse trees.
These parse trees are often generated by human an-
notators. The annotation process is complex, slow
and prone to mistakes as for each sentence a full cor-
rect feedback is required.
We describe light-feedback learning which suits
learning problems with complex or structured out-
put, like parsing. After building an initial classi-
fier, our algorithm reduces the work of the annota-
tor from a full annotation of the input sentence to
a single bit of information. Specifically, it provides
the annotator with two alternative parses of the in-
put sentence and asks for the single bit indicating
which of the alternatives is better. In 95% of the
sentences both alternatives are identical except for a
single word. See Fig. 2 for an illustration. Thus,
the work of the annotator boils down to deciding
for some specific word in the sentence which of two
possible words should be that word?s head.
We show empirically, through simulation, that us-
ing only 1% of the training set with full annotation,
and the remaining 99% with light annotation, our al-
gorithm achieves an average accuracy of about 80%,
only 5% less than a parser built with full annotated
training data. These results are averaged over 14
languages. With additional simple relaxations, our
algorithm achieves average accuracy of 82.5%, not
far from the performance of an algorithm observing
full annotation of the data. We also evaluate our al-
gorithm under few noise settings, showing that it is
resistant to noise, with a decrease of only 1.5% in
accuracy under about 10% feedback noise. We defer
a discussion of related work to the end of the paper.
2 Dependency Parsing and Parsers
Dependency parsing of a sentence is an intermediate
between shallow-parsing, in which a given sentence
is annotated with its part-of-speech, and between a
full structure over the sentence, such as the ones de-
488
fined using context-free grammar. Given a sentence
with n words a parse tree is defined by constructing
a single directed edge outgoing from each word to
its head, that is the word it depends on according to
syntactic or semantic rules. Additionally, one of the
words of the sentence must be labeled as the root of
the tree. The choice of edges is restricted to induce
trees, i.e. graphs with no loops.
Dependency parsers, such as the MSTParser of
(McDonald et al, 2005), construct directed edges
between words of a given sentence to their argu-
ments. We focus on non-projective parsing with
non-typed (unlabeled) edges. MSTParser produces
a parse tree for a sentence by constructing a full di-
rected graph over the words of the sentence with
weighted edges, and then outputting the maximal
spanning tree (MST) of the graph. Given a true parse
tree (aka as gold labeling) and a predicted parse tree
y?, we evaluate the latter by counting the number of
words that are in agreement with the true labeling.
The MSTParser maintains a linear model for set-
ting the weights of the edges of the full graph. Given
the input sentence x the parser sets the weight of
the edge between words xi and xj to be s(i, j) =
w?f(x, i, j) using a feature function f that maps the
input x and a pair of possibly connected words into
Rd. Example features are the distance between the
two words, words identity and words part-of-speech.
The goal of the learning algorithm is to choose a
proper value of w such that the induced tree for each
sentence x will have high accuracy.
Online Learning: MSTParser is training a model
by processing one example at a time using online
learning. On each round the algorithm receives a
new sentence x and the set of correct edges y. It
then computes the score-value of all possible di-
rected edges, s(i, j) = w ? f (x, i, j) for words i, j
using the current parameters w. The algorithm is
computing the best dependency tree y? of this input
x defined to be the MST of the weighted complete
directed graph induced from the matrix {s(i, j)}. It
then uses the discrepancy between the true parse tree
y and the predicted parse tree y? to modify the weight
vector.
MSTParser specifically employs the MIRA algo-
rithm (Crammer et al, 2006) to update the weight
vector w using a linear update,
w?w+?
?
?
?
(i,j)?y
f(x, i, j)?
?
(i,j)?y?
f(x, i, j)
?
? (1)
for input-dependent scalar ? that is defined by the
algorithm. By construction, correct edges (i, j), that
appear both in the true parse tree y and the predicted
parse tree y?, are not affecting the update, as the
terms in the two sums of Eq. (1) cancel each other.
3 Online Learning with Light Feedback
Supervised learning is a very common paradigm in
machine learning, where we assume having access
to the correct full parse tree of every input sentence.
Many algorithms, including MSTParser, explicitly
assume this kind of feedback. Supervised learn-
ing algorithms achieve good performance in depen-
dency parsing, but they come with a price. Human
annotators are required to fully parse each and ev-
ery sentence in the corpora, a long, tedious and ex-
pensive process, which is also prone to mistakes.
For example, the first phase of the famous penn tree
bank project (Marcus et al, 1993) lasted three years,
in which annotators corrected outputs of automated
machines in a rate of 475 words per hour. For super-
vised learning to be successful, typically a large set
of thousands instances is required, which translates
to a long and expensive annotation phase.
Binary or multi-class prediction tasks, such as
spam filtering or document classification, are sim-
ple in the sense that the label associated with each
instance or input is simple. It is either a single bit
indicating whether the input email is spam or not,
or one of few values from a fixed predefined set if
topics. Dependency parsing is more complex as a
decision is required for every word of the sentence,
and additionally there is a global constraint of the
parse being a tree.
In binary classification or multi-class problems it
is only natural to either annotate (or label) an exam-
ple, or not, since the labels are atomic, they cannot
be decomposed to smaller components. The situa-
tion is different in structured tasks such as depen-
dency parsing (or sequence labeling) where each in-
stance is constructed of many elements that each
needs to be annotated. While there are relations and
489
coupling between the elements it is possible to anno-
tate an instance only partially, such as provide a de-
pendency edge only to several words in a sentence.
We take this approach to the extreme, and con-
sider (for now) that for each sentence only a single
bit of labeling will be provided. The choice of what
bit to require is algorithm and example dependent.
We propose using a light feedback scheme in order
to significantly reduce annotation effort for depen-
dency parsing. First, a base or initial model will be
learned from a very small set of fully annotated ex-
amples, i.e. sentences with full dependency infor-
mation known. Then, in a second training stage the
algorithm works in rounds. On each round the al-
gorithm is provided with a new non-annotated sen-
tence which it annotates, hopefully making the right
decision for most of the words. Then the algorithm
chooses subset of the words (or segments) to be an-
notated by humans. These words are the ones that al-
gorithm estimates to be the hardest, or that their true
label would resolve any ambiguity that is currently
existing with the parsing of the input sentence.
Although such partial annotation task may be eas-
ier and faster to annotate, we realize that even partial
annotation if not limited enough can require eventu-
ally similar effort as annotating the entire sentence.
For example, if for a 25-words sentence annotation
is requested for 5 words scattered over the entire sen-
tence, providing this annotation may require the an-
notator to basically parse the entire sentence.
We thus further restrict the possible feedback re-
quested from the annotator. Specifically, given a
new sentence our algorithm outputs two possible an-
notations, or parse trees, y?A and y?B , and asks for
a single bit from the annotator, indicating whether
parse A is better or parse B is better. We do not
ask the annotator to parse the actual sentence, or de-
cide what is the correct parse, but only to state which
of the parses is quantitatively better. Formally, we
say that parse y?A is better if it contains more correct
edges than y?B . The annotator is asked for a single
bit, and thus must state one of the two parses, even
if both parses are equally good. We denote this la-
beling paradigm as binary, as the annotator provides
binary feedback.
The two parses our algorithms presents to the an-
notator are the highest ranking parse and the sec-
ond highest ranking parse according to the current
model. That is, the parse it would output for x and
the best alternative. The feedback required from the
annotator is only which of the two parses is better,
the annotator does not explicitly indicate which of
the edges are labeled correctly or incorrectly, and
furthermore, the annotator does not provide any ex-
plicit information about the correct edge of any of
the words.
In general, the two alternative parses presented
to the annotator may be significantly different from
each other; they may disagree on the edges of many
words. In this case the task of deciding which of
them is better may be as hard as annotating the en-
tire sentence, and then comparing the resulting an-
notation to both alternatives. In practice, however,
due to our choice of features (as functions of the two
words) and model (linear), and since our algorithm
chooses the two parse-trees ranked highest and sec-
ond highest, the difference between the two alterna-
tives is very small. In fact, we found empirically
that, on average, in 95% of the sentences, they differ
in the labeling of only a single word. That is, both
y?A and y?B agree on all words, except some word xi,
for which the first alternative assigns to some word
xj and the second alternative assign to other word
xk. This is due to the fact that the score of the parses
are additive in the edges. Therefore, the parse tree
ranked second highest is obtained from the highest-
ranked parse tree, where for a single word the edge is
replaced, such that the difference between scores is
minimized. For the remaining 5% of the sentences,
replacing an edge as described causes a loop in the
graph induced over words, and thus more than a sin-
gle edge is modified. To minimize the potential labor
of the annotator we simply ignore these cases, and
present the annotator only two alternatives which are
different in a single edge. We refer to this setting or
scenario as single.
To conclude, given a new non-annotated sentence
x the algorithm uses its current model w to out-
put two annotations y?A and y?B which are different
only on a single word and ask the annotator which
is better. The annotator should decide to which of
two possible words xj and xk to connect the word
xi in question. The annotator then feeds the algo-
rithm a single bit, i.e. a binary labeling, which rep-
resents which alternative is better, and the algorithm
updates its internal model w. Although it may be the
490
Input data A set of n unlabeled sentences {xi}ni=1
Input parameters Initial weight vector learned from
fully annotated data u; Number of Iterations over the un-
labeled data T
Initialize w ? u
For t = 1, . . . , T
? For i = 1, . . . , n
? Compute the two configurations y?A and y?B
with highest scores of xi using w
? Ask for feedback : y?A vs. y?B
? Get feedback ? ? {+1,?1}
(or ? ? {+1, 0,?1} in Sec. 5)
? Compute the value of ? using the MIRA algo-
rithm ( or just set ? = 1 for simplicity)
? Update
w+??
?
(i,j)?(y?A/y?B?y?B/y?A)
(?1)[[(i,j)?y?B ]]f(x, i, j)
Output: Weight vector w
Figure 1: The Light-Feedback learning algorithm
case that both alternatives are equally good (or bad),
which occurs only when both assign the wrong word
to xi, that is not xj nor xk are the correct dependents
of xi, the annotator is still required to respond with
one alternative, even though a wrong edge is recom-
mended. Although this setting may induce noise, we
consider it since a human annotator, that is asked to
provide a quick light feedback, will tend to choose
one of the two proposed options, the one that seems
more reasonable, even if it is not correct. We refer to
this combined setting of receiving a binary feedback
only about a single word as Binary-Single. Below
we discuss alternative models where the annotator
may provide additional information, which we hy-
pothesize, would be for the price of labor.
Finally, given the light-feedback ? from the anno-
tator, where ? = +1 if the first parse y?A is preferred
over the second parse y?B , and ? = ?1 otherwise,
we employ a single online update,
w ?w + ??
?
?
?
(i,j)?y?A
f(x, i, j)?
?
(i,j)?y?B
f(x, i, j)
?
?
Pseudocode of the algorithm appears in Fig. 1.
From the last equation we note that the update de-
pends only on the edges that are different between
Figure 2: Example of single edge feedback. The solid blue ar-
rows describe the proposed parse and the two dashed red arrows
are the requested light feedback.
the two alternatives. This provides us the flexibil-
ity of what to show the annotator. One extreme is
to provide the annotator with (almost) a full depen-
dency parse tree, that both alternatives agree on, as
well as the dilemma. This provides the annotator
some context to assist of making a right decision and
fast. The other extreme, is to provide the annotator
only the edges for which the algorithm is not sure
about, omitting any edges both alternatives agree on.
This may remove labeling noise induced by erro-
neous edges both alternatives mistakenly agree on.
Formally, these options are equivalent, and the de-
cision which to use may even be dependent on the
individual annotator.
An example of a light-feedback request is shown
in Fig. 2. The sentence is 12 words long and
the parser succeeded to assign correct edges for 11
words. It was uncertain whether there was a ?sale by
first boston corps? - having the edge ?by?sale? (in-
correct), or there was an ?offer by first boston corps?
- having the edge ?by?offered? (correct). In this
example, a human annotator can easily clarify the
dilemma.
4 Evaluation
We evaluated the light feedback model using 14 lan-
guages: English (the Penn Tree Bank) and the re-
maining 13 were used in CoNLL 2006 shared task1.
The number of training sentences in the training
datasets is ranging is between about 1.5?57K, with
an average of about 14K sentences and 50K?700K
words. The test sets contain an average of ? 590
sentences and ?10K words for all datasets. The av-
erage number of words per sentence vary from 6 in
Chinese to 37 in Arabic.
1Arabic, Bulgarian, Chinese, Czech, Danish, Dutch, Ger-
man, Japanese, Portuguese, Slovene, Spanish, Swedish and
Turkish . See http://nextens.uvt.nl/?conll/
491
Experimental Setup For each of the languages
we split the data into two parts of relative fraction of
p and 1?p for p = 10%, 5% and 1% and performed
training in two stages. First, we used the smaller set
to build a parser using standard supervised learning
procedure. Specifically, we used MSTParser and ran
the MIRA online learning algorithm for 5 iterations.
This process yielded our initial parser. Second, the
larger portion, which is the remaining of the training
set, was used to improve the initial parser using the
light feedback algorithm described above. Our algo-
rithm iterates over the sentences of the larger subset
and each sentence was parsed by the current parser
(parameterized by w) and asked for a preference be-
tween two specific parses for that sentence. Given
this feedback, the algorithm updated its model and
proceeded for the next sentence. The true parse of
these sentences was only used to simulate light feed-
back and it was never provided to the algorithm. The
performance of all the trained parsers was evaluated
on a fixed test set. We performed five iterations of
the larger subset during the light feedback training.
4.1 Results
The results of the light-feedback training after only
a single iteration are given in the two left plots of
Fig. 3. One plot shows the performance averaged
over all languages, and second plot show the results
for English. The black horizontal line shows the ac-
curacy achieved by training the parser on the entire
annotated data using the MIRA algorithm for 10 it-
erations. The predicted edge accuracy of the parser
trained on the entire annotated dataset ranges from
77% on Turkish to 93% on Japanese, with an aver-
age of 85%. This is our skyline.
The blue bars in each plot shows the accuracy of
a parser trained with only a fraction of the dataset
- 10% (left group), via 5% (middle) to 1% (right
group). As expected reducing the amount of training
data causes degradation in performance, from an ac-
curacy of about 76.3% (averaged over all languages)
via 75% to 70.1% when training only with 1% of the
data. These performance levels are our baselines,
one per specific amount of fully annotated data and
lightly annotated data.
The red bar in each pair, shows the contribution
of a single training epoch with light-feedback on the
performance. We see that training with light feed-
back improves the performance of the final parser.
Most noticeably, is when using only 1% of the fully
annotated data for initial training, and the remaining
99% of the training data with light feedback. The
accuracy on test set improves from 70.1% to 75.6%,
an absolute increase of 5.5%. These results are av-
eraged over all languages, individual results for En-
glish are also shown. In most languages, including
those not shown, these trends remain: when reduc-
ing the fraction of data used for fully supervised
training the performance decreases, and light feed-
back improves it, most substantially for the smallest
fraction of 1%.
We also evaluated the improvement in accuracy
on the test set by allowing more than a single itera-
tion over the larger fraction of the training set. The
results are summarized in two right plots of Fig. 3,
accuracy averaged over all languages (left), and for
English (right). Each line refers to a different ratio
of split between full supervised learning and light
feedback learning - blue for 90%, green for 95% and
red for 99%. The x-axis is the number of light feed-
back iterations, from zero up to five. The y-axis is
the accuracy. In general more iterations translates to
improvement in performance. For example, build-
ing a parser with only 1% of the training data yields
70.1% accuracy on the test set, a single iteration of
light-feedback on the remaining 99% improves the
performance to 75.6%, each of the next iterations
improves the accuracy by about 1? 2% up to an ac-
curacy of about 80%, which is only 5% lower than
the skyline. We note again, that the skyline was ob-
tained by using full feedback on the entire training
set, while our parser used at most five bits of feed-
back per sentence from the annotator, one bit per it-
eration.
As noted above, on each sentence, and each it-
eration, our algorithm presents a parsing query or
?dilemma?: should word a be assigned to word b
or word c. These queries are generated indepen-
dently of the previous queries shown, and in fact the
same query may be presented again in a later iter-
ation although already shown in an early one. We
thus added a memory storage of all queries to the
algorithm. When a query is generated by the algo-
rithm, it first checks if an annotation of it already
exists in memory. If this is the case, then no query is
issued to the annotator, and the algorithm simulates
492
90 95 9970
7274
7678
8082
8486
Average
%LightFeedback
Accurac
y on tes
t set
 
 
w/o light feedbackw light feedbackwith ALL data 90 95 9978
8082
8486
8890
English
%LightFeedback
Accurac
y on tes
t set
 
 
w/o light feedbackw light feedbackwith ALL data 0 1 2 3 4 570
7274
7678
8082
8486
Average
Iteration
Accurac
y on test
 set
 
 
909599with ALL data 0 1 2 3 4 578
8082
8486
8890
English
Iteration
Accurac
y on test
 set
 
 
909599with ALL data
Figure 3: Two left plots: Evaluation in Binary-Single light feedback setting. Averaged accuracy over all languages (left) and for
English. The horizontal black line shows the accuracy when training the parser on the entire annotated training data - ?skyline?.
Each pair of bars shows the results for a parser trained with small amount of fully annotated data (left blue bar) and a parser that
was then trained with a single iteration of light feedback on the remaining training data (right red bar). Two right plots: Evaluation
of training with up to five iterations of binary-single light feedback. The plots show the average accuracy (left), and for English.
Each line refers to a different ratio of split between full supervised learning and light feedback learning. The x-axis is the number
of iterations of light feedback, from zero to five.
a query and response using the stored information.
The fraction of new queries, that were actually
presented to the annotator, when light-training with
99% of the training set, is shown in the left panel
of Fig. 4. Each line corresponds to one language.
The languages are ordered in the legend according
to the average number of words per sentence: from
Chinese (6) to Arabic (37). Each point shows the
fraction of new queries (from the total number of
sentences with light-feedback) (y-axis) vs. the itera-
tion (x-axis). Two trends are observed. First, in later
iterations there are less and less new queries (or need
for an actual interaction with the annotator). By def-
inition, all queries during the first iteration are new,
and the fraction of new queries after five iteration
ranges from about 20% (Japanese and Chinese) to a
bit less than 80% (Arabic).
The second trend is across the average number of
words per sentence, the larger this number is, the
more new queries there are in multiple iterations.
For example, in Arabic (37 words per sentence) and
Spanish (28) about 80% of the light-training sen-
tences induce new queries in the fifth iteration, while
in Chinese (6) and Japanese (8) only about 20%.
As expected, longer sentences require, on average,
more queries before getting their parse correctly.
We can also compare the performance improve-
ment achieved by light feedbacks with the per-
formance achieved by using the same amount of
labeled-edges using fully annotated sentences in
standard supervised training. The average sentence
length across all languages is 18 words. Thus, re-
ceiving feedback regarding a single word in a sen-
tence equals to about 1/18 ? 5.5% of the informa-
tion provided by a fully annotated sentence. There-
fore, we may view the light-feedback provided for
99% of the dataset as about equal to additional 5.5%
of fully annotated data.
From the second plot from the right of Fig. 3, we
see that by training with 1% of fully annotated data
and a single iteration of light feedback over the re-
maining 99% of the data, the parser performance
is 75.6% (square markers at x = 1), compared to
75% obtained by training with 5% of fully anno-
tated data (diamond markers at x = 0). A second
iteration of light feedback on 99% of the dataset
can be viewed as additional . 5% of labeled data
(accounting for repeating queries). After the sec-
ond light feedback iteration, the parser performance
is 77.8% (square markers at x = 2), compared to
76.3% achieved when training with 10% of fully an-
notated data (circle markers at x = 0). Similar rela-
tions can be observed for English in the right plot of
Fig. 3. From these observations, we learn that on av-
erage, for about the same amount of labeled edges,
light feedback learning gains equal, or even better,
performance compared with fully labeled sentences.
5 Light Feedback Variants
Our current model is restrictive in two ways: first,
the algorithm does not pass to the annotators exam-
ples for which the disagreements is larger than one
word; and second, the annotator must prefer one of
the two alternatives. Both restrictions were set to
make the annotators? work easier. We now describe
the results of experiments in which one or even both
493
1 2 3 4 50
0.10.2
0.30.4
0.50.6
0.70.8
0.91
IterationsF
raction
 of Ne
w Edg
es Que
ried by
 the An
notato
r
 
 
chinese ( 6)japanese ( 8)turkish (12)dutch (14)bulgarian (15)swedish (15)slovene (16)danish (18)portuguese (20)english (23)spanish (28)arabic (37) 90 95 990.78
0.790.8
0.810.82
0.830.84
0.85
% of Light feedback data
Accura
cy on 
test se
t
 
 
Binary?SingleBinary?MultiTernary?SingleTernary?Multi 0 5 10 15 20 25 300.7
0.720.74
0.760.78
0.80.82
0.84
% of feedback noise
Accura
cy on t
est set
 
 Binary?MultiTernary?Multi
Figure 4: Left: the fraction of new queries presented to the annotator after each of the five iterations (x-axis) for all 14 languages,
when light-training with 99% of the entire training data. Middle: comparison of the accuracy achieved using the four light feedback
models using different fraction of the data for light feedback stage. The results are averaged over all the languages. Right: Effect of
light-feedback noise on the accuracy of the trained model. Results are averaged over all languages for two light feedback settings,
the ternary-multi and binary-multi. The plots show the performance measured on test set according the amount of feedback noise
added. The black line is the baseline of the initial parser trained on 1% of annotated data.
restrictions are relaxed, which may make the work
of the annotator harder, but as we shall see, improves
performance.
Our first modification is to allow the algorithm to
pass the annotator also queries on two alternatives
y?A and y?B that differ on more than a single edge.
As mentioned before, we found empirically that this
arises in only ?5% of the instances. In most cases
the two alternatives differ in two edges, but in some
cases the alternatives differ in up to five edges. Typ-
ically when the alternatives differ on more than a
single edge, the words in question are close to each
other in the sentence (in terms of word-distance)
and are syntactically related to each other. For ex-
ample, if changing the edge (i, j) to (i, k) forms a
loop in the dependency graph then also another edge
(k, l) must be changed to resolve the loop, so the
two edges different between the alternatives are re-
lated. Nevertheless, even if the two alternatives are
far from being similar, the annotator is still required
to provide only a binary feedback, indicating a strict
preference between the two alternatives. We refer to
this model as Binary-Multi, for binary feedback and
possibly multiple different edge between the alter-
natives.
Second, we enrich the number of possible re-
sponses of the annotator from two to three, giving
the annotator the option to respond that the two al-
ternatives y?A and y?B are equally good (or bad), and
no one should be preferred by the other. In this case
we set ? = 0 in the algorithm of Fig. 1, and as can
be seen in the pseudocode, this case does not modify
the weight vector w associated with the parser. Such
feedback will be received when both parses have the
same number of errors. (We can also imagine a hu-
man annotator using the equal feedback to indicate
?don?t know?). For the common case of single edge
difference between the two parses, this means that
both proposed edges are incorrect. Since there are
three possible responds we call this setting ternary.
This setting can be combined with the previous one
and thus we have in fact two new settings. The third
setting is when only single edges are presented to the
annotator, yet three possible responds are optional.
We call this setting Ternary-Single . The fourth, is
when the two alternatives may differ in more than a
single edge and three possible responds are optional
- Ternary-Multi setting.
The accuracy, averaged over all 14 languages, af-
ter 5 light feedback iterations, for all four settings
is shown in the middle plate of Fig. 4. Each of
the three groups summarizes the results for differ-
ent split of the training set to full training and light-
training: 90%, 95% and 99% (left to right; portion
of light training). The horizontal black line shows
the accuracy skyline (85% obtained by using all the
training set in full supervised learning). Each bar in
each group shows the results for one of the four set-
tings: Binary-Single, Binary-Multi, Ternary-Single
and Ternary-Multi. We focus our discussion in the
99% split. The averaged accuracy using Binary-
Single feedback setting is about 80% (left bar). Re-
laxing the type of input to include alternatives that
differ on more than one edge, improves accuracy by
1.4% (second bar from left). Slightly greater im-
provement is shown when relaxing the type of feed-
494
back, from binary to ternary (third bar from left).
Finally, relaxing both constraints yields an improve-
ment of additional 1% to an averaged accuracy of
82.5% which is only 2.5% lower than the skyline.
Moving to the other splits of 95, 90% we observe
that relaxing the feedback from binary to ternary im-
proves the accuracy more than requiring to provide a
preference of parses that differ on more than a single
word.
6 Noisy Light Feedback
In the last section we discussed relaxations that re-
quires slightly more effort from the annotator to gain
higher test accuracy. The intent of the light feed-
back is to build a high-accuracy parser, yet faster
and with less human effort compared with full su-
pervised learning, or alternatively, allow collecting
feedbacks from non-experts. We now evaluate the
effect of light-feedback noise, which may be a con-
sequence of asking the annotator to perform quick
(and rough) light-feedback. We experiment with two
settings in which the feedback of the annotator is
either binary or ternary, in the multi settings, when
99% of the training-data is used for light-feedback.
These settings refer to the second and fourth bar in
the right group of the middle plate of Fig. 4.
We injected independent feedback errors to a frac-
tion of  of the queries, where  is ranging between
0? 30%. In the Binary-Multi setting, we flipped the
binary preference with probability . For example, if
y?A is better than y?B then with probability  the light
feedback was the other way around. In the Ternary-
Multi setting we changed the correct feedback to one
of the other two possible feedbacks with probabil-
ity , the specific alternative chosen was chosen uni-
formly. E.g., if indeed y?A is preferred over y?B , then
with probability /2 the feedback was that y?B is pre-
ferred and with probability /2 that both are equal.
The accuracy vs. noise level for both settings is
presented in the right panel of Fig. 4. The black line
shows the baseline performance after training an ini-
tial parser on 1% of annotated data. Performance
of the parser trained using the Binary-Multi setting
drops by only 1% from 81.4% to 80.4% at error rate
of 5% and eventually as the feedback noise increase
to 30% the performance drops to 70% - the perfor-
mance level achieved by the initial trained model.
The accuracy of the parser trained in the richer
Ternary-Multi setting suffers only 1% performance
decrease at error rate of 10%, and eventually 5% de-
crease from 82.5% to 77.5% as the feedback noise
increase to 30%, still a 7.5% improvement over the
initial trained parser.
We hypothesize that learning with ternary feed-
back is more robust to noise, as in half of the noisy
feedbacks when there is a strict preference between
the alternatives, the effect of the noise is not to
update the model and practically ignore the input.
Clearly, this is preferable than the other outcome
of the noise, that forces the algorithm to make the
wrong update with respect to the true preference.
We also experimented with sentence depended
noise by training a secondary parser on a subset of
the training set, and emulating the feedback-bit us-
ing its output. Its averaged test error (=noise level)
is 22%. Yet, the accuracy obtained by our algo-
rithm with it is 77%, about the same as achieving
with 30% random annotation noise. We hypothesize
this is since the light-feedbacks are requested specif-
ically on the edges harder to predict, where the error
rate is higher than the 22% average error rate of the
secondary parser.
7 Related work
Weak-supervision, semi-supervised and active
learning (e.g. (Chapelle et al, 2006), (Tong and
Koller, 2001)) are general approaches related to the
light-feedback approach. These approaches build
on access to a small set of labeled examples and a
large set of unlabeled examples.
The work of Hall et al (2011) is the most simi-
lar to the light feedback settings we propose. They
apply an automatic implicit feedback approach for
improving the performance of dependency parsers.
The parser produces the k-best parse trees and an
external system that uses these parse trees provides
feedback as a score for each of the parses. In our
work, we focus on minimal updates by both restrict-
ing the number of compared parses to two, and hav-
ing them being almost identical (up to a single edge).
Hwa (1999) investigates training a phrase struc-
ture parser using partially labeled data in several set-
tings. In one of the settings, a parser is first trained
using a large fully labeled dataset from one domain
495
and then adapted to another domain using partial la-
beling. The parts of the data that are labeled are se-
lected in one of two approaches. In the first approach
phrases are randomly selected to be annotated. In
the second approach the phrases are selected accord-
ing to their linguistic categories based on predefined
rules. In both cases, the true phrases are provided. In
our work, we train the initial parser on small subset
of the data from the same domain. Additionally, the
feedback queries are selected dynamically according
to the edges estimated to be hardest for the parser.
Finally, we request only limited feedback and the
true parse is never provided directly.
Chang et al (2007) use a set of domain specific
rules as automatic implicit feedback for training in-
formation extraction system. For example, they use
a set of 15 simple rules to specify the expected for-
mats of fields to be extracted from advertisements.
The light feedback regarding a prediction is the
number of rules that are broken. That feedback is
used to update the prediction model.
Baldridge and Osborne (2004) learns an HPSG
parser using active learning to choose sentences to
be annotated from a large unlabeled pool. Then,
like our algorithm the annotator is presented with a
proposed parse with several local alternatives sub-
parse-trees. Yet, the annotator manually provides
the correct parse, if it is not found within the pro-
posed alternatives. Kristjansson et al (2004) em-
ploy similar approach of combining active learning
with corrective feedback for information extraction.
Instances with lowest confidence using the current
model are chosen to be annotated. Few alternative
labels are shown to the user, yet again, the correct
labeling is added manually if needed. The alterna-
tives shown to the user are intended to reduce the
effort of obtaining the right label, but eventually the
algorithm receives the correct prediction. Our al-
gorithm is passive about examples (and active only
about subset of the labels), while their algorithm
uses active learning to also choose examples. We
plan to extend our work in this direction. Addition-
ally, in these works, the feedback requests involve
many alternatives and providing the true annotation,
in oppose to the limited binary or ternary feedback.
Yet our results show that despite of these limitations
the trained parser achieved performance nor far from
the performance of a parser training using the entire
annotated dataset.
Finally, our setting is related to bandits (Cesa-
Bianchi and Lugosi, 2006) where the feedback is
extremely limited, a binary success-failure bit.
8 Summary
We showed in a series of experimental simulations
that using light-feedback it is possible to train a de-
pendency parser that achieves parsing performance
not far from standard supervised training. Further-
more, very little amount of fully annotated data,
even few tens of sentences, is sufficient for build-
ing an initial parser which can then be significantly
improved using light-feedbacks.
While light-feedback training and standard super-
vised training with about the same number of to-
tal annotated edges may achieve close performance,
we still view it as a possible alternative training
framework. The reduction of the general annota-
tion task into focused and small feedback requests,
opens possibilities for receiving these feedbacks be-
yond expert labeling. In our ongoing work we study
feedbacks from a large group of non-experts, and
possibly even automatically. Additionally, we inves-
tigate methods for selecting light-feedback queries
that are not necessarily derived from the highest
scoring parse and the best alternative parse. For ex-
ample, selecting queries that would be easy to an-
swer by non-experts.
496
References
[Baldridge and Osborne2004] J. Baldridge and M. Os-
borne. 2004. Active learning and the total cost of
annotation. In EMNLP.
[Cesa-Bianchi and Lugosi2006] N. Cesa-Bianchi and
G. Lugosi. 2006. Prediction, Learning, and Games.
Cambridge University Press, New York, NY, USA.
[Chang et al2007] Ming-Wei Chang, Lev Ratinov, and
Dan Roth. 2007. Guiding semi-supervision with
constraint-driven learning. In In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics.
[Chapelle et al2006] O. Chapelle, B. Scho?lkopf, and
A. Zien, editors. 2006. Semi-Supervised Learning.
MIT Press, Cambridge, MA.
[Crammer et al2006] K. Crammer, O. Dekel, J. Keshet,
S. Shalev-Shwartz, and Y. Singer. 2006. Online
passive-aggressive algorithms. JMLR, 7:551?585.
[Hall et al2011] Keith Hall, Ryan McDonald, Jason
Katz-Brown, and Michael Ringgaard. 2011. Training
dependency parsers by jointly optimizing multiple ob-
jectives. In In Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics.
[Hwa1999] Rebecca Hwa. 1999. Supervised grammar
induction using training data with limited constituent
information. CoRR, cs.CL/9905001.
[Kristjansson et al2004] T. Kristjansson, A. Culotta,
P. Viola, and A. McCallum. 2004. Interactive infor-
mation extraction with constrained conditional random
fields. In AAAI, pages 412?418.
[Marcus et al1993] Mitchell P. Marcus, Mary Ann
Marcinkiewicz, and Beatrice Santorini. 1993. Build-
ing a large annotated corpus of english: the penn tree-
bank. Comput. Linguist., 19:313?330, June.
[McDonald et al2005] R. McDonald, F. Pereira, K. Rib-
arov, and J. Hajic. 2005. Non-projective depen-
dency parsing using spanning tree algorithms. In
HLT/EMNLP.
[Tong and Koller2001] S. Tong and D. Koller. 2001.
Support vector machine active learning with applica-
tions to text classification. In JMLR, pages 999?1006.
497
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 573?576,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Are You Sure? Confidence in Prediction of Dependency Tree Edges
Avihai Mejer
Department of Electrical Engineering
Technion-Israel Institute of Technology
Haifa 32000, Israel
amejer@tx.technion.ac.il
Koby Crammer
Department of Electrical Engineering
Technion-Israel Institute of Technology
Haifa 32000, Israel
koby@ee.technion.ac.il
Abstract
We describe and evaluate several methods for
estimating the confidence in the per-edge cor-
rectness of a predicted dependency parse. We
show empirically that the confidence is asso-
ciated with the probability that an edge is se-
lected correctly and that it can be used to de-
tect incorrect edges very efficiently. We eval-
uate our methods on parsing text in 14 lan-
guages.
1 Introduction
Dependency parsers construct directed edges be-
tween words of a given sentence to their arguments
according to syntactic or semantic rules. We use
MSTParser of McDonald et al (2005) and focus
on non-projective dependency parse trees with non-
typed (unlabeled) edges. MSTParser produces a
parse tree for a sentence by constructing a full, di-
rected and weighted graph over the words of the
sentence, and then outputting the maximal spanning
tree (MST) of the graph. A linear model is em-
ployed for computing the weights of the edges using
features depending on the two words the edge con-
nects. Example features are the distance between the
two words, words identity and words part-of-speech.
MSTParser is training a model using online learning
and specifically the MIRA algorithm (Crammer et
al., 2006). The output of MSTParser is the highest
scoring parse tree, it is not accompanied by any ad-
ditional information about its quality.
In this work we evaluate few methods for estimat-
ing the confidence in the correctness of the predic-
tion of a parser. This information can be used in
several ways. For example, when using parse trees
as input to another system such as machine transla-
tion, the confidence information can be used to cor-
rect inputs with low confidence. Another example
is to guide manual validation to outputs which are
more likely to be erroneous, saving human labor.
We adapt methods proposed by Mejer and Cram-
mer (2010) in order to produce per-edge confidence
estimations in the prediction. Specifically, one ap-
proach is based on sampling, and another on a gen-
eralization of the concept of margin. Additionally,
we propose a new method based on combining both
approaches, and show that is outperforms both.
2 Confidence Estimation In Prediction
MSTParser produces the highest scoring parse trees
using the trained linear model with no additional
information about the confidence in the predicted
tree. In this work we compute per-edge confidence
scores, that is, a numeric confidence value, for
all edges predicted by the parser. Larger score
values indicate higher confidence. We use three
confidence estimation methods that were proposed
for sequence labeling (Mejer and Crammer, 2010),
adapted here for dependency parsing. A fourth
method, described in Sec. 3, is a combination of the
two best performing methods.
The first method, named Delta, is a margin-based
method. For computing the confidence of each edge
the method generates an additional parse-tree, which
is the best parse tree that is forced not to contain the
specific edge in question. The confidence score of
the edge is defined as the difference in the scores be-
573
tween the two parse trees. The score of a tree is the
sum of scores of the edges it contains. These con-
fidence scores are always positive, yet not limited
to [0, 1]. Delta method does not require parameter
tuning.
The second method, named Weighted K-Best
(WKB), is a deterministic method building on prop-
erties of the inference algorithm. Specifically,
we use k-best Maximum Spanning Tree algorithm
(Hall, 2007) to produce the K parse trees with the
highest score. This collection of K-trees is used to
compute the confidence in a predicted edge. The
confidence score is defined to be the weighted-
fraction of parse trees that contain the edge. The
contribution of different trees to compute this frac-
tion is proportional to their absolute score, where the
tree with the highest score has the largest contribu-
tion. Only trees with positive scores are included.
The computed score is in the range [0, 1]. The value
of K was tuned using a development set (optimiz-
ing the average-precision score of detecting incor-
rect edges, see below) and for most datasets K was
set to a value between 10? 20.
The third method, K Draws by Fixed Standard
Deviation (KD-Fix) is a probabilistic method. Here
we sample K weight vectors using a Gaussian dis-
tribution, for which the mean parameters are the
learned model and isotropic covariance matrix with
fixed variance s2. The value s is tuned on a develop-
ment set (optimizing the average-precision score of
detecting incorrect edges). The confidence of each
edge is the probability of this edge induced from the
distribution over parameters. We approximate this
quantity by sampling K parse trees, each obtained by
finding the MST when scores are computed by one
of K sampled models. Finally, the confidence score
of each edge predicted by the model is defined to
be the fraction of parse trees among the K trees that
contain this edge. Formally, the confidence score is
? = j/K where j is the number of parse trees that
contain this edge (j ? {0. . .K}) so the score is in
the range [0, 1]. We set K = 50.
Finally, we describe below a fourth method,
we call KD-Fix+Delta, which is a weighted-linear
combination of KD-Fix and Delta.
3 Evaluation
We evaluated the algorithms using 13 languages
used in CoNLL 2006 shared task1, and the English
Penn Treebank. The number of training sentences is
between 1.5-72K, with an average of 20K sentences
and 50K-1M words. The test sets contain ? 400
sentences and ?6K words for all datasets, except
English with 2.3K sentences and 55K words. Pa-
rameter tuning was performed on development sets
with 200 sentences per dataset. We trained a model
per dataset and used it to parse the test set. Pre-
dicted edge accuracy of the parser ranges from 77%
on Turkish to 93% on Japanese, with an average of
85%. We then assigned each predicted edge a confi-
dence score using the various confidence estimation
methods.
Absolute Confidence: We first evaluate the accu-
racy of the actual confidence values assigned by all
methods. Similar to (Mejer and Crammer, 2010) we
grouped edges according to the value of their con-
fidence. We used 20 bins dividing the confidence
range into intervals of size 0.05. Bin indexed j
contains edges with confidence value in the range
[ j?120 ,
j
20 ] , j = 1..20. Let bj be the center value of
bin j and let cj be the fraction of edges predicted
correctly from the edges assigned to bin j. For a
good confidence estimator we expect bj ? cj .
Results for 4 datasets are presented in Fig. 1. Plots
show the measured fraction of correctly predicted
edges cj vs. the value of the center of bin bj . Best
performance is obtained when a line corresponding
to a method is close to the line y = x. Results are
shown for KD-Fix and WKB; Delta is omitted as it
produces confidence scores out of [0, 1]. In two of
the shown plots (Chinese and Swedish) KD-Fix (cir-
cles) follows closely the expected accuracy line. In
another plot (Danish) KD-Fix is too pessimistic with
line above y = x and in yet another case (Turkish) it
is too optimistic. The distribution of this qualitative
behavior among the 14 datasets is: too optimistic
in 2 datasets, too pessimistic in 7 and close to the
line y = x in 5 datasets. The confidence scores
produced by the WKB are in general worse than
KD-Fix, too optimistic in some confidence range
1Arabic, Bulgarian, Chinese, Czech, Danish, Dutch, Ger-
man, Japanese, Portuguese, Slovene, Spanish, Swedish and
Turkish . See http://nextens.uvt.nl/?conll/
574
0 0.2 0.4 0.6 0.8 100.2
0.40.6
0.81
Expected Accuracy (bin center)
Actual A
ccuracy
Chinese
 
 
KD?FixWKB 0 0.2 0.4 0.6 0.8 100.2
0.40.6
0.81
Expected Accuracy (bin center)
Actual A
ccuracy
Swedish
 
 
KD?FixWKB 0 0.2 0.4 0.6 0.8 100.2
0.40.6
0.81
Expected Accuracy (bin center)
Actual A
ccuracy
Turkish
 
 
KD?FixWKB 0 0.2 0.4 0.6 0.8 100.2
0.40.6
0.81
Expected Accuracy (bin center)
Actual A
ccuracy
Danish
 
 
KD?FixWKB
Figure 1: Evaluation of KD-Fix and WKB by comparing predicted accuracy vs. actual accuracy in each bin on 4 datasets. Best
performance is obtained for curves close to the line y=x (black line). Delta method is omitted as its output is not in the range [0, 1].
KD WKB Delta KD-Fix Random
Fix +Delta
Avg-Prec 0.535 0.304 0.518 0.547 0.147
Prec @10% 0.729 0.470 0.644 0.724 0.145
Prec @90% 0.270 0.157 0.351 0.348 0.147
RMSE 0.084 0.117 - - 0.458
Table 1: Row 1: Average precision in ranking all edges ac-
cording confidence values. Rows 2-3: Precision in detection of
incorrect edges when detected 10% and 90% of all the incorrect
edges. Row 4: Root mean square error. All results are averaged
over all datasets.
and too pessimistic in another range. We computed
the root mean square-error (RMSE) in predicting the
bin center value given by
?
(
?
j nj(bj?cj)
2)/(
?
j nj) ,
where nj is the number of edges in the jth bin.
The results, summarized in the 4th row of Table 1,
support the observation that KD-Fix performs better
than WKB, with smaller RMSE.
Incorrect Edges Detection: The goal of this task
is to efficiently detect incorrect predicted-edges.
We ranked all predicted edges of the test-set (per
dataset) according to their confidence score, order-
ing from low to high. Ideally, erroneous edges by
the parser are ranked at the top. A summary of
the average precision, computed at all ranks of erro-
neous edges, (averaged over all datasets, due to lack
of space), for all confidence estimation methods is
summarized in the first row of Table 1. The aver-
age precision achieved by random ordering is about
equal to the error rate for each dataset. The Delta
method improves significantly over both the random
ordering and WKB. KD-Fix achieves the best per-
formance in 12 of 14 datasets and the best average-
performance. These results are consistent with the
results obtained for sequence labeling by Mejer and
Crammer (2010).
Average precision summarizes the detection of
all incorrect edges into a single number. More re-
fined analysis is encapsulated in Precision-Recall
(PR) plots, showing the precision as more incorrect
edges are detected. PR plots for three datasets are
shown in Fig. 2. From these plots (applied also to
other datasets, omitted due to lack of space) we ob-
serve that in most cases KD-Fix performs signifi-
cantly better than Delta in the early detection stage
(first 10-20% of the incorrect edges), while Delta
performs better in late detection stages (last 10-20%
of the incorrect edges). The second and third rows of
Table 1 summarize the precision after detecting only
10% incorrect edges and after detecting 90% of the
incorrect edges, averaged over all datasets. For ex-
ample, in Czech and Portuguese plots of Fig. 2, we
observe an advantage of KD-Fix for low recall and
an advantage of Delta in high recall. Yet for Ara-
bic, for example, KD-Fix outperforms Delta along
the entire range of recall values.
KD-Fix assigns at most K distinct confidence val-
ues to each edge - the number of models that agreed
on that particular edge. Thus, when edges are ranked
according to the confidence, all edges that are as-
signed the same value are ordered randomly. Fur-
thermore, large fraction of the edges, ? 70 ? 80%,
are assigned one of the top-three scores (i.e. K-2,
K-1, K). As a results, the precision performance of
KD-Fix drops sharply for recall values of 80% and
above. On the other hand, we hypothesize that the
lower precision of Delta at low recall values (dia-
mond in Fig. 2) is because by definition Delta takes
into account only two parses, ignoring additional
possible parses with score close to the highest score.
This makes Delta method more sensitive to small
differences in score values compared to KD-Fix.
Based on this observation, we propose combin-
ing both KD-Fix and Delta. Our new method sets
the confidence score of an edge to be a weighted
mean of the score values of KD-Fix and Delta, with
weights a and 1-a, respectively. We use a value
575
20 40 60 80 1000.2
0.40.6
0.8
Recall as Percent of Incorect Edges
Precision
Czech
 
 KD?FixDeltaKD?Fix+DeltaWKBRandom
20 40 60 80 1000.2
0.40.6
0.8
Recall as Percent of Incorect Edges
Precision
Portuguese
 
 KD?FixDeltaKD?Fix+DeltaWKBRandom
20 40 60 80 1000.2
0.40.6
0.8
Recall as Percent of Incorect Edges
Precision
Arabic
 
 KD?FixDeltaKD?Fix+DeltaWKBRandom
0 20 40 60 800.20.3
0.40.5
0.60.7
K
Average 
Precision
 
 
ArabicChineseDanishDutchSloveneSpanish
Figure 2: (Best shown in color.) Three left plots: Precision in detection of incorrect edges as recall increases. Right plot: Effect of
K value on KD-Fix method performance (for six languages, the remaining languages follow similar trend, omitted for clarity).
a ? 1, so if the confidence value of two edges ac-
cording to KD-Fix is different, the contribution of
the score from Delta is negligible, and the final score
is very close as score of only KD-Fix. On the other
hand, if the score of KD-Fix is the same, as hap-
pens for many edges at high recall values, then Delta
breaks arbitrary ties. In other words, the new method
first ranks edges according to the confidence score
of KD-Fix, then among edges with equal KD-Fix
confidence score a secondary order is employed us-
ing Delta. Not surpassingly, we name this method
KD-Fix+Delta. This new method enjoys the bene-
fits of the two methods. From the first row of Table 1
we see that it achieves the highest average-precision
averaged over the 14 datasets. It improves average-
precision over KD-Fix in 12 of 14 datasets and over
Delta in all 14 datasets. From the second and third
row of the table, we see that it has Precision very
close to KD-Fix for recall of 10% (0.729 vs. 0.724),
and very close to Delta for recall of 90% (0.351 vs.
0.348). Moving to Fig. 2, we observe that the curve
associated with the new method (red ticks) is in gen-
eral as high as the curves associated with KD-Fix
for low values of recall, and as high as the curves
associated with Delta for large values of recall.
To illustrate the effectiveness of the incorrect
edges detection process, Table 2 presents the num-
ber of incorrect edges detected vs. number of edges
inspected for the English dataset. The test set for this
task includes 55K words and the parser made mis-
take on 6, 209 edges, that is, accuracy of 88.8%. We
see that using the ranking induced by KD-Fix+Delta
method, inspection of 550, 2750 and 5500 edges
(1, 5, 10% of all edges), allows detection of 6.6 ?
46% of all incorrect edges, over 4.5 times more ef-
fective than random validation.
Edges inspected Incorrect edges detected
(% of total edges) (% of incorrect edges)
550 (1%) 412 (6.6%)
2,750 (5%) 1,675 (27%)
5,500 (10%) 2,897 (46%)
Table 2: Number of incorrect edges detected, and the corre-
sponding percentage of all mistakes, after inspecting 1 ? 10%
of all edges, using ranking induced by KD-Fix+Delta method.
Effect of K value on KD-Fix method perfor-
mance The right plot of Fig. 2 shows the average-
precision of detecting incorrect edges on the test set
using the KD-Fix method for K values ranging be-
tween 2 and 80. We see that even with K = 2,
only two samples per sentence, the average preci-
sion results are much better than random ranking in
all tasks. AsK is increased the results improve until
reaching maximal results at K ? 30. Theoretical
calculations, using concentration inequalities, show
that accurate estimates based on the sampling proce-
dure requires K ? 102 ? 103. Yet, we see that for
practical uses, smaller K values by 1 ? 2 order of
magnitude is suffice.
References
[Crammer et al2006] K. Crammer, O. Dekel, J. Keshet,
S. Shalev-Shwartz, and Y. Singer. 2006. Online
passive-aggressive algorithms. JMLR, 7:551?585.
[Hall2007] Keith Hall. 2007. k-best spanning tree pars-
ing. In In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics.
[McDonald et al2005] R. McDonald, F. Pereira, K. Rib-
arov, and J. Hajic. 2005. Non-projective depen-
dency parsing using spanning tree algorithms. In
HLT/EMNLP.
[Mejer and Crammer2010] A. Mejer and K. Crammer.
2010. Confidence in structured-prediction using
confidence-weighted models. In EMNLP.
576
Proceedings of the ACL 2010 Conference Short Papers, pages 377?381,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Learning Better Data Representation using Inference-Driven Metric
Learning
Paramveer S. Dhillon
CIS Deptt., Univ. of Penn.
Philadelphia, PA, U.S.A
dhillon@cis.upenn.edu
Partha Pratim Talukdar?
Search Labs, Microsoft Research
Mountain View, CA, USA
partha@talukdar.net
Koby Crammer
Deptt. of Electrical Engg.
The Technion, Haifa, Israel
koby@ee.technion.ac.il
Abstract
We initiate a study comparing effective-
ness of the transformed spaces learned by
recently proposed supervised, and semi-
supervised metric learning algorithms
to those generated by previously pro-
posed unsupervised dimensionality reduc-
tion methods (e.g., PCA). Through a va-
riety of experiments on different real-
world datasets, we find IDML-IT, a semi-
supervised metric learning algorithm to be
the most effective.
1 Introduction
Because of the high-dimensional nature of NLP
datasets, estimating a large number of parameters
(a parameter for each dimension), often from a
limited amount of labeled data, is a challenging
task for statistical learners. Faced with this chal-
lenge, various unsupervised dimensionality reduc-
tion methods have been developed over the years,
e.g., Principal Components Analysis (PCA).
Recently, several supervised metric learning al-
gorithms have been proposed (Davis et al, 2007;
Weinberger and Saul, 2009). IDML-IT (Dhillon et
al., 2010) is another such method which exploits
labeled as well as unlabeled data during metric
learning. These methods learn a Mahalanobis dis-
tance metric to compute distance between a pair
of data instances, which can also be interpreted as
learning a transformation of the input data, as we
shall see in Section 2.1.
In this paper, we make the following contribu-
tions:
Even though different supervised and semi-
supervised metric learning algorithms have
recently been proposed, effectiveness of the
transformed spaces learned by them in NLP
? Research carried out while at the University of Penn-
sylvania, Philadelphia, PA, USA.
datasets has not been studied before. In
this paper, we address that gap: we com-
pare effectiveness of classifiers trained on the
transformed spaces learned by metric learn-
ing methods to those generated by previ-
ously proposed unsupervised dimensionality
reduction methods. We find IDML-IT, a
semi-supervised metric learning algorithm to
be the most effective.
2 Metric Learning
2.1 Relationship between Metric Learning
and Linear Projection
We first establish the well-known equivalence be-
tween learning a Mahalanobis distance measure
and Euclidean distance in a linearly transformed
space of the data (Weinberger and Saul, 2009). Let
A be a d?d positive definite matrix which param-
eterizes the Mahalanobis distance, dA(xi, xj), be-
tween instances xi and xj , as shown in Equation
1. Since A is positive definite, we can decompose
it as A = P>P , where P is another matrix of size
d? d.
dA(xi, xj) = (xi ? xj)
>A(xi ? xj) (1)
= (Pxi ? Pxj)
>(Pxi ? Pxj)
= dEuclidean(Pxi, Pxj)
Hence, computing Mahalanobis distance pa-
rameterized by A is equivalent to first projecting
the instances into a new space using an appropriate
transformation matrix P and then computing Eu-
clidean distance in the linearly transformed space.
In this paper, we are interested in learning a better
representation of the data (i.e., projection matrix
P ), and we shall achieve that goal by learning the
corresponding Mahalanobis distance parameterA.
We shall now review two recently proposed
metric learning algorithms.
377
2.2 Information-Theoretic Metric Learning
(ITML): Supervised
Information-Theoretic Metric Learning (ITML)
(Davis et al, 2007) assumes the availability of
prior knowledge about inter-instance distances. In
this scheme, two instances are considered simi-
lar if the Mahalanobis distance between them is
upper bounded, i.e., dA(xi, xj) ? u, where u
is a non-trivial upper bound. Similarly, two in-
stances are considered dissimilar if the distance
between them is larger than certain threshold l,
i.e., dA(xi, xj) ? l. Similar instances are rep-
resented by set S, while dissimilar instances are
represented by set D.
In addition to prior knowledge about inter-
instance distances, sometimes prior information
about the matrix A, denoted by A0, itself may
also be available. For example, Euclidean dis-
tance (i.e., A0 = I) may work well in some do-
mains. In such cases, we would like the learned
matrixA to be as close as possible to the prior ma-
trix A0. ITML combines these two types of prior
information, i.e., knowledge about inter-instance
distances, and prior matrix A0, in order to learn
the matrix A by solving the optimization problem
shown in (2).
min
A0
Dld(A,A0) (2)
s.t. tr{A(xi ? xj)(xi ? xj)
>} ? u,
?(i, j) ? S
tr{A(xi ? xj)(xi ? xj)
>} ? l,
?(i, j) ? D
whereDld(A,A0) = tr(AA
?1
0 )? log det(AA
?1
0 )
?n, is the LogDet divergence.
To handle situations where exactly solving the
problem in (2) is not possible, slack variables may
be introduced to the ITML objective. To solve this
optimization problem, an algorithm involving re-
peated Bregman projections is presented in (Davis
et al, 2007), which we use for the experiments re-
ported in this paper.
2.3 Inference-Driven Metric Learning
(IDML): Semi-Supervised
Notations: We first define the necessary notations.
Let X be the d ? n matrix of n instances in a
d-dimensional space. Out of the n instances, nl
instances are labeled, while the remaining nu in-
stances are unlabeled, with n = nl+nu. Let S be
a n ? n diagonal matrix with Sii = 1 iff instance
xi is labeled. m is the total number of labels. Y
is the n?m matrix storing training label informa-
tion, if any. Y? is the n?m matrix of estimated la-
bel information, i.e., output of any classifier, with
Y?il denoting score of label l at node i. .
The ITML metric learning algorithm, which we
reviewed in Section 2.2, is supervised in nature,
and hence it does not exploit widely available un-
labeled data. In this section, we review Infer-
ence Driven Metric Learning (IDML) (Algorithm
1) (Dhillon et al, 2010), a recently proposed met-
ric learning framework which combines an exist-
ing supervised metric learning algorithm (such as
ITML) along with transductive graph-based la-
bel inference to learn a new distance metric from
labeled as well as unlabeled data combined. In
self-training styled iterations, IDML alternates be-
tween metric learning and label inference; with
output of label inference used during next round
of metric learning, and so on.
IDML starts out with the assumption that ex-
isting supervised metric learning algorithms, such
as ITML, can learn a better metric if the number
of available labeled instances is increased. Since
we are focusing on the semi-supervised learning
(SSL) setting with nl labeled and nu unlabeled
instances, the idea is to automatically label the
unlabeled instances using a graph based SSL al-
gorithm, and then include instances with low as-
signed label entropy (i.e., high confidence label
assignments) in the next round of metric learning.
The number of instances added in each iteration
depends on the threshold ?1. This process is con-
tinued until no new instances can be added to the
set of labeled instances, which can happen when
either all the instances are already exhausted, or
when none of the remaining unlabeled instances
can be assigned labels with high confidence.
The IDML framework is presented in Algo-
rithm 1. In Line 3, any supervised metric
learner, such as ITML, may be used as the
METRICLEARNER. Using the distance metric
learned in Line 3, a new k-NN graph is constructed
in Line 4 , whose edge weight matrix is stored in
W . In Line 5 , GRAPHLABELINF optimizes over
the newly constructed graph, the GRF objective
(Zhu et al, 2003) shown in (3).
min
Y? ?
tr{Y?
?>LY?
?
}, s.t. S?Y? = S?Y?
?
(3)
where L = D ?W is the (unnormalized) Lapla-
1During the experiments in Section 3, we set ? = 0.05
378
Algorithm 1: Inference Driven Metric Learn-
ing (IDML)
Input: instancesX , training labels Y , training
instance indicator S, label entropy threshold ?,
neighborhood size k
Output: Mahalanobis distance parameter A
1: Y? ? Y , S? ? S
2: repeat
3: A? METRICLEARNER(X, S?, Y? )
4: W ? CONSTRUCTKNNGRAPH(X,A, k)
5: Y?
?
? GRAPHLABELINF(W, S?, Y? )
6: U ? SELECTLOWENTINST(Y?
?
, S?, ?)
7: Y? ? Y? + UY?
?
8: S? ? S? + U
9: until convergence (i.e., Uii = 0, ?i)
10: return A
cian, and D is a diagonal matrix with Dii =?
jWij . The constraint, S?Y? = S?Y?
?
, in (3)
makes sure that labels on training instances are not
changed during inference. In Line 6, a currently
unlabeled instance xi (i.e., S?ii = 0) is consid-
ered a new labeled training instance, i.e., Uii = 1,
for next round of metric learning if the instance
has been assigned labels with high confidence in
the current iteration, i.e., if its label distribution
has low entropy (i.e., ENTROPY(Y?
?
i:) ? ?). Fi-
nally in Line 7, training instance label information
is updated. This iterative process is continued till
no new labeled instance can be added, i.e., when
Uii = 0 ?i. IDML returns the learned matrix A
which can be used to compute Mahalanobis dis-
tance using Equation 1.
3 Experiments
3.1 Setup
Dataset Dimension Balanced
Electronics 84816 Yes
Books 139535 Yes
Kitchen 73539 Yes
DVDs 155465 Yes
WebKB 44261 Yes
Table 1: Description of the datasets used in Sec-
tion 3. All datasets are binary with 1500 total in-
stances in each.
Description of the datasets used during experi-
ments in Section 3 are presented in Table 1. The
first four datasets ? Electronics, Books, Kitchen,
and DVDs ? are from the sentiment domain and
previously used in (Blitzer et al, 2007). WebKB
is a text classification dataset derived from (Sub-
ramanya and Bilmes, 2008). For details regard-
ing features and data pre-processing, we refer the
reader to the origin of these datasets cited above.
One extra preprocessing that we did was that we
only considered features which occurred more 20
times in the entire dataset to make the problem
more computationally tractable and also since the
infrequently occurring features usually contribute
noise. We use classification error (lower is better)
as the evaluation metric. We experiment with the
following ways of estimating transformation ma-
trix P :
Original2: We set P = I , where I is the
d ? d identity matrix. Hence, the data is not
transformed in this case.
RP: The data is first projected into a lower
dimensional space using the Random Pro-
jection (RP) method (Bingham and Mannila,
2001). Dimensionality of the target space
was set at d
?
= logn
2log 1
, as prescribed in
(Bingham and Mannila, 2001). We use the
projection matrix constructed by RP as P . 
was set to 0.25 for the experiments in Sec-
tion 3, which has the effect of projecting the
data into a much lower dimensional space
(84 for the experiments in this section). This
presents an interesting evaluation setting as
we already run evaluations in much higher di-
mensional space (e.g., Original).
PCA: Data instances are first projected into
a lower dimensional space using Principal
Components Analysis (PCA) (Jolliffe, 2002)
. Following (Weinberger and Saul, 2009), di-
mensionality of the projected space was set
at 250 for all experiments. In this case, we
used the projection matrix generated by PCA
as P .
ITML: A is learned by applying ITML (see
Section 2.2) on the Original space (above),
and then we decompose A as A = P>P to
obtain P .
2Note that ?Original? in the results tables refers to orig-
inal space with features occurring more than 20 times. We
also ran experiments with original set of features (without
any thresholding) and the results were worse or comparable
to the ones reported in the tables.
379
Datasets Original RP PCA ITML IDML-IT
?? ? ?? ? ?? ? ?? ? ?? ?
Electronics 31.3? 0.9 42.5? 1.0 46.4? 2.0 33.0? 1.0 30.7?0.7
Books 37.5? 1.1 45.0? 1.1 34.8? 1.4 35.0? 1.1 32.0?0.9
Kitchen 33.7? 1.0 43.0? 1.1 34.0? 1.6 30.9? 0.7 29.0?1.0
DVDs 39.0? 1.2 47.7? 1.2 36.2? 1.6 37.0? 0.8 33.9?1.0
WebKB 31.4? 0.9 33.0? 1.0 27.9? 1.3 28.9? 1.0 25.5?1.0
Table 2: Comparison of SVM % classification errors (lower is better), with 50 labeled instances (Sec.
3.2). nl=50. and nu = 1450. All results are averaged over ten trials. All hyperparameters are tuned on a
separate random split.
Datasets Original RP PCA ITML IDML-IT
?? ? ?? ? ?? ? ?? ? ?? ?
Electronics 27.0? 0.9 40.0? 1.0 41.2? 1.0 27.5? 0.8 25.3?0.8
Books 31.0? 0.7 42.9? 0.6 31.3? 0.7 29.9? 0.5 27.7?0.7
Kitchen 26.3? 0.5 41.9? 0.7 27.0? 0.9 26.1? 0.8 24.8?0.9
DVDs 34.7? 0.4 46.8? 0.6 32.9? 0.8 34.0? 0.8 31.8?0.9
WebKB 25.7? 0.5 31.1? 0.5 24.9? 0.6 25.6? 0.4 23.9?0.4
Table 3: Comparison of SVM % classification errors (lower is better), with 100 labeled instances (Sec.
3.2). nl=100. and nu = 1400. All results are averaged over ten trials. All hyperparameters are tuned on
a separate random split.
IDML-IT: A is learned by applying IDML
(Algorithm 1) (see Section 2.3) on the Orig-
inal space (above); with ITML used as
METRICLEARNER in IDML (Line 3 in Al-
gorithm 1). In this case, we treat the set of
test instances (without their gold labels) as
the unlabeled data. In other words, we essen-
tially work in the transductive setting (Vap-
nik, 2000). Once again, we decompose A as
A = P>P to obtain P .
We also experimented with the supervised
large-margin metric learning algorithm (LMNN)
presented in (Weinberger and Saul, 2009). We
found ITML to be more effective in practice than
LMNN, and hence we report results based on
ITML only. Each input instance, x, is now pro-
jected into the transformed space as Px. We
now train different classifiers on this transformed
space. All results are averaged over ten random
trials.
3.2 Supervised Classification
We train a SVM classifier, with an RBF kernel, on
the transformed space generated by the projection
matrix P . SVM hyperparameter, C and RBF ker-
nel bandwidth, were tuned on a separate develop-
ment split. Experimental results with 50 and 100
labeled instances are shown in Table 2, and Ta-
ble 3, respectively. From these results, we observe
that IDML-IT consistently achieves the best per-
formance across all experimental settings. We also
note that in Table 3, performance difference be-
tween ITML and IDML-IT in the Electronics and
Kitchen domains are statistically significant.
3.3 Semi-Supervised Classification
In this section, we trained the GRF classifier (see
Equation 3), a graph-based semi-supervised learn-
ing (SSL) algorithm (Zhu et al, 2003), using
Gaussian kernel parameterized by A = P>P to
set edge weights. During graph construction, each
node was connected to its k nearest neighbors,
with k treated as a hyperparameter and tuned on
a separate development set. Experimental results
with 50 and 100 labeled instances are shown in
Table 4, and Table 5, respectively. As before, we
experimented with nl = 50 and nl = 100. Once
again, we observe that IDML-IT is the most effec-
tive method, with the GRF classifier trained on the
data representation learned by IDML-IT achieving
best performance in all settings. Here also, we ob-
serve that IDML-IT achieves the best performance
across all experimental settings.
380
Datasets Original RP PCA ITML IDML-IT
?? ? ?? ? ?? ? ?? ? ?? ?
Electronics 47.9? 1.1 49.0? 1.2 43.2? 0.9 34.9? 0.5 34.0?0.5
Books 50.0? 1.0 49.4? 1.0 47.9? 0.7 42.1? 0.7 40.6?0.7
Kitchen 49.8? 1.1 49.6? 0.9 48.6? 0.8 31.1? 0.5 30.0?0.5
DVDs 50.1? 0.5 49.9? 0.7 49.4? 0.6 42.1? 0.4 41.2?0.5
WebKB 33.1? 0.4 33.1? 0.3 33.1? 0.3 30.0? 0.4 28.7?0.5
Table 4: Comparison of transductive % classification errors (lower is better) over graphs constructed
using different methods (see Section 3.3), with nl = 50 and nu = 1450. All results are averaged over
ten trials. All hyperparameters are tuned on a separate random split.
Datasets Original RP PCA ITML IDML-IT
?? ? ?? ? ?? ? ?? ? ?? ?
Electronics 43.5? 0.7 47.2? 0.8 39.1? 0.7 31.3? 0.2 30.8?0.3
Books 48.3? 0.5 48.9? 0.3 43.3? 0.4 35.2? 0.5 33.3?0.6
Kitchen 45.3? 0.6 48.2? 0.5 41.0? 0.7 30.7? 0.6 29.9?0.3
DVDs 48.6? 0.3 49.3? 0.5 45.9? 0.5 42.6? 0.4 41.7?0.3
WebKB 33.4? 0.4 33.4? 0.4 33.4? 0.3 30.4? 0.5 28.6?0.7
Table 5: Comparison of transductive % classification errors (lower is better) over graphs constructed
using different methods (see Section 3.3), with nl = 100 and nu = 1400. All results are averaged over
ten trials. All hyperparameters are tuned on a separate random split.
4 Conclusion
In this paper, we compared the effectiveness
of the transformed spaces learned by recently
proposed supervised, and semi-supervised metric
learning algorithms to those generated by previ-
ously proposed unsupervised dimensionality re-
duction methods (e.g., PCA). To the best of our
knowledge, this is the first study of its kind in-
volving NLP datasets. Through a variety of ex-
periments on different real-world NLP datasets,
we demonstrated that supervised as well as semi-
supervised classifiers trained on the space learned
by IDML-IT consistently result in the lowest clas-
sification errors. Encouraged by these early re-
sults, we plan to explore further the applicability
of IDML-IT in other NLP tasks (e.g., entity classi-
fication, word sense disambiguation, polarity lexi-
con induction, etc.) where better representation of
the data is a pre-requisite for effective learning.
Acknowledgments
Thanks to Kuzman Ganchev for providing detailed
feedback on a draft of this paper. This work
was supported in part by NSF IIS-0447972 and
DARPA HRO1107-1-0029.
References
E. Bingham and H. Mannila. 2001. Random projec-
tion in dimensionality reduction: applications to im-
age and text data. In ACM SIGKDD.
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biogra-
phies, bollywood, boom-boxes and blenders: Do-
main adaptation for sentiment classification. In
ACL.
J.V. Davis, B. Kulis, P. Jain, S. Sra, and I.S. Dhillon.
2007. Information-theoretic metric learning. In
ICML.
P. S. Dhillon, P. P. Talukdar, and K. Crammer. 2010.
Inference-driven metric learning for graph construc-
tion. Technical Report MS-CIS-10-18, CIS Depart-
ment, University of Pennsylvania, May.
IT Jolliffe. 2002. Principal component analysis.
Springer verlag.
A. Subramanya and J. Bilmes. 2008. Soft-Supervised
Learning for Text Classification. In EMNLP.
V.N. Vapnik. 2000. The nature of statistical learning
theory. Springer Verlag.
K.Q. Weinberger and L.K. Saul. 2009. Distance metric
learning for large margin nearest neighbor classifica-
tion. The Journal of Machine Learning Research.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using Gaussian fields and har-
monic functions. In ICML.
381
