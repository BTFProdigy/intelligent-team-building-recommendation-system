Proceedings of the Workshop on Multilingual Language Resources and Interoperability, pages 40?49,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Multilingual Collocation Extraction: Issues and Solutions
Violeta Seretan
Language Technology Laboratory
University of Geneva
2, rue de Candolle, 1211 Geneva
Violeta.Seretan@latl.unige.ch
Eric Wehrli
Language Technology Laboratory
University of Geneva
2, rue de Candolle, 1211 Geneva
Eric.Wehrli@latl.unige.ch
Abstract
Although traditionally seen as a language-
independent task, collocation extraction
relies nowadays more and more on the
linguistic preprocessing of texts (e.g.,
lemmatization, POS tagging, chunking or
parsing) prior to the application of sta-
tistical measures. This paper provides
a language-oriented review of the exist-
ing extraction work. It points out sev-
eral language-specific issues related to ex-
traction and proposes a strategy for cop-
ing with them. It then describes a hybrid
extraction system based on a multilingual
parser. Finally, it presents a case-study on
the performance of an association measure
across a number of languages.
1 Introduction
Collocations are understood in this paper as ?id-
iosyncratic syntagmatic combination of lexical
items? (Fontenelle, 1992, 222): heavy rain, light
breeze, great difficulty, grow steadily, meet re-
quirement, reach consensus, pay attention, ask a
question. Unlike idioms (kick the bucket, lend a
hand, pull someone?s leg), their meaning is fairly
transparent and easy to decode. Yet, differently
from the regular productions, (big house, cultural
activity, read a book), collocational expressions
are highly idiosyncratic, since the lexical items
a headword combines with in order to express
a given meaning is contingent upon that word
(Mel?c?uk, 2003).
This is apparent when comparing a colloca-
tion?s equivalents across different languages. The
English collocation ask a question translates as
poser une question in French (lit., ?put a question),
and as fare una domanda, hacer una pregunta in
Italian and Spanish (lit., to make a question).
As it has been pointed out by many researchers
(Cruse, 1986; Benson, 1990; McKeown and
Radev, 2000), collocations cannot be described
by means of general syntactic and semantic rules.
They are arbitrary and unpredictable, and there-
fore need to be memorized and used as such. They
constitute the so-called ?semi-finished products?
of language (Hausmann, 1985) or the ?islands of
reliability? (Lewis, 2000) on which the speakers
build their utterances.
2 Motivation
The key importance of collocations in text pro-
duction tasks such as machine translation and nat-
ural language generation has been stressed many
times. It has been equally shown that collocations
are useful in a range of other applications, such as
word sense disambiguation (Brown et al, 1991)
and parsing (Alshawi and Carter, 1994).
The NLP community fully acknowledged the
need for an appropriate treatment of multi-word
expressions in general (Sag et al, 2002). Collo-
cations are particularly important because of their
prevalence in language, regardless of the domain
or genre. According to Jackendoff (1997, 156)
and Mel?c?uk (1998, 24), collocations constitute
the bulk of a language?s lexicon.
The last decades have witnessed a considerable
development of collocation extraction techniques,
that concern both monolingual and (parallel) mul-
tilingual corpora.
We can mention here only part of this work:
(Berry-Rogghe, 1973; Church et al, 1989;
Smadja, 1993; Lin, 1998; Krenn and Evert, 2001)
for monolingual extraction, and (Kupiec, 1993;
Wu, 1994; Smadja et al, 1996; Kitamura and Mat-
40
sumoto, 1996; Melamed, 1997) for bilingual ex-
traction via alignment.
Traditionally, collocation extraction was con-
sidered a language-independent task. Since collo-
cations are recurrent, typical lexical combinations,
a wide range of statistical methods based on word
co-occurrence frequency have been heavily used
for detecting them in text corpora. Among the
most often used types of lexical association mea-
sures (henceforth AMs) we mention: statistical
hypothesis tests (e.g., binomial, Poisson, Fisher, z-
score, chi-squared, t-score, and log-likelihood ra-
tio tests), that measure the significance of the asso-
ciation between two words based on a contingency
table listing their joint and marginal frequency,
and Information-theoretic measures (Mutual In-
formation ? henceforth MI ? and its variants),
that quantity of ?information? shared by two ran-
dom variables. A detailed review of the statistical
methods employed in collocation extraction can be
found, for instance, in (Evert, 2004). A compre-
hensive list of AMs is given (Pecina, 2005).
Very often, in addition to the information on co-
occurrence frequency, language-specific informa-
tion is also integrated in a collocation extraction
system (as it will be seen in section 3):
- morphological information, in order to count
inflected word forms as instances of the same
base form. For instance, ask questions, asks
question, asked question are all instances of
the same word pair, ask - question;
- syntactic information, in order to recognize a
word pair even if subject to (complex) syntac-
tic transformations: ask multiple questions,
question asked, questions that one might ask.
The language-specific modules thus aim at cop-
ing with the problem of morphosyntactic varia-
tion, in order to improve the accuracy of frequency
information. This becomes truly important espe-
cially for free-word order and for high-inflection
languages, for which the token(form)-based fre-
quency figures become too skewed due to the high
lexical dispersion. Not only the data scattering
modify the frequency numbers used by AMs, but
it also alters the performance of AMs, if the the
probabilities in the contingency table become very
low.
Morphosyntactic information has in fact been
shown to significantly improve the extraction re-
sults (Breidt, 1993; Smadja, 1993; Zajac et al,
2003). Morphological tools such as lemmatizers
and POS taggers are being commonly used in ex-
traction systems; they are employed both for deal-
ing with text variation and for validating the can-
didate pairs: combinations of function words are
typically ruled out (Justeson and Katz, 1995), as
are the ungrammatical combinations in the sys-
tems that make use of parsers (Church and Hanks,
1990; Smadja, 1993; Basili et al, 1994; Lin, 1998;
Goldman et al, 2001; Seretan et al, 2004).
Given the motivations for performing a
linguistically-informed extraction ? which were
also put forth, among others, by Church and
Hanks (1990, 25), Smadja (1993, 151) and Heid
(1994) ? and given the recent development of
linguistic analysis tools, it seems plausible that the
linguistic structure will be more and more taken
into account by collocation extraction systems.
The rest of the paper is organized as follows. In
section 3 we provide a language-oriented review
of the existing collocation extraction work. Then
we highlight, in section 4, a series of problems that
arise in the transfer of methodology to a new lan-
guage, and we propose a strategy for dealing with
them. Section 5 describes an extraction system,
and, finally, section 6 presents a case-study on the
collocations extracted for four languages, illustrat-
ing the cross-lingual variation in the performance
of a particular AM.
3 Overview of Extraction Work
3.1 English
As one might expect, the bulk of the collocation
extraction work concerns the English language:
(Choueka, 1988; Church et al, 1989; Church and
Hanks, 1990; Smadja, 1993; Justeson and Katz,
1995; Kjellmer, 1994; Sinclair, 1995; Lin, 1998),
among many others1.
Choueka?s method (1988) detects n-grams (ad-
jacent words) only, by simply computing the co-
occurrence frequency. Justeson and Katz (1995)
apply a POS-filter on the pairs they extract. As in
(Kjellmer, 1994), the AM they use is the simple
frequency.
Smadja (1993) employs the z-score in conjunc-
tion with several heuristics (e.g., the systematic
occurrence of two lexical items at the same dis-
tance in text) and extracts predicative collocations,
1E.g., (Frantzi et al, 2000; Pearce, 2001; Goldman et al,
2001; Zaiu Inkpen and Hirst, 2002; Dias, 2003; Seretan et al,
2004; Pecina, 2005), and the list can be continued.
41
rigid noun phrases and phrasal templates. He then
uses the a parser in order to validate the results.
The parsing is shown to lead to an increase in ac-
curacy from 40% to 80%.
(Church et al, 1989) and (Church and Hanks,
1990) use POS information and a parser to extract
verb-object pairs, which then they rank according
to the mutual information (MI) measure they in-
troduce.
Lin?s (1998) is also a hybrid approach that relies
on a dependency parser. The candidates extracted
are then ranked with MI.
3.2 German
German is the second most investigated language,
thanks to the early work of Breidt (1993) and,
more recently, to that of Krenn and Evert, such as
(Krenn and Evert, 2001; Evert and Krenn, 2001;
Evert, 2004) centered on evaluation.
Breidt uses MI and t-score and compares the
results accuracy when various parameters vary,
such as the window size, presence vs. absence
of lemmatization, corpus size, and presence vs.
absence of POS and syntactic information. She
focuses on N-V pairs2 and, despite the lack of
syntactic analysis tools at the time, by simulating
parsing she comes to the conclusion that ?Very
high precision rates, which are an indispensable
requirement for lexical acquisition, can only real-
istically be envisaged for German with parsed cor-
pora? (Breidt, 1993, 82).
Later, Krenn and Evert (2001) used a German
chunker to extract syntactic pairs such as P-N-V.
Their work put the basis of formal and system-
atic methods in collocation extraction evaluation.
Zinsmeister and Heid (2003; 2004) focused on
N-V and A-N-V combinations identified using a
stochastic parser. They applied machine learning
techniques in combination to the log-likelihood
measure (henceforth LL) for distinguishing trivial
compounds from lexicalized ones.
Finally, Wermter and Hahn (2004) identified
PP-V combinations using a POS tagger and a
chunker. They based their method on a linguistic
criterion (that of limited modifiability) and com-
pared their results with those obtained using the
t-score and LL tests.
2The following abbreviations are used in this paper: N -
noun, V - verb, A - adjective, Adv - adverb, Det - determiner,
Conj - conjunction, P - preposition.
3.3 French
Thanks to the outstanding work of Gross on
lexicon-grammar (1984), French is one of the
most studied languages in terms of distributional
and transformational potential of words. This
work has been carried out before the computer era
and the advent of corpus linguistics, while auto-
matic extraction was later performed, for instance,
in (Lafon, 1984; Daille, 1994; Bourigault, 1992;
Goldman et al, 2001).
Daille (1994) aimed at extracting compound
nouns, defined a priori by means of certain syn-
tactic patterns, like N-A, N-N, N-a`-N, N-de-N, N
P Det N. She used a lemmatizer and a POS-tagger
before applying a series of AMs, which she then
evaluated against a domain-specific terminology
dictionary and against a gold-standard manually
created from the extraction corpus.
Similarly, Bourigault (1992) extracted noun-
phrases from shallow-parsed text, and Goldman et
al. (2001) extracted syntactic collocations by us-
ing a full parser and applying the LL test.
3.4 Other Languages
In addition to English, German and French, other
languages for which notable collocation extraction
work was performed, are ? as we are aware of ?
the following:
? Italian: early extraction work was carried out
by Calzolari and Bindi (1990) and employed
MI. It was followed by (Basili et al, 1994),
that made use of parsing information;
? Korean: (Shimohata et al, 1997) used an ad-
jacency n-gram model, and (Kim et al, 1999)
relied on POS-tagging;
? Chinese: (Huang et al, 2005) used POS in-
formation, while (Lu et al, 2004) applied ex-
traction techniques similar to Xtract system
(Smadja, 1993);
? Japanese: (Ikehara et al, 1995) was based on
an improved n-gram method.
As for multilingual extraction via alignment
(where collocations are first detected in one lan-
guage and then matched with their translation in
another language), most or the existing work con-
cern the English-French language pair, and the
Hansard corpus of Canadian Parliament proceed-
ings. Wu (1994) signals a number of problems
42
that non-Indo-European languages pose for the
existing alignment methods based on word- and
sentence-length: in Chinese, for instance, most of
the words are just one or two characters long, and
there are no word delimiters. This result suggests
that the portability of existing alignment methods
to new language pairs is questionable.
We are not concerned here with extraction via
alignment. We assume, instead, that multilingual
support in collocation extraction means the cus-
tomization of the extraction procedure for each
language. This topic will be addressed in the next
sections.
4 Multilingualism: Why and How?
4.1 Some Issues
As the previous section showed, many systems of
collocation extraction rely on the linguistic pre-
processing of source corpora in order to support
the candidate identification process. Language-
specific information, such as the one derived from
morphological and syntactic analysis, was shown
to be highly beneficial for extraction. Moreover,
the possibility to apply the association measures
on syntactically homogenous material is argued to
benefit extraction, as the performance of associa-
tion measures might vary with the syntactic con-
figurations because of the differences in distribu-
tion (Krenn and Evert, 2001).
The lexical distribution is therefore a relevant
issue from the perspective of multilingual colloca-
tion extraction. Different languages show different
proportions of lexical categories (N, V, A, Adv,
P, etc.) which are evenly distributed across syn-
tactic types3. Depending on the frequency num-
bers, a given AM could be more suited for a spe-
cific syntactic configuration in one language, and
less suited for the same configuration in another.
Ideally, each language should be assigned a suit-
able set of AMs to be applied on syntactically-
homogenous data.
Another issue that is relevant in the multi-
lingualism perspective is that of the syntactic
configurations characterizing collocations. Sev-
eral such relations (e.g., noun-adjectival modifier,
predicate-argument) are likely to remain constant
through languages, i.e., to be judged as colloca-
tionally interesting in many languages. However,
3For instance, V-P pairs are more represented in English
than in other languages (as phrasal verbs or verb-particle con-
structions).
other configurations could be language-specific
(like P-N-V in German, whose English equiva-
lent is V-P-N). Yet other configurations might have
no counterpart at all in another language (e.g., the
French P-A pair a` neuf is translated into English
as a Conj-A pair, as new).
Finding all the collocationally-relevant syntac-
tic types for a language is therefore another prob-
lem that has to be solved in multilingual extrac-
tion. Since a priori defining these types based
on intuition does not ensure the necessary cover-
age, an alternative proposal is to induce them from
POS data and dependency relations, as in (Seretan,
2005).
The morphoyntactic differences between lan-
guages also have to be taken into account. With
English as the most investigated language, several
hypotheses were put forth in extraction and be-
came common place.
For instance, using a 5-words window as search
space for collocation pairs is a usual practice, since
this span length was shown sufficient to cover a
high percentage of syntactic co-occurrences in En-
glish. But ? as suggested by other researchers,
e.g., (Goldman et al, 2001) ?, this assumption
does not necessary hold for other languages.
Similarly, the higher inflection and the higher
transformation potential shown by some lan-
guages pose additional problems in extraction,
which were rather ignored for English. As Kim et
al. (1999) notice, collocation extraction is particu-
larly difficult in free-order languages like Korean,
where arguments scramble freely. Breidt (1993)
also pointed out a couple of problems that makes
extraction for German more difficult than for En-
glish: the strong inflection for verbs, the variable
word-order, and the positional ambiguity of the ar-
guments. She shows that even distinguishing sub-
jects from objects is very difficult without parsing.
4.2 A Strategy for Multilingual Extraction
Summing up the previous discussion, the cus-
tomization of collocation extraction for a given
language needs to take into account:
- the syntactic configurations characterizing
collocations,
- the lexical distribution over syntactic config-
urations,
- the adequacy of AMs to these configurations.
43
These are language-specific parameters which
need to be set in a successful multilingual extrac-
tion procedure. Truly multilingual systems have
not been developed yet, but we suggest the fol-
lowing strategy for building such a system:
A. parse the source corpus, extract all the syn-
tactic pairs (e.g., head-modifier, predicate-
argument) and rank them with a given AM,
B. analyze the results and find the syntactic con-
figurations characterizing collocations,
C. evaluate the adequacy of AMs for ranking col-
locations in each syntactic configuration, and
find the most convenient mapping configura-
tions - AMs.
Once customized for a language, the extraction
procedure involves:
Stage 1. parsing the source corpus for extract-
ing the lexical pairs in the relevant,
language-specific syntactic configura-
tions found in step B;
Stage 2. ranking the pairs from each syntactic
class with the AM assigned in step C.
5 A Multilingual Collocation Extractor
Based on Parsing
Ever since the collocation was brought to the at-
tention of linguists in the framework of contextu-
alism (Firth, 1957; Firth, 1968), it has been pre-
ponderantly seen as a pure statistical phenomenon
of lexical association. In fact, according to a well-
known definition, ?a collocation is an arbitrary and
recurrent word combination? (Benson, 1990).
This approach was at the basis of the computa-
tional work on collocation, although there exist an
alternative approach ? the linguistic, or lexico-
graphic one ? that imposes a restricted view on
collocation, which is seen first of all as an expres-
sion of language.
The existing extraction work (section 3) shows
that there is a growing interest in adopting the
more restricted (linguistic) view. As mentioned in
section 3, the importance of parsing for extraction
was confirmed by several evaluation experiments.
With the recent development in the field of linguis-
tic analysis, hybrid extraction systems (i.e., sys-
tems relying on syntactical analysis for colloca-
tion extraction) are likely to become the rule rather
than the exception.
Our system (Goldman et al, 2001; Seretan and
Wehrli, 2006) is ? to our knowledge ? the first
to perform the full syntactic analysis as support for
collocation extraction; similar approaches rely on
dependency parsers or on chunking.
It is based on a symbolic parser that was de-
veloped over the last decade (Wehrli, 2004) and
achieves a high level of performance, in terms of
accuracy, speed and robustness. The languages it
supports are, for the time being, French, English,
Italian, Spanish and German. A few other lan-
guages are being also implemented in the frame-
work of a multilingualism project.
Provided that collocation extraction can be seen
as a two-stage process (where, in stage 1, collo-
cation candidates are identified in the text corpora,
and in stage 2, they are ranked according to a given
AM, cf. section 4.2), the role of the parser is to
support the first stage. A pair of lexical items is
selected as a candidate only if there exist a syntac-
tic relation holding between the two items.
Unlike the traditional, window-based methods,
candidate selection is based on syntactic proxim-
ity (as opposed to textual proximity). Another
peculiarity of our system is that candidate pairs
are identified as the parsing goes on; in other ap-
proaches, they are extracted by post-processing
the output of syntactic tools.
The candidate pairs identified are classified into
syntactically homogenous sets, according to the
syntactic relations holding between the two items.
Only certain predefined syntactic relations are
kept, that were judged as collocationally rele-
vant after multiple experiments of extraction and
data analysis (e.g., adjective-noun, verb-object,
subject-verb, noun-noun, verb-preposition-noun).
The sets obtained are then ranked using the log-
likelihood ratios test (Dunning, 1993).
More details about the system and its perfor-
mance can be found in (Seretan and Wehrli, 2006).
The following examples (taken from the extraction
experiment we will describe below) illustrate its
potential to detect collocation candidates, even if
these are subject to complex syntactic transforma-
tions:
1.a) atteindre objectif (Fr): Les objec-
tifs fixe?s a` l?e?chelle internationale
visant a` re?duire les e?missions ne
peuvent pas e?tre atteints a` l?aide de
ces seuls programmes.
1.b) accogliere emendamento (It):
44
Posso pertanto accogliere in parte
e in linea di principio gli emenda-
menti nn. 43-46 e l?emendamento
n. 85.
1.c) reforzar cooperacio?n (Es): Quer-
emos permitir a los pases que lo
deseen reforzar, en un contexto
unitario, su cooperacio?n en cierto
nu?mero de sectores.
The collocation extractor is part of a bigger sys-
tem (Seretan et al, 2004) that integrates a con-
cordancer and a sentence aligner, and that sup-
ports the visualization, the manual validation and
the management of a multilingual terminology
database. The validated collocations are used for
populating the lexicon of the parser and that of a
translation system (Wehrli, 2003).
6 A Cross-Lingual Extraction
Experiment
A collocation extraction experiment concern-
ing four different languages (English, Spanish,
French, Italian) has been conducted on a parallel
subcorpus of 42 files from the European Parlia-
ment proceedings. Several statistics and extraction
results are reported in Table 1.
Statistics English Spanish Italian French
tokens 2526403 2666764 2575858 2938118
sent/file 2329.1 2513.7 2331.6 2392.8
complete
parses 63.4% 35.5% 46.8% 63.7%
tokens/sent 25.8 25.3 26.3 29.2
extr. pairs
(tokens) 617353 568998 666122 565287
token/type 2.6 2.5 2.3 2.3
LL is def. 85.9% 90.6% 83.5% 92.8%
Table 1: Extraction statistics
We computed the distribution of pair tokens
according to the syntactic type and noted that
the most marked distributional difference among
these languages concern the following types: N-A
(7.12), A-N (4.26), V-O (2.68), V-P (4.16), N-P-N
(3.81)4.
Unsurprisingly, the Romance languages are less
different in terms of syntactic co-occurrence dis-
tribution, and the deviation of English from the
Romance mean is more pronounced ? in particu-
lar, for N-A (9.72), V-P (5.63), A-N (5.25), N-P-N
4The numbers represent the values the standard deviation
of the relative percentages in the whole lists of pairs.
(4.77), and V-O (3.57). These distributional differ-
ences might account for the types of collocations
highlighted by a particular AM (such as LL) in a
language vs. another. Figure 1 displays the rela-
tive proportions of 3 syntactic types ? adjective-
noun, subject-verb and verb-object ? that can be
found at different levels in the significance list re-
turned by LL.
Figure 1: Cross-lingual proportions of A-N, S-V
and V-O pairs at different levels in the significance
lists
We performed a contrastive analysis of results,
by carrying out a case-study aimed at checking
the LL performance variability across languages.
The study concerned the verb-object collocations
having the noun policy as the direct object. We
specifically focused on the best-scored collocation
extracted from the French corpus, namely mener
une politique (lit., conduct a policy).
We looked at the translation equivalents of its
74 instances identified by our extraction system
in the corpus. The analysis revealed that ? at
least in this particular case ? the verbal collo-
cates of this noun are highly scattered: pursue,
implement, conduct, adopt, apply, develop, have,
draft, launch, run, carry out for English; prac-
ticar, llevar a cabo, desarrollar, realizar, aplicar,
seguir, hacer, adoptar, ejercer for Spanish; con-
durre, attuare, portare avanti, perseguire, pratti-
care, adottare, fare for Italian (among several oth-
ers). Some of the collocates (those listed first) are
more prominently used. But generally they are
highly dispersed, and this might indicate a bigger
difficulty for LL to pinpoint the best collocate in a
language vs. another.
We also observed that quite frequently (in about
25% of the cases) the collocation did not conserve
its syntactic configuration. Either the verb ? here,
45
the equivalent for the French mener ? is omitted
in translations (like in 2.b below):
2.a) des contradictions existent dans la
politique qui est mene?e (Fr);
2.b) we are dealing with contradictory
policy (En),
or, in a few other cases, the whole collocation
disappears, since paraphrased with a completely
different syntactic construction:
3.a) direction qui a mene? une politique
insense?e de re?duction de personnel
(Fr);
3.b) a management that foolishly en-
gaged in staff reductions (En).
In order to quantify the impact such factors have
on the performance of the AM considered, we
further scrutinized the collocates list for politique
proposed by LL test for each language (see Table
2). The rank of a pair in the whole list of verb-
object collocations extracted, as assigned by the
LL test, is shown in the last column. In these sig-
nificance lists, the collocations with politique as an
object constitute a small fraction, and from these,
only the top collocations are displayed in Table 2.
The threshold was manually defined in accordance
with our intuition that the lower-scored pairs ob-
served manifest less a collocational strength. It
happens to be situated around the LL value of 20
for each language (and is of course specific to the
size of our corpus and to the number of V-O tokens
identified therein).
If we consider the LL rank as the success mea-
sure for collocate detection, we can infer that the
collocates of the word under investigation are eas-
ier to found in French, as compared to English,
Italian or Spanish, because the value in the first
row of the last column is smaller. This holds if we
are interested in only one (the most salient) collo-
cate for a word.
If we measure the success of retrieving all the
collocates (by considering, for instance, the speed
to access them in the results list ? the higher the
rank, the better), then French can be again consid-
ered the easiest because overall, the positions in
the V-O list are higher (i.e., the mean of the rank
column is smaller) with respect to Spanish, Italian
and, respectively, English.
This latter result corresponds, approximately,
to the order given by relative proportion of V-O
Language collocate freq LL score rank
French mener 74 376.8 45
politique e?laborer 17 50.1 734
adapter 5 48.3 780
axer 8 41.4 955
pratiquer 9 39.7 1011
de?velopper 13 28.1 1599
adapter 8 25.2 1867
poursuivre 11 24.4 1943
English pursue 39 214.9 122
policy implement 38 108.7 325
develop 30 81.1 473
conduct 8 28.9 2014
harmonize 9 28.2 2090
gear 5 27.7 2201
need 25 24.9 2615
apply 16 23.3 2930
Spanish practicar 17 98.7 246
pol??tica desarrollar 27 82.4 312
aplicar 25 65.7 431
seguir 17 33.5 1003
coordinar 8 31.0 1112
basar 11 25.1 1473
orientar 6 22.5 1707
adaptar 5 20.0 1987
construir 6 19.4 2057
Italian attuare 23 79.5 382
politica perseguire 14 46.4 735
praticare 8 37.6 976
seguire 18 30.2 1314
portare 12 29.7 1348
rivedere 9 26.0 1607
riformare 7 25.6 1639
sviluppare 12 22.1 1975
adottare 20 21.2 2087
Table 2: Verbal collocates for the headword policy
pairs in each language (Spanish 15.12%, French
15.14%, Italian 17.06%, and English 20.82%).
Given that in English V-O pairs are more numer-
ous and the verbs also participate in V-P construc-
tions, it might seem reasonable to expect lower
LL scores for V-O collocations in English vs. the
other 3 languages.
In general, we expect a correlation between ex-
traction difficulty and the distributional properties
of co-occurrence types.
7 Conclusion
The paper pointed out several issues that oc-
cur in transfering a hybrid collocation extraction
methodology (that combines linguistic with statis-
tic information) to a new language.
Besides the questionable availability of
language-specific text analysis tools for the new
language, a number of issues that are relevant to
extraction proper were addressed: the changes
in the distribution of (syntactic) word pairs, and
the need to find, for each language, the most
46
appropriate association measure to apply for each
syntactic type (given that AMs are sensitive to
distributions and syntactic types); the lack of
a priori defined syntactic types for a language;
and, finally, the portability of some widely used
techniques (such as the window method) from
English to other languages exhibiting a higher
word order freedom.
It is again in the multilingualism perspective
that the inescapable need for preprocessing the
text emerged (cf. different researchers cited in sec-
tion 3): highly inflected languages need lemma-
tizers, free-word order languages need structural
information in order to guarantee acceptable re-
sults. As language tools become nowadays more
and more available, we expect the collocation ex-
traction (and terminology acquisition in general)
to be exclusively performed in the future by re-
lying on linguistic analysis. We therefore believe
that multilingualism is a true concern for colloca-
tion extraction.
The paper reviewed the extraction work in a
language-oriented fashion, while mentioning the
type of linguistic preprocessing performed when-
ever it was the case, as well as the language-
specific issues identified by the authors. It then
proposed a strategy for implementing a multilin-
gual extraction procedure that takes into account
the language-specific issues identified.
An extraction system for four different lan-
guages, based on full parsing, was then described.
Finally, an experiment was carried out as a case
study, which pointed out several factors that might
determine a particular AM to perform differently
across languages. The experiment suggested that
log-likelihood ratios test might highlight certain
verb-object collocations easier in French than in
Spanish, Italian and English (in terms of salience
in the significance list).
Future work needs to extend the type of cross-
linguistic analysis initiated here, in order to pro-
vide more insights on the differences expected at
extraction between one language and another and
on the responsible factors, and, accordingly, to de-
fines strategies to deal with them.
Acknowledgements
The research described in this paper has been sup-
ported in part by a grant from the Swiss National
Foundation (No. 101412-103999).
References
Hiyan Alshawi and David Carter. 1994. Training
and scaling preference functions for disambiguation.
Computational Linguistics, 20(4):635?648.
Roberto Basili, Maria Teresa Pazienza, and Paola Ve-
lardi. 1994. A ?not-so-shallow? parser for colloca-
tional analysis. In Proceedings of the 15th confer-
ence on Computational linguistics, pages 447?453,
Kyoto, Japan. Association for Computational Lin-
guistics.
Morton Benson. 1990. Collocations and general-
purpose dictionaries. International Journal of Lexi-
cography, 3(1):23?35.
Godelieve L. M. Berry-Rogghe. 1973. The com-
putation of collocations and their relevance to lex-
ical studies. In A. J. Aitken, R. W. Bailey, and
N. Hamilton-Smith, editors, The Computer and Lit-
erary Studies, pages 103?112. Edinburgh.
Didier Bourigault. 1992. Surface grammatical analysis
for the extraction of terminological noun phrases. In
Proceedings of the 15th International Conference on
Computational Linguistics, pages 977?981, Nantes,
France.
Elisabeth Breidt. 1993. Extraction of V-N-collocations
from text corpora: A feasibility study for Ger-
man. In Proceedings of the Workshop on Very
Large Corpora: Academic and Industrial Perspec-
tives, Columbus, U.S.A.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1991. Word-
sense disambiguation using statistical methods. In
Proceedings of the 29th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 1991),
pages 264?270, Berkeley, California.
Nicoletta Calzolari and Remo Bindi. 1990. Acqui-
sition of lexical information from a large textual
Italian corpus. In Proceedings of the 13th Inter-
national Conference on Computational Linguistics,
pages 54?59, Helsinki, Finland.
Yaacov Choueka. 1988. Looking for needles in a
haystack, or locating interesting collocational ex-
pressions in large textual databases. In Proceedings
of the International Conference on User-Oriented
Content-Based Text and Image Handling, pages
609?623, Cambridge, U.S.A.
Kenneth Church and Patrick Hanks. 1990. Word as-
sociation norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22?29.
Kenneth Church, William Gale, Patrick Hanks, and
Donald Hindle. 1989. Parsing, word associations
and typical predicate-argument relations. In Pro-
ceedings of the International Workshop on Parsing
Technologies, pages 103?112, Pittsburgh. Carnegie
Mellon University.
47
D. Alan Cruse. 1986. Lexical Semantics. Cambridge
University Press, Cambridge.
Be?atrice Daille. 1994. Approche mixte pour
l?extraction automatique de terminologie : statis-
tiques lexicales et filtres linguistiques. Ph.D. thesis,
Universite? Paris 7.
Gae?l Dias. 2003. Multiword unit hybrid extraction.
In Proceedings of the ACL Workshop on Multiword
Expressions, pages 41?48, Sapporo, Japan.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61?74.
Stefan Evert and Brigitte Krenn. 2001. Methods for
the qualitative evaluation of lexical association mea-
sures. In Proceedings of the 39th Annual Meeting
of the Association for Computational Linguistics,
pages 188?195, Toulouse, France.
Stefan Evert. 2004. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis,
University of Stuttgart.
John Rupert Firth, 1957. Papers in Linguistics 1934-
1951, chapter Modes of Meaning, pages 190?215.
Oxford Univ. Press, Oxford.
J. R. Firth. 1968. A synopsis of linguistic theory,
1930?55. In F.R. Palmer, editor, Selected papers
of J. R. Firth, 1952-1959. Indiana University Press,
Bloomington.
Thierry Fontenelle. 1992. Collocation acquisition
from a corpus or from a dictionary: a comparison.
Proceedings I-II. Papers submitted to the 5th EU-
RALEX International Congress on Lexicography in
Tampere, pages 221?228.
Katerina T. Frantzi, Sophia Ananiadou, and Hideki
Mima. 2000. Automatic recognition of multi-word
terms: the C-value/NC-value method. International
Journal on Digital Libraries, 2(3):115?130.
Jean-Philippe Goldman, Luka Nerima, and Eric
Wehrli. 2001. Collocation extraction using a syn-
tactic parser. In Proceedings of the ACL Workshop
on Collocations, pages 61?66, Toulouse, France.
Maurice Gross. 1984. Lexicon-grammar and the syn-
tactic analysis of French. In Proceedings of the 22nd
conference on Association for Computational Lin-
guistics, pages 275?282, Morristown, NJ, USA.
Franz Iosef Hausmann. 1985. Kollokationen im
deutschen wo?rterbuch. ein beitrag zur theorie des
lexikographischen beispiels?. In Henning Bergen-
holtz and Joachim Mugdan, editors, Lezikographie
und Grammatik. Akten des Essener Kolloquiums zur
Grammatik im Wo?rterbuch., Lexicographica. Series
Major 3, pages 118?129.
Ulrich Heid. 1994. On ways words work together -
research topics in lexical combinatorics. In W. Mar-
tin, W. Meijs, M. Moerland, E. ten Pas, P. van
Sterkenburg, and P. Vossen, editors, Proceedings of
the VIth Euralex International Congress (EURALEX
?94), pages 226?257, Amsterdam.
Chu-Ren Huang, Adam Kilgarriff, Yiching Wu, Chih-
Ming Chiu, Simon Smith, Pavel Rychly, Ming-Hong
Bai, and Keh-Jiann Chen. 2005. Chinese Sketch
Engine and the extraction of grammatical colloca-
tions. In Proceedings of the Fourth SIGHAN Work-
shop on Chinese Language Processing, pages 48?
55, Jeju Island, Republic of Korea.
Satoru Ikehara, Satoshi Shirai, and Tsukasa Kawaoka.
1995. Automatic extraction of uninterrupted collo-
cations by n-gram statistics. In Proceedings of first
Annual Meeting of the Association for Natural Lan-
guage Processing, pages 313?316.
Ray Jackendoff. 1997. The Architecture of the Lan-
guage Faculty. MIT Press, Cambridge, MA.
John S. Justeson and Slava M. Katz. 1995. Technical
terminology: Some linguistis properties and an al-
gorithm for identification in text. Natural Language
Engineering, 1:9?27.
Seonho Kim, Zooil Yang, Mansuk Song, and Jung-Ho
Ahn. 1999. Retrieving collocations from Korean
text. In Proceedings of the 1999 Joint SIGDAT Con-
ference on Empirical Methods in Natural Language
Processing and Very Large Corpora, pages 71?81,
Maryland, U.S.A.
Mihoko Kitamura and Yuji Matsumoto. 1996. Auto-
matic extraction of word sequence correspondences
in parallel corpora. In Proceedings of the 4th Work-
shop on Very Large Corpora, pages 79?87, Copen-
hagen, Denmark, August.
Go?ran Kjellmer. 1994. A Dictionary of English Collo-
cations. Claredon Press, Oxford.
Brigitte Krenn and Stefan Evert. 2001. Can we do
better than frequency? A case study on extracting
PP-verb collocations. In Proceedings of the ACL
Workshop on Collocations, pages 39?46, Toulouse,
France.
Julian Kupiec. 1993. An algorithm for finding noun
phrase correspondences in bilingual corpora. In 31st
Annual Meeting of the Association for Computa-
tional Linguistics, pages 17?22, Columbus, Ohio,
U.S.A.
P. Lafon. 1984. De?pouillement et statistique en
le?xicometrie. Slatkine-Champion, Paris.
Michael Lewis. 2000. Teaching Collocations. Further
Developments In The Lexical Approach. Language
Teaching Publications, Hove.
Dekang Lin. 1998. Extracting collocations from text
corpora. In First Workshop on Computational Ter-
minology, pages 57?63, Montreal.
48
Qin Lu, Yin Li, and Ruifeng Xu. 2004. Improving
Xtract for Chinese collocation extraction. In Pro-
ceedings of IEEE International Conference on Natu-
ral Language Processing and Knowledge Engineer-
ing, pages 333?338.
Kathleen R. McKeown and Dragomir R. Radev. 2000.
Collocations. In Robert Dale, Hermann Moisl,
and Harold Somers, editors, A Handbook of Nat-
ural Language Processing, pages 507?523. Marcel
Dekker, New York, U.S.A.
I. Dan Melamed. 1997. A portable algorithm for
mapping bitext correspondence. In Proceedings of
the 35th Conference of the Association for Com-
putational Linguistics (ACL?97), pages 305?312,
Madrid, Spain.
Igor Mel?c?uk. 1998. Collocations and lexical func-
tions. In Anthony P. Cowie, editor, Phraseology.
Theory, Analysis, and Applications, pages 23?53.
Claredon Press, Oxford.
Igor Mel?c?uk. 2003. Collocations: de?finition, ro?le et
utilite?. In Francis Grossmann and Agne`s Tutin, ed-
itors, Les collocations: analyse et traitement, pages
23?32. Editions ?De Werelt?, Amsterdam.
Darren Pearce. 2001. Synonymy in collocation extrac-
tion. In WordNet and Other Lexical Resources: Ap-
plications, Extensions and Customizations (NAACL
2001 Workshop), pages 41?46, Pittsburgh, U.S.A.
Pavel Pecina. 2005. An extensive empirical study of
collocation extraction methods. In Proceedings of
the ACL Student Research Workshop, pages 13?18,
Ann Arbor, Michigan, June.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Pro-
ceedings of the Third International Conference on
Intelligent Text Processing and Computational Lin-
guistics (CICLING 2002), pages 1?15, Mexico City.
Violeta Seretan and Eric Wehrli. 2006. Accurate col-
location extraction using a multilingual parser. In
Proceedings of COLING/ACL 2006. To appear.
Violeta Seretan, Luka Nerima, and Eric Wehrli. 2004.
A tool for multi-word collocation extraction and vi-
sualization in multilingual corpora. In Proceedings
of the Eleventh EURALEX International Congress,
EURALEX 2004, pages 755?766, Lorient, France.
Violeta Seretan. 2005. Induction of syntactic col-
location patterns from generic syntactic relations.
In Proceedings of Nineteenth International Joint
Conference on Artificial Intelligence (IJCAI 2005),
pages 1698?1699, Edinburgh, Scotland, July.
Sayori Shimohata, Toshiyuki Sugio, and Junji Nagata.
1997. Retrieving collocations by co-occurrences
and word order constraints. In Proceedings of the
Annual Meeting of the Association for Computa-
tional Linguistics, pages 476?481, Madrid, Spain.
John Sinclair. 1995. Collins Cobuild English Dictio-
nary. Harper Collins, London.
Frank Smadja, Kathleen McKeown, and Vasileios
Hatzivassiloglou. 1996. Translating collocations for
bilingual lexicons: a statistical approach. Computa-
tional Linguistics, 22(1):1?38.
Frank Smadja. 1993. Retrieving collocations from
text: Xtract. Computational Linguistics, 19(1):143?
177.
Eric Wehrli. 2003. Translation of words in context.
In Proceedings of Machine Translation Summit IX,
pages 502?504, New Orleans, Lousiana, U.S.A.
Eric Wehrli. 2004. Un mode`le multilingue d?analyse
syntaxique. In A. Auchlin et al, editor, Structures
et discours - Me?langes offerts a` Eddy Roulet, pages
311?329. E?ditions Nota bene, Que?bec.
Joachim Wermter and Udo Hahn. 2004. Collocation
extraction based on modifiability statistics. In Pro-
ceedings of the 20th International Conference on
Computational Linguistics (COLING 2004), pages
980?986, Geneva, Switzerland.
Dekai Wu. 1994. Aligning a parallel English-Chinese
corpus statistically with lexical criteria. In Proceed-
ings of the 32nd Annual Meeting of the Association
for Computational Linguistics (ACL 1994), pages
80?87, Las Cruces (New Mexico), U.S.A.
Diana Zaiu Inkpen and Graeme Hirst. 2002. Ac-
quiring collocations for lexical choice between near-
synonyms. In Proceedings of the ACL-02 Workshop
on Unsupervised Lexical Acquisition, pages 67?76,
Philadephia, Pennsylvania.
Re?mi Zajac, Elke Lange, and Jin Yang. 2003. Cus-
tomizing complex lexical entries for high-quality
MT. In Proceedings of the Ninth Machine Trans-
lation Summit, New Orleans, U.S.A.
Heike Zinsmeister and Ulrich Heid. 2003. Signif-
icant triples: Adjective+Noun+Verb combinations.
In Proceedings of the 7th Conference on Compu-
tational Lexicography and Text Research (Complex
2003), Budapest.
Heike Zinsmeister and Ulrich Heid. 2004. Colloca-
tions of complex nouns: Evidence for lexicalisation.
In Proceedings of KONVENS 2004, Vienna, Austria.
49
131
132
133
134
131
132
133
134
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 953?960,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Accurate Collocation Extraction Using a Multilingual Parser
Violeta Seretan
Language Technology Laboratory
University of Geneva
2, rue de Candolle, 1211 Geneva
Violeta.Seretan@latl.unige.ch
Eric Wehrli
Language Technology Laboratory
University of Geneva
2, rue de Candolle, 1211 Geneva
Eric.Wehrli@latl.unige.ch
Abstract
This paper focuses on the use of advanced
techniques of text analysis as support for
collocation extraction. A hybrid system is
presented that combines statistical meth-
ods and multilingual parsing for detecting
accurate collocational information from
English, French, Spanish and Italian cor-
pora. The advantage of relying on full
parsing over using a traditional window
method (which ignores the syntactic in-
formation) is first theoretically motivated,
then empirically validated by a compara-
tive evaluation experiment.
1 Introduction
Recent computational linguistics research fully ac-
knowledged the stringent need for a systematic
and appropriate treatment of phraseological units
in natural language processing applications (Sag
et al, 2002). Syntagmatic relations between words
? also called multi-word expressions, or ?id-
iosyncratic interpretations that cross word bound-
aries? (Sag et al, 2002, 2) ? constitute an im-
portant part of the lexicon of a language: accord-
ing to Jackendoff (1997), they are at least as nu-
merous as the single words, while according to
Mel?c?uk (1998) they outnumber single words ten
to one.
Phraseological units include a wide range of
phenomena, among which we mention compound
nouns (dead end), phrasal verbs (ask out), idioms
(lend somebody a hand), and collocations (fierce
battle, daunting task, schedule a meeting). They
pose important problems for NLP applications,
both text analysis and text production perspectives
being concerned.
In particular, collocations1 are highly problem-
atic, for at least two reasons: first, because their
linguistic status and properties are unclear (as
pointed out by McKeown and Radev (2000), their
definition is rather vague, and the distinction from
other types of expressions is not clearly drawn);
second, because they are prevalent in language.
Mel?c?uk (1998, 24) claims that ?collocations make
up the lions share of the phraseme inventory?, and
a recent study referred in (Pearce, 2001) showed
that each sentence is likely to contain at least one
collocation.
Collocational information is not only useful, but
also indispensable in many applications. In ma-
chine translation, for instance, it is considered ?the
key to producing more acceptable output? (Orliac
and Dillinger, 2003, 292).
This article presents a system that extracts ac-
curate collocational information from corpora by
using a syntactic parser that supports several lan-
guages. After describing the underlying method-
ology (section 2), we report several extraction re-
sults for English, French, Spanish and Italian (sec-
tion 3). Then we present in sections 4 and 5 a com-
parative evaluation experiment proving that a hy-
brid approach leads to more accurate results than a
classical approach in which syntactic information
is not taken into account.
2 Hybrid Collocation Extraction
We consider that syntactic analysis of source cor-
pora is an inescapable precondition for colloca-
tion extraction, and that the syntactic structure of
source text has to be taken into account in order to
ensure the quality and interpretability of results.
1To put it simply, collocations are non-idiomatical, but
restricted, conventional lexical combinations.
953
As a matter of fact, some of the existing colloca-
tion extraction systems already employ (but only
to a limited extent) linguistic tools in order to sup-
port the collocation identification in text corpora.
For instance, lemmatizers are often used for recog-
nizing all the inflected forms of a lexical item, and
POS taggers are used for ruling out certain cate-
gories of words, e.g., in (Justeson and Katz, 1995).
Syntactic analysis has long since been recog-
nized as a prerequisite for collocation extraction
(for instance, by Smadja2), but the traditional sys-
tems simply ignored it because of the lack, at that
time, of efficient and robust parsers required for
processing large corpora. Oddly enough, this situ-
ation is nowadays perpetuated, in spite of the dra-
matic advances in parsing technology. Only a few
exceptions exists, e.g., (Lin, 1998; Krenn and Ev-
ert, 2001).
One possible reason for this might be the way
that collocations are generally understood, as a
purely statistical phenomenon. Some of the best-
known definitions are the following: ?Colloca-
tions of a given word are statements of the ha-
bitual and customary places of that word? (Firth,
1957, 181); ?arbitrary and recurrent word combi-
nation? (Benson, 1990); or ?sequences of lexical
items that habitually co-occur? (Cruse, 1986, 40).
Most of the authors make no claims with respect to
the grammatical status of the collocation, although
this can indirectly inferred from the examples they
provide.
On the contrary, other definitions state explic-
itly that a collocation is an expression of language:
?co-occurrence of two or more lexical items as
realizations of structural elements within a given
syntactic pattern? (Cowie, 1978); ?a sequence of
two or more consecutive words, that has character-
istics of a syntactic and semantic unit? (Choueka,
1988). Our approach is committed to these later
definitions, hence the importance we lend to us-
ing appropriate extraction methodologies, based
on syntactic analysis.
The hybrid method we developed relies on the
parser Fips (Wehrli, 2004), that implements the
Government and Binding formalism and supports
several languages (besides the ones mentioned in
2?Ideally, in order to identify lexical relations in a corpus
one would need to first parse it to verify that the words are
used in a single phrase structure. However, in practice, free-
style texts contain a great deal of nonstandard features over
which automatic parsers would fail. This fact is being seri-
ously challenged by current research (...), and might not be
true in the near future? (Smadja, 1993, 151).
the abstract, a few other are also partly dealt with).
We will not present details about the parser here;
what is relevant for this paper is the type of syn-
tactic structures it uses. Each constituent is rep-
resented by a simplified X-bar structure (without
intermediate level), in which to the lexical head is
attached a list of left constituents (its specifiers)
and right constituents (its complements), and each
of these are in turn represented by the same type
of structure, recursively.
Generally speaking, a collocation extraction can
be seen as a two-stage process:
I. in stage one, collocation candidates are iden-
tified from the text corpora, based on criteria
which are specific to each system;
II. in stage two, the candidates are scored and
ranked using specific association measures
(a review can be found in (Manning and
Schu?tze, 1999; Evert, 2004; Pecina, 2005)).
According to this description, in our approach
the parser is used in the first stage of extraction,
for identifying the collocation candidates. A pair
of lexical items is selected as a candidate only if
there is a syntactic relation holding between the
two items (one being the head of the current parse
structure, and the other the lexical head of its spec-
ifier/complement). Therefore, the criterion we em-
ploy for candidate selection is the syntactic prox-
imity, as opposed to the linear proximity used by
traditional, window-based methods.
As the parsing goes on, the syntactic word pairs
are extracted from the parse structures created,
from each head-specifier or head-complement re-
lation. The pairs obtained are then partitioned
according to their syntactic configuration (e.g.,
noun + adjectival or nominal specifier, noun +
argument, noun + adjective in predications, verb
+ adverbial specifier, verb + argument (subject,
object), verb + adjunt, etc). Finally, the log-
likelihood ratios test (henceforth LLR) (Dunning,
1993) is applied on each set of pairs. We call
this method hybrid, since it combines syntactic
and statistical information (about word and co-
occurrence frequency).
The following examples ? which, like all the
examples in this paper, are actual extraction re-
sults ? demonstrate the potential of our system
to detect collocation candidates, even if subject to
complex syntactic transformations.
954
1.a) raise question: The question of
political leadership has been raised
several times by previous speakers.
1.b) play role: What role can Canada?s
immigration program play in help-
ing developing nations... ?
1.c) make mistake: We could look back
and probably see a lot of mistakes
that all parties including Canada
perhaps may have made.
3 Multilingual Extraction Results
In this section, we present several extraction re-
sults obtained with the system presented in sec-
tion 2. The experiments were performed on data
in the four languages, and involved the following
corpora: for English and French, a subpart or the
Hansard Corpus of proceedings from the Canadian
Parliament; for Italian, documents from the Swiss
Parliament; and for Spanish, a news corpus dis-
tributed by the Linguistic Data Consortium.
Some statistics on these corpora, some process-
ing details and quantitative results are provided in
Table 1. The first row lists the corpora size (in
tokens); the next three rows show some parsing
statistics3, and the last rows display the number of
collocation candidates extracted and of candidates
for which the LLR score could be computed4.
Statistics English French Spanish Italian
tokens 3509704 1649914 1023249 287804
sentences 197401 70342 67502 12008
compl. parse 139498 50458 13245 4511
avg. length 17.78 23.46 15.16 23.97
pairs 725025 370932 162802 58258
(extracted) 276670 147293 56717 37914
pairs 633345 308410 128679 47771
(scored) 251046 131384 49495 30586
Table 1: Extraction statistics
In Table 2 we list the top collocations (of length
two) extracted for each language. We do not
specifically discuss here multilingual issues in col-
location extraction; these are dealt with in a sepa-
rate paper (Seretan and Wehrli, 2006).
3The low rate of completely parsed sentences for Spanish
and Italian are due to the relatively reduced coverage of the
parsers of these two languages (under development). How-
ever, even if a sentence is not assigned a complete parse tree,
some syntactic pairs can still be collected from the partial
parses.
4The log-likelihood ratios score is undefined for those
pairs having a cell of the contingency table equal to 0.
Language Key1 Key2 LLR score
English federal government 7229.69
reform party 6530.69
house common 6006.84
minister finance 5829.05
acting speaker 5551.09
red book 5292.63
create job 4131.55
right Hon 4117.52
official opposition 3640.00
deputy speaker 3549.09
French premier ministre 4317.57
bloc que?be?cois 3946.08
discours tro?ne 3894.04
ve?rificateur ge?ne?ral 3796.68
parti re?formiste 3615.04
gouvernement fe?de?ral 3461.88
missile croisie`re 3147.42
Chambre commune 3083.02
livre rouge 2536.94
secre?taire parlementaire 2524.68
Spanish banco central 4210.48
millo?n do?lar 3312.68
millo?n peso 2335.00
libre comercio 2169.02
nuevo peso 1322.06
tasa intere?s 1179.62
deuda externo 1119.91
ca?mara representante 1015.07
asamblea ordinario 992.85
papel comercial 963.95
Italian consiglio federale 3513.19
scrivere consiglio 594.54
unione europeo 479.73
servizio pubblico 452.92
milione franco 447.63
formazione continuo 388.80
iniziativa popolare 383.68
testo interpellanza 377.46
punto vista 373.24
scrivere risposta 348.77
Table 2: Top ten collocations extracted for each
language
The collocation pairs obtained were further pro-
cessed with a procedure of long collocations ex-
traction described elsewhere (Seretan et al, 2003).
Some examples of collocations of length 3, 4
and 5 obtained are: minister of Canadian her-
itage, house proceed to statement by, secretary to
leader of gouvernment in house of common (En),
question adresser a` ministre, programme de aide
a` re?novation re?sidentielle, agent employer force
susceptible causer (Fr), bolsa de comercio local,
peso en cuota de fondo de inversio?n, permitir uso
de papel de deuda esterno (Sp), consiglio federale
disporre, creazione di nuovo posto di lavoro, cos-
tituire fattore penalizzante per regione (It)5.
5Note that the output of the procedure contains lemmas
rather than inflected forms.
955
4 Comparative Evaluation Hypotheses
4.1 Does Parsing Really Help?
Extracting collocations from raw text, without pre-
processing the source corpora, offers some clear
advantages over linguistically-informed methods
such as ours, which is based on the syntactic anal-
ysis: speed (in contrast, parsing large corpora of
texts is expected to be much more time consum-
ing), robustness (symbolic parsers are often not
robust enough for processing large quantities of
data), portability (no need to a priori define syn-
tactic configurations for collocations candidates).
On the other hand, these basic systems suffer
from the combinatorial explosion if the candidate
pairs are chosen from a large search space. To
cope with this problem, a candidate pair is usu-
ally chosen so that both words are inside a context
(?collocational?) window of a small length. A 5-
word window is the norm, while longer windows
prove impractical (Dias, 2003).
It has been argued that a window size of 5 is
actually sufficient for capturing most of the col-
locational relations from texts in English. But
there is no evidence sustaining that the same holds
for other languages, like German or the Romance
ones that exhibit freer word order. Therefore, as
window-based systems miss the ?long-distance?
pairs, their recall is presumably lower than that of
parse-based systems. However, the parser could
also miss relevant pairs due to inherent analysis
errors.
As for precision, the window systems are sus-
ceptible to return more noise, produced by the
grammatically unrelated pairs inside the colloca-
tional window. By dividing the number of gram-
matical pairs by the total number of candidates
considered, we obtain the overall precision with
respect to grammaticality; this result is expected to
be considerably worse in the case of basic method
than for the parse-based methods, just by virtue
of the parsing task. As for the overall precision
with respect to collocability, we expect the propor-
tional figures to be preserved. This is because the
parser-based methods return less, but better pairs
(i.e., only the pairs identified as grammatical), and
because collocations are a subset of the grammat-
ical pairs.
Summing up, the evaluation hypothesis that can
be stated here is the following: parse-based meth-
ods outperform basic methods thanks to a drastic
reduction of noise. While unquestionable under
the assumption of perfect parsing, this hypothesis
has to be empirically validated in an actual setting.
4.2 Is More Data Better Than Better Data?
The hypothesis above refers to the overall preci-
sion and recall, that is, relative to the entire list of
selected candidates. One might argue that these
numbers are less relevant for practice than they
are from a theoretical (evaluation) perspective, and
that the exact composition of the list of candi-
dates identified is unimportant if only the top re-
sults (i.e., those pairs situated above a threshold)
are looked at by a lexicographer or an application.
Considering a threshold for the n-best candi-
dates works very much in the favor of basic meth-
ods. As the amount of data increases, there is
a reduction of the noise among the best-scored
pairs, which tend to be more grammatical because
the likelihood of encountering many similar noisy
pairs is lower. However, as the following example
shows, noisy pairs may still appear in top, if they
occur often in a longer collocation:
2.a) les essais du missile de croisie`re
2.b) essai - croisie`re
The pair essai - croisie`re is marked by the basic
systems as a collocation because of the recurrent
association of the two words in text as part or the
longer collocation essai du missile de croisie`re. It
is an grammatically unrelated pair, while the cor-
rect pairs reflecting the right syntactic attachment
are essai missile and missile (de) croisie`re.
We mentioned that parsing helps detecting the
?long-distance? pairs that are outside the limits
of the collocational window. Retrieving all such
complex instances (including all the extraposition
cases) certainly augment the recall of extraction
systems, but this goal might seem unjustified, be-
cause the risk of not having a collocation repre-
sented at all diminishes as more and more data
is processed. One might think that systematically
missing long-distance pairs might be very simply
compensated by supplying the system with more
data, and thus that larger data is a valid alternative
to performing complex processing.
While we agree that the inclusion of more data
compensates for the ?difficult? cases, we do con-
sider this truly helpful in deriving collocational
information, for the following reasons: (1) more
data means more noise for the basic methods; (2)
some collocations might systematically appear in
956
a complex grammatical environment (such as pas-
sive constructions or with additional material in-
serted between the two items); (3) more impor-
tantly, the complex cases not taken into account
alter the frequency profile of the pairs concerned.
These observations entitle us to believe that,
even when more data is added, the n-best precision
might remain lower for the basic methods with re-
spect to the parse-based ones.
4.3 How Real the Counts Are?
Syntactic analysis (including shallower levels of
linguistic analysis traditionally used in collocation
extraction, such as lemmatization, POS tagging, or
chunking) has two main functions.
On the one hand, it guides the extraction system
in the candidate selection process, in order to bet-
ter pinpoint the pairs that might form collocations
and to exclude the ones considered as inappropri-
ate (e.g., the pairs combining function words, such
as a preposition followed by a determiner).
On the other, parsing supports the association
measures that will be applied on the selected can-
didates, by providing more exact frequency infor-
mation on words ? the inflected forms count as
instances of the same lexical item ? and on their
co-occurrence frequency ? certain pairs might
count as instance of the same pair, others do not.
In the following example, the pair loi modifier
is an instance of a subject-verb collocation in 3.a),
and of a verb-object collocation type in 3.b). Basic
methods are unable to distinguish between the two
types, and therefore count them as equivalent.
3.a) Loi modifiant la Loi sur la respons-
abilite? civile
3.b) la loi devrait e?tre modifie?e
Parsing helps to create a more realistic fre-
quency profile for the candidate pairs, not only be-
cause of the grammaticality constraint it applies
on the pairs (wrong pairs are excluded), but also
because it can detect the long-distance pairs that
are outside the collocational window.
Given that the association measures rely heav-
ily on the frequency information, the erroneous
counts have a direct influence on the ranking of
candidates and, consequently, on the top candi-
dates returned. We believe that in order to achieve
a good performance, extraction systems should be
as close as possible to the real frequency counts
and, of course, to the real syntactic interpretation
provided in the source texts6.
Since parser-based methods rely on more accu-
rate frequency information for words and their co-
occurrence than window methods, it follows that
the n-best list obtained with the first methods will
probably show an increase in quality over the sec-
ond.
To conclude this section, we enumerate the hy-
potheses that have been formulated so far: (1)
Parse methods provide a noise-freer list of collo-
cation candidates, in comparison with the window
methods; (2) Local precision (of best-scored re-
sults) with respect to grammaticality is higher for
parse methods, since in basic methods some noise
still persists, even if more data is included; (3) Lo-
cal precision with respect to collocability is higher
for parse methods, because they use a more realis-
tic image of word co-occurrence frequency.
5 Comparative Evaluation
We compare our hybrid method (based on syntac-
tic processing of texts) against the window method
classically used in collocation extraction, from the
point of view of their precision with respect to
grammaticality and collocability.
5.1 The Method
The n-best extraction results, for a given n (in our
experiment, n varies from 50 to 500 at intervals
of 50) are checked in each case for grammatical
well-formedness and for lexicalization. By lexi-
calization we mean the quality of a pair to con-
stitute (part of) a multi-word expression ? be it
compound, collocation, idiom or another type of
syntagmatic lexical combination. We avoid giving
collocability judgments since the classification of
multi-word expressions cannot be made precisely
and with objective criteria (McKeown and Radev,
2000). We rather distinguish between lexicaliz-
able and trivial combinations (completely regular
productions, such as big house, buy bread, that
do not deserve a place in the lexicon). As in
(Choueka, 1988) and (Evert, 2004), we consider
that a dominant feature of collocations is that they
are unpredictable for speakers and therefore have
to be stored into a lexicon.
6To exemplify this point: the pair de?veloppement hu-
main (which has been detected as a collocation by the basic
method) looks like a valid expression, but the source text con-
sistently offers a different interpretation: de?veloppement des
ressources humaines.
957
Each collocation from the n-best list at the
different levels considered is therefore annotated
with one of the three flags: 1. ungrammatical;
2. trivial combination; 3. multi-word expression
(MWE).
On the one side, we evaluate the results of our
hybrid, parse-based method; on the other, we sim-
ulate a window method, by performing the fol-
lowing steps: POS-tag the source texts; filter the
lexical items and retain only the open-class POS;
consider all their combinations within a colloca-
tional window of length 5; and, finally, apply the
log-likelihood ratios test on the pairs of each con-
figuration type.
In accordance with (Evert and Kermes, 2003),
we consider that the comparative evaluation of
collocation extraction systems should not be done
at the end of the extraction process, but separately
for each stage: after the candidate selection stage,
for evaluating the quality (in terms of grammati-
cality) of candidates proposed; and after the ap-
plication of collocability measures, for evaluating
the measures applied. In each of these cases, dif-
ferent evaluation methodologies and resources are
required. In our case, since we used the same mea-
sure for the second stage (the log-likelihood ratios
test), we could still compare the final output of ba-
sic and parse-based methods, as given by the com-
bination of the first stage with the same collocabil-
ity measure.
Again, similarly to Krenn and Evert (2001), we
believe that the homogeneity of data is important
for the collocability measures. We therefore ap-
plied the LLR test on our data after first partition-
ing it into separate sets, according to the syntacti-
cal relation holding in each candidate pair. As the
data used in the basic method contains no syntac-
tic information, the partitioning was done based on
POS-combination type.
5.2 The Data
The evaluation experiment was performed on the
whole French corpus used in the extraction exper-
iment (section 2), that is, a subpart of the Hansard
corpus of Canadian Parliament proceedings. It
contains 112 text files totalling 8.43 MB, with
an average of 628.1 sentences/file and 23.46 to-
kens/sentence (as detected by the parser). The to-
tal number of tokens is 1, 649, 914.
On the one hand, the texts were parsed and
370, 932 candidate pairs were extracted using the
hybrid method we presented. Among the pairs ex-
tracted, 11.86% (44, 002 pairs) were multi-word
expressions identified at parse-time, since present
in the parser?s lexicon. The log-likelihood ratios
test was applied on the rest of pairs. A score
could be associated to 308, 410 of these pairs (cor-
responding to 131, 384 types); for the others, the
score was undefined.
On the other hand, the texts were POS-tagged
using the same parser as in the first case. If in the
first case the candidate pairs were extracted dur-
ing the parsing, in the second they were generated
after the open-class filtering. From 673, 789 POS-
filtered tokens, a number of 1, 024, 888 combina-
tions (560, 073 types) were created using the 5-
length window criterion, while taking care not to
cross a punctuation mark. A score could be asso-
ciated to 1, 018, 773 token pairs (554, 202 types),
which means that the candidate list is considerably
larger than in the first case. The processing time
was more than twice longer than in the first case,
because of the large amount of data to handle.
5.3 Results
The 500 best-scored collocations retrieved with
the two methods were manually checked by three
human judges and annotated, as explained in 5.1,
as either ungrammatical, trivial or MWE. The
agreement statistics on the annotations for each
method are shown in Table 3.
Method Agr. 1,2,3 1,2 1,3 2,3
parse observed 285 365 362 340
k-score 55.4% 62.6% 69% 64%
window observed 226 339 327 269
k-score 43.1% 63.8% 61.1% 48%
Table 3: Inter-annotator agreement
For reporting n-best precision results, we used
as reference set the annotated pairs on which at
least two of the three annotators agreed. That
is, from the 500 initial pairs retrieved with each
method, 497 pairs were retained in the first case
(parse method), and 483 pairs in the second (win-
dow method).
Table 4 shows the comparative evaluation re-
sults for precision at different levels in the list
of best-scored pairs, both with respect to gram-
maticality and to collocability (or, more exactly,
the potential of a pair to constitute a MWE). The
numbers show that a drastic reduction of noise is
achieved by parsing the texts. The error rate with
958
Precision (gram.) Precision (MWE)
n window parse window parse
50 94.0 96.0 80.0 72.0
100 91.0 98.0 75.0 74.0
150 87.3 98.7 72.7 73.3
200 85.5 98.5 70.5 74.0
250 82.8 98.8 67.6 69.6
300 82.3 98.7 65.0 69.3
350 80.3 98.9 63.7 67.4
400 80.0 99.0 62.5 67.0
450 79.6 99.1 61.1 66.0
500 78.3 99.0 60.1 66.0
Table 4: Comparative evaluation results
respect to grammaticality is, on average, 15.9%
for the window method; with parsing, it drops to
1.5% (i.e., 10.6 times smaller).
This result confirms our hypothesis regarding
the local precision which was stated in section 4.2.
Despite the inherent parsing errors, the noise re-
duction is substantial. It is also worth noting that
we compared our method against a rather high
baseline, as we made a series of choices suscep-
tible to alleviate the candidates identification with
the window-based method: we filtered out func-
tion words, we used a parser for POS-tagging (that
eliminated POS-ambiguity), and we filtered out
cross-punctuation pairs.
As for the MWE precision, the window method
performs better for the first 100 pairs7); on the re-
maining part, the parsing-based method is on aver-
age 3.7% better. The precision curve for the win-
dow method shows a more rapid degradation than
it does for the other. Therefore we can conclude
that parsing is especially advantageous if one in-
vestigates more that the first hundred results (as
it seems reasonable for large extraction experi-
ments).
In spite of the rough classification we used in
annotation, we believe that the comparison per-
formed is nonetheless meaningful since results
should be first checked for grammaticality and
?triviality? before defining more difficult tasks
such as collocability.
6 Conclusion
In this paper, we provided both theoretical and em-
pirical arguments in the favor of performing syn-
tactic analysis of texts prior to the extraction of
collocations with statistical methods.
7A closer look at the data revealed that this might be ex-
plained by some inconsistencies between annotations.
Part of the extraction work that, like ours, re-
lies on parsing was cited in section 2. Most of-
ten, it concerns chunking rather than complete
parsing; specific syntactic configurations (such as
adjective-noun, preposition-noun-verb); and lan-
guages other than the ones we deal with (usually,
English and German). Parsing has been also used
after extraction (Smadja, 1993) for filtering out in-
valid results. We believe that this is not enough
and that parsing is required prior to the applica-
tion of statistical tests, for computing a realistic
frequency profile for the pairs tested.
As for evaluation, unlike most of the existing
work, we are not concerned here with compar-
ing the performance of association measures (cf.
(Evert, 2004; Pecina, 2005) for comprehensive
references), but with a contrastive evaluation of
syntactic-based and standard extraction methods,
combined with the same statistical computation.
Our study finally clear the doubts on the use-
fulness of parsing for collocation extraction. Pre-
vious work that quantified the influence of parsing
on the quality of results suggested the performance
for tagged and parsed texts is similar (Evert and
Kermes, 2003). This result applies to a quite rigid
syntactic pattern, namely adjective-noun in Ger-
man. But a preceding study on noun-verb pairs
(Breidt, 1993) came to the conclusion that good
precision can only be achieved for German with
parsing. Its author had to simulate parsing because
of the lack, at the time, of parsing tools for Ger-
man. Our report, that concerns an actual system
and a large data set, validates Breidt?s finding for
a new language (French).
Our experimental results confirm the hypothe-
ses put forth in section 4, and show that parsing
(even if imperfect) benefits to extraction, notably
by a drastic reduction of the noise in the top of
the significance list. In future work, we consider
investigating other levels of the significance list,
extending the evaluation to other languages, com-
paring against shallow-parsing methods instead of
the window method, and performing recall-based
evaluation as well.
Acknowledgements
We would like to thank Jorge Antonio Leoni de
Leon, Mar Ndiaye, Vincenzo Pallotta and Yves
Scherrer for participating to the annotation task.
We are also grateful to Gabrielle Musillo and to
the anonymous reviewers of an earlier version of
959
this paper for useful comments and suggestions.
References
Morton Benson. 1990. Collocations and general-
purpose dictionaries. International Journal of Lexi-
cography, 3(1):23?35.
Elisabeth Breidt. 1993. Extraction of V-N-collocations
from text corpora: A feasibility study for Ger-
man. In Proceedings of the Workshop on Very
Large Corpora: Academic and Industrial Perspec-
tives, Columbus, U.S.A.
Yaacov Choueka. 1988. Looking for needles in a
haystack, or locating interesting collocational ex-
pressions in large textual databases expressions in
large textual databases. In Proceedings of the In-
ternational Conference on User-Oriented Content-
Based Text and Image Handling, pages 609?623,
Cambridge, MA.
Anthony P. Cowie. 1978. The place of illustrative ma-
terial and collocations in the design of a learner?s
dictionary. In P. Strevens, editor, In Honour of A.S.
Hornby, pages 127?139. Oxford: Oxford University
Press.
D. Alan Cruse. 1986. Lexical Semantics. Cambridge
University Press, Cambridge.
Gae?l Dias. 2003. Multiword unit hybrid extraction.
In Proceedings of the ACL Workshop on Multiword
Expressions, pages 41?48, Sapporo, Japan.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61?74.
Stefan Evert and Hannah Kermes. 2003. Experi-
ments on candidate data for collocation extraction.
In Companion Volume to the Proceedings of the 10th
Conference of The European Chapter of the Associ-
ation for Computational Linguistics, pages 83?86,
Budapest, Hungary.
Stefan Evert. 2004. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations Word Pairs and
Collocations. Ph.D. thesis, University of Stuttgart.
John Rupert Firth. 1957. Papers in Linguistics 1934-
1951. Oxford Univ. Press, Oxford.
Ray Jackendoff. 1997. The Architecture of the Lan-
guage Faculty. MIT Press, Cambridge, MA.
John S. Justeson and Slava M. Katz. 1995. Technical
terminology: Some linguistis properties and an al-
gorithm for identification in text. Natural Language
Engineering, 1:9?27.
Brigitte Krenn and Stefan Evert. 2001. Can we do
better than frequency? A case study on extracting
PP-verb collocations. In Proceedings of the ACL
Workshop on Collocations, pages 39?46, Toulouse,
France.
Dekang Lin. 1998. Extracting collocations from text
corpora. In First Workshop on Computational Ter-
minology, pages 57?63, Montreal.
Christopher Manning and Heinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. MIT Press, Cambridge, Mass.
Kathleen R. McKeown and Dragomir R. Radev. 2000.
Collocations. In Robert Dale, Hermann Moisl,
and Harold Somers, editors, A Handbook of Nat-
ural Language Processing, pages 507?523. Marcel
Dekker, New York, U.S.A.
Igor Mel?c?uk. 1998. Collocations and lexical func-
tions. In Anthony P. Cowie, editor, Phraseology.
Theory, Analysis, and Applications, pages 23?53.
Claredon Press, Oxford.
Brigitte Orliac and Mike Dillinger. 2003. Collocation
extraction for machine translation. In Proceedings
of Machine Translation Summit IX, pages 292?298,
New Orleans, Lousiana, U.S.A.
Darren Pearce. 2001. Synonymy in collocation extrac-
tion. In WordNet and Other Lexical Resources: Ap-
plications, Extensions and Customizations (NAACL
2001 Workshop), pages 41?46, Carnegie Mellon
University, Pittsburgh.
Pavel Pecina. 2005. An extensive empirical study of
collocation extraction methods. In Proceedings of
the ACL Student Research Workshop, pages 13?18,
Ann Arbor, Michigan, June. Association for Com-
putational Linguistics.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Pro-
ceedings of the Third International Conference on
Intelligent Text Processing and Computational Lin-
guistics (CICLING 2002), pages 1?15, Mexico City.
Violeta Seretan and Eric Wehrli. 2006. Multilingual
collocation extraction: Issues and solutions solu-
tions. In Proceedings or COLING/ACL Workshop
on Multilingual Language Resources and Interoper-
ability, Sydney, Australia, July. To appear.
Violeta Seretan, Luka Nerima, and Eric Wehrli. 2003.
Extraction of multi-word collocations using syn-
tactic bigram composition. In Proceedings of
the Fourth International Conference on Recent Ad-
vances in NLP (RANLP-2003), pages 424?431,
Borovets, Bulgaria.
Frank Smadja. 1993. Retrieving collocations form
text: Xtract. Computational Linguistics, 19(1):143?
177.
Eric Wehrli. 2004. Un mode`le multilingue d?analyse
syntaxique. In A. Auchlin et al, editor, Structures
et discours - Me?langes offerts a` Eddy Roulet, pages
311?329. E?ditions Nota bene, Que?bec.
960
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1008?1015,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
User Requirements Analysis for Meeting Information Retrieval  
Based on Query Elicitation 
Vincenzo Pallotta 
Department of Computer Science 
University of Fribourg 
Switzerland 
Vincenzo.Pallotta@unifr.ch 
Violeta Seretan 
Language Technology Laboratory  
University of Geneva 
Switzerland 
seretan@lettres.unige.ch 
Marita Ailomaa 
Artificial Intelligence Laboratory  
Ecole Polytechnique F?d?rale  
de Lausanne (EPFL), Switzerland 
Marita.Ailomaa@epfl.ch 
 
 
 
Abstract 
We present a user requirements study for 
Question Answering on meeting records 
that assesses the difficulty of users ques-
tions in terms of what type of knowledge is 
required in order to provide the correct an-
swer. We grounded our work on the em-
pirical analysis of elicited user queries. We 
found that the majority of elicited queries 
(around 60%) pertain to argumentative 
processes and outcomes. Our analysis also 
suggests that standard keyword-based In-
formation Retrieval can only deal success-
fully with less than 20% of the queries, and 
that it must be complemented with other 
types of metadata and inference. 
1 Introduction 
Meeting records constitute a particularly important 
and rich source of information. Meetings are a 
frequent and sustained activity, in which multi-
party dialogues take place that are goal-oriented 
and where participants perform a series of actions, 
usually aimed at reaching a common goal: they 
exchange information, raise issues, express 
opinions, make suggestions, propose solutions, 
provide arguments (pro or con), negotiate 
alternatives, and make decisions. As outcomes of 
the meeting, agreements on future action items are 
reached, tasks are assigned, conflicts are solved, 
etc. Meeting outcomes have a direct impact on the 
efficiency of organization and team performance, 
and the stored and indexed meeting records serve 
as reference for further processing (Post et al, 
2004). They can also be used in future meetings in 
order to facilitate the decision-making process by 
accessing relevant information from previous 
meetings (Cremers et al, 2005), or in order to 
make the discussion more focused (Conklin, 2006).  
Meetings constitute a substantial and important 
source of information that improves corporate or-
ganization and performance (Corrall, 1998; Ro-
mano and Nunamaker, 2001). Novel multimedia 
techniques have been dedicated to meeting record-
ing, structuring and content analysis according to 
the metadata schema, and finally, to accessing the 
analyzed content via browsing, querying or filter-
ing (Cremers et al, 2005; Tucker and Whittaker, 
2004). 
This paper focuses on debate meetings (Cugini 
et al, 1997) because of their particular richness in 
information concerning the decision-making proc-
ess. We consider that the meeting content can be 
organized on three levels: (i) factual level (what 
happens: events, timeline, actions, dynamics); (ii) 
thematic level (what is said: topics discussed and 
details); (iii) argumentative level (which/how com-
mon goals are reached).  
The information on the first two levels is ex-
plicit information that can be usually retrieved di-
rectly by searching the meeting records with ap-
propriate IR techniques (i.e., TF-IDF). The third 
level, on the contrary, contains more abstract and 
tacit information pertaining to how the explicit in-
formation contributes to the rationale of the meet-
ing, and it is not present as such in raw meeting 
data: whether or not the meeting goal was reached, 
what issues were debated, what proposals were 
made, what alternatives were discussed, what ar-
guments were brought, what decisions were made, 
what task were assigned, etc.  
The motivating scenario is the following: A user 
1008
needs information about a past meeting, either in 
quality of a participant who wants to recollect a 
discussion (since the memories of co-participants 
are often inconsistent, cf. Banerjee et al, 2005), or 
as a non-participant who missed that meeting. 
Instead of consulting the entire meeting-related 
information, which is usually heterogeneous and 
scaterred (audio-video recordings, notes, minutes, 
e-mails, handouts, etc.), the user asks natural 
language questions to a query engine which 
retrieves relevant information from the meeting 
records. 
In this paper we assess the users' interest in 
retrieving argumentative information from 
meetings and what kind of knowledge is required 
for answering users' queries. Section 2 reviews 
previous user requirements studies for the meeting 
domain. Section 3 describes our user requirements 
study based on the analysis of elicited user queries, 
presents its main findings, and discusses the 
implications of these findings for the design of 
meeting retrieval systems. Section 4 concludes the 
paper and outlines some directions for future work. 
2  Argumentative Information in Meeting 
Information Retrieval 
Depending on the meeting browser type1, different 
levels of meeting content become accessible for 
information retrieval. Audio and video browsers 
deal with factual and thematic information, while 
artifact browsers might also touch on deliberative 
information, as long as it is present, for instance, in 
the meeting minutes. In contrast, derived-data 
browsers aim to account for the argumentative in-
formation which is not explicitly present in the 
meeting content, but can be inferred from it. If 
minutes are likely to contain only the most salient 
deliberative facts, the derived-data browsers are 
much more useful, in that they offer access to the 
full meeting record, and thus to relevant details 
about the deliberative information sought. 
2 .1  Importance of Argumentative Structure  
As shown by Rosemberg and Silince (1999), track-
ing argumentative information from meeting dis-
                                                
1 (Tucker and Whittaker, 2004) identifies 4 types of meeting 
browsers: audio browsers, video browsers, artifacts browsers 
(that exploit meeting minutes or other meeting-related docu-
ments), and browsers that work with derived data (such as 
discourse and temporal structure information). 
cussions is of central importance for building pro-
ject memories since, in addition to the "strictly fac-
tual, technical information", these memories must 
also store relevant information about deci-
sion-making processes. In a business context, the 
information derived from meetings is useful for 
future business processes, as it can explain phe-
nomena and past decisions and can support future 
actions by mining and assessment (Pallotta et al, 
2004). The argumentative structure of meeting dis-
cussions, possibly visualized in form of argumen-
tation diagrams or maps, can be helpful in meeting 
browsing. To our knowledge, there are at least 
three meeting browsers that have adopted argu-
mentative structure: ARCHIVUS (Lisowska et al, 
2004b), ViCoDe (Marchand-Maillet and Bruno, 
2005), and the Twente-AMI JFerret browser 
(Rienks and Verbree, 2006).  
2 .2  Query Elicitation Studies  
The users' interest in argumentation dimension of 
meetings has been highlighted by a series of recent 
studies that attempted to elicit the potential user 
questions about meetings (Lisowska et al, 2004a; 
Benerjee at al., 2005; Cremers et al, 2005). 
The study of Lisowska et al (2004a), part of the 
IM2 research project2, was performed in a simu-
lated environment in which users were asked to 
imagine themselves in a particular role from a se-
ries of scenarios. The participants were both IM2 
members and non-IM2 members and produced 
about 300 retrospective queries on recorded meet-
ings. Although this study has been criticized by 
Post et al (2004), Cremers et al (2005), and Ban-
erjee et al (2005) for being biased, artificial, ob-
trusive, and not conforming to strong HCI method-
ologies for survey research, it shed light on poten-
tial queries and classified them in two broad cate-
gories, that seem to correspond to our argumenta-
tive/non-argumentative distinction (Lisowska et 
al., 2004a: 994): 
? ?elements related to the interaction among par-
ticipants: acceptance/rejection, agree-
ment/disagreement; proposal, argumentation 
(for and against); assertions, statements; deci-
sions; discussions, debates; reactions; ques-
tions; solutions?; 
                                                
2 http://www.im2.ch 
1009
?  ?concepts from the meeting domains: dates, 
times; documents; meeting index: current, pre-
vious, sets; participants; presentations, talks; 
projects; tasks, responsibilities; topics?.  
Unfortunately, the study does not provide precise 
information on the relative proportions of queries 
for the classification proposed, but simply suggests 
that overall more queries belong to the second 
category, while queries requiring understanding of 
the dialogue structure still comprise a sizeable 
proportion. 
The survey conducted by Banerjee et al (2005) 
concerned instead real, non-simulated interviews 
of busy professionals about actual situations, re-
lated either to meetings in which they previously 
participated, or to meetings they missed. More than 
half of the information sought by interviewees 
concerned, in both cases, the argumentative dimen-
sion of meetings. 
For non-missed meetings, 15 out of the 26 in-
stances (i.e., 57.7%) concerned argumentative as-
pects: what the decision was regarding a topic (7); 
what task someone was assigned (4); who made a 
particular decision (2); what was the participants' 
reaction to a particular topic (1); what the future 
plan is (1). The other instances (42.3%) relate to 
the thematic dimension, i.e., specifics of the dis-
cussion on a topic (11).  
As for missed meetings, the argumentative in-
stances were equally represented (18/36): decisions 
on a topic (7); what task was assigned to inter-
viewee (4); whether a particular decision was made 
(3); what decisions were made (2); reasons for a 
decision (1); reactions to a topic (1). The thematic 
questions concern topics discussed, announce-
ments made, and background of participants.  
The study also showed that the recovery of in-
formation from meeting recordings is significantly 
faster when discourse annotations are available, 
such as the distinction between discussion, presen-
tation, and briefing. 
Another unobtrusive user requirements study 
was performed by Cremers et al (2005) in a "semi-
natural setting" related to the design of a meeting 
browser. The top 5 search interests highlighted by 
the 60 survey participants were: decisions made, 
participants/speakers, topics, agenda items, and 
arguments for decision. Of these, the ones shown 
in italics are argumentative. In fact, the authors 
acknowledge the necessity to include some "func-
tional" categories as innovative search options. 
Interestingly, from the user interface evaluation 
presented in their paper, one can indirectly infer 
how salient the argumentative information is per-
ceived by users: the icons that the authors intended 
for emotions, i.e., for a emotion-based search facil-
ity, were actually interpreted by users as referring 
to people?s opinion: What is person X's opinion?  ? 
positive, negative, neutral. 
3  User Requirements Analysis 
The existing query elicitation experiments reported 
in Section 2 highlighted a series of question types 
that users typically would like to ask about meet-
ings. It also revealed that the information sought 
can be classified into two broad categories: argu-
mentative information (about the argumentative 
process and the outcome of debate meetings), and 
non-argumentative information (factual, i.e., about 
the meeting as a physical event, or thematic, i.e., 
about what has been said in terms of topics). 
The study we present in this section is aimed at 
assessing how difficult it is to answer the questions 
that users typically ask about a meeting. Our goal 
is to provide insights into:  
? how many queries can be answered using stan-
dard IR techniques on meeting artefacts only 
(e.g., minutes, written agenda, invitations); 
? how many queries can be answered with IR on 
meeting recordings; 
? what kind of additional information and infer-
ence is needed when IR does not apply or it is 
insufficient (e.g., information about the par-
ticipants and the meeting dynamics, external 
information about the meeting?s context such 
as the relation to a project, semantic interpreta-
tion of question terms and references, compu-
tation of durations, aggregation of results, etc). 
Assessing the level of difficulty of a query based 
on the two above-mentioned categories might not 
provide insightful results, because these would be 
too general, thus less interpretable. Also, the com-
plex queries requiring mixed information would 
escape observation because assigned to a too gen-
eral class. We therefore considered it necessary to 
perform a separate analysis of each query instance, 
as this provides not only detailed, but also trace-
able information. 
1010
3 .1  Data: Collecting User Queries 
Our analysis is based on a heterogeneous collec-
tion of queries for meeting data. In general, an un-
biased queries dataset is difficult to obtain, and the 
quality of a dataset can vary if the sample is made 
of too homogenous subjects (e.g., people belong-
ing to the same group as members of the same pro-
ject). In order to cope with this problem, our strat-
egy was to use three different datasets collected in 
different settings:  
? First, we considered the I M2 dataset  collected 
by Lisowska et al (2004a), the only set of user 
queries on meetings available to date. It com-
prises 270 questions (shortly described in Sec-
tion 2) annotated with a label showing whether 
or not the query was produced by an IM2-
member. These queries are introspective and 
not related to any particular recorded meeting. 
? Second, we cross-validated this dataset with a 
large corpus of 294 natural language state-
ments about existing meetings records. This 
dataset, called the B ET observations  (Wellner 
et al, 2005), was collected by subjects who 
were asked to watch several meeting record-
ings and to report what the meeting partici-
pants appeared to consider interesting. We use 
it as a ?validation? set for the IM2 queries: an 
IM2 query is considered as ?realistic? or ?em-
pirically grounded? if there is a BET observa-
tion that represents a possible answer to the 
query. For instance, the query Why was the 
proposal made by X not accepted?  matches the 
BET observation Denis eliminated Silence of 
the Lambs as it was too violent . 
? Finally, we collected a new set of ?real? queries 
by conducting a survey of user requirements 
on meeting querying in a natural business set-
ting. The survey involved 3 top managers from 
a company and produced 35 queries. We called 
this dataset Manager Survey Set  (MS-Set). 
The queries from the IM2-set (270 queries) and the 
MS-Set (35 queries) were analyzed by two differ-
ent teams of two judges. Each team discussed each 
query, and classified it along the two main dimen-
sions we are interested in: 
? query type : the type of meeting content to 
which the query pertains; 
? query difficulty : the type of information re-
quired to provide the answer. 
3 .2  Query Type Analysis 
Each query was assigned exactly one of the follow-
ing four possible categories (the one perceived as 
the most salient): 
1. factual: the query pertains to the factual meet-
ing content; 
2. thematic: the query pertains to the thematic 
meeting content; 
3.  process : the query pertains to the argumenta-
tive meeting content, more precisely to the ar-
gumentative process; 
4. outcome: the query pertains to the argumenta-
tive meeting content, more precisely to the 
outcome of the argumentative process. 
IM 2- s et 
(s iz e:2 70) 
MS-S et  
(s iz e: 3 5) Cate go ry 
Tea m 1  Tea m 2  Tea m 1  Tea m 2  
Fac tu al 24. 8 %  20. 0 %  20. 0 %  
The m atic 18. 5 %  45. 6 %  20. 0 %  11. 4 %  
Proc es s 30. 0 %  32. 6 %  22. 9 %  28. 6 %  
Outc o me 26. 7 %  21. 8 %  37. 1 %  40. 0 %  
Proc es s + O utc o me 56. 7 %  54. 4 %  60. 0 %  68. 6 %  
Table 1. Query classification according to the 
meeting content type. 
Results from this classification task for both query 
sets are reported in Table 1. In both sets, the 
information most sought was argumentative: about 
55% of the IM2-set queries are argumentative 
(process or outcome). This invalidates the initial 
estimation of Lisowska et al (2004a:994) that the 
non-argumentative queries prevail, and confirms 
the figures obtained in (Banerjee et al, 2005), ac-
cording to which 57.7% of the queries are argu-
mentative. In our real managers survey, we ob-
tained even higher percentages for the argumenta-
tive queries (60% or 68.6%, depending on the an-
notation team). The argumentative queries are fol-
lowed by factual and thematic ones in both query 
sets, with a slight advantage for factual queries. 
The inter-annotator agreement for this first clas-
sification is reported in Table 2. The proportion of 
queries on which annotators agree in classifying 
them as argumentative is significantly high. We 
only report here the agreement results for the indi-
vidual argumentative categories (Process, Out-
come) and both (Process & Outcome). There were 
213 queries (in IM2-set) and 30 queries (in MS-
1011
set) that were consistently annotated by the two 
teams on both categories. Within this set, a high 
percentage of queries were argumentative, that is, 
they were annotated as either Process or Outcome 
(label AA in the table). 
IM 2- s et (s iz e: 27 0) MS-s e t (s iz e: 3 5)  C ate go ry rati o k app a  rati o k app a  
Proc es s 84. 8 %  82. 9 %  88. 6 %  87. 8 %  
Outc o me 90. 7 %  89. 6 %  91. 4 %  90. 9 %  
Proc es s & 
Outc o me 78. 9 %  76. 2 %  85. 7 %  84. 8 %  
AA 11 7/2 13 =  54. 9 %   
19/ 30 =  
63. 3 %   
Table 2. Inter-annotator agreement for query-type 
classification. 
Furthermore, we provided a re-assessment of the 
proportion of argumentative queries with respect to 
query origin for the IM2-set (IM2 members vs. 
non-IM2 members): non-IM2 members issued 
30.8% of agreed argumentative queries, a propor-
tion that, while smaller compared to that of IM2 
members (69.2%), is still non-negligible. This con-
trasts with the opinion expressed in (Lisowska et 
al., 2004a) that argumentative queries are almost 
exclusively produced by IM2 members.  
Among the 90 agreed IM2 queries that were 
cross-validated with the BET-observation set, 
28.9% were argumentative. We also noted that the 
ratio of BET statements that contain argumentative 
information is quite high (66.9%). 
3 .3  Query Difficulty Analysis 
In order to assess the difficulty in answering a 
query, we used the following categories that the 
annotators could assign to each query, according to 
the type of information and techniques they judged 
necessary for answering it: 
1. Role of I R : states the role of standard3  Informa-
tion Retrieval (in combination with Topic Ex-
traction4) techniques in answering the query. 
Possible values:  
a. Irrelevant (IR techniques are not appli-
cable).  Example: What decisions have 
been made?  
                                                
3  By standard IR we mean techniques based on bag-of-word 
search and TF-IDF indexing. 
4 Topic extraction techniques are based on topic shift detec-
tion (Galley et al, 2003) and keyword extraction (van der Plas 
et al, 2004). 
b. successful (IR techniques are sufficient). 
Example: Was the budget approved?  
c. insufficient (IR techniques are necessary, 
but not sufficient alone since they re-
quire additional inference and informa-
tion, such as argumentative, cross-
meeting, external corporate/project 
knowledge). Example: Who rejected the 
proposal made by X on issue Y?  
2. Artefacts : information such as agenda, min-
utes of previous meetings, e-mails, invita-
tions and other documents related and avail-
able before the meeting. Example: Who was 
invited to the meeting?  
3.  Recordings : the meeting recordings (audio, 
visual, transcription). This is almost always 
true, except for queries where Artefacts or 
Metadata are sufficient, such as What was 
the agenda?,  Who was invited to the meet-
ing? ). 
4 .  Metadata : context knowledge kept in static 
metadata (e.g., speakers, place, time). Ex-
ample: Who were the participants at the 
meeting? 
5. Dialogue Acts & Adjacency Pairs : Example: 
What was John?s response to my comment 
on the last meeting?  
6. Argumentation : metadata (annotations) 
about the argumentative structure of the 
meeting content. Example: Did  everybody 
agree on the decisions, or were there differ-
ences of opinion?  
7.  Semantics : semantic interpretation of terms 
in the query and reference resolution, in-
cluding deictics (e.g., for how long, usually, 
systematically, criticisms; this, about me, I ). 
Example: What decisions got made easily ?  
The term requiring semantic interpretation is 
underlined.  
8. Inference : inference (deriving information 
that is implicit), calculation, and aggregation 
(e.g., for ?command? queries asking for lists 
of things ? participants, issues, proposals). 
Example: What would be required from me?  
1012
9. Multiple meetings : availability of multiple 
meeting records. Example: Who usually at-
tends the project meetings?  
10. External : related knowledge, not explicitly 
present in the meeting records (e.g., infor-
mation about the corporation or the projects 
related to the meeting). Example: Did some-
body talk about me or about my work?  
Results of annotation reported on the two query 
sets are synthesized in Table 3: IR is sufficient for 
answering 14.4% of the IM2 queries, and 20% of 
the MS-set queries. In 50% and 25.7% of the cases, 
respectively, it simply cannot be applied (irrele-
vant). Finally, IR alone is not enough in 35.6% of 
the queries from the IM2-set, and in 54.3% of the 
MS-set; it has to be complemented with other 
techniques.  
IM 2- s et MS-s e t 
IR is : all  
qu eri es AA 
all  
qu eri es AA 
Suff ic ie nt 39/ 27 0 =  14. 4 %  
1/1 17 =  
0.8 %  
7/3 5 =  
20. 0 %  
1/1 9 =  
5.3 %  
Irrel ev a nt 13 5/2 70 =  50. 0 %   
55/ 11 7 =  
47. 0 %  
9/3 5 =  
25. 7 %  
3/1 9 =  
15. 8 %  
Ins uf fic i ent  96/ 27 0 =  35. 6 %  
61/ 11 7 =  
52. 1 %  
19/ 35 =  
54. 3 %  
15/ 19 =  
78. 9 %  
Table 3.  The role of IR (and topic extraction) in 
answering users? queries. 
If we consider agreed argumentative queries 
(Section 3.2), IR is effective in an extremely low 
percentage of cases (0.8% for IM2-set and 5.3% 
for MS-Set). IR is insufficient in most of the cases 
(52.1% and 78.9%) and inapplicable in the rest of 
the cases (47% and 15.8%). Only one argumenta-
tive query from each set was judged as being an-
swerable with IR alone: What were the decisions to 
be made (open questions) regarding the topic t1? 
When is the NEX T M E E TIN G planned? (e.g. to 
follow up on action items) . 
Table 4 shows the number of queries in each set 
that require argumentative information in order to 
be answered, distributed according to the query 
types. As expected, no argumentation information 
is necessary for answering factual queries, but 
some thematic queries do need it, such as What 
was decided about topic T?  (24% in the IM2-set 
and 42.9% in the M.S.-set).  
Overall, the majority of queries in both sets re-
quire argumentation information in order to be an-
swered (56.3% from IM2 queries, and 65.7% from 
MS queries). 
IM 2- s et, An no ta tio n 1  MS-s e t, A nn ot ati on 1 
Cate go ry  tot al  Req.  arg. Rati o Tot al  
Req.  
arg. Rati o 
Fac tu al 67  0  0%  7  0  0%  
The m atic  50  12  24. 0 %  7  3  42. 9 %  
Proc es s 81  73  90. 1 %  8  7  87. 5 %  
Outc o me  72  67  93. 1 %  13  13  10 0%  
All 27 0  15 2  56. 3 %  35  23  65. 7 %  
Table 4. Queries requiring argumentative informa-
tion. 
We finally looked at what kind of information is 
needed in those cases where IR is perceived as in-
sufficient or irrelevant. Table 5 lists the most fre-
quent combinations of information types required 
for the IM2-set and the MS-set. 
3 .4  Summary of Findings 
The analysis of the annotations obtained for the 
305 queries (35 from the Manager Survey set, and 
270 from the IM2-set) revealed that: 
? The information most sought by users from 
meetings is argumentative (i.e., pertains to the 
argumentative process and its outcome). It 
constitutes more than half of the total queries, 
while factual and thematic information are 
similar in proportions (Table 1); 
? There was no significant difference in this re-
spect between the IM2-set and the MS-set 
(Table 1); 
? The decision as to whether a query is argumen-
tative or not is easy to draw, as suggested by 
the high inter-annotator agreement shown in 
Table 2; 
? Standard IR and topic extraction techniques 
are perceived as insufficient in answering most 
of the queries. Only less than 20% of the 
whole query set can be answered with IR, and 
almost no argumentative question (Table 3). 
? Argumentative information is needed in an-
swering the majority of the queries (Table 4); 
? When IR alone fails, the information types that 
are needed most are (in addition to recordings): 
Argumentation, Semantics, Inference, and 
Metadata (Table 5); see Section 3.3 for their 
description. 
 
1013
 IR a lo ne fa ils IM 2-s et 
Inf orm at io n ty p es IR i ns uff ic ie nt             96 c as es   3 5.6 % IR irr el ev an t         13 5 c as es    50 %  
Artef ac ts         x     
Rec ord in gs x x x x x x x x x x x   
Me ta- da ta   x  x   x  x  x x 
Dlg ac ts & A dj . p airs              
Argu m en tat io n x x x x x x x x x  x   
Se ma ntic s x x x x x   x x x x x  
Inf ere nc e x  x x   x x x x x x  
Mu lti pl e me et in gs    x        x  
Ex tern al              
                  Cas es 15 11 9 8 7 5 4 14 9 8 8 7 5 
                  Ra tio (% )  15. 6  11. 5  9.4  8.3  7.3  5.2  4.2  10. 4  6.7  5.9  5.9  5.2  3.7  
 
IR a lo ne fa ils MS-s e t 
Inf orm at io n ty p es IR i ns uff ic ie nt     19 c as es   5 4.3 %  IR irr el ev an t   9 c as es   54. 3 %  
Artef ac ts     x x 
Rec ord in gs x x x x   
Me ta- da ta     x x 
Dlg ac ts & A dj . p airs       
Argu m en tat io n x x x x   
Se ma ntic s x  x x x  
Inf ere nc e x x  x x  
Mu lti pl e me et in gs       
Ex tern al    x   
                  Cas es 6 4 2 2 2 2 
                  Ra tio (% )  31. 6 21 10. 5 10. 5 22. 2 22. 2 
Table 5. Some of the most frequent combinations of information required for answering the queries in the 
IM2-Set and in the MS-set when IR alone fails. 
3 .5  Discussion 
Searching relevant information through the re-
corded meeting dialogues poses important prob-
lems when using standard IR indexing techniques 
(Baeza-Yates and Ribeiro-Nieto, 2000), because 
users ask different types of queries for which a 
single retrieval strategy (e.g., keywords-based) is 
insufficient. This is the case when looking at an-
swers that require some sort of entailment, such as 
inferring that a proposal has been rejected when a 
meeting participant says Are you kidding? .  
Spoken-language information retrieval (Vinci-
arelli, 2004) and automatic dialogue-act extraction 
techniques (Stolke et al, 2000; Clark and Popescu-
Belis, 2004; Ang et al, 2005) have been applied to 
meeting recordings and produced good results un-
der the assumption that the user is interested in 
retrieving either topic-based or dialog act-based 
information. But this assumption is partially in-
validated by our user query elicitation analysis, 
which showed that such information is only sought 
in a relatively small fraction of the users? queries. 
A particular problem for these approaches is that 
the topic looked for is usually not a query itself 
( Was topic T mentioned?) , but just a parameter in 
more structured questions ( What was decided 
about T? ). Moreover, the relevant participants? 
contributions (dialog acts) need to be retrieved in 
combination, not in isolation (The reactions  to the 
proposal made by X ). 
4  Conclusion and Future Work 
While most of the research community has ne-
glected the importance of argumentative queries in 
meeting information retrieval, we provided evi-
dence that this type of queries is actually very 
common. We quantified the proportion of queries 
involving the argumentative dimension of the 
meeting content by performing an in-depth analy-
sis of queries collected in two different elicitation 
surveys. The analysis of the annotations obtained 
for the 305 queries (270 from the IM2-set, 35 from 
MS-set) was aimed at providing insights into dif-
ferent matters: what type of information is typi-
cally sought by users from meetings; how difficult 
it is, and what kind of information and techniques 
are needed in order to answer user queries.  
This work represents an initial step towards a 
better understanding of user queries on the meeting 
domain. It could provide useful intuitions about 
1014
how to perform the automatic classification of an-
swer types and, more importantly, the automatic 
extraction of argumentative features and their rela-
tions with other components of the query (e.g., 
topic, named entities, events). 
In the future, we intend to better ground our first 
empirical findings by i) running the queries against 
a real IR system with indexed meeting transcripts 
and evaluate the quality of the obtained answers; 
ii) ask judges to manually rank the difficulty of 
each query, and iii) compare the two rankings. We 
would also like to see how frequent argumentative 
queries are in other domains (such as TV talk 
shows or political debates) in order to generalize 
our results. 
Acknowledgements 
We wish to thank Martin Rajman and Hatem 
Ghorbel for their constant and valuable feedback. 
This work has been partially supported by the 
Swiss National Science Foundation NCCR IM2 
and by the SNSF grant no. 200021-116235. 
References 
Jeremy Ang, Yang Liu and Elizabeth Shriberg. 2005. 
Automatic Dialog Act Segmentation and Classification in 
Multiparty Meetings. In Proceedings of IE E E IC A S S P 
2 0 0 5 , Philadelphia, PA, USA. 
Ricardo Baeza-Yates and Berthier Ribeiro-Nieto. 2000. 
Modern Information Retrieval . Addison Wesley. 
Satanjeev Banerjee, Carolyn Rose and Alexander I. Rudnicky. 
2005. The Necessity of a Meeting Recording and Playback 
System, and the Benefit of Topic-Level Annotations to 
Meeting Browsing. In Proceedings of INT E R A C T 2 0 0 5 , 
Rome, Italy. 
Alexander Clark and Andrei Popescu-Belis. 2004. Multi-level 
Dialogue Act Tags. In Proceedings of SIG D IA L ' 0 4 , pages 
163?170. Cambridge, MA, USA. 
Jeff Conklin. 2006. Dialogue Mapping: Building Shared 
Understanding of Wicked Problems . John Wiley & Sons. 
Sheila Corrall. 1998. Knowledge management. Are we in the 
knowledge management business? A R IA D N E : the Web 
version,  18. 
Anita H.M Cremers, Bart Hilhorst and Arnold P.O.S 
Vermeeren. 2005. ?What was discussed by whom, how, 
when and where?" personalized browsing of annotated 
multimedia meeting recordings. In Proceedings of HC I 
2 0 0 5 , pages 1?10, Edinburgh, UK.  
John Cugini, Laurie Damianos, Lynette Hirschman, Robyn 
Kozierok, Jeff Kurtz, Sharon Laskowski and Jean Scholtz. 
1997. Methodology for evaluation of collaborative 
systems. Technical Report Rev. 3.0, The Evaluation 
Working Group of the DARPA Intelligent Collaboration 
and Visualization Program. 
Michel Galley, Kathleen McKeown, Eric Fosler-Lussier and 
Hongyan Jing. 2003. Discourse Segmentation of Multi-
Party Conversation. In Proceedings of AC L 2 0 0 3 , pages 
562?569, Sapporo, Japan. 
Agnes Lisowska, Andrei Popescu-Belis and Susan Armstrong. 
2004a. User Query Analysis for the Specification and 
Evaluation of a Dialogue Processing and Retrieval System. 
In Proceedings LR E C 2 0 0 4 , pages 993?996, Lisbon, 
Portugal. 
Agnes Lisowska, Martin Rajman and Trung H. Bui. 2004b. 
ARCHIVUS: A System for Accesssing the Content of 
Recorded Multimodal Meetings. In Proceedings of ML M I 
2 0 0 4 , Martigny, Switzerland. 
St?phane Marchand-Maillet and Eric Bruno. 2005. Collection 
Guiding: A new framework for handling large multimedia 
collections. In Proceeding of AV IV DiLib05 , Cortona, Italy. 
Vincenzo Pallotta, Hatem Ghorbel, Afzal Ballim, Agnes 
Lisowska and St?phane Marchand-Maillet. 2004. Towards 
meeting information systems: Meeting knowledge 
management. In Proceedings of ICE IS 2 0 0 5 , pages 464?
469, Porto, Portugal. 
Lonneke van der Plaas, Vincenzo Pallotta, Martin Rajman and 
Hatem Ghorbel. 2004. Automatic keyword extraction from 
spoken text: A comparison between two lexical resources: 
the EDR and WordNet. In Proceedings of the LR E C 2 0 0 4 , 
pages 2205?2208, Lisbon, Portugal. 
Wilfried M. Post, Anita H.M. Cremers and Olivier Blanson 
Henkemans. 2004. A Research Environment for Meeting 
Behavior. In Proceedings of the 3rd Workshop on Social 
Intelligence Design,  pages 159?165, University of Twente, 
Enschede, The Netherlands. 
Rutger Rienks and Daan Verbree. 2006. About the Usefulness 
and Learnability of Argument?Diagrams from Real 
Discussions. In Proceedings of ML MI 2 0 0 6 , Washington 
DC, USA. 
Nicholas C. Romano Jr. and Jay F. Nunamaker Jr. 2001. 
Meeting Analysis: Findings from Research and Practice. In 
Proceedings of HIC S S-3 4 , Maui, HI, IEEE Computer 
Society. 
Duska Rosemberg and John A.A. Silince. 1999. Common 
ground in computer-supported collaborative argumentation. 
In Proceedings of the CL S C L 9 9 , Stanford, CA, USA. 
Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth 
Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor, 
Rachel Martin, Carol Van Ess-Dykema and Marie Meteer. 
2000. Dialog Act Modeling for Automatic Tagging and 
Recognition of Conversational Speech. Computational 
Linguistics,  26(3):339?373.  
Simon Tucker and Steve Whittaker. 2004. Accessing 
multimodal meeting data: systems, problems and 
possibilities. In Proceedings of ML M I 2 0 0 4 , Martigny, 
Switzerland. 
Alessandro Vinciarelli. 2004. Noisy text categorization. In 
Proceedings of ICP R 2 0 0 4 , Cambridge, UK. 
Pierre Wellner, Mike Flynn, Simon Tucker, Steve Whittaker. 
2005. A Meeting Browser Evaluation Test. In Proceedings 
of CHI 2 0 0 5 , Portand, Oregon, USA. 
1015
Proceedings of the EACL 2009 Demonstrations Session, pages 45?48,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
A Tool for Multi-Word Expression Extraction in Modern Greek
Using Syntactic Parsing
Athina Michou
University of Geneva
Geneva, Switzerland
Athina.Michou@unige.ch
Violeta Seretan
University of Geneva
Geneva, Switzerland
Violeta.Seretan@unige.ch
Abstract
This paper presents a tool for extrac-
ting multi-word expressions from cor-
pora in Modern Greek, which is used to-
gether with a parallel concordancer to aug-
ment the lexicon of a rule-based machine-
translation system. The tool is part of a
larger extraction system that relies, in turn,
on a multilingual parser developed over
the past decade in our laboratory. The
paper reviews the various NLP modules
and resources which enable the retrieval
of Greek multi-word expressions and their
translations: the Greek parser, its lexical
database, the extraction and concordanc-
ing system.
1 Introduction
In today?s multilingual society, there is a pressing
need for building translation resources, such as
large-coverage multilingual lexicons, translation
systems or translation aid tools, especially due to
the increasing interest in computer-assisted trans-
lation.
This paper presents a tool intended to as-
sist translators/lexicographers dealing with Greek1
as a source or a target language. The tool
deals specifically with multi-lexeme lexical items,
also called multi-word expressions (henceforth
MWEs). Its main functionalities are: 1) the robust
parsing of Greek text corpora and the syntax-based
detection of word combinations that are likely to
constitute MWEs, and 2) concordance and align-
ment functions supporting the manual creation of
monolingual and bilingual MWE lexicons.
The tool relies on a symbolic parsing technol-
ogy, and is part of FipsCo, a larger extraction sys-
tem (Seretan, 2008) which has previously been
1For the sake of simplicity, we will henceforth use the
term Greek to refer to Modern Greek.
used to build MWE resources for other languages,
including English, French, Spanish, and Italian.
Its extension to Greek will ultimately enable the
inclusion of this language in the list of languages
supported by an in-house translation system.
The paper is structured as follows. Section 2 in-
troduces the Greek parser and its lexical database.
Section 3 provides a description of Greek MWEs,
including a syntactic classification for these. Sec-
tion 4 presents the extraction tool, and Section 5
concludes the paper.
2 The Greek parser
The Greek parser is part of Fips, a multilin-
gual symbolic parser that deals, among other lan-
guages, with English, French, Spanish, Italian,
and German (Wehrli, 2007). The Greek version,
FipsGreek (Michou, 2007), has recently reached
an acceptable level of lexical and grammatical
coverage.
Fips relies on generative grammar concepts, and
is basically made up of a generic parsing module
which can be refined in order to suit the specific
needs of a particular language. Currently, there
are approximately 60 grammar rules defined for
Greek, allowing for the complete parse of about
50% of the sentences in a corpus like Europarl
(Koehn, 2005), which contains proceedings of
the European Parliament. For the remaining sen-
tences, partial analyses are instead proposed for
the chunks identified.
One of the key components of the parser is its
(manually-built) lexicon. It contains detailed mor-
phosyntactic and semantic information, namely,
selectional properties, subcategorization informa-
tion, and syntactico-semantic features that are
likely to influence the syntactic analysis.
The Greek monolingual lexicon presently con-
tains about 110000 words corresponding to 16000
45
lexemes,2 and a limited number of MWEs (about
500). The bilingual lexicon used by our trans-
lation system contains slightly more than 8000
Greek-French/French-Greek equivalents.
3 MWEs in Modern Greek
Greek is a language which exhibits a high MWE
productivity, with new compound words being
created especially in the science and technology
domains. Sometimes, existing words are trans-
formed in order to denote new concepts; also, nu-
merous neologisms are created or borrowed from
other languages.
A frequent type of multi-word constructions
in Greek are special noun phrases, called lexical
phrases (Anastasiadi-Symeonidi, 1986) or loose
multi-word compounds (Ralli, 2005):
- Adjective+Noun: anoiqt? ??lassa
(anichti thalassa, ?open sea?),
paidik? qar? (pediki chara, ?kinder-
garten?);
- Noun+NounGEN : z?nh asfale?ac (zo-
ni asfalias, ?safety belt?), f?roc
eisod?matoc (foros isodimatos,
?income tax?);
- Noun+NounNOM (head-complement re-
lation): paid?-?a?ma (pedi-thavma,
?child prodigy?), suz?thsh-mara??nioc
(syzitisi-marathonios, ?marathon
talks?) ;
- NounNOM+NounNOM (coordina-
tion relation): kanap?c-kreb?ti
(kanapes-krevati, ?sofa bed?), gia-
tr?c-nosok?moc (yiatros-nosokomos,
?doctor-nurse?).
A large body of Greek MWEs constitute collo-
cations (typical word associations whose meaning
is easy to decode, but whose component items are
difficult to predict), such as katarr?ptw ?na rek?r
(kataripto ena rekor, ?to break a record?),
in which the verbal collocate katarr?ptw (?shake
down?) is unpredictable. Collocations may occur
in a wide range of syntactic types. Some of the
configurations taken into account in our work are:
- Noun(Subject)+Verb: h suz?thsh l?gei (i
sizitisi liyi, ?discussion ends?);
2Most of the inflected forms were automatically obtained
through morphological generation; that is, the base word was
combined with the appropriate suffixes, according to a given
inflectional paradigm. A number of 25 inflection classes have
been defined for Greek nouns, 11 for verbs, and 10 for adjec-
tives.
- Adjective+Noun: janatik? poin?
(thanatiki pini, ?death penalty?);
- Verb+Noun(Object): diatr?qw k?nduno
(diatrecho kindino, ?run a risk?);
- Verb+Preposition+Noun(Argument):
katadik?zw se ??nato (katadikazo
se thanato, ?to sentence to death?);
- Verb+Preposition: prosanatol?zomai proc
(prosanatolizome pros, ?to orient
to?);
- Noun+Preposition+Noun: protrop? gia
an?ptuxh (protropi yia anaptiksi,
?incitement to development?);
- Preposition+Noun: up? suz?thsh (ipo
sizitisi, ?under discussion?);
- Verb+Adverb: qeirokrot? jerm?
(xirokroto therma, ?applause
warmly?);
- Adverb+Adjective: genetik? tropopoih-
m?noc (yenetika tropopiimenos,
?genetically modified?);
- Adjective+Preposition: exarthm?noc ap?
(eksartimenos apo, ?dependent on?).
In addition, Greek MWEs cover other types of
constructions, such as:
- one-word compounds: erujr?dermoc
(erithrodermos, ?red skin?), luk?skulo
(likoskylo, ?wolfhound?);
- adverbial phrases: ek twn prot?rwn (ek
ton proteron, ?a priori, in principle?);
- idiomatic expressions (whose meaning
is difficult to decode): g?nomai qal? na
me pat?seic (yinome xali na me
patisis, literally, become a carpet to walk
all over; ?be ready to satisfy any wish?).
4 The MWE Extraction Tool
MWEs constitute a high proportion of the lexicon
of a language, and are crucial for many NLP tasks
(Sag et al, 2002). This section introduces the tool
we developed for augmenting the coverage of our
monolingual/bilingual MWE lexicons.
4.1 Extraction
As we already mentioned, the Greek MWE extrac-
tor is part of FipsCo, a larger extraction system
based on a symbolic parsing technology (Seretan,
2008) which we previously applied on text corpora
in other languages. The recent development of the
Greek parser enabled us to extend it and apply it
to Greek.
46
Figure 1: Screen capture of the parallel concordancer, showing an instance of the collocation epitugq?nw
isorrop?a (?strike balance?) and the aligned context in the target language, English.
The extractor is designed as a module which is
plugged into the parser. After a sentence from the
source corpus is parsed, the extractor traverses the
output structure and identifies as a potential MWE
the words found in one of the syntactic configura-
tions listed in Section 3.
Once all MWE candidates are collected from
the corpus, they are divided into subsets according
to their syntactic configuration. Then, each subset
undergo a statistical analysis process whose aim
is to detect those candidates that are highly cohe-
sive. A strong association between the items of
a candidate indicates that this is likely to consti-
tute a collocation. The strength of association can
be measured with one of the numerous associa-
tion measures implemented in our extractor. By
default, the log-likelihood ratio measure (LLR) is
proposed, since it was shown to be particularly
suited to language data (Dunning, 1993).
In our extractor, the items of each candidate ex-
pression represent base word forms (lemmas) and
they are considered in the canonical order implied
by the given syntactic configuration (e.g., for a
verb-object candidate, the object is postverbal in
SVO languages like Greek). Even if the candidate
occurs in corpus in a different morphosyntactic re-
alizations, its various occurrences are successfully
identified as instances of the same type thanks to
the syntactic analysis performed with the parser.
4.2 Visualization
The extraction tool also provides visualization
functions which facilitate the consultation and
interpretation of results by users?e.g., lexi-
cographers, terminologists, translators, language
learners?by displaying them in the original con-
text. The following functions are provided:
Filtering and sorting The results which will
be displayed can be selected according to seve-
47
ral criteria: the syntactic configuration (i.e., users
can select only one or several configurations they
are interested in), the LLR score, the corpus fre-
quency (users can specify the limits of the de-
sired interval),3 the words involved (users can look
up MWEs containing specific words). Also, the
selected results can be ordered by score or fre-
quency, and users can filter them according to the
rank obtained.
Concordance The (filtered) results are dis-
played on a concordancing interface, similar to the
one shown in Figure 1. The list on the left shows
the MWE candidates that were extracted. When
an item of the list is selected, the text panel on
the right displays the context of its first instance
in the source document. The arrow buttons be-
neath allow users to navigate through all the in-
stances of that candidate. The whole content of
the source document is accessible, and it is auto-
matically scrolled to the current instance; the com-
ponent words and the sentence in which they occur
are highlighted in different colors.
Alignment If parallel corpora are available, the
results can be displayed in a sentence-aligned con-
text. That is, the equivalent of the source sen-
tence in the target document containing the trans-
lation of the source document is also automatically
found, highlighted and displayed next to the origi-
nal context (see Figure 1). Thus, users can see how
a MWE has previously been translated in a given
context.
Validation The tool also provides functiona-
lities allowing users to create a database of manu-
ally validated MWEs from among the candidates
displayed on the (parallel) concordancing inter-
faces. The database can store either monolin-
gual or bilingual entries; most of the informa-
tion associated to an entry?such as lexeme in-
dexes, syntactic type, source sentence?is auto-
matically filled-in by the system. For bilingual en-
tries, a translation must be provided by the user,
and this can be easily retrieved manually from
the target sentence showed in the parallel concor-
dancer (thus, for the collocation shown in Figure
1, the user can find the English equivalent strike
balance).
3Thus, users can specify themselves a threshold (in other
systems it is arbitrarily predefined).
5 Conclusion
We presented a MWE extractor with advanced
concordancing functions, which can be used
to semi-automatically build Greek monolin-
gual/bilingual MWE lexicons. It relies on a
deep syntactic approach, whose benefits are mani-
fold: retrieval of grammatical results, interpre-
tation of syntactic constituents in terms of ar-
guments, disambiguation of lexemes with multi-
ple readings, and grouping of all morphosyntactic
variants of MWEs.
Our system is most similar to Termight (Dagan
and Church, 1994) and TransSearch (Macklovitch
et al, 2000). To our knowledge, it is the first of
this type for Greek.
Acknowledgements
This work has been supported by the Swiss Na-
tional Science Foundation (grant 100012-117944).
The authors would like to thank Eric Wehrli for his
support and useful comments.
References
Anna Anastasiadi-Symeonidi. 1986. The neology in the
Common Modern Greek. Triandafyllidi?s foundation,
Thessaloniki. In Greek.
Ido Dagan and Kenneth Church. 1994. Termight: Identifying
and translating technical terminology. In Proceedings of
ANLP, pages 34?40, Stuttgart, Germany.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguistics,
19(1):61?74.
Philipp Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In Proceedings of MT Summit
X, pages 79?86, Phuket, Thailand.
Elliott Macklovitch, Michel Simard, and Philippe Langlais.
2000. TransSearch: A free translation memory on the
World Wide Web. In Proceedings of LREC 2000, pages
1201?1208, Athens, Greece.
Athina Michou. 2007. Analyse syntaxique et traitement au-
tomatique du syntagme nominal grec moderne. In Pro-
ceedings of TALN 2007, pages 203?212, Toulouse, France.
Angela Ralli. 2005. Morphology. Patakis, Athens. In Greek.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expressions:
A pain in the neck for NLP. In Proceedings of CICLING
2002, pages 1?15, Mexico City.
Violeta Seretan. 2008. Collocation extraction based on syn-
tactic parsing. Ph.D. thesis, University of Geneva.
Eric Wehrli. 2007. Fips, a ?deep? linguistic multilingual
parser. In Proceedings of ACL 2007 Workshop on Deep
Linguistic Processing, pages 120?127, Prague, Czech Re-
public.
48
Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 28?36,
Beijing, August 2010
Sentence Analysis and Collocation Identification
Eric Wehrli, Violeta Seretan, Luka Nerima
Language Technology Laboratory
University of Geneva
{Eric.Wehrli, Violeta.Seretan, Luka.Nerima}@unige.ch
Abstract
Identifying collocations in a sentence, in
order to ensure their proper processing in
subsequent applications, and performing
the syntactic analysis of the sentence are
interrelated processes. Syntactic informa-
tion is crucial for detecting collocations,
and vice versa, collocational information
is useful for parsing. This article describes
an original approach in which collocations
are identified in a sentence as soon as pos-
sible during the analysis of that sentence,
rather than at the end of the analysis, as in
our previous work. In this way, priority is
given to parsing alternatives involving col-
locations, and collocational information
guide the parser through the maze of alter-
natives. This solution was shown to lead
to substantial improvements in the perfor-
mance of both tasks (collocation identifi-
cation and parsing), and in that of a sub-
sequent task (machine translation).
1 Introduction
Collocations1 constitute a central language phe-
nomenon and an impressive amount of work has
been devoted over the past decades to the automa-
tic acquisition of collocational resources ? as at-
tested, among others, by initiatives like the MWE
2008 shared task aimed at creating a repository of
reference data (Gre?goire et al, 2008). However,
little or no reference exist in the literature about
1We adopt the lexicographic understanding for the term
collocation (Benson et al, 1986), as opposed to the British
contextualist tradition focused on statistical co-occurrence
(Firth, 1957; Sinclair, 1991).
the actual use made of these resources in other
NLP applications.
In this paper, we consider the particular appli-
cation of syntactic parsing. Just as other types of
multi-word expressions (henceforth, MWEs), col-
locations are problematic for parsing because they
have to be recognised and treated as a whole, ra-
ther than compositionally, i.e., in a word by word
fashion (Sag et al, 2002). The standard approach
in dealing with MWEs in parsing is to apply
a ?words-with-spaces? preprocessing step, which
marks the MWEs in the input sentence as units
which will later be integrated as single blocks in
the parse tree built during analysis.
We argue that such an approach, albeit suffi-
ciently appropriate for some subtypes of MWEs2,
is not really adequate for processing colloca-
tions. Unlike other expressions that are fixed or
semi-fixed3, collocations do not allow a ?words-
with-spaces? treatment because they have a high
morpho-syntactic flexibility.
There is no systematic restriction, for instance,
on the number of forms a lexical item (such as a
verb) may have in a collocation, on the order of
items in a collocation, or on the number of words
that may intervene between these items. Collo-
cations are situated at the intersection of lexicon
and grammar; therefore, they cannot be accounted
for merely by the lexical component of a parsing
system, but have to be integrated to the grammati-
cal component as well, as the parser has to consi-
2Sag et al (2002) thoroughly discusses the extend to
which a ?words-with-spaces? approach is appropriate for dif-
ferent kinds of MWEs.
3For instance, compound words: by and large, ad hoc;
named entities: New York City; and non-decomposable
idioms: shoot the breeze.
28
der all the possible syntactic realisations of collo-
cations.
Alternatively, a post-processing approach (such
as the one we pursued previously in Wehrli et
al. (2009b)) would identify collocations after the
syntactic analysis has been performed, and out-
put a parse tree in which collocational relations
are highlighted between the composing items, in
order to inform the subsequent processing appli-
cations (e.g., a machine translation application).
Again, this solution is not fully appropriate, and
the reason lies with the important observation that
prior collocational knowledge is highly relevant
for parsing. Collocational restrictions are, along
with other types of information like selectional
preferences and subcategorization frames, a major
means of structural disambiguation. Collocational
relations between the words in a sentence proved
very helpful in selecting the most plausible among
all the possible parse trees for a sentence (Hindle
and Rooth, 1993; Alshawi and Carter, 1994; Ber-
thouzoz and Merlo, 1997; Wehrli, 2000). Hence,
the question whether collocations should be iden-
tified in a sentence before or after parsing is not an
easy one. The previous literature on parsing and
collocations fails to provide insightful details on
how this circular issue is (or can be) solved.
In this paper, we argue that the identification of
collocations and the construction of a parse tree
are interrelated processes, that must be accounted
for simultaneously. We present a processing mo-
del in which collocations, if present in a lexicon,
are identified in the input sentence during the ana-
lysis of that sentence. At the same time, they are
used to rank competing parsing hypotheses.
The paper is organised as follows. Section 2
reviews the previous work on the interrelation
between parsing and processing of collocations
(or, more generally, MWEs). Section 3 introduces
our approach, and section 4 evaluates it by compa-
ring it against the standard non-simultaneous ap-
proach. Section 5 provides concluding remarks
and presents directions for future work.
2 Related Work
Extending the lexical component of a parser with
MWEs was proved to contribute to a significant
improvement of the coverage and accuracy of par-
sing results. For instance, Brun (1998) compared
the coverage of a French parser with and wi-
thout terminology recognition in the preproces-
sing stage. She found that the integration of 210
nominal terms in the preprocessing components of
the parser resulted in a significant reduction of the
number of alternative parses (from an average of
4.21 to 2.79). The eliminated parses were found
to be semantically undesirable. No valid analy-
sis were ruled out. Similarly, Zhang and Kor-
doni (2006) extended a lexicon with 373 additio-
nal MWE lexical entries and obtained a significant
increase in the coverage of an English grammar
(14.4%, from 4.3% to 18.7%).
In the cases mentioned above, a ?words-with-
spaces? approach was used. In contrast, Ale-
gria et al (2004) and Villavicencio et al (2007)
adopted a compositional approach to the enco-
ding of MWEs, able to capture more morpho-
syntactically flexible MWEs. Alegria et al (2004)
showed that by using a MWE processor in the pre-
processing stage of their parser (in development)
for Basque, a significant improvement in the POS-
tagging precision is obtained. Villavicencio et al
(2007) found that the addition of 21 new MWEs
to the lexicon led to a significant increase in the
grammar coverage (from 7.1% to 22.7%), without
altering the grammar accuracy.
An area of intensive research in parsing is
concerned with the use of lexical preferences, co-
occurrence frequencies, collocations, and contex-
tually similar words for PP attachment disambi-
guation. Thus, an important number of unsupervi-
sed (Hindle and Rooth, 1993; Ratnaparkhi, 1998;
Pantel and Lin, 2000), supervised (Alshawi and
Carter, 1994; Berthouzoz and Merlo, 1997), and
combined (Volk, 2002) methods have been deve-
loped to this end.
However, as Hindle and Rooth (1993) pointed
out, the parsers used by such methods lack pre-
cisely the kind of corpus-based information that
is required to resolve ambiguity, because many
of the existing attachments may be missing or
wrong. The current literature provides no indi-
cation about the manner in which this circular
problem can be circumvented, and on whether
flexible MWEs should be processed before, du-
ring or after the sentence analysis takes place.
29
3 Parsing and Collocations
As argued by many researchers ? e.g., Heid (1994)
? collocation identification is best performed on
the basis of parsed material. This is due to the
fact that collocations are co-occurrences of lexi-
cal items in a specific syntactic configuration. The
collocation break record, for instance, is obtained
only in the configurations where break is a verb
whose direct object is (semantically) headed by
the lexical item record. In other words, the collo-
cation is not defined in terms of linear proximity,
but in terms of a specific grammatical relation.
As the examples in this section show, the rela-
tive order of the two items is not relevant, nor is
the distance between the two terms, which is unli-
mited as long as the grammatical relation holds4.
In our system, the grammatical relations are com-
puted by a syntactic parser, namely, Fips (Wehrli,
2007; Wehrli and Nerima, 2009). Until now, the
collocation identification process took place at the
end of the parse in a so-called ?interpretation?
procedure applied to the complete parse trees. Al-
though quite successful, this way of doing pre-
sents a major drawback: it happens too late to
help the parser. This section discusses this point
and describes the alternative that we are currently
developing, which consists in identifying colloca-
tions as soon as possible during the parse.
One of the major hurdles for non-deterministic
parsers is the huge number of alternatives that
must be considered. Given the high fre-
quency of lexical ambiguities, the high level of
non-determinism of natural language grammars,
grammar-based parsers are faced with a number
of alternatives which grows exponentially with the
length of the input sentence. Various methods
have been proposed to reduce that number, and
in most cases heuristics are added to the parsing
algorithm to limit the number of alternatives. Wi-
thout such heuristics, the performance of a parser
might not be satisfactory enough for large scale
applications such as machine translation or other
tasks involving large corpora.
We would like to argue, along the lines of
previous work (section 2), that collocations can
4Goldman et al (2001) report examples in which the dis-
tance between the two terms of a collocation can exceed 30
words.
contribute to the disambiguation process so cru-
cial for parsing. To put it differently, identifying
collocations should not be seen as a burden, as an
additional task the parser should perform, but on
the contrary as a process which may help the par-
ser through the maze of alternatives. Collocations,
in their vast majority, are made of frequently used
terms, often highly ambiguous (e.g., break record,
loose change). Identifying them and giving them
high priority over alternatives is an efficient way
to reduce the ambiguity level. Ambiguity reduc-
tion through the identification of collocations is
not limited to lexical ambiguities, but also applies
to attachment ambiguities, and in particular to the
well-known problem of PP attachment. Consider
the following French examples in which the pre-
positions are highlighted:
(1)a. ligne de partage des eaux (?watershed?)
b. syste`me de gestion de base de donne?es (?da-
tabase management system?)
c. force de maintien de la paix (?peacekeeping
force?)
d. organisation de protection de
l?environnement (?environmental protection
agency?)
In such cases, the identification of a noun-
preposition-noun collocation will prevent or dis-
courage any other type of prepositional attach-
ment that the parser would otherwise consider.
3.1 The Method
To fulfill the goal of interconnecting the parsing
procedure and the identification of collocations,
we have incorporated the collocation identifica-
tion mechanism within the constituent attachment
procedure of our parser Fips (Wehrli, 2007). This
parser, like many grammar-based parsers, uses
left attachment and right attachment rules to build
respectively left subconstituents and right sub-
constituents. Given the fact that Fips? rules always
involve exactly two constituents ? see Wehrli
(2007) for details ? it is easy to add to the attach-
ment mechanism the task of collocation identifica-
tion. To take a very simple example, when the rule
attaching a prenominal adjective to a noun applies,
the collocation identification procedure is invo-
ked. It first verifies that both terms bear the lexical
30
feature [+partOfCollocation], which signals that a
given word is associated in our lexical database to
one or several collocations, and then searches the
collocation database for an adjective-noun collo-
cation with those two terms. If successful, the cor-
responding parse tree will be given a high priority.
With examples such as loose change, the iden-
tification of the collocation will immediately re-
legate any (partial) analysis based on the verbal
reading of either terms.
To take a somewhat more complex example,
consider a verb-object collocation such as break
record, as in example (2)5.
(2)a. John broke a record.
b. [TP [DP John ] broke [DP a [NP record ] ] ]
Here, it is a right attachment rule which will
trigger the identification procedure. To be precise,
the right attachment rule in this case concerns the
attachment of the noun record as complement
of the indefinite determiner (head of the DP
direct object of the verb). The identification
procedure considers, in turn, all the governing
nodes dominating the noun record, halting at the
first node of category Noun6, Verb or Adjective.
In our example, the determiner node and then
the verb node will be considered. Notice that the
procedure will, quite correctly, identify a colloca-
tion in the French example (3a), but not in (3b),
although both structures are identical. The reason
has to do with the fact that the noun governing
record in the first example is a [+number] noun,
that is a classifier noun which is transparent for
the identification procedure7.
(3)a. Jean a battu un grand nombre de records.
?Jean broke a large number of records?
5We use the following labels in our phrase-structure re-
presentations: TP-Tense phrase, for simple sentence (the S of
standard CFG), CP-Complementizer phrase, for a sentence
with a conjunction or a complementizer, DP-Determiner
phrase for standard noun phrases (we assume the DP hy-
pothesis, whereby the determiner constitutes the syntac-
tic head of a noun phrase), NP-Noun phrase for nominal
projections (nouns with their modifiers/complements), VP-
Verb phrase, PP-Prepositional phrase, AP-Adjectival phrase,
AdvP-Adverbial phrase, FP-Functional phrase (used for se-
condary predicates).
6Unless the node it marked [+number], as we will see
shortly.
7See Fontenelle (1999) for a detailed account of transpa-
rent nouns.
b. Jean a battu le de?tenteur du record.
?Jean has beaten the holder of the record?
As in the other examples, an analysis in which
a collocation has been found is given high prio-
rity over alternatives. In the case of (2), this will
relegate potential analyses based on the adjectival
reading of broke or the verbal reading of record.
Notice that exactly the same procedure applies
when the trace of an extraposed element is (right)
inserted, as in the examples (4), which illustrate
the case of wh-interrogative (a), relative clause
(b), tough-movement (c).
(4)a. Which record will Paul try to break ?
b. The record Paul broke was very old.
c. This record is difficult to break.
In all such cases, that is, when the right inserted
element is a trace, the identification procedure
will consider its antecedent, or to be more precise,
the semantic head of its antecedent. Finally, the
grammatical processes involved in example (4a,c)
can combine as in the more complex example (5),
for which we give the slightly simplified structure
with the chain of elements with index i extending
from the fronted wh-phrase which record to the
direct object position of the verb break, via the
direct object position of the verb consider and the
subject position of the secondary predicate (FP)
headed by the [+tough] adjective difficult.
(5)a. Which record did Paul consider difficult to
break ?
b. [ CP [ DP which record]i [ TP did [ DP Paul
] [ VP consider ][ DP e]i [ FP [ DP e]i [ AP
difficult [ TP to [ VP break [ DP e]i ] ] ] ] ] ]
3.2 Complex Collocations
As stated, for instance, by (Heid, 1994), colloca-
tions can involve more than two (main) terms and
it is possible to adopt a recursive definition of col-
locations, i.e., complex collocations can be vie-
wed as collocations of collocations. The colloca-
tion identification procedure has been extended to
handle such cases. Consider examples (6) below.
(6)a. La voiture tombera probablement en panne
d?essence.
?the car will probably run out of gas?
31
b. natural language processing
c. He broke a world record.
In the French sentence (6a), panne d?essence
(literally, ?breakdown of gas?, ?out of gas?) is
a collocation of type Noun+Prep+Noun, which
combines with the verb tomber (literally, ?to
fall?) to form a larger collocation of type
Verb+PrepObject tomber en panne d?essence (?to
run out of gas?). Given the strict left to right
processing order assumed by the parser, it will
first identify the collocation tomber en panne (?to
break down?) when attaching the word panne.
Then, reading the last word, essence (?gas?), the
parser will first identify the collocation panne
d?essence. Since that collocation bears the lexi-
cal feature [+partOfCollocation], the identifica-
tion procedure goes on, through the governors
of that item. The search succeeds with the verb
tomber, and the collocation tomber en panne
d?essence (?run out of gas?) is identified.
4 Evaluation Experiments
In this section, we describe the experiments we
performed in order to evaluate the precision and
recall of the method introduced in section 3, and
to compare it against the previous method (fully
described in Wehrli et al (2009b)). We extend
this comparison by performing a task-based eva-
luation, which investigates the impact that the new
method has on the quality of translations produ-
ced by a machine translation system relying on
our parser (Wehrli et al, 2009a).
4.1 Precision Evaluation
The data considered in this experiment consist of
a subpart of a corpus of newspaper articles collec-
ted from the on-line version of The Economist8,
containing slightly more that 0.5 million words.
On these data, we run two versions of our parser:
? V1: a version implementing the previous me-
thod of collocation identification,
? V2: a version implementing the new method
described in section 3.
8URL:http://www.economist.com/
(accessed June, 2010).
The lexicon of the parser was kept constant,
which is to say that both versions used the same
lexicon (which contains slightly more than 7500
English collocation entries), only the parsing mo-
dule handling collocations was different. From
the output of each parser version, we collected
statistics on the number of collocations (present
in the lexicon) that were identified in the test cor-
pus. More precisely, we traversed the output trees
and counted the items that were marked as col-
location heads, each time this was the case (note
that an item may participate in several colloca-
tions, not only one). Table 1 presents the num-
ber of collocations identified, both with respect to
collocation instances and collocation types.
V1 V2 common V1 only V2 only
Tokens 4716 5412 4347 399 1003
Types 1218 1301 1182 143 368
Table 1. Collocation identification results.
As the results show, the new method (column
V2) is more efficient in retrieving collocation ins-
tances. It detects 696 more instances, which cor-
respond to an increase of 14.8% relative to the
previous method (column V1). As we lack the
means to compare on a large scale the correspon-
ding syntactic trees, we can only speculate that the
increase is mainly due to the fact that more appro-
priate analyses are produced by the new method.
A large number of instances are found by both
versions of the parser. The difference between
the two methods is more visible for some syn-
tactic types than for others. Table 2 details the
number of instances of each syntactic type which
are retrieved exclusively by one method or by the
other.
To measure the precision of the two methods,
we randomly selected 20 collocation instances
among those identified by each version of the par-
ser, V1 and V2, and manually checked whether
these instances are correct. Correctness means
that in the given context (i.e., the sentence in
which they were identified), the word combina-
tion marked as instance of a lexicalized colloca-
tion is indeed an instance of that collocation. A
counterexample would be, for instance, to mark
the pair decision - make in the sentence in (7) as
32
Syntactic type V1 V2 Difference V2-V1
A-N 72 152 80
N-N 63 270 207
V-O 22 190 168
V-P-N 6 10 4
N-P-N 1 62 61
V-A 25 166 141
P-N 200 142 -58
N&N 6 2 -4
Adv-Adv 4 9 5
Table 2. Differences between the two methods:
number of tokens retrieved exclusively by each
method.
an instance of the verb-object collocation to make
a decision, which is an entry in our lexicon.
(7)a. The decision to make an offer to buy or sell
property at price is a management decision
that cannot be delegated to staff.
Since judging the correctness of a collocation ins-
tance in context is a rather straightforward task,
we do not require multiple judges for this evalua-
tion. The precision obtained is 90% for V1, and
100% for V2.
The small size of test set is motivated by the
fact that the precision is expected to be very high,
since the presence of both collocation components
in a sentence in the relevant syntactic relation al-
most certainly means that the recognition of the
corresponding collocation is justified. Exceptions
would correspond to a minority of cases in which
the parser either wrongly establishes a relation
between two items which happen to belong to an
entry in the lexicon, or the two items are related
but the combination corresponds to a literal usage
(examples are provided later in this section).
The errors of V1 correspond, in fact, to cases in
which a combination of words used literally was
wrongly attributed to a collocation: in example
(8a), V1 assigned the words on and business to
the lexical entry on business, and in example (8b),
it assigned in and country to the entry in the coun-
try9.
(8)a. It is not, by any means, specific to the
countryside, but it falls especially heavily on
small businesses.
9V1 makes the same error on (8a), but does better on (8b).
These expressions are frozen and should not be treated as
standard collocations.
b. Industrial labour costs in western Germany
are higher than in any other country.
To better pinpoint the difference between V1
and V2, we performed a similar evaluation on an
additional set of 20 instances, randomly selected
among the collocations identified exclusively by
each method. Thus, the precision of V1, when
measured on the tokens in ?V1 only?, was 65%.
The precision of V2 on ?V2 only? was 90%. The
2 errors of V2 concern the pair in country, found
in contexts similar to the one shown in example
(8b). The errors of V1 also concerned the same
pair, with one exception ? the identification of the
collocation world trade from the context the des-
truction of the World Trade Centre. Since World
Trade Centre is not in the parser lexicon, V1 ana-
lysed it and assigned the first two words to the en-
try world trade. World was wrongly attached to
Trade, rather than to Centre.
When reported on the totality of the instances
tested, the precision of V1 is 77.5% and that of
V2 is 95%. Besides the increase in the precision
of identified collocations, the new method also
contributes to an increase in the parser coverage10,
from 81.7% to 83.3%. The V1 parser version suc-
ceeds in building a complete parse tree for 23187
of the total 28375 sentences in the corpus, while
V2 does so for 23629 sentences.
4.2 Recall Evaluation
To compare the recall of two methods we perfor-
med a similar experiment, in which we run the two
versions of the parser, V1 and V2, on a small col-
lection of sentences containing annotated colloca-
tion instances. These sentences were randomly
selected from the Europarl corpus (Koehn, 2005).
The collocations they contain are all verb-object
collocations. We limit our present investigation
to this syntactic type for two reasons: a) anno-
tating a corpus with all instances of collocation
entries in the lexicon would be a time-consuming
task; and b) verb-object collocations are among
the most syntactically flexible and therefore diffi-
cult to detect in real texts. Thus, this test set pro-
vides realistic information on recall.
10Coverage refers more precisely to the ratio of sentences
for which a complete parse tree could be built.
33
The test set is divided in two parts: 100 sen-
tences are in English, and 100 other in Italian,
which allows for a cross-linguistic evaluation of
the two methods. Each sentence contains one an-
notated collocation instance, and there are 10 ins-
tances for a collocation type. Table 3 lists the col-
location types in the test set (the even rows in co-
lumn 2 display the glosses for the words in the
Italian collocations).
English Italian
bridge gap assumere atteggiamento
?assume? ?attitude?
draw distinction attuare politica
?carry out? ?policy?
foot bill avanzare proposta
?advance? ?proposal?
give support avviare dialogo
?start? ?dialogue?
hold presidency compiere sforzo
?commit? ?effort?
meet condition dare contributo
?give? ?contribution?
pose threat dedicare attenzione
?dedicate? ?attention?
reach compromise operare scelta
?operate? ?choice?
shoulder responsibility porgere benvenuto
?give? ?welcome?
strike balance raggiungere intesa
?reach? ?understanding?
Table 3. Collocation types in the test set.
The evaluation results are presented in table 4.
V1 achieves 63% recall performance on the En-
glish data, and 44% on the Italian data. V2 shows
considerably better results: 76% on English and
66% on Italian data. The poorer performance
of both methods on Italian data is explained by
the difference in performance between the English
and Italian parsers, and more precisely, by the dif-
ference in their grammatical coverage. The En-
glish parser succeeds in building a complete parse
tree for more than 70% of the sentences in the test
set, while the Italian parser only for about 60%.
As found in the previous experiment (presen-
ted in section 4.1), for both languages considered
in this experiment, the new method of processing
collocations contributes to improving the parsing
coverage. The coverage of the English parser in-
creases from 71% to 76%, and that of the Italian
parser from 57% to 61%.
V1 V2 Common V1 only V2 only
English 63 76 61 2 15
Italian 44 66 42 2 24
Table 4. Recall evaluation results: number of cor-
rect collocation instances identified.
4.3 Task-based Evaluation
In addition to reporting the performance results by
using the standard measures of precision and re-
call, we performed a task-based performance eva-
luation, in which we quantified the impact that the
newly-proposed method has on the quality of the
output of a machine translation system. As the
examples in table 3 suggest, a literal translation of
collocations is rarely the most appropriate. In fact,
as stated by Orliac and Dillinger (2003), know-
ledge of collocations is crucial for machine trans-
lation systems. An important purpose in iden-
tifying collocations with our parser is to enable
their proper treatment in our translation system, a
rule-based system that performs syntactic transfer
by relying on the structures produced by the par-
ser.
In this system, the translation of a collocation
takes place as follows. When the parser identi-
fies a collocation in the source sentence, its com-
ponent words are marked as collocation mem-
bers, in order to prevent their literal translation.
When the transfer module processes the collo-
cation head, the system checks in the bilingual
lexicon whether an entry exists for that colloca-
tion. If not, the literal translation will apply;
otherwise, the transfer module projects a target-
language structure as specified in the correspon-
ding target lexical entry. More precisely, the trans-
fer yields a target language abstract representa-
tion, to which grammatical transformations and
morphological generation will apply to create the
target sentence. The identification of collocations
in the source text is a necessary, yet not a sufficient
condition for their successful translation.
In this experiment, we considered the test set
described in section 4.2 and we manually eva-
luated the translation obtained for each colloca-
tion instance. Both subsets (100 English sen-
tences and 100 Italian sentences) were translated
into French. We compared the translations obtai-
34
Task Measure Test set Language Increase
Collocation identification precision 40 instances English 17.5%
recall 200 instances English, Italian 17.5%
100 instances English 13%
100 instances Italian 22%
Collocation translation precision 200 instances {English, Italian}-French 13%
100 instances English-French 10%
100 instances Italian-French 16%
Parsing coverage 28375 sentences English 1.6%
200 sentences English 5%
200 sentences Italian 4%
Table 5. Summary of evaluation results.
ned by relying on the versions V1 and V2 of our
parser (recall that V2 corresponds to the newly-
proposed method and V1 to the previous method).
The use of automatic metrics for evaluating the
translation output was not considered appropriate
in this context, since such n-gram based metrics
underestimate the effect that the substitution of a
single word (like in our case, the verb in a verb-
object collocation) has on the fluency, adequacy,
and even on the interpretability of the output sen-
tence.
The comparison showed that, for both language
pairs considered (English-French and Italian-
French), the version of parser which integrates the
new method is indeed more useful for the ma-
chine translation system than the previous version.
When V2 was used, 10 more collocation instances
were correctly translated from English to French
than when using V1. For the Italian-French pair,
V2 helped correctly translating 16 more colloca-
tion instances in comparison with V1. This cor-
responds to an increase in precision of 13% on the
whole test set of 200 sentences. The increase in
performance obtained in all the experiments des-
cribed in this section is summarized in table 5.
5 Conclusion
In this paper, we addressed the issue of the inter-
connection between collocation identification and
syntactic parsing, and we proposed an original so-
lution for identifying collocations in a sentence as
soon as possible during the analysis (rather than at
the end of the parsing process). The major advan-
tage of this approach is that collocational informa-
tion may be used to guide the parser through the
maze of alternatives.
The experimental results performed showed
that the proposed method, which couples parsing
and collocation identification, leads to substan-
tial improvements in terms of precision and re-
call over the standard identification method, while
contributing to augment the coverage of the par-
ser. In addition, it was shown that it has a posi-
tive impact on the results of a subsequent appli-
cation, namely, machine translation. Future work
will concentrate on improving our method so that
it accounts for all the possible syntactic configu-
rations of collocational attachments, and on exten-
ding its recall evaluation to other syntactic types.
Acknowledgements
Thanks to Lorenza Russo and Paola Merlo for a
thorough reading and comments. Part of the re-
search described in this paper has been supported
by a grant from the Swiss National Science Foun-
dation, grant no 100015-117944.
References
Alegria, In?aki, Olatz Ansa, Xabier Artola, Nerea
Ezeiza, Koldo Gojenola, and Ruben Urizar. 2004.
Representation and treatment of multiword expres-
sions in basque. In Second ACL Workshop on Mul-
tiword Expressions: Integrating Processing, pages
48?55, Barcelona, Spain.
Alshawi, Hiyan and David Carter. 1994. Training
and scaling preference functions for disambigua-
tion. Computational Linguistics, 20(4):635?648.
Benson, Morton, Evelyn Benson, and Robert Ilson.
1986. The BBI Dictionary of English Word Combi-
nations. John Benjamins, Amsterdam/Philadelphia.
Berthouzoz, Cathy and Paola Merlo. 1997. Statis-
tical ambiguity resolution for principle-based par-
sing. In Nicolov, Nicolas and Ruslan Mitkov, edi-
35
tors, Recent Advances in Natural Language Pro-
cessing: Selected Papers from RANLP?97, Current
Issues in Linguistic Theory, pages 179?186. John
Benjamins, Amsterdam/Philadelphia.
Brun, Caroline. 1998. Terminology finite-state pre-
processing for computational LFG. In Proceedings
of the 36th Annual Meeting of the Association for
Computational Linguistics and 17th International
Conference on Computational Linguistics, pages
196?200, Morristown, NJ, USA.
Firth, John R. 1957. Papers in Linguistics 1934-1951.
Oxford Univ. Press, Oxford.
Fontenelle, Thierry. 1999. Semantic resources for
word sense disambiguation: a sine qua non? Lin-
guistica e Filologia, (9):25?43. Dipartimento di
Linguistica e Letterature Comparate, Universita` de-
gli Studi di Bergamo.
Goldman, Jean-Philippe, Luka Nerima, and Eric
Wehrli. 2001. Collocation extraction using a syn-
tactic parser. In Proceedings of the ACL Work-
shop on Collocation: Computational Extraction,
Analysis and Exploitation, pages 61?66, Toulouse,
France.
Gre?goire, Nicole, Stefan Evert, and Brigitte Krenn,
editors. 2008. Proceedings of the LREC Workshop
Towards a Shared Task for Multiword Expressions
(MWE 2008). European Language Resources Asso-
ciation (ELRA), Marrakech, Morocco.
Heid, Ulrich. 1994. On ways words work together
? research topics in lexical combinatorics. In Pro-
ceedings of the 6th Euralex International Congress
on Lexicography (EURALEX ?94), pages 226?257,
Amsterdam, The Netherlands.
Hindle, Donald and Mats Rooth. 1993. Structural am-
biguity and lexical relations. Computational Lin-
guistics, 19(1):103?120.
Koehn, Philipp. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of
The Tenth Machine Translation Summit (MT Sum-
mit X), pages 79?86, Phuket, Thailand, September.
Orliac, Brigitte and Mike Dillinger. 2003. Collocation
extraction for machine translation. In Proceedings
of Machine Translation Summit IX, pages 292?298,
New Orleans, Lousiana, USA.
Pantel, Patrick and Dekang Lin. 2000. An unsuper-
vised approach to prepositional phrase attachment
using contextually similar words. In Proceedings
of the 38th Annual Meeting of the Association for
Computational Linguistics, pages 101?108, Hong
Kong, China.
Ratnaparkhi, Adwait. 1998. Statistical models for un-
supervised prepositional phrase attachment. In Pro-
ceedings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics,
pages 1079?1085, Montreal, Quebec, Canada.
Sag, Ivan A., Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Pro-
ceedings of the Third International Conference on
Intelligent Text Processing and Computational Lin-
guistics (CICLING 2002), pages 1?15, Mexico City.
Sinclair, John. 1991. Corpus, Concordance, Colloca-
tion. Oxford University Press, Oxford.
Villavicencio, Aline, Valia Kordoni, Yi Zhang, Marco
Idiart, and Carlos Ramisch. 2007. Validation and
evaluation of automatically acquired multiword ex-
pressions for grammar engineering. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computatio-
nal Natural Language Learning (EMNLP-CoNLL),
pages 1034?1043, Prague, Czech Republic, June.
Volk, Martin. 2002. Combining unsupervised and
supervised methods for PP attachment disambi-
guation. In Proceedings of the 19th Internatio-
nal Conference on Computational Linguistics (CO-
LING?02), pages 25?32, Taipei, Taiwan.
Wehrli, Eric and Luka Nerima. 2009. L?analyseur
syntaxique Fips. In Proceedings of the IWPT 2009
ATALA Workshop: What French parsing systems?,
Paris, France.
Wehrli, Eric, Luka Nerima, and Yves Scherrer. 2009a.
Deep linguistic multilingual translation and bilin-
gual dictionaries. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
90?94, Athens, Greece. Association for Computa-
tional Linguistics.
Wehrli, Eric, Violeta Seretan, Luka Nerima, and Lo-
renza Russo. 2009b. Collocations in a rule-based
MT system: A case study evaluation of their trans-
lation adequacy. In Proceedings of the 13th Annual
Meeting of the European Association for Machine
Translation, pages 128?135, Barcelona, Spain.
Wehrli, Eric. 2000. Parsing and collocations. In
Christodoulakis, D., editor, Natural Language Pro-
cessing, pages 272?282. Springer Verlag.
Wehrli, Eric. 2007. Fips, a ?deep? linguistic multilin-
gual parser. In ACL 2007 Workshop on Deep Lin-
guistic Processing, pages 120?127, Prague, Czech
Republic.
Zhang, Yi and Valia Kordoni. 2006. Automated deep
lexical acquisition for robust open texts processing.
In Proceedings of LREC-2006, pages 275?280, Ge-
noa, Italy.
36
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 125?127,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
FipsCoView: On-line Visualisation of Collocations
Extracted from Multilingual Parallel Corpora
Violeta Seretan
School of Informatics
University of Edinburgh
violeta.seretan@gmail.com
Eric Wehrli
Language Technology Laboratory
University of Geneva
eric.wehrli@unige.ch
Abstract
We introduce FipsCoView, an on-line inter-
face for dictionary-like visualisation of collo-
cations detected from parallel corpora using a
syntactically-informed extraction method.
1 Introduction
Multilingual (parallel) corpora?e.g., Europarl
(Koehn, 2005)?represent a valuable resource
for tasks related to language production that is
exploitable in a wide variety of settings, such as
second language learning, lexicography, as well as
human or automatic translation. We focus on lexi-
cographic exploitation of such resources and present
a system, called FipsCoView,1 which is specifically
aimed at supporting the work of lexicographers who
compile multilingual collocation resources.
Collocation, a rather ill-defined linguistic con-
cept referring to a large and heterogeneous sub-class
of multi-word expressions, is understood here as a
combination of words that produces natural-soun-
ding speech and writing (Lea and Runcie, 2002)
and that has syntactic and semantic properties which
cannot be entirely predicted from those of its com-
ponents and therefore has to be listed in a lexicon
(Evert, 2004). Collocations are particularly interest-
ing from a translation point of view, and our system
can also be used to facilitate the task of translators
looking for the right translation of a word in context.
The usage scenario is the following. Given a
word, like money, our system provides a concise and
intuitive presentation of the list of collocations with
1Available at http://tinyurl.com/FipsCoView.
that word, which have previously been detected in
the source language version of the parallel corpus.
By selecting one of the items in this list, e.g., money
laundering, users will be able to see the contexts of
that item, represented by the sentences in which it
occurs. In addition, users can select a target lan-
guage from the list of other languages in which the
multilingual corpus is available2 and visualise the
target language version of the source sentences.
This presentation enables users to find potential
translation equivalents for collocations by inspecting
the target sentences. Thus, in the case of French, the
preferred equivalent found is blanchiment d?argent,
lit., ?money whitening?, rather than the literal trans-
lation from English, *lavage d?argent. In the case of
Italian, this is riciclaggio di denaro, lit., ?recycling
of money?, rather than the literal translation ?lavag-
gio di soldi, also possible but much less preferred.
Access to target sentences is important as it allows
users to see how the translation of a collocation vary
depending on the context. Besides, it provides use-
ful usage clues, indicating, inter alia, the allowed or
preferred morphosyntactic features of a collocation.
In this paper, we present the architecture of
FipsCoView and outline its main functionalities.
This system is an extension of FipsCo, a larger
fully-fledged off-line system, which, in turn, is in-
tegrated into a complex framework for process-
ing multi-word expressions (Seretan, 2009). While
the off-line system finds direct applicability in our
on-going projects of large-scale multilingual syntac-
2Europarl includes 11 languages: French, Italian, Spanish,
Portuguese, English, Dutch, German, Danish, Swedish, Greek,
Finnish. Note that our tool is not tailored to this specific corpus.
125
Figure 1: FipsCoView: System architecture.
tic parsing (Wehrli, 2007) and syntax-based machine
translation (Wehrli et al, 2009), the on-line version
is designed to offer access to the derived collocation
resources to a broader community.
2 Architecture and Main Functionalities
Figure 1 shows the architecture of FipsCoView. The
main system modules are the collocation extraction
module, the search & visualisation module, the con-
cordancing and the sentence alignment modules.
The processing flow is pipelined. The key mod-
ule of the system, collocation extraction, relies on
a syntax-based methodology that combines lexi-
cal statistics with syntactic information provided by
Fips, a deep symbolic parser (Wehrli, 2007). This
methodology is fully described and evaluated in
Seretan (2011). In principle, the extraction takes
place only once, but new corpora can be processed
later and results are cumulated. The sentence align-
ment (Nerima et al, 2003) is performed partially,
i.e., only for the sentences actually displayed by the
concordancing module. It is done on the fly, thus
eliminating the need of pre-aligning the corpora.
The role of the concordancing module is to
present the sentence contexts for a selected colloca-
tion (cf. scenario described in ?1). The words in this
collocation are highlighted for readability. The list
of sentences is displayed in the order given by the
syntactic variation of collocations, that is, the collo-
cation instances for which the distance between the
components is larger are displayed first. This func-
tionality is designed to support the work of users in-
specting the syntactic properties of collocations.
The search & visualisation module takes as input
the word entered by the user in the system interface,
performs a search in the database that stores the col-
location extraction results, and provides a one-page
presentation of the collocational information related
to the sought word. Users can set visualisation pa-
rameters such as the minimal frequency and associa-
tion score, which limit the displayed results accord-
ing to the number of occurrences in the corpus and
the ?association strength? between the component
words, as given by the lexical association measure
used to extract collocations. The measure we typi-
cally use is log-likelihood ratio (Dunning, 1993); see
Pecina (2008) for an inventory of measures.
Depending on these parameters, the automatically
created collocation entry is more or less exhaustive
(the output adapts to the specific user?s purpose). A
different sub-entry is created for each part of speech
of the sought word (for instance, report can either
be a noun or a verb). Under each sub-entry, colloca-
tions are organised by syntactic type, e.g., adjective-
noun (comprehensive report), noun-noun (initiative
report), subject-verb (report highlights), verb-object
(produce a report). To avoid redundancy, only the
collocating words are shown. The sought word is
understood and is replaced by a tilde character, in
a paper dictionary style. Unlike in paper dictionary
presentations, the online presentation benefits from
the HTML environment by using colours, adapt-
ing the font size so that it reflects the association
strength (the most important combinations are more
visually salient), displaying additional information
such as score and frequency, and using hyper-links
for navigating from one word to another.
With respect to similar systems (Barlow, 2002;
Scott, 2004; Kilgarriff et al, 2004; Charest et al,
2007; Rayson, 2009; Fletcher, 2011), our system
uniquely combines parallel concordancing with col-
location detection based on deep syntactic process-
ing. It is available for English, French, Spanish and
Italian and it is being extended to other languages.
Acknowledgement
This work is partly supported by the Swiss National
Science Foundation (grant no. PA00P1 131512).
126
References
Michael Barlow. 2002. Paraconc: Concordance software
for multilingual parallel corpora. In Proceedings of
the Third International Conference on Language Re-
sources and Evaluation. Workshop on Language Re-
sources in Translation Work and Research, pages 20?
24, Las Palmas, Spain.
Simon Charest, E?ric Brunelle, Jean Fontaine, and
Bertrand Pelletier. 2007. E?laboration automatique
d?un dictionnaire de cooccurrences grand public. In
Actes de la 14e confe?rence sur le Traitement Au-
tomatique des Langues Naturelles (TALN 2007), pages
283?292, Toulouse, France, June.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74.
Stefan Evert. 2004. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis,
University of Stuttgart.
William H. Fletcher. 2011. Phrases in english: Online
database for the study of English words and phrases.
http://phrasesinenglish.org. Accessed
March, 2011.
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David
Tugwell. 2004. The Sketch Engine. In Proceedings of
the Eleventh EURALEX International Congress, pages
105?116, Lorient, France.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of The
Tenth Machine Translation Summit (MT Summit X),
pages 79?86, Phuket, Thailand, September.
Diana Lea and Moira Runcie, editors. 2002. Oxford Col-
locations Dictionary for Students of English. Oxford
University Press, Oxford.
Luka Nerima, Violeta Seretan, and Eric Wehrli. 2003.
Creating a multilingual collocation dictionary from
large text corpora. In Companion Volume to the
Proceedings of the 10th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL?03), pages 131?134, Budapest, Hungary.
Pavel Pecina. 2008. Lexical Association Measures: Col-
location Extraction. Ph.D. thesis, Charles University
in Prague.
Paul Rayson. 2009. Wmatrix: a web-based corpus
processing environment. http://ucrel.lancs.
ac.uk/wmatrix. Accessed March, 2011.
Mike Scott. 2004. WordSmith Tools version 4. Oxford
University Press, Oxford.
Violeta Seretan. 2009. An integrated environment for
extracting and translating collocations. In Michaela
Mahlberg, Victorina Gonza?lez-D??az, and Catherine
Smith, editors, Proceedings of the Corpus Linguistics
Conference CL2009, Liverpool, UK.
Violeta Seretan. 2011. Syntax-Based Collocation Ex-
traction. Text, Speech and Language Technology.
Springer, Dordrecht.
Eric Wehrli, Luka Nerima, and Yves Scherrer. 2009.
Deep linguistic multilingual translation and bilingual
dictionaries. In Proceedings of the Fourth Work-
shop on Statistical Machine Translation, pages 90?94,
Athens, Greece. Association for Computational Lin-
guistics.
Eric Wehrli. 2007. Fips, a ?deep? linguistic multilingual
parser. In ACL 2007 Workshop on Deep Linguistic
Processing, pages 120?127, Prague, Czech Republic.
127
Workshop on Humans and Computer-assisted Translation, pages 66?71,
Gothenburg, Sweden, 26 April 2014.
c?2014 Association for Computational Linguistics
The ACCEPT Portal: An Online Framework for the Pre-editing and
Post-editing of User-Generated Content
Violeta Seretan
FTI/TIM
University of Geneva
Switzerland
Violeta.Seretan@unige.ch
Johann Roturier
Symantec Ltd.
Dublin, Ireland
johann roturier@symantec.com
David Silva
Symantec Ltd.
Dublin, Ireland
David Silva@symantec.com
Pierrette Bouillon
FTI/TIM
University of Geneva
Switzerland
Pierrette.Bouillon@unige.ch
Abstract
With the development of Web 2.0, a
lot of content is nowadays generated on-
line by users. Due to its characteristics
(e.g., use of jargon and abbreviations, ty-
pos, grammatical and style errors), the
user-generated content poses specific chal-
lenges to machine translation. This pa-
per presents an online platform devoted to
the pre-editing of user-generated content
and its post-editing, two main types of hu-
man assistance strategies which are com-
bined with domain adaptation and other
techniques in order to improve the trans-
lation of this type of content. The plat-
form has recently been released publicly
and is being tested by two main types of
user communities, namely, technical fo-
rum users and volunteer translators.
1 Introduction
User-generated content ? i.e., information posted
by Internet users in social communication chan-
nels like blogs, forum posts, social networks ? is
one of the main sources of information available
today. Huge volumes of such content are created
each day, reach a very broad audience instantly.
1
The democratisation of content creation due
to the emergence of the Web 2.0 paradigm also
means a diversification of the languages used on
the Internet.
2
Despite its availability, the new con-
tent is only accessible to the speakers of the lan-
guage in which it was created. The automatic
translation of user-generated content is therefore
one of the key issues to be addressed in the field of
human language technologies. However, as stated
1
For instance, 58 million tweets are sent on aver-
age per day (http://www.statisticbrain.com/
twitter-statistics/).
2
See http://en.wikipedia.org/wiki/
Languages_used_on_the_Internet for statistics.
by Jiang et al. (2012), despite the obvious bene-
fits, there are relatively little attempts at translating
user-generated content.
The reason may lie in the fact that user-ge-
nerated content is very challenging for machine
translation. As shown, among others, by Nagara-
jan and Gamon (2011), there are several charac-
teristics of this content that pose new process-
ing challenges with respect to traditional content:
informal style, slang, abbreviations, specific ter-
minology, irregular grammar and spelling. In-
deed, Internet users are rarely professional writ-
ers.
3
They often write in a language which is not
their own, and sacrifice quality for speed, not pay-
ing attention to spelling, punctuation, or grammar
rules.
The ACCEPT project
4
addresses these chal-
lenges by developing a technology integrating
modules for automatic and manual content pre-
editing, statistical machine translation, as well
as output evaluation and post-editing. Thus, the
project aims to improve the translation of user-ge-
nerated content by proposing a full workflow, in
which the participation of humans is essential.
The application scenario considered in the
project are user communities sharing specific in-
formation on a given topic. The project focuses,
more specifically, on the following use cases:
1. the commercial use case, in which the tar-
get community is the user community built
around a software company in order for
members to help each other with issues re-
lated to products;
2. the NGO use case, in which non-go-
vernmental organisations such as Doctors
Without Borders produce health-care content
for distributions in areas of need.
3
Even when they are, as in the case of government agen-
cies, the type of content produced (e.g., tweets) still poses
?multiple challenges? to translation (Gotti et al., 2013).
4
http://www.accept-project.eu/
66
The language pairs considered in the project are
English to French, German and Japanese, as well
as French into English for the first use case (in-
volving technical forum information), and French
to and from English for the second use case (in-
volving healthcare information).
Past halfway into its research program, the
project has accomplished significant progress in
the main areas mentioned above (pre-editing, sta-
tistical machine translation, post-editing, and eval-
uation). The ACCEPT technology has recently
been released to the broad public as an on-
line framework, which demonstrates the different
modules of the workflow and provides access to
associated software components (plug-ins, APIs),
as well as to documentation. The pre-editing tech-
nology has been deployed on the targeted user fo-
rum
5
, allowing users to check their messages be-
fore posting them. The post-editing technology is
being used by a community of translators, which
provide pro-bono translation services to the NGOs
considered in our second use case.
In this paper, we describe the framework by pre-
senting its architecture and main modules (Sec-
tion 2). We discuss related work in Section 3 and
conclude in Section 4.
2 The Framework
The ACCEPT technology has been made acces-
sible to a broad audience in the form of an on-
line framework, i.e., an integrated environment
where registered users can perform pre-editing,
post-editing and evaluation work. The framework
? henceforth, the ACCEPT Portal ? is hosted on a
cloud computing infrastructure and is available at
www.accept-portal.eu.
2.1 Architecture of the Framework
As explained in Section 1, the ACCEPT techno-
logy consists of the following main modules:
1. Pre-editing module;
2. Machine translation module,
3. Post-editing module,
4. Evaluation module.
The typical workflow is incremental, but the
modules are independent. They can be used both
within and outside the portal, as they are built on a
REST API facilitating integration.
5
https://community.norton.com/
In the remaining of this section, we introduce
each of the framework modules.
6
2.2 Pre-editing Module
The pre-editing module leverages existing ling-
ware which provides authoring support rules
aimed at language professionals, by relying on
shallow language processing (Bredenkamp et al.,
2000). The existing English checker and the lin-
guistic resources on which it relies have been ex-
tended and adapted to suit the type of data gener-
ated by community users. In particular, the soft-
ware extension consisted of designing a number
of pre-editing rules aimed at source normalisation,
for the purpose of making the input text easier
to handle by the SMT systems. In the case of
French, the pre-editing rules have been designed
from scratch. The pre-editing rules pertain to the
levels of spelling, grammar, style and terminology.
They are defined using the original lingware?s rule
formalism and are incorporated into a server dedi-
cated to the project.
The rule development was corpus-driven and
was performed on data collected for this purpose.
A stable set of pre-edition rules is available in
the portal for each of the domains and source
languages considered (i.e., technical forum and
heathcare data in English and French). The rules
are described in detail in the project deliverable
D 2.2 (2013).
The rules proposed have been evaluated individ-
ually and in combination (Roturier et al., 2012;
Gerlach et al., 2013; Seretan et al., 2014). As
a general observation, it is important to notice
that, for SMT, the improvement of the input text
does not go hand in hand with the improvement of
translation. For example, in French the rule for
correcting verbal forms to the subjunctive tense
had a negative impact since the subjunctive is not
frequent in the training data. Conversely, it was
possible to define lexical reformulations which de-
graded the quality of the input text, but had a po-
sitive impact on translation quality.
The combined impact of the rule applica-
tion was measured in a variety of settings in a
large-scale evaluation campaign involving transla-
tion students (Seretan et al., 2014). As the rules
are divided into two major groups, those automati-
cally applicable and those requiring human inter-
6
The MT module will be omitted, as it is not part of the
portal. The interested reader is referred to D 4.2 (2013).
67
Figure 1: The ACCEPT Pre-edit plug-in in action (screen capture)
vention, the evaluation was carried out for the full
set of rules, as well as for the automatic rules only.
In addition, the evaluation was performed in both
a monolingual and a bilingual setting, i.e., with
the evaluators having or not access to the source
text, and it involved evaluation scales of different
granularities. The evaluation results showed a sys-
tematic statistically significant improvement over
the baseline when pre-editing is performed on the
source content. More details about the evalua-
tion methodology and results can be found in the
project deliverable D 9.2.2 (2013).
A data excerpt illustrating the impact of pre-
editing on translation quality is presented in Ex-
ample 1 below. The simple correction of an ac-
cented letter, du? d?u, leads to the change of seve-
ral target words, and to a much better translation of
the input sentence.
1. a) Source (original):
J?ai du m?absenter hier apr`es midi.
b) Source (pre-edited):
J?ai d?u m?absenter hier apr
`es midi.
c) Target (original):
I have the leave me yesterday afternoon.
d) Target (pre-edited):
I had to leave yesterday afternoon.
The pre-editing component of the ACCEPT
technology is available as a JQuery plug-in, which
can be downloaded and installed by Web applica-
tion owners, so that it can be used with text areas
and other text-bearing elements. APIs and ac-
companying documentation have also been made
available, so that the pre-editing rules can be
leveraged in automatic steps, without the plug-in,
across devices and platforms. A demo site illus-
trating the use of the plug-in in a TinyMCE envi-
ronment is available on the portal (see Figure 1).
The latest developments of the pre-editing mo-
dule include the possibility for users to customise
the application of rule sets, in particular, to ignore
specific rules and to manage their own dictionary,
in order to prevent the activation of checking flags.
2.3 Post-editing Module
The post-editing module of the framework (see
also Roturier et al., (2013)) is designed to fulfil
the project?s objective of collecting post-editing
data in order to learn correction rules and, through
feedback loops, to integrate them into the SMT
engines (with the goal of automating corrections
whenever possible). The project relies on the par-
ticipation of volunteer community members, who
are subject matter experts, native speakers of the
68
Figure 2: The ACCEPT Portal showing the post-editing demo (screen capture)
target language and, possibly, of the source lan-
guage. Accordingly, the post-editing environment
(see Figure 2) provides functionalities for both
monolingual and bilingual post-editing.
The post-editing text is organised in tasks be-
longing to post-editing projects. The latter are
created and managed by project administrators,
by defining the project settings (e.g., source and
target languages, monolingual or bilingual mode,
collaborative or non-collaborative type
7
), upload-
ing the text for each task
8
, inviting participants by
e-mail, and monitoring revision progress.
The post-editors edit the target text in a
sentence-by-sentence fashion. They have access
to the task guidelines and to help documentation.
The interface of the post-editing window displays
the whole text, through which they can navigate
with next-previous buttons or by clicking on a
specific sentence. Users can check the text they
are editing by accessing, with a button, the con-
tent checking technology described in Section 2.2.
Their actions ? in terms of keystrokes and usage
7
In a collaborative editing scenario, users may see edits
from other users and do not have to repeat them when work-
ing on the same project task. Conflicts are avoided by pre-
venting concurrent access.
8
Currently, the JSON format is used for the input data.
of translation options ? and time spent editing are
recorded in the portal.
9
When they are done edi-
ting, they can click on a button marking the com-
pletion of the task. At any time, they can interrupt
their work and save their results for later.
Users can enter a comment on the post-editing
task they have performed. The feedback elicited
from users include the difficulty of the task and
their sentiment (Was it easy to post-edit? Did you
enjoy the post-editing task?). For systematically
collecting user feedback, the project administra-
tors can specify on the project configuration page
a link to a post-task survey, which will be sent to
users after completing their tasks.
The post-editing module includes a JQuery
plug-in for deployment in any Web-based envi-
ronment; a dedicated section of the portal; APIs
enabling the use of the post-editing functionality
outside the portal; and sample evaluation projects
for several language pairs.
The post-editing technology has been exten-
sively used in specific post-editing campaigns in-
volving translator volunteers and Amazon Me-
chanical Turk
10
workers. The campaigns, includ-
9
The post-editing data is exported in XLIFF format.
10
The integration was done via the ACCEPT API.
69
ing reports on post-task surveys, are documented
inter alia in deliverable D 8.1.2 (2013). A notable
finding was that professional translators, who were
reticent towards MT before the task, had a more
positive sentiment after post-editing and their mo-
tivation to post-edit in the future increased.
2.4 Evaluation Module
The role of the evaluation module is to support the
collection of user ratings for assessing the quality
of source, machine-translated and post-edited con-
tent, and, ultimately, to support the development
of the technology created in the project.
This module groups several software compo-
nents: an evaluation environment available as a
section of the portal; APIs enabling the collection
of user evaluations in-context; and a third com-
ponent which is a customisation of the Appraise
toolkit for the collaborative collection of human
judgements (Federmann, 2012).
As in the case of post-editing module, this mod-
ule provides functionality for creating and man-
aging projects. Using the evaluation environ-
ment/APIs, project creators can define question
categories, add questions and possible answers,
and upload evaluation data (in JSON format). For
traditional evaluation projects, the Appraise sys-
tem is used instead.
3 Related Work
Transforming the source text in order to better
fit the needs of machine translation is a well-
investigated area of research. Strategies like
source control, source re-ordering, or source sim-
plification at the lexical or structural level have
been largely explored; for reviews, see, for in-
stance, Huhn (2013), Kazemi (2013), and Feng
(2008), respectively.
User-generated content has been investigated
in the context of machine translation in recent
work dealing specifically with spelling correc-
tion (Bertoldi et al., 2010; Formiga and Fonol-
losa, 2012); lexical normalisation by substituting
ill-formed words with their correct counterpart,
e.g., makn ? making (Han and Baldwin, 2011);
missing word ? e.g., zero-pronoun ? recovery and
punctuation correction (Wang and Ng, 2013).
Rather than focusing on specific phenomena or
Web genres (i.e., tweets), we adopt a more gen-
eral approach in which we address the problem of
source normalisation at multiple levels ? punctua-
tion, spelling, grammar, and style ? for any type of
linguistically imperfect text.
Another peculiarity of our approach is that it
is rule-based and does not require parallel data
for learning corrections. In exchange, a limi-
tation of our pre-editing approach is that it is
language-dependent, as the underlying technology
is based on shallow analysis and is therefore time-
expensive to extend to a new language.
The post-editing technology differs from exist-
ing (standalone or Web-based) dedicated tools ?
e.g., iOmegaT
11
or MateCat
12
? in that it is tai-
lored to community users, and, consequently, it
is lighter, it generates more concise reports, and
a simpler interface replaces the grid-like format
for presenting data. Another specificity is that it
is sufficiently flexible to be used in other environ-
ments (e.g., Amazon Mechanical Turk, cf. ?2.3).
4 Conclusion
The technology outlined in this paper demon-
strates a specific case of human-computer interac-
tion, in which, for the first time, several modules
are integrated in a full process in which human
pre-editors, post-editors and evaluators play a key
role for improving the translation of community
content. The technology is freely accessible in the
online portal, has been deployed on a major user
forum, and can be downloaded for integration in
other Web-based environments. Since it is built on
top of a REST API, it is portable across devices
and platforms. The technology would be useful to
anyone who needs information instantly and relia-
bly translated, despite linguistic imperfections.
One of the main future developments concerns
the further improvement of SMT, by exploring,
in particular, the use of text analytics and senti-
ment detection. In addition, by incorporating post-
editing rules and developing techniques to change
the phrase table and system parameters dynam-
ically, it will be possible to reduce the amount
of error corrections that human post-editors have
to perform repeatedly. Another major develop-
ment (joint work with the CASMACAT European
project) will focus on novel types of assistance for
translators, aimed specifically at helping transla-
tors by identifying problematic parts of the ma-
chine translation output and signalling the para-
phrases that are more likely to be useful.
11
http://try-and-see-mt.org/
12
http://www.matecat.com/
70
Acknowledgments
The research leading to these results has received
funding from the European Community?s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement n
o
288769.
References
Nicola Bertoldi, Mauro Cettolo, and Marcello Fe-
derico. 2010. Statistical machine translation of texts
with misspelled words. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 412?419, Los Angeles,
California.
Andrew Bredenkamp, Berthold Crysmann, and Mirela
Petrea. 2000. Looking for errors: A declarative for-
malism for resource-adaptive language checking. In
Proceedings of the Second International Conference
on Language Resources and Evaluation, Athens,
Greece.
2013. ACCEPT deliverable D 2.2 Definition of
pre-editing rules for English and French (final
version). http://www.accept.unige.
ch/Products/D2_2_Definition_of_
Pre-Editing_Rules_for_English_and_
French_with_appendixes.pdf.
2013. ACCEPT deliverable D 9.2.2: Survey of
evaluation results. http://www.accept.
unige.ch/Products/D_9_2_Survey_of_
evaluation_results.pdf.
2013. ACCEPT deliverable D 4.2 Report on
robust machine translation: domain adap-
tation and linguistic back-off. http:
//www.accept.unige.ch/Products/
D_4_2_Report_on_robust_machine_
translation_domain_adaptation_and_
linguistic_back-off.pdf.
2013. ACCEPT deliverable D 8.1.2 Data and
report from user studies - Year 2. http:
//www.accept.unige.ch/Products/D_
8_1_2_Data_and_report_from_user_
studies_-_Year_2.pdf.
Christian Federmann. 2012. Appraise: An open-
source toolkit for manual evaluation of machine
translation output. The Prague Bulletin of Mathe-
matical Linguistics (PBML), 98:25?35.
Lijun Feng. 2008. Text simplification: A survey.
Technical report, CUNY.
Llu??s Formiga and Jos?e A. R. Fonollosa. 2012. Dea-
ling with input noise in statistical machine transla-
tion. In Proceedings of COLING 2012: Posters,
pages 319?328, Mumbai, India.
Johanna Gerlach, Victoria Porro, Pierrette Bouillon,
and Sabine Lehmann. 2013. La pr?e?edition avec
des r`egles peu co?uteuses, utile pour la TA statistique
des forums ? In Actes de la 20e conf?erence sur
le Traitement Automatique des Langues Naturelles
(TALN?2013), pages 539?546, Les Sables d?Olonne,
France.
Fabrizio Gotti, Philippe Langlais, and Atefeh Farzin-
dar. 2013. Translating government agencies? tweet
feeds: Specificities, problems and (a few) solutions.
In Proceedings of the Workshop on Language Anal-
ysis in Social Media, pages 80?89, Atlanta, Georgia.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a #twit-
ter. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 368?378, Port-
land, Oregon.
Jie Jiang, Andy Way, and Rejwanul Haque. 2012.
Translating user-generated content in the social net-
working space. In Proceedings of the Tenth Biennial
Conference of the Association for Machine Transla-
tion in the Americas (AMTA-2012), San Diego, Cali-
fornia.
Arefeh Kazemi, Amirhassan Monadjemi, and Moham-
madali Nematbakhsh. 2013. A quick review on re-
ordering approaches in statistical machine transla-
tion systems. IJCER, 2(4).
Tobias Kuhn. 2013. A survey and classification of con-
trolled natural languages. Computational Linguis-
tics.
Meenakshi Nagarajan and Michael Gamon, editors.
2011. Proceedings of the Workshop on Language
in Social Media (LSM 2011). Portland, Oregon.
Johann Roturier, Linda Mitchell, Robert Grabowski,
and Melanie Siegel. 2012. Using automatic ma-
chine translation metrics to analyze the impact of
source reformulations. In Proceedings of the Con-
ference of the Association for Machine Translation
in the Americas (AMTA), San Diego, California.
Johann Roturier, Linda Mitchell, and David Silva.
2013. The ACCEPT post-editing environment: a
flexible and customisable online tool to perform and
analyse machine translation post-editing. In Pro-
ceedings of MT Summit XIV Workshop on Post-edi-
ting Technology and Practice, Nice, France.
Violeta Seretan, Pierrette Bouillon, and Johanna Ger-
lach. 2014. A large-scale evaluation of pre-edi-
ting strategies for improving user-generated content
translation. In Proceedings of the 9th Edition of the
Language Resources and Evaluation Conference,
Reykjavik, Iceland.
Pidong Wang and Hwee Tou Ng. 2013. A beam-
search decoder for normalization of social media
text with application to machine translation. In Pro-
ceedings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
471?481, Atlanta, Georgia.
71
