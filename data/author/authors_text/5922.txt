Automatic construction of parallel English-Chinese corpus for 
cross-language information retrieval 
J i ang  Chen and  J ian -Yun  N ie  
D~partement d ' In format ique et Recherche Op~rationnel le 
Universit~ de Montreal  
C.P. 6128, succursale CENTRE-V ILLE  
Montreal  (Quebec), Canada  H3C 3J7 
{chen, nie} @iro. umontreal, ca 
Abst rac t  
A major obstacle to the construction ofa probabilis- 
tic translation model is the lack of large parallel cor- 
pora. In this paper we first describe a parallel text 
mining system that finds parallel texts automatically 
on the Web. The generated Chinese-English paral- 
lel corpus is used to train a probabilistic translation 
model which translates queries for Chinese-English 
cross-language information retrieval (CLIR). We will 
discuss ome problems in translation model training 
and show the preliminary CUR results. 
1 In t roduct ion  
Parallel texts have been used in a number of studies 
in computational linguistics. Brown et al (1993) 
defined a series of probabilistic translation models 
for MT purposes. While people may question the 
effectiveness of using these models for a full-blown 
MT system, the models are certainly valuable for de- 
veloping translation assistance tools. For example, 
we can use such a translation model to help com- 
plete target ext being drafted by a human transla- 
tor (Langlais et al, 2000). 
Another utilization is in cross-language informa- 
tion retrieval (CLIR) where queries have to be trans- 
lated from one language to another language in 
which the documents are written. In CLIR, the qual- 
ity requirement for translation is relatively low. For 
example, the syntactic aspect is irrelevant. Even if 
the translated word is not a true translation but is 
strongly related to the original query, it is still help- 
ful. Therefore, CLIR is a suitable application for 
such a translation model. 
However, a major obstacle to this approach is the 
lack of parallel corpora for model training. Only 
a few such corpora exist, including the Hansard 
English-French corpus and the HKUST English- 
Chinese corpus (Wu, 1994). In this paper, we will 
describe a method which automatically searches for 
parallel texts on the Web. We will discuss the text 
mining algorithm we adopted, some issues in trans- 
lation model training using the generated parallel 
corpus, and finally the translation model's perfor- 
mance in CLIR. 
2 Para l le l  Text  M in ing  A lgor i thm 
The PTMiner system is an intelligent Web agent 
that is designed to search for large amounts of paral- 
lel text on the Web. The mining algorithm is largely 
language independent. It can thus be adapted to 
other language pairs with only minor modifications. 
Taking advantage ofWeb search engines as much 
as possible, PTMiner implements he following steps 
(illustrated in Fig. 1): 
1 Search for candidate sites - Using existing Web 
search engines, search for the candidate sites 
that may contain parallel pages; 
2 File name fetching - For each candidate site, 
fetch the URLs of Web pages that are indexed 
by the search engines; 
3 Host crawling - Starting from the URLs col- 
lected in the previous tep, search through each 
candidate site separately for more URLs; 
4 Pair scan - From the obtained URLs of each 
site, scan for possible parallel pairs; 
5 Download and verifying - Download the parallel 
pages, determine file size, language, and charac- 
ter set of each page, and filter out non-parallel 
pairs. 
2.1 Search for candidate Sites 
We take advantage of the huge number of Web sites 
indexed by existing search engines in determining 
candidate sites. This is done by submitting some 
particular equests to the search engines. The re- 
quests are determined according to the following ob- 
servations. In the sites where parallel text exists, 
there are normally some pages in one language con- 
taining links to the parallel version in the other lan- 
guage. These are usually indicated by those links' 
anchor texts 1. For example, on some English page 
there may be a link to its Chinese version with 
the anchor text "Chinese Version" or "in Chinese". 
1An anchor text  is a piece of text on a Web page which, 
when clicked on, will take you to another linked page. To 
be helpful, it usual ly  contains the key information about the 
l inked page. 
21 
Figure 1: The workflow of the mining process. 
The same phenomenon can be observed on Chinese 
pages. Chances are that a site with parallel texts 
will contain such links in some of its documents. 
This fact is used as the criterion in searching for 
candidate sites. 
Therefore, to determine possible sites for English- 
Chinese parallel texts, we can request an English 
document containing the following anchor: 
anchor : "engl ish version H \["in english", ...\]. 
Similar requests are sent for Chinese documents. 
From the two sets of pages obtained by the above 
queries we extract wo sets of Web sites. The union 
of these two sets constitutes then the candidate sites. 
That  is to say, a site is a candidate site when it 
is found to have either an English page linking to 
its Chinese version or a Chinese page linking to its 
English version. 
2.2 File Name Fetching 
We now assume that a pair of parallel texts exists on 
the same site. To search for parallel pairs on a site, 
PTMiner first has to obtain all (or at least part of) 
the HTML file names on the site. From these names 
pairs are scanned. It is possible to use a Web crawler 
to explore the candidate sites completely. However, 
we can take advantage of the search engines again to 
accelerate the process. As the first step, we submit 
the following query to the search engines: 
host : hostname 
to fetch the Web pages that they indexed from this 
site. If we only require a small amount of parallel 
texts, this result may be sufficient. For our purpose, 
however, we need to explore the sites more thor- 
oughly using a host crawler. Therefore, we continue 
our search for files with a host crawler which uses 
the documents found by the search engines as the 
starting point. 
2.3 Host Crawling 
A host crawler is slightly different from a Web 
crawler. Web crawlers go through innumerable 
pages and hosts on the Web. A host crawler is a 
Web crawler that crawls through documents on a 
given host only. A breadth-first crawling algorithm 
is applied in PTMiner as host crawler. The principle 
is that when a link to an unexplored ocument on 
the same site is found in a document, it is added to 
a list that will be explored later. In this way, most 
file names from the candidate sites are obtained. 
2.4 Pair Scan 
After collecting file names for each candidate site, 
the next task is to determine the parallel pairs. 
Again, we try to use some heuristic rules to guess 
which files may be parallel texts before downloading 
them. The rules are based on external features of 
the documents. By external feature, we mean those 
features which may be known without analyzing the 
contents of the file, such as its URL, size, and date. 
This is in contrast with the internal features, such as 
language, character set, and HTML structure, which 
cannot be known until we have downloaded the page 
and analyzed its contents. 
The heuristic criterion comes from the following 
observation: We observe that parallel text pairs usu- 
ally have similar name patterns. The difference be- 
tween the names of two parailel pages usually lies 
in a segment which indicates the language. For ex- 
ample, "file-ch.html" (in Chinese) vs. "file-en.html" 
(in English). The difference may also appear in the 
path, such as ".../chinese/.../fi le.html" vs. ".../en- 
glish/.../f i le.html'. The name patterns described 
above are commonly used by webmasters to help or- 
ganize their sites. Hence, we can suppose that a 
pair of pages with this kind of pattern are probably 
parallel texts. 
22
First, we establish four lists for English pre- 
fixes, English suffixes, Chinese prefixes and Chi- 
nese suffixes. For example: Engl ish P re f ix  = 
{e, en, e_, en_, e - ,  en - ,  ...}. For each file in one lan- 
guage, if a segment in its name corresponds to one 
of the language affixes, several new names are gener- 
ated by changing the segment to the possible corre- 
sponding affixes of the other language. If a generated 
name corresponds to an existing file, then the file is 
considered as a candidate parallel document of the 
original file. 
2.5 Filtering 
Next, we further examine the contents of the paired 
files to determine if they are really parallel according 
to various external and internal features. This may 
further improve the pairing precision. The following 
methods have been implemented in our system. 
2.5.1 Text Length 
Parallel files often have similar file lengths. One sim- 
ple way to filter out incorrect pairs is to compare 
the lengths of the two files. The only problem is to 
set a reasonable threshold that will not discard too 
many good pairs, i.e. balance recall and precision. 
The usual difference ratio depends on the language 
pairs we are dealing with. For example, Chinese- 
English parallel texts usually have a larger differ- 
ence ratio than English-French parallel texts. The 
filtering threshold had to be determined empirically, 
from the actual observations. For Chinese-English, 
a difference up to 50% is tolerated. 
2.5.2 Language and  Character Set 
It is also obvious that the two files of a pair have 
to be in the two languages of interest. By auto- 
matically identifying language and character set, we 
can filter out the pairs that do not satisfy this basic 
criterion. Some Web pages explicitly indicate the 
language and the character set. More often such 
information is omitted by authors. We need some 
language identification tool for this task. 
SILC is a language and encoding identification 
system developed by the RALI laboratory at the 
University of Montreal. It employs a probabilistic 
model estimated on tri-grams. Using these mod- 
els, the system is able to determine the most proba- 
ble language and encoding of a text (Isabelle et al, 
1997). 
2.5.3 HTML Structure and Alignment 
In the STRAND system (Resnik, 1998), the candi- 
date pairs are evaluated by aligning them according 
to their HTML structures and computing confidence 
values. Pairs are assumed to be wrong if they have 
too many mismatching markups or low confidence 
values. 
Comparing HTML structures seems to be a sound 
way to evaluate candidate pairs since parallel pairs 
usually have similar HTML structures. However, we 
also noticed that parallel texts may have quite dif- 
ferent HTML structures. One of the reasons is that 
the two files may be created using two HTML ed- 
itors. For example, one may be used for English 
and another for Chinese, depending on the language 
handling capability of the editors. Therefore, cau- 
tion is required when measuring structure difference 
numerically. 
Parallel text alignment is still an experimental 
area. Measuring the confidence values of an align- 
ment is even more complicated. For example, the 
alignment algorithm we used in the training of the 
statistical translation model produces acceptable 
alignment results but it does not provide a confi- 
dence value that we can "confidently" use as an eval- 
uation criterion. So, for the moment his criterion is 
not used in candidate pair evaluation. 
3 Generated  Corpus  and Trans la t ion  
Mode l  Tra in ing  
In this section, we describe the results of our parallel 
text mining and translation model training. 
3.1 The Corpus 
Using the above approach for Chinese-English, 185 
candidate sites were searched from the domain hk. 
We limited the mining domain to hk because Hong 
Kong is a bilingual English-Chinese city where high 
quality parallel Web sites exist. Because of the small 
number of candidate sites, the host crawler was used 
to thoroughly explore each site. The resulting cor- 
pus contains 14820 pairs of texts including 117.2Mb 
Chinese texts and 136.5Mb English texts. The entire 
mining process lasted about a week. Using length 
comparison and language identification, we refined 
the precision of the corpus to about 90%. The preci- 
sion is estimated by examining 367 randomly picked 
pairs. 
3.2 Statistical Translation Model 
Many approaches in computational linguistics try to 
extract ranslation knowledge from previous trans- 
lation examples. Most work of this kind establishes 
probabilistic models from parallel corpora. Based 
on one of the statistical models proposed by Brown 
et al (1993), the basic principle of our translation 
model is the following: given a corpus of aligned sen- 
tences, if two words often co-occur in the source and 
target sentences, there is a good likelihood that they 
are translations of each other. In the simplest case 
(model 1), the model earns the probability, p(tls), of 
having a word t in the translation of a sentence con- 
taining a word s. For an input sentence, the model 
then calculates a sequence of words that are most 
probable to appear in its translation. Using a sim- 
ilar statistical model, Wu (1995) extracted a large- 
scale English-Chinese l xicon from the HKUST cor- 
23  
<s id="00~"> 
<HTML> <HEAD> 
<META HTrP-EQUIV="Content-type" 
CONTENT="text/html; charset--iso-8859-1"> 
<META HTI'P-EQUIV="Content-language" 
CONTENT="Western"> 
</s> 
<s id="0001"> 
<TITLE>Journal of Primary Education 1996, 
VoI., No. l&2, pp. 19-27 </TITLE> 
</HEAD> 
</s> 
<s id="0002"> 
<BODY BACKGROUND=".Jgif/pejbg.jpg" 
TEXT="#000(3(O" BGCOLOR="#ffffff"> 
<CENTER> 
</s> 
<s id="0003"> 
<HI>Journal of Primary Education </HI> 
</s> 
<s id="0004"> 
<HR> <B>Volume 6, No l&2, pp. 19-27 (May, 
1996) </B> <HR> 
</s> 
<s id="0005"> 
<H3>Principles for Redesigning Teacher 
Education </H3> Alan TOM </CENTER> 
</s> 
<s id="0006"> 
<P> <B> <I> Abstract </I> </B> 
</s> 
<s id="0000"> 
<HTML> <HEAD> 
<META H'ITP-EQUW="Content-type" 
CONTENT="text/html; charset=bigS"> 
<META HTTP-EQUIV="Content-language" 
CONTENT="zh"> 
<Is> 
<s id="0001"> 
<TITLE> Journal of Primary Education 1996, 
Vol., No. l&2, Page 19-27 </TITLE> 
</HEAD> 
</s> 
<s id="0002"> 
<BODY BACKGROUND=".Jgif/pejbg.jpg" 
TEXT="#000000" BGCOLOR="#ffffff"> <A 
HREF="/erdpej/b2g__pej.phtml?URL=%2fen%2fp 
ej%2f0601%2f0601019c.htm"> 
<IMG SRC="/en/gif/kan.gif" ALT="~"  
BORDER=0 ALIGN=R IGHT> </A> <CENTER> 
</s> 
<s id="0003"> 
<H2>~ ~ 11I ~ O.</H2> 
</s> 
<s id="0004"> 
<HR> (~:~h-fv-c?.JLJl) ~,-\]'?~.. 
</s> 
<s id="0005"> 
~ 19-27\]~ <I-1R> 
</s> 
Figure 2: An alignment example using pure length-based method. 
pus which is built manually. In our case, the prob- 
abilistic translation model will be used for CLIR. 
The requirement on our translation model may be 
less demanding: it is not absolutely necessary that 
a word t with high p(tls ) always be a true trans- 
lation of s. It is still useful if t is strongly related 
to s. For example, although "railway" is not a true 
translation of "train" (in French), it is highly useful 
to include "railway" in the translation of a query on 
"train". This is one of the reasons why we think a 
less controlled parallel corpus can be used to train a 
translation model for CLIR. 
3.3 Parallel Text Al ignment 
Before the mined documents can be aligned into par- 
allel sentences, the raw texts have to undergo a se- 
ries of some preprocessing, which, to some extent, is 
language dependent. For example, the major opera- 
tions on the Chinese-English corpus include encod- 
ing scheme transformation (for Chinese), sentence 
level segmentation, parallel text alignment, Chinese 
word segmentation (Nie et al, 1999) and English 
expression extraction. 
The parallel Web pages we collected from vari- 
ous sites are not all of the same quality. Some are 
highly parallel and easy to align while others can be 
very noisy. Aligning English-Chinese parallel texts 
is already very difficult because of the great differ- 
ences in the syntactic structures and writing sys- 
tems of the two languages. A number of alignment 
techniques have been proposed, varying from statis- 
tical methods (Brown et al, 1991; Gale and Church, 
1991) to lexical methods (Kay and RSscheisen, 1993; 
Chen, 1993). The method we adopted is that of 
Simard et al (1992). Because it considers both 
length similarity and cognateness as alignment cri- 
teria, the method is more robust and better able 
to deal with noise than pure length-based methods. 
Cognates are identical sequences of characters in cor- 
responding words in two languages. They are com- 
monly found in English and French. In the case of 
English-Chinese alignment, where there are no cog- 
nates shared by the two languages, only the HTML 
markup in both texts are taken as cognates. Be- 
cause the HTML structures of parallel pages are nor- 
mally similar, the markup was found to be helpful 
for alignment. 
To illustrate how markup can help with the align- 
ment, we align the same pair with both the pure 
length-based method of Gale & Church (Fig. 2), 
and the method of Simard et al (Fig. 3). First of 
all, we observe from the figures that the two texts are 
24
<s id="0000"> 
<HTML> <HEAD> 
<META HTTP-EQUIV="Content-type" 
CONTENT="text/html; charset=iso-8859-1 "> 
<META HTTP-EQUIV="Content-language" 
CONTENT="Westem"> 
</s> 
<s id="0001"> 
<TITLE>Journal of Primary Education 1996, 
Vol., No. l&2, pp. 19-27 </TITLE> 
</HEAD> 
</s> 
<s id="0002"> 
<BODY BACKGROUND=-". Jgif/pejbg.jpg" 
TEXT="#000000" BGCOLOR="#ffffff"> 
<CENTER> 
</s> 
<s id="0003"> 
<H 1 >Journal of Primary Education </H 1 > 
<Is> 
<s id="0004"> 
<HR> <B>Volume 6,No l&2, pp. 19-27 (May, 
1996) </B> <HR> 
</$> 
<s id="0000"> 
<HTML> <HEAD> 
<META HTrP-EQUIV="Content-type" 
CONTENT="text/html; charset=big5"> 
<META H'lTP-EQUIV="Content-language" 
CONTENT="zh"> 
<Is> 
<s id="0001"> 
:<TITLE> Journal of Primary Education 1996, 
Vol., No. l&2, Page 19-27 </TITLE> 
</HEAD> 
</s> 
<s id="0002"> 
<BODY BACKGROUND=-". Jgiffpejbg.jpg" 
TEXT="#O00000" BGCOLOR="#fffffff> <A 
HREF="/ergpej/b2g_pej.phtml?URL=%2fen%2fp 
ej %2f0601%2 f0601019c.htm"> 
<IMG SRC="/erdgif/kan.gif" ALT="~k" 
BORDER={) ALIGN=R IGHT> </A> <CEHTEIL~ 
</s> 
<s id="0003"> 
<H2>~k ~ ~ ~\[1.</H2> 
</s> 
<s id="0004"> 
<HR> (~t~-~?-#cJL.~) ,-~?~. 
</s> 
<s id="0005"> 
~ $ ~  19-27 \]~ <HR> 
<\]s> 
<s id="0005"> <s id="0006"> 
<H3>Principles for Redesigning Teacher <H3>.~ k~4Vt ~'~ ~ ~J </H3> Alan TOM 
Education </H3> Alan TOM </CENTER> </CENTER> 
<Is> <Is> 
<s id="0006"> <s id="0007"> 
<P> <B> <I> Abstract </I> </B> <P> <I> <B> ~4\[- </B> </I> <P> 
</s> </s> 
Figure 3: An alignment example considering cognates. 
divided into sentences. The sentences are marked by 
<s id="xxxx"> and </s>.  Note that we determine 
sentences not only by periods, but also by means of 
HTML markup. 
We further notice that it is difficult to align sen- 
tences 0002. The sentence in the Chinese page is 
much longer than its counterpart in the English page 
because some additional information (font) is added. 
The length-based method thus tends to take sen- 
tence 0002, 0003, and 0004 in the English page as 
the translation of sentence 0002 in the Chinese page 
(Fig. 2), which is wrong. This in turn provocated 
the three following incorrect alignments. As we can 
see in Fig. 3, the cognate method did not make the 
same mistake because of the noise in sentence 0002. 
Despite their large length difference, the two 0002 
sentences are still aligned as a 1-1 pair, because the 
sentences in the following 4 alignments (0003 - 0003; 
0004 - 0004, 0005; 0005 - 0006; 0006 - 0007) have 
rather similar HTML markups and are taken by the 
program to be the most likely alignments. 
Beside HTML markups, other criteria may also 
be incorporated. For example, it would be helpful 
to consider strong correspondence b tween certain 
English and Chinese words, as in (Wu, 1994). We 
hope to implement such correspondences in our fu- 
ture research. 
3.4 Lex icon  Eva luat ion  
To evaluate the precision of the English-Chinese 
translation model trained on the Web corpus, we 
examined two sample lexicons of 200 words, one in 
each direction. The 200 words for each lexicon were 
randomly selected from the training source. We ex- 
amined the most probable translation for each word. 
The Chinese-English lexicon was found to have a 
precision of 77%. The English-Chinese l xicon has 
a higher precision of 81.5%. Part of the lexicons 
are shown in Fig. 4, where t / f  indicates whether a 
translation is true or false. 
These precisions seem to be reasonably high. 
They are quite comparable to that obtained by Wu 
(1994) using a manual Chinese-English parallel cor- 
pus. 
3.5 Effect  o f  S topwords  
We also found that stop-lists have significant effect 
on the translation model. Stop-list is a set of the 
most frequent words that we remove from the train- 
2fi 
English word 
a .n l .  
access 
adaptation 
add 
adopt 
agent 
agree 
airline 
amendment 
, appliance 
apply 
attendance 
auditor 
- ,average 
base_on 
t/f 
t 
f 
t 
t 
t 
t 
t 
t 
t 
t 
t 
t 
f 
t 
f 
Translmion Probability Chinese word 
~'~- 0.201472 ~t l :  
~"  0.071705 "~"  
~f~.,~ 0.179633 JllL~ 
0.317435 
~ 0.231637 ~.~ 
1~tA~ 0.224902 4J~'~ 
0.36569 
0.344001 
0.367518 
J~ 4~ 0.136319 
i~.~I 0.19448 J~  
~',1~ 0.171769 ,~- JJ~ 
*~ 0.15011 -~-~ 
~- ~ 0.467646 * *~ 
0.107304 
Figure 4: Part of the evaluation lexicons. 
t/f 
t 
t 
t 
t 
t 
f 
t 
f 
t 
t 
t 
t 
t 
t 
t 
Translation Probability 
office 0.375868 
protection 0.343071 
report 0.358592 
prepare 0.189513 
loca l  0.421837 
follow 0.023685 
standard 0.445453 
adu l t  0.044959 
inadequate 0.093012 
part 0.313676 
financial 0.16608 
visit 0.309642 
bill 0.401997 
vehicle 0.467034 
saving 0.176695 
Figure 5: Effect of stop lists in C-E translation. 
ing source. Because these words exist in most align- 
ments, the statistical model cannot derive correct 
translations for them. More importantly, their ex- 
istence greatly affects the accuracy of other transla- 
tions. They can be taken as translations for many 
words. 
A priori, it would seem that both the English and 
Chinese stop-lists hould be applied to eliminate the 
noise caused by them. Interestingly, from our ob- 
servation and analysis we concluded that for better 
precision, only the stop-list of the target language 
should be applied in the model training. 
We first explain why the stop-list of the target lan- 
guage has to be applied. On the left side of Fig. 5, 
if the Chinese word C exists in the same alignments 
with the English word E more than any other Chi- 
nese words, C will be the most probable translation 
for E. Because of their frequent appearance, some 
Chinese stopwords may have more chances to be in 
the same alignments with E. The probability of the 
translation E --+ C is then reduced (maybe ven less 
than those of the incorrect ones). This is the reason 
why many English words are translated to "~ '  (of) 
by the translation model trained without using the 
Chinese stop-list. 
We also found that it is not necessary to remove 
the stopwords of the source language. In fact, as il- 
lustrated on the right side of Fig. 5, the existence of 
the English stopwords has two effects on the proba- 
bility of the translation E -~ C: 
1 They may often be found together with the Chi- 
nese word C. Owing to the Expectation Maxi- 
mization algorithm, the probability of E -~ C 
may therefore be reduced. 
2 On the other hand, there is a greater likelihood 
that English stopwords will be found together 
with the most frequent Chinese words. Here, 
we use the term "Chinese frequent words" in- 
stead of "Chinese stopwords" because ven if a 
stop-list is applied, there may still remain some 
common words that have the same effect as the 
stopwords. The coexistence ofEnglish and Chi- 
nese frequent words reduces the probability that 
the Chinese frequent words are the translations 
of E, and thus raise the probability of E -+ C. 
The second effect was found to be more signifi- 
cant than the first, since the model trained without 
the English stopwords has better precision than the 
model trained with the English stopwords. For the 
correct ranslations given by both models, the model 
26
Mono-Lingual IR 
Translation Model 
Dictionary 
C-E CLIR 
0.3861 
0.1504 (39.0%mono) 
0.1530 (39.6%mono) 
0.2583 (66.9%mono) 
E-C CLIR 
0.3976 
0.1841 (46.3%mono) 
0.1427 (35.9%mono) 
0.2232 (56.1%mono) 
Table 1: CLIR results. 
trained without considering the English stopwords 
gives higher probabilities. 
4 Eng l i sh -Ch inese  CL IR  Resu l ts  
Our final goal was to test the performance of the 
translation models trained on the Web parallel cor- 
pora in CLIR. We conducted CLIR experiments u - 
ing the Smart IR system. 
4.1 Results  
The English test corpus (for C-E CLIR) was the 
AP corpus used in TREC6 and TREC7. The short 
English queries were translated manually into Chi- 
nese and then translated back to English by the 
translation model. The Chinese test corpus was the 
one used in the TREC5 and TREC6 Chinese track. 
It contains both Chinese queries and their English 
translations. 
Our experiments on these two corpora produced 
the results hown in Tab. 1. The precision of mono- 
lingual IR is given as benchmark. In both E-C and 
C-E CLIR, the translation model achieved around 
40% of monolingual precision. To compare with the 
dictionary-based approach, we employed a Chinese- 
English dictionary, CEDICT (Denisowski, 1999), 
and an English-Chinese online dictionary (Anony- 
mous, 1999a) to translate queries. For each word 
of the source query, all the possible translations 
given by the dictionary are included in the translated 
query. The Chinese-English dictionary has about 
the same performace as the translation model, while 
the English-Chinese dictionary has lower precision 
than that of the translation model. 
We also tried to combine the translations given by 
the translation model and the dictionary. In both 
C-E and E-C CLIR, significant improvements were 
achieved (as shown in Tab. 1). The improvements 
show that the translations given by the translation 
model and the dictionary complement each other 
well for IR purposes. The translation model may 
give either exact ranslations orincorrect but related 
words. Even though these words are not correct in 
the sense of translation, they are very possibly re- 
lated to the subject of the query and thus helpful 
for IR purposes. The dictionary-based approach ex- 
pands a query along another dimension. It gives 
all the possible translations for each word including 
those that are missed by the translation model. 
4.2 Comparison Wi th  MT Systems 
One advantage of a parallel text-based translation 
model is that it is easier to build than an MT system. 
Now that we have examined the CLIR performance 
of the translation model, we will compare it with 
two existing MT systems. Both systems were tested 
in E-C CLIR. 
4.2.1 Sunshine WebTran Server 
Using the Sunshine WebTran server (Anonymous, 
1999b), an online Engiish-Chinese MT system, to 
translate the 54 English queries, we obtained an 
average precision of 0.2001, which is 50.3% of the 
mono-lingual precision. The precision is higher than 
that obtained using the translation model (0.1804) 
or the dictionary (0.1427) alone, but lower than the 
precison obtained using them together (0.2232). 
4.2.2 Transperfect 
Kwok (1999) investigated the CLIR performance of
an English-Chinese MT software called Transper- 
fect, using the same TREC Chinese collection as we 
used in this study. Using the MT software alone, 
Kwok achieved 56% of monolingual precision. The 
precision is improved to 62% by refining the trans- 
lation with a dictionary. Kwok also adopted pre- 
translation query expansion, which further improved 
the precison to 70% of the monolingual results. 
In our case, the best E-C CLIR precison using the 
translation model (and dictionary) is 56.1%. It is 
lower than what Kwok achieved using Transperfect, 
however, the difference is not large. 
4.3 Further  Problems 
The Chinese-English translation model has a fax 
lower CLIR performance than that of the English- 
French model established using the same method 
(Nie et al, 1999). The principal reason for this is the 
fact that English and Chinese are much more differ- 
ent than English and French. This problem surfaced 
in many phases of this work, from text alignment to 
query translation. Below, we list some further fac- 
tors affecting CLIR precision. 
? The Web-collected corpus is noisy and it is dif- 
ficult to align English-Chinese t xts. The align- 
ment method we employed has performed more 
poorly than on English-French alignment. This 
in turn leads to poorer performance of the trans- 
lation model. In general, we observe a higher 
27 
variability in Chinese-English translations than 
in English-French translations. 
? For E-C CLIR, although queries in both lan- 
guages were provided, the English queries were 
not strictly translated from the original Chi- 
nese ones. For example, A Jg ,~ (human right 
situation) was translated into human right is- 
sue. We cannot expect he translation model 
to translate issue back to ~ (situation). 
? The training source and the CLIR collections 
were from different domains. The Web cor- 
pus are retrieved from the parallel sites in Hong 
Kong while the Chinese collection is from Peo- 
ple's Daily and Xinhua News Agency, which are 
published in mainland China. As the result, 
some important erms such as ~$ $ (most- 
favored-nation) and --- I!! ~ ~ (one-nation-two- 
systems) in the collection are not known by the 
model. 
5 Summary  
The goal of this work was to investigate he feasibil- 
ity of using a statistical translation model trained on 
a Web-collected corpus to do English-Chinese CLIR. 
In this paper, we have described the algorithm and 
implementation we used for parallel text mining, 
translation model training, and some results we ob- 
tained in CLIR experiments. Although further work 
remains to be done, we can conclude that it is pos- 
sible to automatically construct a Chinese-English 
parallel corpus from the Web. The current system 
can be easily adapted to other language pairs. De- 
spite the noisy nature of the corpus and the great 
difference in the languages, the evaluation lexicons 
generated by the translation model produced accept- 
able precision. While the current CLIR results are 
not as encouraging asthose of English-French CLIR, 
they could be improved in various ways, such as im- 
proving the alignment method by adapting cognate 
definitions to HTML markup, incorporating a lexi- 
con and/or removing some common function words 
in translated queries. 
We hope to be able to demonstrate in the near 
future that a fine-tuned English-Chinese translation 
model can provide query translations for CLIR with 
the same quality produced by MT systems. 
Re ferences  
Anonymous. 1999a. Sunrain.net - English-Chinese 
dictionary, http://sunrain.net/r_ecdict _e.htm. 
Anonymous. 1999b. Sunshine WebTran server. 
http://www.readworld.com/translate.htm. 
P. F. Brown, J. C. Lai, and R. L. Mercer. 1991. 
Aligning sentences in parallel corpora. In 29th 
Annual Meeting of the Association for Computa- 
tional Linguistics, pages 89-94, Berkeley, Calif. 
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, 
and R. L. Mercer. 1993. The mathematics of ma- 
chine translation: Parameter estimation. Compu- 
tational Linguistics, 19:263-311. 
S. F. Chen. 1993. Aligning sentences in bilingual 
corpora using lexical information. In Proceedings 
of the 31th Annual Meeting of the Association for 
Computational Linguistics, pages 9-16, Colum- 
bus, Ohio. 
Paul Denisowski. 1999. Cedict (chinese-english dic- 
tionary) project, http://www.mindspring.com/ 
paul_denisowski/cedict.html. 
William A. Gale and Kenneth W. Church. 1991. A 
program for aligning sentences in bilingual cor- 
pora. In Proceedings of the 29th Annual Meeting 
of the Association for Computational Linguistics, 
pages 177-184, Berkeley, Calif. 
P. Isabelle, G. Foster, and P. Plamondon. 
1997. SILC: un syst~me d'identification 
de la langue et du codage, http://www- 
rali.iro.umontreal.ca/ProjetSILC.en.html. 
M. Kay and M. RSscheisen. 1993. Text-translation 
alignment. Computational Linguistics, 19:121- 
142. 
K. L. Kwok. 1999. English-chinese cross-language 
retrieval based on a translation package. In Work- 
shop of Machine Translation for Cross Language 
Information Retrieval, Machine Translation Sum- 
mit VII, Singapore. 
P. Langlais, G. Foster, and G. Lapalme. 2000. Unit 
completion for a computer-aided translation typ- 
ing system. In Applied Natural Language Pro- 
cessing Conference (ANLP), Seattle, Washington, 
May. 
Jianyun Nie, Michel Simard, Pierre Isabelle, and 
Richard Durand. 1999. Cross-language informa- 
tion retrieval based on parallel texts and auto- 
matic mining parallel texts from the Web. In 
ACM SIGIR '99, pages 74-81, August. 
Philip Resnik. 1998. Parallel stands: A preliminary 
investigation i to mining the Web for bilingual 
text. In AMTA '98, October. 
Michel Simard, George F. Foster, and Pierre Is- 
abelle. 1992. Using cognates to align sentences 
in bilingual corpora. In Proceedings of TMI-92, 
Montreal, Quebec. 
Dekai Wu. 1994. Aligning a parallel English- 
Chinese corpus statistically with lexical criteria. 
In ACL-9$: 32nd Annual Meeting of the Assoc. 
for Computational Linguistics, pages 80-87, Las 
Cruces, NM, June. 
Dekai Wu. 1995. Large-scale automatic extraction 
of an English-Chinese l xicon. Machine Transla- 
tion, 9(3-4):285-313. 
28 
A Comparative Study for Query Translation using 
Linear Combination and Confidence Measure 
Youssef Kadri 
Laboratoire RALI, DIRO 
Universit? de Montr?al 
CP 6128, Montr?al, Canada, H3C3J7 
kadriyou@iro.umontreal.ca
Jian-Yun Nie 
Laboratoire RALI, DIRO 
Universit? de Montr?al 
CP 6128, Montr?al, Canada, H3C3J7 
nie@iro.umontreal.ca 
 
 
Abstract 
In Cross Language Information Retrieval 
(CLIR), query terms can be translated to 
the document language using Bilingual 
Dictionaries (BDs) or Statistical Transla-
tion Models (STMs). Combining different 
translation resources can also be used to 
improve the performance. Unfortunately, 
the most studies on combining multiple re-
sources use simple methods such as linear 
combination. In this paper, we drew up a 
comparative study between linear combina-
tion and confidence measures to combine 
multiple translation resources for the pur-
pose of CLIR. We show that the linear 
combination method is unable to combine 
correctly different types of resources such 
as BDs and STMs. While the confidence 
measure method is able to re-weight the 
translation candidate more radically than in 
linear combination. It reconsiders each 
translation candidate proposed by different 
resources with respect to additional fea-
tures. We tested the two methods on differ-
ent test CLIR collections and the results 
show that the confidence measure outper-
forms the linear combination method. 
1 Introduction 
Cross Language Information Retrieval (CLIR) tries 
to determine documents written in a language from 
a query written in another language. Query transla-
tion is widely considered as the key problem in this 
task (Oard, 1998). In previous researches, various 
approaches have been proposed for query transla-
tion: using a bilingual dictionary, using an off-the-
shelf machine translation system or using a parallel 
corpus. It is also found that when multiple transla-
tion resources are used, the translation quality can 
be improved, comparing to using only one transla-
tion resource (Xu, 2005). Indeed, every translation 
tool or resource has its own limitations. For exam-
ple, a bilingual dictionary can suggest common 
translations, but they remain ambiguous ? transla-
tions for different senses of the source word are 
mixed up. Machine translation systems usually 
employ sophisticated methods to determine the 
best translation sentence, for example, syntactic 
analysis and some semantic analysis. However, it 
usually output only one translation for a source 
word, while it is usually preferred that a source 
query word be translated by multiple words in or-
der to produce a desired query expansion effect. In 
addition, the only word choice made by a machine 
translation system can be wrong. Finally, parallel 
corpora contain useful information about word 
translation in particular areas. One can use such a 
corpus to train a statistical translation model, 
which can then be used to translate a query. This 
approach has the advantage that few manual inter-
ventions are required to produce the statistical 
translation model. In addition, each source word 
can be translated by several related target words 
and the latter being weighted. However, among the 
proposed translation words, there may be irrelevant 
ones. 
Therefore, one can take advantage of several 
translation resources and tools in order to produce 
better query translations. The key problem is the 
way to combine the resources.  
A common method used in previous studies is to 
assign a weight to each resource. Then all the 
translation candidates are weighted and then com-
bined linearly (Nie, 2000). However, this kind of 
combination assigns a single confidence score to 
181
all the translations from the same translation re-
source. In reality, a translation resource does not 
cover all the words with equal confidence. For 
some words, its translations can be accurate, while 
for some others, they are inappropriate. By using a 
linear combination, the relative order among the 
translation candidates is not changed. In practice, a 
translation with a low score can turn out to be a 
better translation when other information becomes 
available. 
For example, the English word ?nutritional? is 
translated into French by a statistical translation 
model trained on a set of parallel texts as follows: 
{nutritive 0.32 (nutritious), alimentaire 0.21 (food)}. 
We observe that the most common translation 
word ?alimentaire? only takes the second place 
with lower probability than ?nutritive?. If these 
translations are combined linearly with another 
resource (say a BD), it is unlikely that the correct 
translation word ?alimentaire? gain larger weight 
than ?nutritive?.  
This example shows that we have to reconsider 
the relative weights of the translation candidates 
when another translation resource is available. The 
purpose of this reconsideration is to determine how 
reasonable a translation candidate is given all the 
information now available. In so doing, the initial 
ranking of translation candidates can be changed. 
As a matter of fact using the method of confidence 
measures that we propose in this paper, we are able 
to reorder the translation candidates as follows:  
  {alimentaire 0.38, nutritive 0.23, valeur 0.11 (value)}. 
The weight of the correct translation ?alimen-
taire? is considerably increased.  
In this paper, we will propose to use a new 
method based on confidence measure to re-weight 
the translation candidates. In the re-weighting, the 
original weight according to each translation re-
source is only considered as one factor. The final 
weight is determined by combining all the avail-
able factors. In our implementation, the factors are 
combined in neural networks, which produce a fi-
nal confidence measure for each of the translation 
candidates. This final weight is not a simple linear 
combination of the original weights, but a re-
calculation according to all the information avail-
able, which is not when each translation resource is 
estimated separately. 
The advantages of this approach are twofold. On 
one hand, the confidence measure allows us to ad-
just the original weights of the translations and to 
select the best translation terms according to all the 
information. On the other hand, the confidence 
measures also provide us with a new weighting for 
the translation candidates that are comparable 
across different translation resources. Indeed, when 
we try to combine a statistical translation model 
with a bilingual dictionary, we had to assign a 
weight to a candidate from the bilingual dictionary. 
This weight is not directly compatible with the 
probability assigned in the former. 
In the remaining sections of this paper, we will 
first describe the principle of confidence measure 
in section 2. In section 3, we will compare two 
methods to combine different translation resources: 
linear combination and confidence measure. Sec-
tion 4 provides a description on how the parame-
ters are tuned. Section 5 outlines the different steps 
for computing confidence measures. Finally, we 
present the results of our experiments on both Eng-
lish-French and English-Arabic CLIR. Our ex-
periments will show that the method using confi-
dence measure significantly outperforms the tradi-
tional approach using linear combination. 
2 Confidence measure 
Confidence measure is often used to re-rank or re-
weight some outputs produced by separate means. 
For example, in speech recognition and under-
standing (Hazen et  al., 2002), one tries to re-rank 
the result of speech recognition according to addi-
tional information using confidence measure. Gan-
drabur et al (2003) used confidence measures in a 
translation prediction task. The goal is to re-rank 
the translation candidates according to additional 
information. Confidence measure is defined as the 
probability of correctness of a candidate. In the 
case of translation, given a candidate translation tE 
for a source word tF, the confidence measure is 
),,|( FttcorrectP EF , where F is a set of other fea-
tures of the translation context (e.g. the POS-tag of 
the word, the previous translations words, etc.). In 
both applications, significant gains have been ob-
served when using a confidence estimation layer 
within the translation models. 
The problem of query translation is similar to 
general translation described in (Gandrabur et al 
2003). We are presented with several translation 
resources, each being built separately. Our goal 
now is to use all of them together. As we discussed 
earlier, we want to take advantage of the additional 
information (other translation resources as well as 
182
additional linguistic analysis on the query) in order 
to re-weight each of the translation candidates. 
In previous studies, neural networks have been 
commonly used to produce confidence measures. 
The inputs to the neural networks are translation 
candidates from different resources, their original 
weights and various other properties of them (e.g. 
POS-tag, probability in a language model, etc.). 
The output of the neural networks is a confidence 
measure assigned to a translation candidate from a 
translation resource. This confidence measure is 
used to re-rank the whole set of candidates from all 
the resources. 
In this study, we will use the same approach to 
combine different translation resources and to pro-
duce confidence measures. 
The neural networks need to be trained on a set 
of training data. Such data are available in both 
speech recognition and machine translation. How-
ever, in the case of CLIR, the goal of query transla-
tion is not strictly equivalent to machine transla-
tion. Indeed, in query translation, we are not lim-
ited to the correct literal translations. Not literal 
translation words that are strongly related to the 
query are also highly useful. These latter related 
words can produce a desired query expansion ef-
fect in IR.  
Given this situation, we can no longer use a par-
allel corpus as our training data as in the case of 
machine translation. Modifications are necessary. 
We will describe the modified way we use to cre-
ate the training data in section 4. The informative 
features we use will be described n section 5.2. 
3 General CLIR Problem 
Assume a query QE written in a source language E 
and a document DF written in a target language F, 
we would like to determine a score of relevance of 
DF to QE. However, as they are not directly compa-
rable, a form of translation is needed. Let us de-
scribe the model that we will use to determine its 
score. 
Various theoretical models have been developed 
for IR, including vector space model, Boolean 
model and probabilistic model. Recently, language 
modeling is widely used in IR, and it has been 
show to produce very good experimental results. In 
addition, language modeling also provides a solid 
theoretical framework for integrating more aspects 
in IR such as query translation. Therefore, we will 
use it as our basic framework in this study. 
In language modeling framework, the relevance 
score of the document DF to the query QE is deter-
mined as the negative KL-divergence between the 
query?s language model and the document?s lan-
guage model (Zhai, 2001a). It is defined as fol-
lows: 
??
Ft
FFEFFE DtpQtpDQR )|(log)|(),(             (1) 
To avoid the problem of attributing zero prob-
ability to query terms not occurring in document 
DF, smoothing techniques are used to estimate 
p(tF|DF). One can use the Jelinek-Mercer smooth-
ing technique which is a method of interpolating 
between the document and collection language 
models (Zhai, 2001b). The smoothed p(tF|DF) is 
calculated as follows: 
)|()|()1()|( FFMLFFMLFF CtpDtpDtp ?? +?=         (2) 
where
||
),(
)|(
F
FF
FFML D
Dttf
Dtp = and
||
),(
)|(
F
FF
FFML C
Cttf
Ctp =  
are the maximum likelihood estimates of a uni-
gram language model based on respectively the 
given document DF and the collection of docu-
ments CF. ? is a parameter that controls the influ-
ence of each model. 
In CLIR, the term )|( EF Qtp in equation (1) rep-
resenting the query model can be estimated as fol-
lows: ?? ==
EE q
EEEEF
q
EEFEF QqpQqtpQqtpQtp )|(),|()|,()|(  
                       ??
Eq
EEMLEF Qqpqtp )|()|(             (3) 
where )|( EEML Qqp  is the maximum likelihood 
estimation:
||
),(
)|(
E
EE
EEML Q
Qqtf
Qqp = and )|( EF qtp is 
the translation model. Putting (3) in (1), we obtain 
the general CLIR score formula: ???
F Et
FF
q
EEMLEFFE DtpQqpqtpDQR )|(log)|()|(),(  (4) 
In our work, we do not change the document 
model )|( FF Dtp  from monolingual IR. Our focus 
will be put on the estimation of the translation 
model )|( EF qtp - the translation probability from a 
source query term qE to a target word tF, in particu-
lar, when several translation resources are avail-
able.  
Let us now describe two different ways to com-
bine different translation resources for the estima-
tion of )|( EF qtp : by linear combination and by con-
fidence measure.  
183
4 Linear Combination 
The first intuitive method to combine different 
translation resources is by a linear combination. 
This means that the final translation model is esti-
mated as follows:  
?=
i
EFiiqEF qtpzqtp E )|()|( ?                  (5) 
where ?i is the parameter assigned to the transla-
tion resource i and 
Eq
z is a normalization factor so 
that 1)|( =?
Ft
EF qtp . )|( EFi qtp is the probability 
of translating the source word qE to the target word 
tF  by the resource i. 
In order to determine the appropriate parameter 
for each translation resource, we use the EM algo-
rithm to find values which maximize the log-
likelihood LL of a set C of training data according 
to the combined model, i.e.: 
)()|(log),()(
),(
||
1 1
||
1
iijk
Cef
f
j
n
k
e
i
k epeftefpCLL ? ? ??
? = = =
= ?  (6) 
Where (f, e)?C is a pair of parallel sentences; 
||
),(#
),(
C
ef
efp = is the prior probability of the pair of 
sentences (f, e) in the corpus C, |f| is the length of 
the target sentence f and |e| is the length of the 
source sentence e. ?k is the coefficient related to 
resource k that we want to optimize and n is the 
number of resources. tk(fj|ei) is the probability of 
translating the source word ei with the target word 
fj with each resource. p(ei) is the prior probability 
of the source word ei in the corpus C. Note that the 
validation data set C on which we optimize the 
parameters must be different from the one used to 
train our baseline models.  
The training corpora are as follows: For English-
Arabic, we use the Arabic-English parallel news 
corpus1. This corpus consists of around 83 K pairs 
of aligned sentences. For English-French, we use a 
bitext extracted from two parallel corpora: The 
Hansard 2  corpus and the Web corpus (Kadri, 
2004). It consists of around 60 K pairs of aligned 
sentences.  
                                                 
1 http://www.ldc.upenn.edu/ 
Arabic-English Parallel News Part 1 (LDC2004T18) 
2 LDC provides a version of this corpus: 
http://www.ldc.upenn.edu/. 
The component models for English-Arabic 
CLIR are: a STM built on a set of parallel Web 
pages (Kadri, 2004), another STM built on the 
English-Arabic United Nations corpus (Fraser, 
2002), Ajeeb3 bilingual dictionary and Almisbar4 
bilingual dictionary. For English-French CLIR, we 
use three component models: a STM built on Han-
sard corpus, another STM built on parallel Web 
pages and the Freedict5 bilingual dictionary.  
5 Using Confidence Measures 
The question considered in confidence measure is: 
Given a translation candidate, is it correct and how 
confident are we on its correctness? 
Confidence measure aims to answer this ques-
tion. Given a translation candidate tF for a source 
term qE and a set F of other features, confidence 
measure corresponds to ),,|1( FqtCp EFi = . We can 
use this measure as an estimate of )|( EF qtp , i.e.: ? ==
i
EFiqEF FqtCpzqtp E ),,|1()|(        (7) 
where F is the set of features that we use. We will 
see several features to help determine the confi-
dence measure of a translation candidate, for ex-
ample, the translation probability, the reverse 
translation probability, language model features, 
and so on. We will describe these features in more 
detail in section 5.2. 
In general, we can consider confidence measure 
as P(C=1|X), given X? the source word, a transla-
tion and a set of features. We use a Multi Layer 
Perceptron (MLP) to estimate the probability of 
correctness P(C=1|X) of a translation. Neural net-
works have the ability to use input data of different 
natures and they are well-suited for classification 
tasks.  
Our training data can be viewed as a set of pairs 
(X,C), where X is a vector of features relative to a 
translation6 used as the input of the network, and C 
is the desired output (the correctness of the transla-
tion 0/1). The MLP implements a non-linear map-
ping of the input features by combining layers of 
linear transformation and non-linear transfer func-
tion. Formally, the MLP implements a discriminant 
function for an input X of the form: 
                                                 
3 http://www.ajeeb.com/ 
4 http://www.almisbar.com/ 
5 http://www.freedict.com/ 
6 By translation, we mean the pair of source word and its 
translation. 
184
     ))(();( XWhVoXg ??=?                             (8) 
where ? ={W,V}, W is a matrix of weights be-
tween input and hidden layers and V is a vector of 
weights between hidden and output layers; h is an 
activation function for the hidden units which non-
linearly transforms the linear combination of in-
puts XW ? ; o is also a non-linear activation func-
tion but for the output unit, that transforms the 
MLP output to the probability estimate P(C=1|X). 
Under these conditions, our MLP was trained to 
minimize an objective function of error rate (Sec-
tion 4.1).  
In our experiments, we used a batch gradient de-
scent optimizer. During the test stage, the confi-
dence of a translation X is estimated with the 
above discriminant function g(X; ?); where ? is the 
set of weights optimized during the learning stage. 
These parameters are expected to correlate with the 
true probability of correctness P(C=1|X).  
5.1 The objective function to minimize 
A natural metric for evaluating probability esti-
mates is the negative log-likelihood (or cross en-
tropy CE) assigned to the test corpus by the model 
normalized by the number of examples in the test 
corpus (Blatz et al, 2003). This metric evaluates 
the probabilities of correctness. It measures the 
cross entropy between the empirical distribution on 
the two classes (correct/incorrect) and the confi-
dence model distribution across all the examples 
X(i) in the corpus. Cross entropy is defined as fol-
lows: 
     ??=
i
ii
n XCPCE )|(log
)()(1                     (9) 
where C(i) is 1 if the translation X(i) is correct, 0 
otherwise. To remove dependence on the prior 
probability of correctness, Normalized Cross En-
tropy (NCE) is used: 
bb CECECENCE )( ?=                              (10) 
The baseline CEb is a model that assigns fixed 
probabilities of correctness based on the empirical 
class frequencies: 
)/log()/()/log()/( 1100 nnnnnnnnCEb ??=         (11) 
where n0 and n1 are the numbers of correct and in-
correct translations among n test cases. 
5.2 Features 
The MLP tends to capture the relationship between 
the correctness of the translation and the features, 
and its performance depends on the selection of 
informative features.  
We selected intuitively seven classes of features 
hypothesized to be informative for the correctness 
of a translation.  
Translation model index: an index represent-
ing the resource of translation that produced the 
translation candidate. 
Translation probabilities: the probability of 
translating a source word with a target word. These 
probabilities are estimated with IBM model 1 
(Brown et al, 1993) on parallel corpora. For trans-
lations from bilingual dictionaries, as no probabil-
ity is provided, we carry out the following process 
to assign a probability to each translation pair (e, f) 
in a bilingual dictionary: We trained a statistical 
translation model on a parallel corpus. Then for 
each translation pair (e,f) of the bilingual diction-
ary, we looked up the resulting translation model 
and extracted the probability assigned by this 
translation model to the translation pair in ques-
tion. Finally, the probability is normalized by the 
Laplace smoothing method: 
?
=
+
+= n
i
iSTM
STM
BD
efp
efp
efp
1
1)|(
1)|(
)|(
                     (12) 
Where n is the number of translations proposed by 
the bilingual dictionary to the word e. 
Translation ranking: This class of features in-
cludes two features: The rank of the translation 
provided by each resource and the probability dif-
ference between the translation and the highest 
probability translation. 
Reverse translation information: This in-
cludes the probability of translation of a target 
word to a source word. Other features measure the 
rank of source word in the list of translations of the 
target word and if the source word holds in the best 
translations of the target word. 
Translation ?Voting?: This feature aims to 
know whether the translation is voted by more than 
one resource. The more a same translation is voted 
the more likely it may be correct. 
Source sentence-related features: One feature 
measures the frequency of the source word in the 
source sentence. Another feature measures the 
number of source words in the source sentence that 
have a translation relation with the translation in 
question. 
185
Language model features: We use the uni-
gram, the bigram and the trigram language models 
for source and target words on the training data. 
5.3 Training for confidence measures 
The corpus used for training confidence is the 
same as the corpus for tuning parameters for the 
linear combination. It is a set of aligned sentences. 
Source sentences are translated to the target lan-
guage word by word using baseline models. We 
translated each source word with the most prob-
able7 translations for the translation models and the 
best five translations provided by the bilingual dic-
tionaries. Translations are then compared to the 
reference sentence to build a labeled corpus: a 
translation of a source word is considered to be 
correct if it occurs in the reference sentence. The 
word order is ignored, but the number of occur-
rences is taken into account. This metric fits well 
our context of IR: IR models are based on ?bag of 
words? principle and the order of words is not con-
sidered. 
We test with various numbers of hidden units 
(from 5 to 100). We used the NCE metric to com-
pare the performance of different architectures. 
The MLP with 50 hidden units gave the best per-
formance. 
To test the performance of individual features, 
we experimented with each class of features alone. 
The best features are the translation ?voting?, lan-
guage model features and the translation probabili-
ties. The translation ?voting? is very informative 
because it presents the translation probability at-
tributed by each resource to the translation in ques-
tion. The translation ranking, the reverse transla-
tion information, the translation model index and 
the source sentence-related features provide some 
marginally useful information. 
6 CLIR experiments 
The experiments are designed to test whether the 
confidence measure approach is effective for query 
translation, and how it compares with the tradi-
tional linear combination. We will conduct two 
series of experiments, one for English-French 
CLIR and another for English-Arabic CLIR. 
                                                 
7 The translations with the probability p(f|e)?0.1 
6.1 Experimental setup 
English-French CLIR: We use English queries 
to retrieve French documents. In our experiments, 
we use two document collections: one from TREC8 
and another from CLEF9 (SDA). Both collections 
contain newspaper articles. TREC collection con-
tains 141 656 documents and CLEF collection 
44 013 documents. We use 4 query sets: 3 from 
TREC (TREC6 (25 queries), TREC7 (28 queries), 
TREC8 (28 queries)) and one from CLEF (40 que-
ries). 
English-Arabic CLIR: For these experiments, 
we use English queries to retrieve Arabic docu-
ments. The test corpus is the Arabic TREC collec-
tion which contains 383 872 documents. For top-
ics, we use two sets: TREC2001 (25 queries) and 
TREC2002 (50 queries). 
Documents and queries are stemmed and stop-
words are removed. The Porter stemming is used 
to stem English queries and French documents. 
Arabic documents are stemmed using linguistic-
based stemming method (Kadri, 2006). The query 
terms are translated with the baseline models (Sec-
tion 4). The resulting translations are then submit-
ted to the information retrieval process. We tested 
with different ways to assign weights to translation 
candidates: translations from each resource, linear 
combination and confidence measures. 
When using each resource separately, we attrib-
ute the IBM 1 translation probabilities to our trans-
lations. For each query term, we take only transla-
tions with the probability p(f|e)?0.1 when using 
translation models and the five best translations 
when using bilingual dictionaries.  
6.2 Linear combination (LC) 
The tuned parameters assigned to each transla-
tion resource are as follows: 
English-Arabic CLR:  
STM-Web: 0.29, STM-UN: 0.34,  
Ajeeb BD: 0.14, Almisbar BD: 0.22. 
English-French CLR:  
STM-Web: 0.3588, STM-Hansard: 0.6408, 
Freedict BD: 0.0003. 
These weights produced the best log-likelihood 
of the training data. 
                                                 
8 http://trec.nist.gov/ 
9 http://www.clef-campaign.org/ 
186
For CLIR, the above combinations are used to 
combine translation candidates from different re-
sources. The tables below show the CLIR effec-
tiveness (mean average precision - MAP) of indi-
vidual models and the linear combination.  
Translation 
Model 
TREC 
2001 
TREC 
2002 
Merged 
TREC 
2001/2002 
Monolingual IR (0.33) (0.28) (0.31) 
STM-Web 0.14 (42%) 0.04 (17%) 0.07 (25%) 
STM-UN 0.11 (33%) 0.09 (34%) 0.10 (33%) 
Ajeeb BD 0.27 (81%) 0.19 (70%) 0.22 (70%) 
Almisbar BD 0.17 (51%) 0.16 (58%) 0.16 (54%) 
Linear Comb. 0.24 (72%) 0.20 (71%) 0.21 (67%) 
Table1. English-Arabic CLIR performance (MAP) 
with individual models and linear combination 
Trans. Model TREC6 TREC7 TREC8 CLEF 
Monolingual IR 0.39 0.34 0.44 0.40 
STM-Web 0.22 (56%) 0.17 (50%) 0.22 (50%) 0.29 (72%)
STM-Hansard 0.25 (64%) 0.24 (70%) 0.33 (75%) 0.30 (75%)
Freedict BD 0.17 (43%) 0.11 (32%) 0.13 (29%) 0.14 (35%)
Linear Comb. 0.26 (66%) 0.26 (76%) 0.36 (81%) 0.30 (75%)
Table2. English-French CLIR performance (MAP) with 
individual models and linear combination 
We observe that the performance is quite differ-
ent from one model to another. The low score re-
corded by the STMs for English-Arabic CLIR 
compared to the score of STMs for English-French 
CLIR is possibly due to the small data set on which 
the English-Arabic STMs are trained. A set of 
2816 English-Arabic pairs of documents is not 
enough to build a reasonable STM. For English-
Arabic CLIR, BDs present better performance than 
STMs because they cover almost all query terms 
and they provide multiple good translations to each 
query term. When combining all the resources, the 
performance is supposed to be better because we 
would like to take advantage of each of the models. 
However, we see that the combined model per-
forms even worse than one of the models - Ajeeb 
BD for English-Arabic CLIR. This shows that the 
linear combination is not necessarily a good way to 
combine different translation resources. 
An example of English queries is shown in Ta-
ble 3: ?What measures are being taken to develop 
tourism in Cairo??. The Arabic translation pro-
vided by TREC to the word ?measures? is: 
?????????. We see clearly that translations with dif-
ferent resources are different. Some resources pro-
pose inappropriate translations such as ??????? or 
???????. Even if two resources suggest the same 
translations, the weights are different. For this 
query, the linear combination produces better 
query translation terms than every resource taken 
alone: The most probable translations are selected 
from the combined list. However, this method is 
unable to attribute an appropriate weight to the best 
translation ?????????; it is selected but ranked at 
third position with a weak weight.  
Trans. model Translation(s) of word ?measures? 
Ajeeb BD 0.05 ????? (measure), 0.05 ???? (caliber), ???? 
0.05 (measurement), 0.05 ????? (measure-
ment), 0.05 ????? (standard), 0.05 ????? 
(standard), 0.05 ????? (balance) 
Almisbar BD 0.05 ??????? (procedures), 0.03 ??? ,0.03 ????? 
(measurement), 0.03 ????? (amount) 
STM-UN 0.69 ?????? (measures) 
STM-Web 0.09 ??????? 
Linear Comb, ???? ,0.029 ??????? ,0.037 ????? ,0.61 ?????? 
0.020  
Table3. Translation examples 
6.3 CLIR with Confidence Measures (CM) 
In these experiments, we use confidence meas-
ures as weights for translations. According to these 
confidence measures, we select the translations 
with the best confidences for each query term. The 
following tables show the results: 
Collection TREC 2001 TREC 2002 TREC01-02 
MAP of LC 0.2426 0.2032 0.2163 
MAP of CM 0.2775(14.35%) 0.2052 (1%) 0.2290 (5.87 %)
Table4. Comparison of English-Arabic CLIR between 
linear combination and confidence measures 
Collection TREC6 TREC7 TREC8 CLEF 
MAP of LC 0.2692 0.2630 0.3605 0.3071 
MAP of CM 0.2988 
(10.99%) 
0.2699 
(2.62%) 
0.3761 
(4.32%) 
0.3230 
(5.17 %) 
Table5. Comparison of English-French CLIR be-
tween linear combination and confidence measures 
In terms of MAP, we see clearly that the results 
using confidence measures are better than those 
obtained with the linear combination. The two-
tailed t-test shows that the improvement brought 
by confidence measure over linear combination is 
statistically significant at the level P<0.05. This 
improvement in CLIR performance is attributed to 
the ability of confidence measure to re-weight each 
translation candidate. The final sets of translations 
(and their probabilities) are more reasonable than 
in linear combination. The tables below show some 
examples where we get a large improvement in 
average precision when using confidence measures 
to combine resources. The first example is the 
TREC 2001 query ?What measures are being taken 
to develop tourism in Cairo??. The translation of 
the query term ?measures? to Arabic using the two 
187
methods is presented in table 6. The second exam-
ple is the TREC6 query ?Acupuncture?. Table 7 
presents the translation of this query term is to 
French using the two techniques: 
Trans.Model Translation(s) of term ?measures? 
Linear Comb. ???? ,0.029 ??????? ,0.037 ????? ,0.61 ?????? 
0.020 
Conf. meas. 0.06 ???? ,0.10 ??? ,0.51 ??????? 
Table6. Translation examples to Arabic  
Trans.model Translation(s) of term ?Acupuncture? 
Linear Comb. Acupuncture 0.13 (acupuncture), sevrage 
0.13 (severing), hypnose 0.13 (hypnosis) 
Conf. meas. Acupuncture 0.21, sevrage 0.17, hypnose 
0.14 
Table7. Translation examples to French  
In the example of table 6, confidence measure 
has been able to redeem the best translation 
????????? and rescore it with a stronger weight than 
the other incorrect or inappropriate ones. The same 
effect is observed in the example of table 7. Confi-
dence measure has been able to increase the correct 
translation ?acupuncture? to a higher level than the 
other incorrect ones. These examples show the po-
tential advantage of confidence measure over lin-
ear combination: The confidence measure does not 
blindly trust all the translations from different re-
sources. It tests their validity on new validation 
data. Thus, the translation candidates are rescored 
and filtered according to a more reliable weight.  
7 Conclusion 
Multiple translation resources are believed to con-
tribute in improving the quality of query transla-
tion. However, in most previous studies, only lin-
ear combination has been used. In this study, we 
propose a new method based on confidence meas-
ure to combine different translation resources. The 
confidence measure estimates the probability of 
correctness of a translation, given a set of features 
available. The measure is used to weight the trans-
lation candidates in a unified manner. It is also ex-
pected that the new measure is more reasonable 
than the original measures because of the use of 
additional features. Our experiments on both Eng-
lish-Arabic and English-French CLIR have shown 
that confidence measure is a better way to combine 
translation resources than linear combination. This 
shows that confidence measure is a promising ap-
proach to combine non homogenous resources and 
can be further improved on several aspects. For 
example, we can optimize this technique by identi-
fying other informative features. Other techniques 
for computing confidence estimates can also be 
used in order to improve the performance of CLIR. 
References 
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. 
Goutte, A. Kulesza, A. Sanchis and N. Ueffing. 2003. 
Confidence estimation for machine translation. 
Technical Report, CLSP/JHU 2003 Summer Work-
shop, Baltimore MD. 
P. F. Brown, S. A. Pietra, V. J. Pietra and R. L. Mercer. 
1993. The mathematics of statistical machine transla-
tion: Parameter estimation. Computational Linguis-
tics, 19(2):263?311. 
A. Fraser, J. Xu and R. Weischedel. 2002. TREC 
2002 Cross-lingual Retrieval at BBN. TREC11 
conference. 
S. Gandrabur and G. Foster. 2003. Confidence Estima-
tion for Text Prediction. Proceedings of the CoNLL 
2003 Conference, Edmonton. 
T. J. Hazen, T. Burianek, J. Polifroni and S. Seneff. 
2002. Recognition confidence scoring for use in 
speech understanding systems. Computer Speech and 
Language, 16:49-67. 
Y. Kadri and J. Y. Nie. 2004. Query translation for 
English-Arabic cross language information retrieval. 
Proceedings of the TALN conference. 
Y. Kadri and J. Y. Nie. 2006. Effective stemming for 
Arabic information retrieval. The challenge of Ara-
bic for NLP/MT Conference. The British Computer 
Society. London, UK. 
J. Y. Nie, M. Simard and G Foster. 2000. Multilingual 
information retrieval based on parallel texts from the 
Web. In LNCS 2069, C. Peters editor, 
CLEF2000:188-201, Lisbon. 
D. W. Oard and A. Diekema. 1998. Cross-Language 
Information Retrieval. In M. Williams (ed.), Annual 
review of Information science, 1998:223-256. 
J. Xu and R. Weischedel. 2005. Empirical studies on the 
impact of lexical resources on CLIR performance. In-
formation processing & management, 41(3):475-487. 
C. Zhai and J. Lafferty. 2001a. Model-based feedback in 
the language modeling approach to information re-
trieval. CIKM 2001 Conference. 
C. Zhai and J. Lafferty. 2001b. A study of smoothing 
methods for language models applied to ad hoc in-
formation retrieval. Proceedings of the ACM-SIGIR. 
188
c? 2003 Association for Computational Linguistics
Embedding Web-Based Statistical
Translation Models in Cross-Language
Information Retrieval
Wessel Kraaij? Jian-Yun Nie? Michel Simard?
TNO TPD Universite? de Montre?al Universite? de Montre?al
Although more and more language pairs are covered by machine translation (MT) services, there
are still many pairs that lack translation resources. Cross-language information retrieval (CLIR)
is an application that needs translation functionality of a relatively low level of sophistication,
since current models for information retrieval (IR) are still based on a bag of words. The Web
provides a vast resource for the automatic construction of parallel corpora that can be used to train
statistical translation models automatically. The resulting translation models can be embedded in
several ways in a retrieval model. In this article, we will investigate the problem of automatically
mining parallel texts from the Web and different ways of integrating the translation models
within the retrieval process. Our experiments on standard test collections for CLIR show that the
Web-based translation models can surpass commercial MT systems in CLIR tasks. These results
open the perspective of constructing a fully automatic query translation device for CLIR at a very
low cost.
1. Introduction
Finding relevant information in any language on the increasingly multilingual World
Wide Web poses a real challenge for current information retrieval (IR) systems. We
will argue that the Web itself can be used as a translation resource in order to build
effective cross-language IR systems.
1.1 Information Retrieval and Cross-Language Information Retrieval
The goal of IR is to find relevant documents from a large collection of documents or
from the World Wide Web. To do this, the user typically formulates a query, often in
free text, to describe the information need. The IR system then compares the query
with each document in order to evaluate its similarity (or probability of relevance)
to the query. The retrieval result is a list of documents presented in decreasing order
of similarity. The key problem in IR is that of effectiveness, that is, how good an IR
system is at retrieving relevant documents and discarding irrelevant ones.
Because of the information explosion that has occurred on the Web, people are
more in need of effective IR systems than ever before. The search engines currently
available on the Web are IR systems that have been created to answer this need. By
querying these search engines, users are able to identify quickly documents contain-
ing the same keywords as the query they enter. However, the existing search engines
provide only monolingual IR; that is, they retrieve documents only in the same lan-
? TNO TPD, PO BOX 155, 2600 AD Delft, The Netherlands. E-mail: kraaij@tpd.tno.nl
? DIRO, Universite? de Montre?al, CP. 6128, succ. Centre-ville, Montreal, Qc. H3C 3J7 Canada. E-mail:
{nie, simardm}@iro.umontreal.ca
382
Computational Linguistics Volume 29, Number 3
guage as the query. To be more precise: Search engines usually do not consider the
language of the keywords when the keywords of a query are matched against those
of the documents. Identical keywords are matched, whatever their languages are. For
example, the English word son can match the French word son (?his? or ?her?). Current
search engines do not provide the functionality for cross-language IR (CLIR), that is,
the ability to retrieve relevant documents written in languages different from that of
the query (without the query?s being translated manually into the other language(s)
of interest).
As the Web has grown, more and more documents on the Web have been written in
languages other than English, and many Internet users are non-native English speak-
ers. For many users, the barrier between tbe language of the searcher and the langage
in which documents are written represents a serious problem. Although many users
can read and understand rudimentary English, they feel uncomfortable formulating
queries in English, either because of their limited vocabulary in English, or because
of the possible misusage of English words. For example, a Chinese user may use eco-
nomic instead of cheap or economical or inexpensive in a query because these words have
a similar translation in Chinese. An automatic query translation tool would be very
helpful to such a user. On the other hand, even if a user masters several languages, it
is still a burden for him or her to formulate several queries in different languages. A
query translation tool would also allow such a user to retrieve relevant documents in
all the languages of interest with only one query. Even for users with no understand-
ing of a foreign language, a CLIR system might still be useful. For example, someone
monitoring a competitor?s developments with regard to products similar to those he
himself produces might be interested in retrieving documents describing the possible
products, even if he does not understand them. Such a user might use machine trans-
lation systems to get the gist of the contents of the documents he retrieves through
his query. For all these types of users, CLIR would represent a useful tool.
1.2 Possible Approaches to CLIR
From an implementation point of view, the only difference between CLIR and the
classical IR task is that the query language differs from the document language. It is
obvious that to perform in an effective way the task of retrieving documents that are
relevant to a query when the documents are written in a different language than the
query, some form of translation is required. One might conjecture that a combination
of two existing fields, IR and machine translation (MT), would be satisfactory for
accomplishing the combined translation and retrieval task. One could simply translate
the query by means of an MT system, then use existing IR tools, obviating the need
for a special CLIR system.
This approach, although feasible, is not the only possible approach, nor is it neces-
sarily the best one. MT systems try to translate text into a well-readable form governed
by morphological, syntactic, and semantic constraints. However, current IR models are
based on bag-of-words models. They are insensitive to word order and to the syntac-
tic structure of queries. For example, with current IR models, the query ?computer
science? will usually produce the same retrieval results as ?science computer.? The
complex process used in MT for producing a grammatical translation is not fully ex-
ploited by current IR models. This means that a simpler translation approach may
suffice to implement the translation step.
On the other hand, MT systems are far from perfect. They often produce incorrect
383
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
translations. For example, Systran1 translates the word drug as drogue (illegal substance)
in French for both drug traffic and drug administration office. Such a translation error
will have a substantial negative impact on the effectiveness of any CLIR system that
incorporates it. So even if MT systems are used as translation devices, they may need to
be complemented by other, more robust translation tools to improve their effectiveness.
In the current study, we will use statistical translation models as such a complementary
tool.
Queries submitted to IR systems or search engines are often very short. In par-
ticular, the average length of queries submitted to the search engines on the Web is
about two words (Jansen et al 2001). Such short queries are generally insufficient to
describe the user?s information need in a precise and unambiguous way. Many im-
portant words are missing from them. For example, a user might formulate the query
?Internet connection? in order to retrieve documents about computer networks, Inter-
net service providers, or proxies. However, under the current bag-of-words approach,
the relevant documents containing these terms are unlikely to be retrieved. To solve
this problem, a common approach used in IR is query expansion, which tries to add
synonyms or related words to the original query, making the expanded query a more
exhaustive description of the information need. The words added to the query dur-
ing query expansion do not need to be strict synonyms to improve the query results.
However, they do have to be related, to some degree, to the user?s information need.
Ideally, the degree of the relatedness should be weighted, with a strongly related word
weighted more heavily in the expanded query than a less related one.
MT systems act in a way opposite to the query expansion process: Only one trans-
lation is generally selected to express a particular meaning. 2 In doing so, MT systems
employed in IR systems in fact restrict the possible query expansion effect during
the translation process. We believe that CLIR can benefit from query translation that
provides multiple translations for the same meaning. In this regard, the tests carried
out by Kwok (1999) with a commercial MT system for Chinese-English CLIR are quite
interesting. His experiments show that it is much better to use the intermediate transla-
tion data produced by the MT system than the final translation itself. The intermediate
data contain, among other things, all the possible translation words for query terms.
Kwok?s work clearly demonstrates that using an MT system as a black box is not the
most effective choice for query translation in CLIR. However, few MT systems allow
one to access the intermediate stages of the translations they produce.
Apart from the MT approach, queries can also be translated by using a machine-
readable bilingual dictionary or by exploiting a set of parallel texts (texts with their
translations). High-quality bilingual dictionaries are expensive, but there are many free
on-line translation dictionaries available on the Web that can be used for query trans-
lation. This approach has been applied in several studies (e.g., Hull and Grefenstette
1996; Hiemstra and Kraaij 1999). However, free bilingual dictionaries often suffer from
a poor coverage of the vocabulary in the two languages with which they deal, and from
the problem of translation ambiguity, because usually no information is provided to
allow for disambiguation. Several previous studies (e.g., Nie et al 1999), have shown
that using a translation dictionary alone would produce much lower effectiveness than
an MT system. However, a dictionary complemented by a statistical language model
(Gao et al 2001; Xu, Weischedel, and Nguyen 2001) has produced much better results
than when the dictionary is used alone.
1 We used the free translation service provided at ?http://babelfish.altavista.com/? in October 2002.
2 Although there is no inherent obstacle preventing MT systems from generating multiple translations, in
practice, only one translation is produced.
384
Computational Linguistics Volume 29, Number 3
In this article, the use of a bilingual dictionary is not our focus. We will concentrate
on a third alternative for query translation: an approach based on parallel texts. Paral-
lel texts are texts accompanied by their translation in one or several other languages (Ve?ronis
2000). They contain valuable translation examples for both human and machine trans-
lation. A number of studies in recent years (e.g., Nie et al 1999; Franz et al 2001;
Sheridan, Ballerini, and Scha?uble 1998; Yang et al 1998) have explored the possibil-
ity of using parallel texts for query translation in CLIR. One potential advantage of
such an approach is that it provides multiple translations for the same meaning. The
translation of a query would then contain not only words that are true translations
of the query, but also related words. This is the query expansion effect that we want
to produce in IR. Our experimental results have confirmed that this approach can be
very competitive with the MT approach and yield much better results than a simple
dictionary-based approach, while keeping the development cost low.
However, one major obstacle to the use of parallel texts for CLIR is the unavail-
ability of large parallel corpora for many language pairs. Hence, our first goal in the
research presented here was to develop an automatic mining system that collects par-
allel pages on the Web. The collected parallel Web pages are used to train statistical
translation models (TMs) that are then applied to query translation. Such an approach
offers the advantage of enabling us to build a CLIR system for a new language pair
without waiting for the release of an MT system for that language pair. The number
of potential language pairs supported by Web-based translation models is large if one
includes transitive translation using English as a pivot language. English is often one
of the languages of those Web pages for which parallel translations are available.
The main objectives of this article are twofold: (1) We will show that it is possible
to obtain large parallel corpora from the Web automatically that can form the basis for
an effective CLIR system, and (2) we will compare several ways to embed translation
models in an IR system to exploit these corpora for cross-language query expansion.
Our experiments will show that these translation tools can result in CLIR of com-
parable effectiveness to MT systems. This in turn will demonstrate the feasibility of
exploiting the Web as a large parallel corpus for the purpose of CLIR.
1.3 Problems in Query Translation
Now let us turn to query translation problems. Previous studies on CLIR have iden-
tified three problems for query translation (Grefenstette 1998): identifying possible
translations, pruning unlikely translations, and weighting the translation words.
Finding translations. First of all, whatever translation tool is employed in trans-
lating queries has to provide a good coverage of the source and target vocabularies.
In a dictionary-based approach to CLIR, we will encounter the same problems that
have been faced in MT research: phrases, collocations, idioms, and domain-specific
terminology are often translated incorrectly. These classes of expressions require a so-
phisticated morphological analysis, and furthermore, domain-specific terms challenge
the lexical coverage of the transfer dictionaries. A second important class of words
that can pose problems for CLIR, particularly that involving news article retrieval,
is proper names. The names of entities such as persons or locations are frequently
used in queries for news articles, and their translation is not always trivial. Often, the
more commonly used geographical names of countries or their capitals have a dif-
ferent spelling in different languages (e.g., Milan/Milano/Milaan) or translations that
are not related to the same morphological root (e.g., Germany/Allemagne/Duitsland).
The names of organizations and their abbreviations are also a notorious problem; for
example, the United Nations can be referred to as UN, ONU, VN, etc. (disregarding
the problem of morphological normalization of abbreviations). When proper names
385
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
have to be translated from one language to another with a different script, like Cyril-
lic, Arabic, or Chinese, this problem is even more acute. The process of defining the
spelling of a named entity in a language with a different script from the originating
language is called transliteration and is based on a phonemic representation of the
named entity. Unfortunately different national ?standards? are used for transliteration
in different languages that use the same alphabet (e.g., the former Russian president?s
name in Latin script has been transliterated as Jeltsin, Eltsine, Yeltsin, and Jelzin.
Pruning translation alternatives. A word or a term often has multiple transla-
tions. Some of them are appropriate for a particular query and the others are not. An
important question is how to keep the appropriate translations while eliminating the
inappropriate ones. Because of the particularities of IR, it might improve the results
to retain multiple translations that display small differences in sense, as in query ex-
pansion. So it could be beneficial to keep all related senses for the matching process,
together with their probabilities.
Weighting translation alternatives. Closely related to the previous point is the
question of how to deal with translation alternatives. The weighting of words in doc-
uments and in the query is of crucial importance in IR. A word with a heavy weight
will influence the results of retrieval more than a low-weight word. In CLIR it is also
important to assign appropriate weights to translation words. Pruning translations can
be viewed as an extreme Boolean way of weighting translations. The intuition is that,
just as in query expansion, it may well be beneficial to assign a heavier weight to the
?main? translation and a lighter weight to related translations.
1.4 Integration of Query Translation with Retrieval
The problem of ?weighting of translation alternatives,? identified by Grefenstette,
refers to the more general problem of designing an architecture for a CLIR system
in which translation and document ranking are integrated in a way that maximizes
retrieval effectiveness.
The MT approach clearly separates translation from retrieval: A query is first
translated, and the result of the translation is subsequently submitted to an IR system
as a new query. At the retrieval phase, one no longer knows how certain a translated
word is with respect to the other translated words in the translated query. All the
translation words are treated as though they are totally certain. Indeed, an MT system
is used as a black box. In this article, we consider translation to be an integral part of
the IR process that has to be considered together with the retrieval step.
From a more theoretical point of view, CLIR is a process that, taken as a whole, is
composed of query translation, document indexing, and document matching. The two
first subprocesses try to transform the query and the documents into a comparable
internal representation form. The third subprocess tries to compare the representa-
tions to evaluate the similarity. In previous studies on CLIR, the first subprocess is
clearly separated from the latter two, which are integrated in classical IR systems. An
approach that considers all three subprocesses together will have the advantage of
accounting better for the uncertainty of translation during retrieval. More analysis on
this point is provided in Nie (2002). This article follows the same direction as Nie?s.
We will show in our experiments that an integrated approach can produce very high
CLIR effectiveness.
An attractive framework for integrating translation and retrieval is the probabilistic
framework, although estimating translation probabilities is not always straightforward
using this framework.
In summary, because CLIR does not necessarily require a unique translation of a
text (as MT does), approaches other than fully automatic MT might provide interesting
386
Computational Linguistics Volume 29, Number 3
characteristics for CLIR that are complementary to those of MT approaches. This could
result in greater precision,3 since an MT system might choose the wrong translation
for the query term(s), and/or a higher rate of recall,4 since multiple translations are
accommodated, which could retrieve documents via related terminology.
In this article we will investigate the effectiveness of CLIR systems based on
probabilistic translation models trained on parallel texts mined from the Web. Glob-
ally, our approach to the CLIR problem can be viewed informally as ?cross-lingual
sense matching.? Both query and documents are modeled as a distribution over se-
mantic concepts, which in reality is approximated by a distribution over words. The
challenge for CLIR is to measure to what extent these concepts (or word senses) are
related. From this point of view, our approach is similar in principle to that using
latent semantic analysis (LSI) (Dumais et al 1997), which also tries to create semantic
similarity between documents, queries, and terms by transposing them into a new
vector space. An alternative way of integrating translation and IR is to create ?struc-
tured queries,? in which translations are modeled as synonyms (Pirkola 1998). Since
this approach is simple and effective, we will use it as one of the reference systems in
our experiments.
The general approach of this article will be implemented in several different ways,
each fully embedded in the retrieval models tested. A series of experiments on CLIR
will be conducted in order to evaluate these models. The results will clearly show that
Web-based translation models are as effective as (and sometimes more effective than)
off-the-shelf commercial MT systems.
The remainder of the article is organized as follows: Section 2 discusses the pro-
cedure we used to construct parallel corpora from the Web, and Section 3 describes
the procedure we used to train the translation models. Section 4 describes the proba-
bilistic IR model that we employed and various ways of embedding translation into
a retrieval model. Section 5 presents our experimental results. The article ends with a
discussion and conclusion section.
2. PTMiner
It has been shown that by using a large parallel corpus, one can produce CLIR ef-
fectiveness close to that obtained with an MT system (Nie et al 1999). In previous
studies, parallel texts have been exploited in several ways: using a pseudofeedback ap-
proach, capturing global cross-language term associations, transposing to a language-
independent semantic space, and training a statistical translation model.
Using a pseudofeedback approach. In Yang et al (1998) parallel texts are used
as follows. A given query in the source language is first used to retrieve a subset
of texts from the parallel corpus. The corresponding subset in the target language is
considered to provide a description of the query in the target language. From this
subset of documents, a set of weighted words is extracted, and this set of words is
used as the query ?translation.?
Capturing global cross-language term associations. A more advanced and theo-
retically better-motivated approach is to index concatenated parallel documents in the
dual space of the generalized vector space model (GVSM), where terms are indexed
by documents (Yang et al 1998). An approach related to GVSM is to build a so-called
similarity thesaurus on the parallel or comparable corpus. A similarity thesaurus is an
3 Precision is defined as the proportion of relevant documents among all the retrieved documents.
4 Recall is the proportion of relevant documents retrieved among all the relevant documents in a
collection.
387
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
information structure (also based on the dual space of indexing terms by documents)
in which associated terms are computed on the basis of global associations between
terms as measured by term co-occurrence on the document level (Sheridan, Ballerini,
and Scha?uble 1998). Recently, the idea of using the dual space of parallel documents
for cross-lingual query expansion has been recast in a language-modeling framework
(Lavrenko, Choquette, and Croft 2002).
Transposing to a language-independent semantic space. The concatenated doc-
uments can also be transposed in a language-independent space by applying latent
semantic indexing (Dumais et al 1997; Yang et al 1998). The disadvantage of this
approach is that the concepts in this space are hard to interpret and that LSI is com-
putationally demanding. It is currently not feasible to perform such a transposition on
a Web scale.
Training a statistical translation model. Approaches that involve training a statis-
tical translation model have been explored in, for example, Nie et al (1999) and Franz
et al (2001). In Nie et al?s approach, statistical translation models (usually IBM model
1) are trained on a parallel corpus. The models are used in a straightforward way: The
source query is submitted to the translation model, which proposes a set of translation
equivalents, together with their probability. The latter are then used as a query for the
retrieval process, which is based on a vector space model. Franz et al?s approach uses
a better founded theoretical framework: the OKAPI probabilistic IR model (Robert-
son and Walker 1994). The present study uses a different probabilistic IR model, one
based on statistical language models (Hiemstra 2001; Xu, Weischedel, and Nguyen
2001). This IR model facilitates a tighter integration of translation and retrieval. An
important difference between statistical translation approaches and approaches based
on document alignment discussed in the previous paragraph is that translation models
perform alignment at a much more refined level. Consequently, the alignments can
be used to estimate translation relations in a reliable way. On the other hand, the ad-
vantage of the CLIR approaches that rely simply on alignment at the document level
is that they can also handle comparable corpora, that is, documents that discuss the
same topic but are not necessarily translations of each other (Laffling 1992).
Most previous work on parallel texts has been conducted on a few manually
constructed parallel corpora, notably the Canadian Hansard corpus. This corpus5 con-
tains many years? debates in the Canadian parliament in both English and French,
amounting to several dozens of millions of words in each language. The European
parliament documents represent another large parallel corpus in several European
languages. However, the availability of this corpus is much more restricted than the
Canadian Hansard. The Hong Kong government publishes official documents in both
Chinese and English. They form a Chinese-English parallel corpus, but again, its size
is much smaller than that of the Canadian Hansard. For many other languages, no
large parallel corpora are available for the training of statistical models.
LDC has tried to collect additional parallel corpora, resorting at times to man-
ual collection (Ma 1999). Several other research groups (for example, the RALI lab
at Universite? de Montre?al) have also tried to acquire manually constructed parallel
corpora. However, manual collection of large corpora is a tedious task that is time-
and resource-consuming. On the other hand, we observe that the increasing usage of
different languages on the Web results in more and more bilingual and multilingual
sites. Many Web pages are now translated into different languages. The Web contains
5 LDC provides a version containing texts from the mid-1970s through 1988; see
?http://www.ldc.upenn.edu/?.
388
Computational Linguistics Volume 29, Number 3
a large number of parallel Web pages in many languages (usually with English). If
these can be extracted automatically, then this would help solve, to some extent, the
problem of parallel corpora. PTMiner (for Parallel Text Miner) was built precisely for
this purpose.
Of course, an automatic mining program is unable to understand the texts it ex-
tracts and hence to judge in a totally reliable way whether they are parallel. However,
CLIR is quite error-tolerant. As we will show, a noisy parallel corpus can still be very
useful for CLIR.
2.1 General Principles of Automatic Mining
Parallel Web pages usually are not published in isolation; they are often linked to
one another in some way. For example, Resnik (1998) observed that some parallel
Web pages are often referenced in the same parent index Web page. In addition,
the anchor text of such links usually identifies the language. For example, if a Web
page ?index.html? provides links to both English and French versions of a page it
references, and the anchor texts of the links are respectively ?English version? and
?French version,? then the referenced versions are probably parallel pages in English
and French. To locate such pages, Resnik first sends a query of the following form to
the Web search engine AltaVista, which returns the parent indexing pages:
anchor: English AND anchor: French
Then the referenced pages in both languages are retrieved and considered to be par-
allel. Applying this method, Resnik was able to mine 2,491 pairs of English-French
Web pages. Other researchers have adapted his system to mine 3,376 pairs of English-
Chinese pages and 59 pairs of English-Basque pages.
We observe, however, that only a small portion of parallel Web sites are organized
in this way. Many other parallel pages cannot be found with Resnik?s method. The
mining system we employ in the research presented here uses different criteria from
Resnik?s; and we also incorporate an exploration process (i.e., a host crawler) in order
to discover Web pages that have not been indexed by the existing search engines.
The mining process in PTMiner is divided into two main steps: identification of
candidate parallel pages, and verification of their parallelism. The overall process is
organized into the following steps:
1. Determining candidate sites. Identify Web sites that may contain
parallel pages. In our approach, we adopt a simple definition of Web
site: a host corresponding to a distinct DNS (domain name system)
address (e.g., ?www.altavista.com? and ?geocities.yahoo.com?).
2. File name fetching. Identify a set of Web pages on each Web site that are
indexed by search engines.
3. Host crawling. Use the URLs collected in the previous step as seeds to
further crawl each candidate site for more URLs.
4. Pair scanning by names. Construct pairs of Web pages on the basis of
pattern matching between URLs (e.g., ?index.html? vs. ?index f.html?).
5. Text filtering. Filter the candidate parallel pages further according to
several criteria that operate on their contents.
In the following subsections, we describe each of these steps in more detail.
389
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
2.2 Identification of Candidate Web Sites
In addition to the organization of parallel Web pages exploited by Resnik?s method,
another common characteristic of parallel Web pages is that they cross-reference one
another. For example, an English Web page may contain a pointer to the French ver-
sion, and vice versa, and the anchor text of these pointers usually indicates the lan-
guage of the other page. This phenomenon is common because such an anchor text
shows the reader that a version in another language is available.
In considering both ways of organizing parallel Web pages, we see that a common
feature is the existence of a link with an anchor text identifying a language. This is
the criterion we use in PTMiner to detect candidate Web sites: the existence of at least
one Web page containing such a link. Candidate Web sites are identified via requests
sent to a search engine (e.g., AltaVista or Google). For example, the following request
asks for pages in English that contain a link with one of the required anchor texts.
anchor: French version, in French, en Franc?ais, . . .
language: English
The hosts extracted from the responses are considered to be candidate sites.
2.3 File Name Fetching
It is assumed that parallel pages are stored on the same Web site. This is not always
true, but this assumption allows us to minimize the exploration of the Web and to
avoid considering many unlikely candidates.
To search for parallel pairs of pages from each candidate site, PTMiner first asks
the search engine for all the Web pages from a particular site that it has indexed, via
a request of the following form:
host: <hostname>
However, the results of this step may not be exhaustive, because
? search engines typically do not index all the Web pages of a site.
? most search engines allow users to retrieve a limited number of
documents (e.g., 1,000 in AltaVista).
Therefore, we continue our search with a host crawler, which uses the Web pages
found by the search engines as seeds.
2.4 Host Crawling
A host crawler is slightly different from a Web crawler or a robot in that a host crawler
can only exploit one Web site at a time. A breadth-first crawling algorithm is used in
the host-crawling step of PTMiner?s mining process. The principle is that if a retrieved
Web page contains a link to an unexplored document on the same site, this document
is added to the list of pages to be explored later. This crawling step allows us to obtain
more Web pages from the candidate sites.
2.5 Pair Scanning by Names
Once a large set of URLs has been identified, the next task is to find parallel pairs
among them. In our experience, many parallel Web pages have very similar file names.
390
Computational Linguistics Volume 29, Number 3
For example, an English Web page with the file name ?index.html? often corresponds
to a French translation with a file name such as ?index f.html?. The only difference
between the two file names is a segment that identifies the language of the file. This
similarity in file names is by no means an accident. In fact, this is a common way for
Webmasters to keep track of a large number of documents in different versions.
This same observation also applies to URL paths. For example, the following two
URLs are also similar in name:
?http://www.asite.ca/en/afile.html? and ?http://www.asite.ca/fr/afile.html?.
To find similarly named URLs, we define lists of prefixes and suffixes for both the
source and the target languages. For example:
EnglishPrefix = {(emptychar), e, en, english, e , en , english , . . .}
Once a possible source language prefix is identified in an URL, it is replaced with a
prefix in the target language, and we then test if this URL is found on the Web site.
2.6 Filtering by Contents
The file pairs identified in previous steps are further verified in regard to their contents.
In PTMiner, the following criteria are used for verification: file length, HTML structure,
and language and character set.
2.6.1 File Length. The ratio of the lengths of a pair of parallel pages is usually com-
parable to the typical length ratio of the two languages (especially when the text is
long enough). Hence, a simple verification is to compare the lengths of the two files.
As many Web documents are quite short, we tolerate some difference (up to 40% from
the typical ratio).
2.6.2 HTML Structure. Parallel Web pages are usually designed to have similar lay-
outs. This often means that the two parallel pages have similar HTML structures.
However, the HTML structures of parallel pages may also be quite different from one
another. Pages may look similar and still have different HTML markups. Therefore, a
certain amount of flexibility is also employed in this step.
In our approach, we first determine a set of meaningful HTML tags that affect
the appearance of the page and extract them from both files (e.g., <p> and <H1>, but
not <meta> and <font>). A ?diff?-style comparison will reveal how different the two
extracted sequences of tags are. A threshold is set to filter out the pairs of pages that
are not similar enough in HTML structure.
At this stage, nontextual parts of the pages are also removed. If a page does not
contain enough text, it is also discarded.
2.6.3 Language and Character Set. When we query search engines for documents in
one specific language, the returned documents may actually be in a different language
from the one we specified. This problem is particularly serious for Asian languages.
When we ask for Chinese Web pages, we often obtain Korean Web pages, because the
language of the documents has not been identified accurately by the search engines.
Another, more important factor that makes it necessary to use a language detector is
that during host crawling and pair scanning, no verification is done with regard to
languages. All files with an en suffix in their names, for example, are assumed to be
English pages, which may be an erroneous assumption.
391
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
To filter out the files not in the required languages, the SILC system (Isabelle,
Simard, and Plamondon 1998) is used. SILC employs n-gram statistical language mod-
els to determine the most probable language and encoding schema for a text. It has
been trained on a number of large corpora for several languages. The accuracy of the
system is very high. When a text contains at least 50 characters, its accuracy is almost
perfect. SILC can filter out a set of file pairs that are not in the required languages.
Our utilization of HTML structure to determine whether two pages are parallel
is similar to that of Resnik (1998), who also exploits an additional criterion similar to
length-based sentence alignment in order to determine whether the segments in corre-
sponding HTML structures have similar lengths. In the current PTMiner, this criterion
is not incorporated. However, we have included the sentence-alignment criterion as a
later filtering step in Nie and Cai (2001): If a pair of texts cannot be aligned reasonably
well, then that pair is removed. This technique is shown to bring a large improvement
for the English-Chinese corpus. A similar approach could also be envisioned for the
corpora of European languages, but in the present study, such an approach is not used.
2.7 Mining Results
PTMiner uses heuristics that are mostly language-independent. This allows us to adapt
it easily for different language pairs by changing a few parameters (e.g., prefix and
suffix lists of file name). It is surprising that so simple an approach is nevertheless very
effective. We have been able, using PTMiner, to construct large parallel corpora from
the Web for the following language pairs: English-French, English-Italian, English-
German, English-Dutch, and English-Chinese. The sizes of these corpora are shown in
Table 1.
One question that may be raised is how accurate the mining results are, or how
parallel the pages identified are. Actually, it is very difficult to answer this question. We
have not undertaken an extensive evaluation but have only performed a simple evalu-
ation with a set of samples. For English-French, from 60 randomly selected candidate
sites, AltaVista indexed about 8,000 pages in French. From these, the pair-scanning
step identified 4,000 pages with equivalents in English. This showed that the lower
bound of recall of pairscanning is 50%. The equivalence of the pair pages identified
was judged by an undergraduate student who participated in developing the prelim-
inary version of PTMiner. The criterion used to judge the equivalence of two pages
was subjective, with the general guideline being whether two pages describe the same
contents and whether they have similar structures. To evaluate precision, 164 pairs
of pages from the 4,000 identified were randomly selected and manually checked. It
Table 1
Automatically mined corpora. n.a. = not available.
English-French English-German English-Italian
Number of pairs 18,807 10,200 8,504
Size (MB) 174/198 77/100 50/68
Number of words (M) 6.7/7.1 1.8/1.8 1.2/1.3
English-Dutch English-Chinese
24,738 14,820
n.a. 74/51
n.a. 9.2/9.9
392
Computational Linguistics Volume 29, Number 3
turned out that 162 of them were truly parallel. This shows that the precision is close
to 99%.
For an English-Chinese corpus, a similar evaluation has been reported in Chen
and Nie (2000). This evaluation was done by a graduate student working on PTMiner.
Among 383 pairs randomly selected at the pair-scanning step, 302 pairs were found
to be really parallel. The precision ratio is 79%, which is not as good as that of the
English-French case. There are several reasons for this:
? Incorrect links. It may be that a page is outdated but still indexed by the
search engines. A pair including that page will be eliminated in the
content-filtering step.
? Pages that are designed to be parallel, although the contents are not all translated
yet. One version of a page may be a simplified version of the other. Some
cases of this type can also be filtered out in the content-filtering step, but
some will still remain.
? Pages that are valid parallel pairs yet consist mostly of graphics rather than text.
These pages cannot be used for the training of translation models.
? Pairs that are not parallel at all. Filenames of some nonparallel pages may
accidentally match the naming rules. For example, ? . . ./et.html? versus
? . . ./etc.html?.
Related to the last reason, we also observed that the names of parallel Chinese
and English pages may be very different from one another. For example, it is frequent
practice to use the Pinyin translation as the name of a Chinese page of the correspond-
ing English file name (e.g., ?fangwen.html? vs. ?visit.html?). Another convention is to
use numbers as the filenames. For example ?1.html? would correspond to ?2.html?.
In either of these cases, our pair-scanning approach based on name similarity will
fail to recognize the pair. Overall, the naming of Chinese files is much more variable
and flexible than the naming of files for European languages. Hence, there exist fewer
evident heuristics for Chinese than for the European languages that would allow us
to enlarge the coverage and improve the precision of pair scanning.
Given the potentially large number of erroneously identified parallel pairs, a ques-
tion naturally arises: Can such a noisy corpus actually help CLIR? We will examine
this question in Section 4. In the next section we will briefly describe how statistical
translation models are trained on parallel corpora. We will focus in our discussion on
the following languages: English, French, and Italian. The resulting translation models
will be evaluated in a CLIR task.
3. Building the Translation Models
Bilingual pairs of documents collected from the Web are used as training material
for the statistical translation models that we exploit for CLIR. In practice, this mate-
rial must be organized into a set of small pairs of corresponding segments (typically,
sentences), each consisting of a sequence of word tokens. We start by presenting the
details of this preparatory step and then discuss the actual construction of the trans-
lation models.
3.1 Preparing the Corpus
3.1.1 Format Conversion, Text Segmentation, and Sentence Alignment. The collec-
tion process described in the previous section provides us with a set of pairs of HTML
393
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
files. The first step in preparing this material is to extract the textual data from the
files and organize them into small, manageable chunks (sentences).
In doing so, we try to take advantage of the HTML markup. For instance, we
know that <P> tags normally identify paragraphs, <LI> tags mark list items that can
also often be interpreted as paragraphs, <Hn> tags are normally used to mark section
headers and may therefore be taken as sentences, and so on.
Unfortunately, a surprisingly large number of HTML files on the Web are badly
formatted, which calls for much flexibility on the part of Web browsers. To help cope
with this situation, we employ a freely distributed tool called tidy (Ragget 1998), which
attempts to clean up HTML files, so as to make them XML-compliant. This cleanup
process mostly consists in normalizing tag names to the standard XHTML lower-
case convention, wrapping tag attributes within double quotes and, most importantly,
adding missing tags so as to end up with documents with balancing opening and
closing tags.
Once this cleanup is done, we can parse the files with a standard SGML parser
(we use nsgmls [Clark 2001]) and use the output to produce documents in the standard
cesAna format. This SGML format, proposed as part of the Corpus Encoding Standard
(CES) (Ide, Priest-Dorman, and Ve?ronis 1995) has provisions for annotating simple
textual structures such as sections, paragraphs, and sentences. In addition to the cues
provided by the HTML tags, we employ a number of heuristics, as well as language-
specific lists of common abbreviations and acronyms, to locate sentence boundaries
within paragraphs. When, as sometimes happens, the tidy program fails to make sense
of its input on a particular file, we simply remove all SGML markup from the file
and treat the document as plain text, which means that we must rely solely on our
heuristics to locate paragraph and sentence boundaries.
Once the textual data have been extracted from pairs of documents and are neatly
segmented into paragraphs and sentences, we can proceed with sentence alignment.
This operation produces what we call couples, that is, minimal-size pairs of corre-
sponding segments between two documents. In the vast majority of cases, couples
consist of a single pair of sentences that are translations of one another (what we
call 1-to-1 couples). However, there are sometimes ?larger? couples, as when a single
sentence in one language translates into two or more sentences in the other language
(1-to-N or N-to-1), or when sentences translate many to many (N-to-M). Conversely,
there are also ?smaller? couples, such as when a sentence from either one of the two
texts does not appear in the other (0-to-1 or 1-to-0).
Our sentence alignments are carried out by a program called sfial, an improved
implementation of the method described in Simard, Foster, and Isabelle (1992). For a
given pair of documents, this program uses dynamic programming to compute the
alignment that globally maximizes a statistical-based scoring function. This function
takes into account the statistical distribution of translation patterns (1-to-1, 1-to-N, etc.)
and the relative sizes of the aligned text segments, as well as the number of ?cognate?
words within couples, that is, pairs of words with similar orthographies in the two
languages (e.g. statistical in English vs. statistique in French).
The data produced up to this point in the preparation process constitutes what
we call a Web-aligned corpus (WAC).
3.1.2 Tokenization, Lemmatization, and Stopwords. Since our goal is to use trans-
lation models in an IR context, it seems natural to have both the translation models
and the IR system operate on the same type of data. The basic indexing units of our
IR systems are word stems. Stemming is an IR technique whereby morphologically
related word forms are reduced to a common form: a stem. Such a stem does not
394
Computational Linguistics Volume 29, Number 3
necessarily have to be a linguistic root form. The principal function of the stem is
to serve as an index term in the vocabulary of index terms. Stemming is a form of
conflation: Equivalence classes of tokens help to reduce the variance in index terms.
Most stemming algorithms fall into two categories: (1) suffix strippers, and (2) full
morphological normalization (sometimes referred to as ?linguistic stemming? in the
IR literature). Suffix strippers remove suffixes in an iterative fashion using rudimental
morphological knowledge encoded in context-sensitive patterns. The advantage of al-
gorithms of this type (e.g., Porter 1980) is their simplicity and efficiency, although this
advantage applies principally to languages with a relatively simple morphology, like
English. A different way of generating conflation classes is to employ full morpholog-
ical analysis. This process usually consists of two steps: First the texts are POS-tagged
in order to eliminate each token?s part-of-speech ambiguity, and then word forms are
reduced to their root form, a process that we refer to as lemmatization. More informa-
tion about the relative utility of morphological normalization techniques in IR systems
can be found in, for example, Hull (1996), Kraaij and Pohlmann (1996), and Braschler
and Ripplinger (2003).
Lemmatizing and removing stopwords from the training material is also beneficial
for statistical translation modeling, helping to reduce the problem of data sparseness
in the training set. Furthermore, function words and morpho-syntactic features typi-
cally arise from grammatical constraints intrinsic to a language, rather than as direct
realizations of translated concepts. Therefore, we expect that removing them helps
the translation model focus on meaning rather than form. In fact, it has been shown
in Chen and Nie (2000) that the removal of stopwords from English-Chinese train-
ing material improves both the translation accuracy of the translation models and the
effectiveness of CLIR. We expect a similar effect for European languages.
We also have to tokenize the texts, that is, to identify individual word forms.
Because we are dealing with Romance languages, this step is fairly straightforward:6
We essentially segment the text using blank spaces and punctuation. In addition, we
rely on a small number of language-specific rules to deal, for example, with elisions
in French (l?amour ? l? + amour) and Italian (dell?arte ? dell? + arte), contractions in
French (au ? a` + le), possessives in English (Bob?s ? Bob + ?s), etc.
Once we have identified word tokens, we can lemmatize or stem them. For Italian,
we relied on a simple, freely distributed stemmer from the Open Muscat project.7
For French and English, we have access to more sophisticated tools that compute
each token?s lemma based on its part of speech (we use the HMM-based POS tagger
proposed in Foster (1991) and extensive dictionaries with morphological information.
As a final step, we remove stopwords.
Usually, 1-1 alignments are more reliable than other types of alignment. It is a
common practice to use only these alignments for model training, and this is what we
do.
Table 2 provides some statistics on the processed corpora.
3.2 Translation Models
In statistical translation modeling, we take the view that each possible target language
text is a potential translation for any given source language text, but that some trans-
lations are more likely than others. In the terms of Brown et al (1990), a noisy-channel
translation model is one that captures this state of affairs in a statistical distribution
6 The processing on Chinese is described in Chen and Nie (2000).
7 Currently distributed by OMSEEK:
?http://cvs.sourceforge.net/cgi-bin/viewcvs.cgi/omseek/om/languages/?.
395
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
Table 2
Sentence-aligned corpora.
English-French English-Italian
Number of 1-1 alignments 1018K 196K
Number of tokens 6.7M/7.1M 1.2M/1.3M
Number of unique stems 200K/173K 102K/87K
P(T | S), where S is a source language text and T is a target language text.8 With such
a model, translating S amounts to finding the target language text T? that maximizes
P(T | S).
Modeling P(T | S) is, of course, complicated by the fact that there is an infinite
number of possible source and target language texts, and so much of the work of the
last 15 years or so in statistical machine translation has been aimed at finding ways
to overcome this complexity by making various simplifying assumptions. Typically,
P(T | S) is rewritten as
P(T | S) = P(T)P(S | T)
P(S)
following Bayes? law. This decomposition of P(T | S) is useful in two ways: first,
it makes it possible to ignore P(S) when searching for T?; second, it allows us to
concentrate our efforts on the lexical aspects of P(S | T), leaving it to P(T) (the ?target
language model?) to take care of syntactic and other language-specific aspects.
In one of the simplest and earliest statistical translation models, IBM?s Model 1,
it is assumed that P(S | T) can be approximated by a computation that uses only
?lexical? probabilities P(s | t) over source and target language words s and t. In other
words, this model completely disregards the order in which the individual words of
S and T appear. Although this model is known to be too weak for general translation,
it appears that it can be quite useful for an application such as CLIR, because many
IR systems also disregard word order, viewing documents and queries as unordered
bags of words.
The P(s | t) distribution is estimated from a corpus of aligned sentences like the
one we have produced from our Web-mined collection of bilingual documents, using
the expectation maximization (EM) algorithm (Baum 1972) to find the parameters
that maximize the likelihood of the training set. As in all machine-learning problems,
especially those related to natural language, data sparseness is a critical issue in this
process. Even with a large training corpus, many pairs of words (s, t) occur at very
low frequencies, and most never occur at all, making it impossible to obtain reliable
estimates for the corresponding P(s | t). Without adequate smoothing techniques, low-
frequency events can have disastrous effects on the global behavior of the model, and
unfortunately, in natural languages, low-frequency events are the norm rather than
the exception.
The goal of translation in CLIR is different from that in general language process-
ing. In the latter case it is important to enable a model to handle low-frequency words
and unknown words. For CLIR the coverage of low-frequency words or unknown
words by the model is less problematic. Even if a low-frequency word is translated
8 The model is referred to as noisy-channel because it takes the view that S is the result of some input
signal T?s being corrupted while passing through a noisy channel. In this context, the goal is to recover
the initial input, given the corrupted output.
396
Computational Linguistics Volume 29, Number 3
incorrectly, the global IR effectiveness will often not be significantly affected, because
low-frequency words likely do not appear often in the document collection to be
searched or other terms in the query could compensate for this gap. Most IR algo-
rithms are based on a term-weighting function that favors terms that occur frequently
in a document but occur infrequently in the document collection. This means that the
best index terms have a medium frequency (Salton and McGill 1983). Stopwords and
(near) hapaxes are less important for IR; limited coverage of very infrequent words in
a translation model is therefore not critical for the performance of a CLIR system.
Proper nouns are special cases of unknown words. When they appear in a query,
they usually denote an important part of the user?s intention. However, we can adopt
a special approach to cope with these unknown words in CLIR without integrating
them as the generalized case in the model. For example, one can simply retain all the
unknown words in the query translation. This approach works well for most cases
in European languages. We have previously shown that a fuzzy-matching approach
based on n-grams offers an effective means of overcoming small spelling variations in
proper noun spelling (Kraaij, Pohlmann, and Hiemstra 2000).
The model pruning techniques developed in computational linguistics are also
useful for the models used in CLIR. The beneficial effect is that unreliable (or low-
probability) translations can be removed. In Section 4, model smoothing will be moti-
vated from a more theoretical point of view. Here, let us first outline the two variations
we used to prune the models.
The first one is simple, yet effective in our application: We consider unreliable all
parameters (translation probabilities) whose value falls below some preset threshold
(in practice, 0.1 works well). These parameters are simply discarded from the model.
The remaining parameters are then renormalized so that all marginal distributions
sum to one.
Another pruning technique is based on the relative contribution to the entropy
of the model. We retain the N most reliable parameters (in practice, N = 100K works
well). The reliability of a parameter is measured with regard to its contribution to the
model?s entropy (Foster 2000). In other words, we discard the parameters that least
affect the overall probability of the training set. The remaining parameters are then
renormalized so that all marginal distributions sum to one.
Of course, as a result of this, most pairs of words (s, t) are unknown to the trans-
lation model (translation probability equals zero). As previously discussed, however,
this will not have a disastrous effect on CLIR; on the contrary, some positive effect can
be expected as long as there is at least one translation for each source term.
One important characteristic of these noisy-channel models is that they are ?di-
rectional.? Depending on the intended use, it must be determined beforehand which
language is the source and which the target for each pair of languages. Although ?re-
verse? parameters can theoretically be obtained from the model through Bayes? rule,
it is often more practical to train two separate models if both directions are needed.
This topic is also discussed in the next section.
4. Embedding Translation into the IR Model
When CLIR is considered simply as a combination of separate MT and IR components,
the embedding of the two functions is not a problem. However, as we explained in
Section 1, there are theoretical motivations for embedding translation into the retrieval
model. Since translation models provide more than one translation, we will try to
exploit this extra information, in order to enhance retrieval effectiveness. In Section 4.1
we will first introduce a monolingual probabilistic IR model based on cross entropy
397
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
between a unigram language model for the query and one for the document. We
discuss the relationship of this model to IR models based on generative language
models. Subsequently, we show several ways to add translation to the model: One
can either translate the query language model from the source language into the target
language (i.e., the document language) before measuring the cross entropy, or translate
the document model from the target language into the source language and then
measure the cross entropy.
4.1 Monolingual IR Based on Unigram Language Models
Recently, a new approach to IR based on statistical language models has gained wide
acceptance. The approach was developed independently by several groups (Ponte
and Croft 1998; Miller, Leek, and Schwartz 1999; Hiemstra 1998) and has yielded
results on several IR standardized evaluation tasks that are comparable to or better
than those obtained using the existing OKAPI probabilistic model. In comparison
with the OKAPI model, the IR model based on generative language models has some
important advantages: It contains fewer collection-dependent tuning parameters and
is easy to extend. For a more detailed discussion of the relationships between the
classical (discriminative) probabilistic IR models and recent generative probabilistic IR
models, we refer the reader to Kraaij and Spitters (2003). Probably the most important
idea in the language-modeling approach to IR is that documents are scored on the
probability that they generate the query; that is, the problem is reversed, an idea that
has successfully been applied in speech recognition. There are various reasons that
this approach has proven fruitful, probably the most important being that documents
contain much more data for estimating the parameters of a probabilistic model than do
ad hoc queries (Lafferty and Zhai 2001b). For ad hoc retrieval, one could describe the
query formulation process as follows: A user has an ideal relevant document in mind
and tries to describe it by mentioning some of the salient terms that he thinks occur in
the document, interspersed with some query stop phrasing like ?Relevant documents
mention. . . .? For each document in the collection, we can compute the probability
that the query is generated from a model representing that document. This generation
process can serve as a coarse way of modeling the user?s query formulation process.
The query likelihood given each document can directly be used as a document-ranking
function. Formula (1) shows the basic language model, in which a query Q consists of
a sequence of terms T1, T2, . . . , Tm that are sampled independently from a document
unigram model for document dk (Table 3 presents an explanation of the most important
symbols used in equations (1)?(12)):
P(Q | Dk) = P(T1, T2, . . . , Tm | Dk) ?
m
?
j=1
P(Tm | MDk) (1)
In this formula MDk denotes a language model of Dk. It is indeed an approximation of
Dk. Now, if a query is more probable given a language model based on document D1
than given a language model based on document D2, we can then hypothesize that
document D1 is more likely to be relevant to the query than document D2. Thus the
probability of generating a certain query given a document-based language model can
serve as a score for ranking documents with respect to topical relevance. It is common
practice to work with log probabilities, which has the advantage of reducing products
to summations. We will therefore rewrite (1) in logarithmic form. Since terms might
occur more than once in a query, we prefer to work with types ?i instead of tokens
398
Computational Linguistics Volume 29, Number 3
Ti. So c(Q, ?i) is the number of occurrences of ?i in Q (query term frequency); we will
also omit the document subscript k in the following presentation:
log P(Q | D) =
n
?
i=1
c(Q, ?i) log P(?i | MD) (2)
A second core technique from speech recognition that plays a vital role in language
models for IR is smoothing. One obvious reason for smoothing is to avoid assigning
zero probabilities for terms that do not occur in a document because the term prob-
abilities are estimated using maximum-likelihood estimation.9 If a single query term
does not occur in a document, this would result in a zero probability of generating
that query, which might not be desirable in many cases, since documents discuss a
certain topic using only a finite set of words. It is very well possible that a term that
is highly relevant for a particular topic may not appear in a given document, since
it is a synonym for other terms that are also highly relevant. Longer documents will
in most cases have a better coverage of relevant index terms (and consequently better
probability estimates) than short documents, so one could let the level of smoothing
depend on the length of the document (e.g., Dirichlet priors). A second reason for
smoothing probability estimates of a generative model for queries is that queries con-
sist of (1) terms that have a high probability of occurrence in relevant documents and
(2) terms that are merely used to formulate a proper query statement (e.g., ?Docu-
ments discussing only X are not relevant?). A mixture of a document language model
and a language model of typical query terminology (estimated on millions of queries)
would probably give good results (in terms of a low perplexity).
We have opted for a simple approach that addresses both issues, namely, applying
a smoothing step based on linear interpolation with a background model estimated on
a large document collection, since we do not have a collection of millions of queries:
log P(Q | D) =
n
?
i=1
c(Q, ?i) log((1 ? ?)P(?i | MD) + ?P(?i | MC)) (3)
Here, P(?i | MC) denotes the marginal probability of observing the term ?i, which can be
estimated on a large background corpus, and ? is the smoothing parameter. A common
range for ? is 0.5?0.7, which means that document models have to be smoothed quite
heavily for optimal performance. We hypothesize that this is mainly due to the query-
modeling role of smoothing. Linear interpolation with a background model has been
frequently used to smooth document models (e.g., Miller, Leek, and Schwartz 1999;
Hiemstra 1998). Recently other smoothing techniques (Dirichlet, absolute discounting)
have also been evaluated. An initial attempt to account for the two needs for smoothing
(sparse data problem, query modeling) with separate specialized smoothing functions
yielded positive results (Zhai and Lafferty 2002).
We have tested the model corresponding to formula (3) in several different IR
applications: monolingual information retrieval, filtering, topic detection, and topic
tracking (cf. Allen [2002] for a task description of the latter two tasks). For several
of these applications (topic tracking, topic detection, collection fusion), it is important
9 The fact that language models have to be smoothed seems to contradict the discussion in Section 3, in
which we stated that rare terms are not critical for IR effectiveness, but it actually does not. Smoothing
helps to make the distinction between absent important terms (middle-frequency terms) and absent
nonimportant terms (high-frequency terms). The score of a document that misses important terms
should be lowered more than that of a document that misses an unimportant term.
399
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
Table 3
Common symbols used in equations (1)?(12) and their
explanations.
Symbol Explanation
Q Query has representation Q = {T1, T2, . . . , Tn}
D Query has representation D = {T1, T2, . . . , Tn}
MQ Query language model
MD Document language model
MC Background language model
?i index term
si term in the source language
ti term in the target language
? smoothing parameter
c(x) counts of x
that scores be comparable across different queries (Spitters and Kraaij 2001). The basic
model does not provide such comparability of scores, so it has to be extended with
score normalization. There are two important steps in doing this. First of all, we would
like to normalize across query specificity. The generative model will produce low scores
for specific queries (since the average probability of occurrence is low) and higher
scores for more general queries. Normalization can be accomplished by modeling the
IR task as a likelihood ratio (Ng 2000). For each term in the query, the log-likelihood
ratio (LLR) model judges how surprising it is to see the term, given the document
model in comparison with the background model:
LLR(Q | D) = log P(Q | MD)
P(Q | MC)
=
n
?
i=1
c(Q, ?i) log
((1 ? ?)P(?i | MD) + ?P(?i | MC))
P(?i | MC)
(4)
In (4), P(Q | MC) denotes the generative probability of the query given a language
model estimated on a large background corpus C. Note that P(Q | MC) is a query-
dependent constant and does not affect document ranking. Actually, model (4) has a
better justification than model (3), since it can be seen as a direct derivative of the
log-odds of relevance if we assume uniform priors for document relevance:
log
P(R | D, Q)
P(R? | D, Q)
= log
P(Q | R, D)
P(Q | R?, D)
+ log
P(R | D)
P(R? | D)
? log P(Q | MD)
P(Q | MC)
+ K (5)
In (5), R refers to the event that a user likes a particular document (i.e., the document
is relevant).
The scores of model (4) still depend on the query length, which can be easily
normalized by dividing the scores by the query length (
?
i c(Q, ?i)). This results in
formula (6) for the normalized log-likelihood ratio (NLLR) of the query:
NLLR(Q | D) =
n
?
i=1
c(Q, ?i)
?
i c(Q, ?i)
log
((1 ? ?)P(?i | MD) + ?P(?i | MC))
P(?i | MC)
(6)
A next step is to view the normalized query term counts c(Q, ?i)/
?
i c(Q, ?i) as
maximum-likelihood estimates of a probability distribution representing the query
P(?i | MQ). The NLLR formula can now be reinterpreted as a relationship between the
400
Computational Linguistics Volume 29, Number 3
two probability distributions P(? | MQ), P(? | MD) normalized by the the third distribu-
tion P(? | MC). The model measures how much better than the background model the
document model can encode events from the query model; or in information-theoretic
terms, it can be interpreted as the difference between two cross entropies:
NLLR(Q | D) =
n
?
i=1
P(?i | Q) log
P(?i | Dk)
P(?i | C)
= H(X | c)? H(X | d) (7)
In (7), X is a random variable with the probability distribution p(?i) = p(?i | MQ), and
c and d are probability mass functions representing the marginal distribution and the
document model. Cross entropy is a measure of our average surprise, so the better a
document model ?fits? a particular query distribution, the higher its score will be.10
The representation of both the query and a document as samples from a dis-
tribution representing, respectively, the user?s request and the document author?s
?mindset? has several advantages. Traditional IR techniques like query expansion
and relevance feedback can be reinterpreted in an intuitive framework of probabil-
ity distributions (Lafferty and Zhai 2001a; Lavrenko and Croft 2001). The framework
also seems suitable for cross-language retrieval. We need only to extend the model
with a translation function, which relates the probability distribution in one language
to the probability distribution function in another language. We will present several
solutions for this extension in the next section.
The NLLR also has a disadvantage: It is less easy in the NLLR to integrate prior
information about relevance into the model (Kraaij, Westerveld, and Hiemstra 2002),
which can be done in a straightforward way in formula (1), by simple multiplication.
CLIR is a special case of ad hoc retrieval, and usually a document length?based prior
can enhance results significantly. A remedy that has proven to be effective is linear
interpolation of the NLLR score with a prior log-odds ratio log (P(R | D)/P(?R | D)
(Kraaij 2002). For reasons of clarity, we have chosen not to include this technique in
the experiments presented here.
In the following sections, we will describe several ways to extend the monolingual
IR model with translation. The section headings include the run tags that will be used
in Section 5 to describe the experimental results.
4.2 Estimating the Query Model in the Target Language (QT)
In Section 4.1, we have seen that the basic retrieval model measures the cross entropy
between two language models: a language model of the query and a language model
of the document.11 Instead of translating a query before estimating a query model
(the external approach), we propose to estimate the query model directly in the target
language. We will do this by decomposing the problem into two components that are
easier to estimate:
P(ti | MQs) =
L
?
j
P(sj, ti | MQs) =
L
?
j
P(ti | sj, MQs)P(sj | MQs) ?
L
?
j
P(ti | sj)P(sj | MQs)
(8)
where L is the size of the source vocabulary. Thus, P(ti | MQs) can be approximated by
combining the translation model P(ti | sj), which we can estimate on the parallel Web
corpus, and the familiar P(sj | MQs), which can be estimated using relative frequencies.
10 The NLLR can also be reformulated as a difference of two Kullback-Leibler divergences (Ng 2000).
11 We omit the normalization with the background model in the formula for presentation reasons.
401
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
This simplified model, from which we have dropped the dependency of P(ti | sj)
on Q, can be interpreted as a way of mapping the probability distribution function
in the source language event space P(sj | MQs) onto the event space of the target
language vocabulary. Since this probabilistic mapping function involves a summation
over all possible translations, mapping the query model from the source language can
be implemented as the matrix product of a vector representing the query probability
distribution over source language terms with the translation matrix P(ti | sj).12 The
result is a probability distribution function over the target language vocabulary.
Now we can substitute the query model P(?i | MQ) in formula (7) with the target
language query model in (8) and, after a similar substitution operation for P(?i | MC),
we arrive at CLIR model QT:
NLLR-QT(Qs | Dt) =
n
?
i=1
L
?
j=1
P(ti | sj)P(sj | MQs) log
(1 ? ?)P(ti | MDt) + ?P(ti | MCt)
P(ti | MCt)
(9)
4.3 Estimating the Document Model in the Source Language (DT)
Another way to embed translation into the IR model is to estimate the document
model in the source language:
P(si | MDt) =
N
?
j
P(si, tj | MDt) =
N
?
j
P(si | tj, MDt)P(tj | MDt) ?
N
?
j
P(si | tj)P(tj | MDt)
(10)
where N is the size of the target vocabulary. Obviously, we need a translation model
in the reverse direction for this approach. Now we can substitute (10) for P(?i | MD)
in formula (6), yielding CLIR model DT:
NLLR-DT(Qs | Dt) =
n
?
i=1
P(si | MQs) log
?N
j=1 P(si | tj)((1 ? ?)P(tj | MDt) + ?P(tj | MCt))
?N
j=1 P(si | tj)P(tj | MCt)
(11)
It is important to realize that both the QT and DT models are based on context-
insensitive translation, since translation is added to the IR model after the indepen-
dence assumption (1) has been made. Recently, a more complex CLIR model based on
relaxed assumptions?context-sensitive translation but term independence?based IR?
has been proposed in Federico and Bertoldi (2002). In experiments on the CLEF test
collections, the aforementioned model also proved to be more effective; however, it has
the disadvantage of reducing efficiency through its use of a Viterbi search procedure.
4.4 Variant Models and Baselines
In this section we will discuss several variant instantiations of the QT and DT models
that help us measure the importance of the number of translations (pruning) and the
weighting of translation alternatives. We also present several baseline CLIR algorithms
taken from the literature and discuss their relationship to the QT and DT models.
12 For presentation reasons, we have replaced the variable ? used in Section 4.1 with s and t for a term in
the source and target language, respectively.
402
Computational Linguistics Volume 29, Number 3
4.4.1 External Translation (MT, NAIVE). As we argued in Section 1, the most simple
solution to CLIR is to use an MT system to translate the query and use the translation
as the basis for a monolingual search operation in the target language. This solution
does not require any modification to the standard IR model as presented in Section 4.1.
We will refer to this model as the external-translation approach. The translated query
is used to estimate a probability distribution for the query in the target language. Thus,
the order of operations is: (1) translate the query using an external tool; (2) estimate
the parameters P(ti | MQt) of a language model based on this translated query.
In our experimental section below, we will list results with two different instantia-
tions of the external-translation approach: (1) MT: query translation by Systran, which
attempts to use high-level linguistic analysis, context-sensitive translation, extensive
dictionaries, etc., and (2) NAIVE: naive replacement of each query term with its trans-
lations (not weighted). The latter approach is often implemented using bilingual word
lists for CLIR. It is clear that this approach can be problematic for terms with many
translations, since they would then be assigned a higher relative importance. The
NAIVE method is included here only to study the effect of the number of translations
on the effectiveness of various models.
4.4.2 Best-Match Translation (QT-BM). In Section 3.2 we explained that there are
different possible strategies for pruning the translation model. An extreme pruning
method is best match, in which only the best translation is kept. A best-match transla-
tion model for query model translation (QT-BM) could also be viewed as an instance
of the external translation model, but one that uses a corpus-based disambiguation
method. Each query term is translated by the most frequent translation in the Web
corpus, disregarding the query context.
4.4.3 Equal Probabilities (QT-EQ). If we don?t know the precise probability of each
translation alternative for a given term, the best thing to do is to fall back on uniform
translation probabilities. This situation arises, for example, if we have only standard
bilingual dictionaries. We hypothesize that this approach will be more effective than
NAIVE but less effective than QT.
4.4.4 Synonym-Based Translation (SYN). An alternative way to embed translation
into the retrieval model is to view translation alternatives as synonyms. This is, of
course, something of an idealization, yet there is certainly some truth to the approach
when translations are looked up in a standard bilingual dictionary. Strictly speaking,
when terms are pure synonyms, they can be substituted for one another. Combining
translation alternatives with the synonym operator of the INQUERY IR system (Broglio
et al 1995), which conflates terms on the fly, has been shown to be an effective way
of improving the performance of dictionary-based CLIR systems (Pirkola 1998). In
our study of stemming algorithms (Kraaij and Pohlmann 1996), we independently
implemented the synonym operator in our system. This on-line conflation function
replaces the members of the equivalence class with a class ID, usually a morphological
root form. We have used this function to test the effectiveness of a synonymy-based
CLIR model in a language model IR setting.
The synonym operator for CLIR can be formalized as the following class equiva-
lence model (assuming n translations tj for term si and N unique terms in the target
language):
P(class(si) | MDt) =
?n
j c(tj, Dt)
?N
j c(tj, Dt)
=
N
?
j
?(si, tj)P(tj | MDt) (12)
403
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
where P(class(si) | MDt) is the probability that a member of the equivalence class of si
is generated by the language model MDt and
?(si, tj) =
{
1 if tj ? class(si)
0 if tj /? class(si)
(13)
Here c(tj, Dt) is the term frequency (counts) of term tj in document Dt.
The synonym class function ?(si, tj) can be interpreted as a special instantiation
of the translation model P(si | tj) in (10), namely, P(si | tj) = 1 for all translations tj
of si. Of course, this does not yield a valid probability function, since the translation
probabilities for all translations si of a certain tj do not sum to one, because the pseudo?
synonym classes are not disjunct because of sense ambiguity. But the point is that the
structure of a probabilistic version of the SYN model is similar to that of the DT model,
namely, one in which all translations have a reverse translation probability P(si | tj)
equal to one. This is obviously just an approximation of reality. We therefore expect that
this model will be less effective than the QT and DT models. In our implementation
of the SYN model, we formed equivalence classes by looking up all translations of a
source term si in the translation model P(tj | si). The translations receive a weight of
one and are used as pseudo translation?probabilities13 in the model corresponding to
formula (11).
4.5 Related Work
In dictionary-based approaches, the number of translation alternatives is usually not
as high as in (unpruned) translation models, so these alternatives can be used in
some form of query expansion (Hull and Grefenstette 1996; Savoy 2002). However, it
is well known that most IR models break down when the number of translations is
high. To remedy this, researchers have tried to impose query structure, for example,
by collecting translation alternatives in an equivalence class (Pirkola 1998), or via a
quasi-Boolean structure (Hull 1997).
The idea of embedding a translation step into an IR model based on query like-
lihood was developed independently by several researchers (Hiemstra and de Jong
1999; Kraaij, Pohlmann, and Hiemstra 2000; Berger and Lafferty 2000). Initially trans-
lation probabilities were estimated from machine-readable dictionaries, using simple
heuristics (Hiemstra et al 2001). Other researchers have successfully used models sim-
ilar to DT, in combination with translation models trained on parallel corpora, though
not from the Web (McNamee and Mayfield 2001; Xu, Weischedel, and Nguyen 2001).
5. Experiments
We carried out several contrastive experiments to gain more insight into the relative
effectiveness of the various CLIR models presented in Sections 4.2?4.4. We will first
outline our research questions, before describing the experiments in more detail.
5.1 Research Questions
The research questions we are hoping to answer are the following:
1. How do CLIR systems based on translation models perform with respect
to reference systems (e.g., monolingual, MT )?
13 It may be better to view them as mixing weights in this case.
404
Computational Linguistics Volume 29, Number 3
2. Which manner of embedding a translation model is most effective for
CLIR? How does a probabilistically motivated embedding compare with
a synonym-based embedding?
3. Is there a query expansion effect, and if so, how can we exploit it?
4. What is the relative importance of pruning versus weighting?
5. Which models are robust against noisy translations?
The first two questions concern the main goal of our experiments: What is the effec-
tiveness of a probabilistic CLIR system in which translation models mined from the
Web are an integral part of the model, compared to that of CLIR models in which
translation is merely an external component? The remaining questions help us to un-
derstand the relative importance of various design choices in our approach, such as
pruning and translation model orientation.
5.2 Experimental Conditions
We have defined a set of contrastive experiments in order to help us answer the
research questions presented above. These experiments seek to compare:
1. The effectiveness of approaches incorporating a translation model
produced from the Web to that of a monolingual baseline and an
off-the-shelf external query translation approach based on Systran (MT).
2. The effectiveness of embedding query model translation (QT) and that of
document model translation (DT).
3. The effectiveness of using the entire set of translations, each of which is
weighted, (QT) to that of using just the most probable translation
(QT-BM).
4. The effectiveness of weighted query model translation (QT) to that of
equally weighted translations (QT-EQ) and nonweighted translations
(NAIVE).
5. The effectiveness of treating translations as synonyms (SYN) with that of
weighted translations (QT) and equally weighted translations (QT-EQ).
6. Different translation model pruning strategies: best N parameters or
thresholding probabilities.
Each strategy is represented by a run tag, as shown in Table 4.
Table 5 illustrates the differences among the different translation methods. It lists,
for several CLIR models, the French translations of the word drug taken from one of
the test queries that talks about drug policy.
The translations in Table 5 are provided by the translation models P(e | f ) and
P(f | e). The translation models have been pruned by discarding the translations with
P < 0.1 and renormalizing the model (except for SYN), or by retaining the 100K best
parameters of the translation model. The first pruning method (probability threshold)
has a very different effect on the DT method than on the QT method: The number of
terms that translate into drug, according to P(e | f ), is much larger than the number
of translations of drug found in P(f | e). There are several possible explanations for
this: Quite a few French terms, including the verb droguer and the compounds pharma-
core?sistance and pharmacothe?rapie, all translate into an English expression or compound
405
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
Table 4
Explanation of run tags.
Matching
Run Tag Short Description Language Section
MONO monolingual run 4.1, 5.5
MT Systran external query translation target 4.4.1, 5.5
NAIVE equal probabilities target 4.4.1
QT translation of the query language model target 4.2
DT translation of the document language model source 4.3
QT-BM best match, one translation per word target 4.4.2
QT-EQ equal probabilities target 4.4.3
SYN synonym run based on forward equal probabilities source 4.4.4
Table 5
Example translations: Stems and probabilities with different CLIR methods.
Run ID Translation Translation
Model
MT drogues
QT <drogue, 0.55; medicament, 0.45> P(f | e) ? 0.1
QT-EQ <drogue, 0.5; medicament, 0.5>
QT-BM <drogue, 1.0>
SYN <drogue, 1.0; medicament, 1.0>
NAIVE <drogue, 1.0; medicament, 1.0>
DT <antidrogue, 1.0; drogue, 1.0; droguer, 1.0; drug,
1.0; me?dicament, 0.79; drugs, 0.70; drogue?, 0.61;
narcotrafiquants, 0.57; relargage, 0.53; pharmacovigi-
lance, 0.49; pharmacore?sistance, 0.47; me?dicamenteux,
0.36; ste?ro??diens, 0.35, stupe?fiant, 0.34; assurance-
me?dicaments, 0.33; surdose, 0.28; pharmacore?sistants,
0.28; pharmacode?pendance, 0.27; pharmacothe?rapie,
0.25; alcoolisme, 0.24; toxicomane, 0.23; bounce, 0.23; an-
ticance?reux, 0.22; anti-inflammatoire, 0.17; selby, 0.16; es-
cherichia, 0.14; homelessness, 0.14; anti-drogues, 0.14; an-
tidiarrhe?ique, 0.12; imodium, 0.12; surprescription, 0.10>
P(e | f ) ? 0.1
QT <drogue, 0.45; medicament, 0.35; consommation, 0.06; re-
lier, 0.03; consommer, 0.02; drug, 0.02; usage, 0.02; toxico-
manie, 0.01; substance, 0.01; antidrogue, 0.01; utilisation,
0.01; lier, 0.01; the?rapeutique, 0.01; actif, 0.01; pharmaceu-
tique, 0.01>
P(e | f ), 100K
DT <reflexions, 1; antidrogue, 1; narcotrafiquants, 1;
drug, 1; droguer, 0.87; drogue, 0.83; drugs, 0.81;
me?dicament, 0.67; pharmacore?sistance, 0.47; pharma-
core?sistants, 0.44; me?dicamenteux, 0.36; stupe?fiant, 0.34;
assurance-me?dicaments, 0.33; pharmacothe?rapie, 0.33;
amphe?tamine, 0.18; toxicomane, 0.17; me?morandum,
0.10; toxicomanie, 0.08; architectural, 0.08; pharmacie,
0.07; pharmaceutique, 0.06; the?rapeutique, 0.04; sub-
stance, 0.01>
P(f | e), 100K
406
Computational Linguistics Volume 29, Number 3
involving the word drug. Since our translation model is quite simple, these compound-
compound translations are not learned.14 A second factor that might play a role is the
greater verbosity of French texts compared to their English equivalents (cf. Table 2).
For the models that have been pruned using the 100K-best-parameters criterion, the
differences between QT and DT are smaller. Both methods yield multiple translations,
most of which seem related to drug, so there is a clear potential for improved recall as
a result of the query expansion effect. Notice, however, that the expansion concerns
both the medical and the narcotic senses of the word drug. We will see in the following
section that the CLIR model is able to take advantage of this query expansion effect,
even if the expansion set is noisy and not disambiguated.
5.3 The CLEF Test Collection
To answer the research questions stated in section 5.1, we carried out a series of ex-
periments on a combination of the CLEF-2000, -2001 and -2002 test collections.15 This
joint test collection consists of documents in several languages (articles from major
European newspapers from the year 1994), 140 topics describing different informa-
tion needs (also in several languages) and their corresponding relevance judgments.
(Relevance judgments are a human-produced resource that states, for a subset of a
document collection, whether a document is relevant for a particular query.) We used
only the English, Italian, and French data for the CLIR experiments reported here. The
main reason for this limitation was that the IR experiments and translation models
were developed at two different sites equipped with different proprietary tools. We
chose language pairs for which the lemmatization/stemming step for both the trans-
lation model training and indexing system were equivalent. A single test collection
was created by merging the three topic sets in order to increase the reliability of our
results and sensitivity of significance tests. Each CLEF topic consists of three parts:
title, description, and narrative. An example is given below:
<num> C001
<title> Architecture in Berlin
<description> Find documents on architecture in Berlin.
<narrative> Relevant documents report, in general, on the
architectural features of Berlin or, in particular, on the
reconstruction of some parts of the city after the fall of the
Wall.
We used only the title and description parts of the topics and concatenated
these simply to form the queries. Table 6 lists some statistics on the test collection.16
The documents were submitted to the preprocessing (stemming/lemmatization)
procedure we described in Section 3.1.2. However, for English and French lemmatiza-
tion, we used the Xelda tools from XRCE,17 which perform morphological normaliza-
tion slightly differently from the one described in Section 3.1.2. However, since the two
14 A more extreme case is query C044 about the ?tour de france.? According to the P(e | f ) > 0.1
translation model, there are 902 French words that translate into the ?English? word de. This is mostly
due to French proper names, which are left untranslated in the English parallel text.
15 CLEF = Cross Language Evaluation Forum, ?www.clef-campaign.org?.
16 Topics without relevant documents in a subcollection were discarded.
17 Available at ?http://www.xrce.xerox.com/competencies/ats/xelda/summary.html?.
407
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
Table 6
Statistics on the test collection.
French English Italian
Document Source Le Monde Los Angeles Times La Stampa
Number of documents 44,013 110,250 58,051
Number of topics 124 122 125
Number of relevant documents 1,189 2,256 1,878
lemmatization strategies are based on the same principle (POS tagging plus inflection
removal), the small differences in morphological dictionaries and POS tagging had no
significant influence on retrieval effectiveness.18
All runs use a smoothing parameter ? = 0.3. This value had been shown to work
well for CLIR experiments with several other collections.
5.4 Measuring Retrieval Effectiveness
The effectiveness of retrieval systems can be evaluated using several measures. The
basic measures are precision and recall, which cannot be applied directly, since they
assume clearly separated classes of relevant and nonrelevant documents. The most
widely accepted measure for evaluating effectiveness of ranked retrieval systems is
the average uninterpolated precision, most often referred to as mean average precision
(MAP), since the measure is averaged first over relevant documents and then across
topics. Other measures, such as precision at a fixed rank, interpolated precision, or
R-precision, are strongly correlated to the mean average precision, so they do not really
provide additional information (Tague-Sutcliffe and Blustein 1995; Voorhees 1998).
The average uninterpolated precision for a given query and a given system version
can be computed as follows: First identify the rank number n of each relevant docu-
ment in a retrieval run. The corresponding precision at this rank number is defined
as the number of relevant documents found in the ranks equal to or higher than the
respective rank r divided by n. Relevant documents that are not retrieved are assigned
a precision of zero. The average precision for a given query is defined as the average
value of the precision pr over all known relevant documents dij for that query. Finally,
the mean average precision can be calculated by averaging the average precision over
all M queries:
MAP =
1
M
M
?
j=1
1
Nj
Nj
?
i=1
pr(dij), where pr(dij) =
{ rni
ni
, if dij retrieved and ni ? C
0, in other cases
(14)
Here, ni denotes the rank of the document dij, which has been retrieved and is relevant
for query j, rni is the number of relevant documents found up to and including rank
ni, Nj is the total number of relevant documents of query j, M is the total number of
queries, and C is the cutoff rank (C is 1,000 for TREC experiments).
18 We have not been able to substantiate this claim with quantitative figures but did analyze the lemmas
that were not found in the translation dictionaries during query translation. We did not find any
structural mismatches.
408
Computational Linguistics Volume 29, Number 3
Since we compared many different system versions, which do not always dis-
play a large difference in effectiveness, it is desirable to perform significance tests on
the results. However, it is well known that parametric tests for data resulting from
IR experiments are not very reliable, since the assumptions of these tests (normal or
symmetric distribution, homogeneity of variances) are usually not met. We checked
the assumptions for an analysis of variance (by fitting a linear model for a within-
subjects design) and found that indeed the distribution of the residual error was quite
skewed, even after transformation of the data. Therefore, we resorted to a nonpara-
metric alternative for the analysis of variance, the Friedman test (Conover 1980). This
test is preferable, for the analysis of groups of runs instead, to multiple sign-tests or
Wilcoxon signed-rank tests, since it provides overall alpha protection. This means that
we first test whether there is any significant difference at all between the runs, before
applying multiple-comparison tests. Applying just a large number of paired signifi-
cance tests at the 0.05 significance level without a global test leads very quickly to a
high overall alpha. After applying the Friedman test, we ran Fisher?s LSD multiple-
comparison tests (recommended by Hull) to identify equivalence classes of runs (Hull
1993; Hull, Kantor, and Ng 1999). An equivalence class is a group of runs that do
not differ significantly (e.g., in terms of mean average precision) from one another in
terms of performance.
5.5 Baseline Systems
We decided to have two types of baseline runs. It is standard practice to take a mono-
lingual run as a baseline. This run is based on an IR system using document ranking
formula (6). Contrary to runs described in Kraaij (2002), we did not use any additional
performance-enhancing devices, like document length?based priors or fuzzy match-
ing, in order to focus on the basic retrieval model extensions, avoiding interactions.
Systran was used as an additional cross-language baseline, to serve as a reference
point for cross-language runs. Notice that the lexical coverage of MT systems varies
considerably across language pairs; in particular, the French-English version of Systran
is quite good in comparison with those available for other language pairs. We accessed
the Web-based version of Systran (December 2002), marketed as Babelfish, using the
Perl utility babelfish.pm and converted the Unicode output to the ISO-Latin1 character
set to make it compatible with the Xelda-based morphology.
5.6 Results
Table 7 shows the results for the different experimental conditions in combination
with a translation model pruned with the probability threshold criterion P > 0.1 (cf.
Section 3.2). For each run, we computed the mean average precision using the standard
evaluation tool trec eval. We performed Friedman tests on all the runs based on the
Web translation models, because these are the runs in which we are most interested;
furthermore, one should avoid adding runs that are quite different to a group that is
relatively homogeneous, since this can easily lead to a false global-significance test.
The Friedman test (as measured on the F distribution) proved significant at the p <
0.05 level in all cases, so we created equivalence classes using Fisher?s LSD method,
which are denoted by letters. Letters are assigned to classes in decreasing order of
performance; so if a run is a member of equivalence class a, it is one of the best runs
for that particular experimental condition.
The last four rows of the table provide some additional statistics on the query
translation process. For both the forward (P(t | s),fw) and the reverse (P(s | t),rev)
409
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
Table 7
Mean average precision and translation statistics (p > 0.1).
English- French- English- Italian-
Run ID French English Italian English
MONO 0.4233 0.4705 0.4542 0.4705
MT 0.3478 0.4043 0.3060 0.3249
QT a:0.3760 a:0.4126 a,b:0.3298 a:0.3526
DT a:0.3677 a,b:0.4090 a:0.3386 a,b:0.3328
SYN a:0.3730 b,c:0.3987 a,b:0.3114 b:0.3498
QT-EQ a:0.3554 a,b:0.3987 c,d:0.3035 b,c:0.3299
QT-BM a:0.3463 c,d:0.3769 b,c:0.3213 b:0.3221
NAIVE b:0.3303 d:0.3596 d:0.2881 c:0.3183
Percentage of missed forward 9.6 13.54 16.79 9.17
Percentage of missed reverse 9.08 14.04 15.48 11.31
Number of translations forward 1.65 1.66 1.86 2.13
Number of translations reverse 22.72 29.6 12.00 22.95
Table 8
Mean average precision and translation statistics (best 100K parameters).
English- French- English- Italian-
Run ID French English Italian English
MONO 0.4233 0.4705 0.4542 0.4705
MT 0.3478 0.4043 0.3060 0.3249
DT a:0.3909 a:0.4073 a:0.3728 a:0.3547
QT a,b:0.3878 a:0.4194 a:0.3519 a:0.3678
QT-BM b:0.3436 b:0.3702 b:0.3236 b:0.3124
SYN c:0.3270 b:0.3643 b:0.2958 c:0.2808
QT-EQ c:0.3102 b:0.3725 c:0.2602 c:0.2595
NAIVE d:0.2257 c:0.2329 d:0.2281 d:0.2021
Percentage of missed forward 11.04 14.65 16.06 9.36
Percentage of missed reverse 10.39 16.81 15.76 10.53
Number of translations forward 7.04 7.00 6.36 7.23
Number of translations reverse 10.51 12.34 13.32 17.20
translation model, we list the percentage of missed translations19 of unique query
terms and the average number of translations per unique query term. Table 8 shows
the results for the same experimental conditions, but this time the translation models
were pruned by taking the n best translation relations according to an entropy criterion,
where n = 100, 000.
Several other similar pruning methods have also been tested on the CLEF-2000
subset of the data (e.g. P > 0.01, P > 0.05, 1M parameters, 10K parameters). How-
ever, the two cases shown in Tables 7 and 8 represent the best of the two families
of pruning techniques. Our goal was not to do extensive parameter tuning in or-
der to find the best-performing combination of models, but rather to detect some
broad characteristics of the pruning methods and their interactions with the retrieval
model.
19 This figure includes proper nouns.
410
Computational Linguistics Volume 29, Number 3
Table 9
Mean average precision of combination run, compared to baselines.
Run ID English-French French-English English-Italian Italian-English
MONO 0.4233 0.4705 0.4542 0.4705
MT 0.3478 (82%) 0.4043 (86%) 0.3060 (67%) 0.3249 (69%)
DT+QT 0.4042 (96%) 0.4273 (87%) 0.3837 (84%) 0.3785 (80%)
Since the pruned forward and reverse translation models yield different translation
relations (cf. Table 5), we hypothesized that it might be effective to combine them.
Instead of combining the translation probabilities directly, we chose to combine the
results of the QT and DT models by interpolation of the document scores. Results
for combinations based on the 100K models are shown in Table 9. Indeed, for all
the language pairs, the combination run improves upon each of its component runs.
This means that each component run can compensate for missing translations in the
companion translation model.
5.7 Discussion
5.7.1 Web-Based CLIR versus MT-Based CLIR. Our first observation when examining
the data is that the runs based on translation models are comparable to or better than
the Systran run. Sign tests showed that there was no significant difference between the
MT and QT runs for English-French and French-English language pairs. The QT runs
were significantly better at the p = 0.01 level for the Italian-English and English-Italian
language pairs.
This is a very significant result, particularly since the performance of CLIR with
Systran has often been among the best in the previous CLIR experiments in TREC and
CLEF. These results show that the Web-based translation models are effective means for
CLIR tasks. The better results obtained with the Web-based translation models confirm
our intuition, stated in Section 1, that there are better tools for query translation in
CLIR than off-the-shelf commercial MT systems.
Compared to the monolingual runs, the best CLIR performance with Web-based
translation models varies from 74.1% to 93.7% (80% to 96% for the combined QT+DT
models) of the monolingual run. This is within the typical range of CLIR performance.
More generally, this research successfully demonstrates the enormous potential of par-
allel Web pages and Web-based MT.
We cannot really compare performance across target languages, since the relevant
documents are not distributed in a balanced way: Some queries do not yield any rele-
vant document in some languages. This partly explains why the retrieval effectiveness
of the monolingual Italian-Italian run is much higher than the monolingual French
and English runs. We can, however, compare methods within a given language pair.
5.7.2 Comparison of Query Model Translation (QT), Document Model Translation
(DT), and Translations Modeled as Synonyms (SYN). Our second question in Section
5.1 concerned the relative effectiveness of the QT and DT models. The experimental
results show that there is no clear winner; differences are small and not significant.
There seems to be some correlation with translation direction, however: The QT models
perform better than DT on the X-English pairs, and the DT models perform better
on the English-X pairs. This might indicate that the P(e | f ) and P(e | i) translation
models are more reliable than their reverse counterparts. A possible explanation for
411
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
this could be that the average English sentence is shorter than the corresponding
French and Italian sentence. The average number of tokens per sentence is, respectively,
6.6/6.9 and 5.9/6.9 for English/French and English/Italian corpora. This may lead to
more reliable estimates for P(e | f ) and P(e | i) than the reverse. However, further
investigation is needed to confirm this, since differences in morphology could also
contribute to the observed effect. Still, the fact that QT models perform just as well as
DT models in combination with translation models is a new result.
We also compared our QT and DT models to the synonym-based approach (SYN)
(Pirkola 1998). Both the QT and DT models were significantly more effective than the
synonym-based model. The latter seems to work well when the number of translations
is relatively small but cannot effectively handle the large number of (pseudo)translations
produced by our 100K translation models. The synonym-based model usually per-
forms better than the models based on query translation with uniform probabilities,
but the differences are not significant in most cases.
5.7.3 Query Expansion Effect. In Section 1 we argued that using just one translation
(as MT does) is probably a suboptimal strategy for CLIR, since there is usually more
than one good translation for a particular term. Looking at probabilistic dictionaries,
we have also seen that the distinction between a translation and a closely related term
cannot really be made on the basis of some thresholding criterion. Since it is well
known in IR that adding closely related terms can improve retrieval effectiveness, we
hypothesized that adding more than one translation would also help. The experimen-
tal results confirm this effect. In all but one case (English-French, P > 0.1), using all
translations (QT) yielded significantly better performance than choosing just the most
probable translation (QT-BM). For the P > 0.1 models, the average number of transla-
tions in the forward direction is only 1.65, so the potential for a query expansion effect
is limited, which could explain the nonsignificant difference for the English-French
case.
Unfortunately, we cannot say whether the significant improvement in effectiveness
occurs mainly because the probability of giving at least one good translation (which is
probably the most important factor for retrieval effectiveness [Kraaij 2002; McNamee
and Mayfield 2002]) is higher for QT or indeed because of the query expansion effect.
A simulation experiment is needed to quantify the relative contributions. Still, it is
of great practical importance that more (weighted) translations can enhance retrieval
effectiveness significantly.
5.7.4 Pruning and Weighting. A related issue is the question of whether it is more
important to prune translations or to weight them. Grefenstette (cf. Section 1) originally
pointed out the importance of pruning and weighting translations for dictionary-based
CLIR. Pruning was seen as a means of removing unwanted senses in a dictionary-
based CLIR application. Our experiments confirm the importance of pruning and
weighting, but in a slightly different manner. In a CLIR approach based on a Web
translation model, the essential function of pruning is to remove spurious translations.
Polluted translation models will result in a very poor retrieval effectiveness. As far
as sense disambiguation is concerned, we believe that our CLIR models can handle
sense ambiguity quite well. Our best-performing runs, based on the 100K models,
have on average seven translations per term! Too much pruning (e.g., best match) is
suboptimal. However, the more translation alternatives we add, the more important
their relative weighting becomes.
We have compared weighted translations (QT) with uniform translation proba-
bilities (QT-EQ). In each of the eight comparisons (four language pairs, two pruning
412
Computational Linguistics Volume 29, Number 3
techniques), weighting results in an improved retrieval effectiveness. The difference
is significant in six of the eight cases. Differences are not significant for the P < 0.1
English-French and French-English translation models. We think this is due to the
small number of translations; a uniform translation probability will not differ radi-
cally from the estimated translation probabilities.
The importance of weighting is most evident when the 100K translation models
are used. These models yield seven translations on average for each term. The CLIR
models based on weighted translations are able to exploit the additional information
and show improved effectiveness with respect to the P < 0.1 models. The performance
of unweighted CLIR models (QT-EQ and SYN) is seriously impaired by the higher
number of translations.
The comparison of the naive dictionary-like replacement method, which does not
involve any normalization for the number of translations per term (NAIVE), with QT-
EQ shows that normalization (i.e. a minimal probabilistic embedding) is essential. The
NAIVE runs have the lowest effectiveness of all variant systems (with significant dif-
ferences). Interestingly, it seems better to select just the one most probable translation
than taking all translations unweighted.
5.7.5 Robustness. We pointed out in the previous section that the weighted models
are more robust, in the sense that they can handle a large number of translations.
We found, however, that the query model translation method (QT) and the docu-
ment model translation method (DT) display a considerable difference in robustness
to noisy translations. Initially we expected that the DT method (in which the match-
ing takes place in the source language) would yield the best results, since this model
has previously proven to be successful for several quite different language pairs (e.g.,
European languages, Chinese, and Arabic using parallel corpora or dictionaries as
translation devices [McNamee and Mayfield 2001; Xu, Weischedel, and Nguyen 2001;
Hiemstra et al 2001]).
However, our initial DT runs obtained extremely poor results. We discovered that
this was largely due to noisy translations from the translation model (pruned by the
P < 0.1 or 100K method), which is based on Web data. There are many terms in
the target language that occur very rarely in the parallel Web corpus. The translation
probabilities for these terms (based on the most probable alignments) are therefore
unreliable. Often these rare terms (and nonwords like xc64) are aligned with more
common terms in the other language and are not pruned by the default pruning criteria
(P > 0.1 or best 100K parameters), since they have high translation probabilities.
This especially poses a problem for the DT model, since it includes a summation
over all terms in the target language that occur in the document and have a nonzero
translation probability. We devised a supplementary pruning criterion to remove these
noisy translations, discarding all translations for which the source term has a marginal
probability in the translation model that is below a particular value (typically between
10?6 and 10?5). Later we discovered that a simple pruning method was even more
effective: discarding all translations for which either the source or target term contains
a digit. The results in Tables 7 and 8 are based on the latter additional pruning criterion.
The QT approach is less sensitive to noisy translations arising from rare terms in the
target language, because it is easy to remove these translations using a probability
threshold. We deduce that extra care therefore has to be taken to prune translation
models for the document model translation approach to CLIR.
413
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
6. Conclusions
Statistical translation models require large parallel corpora, and unfortunately, only
a few manually constructed ones are available. In this article, we have explored the
possibility of automatically mining the Web for parallel texts in order to construct
such corpora. Translation models are then trained on these corpora. We subsequently
examined different ways to embed the resulting translation models in a cross-language
information retrieval system.
To mine parallel Web pages, we constructed a mining system called PTMiner. This
system employs a series of heuristics to locate candidate parallel pages and determine
whether they are indeed parallel. We have successfully used PTMiner to construct cor-
pora for a number of different language pairs: English-French, English-Italian, English-
German, English-Dutch, and English-Chinese. The language-independent characteris-
tics of PTMiner allowed us to adapt it quite easily to different language pairs.
The heuristics used in the mining process seem to be effective. Although the system
cannot collect all pairs of parallel pages, our preliminary evaluation shows that its
precision is quite high. (The recall ratio is less important in this context because of the
abundance of parallel pages on the Web.)
The mining results?parallel corpora?are subsequently used to train statistical
translation models, which are exploited in a CLIR system. The major advantage of
this approach is that it can be fully automated, avoiding the tedious work of man-
ual collection of parallel corpora. On the other hand, compared to manually prepared
parallel corpora, our mining results contain more noise (i.e., nonparallel pages). For
a general translation task this may be problematic; for CLIR, however, the noise con-
tained in the corpora is less dramatic. In fact, IR is strongly error tolerant. A small
proportion of incorrect translation words can be admitted without a major impact
on global effectiveness. Our experiments showed that a CLIR approach based on the
mined Web corpora can in fact outperform a good MT system (Systran). This confirms
our initial hypothesis that noisy parallel corpora can be very useful for applications
such as CLIR. Our demonstration that the Web can indeed be used as a large parallel
corpus for tasks such as CLIR is the main contribution of this article.
Most previous work on CLIR has separated the translation stage from the retrieval
stage (i.e., query translation is considered as a preprocessing step for monolingual IR).
In this article, we have integrated translation and retrieval within the same framework.
The advantage of this integration is that we do not need to obtain the optimal transla-
tion of a source query, and then an optimal retrieval result given a query translation,
but instead aim for the optimal global effect. The comparisons between our approach
and simulated external approaches clearly show that an integrated approach performs
better.
We also compared two ways of embedding translation models within a CLIR sys-
tem: (1) translating the source query model into the target (document) language, and
(2) translating the document model into the source language.20 Both embedding meth-
ods produced very good results compared to our reference run with Systran. However,
it is still too early to assert which embedding method is superior. We did observe a
significant difference in robustness between the two methods: The document model
translation method is much more sensitive to spurious translations, since the model
incorporates into a query term all source terms that have a nonzero translation proba-
bility. We devised two supplementary pruning techniques that effectively removed the
20 Another method that interprets multiple translations as synonyms is a special case of the latter.
414
Computational Linguistics Volume 29, Number 3
noisy terms: removing terms containing digits, and removing translations based on
source terms with a low marginal probability. (This latter approach is perhaps more
principled.)
On the use of statistical translation models for CLIR, we have demonstrated that
this naturally produces a desired query expansion effect, resulting in more related
documents being found. In our experimental evaluation, we saw that it is usually
better to include more than one translation, and to weigh these translations according
to the translation probabilities, rather than using the resulting translation model as
a bilingual lexicon for external translation. This effect partly accounts for the success
of our approach in comparison with an MT-based approach, which retains only one
translation per sense. However, this technique should not be exaggerated; otherwise,
too much noise will be introduced. To avoid this, it is important to incorporate pruning.
We investigated several ways to prune translation models. The best results were
obtained with a pruning method based on the top 100K parameters of the transla-
tion model. The translation models pruned with the best 100K parameters method
produced more than seven translations per word on average, demonstrating the ca-
pability of the CLIR model to handle translation ambiguity and exploit co-occurrence
information from the parallel corpus for query expansion.
There are several ways in which our approach can be improved. First, regarding
PTMiner, more or better heuristics could be integrated into the mining algorithm. As
we mentioned, parallel Web sites are not always organized in the ways we would
expect. This is particularly the case for those in non-European languages such as Chi-
nese and Japanese. Hence, one of the questions we wish to investigate is how to
extend the coverage of PTMiner to more parallel Web pages. One possible improve-
ment would be to integrate a component that ?learns? the organization patterns of
a particular Web site (assuming, of course, that the Web site is organized in a con-
sistent way). Preliminary tests have shown that this is possible to some extent: We
can recognize dynamically that the parallel pages on ?www.operationid.com? are at
?www.operationcarte.com? or that the file ?index1.html? corresponds to ?index2.html?.
Such criteria complement the ones currently employed in PTMiner.
In its current form, PTMiner scans candidates for parallel Web sites according to
similarities in their file names. This step does not exploit the hyperlinks between the
pages, whereas we know that two pages that are referenced at comparable structural
positions in two parallel pages have a very high chance of themselves being parallel.
Exploiting hyperlink structure to (help) find parallel Web pages could well improve
the quality of PTMiner.
When the mining results are not fully parallel, it would be interesting to attempt
to clean them in order to obtain a higher-quality training material. One possible ap-
proach for doing this would be to use sentence alignment as an additional filter, as
we mentioned earlier. This approach has been applied successfully to our English-
Chinese Web corpus. The cleaned corpus results in both higher translation accuracy
and higher CLIR effectiveness. However, this approach has still to be tested for the
European languages.
In this study, we hypothesized that IBM Model 1 is appropriate for CLIR, primarily
because word order is not important for IR. Although it is true that word order is not
important in current IR approaches, it is definitely important to consider context words
during the translation. For example, when deciding how to translate the French word
tableau (which may refer to a painting, a blackboard, a table [of data], etc.), if we
observe artistique (?artistic?) next to it, then it is pretty certain that tableau refers to a
painting. A more sophisticated translation model than IBM Model 1 could produce a
better selection of translation words.
415
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
We also rely solely on word translation in our approach, although it is well known
that this simplistic approach cannot correctly translate compound terms such as pomme
de terre (?potato?) and cul de sac (?no exit?). Incorporating the translation of compound
terms in a translation model should result in additional improvements for CLIR. Our
preliminary experiments (Nie and Dufort 2002) on integrating the translation of com-
pounds certainly showed this, with improvement of up to 70% over a word-based
approach. This direction warrants further investigation.
Finally, all our efforts thus far to mine parallel Web pages have involved English.
How can we deal with CLIR between, say, Chinese and German, for which there are
few parallel Web sites? One possible solution would be to use English as a pivot
language, even though the two-step translation involved would certainly reduce ac-
curacy and introduce more noise. Nevertheless, several authors have shown that a
pivot approach can still produce effective retrieval and can at least complement a
dictionary-based approach (Franz, McCarley, and Ward 2000; Gollins and Sanderson
2001; Lehtokangas and Airio 2002).
Acknowledgments
This work was partly funded by a research
grant from the Dutch Telematics Institute:
DRUID project. We would like to thank
Xerox Research Center Europe (XRCE) for
making its Xelda toolkit available to us. We
would also like to thank George Foster for
making his statistical MT toolkit available
and for many interesting discussions.
Special thanks are due to Jiang Chen, who
contributed to the building of PTMiner.
Finally, we want to thank Elliott
Macklovitch and the two anonymous
reviewers for their constructive comments
and careful review. Part of this work was
carried out while the first author was
visiting the RALI laboratory at Universite?
de Montre?al.
References
Allen, James, editor. 2002. Event-Based
Information Organization. Kluwer
Academic, Boston.
Baum, L. E. 1972. An inequality and
associated maximization technique in
statistical estimations of probabilistic
functions of Markov processes.
Inequalities, 3:1?8.
Berger, Adam and John Lafferty. 2000. The
Weaver system for document retrieval. In
Ellen M. Voorhees and Donna K. Harman,
editors, The Eighth Text Retrieval Conference
(TREC-8), volume 8. National Institute of
Standards and Technology Special
Publication 500-246, Gaithersburg, MD.
Braschler, Martin and Ba?rbel Ripplinger.
2003. Stemming and decompounding for
German text retrieval. In Fabrizio
Sebastiani, editor, Advances in Information
Retrieval: 25th European Conference on IR
Research (ECIR 2003), Pisa, Italy, April 2003,
Proceedings. Lecture Notes in Computer
Science 2633. Springer, Berlin.
Broglio, John, James P. Callan, W. Bruce
Croft, and Daniel W. Nachbar. 1995.
Document retrieval and routing using the
INQUERY system. In Donna K. Harman,
editor, The Third Text Retrieval Conference,
volume 4. National Institute of Standards
and Technology Special Publication
500-236, Gaithersburg, MD, pages 29?38.
Brown, Peter F., John Cocke, Stephen
A. Della Pietra, Vincent J. Della Pietra,
Fredrick Jelinek, John D. Lafferty,
Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine
translation. Computational Linguistics,
16(2):79?85.
Chen, Jiang and Jian-Yun Nie. 2000. Web
parallel text mining for Chinese-English
cross-language information retrieval. In
Proceedings of NAACL-ANLP, Seattle.
Clark, James. 2001. SP?An SGML System
Conforming to International Standard ISO
8879?Standard Generalized Markup
Language. Available at
?http://www.jclark.com/sp/?.
Conover, William Jay. 1980. Practical
Nonparametric Statistics. Wiley, London.
Croft, W. Bruce, Alistair Moffat,
C. J. ?Keith? van Rijsbergen, Ross
Wilkinson, and Justin Zobel, editors. 1998.
Proceedings of the 21st Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR
?98). ACM Press.
Dumais, Susan T., Todd A. Letsche,
Michael L. Littman, and Thomas K.
Landauer. 1997. Automatic cross-language
retrieval using latent semantic indexing.
In AAAI Spring Symposium on
416
Computational Linguistics Volume 29, Number 3
Cross-Language Text and Speech Retrieval,
Palo Alto, CA.
Federico, Marcello and Nicola Bertoldi.
2002. Statistical cross-language
information retrieval using N-best query
translations. In Micheline Beaulieu,
Ricardo Baeza-Yates, Sung Hyon Myaeng,
and Kalervo Ja?rvelin, editors, Proceedings
of the 25th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval (SIGIR 2002). ACM
Press, New York.
Foster, George F. 1991. Statistical lexical
disambiguation. Master?s thesis, McGill
University, School of Computer Science.
Foster, George. 2000. A maximum
entropy/minimum divergence translation
model. In Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics (ACL), Hong Kong.
Franz, Martin, J. Scott McCarley, and
R. Todd Ward. 2000. Ad hoc,
cross-language and spoken document
retrieval at IBM. In Ellen M. Voorhees and
Donna K. Harman, editors, The Eighth Text
Retrieval Conference (TREC-8), volume 8.
National Institute of Standards and
Technology Special Publication 500-246,
Gaithersburg, MD.
Franz, Martin, J. Scott McCarley, Todd Ward,
and Wei-Jing Zhu. 2001. Quantifying the
utility of parallel corpora. In W. Bruce
Croft, David J. Harper, Donald H. Kraft,
and Justin Zobel, editors, Proceedings of the
24th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval (SIGIR 2001). ACM
Press, New York.
Gao, Jianfeng, Jian-Yun Nie, Endong Xun,
Jian Zhang, Ming Zhou, and Changning
Huang. 2001. Improving query translation
for cross-language information retrieval
using statistical models. In W. Bruce
Croft, David J. Harper, Donald H. Kraft,
and Justin Zobel, editors, Proceedings of the
24th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval (SIGIR 2001). ACM
Press, New York.
Gollins, Tim and Mark Sanderson. 2001.
Improving cross language retrieval with
triangulated translation. In W. Bruce
Croft, David J. Harper, Donald H. Kraft,
and Justin Zobel, editors, Proceedings of the
24th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval (SIGIR 2001). ACM
Press, New York.
Grefenstette, Gregory. 1998. The problem of
cross-language information retrieval. In
Gregory Grefenstette, editor,
Cross-Language Information Retrieval.
Kluwer Academic, Boston, pages 1?9.
Harman, Donna K., editor. 1995. The Third
Text Retrieval Conference (TREC-3),
volume 4. National Institute of Standards
and Technology Special Publication
500-236.
Hearst, Marti, Fred Gey, and Richard Tong,
editors. 1999. Proceedings of the 22nd
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval (SIGIR ?99). ACM Press.
Hiemstra, Djoerd. 1998. A linguistically
motivated probabilistic model of
information retrieval. In Christos
Nicolaou and Constantine Stephanides,
editors, Research and Advanced Technology
for Digital Libraries?Second European
Conference (ECDL?98), Proceedings. Lecture
Notes in Computer Science 1513. Springer
Verlag, Berlin.
Hiemstra, Djoerd. 2001. Using Language
Models for Information Retrieval. Ph.D.
thesis, University of Twente, Enschede,
the Netherlands.
Hiemstra, Djoerd and Franciska de Jong.
1999. Disambiguation strategies for
cross-language information retrieval. In
European Conference on Digital Libraries,
pages 274?293.
Hiemstra, Djoerd and Wessel Kraaij. 1999.
Twenty-one at TREC-7: Ad hoc and cross
language track. In Ellen M. Voorhees and
Donna K. Harman, editors, The Seventh
Text Retrieval Conference (TREC-7),
volume 7. National Institute of Standards
and Technology Special Publication
500-242, Gaithersburg, MD.
Hiemstra, Djoerd, Wessel Kraaij, Rene?e
Pohlmann, and Thijs Westerveld. 2001.
Translation resources, merging strategies
and relevance feedback. In Carol Peters,
editor, Cross-Language Information Retrieval
and Evaluation. Lecture Notes in Computer
Science 2069. Springer Verlag, Berlin.
Hull, David. 1993. Using statistical testing
in the evaluation of retrieval experiments.
In Robert Korfhage, Edie Rasmussen, and
Peter Willett, editors, Proceedings of the
16th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval (SIGIR ?93), pages
329?338. ACM Press, New York.
Hull, David. 1996. Stemming algorithms?a
case study for detailed evaluation. Journal
of the American Society for Information
Science, 47(1): 47?84.
Hull, David. 1997. Using structured queries
for disambiguation in cross-language
information retrieval. In David Hull and
Douglas Oard, editors, AAAI Symposium
417
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
on Cross-Language Text and Speech Retrieval.
American Association for Artificial
Intelligence. Available at
?http://www.aaai.org/Press/Reports/
Symposia/Spring/ss-97-05.html?.
Hull, David and Gregory Grefenstette. 1996.
Querying across languages: A
dictionary-based approach to multilingual
information retrieval. In Hans-Peter Frei,
Donna Harman, Peter Scha?uble, and Ross
Wilkinson, editors, Proceedings of the
19th Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval (SIGIR ?96).
ACM Press, New York, pages 49?57.
Hull, David, Paul B. Kantor, and
Kwong Bor Ng. 1999. Advanced
approaches to the statistical analysis of
TREC information retrieval experiments.
Unpublished report, Rutgers University,
New Brunswick, NJ.
Ide, Nancy, G. Priest-Dorman and Jean
Ve?ronis. 1995. Corpus encoding standard.
Available at ?http://www.cs.vassar.edu/
CES/?.
Isabelle, Pierre, Michel Simard, and Pierre
Plamondon. 1998. Demo. Available at
?http://www.rali.iro.umontreal.ca/
SILC/SILC.en.cgi?.
Jansen, Bernard J., Amanda Spink, Deitmar
Wolfram, and Tefko Saracevic. 2001.
Searching the Web: The public and their
queries. Journal of the American Society for
Information Science and Technology,
53(3):226?234.
Kraaij, Wessel. 2002. TNO at CLEF-2001:
Comparing translation resources. In Carol
Peters, Martin Braschler, Julio Gonzalo,
and Michael Kluck, editors, Evaluation of
Cross-Language Information Retrieval
Systems: Second Workshop of the
Cross-Language Evaluation Forum (CLEF
2001). Springer Verlag, Berlin.
Kraaij, Wessel and Rene?e Pohlmann. 1996.
Viewing stemming as recall enhancement.
In Hans-Peter Frei, Donna Harman, Peter
Scha?uble, and Ross Wilkinson, editors,
Proceedings of the 19th Annual
International ACM SIGIR Conference on
Research and Development in
Information Retrieval (SIGIR ?96). ACM
Press, New York, pages 40?48.
Kraaij, Wessel, Rene?e Pohlmann, and Djoerd
Hiemstra. 2000. Twenty-one at TREC-8:
Using language technology for
information retrieval. In Ellen M.
Voorhees and Donna K. Harman, editors,
The Eighth Text Retrieval Conference
(TREC-8), volume 8. National Institute of
Standards and Technology Special
Publication 500-246, Gaithersburg, MD.
Kraaij, Wessel and Martijn Spitters. 2003.
Language models for topic tracking. In
Bruce Croft and John Lafferty, editors,
Language Models for Information Retrieval.
Kluwer Academic, Boston.
Kraaij, Wessel, Thijs Westerveld, and Djoerd
Hiemstra. 2002. The importance of prior
probabilities for entry page search. In
Micheline Beaulieu, Ricardo Baeza-Yates,
Sung Hyon Myaeng, and Kalervo
Ja?rvelin, editors, Proceedings of the 25th
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval (SIGIR 2002). ACM Press, New
York.
Kwok, K. L. 1999. English-Chinese
cross-language retrieval based on a
translation package. In Workshop: Machine
Translation for Cross Language Information
Retrieval, Singapore, Machine Translation
Summit VII, pages 8?13.
Lafferty, John and Chengxiang Zhai. 2001a.
Document language models, query
models, and risk minimization for
information retrieval. In W. Bruce Croft,
David J. Harper, Donald H. Kraft, and
Justin Zobel, editors, Proceedings of the 24th
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval (SIGIR 2001). ACM Press, New
York.
Lafferty, John and Chengxiang Zhai. 2001b.
Probabilistic IR models based on
document and query generation. In Jamie
Callan, Bruce Croft, and John Lafferty,
editors, Proceedings of the Workshop on
Language Modeling and Information
Retrieval, Pittsburgh.
Laffling, John. 1992. On constructing a
transfer dictionary for man and machine.
Target, 4(1):17?31.
Lavrenko, Victor, Martin Choquette, and W.
Bruce Croft. 2002. Cross-lingual relevance
models. In Micheline Beaulieu, Ricardo
Baeza-Yates, Sung Hyon Myaeng, and
Kalervo Ja?rvelin, editors, Proceedings of the
25th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval (SIGIR 2002). ACM
Press, New York.
Lavrenko, Victor and W. Bruce Croft. 2001.
Relevance-based language models. In W.
Bruce Croft, David J. Harper, Donald H.
Kraft, and Justin Zobel, editors,
Proceedings of the 24th Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR
2001). ACM Press, New York.
Lehtokangas, Raija and Eija Airio. 2002.
Translation via a pivot language
challenges direct translation in CLIR. In
418
Computational Linguistics Volume 29, Number 3
Proceedings of the SIGIR 2002 Workshop:
Cross-Language Information Retrieval: A
Research Roadmap, Tampere, Finland.
Ma, Xiaoyi. 1999. Parallel text collections at
the Linguistic Data Consortium. In
Machine Translation Summit VII, Singapore.
McNamee, Paul and James Mayfield. 2001.
A language-independent approach to
European text retrieval. In Carol Peters,
editor, Cross-Language Information Retrieval
and Evaluation. Lecture Notes in Computer
Science 2069. Springer Verlag, Berlin.
McNamee, Paul and James Mayfield. 2002.
Comparing cross-language query
expansion techniques by degrading
translation reources. In Micheline
Beaulieu, Ricardo Baeza-Yates, Sung Hyon
Myaeng, and Kalervo Ja?rvelin, editors,
Proceedings of the 25th Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR
2002). ACM Press, New York.
Miller, David R. H., Tim Leek, and
Richard M. Schwartz. 1999. A hidden
Markov model information retrieval
system. In Marti Hearst, Fred Gey, and
Richard Tong, editors, Proceedings of the
22nd Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval (SIGIR ?99).
ACM Press, New York, pages 214?221.
Ng, Kenney. 2000. A maximum likelihood
ratio information retrieval model. In Ellen
M. Voorhees and Donna K. Harman,
editors, The Eighth Text Retrieval Conference
(TREC-8), volume 8. National Institute of
Standards and Technology Special
Publication 500-246, Gaithersburg, MD.
Nie, Jian-Yun. 2002. Query expansion and
query translation as logical inference.
Journal of the American Society for
Information Science and Technology, 54(4):
340?351.
Nie, Jian-Yun and Jian Cai. 2001. Filtering
noisy parallel corpora of Web pages. In
IEEE Symposium on NLP and Knowledge
Engineering, Tucson, AZ, pages 453?458.
Nie, Jian-Yun and Jean-Franc?ois Dufort.
2002. Combining words and compound
terms for monolingual and cross-language
information retrieval. In Proceedings of
Information 2002, Beijing, pages 453?458.
Nie, Jian-Yun, Michel Simard, Pierre
Isabelle, and Richard Durand. 1999.
Cross-language information retrieval
based on parallel texts and automatic
mining of parallel texts from the Web. In
Marti Hearst, Fred Gey, and Richard
Tong, editors, Proceedings of the 22nd
Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval (SIGIR ?99).
ACM Press, New York, pages 74?81.
Pirkola, Ari. 1998. The effects of query
structure and dictionary setups in
dictionary-based cross-language
information retrieval. In W. Bruce Croft,
Alistair Moffat, C. J. ?Keith? van
Rijsbergen, Ross Wilkinson, and Justin
Zobel, editors, Proceedings of the 21st
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval (SIGIR ?98). ACM Press, New
York, pages 55?63.
Ponte, Jay M. and W. Bruce Croft. 1998. A
language modeling approach to
information retrieval. In W. Bruce Croft,
Alistair Moffat, C. J. ?Keith? van
Rijsbergen, Ross Wilkinson, and Justin
Zobel, editors, Proceedings of the 21st
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval (SIGIR ?98). ACM Press, New
York, pages 275?281.
Porter, Martin F. 1980. An algorithm for
suffix stripping. Program, 14(3):130?137.
Ragget, Dave. 1998. Clean up your Web
pages with HTML TIDY. Available at
?http://www.w3.org/People/
Raggett/tidy/?.
Resnik, Philip. 1998. Parallel stands: A
preliminary investigation into mining the
Web for bilingual text. In Proceedings of
AMTA. Lecture Notes in Computer
Science 1529. Springer, Berlin.
Robertson, Stephen E. and Steve Walker.
1994. Some simple effective
approximations to the 2-Poisson model
for probabilistic weighted retrieval. In W.
Bruce Croft and C. J. ?Keith? van
Rijsbergen, editors, Proceedings of the 17th
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval (SIGIR ?94), pages 232?241. ACM
Press, New York.
Salton, G. and M. J. McGill. 1983.
Introduction to Modern Information Retrieval.
McGraw-Hill, New York.
Savoy, Jacques. 2002. Report on CLEF-2001
experiments. In Carol Peters, Martin
Braschler, Julio Gonzalo, and Michael
Kluck, editors, Evaluation of Cross-Language
Information Retrieval Systems: Second
Workshop of the Cross-Language Evaluation
Forum (CLEF 2001). Springer Verlag,
Berlin.
Sheridan, Paraic, Jean Paul Ballerini, and
Peter Scha?uble. 1998. Building a large
multilingual text collection from
comparable news documents. In Gregory
Grefenstette, editor, Cross-Language
Information Retrieval. Kluwer Academic,
419
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
pages 137?150.
Simard, Michel, George Foster, and Pierre
Isabelle. 1992. Using Cognates to Align
Sentences in Bilingual Corpora. In
Proceedings of the Fourth Conference on
Theoretical and Methodological Issues in
Machine Translation (TMI), pages 67?82,
Montre?al, Que?bec.
Spitters, Martijn and Wessel Kraaij. 2001.
Using language models for tracking
events of interest over time. In Proceedings
of the Workshop on Language Models for
Information Retrieval (LMIR2001),
Pittsburgh.
Tague-Sutcliffe, Jean and James Blustein.
1995. A statistical analysis of the TREC-3
data. In Donna K. Harman, editor, The
Third Text Retrieval Conference, volume 4.
National Institute of Standards and
Technology Special Publication 500-236,
Gaithersburg, MD, pages 385?398.
Ve?ronis, Jean, editor. 2000. Parallel Text
Processing. Kluwer Academic, Dordrecht,
the Netherlands.
Voorhees, Ellen M. 1998. Variations in
relevance judgements and the
measurement of retrieval effectiveness. In
W. Bruce Croft, Alistair Moffat, C. J.
?Keith? van Rijsbergen, Ross Wilkinson,
and Justin Zobel, editors, Proceedings of the
21st Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval (SIGIR ?98). ACM
Press, New York. pages 315?323.
Xu, Jinxi, Ralph Weischedel, and Chanh
Nguyen. 2001. Evaluating a probabilistic
model for cross-lingual information
retrieval. In W. Bruce Croft, David J.
Harper, Donald H. Kraft, and Justin
Zobel, editors, Proceedings of the 24th
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval (SIGIR 2001). ACM Press, New
York.
Yang, Yiming, Jaime G. Carbonell, Ralph
Brown, and Robert E. Frederking. 1998.
Translingual information retrieval:
Learning from bilingual corpora. Artificial
Intelligence Journal, 103(1?2):323?345.
Zhai, ChengXiang and John Lafferty. 2002.
Two-stage language models for
information retrieval. In Micheline
Beaulieu, Ricardo Baeza-Yates, Sung Hyon
Myaeng, and Kalervo Ja?rvelin, editors,
Proceedings of the 25th Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR
2002). ACM Press, New York.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 463?470,
New York, June 2006. c?2006 Association for Computational Linguistics
	

	


	
		
	

Proceedings of NAACL HLT 2009: Short Papers, pages 165?168,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Search Result Re-ranking by Feedback Control Adjustment for
Time-sensitive Query
Ruiqiang Zhang? and Yi Chang? and Zhaohui Zheng?
Donald Metzler? and Jian-yun Nie?
?Yahoo! Labs, 701 First Avenue, Sunnyvale, CA94089
?University of Montreal, Montreal, Quebec,H3C 3J7, Canada
?{ruiqiang,yichang,zhaohui,metzler}@yahoo-inc.com
?nie@iro.umontreal.ca
Abstract
We propose a new method to rank a special
category of time-sensitive queries that are year
qualified. The method adjusts the retrieval
scores of a base ranking function according
to time-stamps of web documents so that the
freshest documents are ranked higher. Our
method, which is based on feedback control
theory, uses ranking errors to adjust the search
engine behavior. For this purpose, we use
a simple but effective method to extract year
qualified queries by mining query logs and a
time-stamp recognition method that considers
titles and urls of web documents. Our method
was tested on a commercial search engine. The
experiments show that our approach can sig-
nificantly improve relevance ranking for year
qualified queries even if all the existing meth-
ods for comparison failed.
1 Introduction
Relevance ranking plays a crucial role in search
engines. There are many proposed machine learn-
ing based ranking algorithms such as language
modeling-based methods (Zhai and Lafferty, 2004),
RankSVM (Joachims, 2002), RankBoost (Freund et al,
1998) and GBrank (Zheng et al, 2007). The input to
these algorithms is a set of feature vectors extracted from
queries and documents. The goal is to find the parameter
setting that optimizes some relevance metric given
training data. While these machine learning algorithms
can improve average relevance, they may be ineffctive
for certain special cases. Time-sensitive queries are one
such special case that machine-learned ranking functions
may have a hard time learning, due to the small number
of such queries.
Consider the query ?sigir? (the name of a conference),
which is time sensitive. Table 1 shows two example
search result pages for the query, SERP1 and SERP2. The
query: sigir
SERP1 url1: http://www.sigir.org
url2: http://www.sigir2008.org
url3: http://www.sigir2004.org
url4: http://www.sigir2009.org
url5: http://www.sigir2009.org/schedule
SERP2 url1: http://www.sigir.org
url2: http://www.sigir2009.org
url3: http://www.sigir2009.org/schedule
url4: http://www.sigir2008.org
url5: http://www.sigir2004.org
Table 1: Two contrived search engine result pages
ranking of SERP2 is clearly better than that of SERP1 be-
cause the most recent event, ?sigir2009?, is ranked higher
than other years.
Time is an important dimension of relevance in web
search, since users tend to prefer recent documents to old
documents. At the time of this writing (February 2009),
none of the major commercial search engines ranked the
homepage for SIGIR 2009 higher than previous SIGIR
homepages for the query ?sigir?. One possible reason for
this is that ranking algorithms are typically based on an-
chor text features, hyperlink induced features, and click-
through rate features. However, these features tend to fa-
vor old pages more than recent ones. For example, ?si-
gir2008? has more links and clicks than ?sigir2009? be-
cause ?sigir2008? has existed longer time and therefore
has been visited more. It is less likely that newer web
pages from ?sigir2009? can be ranked higher using fea-
tures that implicitly favor old pages.
However, the fundamental problem is that current ap-
proaches have focused on improving general ranking al-
gorithms. Methods for improving ranking of specific
types of query like temporal queries are often overlooked.
Aiming to improve ranking results, some methods of
re-ranking search results are proposed, such as the work
by (Agichtein et al, 2006) and (Teevan et al, 2005).
165
Search Engine
Detector 
Controller
error R(q, yn)R(q, yo)
_
+
Figure 1: Feedback control for search engine
These work uses user search behavior information or per-
sonalization information as features that are integrated
into an enhanced ranking model. We propose a novel
method of re-ranking search results. This new method
is based on feedback control theory, as illustrated in 1.
We make a Detector to monitor search engine (SE) out-
put and compare it with the input, which is the desired
search engine ranking. If an error is found, we design
the controller that uses the error to adjust the search en-
gine output, such that the search engine output tracks the
input. We will detail the algorithm in Section 4.1.
Our method was applied to a special class of time-
sensitive query, year qualified queries (YQQs). For this
category, we found users either attached a year with the
query explicitly, like ?sigir 2009?, or used the query only
without a year attached,like ?sigir?. We call the former
explicit YQQs, and the latter implicit YQQs. Using query
log analysis, we found these types of queries made up
about 10% of the total query volume. We focus exclu-
sively on implicit YQQs by translating the user?s im-
plicit intention as the most recent year. Explicit YQQs
are less interesting, because the user?s temporal inten-
tion is clearly specified in the query. Therefore, rank-
ing for these types of queries is relatively straightfor-
ward. Throughout the remainder of this paper, we use
the ?YQQ? to refer to implicit YQQs, unless otherwise
stated.
2 Adaptive score adjustment
Our proposed re-ranking model is shown in Eq. 1, as be-
low.
F(q, d) =
{
R(q, d) if q < YQQ
R(q, d) + Q(q, d) otherwise
Q(q, d) =
{ (e(do, dn) + k)e??(q) if y(d) = yn
0 otherwise
e(do, dn) = R(q, do) ? R(q, dn)
(1)
This work assumes that a base ranking function is used
to rank documents with respect to an incoming query. We
denote this base ranking function as R(q, d). This ranking
function is conditioned on a query q and a document d. It
is assumed to model the relevance between q and d.
Our proposed method is flexible for all YQQ queries.
Suppose the current base ranking function gives the re-
sults as SERP1 of Table 1. To correct the ranking, we
propose making an adjustment to R(q, d).
In Eq. 1, F(q, d) is the final ranking function. If the
query is not an YQQ, the base ranking function is used.
Otherwise, we propose an adjustment function, Q(q, d) ,
to adjust the base ranking function. Q(q, d) is controlled
by the ranking error, e(do, dn), signifying the base func-
tion ranking error if the newest web page dn is ranked
lower than the oldest web page do. y(d) is the year that
the event described by d has occurred or will occur. If
yo and yn indicate the oldest year and the newest year,
then y(do) = yo, y(dn) = yn. R(q, do) and R(q, dn) are the
base ranking function scores for the oldest and the newest
documents.
k is a small shift value for direction control. When
k < 0, the newest document is adjusted slightly under the
old one. Otherwise, it is adjusted slightly over the old
one. Experiments show k > 0 gave better results. The
value of k is determined in training.
?(q) is the confidence score of a YQQ query, mean-
ing the likelihood of a query to be YQQ. The confidence
score is bigger if a query is more likely to be YQQ. More
details are given in next section. ? is a weighting param-
eter for adjusting ?(q).
The exp function e??(q) is a weighting to control boost-
ing value. A higher value, confidence ?, a larger boosting
value, Q(q, d).
Our method can be understood by feedback control
theory, as illustrated in Fig. 1. The ideal input is R(q, yo)
representing the desired ranking score for the newest
Web page, R(q, yn). But the search engine real output
is R(q, yn). Because search engine is a dynamic system,
its ranking is changing over time. This results in ranking
errors, e(do, dn) = R(q, do) ? R(q, dn). The function of
?Controller? is to design a function to adjust the search
engine ranking so that the error approximates to zero,
e(do, dn) = 0. For this work, ?Controller? is Q(q, d).
?Detector? is a document year-stamp recognizer, which
will be described more in the next section. ?Detector?
is used to detect the newest Web pages and their ranking
scores. Fig. 1 is an ideal implementation of our methods.
We cannot carry out real-time experiments in this work.
Therefore, the calculation of ranking errors was made in
offline training.
3 YQQ detection and year-stamp
recognition
To implement Eq. 1, we need to find YQQ queries and to
identify the year-stamp of web documents.
Our YQQ detection method is simple, efficient, and
relies only on having access to a query log with frequency
information. First, we extracted all explicit YQQs from
166
query log. Then, we removed all the years from explicit
YQQs. Thus, implicit YQQs are obtained from explicit
YQQs. The implicit YQQs are saved in a dictionary. In
online test, we match input queries with each of implicit
YQQs in the dictionary. If an exact match is found, we
regard the input query as YQQ, and apply Eq. 1 to re-rank
search results.
After analyzing samples of the extracted YQQs, we
group them into three classes. One is recurring-event
query, like ?sigir?, ?us open tennis?; the second is news-
worthy query, like ?steve ballmer?, ?china foreign re-
serves?; And the class not belong to any of the above
two, like ?christmas?, ?youtube?. We found our proposed
methods were the most effective for the first category. In
Eq. 1, we can use confidence ?(q) to distinguish the three
categories and their change of ranking as shown in Eq.1,
that is defined as below.
?(q) =
?
y w(q, y)
#(q) +?y w(q, y)
(2)
where w(q, y) = #(q.y)+#(y.q). #(q.y) denotes the num-
ber of times that the base query q is post-qualified with
the year y in the query log. Similarly, #(y.q) is the num-
ber of times that q is pre-qualified with the year y. This
weight measures how likely q is to be qualified with y,
which forms the basis of our mining and analysis. #(q) is
the counts of independent query, without associating with
any other terms.
We also need to know the year-stamp y(d) for each
web document so that the ranking score of a document
is updated if y(d) = yn is satisfied. We can do this
from a few sources such as title, url, anchar text, and
extract date from documents that is possible for many
news pages. For example, from url of the web page,
?www.sigir2009.org?, we detect its year-stamp is 2009.
We have also tried to use some machine generated
dates. However, in the end we found such dates are in-
accurate and cannot be trusted. For example, discovery
time is the time when the document was found by the
crawler. But a web document may exist several years be-
fore a crawler found it. We show the worse effect of using
discovery time in the experiments.
4 Experiments
We will describe the implementation methods and experi-
mental results in this section. Our methods include offline
dictionary building and online test. In offline training, our
first step is to mine YQQs. A commercial search engine
company provided us with six months of query logs. We
extracted a list of YQQs using Section 3?s method. For
each of the YQQs, we run the search engine and output
the top N results. For each document, we used the method
described in Section 3 to recognize the year-stamp and
find the oldest and the newest page. If there are multiple
urls with the same yearstamp, we choose the first oldest
and the first most recent. Next,we calculated the boost-
ing value according to Eq. 1. Each query has a boosting
value. For online test, a user?s query is matched with each
of the YQQs in the dictionary. If an exact match is found,
the boosting value will be added to the base ranking score
iff the document has the newest yearstamp.
For evaluating our methods, we randomly extracted
600 YQQs from the dictionary. We extracted the top-5
search results for each of queries using the base ranking
function and the proposed ranking function. We asked
human editors to judge all the scraped results. We used
five judgment grades: Perfect, Excellent, Good, Fair,
and Bad. Editors were instructed to consider temporal
issues when judging. For example, sigir2004 is given
a worse grade than sigir2009. To avoid bias, we ad-
vised editors to retain relevance as their primary judg-
ment criteria. Our evaluation metric is relative change
in DCG, %?dcg = DCGproposed?DCGbaselineDCGbaseline , where DCG is
the traditional Discounted Cumulative Gain (Jarvelin and
Kekalainen, 2002).
4.1 Effect of the proposed boosting method
Our experimental results are shown in Table 2, where we
compared our work with the existing methods. While we
cannot apply (Li and Croft, 2003)?s approach directly be-
cause first, our search engine is not based on language
modeling; second, it is impossible to obtain exact times-
tamp for web pages as (Li and Croft, 2003) did in the
track evaluation. However, we tried to simulate (Li and
Croft, 2003)?s approach in web search by using the linear
integration method exactly as the same as(Li and Croft,
2003) by adding a time-based function with our base
ranking function. For the timestamp, we used discovery
time in the time-based function. The parameters (?, ?)
have the exact same meaning as in (Li and Croft, 2003)
but were tuned according to our base ranking function.
With regards to the approach by (Diaz and Jones, 2004),
we ranked the web pages in decreasing order of discov-
ery time. Our own approaches were tested under options
with and without using adaptation. For no adaption, we
let the e of Eq.1 equal to 0, meaning no score difference
between the oldest document and the newest document
was captured, but a constant value was used. It is equiv-
alent to an open loop in Fig.1. For adaption, we used the
ranking errors to adjust the base ranking. In the Table we
used multiple ks to show the effect of changing k. Using
different k can have a big impact on the performance. The
best value we found was k = 0.3. In this experiment, we
let ?(q) = 0 so that the result responds to k only.
Our approach is significantly better than the existing
methods. Both of the two existing methods produced
worse results than the baseline, which shows the ap-
167
Li & Croft (?, ?)=(0.2,2.0) -0.5
(?, ?)=(0.2,4.0) -1.2
Diaz & Jones -4.5?
No adaptation (e = 0, k=0.3 1.2
open loop) k=0.4 0.8
Adaptation (closed loop) k=0.3 6.6?
k=0.4 6.2?
Table 2: %?dcg of proposed method comparing with
existing methods.A sign ??? indicates statistical signifi-
cance (p-value<0.05)
? 0 0.2 0.4 0.6 0.8 1.0
%?dcg 6.6? 7.8? 8.4? 4.5 2.1 -0.2?
Table 3: Effect of confidence as changing ?.
proaches may be inappropriate for Web search. Not sur-
prisingly, using adaption achieved much better results
than without using adaption. Thus, these experiments
prove the effectiveness of our proposed methods.
Another important parameter in the Eq.1 is the confi-
dence score ?(q), which indicates the confidence of query
to be YQQ. In Eq. 1, ? is used to adjusting ?(q). We
observed dcg gain for each different ?. The results are
shown in Table 3. The value of ? needs to be tuned for
different base ranking functions. A higher ? can hurt per-
formance. In our experiments, the best value of 0.4 gave a
8.4% statistically significant gain in DCG. The ? = 0 set-
ting means we turn off confidence, which results in lower
performance. Thus, using YQQ confidence is effective.
5 Discussions and conclusions
In this paper, we proposed a novel approach to solve
YQQ ranking problem, which is a problem that seems
to plague most major commercial search engines. Our
approach for handling YQQs does not involve any query
expansion that adds a year to the query. Instead, keeping
the user?s query intact, we re-rank search results by ad-
justing the base ranking function. Our work assumes the
intent of YQQs is to find documents about the most recent
year. For this reason, we use YQQ confidence to measure
the probability of this intent. As our results showed, our
proposed method is highly effective. A real example is
given in Fig. 2 to show the significant improvement by
our method.
Our adaptive methods are not limited to YQQs only.
We believe this framework can be applied to any category
of queries once a query classification and a score detector
have been implemented.
Figure 2: Ranking improvement for query ICML by our
method: before re-rank(left) and after(right)
References
Eugene Agichtein, Eric Brill, and Susan Dumais. 2006.
Improving web search ranking by incorporating user
behavior information. In SIGIR ?06, pages 19?26.
Fernando Diaz and Rosie Jones. 2004. Using temporal
profiles of queries for precision prediction. In Proc.
27th Ann. Intl. ACM SIGIR Conf. on Research and De-
velopment in Information Retrieval, pages 18?24, New
York, NY, USA. ACM.
Yoav Freund, Raj D. Iyer, Robert E. Schapire, and Yoram
Singer. 1998. An efficient boosting algorithm for
combining preferences. In ICML ?98: Proceedings
of the Fifteenth International Conference on Machine
Learning, pages 170?178.
Kalervo Jarvelin and Jaana Kekalainen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Transactions on Information Systems, 20:2002.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In KDD ?02: Proceedings
of the eighth ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 133?
142.
Xiaoyan Li and W. Bruce Croft. 2003. Time-based
language models. In Proc. 12th Intl. Conf. on Infor-
mation and Knowledge Management, pages 469?475,
New York, NY, USA. ACM.
Jaime Teevan, Susan T. Dumais, and Eric Horvitz. 2005.
Personalizing search via automated analysis of inter-
ests and activities. In SIGIR ?05, pages 449?456.
Chengxiang Zhai and John Lafferty. 2004. A study of
smoothing methods for language models applied to in-
formation retrieval. ACM Trans. Inf. Syst., 22(2):179?
214.
Zhaohui Zheng, Keke Chen, Gordon Sun, and Hongyuan
Zha. 2007. A regression framework for learning rank-
ing functions using relative relevance judgments. In
SIGIR ?07, pages 287?294.
168
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 585?592,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An Iterative Implicit Feedback Approach to Personalized Search 
 
Yuanhua Lv 1, Le Sun 2, Junlin Zhang 2, Jian-Yun Nie 3, Wan Chen 4, and Wei Zhang 2 
1, 2 Institute of Software, Chinese Academy of Sciences, Beijing, 100080, China 
3 University of Montreal, Canada 
1 lvyuanhua@gmail.com 
2 {sunle, junlin01, zhangwei04}@iscas.cn 
3 nie@iro.umontreal.ca   4 chenwan@nus.edu.sg 
 
Abstract 
General information retrieval systems are 
designed to serve all users without con-
sidering individual needs. In this paper, 
we propose a novel approach to person-
alized search. It can, in a unified way, 
exploit and utilize implicit feedback in-
formation, such as query logs and imme-
diately viewed documents. Moreover, our 
approach can implement result re-ranking 
and query expansion simultaneously and 
collaboratively. Based on this approach, 
we develop a client-side personalized web 
search agent PAIR (Personalized Assis-
tant for Information Retrieval), which 
supports both English and Chinese. Our 
experiments on TREC and HTRDP col-
lections clearly show that the new ap-
proach is both effective and efficient. 
1 Introduction 
Analysis suggests that, while current information 
retrieval systems, e.g., web search engines, do a 
good job of retrieving results to satisfy the range 
of intents people have, they are not so well in 
discerning individuals? search goals (J. Teevan et 
al., 2005). Search engines encounter problems 
such as query ambiguity and results ordered by 
popularity rather than relevance to the user?s in-
dividual needs. 
To overcome the above problems, there have 
been many attempts to improve retrieval accuracy 
based on personalized information. Relevance 
Feedback (G. Salton and C. Buckley, 1990) is the 
main post-query method for automatically im-
proving a system?s accuracy of a user?s individual 
need. The technique relies on explicit relevance 
assessments (i.e. indications of which documents 
contain relevant information). Relevance feed-
back has been proved to be quite effective for 
improving retrieval accuracy (G. Salton and C. 
Buckley, 1990; J. J. Rocchio, 1971). However, 
searchers may be unwilling to provide relevance 
information through explicitly marking relevant 
documents (M. Beaulieu and S. Jones, 1998). 
Implicit Feedback, in which an IR system un-
obtrusively monitors search behavior, removes 
the need for the searcher to explicitly indicate 
which documents are relevant (M. Morita and Y. 
Shinoda, 1994). The technique uses implicit 
relevance indications, although not being as ac-
curate as explicit feedback, is proved can be an 
effective substitute for explicit feedback in in-
teractive information seeking environments (R. 
White et al, 2002). In this paper, we utilize the 
immediately viewed documents, which are the 
clicked results in the same query, as one type of 
implicit feedback information. Research shows 
that relative preferences derived from immedi-
ately viewed documents are reasonably accurate 
on average (T. Joachims et al, 2005). 
Another type of implicit feedback information 
that we exploit is users? query logs. Anyone who 
uses search engines has accumulated lots of click 
through data, from which we can know what 
queries were, when queries occurred, and which 
search results were selected to view. These query 
logs provide valuable information to capture us-
ers? interests and preferences. 
Both types of implicit feedback information 
above can be utilized to do result re-ranking and 
query expansion, (J. Teevan et al, 2005; Xuehua 
Shen. et al, 2005) which are the two general ap-
proaches to personalized search. (J. Pitkow et al, 
2002) However, to the best of our knowledge, 
how to exploit these two types of implicit feed-
back in a unified way, which not only brings col-
laboration between query expansion and result 
re-ranking but also makes the whole system more 
concise, has so far not been well studied in the 
previous work. In this paper, we adopt HITS al-
gorithm (J. Kleinberg, 1998), and propose a 
585
HITS-like iterative approach addressing such a 
problem. 
Our work differs from existing work in several 
aspects: (1) We propose a HITS-like iterative 
approach to personalized search, based on which, 
implicit feedback information, including imme-
diately viewed documents and query logs, can be 
utilized in a unified way. (2) We implement re-
sult re-ranking and query expansion simultane-
ously and collaboratively triggered by every 
click. (3) We develop and evaluate a client-side 
personalized web search agent PAIR, which 
supports both English and Chinese. 
The remaining of this paper is organized as 
follows. Section 2 describes our novel approach 
for personalized search. Section 3 provides the 
architecture of PAIR system and some specific 
techniques. Section 4 presents the details of the 
experiment. Section 5 discusses the previous 
work related to our approach. Section 6 draws 
some conclusions of our work. 
2 Iterative Implicit Feedback Approach 
We propose a HITS-like iterative approach for 
personalized search. HITS (Hyperlink-Induced 
Topic Search) algorithm, first described by (J. 
Kleinberg, 1998), was originally used for the 
detection of high-score hub and authority web 
pages. The Authority pages are the central web 
pages in the context of particular query topics. 
The strongest authority pages consciously do not 
link one another1 ? they can only be linked by 
some relatively anonymous hub pages. The mu-
tual reinforcement principle of HITS states that a 
web page is a good authority page if it is linked by 
many good hub pages, and that a web page is a 
good hub page if it links many good authority 
pages. A directed graph is constructed, of which 
the nodes represent web pages and the directed 
edges represent hyperlinks. After iteratively 
computing based on the reinforcement principle, 
each node gets an authority score and a hub score. 
In our approach, we exploit the relationships 
between documents and terms in a similar way to 
HITS. Unseen search results, those results which 
are retrieved from search engine yet not been 
presented to the user, are considered as ?authority 
pages?. Representative terms are considered as 
?hub pages?. Here the representative terms are the 
terms extracted from and best representing the 
implicit feedback information. Representative 
terms confer a relevance score to the unseen 
                                                          
1 For instance, There is hardly any other company?s Web 
page linked from ?http://www.microsoft.com/? 
search results ? specifically, the unseen search 
results, which contain more good representative 
terms, have a higher possibility of being relevant; 
the representative terms should be more repre-
sentative, if they occur in the unseen search re-
sults that are more likely to be relevant. Thus, 
also there is mutual reinforcement principle ex-
isting between representative terms and unseen 
search results. By the same token, we constructed 
a directed graph, of which the nodes indicate un-
seen search results and representative terms, and 
the directed edges represent the occurrence of the 
representative terms in the unseen search results. 
The following Table 1 shows how our approach 
corresponds to HITS algorithm. 
 
The Directed Graph 
Approaches
Nodes Edges 
HITS Authority Pages Hub Pages Hyperlinks
Our  
Approach
Unseen Search 
Results 
Representative 
Terms Occurrence
2
Table 1. Our approach versus HITS. 
 
Because we have already known that the rep-
resentative terms are ?hub pages?, and that the 
unseen search results are ?authority pages?, with 
respect to the former, only hub scores need to be 
computed; with respect to the latter, only author-
ity scores need to be computed. 
Finally, after iteratively computing based on 
the mutual reinforcement principle we can 
re-rank the unseen search results according to 
their authority scores, as well as select the repre-
sentative terms with highest hub scores to ex-
pand the query. Below we present how to con-
struct a directed graph to begin with. 
2.1 Constructing a Directed Graph 
We can view the unseen search results and the 
representative terms as a directed graph G = (V, E). 
A sample directed graph is shown in Figure 1: 
 
 
Figure 1. A sample directed graph. 
 
The nodes V correspond to the unseen search 
results (the rectangles in Figure 1) and the repre-
                                                          
2 The occurrence of the representative terms in the unseen 
search results. 
586
sentative terms (the circles in Figure 1); a di-
rected edge ?p?q?E? is weighed by the fre-
quency of the occurrence of a representative term 
p in an unseen search result q (e.g., the number 
put on the edge ?t1?r2? indicates that t1 occurs 
twice in r2). We say that each representative term 
only has an out-degree which is the number of the 
unseen search results it occurs in, as well as that 
each unseen search result only has an in-degree 
which is the count of the representative terms it 
contains. Based on this, we assume that the un-
seen search results and the representative terms 
respectively correspond to the authority pages 
and the hub pages ? this assumption is used 
throughout the proposed algorithm. 
2.2 A HITS-like Iterative Algorithm 
In this section, we present how to initialize the 
directed graph and how to iteratively compute the 
authority scores and the hub scores. And then 
according to these scores, we show how to re-rank 
the unseen search results and expand the initial 
query. 
Initially, each unseen search result of the query 
are considered equally authoritative, that is, 
0 0 0
1 2 | |
1 | |
Y
Yy y y= ?= =                  (1) 
Where vector Y indicates authority scores of the 
overall unseen search results, and |Y| is the size of 
such a vector. Meanwhile, each representative 
term, with the term frequency tfj in the history 
query logs that have been judged related to the 
current query, obtains its hub score according to 
the follow formulation: 
0
| |
1
X
j i
j itf tfx == ?                       (2) 
Where vector X indicates hub scores of the overall 
representative terms, and |X| is the size of the 
vector X. The nodes of the directed graph are 
initialized in this way. Next, we associate each 
edge with a weight: 
,( )ji i jw tft r? =                     (3) 
Where tfi,j indicates the term frequency of the 
representative term ti occurring in the unseen 
search result rj; ?w(ti? rj)? is the weight of edge 
that link from ti to rj. For instance, in Figure 1, 
w(t1? r2) = 2. 
After initialization, the iteratively computing of 
hub scores and authority scores starts. 
The hub score of each representative term is 
re-computed based on three factors: the authority 
scores of each unseen search result where this 
term occurs; the occurring frequency of this term 
in each unseen search result; the total occurrence 
of every representative term in each unseen search 
result. The formulation for re-computing hub 
scores is as follows: 
( 1)
:
:
( )
( )'
k ji
i
jnji
jn
k
jj
n
w
wt r
t r
t ryx t r
+
?  ?
?  ?
?= ?? ?
    (4) 
Where x`i(k+1) is the hub score of a representative 
term ti after (k+1)th iteration; yjk is the authority 
score of an unseen search result rj after kth itera-
tion; ??j: ti?rj? indicates the set of all unseen 
search results those ti occurs in; ??n: tn?rj? in-
dicates the set of all representative terms those rj 
contains. 
The authority score of each unseen search re-
sult is also re-computed relying on three factors: 
the hub scores of each representative term that 
this search result contains; the occurring fre-
quency of each representative term in this search 
result; the total occurrence of each representative 
term in every unseen search results. The formu-
lation for re-computing authority scores is as 
follows: 
( 1)
:
:
( )
( )'
k k
ij
miji
mi
ji
i
m
w
wt r
t r
t ry x t r
+
?  ?
?  ?
?= ?? ?
    (5) 
Where y`j(k+1) is the authority score of an unseen 
search result rj after (k+1)th iteration; xik  is the 
hub score of a representative term ti after kth it-
eration; ??i: ti?rj? indicates the set of all repre-
sentative terms those rj contains; ??m: ti?rm? 
indicates the set of all unseen search results those 
ti occurs in. 
After re-computation, the hub scores and the 
authority scores are normalized to 1. The formu-
lation for normalization is as follows: 
| | | |
1 1
and
' '
' '
j i
iY Xj
kkk k
y xy x
y x= =
=   =
? ?
              (6) 
The iteration, including re-computation and 
normalization, is repeated until the changes of the 
hub scores and the authority scores are smaller 
than some predefined threshold ? (e.g. 10-6). 
Specifically, after each repetition, the changes in 
authority scores and hub scores are computed 
using the following formulation: 
2 2( 1) ( 1)
| | | |
1 1
( ) ( )
k k k k
i ij j
Y x
j i
c y y x x
+ +
= =
= ? + ?? ?        (7) 
The iteration stops if c<?. Moreover, the itera-
tion will also stop if repetition has reached a 
587
predefined times k (e.g. 30). The procedure of the 
iteration is shown in Figure 2. 
As soon as the iteration stops, the top n unseen 
search results with highest authority scores are 
selected and recommended to the user; the top m 
representative terms with highest hub scores are 
selected to expand the original query. Here n is a 
predefined number (in PAIR system we set n=3, 
n is given a small number because using implicit 
feedback information is sometimes risky.) m is 
determined according to the position of the big-
gest gap, that is, if ti ? ti+1 is bigger than the gap 
of any other two neighboring ones of the top half 
representative terms, then m is given a value i. 
Furthermore, some of these representative terms 
(e.g. top 50% high score terms) will be again used 
in the next time of implementing the iterative 
algorithm together with some newly incoming 
terms extracted from the just now click. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. The HITS-like iterative algorithm. 
 
3 Implementation 
3.1 System Design 
In this section, we present our experimental sys-
tem PAIR, which is an IE Browser Helper Object 
(BHO) based on the popular Web search engine 
Google. PAIR has three main modules: Result 
Retrieval module, User Interactions module, and 
Iterative Algorithm module. The architecture is 
shown in Figure 3. 
The Result Retrieval module runs in back-
grounds and retrieves results from search engine. 
When the query has been expanded, this module 
will use the new keywords to continue retrieving. 
The User Interactions module can handle three 
types of basic user actions: (1) submitting a query; 
(2) clicking to view a search result; (3) clicking 
the ?Next Page? link. For each of these actions, 
the system responds with: (a) exploiting and ex-
tracting representative terms from implicit feed-
back information; (b) fetching the unseen search 
results via Results Retrieval module; (c) sending 
the representative terms and the unseen search 
results to Iterative Algorithm module. 
 
 
Figure 3. The architecture of PAIR. 
 
The Iterative Algorithm module implements 
the HITS-like algorithm described in section 2. 
When this module receives data from User In-
teractions module, it responds with: (a) iteratively 
computing the hub scores and authority scores; (b) 
re-ranking the unseen search results and expand-
ing the original query. 
Some specific techniques for capturing and 
exploiting implicit feedback information are de-
scribed in the following sections. 
3.2 Extract Representative Terms from 
Query Logs 
We judge whether a query log is related to the 
current query based on the similarity between the 
query log and the current query text. Here the 
query log is associated with all documents that 
the user has selected to view. The form of each 
query log is as follows 
<query text><query time> [clicked documents]* 
The ?clicked documents? consist of URL, title 
and snippet of every clicked document. The rea-
son why we utilize the query text of the current 
query but not the search results (including title, 
snippet, etc.) to compute the similarity, is out of 
consideration for efficiency. If we had used the 
search results to determine the similarity, the 
computation could only start once the search en-
gine has returned the search results. In our method, 
instead, we can exploit query logs while search 
engine is doing retrieving. Notice that although 
our system only utilizes the query logs in the last 
24 hours; in practice, we can exploit much more 
because of its low computation cost with respect 
to the retrieval process performed in parallel.
Iterate (T, R, k, ?) 
T: a collection of m terms 
R: a collection of n search results 
k: a natural number 
?: a predefined threshold 
Apply (1) to initialize Y. 
Apply (2) to initialize X. 
Apply (3) to initialize W. 
For i = 1, 2?, k 
Apply (4) to (Xi-1, Yi-1) and obtain X`i. 
Apply (5) to (Xi-1, Yi-1) and obtain Y`i. 
Apply (6) to Normalize X`i and Y`i, and respectively 
obtain Xi and Yi. 
Apply (7) and obtain c. 
If c<?, then break. 
End 
Return (X, Y). 
588
Table 2. Sample results of re-ranking. The search results in boldface are the ones that our system rec-
ommends to the user. ?-3? and ?-2? in the right side of some results indicate the how their ranks descend. 
 
We use the standard vector space retrieval 
model (G. Salton and M. J. McGill, 1983) to 
compute the similarity. If the similarity between 
any query log and the current query exceeds a 
predefined threshold, the query log will be con-
sidered to be related to current query. Our system 
will attempt to extract some (e.g. 30%) represen-
tative terms from such related query logs ac-
cording to the weights computed by applying the 
following formulation: 
( )i i iw f idftt =                      (8) 
Where tfi and idfi respectively are the term fre-
quency and inverse document frequency of ti in 
the clicked documents of a related query log. 
This formulation means that a term is more rep-
resentative if it has a higher frequency as well as 
a broader distribution in the related query log. 
3.3 Extract Representative Terms from 
Immediately Viewed Documents 
The representative terms extracted from immedi-
ately viewed documents are determined based on 
three factors: term frequency in the immediately 
viewed document, inverse document frequency in 
the entire seen search results, and a discriminant 
value. The formulation is as follows:  
( ) ( )Ni ii i
r ddw d
x xtf idfx x= ? ?             (9) 
Where tfxidr is the term frequency of term xi in the 
viewed results set dr; tfxidr is the inverse document 
frequency of xi in the entire seen results set dN. 
And the discriminant value d(xi) of xi is computed 
using the weighting schemes F2 (S. E. Robertson 
and K. Sparck Jones, 1976) as follows: 
( ) ln
( ) ( )i
r Rd
n r N Rx = ? ?
                 (10) 
Where r is the number of the immediately viewed 
documents containing term xi; n is the number of 
the seen results containing term xi; R is the num-
ber of the immediately viewed documents in the 
query; N is the number of the entire seen results.  
3.4 Sample Results 
Unlike other systems which do result re-ranking 
and query expansion respectively in different 
ways, our system implements these two functions 
simultaneously and collaboratively ?  Query 
expansion provides diversified search results 
which must rely on the use of re-ranking to be 
moved forward and recommended to the user. 
 
 
Figure 4. A screen shot for query expansion. 
 
After iteratively computing using our approach, 
the system selects some search results with top 
highest authority scores and recommends them to 
the user. In Table 2, we show that PAIR suc-
cessfully re-ranks the unseen search results of 
?jaguar? respectively using the immediately 
Google result PAIR result  
query = ?jaguar? query = ?jaguar? After the 4th result being clicked 
query = ?jaguar? 
?car? ? query logs 
1 Jaguar www.jaguar.com/ 
Jaguar 
www.jaguar.com/ 
Jaguar UK - Jaguar Cars 
www.jaguar.co.uk/ 
2 Jaguar CA - Jaguar Cars www.jaguar.com/ca/en/ 
Jaguar CA - Jaguar Cars 
www.jaguar.com/ca/en/ 
Jaguar UK - R is for? 
www.jaguar-racing.com/ 
3 Jaguar Cars www.jaguarcars.com/ 
Jaguar Cars 
www.jaguarcars.com/ 
Jaguar 
www.jaguar.com/ 
4 Apple - Mac OS X www.apple.com/macosx/ 
Apple - Mac OS X 
www.apple.com/macosx/ 
Jaguar CA - Jaguar Cars 
www.jaguar.com/ca/en/                      -2 
5 Apple - Support ? www.apple.com/support/... 
Amazon.com: Mac OS X 10.2? 
www.amazon.com/exec/obidos/... 
Jaguar Cars 
www.jaguarcars.com/                        -2 
6 Jaguar UK - Jaguar Cars www.jaguar.co.uk/ 
Mac OS X 10.2 Jaguar? 
arstechnica.com/reviews/os? 
Apple - Mac OS X 
www.apple.com/macosx/                     -2 
7 Jaguar UK - R is for? www.jaguar-racing.com/ 
Macworld: News: Macworld? 
maccentral.macworld.com/news/? 
Apple - Support ? 
www.apple.com/support/...                    -2 
8 Jaguar dspace.dial.pipex.com/? 
Apple - Support? 
www.apple.com/support/...                -3 
Jaguar 
dspace.dial.pipex.com/? 
9 Schr?dinger -> Home www.schrodinger.com/ 
Jaguar UK - Jaguar Cars 
www.jaguar.co.uk/                       -3 
Schr?dinger -> Home 
www.schrodinger.com/ 
10 Schr?dinger -> Site Map www.schrodinger.com/... 
Jaguar UK - R is for? 
www.jaguar-racing.com/                  -3 
Schr?dinger -> Site Map 
www.schrodinger.com/... 
589
viewed documents and the query logs. Simulta-
neously, some representative terms are selected 
to expand the original query. In the query of 
?jaguar? (without query logs), we click some 
results about ?Mac OS?, and then we see that a 
term ?Mac? has been selected to expand the 
original query, and some results of the new query 
?jaguar Mac? are recommended to the user under 
the help of re-ranking, as shown in Figure 4. 
4 Experiment 
4.1 Experimental Methodology 
It is a challenge to quantitatively evaluate the 
potential performance improvement of the pro-
posed approach over Google in an unbiased way 
(D. Hawking et al, 1999; Xuehua Shen et al, 
2005). Here, we adopt a similar quantitative 
evaluation as what Xuehua Shen et al (2005) do 
to evaluate our system PAIR and recruit 9 stu-
dents who have different backgrounds to partici-
pate in our experiment. We use query topics from 
TREC 2005 and 2004 Hard Track, TREC 2004 
Terabyte track for English information retrieval,3 
and use query topics from HTRDP 2005 Evalua-
tion for Chinese information retrieval.4 The rea-
son why we utilize multiple TREC tasks rather 
than using a single one is that more queries are 
more likely to cover the most interesting topics 
for each participant. 
Initially, each participant would freely choose 
some topics (typically 5 TREC topics and 5 
HTRDP topics). Each query of TREC topics will 
be submitted to three systems: UCAIR 5 (Xue-
hua Shen et al, 2005), ?PAIR No QE? (PAIR 
system of which the query expansion function is 
blocked) and PAIR. Each query of HTRDP topics 
needs only to be submitted to ?PAIR No QE? and 
PAIR. We do not evaluate UCAIR using HTRDP 
topics, since it does not support Chinese. For each 
query topic, the participants use the title of the 
topic as the initial keyword to begin with. Also 
they can form some other keywords by them-
selves if the title alone fails to describe some de-
tails of the topic. There is no limit on how many 
queries they must submit. During each query 
process, the participant may click to view some 
results, just as in normal web search. 
Then, at the end of each query, search results 
from these different systems are randomly and 
anonymously mixed together so that every par-
                                                          
3 Text REtrieval Conference. http://trec.nist.gov/ 
4 2005 HTRDP Evaluation. http://www.863data.org.cn/ 
5 The latest version released on November 11, 2005. 
http://sifaka.cs.uiuc.edu/ir/ucair/ 
ticipant would not know where a result comes 
from. The participants would judge which of 
these results are relevant. 
At last, we respectively measure precision at 
top 5, top 10, top 20 and top 30 documents of 
these system. 
4.2 Results and Analysis 
Altogether, 45 TREC topics (62 queries in all) are 
chosen for English information retrieval. 712 
documents are judged as relevant from Google 
search results. The corresponding number of 
relevant documents from UCAIR, ?PAIR No QE? 
and PAIR respectively is: 921, 891 and 1040. 
Figure 5 shows the average precision of these four 
systems at top n documents among such 45 TREC 
topics. 
 
 
Figure 5. Average precision for TREC topics. 
 
45 HTRDP topics (66 queries in all) are chosen 
for Chinese information retrieval. 809 documents 
are judged as relevant from Google search results. 
The corresponding number of relevant documents 
from ?PAIR No QE? and PAIR respectively is: 
1198 and 1416. Figure 6 shows the average pre-
cision of these three systems at top n documents 
among such 45 HTRDP topics. 
 
 
Figure 6. Average precision for HTRDP topics. 
 
PAIR and ?PAIR No QE? versus Google 
We can see clearly from Figure 5 and Figure 6 
that the precision of PAIR is improved a lot 
comparing with that of Google in all measure-
590
ments. Moreover, the improvement scale in-
creases from precision at top 10 to that of top 30. 
One explanation for this is that the more implicit 
feedback information generated, the more repre-
sentative terms can be obtained, and thus, the 
iterative algorithm can perform better, leading to 
more precise search results. ?PAIR No QE? also 
significantly outperforms Google in these meas-
urements, however, with query expansion, PAIR 
can perform even better. Thus, we say that result 
re-ranking and query expansion both play an 
important role in PAIR. 
Comparing Figure 5 with Figure 6, one can see 
that the improvement of PAIR versus Google in 
Chinese IR is even larger than that of English IR. 
One explanation for this is that: before imple-
menting the iterative algorithm, each Chinese 
search result, including title and snippet, is seg-
mented into words (or phrases). And only the 
noun, verb and adjective of these words (or 
phrases) are used in next stages, whereas, we only 
remove the stop words for English search result. 
Another explanation is that there are some Chi-
nese web pages with the same content. If one of 
such pages is clicked, then, occasionally some 
repetition pages are recommended to the user. 
However, since PAIR is based on the search re-
sults of Google and the information concerning 
the result pages that PAIR can obtained is limited, 
which leads to it difficult to avoid the replica-
tions. 
PAIR and ?PAIR No QE? versus UCAIR 
In Figure 5, we can see that the precision of 
?PAIR No QE? is better than that of UCAIR 
among top 5 and top 10 documents, and is almost 
the same as that of UCAIR among top 20 and top 
30 documents. However, PAIR is much better 
than UCAIR in all measurements. This indicates 
that result re-ranking fails to do its best without 
query expansion, since the relevant documents in 
original query are limited, and only the re-ranking 
method alone cannot solve the ?relevant docu-
ments sparseness? problem. Thus, the query ex-
pansion method, which can provide fresh and 
relevant documents, can help the re-ranking 
method to reach an even better performance. 
Efficiency of PAIR 
The iteration statistic in evaluation indicates that 
the average iteration times of our approach is 22 
before convergence on condition that we set the 
threshold ? = 10-6. The experiment shows that the 
computation time of the proposed approach is 
imperceptible for users (less than 1ms.) 
5 Related Work 
There have been many prior attempts to person-
alized search. In this paper, we focus on the re-
lated work doing personalized search based on 
implicit feedback information. 
Some of the existing studies capture users? in-
formation need by exploiting query logs. For 
example, M. Speretta and S. Gauch (2005) build 
user profiles based on activity at the search site 
and study the use of these profiles to provide 
personalized search results. F. Liu et al (2002) 
learn user's favorite categories from his query 
history. Their system maps the input query to a set 
of interesting categories based on the user profile 
and confines the search domain to these catego-
ries. Some studies improve retrieval performance 
by exploiting users? browsing history (F. Tanud-
jaja and L. Mu, 2002; M. Morita and Y. Shinoda, 
1994) or Web communities (A. Kritikopoulos 
and M. Sideri, 2003; K. Sugiyama et al, 2004) 
Some studies utilize client side interactions, for 
example, K. Bharat (2000) automatically discov-
ers related material on behalf of the user by 
serving as an intermediary between the user and 
information retrieval systems. His system ob-
serves users interacting with everyday applica-
tions and then anticipates their information needs 
using a model of the task at hand. Some latest 
studies combine several types of implicit feed-
back information. J. Teevan et al (2005) explore 
rich models of user interests, which are built 
from both search-related information, such as 
previously issued queries and previously visited 
Web pages, and other information about the user 
such as documents and email the user has read 
and created. This information is used to re-rank 
Web search results within a relevance feedback 
framework. 
Our work is partly inspired by the study of 
Xuehua Shen et al (2005), which is closely re-
lated to ours in that they also exploit immediately 
viewed documents and short-term history queries, 
implement query expansion and re-ranking, and 
develop a client-side web search agents that per-
form eager implicit feedback. However, their 
work differs from ours in three ways: First, they 
use the cosine similarity to implement query ex-
pansion, and use Rocchio formulation (J. J. 
Rocchio, 1971) to re-rank the search results. 
Thus, their query expansion and re-ranking are 
computed separately and are not so concise and 
collaborative. Secondly, their query expansion is 
based only on the past queries and is imple-
mented before the query, which leads to that 
591
their query expansion does not benefit from 
user?s click through data. Thirdly, they do not 
compute the relevance of search results and the 
relativity of expanded terms in an iterative fash-
ion. Thus, their approach does not utilize the re-
lation among search results, among expanded 
terms, and between search results and expanded 
terms. 
6 Conclusions 
In this paper, we studied how to exploit implicit 
feedback information to improve retrieval accu-
racy. Unlike most previous work, we propose a 
novel HITS-like iterative algorithm that can 
make use of query logs and immediately viewed 
documents in a unified way, which not only 
brings collaboration between query expansion 
and result re-ranking but also makes the whole 
system more concise. We further propose some 
specific techniques to capture and exploit these 
two types of implicit feedback information. Us-
ing these techniques, we develop a client-side 
web search agent PAIR. Experiments in English 
and Chinese collections show that our approach 
is both effective and efficient. 
However, there is still room to improve the 
performance of the proposed approach, such as 
exploiting other types of personalized informa-
tion, choosing some more effective strategies to 
extract representative terms, studying the effects 
of the parameters used in the approach, etc. 
Acknowledgement 
We would like to thank the anonymous review-
ers for their helpful feedback and corrections, 
and to the nine participants of our evaluation ex-
periments. Additionally, this work is supported 
by the National Science Fund of China under 
contact 60203007. 
References 
A. Kritikopoulos and M. Sideri, 2003. The Compass 
Filter: Search engine result personalization using 
Web communities. In Proceedings of ITWP, pages 
229-240. 
D. Hawking, N. Craswell, P.B. Thistlewaite, and D. 
Harman, 1999. Results and challenges in web 
search evaluation. Computer Networks, 
31(11-16):1321?1330. 
F. Liu, C. Yu, and W. Meng, 2002. Personalized web 
search by mapping user queries to categories. In 
Proceedings of CIKM, pages 558-565. 
F. Tanudjaja and L. Mu, 2002. Persona: a contextual-
ized and personalized web search. HICSS. 
G. Salton and M. J. McGill, 1983. Introduction to 
Modern Information Retrieval. McGraw-Hill. 
G. Salton and C. Buckley, 1990. Improving retrieval 
performance by relevance feedback. Journal of the 
American Society for Information Science, 
41(4):288-297. 
J. J. Rocchio, 1971. Relevance feedback in informa-
tion retrieval. In The SMART Retrieval System :  
Experiments in Automatic Document Processing, 
pages 313?323. Prentice-Hall Inc. 
J. Kleinberg, 1998. Authoritative sources in a hyper-
linked environment. ACM, 46(5):604?632. 
J. Pitkow, H. Schutze, T. Cass, R. Cooley, D. 
Turnbull, A. Edmonds, E. Adar, and T. Breuel, 
2002. Personalized search. Communications of the 
ACM, 45(9):50-55. 
J. Teevan, S. T. Dumais, and E. Horvitz, 2005. Per-
sonalizing search via automated analysis of interests 
and activities. In Proceedings of SIGIR, pages 
449-456. 
K. Bharat, 2000. SearchPad: Explicit capture of 
search context to support Web search. Computer 
Networks, 33(1-6): 493-501. 
K. Sugiyama, K. Hatano, and M. Yoshikawa, 2004. 
Adaptive Web search based on user profile con-
structed without any effort from user. In Proceed-
ings of WWW, pages 675-684. 
M. Beaulieu and S. Jones, 1998. Interactive searching 
and interface issues in the okapi best match retrieval 
system. Interacting with Computers, 10(3):237-248. 
M. Morita and Y. Shinoda, 1994. Information filtering 
based on user behavior analysis and best match text 
retrieval. In Proceedings of SIGIR, pages 272?281. 
M. Speretta and S. Gauch, 2005. Personalizing search 
based on user search history. Web Intelligence, 
pages 622-628. 
R. White, I. Ruthven, and J. M. Jose, 2002. The use of 
implicit evidence for relevance feedback in web 
retrieval. In Proceedings of ECIR, pages 93?109. 
S. E. Robertson and K. Sparck Jones, 1976. Relevance 
weighting of search terms. Journal of the 
American Society for Information Science, 
27(3):129-146. 
T. Joachims, L. Granka, B. Pang, H. Hembrooke, and 
G. Gay, 2005. Accurately Interpreting Clickthrough 
Data as Implicit Feedback, In Proceedings of 
SIGIR, pages 154-161. 
Xuehua Shen, Bin Tan, and Chengxiang Zhai, 2005. 
Implicit User Modeling for Personalized Search. In 
Proceedings of CIKM, pages 824-831. 
592
Proceedings of ACL-08: HLT, pages 148?155,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Selecting Query Term Alterations for Web Search by Exploiting Query 
Contexts 
 
Guihong Cao Stephen Robertson Jian-Yun Nie 
Dept. of Computer Science and 
Operations Research 
Microsoft Research at 
Cambridge 
Dept. of Computer Science and 
Operations Research 
University of Montreal, Canada Cambridge, UK University of Montreal, Canada 
caogui@iro.umontreal.ca ser@microsoft.com nie@iro.umontreal.ca 
 
  
 
 
Abstract 
Query expansion by word alterations (alterna-
tive forms of a word) is often used in Web 
search to replace word stemming. This allows 
users to specify particular word forms in a 
query. However, if many alterations are 
added, query traffic will be greatly increased. 
In this paper, we propose methods to select 
only a few useful word alterations for query 
expansion. The selection is made according to 
the appropriateness of the alteration to the 
query context (using a bigram language 
model), or according to its expected impact 
on the retrieval effectiveness (using a regres-
sion model). Our experiments on two TREC 
collections will show that both methods only 
select a few expansion terms, but the retrieval 
effectiveness can be improved significantly. 
1 Introduction 
Word stemming is a basic NLP technique used in 
most of Information Retrieval (IR) systems. It 
transforms words into their root forms so as to in-
crease the chance to match similar words/terms 
that are morphological variants. For example, with 
stemming, ?controlling? can match ?controlled? 
because both have the same root ?control?. Most 
stemmers, such as the Porter stemmer (Porter, 
1980) and Krovetz stemmer (Krovetz, 1993), deal 
with stemming by stripping word suffixes accord-
ing to a set of morphological rules. Rule-based ap-
proaches are intuitive and easy to implement. 
However, while in general, most words can be 
stemmed correctly; there is often erroneous stem-
ming that unifies unrelated words. For instance, 
?jobs? is stemmed to ?job? in both ?find jobs in 
Apple? and ?Steve Jobs at Apple?. This is particu-
larly problematic in Web search, where users often 
use special or new words in their queries. A stan-
dard stemmer such as Porter?s will wrongly stem 
them.  
To better determine stemming rules, Xu and 
Croft (1998) propose a selective stemming method 
based on corpus analysis. They refine the Porter 
stemmer by means of word clustering: words are 
first clustered according to their co-occurrences in 
the text collection. Only word variants belonging 
to the same cluster will be conflated.  
Despite this improvement, the basic idea of 
word stemming is to transform words in both doc-
uments and queries to a standard form. Once this is 
done, there is no means for users to require a spe-
cific word form in a query ? the word form will be 
automatically transformed, otherwise, it will not 
match documents. This approach does not seem to 
be appropriate for Web search, where users often 
specify particular word forms in their queries. An 
example of this is a quoted query such as ?Steve 
Jobs?, or ?US Policy?. If documents are stemmed, 
many pages about job offerings or US police may 
be returned (?policy? conflates with ?police? in 
Porter stemmer). Another drawback of stemming is 
that it usually enhances recall, but may hurt preci-
sion (Kraaij and Pohlmann, 1996). However, gen-
eral Web search is basically a precision-oriented 
task.  
One alternative approach to word stemming is to 
do query expansion at query time.  The original 
query terms are expanded by their related forms 
having the same root. All expansions can be com-
bined by the Boolean operator ?OR?.  For example, 
148
the query ?controlling acid rain? can be expanded 
to ?(control OR controlling OR controller OR con-
trolled OR controls) (acid OR acidic OR acidify) 
(rain OR raining OR rained OR rains)?. We will 
call each such expansion term an alteration to the 
original query term. Once a set of possible altera-
tions is determined, the simplest approach to per-
form expansion is to add all possible alterations. 
We call this approach Naive Expansion. One can 
easily show that stemming at indexing time is 
equivalent to Naive Expansion at retrieval time. 
This approach has been adopted by most commer-
cial search engines (Peng et al, 2007). However, 
the expansion approaches proposed previously can 
have several serious problems: First, they usually 
do not consider expansion ambiguity ? each query 
term is usually expanded independently. However, 
some expansion terms may not be appropriate. The 
case of ?Steve Jobs? is one such example, for 
which the word ?job? can be proposed as an ex-
pansion term. Second, as each query term may 
have several alterations, the na?ve approach using 
all the alterations will create a very long query. As 
a consequence, query traffic (the time required for 
the evaluation of a query) is greatly increased. 
Query traffic is a critical problem, as each search 
engine serves millions of users at the same time. It 
is important to limit the query traffic as much as 
possible. 
In practice, we can observe that some word al-
terations are irrelevant and undesirable (as in the 
?Steve Jobs? case), and some other alterations have 
little impact on the retrieval effectiveness (for ex-
ample, if we expand a word by a rarely used word 
form). In this study, we will address these two 
problems. Our goal is to select only appropriate 
word alterations to be used in query expansion. 
This is done for two purposes: On the one hand, 
we want to limit query traffic as much as possible 
when query expansion is performed. On the other 
hand, we also want to remove irrelevant expansion 
terms so that fewer irrelevant documents will be 
retrieved, thereby improve the retrieval effective-
ness. 
To deal with the two problems we mentioned 
above, we will propose two methods to select al-
terations. In the first method, we make use of the 
query context to select only the alterations that fit 
the query. The query context is modeled by a bi-
gram language model. To reduce query traffic, we 
select only one alteration for each query term, 
which is the most coherent with the bigram model. 
We call this model Bigram Expansion. Despite the 
fact that this method adds far fewer expansion 
terms than the na?ve expansion, our experiments 
will show that we can achieve comparable or even 
better retrieval effectiveness. 
Both the Naive Expansion and the Bigram Ex-
pansion determine word alterations solely accord-
ing to general knowledge about the language 
(bigram model or morphological rules), and no 
consideration about the possible effect of the ex-
pansion term is made. In practice, some alterations 
will have virtually no impact on retrieval effec-
tiveness. They can be ignored. Therefore, in our 
second method, we will try to predict whether an 
alteration will have some positive impact on re-
trieval effectiveness. Only the alterations with pos-
itive impact will be retained. In this paper, we will 
use a regression model to predict the impact on 
retrieval effectiveness. Compared to the bigram 
expansion method, the regression method results in 
even fewer alterations, but experiments show that 
the retrieval effectiveness is even better.  
Experiments will be conducted on two TREC 
collections, Gov2 data for Web Track and 
TREC6&7&8 for ad-hoc retrieval. The results 
show that the two methods we propose both out-
perform the original queries significantly with less 
than two alterations per query on average. Com-
pared to the Naive Expansion method, the two me-
thods can perform at least equally well, while 
query traffic is dramatically reduced.  
In the following section, we provide a brief re-
view of related work. Section 3 shows how to gen-
erate alteration candidates using a similar approach 
to Xu and Croft?s corpus analysis (1998). In sec-
tion 4 and 5, we describe the Bigram Expansion 
method and Regression method respectively. Sec-
tion 6 presents some experiments on TREC 
benchmarks to evaluate our methods. Section 7 
concludes this paper and suggests some avenues 
for future work.  
2 Related Work 
Many stemmers have been implemented and used 
as standard processing in IR. Among them, the 
Porter stemmer (Porter, 1980) is the most widely 
used. It strips term suffixes step-by-step according 
to a set of morphological rules. However, the Por-
ter stemmer sometimes wrongly transforms a term 
into an unrelated root. For example, it will unify 
149
?news? and ?new?, ?execute? and ?executive?. On 
the other hand, it may miss some conflations, such 
as ?mice? and ?mouse?, ?europe? and ?european?. 
Krovetz (1993) developed another stemmer, which 
uses a machine-readable dictionary, to improve the 
Porter stemmer. It avoids some of the Porter 
stemmer?s wrong stripping, but does not produce 
consistent improvement in IR experiments.  
Both stemmers use generic rules for English to 
strip each word in isolation. In practice, the re-
quired stemming may vary from one text collection 
to another. Therefore, attempts have been made to 
use corpus analysis to improve existing rule-based 
stemmers. Xu and Croft (1998) create equivalence 
clusters of words which are morphologically simi-
lar and occur in similar contexts. 
As we stated earlier, the stemming-based IR ap-
proaches are not well suited to Web search. Query 
expansion has been used as an alternative (Peng et 
al. 2007). To limit the number of expansion terms, 
and thus the query traffic, Peng et al only use al-
terations for some of the query words: They seg-
ment each query into phrases and only the head 
word in each phrase is expanded. The assumptions 
are: 1)Queries issued in Web search often consist 
of noun phrases. 2) Only the head word in the noun 
phrase varies in form and needs to be expanded. 
However, both assumptions may be questionable. 
Their experiments did not show that the two as-
sumptions hold.  
Stemming is related to query expansion or query 
reformulation (Jones et al, 2006; Anick, 2003; Xu 
and Croft, 1996), although the latter is not limited 
to word variants. If the expansion terms used are 
those that are variant forms of a word, then query 
expansion can produce the same effect as word 
stemming. However, if we add all possible word 
alterations, query expansion/reformulation will run 
the risk of adding many unrelated terms to the 
original query, which may result in both heavy 
traffic and topic drift. Therefore, we need a way to 
select the most appropriate expansion terms. In 
(Peng et al 2007), a bigram language model is 
used to determine the alteration of the head word 
that best fits the query. In this paper, one of the 
proposed methods will also use a bigram language 
model of the query to determine the appropriate 
alteration candidates. However, in our approach, 
alterations are not limited to head words. In addi-
tion, we will also propose a supervised learning 
method to predict if an alteration will have a posi-
tive impact on retrieval effectiveness. To our 
knowledge, no previous method uses the same ap-
proach. 
In the following sections, we will describe our 
approach, which consists of two steps: the genera-
tion of alteration candidates, and the selection of 
appropriate alterations for a query. The first step is 
query-independent using corpus analysis, while the 
second step is query-dependent. The selected word 
alterations will be OR-ed with the original query 
words. 
3  Generating Alteration Candidates 
Our method to generate alteration candidates can 
be described as follows. First, we do word cluster-
ing using a Porter stemmer. All words in the vo-
cabulary sharing the same root form are grouped 
together. Then we do corpus analysis to filter out 
the words which are clustered incorrectly, accord-
ing to word distributional similarity, following (Xu 
and Croft, 1998; Lin 1998). The rationale behind 
this is that words sharing the same meaning tend to 
occur in the same contexts.  
The context of each word in the vocabulary is 
represented by a vector containing the frequencies 
of the context words which co-occur with the word 
within a predefined window in a training corpus. 
The window size is set empirically at 3 words and 
the training corpus is about 1/10 of the GOV2 cor-
pus (see section 5 for details about the collection). 
Similarity is measured by the cosine distance be-
tween two vectors. For each word, we select at 
most 5 similar words as alteration candidates.  
In the next sections, we will further consider ways 
to select appropriate alterations according to the 
query. 
4 Bigram Expansion Model for Alteration 
Selection 
In this section, we try to select the most suitable 
alterations according to the query context. The 
query context is modeled by a bigram language 
model as in (Peng et al 2007).  
Given a query described by a sequence of 
words, we consider each of the query word as rep-
resenting a concept ci. In addition to the given 
word form, ci can also be expressed by other alter-
native forms. However, the appropriate alterations 
do not only depend on the original word of ci, but 
also on other query words or their alterations.  
150
  
 
 
 
 
 
 
Figure 1: Considering all Combinations to Calculate the 
Plausibility of Alterations 
 
Accordingly, a confidence weight is determined 
for each alteration candidate. For example, in the 
query ?Steve Jobs at Apple?, the alteration ?job? of 
?jobs? should have a low confidence; while in the 
query ?finding jobs in Apple?, it should have a 
high confidence.  
One way to measure the confidence of an altera-
tion is the plausibility of its appearing in the query. 
Since each concept may be expressed by several 
alterations, we consider all the alterations of con-
text concepts when calculating the plausibility of a 
given word. Suppose we have the query ?control-
ling acid rain?. The second concept has two altera-
tions - ?acidify? and ?acidic?. For each of the 
alterations, our method will consider all the com-
binations with other words, as illustrated in figure 
1, where each combination is shown as a path. 
More precisely, for a query of n words (or their 
corresponding concepts), let ei,j?ci, j=1,2,?,|ci| be 
the alterations of concept ci. Then we have: 
?
? ? ? ?
=
= = =? =+
?
?
+
+
=
||
1, ,,,2,1
||
1,1
||
1,2
||
1,1
||
1,1
),...,,...,,(...            
......)(
21
1
1
2
2
1
1
1
1
n
n ni
i
i
i
i
c
jn jnjijj
c
j
c
j
c
ji
c
jiij
eeeeP
eP
            (1) 
In equation 1, 
ni jnjijj eeee ,,,2,1 ,...,,...,, 21 is a path 
passing through ei,j. For simplicity, we abbreviate it 
as e1e2?ei?en. In this work, we used bigram lan-
guage model to calculate the probability of each 
path. Then we have: 
?
=
?
=
n
k kkni
eePePeeeeP
2 1121
)|()(),...,,...,,(               (2) 
P(ek|ek-1) is estimated with a back-off bigram lan-
guage model (Goodman, 2001). In the experiments 
with TREC6&7&8, we train the model with all 
text collections; while in the experiments with 
Gov2 data, we only used about 1/10 of the GOV2 
data to train the bigram model because the whole 
Gov2 collection is too large.   
Directly calculating P(eij) by summing the prob-
abilities of all paths passing through eij is an NP 
problem (Rabiner, 1989), and is intractable if the 
query is long. Therefore, we use the forward-
backward algorithm (Bishop, 2006) to calculate 
P(eij) in a more efficient way. After calculating 
P(eij) for each ci, we select one alteration which 
has the highest probability. We limit the number of 
additional alterations to 1 in order to limit query 
traffic. Our experiments will show that this is often 
sufficient. 
5 Regression Model for Alteration Selec-
tion 
None of the previous selection methods considers 
how well an alteration would perform in retrieval. 
The Bigram Expansion model assumes that the 
query replaced with better alterations should have 
a higher likelihood. This approach belongs to the 
family of unsupervised learning. In this section, we 
introduce a method belonging to supervised learn-
ing family. This method develops a regression 
model from a set of training data, and it is capable 
of predicting the expected change in performance 
when the original query is augmented by this al-
teration. The performance change is measured by 
the difference in the Mean Average Precision 
(MAP) between the augmented and the original 
query. The training instances are defined by the 
original query string, an original query term under 
consideration and one alteration to the query term. 
A set of features will be used, which will be de-
fined later in this section.  
5.1 Linear Regression Model  
The goal of the regression model is to predict the 
performance change when a query term is aug-
mented with an alteration. There are several re-
gression models, ranging from the simplest linear 
regression model to non-linear alternatives, such as 
a neural network (Duda et al, 2001), a Regression 
SVM (Bishop, 2006). For simplicity, we use linear 
regression model here. We denote an instance in 
the feature space as X, and the weights of features 
are denoted as W. Then the linear regression model 
is defined as: 
f(X)=WTX                                                             (3) 
where WT is the transpose of W. However, we will 
have a technical problem if we set the target value 
to the performance change directly: The range of 
controlling 
control 
controlled 
controller 
acidify 
acidic 
rain 
rains 
raining 
151
values of f(X) is ),( +??? , while the range of per-
formance change is [-1,1]. The two value ranges do 
not match. This inconsistency may result in severe 
problems when the scales of feature values vary 
dramatically (Duda et al, 2001). To solve this 
problem, we do a simple transformation on the per-
formance change. Let the change be ]1,1[??y , then 
the transformed performance change is: 
]1,1[     
1
1log)( ??
+?
++
= y
y
yy
?
??                            (4) 
where ? is a very small positive real number (set to 
be 1e-37 in the experiments), which acts as a 
smoothing factor. The value of )(y? can be an arbi-
trary real number.  )(y?  is a monotonic function 
defined in the range of [-1,1]. Moreover, the fixed 
point of )(y? is 0, i.e., yy =)(? when y=0. This 
property is nice; it means that the expansion brings 
positive improvement if and only if f(X)>0, which 
makes it easy to determine which alteration is bet-
ter.  
We train the regression model by minimizing 
the mean square error. Suppose there are training 
instances X1,X2,?,Xm, and the corresponding per-
formance change is yi, i=1,2,?,m. We calculate 
the mean square error with the following equation: 
?
=
?=
m
i ii
T yXWWerr
1
2))(()( ?                                 (5) 
Then the optimal weight is defined as: 
?
=
?=
=
m
i ii
T
W
W
yXW
WerrW
1
2
*
))((minarg       
)(minarg
?
                  (6) 
Because err(W) is a convex function of W, it has 
a global minimum and obtains its minimum when 
the gradient is zero (Bazaraa et al, 2006). Then we 
have: 
0))(()(
1*
*
=?=
?
? ?
=
m
i
T
iii
T XyXW
W
Werr ?  
So,  ??
==
=
m
i
T
ii
m
i
T
ii
T XyXXW
11
* )(?  
In fact,  ?
=
m
i
T
ii XX1  is a square matrix, we denote 
it as XXT. Then we have: [ ]?
=
?
=
m
i ii
T XyXXW
1
1* )()( ?                                   (7) 
The matrix  XXT is an ll ? square matrix, where l 
is the number of features. In our experiments, we 
only use three features. Therefore the optimal 
weights can be calculated efficiently even we have 
a large number of training instances. 
5.2 Constructing Training Data 
As a supervised learning method, the regression 
model is trained with a set of training data. We 
illustrate here the procedure to generate training 
instances with an example.  
Given a query ?controlling acid rain?, we obtain 
the MAP of the original query at first. Then we 
augment the query with an alteration to the original 
term (one term at a time) at each time. We retain 
the MAP of the augmented query and compare it 
with the original query to obtain the performance 
change. For this query, we expand ?controlling? by 
?control? and get an augmented query ?(control-
ling OR control) acid rain?. We can obtain the dif-
ference between the MAP of the augmented query 
and that of the original query. By doing this, we 
can generate a series of training instances consist-
ing of the original query string, the original query 
term under consideration, its alteration and the per-
formance change, for example: 
<controlling acid rain, controlling, control,  0.05> 
Note that we use MAP to measure performance, 
but we could well use other metrics such as NDCG 
(Peng et al, 2007) or P@N (precision at top-N 
documents).  
5.3 Features Used for Regression Model 
Three features are used. The first feature reflects to 
what degree an alteration is coherent with the other 
terms. For example, for the query ?controlling acid 
rain?, the coherence of the alteration ?acidic? is 
measured by the logarithm of its co-occurrence 
with the other query terms within a predefined 
window (90 words) in the corpus. That is: 
log(count(controlling?acidic?rain|window)+0.5) 
where ??? means there may be some words be-
tween two query terms. Word order is ignored.  
The second feature is an extension to point-wise 
mutual information (Rijsbergen, 1979), defined as 
follows: 
???
?
???
?
)()()(
)|......(log
rainPacidicPgcontrollinP
windowrainacidicgcontrollinP
 
where P(controlling?acidic?rain|window) is the 
co-occurrence probability of the trigram containing 
acidic within a predefined window (50 words). 
P(controlling), p(acidic), P(rain) are probabilities 
of the three words in the collection. The three 
words are defined as: the term under consideration, 
the first term to the left of that term, and the first 
term to the right. If a query contains less than 3 
152
terms or the term under consideration is the begin-
ning/ending term in the query, we will set the 
probability of the missed term/terms to be 1. 
Therefore, it becomes point-wise mutual informa-
tion when the query contains only two terms. In 
fact, this feature is supplemental to the first feature. 
When the query is very long and the first feature 
always obtains a value of log(0.5), so it does not 
have any discriminative ability. On the other hand, 
the second feature helps because it can capture 
some co-occurrence information no matter how 
long the query is.  
The last feature is the bias, whose value is al-
ways set to be 1.0.   
The regression model is trained in a leave-one-
out cross-validation manner on three collections; 
each of them is used in turn as a test collection 
while the two others are used for training.  For 
each incoming query, the regression model pre-
dicts the expected performance change when one 
alteration is used. For each query term, we only 
select the alteration with the largest positive per-
formance change. If none of its alterations produce 
a positive performance change, we do not expand 
the query term. This selection is therefore more 
restrictive than the Bigram Expansion Model. 
Nevertheless, our experiments show that it im-
proves retrieval effectiveness further. 
6 Experiments 
6.1 Experiment Settings 
In this section, our aim is to evaluate the two con-
text-sensitive word alteration selection methods. 
The ideal evaluation corpus should be composed of 
some Web data. Unfortunately, such data are not 
publicly available and the results also could not be 
compared with other published results. Therefore, 
we use two TREC collections. The first one is the 
ad-hoc retrieval test collections used for 
TREC6&7& 8. This collection is relative small and 
homogeneous. The second one is the Gov2 data. It 
is obtained by crawling the entire .gov domain and 
has been used for three TREC Terabyte tracks 
(TREC2004-2006). Table 1 shows some statistics 
of the two collections. For each collection, we use 
150 queries. Since the Regression model needs 
some data for training, we divided the queries into 
three parts, each containing 50 queries. We then 
use leave-one-out cross-validation. The evaluation 
metrics shown below are the average value of the  
 
Name Description Size 
(GB) 
#Doc Query 
TREC6 
&7&8 
TREC disk4&5, 
Newpapers 
1.7 500,447 301-450 
Gov2 2004 crawl of entire 
.gov domain 
427 25,205,179 701-850 
Table1: Overview of Test Collections 
 
three-fold cross-validation. Because the queries in 
Web are usually very short, we use only the title 
field of each query.  
To correspond to Web search practice, both 
documents and queries are not stemmed. We do 
not filter the stop words either.  
Two main metrics are used: the Mean Average 
Precision (MAP) for the top 1000 documents to 
measure retrieval effectiveness, and the number of 
terms in the query to reflect query traffic. In addi-
tion, we also provide precision for the top 30 doc-
uments (P@30) to show the impact on top ranked 
documents. We also conducted t-tests to determine 
whether the improvement is statistically significant. 
The Indri 2.5 search engine (Strohman et al, 
2004) is used as our basic retrieval system. It pro-
vides for a rich query language allowing disjunc-
tive combinations of words in queries.  
6.2 Experimental Results 
The first baseline method we compare with only 
uses the original query, which is named Original. 
In addition to this, we also compare with the fol-
lowing methods: 
Na?ve Exp: The Na?ve expansion model expands 
each query term with all terms in the vocabu-
lary sharing the same root with it. This model is 
equivalent to the traditional stemming method. 
UMASS: This is the result reported in (Metzler et al, 
2006) using Porter stemming for both document 
and query terms. This reflects a state-of-the-art 
result using Porter stemming. 
Similarity: We select the alterations (at most 5) 
with the highest similarity to the original term. 
This is the method described in section 3.  
The two methods we propose in this paper are the 
following ones: 
Bigram Exp: the alteration is chosen by a Bigram 
Expansion model. 
Regression: the alteration is chosen by a Regres-
sion model.  
 
153
Model P@30 #term MAP Imp. 
Original 0.4701 158 0.2440 ---- 
UMASS ------- ------- 0.2666 9.26 
Na?ve Exp 0.4714 1345 0.2653 8.73 
Similarity 0.4900 303 0.2689 10.20* 
Bigram Exp 0.5007 303 0.2751 12.75** 
Regression 0.5054 237 0.2773 13.65** 
Table 2: Results of Query 701-750 Over Gov2 Data 
 
Model P@30 #term MAP Imp. 
Original 0.4907 158 0.2738 ---- 
UMASS ------- ------- 0.3251 18.73 
Naive Exp 0.5213 1167 0.3224 17.75** 
Similarity 0.5140 290 0.3043 11.14** 
Bigram Exp. 0.5153 290 0.3107 13.47** 
Regression 0.5140 256 0.3144 14.82** 
Table 3: Results of Query 751-800 over Gov2 Data 
 
Model P@30 #term MAP Imp. 
Original 0.4710 154 0.2887 ---- 
UMASS ------- ------- 0.2996 3.78 
Na?ve Exp 0.4633 1225 0.2999 3.87 
Similarity 0.4710 288 0.2976 3.08 
Bigram Exp 0.4730 288 0.3137 8.66** 
Regression 0.4748 237 0.3118 8.00* 
Table 4: Results of Query 801-850 over Gov2 Data 
 
Model P@30 #term MAP Imp. 
Original 0.2673 137 0.1669 ---- 
Na?ve Exp 0.3053 783 0.2146 28.57** 
Similarity 0.3007 255 0.2020 21.03** 
Bigram Exp 0.3033 255 0.2091 25.28** 
Regression 0.3113 224 0.2161 29.48** 
Table 5: Results of Query 301-350 over TREC6&7&8 
 
Model P@30 #term MAP Imp. 
Original 0.2820 126 0.1639 ----- 
Naive Exp 0.2787 736 0.1665 1.59 
Similarity 0.2867 244 0.1650 0.67 
Bigram Exp. 0.2800 244 0.1641 0.12 
Regression 0.2867 214 0.1664 1.53 
Table 6: Results of Query 351-400 over TREC6&7&8 
 
Model P@30 #term MAP Imp. 
Original 0.2833 124 0.1759 ----- 
Na?ve Exp 0.3167 685 0.2138 21.55** 
Similarity 0.3080 240 0.2066 17.45** 
Bigram Exp 0.3133 240 0.2080 18.25** 
Regression 0.3220 187 0.2144 21.88** 
Table7: Results of Query 401-450 over TREC6&7&8 
 
Tables 2, 3, 4 show the results of Gov2 data 
while table 5, 6, 7 show the results of the 
TREC6&7&8 collection. In the tables, the * mark 
indicates that the improvement over the original 
model is statistically significant with p-value<0.05, 
and ** means the p-values<0.01.  
From the tables, we see that both word stem-
ming (UMASS) and expansion with word altera-
tions can improve MAP for all six tasks. In most 
cases (except in table 4 and 6), it also improve the 
precision of top ranked documents. This shows the 
usefulness of word stemming or word alteration 
expansion for IR. 
We can make several additional observations: 
1). Stemming Vs Expansion. UMASS uses docu-
ment and query stemming while Naive Exp uses 
expansion by word alteration. We stated that both 
approaches are equivalent. The equivalence is 
confirmed by our experiment results: for all Gov2 
collections, these approaches perform equiva-
lently.  
2). The Similarity model performs very well. Com-
pared with the Na?ve Expansion model, it pro-
duces quite similar retrieval effectiveness, while 
the query traffic is dramatically reduced. This 
approach is similar to the work of Xu and Croft 
(1998), and can be considered as another state-of-
the-art result. 
3). In comparison, the Bigram Expansion model 
performs better than the Similarity model. This 
shows that it is useful to consider query context 
in selecting word alterations. 
4). The Regression model performs the best of all 
the models. Compared with the Original query, it 
adds fewer than 2 alterations for each query on 
average (since each group has 50 queries); never-
theless we obtained improvements on all the six 
collections. Moreover, the improvements on five 
collections are statistically significant. It also per-
forms slightly better than the Similarity and Bi-
gram Expansion methods, but with fewer 
alterations. This shows that the supervised learn-
ing approach, if used in the correct way, is supe-
rior to an unsupervised approach. Another 
advantage over the two other models is that the 
Regression model can reduce the number of al-
terations further. Because the Regression model 
selects alterations according to their expected 
improvement, the improvement of the alterations 
to one query term can be compared with that of 
the alterations to other query terms. Therefore, 
we can select at most one optimal alteration for 
the whole query. However, with the Similarity or 
Bigram Expansion models, the selection value, 
either similarity or query likelihood, cannot be 
154
compared across the query terms. As a conse-
quence, more alterations need to be selected, 
leading to heavier query traffic.  
7 Conclusion  
Traditional IR approaches stem terms in both doc-
uments and queries. This approach is appropriate 
for general purpose IR, but is ill-suited for the spe-
cific retrieval needs in Web search such as quoted 
queries or queries with a specific word form that 
should not be stemmed. The current practice in 
Web search is not to stem words in index, but ra-
ther to perform a form of expansion using word 
alteration. 
However, a na?ve expansion will result in many 
alterations and this will increase the query traffic. 
This paper has proposed two alternative methods 
to select precise alterations by considering the 
query context. We seek to produce similar or better 
improvements in retrieval effectiveness, while lim-
iting the query traffic. 
In the first method proposed ? the Bigram Ex-
pansion model, query context is modeled by a bi-
gram language model. For each query term, the 
selected alteration is the one which maximizes the 
query likelihood. In the second method - Regres-
sion model, we fit a regression model to calculate 
the expected improvement when the original query 
is expanded by an alteration. Only the alteration 
that is expected to yield the largest improvement to 
retrieval effectiveness is added. 
The proposed methods were evaluated on two 
TREC benchmarks: the ad-hoc retrieval test collec-
tion for TREC6&7&8 and the Gov2 data. Our ex-
perimental results show that both proposed 
methods perform significantly better than the orig-
inal queries. Compared with traditional word 
stemming or the na?ve expansion approach, our 
methods can not only  improve retrieval effective-
ness, but also greatly reduce the query traffic. 
This work shows that query expansion with 
word alterations is a reasonable alternative to word 
stemming. It is possible to limit the query traffic by 
a query-dependent selection of word alterations. 
Our work shows that both unsupervised and super-
vised learning can be used to perform alteration 
selection. 
Our methods can be further improved in several 
aspects. For example, we could integrate other fea-
tures in the regression model, and use other non-
linear regression models, such as Bayesian regres-
sion models (e.g. Gaussian Process regression) 
(Rasmussen and Williams, 2006). The additional 
advantage of these models is that we can not only 
obtain the expected improvement in retrieval effec-
tiveness for an alteration, but also the probability 
of obtaining an improvement (i.e. the robustness of 
the alteration).  
Finally, it would be interesting to test the ap-
proaches using real Web data. 
References  
Anick, P. (2003) Using Terminological Feedback for 
Web Search Refinement: a Log-based Study. In 
SIGIR, pp. 88-95. 
Bazaraa, M., Sherali, H., and Shett, C. (2006). Nonlin-
ear Programming, Theory and Algorithms. John 
Wiley & Sons Inc.  
Bishop, C. (2006). Pattern Recognition and Machine 
Learning. Springer.  
Duda, R.,  Hart, P.,  and Stork, D. (2001). Pattern Clas-
sification, John Wiley & Sons, Inc.  
Goodman, J. (2001). A Bit of Progress in Language 
Modeling. Technical report. 
Jones, R., Rey, B., Madani, O., and Greiner, W. (2006). 
Generating Query Substitutions.  In WWW2006, pp. 
387-396 
Kraaij, W. and Pohlmann, R. (1996) Viewing Stemming 
as Recall Enhancement. Proc. SIGIR, pp. 40-48. 
Krovetz, R. (1993). Viewing Morphology as an Infer-
ence Process.  Proc. ACM SIGIR, pp. 191-202.  
Lin, D. (1998). Automatic Retrieval and Clustering of 
Similar Words. In COLING-ACL, pp. 768-774. 
Metzler, D., Strohman, T. and Croft, B. (2006). Indri 
TREC Notebook 2006: Lessons learned from Three 
Terabyte Tracks. In the Proceedings of TREC 2006.  
Peng, F., Ahmed, N., Li, X., and Lu, Y. (2007). Context 
Sensitive Stemming for Web Search. Proc. ACM 
SIGIR, pp. 639-636 .  
Porter, M. (1980) An Algorithm for Suffix Stripping. 
Program, 14(3): 130-137. 
Rabiner, L. (1989). A Tutorial on Hidden Markov Mod-
els and Selected Applications in Speech Recognition. 
In Proceedings of IEEE Vol. 77(2), pp. 257-286.  
Rijsbergen, V. (1979). Information Retrieval. Butter-
worths, second version.  
Strohman, T., Metzler, D. and Turtle, H., and Croft, B. 
(2004). Indri: A Language Model-based Search En-
gine for Complex Queries. In Proceedings of the In-
ternational conference on Intelligence Analysis.  
Xu, J. and Croft, B. (1996). Query Expansion Using 
Local and Global Document Analysis. Proc. ACM 
SIGIR, pp. 4-11.  
Xu, J. and Croft, B. (1998). Corpus-based Stemming 
Using Co-occurrence of Word Variants. ACM 
TOIS, 16(1): 61-81.  
155
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 551?559,
Sydney, July 2006. c?2006 Association for Computational Linguistics
 
Context-Dependent Term Relations for Information Retrieval 
 
Jing Bai      Jian-Yun Nie     Guihong Cao
DIRO, University of Montreal 
CP. 6128, succ. Centre-ville, Montreal,  
Quebec H3C 3J7, Canada 
{baijing,nie,caogui}@iro.umontreal.ca 
 
 
 
Abstract 
Co-occurrence analysis has been used to 
determine related words or terms in many 
NLP-related applications such as query 
expansion in Information Retrieval (IR). 
However, related words are usually 
determined with respect to a single word, 
without relevant information for its 
application context. For example, the word 
?programming? may be considered to be 
strongly related to ?Java?, and applied 
inappropriately to expand a query on ?Java 
travel?. To solve this problem, we propose 
to add another context word in the relation 
to specify the appropriate context of the 
relation, leading to term relations of the 
form ?(Java, travel) ? Indonesia?. The 
extracted relations are used for query 
expansion in IR. Our experiments on 
several TREC collections show that this 
new type of context-dependent relations 
performs much better than the traditional 
co-occurrence relations.  
1. Introduction 
A query usually is a poor expression of an 
information need. This is not only due to its short 
length (usually a few words), but also due to the 
inability of users to provide the best terms to 
describe their information need. At best, one can 
expect that some, but not all, relevant terms are 
used in the query. Query expansion thus aims to 
improve query expression by adding related 
terms to the query. However, the effect of query 
expansion is strongly determined by the term 
relations used (Peat and Willett, 1991). For 
example, even if ?programming? is strongly 
related to ?Java?, if this relation is used to 
expand a query on ?Java travel?, the retrieval 
result will likely deteriorate because the 
irrelevant term ?programming? is introduced, 
leading to the retrieval of irrelevant documents 
about ?programming?.  
    A number of attempts have been made to deal 
with the problem of selecting appropriate 
expansion terms. For example, Wordnet has been 
used in (Voorhees, 1994) to determine the 
expansion terms. However, the experiments did 
not show improvement on retrieval effectiveness. 
Many experiments have been carried out using 
associative relations extracted from term co-
occurrences; but they showed variable results 
(Peat and Willett, 1991). In (Qiu and Frei, 1993), 
it is observed that one of the reasons is that one 
tried to determine expansion terms according to 
each original query term separately, which may 
introduce much noise. Therefore, they proposed 
to determine the expansion terms by summing up 
the relations of a candidate expansion term to 
each of the query terms. In so doing, a candidate 
expansion term is preferred if it has a strong 
relationship with many of the query terms. 
However, it is still difficult to prevent the 
expansion process from adding ?programming? 
to a query on ?Java travel? because of its very 
strong relation with ?Java?. 
The approach used in (Qiu and Frei, 1993) 
indeed tries to correct a handicap inherent in the 
relations: as term relations are created between 
two single words such as ?Java ? 
programming?, no information is available to 
help determine the appropriate context to apply 
it. The approach used in (Qiu and Frei, 1993) can 
simply alleviate the problem without solving it 
radically. 
    In this paper, we argue that the solution lies in 
the relations themselves. They have to contain 
more information to help determine the 
appropriate context to apply them. We thus 
propose a way to add some context information 
into the relations: we introduce an additional 
word into the condition part of the relation, such 
as ?(Java, computer) ? programming?, which 
551
 means ?programming? is related to ?(Java, 
computer)? together. In so doing, we would be 
able to prevent from extracting and applying a 
relation such as ?(Java, travel) ? 
programming?.  
    In this paper, we will test the extracted 
relations in query expansion for IR. We choose to 
implement query expansion within the language 
modeling (LM) framework because of its 
flexibility and high performance. The 
experiments on several TREC collections will 
show that our query expansion approach can 
bring large improvements in retrieval 
effectiveness. 
    In the following sections, we will first review 
some of the relevant approaches on query 
expansion and term relation extraction. Then we 
will describe our general IR models and the 
extraction of term relations. The experimental 
results will be reported and finally some 
conclusions will be drawn. 
2. Query Expansion and Term Relations 
It has been found that a key factor that 
determines the effect of query expansion is the 
selection of appropriate expansion terms (Peat 
and Willett, 1991). To determine expansion 
terms, one possible resource is thesauri 
constructed manually, such as Wordnet. Thesauri 
contain manually validated relations between 
terms, which can be used to suggest related 
terms. (Voorhees, 1994) carried out a series of 
experiments on selecting related terms (e.g. 
synonyms, hyonyms, etc.) from Wordnet. 
However, the experiments did not show that this 
can improve retrieval effectiveness. Some of the 
reasons are as follows: Although Wordnet 
contains many relations validated by human 
experts, the coverage is far from complete for the 
purposes of IR: not only linguistically motivated 
relations, but also association relations, are useful 
in IR. Another problem is the lack of information 
about the appropriate context to apply relations. 
For example, Wordnet contains two synsets for 
?computer?, one for the sense of ?machine? and 
another for ?human expert?. It is difficult to 
automatically select the correct synset to expand 
the word ?computer? even if we know that the 
query?s area is computer science. 
Another often used resource is associative 
relations extracted from co-occurrences: two 
terms that co-occur frequently are thought to be 
associated to each other (Jing and Croft, 1994). 
However, co-occurrence relations are noisy: 
Frequently co-occurring terms are not necessarily 
related. On the other hand, they can also miss 
true relations. The most important problem is still 
that of ambiguity: when one term is associated 
with another, it may be related for one sense and 
not for other possible senses. It is then difficult to 
determine when the relation applies. 
In most of the previous studies, relations 
extracted are restricted between one word and 
another. This limitation makes the relations 
ambiguous, and their utilization in query 
expansion often introduces undesired terms. We 
believe that the key to make a relation less 
ambiguous is to add some contextual 
information. 
In an attempt to select better expansion terms, 
(Qiu and Frei, 1993) proposed the following 
approach to select expansion terms: terms are 
selected according to their relation to the whole 
query, which is calculated as the sum of their 
relations to each of the query terms. Therefore, a 
term that is related to several query terms will be 
favored. In a similar vein, (Bai et al 2005) also 
try to determine the relationship of a word to a 
group of words by combining its relationships to 
each of the words in the group. This can indeed 
select better expansion terms. The consideration 
of other query terms produces a weak contextual 
effect. However, this effect is limited due to the 
nature of the relations extracted, in which a term 
depends on only one other term. Much of the 
noise in the sets will remain after selection.  
For a query composed of several words, what 
we would really like to have is a set of terms that 
are related to all the words taken together (and 
not separately). By combining words in the 
condition part such as ?(Java, travel)? or ?(base, 
bat)?, each word will serve as a context to the 
other in order to constrain the related terms. In 
these cases, we would expect that ?hotel?, 
?island? or ?Indonesia? would co-occur much 
more often with ?(Java, travel)? than 
?programming?, and ?ball?, ?catcher? etc. co-
occur much more often with ?(base, bat)? than 
?animal? or ?foundation?. 
One naturally would suggest that compound 
terms can be used for this purpose. However, for 
many queries, it is difficult to form a legitimate 
compound term. Even if we can detect one 
occurrence of a compound, we may miss others 
that use its variants. For example, if ?Java travel? 
is used as a query, we will likely be able to 
consider it as a compound term. The same 
compound (or its variant) would be difficult to 
552
 detect in a document talking about traveling to 
Java: the two words may appear at some distance 
or not in some specific syntactic structure as 
required in (Lin, 1997). This will lead to the 
problem of mismatching between document and 
query. 
In fact, compound terms are not the only way 
to add contextual information to a word. By 
putting two words together (without forming a 
compound term), we usually obtain a more 
precise sense for each word. For example, from 
?Java travel?, we can guess that the intended 
meaning is likely related to ?traveling to Java 
Island?. People will not interpret this 
combination in the sense of ?Java 
programming?. In the same way, people would 
not consider ?animal? to be a related term to 
?base, bat?. These examples show that in a 
combination of words, each word indeed serves 
to specify a context to interpret another word. It 
then suggests the following approach: we can 
adjunct some additional word(s) in the condition 
part of a relation, such as ?(Java, travel) ? 
Indonesia?, which means ?Indonesia? is related 
to ?(Java, travel)? together. It is expected that 
one would not obtain ?(Java, travel) ? 
programming?. 
Owing to the context effect explained above, 
we will call the relations with multiple words in 
the condition part context-dependent relations. In 
order to limit the computation complexity, we 
will only consider adding one additional word 
into relations.  
The proposed approach follows the same 
principle as (Yarowsky, 1995), which tried to 
determine the appropriate word sense according 
to one relevant context word. However, the 
requirement for query expansion is less than 
word sense disambiguation: we do not need to 
know the exact word sense to make expansion. 
We only need to determine the relevant 
expansion terms. Therefore, there is no need to 
determine manually a set of seeds before the 
learning process takes place. 
To some extent, the proposed approach is also 
related to (Sch?tze and Pedersen, 1997), which 
calculate term similarity according to the words 
appearing in the same context, or to second-order 
co-occurrences. However, a key difference is that 
(Sch?tze and Pedersen, 1997) consider only 
separate context words, while we consider 
multiple context words together. 
Once term relations are determined, they will 
be used in query expansion. The basic IR process 
will be implemented in a language modeling 
framework. This framework is chosen for its 
flexibility to integrate term relations. Indeed, the 
LM framework has proven to be capable of 
integrating term relations and query expansion 
(Bai et al, 2005; Berger and Lafferty, 1999; Zhai 
and Lafferty, 2001). However, none of the above 
studies has investigated the extraction of strong 
context-dependent relations from text collections. 
In the next section, we will describe the 
general LM framework and our query expansion 
models. Then the extraction of term relation will 
be explained. 
3. Context-Dependent Query Expansion 
in Language Models 
The basic IR approach based on LM (Ponte and 
Croft, 1998) determines the score of relevance of 
a document D by its probability to generate the 
query Q. By assuming independence between 
query terms, we have: 
??
??
?=
Qw
i
Qw
i
ii
DwPDwPDQP )|(log)|()|(  
where )|( DwP i denotes the probability of a word 
in the language model of the document D. As no 
ambiguity will arise, we will use D to mean both 
the language model of the document and the 
document itself (similarly for a query model and 
a query Q). 
Another score function is based on KL-
divergence or cross entropy between the 
document model and the query model: 
?
?
=
Vw
ii
i
DwPQwPQDscore )|(log)|(),(
 
where V is the vocabulary. Although we have 
both document and query models in the above 
formulation, usually only the document model is 
smoothed, while the query model uses Maximum 
Likelihood Estimation (MLE) )|( QwP iML . Then 
we have: 
?
?
=
Qw
iiML
i
DwPQwPQDscore )|(log)|(),(  
However, it is obvious that a distance (KL-
divergence) measured between a short query of a 
few words and a document cannot be precise. A 
better expression would contain all the related 
terms. The construction of a better query 
expression is the very motivation for query 
expansion in traditional IR systems. It is the same 
in LM for IR: to create a better query expression 
(model) to be able to measure the distance to a 
553
 document in a more precise way. The key to 
creating the new model is the integration of term 
relations. 
3.1 LM for Query Expansion 
Term relations have been used in several recent 
language models in IR. (Berger and Lafferty, 
1999) proposed a translation model that expands 
the document model. The same approach can also 
be used to expand the query model. Following 
(Berger and Lafferty, 1999), we arrive at the first 
expansion model as follows, which has also been 
used in (Bai et al, 2005): 
Model 1: Context-independent query 
expansion model (CIQE) 
??
??
==
Qq
jMLjiR
Vq
jiRiR
jj
QqPqwPQqwPQwP )|()|()|,()|(  
In this model, each original query term qj is 
expanded by related terms wi. The relations 
between them are determined by )|( jiR qwP . We 
will explain how this probability is defined in 
Section 3.2. However, we can already see here 
that wi is determined solely by one of the query 
term qj. So, we call this model ?context-
independent query expansion model? (CIQE). 
The above expanded query model enables us 
to obtain new related expansion terms, to which 
we also have to add the original query. This can 
be obtained through the following smoothing: 
?
?
?
+=
Qq
jMLjiR
iMLi
j
QqPqwP
QwPQwP
)|()|()1(                   
)|()|(
1
1
?
?
      (1) 
where 1? is a smoothing parameter. 
However, if the query model is expanded on 
all the vocabulary (V), the query evaluation will 
be very time consuming because the query and 
the document have to be compared on every word 
(dimension). In practice, we observe that only a 
small number of terms have strong relations with 
a given term, and the terms having weak relations 
usually are not truly related. So we can limit the 
expansion terms only to the strongly related ones. 
By doing this, we can also expect to filter out 
some noise and considerably reduce the retrieval 
time. 
Suppose that we have selected a set E of 
strong expansion terms. Then we have: 
?
?
??
?
?
=
QEw
ii
Vw
ii
i
i
DwPQwP
DwPQwPQDscore
)|(log)|(                    
)|(log)|(),(
 
This query expansion method uses the same 
principle as (Qiu and Frei, 1993), but in a LM 
setting: the selected expansion terms are those 
that are strongly related to all the query terms 
(this is what the summation means). The 
approach used in (Bai et al, 2005) is slightly 
different: A context vector is first built for each 
word; then a context vector for a group of words 
(e.g. a multi-word query) is composed from the 
context vectors of the words of the group; finally 
related terms to the group of words are 
determined according to the similarity of their 
context vectors to that of the group. This last step 
uses second-order co-occurrences similarly to 
(Sch?tze and Pedersen, 1997). In both (Qiu and 
Frei, 1993) and (Bai et al, 2005), the terms 
related to a group of words are determined from 
their relations to each of the words in the group, 
while the latter relations are extracted separately. 
Irrelevant expansion terms can be retained. 
As we showed earlier, in many cases, when 
one additional word is used with another word, 
the sense of each of them can usually be better 
determined. This additional word may be 
sufficient to interpret correctly many multi-word 
user queries. Therefore, our goal is to extract 
stronger context-dependent relations of the form 
(qj qk) ? wi, or to build a probability 
function )|( kjiR qqwP . Once this function is 
determined, it can be integrated into a new 
language model as follows. 
Model 2: Context-dependent query expansion 
model (CDQE) 
?
?
?
?
?
=
Qqq
kjkjiR
Vqq
kjkjiRiR
kj
kj
QqqPqqwP
QqqPqqwPQwP
,
,
)|()|(                 
)|()|()|(
 
As )|( kjiR qqwP  is a relation with two terms as 
condition, we will also call it a biterm relation. 
The name ?biterm? is due to (Srikanth and 
Srihari, 2002), which means two terms co-
occurring within some distance. Similarly, 
)|( jiR qwP  will be called unigram relation. The 
corresponding query models will be called biterm 
relation model and unigram relation model.  
As in general LM, the biterm relation model 
can be smoothed with a unigram model. Then we 
have the following score function: 
?
?
?
+=
Qqq
kjkjiR
iMLiR
kj
QqqPqqwP
QwPQwP
,
2
2
)|()|()1(                    
)|()|(
?
?
  (2) 
554
 where 2?  is another smoothing parameter. 
3.2 Extraction of Term Relations 
The key problem now is to obtain the relations 
we need: )|( jiR wwP  and )|( kjiR wwwP . For the first 
probability, as in many previous studies, we 
exploit term co-occurrences. )|( jiR wwP  could be 
built as a traditional bigram model. However, this 
is not a good approach for IR because two related 
terms do not necessarily co-occur side by side. 
They often appear at some distance. Therefore, 
this model is indeed a biterm model (Srikanth 
and Srihari, 2002), i.e., we allow two terms be 
separated within some distance. We use the 
following formula to determine this probability: 
?=
lw
jl
ji
jiR
wwc
wwc
wwP
),(
),()|(  
where ),( ji wwc  is the frequency of co-occurrence 
of the biterm ),( ji ww , i.e. two terms in the same 
window of fixed size across the collection. In our 
case, we set the window size at 10 (because this 
size turned out to be reasonable in our pilot 
experiments). 
For )|( kji wwwP , we further extend the biterm 
to triterm, and we use the frequency of co-
occurrences of three terms ),,( kji wwwc  within the 
same windows in the document collection: 
?=
lw
kjl
kji
kjiR
wwwc
wwwc
wwwP
),,(
),,()|(  
The number of relations determined in this 
way can be very large. The upper bound for 
)|( ji wwP  and )|( kji wwwP  are respectively 
O(|V|2) and O(|V|3). However, many relations 
have very low probabilities and are often noise. 
As we only consider a subset of strong expansion 
terms, the relations with low probability are 
almost never used. Therefore, we set two filtering 
criteria: 
? The biterm in the condition of a relation should 
be higher than a threshold (10 in our case); 
? The probability of a relation should be higher 
than another threshold (0.0001 in our case). 
? One more filtering criterion is mutual 
information (MI), which reflects the 
relatedness of two terms in their combination 
),( kj ww . To keep a relation )|( kji wwwP , we 
require ),( kj ww  be a meaningful combination. 
We use the following pointwise MI (Church 
and Hanks 1989): 
)()(
),(
log),(
kj
kj
kj
wPwP
wwP
wwMI =  
 We only keep meaningful combinations such 
that 0),( >kj wwMI .  
By these filtering criteria, we are able to 
reduce considerably the number of biterms and 
triterms. For example, on a collection of about 
200MB, with a vocabulary size of about 148K, 
we selected only about 2.7M useful biterms and 
about 137M triterms, which remain tractable. 
3.3 Probability of Biterms 
In LM used in IR, each query term is attributed 
the same weight. This is equivalent to a uniform 
probability distribution, i.e.: 
U
i QQqP ||
1)|( =  
where |Q|U is the number of unigrams in the 
query. In CIQE model, we use the same method.  
In CDQE, we also need to attribute a 
probability )|( QqqP kj , to the biterm ),( kj qq . 
Several options are possible. 
Uniform probability 
This simple approach distributes the probability 
uniformly among all biterms in the query, i.e.: 
B
kj QQqqP ||
1)|( =  
where BQ ||  is the number of biterms in Q.  
According to mutual information 
In a query, if two words are strongly associated, 
this also means that their association is more 
meaningful to the query, thus should be weighted 
higher. Therefore, a natural way to assign a 
probability to a biterm in the query is to use 
mutual information, which denotes the strength 
of association between two words. We use again 
the pointwise mutual information MI(qj, qk). If it 
is negative, we consider that the biterm is not 
meaningful, and is ignored. Therefore, we arrive 
at the following probability function: 
?
?
=
Qqq
ml
kj
kj
ml
qqMI
qqMIQqqP
)(
),(
),()|(  
where Qqq ml ?)(  means all the meaningful 
biterms in the query.  
555
 Statistical parsing 
In (Gao et al, 2002), a statistical parsing 
approach is used to determine the best 
combination of translation words for a query. The 
approach is similar to building a minimal 
spanning tree, which is also used in (Smeaton and 
Van Rijsbergen, 1983), to select the strongest 
term relations that cover the whole query. This 
approach can also be used in our model to 
determine the minimal set of the strongest 
biterms that cover the query.  
In our experiments, we tested all the three 
weighting schemas. It turns out that the best 
weighting is the one with MI. Therefore, in the 
next section, we will only report the results with 
the second option. 
4. Experimental Evaluation 
We evaluate query expansion with different 
relations on four TREC collections, which are 
described in Table 1. All documents have been 
processed in a standard manner: terms are 
stemmed using Porter stemmer and stopwords are 
removed. We only use titles of topics as queries, 
which contain 3.58 words per query on average.  
Table 1. TREC collection statistics 
Coll. Description Size (Mb) Vocab. # Doc. Query 
AP Associated Press (1988-89) 491 196,933 164,597 51-100 
SJM 
San Jose 
Mercury News 
(1991) 
286 146,514 90,257 101-150 
WSJ 
Wall Street 
Journal (1990-
92) 
242 121,946 74,520 51-100 
In our experiments, the document model 
remains the same while the query model changes. 
The document model uses the following Dirichlet 
smoothing: 
?
?
+
+
=
U
iMLi
i D
CwPDwtf
DwP ||
)|(),()|(
 
where ),( Dwtf i is the term frequency of wi in D, 
)|( CwP iML  is the collection model and ?  is the 
Dirichlet prior, which is set at 1000 following 
(Zhai and Lafferty, 2001).  
There are two other smoothing parameters 1? , 
and 2?  to be determined. In our experiments, we 
use a simple method to set them: the parameters 
are tuned empirically using a training collection 
containing AP1989 documents and queries 101-
150. These preliminary tests suggest that the best 
value of 1?  and 2?  (in Equations 1-2) are 
relatively stable (we will show this later). In the 
experiments reported below, we will use 4.01 =? ,  
and 3.02 =? . 
4.1 Experimental Results 
The main experimental results are described in 
Table 2, which reports average precision with 
different methods as well as the number of 
relevant documents retrieved. UM is the basic 
unigram model without query expansion (i.e. we 
use MLE for the query model, while the 
document model is smoothed with Dirichlet 
method). CIQE is the context-independent query 
expansion model using unigram relations (Model 
1). CDQE is the context-dependent query 
expansion model using biterm relations (Model 
2). In the table, we also indicate whether the 
improvement in average precision obtained is 
statistically significant (t-test). 
Table 2. Avg. precision and Recall  
Coll. 
#Rel. UM CIQE CDQE 
0.2767 0.2902 (+5%*) 0.3383  (+22%**) 
             [+17%**] AP 6101 3677 3897 4029 
0.2017 0.2225 (+10%**) 0.2448 (+21%**) 
            [+10%*] SJM 2559 1641 1761 1873 
0.2373 0.2393 (+1%) 0.2710 (+14%**) 
            [+13%*] WSJ 2172 1588 1626 1737 
* and ** indicate that the difference is statistically 
significant according to t-test: * indicates p<0.05, ** 
indicates p<0.01; (.) is compared to UM and [.] is 
compared to CIQE. 
CIQE and CDQE vs. UM 
It is interesting to observe that query expansion, 
either by CIQE or CDQE, consistently 
outperforms the basic unigram model on all the 
collections. In all the cases except CIQE for 
WSJ, the improvements in average precision are 
statistically significant. At the same time, the 
increases in the number of relevant documents 
retrieved are also consistent with those in average 
precision. 
The improvement scales obtained with CIQE 
are relatively small: from 1% to 10%. These 
correspond to the typical figure using this 
method.  
Comparing CIQE and CDQE, we can see that 
context-dependent query expansion (CDQE) 
556
 always produces better effectiveness than 
context-independent expansion (CIQE). The 
improvements range between 10% and 17%. All 
the improvements obtained by CDQE are 
statistically significant. This result strongly 
suggests that in general, the context-dependent 
term relations identify better expansion terms 
than context-independent unigram relations. This 
confirms our earlier hypothesis.  
Indeed, when we look at the expansion 
results, we see that the expansion terms 
suggested by biterm relations are usually better. 
For example, the (stemmed) expansion terms for 
the query ?insider trading? suggested 
respectively by CIQE and CDQE are as follows: 
CIQE:  stock:0.0141 market:0.0113 US:0.0112 
year:0.0102 exchang:0.0101 trade:0.0092 
report:0.0082 price:0.0076 dollar:0.0071 
1:0.0069 govern:0.0066 state:0.0065 
futur:0.0061 million:0.0061 dai:0.0060 
offici:0.0059 peopl:0.0059 york:0.0057 
issu:0.0057 ? 
CDQE:  secur:0.0161 charg:0.0158 stock:0.0137 
scandal:0.0128 boeski:0.0125 inform:0.0119 
street:0.0113 wall:0.0112 case:0.0106 
year:0.0090 million:0.0086 investig:0.0082 
exchang:0.0080 govern:0.0077 sec:0.0077 
drexel:0.0075 fraud:0.0071 law:0.0063 
ivan:0.0060 ? 
We can see that in general, the terms suggested 
by CDQE are much more relevant. In particular, 
it has been able to suggest ?boeski? (Boesky) 
who is involved in an insider trading scandal. 
Several other terms are also highly relevant, such 
as scandal, investing, sec, drexel, fraud, etc. 
The addition of these new terms does not only 
improve recall. Precision of top-ranked 
documents is also improved. This can be seen in 
Figure 1 where we compare the full precision-
recall curve for the AP collection for the three 
models. We can see that at all the recall levels, 
the precision values always follow the following 
order: CDQE > UM. The same observation is 
also made on the other collections. This shows 
that the CDQE method does not increase recall to 
the detriment of precision, but both of them. In 
contrast, CIQE increases precision at all but 0.0 
recall points: the precision at the 0.0 recall point 
is 0.6565 for CIQE and 0.6699 for UM. This 
shows that CIQE can slightly deteriorate the top-
ranked few documents. 
Figure 1. Comparison of three models on AP 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall
Pr
e
c
is
io
n CDQE
CIQE
UM
 
CDQE vs. Pseudo-relevance feedback 
Pseudo-relevance feedback is widely considered 
to be an effective query expansion method. In 
many previous experiments, it produced very 
good results. The mixture model (Zhai and 
Lafferty, 2001) is a representative and effective 
method to implement pseudo-relevance feedback: 
It uses a set of feedback documents to smooth the 
original query model. Compared to the mixture 
model, our CDQE method is also more effective: 
By manually tuning the parameters of the mixture 
model to their best, we obtained the average 
precisions of 0.3171, 0.2393 and 0.2565 
respectively for AP, SJM and WSJ collections. 
These values are lower than those obtained with 
CDQE, which has not been heavily tuned.  
For the same query ?insider trading?, the mixture 
model determines the following expansion terms: 
Mixture: stock:0.0259256 secur:0.0229553 
market:0.0157057 sec:0.013992 
inform:0.011658 firm:0.0110419 
exchang:0.0100346 law:0.00827076 
bill:0.007996 case:0.00764544 
profit:0.00672575 investor:0.00662856 
japan:0.00625859 compani:0.00609675 
commiss:0.0059618 foreign:0.00582441 
bank:0.00572947 investig:0.00572276 
We can see that some of these terms overlap with 
those suggested by biterm relations. However, 
interesting words such as boeski, drexel and 
scandal are not suggested. 
The above comparison shows that our method 
outperforms the state-of-the-art methods of query 
expansion developed so far. 
4.2 Effect of the Smoothing Parameter  
In the previous experiments, we have fixed the 
smoothing parameters. In this series of tests, we 
557
 analyze the effect of this smoothing parameter on 
retrieval effectiveness. The following figure 
shows the change of average precision (AvgP) 
using CDQE (Model 2) along with the change of 
the parameter 2? (UM is equivalent to 12 =? ).  
Figure 2. Effectiveness w.r.t. 2?  
0.15
0.2
0.25
0.3
0.35
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Lambda
A
v
g.
P AP
WSJ
SJM
 
We can see that for all the three collections, 
the effectiveness is good when the parameter is 
set in the range of 0.1-0.5. The best value for 
different collections remains stable: 0.2-0.3.  
The effect of 1?  on Model 1 is slightly 
different, but we observe the same trend. 
4.3 Number of Expansion Terms 
In the previous tests, we limit the number of 
expansion terms to 80. When different numbers 
of expansion terms are used, we obtain different 
effectiveness measures. The following figure 
shows the variation of average precision (AvgP) 
with different numbers of expansion terms, using 
CDQE method.  
Figure 3. Effectiveness w.r.t. #expansion terms 
0.15
0.20
0.25
0.30
0.35
10 20 40 80 150 300
No. expansion terms
Av
g.
P AP
WSJ
SJM
 
We can see that when more expansion terms 
are added, the effectiveness does not always 
increase. In general, a number around 80 will 
produce good results. In some cases, even if 
better effectiveness can be obtained with more 
expansion terms, the retrieval time is also longer. 
The number 80 seems to produce a good 
compromise between effectiveness and retrieval 
speed: the retrieval time remains less than 1 sec. 
per query. 
4.4 Suitability of Relations Across 
Collections 
In many real applications (e.g. Web search), we 
do not have a static document collection from 
which relations can be extracted. The question is 
whether it is possible and beneficial to extract 
relations from one text collection and use them to 
retrieve documents in another text collection. Our 
intuition is that this is possible because the 
relations (especially context-dependent relations) 
encode general knowledge, which can be applied 
to a different collection. In order to show this, we 
extracted term relations from each collection, and 
applied them on other collections. The following 
tables show the effectiveness produced using 
respectively unigram and bi-term relations. 
Table 3. Cross-utilization of relations 
 
Unigram relation Biterm relation 
   Rel. 
Coll. AP SJM WSJ AP SJM WSJ 
AP 0.2902  0.2803  0.2793 0.3383 0.3057 0.2987 
SJM 0.2271 0.2225 0.2267 0.2424 0.2448 0.2453 
WSJ 0.2541  0.2445  0.2393 0.2816 0.2636 0.2710 
 
From this table, we can observe that relations 
extracted from any collection are useful to some 
degree: they all outperform UM (see Table 2). In 
particular, the relations extracted from AP are the 
best for almost all the collections. This can be 
explained by the larger size and wider coverage 
of the AP collection. This suggests that we do not 
necessarily need to extract term relations from 
the same text collection on which retrieval is 
performed. It is possible to extract relations from 
a large text collection, and apply them to other 
collections. This opens the door to the possibility 
of constructing a general relation base for various 
document collections. 
5. Related Work 
Co-occurrence analysis is a common method to 
determine term relations. The previous studies 
have been limited to relations between two 
words, which we called unigram relations. This 
expansion approach has been integrated both in 
traditional retrieval models (Jing and Croft, 
1994) and in LM (Berger and Lafferty 1999). As 
we observed, this type of relation will introduce 
much noise into the query, leading to unstable 
effectiveness. 
Several other studies tried to filter out noise 
expansion (or translation) terms by considering 
the relations between them (Gao et al, 2002; 
558
 Jang et al 1999; Qiu and Frei, 1993; Bai et al 
2005). However, this is insufficient to detect all 
the noise. The key issue is the ambiguity of 
relations due to the lack of context information in 
the relations. In this paper, we proposed a method 
to add some context information into relations.  
 (Lin, 1997) also tries to solve word ambiguity 
by adding syntactic dependency as context. 
However, our approach does not require 
determining syntactic dependency. The principle 
of our approach is more similar to (Yarowsky, 
1995). Compared to this latter, our approach is 
less demanding: we do not need to identify 
manually the exact word senses and seed context 
words. The process is fully automatic. This 
simplification is made possible due to the 
requirement for IR: only in-context related words 
are required, but not the exact senses.  
Our work is also related to (Smadja and 
McKeown, 1996), which tries to determine the 
translation of collocations. Term combinations or 
biterms we used can be viewed as collocations. 
Again, there is much less constraint for our 
related terms than translations in (Smadja and 
McKeown, 1996). 
6. Conclusions 
In many NLP applications such as IR, we need to 
determine relations between terms. In most 
previous studies, one tries to determine the 
related terms to one single term (word). This 
makes the resulting relations ambiguous. 
Although several approaches have been proposed 
to remove afterwards some of the inappropriate 
terms, this only affects part of the noise, and 
much still remains. In this paper, we argue that 
the solution to this problem lies in the addition of 
context information in the relations between 
terms. We proposed to add another word in the 
condition of the relations so as to help constrain 
the context of application. Our experiments 
confirm that this addition of limited context 
information can indeed improve the quality of 
term relations and query expansion in IR. 
In this paper, we only compared biterm 
relations and unigram relations, the general 
method can be extended to triterm relations or 
more complex relations, provided that they can 
be extracted efficiently.  
This paper only investigated the utilization of 
context-dependent relations in IR. These relations 
can be applied in many other tasks, such as 
machine translation, word sense disambiguation / 
discrimination, and so on. These are some 
interesting research work in the future. 
References 
Bai, J., Song, D., Bruza, P., Nie, J. Y. and Cao, G. 
2005. Query expansion using term relationships in 
language models for information retrieval, ACM 
CIKM, pp. 688-695. 
Berger, A. and Lafferty, J. 1999. Information retrieval 
as statistical translation. ACM SIGIR, pp. 222-229. 
Church, K. W. and Hanks, P. 1989. Word association 
norms, mutual information, and lexicography. ACL, 
Vol. 16, pp. 22-29. 
Gao, J., Nie, J.Y., He, H, Chen, W., Zhou, M. 2002. 
Resolving query translation ambiguity using a 
decaying co-occurrence model and syntactic 
dependency relations. ACM SIGIR, pp. 11-15. 
Jang, M. G., Myaeng, S. H., and Park, S. Y. 1999. 
Using mutual information to resolve query 
translation ambiguities and query term weighting. 
ACL, pp. 223-229. 
Jing, Y. and Croft, W.B. 1994. An association 
thesaurus for information retrieval. RIAO, pp. 146-
160. 
Lin, D. 1997. Using syntactic dependency as local 
context to resolve word sense ambiguity, ACL, pp. 
64-71. 
Peat, H.J. and Willett, P. 1991. The limitations of term 
co-occurrence data for query expansion in document 
retrieval systems. JASIS, 42(5): 378-383. 
Ponte, J. and Croft, W.B. 1998. A language modeling 
approach to information retrieval. ACM SIGIR, pp. 
275-281. 
Qiu, Y. and Frei, H.P. 1993. Concept based query 
expansion. ACM SIGIR, pp.160-169. 
Sch?tze, H. and Pedersen J.O. 1997. A cooccurrence-
based thesaurus and two applications to information 
retrieval, Information Processing and Management, 
33(3): 307-318. 
Smeaton, A. F. and Van Rijsbergen, C. J. 1983. The 
retrieval effects of query expansion on a feedback 
document retrieval system. Computer Journal, 26(3): 
239-246. 
Smadja, F., McKeown, K.R., 1996. Translating 
collocations for bilingual lexicons: A statistical 
approach, Computational Linguistics, 22(1): 1-38. 
Srikanth, M. and Srihari, R. 2002. Biterm language 
models for document retrieval. ACM SIGIR, pp. 425-
426  
Voorhees, E. 1994. Query expansion using lexical-
semantic relations. ACM SIGIR, pp. 61-69. 
Yarowsky, D. 1995. Unsupervised word sense 
disambiguation rivaling supervised methods. ACL, 
pp. 189-196. 
Zhai, C. and Lafferty, J. 2001. Model-based feedback 
in the language modeling approach to information 
retrieval. ACM SIGIR, pp. 403-410.  
559
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 107?115,
Beijing, August 2010
Towards an optimal weighting of context words based on distance
Bernard Brosseau-Villeneuve*#, Jian-Yun Nie*, Noriko Kando#
* Universit? de Montr?al, Email: {brosseab, nie}@iro.umontreal.ca
# National Institute of Informatics, Email: {bbrosseau, kando}@nii.ac.jp
Abstract
Word Sense Disambiguation (WSD) of-
ten relies on a context model or vector
constructed from the words that co-occur
with the target word within the same text
windows. In most cases, a fixed-sized
window is used, which is determined by
trial and error. In addition, words within
the same window are weighted uniformly
regardless to their distance to the target
word. Intuitively, it seems more reason-
able to assign a stronger weight to con-
text words closer to the target word. How-
ever, it is difficult to manually define the
optimal weighting function based on dis-
tance. In this paper, we propose a unsu-
pervised method for determining the op-
timal weights for context words accord-
ing to their distance. The general idea is
that the optimal weights should maximize
the similarity of two context models of the
target word generated from two random
samples. This principle is applied to both
English and Japanese. The context mod-
els using the resulting weights are used
in WSD tasks on Semeval data. Our ex-
perimental results showed that substantial
improvements in WSD accuracy can be
obtained using the automatically defined
weighting schema.
1 Introduction
The meaning of a word can be defined by the
words that accompany it in the text. This is the
principle often used in previous studies on Word
Sense Disambiguation (WSD) (Ide and V?ronis,
1998; Navigli, 2009). In general, the accompa-
nying words form a context vector of the target
word, or a probability distribution of the context
words. For example, under the unigram bag-of-
words assumption, this means building p(x|t) =
count(x,t)?
x? count(x?,t)
, where count(x, t) is the count of
co-occurrences of word x with the target word t
under a certain criterion. In most studies, x and
t should co-occur within a window of up to k
words or sentences. The bounds are usually se-
lected in an ad-hoc fashion to maximize system
performance. Occurrences inside the window of-
ten weight the same without regard to their po-
sition. This is counterintuitive. Indeed, a word
closer to the target word generally has a greater
semantic constraint on the target word than a more
distant word. It is however difficult to define
the optimal weighting function manually. To get
around this, some systems add positional features
for very close words. In information retrieval, to
model the strength of word relations, some studies
have proposed non-uniform weighting methods of
context words, which decrease the importance of
more distant words in the context vector. How-
ever, the weighting functions are defined manu-
ally. It is unclear that these functions can best cap-
ture the impact of the context words on the target
word.
In this paper, we propose an unsupervised
method to automatically learn the optimal weight
of a word according to its distance to the target
word. The general principle used to determine
such weight is that, if we randomly determine
two sets of windows containing the target word
from the same corpus, the meaning ? or mixture
of meanings for polysemic words ? of the target
word in the two sets should be similar. As the con-
text model ? a probability distribution for the con-
text words ? determines the meaning of the target
word, the context models generated from the two
sets should also be similar. The weights of con-
text words at different distance are therefore de-
107
termined so as to maximize the similarity of con-
text models generated from the two sets of sam-
ples. In this paper, we propose a gradient descent
method to find the optimal weights. We will see
that the optimal weighting functions are different
from those used in previous studies. Experimenta-
tion on Semeval-2007 English and Semeval-2010
Japanese lexical sample task data shows that im-
provements can be attained using the resulting
weighting functions on simple Na?ve Bayes (NB)
systems in comparison to manually selected func-
tions. This result validates the general principle
we propose in this paper.
The remainder of this paper is organized as fol-
lows: typical uses of text windows and related
work are presented in Section 2. Our method
is presented in Section 3. In Section 4 to 6,
we show experimental results on English and
Japanese WSD. We conclude in Section 7 with
discussion and further possible extensions.
2 Uses of text windows
Modeling the distribution of words around one
target word, which we call context model, has
many uses. For instance, one can use it to define
a co-occurrence-based stemmer (Xu and Croft,
1998), which uses window co-occurrence statis-
tics to calculate the best equivalence classes for a
group of word forms. In the study of Xu and Croft,
they suggest using windows of up to 100 words.
Context models are also widely used in WSD.
For example, top performing systems on English
WSD tasks in Semeval-2007, such as NUS-ML
(Cai et al, 2007), all made use of bag-of-words
features around the target word. In this case, they
found that the best results can be achieved using a
window size of 3.
Both systems limit the size of their windows for
different purposes. The former uses a large size in
order to model the topic of the documents contain-
ing the word rather than the word?s meaning. The
latter would limit the size because bag-of-words
features further from the target word would not be
sufficiently related to its meaning (Ide and V?ro-
nis, 1998). We see that there is a compromise be-
tween taking fewer, highly related words, or tak-
ing more, lower quality words. However, there is
no principled way to determine the optimal size
of windows. The size is determined by trial and
error.
A more questionable aspect in the above sys-
tems is that for bag-of-words features, all words
in a window are given equal weights. This is
counterintuitive. One can easily understand that
a context word closer to the target word gener-
ally imposes a stronger constraint on the meaning
of the latter, than a more distant context word. It
is then reasonable to define a weighting function
that decreases along with distance. Several studies
in information retrieval (IR) have proposed such
functions to model the strength of dependency be-
tween words. For instance, Gao et al (2002)
proposed an exponential decay function to capture
the strength of dependency between words. This
function turns out to work better than the uniform
weighting in the IR experiments.
Song and Bruza (2003) used a fixed-size slid-
ing window to determine word co-occurrences.
This is equivalent to define a linear decay func-
tion for context words. The context vectors de-
fined this way are used to estimate similarity be-
tween words. A use of the resulting similarity in
query expansion in IR turned out to be successful
(Bai et al, 2005).
In a more recent study, Lv and Zhai (2009) eval-
uated several kernel functions to determine the
weights of context words according to distance,
including Gaussian kernel, cosine kernel, and so
on. As for the exponential and linear decaying
functions, all these kernel functions have fixed
shapes, which are determined manually.
Notice that the above functions have only been
tested in IR experiments. It is not clear how
these functions perform in WSD. More impor-
tantly, all the previous studies have investigated
only a limited number of weighting functions for
context words. Although some improvements us-
ing these functions have been observed in IR, it
is not clear whether the functions can best capture
the true impact of the context words on the mean-
ing of the target word. Although the proposed
functions comply with the general principle that
closer words are more important than more dis-
tant words, no principled way has been proposed
to determine the particular shape of the function
for different languages and collections.
108
In this paper, we argue that there is indeed a hid-
den weighting function that best capture the im-
pact of context words, but the function cannot be
defined manually. Rather, the best function should
be the one that emerges naturally from the data.
Therefore, we propose an unsupervised method to
discover such a function based on the following
principle: the context models for a target word
generated from two random samples should be
similar. In the next section, we will define in detail
how this principle is used.
3 Computing weights for distances
In this section, we present our method for choos-
ing how much a word occurrence should count in
the context model according to its distance to the
target word. In this study, for simplicity, we as-
sume that all word occurrences at a given distance
count equally in the context model. That is, we
ignore other features such as POS-tags, which are
used in other studies on WSD.
Let C be a corpus, W a set of text windows for
the target word w, cW,i,x the count of occurrences
of word x at distance i in W , cW,i the sum of these
counts, and ?i the weight put on one word occur-
rence at distance i. Then,
PML,W (x) =
?
i ?icW,i,x?
i ?icW,i
(1)
is the maximum likelihood estimator for x in the
context model of w. To counter the zero probabil-
ity problem, we apply Dirichlet smoothing with
the collection language model as a prior:
PDir,W (x) =
?
i ?icW,i,x + ?WP (x|C)?
i ?icW,i + ?W
(2)
The pseudo-count ?W can be a constant, or can be
found by using Newton?s method, maximizing the
log likelihood via leave-one-out estimation:
L?1(?|W, C) =?
i
?
x?V ?icW,i,x log
?icW,i,x??i+?P (x|C)?
j ?jcW,j??i+?
The general process, which we call automatic
Dirichlet smoothing, is similar to that described
in (Zhai and Lafferty, 2002).
To find the best weights for our model we pro-
pose the following process:
? Let T be the set of all windows containing
the target word. We randomly split this set
into two sets A and B.
? We want to find ?? that maximizes the sim-
ilarity of the models obtained from the two
sets, by minimizing their mutual cross en-
tropy:
l(?) = H(PML,A, PDir,B) + (3)
H(PML,B , PDir,A)
In other words, we want ?i to represent how much
an occurrence at distance i models the context
better than the collection language model, whose
counts are weighted by the Dirichlet parameter.
We hypothesize that target words occur in limited
contexts, and as we get farther from them, the pos-
sibilities become greater, resulting in sparse and
less related counts. Since two different sets of the
same word are essentially noisy samples of the
same distribution, the weights maximizing their
mutual generation probabilities should model this
phenomenon.
One may wonder why we do not use a distri-
bution similarity metric such as Kullback?Leibler
(KL) divergence or Information Radius (IRad).
The reason is that with enough word occurrences
(big windows or enough samples), the most sim-
ilar distributions are found with uniform weights,
when all word counts are used. KL divergence
is especially problematic as, since it requires
smoothing, the weights will converge to the de-
generate weights ? = 0, where only the identical
smoothing counts remain. Entropy minimization
is therefore needed in the objective function.
To determine the optimal weight of ?i, we pro-
pose a simple gradient descent minimizing (3)
over ?. The following are the necessary deriva-
tives:
?l
??i
= ?H(PML,A, PDir,B)??i
+
?H(PML,B , PDir,A)
??i
?H
(
PML,W , PDir,(T?W )
)
??i
=
109
?
?
x?V
[?PML,W (x)
??i
log PDir,(T?W )(x)+
?PDir,(T?W )(x)
??i
? PML,W (x)PDir,(T?W )(x)
]
?PML,W (x)
??i
= cW,i,x ? PML,W (x)cW,i?
j ?jcW,j
?PDir,W (x)
??i
= cW,i,x ? PDir,W (x)cW,i?
j ?jcW,j + ?W
We use stochastic gradient descent: one word is
selected randomly, it?s gradient is computed, a
small gradient step is done and the process is re-
peated. A pseudo-code of the process can be
found in Algorithm 1.
Algorithm 1 LearnWeight(C, ?, )
? ? 1k
repeat
T ?{Get windows for next word}
(A,B) ?RandomPartition(T )
for W in A,B do
PML,W ?MakeML(W ,?)
?W ?ComputePseudoCount(W ,C)
PDir,W ?MakeDir( PML,W , ?W , C)
end for
grad ? ?H(PML,A, PDir,B) +
?H(PML,B, PDir,A)
? ? ?? ? grad?grad?
until ??i < 
return ?/max{?i}
Now, as the objective function would eventu-
ally go towards putting nearly all weight on ?1,
we hypothesize that the farthest distances should
have a near-zero contribution, and determine the
stop criterion as having one weight go under a
small threshold. Alternatively, a control set of
held out words can be used to observe the progress
of the objective function or the gradient length.
When more and more weight is put on the few
closest positions, the objective function and gra-
dient depends on less counts and will become less
stable. This can be used as a stop criterion.
The above weight learning process is applied
on an English collection and a Japanese collection
with ? =  = 0.001, and ? = 1000. In the next
sections, we will describe both resulting weight-
ing functions in the context of WSD experiments.
4 Classifiers for supervised WSD tasks
Since we use the same systems for both English
and Japanese experiments, we will briefly discuss
the used classifiers in this section. In both tasks,
the objective is to maximize WSD accuracy on
held-out data, given that we have a set of training
text passages containing a sense-annotated target
word.
The first of our baselines, the Most Frequent
Sense (MFS) system always selects the most fre-
quent sense in the training set. It gives us a lower
bound on system accuracies.
Na?ve Bayes (NB) classifiers score classes us-
ing the Bayes formula under a feature indepen-
dence assumption. Let w be the target word in a
given window sample to be classified, the scoring
formula for sense class S is:
Score(w,S) = P (S)PTar(w|S)?Tar??
xi?context(w) PCon(xi|S)
?Con?dist(xi)
where dist(xi) is the distance between the context
word xi and the target word w. The target word
being an informative feature present in all sam-
ples, we use it in a target word language model
PTar . The surrounding words are summed in the
context model PCon as shown in equation (1). As
we can see with the presence of ? in the equation,
the scoring follows the same weighting scheme as
we do when accumulating counts, since the sam-
ples to classify follow the same distribution as the
training ones. Also, when a language model uses
automatic Dirichlet smoothing, the impact of the
features against the prior is controlled with the
manual parameters ?Tar or ?Con. When a man-
ual smoothing parameter is used, it also handles
impact control. Our systems use the following
weight functions:
Uniform: ?i = 11?i??, where ? is a window size
and 1 the indicator function.
Linear: ?i = max{0, 1 ? (i ? 1)?}, where ? is
the decay rate.
110
Exponential: ?i = e?(i?1)? , where ? is the ex-
ponential parameter.
Learned: ?i is the weight learned as shown pre-
viously.
The parameters for NB systems are identical for
all words of a task and were selected by exhaustive
search, maximizing leave-one-out accuracy on the
training set. For each language model, we tried
Laplace, manual Dirichlet and automatic Dirichlet
smoothing.
For the sake of comparison, also we provide a
Support Vector Machine (SVM) classifier, which
produces the best results in Semeval 2007. We
used libSVM with a linear kernel, and regular-
ization parameters were selected via grid search
maximizing leave-one-out accuracy on the train-
ing set. We tested the following windows limits:
all words in sample, current sentence, and various
fixed window sizes. We used the same features
as the NB systems, testing Boolean, raw count,
log-of-counts and counts from weight functions
representations. Although non-Boolean features
had good leave-one-out precision on the training
data, since SVM does not employ smoothing, only
Boolean features kept good results on test data, so
our SVM baseline uses Boolean features.
5 WSD experiments on Semeval-2007
English Lexical Sample
The Semeval workshop holds WSD tasks such as
the English Lexical Sample (ELS) (Pradhan et al,
2007). The task is to maximize WSD accuracy on
a selected set of polysemous words, 65 verbs and
35 nouns, for which passages were taken from the
WSJ Tree corpus. Passages contain a couple of
sentences around the target word, which is manu-
ally annotated with a sense taken from OntoNotes
(Hovy et al, 2006). The sense inventory is quite
coarse, with an average of 3.6 senses per word.
Instances count are listed in Table 1.
Train Test Total
Verb 8988 2292 11280
Noun 13293 2559 15852
Total 22281 4851
Table 1: Number of instances in the ELS data
Figure 1: Weight curve for AP88-90
Since there are only 100 target words and in-
stances are limited in the Semeval collection, we
do not have sufficient samples to estimate the op-
timal weights for context words. Therefore, we
used the AP88-90 corpus of the TREC collection
(CD 1 & 2) in our training process. The AP col-
lection contains 242,918 documents. Since our
classifiers use word stems, the collection was also
stemmed with the Porter stemmer and sets of win-
dows were built for all word stems. To get near-
uniform counts in all distances, only full win-
dows with a size of 100, which was considered
big enough without any doubt, were kept. In order
to get more samples, windows to the right and to
the left were separated. For each target word, we
used 1000 windows. A stoplist of the top 10 fre-
quent words was used, but place holders were left
in the windows to preserve the distances. Mul-
tiple consecutive stop words (ex: ?of the?) were
merged, and the target word stem, being the same
for all samples of a set, was ignored in the con-
struction of context models. The AP collection re-
sults in 32,650 target words containing 5,870,604
windows. The training process described in Sec-
tion 3 is used to determine the best weights of con-
text words. Figure 1 shows the first 40 elements
of the resulting weighting function curve.
As we can see, the curve is neither exponen-
tial, linear, or any of the forms used by Lv and
Zhai. Its form is rather similar to x??, or rather
log?1(? + x) minus some constant. The decrease
111
System Cross-Val (%) Test set (%)
MFS 78.66 77.76
Uniform NB 86.04 84.52
SVM 85.53 85.03
Linear NB 86.89 85.71
Exp. NB 87.80 86.23
Learned NB 88.46 86.70
Table 2: WSD accuracy on Semeval-2007 ELC
rate is initially very high and then reduces as it
becomes closer to zero. This long tail is not
present in any of the previously suggested func-
tions. The large difference between the above op-
timal weighting function and the functions used
in previous studies would indicate that the latter
are suboptimal. Also, as we can see, the rela-
tion between context words and the target word
is mostly gone after a few words. This would
motivate the commonly used very small windows
when using a uniform weights, since using a big-
ger window would further widen the gap between
the used weight and the optimal ones.
Now for the system settings, the context words
were processed the same way as the external cor-
pus. The target word was used without stemming
but had the case stripped. The NB systems used
the concatenation of the AP collection and the
Semeval data for the collection language model.
This is motivated by the fact that the Semeval data
is not balanced: it contains only a small number of
passages containing the target words. This makes
words related to them unusually frequent. The
class priors used an absolute discounting of 0.5 on
class counts. Uniform NB uses a window of size 4,
a Laplace smoothing of 0.65 on PTar and an au-
tomatic Dirichlet with ?Con = 0.7 on PCon. Lin-
ear NB has ? = 0.135, uses a Laplace smoothing
of 0.85 on PTar and an automatic Dirichlet with
?Con = 0.985 on PCon. Exp NB has ? = 0.27,
uses a Laplace smoothing of 2.8 on PTar and an
automatic Dirichlet with ?Con = 1.01 on PCon.
The SVM system uses a window of size 3. Our
system, Learned NB uses a Laplace smoothing of
1.075 on PTar , and an automatic Dirichlet with
?Con = 1.025 on PCon. The results on WSD are
listed in Table 2. WSD accuracy is measured by
the proportion of correctly disambiguated words
among all the word samples. The cross-validation
is performed on the training data with leave-one-
out and is shown as a hint of the capacity of the
models. A randomization test comparing Expo-
nential NB and Learned NB gives a p-value of
0.0508, which is quite good considering the exten-
sive trials used to select the exponential parameter
in comparison to a single curve computed from a
different corpus. This performance is comparable
to the current state of the art. It outperforms most
of the systems participating in the task (Pradhan et
al., 2007). Out of 14 systems, the best results had
accuracies of 89.1*, 89.1*, 88.7, 86.9 and 86.4 (*
indicates post-competition submissions). Notice
that most previous systems used SVM with ad-
ditional features such as local collocations, posi-
tional word features and POS tags. Our approach
only uses bag-of-words in a Na?ve Bayes classi-
fier. Therefore, the performance of our method is
sub-optimal. With additional features and better
classification methods, we can expect that better
performance can be obtained. In future work, we
will investigate the applications of SVM with our
new term weighting scheme, together with addi-
tional types of features.
6 WSD experiments on Semeval-2010
Japanese Lexical Sample
The Semeval-2010 Japanese WSD task (Okumura
et al, 2010) consists of 50 polysemous words
for which examples were taken from the BCCWJ
corpus (Maekawa, 2008). It was manually seg-
mented, POS-tagged, and annotated with senses
taken from the Iwanami Kokugo dictionary. The
selected words have 50 samples for both the train-
ing and test set. The task is identical to the ELS
of the previous experiment.
Since the data was again insufficient to com-
pute the optimal weighting curve, we used the
Mainichi-2005 corpus of NTCIR-8. We tried to
reproduce the same kind of segmentation as the
training data by using the Chasen parser with Uni-
Dic, which nevertheless results in different word
segments as the training data. For the corpus and
Semeval data, conjugations (setsuzoku-to, jod?-
shi, etc.), particles (all jo-shi), symbols (blanks,
kig?, etc.), and numbers were stripped. When a
112
Figure 2: Weight curve for Mainichi 2005
base-form reading was present (for verbs and ad-
jectives), the token was replaced by the Kanjis
(Chinese characters) in the word writing concate-
nated with the base-form reading. This treatment
is somewhat equivalent to the stemming+stop list
of the ELS tasks. The resulting curve can be seen
in Figure 2.
As we can see, the general form of the curve
is similar to that of the English collection, but
is steeper. This suggests that the meaning of
Japanese words can be determined using only
the closest context words. Words further than a
few positions away have very small impact on
the target word. This can be explained by the
grammatical structure of the Japanese language.
While English can be considered a Subject-Verb-
Complement language, Japanese is considered
Subject-Complement-Verb. Verbs, mostly found
at the end of a sentence, can be far apart from their
subject, and vice versa. The window distance is
therefore less useful to capture the relatedness in
Japanese than in English since Japanese has more
non-local dependencies.
The Semeval Japanese test data being part of a
balanced corpus, untagged occurrences of the tar-
get words are plenty, so we can benefit from using
the collection-level counts for smoothing. Uni-
form NB uses a window of size 1, manual Dirich-
let smoothing of 4 for PTar and 90 for the PCon.
Linear NB has ? = 0.955, uses a manual Dirichlet
smoothing of 6.25 on PTar and manual Dirichlet
System Cross-Val (%) Test set (%)
MFS 75.23 68.96
SVM 82.55 74.92
Uniform NB 82.47 76.16
Linear NB 82.63 76.48
Exp. NB 82.68 76.44
Learned NB 82.67 76.52
Table 3: WSD accuracy on Semeval-2010 JWSD
smoothing with ?Con = 65 on PCon. Exp NB
has ? = 2.675, uses a manual Dirichlet smooth-
ing of 6.5 on PTar and a manual Dirichlet of 70
on PCon. The SVM system uses a window size of
1 and Boolean features. Learned NB used a man-
ual Dirichlet smoothing of 4 for PTar and auto-
matic Dirichlet smoothing with ?Con = 0.6 for
PCon. We believe this smoothing is beneficial
only on this system because it uses more words
(the long tail), that makes the estimation of the
pseudo-count more accurate. Results on WSD are
listed in Table 3. As we can see, the difference be-
tween the NB models is less substantial than for
English. This may be due to differences in the
segmentation parameters of our external corpus:
we used the human-checked segmentation found
in the Semeval data for classification, but used a
parser to segment our external corpus for weight
learning. We are positive that the Chasen parser
with the UniDic dictionary was used to create the
initial segmentation in the Semeval data, but there
may be differences in versions and the initial seg-
mentation results were further modified manually.
Another reason for the results could be that the
systems use almost the same weights: Uniform
NB and SVM both used windows of size 1, and
the Japanese curve is steeper than the English one,
making the context model account to almost only
immediately adjacent words. So, even if our con-
text model contains more context words at larger
distances, their weights are very low. This makes
all context model quite similar. Nevertheless, we
still observe some gain in WSD accuracy. These
results show that the curves work as expected even
in different languages. However, the weighting
curve is strongly language-dependent. It could
also be collection-dependent ? we will investigate
113
this aspect in the future, using different collec-
tions.
7 Conclusions
The definition of context vector and context model
is critical in WSD. In previous studies in IR, de-
caying weight along with distance within a text
window have been proposed. However, the de-
caying functions are defined manually. Although
some of the functions produced better results than
the uniform weighting, there is no evidence show-
ing that these functions best capture the impact
of the context words on the meaning of the tar-
get word. This paper proposed an unsupervised
method for finding optimal weights for context
words according to their distance to the target
word. The general idea was to find the weights
that best fit the data, in such a way that the context
models for the same target word generated from
two random windows samples become similar. It
is the first time that this general principle is used
for this purpose. Our experiments on WSD in En-
glish and Japanese suggest the validity of the prin-
ciple.
In this paper, we limited context models to bag-
of-words features, excluding additional features
such as POS-tags. Despite this simple type of fea-
ture and the use of a simple Na?ve Bayes classifier,
the WSD accuracy we obtained can rival the other
state-of-the-art systems with more sophisticated
features and classification algorithms. This result
indicates that a crucial aspect in WSD is the def-
inition of an appropriate context model, and our
weighting method can generate more reasonable
weights of context words than using a predefined
decaying function.
Our experiments also showed that the optimal
weighting function is language-dependent. We
obtained two different functions for English and
Japanese, although their general shapes are simi-
lar. In fact, the optimal weighting function reflects
the linguistic properties: as dependent words in
Japanese can be further away from the target word
due to its linguistic structure, the optimal weight-
ing quickly decays, meaning that we can rely less
on distant context words. This also shows a lim-
itation of this study: distance is not the sole cri-
terion to determine the impact of a context word.
Other factors, such as POS-tag and syntactic de-
pendency, can play an important role in the con-
text model. These additional factors are comple-
mentary to the distance criterion and our approach
can be extended to include such additional fea-
tures. This extension is part of our future work.
Another limitation of straight window distance
is that all words introduce the same distance, re-
gardless of their nature. In our experiments, to
make the distance a more sensible metric, we
merged consecutive stop words in one placeholder
token. The idea behind this it that some words,
such as stop words, should introduce less distance
than others. On the opposite, we can easily un-
derstand that tokens such as commas, full stops,
parentheses and paragraph should introduce a big-
ger distance than regular words. We could there-
fore use a congruence score for a word, an indi-
cator showing on average how much what comes
before is similar to what comes after the word.
Also, we have combined our weighting schema
with NB classifier. Other classifiers such as SVM
could lead to better results. The utilization of our
new weighting schema with SVM is another fu-
ture work.
Finally, the weights computed with our method
has been used in WSD tasks. The weights could
be seen as the expected strength of relation be-
tween two words in a document according to their
distance. The consideration of word relationships
in documents and queries is one of the endeav-
ors in current research in IR. The new weighting
schema could be easily integrated with a depen-
dency model in IR. We plan to perform such inte-
gration in the future.
Acknowledgments
The authors would like to thank Florian Boudin
and Satoko Fujisawa for helpful comments on
this work. This work is partially supported
by Japanese MEXT Grant-in-Aid for Scientific
Research on Info-plosion (#21013046) and the
Japanese MEXT Research Student Scholarship
program.
114
References
Bai, Jing, Dawei Song, Peter Bruza, Jian-Yun Nie, and
Guihong Cao. 2005. Query expansion using term
relationships in language models for information re-
trieval. In CIKM ?05 Proceedings, pages 688?695,
New York, NY, USA. ACM.
Cai, Jun Fu, Wee Sun Lee, and Yee Whye Teh. 2007.
Nus-ml: improving word sense disambiguation us-
ing topic features. In SemEval ?07 Proceedings,
pages 249?252, Morristown, NJ, USA. Association
for Computational Linguistics.
Cheung, Percy and Pascale Fung. 2004. Translation
disambiguation in mixed language queries. Ma-
chine Translation, 18(4):251?273.
Gao, Jianfeng, Ming Zhou, Jian-Yun Nie, Hongzhao
He, and Weijun Chen. 2002. Resolving query trans-
lation ambiguity using a decaying co-occurrence
model and syntactic dependence relations. In SI-
GIR ?02 Proceedings, pages 183?190, New York,
NY, USA. ACM.
Ide, Nancy and Jean V?ronis. 1998. Introduction to
the special issue on word sense disambiguation: the
state of the art. Comput. Linguist., 24(1):2?40.
Lv, Yuanhua and ChengXiang Zhai. 2009. Positional
language models for information retrieval. In SIGIR
?09 Proceedings, pages 299?306, New York, NY,
USA. ACM.
Maekawa, Kikuo. 2008. Compilation of the bal-
anced corpus of contemporary written japanese in
the kotonoha initiative (invited paper). In ISUC
?08 Proceedings, pages 169?172, Washington, DC,
USA. IEEE Computer Society.
Navigli, Roberto. 2009. Word sense disambiguation:
A survey. ACM Comput. Surv., 41(2):1?69.
Okumura, Manabu, Kiyoaki Shirai, Kanako Komiya,
and Hikaru Yokono. 2010. Semeval-2010 task:
Japanese wsd. In SemEval ?10 Proceedings. Asso-
ciation for Computational Linguistics.
Pradhan, Sameer S., Edward Loper, Dmitriy Dligach,
and Martha Palmer. 2007. Semeval-2007 task 17:
English lexical sample, srl and all words. In Se-
mEval ?07 Proceedings, pages 87?92, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Song, D. and P. D. Bruza. 2003. Towards context sen-
sitive information inference. Journal of the Amer-
ican Society for Information Science and Technol-
ogy, 54(4):321?334.
Xu, Jinxi and W. Bruce Croft. 1998. Corpus-
based stemming using cooccurrence of word vari-
ants. ACM Trans. Inf. Syst., 16(1):61?81.
Zhai, ChengXiang and John Lafferty. 2002. Two-
stage language models for information retrieval. In
SIGIR ?02 Proceedings, pages 49?56, New York,
NY, USA. ACM.
115
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 108?115,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Positional Language Models for Clinical Information Retrieval
Florian Boudin
DIRO, Universite? de Montre?al
CP. 6128, succ. Centre-ville
H3C 3J7 Montre?al, Canada
boudinfl@iro.umontreal.ca
Jian-Yun Nie
DIRO, Universite? de Montre?al
CP. 6128, succ. Centre-ville
H3C 3J7 Montre?al, Canada
nie@iro.umontreal.ca
Martin Dawes
Department of Family Medicine
McGill University, 515 Pine Ave
H2W 1S4 Montre?al, Canada
martin.dawes@mcgill.ca
Abstract
The PECO framework is a knowledge repre-
sentation for formulating clinical questions.
Queries are decomposed into four aspects,
which are Patient-Problem (P), Exposure (E),
Comparison (C) and Outcome (O). However,
no test collection is available to evaluate such
framework in information retrieval. In this
work, we first present the construction of a
large test collection extracted from system-
atic literature reviews. We then describe an
analysis of the distribution of PECO elements
throughout the relevant documents and pro-
pose a language modeling approach that uses
these distributions as a weighting strategy. In
our experiments carried out on a collection of
1.5 million documents and 423 queries, our
method was found to lead to an improvement
of 28% in MAP and 50% in P@5, as com-
pared to the state-of-the-art method.
1 Introduction
In recent years, the volume of health and biomedi-
cal literature available in electronic form has grown
exponentially. MEDLINE, the authoritative reposi-
tory of citations from the medical and bio-medical
domain, contains more than 18 million citations.
Searching for clinically relevant information within
this large amount of data is a difficult task that med-
ical professionals are often unable to complete in a
timely manner. A better access to clinical evidence
represents a high impact application for physicians.
Evidence-Based Medicine (EBM) is a widely ac-
cepted paradigm for medical practice (Sackett et al,
1996). EBM is defined as the conscientious, explicit
and judicious use of current best evidence in making
decisions about patient care. Practice EBM means
integrating individual clinical expertise with the best
available external clinical evidence from systematic
research. It involves tracking down the best evi-
dence from randomized trials or meta-analyses with
which to answer clinical questions. Richardson et
al. (1995) identified the following four aspects as the
key elements of a well-built clinical question:
? Patient-problem: what are the patient charac-
teristics (e.g. age range, gender, etc.)? What is
the primary condition or disease?
? Exposure-intervention: what is the main in-
tervention (e.g. drug, treatment, duration, etc.)?
? Comparison: what is the exposure compared
to (e.g. placebo, another drug, etc.)?
? Outcome: what are the clinical outcomes (e.g.
healing, morbidity, side effects, etc.)?
These elements are known as the PECO elements.
Physicians are educated to formulate their clinical
questions in respect to this structure. For example, in
the following question: ?In patients of all ages with
Parkinson?s disease, does a Treadmill training com-
pared to no training allows to increase the walking
distance?? one can identify the following elements:
? P: Patients of all ages with Parkinson?s disease
? E: Treadmill training
? C: No treadmill training
? O: Walking distance
In spite of this well-defined question structure,
physicians still use keyword-based queries when
they search for clinical evidence. An explanation of
108
that is the almost total absence of PECO search in-
terfaces. PubMed1, the most used search interface,
does not allow users to formulate PECO queries
yet. For the previously mentioned clinical question,
a physician would use the query ?Treadmill AND
Parkinson?s disease?. There is intuitively much to
gain by using a PECO structured query in the re-
trieval process. This structure specifies the role of
each concept in the desired documents, which is
a clear advantage over a keyword-based approach.
One can for example differentiate two queries in
which a disease would be a patient condition or a
clinical outcome. This conceptual decomposition of
queries is also particularly useful in a sense that it
can be used to balance the importance of each ele-
ment in the search process.
Another important factor that prevented re-
searchers from testing approaches to clinical infor-
mation retrieval (IR) based on PECO elements is
the lack of a test collection, which contains a set of
documents, a set of queries and the relevance judg-
ments. The construction of such a test collection is
costly in manpower. In this paper, we take advan-
tage of the systematic reviews about clinical ques-
tions from Cochrane. Each Cochrane review ex-
amines in depth a clinical question and survey all
the available relevant publications. The reviews are
written for medical professionals. We transformed
them into a TREC-like test collection, which con-
tains 423 queries and 8926 relevant documents ex-
tracted from MEDLINE. In a second part of this pa-
per, we present a model integrating the PECO frame-
work in a language modeling approach to IR. An in-
tuitive method would try to annotate the concepts
in documents into PECO categories. One can then
match the PECO elements in the query to the ele-
ments detected in documents. However, as previous
studies have shown, it is very difficult to automat-
ically annotate accurately PECO elements in docu-
ments. To by-pass this issue, we propose an alter-
native that relies on the observed positional distri-
bution of these elements in documents. We will see
that different types of element have different distri-
butions. By weighting words according to their posi-
tions, we can indirectly weigh the importance of dif-
ferent types of element in search. As we will show
1www.pubmed.gov
in this paper, this approach turns out to be highly
effective.
This paper is organized as follows. We first briefly
review the previous work, followed by a description
of the test collection we have constructed. Next,
we give the details of the method we propose and
present our experiments and results. Lastly, we con-
clude with a discussion and directions for further
work.
2 Related work
The need to answer clinical questions related to a
patient care using IR systems has been well stud-
ied and documented (Hersh et al, 2000; Niu et al,
2003; Pluye et al, 2005). There are a limited but
growing number of studies trying to use the PECO
elements in the retrieval process. (Demner-Fushman
and Lin, 2007) is one of the few such studies, in
which a series of knowledge extractors is used to
detect PECO elements in documents. These ele-
ments are later used to re-rank a list of retrieved ci-
tations from PubMed. Results reported indicate that
their method can bring relevant citations into higher-
ranking positions, and from these abstracts gener-
ate responses that answer clinicians? questions. This
study demonstrates the value of the PECO frame-
work as a method for structuring clinical questions.
However, as the focus has been put on the post-
retrieval step (for question-answering), it is not clear
whether PECO elements are useful at the retrieval
step. Intuitively, the integration of PECO elements
in the retrieval process can also lead to higher re-
trieval effectiveness.
The most obvious scenario for testing this would
be to recognize PECO elements in documents prior
to indexing. When a PECO-structured query is for-
mulated, it is matched against the PECO elements
in the documents (Dawes et al, 2007). Neverthe-
less, the task of automatically identifying PECO el-
ements is a very difficult one. There are two major
reasons for that. First, previous studies have indi-
cated that there is a low to moderate agreement rate
among humans for annotating PECO elements. This
is due to the lack of standard definition for the el-
ement? boundaries (e.g. can be words, phrases or
sentences) but also to the existence of several lev-
els of annotation. Indeed, there are a high number
109
of possible candidates for each element and one has
to choose if it is a main element (i.e. playing a ma-
jor role in the clinical study) or secondary elements.
Second is the lack of sufficient annotated data that
can be used to train automatic tagging tools.
Despite all these difficulties, several efficient
detection methods have been proposed (Demner-
Fushman and Lin, 2007; Chung, 2009). Nearly all
of them are however restricted to a coarse-grain an-
notation level (i.e. tagging entire sentences as de-
scribing one element). This kind of coarser-grain
identification is more robust and more feasible than
the one at concept level, and it could be sufficient in
the context of IR. In fact, for IR purposes, what is
the most important is to correctly weight the words
in documents and queries. From this perspective,
an annotation at the sentence level may be suffi-
cient. Notwithstanding, experiments conducted us-
ing a collection of documents that were annotated at
a sentence-level only showed a small increase in re-
trieval accuracy (Boudin et al, 2010b) compared to
a traditional bag-of-words approach.
More recently, Boudin et al (2010a) proposed an
alternative to the PECO detection issue that relies
on assigning different weights to words according to
their positions in the document. A location-based
weighting strategy is used to emphasize the most
informative parts of documents. They show that
a large improvement in retrieval effectiveness can
be obtained this way and indicate that the weights
learned automatically are correlated to the observed
distribution of PECO elements in documents. In this
work, we propose to go one step further in this direc-
tion by analyzing the distribution of PECO elements
in a large number of documents and define the posi-
tional probabilities of PECO elements accordingly.
These probabilities will be integrated in the docu-
ment language model.
3 Construction of the test collection
Despite the increasing use of search engines by med-
ical professionals, there is no standard test collection
for evaluating clinical IR. Constructing such a re-
source from scratch would require considerable time
and money. One way to overcome this obstacle is
to use already available systematic reviews. Sys-
tematic reviews try to identify, appraise, select and
synthesize all high quality research evidence rele-
vant to a clinical question. The best-known source
of systematic reviews in the healthcare domain is the
Cochrane collaboration2. It consists of a group of
over 15,000 specialists who systematically identify
and review randomized trials of the effects of treat-
ments. In particular, a review contains a reference
section, listing all the relevant studies to the clinical
question. These references can be considered as rel-
evant documents. In our work, we propose to use
these reviews as a way to semi-automatically build a
test collection. As the reviews are made by special-
ists in the area independently from our study, we can
avoid bias in our test collection.
We gathered a subset of Cochrane systematic re-
views and asked a group of annotators, one professor
and four Master students in family medicine, to cre-
ate PECO-structured queries corresponding to the
clinical questions. As clinical questions answered
in these reviews cover various aspects of one topic,
multiple variants of precise PECO queries were gen-
erated for each review. Moreover, in order to be able
to compare a PECO-based search strategy to a real
world scenario, this group have also provided the
keyword-based queries that they would have used
to search with PubMed. Below is an example of
queries generated from the systematic review about
?Aspirin with or without an antiemetic for acute mi-
graine headaches in adults?:
Keyword-based query
[aspirin and migraine]
PECO-structured queries
1. [adults 18 years or more with migraine]P
[aspirin alone]E
[placebo]C
[pain free]O
2. [adults 18 years or more with migraine]P
[aspirin plus an antiemetic]E
[placebo]C
[pain free]O
3. [adults 18 years or more with migraine]P
[aspirin plus metoclopramide]E
[active comparator]C
[use of rescue medication]O
2www.cochrane.org
110
All the citations included in the ?References? sec-
tion of the systematic review were extracted and
selected as relevant documents. These citations
were manually mapped to PubMed unique identi-
fiers (PMID). This is a long process that was under-
taken by two different workers to minimize the num-
ber of errors. At this step, only articles published in
journals referenced in PubMed are considered (e.g.
conference proceedings are not included).
0 20 40 60 80 100 120
Number of references in each review
0
5
10
15
20
25
Nu
mb
er
 of
 sy
ste
ma
tic
 re
vie
ws
Figure 1: Histogram of the number of queries versus the
number of relevant documents.
We selected in sequential order from the set
of new systematic reviews3 and processed 156
Cochrane reviews. There was no restriction about
the topics covered or the number of included refer-
ences. The resulting test collection is composed of
423 queries and 8926 relevant citations (2596 differ-
ent citations). This number reduces to 8138 citations
once we remove the citations without any text in the
abstract (i.e. certain citations, especially old ones,
only contain a title). Figure 1 shows the statistics
derived from the number of relevant documents by
query. In this test collection, the average number of
documents per query is approximately 19 while the
average length of a document is 246 words.
4 Distribution of PECO elements
The observation that PECO elements are not evenly
distributed throughout the documents is not new. In
fact, most existing tagging methods used location-
based features. This information turns out to be very
useful because of the standard structure of medical
citations. Actually, many scientific journals explic-
itly recommend authors to write their abstracts in
3http://mrw.interscience.wiley.com/
cochrane/cochrane clsysrev new fs.html
compliance to the ordered rhetorical structure: In-
troduction, Methods, Results and Discussion. These
rhetorical categories are highly correlated to the dis-
tributions of PECO elements, as some elements are
more likely to occur in certain categories (e.g. clin-
ical outcomes are more likely to appear in the con-
clusion). The position is thus a strong indicator of
whether a text segment contains a PECO element or
not.
To the best of our knowledge, the first analysis
of the distribution of PECO elements in documents
was described in(Boudin et al, 2010a). A small col-
lection of manually annotated abstracts was used to
compute the probability that a PECO element oc-
curs in a specific part of the documents. This study
is however limited by the small number of anno-
tated documents (approximately 50 citations) and
the moderate agreement rate among human annota-
tors. Here we propose to use our test collection to
compute more reliable statistics.
The idea is to use the pairs of PECO-structured
query and relevant document, assuming that if a doc-
ument is relevant then it should contain the same
elements as the query. Of course, this is obvi-
ously not always the case. Errors can be introduced
by synonyms or homonyms and relevant documents
may not contain all of the elements described in the
query. But, with more than 8100 documents, it is
quite safe to say that this method produce fairly reli-
able results. Moreover, a filtering process is applied
to queries removing all non-informative words (e.g.
stopwords, numbers, etc.) from being counted.
There are several ways to look at the distribution
of PECO elements in documents. One can use the
rhetorical structure of abstracts to do that. However,
the high granularity level of such analysis would
make it less precise for IR purposes. Furthermore,
most of the citations available in PubMed are de-
void of explicitly marked sections. It is possible to
automatically detect these sections but only with a
non-negligible error rate (McKnight and Srinivasan,
2003). In our study, we chose to use a fixed num-
ber of partitions by dividing documents into parts of
equal length. This choice is motivated by its repeata-
bility and ease to implement, but also for compari-
son with previous studies.
We divided each relevant document into 10 parts
of equal length on a word level (from P1 to P10). We
111
computed statistics on the number of query words
that occur in each of these parts. For each PECO el-
ement, the distribution of query words among the
parts of the documents is not uniform (Figure 2).
We observe distinctive distributions, especially for
Patient-Problem and Exposure elements, indicating
that first and last parts of the documents have higher
chance to contain these elements. This gives us a
clear and robust indication on which specific parts
should be enhanced when searching for a given el-
ement. Our proposed model will exploit the typical
distributions of PECO elements in documents.
P1 P2 P3 P4 P5 P6 P7 P8 P9 P100.00
0.05
0.10
0.15
0.20
0.25 P elements
P1 P2 P3 P4 P5 P6 P7 P8 P9 P10
E elements
P1 P2 P3 P4 P5 P6 P7 P8 P9 P10                                           Parts of the documents0.00
0.05
0.10
0.15
0.20
0.25
      
      
      
      
      
      
 Pro
por
tion
 of P
ECO
 ele
men
ts in
 par
t
C elements
P1 P2 P3 P4 P5 P6 P7 P8 P9 P10
O elements
Figure 2: Distribution of each PECO element throughout
the different parts of the documents.
5 Retrieval Method
In this work, we use the language modeling ap-
proach to information retrieval. This approach as-
sumes that queries and documents are generated
from some probability distribution of text (Ponte and
Croft, 1998). Under this assumption, ranking a doc-
ument D as relevant to a query Q is seen as estimat-
ing P(Q|D), the probability thatQwas generated by
the same distribution as D. A typical way to score
a document D as relevant to a query Q is to com-
pute the Kullback-Leibler divergence between their
respective language models:
score(Q,D) =
?
w?Q
P(w|Q) ? logP(w|D) (1)
Under the traditional bag-of-words assumption,
i.e. assuming that there is no need to model term de-
pendence, a simple estimate for P(w|Q) can be ob-
tained by computing Maximum Likelihood Estima-
tion (MLE). It is calculated as the number of times
the word w appears in the query Q, divided by its
length:
P(w|Q) =
count(w,Q)
|Q|
A similar method is employed for estimating
P(w|D). Bayesian smoothing using Dirichlet pri-
ors is however applied to the maximum likelihood
estimator to compensate for data sparseness (i.e.
smoothing probabilities to remove zero estimates).
Given ? the prior parameter and C the collection of
documents, P(w|D) is computed as:
P(w|D) =
count(w,D) + ? ? P(w|C)
|D| + ?
5.1 Model definition
In our model, we propose to use the distribution of
PECO elements observed in documents to empha-
size the most informative parts of the documents.
The idea is to get rid of the problem of precisely
detecting PECO elements by using a positional lan-
guage model. To integrate position, we estimate
a series of probabilities that constraints the word
counts to a specific part of the documents instead of
the entire document. Each document D is ranked by
a weighted linear interpolation. Given a document
D divided in 10 parts p ? [P1, P2 ? ? ?P10], P(w|D)
in equation 1 is redefined as:
P ?(w|D) = ? ? P(w|D) + ? ? Ptitle(w|D)
+? ?
?
pi?D
?e ? Ppi(w|D) (2)
where the ?e weights for each type of element e
are empirically fixed to the values of the distribution
of PECO elements observed in documents. We then
redefine the scoring function to integrate the PECO
query formulation. The idea is to use the PECO
structure as a way to balance the importance of each
element in the retrieval step. The final scoring func-
tion is defined as:
scorefinal(Q,D) =
?
e?PECO
?e ? score(Qe, D)
112
In our model, there are a total of 7 weighting pa-
rameters, 4 corresponding to the PECO elements in
queries (?P, ?E, ?C and ?O) and 3 for the document
language models (?, ? and ?). These parameters
will be determined by cross-validation.
6 Results
In this section, we first describe the details of our
experimental protocol. Then, we present the results
obtained by our model on the constructed test col-
lection.
6.1 Experimental settings
As a collection of documents, we gathered 1.5 mil-
lions of citations from PubMed. We used the fol-
lowing constraints: citations with an abstract, hu-
man subjects, and belonging to one of the follow-
ing publication types: randomized control trials, re-
views, clinical trials, letters, editorials and meta-
analyses. The set of queries and relevance judg-
ments described in Section 3 is used to evaluate
our model. Relevant documents were, if not al-
ready included, added to the collection. Because
each query is generated from a systematic literature
review completed at a time t, we placed an addi-
tional restriction on the publication date of the re-
trieved documents: only documents published be-
fore time t are considered. Before indexing, each
citation is pre-processed to extract its title and ab-
stract text and then converted into a TREC-like doc-
ument format. Abstracts are divided into 10 parts of
equal length (the ones containing less than 10 words
are discarded). The following fields are marked in
each document: title, P1, P2 ? ? ? P10. The following
evaluation measures are used:
? Precision at rank n (P@n): precision computed
on the n topmost retrieved documents.
? Mean Average Precision (MAP): average of
precision measures computed at the point of
each relevant document in the ranked list.
? Number of relevant documents retrieved
All retrieval tasks are performed using an ?out-
of-the-shelf? version of the Lemur toolkit4. We use
the embedded tokenization algorithm along with the
4www.lemurproject.org
standard Porter stemmer. The number of retrieved
documents is set to 1000 and the Dirichlet prior
smoothing parameter to ? = 2000. In all our exper-
iments, we use the KL divergence scoring function
(equation 1) as baseline. Statistical significance is
computed using the well-known Student?s t-test. To
determine reasonable weights and avoid overtuning
the parameters, we use a 10-fold cross-validation op-
timizing the MAP values.
6.2 Experiments
We first investigated the impact of using PECO-
structured queries on the retrieval performance. As
far as we know, no quantitative evaluation of the
increase or decrease of performance in comparison
with a keyword-based search strategy has been re-
ported. Schardt et al (2007) presented a compari-
son between PubMed and a PECO search interface
but failed to demonstrate any significant difference
between the two search protocols. The larger num-
ber of words in PECO-structured queries, on aver-
age 18.8 words per query compared to 4.3 words for
keyword queries, should capture more aspects of the
information need. But, it may also be a disadvan-
tage due to the fact that more noise can be brought
in, causing query-drift issues.
We propose two baselines using the keyword-
based queries. The first baseline (named Baseline-
1) uses keyword queries with the traditional lan-
guage modeling approach. This is one of the state-
of-the-art approaches in current IR research. This
retrieval model considers each word in a query as
an equal, independent source of information. In the
second baseline (named Baseline-2), we consider
multiword phrases. In our test collection, queries
are often composed of multiword phrases such as
?low back pain? or ?early pregnancy?. It is clear
that finding the exact phrase ?heart failure? is a
much stronger indicator of relevance than just find-
ing ?heart? and ?failure? scattered within a docu-
ment. The Indri operator #1 is used to perform
phrase-based retrieval. Phrases are already indicated
in queries by the conjunction and (e.g. vaccine and
hepatitis B). A simple regular expression is used to
recognize the phrases.
Results are presented in Table 1. As expected,
phrase-based retrieval leads to some increase in re-
trieval precision (P@5). However, the number of
113
relevant documents retrieved is decreased. This is
due to the fact that we use exact phrase matching
that can reduce query coverage. One solution would
be to use unordered window features (Indri operator
#uwn) that would require words to be close together
but not necessarily in an exact sequence order (Met-
zler and Croft, 2005).
The PECO queries use PECO-structured queries
as a bag of words. We observe that PECO queries
do not enhance the average precision but increase
the P@5 significantly. The number of relevant doc-
uments retrieved is also larger. These results indi-
cate that formulating clinical queries according to
the PECO framework enhance the retrieval effec-
tiveness.
Model MAP P@5 #rel. ret.
Baseline-1 0.129 0.151 5369
Baseline-2 0.128 0.161? 4645
PECO-queries 0.126 0.172? 5433
Table 1: Comparing the performance measures of
keyword-based and PECO-structured queries in terms of
MAP, precision at 5 and number of relevant documents
retrieved (#rel. ret.). (?: t.test < 0.05)
In a second series of experiments, we evaluated
the model we proposed in Section 5 . We compared
two variants of our model. The first variant (named
Model-1) uses a global ?e distribution fixed accord-
ing to the average distribution of all PECO elements
(i.e. the observed probability that a PECO element
occurs in a document? part, no matter which element
it is). The second variant (named Model-2) uses a
differentiated ?e distribution for each type of PECO
element. The idea is to see if, given the fact that
PECO elements have different distributions in docu-
ments, using an adapted weight distribution for each
element can improve the retrieval effectiveness.
Previous studies have shown that assigning a dif-
ferent weight to each PECO element in the query
leads to better results (Demner-Fushman and Lin,
2007; Boudin et al, 2010a). In order to compare
our model with a similar method, we defined another
baseline (named Baseline-3) by fixing the parame-
ters ? = 0 and ? = 0 in equation 2. We performed
a grid search (from 0 to 1 by step of 0.1) to find
the optimal ? weights. Regarding the last three pa-
rameters in our full models, namely ?, ? and ?, we
conducted a second grid search to find their optimal
values. Performance measures obtained in 10-fold
cross-validation (optimizing the MAP measure) by
these models are presented in Table 2.
A significant improvement is obtained by
the Baseline-3 over the keyword-based approach
(Baseline-2). The PECO decomposition of queries
is particularly useful to balance the importance of
each element in the scoring function. We observe a
large improvement in retrieval effectiveness for both
models over the two baselines. This strongly indi-
cates that a weighting scheme based on the word po-
sition in documents is effective. These results sup-
port our assumption that the distribution of PECO
elements in documents can be used to weight words
in the document language model.
However, we do not observe meaningful differ-
ences between Model-1 and Model-2. This tend to
suggest that a global distribution is likely more ro-
bust for IR purposes than separate distributions for
each type of element. Another possible reason is that
our direct mapping from positional distribution to
probabilities may not be the most appropriate. One
may think about using a different transformation, or
performing some smoothing. We will leave this for
our future work.
7 Conclusion
This paper first presented the construction of a test
collection for evaluating clinical information re-
trieval. From a set of systematic reviews, a group
of annotators were asked to generate structured clin-
ical queries and collect relevance judgments. The
resulting test collection is composed of 423 queries
and 8926 relevant documents. This test collection
provides a basis for researchers to experiment with
PECO-structured queries in clinical IR. The test col-
lection introduced in this paper, along with the man-
ual given to the group of annotators, will be available
for download5.
In a second step, this paper addressed the prob-
lem of using the PECO framework in clinical IR. A
straightforward idea is to identify PECO elements in
documents and use the elements in the retrieval pro-
cess. However, this approach does not work well be-
5http://www-etud.iro.umontreal.ca/?boudinfl/pecodr/
114
Model MAP % rel. P@5 % rel. #rel. ret.
Baseline-2 0.128 - 0.161 - 4645
Baseline-3 0.144 +12.5%? 0.196 +21.7%? 5780
Model-1 0.164 +28.1%? 0.241 +49.7%? 5768
Model-2 0.163 +27.3%? 0.240 +49.1%? 5770
Table 2: 10-fold cross validation scores for the Baseline-2, Baseline-3 and the two variants of our proposed model
(Model-1 and Model-2). Relative increase over the Baseline-2 is given, #rel. ret. is the number of relevant documents
retrieved. (?: t.test < 0.01, ?: t.test < 0.05)
cause of the difficulty to automatically detect these
elements. Instead, we proposed a less demanding
approach that uses the distribution of PECO ele-
ments in documents to re-weight terms in the doc-
ument model. The observation of variable distribu-
tions in our test collection led us to believe that the
position information can be used as a robust indica-
tor of the presence of a PECO element. This strategy
turns out to be promising. On a data set composed
of 1.5 million citations extracted with PubMed, our
best model obtains an increase of 28% for MAP
and nearly 50% for P@5 over the classical language
modeling approach.
In future work, we intend to expand our analy-
sis of the distribution of PECO elements to a larger
number of citations. One way to do that would
be to automatically extract PubMed citations that
contain structural markers associated to PECO cate-
gories (Chung, 2009).
References
Florian Boudin, Jian-Yun Nie, and Martin Dawes. 2010a.
Clinical Information Retrieval using Document and
PICO Structure. In Proceedings of the HLT-NAACL
2010 conference, pages 822?830.
Florian Boudin, Lixin Shi, and Jian-Yun Nie. 2010b. Im-
proving Medical Information Retrieval with PICO El-
ement Detection. In Proceedings of the ECIR 2010
conference, pages 50?61.
Grace Y. Chung. 2009. Sentence retrieval for abstracts
of randomized controlled trials. BMC Medical Infor-
matics and Decision Making, 9(1).
Thomas Owens Sheri Keitz Connie Schardt, Martha
B Adams and Paul Fontelo. 2007. Utilization of the
PICO framework to improve searching PubMed for
clinical questions. BMC Medical Informatics and De-
cision Making, 7(1).
Martin Dawes, Pierre Pluye, Laura Shea, Roland Grad,
Arlene Greenberg, and Jian-Yun Nie. 2007. The iden-
tification of clinically important elements within med-
ical journal abstracts: PatientPopulationProblem, Ex-
posureIntervention, Comparison, Outcome, Duration
and Results (PECODR). Informatics in Primary care,
15(1):9?16.
D. Demner-Fushman and J. Lin. 2007. Answering
clinical questions with knowledge-based and statistical
techniques. Computational Linguistics, 33(1):63?103.
William R. Hersh, Katherine Crabtree, David H. Hickam,
Lynetta Sacherek, Linda Rose, and Charles P. Fried-
man. 2000. Factors associated with successful an-
swering of clinical questions using an information re-
trieval system. Bulletin of the Medical Library Asso-
ciation, 88(4):323?331.
Larry McKnight and Padmini Srinivasan. 2003. Catego-
rization of sentence types in medical abstracts. Pro-
ceedings of the AMIA annual symposium.
Donald Metzler and W. Bruce Croft. 2005. A Markov
random field model for term dependencies. In Pro-
ceedings of the SIGIR conference, pages 472?479.
Yun Niu, Graeme Hirst, Gregory McArthur, and Patricia
Rodriguez-Gianolli. 2003. Answering clinical ques-
tions with role identification. In Proceedings of the
ACL 2003 Workshop on Natural Language Processing
in Biomedicine, pages 73?80.
Pierre Pluye, Roland M. Grad, Lynn G. Dunikowski,
and Randolph Stephenson. 2005. Impact of clinical
information-retrieval technology on physicians: a lit-
erature review of quantitative, qualitative and mixed
methods studies. International Journal of Medical In-
formatics, 74(9):745?768.
Jay M. Ponte and W. Bruce Croft. 1998. A language
modeling approach to information retrieval. In Pro-
ceedings of the SIGIR conference, pages 275?281.
Scott W. Richardson, Mark C. Wilson, Jim Nishikawa,
and Robert S. Hayward. 1995. The well-built clini-
cal question: a key to evidence-based decisions. ACP
Journal Club, 123(3):A12?13.
David L. Sackett, William Rosenberg, J. A. Muir Gray,
Brian Haynes, and W. Scott Richardson. 1996. Ev-
idence based medicine: what it is and what it isn?t.
British medical journal, 312:71?72.
115
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1342?1351,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Summarize What You Are Interested In:
An Optimization Framework for Interactive Personalized Summarization
Rui Yan
Department of Computer
Science and Technology,
Peking University,
Beijing 100871, China
r.yan@pku.edu.cn
Jian-Yun Nie
De?partement d?informatique
et de recherche ope?rationnelle,
Universite? de Montre?al,
Montre?al, H3C 3J7 Que?bec, Canada
nie@iro.umontreal.ca
Xiaoming Li
Department of Computer
Science and Technology,
Peking University,
Beijing 100871, China
lxm@pku.edu.cn
Abstract
Most traditional summarization methods treat
their outputs as static and plain texts, which
fail to capture user interests during summa-
rization because the generated summaries are
the same for different users. However, users
have individual preferences on a particular
source document collection and obviously a
universal summary for all users might not al-
ways be satisfactory. Hence we investigate
an important and challenging problem in sum-
mary generation, i.e., Interactive Personalized
Summarization (IPS), which generates sum-
maries in an interactive and personalized man-
ner. Given the source documents, IPS captures
user interests by enabling interactive clicks
and incorporates personalization by model-
ing captured reader preference. We develop
experimental systems to compare 5 rival al-
gorithms on 4 instinctively different datasets
which amount to 5197 documents. Evalua-
tion results in ROUGE metrics indicate the
comparable performance between IPS and the
best competing system but IPS produces sum-
maries with much more user satisfaction ac-
cording to evaluator ratings. Besides, low
ROUGE consistency among these user pre-
ferred summaries indicates the existence of
personalization.
1 Introduction
In the era of information explosion, people need new
information to update their knowledge whilst infor-
mation on Web is updating extremely fast. Multi-
document summarization has been proposed to ad-
dress such dilemma by producing a summary de-
livering the majority of information content from a
document set, and hence is a necessity.
Traditional summarization methods play an im-
portant role with the exponential document growth
on the Web. However, for the readers, the impact of
human interests has seldom been considered. Tra-
ditional summarization utilizes the same methodol-
ogy to generate the same summary no matter who is
reading. However, users may have bias on what they
prefer to read due to their potential interests: they
need personalization. Therefore, traditional summa-
rization methods are to some extent insufficient.
Topic biased summarization tries for personaliza-
tion by pre-defining human interests as several gen-
eral categories, such as health or science. Readers
are required to select their possible interests before
summary generation so that the chosen topic has
priority during summarization. Unfortunately, such
topic biased summarization is not sufficient for two
reasons: (1) interests cannot usually be accurately
pre-defined by ambiguous topic categories and (2)
user interests cannot always be foreknown. Often
users do not really know what general ideas or detail
information they are interested in until they read the
summaries. Therefore, more flexible interactions
are required to establish personalization.
Due to all the insufficiencies of existed sum-
marization approaches, we introduce a new multi-
document summarization task of Interactive Person-
alized Summarization (IPS) and a novel solution for
the task. Taking a document collection as input, the
system outputs a summary aligned both with source
corpus and with user personalization, which is cap-
tured by flexible human?system interactions. We
1342
build an experimental system on 4 real datasets to
verify the effectiveness of our methods compared
with 4 rivals. The contribution of IPS is manifold
by addressing following challenges:
? The 1st challenge for IPS is to integrate user
interests into traditional summary components. We
measure the utilities of these components and com-
bine them. We formulate the task into a balanced
optimization framework via iterative substitution to
generate summaries with maximum overall utilities.
? The 2nd challenge is to capture user inter-
ests through interaction. We develop an interactive
mechanism of ?click? and ?examine? between read-
ers and summaries and address sparse data by ?click
smoothing? under the scenario of few user clicks.
We start by reviewing previous works. In Section
3 we provide IPS overview, describe user interac-
tion and optimize component combination with per-
sonalization. We conduct empirical evaluation and
demonstrate the experimental system in Section 4.
Finally we draw conclusions in Section 5.
2 Related Work
Multi-Document Summarization (MDS) has drawn
much attention in recent years and gained emphasis
in conferences such as ACL, EMNLP and SIGIR,
etc. General MDS can either be extractive or ab-
stractive. The former assigns salient scores to se-
mantic units (e.g. sentences, paragraphs) of the doc-
uments indicating their importance and then extracts
top ranked ones, while the latter demands informa-
tion fusion(e.g. sentence compression and reformu-
lation). Here we focus on extractive summarization.
Centroid-based method is one of the most popular
extractive summarization method. MEAD (Radev
et al, 2004) and NeATS (Lin and Hovy, 2002) are
such implementations, using position and term fre-
quency, etc. MMR (Goldstein et al, 1999) algorithm
is used to remove redundancy. Most recently, the
graph-based ranking methods have been proposed to
rank sentences or passages based on the ?votes? or
?recommendations? between each other. The graph-
based methods first construct a graph representing
the sentence relationships at different granularities
and then evaluate the saliency score of the sentences
based on the graph. TextRank (Mihalcea and Tarau,
2005) and LexPageRank (Erkan and Radev, 2004)
use algorithms similar to PageRank and HITS to
compute sentence importance. Wan et al improve
the graph-ranking algorithm by differentiating intra-
document and inter-document links between sen-
tences (2007b) and incorporate cluster information
in the graph model to evaluate sentences (2008).
To date, topics (or themes, clusters) in documents
have been discovered and used for sentence selec-
tion for topic biased summarization (Wan and Yang,
2008; Gong and Liu, 2001). Wan et al have
proposed a manifold-ranking method to make uni-
form use of sentence-to-sentence and sentence-to-
topic relationships to generate topic biased sum-
maries (2007a). Leuski et al in (2003) pre-define
several topic concepts, assuming users will foresee
their interested topics and then generate the topic
biased summary. However, such assumption is not
quite reasonable because user interests may not be
forecasted, or pre-defined accurately as we have ex-
plained in last section.
The above algorithms are usually traditional ex-
tensions of generic summarizers. They do not in-
volve interactive mechanisms to capture reader in-
terests, nor do they utilize user preference for per-
sonalization in summarization. Wan et al in (2008)
have proposed a summarization biased to neighbor-
ing reading context through anchor texts. How-
ever, such scenario does not apply to contexts with-
out human-edited anchor texts like Wikipedia they
have used. Our approach can naturally and simulta-
neously take into account traditional summary ele-
ments and user interests and combine both in opti-
mization under a wider practical scenario.
3 Interactive Personalized Summarization
Personalization based on user preference can be
captured via various alternative ways, such as eye-
tracking or mouse-tracking instruments used in (Guo
and Agichtein, 2010). In this study, we utilize inter-
active user clicks/examinations for personalization.
Unlike traditional summarization, IPS supports
human?system interaction by clicking into the sum-
mary sentences and examining source contexts. The
implicit feedback of user clicks indicates what they
are interested in and the system collects preference
information to update summaries if readers wish to.
We obtain an associated tuple <q, c> between a
1343
clicked sentence q and the examined contexts c.
As q has close semantic coherence with neigh-
boring contexts due to consistency in human natural
language, we consider a window of sentences cen-
tered at the clicked sentence q as c, which is a bag of
sentences. The window size k is a parameter to set.
However, click data is often sparse: users are not
likely to click more than 1/10 of total summary sen-
tences within a single generation. We amplify these
tiny hints of user interest by click smoothing.
We change the flat summary structure into a hi-
erarchical organization by extracting important se-
mantic units (denoted as u) and establishing link-
age between them. If the clicked sentence q con-
tains u, we diffuse the click impact to the correlated
units, which makes a single click perform as multi-
ple clicks and the sparse data is smoothed.
Problem Formulation
Input: Given the sentence collection D decom-
posed by documents, D = {s1, s2, . . . , s|D|} and
the clicked sentence record Q = {q1, q2, . . . }, we
generate summaries in sentences. A user click is
associated with a tuple <q, (u), c> where the exis-
tence of u depends on whether q contains u. The
collection of semantic units is denoted as M =
{u1, u2, . . . , u|M |}.
Output: A summary S as a set of sentences
{s1, s2, . . . , s|S|} and S ? D according to the pre-
specified compression rate ? (0 < ? < 1).
After the overview and formulation of IPS prob-
lem, we move on to the major components of User
Interaction and Personalized Summarization.
3.1 User Interaction
Hypertexify Summaries. We hypertexify the sum-
mary structure by establishing linkage between se-
mantic units. There are several possible formats for
semantic units, such as words or n-grams, etc. As
single words are proved to be not illustrative of se-
mantic meanings (Zhao et al, 2011) and n-grams are
rigid in length, we choose to extract semantic units
at a phrase granularity. Among all phrases from
source texts, some are of higher importance to at-
tract user interests, such as hot concepts or popu-
lar event names. We utilize the toolkit provided by
(Zhao et al, 2011) based on graph proximity LDA
(Blei et al, 2003) to extract key phrases and their
corresponding topic. A topic T is represented by
{(u1, pi(u1, T )), (u2, pi(u2, T )), . . . }where pi(u, T )
is the probability of u belonging to topic T . We in-
vert the topic-unit representation in Table 1, where
each u is represented as a topic vector. The corre-
lation corr(.) between ui, uj is measured by cosine
similarity sim(.) on topic distribution vector ~ui, ~uj .
corr(ui, uj) = simtopic(~ui, ~uj) (1)
Table 1: Inverted representation of topic-unit vector.
~u1 pi(u1, T1) pi(u1, T2) . . . pi(u1, Tn)
~u2 pi(u2, T1) pi(u2, T2) . . . pi(u2, Tn)
... ... ... ... ...
~u|M | pi(u|M |, T1) pi(u|M |, T2) . . . pi(u|M |, Tn)
When the summary is hypertexified by established
linkage, users click into the generated summary to
examine what they are interested in. A single click
on one sentence become multiple clicks via click
smoothing when the indicative function I(u|q) = 1.
I(u|q) =
{
1 q contains u;
0 otherwise. (2)
The click smoothing brings pseudo clicks q? asso-
ciated with u? and contexts c?. The entire user feed-
back texts A from q can be written as:
A(q) = I(u|q)
|M |?
j=1
corr(u?, u)(u?+? ?c?)+? ?c (3)
where ? is the weight tradeoff between u and asso-
ciated contexts c. If I(u|q) = 0, only the examined
context c is feedbacked for user preference; other-
wise, correlative contexts with u are taken into con-
sideration, which is a process of impact diffusion.
3.2 Personalized Summarization
Traditional summarization involves two essential re-
quirements: (1) coverage: the summary should
keep alignment with the source collection, which is
proved to be significant (Li et al, 2009). (2) di-
versity: according to MMR principle (Goldstein et
al., 1999) and its applications (Wan et al, 2007b;
Wan and Yang, 2008), a good summary should be
concise and contain as few redundant sentences as
possible, i.e., two sentences providing similar infor-
mation should not both present. According to our
1344
investigation, we observe that a well generated sum-
mary should properly consider a key component of
(3) user interests, which captures user preference to
summarize what they are interested in.
All above requirements involve a measurement
of similarity between two word distributions ?1
and ?2. Cosine, Kullback-Leibler divergence DKL
and Jensen Shannon divergence DJS are all able
to measure the similarity, but (Louis and Nenkova,
2009) indicate the superiority of DJS in summa-
rization task. We also introduce a pair of decreas-
ing/increasing logistic functions, L1(x) = 1/(1 +
ex) and L2(x) = ex/(1 + ex), to map the diver-
gence into interval [0,1]. V is the vocabulary set
and tf denotes the term frequency for word w.
DJS(?1||?2) =
1
2[DKL(?1||?2)+DKL(?2||?1)]
where
DKL(?1||?2) =
?
k?V
p(w|?1)log
p(w|?1)
p(w|?2)
where
p(w|?) = tf(w,?)?
w? tf(w?,?)
.
Modeling Interest for User Utility. Given a gener-
ated summary S, users tend to scrutinize texts rele-
vant to their interests. Texts related to user implicit
feedback are collected as A = ?|Q|i=1A(qi). Intu-
itively, the smaller distance between the word distri-
bution of final summary (?S) and the word distri-
bution of user preference (?A), the higher utility of
user interests Uuser(S) will be, i.e.,
Uuser(S) = L1(DJS(?S ||?A)). (4)
We model the utility of traditional summarization
Utrad(S) using a linear interpolation controlled by
parameter ? between utility from coverage Uc(S)
and utility Ud(S) from diversity:
Utrad(S) = Uc(S) + ? ? Ud(S). (5)
Coverage Utility. The summary should share a
closer word distribution with the source collection
(Allan et al, 2001; Li et al, 2009). A good summary
focuses on minimizing the loss of main information
from the whole collection D. Utility from coverage
Uc(S) is defined as follows and for coverage utility,
smaller divergence is desired.
Uc(S) = L1(DJS(?S ||?D)). (6)
Diversity Utility. Diversity measures the novelty
degree of any sentence s compared with all other
sentences within S, i.e., the distances between all
other sentences and itself. Diversity utility Ud(S) is
an average novelty score for all sentences in S. For
diversity utility, larger distance is desired, and hence
we use the increasing function L2 as follows:
Ud(S) =
1
|S|
?
s?S
L2(DJS(?s||?(S?s))). (7)
3.3 Balanced Optimization Framework
A well generated summary S should be sufficiently
aligned with the original source corpus, and also
be optimized given the user interests. The utility
of an individual summary U(S) is evaluated by the
weighted combination of these components, con-
trolled by parameter ? for balanced weights.
U(S) = Utrad(S) + ? ? Uuser(S) (8)
Given the sentence setD and the compression rate
?, there are ??|D| out of |D| possibilities to generate
S. The IPS task is to predict the optimized sentence
subset of S? from the space of all combinations. The
objective function is as follows:
S? = argmax
S
U(S). (9)
As U(S) is measured based on preferred interests
from user interaction within a generation in our sys-
tem, we extract S iteratively to approximate S?, i.e,
maximize U(S) based on the user feedbacks from
the interaction sessions. Each session is an iteration.
We use a similar framework as we have proposed in
(Yan et al, 2011).
During every session, the top ranked sentences are
strong candidates for the summary to generate and
the rank methodology is based on the metrics U(.).
The algorithm tends to highly rank sentences which
are with both coverage utility and interest utility, and
are diversified in balance: we rank each sentence s
according to U(s) under such metrics.
Consider S(n?1) generated in the (n-1)-th session
which consists of top ?|D| ranked sentences, as well
1345
as the top ?|D| ranked sentences in the n-th iteration
(denoted by O(n)), they have an intersection set of
Z(n) = Sn?1?On. There is a substitutable sentence
set X (n) = S(n?1) ?Z(n) and a new candidate sen-
tence set Y(n) = O(n) ? Z(n). We substitute x(n)
sentences with y(n), where x(n) ? X (n) and y(n)
? Y(n). During every iteration, our goal is to find a
substitutive pair <x,y> for S:
<x,y> : X ? Y ? R.
To measure the performance of such a substitu-
tion, a discriminant utility gain function ?Ux,y
?U (n)x(n),y(n) = U(S
(n))? U(S(n?1))
= U((S(n?1) ? x(n)) ? y(n))? U(S(n?1))
(10)
is employed to quantify the penalty. Therefore, we
predict the substitutive pair by maximizing the gain
function ?Ux,y over the state set R, with a size of?Y
k=0AkXCkY , where <x,y>? R. Finally the ob-
jective function of Equation (9) changes into maxi-
mization of utility gain by substitute x? with y? during
each iteration:
< x?, y? >= argmax
x?X ,y?Y
?Ux,y. (11)
Note that the objectives of interest utility opti-
mization and traditional utility optimization are not
always the same because the word distributions in
these texts are usually different. The substitutive
pair <x,y> may perform well based on the user
preference component while not on the traditional
summary part and vice versa. There is a tradeoff
between both user optimization and traditional opti-
mization and hence we need to balance them by ?.
The objective Equation (11) is actually to maxi-
mize ?U(S) from all possible substitutive pairs be-
tween two iteration sessions to generate S. The al-
gorithm is shown in Algorithm 1. The threshold  is
set at 0.001 in this study.
4 Experiments and Evaluation
4.1 Datasets
IPS can be tested on any document set but a tiny
corpus to summarize may not cover abundant effec-
tive interests to attract user clicks indicating their
Algorithm 1 Regenerative Optimization
1: Input: D, , ?
2: for all s ? D do
3: calculate Utrad(s)
4: end for
5: S ? top ?|D| ranked sentences
6: while new generation=TRUE do
7: collect clicks and update utility from U ? to U
8: if |U(S)? U ?(S)| >  then
9: for all s ? D do
10: calculate U(s)
11: end for
12: O ? top ?|D| ranked sentences by U(s)
13: Z ? S ? O
14: X ? S ?Z , Y ? O ?Z
15: for all <x,y> pair where x ? X ,y ? Y
do
16: ?Ux,y = U((S ? x) ? y)? U(S)
17: end for
18: < x?, y? >= argmax ?Ux,y
19: S ? (S ? x?) ? y?
20: end if
21: end while
preference. Besides, the scenario of small corpus is
not quite practical for the exponential growing web.
Therefore, we test IPS on large real world datasets.
We build 4 news story sets which consist of docu-
ments and reference summaries to evaluate our pro-
posed framework empirically. We downloaded 5197
news articles from 10 selected sources. As shown in
Table 2, three of the sources are in UK, one of them
is in China and the rest are in US. We choose them
because many of these websites provide handcrafted
summaries for their special reports, which serve as
reference summaries. These events belong to differ-
ent categories of Rule of Interpretation (ROI) (Ku-
maran and Allan, 2004). Statistics are in Table 3.
4.2 Experimental System Setups
? Preprocessing. Given a collection of documents,
we first decompose them into sentences. Stop-words
are removed and words stemming is performed.
Then the word distributions can be calculated.
? User Interface Design. Users are required to
specify the overall compression rate ? and the sys-
tem extracts ?|D| sentences according to user utility
1346
Figure 1: A demonstration system for Interactive Personalized Summarization when compression rate ? is specified
(e.g. 5%). For convenience of browsing, we number the selected sentences (see in part 3). Extracted semantic units,
such as ?drilling mud?, are in bold and underlined format (see in part 1). When the user clicks a sentence (part 4), the
clicked sentence ID is kept in the click record (part 2). Mis-clicked records revocation can be operated by clicking
the deletion icon ?X? (see in part 3). Once a sentence is clicked, user can track the sentence into the popup source
document to examine the contexts. The selected sentences are highlighted in the source documents (see in part 5).
Table 2: News sources of 4 datasets
News Sources Nation News Sources Nation
BBC UK Fox News US
Xinhua China MSNBC US
CNN US Guardian UK
ABC US New York Times US
Reuters UK Washington Post US
Table 3: Detailed basic information of 4 datasets.
News Subjects #size #docs #RS Avg.L
1.Influenza A 115026 2557 5 83
2.BP Oil Spill 63021 1468 6 76
3.Haiti Earthquake 12073 247 2 32
4.Jackson Death 37819 925 3 64
#size: total sentence counts; #RS: the number of reference summaries;
Avg.L: average length of reference summary measured in sentences.
and traditional utility. User utility is obtained from
interaction. The system keeps the clicked sentence
records and calculates the user feedback by Equa-
tion (3) during every session. Consider sometimes
users click into the summary due to confusion or
mis-operations, but not their real interests. The sys-
tem supports click records revocation. More details
of the user interface is demonstrated in Figure 1.
4.3 Evaluation Metrics
We include both subjective evaluation from 3 evalu-
ators based on their personalized interests and pref-
erence, and the objective evaluation based on the
widely used ROUGE metrics (Lin and Hovy, 2003).
Evaluator Judgments
Evaluators are requested to express an opinion
over all summaries based on the sentences which
they deem to be important for the news. In general
a summary can be rated in a 5-point scale, where
?1? for ?terrible?, ?2? for ?bad?, ?3? for ?normal?,
?4? for ?good? and ?5? for ?excellent?. Evaluators
are allowed to judge at any scores between 1 and 5,
e.g. a score of ?3.3? is adopted when the evaluator
feels difficult to decide whether ?3? or ?4? is more
1347
appropriate but with preference towards ?3?.
ROUGE Evaluation
The DUC usually officially employs ROUGE
measures for summarization evaluation, which mea-
sures summarization quality by counting overlap-
ping units such as the N-gram, word sequences, and
word pairs between the candidate summary and the
reference summary. We use ROUGE-N as follows:
ROUGE-N =
?
S?{RefSum}
?
N-gram?S
Countmatch(N-gram)
?
S?{RefSum}
?
N-gram?S
Count (N-gram)
whereN stands for the length of the N-gram and N-
gram?RefSum denotes the N-grams in the reference
summaries while N-gram?CandSum denotes the N-
grams in the candidate summaries. Countmatch(N-
gram) is the maximum number of N-gram in the
candidate summary and in the set of reference sum-
maries. Count(N-gram) is the number of N-grams in
the reference summaries or candidate summary.
According to (Lin and Hovy, 2003), among all
sub-metrics in ROUGE, ROUGE-N (N=1, 2) is rela-
tively simple and works well. In this paper, we eval-
uate our experiments using all methods provided by
the ROUGE package (version 1.55) and only report
ROUGE-1, since the conclusions drawn from differ-
ent methods are quite similar. Intuitively, the higher
the ROUGE scores, the similar two summaries are.
4.4 Algorithms for Comparison
We implement the following widely used multi-
document summarization algorithms as the baseline
systems, which are all designed for traditional sum-
marization without user interaction. For fairness we
conduct the same preprocessing for all algorithms.
Random: The method selects sentences ran-
domly for each document collection.
Centroid: The method applies MEAD algorithm
(Radev et al, 2004) to extract sentences according to
the following parameters: centroid value, positional
value, and first-sentence overlap.
GMDS: The Graph-based MDS proposed by
(Wan and Yang, 2008) first constructs a sentence
connectivity graph based on cosine similarity and
then selects important sentences based on the con-
cept of eigenvector centrality.
IPSini: The initial generated summary from IPS
merely models coverage and diversity utility, which
is similar to the previous work described in (Allan et
al., 2001) with different goals and frameworks.
IPS: Our proposed algorithms with personaliza-
tion component to capture interest by user feed-
backs. IPS generates summaries via iterative sen-
tence substitutions within user interactive sessions.
RefSum: As we have used multiple reference
summaries from websites, we not only provide
ROUGE evaluations of the competing systems but
also of the reference summaries against each other,
which provides a good indicator of not only the
upper bound ROUGE score that any system could
achieve, but also human inconsistency among refer-
ence summaries, indicating personalization.
4.5 Overall Performance Comparison
We take the average ROUGE-1 performance and hu-
man ratings on all sets. The overall results are shown
in Figure 2 and details are listed in Tables 4?6.
Figure 2: Overall performance on 6 datasets.
From the results, we have following observations:
? Random has the worst performance as expected,
both in ROUGE-1 scores and human judgements.
? The ROUGE-1 and human ratings of Centroid
and GMDS are better than those of Random. This is
mainly because the Centroid based algorithm takes
into account positional value and first-sentence over-
lap, which facilitates main aspects summarization
and PageRank-based GMDS ranks the sentence us-
ing eigenvector centrality which implicitly accounts
for information subsumption among all sentences.
? In general, the GMDS system slightly outper-
forms Centroid system in ROUGE-1, but the human
judgements of GMDS and Centroid are of no signifi-
cant difference. This is probably due to the difficulty
1348
Table 4: Overall performance comparison on Influenza A.
ROI? category: Science.
Systems R-1 95%-conf. H-1 H-2 H-3
RefSum 0.491 0.44958 3.5 3.0 3.9
Random 0.257 0.75694 1.2 1.0 1.0
Centroid 0.331 0.45073 2.5 3.0 3.5
GMDS 0.364 0.33269 3.0 2.7 3.5
IPSini 0.302 0.21213 2.0 2.5 2.5
IPS 0.337 0.46757 4.8 4.5 4.5
Table 5: Overall performance comparison on BP Oil
Leak. ROI category: Accidents.
Systems R-1 95%-conf. H-1 H-2 H-3
RefSum 0.517 0.48618 4.0 3.3 3.9
Random 0.262 0.64406 1.5 1.0 1.5
Centroid 0.369 0.34743 3.2 3.0 3.5
GMDS 0.389 0.43877 3.5 3.0 3.9
IPSini 0.327 0.53722 3.0 2.5 3.0
IPS 0.372 0.35681 4.8 4.5 4.5
Table 6: Overall performance comparison on Haiti Earth-
quake. ROI category: Disasters.
Systems R-1 95%-conf. H-1 H-2 H-3
RefSum 0.528 0.30450 3.8 4.0 4.0
Random 0.266 0.75694 1.5 1.5 1.8
Centroid 0.362 0.43045 3.6 3.0 4.0
GMDS 0.380 0.33694 3.9 3.5 4.0
IPSini 0.331 0.34120 2.8 2.5 3.0
IPS 0.391 0.40069 5.0 4.7 5.0
Table 7: Overall performance comparison on Michael
Jackson Death. ROI category: Legal Cases.
Systems R-1 95%-conf. H-1 H-2 H-3
RefSum 0.482 0.47052 3.5 3.5 4.0
Random 0.232 0.52426 1.2 1.0 1.5
Centroid 0.320 0.21045 3.0 2.5 2.7
GMDS 0.341 0.30070 3.5 3.3 3.9
IPSini 0.287 0.48526 2.5 2.0 2.2
IPS 0.324 0.36897 5.0 4.5 4.8
?ROI: news categorization defined by Linguistic Data Consortium.
Available at http://www.ldc.upenn.edu/projects/tdt4/annotation
of human judgements on comparable summaries.
? The results of ROUGE-1 and ratings for IPSini
are better than Random but worse than Centroid and
GMDS. The reason in this case may be that IPSini
does not capture sufficient attributes: coverage and
diversity are merely fundamental requirements.
? Traditional summarization considers sentence
selection based on corpus only, and hence neglects
Table 8: Ratings consistency between evaluators: mean
? standard deviation over the 4 datasets.
RefSum Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.35?0.09 0.30?0.33
Evaluator 2 0.50?0.14
Random Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.23?0.04 0.20?0.02
Evaluator 2 0.33?0.06
Centroid Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.45?0.03 0.50?0.12
Evaluator 2 0.55?0.11
GMDS Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.35?0.02 0.35?0.03
Evaluator 2 0.70?0.03
IPSini Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.45?0.01 0.25?0.04
Evaluator 2 0.30?0.06
IPS Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.35?0.01 0.18?0.02
Evaluator 2 0.28?0.04
user interests. Many sentences are extracted due to
arbitrary assumption of reader preference, which re-
sults in a low user satisfaction. Human judgements
under our proposed IPS framework greatly outper-
form baselines, indicating that the appropriate use
of human interests for summarization are beneficial.
The ROUGE-1 performance for IPS is not as ideal
as that of GMDS. This situation may result from the
divergence between user interests and general infor-
mation provided by mass media propaganda, which
again motivates the need for personalization.
Although the high disparities between different
human evaluators have been observed in (Gong and
Liu, 2001), we still examine the consistency among
3 evaluators and their preferred summaries to prove
the motivation of personalization in our work.
4.6 Consistency Analysis for Personalization
The low ROUGE-1 scores of RefSum indicate the
inconsistency among reference summaries. We con-
duct personalization analysis from two perspectives:
(1) human rating consistency and (2) content consis-
tency among human supervised summaries.
We calculate the mean and variance of rating vari-
ations among evaluator judgements, listed in Table
1349
Table 9: Content consistency among evaluators super-
vised summaries.
Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.273 0.398
Evaluator 2 0.289 0.257
Evaluator 3 0.407 0.235
RefSum 0.365 0.302 0.394
8. We see that for Random the average rating vari-
ation is 0.25, for IPS is 0.27, for IPSini is 0.33, for
RefSum is 0.38, for GMDS is 0.47 and for Centroid
is the highest, 0.50. Such phenomenon indicates
for poor generated summaries, such as Random or
IPSini, humans have consensus, but for normal sum-
maries without personalized interests, they are likely
to have disparities, surprisingly, even for RefSum.
General summaries provided by mass media satisfy
part of audiences, but obviously not all of them.
The high rating consistency of IPS indicates peo-
ple tend to favor summaries generated according to
their interests. We next examine content consistency
of these summaries with high rating consistency.
As shown in Table 9, although highly scored,
these human supervised summaries still have low
content consistency (especially Evaluator 2). The
low content consistency between RefSum and su-
pervised summaries shows reader have individual
personalization. Note that the inconsistency among
evaluators is larger than that between RefSum and
supervised summaries, indicating interests take a
high proportion in evaluator supervised summaries.
4.7 Parameter Settings
? controls coverage/diversity tradeoff. We tune ? on
IPSini and apply the optimal ? directly in IPS. Ac-
cording to the statistics in (Yan et al, 2010), the se-
mantic coherent context is about 7 sentences. There-
fore, we empirically choose k=3 for the examined
context window. The number of topics is set at
n=50. We assign an equal weight (? = 1) to seman-
tic units and examined contexts according to analog-
ical research of summarization from implicit feed-
backs via clickthrough data (Sun et al, 2005).
? is the key parameter in IPS approach, control-
ling the weight of user utility during the process of
interactive personalized summarization.
Through Figure 3, we see that when ? is small
Figure 3: ? v.s. human ratings and ROUGE scores.
(? ? [0.01, 0.1]), both human judgements and
ROUGE evaluation scores have little difference.
When ? ? [0.1, 1], ROUGE scores increase signifi-
cantly but human satisfaction shows little response.
? ? [1, 10] brings large user utility enhancement be-
cause user may find what they are interested in but
ROUGE scores start to decay. When ? ? [10, 100],
ROUGE scores drop much because the emphasized
user interests may guide the generated summaries
divergent away from the original corpus.
In Figure 4 we examine how ? attracts user clicks
and regeneration counts until satisfaction. As the re-
sult indicates, both counts increase as ? increases.
When ? is small (from 0.01 to 0.1), readers find
no more interesting aspects through clicks and re-
generations and stop due to the bad user experience.
As ? increases, the system mines more relevant sen-
tences according to personalized interests and hence
attracts user clicks and intention to regenerate.
Figure 4: ? v.s. click counts and regeneration counts.
1350
5 Conclusion
We present an important and novel summariza-
tion problem, Interactive Personalized Summariza-
tion (IPS), which generates summaries based on
human?system interaction for ?interests? and per-
sonalization. We formally formulate IPS as a combi-
nation of user utility and traditional summary utility,
such as coverage and diversity. We implement a sys-
tem under such framework for experiments on real
web datasets to compare all approaches. Through
our experiments we notice that user personalization
of interests plays an important role in summary gen-
eration, which largely increase human ratings due to
user satisfaction. Besides, our experiments indicate
the inconsistency between user preferred summaries
and reference summaries measured by ROUGE, and
hence prove the effectiveness of personalization.
Acknowledgments
This work was partially supported by HGJ 2010
Grant 2011ZX01042-001-001 and NSFC with Grant
No.61073082, 60933004. Rui Yan was supported by
the MediaTek Fellowship.
References
James Allan, Rahul Gupta, and Vikas Khandelwal. 2001.
Temporal summaries of new topics. In Proceedings of
the 24th annual international SIGIR?01, pages 10?18.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet alocation. The Journal of Machine Learning
Research, 3:993?1022.
G. Erkan and D.R. Radev. 2004. Lexpagerank: Prestige
in multi-document text summarization. In Proceed-
ings of EMNLP?04, volume 4.
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and
Jaime Carbonell. 1999. Summarizing text documents:
sentence selection and evaluation metrics. In Proceed-
ings of SIGIR?99, pages 121?128.
Yihong Gong and Xin Liu. 2001. Generic text sum-
marization using relevance measure and latent seman-
tic analysis. In Proceedings of the 24th international
ACM SIGIR conference, SIGIR ?01, pages 19?25.
Q. Guo and E. Agichtein. 2010. Ready to buy or just
browsing?: detecting web searcher goals from inter-
action data. In Proceeding of the 33rd international
ACM SIGIR conference, SIGIR?10, pages 130?137.
Giridhar Kumaran and James Allan. 2004. Text clas-
sification and named entities for new event detection.
In Proceedings of the 27th annual international ACM
SIGIR?04, pages 297?304.
Anton Leuski, Chin-Yew Lin, and Eduard Hovy. 2003.
ineats: interactive multi-document summarization. In
Proceedings of ACL?03, pages 125?128.
Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha,
and Yong Yu. 2009. Enhancing diversity, cover-
age and balance for summarization through structure
learning. In Proceedings of WWW?09, pages 71?80.
Chin-Yew Lin and Eduard Hovy. 2002. From single
to multi-document summarization: a prototype system
and its evaluation. In Proceedings of ACL?02, pages
457?464.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of NAACL?03, pages 71?78.
Annie Louis and Ani Nenkova. 2009. Automatically
evaluating content selection in summarization without
human models. In EMNLP?09, pages 306?314.
R. Mihalcea and P. Tarau. 2005. A language indepen-
dent algorithm for single and multiple document sum-
marization. In Proceedings of IJCNLP, volume 5.
D.R. Radev, H. Jing, and M. Sty. 2004. Centroid-based
summarization of multiple documents. Information
Processing and Management, 40(6):919?938.
Jian-Tao Sun, Dou Shen, Hua-Jun Zeng, Qiang Yang,
Yuchang Lu, and Zheng Chen. 2005. Web-page sum-
marization using clickthrough data. In Proceedings of
SIGIR?05, pages 194?201.
Stephen Wan and Ce?cile Paris. 2008. In-browser sum-
marisation: generating elaborative summaries biased
towards the reading context. In ACL-HLT?08, pages
129?132.
Xiaojun Wan and Jianwu Yang. 2008. Multi-document
summarization using cluster-based link analysis. In
Proceedings of SIGIR?08, pages 299?306.
X. Wan, J. Yang, and J. Xiao. 2007a. Manifold-ranking
based topic-focused multi-document summarization.
In Proceedings of IJCAI, volume 7, pages 2903?2908.
X. Wan, J. Yang, and J. Xiao. 2007b. Single document
summarization with document expansion. In Proceed-
ings of the 22nd AAAI?07, pages 931?936.
Rui Yan, Yu Li, Yan Zhang, and Xiaoming Li. 2010.
Event recognition from news webpages through latent
ingredients extraction. In AIRS?10, pages 490?501.
Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011. Evolution-
ary timeline summarization: a balanced optimization
framework via iterative substitution. In Proceedings
of the 34th annual international ACM SIGIR?11.
Xin Zhao, Jing Jiang, Jing He, Yang Song, Palakorn
Achanauparp, Ee-Peng Lim, and Xiaoming Li. 2011.
Topical Keyphrase Extraction from Twitter. In Pro-
ceedings of ACL-HLT?11.
1351
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 822?830,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Clinical Information Retrieval using Document and PICO Structure
Florian Boudin and Jian-Yun Nie
DIRO, Universite? de Montre?al
CP. 6128, succursale Centre-ville
Montre?al, H3C 3J7 Que?bec, Canada
{boudinfl,nie}@iro.umontreal.ca
Martin Dawes
Department of Family Medicine
McGill University, 515 Pine Ave W
Montre?al, H2W 1S4 Que?bec, Canada
martin.dawes@mcgill.ca
Abstract
In evidence-based medicine, clinical questions
involve four aspects: Patient/Problem (P), In-
tervention (I), Comparison (C) and Outcome
(O), known as PICO elements. In this pa-
per we present a method that extends the lan-
guage modeling approach to incorporate both
document structure and PICO query formu-
lation. We present an analysis of the distri-
bution of PICO elements in medical abstracts
that motivates the use of a location-based
weighting strategy. In experiments carried out
on a collection of 1.5 million abstracts, the
method was found to lead to an improvement
of roughly 60% in MAP and 70% in P@10 as
compared to state-of-the-art methods.
1 Introduction
As the volume of published medical literature con-
tinues to grow exponentially, there is more and more
research for physicians to assess and evaluate and
less time to do so. Evidence-based medicine (EBM)
(Sackett et al, 1996) is a widely accepted paradigm
in medical practice that relies on evidence from
patient-centered clinical research to make decisions.
Taking an evidence-based approach to searching
means doing a systematic search of all the available
literature, individually critically appraising each re-
search study and then applying the findings in clini-
cal practice. However, this is a time consuming ac-
tivity. One way to facilitate searching for a precise
answer is to formulate a well-focused and structured
question (Schardt et al, 2007).
Physicians are educated to formulate their clinical
questions according to several well defined aspects
in EBM: Patient/Problem (P), Intervention (I),
Comparison (C) and Outcome (O), which are called
PICO elements. In many documents in medical lit-
erature (e.g. MEDLINE), one can find the elements
of the PICO structure, but rarely explicitly anno-
tated (Dawes et al, 2007). To identify documents
corresponding to a patient?s state, physicians also
construct their queries according to the PICO struc-
ture. For example, in the question ?In children with
pain and fever how does paracetamol compared
with ibuprofen affect levels of pain and fever?? one
can identify the following PICO elements:
Patient/Problem: children/pain and fever
Intervention: paracetamol
Comparison: ibuprofen
Outcome: levels of pain and fever
Very little work, if any, has been carried out on the
use of these elements in the Information Retrieval
(IR) process. There are several reasons for that. It
is not easy to identify PICO elements in documents,
as well as in the question if these are not explicitly
separated in it. Several studies have been performed
on identifying PICO elements in abstracts (Demner-
Fushman and Lin, 2007; Hansen et al, 2008; Chung,
2009). However, all of them are reporting coarse-
grain (sentence-level) tagging methods that have not
yet been shown to be sufficient for the purpose of
IR. Moreover, there is currently no standard test col-
lection of questions in PICO structure available for
evaluation. On the other hand, the most critical as-
pect in IR is term weighting. One of the purpose
of tagging PICO elements is to assign appropriate
weights to these elements during the retrieval pro-
cess. From this perspective, a semantic tagging of
PICO elements may be a task that goes well beyond
822
that is required for IR. It may be sufficient to have
a method that assigns appropriate weights to ele-
ments rather than recognizing their semantic roles.
In this paper, we will propose an approach to deter-
mine term weights according to document structure.
This method will be compared to that using tagging
of PICO elements.
In this paper, we first report an attempt to manu-
ally annotate the PICO elements in documents by
physicians and use them as training data to build
an automatic tagging tool. It turns out that there
is a high disagreement rate between human anno-
tators. The utilization of the automatic tagging tool
in an IR experiment shows only a small gain in re-
trieval effectiveness. We therefore propose an alter-
native to PICO element detection that uses the struc-
tural information of documents. This solution turns
out to be robust and effective. The alternative ap-
proach is motivated by a strong trend that we ob-
serve in the distribution of PICO elements in docu-
ments. We then make use of both PICO query and
document structure to extend the classical language
modeling approach to IR. Specifically, we investi-
gate how each element of a PICO query should be
weighted and how a location-based weighting strat-
egy can be used to emphasize the most informative
parts (i.e. containing the most PICO elements) of
documents.
The paper is organized as follows. We first briefly
review the previous work, followed by a description
of the method we propose. Next, we present our
experiments and results. Lastly, we conclude with a
discussion and directions for future work.
2 Related work
There have been only a few studies trying to use
PICO elements in the retrieval process. (Demner-
Fushman and Lin, 2007) is one of the few such stud-
ies. The method they describe consists in re-ranking
an initial list of retrieved citations. To this end, the
relevance of a document is scored by the use of de-
tected PICO elements, among other things. Several
other studies aimed to build a Question-Answering
system for clinical questions (Demner-Fushman and
Lin, 2006; Andrenucci, 2008). But again, the focus
has been set on the post-retrieval step, while the doc-
ument retrieval step only uses a standard approach.
In this paper, we argue that IR has much to gain by
using PICO elements.
The task of identifying PICO elements has how-
ever gain more attention. In their paper, (Demner-
Fushman and Lin, 2007) presented a method that
uses either manually crafted pattern-matching rules
or a combination of basic classifiers to detect PICO
elements in medical abstracts. Prior to that, biomed-
ical concepts are labelled by Metamap (Aronson,
2001) while relations between these concepts are
extracted with SemRep (Rindflesch and Fiszman,
2003). Recently, supervised classification using
Support Vector Machines (SVM) was proposed by
(Hansen et al, 2008) to extract the number of trial
participants. In a later study, (Chung, 2009) ex-
tended this work to other elements using Conditional
Random Fields. Although these studies are report-
ing interesting results, they are limited in several as-
pects. First, many are restricted to some segments
of the medical documents (e.g. Method section)
(Chung, 2009), and in most cases, the test collection
is very small (a few hundreds abstracts). Second, the
precision and granularity of these methods have not
yet been shown to be sufficient for the purpose of IR.
The structural information provided by markup
languages (e.g. XML) has been successfully used
to improve the IR effectiveness (INEX, 2002 2009).
For such documents, the structure information can
be used to emphasize some particular parts of the
document. Thereby, a given word should not have
the same importance depending on its position in the
document structure.
Taking into account the structure can be done ei-
ther at the step of querying or at the step of index-
ing. One way to integrate the structure at querying
is to adapt query languages (Fuhr and Gro?johann,
2001). These approaches follow the assumption that
the user knows where the most relevant information
is located. However, (Kamps et al, 2005) showed
that it is preferable to use structure as a search hint,
and not as a strict search requirement
The second approach consists in integrating the
document structure at the indexing step by introduc-
ing a structure weighting scheme (Wilkinson, 1994).
In such a scheme, the weight assigned to a word is
not only based on its frequency but also on its posi-
tion in the document. The structure of a document
can be defined in terms of tags (e.g. title, section),
823
each of those having a weight chosen either empiri-
cally or automatically by the use of optimizing tech-
niques such as genetic algorithms (Trotman, 2005).
3 Using PICO elements in retrieval
In this section, we present an experiment on the
manual annotation of PICO elements. We then de-
scribe an approach to detect these elements in doc-
uments and give some results on the use of these
tagged elements in the retrieval process.
3.1 Manual annotation of PICO elements
We asked medical professionals to manually anno-
tate the PICO elements in a small collection of ab-
stracts from PubMed1. The instructions given to
the annotators were fairly simple. They were asked
to precisely annotate all PICO elements in abstracts
with no restriction about the size of the elements (i.e.
they could be words, phrases or sentences). More
than 50 abstracts were manually annotated this way
by at least two different annotators. Two annotations
by two annotators are considered to agree if they
share some words (i.e. they overlap). We computed
the well known Cohen?s kappa measure as well as an
ad-hoc measure called loose. The latter uses PICO
elements as units and estimates the proportion of el-
ements that have been annotated by both raters.
Measure P-element I/C-element O-element
kappa 0.687 0.539 0.523
loose 0.363 0.136 0.140
Table 1: Agreement measures computed for each ele-
ment. Cohen?s kappa and loose agreement are presented.
We can observe that there is a very low agree-
ment rate between human annotators. The loose
measure indicates that less than 15% of the I, C and
O elements have been marked by both annotators.
This fact shows that such human annotations can be
hardly used to develop an automatic tagging tool for
PICO elements, which requires consistent training
data. We therefore try to develop a coarser-grained
tagging method.
1www.pubmed.gov, PubMed is a service of the US Na-
tional Library of Medicine that includes over 19 million cita-
tions from MEDLINE and other life science journals.
3.2 Automatic detection of PICO elements
Similarly to previous work, we propose a sentence-
level detection method. The identification of PICO
elements can be seen as a classification task. Even
for a coarser-grain classification task, we are still
lack of annotated data. One solution is to use the
structural information embedded in some medical
abstracts for which the authors have clearly stated
distinctive sentence headings. Some recent ab-
stracts in PubMed do contain explicit headings such
as ?PATIENTS?, ?SAMPLE? or ?OUTCOMES?,
that can be used to locate sentences correspond-
ing to PICO elements. Using that information, we
extracted three sets of abstracts: Patient/Problem
(14 279 abstracts), Intervention/Comparison (9 095)
and Outcome (2 394).
Tagging each document goes through a three steps
process. First, the document is segmented into plain
sentences. Then each sentence is converted into a
feature vector using statistical (e.g. position, length)
and knowledge-based features (e.g. MeSH semantic
type). Knowledge-based features were derived ei-
ther from manually crafted cue-words/verbs lists or
semantic types within the MeSH ontology2. Finally,
each vector is submitted to multiple classifiers, one
for each element, allowing to label the correspond-
ing sentence. We use several algorithms imple-
mented in the Weka toolkit3: decision trees, SVM,
multi-layer perceptron and Naive Bayes. Combin-
ing multiple classifiers using a weighted linear com-
bination of their prediction scores achieves the best
results with a f-measure score of 86.3% for P, 67%
for I/C and 56.6% for O in 10-fold cross-validation.
3.3 Use of detected elements in IR
We use language modeling approach to IR in this
work. The idea is that a document is a good match to
a query if its language model is likely to generate the
query (Ponte and Croft, 1998). It is one of the state-
of-the-art approaches in current IR research. Most
language modeling work in IR use unigram lan-
guage models ?also called bags-of-words models?
assuming that there is no structure in queries or doc-
uments. A typical way to score a document d as
relevant to a query q is to use the Kullback-Leibler
2www.nlm.nih.gov/mesh/
3www.cs.waikato.ac.nz/ml/index.html
824
divergence between their respective LMs:
score(q, d) =
?
w?q
P(w | Mq) ? log P(w | Md) (1)
? ?KL(Mq || Md)
whereMq is the LM of the query andMd the LM of
the document. P(w | M?) estimates the probability
of the word w given the language model M?. The
most direct way to estimate these models is to use
Maximum Likelihood estimation over the words:
P(w | M?) =
count(w, ?)
| ? |
where ? is the observed document, count(w, ?) the
number of times the wordw occurs in ? and | ? | the
length of the document. Bayesian smoothing using
Dirichlet priors is then applied to the maximum like-
lihood estimator to compensate for data sparseness.
We propose an approach that extend the basic
LM approach to take into consideration the PICO
element annotation. We assume that each ele-
ment in the document has a different importance
weight. Four more LMs are created, one for each
elements. Given?e the weight of the PICO element
e, P(w | Md) in equation 1 is re-defined as:
P1(w | Md) ? P(w | Md) +
?
e?[P,I,C,O]
?e ? P(w | Me)
The right hand of the above equation is not a prob-
ability function. We could use a normalization to
transform it. However, for the purpose of document
ranking, this will not make any difference. There-
fore, we will keep the un-normalized value.
We performed an extensive series of experiments
using this model on the test collection described in
Section 5. The results are shown in Table 2. It turns
out that the best improvement we were able to obtain
is very small (0.5% of MAP increase). There may
be several reasons for that. First, the accuracy of
the automatic document tagging may be insufficient.
Second, even if elements are correctly identified in
documents, if queries are treated as bags-of-words
then any PICO element can match with any identi-
cal word in the query, whether it describe the same
element or not. However, we also tested a na??ve ap-
proach that matches the PICO elements in queries
with the corresponding elements in documents. But
this approach quickly turns out to be too restrictive
and leads to bad results.
Measure
Weighted elements
P I / C O Best?
MAP increase 0.0% ?0.2% ?0.1% +0.5%
Table 2: Results using the PICO elements automatically
detected in documents (?: wP = 0.5, wI = 0.2).
As we can see, this approach only brings limited
improvement in retrieval effectiveness. This rises
the question of the usability of such tagging method
in its current performance state. We will see in the
next section an alternative solution to this problem
that relies on the distribution of PICO elements in
documents.
4 Method
4.1 Distribution of PICO elements
PICO elements are not evenly distributed in medical
documents, which often follow some implicit writ-
ing convention. An intuitive method is to weight
higher a segment that is more probable to con-
tain PICO elements. The distribution of PICO el-
ements is likely to correlate to the position within
the document. This intuition has been used in most
of the supervised PICO detection methods which
use location-based features. There has been sev-
eral studies that cover the PICO extraction problem.
However, as far as we know, none of them analyses
and uses the positional ditribution of these elements
within the documents for the purpose of IR. Biomed-
ical abstracts can be typically represented by four or-
dered rhetorical categories which are Introduction,
Methods, Results and Discussion (IMRAD) (Sollaci
and Pereira, 2004). The reason is found in the need
for speed when reviewing literature, as this format
allows readers to pick those parts of particular in-
terest. Besides, many scientific journals explicitly
recommended this ordered structure.
The PICO dispersion is highly correlated to these
rhetorical categories as some elements are more
likely to occur in certain categories. For example,
outcomes are more likely to appear in Results and/or
Discussion parts. One could also expect to infer the
825
role played by PICO elements in a clinical study. For
example, the drug pioglitazone has not the same role
in a clinical study if it appears as the main interven-
tion (likely to occur in all parts) or as a comparative
treatment (Methods and/or Results parts).
Instead of analysing the dispersion of PICO ele-
ments into the four IMRAD categories, we choose to
the use automatically splitted parts. There are sev-
eral reasons for that. First, the IMRAD categories
are not explicitely marked in abstracts. An auto-
matic tagging of these would surely result in some
errors. Second, using a low granularity approach
would provide more precise statistics. Furthermore,
if one would use the dispersion of elements as a cri-
terion to estimate how important each part is, an au-
tomatic partition would be a good choice because of
its repeatability and ease to implement.
We divided each manually annotated abstract into
10 parts of equal length (P1 being the begining and
P10 the ending) and computed statistics on the num-
ber of elements than occur in each of these parts.
The Figure 1 shows the proportion of elements for
each part. We can observe that PICO elements are
not evenly distributed throughout the abstracts. Uni-
versally accepted rules that govern medical writing
styles would be the first reason for that. It is clear
that the beginning and ending parts of abstracts do
contain most of the PICO elements. This gives us a
clear indication on which parts should be enhanced
when searching for these elements.
8 9 10 11 12 13% of PICO elements
P10
P9
P8
P7
P6
P5
P4
P3
P2
P1
Par
ts o
f th
e ab
stra
cts
Figure 1: Proportion of PICO elements computed for
each different part of abstracts.
Therefore, there may be several levels of granu-
larity when using the PICO framework in IR. One
can identify each PICO element in the document,
whether it is described by a word, a phrase or a com-
plete sentence. One can also use a coarser-grain
approach, estimating from the distribution across
documents the probability that each part contains a
PICO element. As attempts to precisely locate PICO
elements have shown that this task is particularly
difficult, we propose to get rid this issue by using
the second method.
4.2 Model definitions
We propose three different models that extend the
classical language modeling approach. The first uses
the structural information of documents, the second
takes advantage of the PICO query structure while
the third simply combine the first two models.
Model-1
Attempts to precisely locate PICO elements in doc-
uments have shown that this task is particularly dif-
ficult. We propose to get around this issue by intro-
ducing structural markers to convey document struc-
ture and use them as a means of providing location
information. Accordingly, each document is repre-
sented as a series of successive parts. To integrate
document structure into the ranking function, we es-
timate a series of probabilities that constraints the
word counts to a specific part instead of the entire
document. Each document d is then ranked by a
weighted linear interpolation. Intuitively, the weight
of a part should depend on how much information is
conveyed by its words. Given ?p the weight of the
part p ? [TITLE, P1 ? ? ? P10], P(w | Md) in equation
1 is re-defined as:
P2(w |Md) ? P(w |Md)+
?
p?d
?p ?P(w ? p |Md)
Model-2
The PICO formulation of queries provides informa-
tion about the role of each query word. One idea
is to use this structural decomposition to thoroughly
balance elements in the ranking function. For exam-
ple, the weight given to the drug fluoxetine should be
different depending on whether it refers to the inter-
vention or comparison concept. The same goes for
obesity which can be a problem or an outcome. To
826
integrate this in the ranking function, we define a pa-
rameter ?e that represents the weight given to query
words belonging to the element e ? [P, I, C, O].
f(w, e) = 1 if w ? e, 0 otherwise. We re-defined
P(w | Md) in equation 1 as:
P3(w |Mq) ? P(w |Mq)+
?
e?[P,I,C,O]
?e ?f(w, e)?P(w |Mq)
Model-1+2
This is the combination of the two previously de-
scribed models. We re-defined the scoring function
as:
score(q, d) =
?
w?q
P3(w | Mq) ? log P2(w | Md)
5 Experiments
In this section, we describe the details of our exper-
imental protocol. We then present the results ob-
tained with the three proposed models.
Experimental settings
We gathered a collection of nearly 1.5 million ab-
stracts from PubMed with the following require-
ments: with abstract, humans subjects, in english
and selecting the following publication types: RCT,
reviews, clinical trials, letters, practice guidelines,
editorials and meta-analysis. Prior to the index con-
struction, each abstract is automatically divided into
10 parts of equal length, abstracts containing less
than 10 words are discarded. The following fields
are then marked: TITLE, P1, P2, ... P10 with P1 be-
ing the begining of the document and P10 the end-
ing.
Unfortunately, there is no standard test collection
appropriate for testing the use of PICO in IR and
we had to manually create one. For queries, we use
the Cochrane systematic reviews4 on 10 clinical
questions about different aspects of ?diabetes?.
These reviews contain the best available infor-
mation about an healthcare intervention and are
designed to facilitate the choices that doctors face
in health care. All the documents in the ?Included
studies? section are judged to be relevant for the
4www.cochrane.org/reviews/
question. These included studies are selected by
the reviewers (authors of the review article) and
judged to be highly related to the clinical question.
In our experiments, we consider these documents
as relevant ones. From the 10 selected questions,
professors in family medicine have formulated a set
of 52 queries, each of which was manually anno-
tated according to the PICO structure. The resulting
testing corpus is composed of 52 queries (average
length of 14.7 words) and 378 relevant documents.
Below are some of the alternative formulations of
queries for the question ?Pioglitazone for type 2
diabetes mellitus?:
In patients with type 2 diabetes (P) | does pioglita-
zone (I) | compared to placebo (C) | reduce stroke
and myocardial infarction (O)
In patients with type 2 diabetes who have a high risk
of macrovascular events (P) | does pioglitazone (I) |
compared to placebo (C) | reduce mortality (O)
We use cross-validation to determine reasonable
weights and avoid over-fitting. We have divided the
queries into two groups of 26 queries: Qa and Qb.
The best parameters found for Qa are used to test
on Qb, and vice versa. In our experiments, we use
the KL divergence ranking (equation 1) as baseline.
The following evaluation measures are considered
relevant:
Precision at n (P@n). Precision computed on only
the n topmost retrieved documents.
Mean Average Precision (MAP). Average of preci-
sions computed at the point of each relevant docu-
ment in the ranked list of retrieved documents.
MAP is a popular measure that gives a global
quality score of the entire ranked list of retrieved
documents. In the case of clinical searches, one
could also imagine this scenario: a search performed
by a physician who does not have the time to look
into large sets of results, but for whom it is impor-
tant to have relevant results in the top 10. In such
case, P@10 is also an appropriate measure.
Student?s t-test is performed to determine statis-
tical significance. The Lemur Toolkit5 was used for
5www.lemurproject.org
827
all retrieval tasks. Experiments were performed with
an ?out-of-the-box? version of Lemur, using its tok-
enization algorithm and porter stemmer. The Dirich-
let prior smoothing parameter was set to its default
value ? = 2500.
Experiments with model-1
We first investigated whether assigning a weight to
each part of the document can improve the retrieval
accuracy. It is however difficult to determine a set
of reasonable values for all the parts together, as the
value of one part will affect those of the others. In
this study, we perform a two pass tuning. First, we
consider the ?p weights to be independent. By doing
so, searching for the optimal weight distribution can
be seen as tuning the weight of each part separately.
When searching the optimal weight of a part, the
weight for other parts is assigned 0. Second, these
approximations of the optimum values are used as
initial weights prior to the second pass. The final
weight distribution is obtained by searching for the
best weight combination around the initial values.
The Figure 2 shows the optimal weight distri-
butions along with the best relative MAP increase
for each part. A noticeable improvement is ob-
tained by increasing the weights associated to the ti-
tle/introduction and conclusion of documents. This
is consistent with the results observed on the dis-
tribution of PICO elements in abstracts. Boosting
middle parts of documents seems to have no impact
at all. We can see that the two ?p weight distribu-
tions (1-pass and 2-pass) are very close.
Performance measures obtained by model-1 are
presented in Table 3. With 1-pass tuning, we ob-
serve a MAP score increase of 37.5% and a P@10
increase of 64.1%. After the second pass, scores are
lower with 35% and 60.5% for MAP and P@10 re-
spectively. This result indicates that there is possibly
overfitting when we perform the two pass parameter
tuning. It could also be caused by the limited num-
ber of query in our test collection. However, we can
determine reasonable weights by tuning each part
weight separately.
Experiments with model-2
We have seen that a large improvement could come
from weighting each part accordingly. In a second
series of experiments, we try to assign a different
10
20
30
0.2
0.4
0.6
0.8
1.0
                            Weight parameter ?p
Q26A1-pass2-pass
Title P1 P2 P3 P4 P5 P6 P7 P8 P9 P100
10
20
30
      
      
      
      
      
     M
AP 
incr
eas
e (%
)
Different part of the documents
0.0
0.2
0.4
0.6
0.8
1.0Q26B1-pass2-pass
Figure 2: Best MAP increase for each part p (bar charts),
corresponding 1 and 2-pass ?p weights are also given.
weight to each PICO element in queries. A grid
search was used to find the optimal ?e weights com-
bination. The results are shown in Table 3.
We observe a MAP score increase of 22.5% and
an increase of 11% in P@10. Though the use of
a PICO weighting scheme increases the retrieval
accuracy, there is clearly much to gain by using
the document structure. The optimal [?p, ?i, ?c, ?o]
weights distribution is [0.3, 1.2, 0, 0.1] for Qa and
[0.2, 1, 0, 0.2] for Qb. That means that the most im-
portant words in queries belong to the Intervention
element. This supports the manual search strategy
proposed by (Weinfeld and Finkelstein, 2005), in
which they suggested that I and P elements should
be used first to construct queries, and only if too
many results are obtained that other elements should
be considered.
It is interesting to see that query words belonging
to the Comparison element have to be considered
as the least important part of a query. Even more
so because they are in the same semantic group as
the Intervention words. A reason for that could be
the use of vague words such as ?no-intervention? or
?placebo?. The methodology employed to construct
the queries is also responsible. Indeed, physicians
have focused on producing alternative formulations
of 10 general clinical questions by predominantly
modifying the one of the PICO elements. As a re-
sult, some of them do share the same vague Com-
parison words.
828
Experiments
MAP P@10
Qb?Qa Qa?Qb % Avg. Qb?Qa Qa?Qb % Avg.
Baseline 0.118 0.131 0.219 0.239
Model-1 / 1pass 0.165 0.176 +37.5%? 0.377 0.373 +64.1%?
Model-1 / 2pass 0.165 0.170 +35.0%? 0.354 0.381 +60.5%?
Model-2 0.149 0.168 +22.5%? 0.250 0.258 +11.0%
Model-1+2 0.198 0.202 +61.5%? 0.385 0.392 +70.0%?
Table 3: Cross-validation (train?test) scores for the baseline (Kullback-Leibler divergence), model-1 with 1 and 2-
pass tuning, model-2 and their combination (model-1+2). Relative increase over the baseline is also given (averaged
between Qa and Qb). (?: t.test < 0.01)
Experiments with model-1+2
We have seen that both the use of a location-based
weighting and a PICO-structure weighting scheme
increase the retrieval accuracy. In this last series of
experiments, we analyse the results of their com-
bination. We can observe that fusing model-1 and
model-2 allows us to obtain the best retrieval ac-
curacy with a MAP score increase of 61.5% and a
P@10 increase of 70.0%. It is a large improvement
over the baseline as it means that instead of about
two relevant documents in the top 10, our system
can retrieve nearly four. These results confirm that
both PICO framework and document structure can
be very helpful for the IR process.
6 Conclusion
We presented a language modeling approach that in-
tegrates document and PICO structure for the pur-
pose of clinical IR. A straightforward idea is to de-
tect PICO elements in documents and use the ele-
ments in the retrieval process. However, this ap-
proach does not work well because of the diffi-
culty to arrive at a consistent tagging of these ele-
ments. Instead, we propose a less demanding ap-
proach which assigns different weights to different
parts of a document.
We first analysed the distribution of PICO el-
ements in a manually annotated abstracts collec-
tion. The observed results led us to believe that a
location-based weighting scheme can be used in-
stead of a PICO detection approach. We then ex-
plored whether this strategy can be used as an in-
dicator to refine document relevance. We also pro-
posed a model to integrate the PICO information
provided in queries and investigated how each el-
ement should be balanced in the ranking function.
On a data set composed of 1.5 million abstracts ex-
tracted from PubMed, our method obtains an in-
crease of 61.5% for MAP and 70% for P@10 over
the classical language modeling approach.
This work can be much improved in the future.
For example, the location-based weighting method
can be improved in order to model a different weight
distribution for each PICO element. As the distri-
bution in abstracts is not the same among PICO el-
ements, it is expected that differentiated weighting
schemes could result in better retrieval effectiveness.
In a similar perspective, we are continuing our ef-
forts to construct a larger manually annotated col-
lection of abstracts. It will be thereafter conceiv-
able to use this data to infer the structural weighting
schemes or to train a more precise PICO detection
method. The focused evaluation described in this
paper is a first step. Although the queries are limited
to diabetes, this does not affect the general PICO
structure in queries. We plan to extend the coverage
of queries to other topics in the future.
Acknowledgements
The work described in this paper was funded by
the Social Sciences and Humanities Research Coun-
cil (SSHRC). The authors would like to thank Dr.
Ann McKibbon, Dr. Dina Demner-Fushman, Lorie
Kloda, Laura Shea, Lucas Baire and Lixin Shi for
their contribution in the project.
829
References
A. Andrenucci. 2008. Automated Question-Answering
Techniques and the Medical Domain. In International
Conference on Health Informatics, volume 2, pages
207?212.
A.R. Aronson. 2001. Effective Mapping of Biomedical
Text to the UMLS Metathesaurus: The MetaMap Pro-
gram. In AMIA Symposium.
G. Chung. 2009. Sentence retrieval for abstracts of ran-
domized controlled trials. BMC Medical Informatics
and Decision Making, 9(1):10.
M. Dawes, P. Pluye, L. Shea, R. Grad, A. Green-
berg, and J.Y. Nie. 2007. The identification of
clinically important elements within medical jour-
nal abstracts: Patient-Population-Problem, Exposure-
Intervention, Comparison, Outcome, Duration and
Results (PECODR). Informatics in Primary care,
15(1):9?16.
D. Demner-Fushman and J. Lin. 2006. Answer extrac-
tion, semantic clustering, and extractive summariza-
tion for clinical question answering. In ACL.
D. Demner-Fushman and J. Lin. 2007. Answering
clinical questions with knowledge-based and statistical
techniques. Computational Linguistics, 33(1):63?103.
N. Fuhr and K. Gro?johann. 2001. XIRQL: A query
language for information retrieval in XML documents.
In SIGIR, pages 172?180.
M.J. Hansen, N.O. Rasmussen, and G. Chung. 2008. A
method of extracting the number of trial participants
from abstracts describing randomized controlled trials.
Journal of Telemedicine and Telecare, 14(7):354?358.
INEX. 2002-2009. Proceedings of the INitiative for the
Evaluation of XML Retrieval (INEX) workshop.
J. Kamps, M. Marx, M. de Rijke, and B. Sigurbjo?rnsson.
2005. Structured queries in XML retrieval. In CIKM,
pages 4?11.
J.M. Ponte and W.B. Croft. 1998. A language model-
ing approach to information retrieval. In SIGIR, pages
275?281.
T.C. Rindflesch and M. Fiszman. 2003. The interac-
tion of domain knowledge and linguistic structure in
natural language processing: interpreting hypernymic
propositions in biomedical text. Journal of Biomedical
Informatics, 36(6):462?477.
D.L. Sackett, W. Rosenberg, J.A. Gray, R.B. Haynes, and
W.S. Richardson. 1996. Evidence based medicine:
what it is and what it isn?t. British medical journal,
312(7023):71.
C. Schardt, M. Adams, T. Owens, S. Keitz, and
P. Fontelo. 2007. Utilization of the PICO frame-
work to improve searching PubMed for clinical ques-
tions. BMC Medical Informatics and Decision Mak-
ing, 7(1):16.
L.B. Sollaci and M.G. Pereira. 2004. The introduction,
methods, results, and discussion (IMRAD) structure:
a fifty-year survey. Journal of the Medical Library
Association, 92(3):364.
A. Trotman. 2005. Choosing document structure
weights. Information Processing and Management,
41(2):243?264.
J.M. Weinfeld and K. Finkelstein. 2005. How to answer
your clinical questions more efficiently. Family prac-
tice management, 12(7):37.
R. Wilkinson. 1994. Effective retrieval of structured doc-
uments. In SIGIR, pages 311?317.
830
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 375?378,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
RALI: Automatic weighting of text window distances
Bernard Brosseau-Villeneuve*#, Noriko Kando#, Jian-Yun Nie*
* Universit? de Montr?al, Email: {brosseab, nie}@iro.umontreal.ca
# National Institute of Informatics, Email: {bbrosseau, kando}@nii.ac.jp
Abstract
Systems using text windows to model
word contexts have mostly been using
fixed-sized windows and uniform weights.
The window size is often selected by trial
and error to maximize task results. We
propose a non-supervised method for se-
lecting weights for each window distance,
effectively removing the need to limit win-
dow sizes, by maximizing the mutual gen-
eration of two sets of samples of the same
word. Experiments on Semeval Word
Sense Disambiguation tasks showed con-
siderable improvements.
1 Introduction
The meaning of a word can be defined by the
words that accompany it in the text. This is the
principle often used in previous studies on Word
Sense Disambiguation (WSD) (Ide and V?ronis,
1998; Navigli, 2009). In general, the accompa-
nying words form a context vector of the target
word, or a probability distribution of the context
words. For example, under the unigram bag-of-
word assumption, this means building p(x|t) =
count(x,t)
?
x
?
count(x
?
,t)
, where count(x, t) is the count of
co-occurrences of word x with the target word t
under a certain criterion. In most studies, x and t
should co-occur within a window of up to k words
or sentences. The bounds are usually selected as
to maximize system performance. Occurrences in-
side the window usually weight the same with-
out regard to their position. This is counterintu-
itive. Indeed, a word closer to the target word usu-
ally has a greater semantic constraint on the tar-
get word than a more distant word. Some studies
have also proposed decaying factors to decrease
the importance of more distant words in the con-
text vector. However, the decaying functions are
defined manually. It is unclear that the functions
defined can capture the true impact of the con-
text words on the target word. In this paper, we
propose an unsupervised method to automatically
learn the optimal weight of a word according to its
distance to the target word. The general idea used
to determine such weight is that, if we randomly
determine two sets of texts containing the target
word, the resulting probability distributions for its
context words in the two sets should be similar.
Therefore, the weights of context words at differ-
ent distance are determined so as to maximize the
mutual generation probabilities of two sets of sam-
ples. Experimentation on Semeval-2007 English
and Semeval-2010 Japanese lexical sample task
data shows that improvements can automatically
be attained on simple Naive Bayes (NB) systems
in comparison to the best manually selected fixed
window system.
The remainder of this paper is organized as fol-
lows: example uses of text windows and related
work are presented in Section 2. Our method
is presented in Section 3. In Section 4 and
5, we show experimental results on English and
Japanese WSD. We conclude in Section 6 with
discussion and further possible extensions.
2 Uses of text windows
Modeling the distribution of words around one
target word has many uses. For instance, the
Xu&Croft co-occurrence-based stemmer (Xu and
Croft, 1998) uses window co-occurrence statis-
tics to calculate the best equivalence classes for
a group of word forms. They suggest using win-
dows of up to 100 words. Another example can be
found in WSD systems, where a shorter window is
preferred. In Semeval-2007, top performing sys-
tems on WSD tasks, such as NUS-ML (Cai et al,
2007), made use of bag-of-word features around
the target word. In this case, they found that the
best results can be achieved using a window size
of 3.
375
Both these systems limit the size of their win-
dows for different purposes. The former aims to
model the topic of the documents containing the
word rather than the word?s meaning. The latter
limits the size because bag-of-word features fur-
ther from the target word would not be sufficiently
related to its meaning (Ide and V?ronis, 1998). We
see that because of sparsity issues, there is a com-
promise between taking few, highly related words,
or taking several, lower quality words.
In most current systems, all words in a window
are given equal weight, but we can easily under-
stand that the occurrences of words should gener-
ally count less as they become farther; they form
a long tail that we should use. Previous work pro-
posed using non-linear functions of the distance
to model the relation between two words. For in-
stance, improvements can be obtained by using an
exponential function (Gao et al, 2002). Yet, there
is no evidence that the exponential ? with its man-
ually selected parameter ? is the best function.
3 Computing weights for distances
In this section, we present our method for choos-
ing howmuch a word should count according to its
distance to the target word. First, for some defini-
tions, let C be a corpus, W a set of text windows,
c
W,i,x
the count of occurrences of word x at dis-
tance i in W , c
W,i
the sum of these counts, and ?
i
the weight put on one word at distance i. Then,
P
ML,W
(x) =
?
i
?
i
c
W,i,x
?
i
?
i
c
W,i
(1)
is the maximum likelihood estimator for x. To
counter the zero-probability problem, we apply
Dirichlet smoothing with the collection language
model as a prior:
P
Dir,W
(x) =
?
i
?
i
c
W,i,x
+ ?
W
P (x|C)
?
i
?
i
c
W,i
+ ?
W
(2)
The pseudo-count ?
W
is found by using Newton?s
method via leave-one-out estimation. We follow
the procedure shown in (Zhai and Lafferty, 2002),
but since occurrences have different weights, the
log-likelihood is changed to
L
?1
(?|W, C) = (3)
?
i
?
x?V
?
i
c
W,i,x
log
?
i
c
W,i,x
??
i
+?P (x|C)
?
j
?
j
c
W,j
??
i
+?
To find the best weights for our model we pro-
pose the following:
? Let T be the set of all windows containing
the target word. We randomly split this set
into two sets A and B.
? We want to find ?
?
that maximizes the mu-
tual generation of the two sets, by minimizing
their cross-entropy:
l(?) = H(P
ML,A
, P
Dir,B
) + H(P
ML,B
, P
Dir,A
)
(4)
In other words, we want ?
i
to represent how
much an occurrence at distance i models the con-
text better than the collection language model,
whose counts are controlled by the Dirichlet
pseudo-count. We hypothesize that target words
occurs in limited contexts, and as we get farther
from them, the possibilities become greater, re-
sulting in sparse and less related counts.
3.1 Gradient descent
We propose a simple gradient descent minimiz-
ing (4) over ?. For the following experiments,
we used one single curve for all words in a task.
We used the mini-batch type of gradient descent:
the gradients of a fixed amount of target words are
summed, a gradient step is done, and the proces
is repeated while cycling the data. The starting
state was with all ?
i
to one, the batch size of 50
and a learning rate of 1. We notice that as the al-
gorithm progress, weights on close distances in-
crease and the farthest decrease. As further dis-
tances contribute less and less, middle distances
start to decay more and more, until at some point,
all distances but the closest start to decrease, head-
ing towards a degenerate solution. We therefore
suggest using the observation of several consecu-
tive decreases of all except ?
1
as an end criterion.
We used 10 consecutive steps for our experiments.
4 Experiments on Semeval-2007 English
Lexical Sample
The Semeval workshop holds WSD tasks such as
the English Lexical Sample (ELS) (Pradhan et al,
2007). It consists of a selected set of polysemous
words, contained within passages where a sense
taken from a sense inventory is manually anno-
tated. The task is to create supervised classifiers
maximizing accuracy on test data.
Since there are only 50 words and instances are
few, we judged there was not enough data to com-
pute weights. Instead, we used the AP Newswire
corpus of the TREC collection (CD 1 & 2). Words
376
were stemmed with the Porter stemmer and text
windows were grouped for all words. For sim-
plicity and efficiency, windows to the right and to
the left were considered independent, and we only
kept words with between 30 and 1000 windows.
Also, only windows with a size of 100, which was
considered big enough without any doubt, were
kept. A stop list of the top 10 frequent words was
used, but place holders were left in the windows to
preserve the distances. Multiple consecutive stop
words (ex: ?of the?) were merged, and the tar-
get word, being the same for all samples of a set,
was ignored. This results in 32,650 sets contain-
ing 5,870,604 windows. In Figure 1, we can see
the resulting weight curve.
0 20 40 60 80 100
distance
0.0
0.2
0.4
0.6
0.8
1.0
w
e
i
g
h
t
Figure 1: Weight curve for AP Newswire
Since the curve converges, words over the 100th
distance were assigned the minimumweight found
in the curve. From this we constructed NB models
whose class priors used an absolute discounting of
0.5. The collection language model used the con-
catenation of the AP collection and the Semeval
data. As the unstemmed target word is an impor-
tant feature it was added to the models. It?s weight
was chosen to be 0.7 by maximizing accuracy on
one-held-out cross-validation of the training data.
The results are listed in Table 1.
System Cross-Val (%) Test set (%)
Prior only 78.66 77.76
Best uniform 85.48 83.28
RALI-2 88.23 86.45
Table 1: WSD accuracy on Semeval-2007 ELC
We used two baselines: most frequent sense
(prior only), and the best uniform (except target
word) fixed size window found from extensive
search on the training data. The best settings were
a window of size 4, with a weight of 4.4 on the
target word and a Laplace smoothing of 2.9. The
improvements seen using our system are substan-
tial, beating most of the systems originally pro-
posed for the task (Pradhan et al, 2007). Out
of 15 systems, the best results had accuracies of
89.1*, 89.1*, 88.7, 86.9 and 86.4 (* indicates post-
competition submissions). Notice that most were
using Support Vector Machine (SVM) with bag-
of-word features in a very small window, local col-
locations and POS tags. In our future work, we
will investigate the applications of SVM with our
new term weighting scheme.
5 Experiments on Semeval-2010
Japanese WSD
The Semeval-2010 Japanese WSD task (Okumura
et al, 2010) consists of 50 polysemous words
for which examples were taken from the BC-
CWJ tagged corpus. It was manually segmented,
tagged, and annotated with senses taken from the
Iwanami Kokugo dictionary. The task is identical
to the ELS of the previous experiment.
Since the data was again insufficient to com-
pute curves, we used the Mainichi-2005 corpus of
NTCIR-8. We tried to reproduce the same kind
of segmentation as the training data by using the
Chasen parser with UniDic. For the corpus and
Semeval data, conjugations (setsuzoku-to, jod?-
shi, etc.), particles (all jo-shi), symbols (blanks,
kig?, etc.), and numbers were stripped. When a
base-form reading was present (for verbs and ad-
jectives), the token was replaced by the Kanjis
(chinese characters) in the word writing concate-
nated with the base-form reading. This treatment
is somewhat equivalent to the stemming+stop list
of the ELS tasks. The resulting curve can be seen
in Figure 2.
The NB models are the same as in the previous
experiments. Target words were again added the
same way as in the ELS task. The best fixed win-
dow model was found to have a window size of 1
with a target word weight of 0.6 and used manual
Dirichlet smoothing with a pseudo-count of 110.
We submited two systems with the following set-
tings: RALI-1 used manual Dirichlet smoothing
and 0.9 for the target word. RALI-2 used auto-
377
0 20 40 60 80 100
distance
0.0
0.2
0.4
0.6
0.8
1.0
w
e
i
g
h
t
Figure 2: Weight curve for Mainichi Shinbun 2005
matic Dirichlet smoothing and 1.7 for the target
word weight. Results are listed in Table 2.
System Cross-Val (%) Test set (%)
prior only 75.23 68.96
Best uniform 82.29 76.12
RALI-1 82.77 75.92
RALI-2 83.05 76.36
Table 2: WSD accuracy on Semeval-2010 JWSD
As we can see, the results are not significantly
different from the best uniform model. This may
be due to differences in the segmentation parame-
ters of our external corpus. Another reason could
be that the systems use almost the same weights:
the best fixed window had size 1, and the Japanese
curve is steeper than the English one.
This steeper curve can be explained by the
grammatical structure of the Japanese language.
While English can be considered a Subject-
Verb-Complement language, Japanese is consid-
ered Subject-Complement-Verb. Verbs are mostly
found at the end of the sentence, far from their sub-
ject, and vice versa. The window distance is there-
fore less useful in Japanese than in English since
it has more non-local dependencies. These results
show that the curves work as expected even in dif-
ferent languages.
6 Conclusions
This paper proposed an unsupervised method for
finding weights for counts in text windows ac-
cording to their distance to the target word. Re-
sults from the Semeval-2007 English lexical sam-
ple showed a substantial improvement in preci-
sion. Yet, as we have seen with the Japanese task,
window distance is not always a good indicator of
word relatedness. Fortunately, we can easily imag-
ine extensions to the current scheme that bins word
counts by factors other than word distance. For in-
stance, we could also bin counts by parsing tree
distance, sentence distance or POS-tags.
Acknowledgments
The authors would like to thank Florian Boudin
and Satoko Fujisawa for helpful comments on
this work. This work is partially supported
by Japanese MEXT Grant-in-Aid for Scientific
Research on Info-plosion (#21013046) and the
Japanese MEXT Research Student Scholarship
program.
References
Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh. 2007.
Nus-ml: improving word sense disambiguation us-
ing topic features. In SemEval ?07 Proceedings,
pages 249?252, Morristown, NJ, USA. Association
for Computational Linguistics.
Jianfeng Gao, Ming Zhou, Jian-Yun Nie, Hongzhao
He, and Weijun Chen. 2002. Resolving query trans-
lation ambiguity using a decaying co-occurrence
model and syntactic dependence relations. In SI-
GIR ?02 Proceedings, pages 183?190, New York,
NY, USA. ACM.
Nancy Ide and Jean V?ronis. 1998. Introduction to
the special issue on word sense disambiguation: the
state of the art. Comput. Linguist., 24(1):2?40.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Comput. Surv., 41(2):1?69.
Manabu Okumura, Kiyoaki Shirai, Kanako Komiya,
and Hikaru Yokono. 2010. Semeval-2010 task:
Japanese wsd. In SemEval ?10 Proceedings. Associ-
ation for Computational Linguistics.
Sameer S. Pradhan, Edward Loper, Dmitriy Dligach,
and Martha Palmer. 2007. Semeval-2007 task 17:
English lexical sample, srl and all words. In Se-
mEval ?07 Proceedings, pages 87?92, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Jinxi Xu and W. Bruce Croft. 1998. Corpus-
based stemming using cooccurrence of word vari-
ants. ACM Trans. Inf. Syst., 16(1):61?81.
ChengXiang Zhai and John Lafferty. 2002. Two-stage
language models for information retrieval. In SIGIR
?02 Proceedings, pages 49?56, NewYork, NY, USA.
ACM.
378
