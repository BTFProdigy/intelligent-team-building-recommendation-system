Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 116?127, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Linking Named Entities to Any Database
Avirup Sil?
Temple University
Philadelphia, PA
avi@temple.edu
Ernest Cronin?
St. Joseph?s University
Philadelphia, PA
ernest.cronin@gmail.com
Penghai Nie
St. Joseph?s University
Philadelphia, PA
nph87903@gmail.com
Yinfei Yang
St. Joseph?s University
Philadelphia, PA
yangyin7@gmail.com
Ana-Maria Popescu
Yahoo! Labs
Sunnyvale, CA
amp@yahoo-inc.com
Alexander Yates
Temple University
Philadelphia, PA
yates@temple.edu
Abstract
Existing techniques for disambiguating named
entities in text mostly focus on Wikipedia as
a target catalog of entities. Yet for many
types of entities, such as restaurants and
cult movies, relational databases exist that
contain far more extensive information than
Wikipedia. This paper introduces a new task,
called Open-Database Named-Entity Disam-
biguation (Open-DB NED), in which a system
must be able to resolve named entities to sym-
bols in an arbitrary database, without requir-
ing labeled data for each new database. We
introduce two techniques for Open-DB NED,
one based on distant supervision and the other
based on domain adaptation. In experiments
on two domains, one with poor coverage by
Wikipedia and the other with near-perfect cov-
erage, our Open-DB NED strategies outper-
form a state-of-the-art Wikipedia NED system
by over 25% in accuracy.
1 Introduction
Named-entity disambiguation (NED) is the task of
linking names mentioned in text with an established
catalog of entities (Bunescu and Pasca, 2006; Rati-
nov et al2011). It is a vital first step for se-
mantic understanding of text, such as in grounded
semantic parsing (Kwiatkowski et al2011), as
well as for information retrieval tasks like person
name search (Chen and Martin, 2007; Mann and
Yarowsky, 2003).
NED requires a catalog of symbols, called refer-
ents, to which named-entities will be resolved. Most
NED systems today use Wikipedia as the catalog of
referents, but exclusive focus on Wikipedia as a tar-
get for NED systems has significant drawbacks: de-
spite its breadth, Wikipedia still does not contain all
or even most real-world entities mentioned in text.
As one example, it has poor coverage of entities that
are mostly important in a small geographical region,
such as hotels and restaurants, which are widely dis-
cussed on the Web. 57% of the named-entities in
the Text Analysis Conference?s (TAC) 2009 entity
linking task refer to an entity that does not appear
in Wikipedia (McNamee et al2009). Wikipedia is
clearly a highly valuable resource, but it should not
be thought of as the only one.
Instead of relying solely on Wikipedia, we pro-
pose a novel approach to NED, which we refer to
as Open-DB NED: the task is to resolve an en-
tity to Wikipedia or to any relational database that
meets mild conditions about the format of the data,
described below. Leveraging structured, relational
data should allow systems to achieve strong accu-
racy, as with domain-specific or database-specific
NED techniques like Hoffart et al NED system
for YAGO (Hoffart et al2011). And because of
the availability of huge numbers of databases on
the Web, many for specialized domains, a success-
ful system for this task will cover entities that a
Wikipedia NED or database-specific system cannot.
We investigate two complementary learning
strategies for Open-DB NED, both of which signifi-
cantly relax the assumptions of traditional NED sys-
tems. The first strategy, a distant supervision ap-
proach, uses the relational information in a given
database and a large corpus of unlabeled text to
learn a database-specific model. The second strat-
116
egy, a domain adaptation approach, assumes a sin-
gle source database that has accompanying labeled
data. Classifiers in this setting must learn a model
that transfers from the source database to any new
database, without requiring new training data for the
new database. Experiments show that both strategies
outperform a state-of-the-art Wikipedia NED sys-
tem by wide margins without requiring any labeled
data from the test domain, highlighting the signifi-
cant advantage of having domain-specific relational
data.
The next section contrasts Open-DB NED with
previous work. Section 3 formalizes the task. Sec-
tions 4 and 5 present our distant supervision strategy
and domain-adaptation strategy, respectively. Sec-
tion 6 introduces a technique that is a hybrid of the
two learning strategies. Section 7 describes our ex-
periments, and Section 8 concludes.
2 Previous Work
As mentioned above, restricting the catalog of ref-
erents to Wikipedia, as most recent NED systems
do (Bunescu and Pasca, 2006; Mihalcea and Cso-
mai, 2007; Fader et al2009; Han and Zhao, 2009;
Kulkarni et al2009; Ratinov et al2011), can re-
strict the coverage of the system. Zhou et al2010)
estimate that 23% of names in Yahoo! news arti-
cles have no referent in Wikipedia, and Cucerzan
(2007) estimates the rate at 16% in MSNBC news
articles. There is reason to suspect that these esti-
mates are on the low side, however, as news tends to
cover popular entities, which are most likely to ap-
pear in Wikipedia; the mentions in TAC?s 2009 en-
tity linking task are drawn from both newswire and
blogs, and have a far higher rate (57%) of missing
Wikipedia entries. Lin et al2012) find that 33% of
mentions in a corpus of 500 million Web documents
cannot be linked to Wikipedia.
NED systems that are focused on specific do-
mains (or verticals) greatly benefit from reposito-
ries of domain-specific knowledge, only a subset
of which may be found in Wikipedia. For exam-
ple, Pantel and Fuxman (2011) use a query-click
graph to resolve names in search engine queries to a
large product catalog from a commercial search en-
gine, while Dalvi et al2009; 2012) focus on movie
and restaurant databases. Bellare and McCallum
(2009) use the sequence information available in ci-
tation text to link author, title, and venue names to a
publication database. Open-DB NED systems work
on any database, so they can serve as baselines for
domain-specific NED tasks, as well as provide dis-
ambiguation for domains where no domain-specific
NED system exists.
Numerous previous studies have considered dis-
tant or weak supervision from a single relational
database as an alternative to manual supervision for
information extraction (Hoffmann et al2011; Weld
et al2009; Bellare and McCallum, 2007; Bunescu
and Mooney, 2007; Mintz et al2009; Riedel et al
2010; Yao et al2010). In contrast to these sys-
tems, our distant supervision NED system provides
a meta-algorithm for generating an NED system for
any database and any entity type.
Existing domain adaptation or transfer learning
approaches are inappropriate for the Open-DB NED
task, either because they require labeled data in both
the source and target domains (Daume? III et al
2010; Ben-David et al2010), or because they lever-
age some notion of distributional similarity between
words in the source and target domains (Blitzer et
al., 2006; Huang and Yates, 2009), which does not
apply to the database symbols across the two do-
mains. Instead, our domain adaptation technique
uses domain-independent features of relational data,
which apply regardless of the actual contents of the
database, as explained further below.
3 The Open-DB NED Problem and
Assumptions
3.1 Problem Formulation
A mention is an occurrence of a named-entity
in a document. Formally, a mention m =
(d, start, end) is a triple consisting of a document
d, as well as a start and end position for the men-
tion within the document. We say that d is the
context of m. A relational database is a 2-tuple
(S,R). Here, S is a set of symbols for constants,
attributes, and relations in the database, and R =
{r1, . . . , rn} is a set of relation instances of the form
ri = {(c1,1, . . . , c1,ki), . . . , (cni,1, . . . , cni,ki)},
where each cj is taken from S, ki is the arity of re-
lation ri and ni is the number of known instances
of ri. We will write example database symbols in
117
movie 
id title year 
1 Next Door 1975 
2 Next Door 2005 
3 Next Door 2008 
4 Next Door 2008 
5 Next Door 2010 
? ? ? 
actor 
id name 
1 Nicole Kreux 
2 Richard Ryan 
3 Kristoffer Joner 
4 Lee Perkins 
5 Carla Valentine 
? ? 
acted_in 
movie_id actor_id role 
5 1 Evelyn 
5 2 Bruce 
2 3 John 
1 4 Kid 
3 5 Elana 
? ? ? 
player 
id name height position 
1 Carlos Lee 6?2? LF 
2 Rob Bironas 6?0? K 
3 Chris Johnson 6?3? 3B 
4 Chris Johnson 5?11? RB 
5 Chris Johnson 6?1? DB 
? ? ? 
team 
id name 
1 San Diego Padres 
2 Houston Texans 
3 Tennessee Titans 
4 Oakland Raiders 
5 Houston Astros 
? ? 
plays_for 
player_id team_id 
4 3 
5 2 
3 5 
1 5 
2 3 
? ? 
Figure 1: Example movie database (above) and sports
database (below) in BCNF.
teletype, and mentions in ?quotations.? For a
particular database DB, we refer to its components
as DB.S and DB.R. For a set of databases D, de-
fine the set of referents as SD = (
?
DB?DDB.S)?
{OOD}, where OOD is a special symbol indicat-
ing something that is ?out of database?, or not found
in any of the databases in D.
Given a corpus C, a set of mentions M that oc-
cur in C, and a set of databases D, the Open-DB
NED task is to produce a function f : M ? SD,
which identifies an appropriate target symbol from
one of the databases in D, or determines that the
mention is OOD. Note that this problem formula-
tion assumes no labeled data. This is significantly
more challenging than traditional NED settings, but
allows the system to generalize easily to any new
database. In the domain adaptation section below,
we relax this condition somewhat, to allow labeled
data for a small number of initial databases; the sys-
tem must then transfer what it learns from the la-
beled domains to any new database. Also note that
the focus for this paper is disambiguation; we as-
sume that the set of mentions are correctly demar-
cated in the input text. Previous systems, such as
Lex (Downey et al2007), have investigated the task
of finding correct named-entity boundaries in text.
3.2 Assumptions
To allow our systems to handle arbitrary databases,
we need to make some assumptions about a standard
format for the data. We will assume that databases
are provided in a particular form, called Boyce-Codd
Normal Form (BCNF) (Silberschatz et al2010).
A relational schema is said to be in BCNF when
all redundancy based on functional dependency has
been removed, although other types of redundancy
may still exist. Formally, a schema R is said to
be in BCNF with respect to a set of functional de-
pendencies F if for every one of the dependencies
(X ? Y ) ? F , either
1. Y ? X , meaning this is a trivial functional de-
pendency, or
2. X is a superkey, meaning that X is a set of at-
tributes that together define a unique ID for the
relation.
In practice, this is a relatively safe assumption as
database designers often aim for even stricter normal
forms. For databases not in BCNF, such as tables
extracted from Web pages, standard algorithms ex-
ist for converting them into BCNF, given appropri-
ate functional dependencies, although there are sets
of functional dependencies for which BCNF is not
achievable. Figure 1 shows two example databases
in BCNF. We use these tables as examples through-
out the paper.
We will additionally assume that all attributes, in-
cluding names and nicknames, of entities that are
covered by the database are treated as functional de-
pendencies of the entity. Again, in practice, this
is a fairly safe assumption as this is part of good
database design, but if a database does not con-
form to this, then there will be some entities in the
database that our algorithms cannot resolve to. This
assumption implies that it is enough to use the set of
superkeys for relations as the set of possible refer-
ents; our algorithms make use of this fact.
Finally, we will assume the existence of a func-
tion ?(s, t) which indicates whether the text t is a
valid surface form of database symbol s. Our exper-
iments in Section 7.3 explore several possible simple
definitions for this function.
4 A Distant Supervision Strategy for
Open-DB NED
Our first approach to the Open-DB NED problem re-
lies on the fact that, while many mentions are indeed
ambiguous and difficult to resolve correctly, most
118
mentions have only a very small number of possi-
ble referents in a given database. ?Chris Johnson?
is the name of doubtless thousands of people, but
for articles that are reasonably well-aligned with our
sports database, most of the time the name will refer
to just three different people. Most sports names are
in fact less ambiguous still. Thus, taking a corpus of
unlabeled sports articles, we use the information in
the database to provide (uncertain) labels, and then
train a log-linear model from this probabilistically-
labeled data.
This strategy requires a set of features for the
model. Traditionally, such features would be hand-
crafted for a particular domain and database. As a
first step towards our Open-DB system, we present
a log-linear model for disambiguation, as well as a
simple feature-generation algorithm that produces a
large set of useful features from a BCNF database.
We then present a distant-supervision learning pro-
cedure for this model.
4.1 Disambiguation Model
Let SD be the set of possible referents. We construct
a vector of feature functions f(m, s) describing the
degree to which m and s ? SD appear to match
one another. The feature functions are described be-
low. The model includes a vector of weights w, one
weight per feature function, and sets the probability
of entity s given m and w as:
P (s|m,w) =
exp (w ? f(m, s))
?
s??SD
exp (w ? f(m, s?))
(1)
4.2 Database-driven Feature Generation
Figure 2 shows our algorithm for automatically gen-
erating feature functions fi(m, s) from a BCNF
database. As mentioned above, we only need to con-
sider resolving to database symbols s that are keys,
or unique IDs, for some tuple in a database. For
an entity in the database with key id, the feature
generation algorithm generates two types of feature
functions: attribute counts and similar entity counts.
Each of these features measures the similarity be-
tween the information stored in the database about
the entity id, and the information in the text in d sur-
rounding mention m.
An attribute count feature function fatti,j (m, id)
for the jth attribute of relation ri counts how many
Algorithm: Feature Generation
Input: DB, a database in BCNF
Output: F, a set of feature functions
Initialization: F? ?
Attribute Count Feature Functions:
For each relation ri ? DB.R
For each j in {1, . . . , ki}
Define function fatti,j (m, id):
count? 0
Identify the tuple t ? ri containing id
val? tj
count? count +
ContextMatches(val,m)
return count
F? F ? {fatti,j }
Similar-Entity Count Feature Functions:
For each relation ri ? DB.R
For each j in {1, . . . , ki}
Define function fsimi,j (m, id):
count? 0
Identify the tuple t ? ri containing id
val? tj
Identify the set of similar tuples T ?:
T ? = {t?|t? ? ri, t?j = val}
For each tuple t? ? T ?
For each j? ? {1, . . . , ki}
val? ? t?j
count? count +
ContextMatches(val?,m)
return count
F? F ? {fsimi,j }
Figure 2: Feature generation algorithm. The
ContextMatches(s,m) function counts how many
times a string that matches database symbol s appears
in the context of m. In our implementation, we use all
of d(m) as the context. Matching between strings and
database symbols is discussed in Sec. 7.3.
attributes of the entity id appear near m. For exam-
ple, if id is 5 in the movie relation in Figure 1, the
feature function for attribute year would count how
often 2010 matches the text surrounding mention
m. Defining precisely whether a database symbol
?matches? a word or phrase is a subtle issue; we ex-
plore several possibilities in Section 7.3. In addition
119
to attribute counts for attributes within a single rela-
tion, we also use attributes from relations that have
been inner-joined on primary key and foreign key
pairs. For example, for movies, we include attributes
such as director name, genre, and actor name. High
values for these attribute count features indicate that
the text around m closely matches the information
in the database about entity id, and therefore id is a
strong candidate for the referent of m. We use the
whole document as the context for finding matches,
although other variants are worth future investiga-
tion.
A similar entity count feature function
fsimi,j (m, id) for the jth attribute in relation ri
counts how many entities similar to id are men-
tioned in the neighborhood of m. As an example,
consider a mention of ?Chris Johnson?, id = 3,
and the similar entity feature for the position
attribute of the players relation in the sports
database. The feature function would first identify
that 3B is the position of the player with id = 3. It
would then identify all players that had the same
position. Finally, it would count how often any
attributes of this set of players appear near ?Chris
Johnson?. Likewise, the similar entity feature for
the team id attribute would count how many
teammates of the player with id = 3 appear near
?Chris Johnson?. A high count for this teammate
feature is a strong clue that id is the correct referent
for m, while a high count for players of the same
position is a weak but still valuable clue.
4.3 Parameter Estimation via Distant
Supervision
Using string similarity, we can heuristically deter-
mine that three IDs with name attribute Chris
Johnson are highly likely to be the correct target
for a mention of ?Chris Johnson?. Our distant su-
pervision parameter estimation strategy is to move
as much probability mass as possible onto the set
of realistic referents obtained via string similarity.
Since our features rely on finding attributes and sim-
ilar entities, the side effect of this strategy is that
most of the probability mass for a particular mention
is moved onto the one target ID with high attribute
count and similar entity count features, thus disam-
biguating the entity. Although the string-similarity
heuristic is typically noisy, the strong information in
the database and the fact that many entity mentions
are typically not ambiguous allows the technique to
learn effectively from unlabeled text.
Let ?(m,DB) be a heuristic string-matching
function that returns a set of plausible ID values in
databaseDB for mentionm. The objective function
for this training procedure is a modified marginal log
likelihood (MLL) function that encourages probabil-
ity mass to be placed on the heuristically-matched
targets:
MLL(M,w) =
?
m?M
log
?
id??(m,DB)
P (id|m,w)
This objective is smooth but non-convex. We use
a gradient-based optimization procedure that finds a
local maximum. Our implementation uses an open-
source version of the LBFG-S optimization tech-
nique (Liu and Nocedal, 1989). The gradient of our
objective is given by
?LL(M,w)
?wi
=
?
m?M
Eid??(m,DB) [fi(m, id)]
?Eid?DB.S [fi(m, id)]
where the expectations are taken according to
P (id|m,w).
5 A Domain-Adaptation Strategy for
Open-DB NED
Our domain-adaptation strategy builds an Open-DB
NED system by training it on labeled examples from
an initial database or small set of initial databases.
Unlike traditional NED, however, the purpose in
Open-DB NED is to resolve to any database. Thus
the strategy must take care to build a model that
can transfer what it has learned to a new database,
without requiring additional labeled data for the new
database.
At first, the problem seems intractable ? just
because a system can disambiguate between ?Next
Door?, the 2005 Norwegian film, and ?Next Door?,
the 1975 short film by director Andrew Silver, that
seems to provide little benefit for disambiguating be-
tween different athletes named ?Andre Smith.? The
crux of the problem lies in the fact that database-
driven features are domain-specific. Counting how
many times the director of a movie appears is highly
120
useful in the movie domain, but worthless in the
sports domain.
Our solution works by re-defining the problem in
such a way that we can define domain-independent
and database-independent features. For example,
rather than counting how often the director of
a movie appears in the context around a movie
mention, we create a domain-independent Count
Att(m, s) feature function that counts how often any
attribute of s appears in the context of m. For
movies, Count Att will add together counts for ap-
pearances of a movie?s production year and IMDB
rating, among other attributes. In the sports domain,
Count Att will add together counts for appearances
of a player?s height, position, salary, etc.. But in ei-
ther domain, the feature is well-defined, and in either
domain, larger values of the feature indicate a better
match between m and s. Thus there is a hope for
training a model with domain-independent features
like Count Att on labeled data from one domain, say
movies, and producing a model that has high accu-
racy on the sports domain.
We first formalize the notion of a domain adap-
tation NED model, and then describe our algorithm
for producing such a model. We say that a domain
consists of a database DB as well as a distribution
D(M), whereM is the space of mentions. For in-
stance, the movie domain might consist of the Inter-
net Movie Database (IMDB) and a distribution that
places most probability mass on documents about
movies and Hollywood stars. In domain adapta-
tion, a system observes a set of training examples
(m, s, g(m, s)), where instances m ? M are drawn
from a source domain?s distribution DS and refer-
ents s are drawn from the source domain?s database
DBS . The labels g(m, s) are boolean values in-
dicating a correct or incorrect match between the
mention and referent. The system must then learn
a hypothesis for classifying examples (m, s) drawn
from a target domain?s distributionDT and database
DBT . Note that for domain adaptation, we can-
not use the more traditional problem formulation in
which the referent s is a label (i.e., s = g(m)) for the
mention, since the set of possible referents changes
from domain to domain, and therefore the output of
g would be completely different from one domain to
the next.
Table 1 lists the domain-independent features
Domain-Independent Feature Functions
Count Att:
?
i,j f
att
i,j (m, s)
Count Sim:
?
i,j f
sim
i,j (m, s)
Count All: Count Att + Count Sim
Count Unique:
?
i,j
{
0 if fatti,j (m, s) = 0,
1 if fatti,j (m, s) > 0.
Count Num:
?
i,j|jis a numeric att. f
att
i,j (m, s)
Table 1: Primary feature functions for a domain adapta-
tion approach to NED. These features made the biggest
difference in our experiments, but we also tested varia-
tions such as counting unique numeric attribute appear-
ances, counting unique similar entities, counting relation
name appearances, counting extended attributed appear-
ances, and others.
used in our domain adaptation model. These fea-
tures use the attribute counts and similar entity
counts from the distant supervision model as subrou-
tines. By aggregating over those domain-dependent
feature functions, the domain adaptation system ar-
rives at feature functions that can be defined for any
database, rather than for a specific database.
Note that there is a tradeoff between the do-
main adaptation technique and the distant super-
vision technique. The domain adaptation model
has access to labeled data, unlike the distant su-
pervision model. In addition, the domain adapta-
tion model requires no text whatsoever from the tar-
get domain, not even an unlabeled corpus, to set
weights for the target domain. Once trained, it is
ready for NED over any database that meets our as-
sumptions, out of the box. However, because the
model needs to be able to transfer to arbitrary new
domains, the domain adaptation model is restricted
to domain-independent features, which are ?coarser-
grained.? That is, the distant supervision model has
the ability to place more weight on attributes like
director rather than genre, or team rather than po-
sition, if those attributes are more discriminative.
The domain adaptation model cannot place differ-
ent weights on the different attributes, since those
weights would not transfer across databases.
As with distant supervision, the domain adapta-
tion strategy uses a log-linear model over these fea-
ture functions. We use standard techniques for train-
ing the model using labeled data from the source do-
121
main: conditional log likelihood (CLL) as the objec-
tive function, and LBFG-S for convex optimization.
CLL(L,w) =
?
(m,id,label)?L
logP (label|m, id,w)
The training algorithm is guaranteed to converge to
the globally optimal parameter setting for this objec-
tive function over the training data. The manually
annotated data contains only positive examples; to
generate negative examples, we use the same name-
matching heuristic ?(m,DB) to identify a set of po-
tentially confusing bad matches. On test data, we
use the trained model to choose the id for a given m
with the highest probability of being correct.
6 A Hybrid Model
The distant supervision and domain adaptation
strategies use two very different sources of evidence
for training a disambiguation classifier: the string-
matching heuristic and unlabeled text from the target
domain for the the distant supervision model, and
aggregate features over labeled text from a separate
domain for domain adaptation. This begs the ques-
tion, do these sources of evidence complement one
another? To address this question, we design a Hy-
brid model with features and training strategies from
both distant supervision and domain adaptation.
The training data consists of a set LS of labeled
mentions from a source domain, a source database
DBS , a set of unlabeled mentions MT from the tar-
get domain, and the target-domain database DBT .
The full feature set of the Hybrid model is the union
of the distant supervision feature functions for the
target domain and the domain-independent domain
adaptation feature functions. Note that the distant
supervision feature functions are domain-specific,
so they almost always will be uniformly zero on LS ,
but the domain adaptation feature functions will be
activated on both LS and MT . The combined train-
ing objective for the Hybrid model is:
LL(LS ,MT ,w) = CLL(LS ,w) +MLL(MT ,w)
7 Experiments
Our experiments compare our strategies for Open-
DB NED against one another, as well as against a
Wikipedia NED system from previous work, on two
domains: sports and movies.
7.1 Data
For the movie domain, we collected a set of
156 cult movie titles from an online movie site
(www.olivefilms.com). For each movie title, we ex-
ecuted a Web search using a commercial search en-
gine, and collected the top five documents for each
title from the search engine?s results. Nearly all top-
five results included at least one mention of an en-
tity not found in Wikipedia; overall, only 16% of the
mentions could be linked to Wikipedia. After strip-
ping javascript and html annotations, we removed
documents with fewer than 50 words, leaving a to-
tal of 770 documents. We select one occurrence of
any of the 156 movie titles from each document as
our set of mentions. Many titles are ambiguous not
just among different movies with the same name, but
also among novels, plays, geographical entities, and
assorted other types of entities. To provide labels for
these mentions, we use both a movie database and
Wikipedia. We downloaded the complete data dump
from the online Internet Movie Database (IMDB,
www.imdb.com). For our set of possible referents,
we use the set of all key values in IMDB, and the set
of all Wikipedia articles. Annotators manually la-
beled each mention using this set of referents. Table
2 shows summary statistics about this labeled data.
For the sports domain, we downloaded all player
data from Yahoo!, Inc.?s sports database for the
years 2011-2012 and two American sports leagues,
the National Football League (NFL) and Major
League Baseball (MLB). From the database, we ex-
tracted ambiguous player names and team names,
including names like ?Philadelphia? which may re-
fer to Philadelphia Eagles in the NFL data,
Philadelphia Phillies in the MLB data, or
the city of Philadelphia itself (in both types of
data). We then collected 1300 Yahoo! news arti-
cles which include a mention that partially matches
at least one of these database symbols. We manu-
ally labeled a random sample of 564 mentions from
this data, including 279 player name mentions and
285 city name mentions. Many player name and
place name mentions are ambiguous between the
two sports leagues, as well as with teams or play-
ers from other leagues. In order to focus on the
hardest cases, we specifically exclude mentions like
?Philadelphia? from the labeled data if any of their
122
domain |M | E|?(m,DB)| OOD Wiki
movies 770 2.6 13% 16%
sports 549 4.5 0% 100%
Table 2: Number of mentions, average number of refer-
ents per mention, % of mentions that are OOD, and %
of mentions that are in Wikipedia in our movie and sports
data.
unambiguous completions appears in the same arti-
cle (that is, if either of the team names ?Philadelphia
Eagles? or ?Philadelphia Phillies? appears in the
same article, we exclude the ?Philadelphia? men-
tion). As before, the set of possible referents in-
cludes the symbol OOD, key values from the sports
database, and Wikipedia articles, and a given men-
tion may be labeled with both a sports entity and a
Wikipedia article, if appropriate. All of our data is
available from the last author?s website.
7.2 Evaluation Metric
We report on a version of exact-match accuracy. The
system chooses the most likely label s? for each m.
This is judged correct if s? matches the correct label
s exactly, or (in cases where both a Wikipedia and a
database entity are considered correct) if one of the
labels matches s? exactly. This metric allows systems
to resolve against either reference, Wikipedia or an-
other database, without requiring it to match both if
the same entity appears in both references.
7.3 Exact or Partial Matching?
One important question in the design of our systems
is how to determine the ?match? between database
symbols and text. This question comes into play in
two components of our systems: it affects the com-
putation of feature functions that count how often a
match of some attribute is found in text, and it af-
fects which set of heuristically-determined database
entities are considered to be possible matches for a
given mention.
We experiment with two different matching
strategies between a symbol s and text t, exact
matching and partial matching. Exact matching
?exact(s, t) requires the sequence of characters in s
to appear exactly (modulo character encoding) in t.
For instance, the database value Chris Johnson
System Accuracy
No-Wikipedia Domain Adapt. 0.61
DocSim-Wikipedia Domain Adapt. 0.69
Table 3: Including a simple document-similarity feature
for comparing a mention?s context with a Wikipedia page
provides an 8% improvement over ignoring Wikipedia in-
formation.
would match ?Chris Johnson?, but not ?C. John-
son? or ?Johnson? in text. For partial matching,
we used different tests for numeric and textual en-
tities. For numeric entities, ?partial matched s and
t if the numeric value of one was within 10% of
the other, so that 5312 would match ?5,000.? We
made no attempt to convert numeric phrases, such
as ?3.6 million?, into numeric values. For textual
entities, ?partial matched s and t if at least one
token from each matched exactly. Thus Chris
Johnson matches both ?Chris? and ?C. Johnson?.
We found ?partial to be consistently superior for
computing ?(m,DB), since it has much better re-
call for mentions like ?Philadelphia?. On the other
hand, if we use ?partial for computing our models?
feature functions, like the Count Att(m, s) in the do-
main adaptation model, counts varied widely across
domains. A simple version of the domain adapta-
tion classifier (only the Count All and Count Unique
features) trained on sports data and tested on movies
achieved an accuracy of 24% using ?partial, com-
pared with 61% using ?exact. For all remaining
tests, we used ?exact for computing features, and
?partial for computing ?(m,DB).
7.4 Incorporating Wikipedia referents
Thus far, all of our features work on relational data,
not Wikipedia. In order to allow our systems to link
to Wikipedia, we create a single ?document simi-
larity? feature describing the similarity between the
text around a mention and the text appearing on a
Wikipedia page. We build a vector space model of
both the document containing the mention and the
Wikipedia page, remove stopwords, and use cosine
similarity to compute this feature.
To evaluate the effectiveness of this Wikipedia
feature, we tested two versions of our domain adap-
tation system, both trained on sports data and tested
123
0.13 
0.4 0.43 
0.54 0.65 
0.71 0.72 0.73 
0 
0.21 
0.33 
1 
0.54 0.63 0.62 
0.66 
00.1
0.20.3
0.40.5
0.60.7
0.80.9
1
Accu
racy
 
Open-DB NED Test 
Movies Sports
Figure 3: All three Open-DB NED strategies out-
perform a state-of-the-art Wikipedia NED system by
25% or more on sports and movies, and outperform
a Wikipedia NED system with oracle information by
14% or more on the movie data. Differences between
the Modified Zhou Wikifier and the Open-DB strategies
are statistically significant (p < 0.01, Fisher?s exact test)
on both domains.
on the movies domain. The first version involves
no Wikipedia information whatsoever, thus it has no
reason to select a Wikipedia article over OOD. The
second system includes the document similarity fea-
ture. Table 3 shows the results of these systems. En-
couragingly, our single document similarity feature
produces a significant improvement over the model
without Wikipedia information, so we use this fea-
ture in all of our systems tested below. More so-
phisticated use of Wikipedia is certainly possible,
and an important question for future work is how
to combine Open-DB NED more seamlessly with
Wikipedia NED.
7.5 Comparing Open-DB NED Strategies
For each domain, we compare our domain-
adaptation strategy, distant supervision, and hy-
brid strategies. The domain-adaptation model is
trained on the labeled data for sports when testing
on movies, and vice versa. We use a movies test set
of 180 mentions that is separate from the develop-
ment data used for the above tests. For the distant
supervision strategy, we use the entire collection of
texts from each domain as input (1300 articles for
sports, 770 articles for movies), with the labels re-
moved during training.
We compare against a state-of-the-art Wikipedia
NED system used in production by a major Web
company. This system is a modified version of the
system described by Zhou et al2010), where cer-
tain features have been removed for efficiency. We
refer to this as the Modified-Zhou Wikifier. This
system uses a gradient-boosted decision tree and
multiple local and global features for computing
the similarity between a mention?s context and a
Wikipedia article. We also test a hypothetical sys-
tem, Oracle Wikifier, which is given no information
about entities in IMDB, but is assumed to be able
to correctly resolve any mention that refers to an
entity found in Wikipedia. Thus, this system has
perfect accuracy on mentions that can be found in
Wikipedia, and accuracy similar to a baseline that
predicts randomly on all mentions that fall outside
of Wikipedia1. Oracle-Wikifier serves as an upper
bound on systems that have no access to a domain-
specific database. In addition, we compare against
two standard baselines: a classifier that always pre-
dicts OOD, and a classifier that chooses randomly.
Finally, we compare against a system that trains the
domain adaptation model using distant supervision
(?DA Trained with DS?).
Figure 3 shows our results. All three Open-DB
approaches outperform the baseline techniques on
this test by wide margins, with the Hybrid model in-
creasing by 30% or more over the random baseline.
On the movie domain, the Hybrid model outper-
forms the Oracle Wikifier by nearly 20%. Encour-
agingly, the Hybrid model consistently outperforms
both distant supervision and domain adaptation, sug-
gesting that the two sources of evidence are partially
complementary. Distant supervision performs better
on the movies test, whereas domain adaptation has
the advantage on sports. The differences among all
three Open-DB approaches is relatively small, com-
pared with the difference between these approaches
and Oracle Wikifier on the movie data.
The domain adaptation system outperforms DA
Trained with DS on both domains, suggesting
that labeled data from a separate domain is bet-
ter evidence for parameter estimates than unlabeled
data from the same domain. The distant super-
vision system also outperforms DA Trained with
1Alternatively, one could make the oracle system predict
OOD on all mentions that fall outside of Wikipedia. Random
predictions perform better on our data.
124
DS on both domains, suggesting that the fine-
grained, domain-specific features do in fact provide
more helpful information than the coarser-grained,
domain-independent features of the domain adapta-
tion model.
All of the Open-DB NED systems outperform the
Modified Zhou Wikifier on both data sets by a wide
margin. In fact the Modified Zhou Wikifier has sim-
ilar results on both domains, despite the fact that
Wikipedia has far greater coverage on sports than
movies. In part, the poor performance of the Modi-
fied Zhou Wikifier reflects the difficult nature of the
task. In previous experiments on an MSNBC news
test set it reached 85% accuracy, but a random clas-
sifier there achieved 60% accuracy compared with
21% on our sports data. Another difficulty with
the Modified Zhou Wikifier is its strong preference
for globally common entities. It consistently clas-
sifies mentions that are ambiguous between a city
and a team (like ?Chicago? in ?Chicago sweeps the
Red Sox?) as cities when they should be resolved
to teams, in large part because Chicago is a more
common referent in general text than either of the
baseball teams that play in that city. In sports arti-
cles, however, both meanings are common, and only
the surrounding context can help determine the cor-
rect referent.
Besides wikifiers, NED systems may also be
compared with dictionary-based word sense disam-
biguation techniques like the Lesk algorithm2 (Lesk,
1986). The Lesk algorithm is ?open? in the sense
that it works for arbitrary dictionaries, and it defines
a vector space model of the dictionary definitions
that may be likened to the attribute-value model in
our representation of entities in the database. Our
approach, however, estimates parameters for a sta-
tistical model from data, whereas the Lesk algorithm
uses an equal weight for all attributes. To make an
empirical comparison, we created a variant of the
Lesk algorithm for relational data: we took the dis-
ambiguation model from Eqn. 1, supplied all of
the features from the distant supervision model, and
manually set w = 1. This ?relational Lesk? model
achieves an accuracy of 0.11 on movies, and 0.15
on sports, significantly below the random baseline.
Giving equal weight to noisy attributes like genre
2We thank the reviewers for making this connection.
and more discriminative attributes like director
significantly hurts the performance.
For both the movie and sports domain, approx-
imately 80% of the Hybrid model?s errors are be-
cause of predicting database symbols, when the cor-
rect referent is a Wikipedia page or OOD. This
nearly always occurs because some words in the
context of a mention match an attribute of an in-
correct database referent. For instance, the crime
genre is an attribute for several movies, but it also
matches in contexts surrounding book titles and nu-
merous other entities. In the movie domain, most of
the remaining errors are incorrect OOD predictions
for mentions that should resolve to the database, but
the article contains no attributes or similar entities
to the database entity. In the sports domain, many
of the remaining errors were due to predicting in-
correct player referents. Quite often, this was be-
cause the document discusses a fantasy sports league
or team, where players from different professional
sports teams are mixed together on a ?fantasy team?
belonging to a fan of the sport. Since players in the
fantasy leagues have different teammates than they
do in the database, these articles consistently con-
fuse our methods.
8 Conclusion and Future Work
This paper introduces the task of Open-DB Named
Entity Disambiguation, and presents two distinct
strategies for solving this task. Experiments indicate
that a mixture of the two strategies significantly out-
performs a state-of-the-art Wikipedia NED system,
on a dataset where Wikipedia has good coverage and
on another dataset where Wikipedia has poor cover-
age. The results indicate that there is a significant
benefit to leveraging other sources of knowledge in
addition to Wikipedia, and that it is possible to lever-
age this knowledge without requiring labeled data
for each new source. The initial success of these
Open-DB NED approaches indicates that this task is
a promising area for future research, including ex-
citing extensions that link large numbers of domain-
specific databases to text.
Acknowledgments
This work was supported in part by a gift from Ya-
hoo!, Inc.
125
References
Kedar Bellare and Andrew McCallum. 2007. Learn-
ing extractors from unlabeled text using relevant data-
bases. In Sixth International Workshop on Information
Integration on the Web.
Kedar Bellare and Andrew McCallum. 2009. General-
ized Expectation Criteria for Bootstrapping Extractors
using Record-Text Alignment. In Empirical Methods
in Natural Language Processing (EMNLP-09).
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2010. A theory of learning from different
domains. Machine Learning, 79:151?175.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
Razvan Bunescu and Raymond Mooney. 2007. Learning
to extract relations from the web using minimal super-
vision. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL-
07).
R. Bunescu and M. Pasca. 2006. Using encyclope-
dic knowledge for named entity disambiguation. In
Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-06).
Ying Chen and James Martin. 2007. Towards Ro-
bust Unsupervised Personal Name Disambiguation. In
EMNLP, pages 190?198.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
708?716.
Nilesh N. Dalvi, Ravi Kumar, Bo Pang, and Andrew
Tomkins. 2009. Matching Reviews to Objects using a
Language Model. In EMNLP, pages 609?618.
Nilesh N. Dalvi, Ravi Kumar, and Bo Pang. 2012. Object
matching in tweets with spatial models. In WSDM,
pages 43?52.
Hal Daume? III, Abhishek Kumar, and Avishek Saha.
2010. Frustratingly easy semi-supervised domain
adaptation. In Proceedings of the ACL Workshop on
Domain Adaptation (DANLP).
D. Downey, M. Broadhead, and O. Etzioni. 2007. Lo-
cating complex named entities in web text. In Procs.
of the 20th International Joint Conference on Artificial
Intelligence (IJCAI 2007).
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2009. Scaling wikipedia-based named entity disam-
biguation to arbitrary web text. In Proceedings of
the WikiAI 09 - IJCAI Workshop: User Contributed
Knowledge and Artificial Intelligence: An Evolving
Synergy.
Xianpei Han and Jun Zhao. 2009. Named entity dis-
ambiguation by leveraging Wikipedia semantic knowl-
edge. In Proceeding of the 18th ACM Conference
on Information and Knowledge Management (CIKM),
pages 215?224.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino,
Hagen Furstenau, Manfred Pinkal, Marc Spaniol,
Bilyana Taneva, Stefan Thater, and Gerhard Weikum1.
2011. Robust Disambiguation of Named Entities in
Text. In EMNLP, pages 782?792.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
Based Weak Supervision for Information Extraction of
Overlapping Relations. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics (ACL).
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised se-
quence labeling. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and
Soumen Chakrabarti. 2009. Collective annotation
of wikipedia entities in web text. In Proceedings of
the 15th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (KDD), pages
457?466.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater,
and Mark Steedman. 2011. Lexical Generalization
in CCG Grammar Induction for Semantic Parsing. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP).
M.E. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
SIGDOC Conference.
Thomas Lin, Mausam, and Oren Etzioni. 2012. Entity
linking at web scale. In Knowledge Extraction Work-
shop (AKBC-WEKEX), 2012.
D.C. Liu and J. Nocedal. 1989. On the limited mem-
ory method for large scale optimization. Mathemati-
cal Programming B, 45(3):503?528.
G.S. Mann and D. Yarowsky. 2003. Unsupervised per-
sonal name disambiguation. In CoNLL.
Paul McNamee, Mark Dredze, Adam Gerber, Nikesh
Garera, Tim Finin, James Mayfield, Christine Pi-
atko, Delip Rao, David Yarowsky, and Markus Dreyer.
2009. HLTCOE Approaches to Knowledge Base Pop-
ulation at TAC 2009. In Text Analysis Conference.
Rada Mihalcea and Andras Csomai. 2007. Wikify!:
Linking documents to encyclopedic knowledge. In
126
Proceedings of the Sixteenth ACM Conference on
Information and Knowledge Management (CIKM),
pages 233?242.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the 47th
Annual Meeting of the Association for Computational
Linguistics (ACL-2009), pages 1003?1011.
Patrick Pantel and Ariel Fuxman. 2011. Jigs and Lures:
Associating Web Queries with Structured Entities. In
ACL.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambiguation
to wikipedia. In Proc. of the Annual Meeting of the
Association of Computational Linguistics (ACL).
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the Sixteenth Euro-
pean Conference on Machine Learning (ECML-2010),
pages 148?163.
Avi Silberschatz, Henry F. Korth, and S. Sudarshan.
2010. Database System Concepts. McGraw-Hill,
sixth edition.
Daniel S. Weld, Raphael Hoffmann, and Fei Wu. 2009.
Using Wikipedia to Bootstrap Open Information Ex-
traction. In ACM SIGMOD Record.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2010), pages 1013?1023.
Yiping Zhou, Lan Nie, Omid Rouhani-Kalleh, Flavian
Vasile, and Scott Gaffney. 2010. Resolving surface
forms to wikipedia topics. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (Coling), pages 1335?1343.
127
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 22?32,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Automatic Grading of Scientific Inquiry
Avirup Sil
Computer and Information Sciences
Temple University
Philadelphia, PA
avirup.sil@temple.edu
Angela Shelton
College of Education
Temple University
Philadelphia, PA
angi@temple.edu
Diane Jass Ketelhut
Teaching and Learning, Policy and Leadership
University of Maryland
College Park, MD
djk@umd.edu
Alexander Yates
Computer and Information Sciences
Temple University
Philadelphia, PA
yates@temple.edu
Abstract
The SAVE Science project is an attempt to ad-
dress the shortcomings of current assessments
of science. The project has developed two vir-
tual worlds that each have a mystery or natu-
ral phenomenon requiring scientific explana-
tion; by recording students? behavior as they
investigate the mystery, these worlds can be
used to assess their understanding of the scien-
tific method. Currently, however, the scoring
of the assessment depends either on manual
grading of students? written responses, or, on
multiple choice questions. This paper presents
an automated grader that can combine with
SAVE Science?s virtual worlds to provide a
cheap mechanism for assessments of the abil-
ity to apply scientific methodology. In experi-
ments on over 300 middle school students, our
best automated grader improves by over 50%
relative to the closest system from previous
work in predicting grades supplied by human
judges.
1 Introduction
Education researchers criticize current standardized
tests of science on many grounds. First, they lack
context (Behrens et al, 2007), which complicates a
student?s task of applying classroom-based learning,
as the theory of situated cognition suggests (Brown
et al, 1989). Second, many have criticized such
tests for failing to engage students long enough to
apply their understanding to the question. Further-
more and perhaps worst of all, standardized tests fail
to assess scientific inquiry?the ability of students
to apply the scientific method?authentically rather
than as scientific content (National Research Coun-
cil, 2005; Singley and Taft, 1995).
We consider an assessment conducted by the
Situated Assessment using Virtual Environments
for Science Content and Inquiry (SAVE Science)
project (Ketelhut et al, 2010; Ketelhut et al, 2009),
whose long-term goal is to address the shortcomings
of current standardized tests of science. The assess-
ments from SAVE Science have produced an abun-
dance of data on how students interact with a vir-
tual world, when trying to conduct scientific inquiry.
Observing student behavior in virtual environments
offers the potential for new insights into both how
students learn and what they know. However, this
benefit can only be realized if we can make sense of
the stream of data and text produced by the students.
In this paper, we attempt to automate the process
of grading students in SAVE Science assessments, to
make the evaluations as cost-effective as standard-
ized tests. Unlike most previous systems for au-
tomated grading (Sukkarieh and Stoyanchev, 2009;
Sukkarieh et al, 2004; Higgins et al, 2004; Wang
et al, 2008), the data for this task includes a short
paragraph (usually 50-60 words) natural language
response stating a hypothesis and evidence in sup-
port of it. In addition, there is a wealth of relational
data about student behavior in a virtual environment.
We develop novel predictors for automatically grad-
ing the written responses using a wide variety of nat-
ural language features, as well as features from the
data on student behavior in the virtual world. On
student data from two virtual worlds, our best auto-
mated grader has correlations of r = 0.58 and 0.44
with human judgments, improving over the closest
22
technique from previous work by 56% for the first
world, and by 120% for the second.
The rest of the paper is organized as follows.
The next section contrasts this project with previ-
ous work. Section 3 describes the SAVE Science
project and the student data it has produced. Section
4 details our automated grading models. Section 5
reports on experiments, and Section 6 concludes.
2 Previous Work
Wang et al (2008) have previously conducted a
study on assessing creative problem-solving in sci-
ence education by automatically grading student es-
says. Our techniques improve substantially over
theirs, as we demonstrate empirically. In part, we
improve by including more sophisticated language-
processing features in our model than the unigram
and bigram features they use; as others have noted,
bag-of-words representations and latent semantic
indexing become less useful as word order and
causal relationships become important for judging
an essay?s quality (Malatesta et al, 2002; Wiemer-
Hastings et al, 2005). A secondary reason for our
improvement is that we also have access to non-
linguistic data about the students that we can mine
for additional patterns.
Most previous research on automated grading of
written text focuses on short, factual text (Wiemer-
Hastings et al, 1999; Mohler and Mihalcea, 2009;
Leacock and Chodorow, 2003; Sukkarieh and Stoy-
anchev, 2009; Sukkarieh et al, 2004; Mitchell et al,
2002; Pulman and Sukkarieh, 2005), whereas SAVE
Science?s texts are only partly factual. Responses
are meant to convey a scientific explanation of a
mystery, and therefore, correct responses contain in-
ferences, observations of the world, and causal links
between observations and inferences.
Automatic systems for grading longer responses
typically grade essays for coherence and discourse
structure (Burstein et al, 2001; Higgins et al, 2004),
but these global discourse criteria are only partially
indicative of the quality of a student?s response to the
SAVE Science assessments. To be considered fully
correct in these tests, student responses must contain
factually correct information, as well as causal rela-
tionships that justify the student?s inferences, such
as ?The balls don?t bounce outside because it?s cold,
and lower temperatures decrease pressure.?
3 Assessing Scientific Inquiry Using
Virtual Worlds
We now give a brief overview of SAVE Science,
which aims to complement (or even replace) cur-
rent standardized tests for evaluating students? un-
derstanding of science. We first present the project?s
goals and methodology, and then describe the chal-
lenges involved in creating an automated evalua-
tion of student performance for this new assessment
paradigm.
3.1 The SAVE Science Project
SAVE Science (Ketelhut et al, 2010; Ketelhut et al,
2009; Ketelhut et al, 2012) is a novel project for
evaluating students? understanding of the scientific
method ? problem identification, gathering data,
analyzing data, developing a hypothesis, and com-
municating results ? by asking students to solve
a mystery in a virtual world through the applica-
tion of the scientific method to a content-based prob-
lem. Using immersive virtual environments for as-
sessments is a current area of focus among educa-
tion researchers (Clarke-Midura, 2010); SAVE Sci-
ence is unique in its attempt to assess understand-
ing of both inquiry as well as content. That is, the
test is designed to assess students? ability to apply
their knowledge of the scientific inquiry processes
to a problem they have never seen before, but within
a content area they have just studied. To be suc-
cessful, students must explore a virtual environment,
collect appropriate data about it, and find evidence
that supports their inference about the cause of the
mystery. Part of the reasoning for a particular con-
clusion draws on scientific knowledge learned in the
classroom, but for these mysteries such knowledge
of scientific content is insufficient. Students must
also be able to explore the virtual world and create a
hypothesis about the cause of the problem, based on
their observations and analysis of collected data.
For this study, we concentrate on two virtual
worlds produced by the SAVE Science project team,
Basketball and Weather Trouble. Screenshots of
the two virtual worlds are shown in Figure 1. Stu-
dents are represented by an avatar, or virtual char-
acter, whom they can control in the virtual world
23
Figure 1: Screenshots from SAVE Science?s virtual environments. Left: the Basketball module. Right: the Weather
Trouble module. The bar of icons along the bottom of the screen shows various tools that students may choose to use
in the world, including a map, compass, graphing tool, note pad, and instruments like a barometer and thermometer,
among others. Glowing green arrows indicate ?objects? (sometimes including people) with which the student?s avatar
may interact, by making observations, by taking measurements, or through conversation.
with a mouse or key presses. When the test be-
gins, one character in the world informs the student
of a mystery that the student needs to explain. In
the Weather Trouble world, citizens of Scientopolis
are concerned with the lack of rain recently, and ask
the avatar to determine whether it will rain soon. In
the Basketball world, a basketball tournament staffer
is concerned that students cannot play basketball on
the outdoor playground, because the balls will not
bounce high enough outdoors, even though the same
balls bounce just fine indoors.
Once informed of the mission, the student
(through her or his avatar) explores the world, and
interacts with objects or other characters in the vir-
tual world by ?colliding? with them. Interactions
with characters mostly involve the character telling
the avatar some part of the story of the world through
their eyes (e.g., ?It hasn?t rained here in weeks; I
hope it rains soon!?). The conversation may yield
useful clues, or it may be ?folk science? (e.g., ?The
sheep are lying down, so it is probably going to rain
soon?). When the avatar interacts with an object, the
student can choose from a set of tools to determine
measurements of the object. Measurements that a
student deems interesting can be recorded in the stu-
dent?s clipboard, and a graphing tool allows students
to construct charts from the data in the clipboard.
Once students have finished exploring, collect-
ing data, and analyzing the data, they are asked to
communicate the results by writing a brief expla-
nation for the cause of the mystery for the world.
In addition, students are asked to provide what they
consider to be the top three pieces of evidence for
their explanation. Both the explanation and the
ranked evidence are written in freeform text, con-
sisting of 48.5 words on average for Basketball, and
62.4 for Weather Trouble. We refer to the expla-
nation and ranked evidence collectively as the stu-
dent?s freeform response. These texts are critical
components of the overall data about the student, as
they can be used to assess the student?s ability to
communicate findings.
3.2 Assessing the ability to make scientific
inquiries
The virtual worlds from SAVE Science provide an
abundance of data about each student?s ability to
apply the scientific method, as well as their un-
derstanding of content, but the current assessment
scheme involves either manual grading of freeform
responses, or multiple choice questions. The first
is problematic because of the effort and expense in-
volved; the second is problematic because of the dif-
ficulty in designing multiple choice questions that
accurately assess everything a student has learned
(Wang et al, 2008; Chang and Chiu, 2005; Singley
and Taft, 1995). The focus of this paper is to pro-
vide an automated way of assessing students? ability
to perform scientific inquiry based on their behav-
ior in the virtual world and their freeform responses.
We first describe the current assessment mechanisms
available in SAVE Science?s data, which we then use
24
Score Criteria
4 Provides a correct hypothesis with supporting
data gathered from within the world
3 Provides a correct hypothesis with only folk
or incorrect evidence
2 Provides a somewhat correct answer
1 Provides a hypothesis
0 No hypothesis, or nonsense
Table 1: Rubric for manual scoring of freeform re-
sponses.
Score Example
3 it?s because the air outside is more colder
than the air inside here the cold air causes
the air molecules to gather up toghter tight
toghter causeing the ball to deflate and have
less bounce . . .
1 the wieght isnt up to regulations but the bouce
is ok everyball i bouce it bouced according
to regulartion but almost every ball has the
weight of 1.25 . . .
Figure 2: Example portions of two freeform responses
from Basketball, presented as written by the students.
below as gold standards for automated predictors for
assessment.
Manual grading of the freeform responses uses a
rubric of integer scores from 0 to 4. Guidelines for
the rubric scores are shown in Table 1, and two ex-
ample responses are shown in Figure 2. Two anno-
tators, the first holding a PhD in education and the
second a PhD student in computer science, indepen-
dently judged each response, achieving a high inter-
annotator agreement ? for Basketball, Cohen?s ? =
0.95, Pearson?s ? = 0.98; and for Weather Trouble
? = 0.8, ? = 0.93. For our experiments, we use
the judgments of the first annotator, who helped de-
sign the virtual worlds and has experience in grading
student essays, but the choice of which annotator?s
judgments to use makes little difference to the re-
sults.
The multiple choice questions, which we call quiz
questions, consist of two types, as shown in Table
2. The first type, which we call contextualized ques-
tions, directly test students? understanding of the sci-
entific issues that arise in the virtual environment
of the module. Non-contextualized questions are re-
lated to the topic of the module, but they can be an-
swered correctly using general scientific knowledge
rather than specific knowledge gleaned from explo-
ration of the virtual world. The non-contextualized
questions are taken from the benchmark exams of a
major urban school district.
4 Predictors for Scientific Inquiry Grades
We now focus on the task of building automated pre-
dictors for assessing students? ability to make scien-
tific inquiries. To do this, we turn the grading task
into a classical machine learning problem, in which
the system must learn from a set of training data
(students and their grades) how to predict a grade
for new students included in separate test data. We
focus on two main types of models: ones that can
grade by predicting how many multiple-choice ques-
tions (contextualized, non-contextualized, or both)
a student will answer correctly, and ones that can
predict the manual grade assigned to a freeform re-
sponse.
Unlike typical automated-grading systems for
grading written or spoken natural language, our task
includes a large additional source of evidence for the
predictions: data about the students? behavior in the
virtual world. Our prediction models therefore make
extensive use of both the freeform response and data
from the students? behavior in the world, which we
refer to as world data.
4.1 Models
We use Support Vector Machines with Radial Ba-
sis Function kernels (RBF-SVM) (Pang-Ning et al,
2006; Smola and Scho?lkopf, 1998) for learning
non-linear regression models of grading. Let S be
the set of students evaluated through SAVE Sci-
ence?s virtual environment, and let f : S ? Rn be
a vector-valued feature function providing n real-
valued features for each student, based on the stu-
dent?s freeform response and behavior in the virtual
world. Let g : S ? R be the target grading func-
tion, which provides a real-valued grade for each
student. The hypothesis spaceH for RBF-SVMs in-
cludes functions h : S ? R of the form
h(s) =
m?
i=1
?iK(xi, f(s)) + b (1)
25
Contextualized Questions Non-Contextualized Questions
What variable would you change to cor-
rect this basketball problem?
1. Temperature
A. Make it 75?F
B. Make it 55?F
C. Make it 35?F
2. Court Type
A. Concrete only
B. Wood only
C. Court Type makes little to no differ-
ence
3. Basketball used
A. Replace one Wade Park ball with one
Jordan Gym ball
B. Purchase a new set of balls for Wade
Park
C. New basketballs will not help this
problem
1. A child riding a bicycle notices that the tires are more in-
flated on hot days than on cold days, even though no air is
being added or removed. How can this be explained?
A. A higher temperature of the air in the tires causes the par-
ticles in the air to stick together and take up more space.
B. A higher temperature of the air in the tires causes the num-
ber of particles in the air to increase.
C. A higher temperature of the air in the tires causes the pres-
sure of the air to drop and the volume of the air to increase.
D. A higher temperature of the air in the tires causes both
the pressure and volume of the air to increase.
2. A sample of oxygen is being stored in a closed container
at a constant temperature. What will happen to the gas if
it is transferred to a container with a smaller volume?
A. Its weight will increase
B. Its weight will decrease
C. Its pressure will increase
D. The size of its particles will decrease
Table 2: Complete list of Basketball contextualized and non-contextualized quiz questions. Bold indicates the correct
answer.
where the xi are the support vectors, and K is the
RBF kernel function, given by:
K(x,x?) = exp(???x? x??2) (2)
Here, ?i, b, ? ? R are parameters to be learned from
the training data. We use the Weka (Hall et al, 2009)
toolkit for running standard training and prediction
algorithms with the SVM.
We train models for four distinct prediction tasks,
each defined by a different grading function g(s):
1) g(s) is the manually-assessed grade on stu-
dent s?s freeform responses; 2) g(s) is the num-
ber of correctly-answered contextualized questions;
3) g(s) is the number of correctly-answered non-
contextualized questions; and 4) g(s) is the total
number of correctly-answered quiz questions (the
sum of g(s) from 2 and 3). We use the same feature
function f for all models, which we describe next.
4.2 World Features
From the database that records a student?s activity in
the immersive virtual environment, we extract fea-
tures describing the frequency and types of activi-
ties in which students engaged. For both modules,
we include features for the number of object interac-
tions, the number of distinct objects interacted with,
the total number of measurements made, the number
of measurements saved in the student?s clipboard,
and the number of graphs made. We also include
module-specific features: for example, in the Bas-
ketball assessment module, we counted how many
distinct basketballs were interacted with, how many
measurements were made using each type of tool
available in the Basketball world, whether a given
student created graphs of temperature inside vs. out-
side, or graphs of temperature vs. pressure, etc. In
total, the model contains 69 world features in the
Weather module, and 65 in the Basketball module.
All features conform to the pattern of counts over
particular types of actions the avatar might take. We
call the features from the virtual environment world
features.
We note that the relational data in this world is
large and complex, containing temporal and sequen-
tial information which these features currently ig-
nore. This feature set serves as an initial exploration
of the world data, but we fully expect that future in-
vestigation will improve on this representation. For
26
this paper we are primarily interested in features of
the freeform responses, which we now turn to.
4.3 Natural Language Features
We investigate standard text mining features from
bag-of-words representations and Latent Semantic
Analysis, as well as a variety of features tailored to
the grading task. Spelling is a major problem for
this type of prediction task, but spelling-correctors
are investigated elsewhere (Kernighan et al, 1990)
and are not a focus of this research. We therefore
manually corrected spelling errors throughout the
texts before extracting features and conducting ex-
periments. No correction of grammar or punctuation
was performed.
4.3.1 Latent Semantic Analysis Features
After removing 34 common stopwords, we
extract a bag-of-words representation from the
freeform responses (Manning and Schu?tze, 1999).
We apply Latent Semantic Analysis (LSA) (Lan-
dauer and Dumais, 1997; Steyvers and Griffiths,
2006) to this set of features to produce a smaller
set of 72 latent features for Basketball, and 94 for
Weather Trouble, based on a threshold of retaining
90% of the variance in the data.
4.3.2 Features from Hidden Markov Models
LSA and other topic models identify latent struc-
ture based on document-level cooccurrence statis-
tics, but the ?documents? in our data are short for
topic-modeling purposes, and we have less than
200 of them for each world. As a result, stan-
dard topic modeling techniques may have difficulty
identifying the appropriate structure. We therefore
also consider Hidden Markov Models (HMMs) (Ra-
biner, 1989), generative models which rely both on
cooccurrence within a sentence and on sequence in-
formation for determining model parameters. Fol-
lowing recent work by Huang et al (2011) on
using HMMs to build representations, we esti-
mate parameters for a fully-connected HMM with
100 latent states over the freeform responses us-
ing Expectation-Maximization. We then decode the
HMM over the corpus to produce a Viterbi-optimal
latent state for each word. Finally, we use counts of
these 100 latent states to produce 100 new features
for each freeform response.
4.3.3 Detecting disengagement
A small number of students show little enthusi-
asm for the test, and their responses and general per-
formance are quite poor. Often their freeform re-
sponses are short, or they repeat the same text mul-
tiple times. We include three features that help iden-
tify such cases: the overall length of the response,
the number of times a full sentence is repeated ex-
actly, and the number of tokens that are repeated
across multiple sentences.
4.3.4 Ngram and Pattern Features
While HMM and LSA features help combat spar-
sity in the predictive model, they may ignore the
strong signal from a few expressions that are par-
ticularly important for a domain. By soliciting ad-
vice from domain experts, we selected important
unigrams, bigrams, and trigrams for each module,
and created features that count each of these. Like-
wise, we selected important two-word and three-
word sets, which we call loose patterns, that weakly
indicate that a student understood the problem, if
they all occur in the same response but not neces-
sarily near one another. Again, these words were se-
lected as a result of combination of empirical obser-
vations and expert domain knowledge from design-
ers. For instance, if a response contains the three
words ?temperature,? ?pressure,? and ?because,? it
would match one of these loose patterns. For each
pattern, we create a feature to count the number of
matches in a response.
The selected patterns and ngrams both consist of
three kinds of words: ones that indicate types of
measurable phenomena or properties (e.g., ?temper-
ature?), locations (e.g., ?outside?), or causal or com-
parative words (e.g., ?causes,? ?higher,? ?than,? or
?decrease?). Because the responses discuss numer-
ical observations like temperature and pressure val-
ues, we also allow a wildcard for matching any num-
ber as part of the loose patterns.
4.3.5 Semantic Features
We use the Senna1 semantic role labeling (SRL)
system (Collobert et al, 2011) to automatically iden-
tify predicate-argument relationships in the freeform
responses. In general, the SRL system is only able
1http://ml.nec-labs.com/senna/
27
to identify predicate-argument structures in well-
crafted sentences, which on its own is a good indi-
cator that the student will do well in the evaluation.
In addition, we extract semantic features (SFs) that
count how often certain predicate-argument struc-
tures appear which are indicative of a good answer:
SF1 Count how often the freeform response con-
tains any predicate.
SF2 Count how often the response contains predi-
cates that involve causality, such as ?causes? or
change-of-value predicates like ?increase.?
SF3 Count how often measurement words (e.g.,
temperature, pressure) appear as arguments to
any predicate.
SF4 Count how often measurement words appear as
arguments to the predicates related to causality.
4.4 Feature Selection
We perform feature selection using a correlation-
based technique that tries to identify maximally-
relevant and minimally-redundant features (Hall,
1998; Deng and Moore, 1998). The algorithm eval-
uates the value of a subset of features by considering
the individual correlation between each feature and
the gold standard, as well as the correlation between
features. We use the default parameter settings for
feature selection, as specified in Weka.
5 Experiments
5.1 Experimental Setup
We use a dataset collected by the SAVE Science
project, consisting of the world data, freeform re-
sponses, and quiz answers from public middle-
school students in a major urban area of the United
States. 120 students completed the Weather Trou-
ble module, and 184 students completed Basket-
ball. After manually correcting spelling errors in
the freeform responses, we extracted features as de-
scribed above.
Following Wang et al (2008), we evaluate our re-
gression models using Pearson correlation between
the predicted outcome and the gold standard out-
come. Four different gold standards are consid-
ered for each module: manually-assigned grades for
the freeform text, and three versions of the num-
ber of correctly-answered quiz questions (contextu-
alized only, non-contextualized only, and all). We
use a ?2 test with a threshold of p < 0.05 to deter-
mine statistical significance. We train and test mod-
els using 10-fold cross-validation to reduce variabil-
ity, and the results are averaged over the folds.
We evaluate several variants of our system, in-
cluding a World variant that only includes features
from the world data; an NLP variant that only in-
cludes features from the freeform responses; and a
combined World+NLP variant that includes all fea-
tures before feature selection is performed.
Our evaluation compares against the essay grad-
ing technique by Wang et al Like ours, their sys-
tem uses RBF-SVM regression with default param-
eter settings as implemented in Weka, and like ours
the system is trained on student texts proposing so-
lutions to a science problem (in their case, a high
school chemistry problem). The system is trained
on human judgments of the quality of the student
answers. The major difference between our tech-
nique and theirs lies in the representation of the data;
Wang et al use two types of features: unigrams, and
bigrams that occur at least five times during train-
ing. In our implementation of their technique, we
use a lower threshold for bigrams ? they must oc-
cur at least twice. This is because we have less text
to work with, and the higher threshold yields too
few bigrams. Using the lower threshold improved
performance slightly, so we report only those results
below.
5.2 Results and Discussion
The full system for automatic grading is accurate,
across both worlds and all gold standards. Figure
3 shows the results of predicting human judgments
of the freeform responses, where the World+NLP
system achieves a correlation of 0.58 for Basket-
ball and 0.44 for Weather Trouble. The same sys-
tem achieves 0.55 and 0.54 on the World ques-
tions of Basketball and Weather Trouble, respec-
tively (Figures 4 and 5). Our best models are sta-
tistically significantly different from the Wang et al
model (for predicting contextualized questions for
basketball: p = .009, ?2 = 6.87162; for grading
freeform responses: p ? 0, ?2 = 14.21725). Cor-
relations from World+NLP for other quiz types ?
28
0.26 
0.37 
0.58 0.58 
0.15 0.20 
0.43 0.44 
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
World Wang et al NLP World+NLP
Corr
elat
ion 
Coe
ffici
ent 
Correlation: Predicted vs Human Rubric Scores 
Basketball
Weather
Figure 3: Our NLP features dramatically improve predic-
tion over the Wang et al model for grading freeform sci-
ence essays, by a margin of 0.21 on Basketball and 0.23
on Weather Trouble.
0.33 0.34 
0.45 
0.55 
0.09  0.12 
0.34 0.40 
0.20 
0.33 
0.40 0.46 
0
0.1
0.2
0.3
0.4
0.5
0.6
Wang et al World NLP World+NLP
Corr
elat
ion 
Coe
ffici
ent 
Automatic Grading  of Basketball Quiz Answers  
Contextualized Non-contextualized All
Figure 4: The World+NLP model outperforms both
World and NLP, and substantially outperforms the Wang
et al system.
non-contextualized and all questions ? were some-
what lower, but still statistically significant (p =
.002, ?2 = 10.05986).
The language features are currently the major fac-
tor in the predictive models for automated grad-
ing. The NLP model substantially outperforms both
the simpler Wang et al model and the World-only
model in predicting quiz answers for both worlds.
It achieves correlations that are statistically signifi-
cantly different from the baseline, for all gold stan-
dards and both worlds.
The story in the case of grading freeform essays
is similar. Our NLP model beats the Wang et al
model and the World-only model. Our full model
World+NLP, however, outperforms the NLP model
by only a small fraction. Also, the Wang et al model
performs slightly better than the World-only model
on freeform responses. For Basketball, the correla-
tion coefficient of their model is greater by 0.11 and
for Weather by 0.05. We believe that the NLP-based
0.13 
0.06 
0.53 0.54 
0.00 0.06 
0.46 
0.30 
0.10 0.12 
0.38 0.41 
-0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
Wang et al World NLP World+NLPCorr
elat
ion 
Coe
ffici
ent 
Automatic Grading  of Weather Trouble Quiz Answers  
Contextualized Non-contextualized All
Figure 5: The NLP model substantially outperforms
World and Wang et al on predicting quiz questions for
Weather Trouble, and the combined World+NLP model
achieves a 0.54 correlation for contextualized questions.
models, including Wang et al?s, are outperforming
the World model because the current representation
of the World data fails to capture all of the pertinent
information from students? behavior in the virtual
environments. Our plans for future work include the
development of features that can capture temporal
patterns in student activity.
Each type of language feature appears to pro-
vide a beneficial and complementary source of ev-
idence. We tested the model using only individual
subsets of the NLP features, such as HMM features
only, LSA features only, ngrams and loose patterns
only, and features from semantic role labeling only.
On their own, each set of features provides only a
small improvement over the mean predictor. When
combined with the world features, each subset of
the NLP features again provides only a small im-
provement over the World-only model. For exam-
ple, for predicting Basketball world quiz questions,
World features achieve r = 0.34, World+HMM and
World+LSA achieve 0.35, and World+(ngrams and
loose patterns) achieves 0.39. The relative ranking
of these subsets of features is not consistent across
different tasks; for Weather contextualized ques-
tions, World+HMM is best, and for Weather non-
contextualized questions, World+LSA is best. Fea-
tures selected by the feature selection algorithm also
indicate that the different types of language features
complement one another. The feature selection al-
gorithm for the World+NLP model selects some fea-
tures for every different type we presented, although
the HMM, LSA, loose pattern, and unigram fea-
tures dominate. We believe that the best procedure
29
for developing grading systems for science essays
is therefore to construct a large number of possible
features using a variety of techniques, and then train
a model for a particular task and gold standard. In-
cluding significantly more varieties of features, per-
haps from additional kinds of language models or
NLP pipeline tools, is an important future direction
for further improving the grading accuracy.
While the accuracies of the models for contextu-
alized and non-contextualized questions are broadly
similar, the models themselves are not. For the con-
textualized questions, 4 important world behavior
features were deemed important and non-redundant
by the feature selection algorithm: the number of
distinct collisions, the number of people collided
with, the number of distinct objects (basketballs or
balloons) whose pressure was measured, and the
number of distinct temperature measurements that
were recorded into clipboards. The essential task
in this virtual world is to discover that a decrease
in the temperature of several gas systems (basket-
balls and balloons filled with air) is causing their
pressure to decrease. The model for the contex-
tualized questions thus includes variables that are
highly relevant to a student?s understanding of the
core problem in the world, which in turn indicates
that automated data mining techniques are capable
of identifying when students are learning to prac-
tice the scientific method, by observing student be-
havior. On the other hand, the model for the non-
contextualized questions includes only 2 world fea-
tures: The number of collisions made and number
of different objects whose circumference was mea-
sured. The first one is an indicator of the activity
level of a student and the second variable is an indi-
cator for whether the student has identified the prob-
lem (the basketballs are not bouncing because they
are deflated), but not for the underlying cause of
the problem (the outside temperature causes a drop
in pressure, which causes the basketball circumfer-
ence to decrease). Thus the model that predicts non-
contextualized questions very accurately has little
information about whether the student understood
the core problem of the world or not; instead, it has
information about whether the student is active in
the world. These observations lend some support to
the criticism that the standardized tests are not prop-
erly assessing inquiry.
Performance on the Weather Trouble module is
consistently lower than on Basketball. In part, this
reflects the increased difficulty of this world; human
inter-annotator agreement is a bit lower (? = 0.8
vs. 0.95 on Basketball). However, another large
part of the difference is that the world features pro-
vide far less information in Weather Trouble ? the
World-only model has less than half the correlation
on Weather than on Basketball, for all quiz ques-
tion types. We suspect that the cause is the nature
of the task on the Weather Trouble world, where
temporal information plays a bigger role as measure-
ments of air pressure and wind direction may change
over time. Investigating world features that can dis-
tinguish different patterns of student behavior over
time is an important area for further investigation.
6 Conclusion
Our automated grader uses a wide variety of NLP
pipeline tools to produce features for students? es-
says on the answers to scientific mysteries. The
grader achieves significant correlation with human
judges and multiple choice quiz evaluations, sub-
stantially outperforming a simpler grader from prior
work. The findings of this research suggest that au-
thentic assessments of scientific inquiry through vir-
tual environments can be graded purely automati-
cally, like high stakes multiple choice tests. Ongoing
work on SAVE Science is investigating the differ-
ences in how students respond to standard multiple-
choice tests and tests based on virtual environments.
But the contextualized assessments from SAVE Sci-
ence provide evaluation of scientific inquiry that
multiple choice tests currently do not, and they can
now be graded just as cheaply.
Acknowledgments
This material is based upon work supported
under National Science Foundation Grant No.
0822308/1157534. Any opinions, findings, and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not necessarily
reflect the views of the NSF. We would like to thank
Catherine Schifter and the rest of the SAVE Sci-
ence team for their help and support. We also wish
to thank the anonymous reviewers for their helpful
comments.
30
References
J. T. Behrens, D. Frezzo, R. Mislevy, M. Kroopnick, and
D. Wise. 2007. Structural, Functional, and Semiotic
Symmetries in Simulation-based Games and Assess-
ments. In E. Baker, J. Dickieson, W. Wulfeck, and
H. O?Neil, editors, Assessment of Problem Solving Us-
ing Simulations. Lawrence Erlbaum Associates.
J. S. Brown, A. Collins, , and P. Duguid. 1989. Situated
cognition and the culture of learning. Educational Re-
searcher, 18(1):32?41.
J. Burstein, C. Leacock, and R. Swartz. 2001. Auto-
mated evaluation of essays and short answers. In 5th
International Computer Assisted Assessment Confer-
ence. Loughborough University.
S.-N. Chang and M.-H. Chiu. 2005. The development of
authentic assessment to investigate ninth graders sci-
entific literacy: In the case of scientific cognition con-
cerning the concepts of chemistry and physics. Inter-
national Journal of Science and Mathematics Educa-
tion, 3:117?140.
J. Clarke-Midura. 2010. The Role of Technology in
Science Assessments. Better: Evidence-based Edu-
cation, 3(1).
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12:2493?2537.
Kan Deng and Andrew Moore. 1998. On the greediness
of feature selection algorithms. In Proc. International
Conference on Machine Learning (ICML), June 1998.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11(1).
M. A. Hall. 1998. Correlation-based feature subset
selection for machine learning. In Hamilton, New
Zealand.
D. Higgins, J. Burstein, D. Marcu, and C. Gentile. 2004.
Evaluating multiple aspects of coherence in student
essays. In Proceedings of the annual meeting of the
North American Chapter of the Association for Com-
putational Linguistics, Boston, MA.
Fei Huang, Alexander Yates, Arun Ahuja, and Doug
Downey. 2011. Language Models as Representations
for Weakly Supervised NLP Tasks. In Conference on
Computational Natural Language Learning (CoNLL).
Mark D. Kernighan, Kenneth W. Church, and William A.
Gale. 1990. A spelling correction program based on
a noisy channel model. In Proceedings of the 13th
Conference on Computational Linguistics, pages 205?
210.
D.J. Ketelhut, B. Nelson, and C. Schifter. 2009. Virtual
Environments for Situated Science Assessment. In
Proceedings of the International Conference on Cog-
nition and Exploratory Learning in the Digital Age,
pages 507?508.
D.J. Ketelhut, B. Nelson, C. Schifter, and Y. Kim. 2010.
Using Immersive Virtual Environments to Assess Sci-
ence Content Understanding: The Impact of Context.
In D. G. Kinshuk, J. M. Sampson, P. Spector, D. Isaas,
Ifenthaler, and R. Vasiu, editors, Proceedings of the
IADIS International Conference on Cognition and Ex-
ploratory Learning in the Digital Age (CELDA), pages
227?230.
Diane Jass Ketelhut, Alexander Yates, Avirup Sil, and
Michael Timms. 2012. Applying Educational Data
mining in E-learning environments. In Section within
the New Measurement Paradigm Report, p 47-52.
T.K. Landauer and S.T. Dumais. 1997. A solution to
Platos problem: The latent semantic analysis theory
of acquisition, induction, and representation of knowl-
edge. In Psychological Review, 104.
C. Leacock and M. Chodorow. 2003. C-rater: Auto-
mated scoring of short-answer questions. In Comput-
ers and the Humanities, 37(4):389405.
K.I. Malatesta, P. Wiemer-Hastings, and J. Robertson.
2002. Beyond the short answer question with research
methods tutor. In Proceedings of the Intelligent Tutor-
ing Systems Conference.
Chris Manning and Hinrich Schu?tze. 1999. Foundations
of Statistical Natural Language Processings. MIT
Press.
T. Mitchell, T. Russell, P. Broomhead, and N. Aldridge.
2002. Towards robust computerised marking of free-
text responses. In Proceedings of the 6th International
Computer Assisted Assessment (CAA) Conference.
Michael Mohler and Rada Mihalcea. 2009. Text-to-text
semantic similarity for automatic short answer grad-
ing. In Proceedings of the 12th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, EACL.
National Research Council. 2005. America?s Lab Re-
port: Investigations in High School Science. National
Academies Press.
T. Pang-Ning, M. Steinbach, and V. Kumar. 2006. Intro-
duction to Data Mining. Pearson Addison-Wesley.
S.G. Pulman and J.Z. Sukkarieh. 2005. Automatic
short answer marking. In Proceedings of the Second
workshop on Building Educational Applications Using
NLP.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257?285.
M.K. Singley and H.L. Taft. 1995. Open-ended
approaches to science assessment using computers.
Journal of Science Education and Technology, 4(1):7?
20.
31
A. Smola and B. Scho?lkopf. 1998. A tutorial on support
vector regression. Technical report, Royal Holloway
College, University of London, UK.
Mark Steyvers and Tom Griffiths. 2006. Probabilistic
topic models. In T. Landauer, D. McNamara, S. Den-
nis, and W. Kintsch, editors, Latent Semantic Analy-
sis: A Road to Meaning, pages 427?448. Lawrence
Erlbaum Associates.
J. Sukkarieh and S. Stoyanchev. 2009. Automating
model building in C-rater. In Proceedings of the 2009
Workshop on Applied Textual Inference, pages 6169,
Suntec, Singapore, August.
J.Z. Sukkarieh, S.G. Pulman, and N. Raikes. 2004. Auto-
marking 2: An update on the ucles-oxford university
research into using computational linguistics to score
short, free text responses. In International Association
of Educational Assessment, Philadephia.
H-C. Wang, C-Y. Chang, and T-Y Li. 2008. Assessing
creative problem solving with automated text grading.
In Computers and Education.
P. Wiemer-Hastings, K. Wiemer-Hastings, and
A. Graesser. 1999. Improving an intelligent tu-
tors comprehension of students with latent semantic
analysis. In Artificial Intelligence in Education, pages
535542.
P. Wiemer-Hastings, E. Arnott, and D. Allbritton. 2005.
Initial results and mixed directions for research meth-
ods tutor. In AIED2005 - Supplementary Proceedings
of the 12th International Conference on Artificial In-
telligence in Education, Amsterdam.
32
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 109?118,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Temporal Scoping of Relational Facts based on Wikipedia Data
Avirup Sil
?
Computer and Information Sciences
Temple University
Philadelphia, PA 19122
avi@temple.edu
Silviu Cucerzan
Microsoft Research
One Microsoft Way
Redmond, WA 98052
silviu@microsoft.com
Abstract
Most previous work in information
extraction from text has focused on
named-entity recognition, entity linking,
and relation extraction. Less attention
has been paid given to extracting the
temporal scope for relations between
named entities; for example, the relation
president-Of(John F. Kennedy, USA)
is true only in the time-frame (January
20, 1961 - November 22, 1963). In this
paper we present a system for temporal
scoping of relational facts, which is
trained on distant supervision based on the
largest semi-structured resource available:
Wikipedia. The system employs language
models consisting of patterns automat-
ically bootstrapped from Wikipedia
sentences that contain the main entity of
a page and slot-fillers extracted from the
corresponding infoboxes. This proposed
system achieves state-of-the-art results
on 6 out of 7 relations on the benchmark
Text Analysis Conference 2013 dataset
for temporal slot filling (TSF), and out-
performs the next best system in the TAC
2013 evaluation by more than 10 points.
1 Introduction
Previous work on relation extraction (Agichtein
and Gravano, 2000; Etzioni et al., 2004) by sys-
tems such as NELL (Carlson et al., 2010), Know-
ItAll (Etzioni et al., 2004) and YAGO (Suchanek
et al., 2007) have targeted the extraction of en-
tity tuples, such as president-Of(George W.
Bush, USA), in order to build large knowl-
edge bases of facts. These systems assume
that relational facts are time-invariant. However,
this assumption is not always true, for example
?
This research was carried out during an internship at
Microsoft Research.
president-Of(George W. Bush, USA) holds
within the time-frame (2001-2009) only. In this
paper, we focus on the relatively less explored
problem of attaching temporal scope to relation
between entities. The Text Analysis Conference
(TAC) introduced temporal slot filling (TSF) as
one of the knowledge base population (KBP) tasks
in 2013 (Dang and Surdeanu, 2013). The in-
put to a TAC-TSF system is a binary relation e.g.
per:spouse(Brad Pitt, Jennifer Aniston) and a
document assumed to contain supporting evidence
for the relation. The required output is a 4-tuple
timestamp [T1, T2, T3, T4], where T1 and T2
are normalized dates that provide a range for the
start date of the relation, and T3 and T4 provide
the range for the end of the relationship. Sys-
tems must also output the offsets of the text men-
tions that support the temporal information ex-
tracted. For example, from a text such as ?Pitt
married Jennifer Aniston on July 29, 2000 [...] the
couple divorced five years later in 2005.?, a sys-
tem must extract the normalized timestamp [2000-
07-29, 2000-07-29, 2005-01-01, 2005-12-31], to-
gether with the entity and date offsets that support
the timestamp.
In this paper, we describe TSRF, a system for
temporal scoping of relational facts. For ev-
ery relation type, TSRF uses distant supervision
from Wikipedia infobox tuples to learn a language
model consisting of patterns of entity types, cate-
gories, and word n-grams. Then it uses this trained
relation-specific language model to extract the top
k sentences that support the given relation between
the query entity and the slot filler. In a second
stage, TSRF performs timestamp classification by
employing models which learn ?Start?, ?End? and
?In? predictors of entities in a relationship; it com-
putes the best 4-tuple timestamp [T1, T2, T3, T4]
based on the confidence values associated to the
top sentences extracted. Following the TAC-TSF
task for 2013, TSRF is trained and evaluated for
seven relation types, as shown in Table 1.
109
per:spouse
per:title
per:employee or member of
org:top employees/members
per:cities of residence
per:statesorprovinces of residence
per:countries of residence
Table 1: Types of relations in the TAC-TSF.
The remainder of the paper is organized as fol-
lows: The next section describes related work.
Section 3 introduces the TAC-TSF input and out-
put formats. Section 4 discusses the main chal-
lenges, and Section 5 details our method for tem-
poral scoping of relations. Section 6 describes our
experiments and results, and it is followed by con-
cluding remarks.
2 Related Work
To our knowledge, there are only a small num-
ber of systems that have tackled the temporal
scoping of relations task. YAGO (Wang et al.,
2010) extracts temporal facts using regular expres-
sions from Wikipedia infoboxes, while PRAVDA
(Wang et al., 2011) uses a combination of textual
patterns and graph-based re-ranking techniques to
extract facts and their temporal scopes simultane-
ously. Both systems augment an existing KB with
temporal facts similarly to the CoTS system by
Talukdar et al. (2012a; 2012b). However, their
underlying techniques are not applicable to arbi-
trary text. In contrast, TSRF automatically boot-
straps patterns to learn relation-specific language
models, which can be used then for processing
any text. CoTS, a recent system that is part of
CMU?s NELL (Carlson et al., 2010) project, per-
forms temporal scoping of relational facts by using
manually edited temporal order constraints. While
manual ordering is appealing and can lead to high
accuracy, it is impractical from a scalability per-
spective. Moreover, the main goal of CoTS is to
predict temporal ordering of relations rather than
to scope temporally individual facts. Conversely,
our system automatically extracts text patterns,
and then uses them to perform temporal classi-
fication based on gradient boosted decision trees
(Friedman, 2001).
The TempEval task (Pustejovsky and Verhagen,
2009) focused mainly on temporal event order-
ing. Systems such as (Chambers et al., 2007) and
(Bethard and Martin, 2007) have been successful
Col.1: TEMP72211 Col.7: 1492
Col.2: per:spouse Col.8: 1311
Col.3: Brad Pitt Col.9: 1.0
Col.4: AFP ENG 20081208.0592 Col.10: E0566375
Col.5: Jennifer Aniston Col.11: E0082980
Col.6: 1098
Table 2: Input to a TSF System.
in extracting temporally related events. Sil et al.
(2011a) automatically extract STRIPS represen-
tations (Fikes and Nilsson, 1971) from web text,
which are defined as states of the world before and
after an event takes place. However, all these ef-
forts focus on temporal ordering of either events or
states of the world and do not extract timestamps
for events. By contrast, the proposed system ex-
tracts temporal expressions and also produces an
ordering of the timestamps of relational facts be-
tween entities.
The current state-of-the-art systems for TSF
have been the RPI-Blender system by Artiles et
al. (2011) and the UNED system by Garrido et
al. (2011; 2012). These systems obtained the
top scores in the 2011 TAC TSF evaluation by
outperforming the other participants such as the
Stanford Distant Supervision system (Surdeanu
et al., 2011). Similar to our work, these sys-
tems use distant supervision to assign temporal la-
bels to relations extracted from text. While we
employ Wikipedia infoboxes in conjunction with
Wikipedia text, the RPI-Blender and UNED sys-
tems use tuples from structured repositories like
Freebase. There are major differences in terms of
learning strategies of these systems: the UNED
system uses a rich graph-based document-level
representation to generate novel features whereas
RPI-Blender uses an ensemble of classifiers com-
bining flat features based on surface text and de-
pendency paths with tree kernels. Our system em-
ploys language models based on Wikipedia that
are annotated automatically with entity tags in a
boosted-trees learning framework. A less impor-
tant difference between TSRF and RPI-Blender is
that the latter makes use of an additional tempo-
ral label (Start-And-End) for facts within a time
range; TSRF employs Start, End, and In labels.
3 The Temporal Slot Filling Task
3.1 Input
The input format for a TSF system as instantiated
for the relation per:spouse(Brad Pitt, Jennifer
110
Aniston) is shown in Table 2. The field Column 1
contains a unique query ID for the relation. Col-
umn 2 is the name of the relationship, which also
encodes the type of the target entity. Column 3
contains the name of the query entity, i.e., the sub-
ject of the relation. Column 4 contains a valid doc-
ument ID and Column 5 indicates the slot-filler en-
tity. Columns 6 through 8 are offsets of the slot-
filler, query entity and the relationship justification
in the given text. Column 9 contains a confidence
score set to 1 to indicate that the relation is cor-
rect. Columns 10 and 11 contain the IDs in the
KBP knowledge base of the entity and filler, re-
spectively. All of the above are provided by TAC.
For the query in this example, a TSF system has to
scope temporally the per:spouse relation be-
tween Brad Pitt and Jennifer Aniston.
3.2 Output
Similar to the regular slot filling task in TAC, the
TSF output includes the offsets for at least one
entity mention and up to two temporal mentions
used for the extraction and normalization of
hypothesized answer. For instance, assume that a
system extracts the relative timestamp ?Monday?
and normalizes it to ?2010-10-04? for the relation
org:top employee(Twitter, Williams) using
the document date from the following document:
<DOCID> AFP ENG 20101004.0053.LDC2010T13 </DOCID>
<DATETIME> 2010-10-04 </DATETIME>
<HEADLINE>
Twitter co-founder steps down as CEO
</HEADLINE>
<TEXT>
<P>
Twitter co-founder Evan Williams announced on Monday
that he was stepping down as chief executive [...]
The system must report the offsets for both
?Monday? in the text body and ?2010-10-04? in
the DATETIME block for the justification.
The TAC-TSF task uses the following represen-
tation for the temporal information extracted: For
each relation provided in the input, TSF systems
must produce a 4-tuple of dates: [T1, T2, T3, T4],
which indicates that the relation is true for a pe-
riod beginning at some point in time between T1
and T2 and ending at some time between T3 and
T4. By convention, a hyphen in one of the po-
sitions implies a lack of a constraint. Thus, [-,
20120101, 20120101, -] implies that the relation
was true starting on or before January 1, 2012 and
ending on or after January 1, 2012. As discussed
in the TAC 2011 pilot study by Ji et al. (2011),
there are situations that cannot be covered by this
representation, such as recurring events, for ex-
ample repeated marriages between two persons.
However, the most common situations for the re-
lations covered in this task are captured correctly
by this 4-tuple representation.
4 Challenges
We discuss here some of the main challenges en-
countered in building a temporal scoping system.
4.1 Lack of Annotated Data
Annotation of data for this task is expensive, as
the human annotators must have extensive back-
ground knowledge and need to analyze the evi-
dence in text and reliable knowledge resources. As
per (Ji et al., 2013), a large team of human an-
notators were able to generate only 1,172 training
instances for 8 slots for KBP 2011. The authors
of the study concluded that such amount of data
is not enough for training a supervised temporal
scoping system. They also noted that only 32% of
employee Of queries were found to have poten-
tial temporal arguments, and only one third of the
queries could have reliable start or end dates.
4.2 Date Normalization
Sometimes temporal knowledge is not stated ex-
plicitly in terms of dates or timestamps. For exam-
ple, from the text ?they got married on Valentine?s
Day? a system can extract Valentine?s Day as the
surface form of the start of the per:spouse re-
lation. However, for a temporal scoping system it
needs to normalize the temporal string to the date
of February 14 and the year to which the document
refers to explicitly in text or implicitly, such as the
year in which the document was published.
4.3 Lexico-Syntactic Variety
A relation can be specified in text by employing
numerous syntactic and lexical constructions; e.g.
for the per:spouse relation the patterns ?got
married on [DATE]? and ?vowed to spend eternity
on [DATE]? have the same meaning. Addition-
ally, entities can appear mentioned in text in vari-
ous forms, different from the canonical form given
as input. For instance, Figure 1 shows an example
in which the input entity Bernardo Hees, which is
not in Wikipedia, is mentioned three times, with
two of the mentions using a shorter form (the last
name of the person).
111
org:top_members_employees    America Latina Logistica / NIL    Bernardo Hees / NIL 
 
<HEADLINE> Burger King buyer names future CEO </HEADLINE> 
<DATELINE> NEW YORK 2010-09-09 13:00:29 UTC </DATELINE> 
<TEXT> 
<P> The investment firm buying Burger King has named Bernardo Hees, a Latin 
American railroad executive, to be CEO of the company after it completes its 
$3.26 billion buyout of the fast-food chain. </P> 
<P> 3G Capital is naming Hees to replace John Chidsey, who will become co-
chairman after the deal closes. </P> 
<P> Hees was most recently CEO of America Latina Logistica, Latin America's 
largest railroad company. Alexandre Behring, managing partner at 3G Capital, was 
also a prior CEO of the railroad. </P> 
<P> 3G Capital is expected to begin its effort to acquire the outstanding shares 
of Burger King for $24 per share by Sept. 17. </P> 
</TEXT> 
Figure 1: Example data point from the TAC TSF 2013 training set, with the annotations hypothesized
by our system. The entity mentions identified by the entity linking (EL) component are shown in bold
blue; those that were linked to Wikipedia are also underlined. The highlighting (blue and green) is used
to show the mentions in the coreference chains identified for the two input entities, ?America Latina
Logistica? and ?Bernardo Hees?.
4.4 Inferred Meaning
A temporal scoping system also needs to learn the
inter-dependence of relations, and how one event
affects another. For instance, in our automatically
generated training data, we learn that a death
event specified by n-grams like ?was assassinated?
affects the per:title relation, and it indicates
that the relationship ended at that point. In Fig-
ure 1, while the CEO relationships for Bernardo
Hees with America Latina Logistica and Burger
King are indicated by clear patterns (?was most re-
cently CEO of? and ?to be CEO of?), the temporal
stamping is difficult to achieve in both cases, as
there is no standard normalization for ?recently?
in the former, and it is relative to the completion
of the buyout event in the latter.
4.5 Pattern Trustworthiness
A temporal scoping system should also be able
to model the trustworthiness of text patterns, and
even the evolution of patterns that indicate a rela-
tionship over time. For example, in current news,
the birth of a child does not imply that a couple
is married, although it does carry a strong signal
about the marriage relationship.
5 Learning to Attach Temporal Scope
5.1 Automatically Generating Training Data
As outlined in Section 4, one of the biggest chal-
lenges of a temporal scoping system is the lack
of annotated data to create a strong information
extraction system. Previous work on relation ex-
traction such as (Mintz et al., 2009) has shown
that distant supervision can be highly effective in
building a classifier for this purpose. Similar to
supervised classification techniques, some advan-
tages of using distant supervision are:
? It allows building classifiers with a large number
of features;
? The supervision is provided intrinsically by the
detailed user-contributed knowledge;
? There is no need to expand patterns iteratively.
Mintz et al. also point out that similar to unsuper-
vised systems, distant supervision also allows:
? Using large amounts of unlabeled data such as
the Web and social media;
? Employing techniques that are not sensitive to
the genre of training data.
We follow the same premise as (Cucerzan, 2007;
Weld et al., 2009) that the richness of the
Wikipedia collection, whether semantic, lexical,
syntactic, or structural, is a key enabler in re-
defining the state-of-the-art for many NLP and
IR task. Our target is to use distant supervision
from Wikipedia data to build an automatic tempo-
ral scoping system. However, for most relations,
we find that Wikipedia does not indicate specific
start or end dates in a structured form. In addition
to this, we need our system to be able to predict
whether two entities are currently in a relation-
ship or not based on the document date as well.
112
Hence, in our first step, we build an automatic sys-
tem which takes as input a binary relation between
two entities e.g. per:spouse(Brad Pitt, Jennifer
Aniston) and a number of documents. The system
needs to extract highly ranked/relevant sentences,
which indicate that the two entities are in the tar-
geted relationship. The next component takes as
input the top k sentences generated in the previous
step and extracts temporal labels for the input rela-
tion. Note that our target is to develop algorithms
that are not relation-specific but rather can work
well for a multitude of relations. We elaborate on
these two system components further.
5.1.1 Using Wikipedia as a Resource for
Distant Supervision
Wikipedia is the largest freely available encyclo-
pedic collection, which is built and organized as
a user-contributed knowledge base (KB) of enti-
ties. The current version of the English Wikipedia
contains information about 4.2 million entities.
In addition to the plain text about these entities,
Wikipedia also contains structured components.
One of these is the infobox. Infoboxes contain in-
formation about a large number of relations for the
target entity of the Wikipedia page, e.g. names of
spouses, birth and death dates, residence etc.. Sim-
ilar to structured databases, the infoboxes contain
the most important/useful relations in which enti-
ties take part, while the text of Wikipedia pages
contains mentions and descriptions of these rela-
tions. Because of this, Wikipedia can be seen as a
knowledge repository that contains parallel struc-
tured and unstructured information about entities,
and therefore, can be employed more easily than
Freebase or other structured databases for building
a relation extraction system. Figure 2 shows how
sentences from Wikipedia can be used to train a
system for the temporal slot filling task.
5.1.2 Extracting Relevant Sentences
For every relation, we extract slot-filler names
from infoboxes of each Wikipedia article. We
also leverage Wikipedia?s rich interlinking model
to automatically retrieve labeled entity mentions
in text. Because the format of the text values pro-
vided by different users for the infobox attributes
can vary greatly, we rely on regular expressions to
extract slot-filler names from the infoboxes. For
every relation targeted, we build a large set of reg-
ular expressions to extract entity names and filter
out noise e.g. html tags, redundant text etc..
To extract all occurrences of named-entities in
the Wikipedia text, we relabel each Wikipedia ar-
ticle with Wikipedia interlinks by employing the
entity linking (EL) system by Cucerzan (2012),
which obtained the top scores for the EL task in
successive TAC evaluations. This implementa-
tion takes into account and preserves the inter-
links created by the Wikipedia contributors, and
extracts all other entity mentions and links them to
Wikipedia pages if possible or hypothesizes coref-
erence chains for the mentions of entities that are
not in Wikipedia. The latter are extremely impor-
tant when the slot-filler for a relation is an entity
that does not have a Wikipedia page, as often is
the case with spouses or other family members of
famous people (as shown in Figure 1 for the slot-
filler Bernardo Hees).
As stated in Section 4, temporal information
in text is specified in various forms. To resolve
temporal mentions, we use the Stanford SUTime
(Chang and Manning, 2012) temporal tagger.
The system exhibits strong performance outper-
forming state-of-the-art systems like HeidelTime
(Str?otgen and Gertz, 2010) on the TempEval-2
Task A (Verhagen et al., 2010) in English. SU-
Time is a rule-based temporal tagger that employs
regular expression. Its input is English text in to-
kenized format; its output contains annotations in
the form of TIMEX3 tags. TIMEX3 is a part of
the TimeML annotation language as introduced by
(Pustejovsky et al., 2003) and is used to markup
date and time, events, and their temporal rela-
tions in text. When processing Web text, we of-
ten encounter date expressions that contain a rel-
ative time e.g. ?last Thursday?. To resolve them
to actual dates/time is a non-trivial task. However,
the heuristic of employing the document?s publi-
cation date as the reference works very well in
practice e.g. for a document published on 2011-
07-05, SUTime resolves ?last Thursday? to 2011-
06-30. It provides temporal tags in the following
labels: Time, Duration, Set and Interval. For our
experiments we used Time and Duration.
After running the Stanford SUTime, which au-
tomatically converts date expressions to their nor-
malized form, we collect sets of contiguous sen-
tences from the page that contain one mention of
the targeted entity and one mention of the slot-
filler, as extracted by the entity linking system. We
then build a large language model by bootstrap-
ping textual patterns supporting the relations, sim-
113
ilar to (Agichtein and Gravano, 2000). The general
intuition is that a set of sentences that mention the
two entities are likely to state something about re-
lationships in which they are.
For assigning sentences a relevance score with
respect to a targeted relation, we represent the sen-
tences in an input document (i.e., Wikipedia page)
as d dimensional feature vectors, which incorpo-
rate statistics about how relevant sentences are
to the relation between a query entity q and the
slot filler z. For example, for the per:spouse
relation, one binary feature is ?does the input
sentence contain the n-gram ?QUERY ENTITY
got married??. Note that the various surface
forms/mentions of q and z are resolved to their
canonical target at this stage.
We were able to extract 61,872 tuples of query
entity and slot filler relations from Wikipedia
for the per:spouse relation. Figure 2 shows
how we extract relevant sentences using slot-filler
names from Wikipedia. Consider the following
text (already processed by our EL system and
Stanford SUTime) taken from the Wikipedia page
of Tom Cruise:
On [November 18, 2006|
2006?11?18
],
[Holmes|
Katie Holmes
] and [Cruise|
Tom Cruise
]
were married in [Bracciano|
Bracciano
] . . .
On [June 29, 2012|
2012?06?29
],
[Holmes|
Katie Holmes
] filed for divorce
from [Cruise|
Tom Cruise
] after five and a half
years of marriage.
Considering Tom Cruise as the query entity and
his wife Katie Holmes as the slot filler for the
per:spouse relation, we normalize the above
text to the following form to extract features:
On DATE, SLOT FILLER and
QUERY ENTITY were married in
LOCATION . . .
On DATE, SLOT FILLER filed for divorce
from QUERY ENTITY after five and a half
years of marriage.
Our language model consists of n-grams (n ? 5)
like ?SLOT FILLER and QUERY ENTITY were
married?, ?SLOT FILLER filed for divorce from?
which provides clues for the marriage relation.
These n-grams are then used as features with
an implementation of a gradient boosted decision
trees classifier similar to that described by (Fried-
man, 2001; Burges, 2010). We also use features
provided by the EL system which are based on en-
tity types and categories. We call this ?relation-
ship? classifier RELCL. The output of this step is
In April 2005, Cruise began dating actress KatieHolmes. On April 27 that year, Cruise and Holmes ?dubbed "TomKat" by the media ? made their firstpublic appearance together in Rome. On October 6,2005, Cruise and Holmes announced they wereexpecting a child, and their daughter, Suri, was born inApril 2006. On November 18, 2006, Holmes and Cruisewere married in Bracciano, Italy, in a Scientologyceremony attended by many Hollywood stars. Therehas been widespread speculation that the marriagewas arranged by the Church of Scientology. On June 29,2012, it was announced that Holmes had filed fordivorce from Cruise after five and a half years ofmarriage. On July 9, 2012, it was announced that thecouple had signed a divorce settlement worked out bytheir lawyers. STARTOfmarriage
ENDOfmarriage
Spouse: Katie Holmes
Figure 2: Example of relevant sentences extracted
by using query entity and slot-filler names from
Wikipedia for the per:spouse relation.
a ranked list of sentences which indicate whether
there exists a relationship between the query entity
and the slot filler.
5.1.3 Learning Algorithm
Our objective is to rank the sentences in a docu-
ment based on the premise that entities q and z
are in the targeted relation r. We tackle this rank-
ing task by using gradient boosted decision trees
(GBDT) to learn temporal scope for entity rela-
tions. Previous work such as Sil et al. (2011a;
2011b) used SVMs for ranking event precondi-
tions and (Cucerzan, 2012) and (Zhou et al., 2010)
employed GBDT for ranking entities. GBDT can
achieve high accuracy as they can easily combine
features of different scale and missing values. In
our experiments, GBDT outperforms both SVMs
and MaxEnt models.
We employ the stochastic version of GBDT
similar to (Friedman, 2001; Burges, 2010). Ba-
sically, the model performs a numerical optimiza-
tion in the function space by computing a function
approximation in a sequence of steps. By build-
ing a smaller decision tree at each step, the model
computes residuals obtained in the previous step.
Note that in the stochastic variant of GBDT, for
computing the loss function, the model absorbs
several samples instead of using the whole train-
ing data. The parameters for our GBDT model
were tuned on a development set sampled from
our Wikipedia dump independent from the train-
ing set. These parameters include the number of
regression trees and the shrinkage factor.
114
Figure 3: Architecture of the proposed sys-
tem. Every input document is processed by the
(Cucerzan, 2012) entity linking system and the
Stanford SUTime system. Temporal information
is then extracted automatically using RELCL and
DATECL.
5.1.4 Gathering Relevant Sentences
On the unseen test data, we apply our trained
model and obtain a score for each new sentence s
that contains mentions of entities q and z that are
in a targeted relationship by turning s into a feature
vector as shown previously. Among all sentences
that contain mentions of q and z, we choose the
top k with the highest score. The value of k was
tuned based on the performance of TSRF on our
development set.
5.1.5 Extracting Timestamps
To predict timestamps for each relation, we build
another classifier, DATECL similar to that de-
scribed in the previous section, by using language
models for ?Start?, ?End? and ?In? predictors of
relationship. The ?Start? model predicts T1, T2;
?End? predicts T3, T4 and ?In? predicts T2, T3.
Raw Trigger Features: Similar to previous
work by (Sil et al., 2010) on using discriminative
words as features, each of these models compose
of ?Trigger Words? that indicate when a relation-
ship begins or ends. In the current implemen-
tation, these triggers are chosen manually from
the language model automatically bootstrapped
from Wikipedia. Future directions include how
to automatically learn these triggers. For ex-
ample, for the per:spouse relation, the trig-
gers for ?Start? contain n-grams such as ?mar-
ried since DATE? and ?married SLOT FILLER
on?; the ?End? model contains n-grams such as
?estranged husband QUERY ENTITY?, ?split in
DATE?; the ?In? model contains ?happily mar-
ried?, ?QUERY ENTITY with his wife? etc.. For
an input sentence with query entity q and slot-
filler z, a first class of raw trigger features con-
sists of cosine-similarity(Text(q, z), Triggers(r))
where r ? Start, End, In. Here, Text(q, z) in-
dicates the full sentence as context. We also
employ another feature that computes cosine-
similarity(Context(q, z), Triggers(r)), which con-
structs a mini-sentence Context(q, z) from the
original by choosing windows of three words be-
fore and after q and z, and ignoring duplicates.
External Event Triggers: Our system also
considers the presence of other events as triggers
e.g. a ?death? event signaled by ?SLOT FILLER
died? might imply that a relationship ended on that
timestamp. Similarly, a ?birth? event can imply
that an entity started living in a particular location
e.g. the per:born-In(Obama, Honolulu)
relation from the sentence ?President Obama was
born in Honolulu in 1961? indicates that T1 =
1961-01-01 and T2 = 1961-12-31 for the rela-
tion per:cities of residence(Obama,
Honolulu).
At each step, TSRF extracts the top timestamps
for predicting ?Start?, ?End? and ?In? based on
the confidence values of DATECL. Similar to pre-
vious work by (Artiles et al., 2011), we aggregate
and update the extracted timestamps using the fol-
lowing heuristics:
Step 1: Initialize T= [-?, +?, -?,+ ?]
Step 2: Iterate through the classified timestamps
Step 3: For a new T
?
aggregate :
T&&T
?
= [max(t
1
, t
?
1
),min(t
2
, t
?
2
),
max(t
3
, t
?
3
),min(t
4
, t
?
4
)]
Update only if: t
1
? t
2
; t
3
? t
4
; t
1
? t
4
This novel two-step classification strategy re-
moves noise introduced by distant supervision
training and decides if the extracted (entity, filler,
timestamp) tuples belong to the relation under
consideration or not. For example, for the
per:spouse relation between the entities Brad
Pitt and Jennifer Aniston, TSRF extracts sentences
like ?..On November 22, 2001, Pitt made a guest
appearance in the television series Friends, play-
ing a man with a grudge against Rachel Green,
played by Jennifer Aniston..? and ?Pitt met Jen-
nifer Aniston in 1998 and married her in a private
wedding ceremony in Malibu on July 29, 2000..?.
Note that both sentences contain the query entity
and the slot filler. The system automatically re-
jects the extraction of temporal information from
115
S1 S2 S3 S4 S5 S6 S7 ALL StDev
Baseline 24.70 17.40 15.18 17.83 14.75 21.08 23.20 19.10 3.60
TSRF 31.94 36.06 32.85 40.12 33.04 31.85 27.35 33.15 3.66
RPI-Blender 31.19 13.07 14.93 26.71 29.04 17.24 34.68 23.42 7.98
UNED 26.20 6.88 8.16 15.24 14.47 14.41 19.34 14.79 6.07
CMU-NELL 19.95 7.46 8.47 16.52 13.43 5.65 11.95 11.53 4.77
Abby-Compreno 0.0 2.42 8.56 0.0 13.50 7.91 0.0 5.14 4.99
LDC 69.87 60.22 58.26 72.27 81.10 54.07 91.18 68.84 12.32
Table 3: Results for the TAC-TSF 2013 test set, overall and for individual slots. The slots notation is: S1:
org:top members employees, S2: per:city of residence, S3: per:country of residence, S4: per:employee
or member of, S5: per:spouse, S6: per:statesorprovince of residence, S7: per:title. The score for the
output created by the LDC experts is also shown.
the former even though the sentence contains men-
tions of both entities. This is because the language
model for the marriage relation does not match
well this candidate sentence, which is actually fo-
cussing on the two entities being in the different
relation of co-acting/appearing in the same mo-
tion picture. The latter sentence is determined as
matching the language model for the marriage re-
lation, and TSRF extracts the temporal scope July
29, 2000 and attaches the START label to it. Most
previous systems do not perform this noise re-
moval step, which is a critical component in our
distant supervision approach.
6 Experiments
For evaluation, we train our system on the infobox
tuples and sentences extracted from the Wikipedia
dump of May 2013. We set aside a portion of the
dump as our development data. We chose to use
the top-relevant n-grams based on the performance
on the development data as features. We employ
then the TAC evaluation data, which is publicly
available through LDC.
We utilize the evaluation metric developed for
TAC (Dang and Surdeanu, 2013). In order for a
temporal constraint (T1-T4) to be valid, the doc-
ument must justify both the query relation (which
is similar to the regular English slot filling task)
and the temporal constraint. Since the time in-
formation provided in text may be approximate,
the TAC metric measures the similarity of each
constraint in the key and system response. For-
mally, if the date in the gold standard is k
i
, while
the date hypothesized by the system is r
i
, and
d
i
= |k
i
? r
i
| is their difference measured in
years, then the score for the set of temporal con-
straints on a slot is computed as:
Score(slot) =
1
4
4
?
i=1
c
c + d
i
TAC sets the constant c to one year, so that pre-
dictions that differ from the gold standard by one
year get 50% credit. The absence of a constraint
in T1 or T3 is treated as a value of?? and the ab-
sence of a constraint in T2 or T4 is treated as +?,
which lead to zero-value terms in the scoring sum.
Therefore, the overall achievable score has a range
between 0 and 1.
We compare TSRF against four other TSF sys-
tems: (i) RPI-Blender (Artiles et al., 2011), (ii)
CMU-NELL (Talukdar et al. (2012a; 2012b)),
(iii) UNED (Garrido et al. (2011; 2012)) and (iv)
Abby-Compreno (Kozlova et al., 2012). Most of
these systems employ distant supervision strate-
gies too. RPI-Blender and UNED obtained the top
scores in the 2011 TAC TSF pilot evaluation, and
thus, could be considered as the state-of-the-art at
the time.
We also compare our system with a reasonable
baseline similar to (Ji et al., 2011). This baseline
makes the simple assumption that the correspond-
ing relation is valid at the document date. That
means that it creates a ?within? tuple as follows:
< ??, doc date, doc date, +? >. Hence, this
baseline system for a particular relation always
predicts T2 = T3 = the date of the document.
Table 3 lists the results obtained by our system
on the TAC test set of 201 queries, overall and for
each individual slot, in conjunction with the re-
sults of the other systems evaluated and the output
generated by the LDC human experts. Only two
out of the five systems evaluated, TSRF and RPI-
Blender, are able to beat the ?within? baseline.
TSRF achieves approximately 48% of human
performance (LDC) and outperforms all other sys-
116
TSF Accuracy SF F1 SF Prec SF Recall
LDC 68.8 83.1 97.3 72.5
TSRF 33.1 77.3 96.8 64.4
RPI-Blender 23.4 51.8 69.2 41.4
UNED 14.8 46.6 69.9 35.0
CMU-NELL 11.5 32.2 38.5 27.6
Abby-Compreno 5.1 18.5 53.6 11.2
Table 4: Extraction accuracy for slot-filler men-
tions. TSRF clearly outperforms all systems and
comes close to human performance (LDC).
tems in overall score, as well as for all individ-
ual relations with the exception of per:title,
for which RPI-Blender obtains a better score. In
fact, TSRF outperforms the next best systems
by 10 and 19 points. These two systems ob-
tained the top score in TAC 2011, and outper-
formed other systems such as Stanford (Surdeanu
et al., 2011). TSRF also outperforms CMU-
NELL which employs a very large KB of re-
lational facts already extracted from the Web
and makes use of the Google N-gram corpus
(http://books.google.com/ngrams).
We believe that this large performance differ-
ence is due in part to the fact that TSRF uses a
language model to clean up the noise introduced
by distant supervision before the actual temporal
classification step. Also, the learning algorithm
employed, GBDT, is highly effective in using the
extracted n-grams as features to decide whether
the extracted (entity, filler, time) tuples belong to
the relation under consideration or not. Finally,
Table 4 shows another reason that gives TSRF an
edge in obtaining the best score. The employed EL
component (Cucerzan, 2012) is a state-of-the-art
system for extracting and linking entities, and re-
solving coreference chains. By using this system,
we have been able to extract slot-filler mentions
with a precision of 96.8% at 66.4% recall, which
is substantially higher than the extraction results
of all other systems. Encouragingly, the perfor-
mance of this component also comes close to that
of the LDC annotators, which obtained a precision
of 97.3% at 72.5% recall.
It is also important to note that our system ex-
hibits a balanced performance on the relations
on which it was tested. As shown in column
StDev in Table 3, this system achieves the low-
est standard deviation in the performance across
the relations tested. It is interesting to note also
that TSRF achieves the best performance on the
employee of (S4) and city of residence
(S2) relations even though the system develop-
ment was done on the spouse relation (S1) as an
encouraging sign that our distant supervision al-
gorithm can be transferred successfully across re-
lations for domain-specific temporal scoping.
7 Conclusion and Future Work
The paper described an automatic temporal scop-
ing system that requires no manual labeling ef-
fort. The system uses distant supervision from
Wikipedia to obtain a large training set of tuples
for training. It uses a novel two-step classifica-
tion to remove the noise introduced by the dis-
tant supervision training. The same algorithm
was employed for multiple relations and exhibited
similarly high accuracy. Experimentally, the sys-
tem outperforms by a large margin several other
systems that address this relatively less explored
problem. Future directions of development in-
clude extracting joint slot filler names and tem-
poral information, and leveraging the changes ob-
served over time in Wikipedia for a query entity
and a slot filler in a target relation.
References
E. Agichtein and L. Gravano. 2000. Snowball: Ex-
tracting relations from large plain-text collections.
In Procs. of the Fifth ACM International Conference
on Digital Libraries.
Javier Artiles, Qi Li, Taylor Cassidy, Suzanne
Tamang, and Heng Ji. 2011. CUNY BLENDER
TACKBP2011 Temporal Slot Filling System De-
scription. In TAC.
Steven Bethard and James H Martin. 2007. Cu-tmp:
Temporal relation classification using syntactic and
semantic features. In Proceedings of the 4th Inter-
national Workshop on Semantic Evaluations, pages
129?132.
Chris Burges. 2010. From ranknet to lambdarank to
lambdamart: An overview. Learning, 11:23?581.
Andrew Carlson, Justin Betteridge, Bryan Kisiel,
Burr Settles, Estevam R Hruschka Jr, and Tom M
Mitchell. 2010. Toward an architecture for never-
ending language learning. In AAAI.
Nathanael Chambers, Shan Wang, and Dan Juraf-
sky. 2007. Classifying temporal relations between
events. In Proceedings of the 45th Annual Meeting
of the ACL on Interactive Poster and Demonstration
Sessions, pages 173?176.
Angel X Chang and Christopher Manning. 2012. Su-
time: A library for recognizing and normalizing time
expressions. In LREC, pages 3735?3740.
117
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In EMNLP-
CoNLL, pages 708?716.
Silviu Cucerzan. 2012. The MSR System for Entity
Linking at TAC 2012. In TAC.
Hoa Trang Dang and Mihai Surdeanu. 2013. Task
description for knowledge-base population at TAC
2013. In TAC.
O. Etzioni, M. Cafarella, D. Downey, S. Kok,
A. Popescu, T. Shaked, S. Soderland, D. Weld, and
A. Yates. 2004. Web-Scale Information Extraction
in KnowItAll. In WWW, New York City, New York.
R. Fikes and N. Nilsson. 1971. STRIPS: A new
approach to the application of theorem proving to
problem solving. Artificial Intelligence, 2(3/4):189?
208.
Jerome H Friedman. 2001. Greedy function approx-
imation: a gradient boosting machine. Annals of
Statistics, pages 1189?1232.
Guillermo Garrido, Bernardo Cabaleiro, Anselmo Pe-
nas, Alvaro Rodrigo, and Damiano Spina. 2011. A
distant supervised learning system for the tac-kbp
slot filling and temporal slot filling tasks. In TAC.
Guillermo Garrido, Anselmo Penas, Bernardo Ca-
baleiro, and Alvaro Rodrigo. 2012. Temporally an-
chored relation extraction. In ACL.
Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011.
Overview of the tac2011 knowledge base population
track. In TAC.
Heng Ji, Taylor Cassidy, Qi Li, and Suzanne Tamang.
2013. Tackling representation, annotation and clas-
sification challenges for temporal knowledge base
population. Knowledge and Information Systems,
pages 1?36.
Ekaterina Kozlova, Manicheva Maria, Petrova Elena,
and Tatiana Popova. 2012. The compreno semantic
model as an integral framework for a multilingual
lexical database. In 3rd Workshop on Cognitive As-
pects of the Lexicon (CogALex-III).
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In ACL, pages 1003?
1011.
James Pustejovsky and Marc Verhagen. 2009.
Semeval-2010 task 13: evaluating events, time ex-
pressions, and temporal relations (tempeval-2). In
Proceedings of the Workshop on Semantic Evalua-
tions: Recent Achievements and Future Directions,
pages 112?116.
James Pustejovsky, Jos?e M Castano, Robert Ingria,
Roser Sauri, Robert J Gaizauskas, Andrea Set-
zer, Graham Katz, and Dragomir R Radev. 2003.
Timeml: Robust specification of event and tempo-
ral expressions in text. New directions in question
answering, 3:28?34.
Avirup Sil and Alexander Yates. 2011a. Extracting
STRIPS representations of actions and events. In
RANLP.
Avirup Sil and Alexander Yates. 2011b. Machine
Reading between the Lines: A Simple Evaluation
Framework for Extracted Knowledge Bases. In
Workshop on Information Extraction and Knowl-
edge Acquisition (IEKA).
Avirup Sil, Fei Huang, and Alexander Yates. 2010.
Extracting action and event semantics fromweb text.
In AAAI Fall Symposium on Common-Sense Knowl-
edge (CSK).
Jannik Str?otgen and Michael Gertz. 2010. Heideltime:
High quality rule-based extraction and normaliza-
tion of temporal expressions. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pages 321?324.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In WWW.
Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-
Closky, Angel X Chang, Valentin I Spitkovsky, and
Christopher D Manning. 2011. Stanfords distantly-
supervised slot-filling system. In TAC.
Partha Pratim Talukdar, Derry Wijaya, and Tom
Mitchell. 2012a. Acquiring temporal constraints
between relations. In CIKM.
Partha Pratim Talukdar, Derry Wijaya, and Tom
Mitchell. 2012b. Coupled temporal scoping of rela-
tional facts. In WSDM.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 57?62.
Yafang Wang, Mingjie Zhu, Lizhen Qu, Marc Spaniol,
and Gerhard Weikum. 2010. Timely yago: harvest-
ing, querying, and visualizing temporal knowledge
from wikipedia. In Proceedings of the 13th Interna-
tional Conference on Extending Database Technol-
ogy, pages 697?700. ACM.
YafangWang, Bin Yang, Lizhen Qu, Marc Spaniol, and
Gerhard Weikum. 2011. Harvesting facts from tex-
tual web sources by constrained label propagation.
In CIKM, pages 837?846.
Daniel S. Weld, Raphael Hoffmann, and Fei Wu. 2009.
Using Wikipedia to Bootstrap Open Information Ex-
traction. In ACM SIGMOD Record.
Yiping Zhou, Lan Nie, Omid Rouhani-Kalleh, Flavian
Vasile, and Scott Gaffney. 2010. Resolving surface
forms to wikipedia topics. In COLING, pages 1335?
1343.
118
