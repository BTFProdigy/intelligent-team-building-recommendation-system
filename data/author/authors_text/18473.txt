Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 161?171, Dublin, Ireland, August 23-29 2014.
Learning Task-specific Bilexical Embeddings
Pranava Swaroop Madhyastha Xavier Carreras Ariadna Quattoni
TALP Research Center
Universitat Polit`ecnica de Catalunya
Campus Nord UPC, Barcelona
pranava,carreras,aquattoni@lsi.upc.edu
Abstract
We present a method that learns bilexical operators over distributional representations of words
and leverages supervised data for a linguistic relation. The learning algorithm exploits low-
rank bilinear forms and induces low-dimensional embeddings of the lexical space tailored for
the target linguistic relation. An advantage of imposing low-rank constraints is that prediction
is expressed as the inner-product between low-dimensional embeddings, which can have great
computational benefits. In experiments with multiple linguistic bilexical relations we show that
our method effectively learns using embeddings of a few dimensions.
1 Introduction
We address the task of learning functions that compute compatibility scores between pairs of lexical
items under some linguistic relation. We refer to these functions as bilexical operators. As an instance of
this problem, consider learning a model that predicts the probability that an adjective modifies a noun in
a sentence. In this case, we would like the bilexical operator to capture the fact that some adjectives are
more compatible with some nouns than others. For example, a bilexical operator should predict that the
adjective electronic has high probability of modifying the noun device but little probability of modifying
the noun case.
Bilexical operators can be useful for multiple NLP applications. For example, they can be used to
reduce ambiguity in a parsing task. Consider the following sentence extracted from a weblog: Vynil
can be applied to electronic devices and cases, wooden doors and furniture and walls. If we want to
predict the dependency structure of this sentence we need to make several decisions. In particular, the
parser would need to decide (1) Does electronic modify devices? (2) Does electronic modify cases? (3)
Does wooden modify doors? (4) Does wooden modify furniture? Now imagine that in the corpus used to
train the parser none of these nouns have been observed, then it is unlikely that these attachments can be
resolved correctly. However, if an accurate noun-adjective bilexical operator were available most of the
uncertainty could be resolved. This is because a good bilinear operator would give high probability to the
pairs electronic-device, wooden-door, wooden-furniture and low probability to the pair electronic-case.
The simplest way of inducing a bilexical operator is to learn it from a training corpus. That is, assuming
that we are given some data annotated with a linguistic relation between a modifier and a head (e.g.
adjective and noun) we can simply build a maximum likelihood estimator for Pr(m | h) by counting the
occurrences of modifiers and heads under the target relation. For example, we could consider learning
bilexical operators from sentences annotated with dependency structures. Clearly, this model can not
generalize to head words not present in the training data.
To mitigate this we could consider bilexical operators that can exploit lexical embeddings, such as
a distributional vector-space representation of words. In this case, we assume that for every word we
can compute an n-dimensional vector space representation ?(w) ? R
n
. This representation typically
captures distributional features of the context in which the lexical item can occur. The key point is that
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
161
we do not need a supervised corpus to compute the representation. All we need is a large textual corpus
to compute the relevant statistics. Once we have the representation we can exploit operations in the
induced vector space to define lexical compatibility operators. For example we could define a bilexical
operator as:
Pr(m | h) =
exp {??(m), ?(h)?}
?
m
?
exp {??(m
?
), ?(h)?}
(1)
where ??(x), ?(y)? denotes the inner-product. Alternatively, given an initial high-dimensional distribu-
tional representation computed from a large textual corpus we could first induce a projection to a lower
k dimensional space by performing truncated singular value decomposition. The idea is that the lower
dimensional representation will be more efficient and it will better capture the relevant dimensions of the
distributional representation. The bilexical operator would then take the form of:
Pr(m|h) =
exp {?U?(m), U?(h)?}
?
m
?
exp {?U?(m
?
), U?(h)?}
(2)
where U ? R
k?n
is the projection matrix obtained via SVD. The advantage of this approach is that as
long as we can estimate the distribution of contexts of words we can compute the value of the bilexical
operator. However, this approach has a clear limitation: to design a bilinear operator for a target linguistic
relation we must design the appropriate distributional representation. Moreover, there is no clear way of
exploiting a supervised training corpus.
In this paper we combine both the supervised and distributional approaches and present a learning
algorithm for inducing bilexical operators from a combination of supervised and unsupervised training
data. The main idea is to define bilexical operators using bilinear forms over distributional representa-
tions: ?(x)
>
W?(y), where W ? R
n?n
is a matrix of parameters. We can then train our model on the
supervised training corpus via conditional maximum-likelihood estimation. To induce a low-dimensional
representation, we first observe that the implicit dimensionality of the bilinear form is given by the rank
ofW . In practice controlling the rank ofW can result in important computational savings in cases where
one evaluates a target word x against a large number of candidate words y: this is because we can project
the representations ?(x) and ?(y) down to the low-dimensional space where evaluating the function is
simply an inner-product. This setting is in fact usual, for example for lexical retrieval applications (e.g.
given a noun, sort all adjectives in the vocabulary according to their compatibility), or for parsing (where
one typically evaluates the compatibility between all pairs of words in a sentence).
Consequently with these ideas, we propose to regularize the maximum-likelihood estimation using
a nuclear norm regularizer that serves as a convex relaxation to the rank function. To minimize the
regularized objective we make use of an efficient iterative proximal method that involves computing the
gradient of the function and performing singular value decompositions.
We test the proposed algorithm on several linguistic relations and show that it can predict modifiers
for unknown words more accurately than the unsupervised approach. Furthermore, we compare different
types of regularizers for the bilexical operatorW , and observe that indeed the low-rank regularizer results
in the most efficient technique at prediction time.
In summary, the main contributions of this paper are:
? We propose a supervised framework for learning bilexical operators over distributional representa-
tions, based on learning bilinear forms W .
? We show that we can obtain low-dimensional compressions of the distributional representation by
imposing low-rank constraints to the bilinear form. Combined with supervision, this results in
lexical embeddings tailored for a specific bilexical task.
? In experiments, we show that our models generalize well to unseen word pairs, using only a few
dimensions, and outperforming standard unsupervised distributional approaches. We also present
an application to prepositional phrase attachment.
162
2 Bilinear Models for Bilexical Predictions
2.1 Definitions
Let V be a vocabulary, and let x ? V denote a word. Let H ? V be a set of head words, andM? V be
a set of modifier words. In the noun-adjective relation example, H is the set of nouns andM is the set
of adjectives.
The task is as follows. We are given a training set of l tuples D = {(m,h)
1
, . . . , (m,h)
l
}, where
m ? M and h ? H and we want to learn a model of the conditional distribution Pr(m | h). We want
this model to perform well on all head-modifier pairs. In particular we will test the performance of the
model on heads that do not appear in D.
We assume that we are given access to a distributional representation function ? : V ? R
n
, where
?(x) is the n-dimensional representation of x. Typically, this function is computed from an unsupervised
corpus. We use ?(x)
[i]
to refer to the i-th coordinate of the vector.
2.2 Bilinear Model
Our model makes use of the bilinear form W : R
n
? R
n
? R, where W ? R
n?n
, and evaluates as
?(m)
>
W?(h). We define the bilexical operator as:
Pr(m | h) =
exp
{
?(m)
>
W?(h)
}
?
m
?
?M
exp {?(m
?
)
>
W?(h)}
(3)
Note that the above model is nothing more than a conditional log-linear model defined over n
2
fea-
tures f
i,j
(m,h) = ?(m)
[i]
?(h)
[j]
(this can be seen clearly when we write the bilinear form as
?
n
i=1
?
n
j=1
f
i,j
(m,h)W
i,j
. The reason why it is useful to regard W as a matrix will become evident in
the next section.
Before moving to the next section, let us note that the unsupervised SVD model in Eq. (2) is also a
bilinear model as defined here. This can be seen if we set W = UU
>
, which is a bilinear form of rank
k. The key difference is in the way W is learned using supervision.
3 Learning Low-rank Bilexical Operators
3.1 Low-rank Optimization
Given a training set D and a feature function ?(x) we can do standard conditional max-likelihood opti-
mization and minimize the negative of the log-likelihood function, log Pr(D):
?
(m,h)?D
?(m)
>
W?(h)? log
?
m
?
?M
exp
{
?(m
?
)
>
W?(h)
}
(4)
We would like to control the complexity of the learned model by including some regularization penalty.
Moreover, like in the low-dimensional unsupervised approach we want our model to induce a low-
dimensional representation of the lexical space. The first observation is that the bilinear form computes
a weighted inner product in some space. Consider the singular value decomposition: W = U?V . We
can write the bilinear form as: [?(m)
>
U ] ? [V ?(h)], thus we can regard m? = ?(m)
>
U as a projection
of m and
?
h = V ?(h) as a projection of h. Then the bilinear form can be written as:
?
n
i=1
?
[i,i]
m?
[i]
?
h
[i]
.
The rank of W defines the dimensionality of the induced space. It is easy to see that if W has rank k it
can be factorized as U?V where U ? R
n?k
and V ? R
k?n
.
Since the rank of W determines the dimensionality of the induced space, it would be reasonable to
add a rank minimization penalty in the objective in (4). Unfortunately this would lead to a non-convex
regularized objective. Instead, we propose to use as a regularizer a convex relaxation of the rank function,
the nuclear norm ?W?
?
(the `
1
norm of the singular values of W ). Putting it all together, our learning
algorithm minimizes:
?
(m,h)?D
? log Pr(m | h)) + ??W?
?
(5)
163
Here ? is a constant that controls the trade-off between fitting the data and the complexity of the model.
This objective is clearly convex since both the objective and the regularizer are convex. To minimize it
we use the a proximal gradient algorithm which is described next.
3.2 A Proximal Algorithm for Bilexical Operators
We now describe the learning algorithm that we use to induce the bilexical operators from training data.
We are interested in minimizing the objective (5), or in fact a more general version where we can replace
the regularizer ?W?
?
by standard `
1
or `
2
penalties. For any convex regularizer r(W ) (namely `
1
, `
2
or
the nuclear norm) the objective in (5) is convex. Our learning algorithm is based on a simple optimization
scheme known as forward-backward splitting (FOBOS) (Duchi and Singer, 2009).
This algorithm has convergence rates in the order of 1/
2
, which we found sufficiently fast for our
application. Many other optimization approaches are possible, for example one could express the regu-
larizer as a convex constraint and utilize a projected gradient method which has a similar convergence
rate. Proximal methods are slightly more simple to implement and we chose the proximal approach.
The FOBOS algorithm works as follows. In a series of iterations t = 1 . . . T compute parameter
matrices W
t
as follows:
1. Compute the gradient of the negative log-likelihood, and update the parameters
W
t+0.5
= W
t
? ?
t
g(W
t
)
where ?
t
=
c
?
t
is a step size and g(W
t
) is the gradient of the loss at W
t
.
2. Update W
t+0.5
to take into account the regularization penalty r(W ), by solving
W
t+1
= argmin
W
||W
t+0.5
?W ||
2
2
+ ?
t
?r(W )
For the regularizers we consider, this step is solved using the proximal operator associated with the
regularizer. Specifically:
? For `
1
it is a simple thresholding:
W
t+1
(i, j) = sign(W
t+0.5
(i, j)) ?max(W
t+0.5
(i, j)? ?
t
?, 0)
? For `
2
it is a simple scaling:
W
t+1
=
1
1 + ?
t
?
W
t+0.5
? For nuclear-norm, perform SVD thresholding. Compute the SVD to write W
t+0.5
= USV
>
with S a diagonal matrix and U, V orthogonal matrices. Denote by ?
i
the i-th element on the
diagonal of S. Define a new matrix
?
S with diagonal elements ??
i
= max(?
i
? ?
t
?, 0). Then
set
W
t+1
= U
?
SV
>
Optimizing a bilinear model using nuclear-norm regularization involves the extra cost of performing
SVD of W at each iteration. In our experiments the dimension of W was 2, 000? 2, 000 and computing
SVD was fast, much faster than computing the gradient, which dominates the cost of the algorithm. The
optimization parameters of the method are the regularization constant ?, the step size constant c and the
number of iterations T . In our experiments we ran a range of ? and c values for 200 iterations, and used
a validation set to pick the best configuration.
164
4 Related Work
Research in learning representations for natural language processing can be broadly classified into
two different paradigms based on the learning setting: unsupervised representation learning and semi-
supervised representation learning. Unsupervised representation learning does not require any supervised
training data, while semi-supervised representation learning requires the presence of supervised training
data with the potential advantage that it can adapt the representation to the task at hand.
Unsupervised approaches to learning representations mainly involve representations that are learned
not for a specific task, rather a variety of tasks. These representations rely more on the property of
abstractness and generalization. Further, unsupervised approaches can be roughly categorized into (a)
clustering-based approaches that make use of clusters induced using a notion of distributed similarity,
such as the method by Brown et al. (1992); (b) neural-network-based representations that focus on learn-
ing multilayer neural network in a way to extract features from the data (Morin and Bengio, 2005; Mnih
and Hinton, 2007; Bengio and S?en?ecal, 2008; Mnih and Hinton, 2009); (c) pure distributional approaches
that principally follow the distributional assumption that the words which share a set of contexts are sim-
ilar (Sahlgren, 2006; Turney and Pantel, 2010; Dumais et al., 1988; Landauer et al., 1998; Lund et al.,
1995; V?ayrynen et al., 2007).
We also induce lexical embeddings, but in our case we employ supervision. That is, we follow a
semi-supervised paradigm for learning representations. Semi-supervised approaches initially learn rep-
resentations typically in an unsupervised setting and then induce a representation that is jointly learned
for the task with a labeled corpus. A high-dimensional representation is extracted from unlabeled data,
while the supervised step compresses the representation to be low-dimensional in a way that favors the
the task at hand.
Collobert and Weston (2008) present a neural network language model, where given a sentence, it
performs a set of language processing tasks (from part of speech tagging, chunking, extracting named
entity, extracting semantic roles and decisions on the correctness of the sentence) by using the learned
representations. The representation itself is extracted from unlabeled corpora, while all the other tasks
are jointly trained on labeled corpus.
Socher et al. (2011) present a model based on recursive neural networks that learns vector space rep-
resentations for words, multi-word phrases and sentences. Given a sentence with its syntactic structure,
their model assings vector representations to each of the lexical tokens of the sentence, and then traverses
the syntactic tree bottom-up, such that at each node a vector representation of the corresponding phrase
is obtained by composing the vectors associated with the children.
Bai et al. (2010) use a technique similar to ours, using bilinear forms with low-rank constraints. In
their case, they explicitly look for a low-rank factorization of the matrix, making their optimization
non-convex. As far as we know, ours is the first convex formulation, where we employ a relaxation
of the rank (i.e. the nuclear norm) to make the objective convex. They apply the method to document
ranking, and thus optimize a max-margin ranking loss. In our application to bilexical models, we perform
conditional max-likelihood estimation. Hutchinson et al. (2013) propose an explicitly sparse and low-
rank maximum-entropy language model. The sparse plus low rank setting is learned in such a way that
the low rank component learns the regularities in the training data and the sparse component learns the
exceptions like multiword expressions etc.
Chechik et al. (2010) also learned bilinear operators using max-margin techniques, with pairwise
similarity as supervision, but they did not consider low-rank constraints.
One related area where bilinear operators are used to induce embeddings is distance metric learning.
Weinberger and Saul (2009) used large-margin nearest neighbor methods to learn a non-sparse embed-
ding, but these are computationally intensive and might not be suitable for large-scale tasks in NLP.
5 Experiments on Syntactic Relations
We conducted a set of experiments to test the ability of our algorithm to learn bilexical operators for
several linguistic relations. As supervised training data we use the gold standard dependencies of the
WSJ training section of the Penn Treebank (Marcus et al., 1993). We consider the following relations:
165
 50
 55
 60
 65
 70
 75
 80
 85
 90
1e3 1e4 1e5 1e6 1e7 1e8
pa
irw
ise
 a
cc
ur
ac
y
number of operations
Adjectives given Noun
unsupervised
NN
L1
L2
 66
 68
 70
 72
 74
 76
 78
 80
1e3 1e4 1e5 1e6 1e7 1e8
pa
irw
ise
 a
cc
ur
ac
y
number of operations
Nouns given Adjective
unsupervised
NN
L1
L2
 46
 48
 50
 52
 54
 56
 58
 60
 62
1e3 1e4 1e5 1e6 1e7 1e8
pa
irw
ise
 a
cc
ur
ac
y
number of operations
Objects given Verb
unsupervised
NN
L1
L2
 60
 65
 70
 75
 80
1e3 1e4 1e5 1e6 1e7 1e8
pa
irw
ise
 a
cc
ur
ac
y
number of operations
Verbs given Object
unsupervised
NN
L1
L2
Figure 1: Pairwise accuracy with respect to the number of double operations required to compute the
distribution over modifiers for a head word. Plots for noun-adjective and verb-object relations, in both
directions.
? Noun-Adjective: we model the distribution of adjectives given a noun; and a separate distribution
of nouns given an adjective.
? Verb-Object: we model the distribution of object nouns given a verb; and a separate distribution of
verbs given an object.
? Prepositions: in this case we consider bilexical operators associated with a preposition, which model
the probability of a head noun or verb above the preposition given the noun below the preposition.
We present results for prepositional relations given by ?with?, ?for?, ?in? and ?on?.
The distributional representation ?(x) was computed using the BLLIP corpus (Charniak et al., 2000).
We compute a bag-of-words representation for the context of each lexical item, that is ?(w)
[i]
corre-
sponds to the frequency of word i appearning in the context of w. We use a context window of size 10
and restrict our bag-of-words vocabulary to contain only the 2,000 most frequent words present in the
corpus. Vectors were normalized.
166
 50
 55
 60
 65
 70
 75
1e3 1e4 1e5 1e6 1e7 1e8
pa
irw
ise
 a
cc
ur
ac
y
number of operations
With
unsupervised
NN
L1
L2
 54
 56
 58
 60
 62
 64
 66
 68
 70
 72
1e3 1e4 1e5 1e6 1e7 1e8
pa
irw
ise
 a
cc
ur
ac
y
number of operations
For
unsupervised
NN
L1
L2
 46
 48
 50
 52
 54
 56
 58
1e3 1e4 1e5 1e6 1e7 1e8
pa
irw
ise
 a
cc
ur
ac
y
number of operations
In
unsupervised
NN
L1
L2
 60
 62
 64
 66
 68
 70
 72
 74
 76
 78
 80
1e3 1e4 1e5 1e6 1e7 1e8
pa
irw
ise
 a
cc
ur
ac
y
number of operations
On
unsupervised
NN
L1
L2
Figure 2: Pairwise accuracy with respect to the number of double operations required to compute the
distribution over modifiers for a head word. Plots for four prepositional relations: with, for, in, on. The
distributions are of verbs and objects above the preposition given the noun below the preposition.
To test the performance of our algorithm for each relation we partition the set of heads into a training
and a test set, 60% of the heads are use for training, 10% of the heads are used for validation and 30% of
the heads are used for testing. Then, we consider all observed modifiers in the data to form a vocabulary
of modifier words. The goal of this task is to learn conditional distribution over all these modifers given
a head word without context. In our experiments, the number of modifiers per relation ranges from 2,500
to 7,500 words. For each head word, we create a list of compatible modifiers from the annotated data, by
taking all modifiers that occur at least once with the head. Hence, for each head the set of all modifiers
is partitioned into compatible and non-compatible. For testing, we measure a pairwise accuracy, the
percentage of compatible/non-compatible pairs of modifiers where the former obtains higher probability.
Let us stress that none of the test head words has been observed in training, while the list of modifiers is
the same for training, validation and testing.
We compare the performance of the bilexical model trained with nuclear norm regularization (NN)
with other regularization penalties (L1 and L2). We also compare these supervised methods with an
167
Noun Predicted Adjectives
president executive, senior, chief, frank, former, international, marketing,
assistant, annual, financial
wife former, executive, new, financial, own, senior, old, other, deputy,
major
shares annual, due, net, convertible, average, new, high-yield, initial,
tax-exempt, subordinated
mortgages annualized, annual, three-month, one-year, average, six-month,
conventional, short-term, higher, lower
month last, next, fiscal, first, past, latest, early, previous, new, current
problem new, good, major, tough, bad, big, first, financial, long, federal
holiday new, major, special, fourth-quarter, joint, quarterly, third-quarter,
small, strong, own
Table 1: 10 most likely adjectives for some test nouns.
unsupervised model: a low-dimensional SVD model as in Eq. (2), which corresponds to an inner product
as in Eq. (1) when all dimensions are considered.
To report performance, we measure pairwise accuracy with respect to the capacity of the model in
terms of number of active parameters. To measure the capacity of a model we consider the number of
double operations that are needed to compute, given a head, the scores for all modifiers in the vocabulary
(we exclude the exponentiations and normalization needed to compute the distribution of modifiers given
a head, since this is a constant cost for all the models we compare, and is not needed if we only want to
rank modifiers). Recall that the dimension of ?(x) is n, and assume that there are m total modifiers in
the vocabulary. In our experiments n = 2, 000 and m ranges from 2, 500 to 7, 500. The correspondances
with operations are:
? Assume that the L1 and L2 models have k non-zero weights in W . Then the number of operations
to compute a distribution is km.
? Assume that the NN and the unsupervised models have rank k. We assume that the modifier vectors
are alredy projected down to k dimensions. For a new head, one needs to project it and perform m
inner products, hence the number of operations is kn+ km.
Figure 1 shows the performance of models for noun-adjective and verb-object relations, while Figure 2
shows plots for prepositional relations.
1
The first observation is that supervised approaches outperform
the unsupervised approach. In cases such as noun-adjetive relations the unsupervised approach performs
close to the supervised approaches, suggesting that the pure distributional approach can sometimes work.
But in most relations the improvement obtained by using supervision is very large. When comparing the
type of regularizer, we see that if the capacity of the model is unrestricted (right part of the curves), all
models tend to perform similarly. However, when restricting the size, the nuclear-norm model performs
much better. Roughly, 20 hidden dimensions are enough to obtain the most accurate performances
(which result in? 140, 000 operations for initial representaions of 2, 000 dimensions and 5, 000 modifier
candidates). As an example of the type of predictions, Table 1 shows the most likely adjectives for some
test nouns.
6 Experiments on PP Attachment
We now switch to a standard classification task, prepositional phrase attachment, that we frame as a
bilexical prediction task. We start from the formulation of the task as a binary classification problem by
Ratnaparkhi et al. (1994): given a tuple x = ?v, o, p, n? consisting of a verb v, noun object o, preposition
1
To obtain curves for each model type with respect to a range of number of operations, we first obtained the best model on
validation data and then forced it to have at most k non-zero features or rank k by projecting, for a range of k values.
168
 55
 60
 65
 70
 75
 80
for from with
att
ac
hm
en
t a
cc
ura
cy
bilinear L1
bilinear L2
bilinear NN
linear
interpolated L1
interpolated L2
interpolated NN
Figure 3: Attachment accuracies of linear, bilinear and interpolated models for three prepositions.
p and noun n, decide if the prepositional phrase p-n attaches to v (y = V) or to o (y = O). For example,
in ? meet,demand,for,products? the correct attachment is O.
Ratnaparkhi et al. (1994) define a linear maximum likelihood model of the form
Pr(y | x) = exp{?w, f(x, y)?} ? Z(x)
?1
, where f(x, y) is a vector of d features, w is a parame-
ter vector in R
d
, and Z(x) is the normalizer summing over y = {V, O}. Here we define a bilexical
model of the form that uses a distributional representation ?:
Pr(V|?v, o, p, n?) =
exp{?(v)
>
W
p
V
?(n)}
Z(x)
Pr(O|?v, o, p, n?) =
exp{?(o)
>
W
p
O
?(n)}
Z(x)
(6)
The bilinear model is parameterized by two matricesW
V
andW
O
per preposition, each of which captures
the compatibility between nouns below a certain preposition and heads of V or O prepositional relations,
respectively. Again Z(x) is the normalizer summing over y = {V, O}, but now using the bilinear form.
It is straighforward to modify the learning algorithm in Section 3 such that the loss is a negative log-
likelihood for binary classification, and the regularizer considers the sum of norms of the model matrices.
We ran experiments using the data by Ratnaparkhi et al. (1994). We trained separate models for
different prepositions, focusing on the prepositions that are more ambiguous: for, from, with.
We compare to a linear ?maxent? model following Ratnaparkhi et al. (1994) that uses the same feature
set. Figure 3 shows the test results for the linear model, and bilinear models trained with L1, L2, NN
regularization penalties. The results of the bilinear models are significantly below the accuracy of the
linear model, suggesting that some of the non-lexical features of the linear model (such as prior weighting
of the two classes) might be difficult to capture by the bilinear model over lexical representations. To
check if the bilinear model might complement the linear model or just be worse than it, we tested simple
combinations based on linear interpolations. For a constant ? ? [0, 1] we define:
Pr(y | x) = ? Pr
L
(y | x) + (1? ?) Pr
B
(y | x) . (7)
We search for the best ? on the validation set, and report results of combining the linear model with
each of the three bilinear models. Results are shown also in Figure 3. Interpolation models improve over
linear models, though only the improvement for for is significant (2.6%). Future work should exploit
finer combinations between standard linear features and distributional bilinear forms.
7 Conclusions
We have presented a model for learning bilexical operators that can leverage both supervised and unsu-
pervised data. The model is based on exploiting bilinear forms over distributional representations. The
169
learning algorithm induces a low-dimensional representation of the lexical space by imposing low-rank
constraints on the parameters of the bilinear form. By means of supervision, our model induces two
low-dimensional lexical embeddings, one on each side of the bilexical linguistic relation, and compu-
tations can be expressed as an inner-product between the two embeddings. This factorized form of the
model can have great computational advantages: in many applications one needs to evaluate the function
multiple times for a fixed set of lexical items, for example in dependency parsing. Hence, one can first
project the lexical items to their embeddings, and then compute all pairwise scores as inner-products. In
experiments, we have shown that the embeddings we obtain in a number of linguistic relations can be
modeled with a few hidden dimensions.
As future work, we would like to apply the low-rank approach to other model forms that can employ
lexical embeddings, specially when supervision is available. For example, dependency parsing models,
or models of predicate-argument structures representing semantic roles, exploit bilexical relations. In
these applications, being able to generalize to word pairs that are not observed during training is essential.
We would also like to study how to combine low-rank bilexical operators, which in essence induce
a task-specific representation of words, with other forms of features that capture class or contextual
information. One desires that such combinations can preserve the computational advantages behind
low-rank embeddings.
Acknowledgements
We thank the reviewers for their helpful comments. This work was supported by projects XLike (FP7-
288342), ERA-Net CHISTERA VISEN and TACARDI (TIN2012-38523-C02-00). Xavier Carreras was
supported by the Ram?on y Cajal program of the Spanish Government (RYC-2008-02223).
References
Bing Bai, Jason Weston, David Grangier, Ronan Collobert, Kunihiko Sadamasa, Yanjun Qi, Olivier Chapelle, and
Kilian Weinberger. 2010. Learning to rank with (a lot of) word features. Information Retrieval, 13(3):291?314,
June.
Yoshua Bengio and Jean-S?ebastien S?en?ecal. 2008. Adaptive importance sampling to accelerate training of a neural
probabilistic language model. IEEE Transactions on Neural Networks, 19(4):713?722.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Computational Linguistics, 18:467?479.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall, and Mark Johnson. 2000. BLLIP 1987?89 WSJ Corpus
Release 1, LDC No. LDC2000T43. Linguistic Data Consortium.
Gal Chechik, Varun Sharma, Uri Shalit, and Samy Bengio. 2010. Large scale online learning of image similarity
through ranking. Journal of Machine Learning Research, pages 1109?1135.
Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural
networks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning,
ICML ?08, pages 160?167, New York, NY, USA. ACM.
John Duchi and Yoram Singer. 2009. Efficient online and batch learning using forward backward splitting. Journal
of Machine Learning Research, 10:2899?2934.
Susan T. Dumais, George W. Furnas, Thomas K. Landauer, Scott Deerwester, and Richard Harshman. 1988. Using
latent semantic analysis to improve access to textual information. In SIGCHI Conference on Human Factors in
Computing Systems, pages 281?285. ACM.
Brian Hutchinson, Mari Ostendorf, and Maryam Fazel. 2013. Exceptions in language as learned by the multi-
factor sparse plus low-rank language model. In ICASSP, pages 8580?8584.
Thomas K. Landauer, Peter W. Foltz, and Darrell Laham. 1998. An introduction to latent semantic analysis.
Discourse Processes, 25:259?284.
Kevin Lund, Curt Burgess, and Ruth A. Atchley. 1995. Semantic and associative priming in high-dimensional
semantic space. In Cognitive Science Proceedings, LEA, pages 660?665.
170
Mitchell P. Marcus, Beatrice Santorini, and Mary A. Marcinkiewicz. 1993. Building a Large Annotated Corpus of
English: The Penn Treebank. Computational Linguistics, 19(2):313?330.
Andriy Mnih and Geoffrey E. Hinton. 2007. Three new graphical models for statistical language modelling. In
Proceedings of the 24th International Conference on Machine Learning, pages 641?648.
Andriy Mnih and Geoffrey E. Hinton. 2009. A scalable hierarchical distributed language model. In Advances in
Neural Information Processing Systems, pages 1081?1088.
Frederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural network language model. In AIS-
TATS05, pages 246?252.
Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos. 1994. A maximum entropy model for prepositional phrase
attachment. In Proceedings of the workshop on Human Language Technology, HLT ?94, pages 250?255,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Magnus Sahlgren. 2006. The Word-Space Model: Using distributional analysis to represent syntagmatic and
paradigmatic relations between words in high-dimensional vector spaces. Ph.D. thesis, Stockholm University.
Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011. Semi-
supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing, pages 151?161. Association for Computational Linguis-
tics.
Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37(1):141?188, January.
Jaakko J. V?ayrynen, Timo Honkela, and Lasse Lindqvist. 2007. Towards explicit semantic features using indepen-
dent component analysis. In Proceedings of the Workshop Semantic Content Acquisition and Representation
(SCAR), Stockholm, Sweden. Swedish Institute of Computer Science. SICS Technical Report T2007-06.
Kilian Q. Weinberger and Lawrence K. Saul. 2009. Distance metric learning for large margin nearest neighbor
classification. Journal of Machine Learning Research, 10:207?244, June.
171
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 624?635,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Unsupervised Spectral Learning of WCFG as Low-rank Matrix Completion
Raphae?l Bailly? Xavier Carreras? Franco M. Luque? Ariadna Quattoni?
? Universitat Polite`cnica de Catalunya
Barcelona, 08034
rbailly,carreras,aquattoni@lsi.upc.edu
? Universidad Nacional de Co?rdoba and CONICET
X500HUA Co?rdoba, Argentina
francolq@famaf.unc.edu.ar
Abstract
We derive a spectral method for unsupervised
learning of Weighted Context Free Grammars.
We frame WCFG induction as finding a Han-
kel matrix that has low rank and is linearly
constrained to represent a function computed
by inside-outside recursions. The proposed al-
gorithm picks the grammar that agrees with a
sample and is the simplest with respect to the
nuclear norm of the Hankel matrix.
1 Introduction
Weighted Context Free Grammars (WCFG) define
an important class of languages. Their expressivity
makes them good candidates for modeling a wide
range of natural language phenomena. This expres-
sivity comes at a cost: unsupervised learning of
WCFG seems to be a particularly hard task. And
while it is a well-studied problem, it is still to a great
extent unsolved.
Several methods for unsupervised learning of
WCFG have been proposed. Some rely on heuristics
that are used to build incrementally an approxima-
tion of the unknown grammar (Adriaans et al, 2000;
Van Zaanen, 2000; Tu and Honavar, 2008). Other
methods are based on maximum likelihood estima-
tion, searching for the grammar that has the largest
posterior given the training corpus (Baker, 1979;
Lari and Young, 1990; Pereira and Schabes, 1992;
Klein and Manning, 2002). Several Bayesian in-
ference approaches have also been proposed (Chen,
1995; Kurihara and Sato, 2006; Liang et al, 2007;
Cohen et al, 2010). These approaches perform pa-
rameter estimation by exploiting Markov sampling
techniques.
Recently, for the related problem of unsupervised
dependency parsing, Gormley and Eisner (2013)
proposed a new way of framing the max-likelihood
estimation. In their formulation the problem is ex-
pressed as an integer quadratic program subject to
non-linear constraints. They exploit techniques from
mathematical programming to solve the resulting
optimization.
In spirit, the work by Clark (2001; 2007) is prob-
ably the most similar to our approach since both ap-
proaches share an algebraic view of the problem. In
his case the key idea is to work with an algebraic
representation of a WCFG. The problem of recover-
ing the constituents of the grammar is reduced to the
problem of identifying its syntactic congruence.
In the last years, multiple spectral learning algo-
rithms have been proposed for a wide range of mod-
els (Hsu et al, 2009; Bailly et al, 2009; Bailly et al,
2010; Balle et al, 2011; Luque et al, 2012; Cohen
et al, 2012). Since the spectral approach provides a
good thinking tool to reason about distributions over
??, the question of whether they can be used for un-
supervised learning of WCFG seems natural. Still,
while spectral algorithms for unsupervised learning
of languages can learn regular languages, tree lan-
guages and simple dependency grammars, the fron-
tier to WCFG seems hard to reach.
In fact, the most recent theoretical results on spec-
tral learning of WCFG do not seem to be very en-
couraging. Recently, Hsu et al (2012) showed that
the problem of recovering the joint distribution over
PCFG derivations and their yields is not identifiable.
624
Although, for some simple grammar subclasses (e.g.
independent left and right children), identification in
the weaker sense (over the yields of the grammar)
implies strong identification (e.g. over joint distri-
bution of yields and derivations). In their paper, they
propose a spectral algorithm based on a generaliza-
tion of the method of moments for these restricted
subclasses.
Thus one open direction for spectral research con-
sists on defining subclasses of context free lan-
guages that can be learned (in the strong sense) from
observations of yields. Yet, an alternative research
direction is to consider learnability in the weaker
sense. In this paper we take the second road, and
focus on the problem of approximating the distribu-
tion over yields generated by a WCFG.
Our main contribution is to present a spectral al-
gorithm for unsupervised learning of WCFG. Fol-
lowing ideas from Balle et al (2012), the algo-
rithm is framed as a convex optimization where we
search for a low-rank matrix satisfying two types
of constraints: (1) Constraints derived from observ-
able statistics over yields; and (2) Constraints de-
rived from certain recurrence relations satisfied by a
WCFG. Our derivations of the learning algorithm il-
lustrate the main ingredients behind the spectral ap-
proach to learning functions over ?? which are: (1)
to exploit the recurrence relations satisfied by the
target family of functions and (2) provide algebraic
formulations of these relations.
We alert the reader that although we are able to
frame the problem as a convex optimization, the
number of variables involved is quite large and pro-
hibits a practical implementation of the method on
a realistic scenario. The experiments we present
should be regarded as examples designed to illus-
trate the behavior of the method. More research
is needed to make the optimization more efficient,
and we are optimistic that such improvements can
be achieved by exploiting problem-specific proper-
ties of the optimization. Regardless of this, ours is
a novel way of framing the grammatical inference
problem.
The rest of the paper is organized as follows. Sec-
tion 2 gives preliminaries on WCFG and the type of
functions we will learn. Section 3 establishes that
spectral methods can learn a WCFG from a Han-
kel matrix containing statistics about context-free
cuts. Section 4 presents the unsupervised algorithm,
where we formulate grammar induction as a low-
rank optimization. Section 5 presents experiments,
and finally we conclude the paper.
Notation Let ? be an alphabet. We use ? to de-
note an arbitrary symbol in ?. The set of all fi-
nite strings over ? is denoted by ??, where we
write ? for the empty string. We also use the set
?+ = ?? \ {?}.
We use bold letters to represent column vectors
v and matrices M . We use In to denote the n-
dimensional identity matrix. We use M+ to de-
note the Moore-Penrose pseudoinverse of some ma-
trixM . M?M ? is the Kronecker product between
matricesM ? Rm?n andM ? ? Rp?q resulting in a
matrix in Rmp?nq. The rest of notation will be given
as needed.
2 Weighted Context Free Grammars
In this section we define Weighted Context Free
Grammars (WCFG). We start with a classic defini-
tion and then describe an algebraic form of WCFG
that will be used throughout the paper. We also de-
scribe the fundamental recursions in WCFG.
2.1 WCFG in Classic Form
A WCFG over ? is a tuple G? =
?V,R, T, w?, wT , wR? where
? V is the set of non-terminal symbols. We as-
sume that V = {1, . . . , n} for some natural
number n, and that V ? ? = ?.
? R is a set of binary rules of the form i ? j k
where i, j, k ? V .
? T is a set of unary rules of the form i ? ?
where i ? V and ? ? ?.
? w? : V ? R, with w?(i) being the weight of
starting a derivation with non-terminal i.
? wT : V ? ? ? R, with wT (i ? ?) being the
weight of rule rewriting i into ?.
? wR : V ? V ? V ? R, with wR(i ? j k)
being the weight of rewriting i into j k.
A WCFG G? computes a function gG? : ?
+ ? R
defined as
gG?(x) =
?
i?V
w?(i)??G?(i
?
=? x) , (1)
625
where we define the inside function ??G? : V ??
+ ?
R recursively:
??G?(i
?
=? ?) = wT (i? ?) (2)
??G?(i
?
=? x) =
?
j,k?V
x1,x2??+
s.t. x=x1x2
wR(i? j k) (3)
??G?(j
?
=? x1)??G?(k
?
=? x2) ,
where in the second case we assume |x| > 1. The
inside function ??G?(i
?
=? x) exploits the fundamen-
tal inside recursion in WCFG (Baker, 1979; Lari and
Young, 1990). We will find useful to define the out-
side function ??G? : ?
??V ??? ? R defined recur-
sively as:
??G?(?; i;?) = w?(i) (4)
??G?(x; i; y) =
?
j,k?V
x1???,x2??+
s.t. x=x1x2
wR(j ? k i)? (5)
??G?(x1; j; y) ? ??G?(k
?
=? x2)
+
?
j,k?V
y1??+,y2???
s.t. y=y1y2
wR(j ? i k)?
??G?(x; j; y2) ? ??G?(k
?
=? y1) ,
where in the second case we assume that either
x 6= ? or y 6= ?.
For x, z ? ?? and y ? ?+ we have that
?
i?V
??G?(x; i; z) ? ??G?(i
?
=? y) (6)
is the weight that the grammar G? assigns to a string
xyz that has a cut or bracketing around y. Techni-
cally, it corresponds to the sum of the weights of all
derivations that have a constituent spanning y. In
particular we have that
gG?(x) =
?
i
??G?(?; i;?) ? ??G?(i
?
=? x) .
If x is a string of lengthm, and x[t:t?] is the substring
of x from positions t to t?, it also happens that
gG?(x) =
?
i
??G?(x[1:t?1]; i;x[t+1:m])???G?(i
?
=? x[t]))
for any t between 1 and m.
In this paper we will make frequent use of inside
and outside quantities. Notationally, for outsides the
semi-colon between two strings, i.e. x; z, will sim-
bolize a cut where we can insert an inside string y.
Finally, we note that Probabilistic Context Free
Grammars (PCFG) are a special case of WCFG
where: w?(i) is the probability to start a derivation
with non-terminal i; wR(i ? j k) is the condi-
tional probability of rewriting nonterminal i into j
and k; wT (i ? ?) is the probability of rewriting i
into symbol ?;
?
iw?(i) = 1; and for each i ? V ,?
j,k wR(i ? j k) +
?
? wT (i ? ?) = 1. Un-
der these conditions the function gG? is a probability
distibution over ?+.
2.2 WCFG in Algebraic Form
We now define a WCFG in algebraic form. A
Weighted Context Free Grammar (WCFG) over ?
with n states is a tuple G = ???, {??},A? with:
? An initial vector ?? ? Rn.
? Terminal vectors ?? ? R
n for ? ? ?.
? A bilinear operatorA ? Rn?n
2
.
A WCFG G computes a function gG : ?? ? R
defined as
gG(x) = ?
>
? ?G(x) (7)
where the inside function ?G : ?+ ? Rn is
?G(?) = ?? (8)
?G(x) =
?
x1,x2??+
x=x1x2
A(?G(x1)? ?G(x2)) (9)
We will define the outside function ?G : ?? ?
?? ? Rn as:
?G(?;?) = ?? (10)
?G(x; z)
> =
?
x1???,x2??+
x=x1x2
?G(x1; z)
>A(?G(x2)? In)
+
?
z1??+,z2???
z=z1z2
?G(x; z2)
>A(In ? ?G(z1)) (11)
For x, z ? ?? and y ? ?+ we have that
?G(x; z)
>?G(y) (12)
is the weight that the grammar assigns to the string
xyz with a cut around y. In particular, gG(x) =
?G(?;?)>?G(x).
626
Let us make clear that a WCFG is the same
device in classic or algebraic forms. If G? =
?V,R, T, w?, wT , wR? and G = ???, {??},A?, the
mapping is:
w?(i) = ??(i) (13)
wT (i? ?) = ??[i] (14)
wR(i? j k) = A[i, j, k] (15)
??G?(i
?
=? x) = ?G(x)[i] (16)
??G?(x; i; z) = ?G(x; z)[i] (17)
See Section A.1 for a proof of Eq. 16 and 17.
3 WCFG and Hankel Matrices
In this section we describe Hankel matrices for
WCFG. These matrices explicitly capture inside-
outside recursions employed by WCFG functions,
and are key to a derivation of a spectral learning al-
gorithm that learns a grammar G using statistics of
a training sample.
Let us define some sets. We say that I1 = ?+
is the set of inside strings. The set of composed in-
side strings I2 is the set of elements (x, x?), where
x, x? ? ?+. Intuitively (x, x?) represents two adja-
cent spans with an operation, i.e., it keeps the trace
of the operation that composes x with x? and yields
xx?. We will use the set I = I1 ? I2.
The set of outside contextsO is the set containing
elements ?x; z?, where x, z ? ??. Intuitively, ?x; z?
represents a context where we can insert an inside
element y in between x and z, yielding xyz.
Consider a function f : O ? I ? R. The Hankel
matrix of f is the bi-infinite matrix Hf ? RO?I
such thatHf (o, i) = f(o, i).
In practice we will work with finite sub-blocks of
Hf . To this end we will employ the notion of basis
B = (P,S), where {??, ??} ? P ? O is a set
of outside contexts and ? ? S ? I1 is a set of
inside strings. We will use p = |P| and s = |S|.
Furthermore, we define the inside completion of S
as the set S? = {(x, x?) | x, x? ? S}. Note that
S? ? I2. We say that B? = (P,S?) is the inside
completion of B.
The sub-block of Hf defined by B is the p ? s
matrix HB ? RP?S with HB(o, i) = Hf (o, i) =
f(o, i). In addition toHB, we are interested in these
additional finite vectors and matrices:
? h? ? RS is the s-dimensional vector with co-
ordinates h?(x) = f(??, ??, x).
? h? ? RP is the p-dimensional vector with co-
ordinates h?(o) = f(o, ?).
? HA ? RP?S
?
with HA(o, (x1, x2)) =
f(o, (x1, x2)).
3.1 Hankel Factorizations
If f is computed by a WCFG G, then Hf has rank
n factorization. To see this, consider the follow-
ing matrices. First a matrix S ? Rn?I
1
of inside
vectors for all strings, with column x taking value
Sx = ?G(x). Then a matrix P ? RO?n of out-
side vectors for all contexts, with row ?x; z? tak-
ing value P ?x;z? = ?G(x; z). It is easy to see that
Hf = PS, since Hf (?x; z?, y) = P ?x;z?Sy =
?G(x; z)>?G(y). ThereforeHf has rank n.
The same happens for sub-blocks. If HB is the
sub-block associated with basis B = (P,S), then
the sub-blocks P B ? RP?n and SB ? Rn?S of P
and S also accomplish that HB = P BSB . It also
happens that
h>? = ?
>
? SB (18)
h? = P B?? (19)
HA = P BA(SB ? SB) . (20)
We say that a basis B is complete for f if
rank(HB) = rank(Hf ). The following is a key
result for spectral methods.
Lemma 1. Let B = (P,S) be a complete basis of
dimension n for a function f and let HB ? RP?S
be the Hankel sub-block of f for B. Let h?, h? and
HA be the additional matrices for B. IfHB = PS
is a rank n factorization, then the WCFG G =
???, {??},A? with
?>? = h
>
? S
+ (21)
?? = P
+h? (22)
A = P+HA(S ? S)
+ (23)
computes f .
See proof in Section A.2.
627
3.2 Supervised Spectral Learning of WCFG
The spectral learning method directly exploits
Lemma 1. In a nutshell, the spectral method is:
1. Choose a complete basis B = (P,S) and a di-
mension n.
2. Use training data to compute estimates of the
necessary Hankel matrices: HB, h?, h?,HA.
3. Compute the SVD ofHB,HB = U?V >.
4. Create a truncated rank n factorization of HB
asP nSn, havingP n = Un?n andSn = V >n ,
where we only consider the top n singular val-
ues/vectors of ?,U ,V .
5. Use Lemma 1 to computeG, usingP n and Sn.
Because of Lemma 1, if B is complete and we
have access to the trueHB, h?, h?,HA of a WCFG
target function g?, then the algorithm will compute
a G that exactly computes g?. In practice, we only
have access to empirical estimates of the Hankel ma-
trices. In this case, there exist PAC-style sample
complexity bounds that state that gG will be a close
approximation to g? (Hsu et al, 2009; Bailly et al,
2009; Bailly et al, 2010).
The parameters of the algorithm are the basis and
the dimension of the grammar n. One typically em-
ploys some validation strategy using held-out data.
Empirically, the performance of these methods has
been shown to be good, and similar to that of EM
(Luque et al, 2012; Cohen et al, 2013). It is also
important to mention that in the case that the target
g? is a probability distribution, the function gG will
be close to g?, but it will only define a distribution in
the limit: in practice it will not sum to one, and for
some inputs it might return negative values. This is a
practical difficulty of spectral methods, for example
to apply evaluation metrics like perplexity which are
only defined for distributions.
4 Unsupervised Learning of WCFG
In the previous section we have exposed that if we
have access to estimates of a Hankel matrix of a
WCFG G, we can recover G. However, the statis-
tics in the Hankel require access to strings that have
information about context-free cuts. We will assume
that we only have access to statistics about plain
strings of a distribution, i.e. p(x), which we call
observations. In this scenario, one natural idea is
to search for a Hankel matrix that agrees with the
observations. The method we present in this sec-
tion frames this problem as a low-rank matrix op-
timization problem. We first characterize the space
of solutions to our problem, i.e. Hankel matrices
associated with WCFG that agree with observable
statistics. Then we present the method.
4.1 Characterization of a WCFG Hankel
In this section we describe valid WCFG Hankel ma-
trices using linear constraints.
We first describe an inside-outside basis that is
an extension of the one in the previous section. In-
side elements are the same, namely I = I1 ? I2,
where I1 are strings (x) and I2 are composed
strings (x, x?). The set of outside contexts O1 is
the set containing elements ?x; z?, defined as be-
fore. The set of composed outside contexts has el-
ements ?x, x?; z?, and ?x; z?, z?, where x, z ? ??
and x?, z? ? ?+. These outside contexts keep an
operation open in one of the sides. For example, if
we consider ?x; z?, z? and insert a string y, we obtain
x(y, z?)z, where we use (y, z?) to explicitly denote a
composed inside string. We will use O = O1 ? O2.
In this section, we will assume that I and O are
finite and closed. By closed, we mean that:
? (x) ? I ? (x1, x2) ? I for x = x1x2
? (x1, x2) ? I ? x1 ? I, x2 ? I
? ?x; z? ? O ? ?x1, x2; z? ? O for x = x1x2
? ?x; z? ? O ? ?x; z1, z2? ? O for z = z1z2
? ?x1, x2; z? ? O ? (x2) ? I
? ?x; z1, z2? ? O ? (z1) ? I
We will consider a Hankel matrix H ? RO?I .
Some entries of this matrix will correspond to ob-
servable quantities. Specifically, for any string x ?
I1 for which we know p(x) we can define the fol-
lowing observable constraint:
p(x) = H(??;??, (x)) (24)
The rest of entries of H correspond to a string
with an inside-outside cut, and these are not ob-
servable. Our method will infer the values of these
entries. The following constraints will ensure that
the matrix H is a well defined Hankel matrix for
WCFG:
628
? Hankel constraints: ? ?x; z? ? O, (y1, y2) ? I
H(?x; z?, (y1, y2)) = H(?x, y1; z?, (y2))
= H(?x; y2, z?, (y1)) (25)
? Inside constraints: ? o ? O, (x) ? I
H(o, (x)) =
?
x=x1x2
H(o, (x1, x2)) (26)
? Outside constraints: ? ?x; z? ? O, i ? I
H(?x; z?, i) =
?
x=x1x2
H(?x1, x2; z?, i)
+
?
z=z1z2
H(?x; z1, z2?, i) (27)
Constraint (25) states that composition operations
that result in the same structure should have the same
value. Constraints (26) and (27) ensure that the val-
ues in the Hankel follow the inside-outside recur-
sions that define the computations of a WCFG func-
tion. The following lemma formalizes this concept.
LetH? be the sub-block ofH restricted toO1?I1,
i.e. without compositions.
Lemma 2. If H satisfies constraints (25),(26) and
(27), and if rank(H) = rank(H?) then there exists
a WCFG that generatesH?.
See proof in Section A.3.
4.2 Convex Optimization
We now present the core optimization program be-
hind our method. Let vec(H) be a vector in R|O|?|I|
corresponding to all coefficients of H in column
vector form. Let O be a matrix such that O ?
vec(H) = z represents the observation constraints.
For example, if i-th row of O corresponds to the
Hankel coefficientH(??;??, (x)) then z(i) = p(x).
Let K be a matrix such that K ? vec(H) = 0 rep-
resents the constraints (25), (26) and (27).
The optimization problem is:
minimize
H
rank(H)
subject to ?O ? vec(H)? z?2 ? ?
K ? vec(H) = 0
?H?2 ? 1.
(28)
Intuitively, we look for H that agrees with the ob-
servable statistics and satisfies the inside-outside
constraints. ? is a parameter of the method that con-
trols the degree of error in fitting the observables z.
The ?H?2 ? 1 is satisfied by any Hankel matrix
derived from a true distribution, and is used to avoid
incoherent solutions.
The above optimization problem, however, is
computationally hard because of the rank objective.
We employ a common relaxation of the rank objec-
tive, based on the nuclear norm as in (Balle et al,
2012). The optimization is:
minimize
H
?H??
subject to ?O ? vec(H)? z?2 ? ?
K ? vec(H) = 0
?H?2 ? 1.
(29)
To optimize (29) we employ a projected gradient
strategy, similar to the FISTA scheme proposed by
Beck and Teboulle (2009). The method alternates
between separate projections for the observable con-
straints, the `2 norm, the inside-outside constraints,
and the nuclear norm. Of these, the latter two are the
most expensive.
Elsewhere, we develop theoretical properties of
the optimization (28) applied to finite-state transduc-
tions (Bailly et al, 2013). One can prove that there is
theoretical identifiability of the rank and the param-
eters of an FST distribution, using a rank minimiza-
tion formulation. However, this problem is NP-hard,
and it remains open whether there exists a polyno-
mial method with identifiability results. These re-
sults should generalize to WCFG.
5 Experiments
In this section we describe some experiments with
the learning algorithms for WCFG. Our goal is
to verify that the algorithms can learn some basic
context-free languages, and to study the possibility
of using them on real data.
5.1 Synthetic Experiments
We performed experiments on synthetic data, ob-
tained by choosing a PCFG with random parameters
(? [0, 1]), with a normalization step in order to get
a probability distribution. We built the Hankel ma-
trix from the inside basis {(x)}x?? and outside basis
629
 1e-06
 1e-05
 0.0001
 0.001
 0.01
 0.1
 1
 100  1000  10000  100000  1e+06
KL
 div
erg
enc
e
Sample size
Unsupervised SpectralSupervised SpectralUnsupervised EMSupervised EM
Figure 1: KL divergence for spectral and EM methods,
unsupervised and supervised, for different sizes of learn-
ing sample, on log-log scales. Results are averages over
50 random target PCFG with 2 states and 2 symbols.
{??;??} ? {?x;??, ??;x?}x??. The composed in-
sides for the operator matrix are thus {(x, y)}x,y??.
The matrix in the optimizer has the following struc-
ture
H =
?
?
?
?
?
(y) ? ? ? (y, z)
??;?? (?; y;?) ? ? ? (?; y, z;?)
?x;?? (x; y;?) ? ? ? (x; y, z;?)
??;x? (?; y;x) ? ? ? (?; y, z;x)
... ? ? ? ? ? ? ? ? ?
?
?
?
?
?
The constraints we use are:
K ={H((x; y;?)) = H((?;x; y))}x,y???
{H((?;x; y)) = H((?;x, y;?))}x,y???
{H((x; y;?)) = H((?;x, y;?))}x,y??
and
O ={H((?;x;?)) = pS(x)}x?? ?
{H((?;x; y)) = pS(xy)}x,y?? ?
{H((x; y, z;?)) +H((?;x, y; z)) = pS(xyz)}x,y,z??
We use pS to denote the empirical distribution.
Those are simplified versions of the Hankel, inside,
outside and observation constraints. The set O is
built from the following remarks: (1) (xy) = (x, y)
and (2) (xyz) = (xy, z)+(x, yz). The method uses
statistics for sequences up to length 3.
The algorithm we use for the unsupervised spec-
tral method is a simplified version: we use alter-
natively a hard projection on the constraints (by
 1e-08
 1e-07
 1e-06
 1e-05
 0.0001
 0.001
 0.01
 0.1
 1000  10000  100000  1e+06  1e+07  1e+08  1e+09
KL
 div
erg
enc
e
Sample size
Unsupervised SpectralSupervised Spectral
Figure 2: KL divergence for unsupervised and supervised
spectral methods, for different sizes of learning sample,
on log-log scales. Results are averages over 50 random
target PCFG with 3 states and 6 symbols.
projecting iteratively on each constraint), and a
thresholding-shrinkage operation for the target di-
mension. We use the same trick as FISTA for the
update. We finally use the regular spectral method
on this matrix to get our model.
We compare this method with an unsupervised
EM, and also with supervised versions of spectral
method and EM. We compare the accuracy of the
different models in terms of KL-divergence for se-
quences up to length 10. We run 50 optimization
steps for the unsupervised spectral method, and 200
iterations for the EM methods. Figure 1 shows the
results, corresponding the the geometric mean over
50 experiments on random targets of 2 symbols and
2 states.
For sample size greater than 105, the unsupervised
spectral method seems to provide better solutions
than both EM and supervised EM. The solution, in
terms of KL-divergence, is comparable to the one
obtained with the supervised spectral method. The
computation time of unsupervised spectral method
is almost constant w.r.t. the sample size, around
1.67s, while computation time of unsupervised EM
(resp. supervised EM) is 6.103s (resp. 2.104s) for
sample size 106.
Figure 2 presents learnings curve for random tar-
gets with 3 states and 6 symbols. One can see that,
for big sample sizes (109), the unsupervised spectral
method is losing accuracy compared to the super-
vised method. This is due to a lack of information,
630
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 0  0.01  0.02  0.03  0.04  0.05
L1
 dis
tan
ce
Basis factor
Spectral WFAUnsupervised SpectralSupervised Spectral
Figure 3: Learning errors for different models in terms of
the size of the basis.
and could be overcome by considering a greater ba-
sis (e.g. inside sequences up to length 2 or 3).
5.2 Dyck Languages
We now present experiments using the following
PCFG:
S ? S S (0.2) | aS b (0.4) | a b (0.4)
This PCFG generates a probabilistic version of the
well-known Dyck language or balanced parenthesis
language, an archetypical context-free language.
We do experiments with the following models and
algorithms:
? WFA: a Weighted Finite Automata learned us-
ing spectral methods as described in (Luque et
al., 2012). Parameters: number of states and
size of basis.
? Supervised Spectral: a WCFG learned from
structured strings using the algorithm of sec-
tion 3.2. We choose as basis the most frequent
insides and outsides observed in the training
data. The size of the basis is determined by a
parameter f called the basis factor, that deter-
mines the proportion of total insides and out-
sides that will be in the basis.
? Unsupervised Spectral: a WCFG learned from
strings using the algorithm of Section 4. The
basis is like in the supervised case, but since
context-free cuts in the strings are not observed,
basis size ofH obs. i/o ctr.
1 ? 11 39 ? 159 34 162
6 ? 14 1,163 ? 764 146 6,360
12 ? 18 4,462 ? 2,239 322 25,374
18 ? 22 9,124 ? 4,149 479 52,524
24 ? 26 15,755 ? 6,858 657 89,718
30 ? 29 19,801 ? 8,545 769 112,374
36 ? 34 27,989 ? 11,682 916 156,690
42 ? 37 3,638 ? 15,026 1,035 200,346
48 ? 41 45,192 ? 18,235 1,157 244,398
54 ? 45 53,741 ? 21,196 1,281 284,466
60 ? 48 60,844 ? 23,890 1,382 318,354
Table 1: Problem sizes for the WSJ10 training corpus.
basis / n 5 10 15 20
1 ? 11 1.265 10?3
6 ? 14 7.06 10?4 6.92 10?4
12 ? 18 7.30 10?4 6.28 10?4 6.01 10?4
18 ? 22 7.31 10?4 6.29 10?4 5.84 10?4 5.59 10?4
24 ? 26 7.35 10?4 6.39 10?4 5.88 10?4 5.31 10?4
30 ? 29 7.34 10?4 6.41 10?4 5.86 10?4 5.30 10?4
Table 2: Experiments with the unsupervised spectral
method on the WSJ10 corpus. Results are in terms of
expected L1 on the training set, for different basis and
numbers of states.
all possible inside and outsides of the sample
(i.e. all possible substrings and contexts) are
considered.
We generate a training set by sampling 4,000
strings from the target PCFG and counting the rel-
ative frequency of each. For the supervised model,
we generate strings paired with their context-free
derivation. To measure the quality of the learned
models, we use the L1 distance to the target distri-
bution over a fixed set of strings ??n, for n = 7.1
Figure 3 shows the results for the different mod-
els and for different basis sizes (in terms of the basis
factor f ). Here we can clearly see that the WCFG
models, even the unsupervised one, outperform the
WFA in reproducing the target distribution.
5.3 Natural Language Experiments
Now we present some preliminar tests using natural
language data. For these tests, we used the WSJ10
subset of the Penn Treebank, as Klein and Manning
(2002). This dataset consists of the sentences of
length ? 10 after filtering punctuation and currency.
We removed lexical items and mapped the POS tags
1Given two functions f1 and f2 over strings, the L1 distance
is the sum of the absolute difference over all strings in a set:
?
x |f1(x)? f2(x)|.
631
to the Universal Part-of-Speech Tagset (Petrov et al,
2012), reducing the alphabet to a set of 11 symbols.
Table 1 shows the size of the problem for differ-
ent basis sizes. As described in the previous sub-
section for the unsupervised case, we obtain the ba-
sis by taking the most frequent observed substrings
and contexts. We then compute all yields that can
be generated with this basis, and close the basis to
include all possible insides and outsides with oper-
ations completions, such that we create a Hankel as
described in Section 4.1. Table 1 shows, for each
base, the size of H we induce, the number of ob-
servable constraints (i.e. sentences we train from),
and the number of inside-outside constraints.
With the current implementation of the optimizer
we were only able to run the unsupervised learning
for small basis sizes. Table 2 shows the expected L1
on training data. For a fixed basis, as we increase
the number of states we see that the error decreases,
showing that the method is inducing a Hankel matrix
that explains the observable statistics.
6 Conclusions
We have presented a novel approach for unsuper-
vised learning of WCFG. Our method combines in-
gredients of spectral learning with low-rank convex
optimization methods.
Our method optimizes over a matrix that, even if it
grows polynomially with respect to the size of train-
ing, results in a large problem. To scale the method
to learn languages of the complexity of natural lan-
guages we would need to identify optimization algo-
rithms specially suited for this problem.
A Proofs
A.1 Proof of Inside-Outside Eq. 16 and 17
For the inside function, the base case is trivial. By
induction:
?G(x)[i] =
?
x=x1x2
A(?G(x1)? ?G(x2))[i]
=
?
j,k?V
x=x1x2
A[i, j, k] ? ?G(x1)[j] ? ?G(x2)[k]
=
?
j,k?V
x=x1x2
wR(i? j k) ? ??G?(j
?
=? x1) ? ??G?(k
?
=? x2)
= ??G?(i
?
=? x)
For the outside function, let ei be an n-
dimensional vector with coordinate i to 1 and the
rest to 0. We reformulate the mapping as:
?G(x; z)
>ei = ??G?(x; i; z) (30)
The base case is trivial by definitions. We use the
property of Kronecker products that (v ? In)v? =
(v? v?) and (In? v)v? = (v?? v) for v,v? ? Rn.
We first look at one of the terms of ?G(x; z)>ei:
?G(x1; z)
>A(?G(x2)? In)ei
= ?G(x1; z)
>A(?G(x2)? ei)
=
?
j,k?V
(?G(x1; z)
>ej) ?A[j, k, i] ? ?G(x2)[k]
=
?
j,k?V
??G?(x1; j; z) ? wR(j ? k i) ? ??G?(k
?
=? x2)
Applying the distributive property in ?G(x; z)>ei it
is easy to see that all terms are mapped to the corre-
sponding term in ??G?(x; i; z).
A.2 Proof of Lemma 1
Let G? = ????, {?
?
?},A
?? be a WCFG for f that in-
duces a rank factorizationH = P ?S?. We first show
that there exists an invertible matrixM that changes
the basis of the operators of G into those of G?.
Define M = S?S+ and note that P+P ?S?S+ =
P+HS+ = I implies that M is invertible with
M?1 = P+P ?. We now check that the operators
of G correspond to the operators of G? under this
change of basis. First we see that
A = P+HA(S ? S)
+
= P+P ?A?(S? ? S?)(S ? S)+
= M?1A?(S?S+ ? S?S+)
= M?1A?(M ?M) .
Now, since h? = ??>? S
? and h? = P ???? , it follows
that ?>? = ?
?
?
>M and ?? = M
?1???.
Finally we check that G and G? compute the
same function, namely f(o, i) = ?G(o)>?G(i) =
?G?(o)>?G?(i). We first see that ?G(x) =
632
M?1?G?(x):
?G(?) = ?? = M
?1??? (31)
?G(x) =
?
x=x1x2
A(?G(x1)? ?G(x2)) (32)
=
?
x=x1x2
M?1A?(M ?M)(?G(x1)? ?G(x2))
= M?1
?
x=x1x2
A?(M?G(x1)?M?G(x2))
= M?1
?
x=x1x2
A?(?G?(x1)? ?G?(x2))
It can also be shown that ?G(x; z)> =
?G?(x; z)>M . One must see that in any term:
?G(x1; z)
>A(?G(x2)? In) (33)
= ?G(x1; z)
>M?1A?(M ?M)(?G(x2)? In)
= ?G?(x1; z)
>A?(M?G(x2)?MIn)
= ?G?(x1; z)
>A?(?G?(x2)? In)M
and the relation follows. Finally:
?G(x; z)
>?G(y) (34)
= ?G?(x; z)
>MM?1?G?(y)
= ?G?(x; z)
>?G?(y)
A.3 Proof of Lemma 2
We will use the following sub-blocks ofH:
? H? is the sub-block restricted to O1 ? I1, i.e.
without compositions.
? HA is the sub-block restricted to O1 ? I2, i.e.
inside compositions.
? H ?A is the sub-block restricted to O
2 ? I1, i.e.
outside compositions.
? h>? ? R
I1 is the row ofH? for ??;??.
? h(x) ? RO
1
is the column ofH? for (x).
? h(x1,x2) ? R
O1 is the column of HA for
(x1, x2).
? h??x;z? ? R
I1 is the row of h? for ?x; z?.
? h??x1,x2;z? and h
?
?x;z1,z2? be the rows in R
I1 of
h?A for ?x1, x2; z? and ?x; z1, z2?).
One supposes that rank(H?) = rank(H). We de-
fine G as
?>? = h
>
?H
+
? , ?a = h(a),A = HA(H
+
? ?H
+
? )
Lemma 3. One has that ?G(x) = h(x), and
?G(x1, x2) = h(x1,x2).
Proof. By induction. For sequences of size 1, one
has ?G(x) = ?x = h(x). For the recursive case,
let e(x) be a vector in RI
1
with 1 in the coordinate
of (x) in H?. Let e(x,y) be a vector in RI
2
with 1
in the coordinate of (x, y) in HA. For ?G(x, y),
one has H+? ?G(x) = e(x), and H
+
? ?G(y) =
e(y), thus H
+
? ?G(x) ? H
+
? ?G(y) = e(x,y) and
HA(H+? ?G(x) ? H
+
? ?G(y)) = h(x,y). Finally,
one has that ?G(x) =
?
x=x1x2 ?G(x1, x2) =?
x=x1x2 h(x1,x2) = h(x1x2x3) by the equation
(26).
One has a symmetric result for outside vectors. We
define G? as
?>? = h
>
? , ?a = H
+
? h(a),A = H
+
?HA
Lemma 4. One has that ?G?(?x; z?)> =
h??x;z?, ?G?(?x1, x2; z?)
> = h??x1,x2;z? and
?G?(?x; z1, z2?)> = h
?
?x;z1,z2?.
Proof. (Sketch) Equation (31) is used in the same
way than (27) before. Equation (25) is used to en-
sure a link betweenH ?A andHA.
Let g be the mapping computed by G and
G?. One has that g(o, i) = ?G?(o)>?G?(i) =
?G(o)>?G(i) = ?G?(o)>H+? ?G(i) = H?(o, i).
Acknowledgements We are grateful to Borja Balle
and the anonymous reviewers for providing us with help-
ful comments. This work was supported by a Google
Research Award, and by projects XLike (FP7-288342),
ERA-Net CHISTERA VISEN, TACARDI (TIN2012-
38523-C02-02), BASMATI (TIN2011-27479-C04-03),
SGR-GPLN (2009-SGR-1082) and SGR-LARCA (2009-
SGR-1428). Xavier Carreras was supported by the
Ramo?n y Cajal program of the Spanish Government
(RYC-2008-02223). Franco M. Luque was supported by
the National University of Co?rdoba and by a Postdoc-
toral fellowship of CONICET, Argentinian Ministry of
Science, Technology and Productive Innovation.
633
References
Pieter Adriaans, Marten Trautwein, and Marco Vervoort.
2000. Towards high speed grammar induction on large
text corpora. In SOFSEM 2000: Theory and Practice
of Informatics, pages 173?186. Springer.
Raphae?l Bailly, Franois Denis, and Liva Ralaivola. 2009.
Grammatical inference as a principal component anal-
ysis problem. In Le?on Bottou and Michael Littman,
editors, Proceedings of the 26th International Confer-
ence on Machine Learning, pages 33?40, Montreal,
June. Omnipress.
Raphae?l Bailly, Amaury Habrard, and Franc?ois Denis.
2010. A spectral approach for probabilistic grammat-
ical inference on trees. In Proceedings of the 21st
International Conference Algorithmic Learning The-
ory, Lecture Notes in Computer Science, pages 74?88.
Springer.
Raphae?l Bailly, Xavier Carreras, and Ariadna Quattoni.
2013. Unsupervised spectral learning of finite-state
transducers. In Advances in Neural Information Pro-
cessing Systems 26.
James K. Baker. 1979. Trainable grammars for speech
recognition. In D. H. Klatt and J. J. Wolf, editors,
Speech Communication Papers for the 97th Meeting
of the Acoustical Society of America, pages 547?550.
Borja Balle, Ariadna Quattoni, and Xavier Carreras.
2011. A spectral learning algorithm for finite state
transducers. In Proceedings of ECML PKDD, pages
156?171.
Borja Balle, Ariadna Quattoni, and Xavier Carreras.
2012. Local loss optimization in operator models: A
new insight into spectral learning. In John Langford
and Joelle Pineau, editors, Proceedings of the 29th In-
ternational Conference on Machine Learning (ICML-
2012), ICML ?12, pages 1879?1886, New York, NY,
USA, July. Omnipress.
Amir Beck and Marc Teboulle. 2009. A fast iter-
ative shrinkage-thresholding algorithm for linear in-
verse problems. SIAM J. Img. Sci., 2(1):183?202,
March.
Stanley F Chen. 1995. Bayesian grammar induction for
language modeling. In Proceedings of the 33rd annual
meeting on Association for Computational Linguistics,
pages 228?235. Association for Computational Lin-
guistics.
Alexander Clark. 2001. Unsupervised induction of
stochastic context-free grammars using distributional
clustering. In Proceedings of the 2001 workshop on
Computational Natural Language Learning-Volume 7,
page 13. Association for Computational Linguistics.
Alexander Clark. 2007. Learning deterministic context
free grammars: The omphalos competition. Machine
Learning, 66(1):93?110.
Shay B. Cohen, David M. Blei, and Noah A. Smith.
2010. Variational inference for adaptor grammars.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 564?
572, Los Angeles, California, June. Association for
Computational Linguistics.
Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.
Foster, and Lyle Ungar. 2012. Spectral learning of
latent-variable pcfgs. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 223?231,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.
Foster, and Lyle Ungar. 2013. Experiments with spec-
tral learning of latent-variable pcfgs. In Proceedings
of the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 148?157, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Matthew Gormley and Jason Eisner. 2013. Nonconvex
global optimization for latent-variable models. In Pro-
ceedings of the 51st Annual Meeting of the Association
for Computational Linguistics (ACL), Sofia, Bulgaria,
August. 11 pages.
Daniel Hsu, Sham M. Kakade, and Tong Zhang. 2009. A
spectral algorithm for learning hidden markov models.
In Proceedings of the Annual Conference on Compu-
tational Learning Theory (COLT).
Daniel Hsu, Sham Kakade, and Percy Liang. 2012.
Identifiability and unmixing of latent parse trees. In
P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou,
and K.Q. Weinberger, editors, Advances in Neural In-
formation Processing Systems 25, pages 1520?1528.
Dan Klein and Christopher D Manning. 2002. A genera-
tive constituent-context model for improved grammar
induction. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
128?135. Association for Computational Linguistics.
Kenichi Kurihara and Taisuke Sato. 2006. Variational
bayesian grammar induction for natural language. In
Grammatical Inference: Algorithms and Applications,
pages 84?96. Springer.
Karim Lari and Steve J Young. 1990. The estimation
of stochastic context-free grammars using the inside-
outside algorithm. Computer speech & language,
4(1):35?56.
Percy Liang, Slav Petrov, Michael I Jordan, and Dan
Klein. 2007. The infinite PCFG using hierarchical
dirichlet processes. In EMNLP-CoNLL, pages 688?
697.
634
Franco M. Luque, Ariadna Quattoni, Borja Balle, and
Xavier Carreras. 2012. Spectral learning for non-
deterministic dependency parsing. In Proceedings
of the 13th Conference of the European Chapter of
the Association for Computational Linguistics, pages
409?419, Avignon, France, April. Association for
Computational Linguistics.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed corpora.
In Proceedings of the 30th Annual Meeting of the As-
sociation for Computational Linguistics, pages 128?
135, Newark, Delaware, USA, June. Association for
Computational Linguistics.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC, May.
Kewei Tu and Vasant Honavar. 2008. Unsupervised
learning of probabilistic context-free grammar using
iterative biclustering. In Grammatical Inference: Al-
gorithms and Applications, pages 224?237. Springer.
Menno Van Zaanen. 2000. Abl: Alignment-based learn-
ing. In Proceedings of the 18th conference on Compu-
tational linguistics-Volume 2, pages 961?967. Associ-
ation for Computational Linguistics.
635
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 409?419,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Spectral Learning for Non-Deterministic Dependency Parsing
Franco M. Luque
Universidad Nacional de Co?rdoba
and CONICET
Co?rdoba X5000HUA, Argentina
francolq@famaf.unc.edu.ar
Ariadna Quattoni and Borja Balle and Xavier Carreras
Universitat Polite`cnica de Catalunya
Barcelona E-08034
{aquattoni,bballe,carreras}@lsi.upc.edu
Abstract
In this paper we study spectral learning
methods for non-deterministic split head-
automata grammars, a powerful hidden-
state formalism for dependency parsing.
We present a learning algorithm that, like
other spectral methods, is efficient and non-
susceptible to local minima. We show
how this algorithm can be formulated as
a technique for inducing hidden structure
from distributions computed by forward-
backward recursions. Furthermore, we
also present an inside-outside algorithm
for the parsing model that runs in cubic
time, hence maintaining the standard pars-
ing costs for context-free grammars.
1 Introduction
Dependency structures of natural language sen-
tences exhibit a significant amount of non-local
phenomena. Historically, there have been two
main approaches to model non-locality: (1) in-
creasing the order of the factors of a dependency
model (e.g. with sibling and grandparent relations
(Eisner, 2000; McDonald and Pereira, 2006; Car-
reras, 2007; Martins et al 2009; Koo and Collins,
2010)), and (2) using hidden states to pass in-
formation across factors (Matsuzaki et al 2005;
Petrov et al 2006; Musillo and Merlo, 2008).
Higher-order models have the advantage that
they are relatively easy to train, because estimat-
ing the parameters of the model can be expressed
as a convex optimization. However, they have
two main drawbacks. (1) The number of param-
eters grows significantly with the size of the fac-
tors, leading to potential data-sparsity problems.
A solution to address the data-sparsity problem
is to explicitly tell the model what properties of
higher-order factors need to be remembered. This
can be achieved by means of feature engineering,
but compressing such information into a state of
bounded size will typically be labor intensive, and
will not generalize across languages. (2) Increas-
ing the size of the factors generally results in poly-
nomial increases in the parsing cost.
In principle, hidden variable models could
solve some of the problems of feature engineering
in higher-order factorizations, since they could
automatically induce the information in a deriva-
tion history that should be passed across factors.
Potentially, they would require less feature engi-
neering since they can learn from an annotated
corpus an optimal way to compress derivations
into hidden states. For example, one line of work
has added hidden annotations to the non-terminals
of a phrase-structure grammar (Matsuzaki et al
2005; Petrov et al 2006; Musillo and Merlo,
2008), resulting in compact grammars that ob-
tain parsing accuracies comparable to lexicalized
grammars. A second line of work has modeled
hidden sequential structure, like in our case, but
using PDFA (Infante-Lopez and de Rijke, 2004).
Finally, a third line of work has induced hidden
structure from the history of actions of a parser
(Titov and Henderson, 2007).
However, the main drawback of the hidden
variable approach to parsing is that, to the best
of our knowledge, there has not been any convex
formulation of the learning problem. As a result,
training a hidden-variable model is both expen-
sive and prone to local minima issues.
In this paper we present a learning algorithm
for hidden-state split head-automata grammars
(SHAG) (Eisner and Satta, 1999). In this for-
409
malism, head-modifier sequences are generated
by a collection of finite-state automata. In our
case, the underlying machines are probabilistic
non-deterministic finite state automata (PNFA),
which we parameterize using the operator model
representation. This representation allows the use
of simple spectral algorithms for estimating the
model parameters from data (Hsu et al 2009;
Bailly, 2011; Balle et al 2012). In all previous
work, the algorithms used to induce hidden struc-
ture require running repeated inference on train-
ing data?e.g. Expectation-Maximization (Demp-
ster et al 1977), or split-merge algorithms. In
contrast, spectral methods are simple and very ef-
ficient ?parameter estimation is reduced to com-
puting some data statistics, performing SVD, and
inverting matrices.
The main contributions of this paper are:
? We present a spectral learning algorithm for
inducing PNFA with applications to head-
automata dependency grammars. Our for-
mulation is based on thinking about the dis-
tribution generated by a PNFA in terms of
the forward-backward recursions.
? Spectral learning algorithms in previous
work only use statistics of prefixes of se-
quences. In contrast, our algorithm is able
to learn from substring statistics.
? We derive an inside-outside algorithm for
non-deterministic SHAG that runs in cubic
time, keeping the costs of CFG parsing.
? In experiments we show that adding non-
determinism improves the accuracy of sev-
eral baselines. When we compare our algo-
rithm to EM we observe a reduction of two
orders of magnitude in training time.
The paper is organized as follows. Next section
describes the necessary background on SHAG
and operator models. Section 3 introduces Op-
erator SHAG for parsing, and presents a spectral
learning algorithm. Section 4 presents a parsing
algorithm. Section 5 presents experiments and
analysis of results, and section 6 concludes.
2 Preliminaries
2.1 Head-Automata Dependency Grammars
In this work we use split head-automata gram-
mars (SHAG) (Eisner and Satta, 1999; Eis-
ner, 2000), a context-free grammatical formal-
ism whose derivations are projective dependency
trees. We will use xi:j = xixi+1 ? ? ?xj to de-
note a sequence of symbols xt with i ? t ? j.
A SHAG generates sentences s0:N , where sym-
bols st ? X with 1 ? t ? N are regular words
and s0 = ? 6? X is a special root symbol. Let
X? = X ? {?}. A derivation y, i.e. a depen-
dency tree, is a collection of head-modifier se-
quences ?h, d, x1:T ?, where h ? X? is a word,
d ? {LEFT, RIGHT} is a direction, and x1:T is
a sequence of T words, where each xt ? X is
a modifier of h in direction d. We say that h is
the head of each xt. Modifier sequences x1:T are
ordered head-outwards, i.e. among x1:T , x1 is the
word closest to h in the derived sentence, and xT
is the furthest. A derivation y of a sentence s0:N
consists of a LEFT and a RIGHT head-modifier se-
quence for each st. As special cases, the LEFT se-
quence of the root symbol is always empty, while
the RIGHT one consists of a single word corre-
sponding to the head of the sentence. We denote
by Y the set of all valid derivations.
Assume a derivation y contains ?h, LEFT, x1:T ?
and ?h, RIGHT, x?1:T ??. Let L(y, h) be the derived
sentence headed by h, which can be expressed as
L(y, xT ) ? ? ? L(y, x1) h L(y, x?1) ? ? ? L(y, x
?
T ?).
1
The language generated by a SHAG are the
strings L(y, ?) for any y ? Y .
In this paper we use probabilistic versions of
SHAG where probabilities of head-modifier se-
quences in a derivation are independent of each
other:
P(y) =
?
?h,d,x1:T ??y
P(x1:T |h, d) . (1)
In the literature, standard arc-factored models fur-
ther assume that
P(x1:T |h, d) =
T+1?
t=1
P(xt|h, d, ?t) , (2)
where xT+1 is always a special STOP word, and ?t
is the state of a deterministic automaton generat-
ing x1:T+1. For example, setting ?1 = FIRST and
?t>1 = REST corresponds to first-order models,
while setting ?1 = NULL and ?t>1 = xt?1 corre-
sponds to sibling models (Eisner, 2000; McDon-
ald et al 2005; McDonald and Pereira, 2006).
1Throughout the paper we assume we can distinguish the
words in a derivation, irrespective of whether two words at
different positions correspond to the same symbol.
410
2.2 Operator Models
An operator model A with n states is a tuple
??1, ?>?, {Aa}a?X ?, where Aa ? R
n?n is an op-
erator matrix and ?1, ?? ? Rn are vectors. A
computes a function f : X ? ? R as follows:
f(x1:T ) = ?
>
? AxT ? ? ? Ax1 ?1 . (3)
One intuitive way of understanding operator
models is to consider the case where f computes
a probability distribution over strings. Such a dis-
tribution can be described in two equivalent ways:
by making some independence assumptions and
providing the corresponding parameters, or by ex-
plaining the process used to compute f . This is
akin to describing the distribution defined by an
HMM in terms of a factorization and its corre-
sponding transition and emission parameters, or
using the inductive equations of the forward al-
gorithm. The operator model representation takes
the latter approach.
Operator models have had numerous applica-
tions. For example, they can be used as an alter-
native parameterization of the function computed
by an HMM (Hsu et al 2009). Consider an HMM
with n hidden states and initial-state probabilities
pi ? Rn, transition probabilities T ? Rn?n, and
observation probabilities Oa ? Rn?n for each
a ? X , with the following meaning:
? pi(i) is the probability of starting at state i,
? T (i, j) is the probability of transitioning
from state j to state i,
? Oa is a diagonal matrix, such that Oa(i, i) is
the probability of generating symbol a from
state i.
Given an HMM, an equivalent operator model
can be defined by setting ?1 = pi, Aa = TOa
and ?? = ~1. To see this, let us show that the for-
ward algorithm computes the expression in equa-
tion (3). Let ?t denote the state of the HMM
at time t. Consider a state-distribution vector
?t ? Rn, where ?t(i) = P(x1:t?1, ?t = i). Ini-
tially ?1 = pi. At each step in the chain of prod-
ucts (3), ?t+1 = Axt ?t updates the state dis-
tribution from positions t to t + 1 by applying
the appropriate operator, i.e. by emitting symbol
xt and transitioning to the new state distribution.
The probability of x1:T is given by
?
i ?T+1(i).
Hence, Aa(i, j) is the probability of generating
symbol a and moving to state i given that we are
at state j.
HMM are only one example of distributions
that can be parameterized by operator models.
In general, operator models can parameterize any
PNFA, where the parameters of the model corre-
spond to probabilities of emitting a symbol from
a state and moving to the next state.
The advantage of working with operator mod-
els is that, under certain mild assumptions on the
operator parameters, there exist algorithms that
can estimate the operators from observable statis-
tics of the input sequences. These algorithms are
extremely efficient and are not susceptible to local
minima issues. See (Hsu et al 2009) for theoret-
ical proofs of the learnability of HMM under the
operator model representation.
In the following, we write x = xi:j ? X ? to
denote sequences of symbols, and use Axi:j as a
shorthand for Axj ? ? ?Axi . Also, for convenience
we assume X = {1, . . . , l}, so that we can index
vectors and matrices by symbols in X .
3 Learning Operator SHAG
We will define a SHAG using a collection of op-
erator models to compute probabilities. Assume
that for each possible head h in the vocabulary X?
and each direction d ? {LEFT, RIGHT} we have
an operator model that computes probabilities of
modifier sequences as follows:
P(x1:T |h, d) = (?h,d? )
> Ah,dxT ? ? ? A
h,d
x1 ?
h,d
1 .
Then, this collection of operator models defines
an operator SHAG that assigns a probability to
each y ? Y according to (1). To learn the model
parameters, namely ??h,d1 , ?
h,d
? , {A
h,d
a }a?X ? for
h ? X? and d ? {LEFT, RIGHT}, we use spec-
tral learning methods based on the works of Hsu
et al(2009), Bailly (2011) and Balle et al(2012).
The main challenge of learning an operator
model is to infer a hidden-state space from ob-
servable quantities, i.e. quantities that can be com-
puted from the distribution of sequences that we
observe. As it turns out, we cannot recover the
actual hidden-state space used by the operators
we wish to learn. The key insight of the spectral
learning method is that we can recover a hidden-
state space that corresponds to a projection of the
original hidden space. Such projected space is
equivalent to the original one in the sense that we
411
can find operators in the projected space that pa-
rameterize the same probability distribution over
sequences.
In the rest of this section we describe an algo-
rithm for learning an operator model. We will as-
sume a fixed head word and direction, and drop h
and d from all terms. Hence, our goal is to learn
the following distribution, parameterized by oper-
ators ?1, {Aa}a?X , and ??:
P(x1:T ) = ?>? AxT ? ? ? Ax1 ?1 . (4)
Our algorithm shares many features with the
previous spectral algorithms of Hsu et al(2009)
and Bailly (2011), though the derivation given
here is based upon the general formulation of
Balle et al(2012). The main difference is that
our algorithm is able to learn operator models
from substring statistics, while algorithms in pre-
vious works were restricted to statistics on pre-
fixes. In principle, our algorithm should extract
much more information from a sample.
3.1 Preliminary Definitions
The spectral learning algorithm will use statistics
estimated from samples of the target distribution.
More specifically, consider the function that com-
putes the expected number of occurrences of a
substring x in a random string x? drawn from P:
f(x) = E(x v] x?)
=
?
x??X ?
(x v] x
?)P(x?)
=
?
p,s?X ?
P(pxs) , (5)
where x v] x? denotes the number of times x ap-
pears in x?. Here we assume that the true values
of f(x) for bigrams are known, though in practice
the algorithm will work with empirical estimates
of these.
The information about f known by the algo-
rithm is organized in matrix form as follows. Let
P ? Rl?l be a matrix containing the value of f(x)
for all strings of length two, i.e. bigrams.2. That
is, each entry in P ? Rl?l contains the expected
number of occurrences of a given bigram:
P (b, a) = E(ab v] x) . (6)
2In fact, while we restrict ourselves to strings of length
two, an analogous algorithm can be derived that considers
longer strings to define P . See (Balle et al 2012) for details.
Furthermore, for each b ? X let Pb ? Rl?l denote
the matrix whose entries are given by
Pb(c, a) = E(abc v] x) , (7)
the expected number of occurrences of trigrams.
Finally, we define vectors p1 ? Rl and p? ? Rl
as follows: p1(a) =
?
s?X ? P(as), the probabil-
ity that a string begins with a particular symbol;
and p?(a) =
?
p?X ? P(pa), the probability that
a string ends with a particular symbol.
Now we show a particularly useful way to ex-
press the quantities defined above in terms of the
operators ??1, ?>?, {Aa}a?X ? of P. First, note
that each entry of P can be written in this form:
P (b, a) =
?
p,s?X ?
P(pabs) (8)
=
?
p,s?X ?
?>? As Ab Aa Ap ?1
= (?>?
?
s?X ?
As) Ab Aa(
?
p?X ?
Ap ?1) .
It is not hard to see that, since P is a probability
distribution over X ?, actually ?>?
?
s?X ? As =
~1>. Furthermore, since
?
p?X ? Ap =?
k?0(
?
a?X Aa)
k = (I ?
?
a?X Aa)
?1,
we write ??1 = (I ?
?
a?X Aa)
?1?1. From (8) it
is natural to define a forward matrix F ? Rn?l
whose ath column contains the sum of all hidden-
state vectors obtained after generating all prefixes
ended in a:
F (:, a) = Aa
?
p?X ?
Ap ?1 = Aa ??1 . (9)
Conversely, we also define a backward matrix
B ? Rl?n whose ath row contains the probability
of generating a from any possible state:
B(a, :) = ?>?
?
s?X ?
AsAa = ~1
>Aa . (10)
By plugging the forward and backward matri-
ces into (8) one obtains the factorization P =
BF . With similar arguments it is easy to see
that one also has Pb = BAbF , p1 = B ?1, and
p>? = ?
>
? F . Hence, ifB and F were known, one
could in principle invert these expressions in order
to recover the operators of the model from em-
pirical estimations computed from a sample. In
the next section we show that in fact one does not
need to know B and F to learn an operator model
for P, but rather that having a ?good? factorization
of P is enough.
412
3.2 Inducing a Hidden-State Space
We have shown that an operator model A com-
puting P induces a factorization of the matrix P ,
namely P = BF . More generally, it turns out that
when the rank of P equals the minimal number of
states of an operator model that computes P, then
one can prove a duality relation between opera-
tors and factorizations of P . In particular, one can
show that, for any rank factorization P = QR, the
operators given by ??1 = Q+p1, ??>? = p
>
?R
+,
and A?a = Q+PaR+, yield an operator model for
P. A key fact in proving this result is that the func-
tion P is invariant to the basis chosen to represent
operator matrices. See (Balle et al 2012) for fur-
ther details.
Thus, we can recover an operator model for P
from any rank factorization of P , provided a rank
assumption on P holds (which hereafter we as-
sume to be the case). Since we only have access
to an approximation of P , it seems reasonable to
choose a factorization which is robust to estima-
tion errors. A natural such choice is the thin SVD
decomposition of P (i.e. using top n singular vec-
tors), given by: P = U(?V >) = U(U>P ).
Intuitively, we can think of U and U>P as pro-
jected backward and forward matrices. Now that
we have a factorization of P we can construct an
operator model for P as follows: 3
??1 = U
>p1 , (11)
??>? = p
>
?(U
>P )+ , (12)
A?a = U
>Pa(U
>P )+ . (13)
Algorithm 1 presents pseudo-code for an algo-
rithm learning operators of a SHAG from train-
ing head-modifier sequences using this spectral
method. Note that each operator model in the
3To see that equations (11-13) define a model for P, one
must first see that the matrix M = F (?V >)+ is invertible
with inverse M?1 = U>B. Using this and recalling that
p1 = B?1, Pa = BAaF , p>? = ?
>
?F , one obtains that:
??1 = U
>B?1 = M
?1?1 ,
??>? = ?
>
?F (U
>BF )+ = ?>?M ,
A?a = U
>BAaF (U
>BF )+ = M?1AaM .
Finally:
P(x1:T ) = ?
>
? AxT ? ? ?Ax1 ?1
= ?>?MM
?1AxTM ? ? ?M
?1Ax1MM
?1?1
= ??>?A?xT ? ? ? A?x1 ??1
Algorithm 1 Learn Operator SHAG
inputs:
? An alphabet X
? A training set TRAIN = {?hi, di, xi1:T ?}
M
i=1
? The number of hidden states n
1: for each h ? X? and d ? {LEFT, RIGHT} do
2: Compute an empirical estimate from TRAIN of
statistics matrices p?1, p??, P? , and {P?a}a?X
3: Compute the SVD of P? and let U? be the matrix
of top n left singular vectors of P?
4: Compute the observable operators for h and d:
5: ??h,d1 = U?
>p?1
6: (??h,d? )
> = p?>?(U?
>P? )+
7: A?h,da = U?
>P?a(U?>P? )+ for each a ? X
8: end for
9: return Operators ???h,d1 , ??
h,d
? , A?
h,d
a ?
for each h ? X? , d ? {LEFT, RIGHT}, a ? X
SHAG is learned separately. The running time
of the algorithm is dominated by two computa-
tions. First, a pass over the training sequences to
compute statistics over unigrams, bigrams and tri-
grams. Second, SVD and matrix operations for
computing the operators, which run in time cubic
in the number of symbols l. However, note that
when dealing with sparse matrices many of these
operations can be performed more efficiently.
4 Parsing Algorithms
Given a sentence s0:N we would like
to find its most likely derivation, y? =
argmaxy?Y(s0:N ) P(y). This problem, known as
MAP inference, is known to be intractable for
hidden-state structure prediction models, as it
involves finding the most likely tree structure
while summing out over hidden states. We use
a common approximation to MAP based on first
computing posterior marginals of tree edges (i.e.
dependencies) and then maximizing over the
tree structure (see (Park and Darwiche, 2004)
for complexity of general MAP inference and
approximations). For parsing, this strategy is
sometimes known as MBR decoding; previous
work has shown that empirically it gives good
performance (Goodman, 1996; Clark and Cur-
ran, 2004; Titov and Henderson, 2006; Petrov
and Klein, 2007). In our case, we use the
non-deterministic SHAG to compute posterior
marginals of dependencies. We first explain the
general strategy of MBR decoding, and then
present an algorithm to compute marginals.
413
Let (si, sj) denote a dependency between head
word i and modifier word j. The posterior
or marginal probability of a dependency (si, sj)
given a sentence s0:N is defined as
?i,j = P((si, sj) | s0:N ) =
?
y?Y(s0:N ) : (si,sj)?y
P(y) .
To compute marginals, the sum over derivations
can be decomposed into a product of inside and
outside quantities (Baker, 1979). Below we de-
scribe an inside-outside algorithm for our gram-
mars. Given a sentence s0:N and marginal scores
?i,j , we compute the parse tree for s0:N as
y? = argmax
y?Y(s0:N )
?
(si,sj)?y
log?i,j (14)
using the standard projective parsing algorithm
for arc-factored models (Eisner, 2000). Overall
we use a two-pass parsing process, first to com-
pute marginals and then to compute the best tree.
4.1 An Inside-Outside Algorithm
In this section we sketch an algorithm to com-
pute marginal probabilities of dependencies. Our
algorithm is an adaptation of the parsing algo-
rithm for SHAG by Eisner and Satta (1999) to
the case of non-deterministic head-automata, and
has a runtime cost of O(n2N3), where n is the
number of states of the model, and N is the
length of the input sentence. Hence the algorithm
maintains the standard cubic cost on the sentence
length, while the quadratic cost on n is inher-
ent to the computations defined by our model in
Eq. (3). The main insight behind our extension
is that, because the computations of our model in-
volve state-distribution vectors, we need to extend
the standard inside/outside quantities to be in the
form of such state-distribution quantities.4
Throughout this section we assume a fixed sen-
tence s0:N . Let Y(xi:j) be the set of derivations
that yield a subsequence xi:j . For a derivation y,
we use root(y) to indicate the root word of it,
and use (xi, xj) ? y to refer a dependency in y
from head xi to modifier xj . Following Eisner
4Technically, when working with the projected operators
the state-distribution vectors will not be distributions in the
formal sense. However, they correspond to a projection of a
state distribution, for some projection that we do not recover
from data (namely M?1 in footnote 3). This projection has
no effect on the computations because it cancels out.
and Satta (1999), we use decoding structures re-
lated to complete half-constituents (or ?triangles?,
denoted C) and incomplete half-constituents (or
?trapezoids?, denoted I), each decorated with a di-
rection (denoted L and R). We assume familiarity
with their algorithm.
We define ?I,Ri,j ? R
n as the inside score-vector
of a right trapezoid dominated by dependency
(si, sj),
?I,Ri,j =
?
y?Y(si:j) : (si,sj)?y ,
y={?si,R,x1:t?} ? y? , xt=sj
P(y?)?si,R(x1:t) . (15)
The term P(y?) is the probability of head-modifier
sequences in the range si:j that do not involve
si. The term ?si,R(x1:t) is a forward state-
distribution vector ?the qth coordinate of the
vector is the probability that si generates right
modifiers x1:t and remains at state q. Similarly,
we define ?I,Ri,j ? R
n as the outside score-vector
of a right trapezoid, as
?I,Ri,j =
?
y?Y(s0:isj:n) : root(y)=s0,
y={?si,R,xt:T ?} ? y? , xt=sj
P(y?)?si,R(xt+1:T ) , (16)
where ?si,R(xt+1:T ) ? Rn is a backward state-
distribution vector ?the qth coordinate is the
probability of being at state q of the right au-
tomaton of si and generating xt+1:T . Analogous
inside-outside expressions can be defined for the
rest of structures (left/right triangles and trape-
zoids). With these quantities, we can compute
marginals as
?i,j =
{
(?I,Ri,j )
> ?I,Ri,j Z
?1 if i < j ,
(?I,Li,j)
> ?I,Li,j Z
?1 if j < i ,
(17)
where Z=
?
y?Y(s0:N)
P(y) = (??,R? )> ?
C,R
0,N .
Finally, we sketch the equations for computing
inside scores in O(N3) time. The outside equa-
tions can be derived analogously (see (Paskin,
2001)). For 0 ? i < j ? N :
?C,Ri,i = ?
si,R
1 (18)
?C,Ri,j =
j?
k=i+1
?I,Ri,k
(
(?sk,R? )
> ?C,Rk,j
)
(19)
?I,Ri,j =
j?
k=i
Asi,Rsj ?
C,R
i,k
(
(?
sj ,L
? )> ?
C,L
k+1,j
)
(20)
414
5 Experiments
The goal of our experiments is to show that in-
corporating hidden states in a SHAG using oper-
ator models can consistently improve parsing ac-
curacy. A second goal is to compare the spec-
tral learning algorithm to EM, a standard learning
method that also induces hidden states.
The first set of experiments involve fully unlex-
icalized models, i.e. parsing part-of-speech tag se-
quences. While this setting falls behind the state-
of-the-art, it is nonetheless valid to analyze empir-
ically the effect of incorporating hidden states via
operator models, which results in large improve-
ments. In a second set of experiments, we com-
bine the unlexicalized hidden-state models with
simple lexicalized models. Finally, we present
some analysis of the automaton learned by the
spectral algorithm to see the information that is
captured in the hidden state space.
5.1 Fully Unlexicalized Grammars
We trained fully unlexicalized dependency gram-
mars from dependency treebanks, that is, X are
PoS tags and we parse PoS tag sequences. In
all cases, our modifier sequences include special
START and STOP symbols at the boundaries. 5 6
We compare the following SHAG models:
? DET: a baseline deterministic grammar with
a single state.
? DET+F: a deterministic grammar with two
states, one emitting the first modifier of a
sequence, and another emitting the rest (see
(Eisner and Smith, 2010) for a similar deter-
ministic baseline).
? SPECTRAL: a non-deterministic grammar
with n hidden states trained with the spectral
algorithm. n is a parameter of the model.
? EM: a non-deterministic grammar with n
states trained with EM. Here, we estimate
operators ???1, ???, A?
h,d
a ? using forward-
backward for the E step. To initialize, we
mimicked an HMM initialization: (1) we set
??1 and ??? randomly; (2) we created a ran-
dom transition matrix T ? Rn?n; (3) we
5Even though the operators ?1 and ?? of a PNFA ac-
count for start and stop probabilities, in preliminary experi-
ments we found that having explicit START and STOP sym-
bols results in more accurate models.
6Note that, for parsing, the operators for the START and
STOP symbols can be packed into ?1 and ?? respectively.
One just defines ??1 = ASTART ?1 and ?
?>
? = ?
>
? ASTOP.
 68
 70
 72
 74
 76
 78
 80
 82
 2  4  6  8  10  12  14
un
lab
ele
d a
ttac
hm
ent
 sc
ore
number of states
DetDet+FSpectralEM (5)EM (10)EM (25)EM (100)
Figure 1: Accuracy curve on English development set
for fully unlexicalized models.
created a diagonal matrix Oh,da ? Rn?n,
where Oh,da (i, i) is the probability of gener-
ating symbol a from h and d (estimated from
training); (4) we set A?h,da = TO
h,d
a .
We trained SHAG models using the standard
WSJ sections of the English Penn Treebank (Mar-
cus et al 1994). Figure 1 shows the Unlabeled
Attachment Score (UAS) curve on the develop-
ment set, in terms of the number of hidden states
for the spectral and EM models. We can see
that DET+F largely outperforms DET7, while the
hidden-state models obtain much larger improve-
ments. For the EM model, we show the accuracy
curve after 5, 10, 25 and 100 iterations.8
In terms of peak accuracies, EM gives a slightly
better result than the spectral method (80.51% for
EM with 15 states versus 79.75% for the spectral
method with 9 states). However, the spectral al-
gorithm is much faster to train. With our Matlab
implementation, it took about 30 seconds, while
each iteration of EM took from 2 to 3 minutes,
depending on the number of states. To give a con-
crete example, to reach an accuracy close to 80%,
there is a factor of 150 between the training times
of the spectral method and EM (where we com-
pare the peak performance of the spectral method
versus EM at 25 iterations with 13 states).
7For parsing with deterministic SHAG we employ MBR
inference, even though Viterbi inference can be performed
exactly. In experiments on development data DET improved
from 62.65% using Viterbi to 68.52% using MBR, and
DET+F improved from 72.72% to 74.80%.
8We ran EM 10 times under different initial conditions
and selected the run that gave the best absolute accuracy after
100 iterations. We did not observe significant differences
between the runs.
415
DET DET+F SPECTRAL EM
WSJ 69.45% 75.91% 80.44% 81.68%
Table 1: Unlabeled Attachment Score of fully unlexi-
calized models on the WSJ test set.
Table 1 shows results on WSJ test data, se-
lecting the models that obtain peak performances
in development. We observe the same behavior:
hidden-states largely improve over deterministic
baselines, and EM obtains a slight improvement
over the spectral algorithm. Comparing to previ-
ous work on parsing WSJ PoS sequences, Eisner
and Smith (2010) obtained an accuracy of 75.6%
using a deterministic SHAG that uses informa-
tion about dependency lengths. However, they
used Viterbi inference, which we found to per-
form worse than MBR inference (see footnote 7).
5.2 Experiments with Lexicalized
Grammars
We now turn to combining lexicalized determinis-
tic grammars with the unlexicalized grammars ob-
tained in the previous experiment using the spec-
tral algorithm. The goal behind this experiment
is to show that the information captured in hidden
states is complimentary to head-modifier lexical
preferences.
In this case X consists of lexical items, and we
assume access to the PoS tag of each lexical item.
We will denote as ta and wa the PoS tag and word
of a symbol a ? X? . We will estimate condi-
tional distributions P(a | h, d, ?), where a ? X
is a modifier, h ? X? is a head, d is a direction,
and ? is a deterministic state. Following Collins
(1999), we use three configurations of determin-
istic states:
? LEX: a single state.
? LEX+F: two distinct states for first modifier
and rest of modifiers.
? LEX+FCP: four distinct states, encoding:
first modifier, previous modifier was a coor-
dination, previous modifier was punctuation,
and previous modifier was some other word.
To estimate P we use a back-off strategy:
P(a|h, d, ?) = PA(ta|h, d, ?)PB(wa|ta, h, d, ?)
To estimate PA we use two back-off levels,
the fine level conditions on {wh, d, ?} and the
 72
 74
 76
 78
 80
 82
 84
 86
 2  3  4  5  6  7  8  9  10
un
lab
ele
d a
ttac
hm
ent
 sc
ore
number of states
LexLex+FLex+FCPLex + SpectralLex+F + SpectralLex+FCP + Spectral
Figure 2: Accuracy curve on English development set
for lexicalized models.
coarse level conditions on {th, d, ?}. For PB we
use three levels, which from fine to coarse are
{ta, wh, d, ?}, {ta, th, d, ?} and {ta}. We follow
Collins (1999) to estimate PA and PB from a tree-
bank using a back-off strategy.
We use a simple approach to combine lexical
models with the unlexical hidden-state models we
obtained in the previous experiment. Namely, we
use a log-linear model that computes scores for
head-modifier sequences as
s(?h, d, x1:T ?) = logPsp(x1:T |h, d) (21)
+ logPdet(x1:T |h, d) ,
where Psp and Pdet are respectively spectral and
deterministic probabilistic models. We tested
combinations of each deterministic model with
the spectral unlexicalized model using different
number of states. Figure 2 shows the accuracies of
single deterministic models, together with combi-
nations using different number of states. In all
cases, the combinations largely improve over the
purely deterministic lexical counterparts, suggest-
ing that the information encoded in hidden states
is complementary to lexical preferences.
5.3 Results Analysis
We conclude the experiments by analyzing the
state space learned by the spectral algorithm.
Consider the space Rn where the forward-state
vectors lie. Generating a modifier sequence corre-
sponds to a path through the n-dimensional state
space. We clustered sets of forward-state vectors
in order to create a DFA that we can use to visu-
alize the phenomena captured by the state space.
416
 cc
jj dt nnp
prp$ vbg jjs
rb vbn posjj in dt cd
1 5
7
I
2
0
3
 cc
nns
   cd
 ,
$ nnp
 cd nns
STOP
,
 prp$ rb posjj dt nnp
9
$ nnjjr nnp
 STOP
STOP
  cc
 nn
STOP
  cc
 nn
,
prp$ nn pos
Figure 3: DFA approximation for the generation of NN
left modifier sequences.
To build a DFA, we computed the forward vec-
tors corresponding to frequent prefixes of modi-
fier sequences of the development set. Then, we
clustered these vectors using a Group Average
Agglomerative algorithm using the cosine simi-
larity measure (Manning et al 2008). This simi-
larity measure is appropriate because it compares
the angle between vectors, and is not affected by
their magnitude (the magnitude of forward vec-
tors decreases with the number of modifiers gen-
erated). Each cluster i defines a state in the DFA,
and we say that a sequence x1:t is in state i if its
corresponding forward vector at time t is in clus-
ter i. Then, transitions in the DFA are defined us-
ing a procedure that looks at how sequences tra-
verse the states. If a sequence x1:t is at state i at
time t ? 1, and goes to state j at time t, then we
define a transition from state i to state j with la-
bel xt. This procedure may require merging states
to give a consistent DFA, because different se-
quences may define different transitions for the
same states and modifiers. After doing a merge,
new merges may be required, so the procedure
must be repeated until a DFA is obtained.
For this analysis, we took the spectral model
with 9 states, and built DFA from the non-
deterministic automata corresponding to heads
and directions where we saw largest improve-
ments in accuracy with respect to the baselines.
A DFA for the automaton (NN, LEFT) is shown
in Figure 3. The vectors were originally divided
in ten clusters, but the DFA construction required
two state mergings, leading to a eight state au-
tomaton. The state named I is the initial state.
Clearly, we can see that there are special states
for punctuation (state 9) and coordination (states
1 and 5). States 0 and 2 are harder to interpret.
To understand them better, we computed an esti-
mation of the probabilities of the transitions, by
counting the number of times each of them is
used. We found that our estimation of generating
STOP from state 0 is 0.67, and from state 2 it is
0.15. Interestingly, state 2 can transition to state 0
generating prp$, POS or DT, that are usual end-
ings of modifier sequences for nouns (recall that
modifiers are generated head-outwards, so for a
left automaton the final modifier is the left-most
modifier in the sentence).
6 Conclusion
Our main contribution is a basic tool for inducing
sequential hidden structure in dependency gram-
mars. Most of the recent work in dependency
parsing has explored explicit feature engineering.
In part, this may be attributed to the high cost of
using tools such as EM to induce representations.
Our experiments have shown that adding hidden-
structure improves parsing accuracy, and that our
spectral algorithm is highly scalable.
Our methods may be used to enrich the rep-
resentational power of more sophisticated depen-
dency models. For example, future work should
consider enhancing lexicalized dependency gram-
mars with hidden states that summarize lexical
dependencies. Another line for future research
should extend the learning algorithm to be able
to capture vertical hidden relations in the depen-
dency tree, in addition to sequential relations.
Acknowledgements We are grateful to Gabriele
Musillo and the anonymous reviewers for providing us
with helpful comments. This work was supported by
a Google Research Award and by the European Com-
mission (PASCAL2 NoE FP7-216886, XLike STREP
FP7-288342). Borja Balle was supported by an FPU
fellowship (AP2008-02064) of the Spanish Ministry
of Education. The Spanish Ministry of Science and
Innovation supported Ariadna Quattoni (JCI-2009-
04240) and Xavier Carreras (RYC-2008-02223 and
?KNOW2? TIN2009-14715-C04-04).
417
References
Raphael Bailly. 2011. Quadratic weighted automata:
Spectral algorithm and likelihood maximization.
JMLR Workshop and Conference Proceedings ?
ACML.
James K. Baker. 1979. Trainable grammars for speech
recognition. In D. H. Klatt and J. J. Wolf, editors,
Speech Communication Papers for the 97th Meeting
of the Acoustical Society of America, pages 547?
550.
Borja Balle, Ariadna Quattoni, and Xavier Carreras.
2012. Local loss optimization in operator models:
A new insight into spectral learning. Technical Re-
port LSI-12-5-R, Departament de Llenguatges i Sis-
temes Informa`tics (LSI), Universitat Polite`cnica de
Catalunya (UPC).
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceed-
ings of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007, pages 957?961, Prague, Czech Re-
public, June. Association for Computational Lin-
guistics.
Stephen Clark and James R. Curran. 2004. Parsing
the wsj using ccg and log-linear models. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main Volume,
pages 103?110, Barcelona, Spain, July.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the em algorithm. Journal of the royal sta-
tistical society, Series B, 39(1):1?38.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head-
automaton grammars. In Proceedings of the 37th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 457?464, Univer-
sity of Maryland, June.
Jason Eisner and Noah A. Smith. 2010. Favor
short dependencies: Parsing with soft and hard con-
straints on dependency length. In Harry Bunt, Paola
Merlo, and Joakim Nivre, editors, Trends in Parsing
Technology: Dependency Parsing, Domain Adapta-
tion, and Deep Parsing, chapter 8, pages 121?150.
Springer.
Jason Eisner. 2000. Bilexical grammars and their
cubic-time parsing algorithms. In Harry Bunt and
Anton Nijholt, editors, Advances in Probabilis-
tic and Other Parsing Technologies, pages 29?62.
Kluwer Academic Publishers, October.
Joshua Goodman. 1996. Parsing algorithms and met-
rics. In Proceedings of the 34th Annual Meeting
of the Association for Computational Linguistics,
pages 177?183, Santa Cruz, California, USA, June.
Association for Computational Linguistics.
Daniel Hsu, Sham M. Kakade, and Tong Zhang. 2009.
A spectral algorithm for learning hidden markov
models. In COLT 2009 - The 22nd Conference on
Learning Theory.
Gabriel Infante-Lopez and Maarten de Rijke. 2004.
Alternative approaches for generating bodies of
grammar rules. In Proceedings of the 42nd Meet-
ing of the Association for Computational Lin-
guistics (ACL?04), Main Volume, pages 454?461,
Barcelona, Spain, July.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1?11, Uppsala, Sweden,
July. Association for Computational Linguistics.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schu?tze. 2008. Introduction to Information
Retrieval. Cambridge University Press, Cambridge,
first edition, July.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1994. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19.
Andre Martins, Noah Smith, and Eric Xing. 2009.
Concise integer linear programming formulations
for dependency parsing. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 342?
350, Suntec, Singapore, August. Association for
Computational Linguistics.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL?05),
pages 75?82, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of the 11th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 81?88.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 523?530, Vancouver,
British Columbia, Canada, October. Association for
Computational Linguistics.
Gabriele Antonio Musillo and Paola Merlo. 2008. Un-
lexicalised hidden variable models of split depen-
dency grammars. In Proceedings of ACL-08: HLT,
Short Papers, pages 213?216, Columbus, Ohio,
June. Association for Computational Linguistics.
James D. Park and Adnan Darwiche. 2004. Com-
plexity results and approximation strategies for map
418
explanations. Journal of Artificial Intelligence Re-
search, 21:101?133.
Mark Paskin. 2001. Cubic-time parsing and learning
algorithms for grammatical bigram models. Techni-
cal Report UCB/CSD-01-1148, University of Cali-
fornia, Berkeley.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Association for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 433?
440, Sydney, Australia, July. Association for Com-
putational Linguistics.
Ivan Titov and James Henderson. 2006. Loss mini-
mization in parse reranking. In Proceedings of the
2006 Conference on Empirical Methods in Natu-
ral Language Processing, pages 560?567, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Ivan Titov and James Henderson. 2007. A latent vari-
able model for generative dependency parsing. In
Proceedings of the Tenth International Conference
on Parsing Technologies, pages 144?155, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
419
