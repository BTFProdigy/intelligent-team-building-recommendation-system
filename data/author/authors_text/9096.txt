Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 98?107,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Indirect-HMM-based Hypothesis Alignment for Combining Outputs 
from Machine Translation Systems 
 
Xiaodong He?, Mei Yang? *, Jianfeng Gao?, Patrick Nguyen?, and Robert Moore? 
 
?Microsoft Research ?Dept. of Electrical Engineering 
One Microsoft Way University of Washington 
Redmond, WA 98052 USA Seattle, WA 98195, USA 
{xiaohe,jfgao, panguyen, 
bobmoore}@microsoft.com 
yangmei@u.washington.edu 
 
 
Abstract 
This paper presents a new hypothesis alignment method 
for combining outputs of multiple machine translation 
(MT) systems. An indirect hidden Markov model 
(IHMM) is proposed to address the synonym matching 
and word ordering issues in hypothesis alignment.  
Unlike traditional HMMs whose parameters are trained 
via maximum likelihood estimation (MLE), the 
parameters of the IHMM are estimated indirectly from a 
variety of sources including word semantic similarity, 
word surface similarity, and a distance-based distortion 
penalty. The IHMM-based method significantly 
outperforms the state-of-the-art TER-based alignment 
model in our experiments on NIST benchmark 
datasets.  Our combined SMT system using the 
proposed method achieved the best Chinese-to-English 
translation result in the constrained training track of the 
2008 NIST Open MT Evaluation. 
1 Introduction* 
System combination has been applied successfully 
to various machine translation tasks. Recently, 
confusion-network-based system combination 
algorithms have been developed to combine 
outputs of multiple machine translation (MT) 
systems to form a consensus output (Bangalore, et 
al. 2001, Matusov et al, 2006, Rosti et al, 2007, 
Sim et al, 2007). A confusion network comprises a 
sequence of sets of alternative words, possibly 
including null?s, with associated scores. The 
consensus output is then derived by selecting one 
word from each set of alternatives, to produce the 
sequence with the best overall score, which could 
be assigned in various ways such as by voting, by 
                                                          
* Mei Yang performed this work when she was an intern with 
Microsoft Research. 
using posterior probability estimates, or by using a 
combination of these measures and other features. 
Constructing a confusion network requires 
choosing one of the hypotheses as the backbone 
(also called ?skeleton? in the literature), and other 
hypotheses are aligned to it at the word level. High 
quality hypothesis alignment is crucial to the 
performance of the resulting system combination. 
However, there are two challenging issues that 
make MT hypothesis alignment difficult. First, 
different hypotheses may use different 
synonymous words to express the same meaning, 
and these synonyms need to be aligned to each 
other. Second, correct translations may have 
different word orderings in different hypotheses 
and these words need to be properly reordered in 
hypothesis alignment.  
In this paper, we propose an indirect hidden 
Markov model (IHMM) for MT hypothesis 
alignment. The HMM provides a way to model 
both synonym matching and word ordering. Unlike 
traditional HMMs whose parameters are trained 
via maximum likelihood estimation (MLE), the 
parameters of the IHMM are estimated indirectly 
from a variety of sources including word semantic 
similarity, word surface similarity, and a distance-
based distortion penalty, without using large 
amount of training data. Our combined SMT 
system using the proposed method gave the best 
result on the Chinese-to-English test in the 
constrained training track of the 2008 NIST Open 
MT Evaluation (MT08). 
2 Confusion-network-based MT system 
combination 
The current state-of-the-art is confusion-network-
based MT system combination as described by 
98
 Rosti and colleagues (Rosti et al, 2007a, Rosti et 
al., 2007b). The major steps are illustrated in 
Figure 1. In Fig. 1 (a), hypotheses from different 
MT systems are first collected. Then in Fig. 1 (b), 
one of the hypotheses is selected as the backbone 
for hypothesis alignment. This is usually done by a 
sentence-level minimum Bayes risk (MBR) 
method which selects a hypothesis that has the 
minimum average distance compared to all 
hypotheses. The backbone determines the word 
order of the combined output. Then as illustrated in 
Fig. 1 (c), all other hypotheses are aligned to the 
backbone. Note that in Fig. 1 (c) the symbol ? 
denotes a null word, which is inserted by the 
alignment normalization algorithm described in 
section 3.4. Fig. 1 (c) also illustrates the handling 
of synonym alignment (e.g., aligning ?car? to 
?sedan?), and word re-ordering of the hypothesis. 
Then in Fig. 1 (d), a confusion network is 
constructed based on the aligned hypotheses, 
which consists of a sequence of sets in which each 
word is aligned to a list of alternative words 
(including null) in the same set. Then, a set of 
global and local features are used to decode the 
confusion network.  
  
E1 he have good car 
argmin ( , )B E EE TER E E?? ? ?? ?E E
 
E2 he has nice sedan 
E3 it a nice car        e.g., EB = E1 E4 a sedan he has 
(a)  hypothesis set                    (b) backbone selection 
 
EB he have ? good car      he  have   ?   good   car 
       he   has    ?   nice    sedan 
       it     ?       a   nice    car   
E4 a  ?  sedan  he   has      he   has    a     ?       sedan 
(c)  hypothesis alignment        (d) confusion network 
 
Figure 1: Confusion-network-based MT system 
combination.  
3 Indirect-HMM-based Hypothesis 
Alignment  
In confusion-network-based system combination 
for SMT, a major difficulty is aligning hypotheses 
to the backbone. One possible statistical model for 
word alignment is the HMM, which has been 
widely used for bilingual word alignment (Vogel et 
al., 1996, Och and Ney, 2003). In this paper, we 
propose an indirect-HMM method for monolingual 
hypothesis alignment. 
 
3.1 IHMM for hypothesis alignment  
 
Let 
1 1( ,..., )I Ie e e? denote the backbone, 
1 1( ,..., )J Je e e? ? ??  a hypothesis to be aligned to 1Ie , 
and 
1 1( ,..., )J Ja a a?  the alignment that specifies 
the position of the backbone word aligned to each 
hypothesis word. We treat each word in the 
backbone as an HMM state and the words in the 
hypothesis as the observation sequence. We use a 
first-order HMM, assuming that the emission 
probability 
( | )jj ap e e?
 depends only on the 
backbone word, and the transition probability 
1( | , )j jp a a I?
 depends only on the position of the 
last state and the length of the backbone. Treating 
the alignment as hidden variable, the conditional 
probability that the hypothesis is generated by the 
backbone is given by  
 
 
1
1 1 1
1
( | ) ( | , ) ( | )jJ
JJ I
j j j a
ja
p e e p a a I p e e?
?
? ?? ?? ? ???
 (1) 
  
As in HMM-based bilingual word alignment 
(Och and Ney, 2003), we also associate a null with 
each backbone word to allow generating 
hypothesis words that do not align to any backbone 
word.  
In HMM-based hypothesis alignment, emission 
probabilities model the similarity between a 
backbone word and a hypothesis word, and will be 
referred to as the similarity model. The transition 
probabilities model word reordering, and will be 
called the distortion model. 
 
3.2 Estimation of the similarity model 
 
The similarity model, which specifies the emission 
probabilities of the HMM, models the similarity 
between a backbone word and a hypothesis word. 
Since both words are in the same language, the 
similarity model can be derived based on both 
semantic similarity and surface similarity, and the 
overall similarity model is a linear interpolation of 
the two: 
 
( | ) ( | ) (1 ) ( | )j i sem j i sur j ip e e p e e p e e? ?? ? ?? ? ? ? ?  (2) 
 
99
 where ( | )sem j ip e e?
 and ( | )sur j ip e e?
 reflect the 
semantic and surface similarity between 
je?
 and  
ie , respectively, and ? is the interpolation factor. 
Since the semantic similarity between two 
target words is source-dependent, the semantic 
similarity model is derived by using the source 
word sequence as a hidden layer: 
 
0
( | )
( | ) ( | , )
sem j i
K
k i j k i
k
p e e
p f e p e f e
?
?
???
 
0
( | ) ( | )K k i j k
k
p f e p e f
?
???     (3) 
 
where 
1 1( ,..., )K Kf f f?  is the source sentence. 
Moreover, in order to handle the case that two 
target words are synonyms but neither of them has 
counter-part in the source sentence, a null is 
introduced on the source side, which is represented 
by f0. The last step in (3) assumes that first ei 
generates all source words including null. Then ej? 
is generated by all source words including null.  
In the common SMT scenario where a large 
amount of bilingual parallel data is available, we 
can estimate the translation probabilities from a 
source word to a target word and vice versa via 
conventional bilingual word alignment. Then both 
( | )k ip f e  and ( | )j kp e f?
 in (3) can be derived:  
 
2( | ) ( | )j k s t j kp e f p e f? ??
 
 
where 
2 ( | )s t j kp e f?
 is the translation model from 
the source-to-target word alignment model, and 
( | )k ip f e  , which enforces the sum-to-1 constraint 
over all words in the source sentence, takes the 
following form, 
 
2
2
0
( | )( | )
( | )
t s k i
k i K
t s k i
k
p f ep f e
p f e
?
?
?
 
 
where 
2 ( | )t s k ip f e  is the translation model from 
the  target-to-source word alignment model. In our 
method, 
2 ( | )t s ip null e  for all target words is 
simply a constant pnull, whose value is optimized 
on held-out data 1.  
The surface similarity model can be estimated 
in several ways. A very simple model could be 
based on exact match: the surface similarity model, 
( | )sur j ip e e?
, would take the value 1.0 if e?= e, and 
0 otherwise 2 . However, a smoothed surface 
similarity model is used in our method. If the target 
language uses alphabetic orthography, as English 
does, we treat words as letter sequences and the 
similarity measure can be the length of the longest 
matched prefix (LMP) or the length of the longest 
common subsequence (LCS) between them. Then, 
this raw similarity measure is transformed to a 
surface similarity score between 0 and 1 through 
an exponential mapping,  
 
? ?( | ) exp ( , ) 1sur j i j ip e e s e e?? ?? ?? ? ?? ?    (4) 
 
where ( , )j is e e?
 is computed as 
 
( , )( , ) max(| |,| |)
j i
j i
j i
M e es e e e e
?? ? ?
 
 
and ( , )j iM e e?
 is the raw similarity measure of ej? 
ei, which is the length of the LMP or LCS of ej? 
and ei. and ? is a smoothing factor that 
characterizes the mapping, Thus as ? approaches 
infinity, ( | )sur j ip e e?
 backs off to the exact match 
model. We found the smoothed similarity model of 
(4) yields slightly better results than the exact 
match model. Both LMP- and LCS- based methods 
achieve similar performance but the computation 
of LMP is faster. Therefore, we only report results 
of the LMP-based smoothed similarity model.  
 
3.3 Estimation of the distortion model 
 
The distortion model, which specifies the transition 
probabilities of the HMM, models the first-order 
dependencies of word ordering. In bilingual 
HMM-based word alignment, it is commonly 
assumed that transition probabilities 
                                                          
1  The other direction, 
2 ( | )s t ip e null? , is available from the 
source-to-target translation model. 
2 Usually a small back-off value is assigned instead of 0.  
100
 1( | , )? ?? ?j jp a i a i I
 depend only on the jump 
distance (i - i')  (Vogel et al, 1996):  
 
1
( )( | , )
( )
I
l
c i ip i i I
c l i
?
??? ?
???
             (5) 
 
As suggested by Liang et al (2006), we can 
group the distortion parameters {c(d)}, d= i - i', 
into a few buckets. In our implementation, 11 
buckets are used for c(?-4),  c(-3), ... c(0), ..., c(5), 
c(?6). The probability mass for transitions with 
jump distance larger than 6 and less than -4 is 
uniformly divided. By doing this, only a handful of 
c(d) parameters need to be estimated. Although it 
is possible to estimate them using the EM 
algorithm on a small development set, we found 
that a particularly simple model, described below, 
works surprisingly well in our experiments.  
Since both the backbone and the hypothesis are 
in the same language, It seems intuitive that the 
distortion model should favor monotonic 
alignment and only allow non-monotonic 
alignment with a certain penalty. This leads us to 
use a distortion model of the following form, 
where K is a tuning factor optimized on held-out 
data. 
 
? ? ? ?1 1c d d ??? ? ?, d= ?4, ?, 6   (6) 
 
As shown in Fig. 2, the value of distortion score 
peaks at d=1, i.e., the monotonic alignment, and 
decays for non-monotonic alignments depending 
on how far it diverges from the monotonic 
alignment. 
 
Figure 2, the distance-based distortion parameters 
computed according to (6), where K=2. 
 
Following Och and Ney (2003), we use a fixed 
value p0 for the probability of jumping to a null 
state, which can be optimized on held-out data, and 
the overall distortion model becomes 
 
0
0
              if     state( | , ) (1 ) ( | , )  otherwise
p i nullp i i I p p i i I
??? ? ? ?? ???
 
 
3.4 Alignment normalization 
 
Given an HMM, the Viterbi alignment algorithm 
can be applied to find the best alignment between 
the backbone and the hypothesis, 
 
1
1 1
1
? argmax ( | , ) ( | )jJ
JJ
j j j aa j
a p a a I p e e?
?
? ??? ? ??
  (7) 
 
However, the alignment produced by the 
algorithm cannot be used directly to build a 
confusion network. There are two reasons for this. 
First, the alignment produced may contain 1-N 
mappings between the backbone and the 
hypothesis whereas 1-1 mappings are required in 
order to build a confusion network. Second, if 
hypothesis words are aligned to a null in the 
backbone or vice versa, we need to insert actual 
nulls into the right places in the hypothesis and the 
backbone, respectively. Therefore, we need to 
normalize the alignment produced by Viterbi 
search. 
 
EB ? e2  ?2   ?   
   ?    ?      e2        ?     ?      ? 
           e1'    e2'    e3'   e4'    
Eh e1'    e2'    e3'   e4'  
(a) hypothesis words are aligned to the backbone null  
 
EB e1  ?1  e2  ?2  e3  ?3    
   ?    e1     e2        e3      ? 
           e2'    ?      e1'   
Eh e1'    e2'    ?  
(b) a backbone word is aligned to no hypothesis word 
 
Figure 3: illustration of alignment normalization 
 
First, whenever more than one hypothesis 
words are aligned to one backbone word, we keep 
the link which gives the highest occupation 
probability computed via the forward-backward 
algorithm. The other hypothesis words originally 
 -4                     1                      6  
 1.0 
 0.0 
   c(d) 
  d 
101
 aligned to the backbone word will be aligned to the 
null associated with that backbone word. 
Second, for the hypothesis words that are 
aligned to a particular null on the backbone side, a 
set of nulls are inserted around that backbone word 
associated with the null such that no links cross 
each other. As illustrated in Fig. 3 (a), if a 
hypothesis word e2? is aligned to the backbone 
word e2, a null is inserted in front of the backbone 
word e2 linked to the hypothesis word e1? that 
comes before e2?. Nulls are also inserted for other 
hypothesis words such as e3? and e4? after the 
backbone word e2. If there is no hypothesis word 
aligned to that backbone word, all nulls are 
inserted after that backbone word .3 
For a backbone word that is aligned to no 
hypothesis word, a null is inserted on the 
hypothesis side, right after the hypothesis word 
which is aligned to the immediately preceding 
backbone word. An example is shown in Fig. 3 (b). 
4 Related work 
The two main hypothesis alignment methods for 
system combination in the previous literature are 
GIZA++ and TER-based methods. Matusov et al 
(2006) proposed using GIZA++ to align words 
between different MT hypotheses, where all 
hypotheses of the test corpus are collected to create 
hypothesis pairs for GIZA++ training. This 
approach uses the conventional HMM model 
bootstrapped from IBM Model-1 as implemented 
in GIZA++, and heuristically combines results 
from aligning in both directions. System 
combination based on this approach gives an 
improvement over the best single system. 
However, the number of hypothesis pairs for 
training is limited by the size of the test corpus. 
Also, MT hypotheses from the same source 
sentence are correlated with each other and these 
hypothesis pairs are not i.i.d. data samples. 
Therefore, GIZA++ training on such a data set may 
be unreliable.  
Bangalore et al (2001) used a multiple string-
matching algorithm based on Levenshtein edit 
distance, and later Sim et al (2007) and Rosti et al 
(2007) extended it to a TER-based method for 
hypothesis alignment. TER (Snover et al, 2006) 
                                                          
3  This only happens if no hypothesis word is aligned to a 
backbone word but some hypothesis words are aligned to the 
null associated with that backbone word. 
measures the minimum number of edits, including 
substitution, insertion, deletion, and shift of blocks 
of words, that are needed to modify a hypothesis so 
that it exactly matches the other hypothesis. The 
best alignment is the one that gives the minimum 
number of translation edits. TER-based confusion 
network construction and system combination has 
demonstrated superior performance on various 
large-scale MT tasks (Rosti. et al 2007). However, 
when searching for the optimal alignment, the 
TER-based method uses a strict surface hard match 
for counting edits. Therefore, it is not able to 
handle synonym matching well. Moreover, 
although TER-based alignment allows phrase 
shifts to accommodate the non-monotonic word 
ordering, all non-monotonic shifts are penalized 
equally no matter how short or how long the move 
is, and this penalty is set to be the same as that for 
substitution, deletion, and insertion edits. 
Therefore, its modeling of non-monotonic word 
ordering is very coarse-grained.  
In contrast to the GIZA++-based method, our 
IHMM-based method has a similarity model 
estimated using bilingual word alignment HMMs 
that are trained on a large amount of bi-text data. 
Moreover, the surface similarity information is 
explicitly incorporated in our model, while it is 
only used implicitly via parameter initialization for 
IBM Model-1 training by Matusov et al (2006). 
On the other hand, the TER-based alignment 
model is similar to a coarse-grained, non-
normalized version of our IHMM, in which the 
similarity model assigns no penalty to an exact 
surface match and a fixed penalty to all 
substitutions, insertions, and deletions, and the 
distortion model simply assigns no penalty to a 
monotonic jump, and a fixed penalty to all other 
jumps, equal to the non-exact-match penalty in the 
similarity model. 
There have been other hypothesis alignment 
methods. Karakos, et al (2008) proposed an ITG-
based method for hypothesis alignment, Rosti et al 
(2008) proposed an incremental alignment method, 
and a heuristic-based matching algorithm was 
proposed by Jayaraman and Lavie (2005).  
5 Evaluation 
In this section, we evaluate our IHMM-based 
hypothesis alignment method on the Chinese-to-
English (C2E) test in the constrained training track 
102
 of the 2008 NIST Open MT Evaluation (NIST, 
2008). We compare to the TER-based method used 
by Rosti et al (2007). In the following 
experiments, the NIST BLEU score is used as the 
evaluation metric (Papineni et al, 2002), which is 
reported as a percentage in the following sections.  
 
5.1 Implementation details 
 
In our implementation, the backbone is selected 
with MBR. Only the top hypothesis from each 
single system is considered as a backbone. A 
uniform posteriori probability is assigned to all 
hypotheses. TER is used as loss function in the 
MBR computation.  
Similar to (Rosti et al, 2007), each word in the 
confusion network is associated with a word 
posterior probability. Given a system S, each of its 
hypotheses is assigned with a rank-based score of 
1/(1+r)?, where r is the rank of the hypothesis, and 
? is a rank smoothing parameter. The system 
specific rank-based score of a word w for a given 
system S is the sum of all the rank-based scores of 
the hypotheses in system S that contain the word w 
at the given position (after hypothesis alignment). 
This score is then normalized by the sum of the 
scores of all the alternative words at the same 
position and from the same system S to generate 
the system specific word posterior. Then, the total 
word posterior of w over all systems is a sum of 
these system specific posteriors weighted by 
system weights. 
Beside the word posteriors, we use language 
model scores and a word count as features for 
confusion network decoding. 
Therefore, for an M-way system combination 
that uses N LMs, a total of M+N+1 decoding 
parameters, including M-1 system weights, one 
rank smoothing factor, N language model weights, 
and one weight for the word count feature, are 
optimized using Powell?s method (Brent, 1973) to 
maximize BLEU score on a development set4 . 
Two language models are used in our 
experiments. One is a trigram model estimated 
from the English side of the parallel training data, 
and the other is a 5-gram model trained on the 
English GigaWord corpus from LDC using the 
MSRLM toolkit (Nguyen et al 2007). 
                                                          
4 The parameters of IHMM are not tuned by maximum-BLEU 
training. 
In order to reduce the fluctuation of BLEU 
scores caused by the inconsistent translation output 
length, an unsupervised length adaptation method 
has been devised. We compute an expected length 
ratio between the MT output and the source 
sentences on the development set after maximum- 
BLEU training. Then during test, we adapt the 
length of the translation output by adjusting the 
weight of the word count feature such that the 
expected output/source length ratio is met. In our 
experiments, we apply length adaptation to the 
system combination output at the level of the 
whole test corpus. 
 
5.2  Development and test data  
 
The development (dev) set used for system 
combination parameter training contains 1002 
sentences sampled from the previous NIST MT 
Chinese-to-English test sets: 35% from MT04, 
55% from MT05, and 10% from MT06-newswire. 
The test set is the MT08 Chinese-to-English 
?current? test set, which includes 1357 sentences 
from both newswire and web-data genres. Both 
dev and test sets have four references per sentence. 
As inputs to the system combination, 10-best 
hypotheses for each source sentence in the dev and 
test sets are collected from each of the eight single 
systems. All outputs on the MT08 test set were 
true-cased before scoring using a log-linear 
conditional Markov model proposed by Toutanova 
et al (2008). However, to save computation effort, 
the results on the dev set are reported in case 
insensitive BLEU (ciBLEU) score instead. 
 
5.3  Experimental results 
 
In our main experiments, outputs from a total of 
eight single MT systems were combined. As listed 
in Table 1, Sys-1 is a tree-to-string system 
proposed by Quirk et al, (2005); Sys-2 is a phrase-
based system with fast pruning proposed by Moore 
and Quirk (2008); Sys-3 is a phrase-based system 
with syntactic source reordering proposed by 
Wang et al (2007a); Sys-4 is a syntax-based pre-
ordering system proposed by Li et. al. (2007); Sys-
5 is a hierarchical system proposed by Chiang 
(2007); Sys-6 is a lexicalized re-ordering system 
proposed by Xiong et al (2006); Sys-7 is a two-
pass phrase-based system with adapted LM 
proposed by Foster and Kuhn (2007); and  Sys-8 is 
103
 a hierarchical system with two-pass rescoring 
using a parser-based LM proposed by Wang et al, 
(2007b). All systems were trained within the 
confines of the constrained training condition of 
NIST MT08 evaluation. These single systems are 
optimized with maximum-BLEU training on 
different subsets of the previous NIST MT test 
data. The bilingual translation models used to 
compute the semantic similarity are from the word-
dependent HMMs proposed by He (2007), which 
are trained on two million parallel sentence-pairs 
selected from the training corpus allowed by the 
constrained training condition of MT08.  
 
5.3.1 Comparison with TER alignment 
In the IHMM-based method, the smoothing 
factor for surface similarity model is set to ? = 3, 
the interpolation factor of the overall similarity 
model is set to ? = 0.3, and the controlling factor of 
the distance-based distortion parameters is set to 
K=2. These settings are optimized on the dev set. 
Individual system results and system combination 
results using both IHMM and TER alignment, on 
both the dev and test sets, are presented in Table 1. 
The TER-based hypothesis alignment tool used in 
our experiments is the publicly available TER Java 
program, TERCOM (Snover et al, 2006). Default 
settings of TERCOM are used in the following 
experiments. 
On the dev set, the case insensitive BLEU score 
of the IHMM-based 8-way system combination 
output is about 5.8 points higher than that of the 
best single system. Compared to the TER-based 
method, the IHMM-based method is about 1.5 
BLEU points better. On the MT08 test set, the 
IHMM-based system combination gave a case 
sensitive BLEU score of 30.89%. It outperformed 
the best single system by 4.7 BLEU points and the 
TER-based system combination by 1.0 BLEU 
points. Note that the best single system on the dev 
set and the test set are different. The different 
single systems are optimized on different tuning 
sets, so this discrepancy between dev set and test 
set results is presumably due to differing degrees 
of mismatch between the dev and test sets and the 
various tuning sets. 
 
 
 
 
 
Table 1. Results of single and combined systems 
on the dev set and the MT08 test set  
System Dev 
ciBLEU% 
MT08 
BLEU% 
System 1 34.08 21.75 
System 2 33.78 20.42 
System 3 34.75 21.69 
System 4 37.85 25.52 
System 5 37.80 24.57 
System 6 37.28 24.40 
System 7 32.37 25.51 
System 8 34.98 26.24 
TER 42.11 29.89 
IHMM 43.62 30.89 
 
In order to evaluate how well our method 
performs when we combine more systems, we 
collected MT outputs on MT08 from seven 
additional single systems as summarized in Table 
2. These systems belong to two groups. Sys-9 to 
Sys-12 are in the first group. They are syntax-
augmented hierarchical systems similar to those 
described by Shen et al (2008) using different 
Chinese word segmentation and language models. 
The second group has Sys-13 to Sys-15. Sys-13 is 
a phrasal system proposed by Koehn et al (2003), 
Sys-14 is a hierarchical system proposed by 
Chiang (2007), and Sys-15 is a syntax-based 
system proposed by Galley et al (2006). All seven 
systems were trained within the confines of the 
constrained training condition of NIST MT08 
evaluation.  
We collected 10-best MT outputs only on the 
MT08 test set from these seven extra systems. No 
MT outputs on our dev set are available from them 
at present. Therefore, we directly adopt system 
combination parameters trained for the previous 8-
way system combination, except the system 
weights, which are re-set by the following 
heuristics: First, the total system weight mass 1.0 is 
evenly divided among the three groups of single 
systems: {Sys-1~8}, {Sys-9~12}, and {Sys-
13~15}. Each group receives a total system weight 
mass of 1/3. Then the weight mass is further 
divided in each group: in the first group, the 
original weights of systems 1~8 are multiplied by 
1/3; in the second and third groups, the weight 
mass is evenly distributed within the group, i.e., 
1/12 for each system in group 2, and 1/9 for each 
104
 system in group 35.  Length adaptation is applied to 
control the final output length, where the same 
expected length ratio of the previous 8-way system 
combination is adopted. 
The results of the 15-way system combination 
are presented in Table 3. It shows that the IHMM-
based method is still about 1 BLEU point better 
than the TER-based method. Moreover, combining 
15 single systems gives an output that has a NIST 
BLEU score of 34.82%, which is 3.9 points better 
than the best submission to the NIST MT08 
constrained training track (NIST, 2008). To our 
knowledge, this is the best result reported on this 
task. 
 
Table 2. Results of seven additional single systems 
on the NIST MT08 test set 
System MT08 
BLEU% 
System 9 29.59 
System 10 29.57 
System 11 29.64 
System 12 29.85 
System 13 25.53 
System 14 26.04 
System 15 29.70 
 
Table 3. Results of the 15-way system combination 
on the NIST MT08 C2E test set 
Sys. Comb.  MT08 
BLEU% 
TER 33.81 
IHMM 34.82 
 
5.3.2 Effect of the similarity model  
In this section, we evaluate the effect of the 
semantic similarity model and the surface 
similarity model by varying the interpolation 
weight ? of (2). The results on both the dev and 
test sets are reported in Table 4. In one extreme 
case, ? = 1, the overall similarity model is based 
only on semantic similarity. This gives a case 
insensitive BLEU score of 41.70% and a case 
sensitive BLEU score of 28.92% on the dev and 
test set, respectively. The accuracy is significantly 
improved to 43.62% on the dev set and 30.89% on 
test set when ? = 0.3. In another extreme case, ? = 
                                                          
5 This is just a rough guess because no dev set is available. We 
believe a better set of system weights could be obtained if MT 
outputs on a common dev set were available. 
0, in which only the surface similarity model is 
used for the overall similarity model, the 
performance degrades by about 0.2 point. 
Therefore, the surface similarity information seems 
more important for monolingual hypothesis 
alignment, but both sub-models are useful.  
 
Table 4. Effect of the similarity model 
 Dev 
ciBLEU% 
Test 
BLEU% 
? = 1.0 41.70 28.92 
? = 0.7 42.86 30.50 
? = 0.5 43.11 30.94 
? = 0.3 43.62 30.89 
? = 0.0 43.35 30.73 
 
5.3.3 Effect of the distortion model  
We investigate the effect of the distance-based 
distortion model by varying the controlling factor 
K in (6). For example, setting K=1.0 gives a linear-
decay distortion model, and setting K=2.0 gives a 
quadratic smoothed distance-based distortion 
model. As shown in Table 5, the optimal result can 
be achieved using a properly smoothed distance-
based distortion model. 
 
Table 5. Effect of the distortion model 
 Dev 
ciBLEU% 
Test 
BLEU% 
K=1.0 42.94 30.44 
K=2.0 43.62 30.89 
K=4.0 43.17 30.30 
K=8.0 43.09 30.01 
6 Conclusion 
Synonym matching and word ordering are two 
central issues for hypothesis alignment in 
confusion-network-based MT system combination. 
In this paper, an IHMM-based method is proposed 
for hypothesis alignment. It uses a similarity model 
for synonym matching and a distortion model for 
word ordering. In contrast to previous methods, the 
similarity model explicitly incorporates both 
semantic and surface word similarity, which is 
critical to monolingual word alignment, and a 
smoothed distance-based distortion model is used 
to model the first-order dependency of word 
ordering, which is shown to be better than simpler 
approaches. 
105
 Our experimental results show that the IHMM-
based hypothesis alignment method gave superior 
results on the NIST MT08 C2E test set compared 
to the TER-based method. Moreover, we show that 
our system combination method can scale up to 
combining more systems and produce a better 
output that has a case sensitive BLEU score of 
34.82, which is 3.9 BLEU points better than the 
best official submission of MT08.  
Acknowledgement 
The authors are grateful to Chris Quirk, Arul 
Menezes, Kristina Toutanova, William Dolan, Mu 
Li, Chi-Ho Li, Dongdong Zhang, Long Jiang, 
Ming Zhou, George Foster, Roland Kuhn, Jing 
Zheng, Wen Wang, Necip Fazil Ayan, Dimitra 
Vergyri, Nicolas Scheffer, Andreas Stolcke, Kevin 
Knight, Jens-Soenke Voeckler, Spyros Matsoukas, 
and Antti-Veikko Rosti for assistance with the MT 
systems and/or for the valuable suggestions and 
discussions.  
 
References  
Srinivas Bangalore, German Bordel, and Giuseppe 
Riccardi. 2001. Computing consensus translation 
from multiple machine translation systems. In Proc. 
of IEEE ASRU, pp. 351?354. 
Richard Brent, 1973. Algorithms for Minimization 
without Derivatives. Prentice-Hall, Chapter 7. 
David Chiang. 2007. Hierarchical phrase-based 
translation. Computational Linguistics, 33(2):201?
228. 
George Foster and Roland Kuhn. 2007. Mixture-Model 
Adaptation for SMT. In Proc. of the Second ACL 
Workshop on Statistical Machine Translation. pp. 
128 ? 136. 
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel 
Marcu, Steve DeNeefe, Wei Wang and Ignacio 
Thayer. 2006. Scalable Inference and Training of 
Context-Rich Syntactic Translation Models. In Proc. 
of COLING-ACL, pp. 961?968. 
Xiaodong He. 2007. Using Word-Dependent Transition 
Models in HMM based Word Alignment for 
Statistical Machine Translation. In Proc. of the 
Second ACL Workshop on Statistical Machine 
Translation. 
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word 
matching. In Proc. of EAMT. pp. 143 ? 152. 
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, 
and Markus Dreyer. 2008. Machine Translation 
System Combination using ITG-based Alignments. 
In Proc. of ACL-HLT, pp. 81?84. 
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li, Ming 
Zhou, Yi Guan. 2007. A Probabilistic Approach to 
Syntax-based Reordering for Statistical Machine 
Translation. In Proc. of ACL. pp. 720 ? 727. 
Percy Liang, Ben Taskar, and Dan Klein. 2006. 
Alignment by Agreement. In Proc. of NAACL. pp 
104 ? 111.  
Evgeny Matusov, Nicola Ueffing, and Hermann Ney. 
2006. Computing consensus translation from 
multiple machine translation systems using enhanced 
hypotheses alignment. In Proc. of EACL, pp. 33?40. 
Robert Moore and Chris Quirk. 2007. Faster Beam-
Search Decoding for Phrasal Statistical Machine 
Translation. In Proc. of MT Summit XI. 
Patrick Nguyen, Jianfeng Gao and Milind Mahajan. 
2007. MSRLM: a scalable language modeling 
toolkit. Microsoft Research Technical Report MSR-
TR-2007-144. 
NIST. 2008. The 2008 NIST Open Machine Translation 
Evaluation. www.nist.gov/speech/tests/mt/2008/doc/  
Franz J. Och and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1):19?51. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proc. of ACL, 
pp. 311?318. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
2003. Statistical phrase based translation. In Proc. of 
NAACL. pp. 48 ? 54. 
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. 
Dependency treelet translation: Syntactically 
informed phrasal SMT. In Proc. of ACL. pp. 271?
279. 
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas, 
Richard Schwartz, Necip Fazil Ayan, and Bonnie J. 
Dorr. 2007a. Combining outputs from multiple 
machine translation systems. In Proc. of NAACL-
HLT, pp. 228?235. 
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard 
Schwartz. 2007b. Improved Word-Level System 
Combination for Machine Translation. In Proc. of 
ACL, pp. 312?319. 
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas, 
and Richard Schwartz. 2008. Incremental Hypothesis 
Alignment for Building Confusion Networks with 
Application to Machine Translation System 
Combination, In Proc. of the Third ACL Workshop 
on Statistical Machine Translation, pp. 183?186. 
Libin Shen, Jinxi Xu, Ralph Weischedel. 2008. A New 
String-to-Dependency Machine Translation 
Algorithm with a Target Dependency Language 
Model. In Proc. of ACL-HLT, pp. 577?585. 
106
 Khe Chai Sim, William J. Byrne, Mark J.F. Gales, 
Hichem Sahbi, and Phil C. Woodland. 2007. 
Consensus network decoding for statistical machine 
translation system combination. In Proc. of ICASSP, 
vol. 4. pp. 105?108. 
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea 
Micciulla, and John Makhoul. 2006. A study of 
translation edit rate with targeted human annotation. 
In Proc. of AMTA. 
Kristina Toutanova, Hisami Suzuki and Achim Ruopp. 
2008. Applying Morphology Generation Models to 
Machine Translation. In Proc. of ACL. pp. 514 ? 522. 
Stephan Vogel, Hermann Ney, and Christoph Tillmann. 
1996. HMM-based Word Alignment In Statistical 
Translation. In Proc. of COLING. pp. 836-841. 
Chao Wang, Michael Collins, and Philipp Koehn. 
2007a. Chinese Syntactic Reordering for Statistical 
Machine Translation.  In Proc. of EMNLP-CoNLL. 
pp. 737-745. 
Wen Wang, Andreas Stolcke, Jing Zheng. 2007b. 
Reranking Machine Translation Hypotheses With 
Structured and Web-based Language Models. In 
Proc. of IEEE ASRU. pp. 159 ? 164. 
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. 
Maximum Entropy Based Phrase Reordering Model 
for Statistical Machine Translation. In Proc. of ACL. 
pp. 521 ? 528. 
107
Phrase-Based Backoff Models for Machine Translation of Highly Inflected
Languages
Mei Yang
Department of Electrical Engineering
University of Washington
Seattle, WA, USA
yangmei@ee.washington.edu
Katrin Kirchhoff
Department of Electrical Engineering
University of Washington
Seattle, WA, USA
katrin@ee.washington.edu
Abstract
We propose a backoff model for phrase-
based machine translation that translates
unseen word forms in foreign-language
text by hierarchical morphological ab-
stractions at the word and the phrase level.
The model is evaluated on the Europarl
corpus for German-English and Finnish-
English translation and shows improve-
ments over state-of-the-art phrase-based
models.
1 Introduction
Current statistical machine translation (SMT) usu-
ally works well in cases where the domain is
fixed, the training and test data match, and a large
amount of training data is available. Nevertheless,
standard SMT models tend to perform much bet-
ter on languages that are morphologically simple,
whereas highly inflected languages with a large
number of potential word forms are more prob-
lematic, particularly when training data is sparse.
SMT attempts to find a sentence e? in the desired
output language given the corresponding sentence
f in the source language, according to
e? = argmaxeP (f |e)P (e) (1)
Most state-of-the-art SMT adopt a phrase-based
approach such that e is chunked into I phrases
e?1, ..., e?I and the translation model is defined
over mappings between phrases in e and in f .
i.e. P (f? |e?). Typically, phrases are extracted from
a word-aligned training corpus. Different inflected
forms of the same lemma are treated as different
words, and there is no provision for unseen forms,
i.e. unknown words encountered in the test data
are not translated at all but appear verbatim in the
output. Although the percentage of such unseen
word forms may be negligible when the training
set is large and matches the test set well, it may rise
drastically when training data is limited or from
a different domain. Many current and future ap-
plications of machine translation require the rapid
porting of existing systems to new languages and
domains without being able to collect appropri-
ate training data; this problem can therefore be
expected to become increasingly more important.
Furthermore, untranslated words can be one of the
main factors contributing to low user satisfaction
in practical applications.
Several previous studies (see Section 2 below)
have addressed issues of morphology in SMT, but
most of these have focused on the problem of word
alignment and vocabulary size reduction. Princi-
pled ways of incorporating different levels of mor-
phological abstraction into phrase-based models
have mostly been ignored so far. In this paper we
propose a hierarchical backoff model for phrase-
based translation that integrates several layers of
morphological operations, such that more specific
models are preferred over more general models.
We experimentally evaluate the model on transla-
tion from two highly-inflected languages, German
and Finnish, into English and present improve-
ments over a state-of-the-art system. The rest of
the paper is structured as follows: The following
section discusses related background work. Sec-
tion 4 describes the proposed model; Sections 5
and 6 provide details about the data and baseline
system used in this study. Section 7 provides ex-
perimental results and discussion. Section 8 con-
cludes.
41
2 Morphology in SMT Systems
Previous approaches have used morpho-syntactic
knowledge mainly at the low-level stages of a ma-
chine translation system, i.e. for preprocessing.
(Niessen and Ney, 2001a) use morpho-syntactic
knowledge for reordering certain syntactic con-
structions that differ in word order in the source
vs. target language (German and English). Re-
ordering is applied before training and after gener-
ating the output in the target language. Normaliza-
tion of English/German inflectional morphology
to base forms for the purpose of word alignment is
performed in (Corston-Oliver and Gamon, 2004)
and (Koehn, 2005), demonstrating that the vocab-
ulary size can be reduced significantly without af-
fecting performance.
Similar morphological simplifications have
been applied to other languages such as Roma-
nian (Fraser and Marcu, 2005) in order to de-
crease word alignment error rate. In (Niessen
and Ney, 2001b), a hierarchical lexicon model is
used that represents words as combinations of full
forms, base forms, and part-of-speech tags, and
that allows the word alignment training procedure
to interpolate counts based on the different lev-
els of representation. (Goldwater and McCloskey,
2005) investigate various morphological modifi-
cations for Czech-English translations: a subset
of the vocabulary was converted to stems, pseu-
dowords consisting of morphological tags were in-
troduced, and combinations of stems and morpho-
logical tags were used as new word forms. Small
improvements were found in combination with a
word-to-word translation model. Most of these
techniques have focused on improving word align-
ment or reducing vocabulary size; however, it is
often the case that better word alignment does not
improve the overall translation performance of a
standard phrase-based SMT system.
Phrase-based models themselves have not ben-
efited much from additional morpho-syntactic
knowledge; e.g. (Lioma and Ounis, 2005) do not
report any improvement from integrating part-of-
speech information at the phrase level. One suc-
cessful application of morphological knowledge is
(de Gispert et al, 2005), where knowledge-based
morphological techniques are used to identify un-
seen verb forms in the test text and to generate
inflected forms in the target language based on
annotated POS tags and lemmas. Phrase predic-
tion in the target language is conditioned on the
phrase in the source language as well the corre-
sponding tuple of lemmatized phrases. This tech-
nique worked well for translating from a morpho-
logically poor language (English) to a more highly
inflected language (Spanish) when applied to un-
seen verb forms. Treating both known and un-
known verbs in this way, however, did not result
in additional improvements. Here we extend the
notion of treating known and unknown words dif-
ferently and propose a backoff model for phrase-
based translation.
3 Backoff Models
Generally speaking, backoff models exploit rela-
tionships between more general and more spe-
cific probability distributions. They specify under
which conditions the more specific model is used
and when the model ?backs off? to the more gen-
eral distribution. Backoff models have been used
in a variety of ways in natural language process-
ing, most notably in statistical language modeling.
In language modeling, a higher-order n-gram dis-
tribution is used when it is deemed reliable (deter-
mined by the number of occurrences in the train-
ing data); otherwise, the model backs off to the
next lower-order n-gram distribution. For the case
of trigrams, this can be expressed as:
pBO(wt|wt?1, wt?2) (2)
=
{
dcpML(wt|wt?1, wt?2) if c > ?
?(wt?1, wt?2)pBO(wt|wt?1) otherwise
where pML denotes the maximum-likelihood
estimate, c denotes the count of the triple
(wi, wi?1, wi?2) in the training data, ? is the count
threshold above which the maximum-likelihood
estimate is retained, and dN(wi,wi?1,wi?2) is a dis-
counting factor (generally between 0 and 1) that is
applied to the higher-order distribution. The nor-
malization factor ?(wi?1, wi?2) ensures that the
distribution sums to one. In (Bilmes and Kirch-
hoff, 2003) this method was generalized to a back-
off model with multiple paths, allowing the com-
bination of different backed-off probability esti-
mates. Hierarchical backoff schemes have also
been used by (Zitouni et al, 2003) for language
modeling and by (Gildea, 2001) for semantic role
labeling. (Resnik et al, 2001) used backoff trans-
lation lexicons for cross-language information re-
trieval. More recently, (Xi and Hwa, 2005) have
used backoff models for combining in-domain and
42
out-of-domain data for the purpose of bootstrap-
ping a part-of-speech tagger for Chinese, outper-
forming standard methods such as EM.
4 Backoff Models in MT
In order to handle unseen words in the test data
we propose a hierarchical backoff model that uses
morphological information. Several morphologi-
cal operations, in particular stemming and com-
pound splitting, are interleaved such that a more
specific form (i.e. a form closer to the full word
form) is chosen before a more general form (i.e. a
form that has undergone morphological process-
ing). The procedure is shown in Figure 1 and can
be described as follows: First, a standard phrase
table based on full word forms is trained. If an
unknown word fi is encountered in the test data
with context cfi = fi?n, ..., fi?1, fi+1, ..., fi+m,
the word is first stemmed, i.e. f ?i = stem(fi).
The phrase table entries for words sharing the
same stem are then modified by replacing the
respective words with their stems. If an en-
try can be found among these such that the
source language side of the phrase pair consists of
fi?n, ..., fi?1, stem(fi), fi+1, ..., fi+m, the corre-
sponding translation is used (or, if several pos-
sible translations occur, the one with the high-
est probability is chosen). Note that the con-
text may be empty, in which case a single-word
phrase is used. If this step fails, the model backs
off to the next level and applies compound split-
ting to the unknown word (further described be-
low), i.e.(f ??i1, f ??i2) = split(fi). The match with
the original word-based phrase table is then per-
formed again. If this step fails for either of the
two parts of f ??, stemming is applied again: f ???i1 =
stem(f ??i1) and f ???i2 = stem(f ??i2), and a match with
the stemmed phrase table entries is carried out.
Only if the attempted match fails at this level is the
input passed on verbatim in the translation output.
The backoff procedure could in principle be
performed on demand by a specialized decoder;
however, since we use an off-the-shelf decoder
(Pharaoh (Koehn, 2004)), backoff is implicitly en-
forced by providing a phrase-table that includes
all required backoff levels and by preprocessing
the test data accordingly. The phrase table will
thus include entries for phrases based on full word
forms as well as for their stemmed and/or split
counterparts.
For each entry with decomposed morphological
i
i i
i1 i2 i
i1 i1
i2 i2
i1 i2
Figure 1: Backoff procedure.
forms, four probabilities need to be provided: two
phrasal translation scores for both translation di-
rections, p(e?|f?) and p(f? |e?), and two correspond-
ing lexical scores, which are computed as a prod-
uct of the word-by-word translation probabilities
under the given alignment a:
plex(e?|f?) =
J
?
j=1
1
|j|a(i) = j|
I
?
a(i)=j
p(fj |ei) (3)
where j ranges of words in phrase f? and i ranges
of words in phrase e?. In the case of unknown
words in the foreign language, we need the prob-
abilities p(e?|stem(f?)), p(stem(f?)|e?) (where the
stemming operation stem(f?) applies to the un-
known words in the phrase), and their lexical
equivalents. These are computed by relative fre-
quency estimation, e.g.
p(e?|stem(f?)) = count(e?, stem(f?))count(stem(f?)) (4)
The other translation probabilities are computed
analogously. Since normalization is performed
over the entire phrase table, this procedure has
the effect of discounting the original probability
porig(e?|f?) since e? may now have been generated
by either f? or by stem(f?). In the standard formu-
lation of backoff models shown in Equation 3, this
amounts to:
pBO(e?|f?) (5)
=
{
de?,f?porig(e?|f?) if c(e?, f?) > 0
p(e?|stem(f?)) otherwise
43
where
de?,f? =
1 ? p(e?, stem(f?))
p(e?, f?) (6)
is the amount by which the word-based phrase
translation probability is discounted. Equiva-
lent probability computations are carried out for
the lexical translation probabilities. Similar to
the backoff level that uses stemming, the trans-
lation probabilities need to be recomputed for
the levels that use splitting and combined split-
ting/stemming.
In order to derive the morphological decompo-
sition we use existing tools. For stemming we
use the TreeTagger (Schmid, 1994) for German
and the Snowball stemmer1 for Finnish. A vari-
ety of ways for compound splitting have been in-
vestigated in machine translation (Koehn, 2003).
Here we use a simple technique that considers all
possible ways of segmenting a word into two sub-
parts (with a minimum-length constraint of three
characters on each subpart). A segmentation is ac-
cepted if the subparts appear as individual items
in the training data vocabulary. The only linguis-
tic knowledge used in the segmentation process is
the removal of final <s> from the first part of the
compound before trying to match it to an existing
word. This character (Fugen-s) is often inserted as
?glue? when forming German compounds. Other
glue characters were not considered for simplic-
ity (but could be added in the future). The seg-
mentation method is clearly not linguistically ad-
equate: first, words may be split into more than
two parts. Second, the method may generate mul-
tiple possible segmentations without a principled
way of choosing among them; third, it may gener-
ate invalid splits. However, a manual analysis of
300 unknown compounds in the German develop-
ment set (see next section) showed that 95.3% of
them were decomposed correctly: for the domain
at hand, most compounds need not be split into
more than two parts; if one part is itself a com-
pound it is usually frequent enough in the train-
ing data to have a translation. Furthermore, lexi-
calized compounds, whose decomposition would
lead to wrong translations, are also typically fre-
quent words and have an appropriate translation in
the training data.
1http://snowball.tartarus.org
5 Data
Our data consists of the Europarl training, devel-
opment and test definitions for German-English
and Finnish-English of the 2005 ACL shared data
task (Koehn and Monz, 2005). Both German
and Finnish are morphologically rich languages:
German has four cases and three genders and
shows number, gender and case distinctions not
only on verbs, nouns, and adjectives, but also
on determiners. In addition, it has notoriously
many compounds. Finnish is a highly agglutina-
tive language with a large number of inflectional
paradigms (e.g. one for each of its 15 cases). Noun
compounds are also frequent. On the 2005 ACL
shared MT data task, Finnish to English trans-
lation showed the lowest average performance
(17.9% BLEU) and German had the second low-
est (21.9%), while the average BLEU scores for
French-to-English and Spanish-to-English were
much higher (27.1% and 27.8%, respectively).
The data was preprocessed by lowercasing and
filtering out sentence pairs whose length ratio
(number of words in the source language divided
by the number of words in the target language,
or vice versa) was > 9. The development and
test sets consist of 2000 sentences each. In order
to study the effect of varying amounts of training
data we created several training partitions consist-
ing of random selections of a subset of the full
training set. The sizes of the partitions are shown
in Table 1, together with the resulting percentage
of out-of-vocabulary (OOV) words in the develop-
ment and test sets (?type? refers to a unique word
in the vocabulary, ?token? to an instance in the ac-
tual text).
6 System
We use a two-pass phrase-based statistical MT
system using GIZA++ (Och and Ney, 2000) for
word alignment and Pharaoh (Koehn, 2004) for
phrase extraction and decoding. Word alignment
is performed in both directions using the IBM-
4 model. Phrases are then extracted from the
word alignments using the method described in
(Och and Ney, 2003). For first-pass decoding we
use Pharaoh in n-best mode. The decoder uses a
weighted combination of seven scores: 4 transla-
tion model scores (phrase-based and lexical scores
for both directions), a trigram language model
score, a distortion score, and a word penalty. Non-
monotonic decoding is used, with no limit on the
44
German-English
Set # sent # words oov dev oov test
train1 5K 101K 7.9/42.6 7.9/42.7
train2 25K 505K 3.8/22.1 3.7/21.9
train3 50K 1013K 2.7/16.1 2.7/16.1
train4 250K 5082K 1.3/8.1 1.2/7.5
train5 751K 15258K 0.8/4.9 0.7/4.4
Finnish-English
Set # sent # words oov dev oov test
train1 5K 78K 16.6/50.6 16.4/50.6
train2 25K 395K 8.6/28.2 8.4/27.8
train3 50K 790K 6.3/21.0 6.2/20.8
train4 250K 3945K 3.1/10.4 3.0/10.2
train5 717K 11319K 1.8/6.2 1.8/6.1
Table 1: Training set sizes and percentages of
OOV words (types/tokens) on the development
and test sets.
dev test
Finnish-English 22.2 22.0
German-English 24.6 24.8
Table 2: Baseline system BLEU scores (%) on dev
and test sets.
number of moves. The score combination weights
are trained by a minimum error rate training pro-
cedure similar to (Och and Ney, 2003). The tri-
gram language model uses modified Kneser-Ney
smoothing and interpolation of trigram and bigram
estimates and was trained on the English side of
the bitext. In the first pass, 2000 hypotheses are
generated per sentence. In the second pass, the
seven scores described above are combined with
4-gram language model scores. The performance
of the baseline system on the development and test
sets is shown in Table 2. The BLEU scores ob-
tained are state-of-the-art for this task.
7 Experiments and Results
We first investigated to what extent the OOV rate
on the development data could be reduced by our
backoff procedure. Table 3 shows the percentage
of words that are still untranslatable after back-
off. A comparison with Table 1 shows that the
backoff model reduces the OOV rate, with a larger
reduction effect observed when the training set
is smaller. We next performed translation with
backoff systems trained on each data partition. In
each case, the combination weights for the indi-
German-English
dev set test set
train1 5.2/27.7 5.1/27.3
train2 2.0/11.7 2.0/11.6
train3 1.4/8.1 1.3/7.6
train4 0.5/3.1 0.5/2.9
train5 0.3/1.7 0.2/1.3
Finnish-English
dev set test set
train1 9.1/28.5 9.2/28.9
train2 3.8/12.4 3.7/12.3
train3 2.5/8.2 2.4/8.0
train4 0.9/3.2 0.9/3.0
train5 0.4/1.4 0.4/1.5
Table 3: OOV rates (%) on the development
and test sets under the backoff model (word
types/tokens).
vidual model scores were re-optimized. Table 4
shows the evaluation results on the dev set. Since
the BLEU score alone is often not a good indi-
cator of successful translations of unknown words
(the unigram or bigram precision may be increased
but may not have a strong effect on the over-
all BLEU score), position-independent word error
rate (PER) rate was measured as well. We see im-
provements in BLEU score and PERs in almost
all cases. Statistical significance was measured on
PER using a difference of proportions significance
test and on BLEU using a segment-level paired
t-test. PER improvements are significant almost
all training conditions for both languages; BLEU
improvements are significant in all conditions for
Finnish and for the two smallest training sets for
German. The effect on the overall development set
(consisting of both sentences with known words
only and sentences with unknown words) is shown
in Table 5. As expected, the impact on overall per-
formance is smaller, especially for larger training
data sets, due to the relatively small percentage of
OOV tokens (see Table 1). The evaluation results
for the test set are shown in Tables 6 (for the sub-
set of sentences with OOVs) and 7 (for the entire
test set), with similar conclusions.
The examples A and B in Figure 2 demon-
strate higher-scoring translations produced by the
backoff system as opposed to the baseline sys-
tem. An analysis of the backoff system output
showed that in some cases (e.g. examples C and
45
German-English
baseline backoff
Set BLEU PER BLEU PER
train1 14.2 56.9 15.4 55.5
train2 16.3 55.2 17.3 51.8
train3 17.8 51.1 18.4 49.7
train4 19.6 51.1 19.9 47.6
train5 21.9 46.6 22.6 46.0
Finnish-English
baseline backoff
Set BLEU PER BLEU PER
Set BLEU PER BLEU PER
train1 12.4 59.9 13.6 57.8
train2 13.0 61.2 13.9 59.1
train3 14.0 58.0 14.7 57.8
train4 17.4 52.7 18.4 50.8
train5 16.8 52.7 18.7 50.2
Table 4: BLEU (%) and position-independent
word error rate (PER) on the subset of the devel-
opment data containing unknown words (second-
pass output). Here and in the following tables,
statistically significant differences to the baseline
model are shown in boldface (p < 0.05).
German-English
baseline backoff
Set BLEU PER BLEU PER
train1 15.3 56.4 16.3 55.1
train2 19.0 53.0 19.5 51.6
train3 20.0 49.9 20.5 49.3
train4 22.2 49.0 22.4 48.1
train5 24.6 46.5 24.7 45.6
Finnish-English
baseline backoff
Set BLEU PER BLEU PER
train1 13.1 59.3 14.4 57.4
train2 14.5 59.7 15.4 58.3
train3 16.0 56.5 16.5 56.5
train4 21.0 50.0 21.4 49.2
train5 22.2 50.5 22.5 49.7
Table 5: BLEU (%) and position-independent
word error rate (PER) for the entire development
set.
German-English
baseline backoff
Set BLEU PER BLEU PER
train1 14.3 56.2 15.5 55.1
train2 17.1 54.3 17.6 50.7
train3 17.4 50.8 18.1 49.7
train4 18.9 49.8 18.8 48.2
train5 19.1 46.3 19.4 46.2
Finnish-English
baseline backoff
Set BLEU PER BLEU PER
train1 12.4 59.5 13.5 57.5
train2 13.3 60.7 14.2 59.0
train3 14.1 58.2 15.1 57.3
train4 17.2 54.0 18.4 50.2
train5 16.6 51.8 19.0 49.4
Table 6: BLEU (%) and position-independent
word error rate (PER) for the test set (subset with
OOV words).
D in Figure 2), the backoff model produced a
good translation, but the translation was a para-
phrase rather than an identical match to the ref-
erence translation. Since only a single reference
translation is available for the Europarl data (pre-
venting the computation of a BLEU score based
on multiple hand-annotated references), good but
non-matching translations are not taken into ac-
count by our evaluation method. In other cases
the unknown word was translated correctly, but
since it was translated as single-word phrase the
segmentation of the entire sentence was affected.
This may cause greater distortion effects since the
sentence is segmented into a larger number of
smaller phrases, each of which can be reordered.
We therefore added the possibility of translating
an unknown word in its phrasal context by stem-
ming up to m words to the left and right in the
original sentence and finding translations for the
entire stemmed phrase (i.e. the function stem()
is now applied to the entire phrase). This step
is inserted before the stemming of a single word
f in the backoff model described above. How-
ever, since translations for entire stemmed phrases
were found only in about 1% of all cases, there
was no significant effect on the BLEU score. An-
other possibility of limiting reordering effects re-
sulting from single-word translations of OOVs is
to restrict the distortion limit of the decoder. Our
46
German-English
baseline backoff
Set BLEU PER BLEU PER
train1 15.3 55.8 16.3 54.8
train2 19.4 52.3 19.6 50.9
train3 20.3 49.6 20.7 49.2
train4 22.5 48.1 22.5 47.9
train5 24.8 46.3 25.1 45.5
Finnish-English
baseline backoff
Set BLEU PER BLEU PER
train1 12.9 58.7 14.0 57.0
train2 14.5 59.5 15.3 58.4
train3 15.6 56.6 16.4 56.2
train4 20.6 50.3 21.0 49.6
train5 22.0 50.0 22.3 49.5
Table 7: BLEU (%) and position-independent
word error rate (PER) for the test set (entire test
set).
experiments showed that this improves the BLEU
score slightly for both the baseline and the backoff
system; the relative difference, however, remained
the same.
8 Conclusions
We have presented a backoff model for phrase-
based SMT that uses morphological abstractions
to translate unseen word forms in the foreign lan-
guage input. When a match for an unknown word
in the test set cannot be found in the trained phrase
table, the model relies instead on translation prob-
abilities derived from stemmed or split versions
of the word in its phrasal context. An evalua-
tion of the model on German-English and Finnish-
English translations of parliamentary proceedings
showed statistically significant improvements in
PER for almost all training conditions and signifi-
cant improvements in BLEU when the training set
is small (100K words), with larger improvements
for Finnish than for German. This demonstrates
that our method is mainly relevant for highly in-
flected languages and sparse training data condi-
tions. It is also designed to improve human accep-
tance of machine translation output, which is par-
ticularly adversely affected by untranslated words.
Acknowledgments
This work was funded by NSF grant no. IIS-
0308297. We thank Ilona Pitka?nen for help with
Example A: (German-English):
SRC: wir sind berzeugt davon, dass ein europa des friedens
nicht durch milita?rbu?ndnisse geschaffen wird.
BASE: we are convinced that a europe of peace, not by
milita?rbu?ndnisse is created.
BACKOFF: we are convinced that a europe of peace, not
by military alliance is created.
REF: we are convinced that a europe of peace will not be
created through military alliances.
Example B. (Finnish-English):
SRC: arvoisa puhemies, puhuimme ta?a?lla? eilisiltana
serviasta ja siella? tapahtuvista vallankumouksellisista
muutoksista.
BASE: mr president, we talked about here last night, on
the subject of serbia and there, of vallankumouksellisista
changes.
BACKOFF: mr president, we talked about here last
night, on the subject of serbia and there, of revolutionary
changes.
REF: mr. president, last night we discussed the topic of
serbia and the revolutionary changes that are taking place
there.
Example C. (Finnish-English):
SRC: toivon ta?lta? osin, etta? yhdistyneiden kansakuntien
alaisuudessa ka?yta?vissa? neuvotteluissa pa?a?sta?isiin sell-
aiseen lopputulokseen, etta? kyproksen kreikkalainen ja
turkkilainen va?esto?nosa voisivat yhdessa? nauttia liittymisen
mukanaan tuomista eduista yhdistetyssa? tasavallassa.
BASE: i hope that the united nations in the negotiations
to reach a conclusion that the greek and turkish accession
to the benefi t of the benefi ts of the republic of ydistetyssa?
brings together va?esto?nosa could, in this respect, under the
auspices.
BACKOFF: i hope that the united nations in the nego-
tiations to reach a conclusion that the greek and turkish
communities can work together to bring the benefi ts of the
accession of the republic of ydistetyssa?. in this respect,
under the
REF: in this connection, i would hope that the talks
conducted under the auspices of the united nations will be
able to come to a successful conclusion enabling the greek
and turkish cypriot populations to enjoy the advantages
of membership of the european union in the context of a
reunifi ed republic.
Example D. (German-English):
SRC:so sind wir beim durcharbeiten des textes verfahren,
wobei wir bei einer reihe von punkten versucht haben, noch
einige straffungen vorzunehmen.
BASE: we are in the durcharbeiten procedures of the text,
although we have tried to make a few straffungen to carry
out on a number of issues.
BACKOFF: we are in the durcharbeiten procedures, and
we have tried to make a few streamlining of the text in a
number of points.
REF: this is how we came to go through the text, and
attempted to cut down on certain items in the process.
Figure 2: Translation examples (SRC = source,
BASE = baseline system, BACKOFF = backoff
system, REF = reference). OOVs and their trans-
lation are marked in boldface.
47
the Finnish language.
References
J.A. Bilmes and K. Kirchhoff. 2003. Factored lan-
guage models and generalized parallel backoff. In
Proceedings of the 2003 Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 4?6, Edmonton, Canada.
S. Corston-Oliver and M. Gamon. 2004. Normaliz-
ing German and English inflectional morphology to
improve statistical word alignment. In Robert E.
Frederking and Kathryn Taylor, editors, Proceedings
of the Conference of the Association for Machine
Translation in the Americas, pages 48?57, Washing-
ton, DC.
A. de Gispert, J.B. Marin?o, and J.M. Crego. 2005. Im-
proving statistical machine translation by classifying
and generalizing inflected verb forms. In Proceed-
ings of 9th European Conference on Speech Commu-
nication and Technology, pages 3193?3196, Lisboa,
Portugal.
A. Fraser and D. Marcu. 2005. ISI?s participation in
the Romanian-English alignment task. In Proceed-
ings of the 2005 ACL Workshop on Building and Us-
ing Parallel Texts: Data-Driven Machine Transla-
tion and Beyond, pages 91?94, Ann Arbor, Michi-
gan.
D. Gildea. 2001. Statistical Language Understanding
Using Frame Semantics. Ph.D. thesis, University of
California, Berkeley, California.
S. Goldwater and D. McCloskey. 2005. Improving sta-
tistical MT through morphological analysis. In Pro-
ceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Nat-
ural Language Processing, pages 676?683, Vancou-
ver, British Columbia, Canada.
P. Koehn and C. Monz. 2005. Shared task: statistical
machine translation between European languages.
In Proceedings of the 2005 ACL Workshop on Build-
ing and Using Parallel Texts: Data-Driven Machine
Translation and Beyond, pages 119?124, Ann Ar-
bor, Michigan.
P. Koehn. 2003. Noun Phrase Translation. Ph.D. the-
sis, Information Sciences Institute, USC, Los Ange-
les, California.
P. Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models.
In Robert E. Frederking and Kathryn Taylor, editors,
Proceedings of the Conference of the Association for
Machine Translation in the Americas, pages 115?
124, Washington, DC.
P. Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In Proceedings of MT
Summit X, Phuket, Thailand.
C. Lioma and I. Ounis. 2005. Deploying part-of-
speech patterns to enhance statistical phrase-based
machine translation resources. In Proceedings of the
2005 ACL Workshop on Building and Using Paral-
lel Texts: Data-Driven Machine Translation and Be-
yond, pages 163?166, Ann Arbor, Michigan.
S. Niessen and H. Ney. 2001a. Morpho-syntactic
analysis for reordering in statistical machine trans-
lation. In Proceedings of MT Summit VIII, Santiago
de Compostela, Galicia, Spain.
S. Niessen and H. Ney. 2001b. Toward hierar-
chical models for statistical machine translation of
inflected languages. In Proceedings of the ACL
2001 Workshop on Data-Driven Methods in Ma-
chine Translation, pages 47?54, Toulouse, France.
F.J. Och and H. Ney. 2000. Giza++:
Training of statistical translation mod-
els. http://www-i6.informatik.rwth-
aachen.de/ och/software/GIZA++.html.
F.J. Och and H. Ney. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan.
P. Resnik, D. Oard, and G.A. Levow. 2001. Improved
cross-language retrieval using backoff translation.
In Proceedings of the First International Conference
on Human Language Technology Research, pages
153?155, San Diego, California.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of the Inter-
national Conference on New Methods in Language
Processing, pages 44?49, Manchester, UK.
C. Xi and R. Hwa. 2005. A backoff model for boot-
strapping resources for non-English languages. In
Proceedings of Human Language Technology Con-
ference and Conference on Empirical Methods in
Natural Language Processing, pages 851?858, Van-
couver, British Columbia, Canada.
I. Zitouni, O. Siohan, and C.-H. Lee. 2003. Hierar-
chical class n-gram language models: towards bet-
ter estimation of unseen events in speech recogni-
tion. In Proceedings of 8th European Conference on
Speech Communication and Technology, pages 237?
240, Geneva, Switzerland.
48
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 237?240,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Toward Smaller, Faster, and Better Hierarchical Phrase-based SMT
Mei Yang
Dept. of Electrical Engineering
University of Washington, Seattle, WA, USA
yangmei@u.washington.edu
Jing Zheng
SRI International
Menlo Park, CA, USA
zj@speech.sri.com
Abstract
We investigate the use of Fisher?s exact
significance test for pruning the transla-
tion table of a hierarchical phrase-based
statistical machine translation system. In
addition to the significance values com-
puted by Fisher?s exact test, we introduce
compositional properties to classify phrase
pairs of same significance values. We also
examine the impact of using significance
values as a feature in translation mod-
els. Experimental results show that 1% to
2% BLEU improvements can be achieved
along with substantial model size reduc-
tion in an Iraqi/English two-way transla-
tion task.
1 Introduction
Phrase-based translation (Koehn et al, 2003)
and hierarchical phrase-based translation (Chiang,
2005) are the state of the art in statistical ma-
chine translation (SMT) techniques. Both ap-
proaches typically employ very large translation
tables extracted from word-aligned parallel data,
with many entries in the tables never being used
in decoding. The redundancy of translation ta-
bles is not desirable in real-time applications,
e.g., speech-to-speech translation, where speed
and memory consumption are often critical con-
cerns. In addition, some translation pairs in a table
are generated from training data errors and word
alignment noise. Removing those pairs could lead
to improved translation quality.
(Johnson et al, 2007) has presented a tech-
nique for pruning the phrase table in a phrase-
based SMT system using Fisher?s exact test. They
compute the significance value of each phrase
pair and prune the table by deleting phrase pairs
with significance values smaller than a threshold.
Their experimental results show that the size of the
phrase table can be greatly reduced with no signif-
icant loss in translation quality.
In this paper, we extend the work in (Johnson
et al, 2007) to a hierarchical phrase-based transla-
tion model, which is built on synchronous context-
free grammars (SCFG). We call an SCFG rule a
phrase pair if its right-hand side does not contain a
nonterminal, and otherwise a rewrite rule. Our ap-
proach applies to both the phrase table and the rule
table. To address the problem that many transla-
tion pairs share the same significance value from
Fisher?s exact test, we propose a refined method
that combines significance values and composi-
tional properties of surface strings for pruning the
phrase table. We also examine the effect of using
the significance values as a feature in translation
models.
2 Fisher?s exact test for translation table
pruning
2.1 Significance values by Fisher?s exact test
We briefly review the approach for computing
the significance value of a translation pair using
Fisher?s exact test. In Fisher?s exact test, the sig-
nificance of the association of two items is mea-
sured by the probability of seeing the number of
co-occurrences of the two items being the same
as or higher than the one observed in the sam-
ple. This probability is referred to as the p-value.
Given a parallel corpus consisting of N sentence
pairs, the probability of seeing a pair of phrases
(or rules) (s?,
?
t) with the joint frequency C(s?,
?
t) is
given by the hypergeometric distribution
P
h
(C(s?,
?
t))
=
C(s?)!(N ? C(s?))!C(
?
t)!(N ? C(
?
t))!
N !C(s?,
?
t)!C(s?,?
?
t)!C(?s?,
?
t)!C(?s?,?
?
t)!
where C(s?) and C(
?
t) are the marginal frequencies
of s? and
?
t, respectively. C(s?,?
?
t) is the number
of sentence pairs that contain s? on the source side
237
but do not contain
?
t on the target side, and similar
for the definition of C(?s?,
?
t) and C(?s?,?
?
t). The
p-value is therefore the sum of the probabilities of
seeing the two phrases (or rules) occur as often
as or more often than C(s?,
?
t) but with the same
marginal frequencies
P
v
(C(s?,
?
t)) =
?
?
c=C(s?,
?
t)
P
h
(c)
In practice, p-values can be very small, and thus
negative logarithm p-values are often used instead
as the measure of significance. In the rest of this
paper, the negative logarithm p-value is referred to
as the significance value. Therefore, the larger the
value, the greater the significance.
2.2 Table pruning with significance values
The basic scheme to prune a translation table is
to delete all translation pairs that have significance
values smaller than a given threshold.
However, in practice, this pruning scheme does
not work well with phrase tables, as many phrase
pairs receive the same significance values. In par-
ticular, many phrase pairs in the phrase table have
joint and both marginal frequencies all equal to
1. Such phrase pairs are referred to as triple-1
pairs. It can be shown that the significance value
of triple-1 phrase pairs is log(N). Given a thresh-
old, triple-1 phrase pairs either all remain in the
phrase table or are discarded entirely.
To look closer at the problem, Figure 1 shows
two example tables with their percentages of
phrase pairs that have higher, equal, or lower sig-
nificance values than log(N). When the thresh-
old is smaller than log(N), as many as 35% of
the phrase pairs can be deleted. When the thresh-
old is greater than log(N), at least 90% of the
phrase pairs will be discarded. There is no thresh-
old that prunes the table in the range of 35% to
90%. One may think that it is right to delete all
triple-1 phrase pairs as they occur only once in
the parallel corpus. However, it has been shown
in (Moore, 2004) that when a large number of
singleton-singleton pairs, such as triple-1 phrase
pairs, are observed, most of them are not due to
chance. In other words, most triple-1 phrase pairs
are significant and it is likely that the translation
quality will decline if all of them are discarded.
Therefore, using significance values alone can-
not completely resolve the problem of phrase ta-
ble pruning. To further discriminate phrase pairs
80%90%100% 50%60%70%80%90%100%
>?log
(N)
30%40%50%60%70%80%90%100%
>?log
(N)
=?log
(N)
<?log
(N)
0%10%20%30%40%50%60%70%80%90%100%
>?log
(N)
=?log
(N)
<?log
(N)
0%10%20%30%40%50%60%70%80%90%100%
Table
1
Table
2
>?log
(N)
=?log
(N)
<?log
(N)
0%10%20%30%40%50%60%70%80%90%100%
Table
1
Table
2
>?log
(N)
=?log
(N)
<?log
(N)
Figure 1: Percentages of phrase pairs with higher,
equal, and lower significance values than log(N).
of the same significance values, particularly the
triple-1 phrase pairs, more information is needed.
The Fisher?s exact test does not consider the sur-
face string in phrase pairs. Intuitively, some phrase
pairs are less important if they can be constructed
by other phrase pairs in the decoding phase, while
other phrase pairs that involve complex syntac-
tic structures are usually difficult to construct and
thus become more important. This intuition in-
spires us to explore the compositional property of
a phrase pair as an additional factor. More for-
mally, we define the compositional property of a
phrase pair as the capability of decomposing into
subphrase pairs. If a phrase pair (s?,
?
t) can be de-
composed into K subphrase pairs (s?
k
,
?
t
k
) already
in the phrase table such that
s? = s?
1
s?
2
. . . s?
K
?
t =
?
t
1
?
t
2
. . .
?
t
K
then this phrase pair is compositional; otherwise
it is noncompositional. Our intuition suggests that
noncompositional phrase pairs are more important
as they cannot be generated by concatenating other
phrase pairs in order in the decoding phase. This
leads to a refined scheme for pruning the phrase ta-
ble, in which a phrase pair is discarded when it has
a significance value smaller than the threshold and
it is not a noncompositional triple-1 phrase pair.
The definition of the compositional property does
not allow re-ordering. If re-ordering is allowed,
all phrase pairs will be compositional as they can
always be decomposed into pairs of single words.
In the rule table, however, the percentage of
triple-1 pairs is much smaller, typically less than
10%. This is because rules are less sparse than
phrases in general, as they are extracted with a
shorter length limit, and have nonterminals that
match any span of words. Therefore, the basic
pruning scheme works well with rule tables.
238
3 Experiment
3.1 Hierarchical phrase-based SMT system
Our hierarchical phrase-based SMT system trans-
lates from Iraqi Arabic (IA) to English (EN) and
vice versa. The training corpus consists of 722K
aligned Iraqi and English sentence pairs and has
5.0M and 6.7M words on the Iraqi and English
sides, respectively. A held-out set with 18K Iraqi
and 19K English words is used for parameter tun-
ing and system comparison. The test set is the
TRANSTAC June08 offline evaluation data with
7.4K Iraqi and 10K English words, and the transla-
tion quality is evaluated by case-insensitive BLEU
with four references.
3.2 Results on translation table pruning
For each of the two translation directions IA-to-
EN and EN-to-IA, we pruned the translation ta-
bles as below, where ? represents the significance
value of triple-1 pairs and ? is a small positive
number. Phrase table PTABLE3 is obtained us-
ing the refined pruning scheme, and others are ob-
tained using the basic scheme. Figure 2 shows the
percentages of translation pairs in these tables.
? PTABLE0: phrase table of full size without
pruning.
? PTABLE1: pruned phrase table using the
threshold ? ? ? and thus all triple-1 phrase
pairs remain.
? PTABLE2: pruned phrase table using the
threshold ? + ? and thus all triple-1 phrase
pairs are discarded.
? PTABLE3: pruned phrase table using the
threshold ? + ? and the refined pruning
scheme. All but noncompositional triple-1
phrase pairs are discarded.
? RTABLE0: rule table of full size without
pruning.
? RTABLE1: pruned rule table using the thresh-
old ?+ ?.
Since a hierarchical phrase-based SMT system
requires a phrase table and a rule table at the same
time, performance of different combinations of
phrase and rule tables is evaluated. The baseline
system will be the one using the full-size tables of
PTABLE0 and RTABLE0. Tables 2 and 3 show the
BLEU scores for each combination in each direc-
tion, with the best score in bold.
708090100
PTAB
LE0
5060708090100
PTAB
LE0
PTAB
LE1
30405060708090100
PTAB
LE0
PTAB
LE1
PTAB
LE2
PTAB
LE3
RTAB
LE0
102030405060708090100
PTAB
LE0
PTAB
LE1
PTAB
LE2
PTAB
LE3
RTAB
LE0
RTAB
LE1
0102030405060708090100
IA?to
?EN
EN?to
?IA
PTAB
LE0
PTAB
LE1
PTAB
LE2
PTAB
LE3
RTAB
LE0
RTAB
LE1
0102030405060708090100
IA?to
?EN
EN?to
?IA
PTAB
LE0
PTAB
LE1
PTAB
LE2
PTAB
LE3
RTAB
LE0
RTAB
LE1
Figure 2: The percentages of translation pairs in
phrase and rule tables.
It can be seen that pruning leads to a substan-
tial reduction in the number of translation pairs.
As long phrases are more frequently pruned than
short phrases, the actual memory saving is even
more significant. It is surprising to see that using
pruned tables improves the BLEU scores in many
cases, probably because a smaller translation table
generalizes better on an unseen test set, and some
translation pairs created by erroneous training data
are dropped. Table 1 shows two examples of dis-
carded phrase pairs and their frequencies. Both of
them are incorrect due to human translation errors.
We note that using the pruned rule table
RTABLE1 is very effective and improved BLEU
in most cases except when used with PTABLE0 in
the direction EN-to-IA. Although using the pruned
phrase tables had mixed effect, PTABLE3, which
is obtained through the refined pruning scheme,
outperformed others in all cases. This confirms
the hypothesis that noncompositional phrase pairs
are important and thus suggests that the proposed
compositional property is a useful measure of
phrase pair quality. Overall, the best results are
achieved by using the combination of PTABLE3
and RTABLE1, which gave improvement of 1% to
2% BLEU over the baseline systems. Meanwhile,
this combination is also twice faster than the base-
line system in decoding.
3.3 Results on using significance values as a
feature
The p-value of each translation pair can be used
as a feature in the log-linear translation model,
to penalize those less significant phrase pairs and
rewrite rules. Since component feature values can-
not be zero, a small positive number was added to
p-values to avoid infinite log value. The results
of using p-values as a feature with different com-
binations of phrase and rule tables are shown in
239
Iraqi Arabic phrase English phrase in data Correct English phrase Frequencies
there are four of us there are five of us 1, 29, 1
young men three of four young men three or four 1, 1, 1
Table 1: Examples of pruned phrase pairs and their frequencies C(s?,
?
t), C(s?), and C(
?
t).
RTABLE0 RTABLE1
PTABLE0 47.38 48.40
PTABLE1 47.05 48.45
PTABLE2 47.50 48.70
PTABLE3 47.81 49.43
Table 2: BLEU scores of IA-to-EN systems using
different combinations of phrase and rule tables.
RTABLE0 RTABLE1
PTABLE0 29.92 29.05
PTABLE1 29.62 30.60
PTABLE2 29.87 30.57
PTABLE3 30.62 31.27
Table 3: BLEU scores of EN-to-IA systems using
different combinations of phrase and rule tables.
Tables 4 and 5. We can see that the results ob-
tained by using the full rule table with the fea-
ture of p-values (the columns of RTABLE0 in Ta-
bles 4 and 5) are much worse than those obtained
by using the pruned rule table without the fea-
ture of p-values (the columns of RTABLE1 in Ta-
bles 2 and 3). This suggests that the use of signif-
icance values as a feature in translation models is
not as efficient as the use in translation table prun-
ing. Modest improvement was observed in the di-
rection EN-to-IA when both pruning and the fea-
ture of p-values are used (compare the columns
of RTABLE1 in Tables 3 and 5) but not in the
direction IA-to-EN. Again, the best results are
achieved by using the combination of PTABLE3
and RTABLE1.
4 Conclusion
The translation quality and speed of a hierarchi-
cal phrase-based SMT system can be improved
by aggressive pruning of translation tables. Our
proposed pruning scheme, which exploits both
significance values and compositional properties,
achieved the best translation quality and gave im-
provements of 1% to 2% on BLEU when com-
pared to the baseline system with full-size tables.
The use of significance values in translation table
RTABLE0 RTABLE1
PTABLE0 47.72 47.96
PTABLE1 46.69 48.75
PTABLE2 47.90 48.48
PTABLE3 47.59 49.50
Table 4: BLEU scores of IA-to-EN systems using
the feature of p-values in different combinations.
RTABLE0 RTABLE1
PTABLE0 29.33 30.44
PTABLE1 30.28 30.99
PTABLE2 30.38 31.44
PTABLE3 30.74 31.64
Table 5: BLEU scores of EN-to-IA systems using
the feature of p-values in different combinations.
pruning and in translation models as a feature has
a different effect: the former led to significant im-
provement, while the latter achieved only modest
or no improvement on translation quality.
5 Acknowledgements
Many thanks to Kristin Precoda and Andreas
Kathol for valuable discussion. This work is sup-
ported by DARPA, under subcontract 55-000916
to UW under prime contract NBCHD040058 to
SRI International.
References
Philipp Koehn, Franz J. Och and Daniel Marcu. 2003.
Statistical phrase-based translation. Proceedings of
HLT-NAACL, 48-54, Edmonton, Canada.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. Proceed-
ings of ACL, 263-270, Ann Arbor, Michigan, USA.
J Howard Johnson, Joel Martin, George Foster and
Roland Kuhn. 2007. Improving Translation Quality
by Discarding Most of the Phrasetable. Proceed-
ings of EMNLP-CoNLL, 967-975, Prague, Czech
Republic.
Robert C. Moore. 2004. On Log-Likelihood-Ratios
and the Significance of Rare Events. Proceedings of
EMNLP, 333-340, Barcelona, Spain
240
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 125?128,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Improved Language Modeling for Statistical Machine Translation
Katrin Kirchhoff and Mei Yang
Department of Electrical Engineering
University of Washington, Seattle, WA, 98195
{katrin,yangmei}@ee.washington.edu
Abstract
Statistical machine translation systems
use a combination of one or more transla-
tion models and a language model. While
there is a significant body of research ad-
dressing the improvement of translation
models, the problem of optimizing lan-
guage models for a specific translation
task has not received much attention. Typ-
ically, standard word trigram models are
used as an out-of-the-box component in
a statistical machine translation system.
In this paper we apply language model-
ing techniques that have proved benefi-
cial in automatic speech recognition to the
ACL05 machine translation shared data
task and demonstrate improvements over a
baseline system with a standard language
model.
1 Introduction
Statistical machine translation (SMT) makes use of
a noisy channel model where a sentence e? in the de-
sired language can be conceived of as originating as
a sentence f? in a source language. The goal is to
find, for every input utterance f? , the best hypothesis
e?? such that
e?? = argmaxe?P (e?|f?) = argmaxe?P (f? |e?)P (e?)
(1)
P (f? |e?) is the translation model expressing proba-
bilistic constraints on the association of source and
target strings. P (e?) is a language model specifying
the probability of target language strings. Usually, a
standard word trigram model of the form
P (e1, ..., el) ?
l
?
i=3
P (ei|ei?1, ei?2) (2)
is used, where e? = e1, ..., el . Each word is predicted
based on a history of two preceding words.
Most work in SMT has concentrated on develop-
ing better translation models, decoding algorithms,
or minimum error rate training for SMT. Compara-
tively little effort has been spent on language mod-
eling for machine translation. In other fields, partic-
ularly in automatic speech recognition (ASR), there
exists a large body of work on statistical language
modeling, addressing e.g. the use of word classes,
language model adaptation, or alternative probabil-
ity estimation techniques. The goal of this study was
to use some of the language modeling techniques
that have proved beneficial for ASR in the past and
to investigate whether they transfer to statistical ma-
chine translation. In particular, this includes lan-
guage models that make use of morphological and
part-of-speech information, so-called factored lan-
guage models.
2 Factored Language Models
A factored language model (FLM) (Bilmes and
Kirchhoff, 2003) is based on a representation of
words as feature vectors and can utilize a variety of
additional information sources in addition to words,
such as part-of-speech (POS) information, morpho-
logical information, or semantic features, in a uni-
fied and principled framework. Assuming that each
125
word w can be decomposed into k features, i.e. w ?
f1:K , a trigram model can be defined as
p(f1:K1 , f1:K2 , ..., f 1:KT ) ?
T
?
t=3
p(f1:Kt |f1:Kt?1 , f1:Kt?2 )
(3)
Each word is dependent not only on a single stream
of temporally preceding words, but also on addi-
tional parallel streams of features. This represen-
tation can be used to provide more robust probabil-
ity estimates when a particular word n-gram has not
been observed in the training data but its correspond-
ing feature combinations (e.g. stem or tag trigrams)
has been observed. FLMs are therefore designed to
exploit sparse training data more effectively. How-
ever, even when a sufficient amount of training data
is available, a language model utilizing morpholog-
ical and POS information may bias the system to-
wards selecting more fluent translations, by boost-
ing the score of hypotheses with e.g. frequent POS
combinations. In FLMs, word feature information
is integrated via a new generalized parallel back-
off technique. In standard Katz-style backoff, the
maximum-likelihood estimate of an n-gram with too
few observations in the training data is replaced with
a probability derived from the lower-order (n ? 1)-
gram and a backoff weight as follows:
pBO(wt|wt?1, wt?2) (4)
=
{
dcpML(wt|wt?1, wt?2) if c > ?
?(wt?1, wt?2)pBO(wt|wt?1) otherwise
where c is the count of (wt, wt?1, wt?2), pML
denotes the maximum-likelihood estimate, ? is a
count threshold, dc is a discounting factor and
?(wt?1, wt?2) is a normalization factor. During
standard backoff, the most distant conditioning vari-
able (in this case wt?2) is dropped first, followed
by the second most distant variable etc., until the
unigram is reached. This can be visualized as a
backoff path (Figure 1(a)). If additional condition-
ing variables are used which do not form a tempo-
ral sequence, it is not immediately obvious in which
order they should be eliminated. In this case, sev-
eral backoff paths are possible, which can be sum-
marized in a backoff graph (Figure 1(b)). Paths in
this graph can be chosen in advance based on lin-
guistic knowledge, or at run-time based on statis-
tical criteria such as counts in the training set. It
tW 1tW? 2tW? 3tW?
tW 1tW? 2tW?
tW 1tW?
tW
(a)
F 1F 2F 3F
F
F 1F 2F F 1F 3F F 2F 3F
F 1F F 3FF 2F
(b)
Figure 1: Standard backoff path for a 4-gram lan-
guage model over words (left) and backoff graph
over word features (right).
is also possible to choose multiple paths and com-
bine their probability estimates. This is achieved by
replacing the backed-off probability pBO in Equa-
tion 2 by a general function g, which can be any
non-negative function applied to the counts of the
lower-order n-gram. Several different g functions
can be chosen, e.g. the mean, weighted mean, prod-
uct, minimum or maximum of the smoothed prob-
ability distributions over all subsets of conditioning
factors. In addition to different choices for g, dif-
ferent discounting parameters can be selected at dif-
ferent levels in the backoff graph. One difficulty in
training FLMs is the choice of the best combination
of conditioning factors, backoff path(s) and smooth-
ing options. Since the space of different combina-
tions is too large to be searched exhaustively, we use
a guided search procedure based on Genetic Algo-
rithms (Duh and Kirchhoff, 2004), which optimizes
the FLM structure with respect to the desired crite-
rion. In ASR, this is usually the perplexity of the
language model on a held-out dataset; here, we use
the BLEU scores of the oracle 1-best hypotheses on
the development set, as described below. FLMs have
previously shown significant improvements in per-
plexity and word error rate on several ASR tasks
(e.g. (Vergyri et al, 2004)).
3 Baseline System
We used a fairly simple baseline system trained us-
ing standard tools, i.e. GIZA++ (Och and Ney, 2000)
for training word alignments and Pharaoh (Koehn,
2004) for phrase-based decoding. The training data
126
was that provided on the ACL05 Shared MT task
website for 4 different language pairs (translation
from Finnish, Spanish, French into English); no
additional data was used. Preprocessing consisted
of lowercasing the data and filtering out sentences
with a length ratio greater than 9. The total num-
ber of training sentences and words per language
pair ranged between 11.3M words (Finnish-English)
and 15.7M words (Spanish-English). The develop-
ment data consisted of the development sets pro-
vided on the website (2000 sentences each). We
trained our own word alignments, phrase table, lan-
guage model, and model combination weights. The
language model was a trigram model trained us-
ing the SRILM toolkit, with modified Kneser-Ney
smoothing and interpolation of higher- and lower-
order ngrams. Combination weights were trained
using the minimum error weight optimization pro-
cedure provided by Pharaoh. We use a two-pass de-
coding approach: in the first pass, Pharaoh is run
in N-best mode to produce N-best lists with 2000
hypotheses per sentence. Seven different compo-
nent model scores are collected from the outputs,
including the distortion model score, the first-pass
language model score, word and phrase penalties,
and bidirectional phrase and word translation scores,
as used in Pharaoh (Koehn, 2004). In the second
pass, the N-best lists are rescored with additional
language models. The resulting scores are then com-
bined with the above scores in a log-linear fashion.
The combination weights are optimized on the de-
velopment set to maximize the BLEU score. The
weighted combined scores are then used to select
the final 1-best hypothesis. The individual rescoring
steps are described in more detail below.
4 Language Models
We trained two additional language models to be
used in the second pass, one word-based 4-gram
model, and a factored trigram model. Both were
trained on the same training set as the baseline sys-
tem. The 4-gram model uses modified Kneser-
Ney smoothing and interpolation of higher-order
and lower-order n-gram probabilities. The potential
advantage of this model is that it models n-grams
up to length 4; since the BLEU score is a combina-
tion of n-gram precision scores up to length 4, the
integration of a 4-gram language model might yield
better results. Note that this can only be done in a
rescoring framework since the first-pass decoder can
only use a trigram language model.
For the factored language models, a feature-based
word representation was obtained by tagging the text
with Rathnaparki?s maximum-entropy tagger (Rat-
naparkhi, 1996) and by stemming words using the
Porter stemmer (Porter, 1980). Thus, the factored
language models use two additional features per
word. A word history of up to 2 was considered (3-
gram FLMs). Rather than optimizing the FLMs on
the development set references, they were optimized
to achieve a low perplexity on the oracle 1-best hy-
potheses (the hypotheses with the best individual
BLEU scores) from the first decoding pass. This is
done to avoid optimizing the model on word combi-
nations that might never be hypothesized by the first-
pass decoder, and to bias the model towards achiev-
ing a high BLEU score. Since N-best lists differ for
different language pairs, a separate FLM was trained
for each language pair. While both the 4-gram lan-
guage model and the FLMs achieved a 8-10% reduc-
tion in perplexity on the dev set references compared
to the baseline language model, their perplexities on
the oracle 1-best hypotheses were not significantly
different from that of the baseline model.
5 N-best List Rescoring
For N-best list rescoring, the original seven model
scores are combined with the scores of the second-
pass language models using the framework of dis-
criminative model combination (Beyerlein, 1998).
This approach aims at an optimal (with respect to
a given error criterion) integration of different infor-
mation sources in a log-linear model, whose com-
bination weights are trained discriminatively. This
combination technique has been used successfully
in ASR, where weights are typically optimized to
minimize the empirical word error count on a held-
out set. In this case, we use the BLEU score of
the N-best hypothesis as an optimization criterion.
Optimization is performed using a simplex downhill
method known as amoeba search (Nelder and Mead,
1965), which is available as part of the SRILM
toolkit.
127
Language pair 1st pass oracle
Fi-En 21.8 29.8
Fr-En 28.9 34.4
De-En 23.9 31.0
Es-En 30.8 37.4
Table 1: First-pass (left column) and oracle results
(right column) on the dev set (% BLEU).
Language pair 4-gram FLM both
Fi-En 22.2 22.2 22.3
Fr-En 30.2 30.2 30.4
De-En 24.6 24.2 24.6
Es-En 31.4 31.0 31.3
Table 2: Second-pass rescoring results (% BLEU)
on the dev set for 4-gram LM, 3-gram FLM, and
their combination.
6 Results
The results from the first decoding pass on the de-
velopment set are shown in Table 1. The second
column in Table 1 lists the oracle BLEU scores for
the N-best lists, i.e. the scores obtained by always
selecting the hypothesis known to have the highest
individual BLEU score. We see that considerable
improvements can in principle be obtained by a bet-
ter second-pass selection of hypotheses. The lan-
guage model rescoring results are shown in Table 2,
for both types of second-pass language models indi-
vidually, and for their combination. In both cases we
obtain small improvements in BLEU score, with the
4-gram providing larger gains than the 3-gram FLM.
Since their combination only yielded negligible ad-
ditional improvements, only 4-grams were used for
processing the final evaluation sets. The evaluation
results are shown in Table 3.
Language pair baseline 4-gram
Fi-En 21.6 22.0
Fr-En 29.3 30.3
De-En 24.2 24.8
Es-En 30.5 31.0
Table 3: Second-pass rescoring results (% BLEU)
on the evaluation set.
7 Conclusions
We have demonstrated improvements in BLEU
score by utilizing more complex language models
in the rescoring pass of a two-pass SMT system.
We noticed that FLMs performed worse than word-
based 4-gram models. However, only trigram FLM
were used in the present experiments; larger im-
provements might be obtained by 4-gram FLMs.
The weights assigned to the second-pass language
models during weight optimization were larger than
those assigned to the first-pass language model, sug-
gesting that both the word-based model and the FLM
provide more useful scores than the baseline lan-
guage model. Finally, we observed that the overall
improvement represents only a small portion of the
possible increase in BLEU score as indicated by the
oracle results, suggesting that better language mod-
els do not have a significant effect on the overall sys-
tem performance unless the translation model is im-
proved as well.
Acknowledgements
This work was funded by the National Science
Foundation, Grant no. IIS-0308297. We are grate-
ful to Philip Koehn for assistance with Pharaoh.
References
P. Beyerlein. 1998. Discriminative model combination. In
Proc. ICASSP, pages 481?484.
J.A. Bilmes and K. Kirchhoff. 2003. Factored language mod-
els and generalized parallel backoff. In Proceedings of
HLT/NAACL, pages 4?6.
K. Duh and K. Kirchhoff. 2004. Automatic learning of lan-
guage model structure. In Proceedings of COLING.
P. Koehn. 2004. Pharaoh: a beam search decoder for phrase-
based statistical machine translation models. In Proceedings
of AMTA.
J.A. Nelder and R. Mead. 1965. A simplex method for function
minimization. Computing Journal, 7(4):308?313.
F.J. Och and H. Ney. 2000. Giza++: Training of sta-
tistical translation models. http://www-i6.informatik.rwth-
aachen.de/ och/software/GIZA++.html.
M.F. Porter. 1980. An algorithm for suffix stripping. Program,
14(3):130?137.
A. Ratnaparkhi. 1996. A maximum entropy part-of-speech tag-
ger. In Proceedings EMNLP, pages 133?141.
D. Vergyri et al 2004. Morphology-based language modeling
for Arabic speech recognition. In Proceedings of ICSLP.
128
Proceedings of the Third Workshop on Statistical Machine Translation, pages 123?126,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
The University of Washington Machine Translation System for
ACL WMT 2008
Amittai Axelrod, Mei Yang, Kevin Duh, Katrin Kirchhoff
Department of Electrical Engineering
University of Washington
Seattle, WA 98195
{amittai,yangmei,kevinduh,katrin} @ee.washington.edu
Abstract
This paper present the University of Washing-
ton?s submission to the 2008 ACL SMT shared ma-
chine translation task. Two systems, for English-to-
Spanish and German-to-Spanish translation are de-
scribed. Our main focus was on testing a novel
boosting framework for N-best list reranking and
on handling German morphology in the German-to-
Spanish system. While boosted N-best list reranking
did not yield any improvements for this task, simpli-
fying German morphology as part of the preprocess-
ing step did result in significant gains.
1 Introduction
The University of Washington submitted systems
to two data tracks in the WMT 2008 shared task
competition, English-to-Spanish and German-to-
Spanish. In both cases, we focused on the in-domain
test set only. Our main interest this year was on in-
vestigating an improved weight training scheme for
N-best list reranking that had previously shown im-
provements on a smaller machine translation task.
For German-to-Spanish translation we additionally
investigated simplifications of German morphology,
which is known to be fairly complex due to a large
number of compounds and inflections. In the fol-
lowing sections we first describe the data, baseline
system and postprocessing steps before describing
boosted N-best list reranking and morphology-based
preprocessing for German.
2 Data and Basic Preprocessing
We used the Europarl data as provided (version 3b,
1.25 million sentence pairs) for training the transla-
tion model for use in the shared task. The data was
lowercased and tokenized with the auxiliary scripts
provided, and filtered according to the ratio of the
sentence lengths in order to eliminate mismatched
sentence pairs. This resulted in about 965k paral-
lel sentences for English-Spanish and 950k sentence
pairs for German-Spanish. Additional preprocess-
ing was applied to the German corpus, as described
in Section 5. For language modeling, we addition-
ally used about 82M words of Spanish newswire text
from the Linguistic Data Consortium (LDC), dating
from 1995 to 1998.
3 System Overview
3.1 Translation model
The system developed for this year?s shared task
is a state-of-the-art, two-pass phrase-based statisti-
cal machine translation system based on a log-linear
translation model (Koehn et al 2003). The trans-
lation models and training method follow the stan-
dard Moses (Koehn et al 2007) setup distributed as
part of the shared task. We used the training method
suggested in the Moses documentation, with lexical-
ized reordering (the msd-bidirectional-fe
option) enabled. The system was tuned via Mini-
mum Error Rate Training (MERT) on the first 500
sentences of the devtest2006 dataset.
123
3.2 Decoding
Our system used the Moses decoder to generate
2000 output hypotheses per input sentence during
the first translation pass. For the second pass, the
N-best lists were rescored with the additional lan-
guage models described below. We re-optimized the
model combination weights with a parallelized im-
plementation of MERT over 16 model scores on the
test2007 dataset. Two of these model scores for
each hypothesis were from the two language models
used in our second-pass system, and the rest corre-
spond to the 14 Moses model weights (for reorder-
ing, language model, translation model, and word
penalty).
3.3 Language models
We built all of our language models using the
SRILM toolkit (Stolcke, 2002) with modified
Kneser-Ney discounting and interpolating all n-
gram estimates of order > 1. For first-pass de-
coding we used a 4-gram language model trained
on the Spanish side of the Europarl v3b data. The
optimal n-gram order was determined by testing
language models with varying orders (3 to 5) on
devtest2006; BLEU scores obtained using the
various language models are shown in Table 1. The
4-gram model performed best.
Table 1: LM ngram size vs. output BLEU on the dev sets.
order devtest2006 test2007
3-gram 30.54 30.69
4-gram 31.03 30.94
5-gram 30.85 30.84
Two additional language models were used for
second pass rescoring. First, we trained a large out-
of-domain language model on Spanish newswire
text obtained from the LDC, dating from 1995 to
1998.
We used a perplexity-filtering method to filter out
the least relevant half of the out-of-domain text, in
order to significantly reduce the training time of
the large language model and accelerate the rescor-
ing process. This was done by computing the per-
plexity of an in-domain language model on each
newswire sentence, and then discarding all sen-
tences with greater than average perplexity. This
reduced the size of the training set from 5.8M sen-
tences and 166M tokens to 2.8M sentences and 82M
tokens. We then further restricted the vocabulary to
the union of the vocabulary lists of the Spanish sides
of the de-es and en-es parallel training corpora. The
remaining text was used to train the language model.
The second language model used for rescoring
was a 5-gram model over part-of-speech (POS) tags.
This model was built using the Spanish side of the
English-Spanish parallel training corpus. The POS
tags were obtained from the corpus using Freeling
v2.0 (Atserias et al 2006).
We selected the language models for our transla-
tion system were selected based on performance on
the English-to-Spanish task, and reused them for the
German-to-Spanish task.
4 Boosted Reranking
We submitted an alternative system, based on a
different re-ranking method, called BoostedMERT
(Duh and Kirchhoff, 2008), for each task. Boosted-
MERT is a novel boosting algorithm that uses Mini-
mum Error Rate Training (MERT) as a weak learner
to build a re-ranker that is richer than the standard
log-linear models. This is motivated by the obser-
vation that log-linear models, as trained by MERT,
often do not attain the oracle BLEU scores of the N-
best lists in the development set. While this may be
due to a local optimum in MERT, we hypothesize
that log-linear models based on our K re-ranking
features are also not sufficiently expressive.
BoostedMERT is inspired by the idea of Boosting
(for classification), which has been shown to achieve
low training (and generalization) error due to classi-
fier combination. In BoostedMERT, we maintain a
weight for each N-best list in the development set.
In each iteration, MERT is performed to find the best
ranker on weighted data. Then, the weights are up-
dated based on whether the current ranker achieves
oracle BLEU. For N-best lists that achieve BLEU
scores far lower than the oracle, the weights are in-
creased so that they become the emphasis of next
iteration?s MERT. We currently use the factor e?r
to update the N-best list distribution, where r is the
ratio of the oracle hypothesis? BLEU to the BLEU
of the selected hypothesis. The final ranker is a
124
weighted combination of many such rankers.
More precisely, let wi be the weights trained by
MERT at iteration i. Given any wi, we can gener-
ate a ranking yi over an N-best list where yi is an
N-dimensional vector of predicted ranks. The final
ranking vector is a weighted sum: y =
?T
i=1 ?iyi,
where ?i are parameters estimated during the boost-
ing process. These parameters are optimized for
maximum BLEU score on the development set. The
only user-specified parameter is T , the number of
boosting iterations. Here, we choose T by divid-
ing the dev set in half: dev1 and dev2. First, we
train BoostedMERT on dev1 for 50 iterations, then
pick the T with the best BLEU score on dev2. Sec-
ond, we train BoostedMERT on dev2 and choose the
optimal T from dev1. Following the philosophy of
classifier combination, we sum the final rank vectors
y from each of the dev1- and dev2-trained Boosted-
MERT to obtain our final ranking result.
5 German ? Spanish Preprocessing
German is a morphologically complex language,
characterized by a high number of noun compounds
and rich inflectional paradigms. Simplification of
morphology can produce better word alignment, and
thus better phrasal translations, and can also signifi-
cantly reduce the out-of-vocabulary rate. We there-
fore applied two operations: (a) splitting of com-
pound words and (b) stemming.
After basic preprocessing, the German half of the
training corpus was first tagged by the German ver-
sion of TreeTagger (Schmid, 1994), to identify part-
of-speech tags. All nouns were then collected into
a noun list, which was used by a simple compound
splitter, as described in (Yang and Kirchhoff, 2006).
This splitter scans the compound word, hypothesiz-
ing segmentations, and selects the first segmentation
that produces two nouns that occur individually in
the corpus. After splitting the compound nouns in
the filtered corpus, we used the TreeTagger again,
only this time to lemmatize the (filtered) training
corpus.
The stemmed version of the German text was used
to train the translation system?s word alignments
(through the end of step 3 in the Moses training
script). After training the alignments, they were pro-
jected back onto the unstemmed corpus. The parallel
phrases were then extracted using the standard pro-
cedure. Stemming is only used during the training
stage, in order to simplify word alignment. During
the evaluation phase, only the compound-splitter is
applied to the German input.
6 Results
6.1 English ? Spanish
The unofficial results of our 2nd-pass system for the
2008 test set are shown in Table 2, for recased, unto-
kenized output. We note that the basic second-pass
model was better than the first-pass system on the
2008 task, but not on the 2007 task, whereas Boost-
edMERT provided a minor improvement in the 2007
task but not the 2008 task. This is contrary to previ-
ous results in the Arabic-English IWSLT 2007 task,
where boosted MERT gave an appreciable improve-
ment. This result is perhaps due to the difference in
magnitude between the IWSLT and WMT transla-
tion tasks.
Table 2: En?Es system on the test2007 and test2008
sets.
System test2007 test2008
First-Pass 30.95 31.83
Second-Pass 30.94 32.72
BoostedMERT 31.05 32.62
6.2 German ? Spanish
As previously described, we trained two German-
Spanish translation systems: one via the default
method provided in the Moses scripts, and an-
other using word stems to train the word align-
ments and then projecting these alignments onto
the unstemmed corpus and finishing the training
process in the standard manner. Table 3 demon-
strates that the word alignments generated with
word-stems markedly improved first-pass transla-
tion performance on the dev2006 dataset. How-
ever, during the evaluation period, the worse of the
two systems was accidentally used, resulting in a
larger number of out-of-vocabulary words in the
system output and hence a poorer score. Rerun-
ning our German-Spanish translation system cor-
rectly yielded significantly better system results,
also shown in Table 3.
125
Table 3: De?Es first-pass system on the development
and 2008 test set.
System dev2006 test2008
Baseline 23.9 21.2
Stemmed Alignments 26.3 24.4
6.3 Boosted MERT
BoostedMERT is still in an early stage of experi-
mentation, and we were interested to see whether it
improved over traditional MERT in re-ranking. As it
turns out, the BLEU scores on test2008 and test2007
data for the En-Es track are very similar for both re-
rankers. In our post-evaluation analysis, we attempt
to understand the reasons for similar BLEU scores,
since the weights wi for both re-rankers are quali-
tatively different. We found that out of 2000 En-Es
N-best lists, BoostedMERT and MERT differed on
1478 lists in terms of the final hypothesis that was
chosen. However, although the rankers are choosing
different hypotheses, the chosen strings appear very
similar. The PER of BoostedMERT vs. MERT re-
sults is only 0.077, and manual observation indicates
that the differences between the two are often single
phrase differences in a sentence.
We also computed the sentence-level BLEU for
each ranker with respect to the true reference. This
is meant to check whether BoostedMERT improved
over MERT in some sentences but not others: if the
improvements and degradations occur in the same
proportions, a similar corpus-level BLEU may be
observed. However, this is not the case. For a major-
ity of the 2000 sentences, the sentence-level BLEU
for both systems are the same. Only 10% of sen-
tences have absolute BLEU difference greater than
0.1, and the proportion of improvement/degradation
is similar (each 5%). For BLEU differences greater
than 0.2, the percentage drops to 4%.
Thus we conclude that although BoostedMERT
and MERT choose different hypotheses quite of-
ten, the string differences between their hypotheses
are negligible, leading to similar final BLEU scores.
BoostedMERT has found yet another local optimum
during training, but has not improved upon MERT
in this dataset. We hypothesize that dividing up the
original development set into halves may have hurt
BoostedMERT.
7 Conclusion
We have presented the University of Washing-
ton systems for English-to-Spanish and German-to-
Spanish for the 2008 WMT shared translation task.
A novel method for reranking N-best lists based on
boosted MERT training was tested, as was morpho-
logical simplification in the preprocessing compo-
nent for the German-to-Spanish system. Our con-
clusions are that boosted MERT, though successful
on other translation tasks, did not yield any improve-
ment here. Morphological simplification, however,
did result in significant improvements in translation
quality.
Acknowledgements
This work was funded by NSF grants IIS-0308297
and IIS-0326276.
References
Atserias, J. et al 2006. FreeLing 1.3: Syntactic
and semantic services in an open-source NLP library.
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC 2006).
Genoa, Italy.
Duh, K., and Kirchhoff, K. 2008. Beyond Log-Linear
Models: Boosted Minimum Error Rate Training for
MT Re-ranking. To appear, Proceedings of the Associ-
ation for Computational Linguistics (ACL). Columbus,
Ohio.
Koehn, P. and Och, F.J. and Marcu, D. 2003. Statistical
phrase-based translation. Proceedings of the Human
Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, (HLT/NAACL). Edmonton, Canada.
Koehn, P. 2005. Europarl: A Parallel Corpus for Statis-
tical Machine Translation Proceedings of MT Summit.
Koehn, P. et al 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. Annual Meeting of
the Association for Computational Linguistics (ACL),
demonstration session. Prague, Czech Republic.
Schmid, H. 1994. Probabilistic part-of-speech tagging
using decision trees. International Conference on New
Methods in Language Processing, Manchester, UK.
Stolcke, A. 2002. SRILM - An extensible language mod-
eling toolkit. Proceedings of ICSLP.
Yang, M. and K. Kirchhoff. 2006. Phrase-based backoff
models for machine translation of highly inflected lan-
guages. Proceedings of the 11th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics (EACL 2006). Trento, Italy.
126
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1227?1235,
Beijing, August 2010
Contextual Modeling for Meeting Translation Using Unsupervised Word
Sense Disambiguation
Yang Mei
Department of Electrical Engineering
University of Washington
yangmei@u.washington.edu
Katrin Kirchhoff
Department of Electrical Engineering
University of Washington
katrin@ee.washington.edu
Abstract
In this paper we investigate the challenges
of applying statistical machine translation
to meeting conversations, with a particu-
lar view towards analyzing the importance
of modeling contextual factors such as the
larger discourse context and topic/domain
information on translation performance.
We describe the collection of a small cor-
pus of parallel meeting data, the develop-
ment of a statistical machine translation
system in the absence of genre-matched
training data, and we present a quantita-
tive analysis of translation errors result-
ing from the lack of contextual modeling
inherent in standard statistical machine
translation systems. Finally, we demon-
strate how the largest source of translation
errors (lack of topic/domain knowledge)
can be addressed by applying document-
level, unsupervised word sense disam-
biguation, resulting in performance im-
provements over the baseline system.
1 Introduction
Although statistical machine translation (SMT)
has made great progress over the last decade,
most SMT research has focused on the transla-
tion of structured input data, such as newswire
text or parliamentary proceedings. Spoken lan-
guage translation has mostly concentrated on two-
person dialogues, such as travel expressions or
patient-provider interactions in the medical do-
main. Recently, more advanced spoken-language
data has been addressed, such as speeches (Stu?ker
et al, 2007), lectures (Waibel and Fu?gen, 2008),
and broadcast conversations (Zheng et al, 2008).
Problems for machine translation in these genres
include the nature of spontaneous speech input
(e.g. disfluencies, incomplete sentences, etc.) and
the lack of high-quality training data. Data that
match the desired type of spoken-language inter-
action in topic, domain, and, most importantly, in
style, can only be obtained by transcribing and
translating conversations, which is a costly and
time-consuming process. Finally, many spoken-
language interactions, especially those involving
more than two speakers, rely heavily on the par-
ticipants? shared contextual knowledge about the
domain and topic of the discourse, relationships
between speakers, objects in the real-world en-
vironment, past interactions, etc. These are typ-
ically not modelled in standard SMT systems.
The problem of speech disfluencies has been
addressed by disfluency removal techniques that
are applied prior to translation (Rao et al, 2007;
Wang et al, 2010). Training data sparsity has been
addressed by adding data from out-of-domain re-
sources (e.g. (Matusov et al, 2004; Hildebrandt
et al, 2005; Wu et al, 2008)), exploiting com-
parable rather than parallel corpora (Munteanu
and Marcu, 2005), or paraphrasing techniques
(Callison-Burch et al, 2006). The lack of con-
textual modeling, by contrast, has so far not been
investigated in depth, although it is a generally
recognized problem in machine translation. Early
attempts at modeling contextual information in
machine translation include (Mima et al, 1998),
where information about the role, rank and gen-
der of speakers and listeners was utilized in a
transfer-based spoken-language translation sys-
tem for travel dialogs. In (Kumar et al, 2008)
1227
statistically predicted dialog acts were used in a
phrase-based SMT system for three different di-
alog tasks and were shown to improve perfor-
mance. Recently, contextual source-language fea-
tures have been incorporated into translation mod-
els to predict translation phrases for traveling do-
main tasks (Stroppa et al, 2007; Haque et al,
2009). However, we are not aware of any work ad-
dressing contextual modeling for statistical trans-
lation of spoken meeting-style interactions, not
least due to the lack of a relevant corpus.
The first goal of this study is to provide a quan-
titative analysis of the impact of the lack of con-
textual modeling on translation performance. To
this end we have collected a small corpus of par-
allel multi-party meeting data. A baseline SMT
system was trained for this corpus from freely
available data resources, and contextual transla-
tion errors were manually analyzed with respect
to the type of knowledge sources required to re-
solve them. Our analysis shows that the largest
error category consists of word sense disambigua-
tion errors resulting from a lack of topic/domain
modeling. In the second part of this study we
therefore present a statistical way of incorporat-
ing such knowledge by using a graph-based unsu-
pervised word sense disambiguation algorithm at
a global (i.e. document) level. Our evaluation on
real-world meeting data shows that this technique
improves the translation performance slightly but
consistently with respect to position-independent
word error rate (PER).
2 Data
2.1 Parallel Conversational Data
For our investigations we used a subset of the AMI
corpus (McCowan, 2005), which is a collection of
multi-party meetings consisting of approximately
100 hours of multimodal data (audio and video
recordings, slide images, data captured from dig-
ital whiteboards, etc.) with a variety of existing
annotations (audio transcriptions, topic segmenta-
tions, summaries, etc.). Meetings were recorded
in English and fall into two broad types: sce-
nario meetings, where participants were asked to
act out roles in a pre-defined scenario, and non-
scenario meetings where participants were not re-
stricted by role assignments. In the first case, the
scenario was a project meeting about the devel-
opment of a new TV remote control; participant
roles were project manager, industrial designer,
marketing expert, etc. The non-scenario meet-
ings are about the move of an academic lab to
a new location on campus. The number of par-
ticipants is four. For our study we selected 10
meetings (5 scenario meetings and 5 non-scenario
meetings) and had their audio transcriptions trans-
lated into German (our chosen target language) by
two native speakers each. Translators were able
to simultaneously read the audio transcription of
the meeting, view the video, and listen to the au-
dio, when creating the translation. The transla-
tion guidelines were designed to obtain transla-
tions that match the source text as closely as pos-
sible in terms of style ? for example, translators
were asked to maintain the same level of collo-
quial as opposed to formal language, and to gen-
erally ensure that the translation was pragmati-
cally adequate. Obvious errors in the source text
(e.g. errors made by non-native English speak-
ers among the meeting participants) were not ren-
dered by equivalent errors in the German transla-
tion but were corrected prior to translation. The
final translations were reviewed for accuracy and
the data were filtered semi-automatically by elim-
inating incomplete sentences, false starts, fillers,
repetitions, etc. Although these would certainly
pose problems in a real-world application of spo-
ken language translation, the goal of this study
is not to analyze the impact of speech-specific
phenomena on translation performance (which, as
discussed in Section 1, has been addressed be-
fore) but to assess the impact of contextual infor-
mation such as discourse and knowledge of the
real-world surroundings. Finally, single-word ut-
terances such as yeah, oh, no, sure, etc. were
downsampled since they are trivial to translate and
were very frequent in the corpus; their inclusion
would therefore bias the development and tuning
of the MT system towards these short utterances
at the expense of longer, more informative utter-
ances.
Table 1 shows the word counts of the trans-
lated meetings after the preprocessing steps de-
scribed above. As an indicator of inter-translator
1228
ID type # utter. # word S-BLEU
ES2008a S 224 2327 21.5
IB4001 NS 419 3879 24.5
IB4002 NS 447 3246 30.5
IB4003 NS 476 5118 24.1
IB4004 NS 593 5696 26.9
IB4005 NS 381 4719 30.4
IS1008a S 191 2058 25.8
IS1008b S 353 3661 24.1
IS1008c S 308 3351 19.6
TS3005a S 245 2339 28.1
Table 1: Sizes and symmetric BLEU scores for
translated meetings from the AMI corpus (S = sce-
nario meeting, NS = non-scenario meeting).
agreement we computed the symmetric BLEU
(S-BLEU) scores on the reference translations
(i.e. using one translation as the reference and the
other as the hypothesis, then switching them and
averaging the results). As we can see, scores are
fairly low overall, indicating large variation in the
translations. This is due to (a) the nature of con-
versational speech, and (b) the linguistic proper-
ties of the target language. Conversational data
contain a fair amount of colloquialisms, referen-
tial expressions, etc. that can be translated in a va-
riety of ways. Additionally, German as the target
language permits many variations in word order
that convey slight differences in emphasis, which
is turn is dependent on the translators? interpreta-
tion of the source sentence. German also has rich
inflectional morphology that varies along with the
choice of words and word order (e.g. verbal mor-
phology depends on which subject is chosen).
2.2 SMT System Training Data
Since transcription and translation of multi-
party spoken conversations is extremely time-
consuming and costly, it is unlikely that parallel
conversational data will ever be produced on a suf-
ficiently large scale for a variety of different meet-
ing types, topics, and target languages. In order to
mimic this situation we trained an initial English-
German SMT system on freely available out-of-
domain data resources. We considered the follow-
ing parallel corpora: news text (de-news1, 1.5M
words), EU parliamentary proceedings (Europarl
(Koehn, 2005), 24M words) and EU legal docu-
ments (JRC Acquis2, 35M words), as well as two
generic English-German machine-readable dictio-
naries3,4 (672k and 140k entries, respectively).
3 Translation Systems
We trained a standard statistical phrase-based
English-German translation system from the re-
sources described above using Moses (Hoang and
Koehn, 2008). Individual language models were
trained for each data source and were then lin-
early interpolated with weights optimized on the
development set. Similarly, individual phrase ta-
bles were trained and were then combined into a
single table. Binary indicator features were added
for each phrase pair, indicating which data source
it was extracted from. Duplicated phrase pairs
were merged into a single entry by averaging their
scores (geometric mean) over all duplicated en-
tries. The weights for binary indicator features
were optimized along with all other standard fea-
tures on the development set. Our previous ex-
perience showed that this method worked better
than the two built-in features in Moses for han-
dling multiple translation tables. We found that
the JRC corpus obtained very small weights; it
was therefore omitted from further system de-
velopment. Table 2 reports results from six dif-
ferent systems: the first (System 1) is a system
that only uses the parallel corpora but not the
external dictionaries listed in Section 2.2. Sys-
tem 2 additionally uses the external dictionar-
ies. All systems use two meetings (IB4002 and
IS1008b) as a development set for tuning model
parameters and five meetings for testing (IB4003-
5,IS1008c,TS3005a). For comparison we also
trained a version of the system where a small in-
domain data set (meetings ES2008a, IB4001, and
IS1008a) was added to the training data (System
3). Finally, we also compared our performance
against Google Translate, which is a state-of-the-
art statistical MT system with unconstrained ac-
1www.iccs.inf.ed.ac.uk/?pkoehn/publications/de-news2http://wt.jrc.it/lt/Acquis/
3http://www.dict.cc
4http://www-user.tu-chemnitz.de/?fri/ding
1229
System description
Dev set Eval set
OOV (%) Trans. Scores OOV (%) Trans. Scores
EN DE BLEU PER EN DE BLEU PER
System 1 OOD parallel data only 4.1 17.0 23.8 49.0 6.5 20.5 21.1 49.5
System 2 System 1 + dictionaries 1.5 15.9 24.6 47.3 2.8 16.3 21.7 48.4
System 3 System 1 + ID parallel data 3.5 13.4 24.7 47.2 5.8 19.7 21.9 48.3
System 4 System 2 + ID parallel data 1.2 12.9 25.4 46.1 2.5 15.9 22.0 48.2
System 5 System 4 + web data 1.2 12.8 26.0 45.9 2.5 15.8 22.1 48.1
System 6 Google Translate ? ? 25.1 49.1 ? ? 23.7 50.8
Table 2: System performance using out-of-domain (OOD) parallel data only vs. combination with a
small amount of in-domain (ID) data and generic dictionaries. For each of the development (DEV)
and evaluation (Eval) set, the table displays the percentages of unknown word types (OOV) for English
(EN) and German (DE), as well as the translation scores of BLEU (%) and PER.
cess to the web as training data (System 6). As
expected, translation performance is fairly poor
compared to the performance generally obtained
on more structured genres. The use of exter-
nal dictionaries helps primarily in reducing PER
scores while BLEU scores are only improved no-
ticeably by adding in-domain data. System 6
shows a more even performance across dev and
eval sets than our trained system, which may re-
flect some degree of overtuning of our systems
to the relatively small development set (about 7K
words). However, the PER scores of System 6 are
significantly worse compared to our in-house sys-
tems.
In order to assess the impact of adding web data
specifically collected to match our meeting corpus
we queried a web portal5 that searches a range of
English-German bilingual web resources and re-
turns parallel text in response to queries in either
English or German. As queries we used English
phrases from our development and evaluation sets
that (a) did not already have phrasal translations
in our phrase tables, (b) had a minimum length
of four words, and (c) occurred at least twice in
the test data. In those cases where the search en-
gine returned results with an exact match on the
English side, we word-aligned the resulting paral-
lel text (about 600k words) by training the word
alignment together with the news text corpus. We
then extracted new phrase pairs (about 3k) from
the aligned data. The phrasal scores assigned to
5http://www.linguee.com
the new phrase pairs were set to 1; the lexical
scores were computed from a word lexicon trained
over both the baseline data resources and the par-
allel web data. However, results (Row 5 in Ta-
ble 2) show that performance hardly improved,
indicating the difficulty in finding matching data
sources for conversational speech.
Table 2 also shows the impact of different data
resources on the percentages of unknown word
types (OOV) for both the source and target lan-
guages. The use of external dictionaries gave the
largest reduction of OOV rates (System 1 vs. Sys-
tem 2 and System 3 vs. System 4), followed by the
use of in-domain data (System 1 vs. System 3 and
System 2 vs. System 4). Since they were retrieved
by multi-word query phrases, adding the web data
did not lead to significant reduction on the OOV
rates (System 4 vs. System 5).
Finally, we also explored a hierarchical phrase-
based system as an alternative baseline system.
The system was trained using the Joshua toolkit
(Li et al, 2009) with the same word alignments
and language models as were used in the standard
phrase-based baseline system (System 4). After
extracting the phrasal (rule) tables for each data
source, they were combined into a single phrasal
(rule) table using the same combination approach
as for the basic phrase-based system. However,
the translation results (BLEU/PER of 24.0/46.6
(dev) and 20.8/47.6 (eval), respectively) did not
show any improvement over the basic phrase-
based system.
1230
4 Analysis of Baseline Translations:
Effect of Contextual Information
The output from System 5 was analyzed manu-
ally in order to assess the importance of model-
ing contextual information. Our goal was not to
determine how translation of meeting style data
can be improved in general ? better translations
could certainly be generated by better syntactic
modeling, addressing morphological variation in
German, and generally improving phrasal cover-
age, in particular for sentences involving collo-
quial expressions. However, these are fairly gen-
eral problems of SMT that have been studied pre-
viously. Instead, our goal was to determine the
relative importance of modeling different contex-
tual factors, such as discourse-level information or
knowledge of the real-world environment, which
have not been studied extensively.
We considered three types of contextual in-
formation: discourse coherence information (in
particular anaphoric relations), knowledge of the
topic or domain, and real-world/multimodal infor-
mation. Anaphoric relations affect the translation
of referring expressions in cases where the source
and target languages make different grammatical
distinctions. For example, German makes more
morphological distinctions in noun phrases than
English. In order to correctly translate an expres-
sion like ?the red one? the grammatical features
of the target language expression for the referent
need to be known. This is only possible if a suf-
ficiently large context is taken into account dur-
ing translation and if the reference is resolved cor-
rectly. Knowledge of the topic or domain is rele-
vant for correctly translating content words and is
closely related to the problem of word sense dis-
ambiguation. In our current setup, topic/domain
knowledge could be particularly helpful because
in-domain training data is lacking and many word
translations are obtained from generic dictionar-
ies that do not assign probabilities to compet-
ing translations. Finally, knowledge of the real-
world environment, such as objects in the room,
other speakers present, etc. determines translation
choices. If a speaker utters the expression ?that
one? while pointing to an object, the correct trans-
lation might depend on the grammatical features
Error type % (dev) % (eval)
Word sense 64.5 68.2
Exophora (addressee) 24.3 23.4
Anaphora 10.2 7.8
Exophora (other) 1.0 0.6
Table 3: Relative frequency of different error
types involving contextual knowledge. The total
number of errors is 715, for 315 sentences.
of the linguistic expression for that object; e.g. in
German, the translation could be ?die da?, ?der
da? or ?das da?. Since the participants in our
meeting corpus use slides and supporting docu-
ments we expect to see some effect of such ex-
ophoric references to external objects.
In order to quantify the influence of contextual
information we manually analyzed the 1-best out-
put of System 5, identified those translation errors
that require knowledge of the topic/domain, larger
discourse, or external environment for their res-
olution, classified them into different categories,
and computed their relative frequencies. We then
corrected these errors in the translation output to
match at least one of the human references, in or-
der to assess the maximum possible improvement
in standard performance scores that could be ob-
tained from contextual modeling. The results are
shown in Tables 3 and 4. We observe that out of all
errors that can be related to the lack of contextual
knowledge, word sense confusions are by far the
most frequent. A smaller percentage of errors is
caused by anaphoric expressions. Contrary to our
expectations, we did not find a strong impact of
exophoric references; however, there is one cru-
cial exception where real-world knowledge does
play an important role. This is the correct transla-
tion of the addressee you. In English, this form is
used for the second person singular, second per-
son plural, and the generic interpretation (as in
?one?, or ?people?). German has three distinct
forms for these cases and, additionally, formal and
informal versions of the second-person pronouns.
The required formal/informal pronouns can only
be determined by prior knowledge of the rela-
tionships among the meeting participants. How-
ever, the singular-plural-generic distinction can
potentially be resolved by multimodal informa-
1231
Original Corrected
BLEU (%) PER BLEU (%) PER
dev 26.0 45.9 27.5 44.0
eval 22.1 48.1 23.3 46.0
Table 4: Scores obtained by correcting errors due
to lack of contextual knowledge.
tion such as gaze, head turns, body movements,
or hand gestures of the current speaker. Since
these errors affect mostly single words as opposed
to larger phrases, the impact of the corrections on
BLEU/PER scores is not large. However, for prac-
tical applications (e.g. information extraction or
human browsing of meeting translations) the cor-
rect translation of content words and referring ex-
pressions would be very important. In the remain-
der of the paper we therefore describe initial ex-
periments designed to address the most important
source of contextual errors, viz. word sense con-
fusions.
5 Resolving Word Sense Disambiguation
Errors
The problem of word sense disambiguation
(WSD) in MT has received a fair amount of
attention before. Initial experiments designed
at integrating a WSD component into an MT
system (Carpuat and Wu, 2005) did not meet
with success; however, WSD was subsequently
demonstrated to be successful in data-matched
conditions (Carpuat and Wu, 2007; Chan et al,
2007). The approach pursued by these latter ap-
proaches is to train a supervised word sense clas-
sifier on different phrase translation options pro-
vided by the phrase table of an initial baseline sys-
tem (i.e. the task is to separate different phrase
senses rather than word senses). The input fea-
tures to the classifier consist of word features ob-
tained from the immediate context of the phrase
in questions, i.e. from the same sentence or from
the two or three preceding sentences. The classi-
fier is usually trained only for those phrases that
are sufficiently frequent in the training data.
By contrast, our problem is quite different.
First, many of the translation errors caused by
choosing the wrong word sense relate to words
obtained from an external dictionary that do not
occur in the parallel training data; there is also lit-
tle in-domain training data available in general.
For these reasons, training a supervised WSD
module is not an option without collecting addi-
tional data. Second, the relevant information for
resolving a word sense distinction is often not lo-
cated in the immediately surrounding context but
it is either at a more distant location in the dis-
course, or it is part of the participants? background
knowledge. For example, in many meetings the
opening remarks refer to slides and an overhead
projector. It is likely that subsequent mention-
ing of slide later on during the conversation also
refer to overhead slides (rather than e.g. slide in
the sense of ?playground equipment?), though the
contextual features that could be used to identify
this word sense are not located in the immedi-
ately preceding sentences. Thus, in contrast to su-
pervised, local phrase sense disambiguation em-
ployed in previous work, we propose to utilize
unsupervised, global word sense disambiguation,
in order to obtain better modeling of the topic
and domain knowledge that is implicitly present
in meeting conversations.
5.1 Unsupervised Word Sense
Disambiguation
Unsupervised WSD algorithms have been pro-
posed previously (e.g. (Navigli and Lapata, 2007;
Cheng et al, 2009)). The general idea is to ex-
ploit measures of word similarity or relatedness
to jointly tag all words in a text with their correct
sense. We adopted the graph-based WSD method
proposed in (Sinha and Mihalcea, 2007), which
represents all word senses in a text as nodes in an
undirected graph G = (V,E). Pairs of nodes are
linked by edges weighted by scores indicating the
similarity or relatedness of the words associated
with the nodes. Given such a graph, the likeli-
hood of each node is derived by the PageRank al-
gorithm (Brin and Page, 1998), which measures
the relative importance of each node to the entire
graph by considering the amount of ?votes? the
node receives from its neighboring nodes. The
PageRank algorithm was originally designed for
directed graphs, but can be easily extended to an
undirected graph. Let PR(vi) denote the PageR-
ank score of vi. The PageRank algorithm itera-
1232
tively updates this score as follows:
PR(vi) = (1 ? d) + d
?
(vi,vj)?E
PR(vj)
wij?
k wkj
where wij is the similarity weight of the undi-
rected edge (vi, vj) and d is a damping factor,
which is typically set to 0.85 (Brin and Page,
1998). The outcome of the PageRank algorithm
is numerical weighting of each node in the graph.
The sense with the highest score for each word
identifies its most likely word sense. For our
purposes, we modified the procedure as follows.
Given a document (meeting transcription), we first
identify all content words in the source document.
The graph is then built over all target-language
translation candidates, i.e. each node represents a
word translation. Edges are then established be-
tween all pairs of nodes for which a word similar-
ity measure can be obtained.
5.2 Word Similarity Measures
We follow (Zesch et al, 2008a) in computing
the semantic similarity of German words by ex-
ploiting the Wikipedia and Wiktionary databases.
We use the publicly available toolkits JWPL and
JWKTL (Zesch et al, 2008b) to retrieve relevant
articles in Wikipedia and entries in Wiktionary for
each German word ? these include the first para-
graphs of Wikipedia articles entitled by the Ger-
man word, the content of Wiktionary entries of
the word itself as well as of closely related words
(hypernyms, hyponyms, synonyms, etc.). We then
concatenate all retrieved material for each word to
construct a pseudo-gloss. We then lowercase and
lemmatize the pseudo-glosses (using the lemma-
tizer available in the TextGrid package 6), exclude
function words by applying a simple stop-word
list, and compute a word similarity measure for
a given pair of words by counting the number of
common words in their glosses.
We need to point out that one drawback in this
approach is the low coverage of German content
words in the Wikipedia and Wiktionary databases.
Although the English edition contains millions
of entries, the German edition of Wikipedia and
Wiktionary is much smaller ? the coverage of all
content words in our task ranges between 53% and
6http://www.textgrid.de/en/beta.html
56%, depending on the meeting, which leads to
graphs with roughly 3K to 5K nodes and 8M to
13M edges. Words that are not covered mostly in-
clude rare words, technical terms, and compound
words.
5.3 Experiments and Results
For each meeting, the derived PageRank scores
were converted into a positive valued feature, re-
ferred to as the WSD feature, by normalization
and exponentiation:
fWSD(wg|we) = exp
{
PR(wg)?
wg?H(we) PR(wg)
}
where PR(wg) is the PageRank score for the Ger-
man word wg and H(we) is the set of all transla-
tion candidates for the English word we. Since
they are not modeled in the graph-based method,
multi-words phrases and words that are not found
in the Wikipedia or Wiktionary databases will re-
ceive the default value 1 for their WSD feature.
The WSD feature was then integrated into the
phrase table to perform translation. The new sys-
tem was optimized as before.
It should be emphasized that the standard mea-
sures of BLEU and PER give an inadequate im-
pression of translation quality, in particular be-
cause of the large variation among the reference
translations, as discussed in Section 4. In many
cases, better word sense disambiguation does not
result in better BLEU scores (since higher gram
matches are not affected) or even PER scores
because although a feasible translation has been
found it does not match any words in the refer-
ence translations. The best way of evaluating the
effect of WSD is to obtain human judgments ?
however, since translation hypotheses change with
every change to the system, our original error an-
notation described in Section 4 cannot be re-used,
and time and resource constraints prevented us
from using manual evaluations at every step dur-
ing system development.
In order to loosen the restrictions imposed by
having only two reference translations, we uti-
lized a German thesaurus7 to automatically ex-
tend the content words in the references with syn-
onyms. This can be seen as an automated way of
7http://www.openthesaurus.de
1233
No WSD With WSD
BLEU (%) PER XPER BLEU (%) PER XPER
dev 25.4 46.1 43.4 25.4 45.6 42.9
eval 22.0 48.2 44.6 22.0 47.9 44.0
IB4003 21.4 48.3 44.4 21.4 47.5 43.8
IB4004 22.4 48.5 44.4 23.1 48.4 43.9
IB4005 25.4 45.9 42.4 25.3 45.6 42.2
IS1008c 15.9 52.9 50.0 14.9 52.3 48.6
TS3005a 23.1 45.2 41.9 23.2 45.3 41.7
Table 5: Performance of systems with and without WSD for dev and eval sets as well as individual
meetings in the eval set.
approximating the larger space of feasible trans-
lations that could be obtained by producing addi-
tional human references. Note that the thesaurus
provided synonyms for only roughly 50% of all
content words in the dev and eval set. For each
of them, on average three synonyms are found in
the thesaurus. We use these extended references
to recompute the PER score as an indicator of
correct word selection. All results (BLEU, PER
and extended PER (or XPER)) are shown in Table
5. As expected, BLEU is not affected but WSD
improves the PER and XPER slightly but consis-
tently. Note that this is despite the fact that only
roughly half of all content words received disam-
biguation scores.
Finally, we provide a concrete example of
translation improvements, with improved words
highlighted:
Source:
on the balcony
there?s that terrace
there?s no place inside the building
Translation, no WSD:
auf dem balkon
es ist das absatz
es gibt keinen platz innerhalb des geba?udes
Translation, with WSD:
auf dem balkon
es ist das terrasse
es gibt keinen platz geba?udeintern
References:
auf dem balkon / auf dem balkon
da gibt es die terrasse / da ist die terrasse
es gibt keinen platz im geba?ude / es gibt keinen
platz innen im geba?ude
6 Summary and Conclusions
We have presented a study on statistical transla-
tion of meeting data that makes the following con-
tributions: to our knowledge it presents the first
quantitative analysis of contextual factors in the
statistical translation of multi-party spoken meet-
ings. This analysis showed that the largest im-
pact could be obtained in the area of word sense
disambiguation using topic and domain knowl-
edge, followed by multimodal information to re-
solve addressees of you. Contrary to our ex-
pectations, further knowledge of the real-world
environment (such as objects in the room) did
not show an effect on translation performance.
Second, it demonstrates the application of unsu-
pervised, global WSD to SMT, whereas previ-
ous work has focused on supervised, local WSD.
Third, it explores definitions derived from col-
laborative Wiki sources (rather than WordNet or
existing dictionaries) for use in machine transla-
tion. We demonstrated small but consistent im-
provements even though word coverage was in-
complete. Future work will be directed at improv-
ing word coverage for the WSD algorithm, in-
vestigating alternative word similarity measures,
and exploring the combination of global and local
WSD techniques.
Acknowledgments
This work was funded by the National Science Foundation
under Grant IIS-0840461 and by a grant from the Univer-
sity of Washington?s Provost Office. Any opinions, findings,
and conclusions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily reflect the
views of the funding organizations.
1234
References
S. Brin and L. Page. 1998. ?The Anatomy of a Large-
Scale Hypertextual Web Search Engine?. Proceed-
ings of WWW7.
C. Callison-Burch, P. Koehn and M. Osborne. 2006.
?Improved Statistical Machine Translation Using
Paraphrases?. Proceedings of NAACL.
M. Carpuat and D. Wu. 2005. ?Word sense disam-
biguation vs. statistical machine translation?. Pro-
ceedings of ACL.
M. Carpuat and D. Wu. 2007. ?Improving statistical
machine translation using word sense disambigua-
tion?. Proceedings of EMNLP-CoNLL.
Y.S. Chan and H.T. Ng and D. Chiang 2007. ?Word
sense disambiguation improves statistical machine
translation?. Proceedings of ACL.
P. Chen, W. Ding, C. Bowes and D. Brown. 2009.
?A fully unsupervised word sense disambiguation
method using dependency knowledge?. Proceed-
ings of NAACL.
E. Gabrilovich and S. Markovitch. 2007 ?Computing
semantic relatedness usingWikipedia-based explicit
semantic analysis?. Proceedings of IJCAI.
R. Haque, S.K. Naskar, Y. Ma and A.Way. 2009. ?Us-
ing supertags as source language context in SMT?.
Proceedings of EAMT.
A.S. Hildebrandt, M. Eck, S. Vogel and A. Waibel.
2005. ?Adaptation of the Translation Model for Sta-
tistical Machine Translation using Information Re-
trieval?. Proceedings of EAMT.
H. Hoang and P. Koehn. 2008. ?Design of the Moses
decoder for statistical machine translation?. Pro-
ceedings of SETQA-NLP.
P. Koehn. 2005. ?Europarl: a parallel corpus for statis-
tical machine translation?. Proceedings of MT Sum-
mit.
V. Kumar, R. Sridhar, S. Narayanan and S. Bangalore.
2008. ?Enriching spoken language translation with
dialog acts?. Proceedings of HLT.
Z. Li et al. 2009. ?Joshua: An Open Source Toolkit
for Parsing-based Machine Translation?. Proceed-
ings of StatMT.
E. Matusov, M. Popovic?, R. Zens and H. Ney. 2004.
?Statistical Machine Translation of Spontaneous
Speech with Scarce Resources?. Proceedings of
IWSLT.
A. McCowan. 2005. ?The AMI meeting corpus?,
H. Mima, O. Furuse and H. Iida. 1998. ?Improving
Performance of Transfer-Driven Machine Transla-
tion with Extra-Linguistic Information from Con-
text, Situation and Environment?. Proceedings of
Coling. Proceedings of the International Confer-
ence on Methods and Techniques in Behavioral Re-
search.
D.S. Munteanu and D. Marcu. 2005. ?Improving
machine translation performance by exploiting non-
parallel corpora?. Computational Linguistics.
R. Navigli and M. Lapata. 2007. ?Graph Connectiv-
ity Measures for Unsupervised Word Sense Disam-
biguation?, Proceedings of IJCAI
S. Rao and I. Lane and T. Schultz. 2007. ?Improving
spoken language translation by automatic disfluency
removal?. Proceedings of MT Summit. 31(4).
R. Sinha and R. Mihalcea. 2007. ?Unsupervised
Graph-based Word Sense Disambiguation Using
Measures of Word Semantic Similarity?, Proceed-
ings of IEEE-ICSC
N. Stroppa, A. Bosch and A. Way. 2007. ?Exploiting
Source Similarity for SMT using Context-Informed
Features?. Proceedings of TMI.
S. Stu?ker, C. Fu?gen, F. Kraft and M. Wo?lfel. 2007.
?The ISL 2007 English Speech Transcription Sys-
tem for European Parliament Speeches?. Proceed-
ings of Interspeech.
A. Waibel and C. Fu?gen. 2008. ?Spoken Language
Translation ? Enabling cross-lingual human-human
communication?. Proceedings of Coling).
W. Wang, G. Tur, J. Zheng and N.F. Ayan. 2010.
?Automatic disfluency removal for improving spo-
ken language translation?. Proceedings of ICASSP.
IEEE Signal Processing Magazine
H. Wu, H. Wang and C. Zong. 2008. ?Domain adapta-
tion for statistical machine translation with domain
dictionary and monolingual corpora?,
T. Zesch, C. Mu?ller and Iryna Gurevych. 2008.
?Extracting Lexical Semantic Knowledge from
Wikipedia and Wiktionary?. Proceedings of LREC.
T. Zesch, Christof Mu?ler and Iryna Gurevych. 2008.
?Using Wiktionary for Computing Semantic Relat-
edness?.
J. Zheng, W. Wang and N.F. Ayan. 2008. ?Devel-
opment of SRI?s translation systems for broadcast
news and broadcast conversations?. Proceedings of
Interspeech. Proceedings of AAAI.
1235
