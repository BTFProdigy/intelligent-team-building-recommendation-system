Integrated Feasibility Experiment for Bio-Security: IFE-Bio
A TIDES Demonstration
Lynette Hirschman, Kris Concepcion, Laurie Damianos, David Day, John Delmore, Lisa Ferro,
John Griffith, John Henderson, Jeff Kurtz, Inderjeet Mani, Scott Mardis, Tom McEntee, Keith
Miller, Beverly Nunan, Jay Ponte, Florence Reeder, Ben Wellner, George Wilson, Alex Yeh
The MITRE Corporation
Bedford, Massachusetts, USA and
McLean, Virginia, USA
781-271-7789
lynette@mitre.org
ABSTRACT
As part of MITRE?s work under the DARPA TIDES
(Translingual Information Detection, Extraction and
Summarization) program, we are preparing a series of
demonstrations to showcase the TIDES Integrated Feasibility
Experiment on Bio-Security (IFE-Bio).  The current
demonstration illustrates some of the resources that can be made
available to analysts tasked with monitoring infectious disease
outbreaks and other biological threats.
Keywords
Translation, information extraction, summarization, topic
detection and tracking, system integration.
1. INTRODUCTION
The long-term goal of TIDES is to provide delivery of
information on demand in real-time from live on-line sources. For
IFE-Bio, the resources made available to the analyst include e-
mail, news groups, digital library resources, and eventually (in
later versions), topic-specific segments from broadcast news.
Because of the emphasis on global monitoring, there is a need to
process incoming information in multiple languages.  The system
must deliver the appropriate information content in the
appropriate form and in the appropriate language (taken for now
to be English). This means that the IFE-Bio system will have to
deliver news stories, clusters of relevant documents, threaded
discussions, alerts on new events, tables, summaries (particularly
over document collections), answers to questions, graphs and geo-
spatial  temporal displays of information.
The demonstration system for the Human Language Technology
Conference in March 2001 represents an early stage of the full
IFE-Bio system, with an emphasis on end-to-end processing.
Future demonstrations will make use of MITRE?s Catalyst
architecture, providing an efficient, scalable architecture to
facilitate  integration of multiple stages of linguistic processing.
By June 2001, the IFE-Bio system will provide richer linguistic
processing through the integration of modules contributed by
other TIDES participants. By June 2002, the IFE-Bio system will
include additional functionality, such as real-time broadcast news
feeds, new machine translation components, support for question-
answering, cross-language information retrieval, multi-document
summarization, automatic extraction and normalization of
temporal and spatial information, and automated geospatial and
temporal displays.
2. The IFE-Bio System
The current demonstration (March 2001) highlights the basic
functionality required by an analyst, including:
? Capture of sources, including e-mail, digital library
material, news groups, and web-based resources;
? Categorizing of the sources into multiple orthogonal
hierarchies useful to the analyst, e.g., disease, region, news
source, language;
? Processing of the information through various stages,
including ?zoning? of the text to select the relevant portions
for processing; named entity detection, event detection,
extraction of temporal information, summarization, and
translation from Spanish, Portuguese, and Chinese into
English;
? Access to the information through use of any mail and news
group reader, which allows the analyst to organize, save, and
share the information in a familiar, readily accessible
environment;
? Display of the information in alternate forms, including
color-tagged documents, tables, summaries, graphs, and
geospatial, map-based displays.
Figure 1 below shows the overall functionality envisioned
for the IFE-Bio system, including capture, categorizing,
processing, access and display.
Collection capability for the current IFE-Bio system includes
email, news groups, journals, and Web resources. We have a
complete copy of the ProMED mailings (a moderated source
tracking global infectious disease outbreaks), and are routinely
collecting other information sources from the World Health
Organization and CDC.  In addition, we are collecting several
general global news feeds. Current volume is around 2000
messages per day; we estimate capacity for the current system at
around 4500 messages/day. Once we have integrated a filtering
capability, we expect the volume of messages saved in IFE-Bio
should drop significantly, since many of the global news services
report on a wide range of events and not all need to be passed on
to IFE-Bio analysts.  The categorizing of sources is done based on
the message header. The header is synthesized by extracting key
information about disease name, the country, and other relevant
information such as type of victim and source of information, as
well as date of message receipt.
The processing for the current demonstration system uses a
limited subset of the Catalyst architecture capabilities and a
number of in-house linguistic modules. The linguistic modules in
the current demonstration system include tokenization, sentence
segmentation, part-of-speech tagging, named entity detection,
temporal extraction (Mani and Wilson 2000) and source-specific
event detection.  In addition, we have incorporated the
CyberTrans embedded machine translation system which ?wraps?
available machine translation engines to make them available via
an e-mail or Web interface (Reeder 2000). Single document
summarization is performed by the MITRE WebSumm system
(Mani and Bloedorn 1999).
We carefully chose a light-weight interface mechanism for
delivery of the information to the analyst.  By treating the
incoming streams of data as feeds to a news server, the analyst can
inspect and organize the information using a familiar news and e-
mail browser. The analyst can subscribe to areas of interest, flag
important messages, watch specific threads, and create tailored
filters for monitoring outbreaks. The stories are crossed-posted to
multiple relevant news groups, based on the information in the
header, e.g., a story on Ebola in Africa would be cross posted to
the Africa regional newsgroup and to the Ebola disease
newsgroup. Search by subject and date allow the analyst to select
subsets of the messages for further processing, annotation or
sharing.  The news client provides notification of incoming
messages. In later versions, we plan to integrate topic detection
and tracking capabilities, to provide improved filtering and
routing of messages, as well as detection of new topics.  The use
of this simple delivery mechanism provides a familiar
environment with almost no learning curve, and it avoids issues of
platform and operating system dependence.
Finally, the system makes use of several different devices to
display the information appropriately. Figure 2 shows the layout
of the Netscape news browser interface.  It includes the list of
newsgroups that have been subscribed to (on the left), the list of
messages from the chosen newsgroup (on top), and a particular
message with color-coded named entities (including disease terms
displayed in red, so that they are easy to spot in the message).
What is the status of the
current Ebola outbreak?
The epidemic is contained;
as of 12/22/00, there were 
421 cases with 162 deaths
Interaction
CDC
WHO
Medical
literatureEmail:
ProMed
~ 2500
 stories/day
Internl
News
Sources  Capture
Translingual 
Information 
Detection 
Extraction 
Summarization 
U niden tif ied h emor rhagic  f
U niden tif ied h emor rhagic  f
Ebola hemorr hagic  fever  in
Re :  Ebo la hemorrhagi. ..
R e: Ebola hemo rrha gi...
ProMED
A nnotator
Ja ne Analyst
10 /17/00 1 9:37
10 /17/00 2 0:42
10 /18/00 7 :42
High
Norm al
Normal
read
rep lied
ProMED
10 /18/00 1 2:3 4 High un read
Ebola hemorr hagic  fever  in
Sour ce
D ate
Priority Status
10   99
0   105
1   57
0   10
2   34
0   50
1   1
0   25
5   200
0   45
0   0
0   0
0   0
0   0
0   6
0   32
0   3
0   1
High
Norm al
High
High
Ebola hemorrhagic feve r  -  Ugan da
U nf ilte red
O utbr eak
     C holer a
     D engue  Fe ve r
     Eb ola
I nfras tructure?
N atu ral  Di sas. ..
Spi lls
A cc id en ts
W M D Tra ckin. ..
Sus picious Il ln. ..
Sus picious De.. .
Pos sible  Biol o. ..
Pathogen threa ?
- --- --- ---------------------
W orkspa ce
      E bola
      D ra fts
      Re ports
D isease
R e: Ebola hemo rrha gi...
Location
U NK
U NK
Ebola
Ebola
Ebola
Ebola
Rabies
Rabies
U gan da
U gan da
U gan da
K eny a
U gan da
IHT
ProMED
WHO
Jo e Analyst
D ate
10 /14/00 2 3:06
10 /15/00 1 0:50
10 /16/00 2 1:45
10 /17/00 1 9:12
read
read
read
read
un read
Date: 10/16/00
Disease: Ebo la
Descripto r: hem orrh agic fever
Locatio n:          Ugan da
Disease Date:     10/14/00
Ho spital: mission ary hosp ital  in Gulu
New cases:  at least  7
Total  cases: 51
Total  dead:       31
Ebola hemorr hagic  fever  -
Ugand an M ini stry  ide ntif ies Eb ola  virus as t he c ause of  the outbreak.  KA MP ALA :
The  dreade d Eb ola  virus that struck over 300  peopl e i n Kikwit,  in  t he D emocratic
Rep ub lic  of Con go  in  1995, has ki lled 31  people in northe rn Ugan da.  A  U gandan
M ini s tr y of Heal th  sta tement  said l aboratory test s had r eveale d that  the Ebola vi rus
was  t he caus e of the  epidemi c hemorr hagic feve r whi ch has been r agi ng in the  G ulu
dis trict  since Septe mbe r.   Thr ee  of the dea d wer e s tud ent nur ses , who tre ated the first
Eb ola  patients admitt ed to a  Lac or  mis sionary hosp it al in  Gu lu  tow n.  A  task force
he ade d by G ul u dis trict adm ini str ator, Walte r O ch ora , has bee n se t up to co-or dina te
efforts to control the epi demi c.  F ie ld offic ials i n  Gul u tol d the Ka mpala-based Ne w
H ttp: //ti des2000.mi tre.org/
Pr oM ED /10162000/34n390h.ht ml
U gan da
News Repository
CATALYSTEntity Tagging
Event Extraction
Translation
Summarization
Alerting
Change detection
Threading
Cross-language IR
Topic clustering
Figure 1: Overview of the IFE-Bio Demonstration System
Local,
private
workspace
Documents
automatically
categorized
into shared,
tailorable
hierarchy
Sort by disease, location, source, date, etc.
Associated
meta-data:
header,
event,
summary,
named-entity
Figure 2: Screenshot of IFE-Bio Interface Using News Group Reader
Figure 3: Sample Summarization Automatically Generated by WebSumm
There are multiple display modalities available. The message in
Figure 2 contains a short tabular display in the beginning,
identifying disease, region and victim type. Below that is a URL
to a document summary, created by MITRE?s WebSumm system
(see Figure 3 for a sample summary).   If an incoming message is
in a language other than English, then CyberTrans is called to run
code set and language identification modules, and the language is
translated into English for further processing. Figure 4 below
shows a sample translated message; note that there are a number
of untranslated words, but it is still possible to get the gist of the
message.
In addition, we are working on a mechanism to provide
geographic and eventually, temporal display of outbreak
information. Figure 5 shows the stages of processing involved.
Stage 1 shows onamed entity and temporal tagging to identify the
items of interest. These are combined into disease events by
further linguistic processing; the result is shown in the table in
Stage 2. This spreadsheet of events serves as input for a map-
based display, shown in Stage 3. The graph plots number of new
cases and number of cumulative cases over time.  In the map, the
size of the outer dot represents total number of cases to date, and
the inner dot represents new cases.  This allows the analyst to
visualize spread of the disease, as well as the stage of the outbreak
(spreading or subsiding).
3. REFERENCES
[1] Mani, I. and Bloedorn, E. (1999). "Summarizing
Similarities and Among Related Documents".
Information Retrieval 1(1): 35-67.
[2] Mani, I. and Wilson, G. (2000). "Robust Temporal
Processing of News," Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics (ACL'2000), 69-76. New Brunswick, New
Jersey. Association for Computational Linguistics.
[3] Reeder, F.  (2000) "At Your Service:  Embedded MT
as a Service",  NAACL Workshop on Embedded MT,
March, 2000.
Figure 4: Translation from Portuguese to English Produced by CyberTrans
1. Annotate entities of interest via XML
Dise a se Source Country City_na m eDa te Ca se s Ne w _ca se s De a d
Ebola PROM ED Uganda G ula 26-O ct-2000 182 17 64
Ebola PROM ED Uganda G ula 5-Nov-2000 280 14 89
Ebola PROM ED Uganda G ulu 13-O ct-2000 42 9 30
Ebola PROM ED Uganda G ulu 15-O ct-2000 51 7 31
Ebola PROM ED Uganda G ulu 16-O ct-2000 63 12 33
Ebola PROM ED Uganda G ulu 17-O ct-2000 73 2 35
Ebola PROM ED Uganda G ulu 18-O ct-2000 94 21 39
Ebola PROM ED Uganda G ulu 19-O ct-2000 111 17 41
2. Assemble entities into events
0
50
100
150
200
250
300
350
400
10/
13/
200
0
10/
20/
200
0
10/
27/
200
0
11/
3/2
000
11/
10/
200
0
11/
17/
200
0
11/
24/
200
0
T IME
Nu
m
be
r C
as
es
Cases
New_cases
Dead
3. Display events...
   Total Cases   New Cases
Figure 5: Steps in Extraction to Support Temporal and Geospatial Displays of Disease Outbreak
At Your Service: Embedded MT As a Service 
Florence M. Reeder 
The MITRE Corporation 
1820 Dolley Madison Blvd. 
McLean, VA 22102 
Freeder@mitre.org 
Abstract 
A growing trend in Machine Translation 
(MT) is to view MT as an embedded part of 
an overall process instead of an end result 
itself. For the last four years, we have 
fielded (primarily) Commercial-Off-The- 
Shelf (COTS) MT systems in an operational 
process. MT has been used to facilitate 
cross-language information retrieval (IR), 
topic detection and other, wide-scoped 
scenarios. These uses caused a fundamental 
shift in our views about MT - everything 
from user interface to system evaluation to 
the basic system structures. This paper 
presents our lessons learned in developing 
an MT service for a wide range of user 
needs. 
Introduction .-, 
The foreign language material to be handled by 
the government is increasingly diverse and 
problematic. Foreign language processing needs 
are increasing because of the changing 
conditions of the world. Traditionally, users 
could focus on just a few foreign languages and 
a limited number of sources of foreign language 
materials. As we begin the 21 ~' century, users of 
online materials are faced with having to 
process, utilise and exploit documents that may 
be in one of many languages or a combination of 
languages. It is not feasible to expect a given 
user to know all of the languages related to their 
topic of research. It is equally unrealistic to 
expect o have on-demand translators available 
in every language whenever they are needed. 
Because of the expanding need, tools are being 
developed to automate the use of foreign 
language materials. 
Unlike previous views of tools, the current 
vision for machine translation (MT) is as a small 
part of a larger, mostly automated process. For 
many users, this does not mean yet another tool 
with yet another interface, but a nearly invisible 
companion that incorporates translation and 
necessary support technologies. One such 
system, the Army Research Lab (ARL) 
FALCON system, combines scanning, optical 
character ecognition (OCR), translation and 
filtering into a single process. Another view of 
this is the DARPA Translingual Information 
Detection, Extraction and Summarisation effort 
(TIDES). TIDES represents the pinnacle o f  
information access and is a real challenge for 
MT. MT supports the translingual spects of the 
effort and can be viewed as an embedded tool 
which facilitates other technologies. Finally, the 
integration of MT into the process for 
intelligence analysis serves as the basis for the 
CyberTrans project. For this paper, we will 
discuss the CyberTrans project, the lessons 
learned and the supporting technologies 
necessary for the successful integration of MT 
into other systems. 
1 Proposed Architecture 
1.1 Original Prototype 
The incarnation of CyberTrans grew as a 
demonstration that MT technology could be 
useful in the intelligence analysis process. As a 
result of an MT survey (Benoit et al 1991), MT 
technology was believed to be ready for 
incorporation into an operational environment. 
Initially, CyberTrans was designed as a wrapper 
around Commercial-Off-The-Shelf (COTS) and 
Government-Off-The-Shelf (GOTS) MT 
systems in Unix environments. A client-server 
architecture, implemented in a combination of 
Lisp and C, allowed for uniform user interfaces 
17 
to translation engines (Systran, Globalink and 
Gister). The server software interacted with the 
translation engines and the client software 
interacted with the users. The server interacted 
with client programs through Remote Procedure 
Call (RPC) passing of translation parameters 
(such as language, dictionary and output format) 
and file transfer of translation data. The clients 
provided were: e-mail, web, FrameMaker and 
command line. By providing translation through 
these media, users could translate documents in 
a familiar interface without having to worry 
is much more forgiving of low quality input data 
while automated processing suffers from poor 
input data. This forced the designers to 
implement a series of pre- and post-processing 
tools to be provided in the translation server. 
Initially, they were included in the functional 
architecture as depicted in Figure 1. This 
addition of language tools caused anecessary e- 
design of the architecture from a client-server 
model to an enterprise service model which is 
charactefised by an open architecture view of 
loosely coupled modules performing services for 
CHent 
~ ~ S  RPC Communication 
g ! 
. . . . .  . w V Identify ~~NorInaliseyHandleNot-~~VReassembleN~ 
Language / Spelling / Translated Format 
Code Set A ~  Format Words A ~  
T 
Figure 1: Original Architectural Flow 
about differences between translation products. 
The languages provided in the first prototype 
were those available to the government from 
Systran (French, German, Spanish, Itali.an, 
Portuguese, Russian to English); those 
purchased from Globalink (French, German, 
Spanish, Russian to/from English); and those 
available from the GOTS System Gister 
(language list is unavailable for publication). At 
the time of delivery in 1995/1996, this 
represented a relatively new method for 
delivenng MT technology to general users. 
Shortly after the fielding of the initial prototype, 
the need for additional anguage services to 
accompany translation became apparent. As 
will be discussed in Section 2, the data sent to 
the translation engines pointed out the 
differences between translation i  an interactive 
environment and translation in an embedded, 
automated environment. Interactive translation 
multiple applications. The newer design will be 
discussed in the next section. At this time, other 
system architectures were beginning to be 
introduced into the community such as those 
provided by ALIS Technologies; Systran and in 
FALCON. Because this is a specific lessons 
learned about the CyberTrans experience, it is 
beyond the scope of this paper to compare this 
architecture with other architectures. 
1.2 Updated Design 
Because of the addition of new tools and 
technologies into the CyberTrans model, it 
became necessary to re-engineer the server 
design. As part of the transition of a prototype 
system into a production-quality s stem, the 
reengineering also addressed issues such as 
system administration support, robust operation 
for 24/7 service and metrics. As can sometimes 
be the case, the prototype started being used in 
18 
S Translation 
g ~lr~nstl at\]?n ~ Broker N,,,,...~._~"? J TM ~ kNNDem?n 
Figure 2: Current Architectural Flow 
\ I Translation I 
/~  I I t  t 
i Language I rroc.  inr I 
continuous operation, causing a demand for 
improvement concurrent with ongoing 
operation. The reengineering was shaped by the 
fact that the system had expanded for new 
capabilities (in pre- and post-processing); the 
fact that the system had to remain operational l 
of the time; the fact that the system was being 
used in ways that were unanticipated by 
COTS/GOTS MT developers; the fact that the 
system was to be multi-platform (to include 
PCs) for an expanding list of languages and the 
fact that the system was beginning to be seen as 
providing a service similar to other system 
services (such as e-mail). These factors caused 
the system to be reengineered in an enterprise 
services model as an object-oriented design. 
In this architecture, demon processes broker 
translations - a request for translation is passed 
to the system by a client program; the translation 
is planned out as a series of translation-related 
services; each service is requested from the 
responsible system object and the resulting 
translation is then passed back to the client 
programs. Implemented in a combination of 
C++, Java and Lisp, the new version represents a 
service-oriented architecture. Figure 2 shows 
an updated architecture pictur e. Translation 
services include Systran (French, German, 
Italian, Spanish, Portuguese, Russian, Serbo- 
Croatian, Ukrainian, Chinese, Japanese, Korean 
into English); Globalink (French, German, 
Spanish, Russian to/from English) and Gister 
(language set list unavailable) with plans to 
incorporate engines for languages such as 
Arabic. Language processing services include 
language/code set identification; code set 
conversion; data normalisation, including 
diacritic reinsertion and generalised spell 
checking; format preservation for Hyper-Text 
Mark-up Language (HTML) documents; not- 
translated word preservation and others. The 
clients remain e-mail, Web and FrameMaker. 
Platforms include both Unix and PC platforms 
for clients and with the capability to incorporate 
PC-based tools as part of the service. Having 
described the architectures, we turn to lessons 
learned as a result of having an operational MT 
capability, running 24?7 for over 6000 
translations per month. 
2 Implementing Embedded MT 
The biggest surprise we encountered in fielding 
CyberTrans is related to the expectations of the 
users. The average user initially approaches MT. 
with an almost Star Trek-like view - that it is 
possible for the system to understand and 
translate perfectly any given document 
regardless of content, form, format or even 
language. While this is an unrealistic 
expectation of this or any system, an overriding 
goal which emerges is that embedded MT 
should be as automated as possible. This 
represents a fundamental shift from the 
traditional view of MT as an interactive, user- 
driven process to as a passive, data-driven 
process. We will now describe four areas where 
specific technologies need development for the 
smooth incorporation of MT into a "real-world" 
setting: language and code set identification; 
data normalisation; format preservation and 
lexicon development. Finally we will describe 
software engineering issues and challenges 
which facilitate the straight-forward embedding 
of MT into existing processes. 
2.1 Language / Code Set Identification 
Knowing the language and encoding~ or code 
set, of a document is a necessary first step in 
utilizing on-line text. For automated MT, the 
identification of the language(s) or code set of a 
text is necessary for systems to operate 
efffectively. A Spanish-English translation 
19 
system will not successfully process an Italian 
document and will be even less successful in 
processing a Chinese one. The first requiren:tent, 
then, which enables automated, embedded 
processing is the detection of the languagle(s) 
and code set(s) of a given document. 
In preparing the tools which permit he accurate 
detection of languages and code sets in an 
operational setting, we found characteristics of
the data which carry throughout all of the 
processing we discuss. The first, and foremost, 
is that the data is not clean, well-formed text. 
Frequently documents will have a mix of 
languages (both human and machine), code sets 
(including formatting information) and 
information pieces (such as e-mail headers, 
ASCII-art, etc.). For example, chat is very 
idiomatic and has many pre-defined acronyms. 
Finally, about 10% of translation materials are 
very short - between one and ten words. All of 
these factors contribute to the difficulty of 
preparing a service for language and code set 
identification as well as other natural anguage 
processing (NLP) tools. The implemented 
algorithm for language/code s t identification is
a trainable n-graph algorithm and has been 
discussed in more detail elsewhere (Reeder & 
Geisler, 1998). Currently our language and code 
set identification works for on the order of 30 
languages (mostly European) and about 1130 
code sets (including many ASCII 
transliterations) yet these numbers are still 
insufficient for the data routinely processed by 
CyberTrans. The step after language 
identification is data normalisation and will be 
discussed as the next result of lessons learned 
from CyberTrans. 
2.2 Data Normal isat ion 
Machine translation works best with clean, well- 
formed input text. Operationally, this is an 
ideal, but not reality. In reality, data that is 
being translated can suffer from many types of 
errors including misspellings and grammar 
mistakes, missing diacritics and transliterati:on 
problems, scanning errors and transmission 
obstacles. With her evaluation of MT systems, 
Flanagan (1994) describes reasons for errors in 
translation. MT systems were examined in light 
of the outputs of translation and the types of 
errors that can be generated by the translation 
engine. These include spelling errors, words not 
translated, incorrect accenting, incorrect 
capitalisation as well as grammatical and 
semantic errors. This study does not look at the 
kinds of inputs that can cause failure in a 
translation process. A second paper (Flanagan, 
1996) examines the quality of inputs to the 
translation process, arguing for pre-editing tools 
such as spelling checkers. Yet, this continues to 
be an interactive view of the translation process. 
Another study (Reeder & Loehr, 1998) show at 
least 40% of translation failures (not translated 
tokens) are attributable to the types of errors, or 
non-normalised data, presented here. In an 
embedded process, the system must 
automatically detect and correct errors. 
Language Source 
Segmentation 
Character omissions 
Mixed languages 
Input Source 
Misspellings 
Grammar mistakes 
Missing Diacritics 
Transliterations 
Capitalisation 
Production Source 
Scanning / OCR 
Electronic representation 
Conversion errors 
Acquisition Source 
Network transmission 
Table 1 - Categorisation of Error Types 
Instead of being random, the errors are regular, 
especially in generated or automated documents. 
For instance, a writer of French without a French 
keyboard will systematically omit diacritics. In 
this case, the errors in the document are far from 
random. Along these lines, we have grouped 
similar error sources together. Operational data 
can have one or more of these error types: 
misspellings and grammar mistakes; missing 
diacritics; mixed language documents; improper 
capitalisation; transliteration / transcription / 
code set mismatch; scanning (OCR) errors; web 
page or e-mail specific standards; conversion 
errors; network transmission errors; 
segmentation problems; character omissions. 
These error types can be categorised by the 
origination of the problem as in Table 1. Much 
20 
current CyberTrans work consists of developing 
and transitioning tools which can accurately 
detect and remediate errors, converting 
documents into a standard (normalised) form. 
The order in which the normalisation techniques 
are applied is a subject of ongoing research. 
2.3 Format  Preservat ion 
Documents arrive in many formats that have 
meaning in their structure. For instance, web 
pages contain HTML indicators plus language. 
A challenge for MT is that the HTML should 
not be translated whereas the text must be. The 
fde name rouge,gif should not be translated to 
red.gif if the web page is to be reassembled. 
Consider also, the task of translating a bulleted 
list. It is desirable to maintain a bulleted list 
with appropriate syntax. Table headings and 
labels also should be translated without 
destroying the format of the table. This, too, is a 
matter of ongoing research. 
2.4 Lexicon Update  
The highest portion of the cost of providing a 
machine translation capability reflects the 
amount of lexicography that must be done - as 
much as 70% of the cost of a machine 
translation engine. In addition, the government 
requires specialised lexical repositories which 
reflect unique domains such as military, legal, 
scientific and medical. We must fred ways to 
update lexicons intelligently, using such sources 
as dictionaries, working aids, specialised word 
lists and other information reservoirs to provide 
a broad coverage of words. One current 
approach is to record the list of words which do 
not translate and automate the handling of these. 
An issue in this is determining how to provide 
sufficient context for lexicographers. 
Additionally, different translation engines 
encode lexical entries in widely differing ways, 
meaning that sharing lexicon entries amongst 
translation capabilities is problematic. We are 
working on a lexicon service bureau (LSB) 
designed to facilitate the sharing of lexical 
materials. One part of this is the automatic 
extraction of lexical entries from on-line, 
machine readable dictionaries. Another part is 
the analysis of not-translated words. A final 
portion is research into a specialised category of 
lexical items - named entities. As with other 
processes in this section, we are addressing this 
as part of ongoing research - each advance 
raises the bar for the level of input text that can 
be handled. 
2.5 Software Engineering Challenges 
A final lessons learned from the CyberTrans 
experience relates to the software engineering 
challenges of putting together diverse 
technologies from many vendors for multiple 
purposes. The first of these is the problem of 
API's from COTS systems and GOTS systems. 
Behind our initial command line, fde-based 
interaction lies the fact that translation engines 
do not routinely provide APIs, presenting an 
integration challenge. Platform-specific tools 
also contribute to the integration problem. The 
second software engineering challenge 
stemming from this is the amount of time 
necessary to bring up a translation engine. A 
good translation engine has a lexicon in the tens 
of thousands of entries which takes time to load 
up. Currently, the loading of a translation 
engine takes as much time as all of the rest of 
the pre- and post-processing combined. A third 
challenge is deciding on and enacting a language 
representation. Although Unicode makes good 
strides towards the uniform sharing of data, 
many of the tools needed to convert real data 
into Unicode need to be improved. 
Additionally, current implementations of Java 
and C++ do not have all of the necessary pieces 
for seamlessly handling of a wide range of 
languages. Finally, the challenge is in the 
management and ordering of the translation 
process. To effectively manage a translation, 
requires a translation manager which can be a 
single point of failure. 
Conclusion 
We have identified the lessons learned from a 
specific embedding of MT into an overall 
process. We have identified issues and concerns 
resulting from this experience. We continue to 
refine and examine the issues of supporting MT, 
of making it palatable and viable in multiple 
applications and frameworks. This system is 
just one example of embedding MT. Future 
work must compare this effort to other work in 
the field. 
21 
References 
Benoit, J., Jordan, P., Dorr, B. (1991) Machine 
Translation Technology Survey and 
Evaluation Report. MITRE Techlfical 
Report. 
Flanagan, M. (1994) Error Classification for MT 
Evaluation. In Technology Partnerships for 
Crossing the Language Barrier: 
Proceedings of the First Conference of the 
Association for Machine Translation in the 
Americas, Columbia, MD. 
Flanagan, M. (1996) Two Years OnJine: 
Experiences, Challenges and Trends. In  
Expanding MT Horizons: Proceedings of 
the Second Conference of the Association 
for Machine Translation in the Americas, 
(pp. 192-197). Washington, DC: AMTA. 
Reeder, F. & Geisler, J. (1998) Multi-Byte Issues in 
Encoding / Language Identification. In 
Proceedings of the Embedded MT 
Workshop, AMTA-98. Langhome, PA. 
Reeder, F. & Loehr, D. (1998) Finding the Riight 
Words: An Analysis of Not-Translated 
Words in Machine Translation. In 
Proceedings of the 3rdconference of the 
Association for Machine Translation in the 
Americas, AMTA-98. Langhome, PA. 
22 
Interlingual Annotation of Multilingual Text Corpora 
 
Stephen Helmreich 
David Farwell 
Computing Research Laboratory 
New Mexico State University 
david@crl.nmsu.edu
shelmrei@crl.nmsu.edu
Florence Reeder 
Keith Miller 
Information Discovery & Understanding 
MITRE Corporation 
freeder@mitre.org
keith@mitre.org   
Bonnie Dorr 
Nizar Habash 
Institute for Advanced Computer Studies 
University of Maryland 
bonnie@umiacs.umd.edu
habash@umiacs.umd.edu
 
Eduard Hovy 
Information Sciences Institute 
University of Southern California 
hovy@isi.edu 
Lori Levin 
Teruko Mitamura 
Language Technologies Institute 
Carnegie Mellon University 
lsl@cs.cmu.edu
teruko@cs.cmu.edu 
Owen Rambow 
Advaith Siddharthan 
Department of Computer Science 
Columbia University 
rambow@cs.columbia.edu
as372@cs.columbia.edu  
 
 
Abstract 
This paper describes a multi-site project to 
annotate six sizable bilingual parallel corpora 
for interlingual content. After presenting the 
background and objectives of the effort, we 
describe the data set that is being annotated, 
the interlingua representation language used, 
an interface environment that supports the an-
notation task and the annotation process itself. 
We will then present a preliminary version of 
our evaluation methodology and conclude 
with a summary of the current status of the 
project along with a number of issues which 
have arisen.  
1 Introduction 
This paper describes a multi-site National Science 
Foundation project focusing on the annotation of six 
sizable bilingual parallel corpora for interlingual content 
with the goal of providing a significant data set for im-
proving knowledge-based approaches to machine trans-
lation (MT) and a range of other Natural Language 
Processing (NLP) applications. The project participants 
include the Computing Research Laboratory at NMSU, 
the Language Technologies Institute at CMU, the In-
formation Science Institute at USC, UMIACS at the 
University of Maryland, the MITRE Corporation and 
Columbia University. In the remainder of the paper, we 
first present the background and objectives of the pro-
ject. We then describe the data set that is being anno-
tated, the interlingual representation language being 
used, an interface environment that is designed to sup-
port the annotation task, and the process of annotation 
itself. We will then outline a preliminary version of our 
evaluation methodology and conclude with a summary 
of the current status of the project along with a set of 
issues that have arisen since the project began.  
2 Project Goals and Expected Outcomes 
The central goals of the project are: 
? to produce a practical, commonly-shared system 
for representing the information conveyed by a 
text, or ?interlingua?, 
? to develop a methodology for accurately and 
consistently assigning such representations to 
texts across languages and across annotators, 
? to annotate a sizable multilingual parallel corpus 
of source language texts and translations for IL 
content. 
This corpus is expected to serve as a basis for improving 
meaning-based approaches to MT and a range of other 
natural language technologies.  The tools and annotation 
standards will serve to facilitate more rapid annotation 
of texts in the future. 
3 Corpus 
The target data set is modeled on and an extension of 
the DARPA MT Evaluation data set (White and 
O?Connell 1994) and includes data from the Linguistic 
Data Consortium (LDC) Multiple Translation Arabic, 
Part 1 (Walker et al, 2003). The data set consists of 6 
bilingual parallel corpora. Each corpus is made up of 
125 source language news articles along with three 
translations into English, each produced independently 
by different human translators. However, the source 
news articles for each individual language corpus are 
different from the source articles in the other language 
corpora.  Thus, the 6 corpora themselves are comparable 
to each other rather than parallel. The source languages 
are Japanese, Korean, Hindi, Arabic, French and Span-
ish.  Typically, each article is between 300 and 400 
words long (or the equivalent) and thus each corpus has 
between 150,00 and 200,000 words. Consequently, the 
size of the entire data set is around 1,000,000 words. 
Thus, for any given corpus, the annotation effort is 
to assign interlingual content to a set of 4 parallel texts, 
3 of which are in the same language, English, and all of 
which theoretically communicate the same information. 
The following is an example set from the Spanish cor-
pus: 
S: Atribuy? esto en gran parte a
una pol?tica que durante muchos a?os
tuvo un "sesgo concentrador" y repre-
sent? desventajas para las clases me-
nos favorecidas.
T1: He attributed this in great
part to a type of politics that
throughout many years possessed a
"concentrated bias" and represented
disadvantages for the less favored
classes.
T2: To a large extent, he attrib-
uted that fact to a policy which had
for many years had a "bias toward
concentration" and represented disad-
vantages for the less favored
classes.
T3: He attributed this in great
part to a policy that had a "centrist
slant" for many years and represented
disadvantages for the less-favored
classes.
 
The annotation process involves identifying the 
variations between the translations and then assessing 
whether these differences are significant. In this case, 
the translations are, for the most part, the same although 
there are a few interesting variations.  
For instance, where this appears as the translation 
of esto in the first and third translations, that fact 
appears in the second. The translator choice potentially 
represents an elaboration of the semantic content of the 
source expression and the question arises as to whether 
the annotation of the variation in expressions should be 
different or the same.  
More striking perhaps is the variation between 
concentrated bias, bias toward concen-
tration and centrist slant as the translation 
for sesgo concentrador. Here, the third transla-
tion offers a clear interpretation of the source text au-
thor?s intent. The first two attempt to carry over the 
vagueness of the source expression assuming that the 
target text reader will be able to figure it out. But even 
here, the two translators appear to differ as to what the 
source language text author?s intent actually was, the 
former referring to bias of a certain degree of strength 
and the second to a bias  in a certain direction. Seem-
ingly, then, the annotation of each of these expressions 
should differ. 
Furthermore, each source language has different 
methods of encoding meaning linguistically. The resul-
tant differing types of translation mismatch with English 
should provide insight into the appropriate structure and 
content for an interlingual representation. 
The point is that a multilingual parallel data set of 
source language texts and English translations offers a 
unique perspective and unique problem for annotating 
texts for meaning. 
4 Interlingua 
Due to the complexity of an interlingual annotation as 
indicated by the differences described in the previous 
section, the representation has developed through three 
levels and incorporates knowledge from sources such as 
the Omega ontology and theta grids.  Since this is an 
evolving standard, the three levels will be presented in 
order as building on one another. Then the additional 
data components will be described.  
4.1 Three Levels of Representation 
We now describe three levels of representation, referred 
to as IL0, IL1 and IL2. The aim is to perform the annota-
tion process incrementally, with each level of represen-
tation incorporating additional semantic features and 
removing existing syntactic ones. IL2 is intended as the 
interlingua, that abstracts away from (most) syntactic 
idiosyncrasies of the source language. IL0 and IL1 are 
intermediate representations that are useful starting 
points for annotating at the next level. 
4.1.1 IL0 
IL0 is a deep syntactic dependency representation. It 
includes part-of-speech tags for words and a parse tree 
that makes explicit the syntactic predicate-argument 
structure of verbs. The parse tree is labeled with syntac-
tic categories such as Subject or Object , which refer to 
deep-syntactic grammatical function (normalized for 
voice alternations).  IL0 does not contain function words 
(determiners, auxiliaries, and the like): their contribu-
tion is represented as features.  Furthermore, semanti-
cally void punctuation has been removed.  While this 
representation is purely syntactic, many disambiguation 
decisions, relative clause and PP attachment for exam-
ple, have been made, and the presentation abstracts as 
much as possible from surface-syntactic phenomena.  
Thus, our IL0 is intermediate between the analytical and 
tectogrammatical levels of the Prague School (Haji? et 
al 2001). IL0 is constructed by hand-correcting the out-
put of a dependency parser (details in section 6) and is a 
useful starting point for semantic annotation at  IL1, 
since it allows annotators to see how textual units relate 
syntactically when making semantic judgments.  
4.1.2 IL1 
IL1 is an intermediate semantic representation. It asso-
ciates semantic concepts with lexical units like nouns, 
adjectives,  adverbs and verbs (details of the ontology in 
section 4.2). It also replaces the syntactic relations in 
IL0, like subject and object, with thematic roles, like 
agent, theme and goal (details in section 4.3). Thus, like 
PropBank (Kingsbury et al2002), IL1 neutralizes dif-
ferent alternations for argument realization.  However, 
IL1 is not an interlingua; it does not normalize over all 
linguistic realizations of the same semantics. In particu-
lar, it does not address how the meanings of individual 
lexical units combine to form the meaning of a phrase or 
clause. It also does not address idioms, metaphors and 
other non-literal uses of language.  Further, IL1 does not 
assign semantic features to prepositions; these continue 
to be encoded as syntactic heads of their phrases, al-
though these might have been annotated with thematic 
roles such as location or time. 
4.1.3 IL2 
IL2 is intended to be an interlingua, a representation of 
meaning that is reasonably independent of language. IL2 
is intended to capture similarities in meaning across 
languages and across different lexical/syntactic realiza-
tions within a language. For example, IL2 is expected to 
normalize over conversives (e.g. X bought a book from 
Y vs. Y sold a book to X)  (as does FrameNet (Baker et 
al 1998)) and non-literal language usage (e.g. X started 
its business vs. X opened its doors to customers).  The 
exact definition of IL2 will be the major research con-
tribution of this project. 
4.2 The Omega Ontology 
In progressing from IL0 to IL1, annotators have to se-
lect semantic terms (concepts) to represent the nouns, 
verbs, adjectives, and adverbs present in each sentence.  
These terms are represented in the 110,000-node ontol-
ogy Omega (Philpot et al, 2003), under construction at 
ISI.  Omega has been built semi-automatically from a 
variety of sources, including Princeton's WordNet (Fell-
baum, 1998), NMSU?s Mikrokosmos (Mahesh and Ni-
renburg, 1995), ISI's Upper Model (Bateman et al, 
1989) and ISI's SENSUS (Knight and Luk, 1994).  After 
the uppermost region of Omega was created by hand, 
these various resources? contents were incorporated and, 
to some extent, reconciled.  After that, several million 
instances of people, locations, and other facts were 
added (Fleischman et al, 2003).  The ontology, which 
has been used in several projects in recent years (Hovy 
et al, 2001), can be browsed using the DINO browser at 
http://blombos.isi.edu:8000/dino; this browser forms a 
part of the annotation environment.  Omega remains 
under continued development and extension.  
4.3 The Theta Grids 
Each verb in Omega is assigned one or more theta grids 
specifying the arguments associated with a verb and 
their theta roles (or thematic role).  Theta roles are ab-
stractions of deep semantic relations that generalize 
over verb classes.  They are by far the most common 
approach in the field to represent predicate-argument 
structure.  However, there are numerous variations with 
little agreement even on terminology (Fillmore, 1968; 
Stowell, 1981; Jackendoff, 1972; Levin and Rappaport-
Hovav, 1998). 
The theta grids used in our project were extracted 
from the Lexical Conceptual Structure Verb Database 
(LVD) (Dorr, 2001).  The WordNet senses assigned to 
each entry in the LVD were then used to link the theta 
grids to the verbs in the Omega ontology.  In addition to 
the theta roles, the theta grids specify the mapping be-
tween theta roles and their syntactic realization in argu-
ments, such as Subject, Object or Prepositional Phrase, 
and the Obligatory/Optional nature of the argument, 
thus facilitating IL1 annotation.  For example, one of the 
theta grids for the verb ?load? is listed in Table 1 (at the 
end of the paper). 
Although based on research in LCS-based MT 
(Dorr, 1993; Habash et al 2002), the set of theta roles 
used has been simplified for this project.  This list (see 
Table 2 at the end of the paper), was used in the Inter-
lingua Annotation Experiment 2002 (Habash and 
Dorr).1  
4.4 Incremental Annotation 
As described earlier, the development and annota-
tion of the interlingual notation is incremental in nature.  
This necessitates constraining the types and categories 
of attributes included in the annotation during the be-
ginning phases.  Other topics not addressed here, but 
considered for future work include time, aspect, loca-
tion, modality, type of reference, types of speech act, 
causality, etc.  
Thus, IL2 itself is not a final interlingual representa-
tion, but one step along the way. IL0 and IL1 are also 
intermediate representations, and as such are an occa-
sionally awkward mixture of syntactic and semantic 
information. The decisions as to what to annotate, what 
to normalize, what to represent as features at each level 
are semantically and syntactically principled, but also 
governed by expectations about reasonable annotator 
tasks. What is important is that at each stage of trans-
formation, no information is lost, and the original lan-
guage recoverable in principle from the representation. 
5 Annotation Tool 
We have assembled a suite of tools to be used in the 
annotation process.  Some of these tools are previously 
existing resources that were gathered for use in the pro-
ject, and others have been developed specifically with 
the annotation goals of this project in mind.  Since we 
are gathering our corpora from disparate sources, we 
need to standardize the text before presenting it to 
automated procedures.  For English, this involves sen-
tence boundary detection, but for other languages, it 
may involve segmentation, chunking of text, or other 
?text ecology? operations.  The text is then processed 
with a dependency parser, the output of which is viewed 
and corrected in TrED (Haji?, et al, 2001), a graphi-
cally-based tree editing program, written in Perl/Tk2.  
The revised deep dependency structure produced by this 
process is the IL0 representation for that sentence. 
In order to derive IL1 from the IL0 representation, 
annotators use Tiamat, a tool developed specifically for 
                                                           
1 Other contributors to this list are Dan Gildea and Karin 
Kipper Schuler. 
2 http://quest.ms.mff.cuni.cz/pdt/Tools/Tree_Editors/Tre
d/ 
this project.  This tool enables viewing of the IL0 tree 
with easy reference to all of the IL resources described 
in section 4 (the current IL representation, the ontology, 
and the theta grids).  This tool provides the ability to 
annotate text via simple point-and-click selections of 
words, concepts, and theta-roles.  The IL0 is displayed 
in the top left pane, ontological concepts and their asso-
ciated theta grids, if applicable, are located in the top 
right, and the sentence itself is located in the bottom 
right pane.  An annotator may select a lexical item (leaf 
node) to be annotated in the sentence view; this word is 
highlighted, and the relevant portion of the Omega on-
tology is displayed in the pane on the left.  In addition, 
if this word has dependents, they are automatically un-
derlined in red in the sentence view.  Annotators can 
view all information pertinent to the process of deciding 
on appropriate ontological concepts in this view.  Fol-
lowing the procedures described in section 6, selection 
of concepts, theta grids and roles appropriate to that 
lexical item can then be made in the appropriate panes. 
Evaluation of the annotators? output would be daunt-
ing based solely on a visual inspection of the annotated 
IL1 files.  Thus, a tool was also developed to compare 
the output and to generate the evaluation measures that 
are described in section 7.  The reports generated by the 
evaluation tool allow the researchers to look at both 
gross-level phenomena, such as inter-annotator agree-
ment, and at more detailed points of interest, such as 
lexical items on which agreement was particularly low, 
possibly indicating gaps or other inconsistencies in the 
ontology being used. 
6 Annotation Task 
To describe the annotation task, we first present the 
annotation process and tools used with it as well as the 
annotation manuals.  Finally, setup issues relating to 
negotiating multi-site annotations are discussed. 
6.1 Annotation process 
The annotation process was identical for each text. For 
the initial testing period, only English texts were anno-
tated, and the process described here is for English text. 
The process for non-English texts will be, mutatis mu-
tandis, the same. 
Each sentence of the text is parsed into a depend-
ency tree structure. For English texts, these trees were 
first provided by the Connexor parser at UMIACS 
(Tapanainen and Jarvinen, 1997), and then corrected by 
one of the team PIs. For the initial testing period, anno-
tators were not permitted to alter these structures. Al-
ready at this stage, some of the lexical items are 
replaced by features (e.g., tense), morphological forms 
are replaced by features on the citation form, and certain 
constructions are regularized (e.g., passive) and empty 
arguments inserted.  It is this dependency structure that 
is loaded into the annotation tool and which each anno-
tator then marks up. 
The annotator was instructed to annotate all nouns, 
verbs, adjectives, and adverbs. This involves annotating 
each word twice ? once with a concept from Wordnet 
SYNSET and once with a Mikrokosmos concept; these 
two units of information are merged, or at least inter-
twined in Omega. One of the goals and results of this 
annotation process will be a simultaneous coding of 
concepts in both ontologies, facilitating a closer union 
between them.  
In addition, users were instructed to provide a se-
mantic case role for each dependent of a verb. In many 
cases this was ?NONE? since adverbs and conjunctions 
were dependents of verbs in the dependency tree. LCS 
verbs were identified with Wordnet classes and the LCS 
case frames supplied where possible. The user, how-
ever, was often required to determine the set of roles or 
alter them to suit the text. In both cases, the revised or 
new set of case roles was noted and sent to a guru for 
evaluation and possible permanent inclusion. Thus the 
set of event concepts in the ontology supplied with roles 
will grow through the course of the project. 
6.2 The annotation manuals 
Markup instructions are contained in three manuals: a 
users guide for Tiamat (including procedural instruc-
tions), a definitional guide to semantic roles, and a 
manual for creating a dependency structure (IL0). To-
gether these manuals allow the annotator to (1) under-
stand the intention behind aspects of the dependency 
structure; (2) how to use Tiamat to mark up texts; and 
(3) how to determine appropriate semantic roles and 
ontological concepts. In choosing a set of appropriate 
ontological concepts, annotators were encouraged to 
look at the name of the concept and its definition, the 
name and definition of the parent node, example sen-
tences, lexical synonyms attached to the same node, and 
sub- and super-classes of the node. All these manuals 
are available on the IAMTC website3. 
6.3 The multi-site set up 
For the initial testing phase of the project, all annotators 
at all sites worked on the same texts. Two texts were 
provided by each site as were two translations of the 
same source language (non-English) text. To test for the 
effects of coding two texts that are semantically close, 
since they are both translations of the same source 
document, the order in which the texts were annotated 
differed from site to site, with half the sites marking one 
translation first, and the other half of the sites marking 
the second translation first. Another variant tested was 
                                                           
3 http://sparky.umiacs.umd.edu:8000/IAMTC/annotation
_manual.wiki?cmd=get&anchor=Annotation+Manual 
to interleave the two translations, so that two similar 
sentences were coded consecutively. 
During the later production phase, a more complex 
schedule will be followed, making sure that many texts 
are annotated by two annotators, often from different 
sites, and that regularly all annotators will mark the 
same text. This will help ensure continued inter-coder 
reliability. 
In the period leading up to the initial test phase, 
weekly conversations were held at each site by the an-
notators, going over the texts coded. This was followed 
by a weekly conference call among all the annotators. 
During the test phase, no discussion was permitted. 
One of the issues that arose in discussion was how 
certain constructions should be displayed and whether 
each word should have a separate node or whether cer-
tain words should be combined into a single node. In 
view of the fact that the goal was not to tag individual 
words, but entities and relations, in many cases words 
were combined into single nodes to facilitate this proc-
ess. For instance, verb-particle constructions were com-
bined into a single node. In a sentence like ?He threw it 
up?, ?throw? and ?up? were combined into a single 
node ?throw up? since one action is described by the 
combined words. Similarly, proper nouns, compound 
nouns and copular constructions required specialized 
handling.    In addition, issues arose about whether an-
notators should change dependency trees; and in in-
structing the annotators on how best to determine an 
appropriate ontology node.    
7 Evaluation 
The evaluation criteria and metrics continue to evolve 
and are in the early stages of formation and implementa-
tion.  Several possible courses for evaluating the annota-
tions and resulting structures exist.  In the first of these, 
the annotations are measured according to inter-
annotator agreement.  For this purpose, data is collected 
reflecting the annotations selected, the Omega nodes 
selected and the theta roles assigned.  Then, inter-coder 
agreement is measured by a straightforward match, with 
agreement calculated by a Kappa measure (Carletta, 
1993) and a Wood standard similarity (Habash and 
Dorr, 2002).  This is done for three agreement points:  
annotations, Omega selection and theta role selection.  
At this time, the Kappa statistic?s expected agreement is 
defined as 1/(N+1) where N is the number of choices at 
a given data point.  In the case of Omega nodes, this 
means the number of matched Omega nodes (by string 
match) plus one for the possibility of the annotator trav-
ersing up or down the hierarchy. Multiple measures are 
used because it is important to have a mechanism for 
evaluating inter-coder consistency in the use of the IL 
representation language which does not depend on the 
assumption that there is a single correct annotation of a 
given text.  The tools for evaluation have been modified 
from pervious use (Habash and Dorr, 2002). 
Second, the accuracy of the annotation is measured.  
Here accuracy is defined as correspondence to a prede-
fined baseline.  In the initial development phase, all 
sites annotated the same texts and many of the varia-
tions were discussed at that time, permitting the devel-
opment of a baseline annotation.  While not a useful 
long-term strategy, this produced a consensus baseline 
for the purpose of measuring the annotators? task and 
the solidity of the annotation standard.  
The final measurement technique derives from the 
ultimate goal of using the IL representation for MT, 
therefore, we are measuring the ability to generate accu-
rate surface texts from the IL representation as anno-
tated.  At this stage, we are using an available generator, 
Halogen (Knight and Langkilde, 2000).  A tool to con-
vert the representation to meet Halogen requirements is 
being built.  Following the conversion, surface forms 
will be generated and then compared with the originals 
through a variety of standard MT metrics (ISLE, 2003).   
8 Accomplishments and Issues 
In a short amount of time, we have identified languages 
and collected corpora with translations.  We have se-
lected representation elements, from parser outputs to 
ontologies, and have developed an understanding of 
how their component elements fit together.  A core 
markup vocabulary (e.g., entity-types, event-types and 
participant relations) was selected.  An initial version of 
the annotator?s toolkit (Tiamat) has been developed and 
has gone through alpha testing.  The multi-layered ap-
proach to annotation  decided upon reduces the burden 
on the annotators for any given text as annotations build 
upon one another.  In addition to developing individual 
tools, an infrastructure exists for carrying out a multi-
site annotation project.   
In the coming months we will be fleshing out the 
current procedures for evaluating the accuracy of an 
annotation and measuring inter-coder consistency.  
From this, a multi-site evaluation will be produced and  
results reported.  Regression testing, from the interme-
diate stages and representations will be able to be car-
ried out.  Finally, a growing corpus of annotated texts 
will become available.   
In addition to the issues discussed throughout the 
paper, a few others have not yet been identified.  From a 
content standpoint, looking at IL systems for time and 
location should utilize work in personal name, temporal 
and spatial annotation (e.g., Ferro et al, 2001).  Also, an 
ideal IL representation would also account for causality, 
co-reference, aspectual content, modality, speech acts, 
etc.  At the same time, while incorporating these items, 
vagueness and redundancy must be eliminated from the 
annotation language.  Many inter-event relations would 
need to be captured such as entity reference, time refer-
ence, place reference, causal relationships, associative 
relationships, etc.  Finally, to incorporate these, cross-
sentence phenomena remain a challenge.     
From an MT perspective, issues include evaluating 
the consistency in the use of an annotation language 
given that any source text can result in multiple, differ-
ent, legitimate translations (see Farwell and Helmreich, 
2003) for discussion of evaluation in this light.  Along 
these lines, there is the problem of annotating texts for 
translation without including in the annotations infer-
ences from the source text.   
9 Conclusions  
This is a radically different annotation project from 
those that have focused on morphology, syntax or even 
certain types of semantic content (e.g., for word sense 
disambiguation competitions). It is most similar to 
PropBank (Kingsbury et al2002) and FrameNet (Baker 
et al1998).  However, it is novel in its emphasis on:  (1) 
a more abstract level of mark-up (interpretation); (2) the 
assignment of a well-defined meaning representation to 
concrete texts; and (3) issues of a community-wide con-
sistent and accurate annotation of meaning. 
By providing an essential, and heretofore non-
existent, data set for training and evaluating natural lan-
guage processing systems, the resultant annotated multi-
lingual corpus of translations is expected to lead to 
significant research and development opportunities for 
Machine Translation and a host of other Natural Lan-
guage Processing technologies including Question-
Answering and Information Extraction.  
References 
Baker, C., J. Fillmore and J B. Lowe, 1998.  The Berke-
ley FrameNet Project.  Proceedings of ACL. 
Bateman, J.A., R. Kasper, J. Moore, and R. Whitney. 
1989. A General Organization of Knowledge for 
Natural Language Processing: The Penman Upper 
Model. Unpublished research report, USC / Informa-
tion Sciences Institute, Marina del Rey, CA.  
Carletta, J. C. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Lin-
guistics, 22(2), 249-254 
Conceptual Structures and Documentation, UMCP. 
http://www.umiacs.umd.edu/~bonnie/LCS_Database
_Documentation.html  
Dorr, B. J. 2001.  LCS Verb Database, Online Software 
Database of Lexical  
Dorr, B. J., 1993. Machine Translation: A View from the 
Lexicon, MIT Press, Cambridge, MA. 
Farwell, D., and S. Helmreich.  2003.  Pragmatics-based 
Translation and MT Evaluation.  In Proceedings of 
Towards Systematizing MT Evaluation.  MT-Summit 
Workshop, New Orleans, LA. 
Fellbaum, C. (ed.). 1998. WordNet: An On-line Lexical 
Database and Some of its Applications. MIT Press, 
Cambridge, MA. 
Ferro, L., I. Mani, B. Sundheim and G. Wilson.  2001. 
TIDES Temporal Annotation Guidelines. Version 
1.0.2 MITRE Technical Report, MTR 01W0000041 
Fillmore, C..  1968. The Case for Case. In E. Bach and 
R. Harms, editors, Universals in Linguistic Theory, 
pages 1--88. Holt, Rinehart, and Winston.  
Fleischman, M., A. Echihabi, and E.H. Hovy. 2003. 
Offline Strategies for Online Question Answering: 
Answering Questions Before They Are Asked.  Pro-
ceedings of the ACL Conference. Sapporo, Japan. 
Habash, N. and B. Dorr. 2002. Interlingua Annotation 
Experiment Results. AMTA-2002 Interlingua Reli-
ability Workshop. Tiburon, California, USA. 
Habash, N., B. J. Dorr, and D. Traum, 2002. "Efficient 
Language Independent Generation from Lexical 
Conceptual Structures," Machine Translation, 17:4. 
Haji?, J.; B. Vidov?-Hladk?; P. Pajas.  2001: The Pra-
gue Dependency Treebank: Annotation Structure and 
Support. In Proceeding of the IRCS Workshop on 
Linguistic Databases, pp. . University of Pennsyl-
vania, Philadelphia, USA, pp. 105-114. 
Hovy, E., A. Philpot, J. Ambite, Y. Arens, J. Klavans, 
W. Bourne, and D. Saroz.  2001. Data Acquisition 
and Integration in the DGRC's Energy Data Collec-
tion Project, in Proceedings of the NSF's dg.o 2001. 
Los Angeles, CA. 
ISLE 2003.  Framework for Evaluation of Machine 
Translation in ISLE.  
http://www.issco.unige.ch/projects/isle/femti/ 
Jackendoff, R. 1972. Grammatical Relations and Func-
tional Structure. Semantic Interpretation in Genera-
tive Grammar. The MIT Press, Cambridge, MA. 
Kingsbury, P and M Palmer and M Marcus , 2002.  
Adding Semantic Annotation to the Penn TreeBank. 
Proceedings of the Human Language Technology 
Conference (HLT 2002).  
Knight, K., and I. Langkilde. 2000.  Preserving Ambi-
guities in Generation via Automata Intersection. 
American Association for Artificial Intelligence con-
ference (AAAI). 
Knight, K, and S. K. Luk.  1994. Building a Large-Scale 
Knowledge Base for Machine Translation.  Proceed-
ings of AAAI. Seattle, WA. 
Levin, B. and M. Rappaport-Hovav. 1998. From Lexical 
Semantics to Argument Realization. Borer, H. (ed.) 
Handbook of Morphosyntax and Argument Structure. 
Dordrecht: Kluwer Academic Publishers. 
Mahesh, K., and Nirenberg, S.  1995. A Situated Ontol-
ogy for Practical NLP, in Proceedings on the Work-
shop on Basic Ontological Issues in Knowledge 
Sharing at IJCAI-95. Montreal, Canada. 
Philpot, A., M. Fleischman, E.H. Hovy. 2003. Semi-
Automatic Construction of a General Purpose Ontol-
ogy.  Proceedings of the International Lisp Confer-
ence.  New York, NY. Invited. 
Stowell, T. 1981. Origins of Phrase Structure. PhD the-
sis, MIT, Cambridge, MA.  
Tapanainen, P. and T Jarvinen.  1997.  A non-projective 
dependency parser.  In the 5th Conference on Applied 
Natural Language Processing / Association for Com-
putational Linguistics, Washington, DC. 
White, J., and T. O?Connell.  1994.  The ARPA MT 
evaluation methodologies: evolution, lessons, and fu-
ture approaches.  Proceedings of the 1994 Confer-
ence, Association for Machine Translation in the 
Americas 
Walker, K., M. Bamba, D. Miller, X. Ma, C. Cieri, and 
G. Doddington 2003.  Multiple-Translation Arabic 
Corpus, Part 1. Linguistic Data Consortium (LDC) 
catalog num. LDC2003T18 & ISBN 1-58563-276-7. 
 
 
Role Description Grid Syntax Type 
Agent The entity that does the action Agent:  load 
Theme  with possessed 
SUBJ OBLIGATORY 
Theme The entity that is worked on Agent:  load 
Theme with possessed 
OBJ OBLIGATORY 
Possessed The entity controlled or owned Agent:  load 
Theme  with possessed 
PP OPTIONAL 
Table 1 :  A theta grid for the verb "load" 
 
 
Role and Definition Examples 
Agent:  Agents have the features of volition, sentience, causation and 
independent exist 
? Henry pushed/broke the vase. 
Instrument: An instrument should have causation but no volition. Its 
sentience and existence are not relevant. 
? The Hammer broke the vase. 
? She hit him with a baseball bat 
Experiencer: An experiencer has no causation but is sentient and 
exists independently. Typically an experiencer is the subject of verbs 
like feel, hear, see, sense, smell, notice, detect, etc. 
? John heard the vase shatter.   
? John shivered. 
Theme: The theme is typically causally affected or experiences a 
movement and/or change in state. The theme can appear as the infor-
mation in verbs like acquire, learn, memorize, read, study, etc. It can 
also be a thing, event or state (clausal complement). 
? John went to school.  
? John broke the vase.   
? John memorized his lines.  
? She buttered the bread with marga-
rine.   
Perceived: Refers to a perceived entity that isn't required by the verb 
but further characterizes the situation. The perceived is neither caus-
ally affected nor causative. It doesn't experience a movement or 
change in state. Its volition and sentience are irrelevant. Its existence 
is independent of an experiencer. 
? He saw the play.   
? He looked into the room.  
? The cat's fur feels good to John.   
? She imagined the movie to be loud.    
Predicate: Indicates new modifying information about other thematic 
roles. 
? We considered him a fool.   
? She acted happy.   
Source: Indicates where/when the theme started in its motion, or 
what its original state was, or where its original (possibly abstract) 
location/time was. 
? John left the house. 
Goal: Indicates where the theme ends up in its motion, or what its 
final state is, or where/when its final (possibly abstract) location/time 
is. It also can indicate the thing/event resulting from the verb's occur-
rence (the result). 
? John ran home.   
? John ran to the store.  
? John gave a book to Mary.   
? John gave Mary a book. 
Location: Indicates static locations---as opposed to a source or goal, 
i.e., the (possibly abstract) location of the theme or event. 
? He lived in France.   
? The water fills the box.   
? This cabin sleeps five people 
Time Indicates time. ? John sleeps for five hours.   
? Mary ate during the meeting. 
Beneficiary: Indicates the thing that receives the benefit/result of the 
event/state. 
? John baked the cake for Mary.   
? John baked Mary a cake.  
? An accident happened to him.   
Purpose: Indicates the purpose/reason behind an event/state ? He studied for the exam.  
? He searched for rabbits.  
Possessed: Indicates the possessed entity in verbs such as own, have, 
possess, fit, buy, and carry. 
? John has five bucks.  
? He loaded the cart with hay.   
? He bought it for five dollars 
Proposition: Indicates the secondary event/state ? He wanted to study for the exam. 
Modifier: Indicates a property of a thing such as color, taste, size, 
etc. 
? The red book sitting on the table is 
old.  
Null Indicates no thematic contribution. Typical examples are imper-
sonal it and there. 
? It was raining all morning in Miami. 
 
TABLE 2:  List of Theta Roles 
Is That Your Final Answer? 
Florence Reeder 
George Mason Univ./MITRE Corp. 
1820 Dolley Madison Blvd. 
McLean VA 22102 
703-883-7156 
freeder@mitre.org 
 
 
 
 
 
 
ABSTRACT 
The purpose of this research is to test the efficacy of applying 
automated evaluation techniques, originally devised for the 
evaluation of human language learners, to  the output of machine 
translation  (MT) systems.  We believe that these evaluation 
techniques will provide information about both the human 
language learning process, the translation process and the 
development of machine translation systems.  This, the first 
experiment in a series of experiments, looks at the intelligibility of 
MT output.  A language learning experiment showed that 
assessors can differentiate native from non-native language essays 
in less than 100 words.  Even more illuminating was the factors 
on which the assessors made their decisions.  We tested this to see 
if similar criteria could be elicited from duplicating the 
experiment using machine translation output.  Subjects were given 
a set of up to six extracts of translated newswire text.  Some of the 
extracts were expert human translations, others were machine 
translation outputs.  The subjects were given three minutes per 
extract to determine whether they believed the sample output to be 
an expert human translation or a machine translation.  
Additionally, they were asked to mark the word at which they 
made this decision.  The results of this experiment, along with a 
preliminary analysis of the factors involved in the decision 
making process will be presented here. 
Keywords 
Machine translation, language learning, evaluation. 
1. INTRODUCTION 
Machine translation evaluation and language learner evaluation 
have been associated for many years, for example [5, 7].  One 
attractive aspect of language learner evaluation which 
recommends it to machine translation evaluation is the 
expectation that the produced language is not perfect, well-formed 
language.  Language learner evaluation systems are geared 
towards determining the specific kinds of errors that language 
learners make.  Additionally, language learner evaluation, more 
than many MT evaluations, seeks to build models of language 
acquisition which could parallel (but not correspond directly to) 
the development of MT systems.  These models frequently are 
feature-based and may provide informative metrics for diagnostic 
evaluation for system designers and users.   
 
In a recent experiment along these lines, Jones and Rusk [2] 
present a reasonable idea for measuring intelligibility, that of 
trying to score the English output of translation systems using a 
wide variety of metrics.  In essence, they are looking at the degree 
to which a given output is English and comparing this to human-
produced English.  Their goal was to find a scoring function for 
the quality of English that can enable the learning of a good 
translation grammar.  Their method for accomplishing this is 
through using existing natural language processing applications 
on the translated data and using these to come up with a numeric 
value indicating degree of ?Englishness?.  The measures they 
utilized included syntactic indicators such as word n-grams, 
number of edges in the parse (both Collins and Apple Pie parser 
were used), log probability of the parse, execution of the parse, 
overall score of the parse, etc.  Semantic criteria were based 
primarily on WordNet and incorporated the average minimum 
hyponym path length, path found ratio, percent of words with 
sense in WordNet.  Other semantic criteria utilized mutual 
information measures.   
 
Two problems can be found with their approach.  The first is that 
the data was drawn from dictionaries.  Usage examples in 
dictionaries, while they provide great information, are not 
necessarily representative of typical language use.  In fact, they 
tend to highlight unusual usage patterns or cases.  Second, and 
more relevant to our purposes,  is that they were looking at the 
glass as half-full instead of half-empty.  We believe that our 
results will show that measuring intelligibility is not nearly as 
useful as finding a lack of intelligibility.  This is not new in MT 
evaluation ? as numerous approaches have been suggested to 
identify translation errors, such as [1, 6].  In this instance, 
however, we are not counting errors to come up with a 
intelligibility score as much as finding out how quickly the 
intelligibility can be measured.  Additionally, we are looking to a 
field where the essence of scoring is looking at error cases, that of 
language learning. 
2. SIMPLE LANGUAGE LEARNING 
EXPERIMENT 
The basic part of scoring learner language (particularly second 
language acquisition and English as a second language) consists 
of identifying likely errors and understanding the cause of them.  
From these, diagnostic models of language learning can be built 
and used to effectively remediate learner errors, [3] provide an 
excellent example of this.  Furthermore, language learner testing 
 
 
 
seeks to measure the student's ability to produce language which 
is fluent (intelligible) and correct (adequate or informative).  
These are the same criteria typically used to measure MT system 
capability.1 
 
In looking at different second language acquisition (SLA) testing 
paradigms, one experiment stands out as a useful starting point for 
our purposes.  One experiment in particular serves as the model 
for this investigation.  In their test of language teachers, Meara 
and Babi [3] looked at assessors making a native speaker (L1) / 
language learner (L2) distinction in written essays.2  They showed 
the assessors essays one word at a time and counted the number of 
words it took to make the distinction.   
 
They found that assessors could accurately attribute L1 texts 
83.9% of the time and L2 texts 87.2% of the time for 180 texts 
and 18 assessors.  Additionally, they found that assessors could 
make the L1/L2 distinction in less than 100 words.  They also 
learned that it took longer to confirm that an essay was a native 
speaker?s than a language learner?s.  It took, on average, 53.9 
words to recognize an L1 text and only 36.7 words to accurately 
distinguish an L2 text.  While their purpose was to rate the 
language assessment process, the results are intriguing from an 
MT perspective.   
 
They attribute the fact that L2 took less words to identify to the 
fact that L1 writing ?can only be identified negatively by the 
absence of errors, or the absence of awkward writing.?  While 
they could not readily select features, lexical or syntactic, on 
which evaluators consistently made their evaluation, they 
hypothesize that there is a ?tolerance threshold? for low quality 
writing.  In essence, once the pain threshold had been reached 
through errors, missteps or inconsistencies, then the assessor 
could confidently make the assessment.  It is this finding that we 
use to disagree with Jones and Rusk [2] basic premise.  Instead of 
looking for what the MT system got right, it is more fruitful to 
analyze what the MT system failed to capture, from an 
intelligibility standpoint.  This kind of diagnostic is more difficult, 
as we will discuss later. 
 
We take this as the starting point for looking at assessing the 
intelligibility of MT output.  The question to be answered is does 
this apply to distinguishing between expert translation and MT 
output?  This paper reports on an experiment to answer this 
question.  We believe that human assessors key off of specific 
error types and that an analysis of the results of the experiment 
will enable us to do a program which automatically gets these. 
                                                                
1
 The discussion of whether or not MT output should be compared 
to human translation output is grist for other papers and other 
forums. 
2
 In their experiment, they were examining students learning 
Spanish as a second language. 
3. SHORT READING TEST 
We started with publicly available data which was developed 
during the 1994 DARPA Machine Translation Evaluations [8], 
focusing on the Spanish language evaluation first.  They may be 
obtained at:  http://ursula.georgetown.edu.3  We selected the first 
50 translations from each system and from the reference 
translation.  We extracted the first portion of each translation 
(from 98 to 140 words as determined by sentence boundaries).  In 
addition, we removed headlines, as we felt these served as 
distracters.  Participants were recruited through the author?s 
workplace, through the author?s neighborhood and a nearby 
daycare center.  Most were computer professionals and some were 
familiar with MT development or use.  Each subject was given a 
set of six extracts ? a mix of different machine and human 
translations.  The participants were told to read line by line until 
they were able to make a distinction between the possible authors 
of the text ? a human translator or a machine translator.  The first 
twenty-five test subjects were given no information about the 
expertise of the human translator.  The second twenty-five test 
subjects were told that the human translator was an expert.  They 
were given up to three minutes per text, although they frequently 
required much less time.  Finally, they were asked to circle the 
word at which they made their distinction.  Figure 1 shows a 
sample text. 
 
3001GP 
 
The general secretary of the UN, Butros 
Butros-Ghali, was pronounced on Wednesday in 
favor of a solution "more properly Haitian" 
resulting of a "commitment" negotiated 
between the parts, if the international 
sanctions against Haiti continue being 
ineffectual to restore the democracy in that 
country. 
 
While United States multiplied the last days 
the threats of an intervention to fight to 
compel to the golpistas to abandon the 
power, Butros Ghali estimated in a directed 
report on Wednesday to the general Assembly 
of the UN that a solution of the Haitian 
crisis only it will be able be obtained 
"with a commitment, based on constructive 
and consented grants" by the parts. 
 
 
HUMAN  
MACHINE  
 
Figure 1:  Sample Test Sheet 
4. RESULTS 
Our first question is does this kind of test apply to distinguishing 
between expert translation and MT output? The answer is yes.  
Subjects were able to distinguish MT output from human 
translations 88.4% of the time, overall.  This determination is 
                                                                
3
 Data has since been moved to a new location. 
more straightforward for readers than the native/non-native 
speaker distinction.  There was a degree of variation on a per-
system basis, as captured in Table 1.  Additionally, as presented in 
Table 2, the number of words to determine that a text was human 
was nearly twice the closest system.4 
Table 1:  Percentage correct for each system 
SYSTEM SCORE 
GLOBALINK 93.9% 
LINGSTAT 95.9% 
PANGLOSS 95.9% 
PAHO 69.4% 
SYSTRAN 87.8% 
HUMAN 89.8% 
 
Table 2:  Average Number of Words to Determine 
SYSTEM AVG. # WORDS 
PANGLOSS 17.6 
GLOBALINK 25.9 
SYSTRAN 31.7 
LINGSTAT 33.8 
PAHO 37.6 
HUMAN 62.2 
 
The second question is does this ability correlate with the 
intelligibility scores applied by human raters?  One way to look at 
the answer to this is to view the fact that the more intelligible a 
system output, the harder it is to distinguish from human output.  
So, systems which have lower scores for human judgment should 
have higher intelligibility scores.  Table 3 presents the scores with 
the fluency scores as judged by human assessors. 
Table 3:  Percentage Correct and Fluency Scores 
SYSTEM SCORE FLUENCY 
PANGLOSS 95.9 21.0 
LINGSTAT 95.9 30.4 
GLOBALINK 93.9 42.0 
SYSTRAN 87.8 45.4 
PAHO 69.4 56.7 
 
Indeed, the systems with the lowest fluency scores were most 
easily attributed.  The system with the best fluency score was also 
the one most confused.  Individual articles in the test sample will 
need to be evaluated statistically before a definite correlation can 
be determined, but the results are encouraging.  
 
                                                                
4
 For those texts where the participants failed to mark a specific 
spot, the length of the text was included in the average. 
The final question is are there characteristics of the MT output 
which enable the decision to be made quickly? The initial results 
lead us to believe that it is so.  Not translated words (non proper 
nouns) were generally immediate clues as to the fact that a system 
produced the results.  Other factors included:  incorrect pronoun 
translation; incorrect preposition translation; incorrect 
punctuation.  A more detailed breakdown of the selection criteria 
and the errors occurring before the selected word is currently in 
process. 
5. ANALYSIS 
An area for further analysis is that of the looking at  the details of 
the post-test interviews.  These have consistently shown that the 
deciders utilized error spotting, although the types and 
sensitivities of the errors differed from subject to subject. Some 
errors were serious enough to make the choice obvious where 
others had to occur more than once to push the decision above a 
threshold.  Extending this to a new language pair is also desirable 
as a language more divergent than Spanish from English might 
give different (and possibly even stronger) results.  Finally, we are 
working on constructing a program, using principles from 
Computer Assisted Language Learning (CALL) program design, 
which is aimed to duplicate the ability to assess human versus 
system texts. 
6. ACKNOWLEDGMENTS 
My thanks goes to all test subjects and Ken Samuel for review. 
7. REFERENCES 
[1] Flanagan, M.  1994.  Error Classification for MT 
Evaluation.  In Technology Partnerships for Crossing 
the Language Barrier:  Proceedings of the First 
Conference of the Association  for Machine 
Translation in the Americas, Columbia, MD.  
[2] Jones, D. & Rusk, G.  2000.  Toward a Scoring 
Function for Quality-Driven Machine Translation.  In 
Proceedings of COLING-2000.   
[3] Meara, P. & Babi, A.  1999.  Just a few words:  how 
assessors evaluate minimal texts.  Vocabulary 
Acquisition Research Group Virtual Library. 
www.swan.ac.uk/cals/vlibrary/ab99a.html 
[4] Michaud, L. & K. McCoy.  1999.  Modeling User 
Language Proficiency in a Writing Tutor for Deaf 
Learners of English. In M. Olsen, ed., Computer-
Mediated Language Assessment and Evaluation in 
Natural Language Processing,  Proceedings of a 
Symposium by ACL/IALL. University of Maryland, p. 
47-54  
[5] Somers, H. & Prieto-Alvarez, N.  2000.  Multiple 
Choice Reading Comprehension Tests for Comparative 
Evaluation of MT Systems.  In Proceedings of the 
Workshop on MT Evaluation at AMTA-2000.   
[6] Taylor, K. & J. White.  1998.  Predicting What MT is 
Good for:  User Judgments and Task Performance.  
Proceedings of AMTA-98, p. 364-373. 
[7] Tomita, M., Shirai, M., Tsutsumi, J., Matsumura, M. & 
Yoshikawa, Y.  1993.  Evaluation of MT Systems by 
TOEFL.  In Proceedings of the Theoretical and 
Methodological Implications of Machine Translation 
(TMI-93).   
[8] White, John, et al 1992-1994. ARPA Workshops on 
Machine Translation. Series of 4 workshops on 
comparative evaluation. PRC Inc. McLean, VA. 
[9] Wilks, Y.  (1994)  Keynote: Traditions in the 
Evaluation of MT.  In Vasconcellos, M. (ed.) MT 
Evaluation: Basis for Future Directions. Proceedings of 
a workshop sponsored by the National Science 
Foundation, San Diego, California. 
 
 
