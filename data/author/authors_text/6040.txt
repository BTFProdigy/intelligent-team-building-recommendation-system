1
2
Improved Cross-Language Retrieval
using Backoff Translation
Philip Resnik,1;2 Douglas Oard,2;3 and Gina Levow2
Department of Linguistics,1
Institute for Advanced Computer Studies,2
College of Information Studies,3
University of Maryland
College Park, MD 20742
fresnik,ginag@umiacs.umd.edu, oard@glue.umd.edu
ABSTRACT
The limited coverage of available translation lexicons can pose a se-
rious challenge in some cross-language information retrieval appli-
cations. We present two techniques for combining evidence from
dictionary-based and corpus-based translation lexicons, and show
that backoff translation outperforms a technique based on merging
lexicons.
1. INTRODUCTION
The effectiveness of a broad class of cross-language information
retrieval (CLIR) techniques that are based on term-by-term transla-
tion depends on the coverage and accuracy of the available trans-
lation lexicon(s). Two types of translation lexicons are commonly
used, one based on translation knowledge extracted from bilingual
dictionaries [1] and the other based on translation knowledge ex-
tracted from bilingual corpora [8]. Dictionaries provide reliable ev-
idence, but often lack translation preference information. Corpora,
by contrast, are often a better source for translations of slang or newly
coined terms, but the statistical analysis through which the trans-
lations are extracted sometimes produces erroneous results. In this
paper we explore the question of how best to combine evidencefrom
these two sources.
2. TRANSLATION LEXICONS
Our term-by-term translation technique (described below) requires
a translation lexicon (henceforth tralex) in which each word f is as-
sociated with a ranked set fe
1
; e
2
; : : : e
n
g of translations. We used
two translation lexicons in our experiments.
2.1 WebDict Tralex
We downloadeda freely available, manually constructedEnglish-
French term list from the Web1 and inverted it to French-English
1http://www.freedict.com
.
format. Since the WebDict translations appear in no particular or-
der, we ranked the e
i
based on target language unigram statistics
calculated over a large comparable corpus, the English portion of
the Cross-LanguageEvaluation Forum (CLEF) collection, smoothed
with statistics from the Brown corpus, a balanced corpus covering
many genres of English. All single-word translations are ordered by
decreasing unigram frequency, followed by all multi-word transla-
tions, and finally by any single-word entries not found in either cor-
pus. This ordering has the effect of minimizing the effect of infre-
quent words in non-standard usages or of misspellings that some-
times appear in bilingual term lists.
2.2 STRAND Tralex
Our second lexical resource is a translation lexicon obtained fully
automatically via analysisof parallel French-Englishdocuments from
the Web. A collection of 3,378 document pairs was obtained using
STRAND, our technique for mining the Web for bilingual text [7].
These document pairs were aligned internally, using their HTML
markup, to produce 63,094 aligned text ?chunks? ranging in length
from 2 to 30 words, 8 words on average per chunk, for a total of
500K words per side. Viterbi word-alignments for these paired
chunks were obtained using the GIZA implementation of the IBM
statistical translation models.2 An ordered set of translation pairs
was obtained by treating each alignment link between words as a
co-occurrence and scoring each word pair according to the likeli-
hood ratio [2]. We then rank the translation alternatives in order of
decreasing likelihood ratio score.
3. CLIR EXPERIMENTS
Ranked tralexes are particularly well suited to a simple ranked
term-by-term translation approach. In our experiments, we use top-
2 balanced document translation, in which we produce exactly two
English terms for each French term. For terms with no known trans-
lation, the untranslated French term is generated twice (often appro-
priate for proper names). For French terms with one translation, that
translation is generated twice. For French terms with two or more
translations, we generate the first two translations in the tralex. Thus
balanced translation has the effect of introducing a uniform weight-
ing over the top n translations for each term (here n = 2).
Benefits of the approachinclude simplicity and modularity ? no-
tice that a lexicon containing ranked translations is the only require-
ment, and in particular that there is no need for access to the in-
ternals of the IR system or to the document collection in order to
2http://www.clsp.jhu.edu/ws99/projects/mt/
perform computations on term frequencies or weights. In addition,
the approach is an effective one: in previous experiments we have
found that this balancedtranslation strategy significantly outperforms
the usual (unbalanced) technique of including all known translations [3].
We have also investigated the relationship between balanced trans-
lation and Pirkola?s structured query formulation method [6].
For our experiments we used the CLEF-2000 French document
collection (approximately 21 million words from articles in Le Monde).
Differences in use of diacritics, case, and punctuation can inhibit
matching between tralex entries and document terms, so we normal-
ize the tralex and the documents by converting characters to low-
ercase and removing all diacritic marks and punctuation. We then
translate the documents using the process described above, index
the translated documentswith the Inquery information retrieval sys-
tem, and perform retrieval using ?long? queries formulated by group-
ing all terms in the title, narrative, and description fields of each
English topic description using Inquery?s #sum operator. We report
mean average precision on the 34 topics for which relevant French
documentsexist, basedon the relevancejudgments provided by CLEF.
We evaluated several strategies for using the WebDict and STRAND
tralexes.
3.1 WebDict Tralex
Since a tralex may contain an eclectic mix of root forms and mor-
phological variants, we use a four-stage backoff strategy to maxi-
mize coverage while limiting spurious translations:
1. Match the surface form of a document term to surface forms
of French terms in the tralex.
2. Match the stem of a document term to surface forms of French
terms in the tralex.
3. Match the surface form of a document term to stems of French
terms in the tralex.
4. Match the stem of a document term to stems of French terms in
the tralex.
We used unsupervisedinduction of stemming rules basedon the French
collection to build the stemmer [5]. The process terminates as soon
as a match is found at any stage, and the known translations for that
match are generated. The process may produce an inappropriate
morphological variant for a correct English translation, so we used
Inquery?s English kstem stemmer at indexing time to minimize the
effect of that factor on retrieval effectiveness.
3.2 STRAND Tralex
One limitation of a statistically derived tralex is that any term has
some probability of aligning with any other term. Merely sorting
translation alternatives in order of decreasing likelihood ratio will
thus find some translation alternatives for every French term that ap-
peared at least once in the set of parallel Web pages. In order to limit
the introduction of spurious translations, we included only transla-
tion pairs with at least N co-occurrences in the set used to build the
tralex. We performed runs with N = 1; 2; 3, using the four-stage
backoff strategy described above.
3.3 WebDict Merging using STRAND
When two sources of evidence with different characteristics are
available, a combination-of-evidence strategy can sometimes out-
perform either source alone. Our initial experiments indicated that
the WebDict tralex was the better of the two (see below), so we adopted
a reranking strategy in which the WebDict tralex was refined ac-
cording a voting strategy to which both the original WebDict and
STRAND tralex rankings contributed.
Condition MAP
STRAND (N = 1) 0.2320
STRAND (N = 2) 0.2440
STRAND (N = 3) 0.2499
Merging 0.2892
WebDict 0.2919
Backoff 0.3282
Table 1: Mean Average Precision (MAP), averaged over 34 top-
ics
For each French term that appeared in both tralexes, we gave the
top-ranked translation in each tralex a score of 100, the next a score
of 99, and so on. We then summed the WebDict and STRAND scores
for each translation, reranked the WebDict translations based on that
sum, and then appendedany STRAND-only translations for that French
term. Thus, although both sourcesof evidence were weighted equally
in the voting, STRAND-only evidence received lower precedence
in the merged ranking. For French terms that appeared in only one
tralex, we included those entries unchangedin the merged tralex. In
this experiment run we used a threshold of N = 1, and applied the
four-stage backoff strategy described above to the merged resource.
3.4 WebDict Backoff to STRAND
A possibleweaknessof our merging strategy is that inflected forms
are more common in our STRAND tralex, while root forms are more
common in our WebDict tralex. STRAND tralex entries that were
copied unchangedinto the merged tralex thus often matched in step
1 of the four-stage backoff strategy, preventing WebDict contribu-
tions from being used. With the WebDict tralex outperforming the
STRAND tralex, this factor could hurt our results. As an alterna-
tive to merging, therefore, we also tried a simple backoff strategy in
which we used the original WebDict tralex with the four-stage back-
off strategy described above, to which we added a fifth stage in the
event that fewer than two WebDict tralex matches were found:
5. Match the surface form of a document term to surface forms
of French terms in the STRAND tralex.
We used a threshold of N = 2 for this experiment run.
4. RESULTS
Table 1 summarizes our results. Increasing thresholds seem to
be helpful with the STRAND tralex, although the differences were
not found to be statistically significant by a paired two-tailed t-test
with p < 0:05. Merging the tralexes provided no improvement
over using the WebDict tralex alone, but our backoff strategy pro-
duced a statistically significant 12% improvement in mean average
precision (at p < 0:01) over the next best tralex (WebDict alone).
As Figure 1 shows, the improvement is remarkably consistent, with
only four of the 34 topics adverselyaffected and only one topic show-
ing a substantial negative impact.
Breaking down the backoff results by stage (Table 2), we find
that the majority of query-to-document hits are obtained in the first
stage, i.e. matches of the term?s surface form in the document to a
translation of the surface form in the dictionary. However, the back-
off process improves by-token coverage of terms in documents by
8%, and gives a 3% relative improvement in retrieval results; it also
contributed additional translations to the top-2 set in approximately
30% of the cases, leading to the statistically significant 12% relative
improvement in mean averageprecision as compared to the baseline
using WebDict alone with 4-stage backoff.
Figure 1: WebDict-to-tralex backoff vs. WebDict alone, by
query
Stage (forms) Lexicon matches
1 (surface-surface) 70.38%
2 (stem-surface) 3.18%
3 (surface-stem) 0.46%
4 (stem-stem) 0.98%
5 (STRAND) 8.34%
No match found 16.66%
Table 2: Term matches in 5-stage backoff
5. CONCLUSIONS
There are many ways of combining evidence from multiple trans-
lation lexicons. We use tralexes similar to those usedby Nie et al [4],
but our work differs in our use of balanced translation and a back-
off translation strategy (which produces a stronger baseline for our
WebDict tralex), and in our comparisonof merging and backoff trans-
lation strategies for combining resources. In future work we plan to
explore other combinations of merging and backoff and other merg-
ing strategies, including post-retrieval merging of the ranked lists.
In addition, parallel corpora can be exploited for more than just
the extraction of a non-contextualized translation lexicon. We are
currently engagedin work on lexical selection methods that take ad-
vantage of contextual information, in the context of our research on
machine translation, and we expect that CLIR results will be im-
proved by contextually-informed scoring of term translations.
6. ACKNOWLEDGMENTS
This research was supported in part by Department of Defense
contract MDA90496C1250 and TIDES DARPA/ITO Cooperative
Agreement N660010028910,
7. REFERENCES
[1] L. Ballesteros and W. B. Croft. Resolving ambiguity for
cross-language retrieval. In W. B. Croft, A. Moffat, and C. V.
Rijsbergen, editors, Proceedings of the 21st Annual
International ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 64?71. ACM
Press, Aug. 1998.
[2] T. Dunning. Accurate methods for the statistics of surprise and
coincidence. Computational Linguistics, 19(1):61?74, March
1993.
[3] G.-A. Levow and D. W. Oard. Translingual topic tracking
with PRISE. In Working Notes of the Third Topic Detection
and Tracking Workshop, Feb. 2000.
[4] J.-Y. Nie, M. Simard, P. Isabelle, and R. Durand.
Cross-language information retrieval based on parallel texts
and automatic mining of parallel texts from the web. In
M. Hearst, F. Gey, and R. Tong, editors, Proceedings of the
22nd Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval, pages
74?81, Aug. 1999.
[5] D. W. Oard, G.-A. Levow, and C. I. Cabezas. CLEF
experiments at Maryland: Statistical stemming and backoff
translation. In C. Peters, editor, Proceedings of the First
Cross-Language Evaluation Forum. 2001. To appear.
http://www.glue.umd.edu/oard/research.html.
[6] D. W. Oard and J. Wang. NTCIR-2 ECIR experiments at
Maryland: Comparing structured queries and balanced
translation. In Second National Institute of Informatics (NII)
Test Collection Information Retrieval (NTCIR) workshop.
forthcoming.
[7] P. Resnik. Mining the Web for bilingual text. In 37th Annual
Meeting of the Association for Computational Linguistics
(ACL?99), College Park, Maryland, June 1999.
[8] P. Sheridan and J. P. Ballerini. Experiments in multilingual
information retrieval using the SPIDER system. In
Proceedings of the 19th Annual International ACM SIGIR
Conference on Research and Development in Information
Retrieval, Aug. 1996.
	

		
	
Rapidly Retargetable Interactive Translingual Retrieval
Gina-Anne Levow
Institute for Advanced
Computer Studies
University of Maryland,
College Park, MD 20742
gina@umiacs.umd.edu
Douglas W. Oard
College of Information Studies
Institute for Advanced
Computer Studies
University of Maryland,
College Park, MD 20742
oard@glue.umd.edu
Philip Resnik
Department of Linguistics
Institute for Advanced
Computer Studies
University of Maryland,
College Park, MD 20742
resnik@umiacs.umd.edu
ABSTRACT
This paper describes a system for rapidly retargetable interactive
translingual retrieval. Basic functionality can be achieved for a new
document language in a single day, and further improvements re-
quire only a relatively modest additional investment. We applied
the techniquesfirst to searchChinese collections using English queries,
and have successfully added French, German, and Italian document
collections. We achievethis capability through separation of language-
dependent and language-independent components and through the
application of asymmetric techniques that leverage an extensiveEn-
glish retrieval infrastructure.
Keywords
Cross-language information retrieval
1. INTRODUCTION
Our goal is to producesystems that allow interactive users to present
English queries and retrieve documents in languages that they can-
not read. In this paper we focus on what we call ?rapid retargetabil-
ity?: extending interactive translingual retrieval functionality for a
new document languagerapidly with few language-specificresources.
Our current system can be retargeted to a new language in one day
with only one language-dependent resource: a bilingual term list.1
Our language-independent architecture consists of two main com-
ponents:
1. Document translation and indexing
2. Interactive retrieval
We describe each of these components, demonstrate their effective-
ness for information retrieval tasks, and then concludeby describing
our experience with adding French, German and Italian document
collections to a system that was originally developed for Chinese.
1For Asian languages we also use a language-specificsegmentation
system.
.
2. DOCUMENT TRANSLATION AND IN-
DEXING
We have adopted a document translation architecture for two rea-
sons. First, we support a single query language (English) but multi-
ple document languages, so indexing English terms simplifies query
processing (where interactive response time can be a concern). Sec-
ond, a document translation architecture simplifies the display of
translated documents by decoupling the translation and display pro-
cesses. Gigabyte collections require machine translation that is or-
ders of magnitude faster than present commercial systems. We ac-
complish this using term-by-term translation, in which the basic data
structure is a simple hash table lookup. Any translation requires
some source of translation knowledge?we use a bilingual term list
containing English translation(s) for eachforeign language term. We
typically construct these term lists by harvesting Internet-available
translation resources, so the foreign language terms for which trans-
lations are known are typically an eclectic mix of root and inflected
forms. We accommodate this limitation using a four-stage backoff
statistical stemming approach to enhance translation coverage.
2.1 Preprocessing.
Differences in use of diacritic-s, case, and punctuation can inhibit
matching between term list entries and document terms, so normal-
ization is important. In order to maximize the probability of match-
ing document words with term list entries, we normalize the bilin-
gual term list and the documents by:
 converting characters in Western languages to lowercase,
 removing all accents and diacritics, and
 segmentation, which for Western languages merely involves
separating punctuation from other text by the addition of white
space.
Our preprocessingalso includes conversion of the bilingual term list
and the document collection into standard formats. The preprocess-
ing typically requires about half a day of programmer time.
2.2 Four-Stage Backoff Translation.
Bilingual term lists found on the Web often contain an eclectic
mix of root forms and morphological variants. We thus developed
a four-stage backoff strategy to maximize coverage while limiting
spurious translations:
1. Match the surface form of a document term to surface forms
of source language terms in the bilingual term list.
2. Match the stem of a document term to surfaceforms of source
language terms in the bilingual term list.
3. Match the surface form of a document term to stems of source
language terms in the bilingual term list.
4. Match the stem of a document term to stems of source lan-
guage terms in the bilingual term list.
The process terminates as soon as a match is found at any stage, and
the known translations for that match are generated. Although this
may produce an inappropriate morphological variant for a correct
English translation, use of English stemming at indexing time mini-
mizes the effect of that factor on retrieval effectiveness. Becausewe
are ultimately interested in processing documents in any language,
we may not have a hand-crafted stemmer available for the document
language. We have thus explored the application of rule induction
to learn stemming rules in an unsupervised fashion from the collec-
tion that is being indexed [2].
2.3 Balanced Top-2 Translation.
We produce exactly two English terms for each foreign-language
term. For terms with no known translation, the untranslated term is
generated twice (often appropriate for proper names in the Latin-
1 character set). For terms with one translation, that translation is
generated twice. For terms with two or more known translations,
we generate the ?best? two translations. In prior experiments we
have found that this balanced translation strategy significantly out-
performs the usual (unbalanced) technique of including all known
translations [1]. We establish the ?best? translations by sorting the
bilingual term list in advanceusing only English resources. All single-
word translations are ordered by decreasing unigram frequency in
the Brown corpus, followed by all multi-word translations, and fi-
nally by any single word entries not found in the Brown corpus.
This ordering has the effect of minimizing the effect of infrequent
words in non-standard usages or of misspellings that sometimes ap-
pear in bilingual term lists. This translation strategy allows balanc-
ing of translations in a modular fashion, even when one does not
have access to the internal parameters of the information retrieval
system. We translate  100 MB per hour using Perl on a SPARC
Ultra 5.
2.4 Post-translation Document Expansion.
We implement post-translation document expansion for the for-
eign language stories after translation into English in order to en-
rich the indexing vocabulary beyond that which was available af-
ter term-by-term translation. This is analogous to the process that
Singhal et al applied to monolingual speech retrieval [4].
Term-by-term translation producesa set of English terms that serve
as a noisy representation of the original source language document.
These terms are then treated as a query to a comparable English col-
lection, typically contemporaneous newswire text, from which we
retrieve the five highest ranked documents. From those five docu-
ments, we extract the most selective terms and use them to enrich
the original translations of the documents. For this expansion pro-
cess we select one instance of every term with an IDF value above
an ad hoc threshold that was tuned to yield approximately 50 new
terms. This optional step is the slowest processing stage, with a
throughput of about 20 MB per hour.
2.5 Indexing
The resulting collection is then indexed using Inquery (version
3.1p1), with the kstem stemmer and default English stopword list.
Indexing is the fastest stage in the process, with throughput exceed-
ing one gigabyte per hour.
3. INTERACTIVE RETRIEVAL
Interactive searches are performed using a Web interface. Sum-
mary information for the top-ranked documents is displayedin groups
of ten per page. Document summaries consist of the date and a gloss
translation of the document title. Users can inspect a gloss transla-
tion of the full text of any document if the title is not sufficiently
informative. For both title and full text, the gloss translations are
generated in advance using the same process as translation for in-
dexing, with the following differences in detail:
 Terms added as a result of document expansion are not dis-
played.
 The number of retained translations is separately selectable
for the title and for full text indexing.
 Translations are not duplicated when fewer than the maximum
allowable number of translations are known.
Our goal is to support the process of finding documents, with the
realization that the process of using documents may need to be sup-
ported in some other way (e.g., by forwarding relevant documents
to someone who is able to read that language). We have therefore
designedour interface to highlight the query terms in translated doc-
uments and to facilitate skimming by emphasizing the most com-
mon translation when multiple translations are displayed. We have
found that such displays can support a classification task, even when
the translation is not easy to read [3]. Documents must be classified
by the user as relevant or not relevant, so our classification results
suggest that this can be an effective user interface design.
4. RESULTS
We present results both for component-level performance of our
language-independentretargeting modules and an assessmentof the
overall retargeting process.
4.1 Component-level Evaluation
We applied our retargeting approach and retrieval enhancement
techniquesdescribedabove in the context of the first Cross-Language
Evaluation Forum?s (CLEF) multilingual task. We used the English
language forms of the queries to retrieve English, French, German,
and Italian documents. Below we present comparative performance
measuresfor two of the main processingcomponentsdescribed above
- statistical stemming backoff translation - applied to the English-
French cross-languagesegment of the CLEF task. The post-translation
document expansion component was applied to the smaller Topic
Detection and Tracking (TDT-3) collection to improve retrieval of
Mandarin documents using English.
4.1.1 Baseline CLEF System Configuration
Our baseline run was conducted as follows. We translated the
 44; 000 documents from the 1994 issues of Le Monde. We used
the English-French bilingual term list downloaded from the Web at
http://www.freedict.com. We then inverted the term list
to form a 35,000 term French-English translation resource. We per-
formed the necessary document and term list normalization; in this
case, removing accentsfrom document surface forms to enable match-
ing with the un-accentedterm list entries, converting case, and split-
ting clitic contractions, such as l?horlage, on punctuation. We trained
the statistical stemming rules on a sample of the bilingual term list
and document collection and applied these rules in stemming back-
off. Our default condition was run with top-2 balanced translation
using the Brown corpus as a source of target language unigram fre-
quency information. Translated documents were then indexed with
Stage 1 Stage 2 Stage 3 Stage 4
Match 70% 3% 0.5% 1%
Table 1: Percentage of document terms translated at each stage
of 4-stage backoff translation with statistical stemming.
the InQuery (version 3.1p1) system, using the kstem stemmer for
English stemming and InQuery?s default English stopword list. Long
queries were formed by concatenatingthe title, description, and nar-
rative fields of the original query specification. The resulting word
sequence was enclosed in an InQuery #sum operator, indicating
unweighted sum.
Our figure of merit for the evaluations below is mean (uninter-
polated) average precision computed using trec eval 2 across the 34
topics in the CLEF evaluation for which relevant French documents
are known.
4.1.2 Backoff Translation with Statistical Stemming
We first contrast the above baseline system with the effectiveness
of an otherwise identical run without the stemming backoff compo-
nent. Terms in the documents are thus only translated if there is an
exact match between the surface form in the document and a surface
form in the bilingual term list. We find that mean average preci-
sion for unstemmed translation is 0.19 as compared with 0.2919 for
our baseline system including stemming backoff based on trained
rules. This difference is significant at p < 0:05, by paired t-test,
two-tailed. The per-query effectiveness is illustrated in Figure 1.
Backoff translation improves translation coverage while retaining
relatively high precision of matching in contrast to unstemmed ef-
fectiveness.
Backoff translation improves cross-languageinformation retrieval
effectiveness by improving translation coverage of the terms in the
document collection. Using the statistical stemmer, by-token cover-
age of document terms increased by 7coverage. The different stages
of the four-stage backoff process contributed as illustrated in 1. The
majority of terms match in the Stage 1 exact match, accounting for
70% of the term instances in the documents. The remaining stages
each accountfor between 0.5% and 3% of the document terms, while
20% of document term instances remain untranslatable. However,
this relatively small increase in coverage results in the highly sig-
nificant improvement in retrieval effectiveness above.
4.1.3 Top-2 Balanced Translation
Here we contrast top-2 balanced translation with top-1 transla-
tion. We retain statistical stemming backoff for the top-1 transla-
tion. We replace each French document term with the highest ranked
English translation by target languageunigram frequencyin the Brown
Corpus as detailed above, retaining the original French term when
no translation is found in the bilingual term list. We achieve a mean
average precision of 0.2532 in contrast with the baseline condition.
This difference is significant at p < 0:01 by paired t-test, two-tailed.
We can effectively incorporate additional translations using top-2
balanced translation without degrading performance by introducing
significant additional noise. A query-by-query contrast is presented
in Figure 2.
4.1.4 Document Expansion
We evaluatedpost-translation documentexpansionusing the Topic
Detection and Tracking (TDT-3) collection. For this evaluation, we
used the TDT-1999 topic detection task evaluation framework, but
2Available at ftp://ftp.cs.cornell.edu/pub/smart/.
because out focus in this paper is on ranked retrieval effectiveness
we report mean uninterpolated averageprecision rather than the topic-
weighted detection cost measure typically reported in TDT. In the
topic detection task, the system is presented with one or more exem-
plar stories from the training epoch?a form of query-by-example?
and must determine whether each story in the evaluation epoch ad-
dresses either the same seminal event or activity or some directly
related event or activity. This is generally thought to be a some-
what narrower formulation than the more widely used notion of top-
ical relevance, but it seems to be well suited to query-by-example
evaluations. The TDT-1999 tracking task was multilingual, search-
ing stories in both English and Mandarin Chinese, and multi-modal,
involving both newswire text and broadcast news audio. We fo-
cus on the cross-language spoken document retrieval component of
the tracking task, using English exemplars to identify on-topic sto-
ries in Mandarin Chinese broadcast news audio. We compare top-1
translation of the Mandarin Chinese stories with and without post-
translation document expansion.3 We used the earlier TDT-2 En-
glish newswire text collection as our side collection for expansion.
We perform topic tracking on 60 topics with 4 exemplarseach. Here,
we report the mean average precision on the 55 topics for which
there are on-topic Mandarin audio stories. The mean uninterpolated
averageprecision for retrieval of unexpandeddocuments is 0.36 while
post-translation document expansion raises this figure to 0.41. This
difference is significant at p < 0:01 by paired t-test, two-tailed. The
contrast is illustrated in Figure 3. Interestingly, when we tried this
with French, we noted that expansion tended to select terms from
the few foreign-language documents that happened to be present in
our expansion collection. We have not yet explored that effect in de-
tail, but this observation suggests that the document expansion may
be sensitive to the characteristics of the expansioncollection that are
not immediately apparent.
4.2 The Learning Curve
We havefound that retargeting can be accomplishedquite quickly
(a day without document expansion, three days for TREC-sized col-
lections with document expansion), but only if the required infras-
tructure is in place. Adapting a system that was developed initially
for Chinese to handle French documents required several weeks,
with most of that effort invested in development of four-stage back-
off translation and statistical stemming. Further adapting the system
to handle German documents revealed the importance of compound
splitting, a problem that we will ultimately need to address by incor-
porating a more general segmentationstrategy than we used initially
for Chinese. In extending the system to Italian we have found that
although our statistical stemmer presently performs poorly in that
language, we can achieve quite credible results even with a fairly
small (17,313 term) bilingual term list using a freely available Mus-
cat stemmer (which exist for ten languages). So although it is pos-
sible in concept to retarget to a new language in just a few days, ex-
tending the system typically takes us between one and three weeks
because we are still climbing the learning curve.
5. CONCLUSION
By building on the lessons learned using the TREC, CLEF, NT-
CIR, and TDT collections, we have sought to build an infrastructure
that can be applied to a broad array of languages. Arabic and Ko-
rean collections are expected to become available in the next year,
and we are now evolving our interface to support user studies. Our
approach is distinguished by support for interactive retrieval even
3Since Mandarin Chinese has little surface morphology, we omit
backoff translation in this case.
Figure 1: Comparison of effectiveness of backoff versus unstemmed translation of French documents: Bars above x-axis indicate
backoff transltion outperforms unstemmed translation.
Figure 2: Comparison of effectiveness of top-2 balanced versus top-1 translation of French documents: Bars above x-axis indicate
?Top-2? outperforms ?Top-1?
Figure 3: Comparison of effectiveness of top-1 post-translation document expansion versus bare top-1 translation of Chinese docu-
ments: Bars above x-axis indicate document expansion outperforms bare translation
in languagesfor which machine translation is presently unavailable,
and our ultimate goal is to characterize how closely we can approx-
imate the retrieval effectiveness users would obtain if they had the
best available machine translations for the retrieved documents.
Acknowledgements
This work was supported in part by DARPA contract N6600197C8540
and DARPA cooperative agreement N660010028910.
6. ADDITIONAL AUTHORS
Clara I. Cabezas ( Department of Linguistics, top University of
Maryland, College Park, email: clarac@umiacs.umd.edu)
7. REFERENCES
[1] G.-A. Levow and D. W. Oard. Translingual topic tracking
with PRISE. In Working Notes of the Third Topic Detection
and Tracking Worksho p, Feb. 2000.
http://www.glue.umd.edu/oard/research.html.
[2] D. W. Oard, G.-A. Levow, and C. I. Cabezas. CLEF
experiments at Maryland: Statistical stemming and backof f
translation. In C. Peters, editor, Proceedings of the First
Cross-Language Evaluation Forum. 2001. To appear.
http://www.glue.umd.edu/oard/research .html.
[3] D. W. Oard and P. Resnik. Support for interactive document
selection in cross-language information retrieval. Information
Processing and Management, 35(3):363?379, July 1999.
[4] A. Singhal, J. Choi, D. Hindle, J. Hirschberg, F. Pereira, and
S. Whittaker. AT&T at TREC-7 SDR Track. In Proceedings of
the DARPA Broadcast News Workshop, 1999.
Desparately Seeking Cebuano
Douglas W. Oard, David Doermann, Bonnie Dorr, Daqing He, Philip Resnik, and Amy Weinberg
UMIACS, University of Maryland, College Park, MD, 20742
(oard,doermann,bonnie,resnik,weinberg)@umiacs.umd.edu
William Byrne, Sanjeev Khudanpur and David Yarowsky
CLSP, Johns Hopkins University, 3400 North Charles Street, Barton Hall, Baltimore, MD 21218
(byrne,khudanpur,yarowsky)@jhu.edu
Anton Leuski, Philipp Koehn and Kevin Knight
USC Information Sciences Institute, 4676 Admiralty Way, Marina Del Rey, CA 90292
(leuski,koehn,knight)@isi.edu
Abstract
This paper describes an effort to rapidly de-
velop language resources and component tech-
nology to support searching Cebuano news sto-
ries using English queries. Results from the
first 60 hours of the exercise are presented.
1 Introduction
The Los Angeles Times reported that at about 5:20 P.M.
on Tuesday March 4, 2003, a bomb concealed in a back-
pack exploded at the airport in Davao City, the second
largest city in the Philippines. At least 23 people were
reported dead, with more than 140 injured, and Pres-
ident Arroyo of the Philippines characterized the blast
as a terrorist act. With the 13 hour time difference, it
was then 4:20 A.M on the same date in Washington, DC.
Twenty-four hours later, at 4:13 A.M. on March 5, partic-
ipants in the Translingual Information Detection, Extrac-
tion and Summarization (TIDES) program were notified
that Cebuano had been chosen as the language of interest
for a ?surprise language? practice exercise that had been
planned quite independently to begin on that date. The
notification observed that Cebuano is spoken by 24% of
the population of the Philippines, and that it is the lingua
franca in the south Philippines, where the event occurred.
One goal of the TIDES program is to develop the abil-
ity to rapidly deploy a broad array of language technolo-
gies for previously unforeseen languages in response to
unexpected events. That capability will be formally ex-
ercised for the first time during June 2003, in a month-
long ?Surprise Language Experiment.? To prepare for
that event, the Linguistic Data Consortium (LDC) orga-
nized a ?dry run? for March 5-14 in order to refine pro-
cedures for rapidly developing language resources of the
type that the TIDES community will need during the July
evaluation.
Development of interactive Cross-Language Informa-
tion Retrieval (CLIR) systems that can be rapidly adapted
to accommodate new languages has been the focus of
extensive collaboration between the University of Mary-
land and The Johns Hopkins University, and more re-
cently with the University of Southern California. The
capability for rapid development of necessary language
resources is an essential part of that process, so we had
been planning to participate in the surprise language dry
run to refine our procedures for sharing those resources
with other members of the TIDES community. Naturally,
we chose CLIR as a driving application to focus our ef-
fort. Our goal, therefore, was to build an interactive sys-
tem that would allow a searcher posing English queries
to find relevant Cebuano news articles from the period
immediately following the bombing.
2 Obtaining Language Resources
Our basic approach to development of an agile system for
interactive CLIR relies on three strategies: (1) create an
infrastructure in advance for English as a query language
that makes only minimal assumptions about the docu-
ment language; (2) leverage the asymmetry inherent in
the problem by assembling strong resources for English
in advance; and (3) develop a robust suite of capabilities
to exploit any language resources that can be found for
the ?surprise language.? We defer the first two topics to
the next section, and focus here on the third. We know of
five possible sources of translation expertise:
People. People who know the language are an excellent
source of insight, and universities are an excellent
place to find such people. We were able to locate
a speaker of Cebuano within 50 feet of one of our
offices, and to schedule an interview with a second
Cebuano speaker within 36 hours of the announce-
ment of the language.
Scholarly literature. Major research universities are
also an excellent place to find written materials de-
scribing a broad array of languages. Within 12 hours
of the announcement, reference librarians at the Uni-
versity of Maryland had identified a textbook on
?Beginning Cebuano,? and we had located a copy
at the University of Southern California. Together
with the excellent electronic resources located by the
LDC, this allowed us to develop a rudimentary stem-
mer within 36 hours.
Translation lexicons. Simple bilingual term lists are
available for many language pairs. Using links pro-
vided by the LDC and our own Web searches, we
were able to construct an English-Cebuano term list
with over 14,000 translation pairs within 12 hours of
the announcement. This largely duplicated a simul-
taneous effort at the LDC, and we later merged our
term list with theirs.
Parallel text. Translation-equivalent documents, when
aligned at the word level, provide an excellent
source of information about not just possible trans-
lations, but their relative predominance. Within 24
hours of the announcement, we had aligned Ce-
buano and English versions of the Holy Bible at
the word level using Giza++. An evaluation by a
native Cebuano speaker of a stratified random sam-
ple of 88 translation pairs showed remarkably high
precision. On a 4-point scale with 1=correct and
4=incorrect the most frequent 100 words averaged
1.3, the next 400 most frequent terms averaged 1.6,
and the 500 next most frequent terms after that aver-
aged 1.7. The Bible?s vocabulary covers only about
half of the words found in typical English news text
(counted by-token), so it is useful to have additional
sources of parallel text. For this reason, we have ex-
tended our previously developed STRAND system
to locate likely translations in the Internet Archive.
Those runs were not yet complete when this paper
was submitted.
Printed Dictionaries. People learning a new language
make extensive use of bilingual dictionaries, so we
have developed a system that mimics that process
to some extent. Within 12 hours of the announce-
ment we had zoned page images from a Cebuano-
English dictionary that was available commercially
in Adobe Page Description Format (PDF) to iden-
tify each dictionary entry, performed optical charac-
ter recognition, and parsed the entries to construct a
bilingual term list. We were aided in this process by
the fact that Cebuano is written in a Roman script.
Again, we achieved good precision, with a sampled
word error rate for OCR of 6.9% and a precision for
a random sample of translation pairs of 87%. Part of
speech tags were also extracted, although they are
not used in our process.
As this description illustrates, these five sources pro-
vide complementary information. Since there is some
uncertainty at the outset about how long it will be before
each delivers useful results, we chose a strategy based
on concurrency, balancing our investment over each the
five sources. This allowed us to use whatever resources
became available first to get an initial system running,
with refinements subsequently being made as additional
resources became available. Because Cebuano and En-
glish are written in the same script, we did not need char-
acter set conversion or phonetic cognate matching in this
case. The CLIR system described in the next section
was therefore constructed using only English resources
that were (or could have been) pre-assembled, plus a
Cebuano-English bilingual term list, a rule-based stem-
mer, and the Cebuano Bible.
3 Building a Cross-Language Retrieval
System
Ideally, we would like to build a system that would find
whatever documents the searcher would wish to read in a
fully automatic mode. In practice, fully automatic search
systems are imperfect even in monolingual applications.
We therefore have developed an interactive approach that
functions something like a typical Web search engine: (1)
the searcher poses their query in English, (2) the sys-
tem ranks the Cebuano documents in decreasing order
of likely relevance to the query, (3) the searcher exam-
ines a list of document titles in something approximat-
ing English, and (4) the searcher may optionally exam-
ine the full text of any document in something approx-
imating English. The intent is to support an iterative
process in which searchers learn to better express their
query through experience. We are only able to provide
very rough translations, so we expect that such a sys-
tem would be used in an environment where searchers
could send documents that appear promising off for pro-
fessional translation when necessary.
At the core of our system is the capability to au-
tomatically rank Cebuano documents based on an En-
glish query. We chose a query translation architecture
using backoff translation and Pirkola?s structured query
method, implemented using Inquery version 3.1p1. The
key idea in backoff translation is to first try to find con-
secutive sequences of query words on the English side
of the bilingual term list, where that fails to try to find
the surface form of each remaining English term, to fall
back to stem matching when necessary, and ultimately to
fall back to retaining the English term unchanged in the
hope that it might be a proper name or some other form
of cognate with Cebuano. Accents are stripped from the
documents and all language resources to facilitate match-
ing at that final step.
Although we have chosen techniques that are relatively
robust and therefore require relatively little domain-
specific tuning, stemmer design is an area of uncertainty
that could adversely affect retrieval effectiveness. We
therefore needed a test collection on which we could try
out variants of the Cebuano stemmer. We built this test
collection using 34,000 Cebuano Bible verses and 50 En-
glish questions that we found on the Web for which ap-
propriate Bible verses were known. Each question was
posed as a query using the batch mode of Inquery, and
the rank of the known relevant verse was taken as a mea-
sure of effectiveness. We took the mean reciprocal rank
(the inverse of the harmonic mean) as a figure of merit
for each configuration, and used a paired two-tailed   -
test (with p  0.05) to assess the statistical significance of
observed differences. Our initial configuration, without
stemming, obtained a mean inverse rank of 0.14, which
is a statistically significant improvement over no transla-
tion at all (mean inverse rank 0.02 from felicitous cognate
and loan word matches). The addition of Cebuano stem-
ming resulted in a reduction in mean inverse rank to 0.09.
Although the reduction is not statistically significant in
that case, the result suggests that our initial stemmer is
not yet useful for information retrieval tasks.
The other key capability that is needed is title and doc-
ument translation. We can accomplish this in one of two
ways. The simplest approach is to reverse the bilingual
term list, and to reverse the role of Cebuano and En-
glish in the process described above for query transla-
tion. Our user interface is capable of displaying multi-
ple translations for a single term (arranged horizontally
for compact depiction or vertically for clearer depiction),
but searchers can choose to display only the single most
likely translation. When reliable translation probability
statistics (from parallel text) are not available, we use the
relative word unigram frequency of each translation of a
Cebuano term in a representative English collection as a
substitute for that probability. A more sophisticated way
is to build a statistical machine translation system using
parallel text. We built our first statistical machine trans-
lation system within 40 hours of the announcement, and
one sentence of the resulting translation using each tech-
nique is shown below:
Cebuano: ?ang rebeldeng milf, kinsa
lakip sa nangamatay, nagdala og
backpack nga dunay explosives nga
niguba sa waiting lounge sa airport,
matod sa mga defense official.?
Term-by-term translation:
?(carelessness, circumference,
conveyence) rebeldeng milf, who lakip
(at in of) nangamatay, nagdala og
backpack nga valid explosives nga
niguba (at, in of) waiting lounge
(at, in, of) airport, matod (at, in,
of) mga defense official?
Statistical translation: ?who was
accused of rank, ehud og niguba
waiting lounge defense of those dumah
milf rebeldeng explosives backpack
airport matod official.?
At this point, term-by-term translation is clearly the bet-
ter choice. But as more parallel text becomes available,
we expect the situation to reverse. The LDC is prepar-
ing a set of human reference translations that will allow
us to detect that changeover point automatically using the
NIST variant of the BLEU measure for machine transla-
tion effectiveness.
4 Conclusion
The results reported in this paper were accomplished by
a team of 20 people with expertise in various facets of te
task that invested about 250 person-hours over two and
a half days. As additional Cebuano-specific evaluation
resources are developed, we expect to gain additional in-
sight into the quality of these early resources. Moreover,
once we see what works best for Cebuano by the end of
the process, we plan to revisit our process design with
an eye towards better optimizing our initial time invest-
ments. We expect to be able to address both of those
points in detail by the time of the conference.
This exercise was originally envisioned as a dry run to
work out the kinks in our process, and indeed we have
already learned a lot on that score. First, we learned that
our basic approach seems sound; we built the key com-
ponents of an interactive CLIR system in about 40 hours,
and by the 60-hour point we had some basis for believing
that each of those components could at least minimally
fulfill their role in a fully integrated system. Some of our
time was, however, spent on things that could have been
done in advance. Perhaps the most important of these
was the development of an information retrieval test col-
lection using the Bible. That job, and numerous smaller
ones, are now done, so we expect that we will be able to
obtain similar results with about half the effort next time
around.
Acknowledgments
Thanks to Clara Cabezas, Tim Hackman, Margie Hi-
nonangan, Burcu Karagol-Ayan, Okan Kolak, Huanfeng
Ma, Grazia Russo-Lassner, Michael Subotin, Jianqiang
Wang and the LDC! This work has been supported in part
by DARPA contract N660010028910.
                                                               Edmonton, May-June 2003
                                                                     Tutorials , pg. 2
                                                              Proceedings of HLT-NAACL
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 61?64,
New York, June 2006. c?2006 Association for Computational Linguistics
Investigating Cross-Language Speech Retrieval for a  
Spontaneous Conversational Speech Collection  
 
Diana Inkpen, Muath Alzghool Gareth J.F. Jones Douglas W. Oard 
School of Info. Technology and Eng. School of Computing College of Info. Studies/UMIACS  
University of Ottawa Dublin City University University of Maryland 
Ottawa, Ontario, Canada, K1N 6N5 Dublin 9, Ireland College Park, MD 20742, USA 
{diana,alzghool}@site.uottawa.ca Gareth.Jones@computing.dcu.ie oard@umd.edu 
 
  
Abstract 
Cross-language retrieval of spontaneous 
speech combines the challenges of working 
with noisy automated transcription and lan-
guage translation. The CLEF 2005 Cross-
Language Speech Retrieval (CL-SR) task 
provides a standard test collection to inves-
tigate these challenges. We show that we 
can improve retrieval performance: by care-
ful selection of the term weighting scheme; 
by decomposing automated transcripts into 
phonetic substrings to help ameliorate tran-
scription errors; and by combining auto-
matic transcriptions with manually-assigned 
metadata. We further show that topic trans-
lation with online machine translation re-
sources yields effective CL-SR. 
1 Introduction 
The emergence of large collections of digitized 
spoken data has encouraged research in speech re-
trieval. Previous studies, notably those at TREC 
(Garafolo et al 2000), have focused mainly on 
well-structured news documents. In this paper we 
report on work carried out for the Cross-Language 
Evaluation Forum (CLEF) 2005 Cross-Language 
Speech Retrieval (CL-SR) track (White et al 2005). 
The document collection for the CL-SR task is a 
part of the oral testimonies collected by the USC 
Shoah Foundation Institute for Visual History and 
Education (VHI) for which some Automatic Speech 
Recognition (ASR) transcriptions are available 
(Oard et al, 2004). The data is conversional spon-
taneous speech lacking clear topic boundaries; it is 
thus a more challenging speech retrieval task than 
those explored previously. The CLEF data is also 
annotated with a range of automatic and manually 
generated sets of metadata. While the complete VHI 
dataset contains interviews in many languages, the 
CLEF 2005 CL-SR task focuses on English speech. 
Cross-language searching is evaluated by making 
the topic statements (from which queries are auto-
matically formed) available in several languages. 
This task raises many interesting research ques-
tions; in this paper we explore alternative term 
weighting methods and content indexing strategies.  
The remainder of this paper is structured as fol-
lows: Section 2 briefly reviews details of the CLEF 
2005 CL-SR task; Section 3 describes the system 
we used to investigate this task; Section 4 reports 
our experimental results; and Section 5 gives con-
clusions and details for our ongoing work.   
2 Task description 
The CLEF-2005 CL-SR collection includes 8,104 
manually-determined topically-coherent segments 
from 272 interviews with Holocaust survivors, wit-
nesses and rescuers, totaling 589 hours of speech. 
Two ASR transcripts are available for this data, in 
this work we use transcripts provided by IBM Re-
search in 2004 for which a mean word error rate of 
38% was computed on held out data. Additional, 
metadata fields for each segment include: two sets 
of 20 automatically assigned thesaurus terms from 
different kNN classifiers (AK1 and AK2), an aver-
age of 5 manually-assigned thesaurus terms (MK), 
and a 3-sentence summary written by a subject mat-
ter expert. A set of 38 training topics and 25 test 
topics were generated in English from actual user 
requests. Topics were structured as Title, Descrip-
tion and Narrative fields, which correspond roughly 
to a 2-3 word Web query, what someone might first 
say to a librarian, and what that librarian might ul-
timately understand after a brief reference inter-
view. To support CL-SR experiments the topics 
were re-expressed in Czech, German, French, and 
Spanish by native speakers in a manner reflecting 
61
the way questions would be posed in those lan-
guages. Relevance judgments were manually gener-
ated using by augmenting an interactive search-
guided procedure and purposive sampling designed 
to identify additional relevant segments. See (Oard 
et al 2004) and (White et al 2005) for details.  
3 System Overview 
Our Information Retrieval (IR) system was built 
with off-the-shelf components.  Topics were trans-
lated from French, Spanish, and German into Eng-
lish using seven free online machine translation 
(MT) tools. Their output was merged in order to 
allow for variety in lexical choices. All the transla-
tions of a topic Title field were combined in a 
merged Title field of the translated topics; the same 
procedure was adopted for the Description and Nar-
rative fields. Czech language topics were translated 
using InterTrans, the only web-based MT system 
available to us for this language pair. Retrieval was 
carried out using the SMART IR system (Buckley 
et al 1993) applying its standard stop word list and 
stemming algorithm.  
In system development using the training topics we 
tested SMART with many different term weighting 
schemes combining collection frequency, document 
frequency and length normalization for the indexed 
collection and topics (Salton and Buckley, 1988). In 
this paper we employ the notation used in SMART 
to describe the combined schemes: xxx.xxx. The 
first three characters refer to the weighting scheme 
used to index the document collection and the last 
three characters refer to the weighting scheme used 
to index the topic fields. For example, lpc.atc means 
that lpc was used for documents and atc for queries. 
lpc would apply log term frequency weighting (l) 
and probabilistic collection frequency weighting (p) 
with cosine normalization to the document collec-
tion (c). atc would apply augmented normalized 
term frequency (a), inverse document frequency 
weight (t) with cosine normalization (c). 
One scheme in particular (mpc.ntn) proved to 
have much better performance than other combina-
tions. For weighting document terms we used term 
frequency normalized by the maximum value (m) 
and probabilistic collection frequency weighting (p) 
with cosine normalization (c). For topics we used 
non-normalized term frequency (n) and inverse 
document frequency weighting (t) without vector 
normalization (n). This combination worked very 
well when all the fields of the query were used; it 
also worked well with Title plus Description, but 
slightly less well with the Title field alone. 
4 Experimental Investigation 
In this section we report results from our experi-
mental investigation of the CLEF 2005 CL-SR task. 
For each set of experiments we report Mean unin-
terpolated Average Precision (MAP) computed us-
ing the trec_eval script. The topic fields used are 
indicated as: T for title only, TD for title + descrip-
tion, TDN for title + description + narrative. The 
first experiment shows results for different term 
weighting schemes; we then give cross-language 
retrieval results. For both sets of experiments, 
?documents? are represented by combining the 
ASR transcription with the AK1 and AK2 fields. 
Thus each document representation is generated 
completely automatically. Later experiments ex-
plore two alternative indexing strategies. 
4.1 Comparison of Term Weighting Schemes 
The CLEF 2005 CL-SR collection is quite small by 
IR standards, and it is well known that collection 
size matters when selecting term weighting schemes 
(Salton and Buckley, 1988).  Moreover, the docu-
ments in this case are relatively short, averaging 
about 500 words (about 4 minutes of speech), and 
that factor may affect the optimal choice of weight-
ing schemes as well.  We therefore used the training 
topics to explore the space of available SMART 
term weighting schemes.  Table 1 presents results 
for various weighting schemes with  English topics. 
There are 3,600 possible combinations of weighting 
schemes available: 60 schemes (5 x 4 x 3) for 
documents and 60 for queries. We tested a total of 
240 combinations. In Table 1 we present the results 
for 15 combinations (the best ones, plus some oth-
ers to illustate  the diversity of the results). mpc.ntn 
is still the best for the test topic set; but, as shown, a 
few other weighting schemes achieve similar per-
formance. Some of the weighting schemes perform 
better when indexing all the topic fields (TDN), 
some on TD, and some on title only (T). npn.ntn 
was best for TD and lsn.ntn and lsn.atn are best for 
T. The mpc.ntn weighting scheme is used for all 
other experiments in this section.  We are investi-
gating the reasons for the effectiveness of this 
weighting scheme in our experiments. 
62
TDN TD T  Weighting 
scheme Map Map Map 
1 Mpc.mts 0.2175 0.1651 0.1175 
2 Mpc.nts 0.2175 0.1651 0.1175 
3 Mpc.ntn  0.2176 0.1653 0.1174 
4 npc.ntn 0.2176 0.1653 0.1174 
5 Mpc.mtc 0.2176 0.1653 0.1174 
6 Mpc.ntc 0.2176 0.1653 0.1174 
7 Mpc.mtn 0.2176 0.1653 0.1174 
8 Npn.ntn 0.2116 0.1681 0.1181 
9 lsn.ntn 0.1195 0.1233 0.1227 
10 lsn.atn 0.0919 0.1115 0.1227 
11 asn.ntn 0.0912 0.0923 0.1062 
12 snn.ntn 0.0693 0.0592 0.0729 
13 sps.ntn 0.0349 0.0377 0.0383 
14 nps.ntn 0.0517 0.0416 0.0474 
15 Mtc.atc 0.1138 0.1151 0.1108 
Table 1. MAP, 25 English test topics. Bold=best scores. 
4.2 Cross-Language Experiments 
Table 2 shows our results for the merged ASR, 
AK1 and AK2 documents with multi-system topic 
translations for French, German and Spanish, and 
single-system Czech translation. We can see that 
Spanish topics perform well compared to monolin-
gual English. However, results for German and 
Czech are much poorer. This is perhaps not surpris-
ing for the Czech topics where only a single transla-
tion is available. For German, the quality of 
translation was sometimes low and some German 
words were retained untranslated. For French, only 
TD topic fields were available.  In this case we can 
see that cross-language retrieval effectiveness is 
almost identical to monolingual English. Every re-
search team participating in the CLEF 2005 CL-SR 
task submitted at least one TD English run, and 
among those our mpc.ntn system yielded the best 
MAP (Wilcoxon signed rank test for paired sam-
ples, p<0.05). However, as we show in Table 4, 
manual metadata can yield better retrieval effec-
tiveness than automatic description.  
 
Topic 
Language 
System Map Fields 
English Our system 0.1653 TD 
English Our system 0.2176 TDN 
Spanish Our system 0.1863 TDN 
French Our system 0.1685 TD 
German Our system 0.1281 TDN 
Czech Our system 0.1166 TDN 
Table 2. MAP, cross-language, 25 test topics 
Language Map Fields Description 
English 0.1276 T Phonetic 
English 0.2550 TD Phonetic 
English 0.1245 T Phonetic+Text 
English 0.2590 TD Phonetic+Text 
Spanish 0.1395 T Phonetic 
Spanish 0.2653 TD Phonetic 
Spanish 0.1443 T Phonetic+Text 
Spanish 0.2669 TD Phonetic+Text 
French 0.1251 T Phonetic 
French 0.2726 TD Phonetic 
French 0.1254 T Phonetic+Text 
French 0.2833 TD Phonetic+Text 
German 0.1163 T Phonetic 
German 0.2356 TD Phonetic 
German 0.1187 T Phonetic+Text 
German 0.2324 TD Phonetic+Text 
Czech 0.0776 T Phonetic 
Czech 0.1647 TD Phonetic 
Czech 0.0805 T Phonetic+Text 
Czech 0.1695 TD Phonetic+Text 
Table 3. MAP, phonetic 4-grams, 25 test topics. 
4.3 Results on Phonetic Transcriptions 
In Table 3 we present results for an experiment 
where the text of the collection and topics, without 
stemming, is transformed into a phonetic transcrip-
tion. Consecutive phones are then grouped into 
overlapping n-gram sequences (groups of n sounds, 
n=4 in our case) that we used for indexing. The 
phonetic n-grams were provided by Clarke (2005), 
using NIST?s text-to-phone tool1. For example, the 
phonetic form for the query fragment child survi-
vors is: ch_ay_l_d s_ax_r_v ax_r_v_ay r_v_ay_v 
v_ay_v_ax ay_v_ax_r v_ax_r_z. 
The phonetic form helps compensate for the 
speech recognition errors. With TD queries, the re-
sults improve substantially compared with the text 
form of the documents and queries (9% relative). 
Combining phonetic and text forms (by simply in-
dexing both phonetic n-grams and text) yields little 
additional improvement. 
4.4 Manual summaries and keywords 
Manually prepared transcripts are not available 
for this test collection, so we chose to use manually 
assigned metadata as a reference condition.  To ex-
plore the effect of merging automatic and manual 
fields, Table 4 presents the results combining man-
                                                          
1 http://www.nist.gov/speech/tools/ 
63
ual keywords and manual summaries with ASR 
transcripts, AK1, and AK2. Retrieval effectiveness 
increased substantially for all topic languages. The 
MAP score improved with 25% relative when add-
ing the manual metadata for English TDN.  
Table 4 also shows comparative results between 
and our results and results reported by the Univer-
sity of Maryland at CLEF 2005 using a widely used 
IR system (InQuery) that has a standard term 
weighting algorithm optimized for large collections. 
For English TD, our system is 6% (relative) better 
and for French TD 10% (relative) better.  The Uni-
versity of Maryland results with only automated 
fields are also lower than the results we report in 
Table 2 for the same fields. 
 
Table 4. MAP, indexing all fields (MK, summaries, 
ASR transcripts, AK1 and AK2), 25 test topics. 
Language System Map Fields 
English Our system 0.4647 TDN 
English Our system 0.3689 TD 
English InQuery 0.3129 TD 
English Our system 0.2861 T 
Spanish Our system 0.3811 TDN 
French Our system 0.3496 TD 
French InQuery 0.2480 TD 
French Our system 0.3496 TD 
German Our system 0.2513 TDN 
Czech Our system 0.2338 TDN 
5 Conclusions and Further Investigation 
The system described in this paper obtained the best 
results among the seven teams that participated in 
the CLEF 2005 CL-SR track. We believe that this 
results from our use of the 38 training topics to find 
a term weighting scheme that is particularly suitable 
for this collection. Relevance judgments are typi-
cally not available for training until the second year 
of an IR evaluation; using a search-guided process 
that does not require system results to be available 
before judgments can be performed made it possi-
ble to accelerate that timetable in this case.  Table 2 
shows that performance varies markedly with the 
choice of weighting scheme.  Indeed, some of the 
classic weighting schemes yielded much poorer 
results than the one  we ultimately selected. In this 
paper we presented results on the test queries, but 
we observed similar effects on the training queries. 
On combined manual and automatic data, the 
best MAP score we obtained for English topics is 
0.4647. On automatic data, the best MAP is 0.2176. 
This difference could result from ASR errors or 
from terms added by human indexers that were not 
available to the ASR system to be recognized. In 
future work we plan to investigate methods of re-
moving or correcting some of the speech recogni-
tion errors in the ASR transcripts using semantic 
coherence measures. 
In ongoing further work we are exploring the re-
lationship between properties of the collection and 
the weighting schemes in order to better understand 
the underlying reasons for the demonstrated effec-
tiveness of the mpc.ntn weighting scheme.  
The challenges of CLEF CL-SR task will con-
tinue to expand in subsequent years as new collec-
tions are introduced (e.g., Czech interviews in 
2006). Because manually assigned segment bounda-
ries are available only for English interviews, this 
will yield an unknown topic boundary condition 
that is similar to previous experiments with auto-
matically transcribed broadcast news the Text Re-
trieval Conference (Garafolo et al 2000), but with 
the additional caveat that topic boundaries are not 
known for the ground truth relevance judgments.    
References 
Chris Buckley, Gerard Salton, and James Allan. 1993. 
Automatic retrieval with locality information using 
SMART. In Proceedings of the First Text REtrieval 
Conference (TREC-1), pages 59?72. 
Charles L. A. Clarke. 2005. Waterloo Experiments for 
the CLEF05 SDR Track, in Working Notes for the 
CLEF 2005 Workshop, Vienna, Austria 
John S. Garofolo, Cedric G.P. Auzanne and Ellen M. 
Voorhees. 2000. The TREC Spoken Document Re-
trieval Track: A Success Story. In Proceedings of the 
RIAO Conference: Content-Based Multimedia Infor-
mation Access, Paris, France, pages 1-20. 
Douglas W. Oard, Dagobert Soergel, David Doermann, 
Xiaoli Huang, G. Craig Murray, Jianqiang Wang, 
Bhuvana Ramabhadran, Martin Franz and Samuel 
Gustman. 2004. Building an Information Retrieval 
Test Collection for Spontaneous Conversational 
Speech, in  Proceedings of SIGIR, pages 41-48. 
Gerard Salton and Chris Buckley. 1988. Term-weighting 
approaches in automatic retrieval. Information Proc-
essing and Management, 24(5):513-523. 
Ryen W. White, Douglas W. Oard, Gareth J. F. Jones, 
Dagobert Soergel and Xiaoli Huang. 2005. Overview 
of the CLEF-2005 Cross-Language Speech Retrieval 
Track, in Working Notes for the CLEF 2005 Work-
shop, Vienna, Austria 
64
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 182?190,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Phrase-Based Query Degradation Modeling for
Vocabulary-Independent Ranked Utterance Retrieval
J. Scott Olsson
HLT Center of Excellence
Johns Hopkins University
Baltimore, MD 21211, USA
solsson@jhu.edu
Douglas W. Oard
College of Information Studies
University of Maryland
College Park, MD 15213, USA
oard@umd.edu
Abstract
This paper introduces a new approach to rank-
ing speech utterances by a system?s confi-
dence that they contain a spoken word. Multi-
ple alternate pronunciations, or degradations,
of a query word?s phoneme sequence are hy-
pothesized and incorporated into the ranking
function. We consider two methods for hy-
pothesizing these degradations, the best of
which is constructed using factored phrase-
based statistical machine translation. We show
that this approach is able to significantly im-
prove upon a state-of-the-art baseline tech-
nique in an evaluation on held-out speech.
We evaluate our systems using three differ-
ent methods for indexing the speech utter-
ances (using phoneme, phoneme multigram,
and word recognition), and find that degrada-
tion modeling shows particular promise for lo-
cating out-of-vocabulary words when the un-
derlying indexing system is constructed with
standard word-based speech recognition.
1 Introduction
Our goal is to find short speech utterances which
contain a query word. We accomplish this goal
by ranking the set of utterances by our confidence
that they contain the query word, a task known as
Ranked Utterance Retrieval (RUR). In particular,
we are interested in the case when the user?s query
word can not be anticipated by a Large Vocabulary
Continuous Speech Recognizer?s (LVCSR) decod-
ing dictionary, so that the word is said to be Out-Of-
Vocabulary (OOV).
Rare words tend to be the most informative, but
are also most likely to be OOV. When words are
OOV, we must use vocabulary-independent tech-
niques to locate them. One popular approach is to
search for the words in output from a phoneme rec-
ognizer (Ng and Zue, 2000), although this suffers
from the low accuracy typical of phoneme recogni-
tion. We consider two methods for handling this in-
accuracy. First, we compare an RUR indexing sys-
tem using phonemes with two systems using longer
recognition units: words or phoneme multigrams.
Second, we consider several methods for handling
the recognition inaccuracy in the utterance rank-
ing function itself. Our baseline generative model
handles errorful recognition by estimating term fre-
quencies from smoothed language models trained
on phoneme lattices. Our new approach, which we
call query degradation, hypothesizes many alternate
?pronunciations? for the query word and incorpo-
rates them into the ranking function. These degra-
dations are translations of the lexical phoneme se-
quence into the errorful recognition language, which
we hypothesize using a factored phrase-based statis-
tical machine translation system.
Our speech collection is a set of oral history
interviews from the MALACH collection (Byrne
et al, 2004), which has previously been used for
ad hoc speech retrieval evaluations using one-best
word level transcripts (Pecina et al, 2007; Olsson,
2008a) and for vocabulary-independent RUR (Ols-
son, 2008b). The interviews were conducted with
survivors and witnesses of the Holocaust, who dis-
cuss their experiences before, during, and after the
Second World War. Their speech is predominately
spontaneous and conversational. It is often also
emotional and heavily accented. Because the speech
contains many words unlikely to occur within a gen-
eral purpose speech recognition lexicon, it repre-
182
sents an excellent collection for RUR evaluation.
We were graciously permitted to use BBN Tech-
nology?s speech recognition system Byblos (Prasad
et al, 2005; Matsoukas et al, 2005) for our speech
recognition experiments. We train on approximately
200 hours of transcribed audio excerpted from about
800 unique speakers in the MALACH collection. To
provide a realistic set of OOV query words, we use
an LVCSR dictionary previously constructed for a
different topic domain (broadcast news and conver-
sational telephone speech) and discard all utterances
in our acoustic training data which are not covered
by this dictionary. New acoustic and language mod-
els are trained for each of the phoneme, multigram
and word recognition systems.
The output of LVCSR is a lattice of recogni-
tion hypotheses for each test speech utterance. A
lattice is a directed acyclic graph that is used to
compactly represent the search space for a speech
recognition system. Each node represents a point in
time and arcs between nodes indicates a word oc-
curs between the connected nodes? times. Arcs are
weighted by the probability of the word occurring,
so that the so-called ?one-best? path through the lat-
tice (what a system might return as a transcription)
is the path through the lattice having highest proba-
bility under the acoustic and language models. Each
RUR model we consider is constructed using the ex-
pected counts of a query word?s phoneme sequences
in these recognition lattices. We consider three ap-
proaches to producing these phoneme lattices, using
standard word-based LVCSR, phoneme recognition,
and LVCSR using phoneme multigrams. Our word
system?s dictionary contains about 50,000 entries,
while the phoneme system contains 39 phonemes
from the ARPABET set.
Originally proposed by Deligne and Bimbot
(1997) to model variable length regularities in
streams of symbols (e.g., words, graphemes, or
phonemes), phoneme multigrams are short se-
quences of one or more phonemes. We produce a
set of ?phoneme transcripts? by replacing transcript
words with their lexical pronunciation. The set of
multigrams is learned by then choosing a maximum-
likelihood segmentation of these training phoneme
transcripts, where the segmentation is viewed as hid-
den data in an Expectation-Maximization algorithm.
The set of all continuous phonemes occurring be-
tween segment boundaries is then chosen as our
multigram dictionary. This multigram recognition
dictionary contains 16,409 entries.
After we have obtained each recognition lat-
tice, our indexing approach follows that of Olsson
(2008b). Namely, for the word and multigram sys-
tems, we first expand lattice arcs containing multi-
ple phones to produce a lattice having only single
phonemes on its arcs. Then, we compute the ex-
pected count of all phoneme n-grams n ? 5 in the
lattice. These n-grams and their counts are inserted
in our inverted index for retrieval.
This paper is organized as follows. In Section 2
we introduce our baseline RUR methods. In Sec-
tion 3 we introduce our query degradation approach.
We introduce our experimental validation in Sec-
tion 4 and our results in Section 5. We find that
using phrase-based query degradations can signifi-
cantly improve upon a strong RUR baseline. Finally,
in Section 6 we conclude and outline several direc-
tions for future work.
2 Generative Baseline
Each method we present in this paper ranks the ut-
terances by the term?s estimated frequency within
the corresponding phoneme lattice. This general
approach has previously been considered (Yu and
Seide, 2005; Saraclar and Sproat, 2004), on the ba-
sis that it provides a minimum Bayes-risk ranking
criterion (Yu et al, Sept 2005; Robertson, 1977) for
the utterances. What differs for each method is the
particular estimator of term frequency which is used.
We first outline our baseline approach, a generative
model for term frequency estimation.
Recall that our vocabulary-independent indices
contain the expected counts of phoneme sequences
from our recognition lattices. Yu and Seide (2005)
used these expected phoneme sequence counts to es-
timate term frequency in the following way. For a
query term Q and lattice L, term frequency t?fG is
estimated as t?fG(Q,L) = P (Q|L) ?NL, where NL
is an estimate for the number of words in the utter-
ance. The conditional P (Q|L) is modeled as an or-
der M phoneme level language model,
P? (Q|L) =
l?
i=1
P? (qi|qi?M+1, . . . , qi?1,L), (1)
183
so that t?fG(Q,L) ? P? (Q|L) ? NL. The probabil-
ity of a query phoneme qj being generated, given
that the phoneme sequence qj?M+1, . . . , qj?1 =
qj?1j?M+1 was observed, is estimated as
P? (qj |qj?1j?M+1,L) =
EPL [C(qjj?M+1)]
EPL [C(qj?1j?M+1)]
.
Here, EPL [C(qj?1j?M+1)] denotes the expected count
in lattice L of the phoneme sequence qj?1j?M+1. We
compute these counts using a variant of the forward-
backward algorithm, which is implemented by the
SRI language modeling toolkit (Stolcke, 2002).
In practice, because of data sparsity, the language
model in Equation 1 must be modified to include
smoothing for unseen phoneme sequences. We use a
backoff M -gram model with Witten-Bell discount-
ing (Witten and Bell, 1991). We set the phoneme
language model?s order to M = 5, which gave good
results in previous work (Yu and Seide, 2005).
3 Incorporating Query Degradations
One problem with the generative approach is that
recognition error is not modeled (apart from the un-
certainty captured in the phoneme lattice). The es-
sential problem is that while the method hopes to
model P (Q|L), it is in fact only able to model the
probability of one degradation H in the lattice, that
is P (H|L). We define a query degradation as any
phoneme sequence (including the lexical sequence)
which may, with some estimated probability, occur
in an errorful phonemic representation of the audio
(either a one-best or lattice hypothesis). Because of
speaker variation and because recognition is error-
ful, we ought to also consider non-lexical degrada-
tions of the query phoneme sequence. That is, we
should incorporate P (H|Q) in our ranking function.
It has previously been demonstrated that allow-
ing for phoneme confusability can significantly in-
crease spoken term detection performance on one-
best phoneme transcripts (Chaudhari and Picheny,
2007; Schone et al, 2005) and in phonemic lat-
tices (Foote et al, 1997). These methods work by
allowing weighted substitution costs in minimum-
edit-distance matching. Previously, these substitu-
tion costs have been maximum-likelihood estimates
of P (H|Q) for each phoneme, where P (H|Q) is
easily computed from a phoneme confusion matrix
after aligning the reference and one-best hypothesis
transcript under a minimum edit distance criterion.
Similar methods have also been used in other lan-
guage processing applications. For example, in (Ko-
lak, 2005), one-for-one character substitutions, in-
sertions and deletions were considered in a genera-
tive model of errors in OCR.
In this work, because we are focused on construct-
ing inverted indices of audio files (for speed and
to conserve space), we must generalize our method
of incorporating query degradations in the ranking
function. Given a degradation model P (H|Q), we
take as our ranking function the expectation of the
generative baseline estimate NL ? P? (H|L) with re-
spect to P (H|Q),
t?fG(Q,L) =
?
H?H
[
P? (H|L) ?NL
]
?P (H|Q), (2)
where H is the set of degradations. Note that, while
we consider the expected value of our baseline term
frequency estimator with respect to P (H|Q), this
general approach could be used with any other term
frequency estimator.
Our formulation is similar to approaches taken
in OCR document retrieval, using degradations of
character sequences (Darwish and Magdy, 2007;
Darwish, 2003). For vocabulary-independent spo-
ken term detection, perhaps the most closely re-
lated formulation is provided by (Mamou and Ram-
abhadran, 2008). In that work, they ranked ut-
terances by the weighted average of their match-
ing score, where the weights were confidences from
a grapheme to phoneme system?s first several hy-
potheses for a word?s pronunciation. The match-
ing scores were edit distances, where substitution
costs were weighted using phoneme confusability.
Accordingly, their formulation was not aimed at ac-
counting for errors in recognition per se, but rather
for errors in hypothesizing pronunciations. We ex-
pect this accounts for their lack of significant im-
provement using the method.
Since we don?t want to sum over all possible
recognition hypotheses H , we might instead sum
over the smallest setH such that?H?H P (H|Q) ?
?. That is, we could take the most probable degra-
dations until their cumulative probability exceeds
some threshold ?. In practice, however, because
184
degradation probabilities can be poorly scaled, we
instead take a fixed number of degradations and
normalize their scores. When a query is issued,
we apply a degradation model to learn the top few
phoneme sequences H that are most likely to have
been recognized, under the model. In the machine
translation literature, this process is commonly re-
ferred to as decoding.
We now turn to the modeling of query degrada-
tions H given a phoneme sequence Q, P (H|Q).
First, we consider a simple baseline approach in Sec-
tion 3.1. Then, in Section 3.2, we propose a more
powerful technique, using state-of-the-art machine
translation methods to hypothesize our degradations.
3.1 Baseline Query Degradations
Schone et al (2005) used phoneme confusion ma-
trices created by aligning hypothesized and refer-
ence phoneme transcripts to weight edit costs for a
minimum-edit distance based search in a one-best
phoneme transcript. Foote et al (1997) had previ-
ously used phoneme lattices, although with ad hoc
edit costs and without efficient indexing. In this
work, we do not want to linearly scan each phoneme
lattice for our query?s phoneme sequence, preferring
instead to look up sequences in the inverted indices
containing phoneme sequences.
Our baseline degradation approach is related to
the edit-cost approach taken by (Schone et al,
2005), although we generalize it so that it may be
applied within Equation 2 and we consider speech
recognition hypotheses beyond the one-best hypoth-
esis. First, we randomly generate N traversals of
each phonemic recognition lattice. These traver-
sals are random paths through the lattice (i.e., we
start at the beginning of the lattice and move to the
next node, where our choice is weighted by the out-
going arcs? probabilities). Then, we align each of
these traversals with its reference transcript using a
minimum-edit distance criterion. Phone confusion
matrices are then tabulated from the aggregated in-
sertion, substitution, and deletion counts across all
traversals of all lattices. From these confusion ma-
trices, we compute unsmoothed estimates of P (h|r),
the probability of a phoneme h being hypothesized
given a reference phoneme r.
Making an independence assumption, our base-
line degradation model for a query with m
AY K M AA N
Vowel Consonant Semi-vowel Vowel Semi-vowel
Dipthong Voiceless plosive Nasal Back vowel Nasal
Figure 1: Three levels of annotation used by the factored
phrase-based query degradation model.
phonemes is then P (H|Q) = ?mi=1 P (hi|ri). We
efficiently compute the most probable degradations
for a query Q using a lattice of possible degrada-
tions and the forward backward algorithm. We call
this baseline degradation approach CMQD (Confu-
sion Matrix based Query Degradation).
3.2 Phrase-Based Query Degradation
One problem with CMQD is that we only allow in-
sertions, deletions, and one-for-one substitutions. It
may be, however, that certain pairs of phonemes
are commonly hypothesized for a particular refer-
ence phoneme (in the language of statistical machine
translation, we might say that we should allow some
non-zero fertility). Second, there is nothing to dis-
courage query degradations which are unlikely un-
der an (errorful) language model?that is, degrada-
tions that are not observed in the speech hypothe-
ses. Finally, CMQD doesn?t account for similarities
between phoneme classes. While some of these de-
ficiencies could be addressed with an extension to
CMQD (e.g., by expanding the degradation lattices
to include language model scores), we can do bet-
ter using a more powerful modeling framework. In
particular, we adopt the approach of phrase-based
statistical machine translation (Koehn et al, 2003;
Koehn and Hoang, 2007). This approach allows
for multiple-phoneme to multiple-phoneme substi-
tutions, as well as the soft incorporation of addi-
tional linguistic knowledge (e.g., phoneme classes).
This is related to previous work allowing higher or-
der phoneme confusions in bigram or trigram con-
texts (Chaudhari and Picheny, 2007), although they
used a fuzzy edit distance measure and did not in-
corporate other evidence in their model (e.g., the
phoneme language model score). The reader is re-
ferred to (Koehn and Hoang, 2007; Koehn et al,
2007) for detailed information about phrase-based
statistical machine translation. We give a brief out-
line here, sufficient only to provide background for
our query degradation application.
Statistical machine translation systems work by
185
converting a source-language sentence into the most
probable target-language sentence, under a model
whose parameters are estimated using example sen-
tence pairs. Phrase-based machine translation is one
variant of this statistical approach, wherein multiple-
word phrases rather than isolated words are the
basic translation unit. These phrases are gener-
ally not linguistically motivated, but rather learned
from co-occurrences in the paired example transla-
tion sentences. We apply the same machinery to hy-
pothesize our pronunciation degradations, where we
now translate from the ?source-language? reference
phoneme sequence Q to the hypothesized ?target-
language? phoneme sequence H .
Phrase-based translation is based on the noisy
channel model, where Bayes rule is used to refor-
mulate the translation probability for translating a
reference query Q into a hypothesized phoneme se-
quence H as
argmax
H
P (H|Q) = argmax
H
P (Q|H)P (H).
Here, for example, P (H) is the language model
probability of a degradation H and P (Q|H) is the
conditional probability of the reference sequence Q
given H . More generally however, we can incorpo-
rate other feature functions of H and Q, hi(H,Q),
and with varying weights. This is implemented us-
ing a log-linear model for P (H|Q), where the model
covariates are the functions hi(H,Q), so that
P (H|Q) = 1Z exp
n?
i=1
?ihi(H,Q)
The parameters ?i are estimated by MLE and the
normalizing Z need not be computed (because we
will take the argmax). Example feature functions in-
clude the language model probability of the hypoth-
esis and a hypothesis length penalty.
In addition to feature functions being defined on
the surface level of the phonemes, they may also be
defined on non-surface annotation levels, called fac-
tors. In a word translation setting, the intuition is
that statistics from morphological variants of a lex-
ical form ought to contribute to statistics for other
variants. For example, if we have never seen the
word houses in language model training, but have
examples of house, we still can expect houses are to
be more probable than houses fly. In other words,
factors allow us to collect improved statistics on
sparse data. While sparsity might appear to be less
of a problem for phoneme degradation modeling
(because the token inventory is comparatively very
small), we nevertheless may benefit from this ap-
proach, particularly because we expect to rely on
higher order language models and because we have
rather little training data: only 22,810 transcribed
utterances (about 600k reference phonemes).
In our case, we use two additional annotation lay-
ers, based on a simple grouping of phonemes into
broad classes. We consider the phoneme itself, the
broad distinction of vowel and consonant, and a finer
grained set of classes (e.g., front vowels, central
vowels, voiceless and voiced fricatives). Figure 1
shows the three annotation layers we consider for an
example reference phoneme sequence. After map-
ping the reference and hypothesized phonemes to
each of these additional factor levels, we train lan-
guage models on each of the three factor levels of
the hypothesized phonemes. The language models
for each of these factor levels are then incorporated
as features in the translation model.
We use the open source toolkit Moses (Koehn
et al, 2007) as our phrase-based machine transla-
tion system. We used the SRI language model-
ing toolkit to estimate interpolated 5-gram language
models (for each factor level), and smoothed our
estimates with Witten-Bell discounting (Witten and
Bell, 1991). We used the default parameter settings
for Moses?s training, with the exception of modi-
fying GIZA++?s default maximum fertility from 10
to 4 (since we don?t expect one reference phoneme
to align to 10 degraded phonemes). We used default
decoding settings, apart from setting the distortion
penalty to prevent any reorderings (since alignments
are logically constrained to never cross). For the rest
of this chapter, we refer to our phrase-based query
degradation model as PBQD. We denote the phrase-
based model using factors as PBQD-Fac.
Figure 2 shows an example alignment learned
for a reference and one-best phonemic transcript.
The reference utterance ?snow white and the seven
dwarves? is recognized (approximately) as ?no
white a the second walks?. Note that the phrase-
based system is learning not only acoustically plau-
sible confusions, but critically, also confusions aris-
186
N OW W AY T AX DH AX S EH K AX N D W AO K S
S N OW W AY T AE N D DH AX S EH V AX N D W OW R F S
snow white and the seven dwarves
Figure 2: An alignment of hypothesized and reference phoneme transcripts from the multigram phoneme recognizer,
for the phrase-based query degradation model.
ing from the phonemic recognition system?s pe-
culiar construction. For example, while V and
K may not be acoustically similar, they are still
confusable?within the context of S EH?because
multigram language model data has many exam-
ples of the word second. Moreover, while the word
dwarves (D-W-OW-R-F-S) is not present in the
dictionary, the words dwarf (D-W-AO-R-F) and
dwarfed (D-W-AO-R-F-T) are present (N.B., the
change of vowel from AO to OW between the OOV
and in vocabulary pronunciations). While CMQD
would have to allow a deletion and two substitutions
(without any context) to obtain the correct degrada-
tion, the phrase-based system can align the complete
phrase pair from training and exploit context. Here,
for example, it is highly probable that the errorfully
hypothesized phonemes W AO will be followed by
K, because of the prevalence of walk in language
model data.
4 Experiments
An appropriate and commonly used measure for
RUR is Mean Average Precision (MAP). Given a
ranked list of utterances being searched through, we
define the precision at position i in the list as the pro-
portion of the top i utterances which actually contain
the corresponding query word. Average Precision
(AP) is the average of the precision values computed
for each position containing a relevant utterance. To
assess the effectiveness of a system across multi-
ple queries, Mean Average Precision is defined as
the arithmetic mean of per-query average precision,
MAP = 1n
?
n APn. Throughout this paper, when
we report statistically significant improvements in
MAP, we are comparing AP for paired queries us-
ing a Wilcoxon signed rank test at ? = 0.05.
Note, RUR is different than spoken term detec-
tion in two ways, and thus warrants an evaluation
measure (e.g., MAP) different than standard spoken
term detection measures (such as NIST?s actual term
weighted value (Fiscus et al, 2006)). First, STD
measures require locating a term with granularity
finer than that of an utterance. Second, STD mea-
sures are computed using a fixed detection thresh-
old. This latter requirement will be unnecessary in
many applications (e.g., where a user might prefer
to decide themselves when to stop reading down
the ranked list of retrieved utterances) and unlikely
to be helpful for downstream evidence combination
(where we may prefer to keep all putative hits and
weight them by some measure of confidence).
For our evaluation, we consider retrieving
short utterances from seventeen fully transcribed
MALACH interviews. Our query set contains all
single words occurring in these interviews that are
OOV with respect to the word dictionary. This
gives us a total of 261 query terms for evalua-
tion. Note, query words are also not present in
the multigram training transcripts, in any language
model training data, or in any transcripts used for
degradation modeling. Some example query words
include BUCHENWALD, KINDERTRANSPORT, and
SONDERKOMMANDO.
To train our degradation models, we used a held
out set of 22,810 manually transcribed utterances.
We run each recognition system (phoneme, multi-
gram, and word) on these utterances and, for each,
train separate degradation models using the aligned
reference and hypothesis transcripts. For CMQD,
we computed 100 random traversals on each lattice,
giving us a total of 2,281,000 hypothesis and refer-
ence pairs to align for our confusion matrices.
5 Results
We first consider an intrinsic measure of the three
speech recognition systems we consider, namely
Phoneme Error Rate (PER). Phoneme Error Rate
is calculated by first producing an alignment of
187
the hypothesis and reference phoneme transcripts.
The counts of each error type are used to compute
PER = 100 ? S+D+IN , where S,D, I are the num-ber of substitutions, insertions, and deletions respec-
tively, while N is the phoneme length of the refer-
ence. Results are shown in Table 1. First, we see that
the PER for the multigram system is roughly half
that of the phoneme-only system. Second, we find
that the word system achieves a considerably lower
PER than the multigram system. We note, however,
that since these are not true phonemes (but rather
phonemes copied over from pronunciation dictionar-
ies and word transcripts), we must cautiously inter-
pret these results. In particular, it seems reasonable
that this framework will overestimate the strength
of the word based system. For comparison, on the
same train/test partition, our word-level system had
a word error rate of 31.63. Note, however, that au-
tomatic word transcripts can not contain our OOV
query words, so word error rate is reported only to
give a sense of the difficulty of the recognition task.
Table 1 shows our baseline RUR evaluation re-
sults. First, we find that the generative model yields
statistically significantly higher MAP using words
or multigrams than phonemes. This is almost cer-
tainly due to the considerably improved phoneme
recognition afforded by longer recognition units.
Second, many more unique phoneme sequences typ-
ically occur in phoneme lattices than in their word
or multigram counterparts. We expect this will in-
crease the false alarm rate for the phoneme system,
thus decreasing MAP.
Surprisingly, while the word-based recognition
system achieved considerably lower phoneme er-
ror rates than the multigram system (see Table 1),
the word-based generative model was in fact in-
distinguishable from the same model using multi-
grams. We speculate that this is because the method,
as it is essentially a language modeling approach,
is sensitive to data sparsity and requires appropri-
ate smoothing. Because multigram lattices incor-
porate smaller recognition units, which are not con-
strained to be English words, they naturally produce
smoother phoneme language models than a word-
based system. On the other hand, the multigram
system is also not statistically significantly better
than the word-based generative model, suggesting
this may be a promising area for future work.
Table 1 shows results using our degradation mod-
els. Query degradation appears to help all sys-
tems with respect to the generative baseline. This
agrees with our intuition that, for RUR, low MAP on
OOV terms is predominately driven by low recall.1
Note that, at one degradation, CMQD has the same
MAP as the generative model, since the most prob-
able degradation under CMQD is almost always the
reference phoneme sequence. Because the CMQD
model can easily hypothesize implausible degrada-
tions, we see the MAP increases modestly with a
few degradations, but then MAP decreases. In con-
trast, the MAP of the phrase-based system (PBQD-
Fac) increases through to 500 query degradations us-
ing multigrams. The phonemic system appears to
achieve its peak MAP with fewer degradations, but
also has a considerably lower best value.
The non-factored phrase-based system PBQD
achieves a peak MAP considerably larger than the
peak CMQD approach. And, likewise, using addi-
tional factor levels (PBQD-Fac) also considerably
improves performance. Note especially that, using
multiple factor levels, we not only achieve a higher
MAP, but also a higher MAP when only a few degra-
dations are possible.
To account for errors in phonemic recognition, we
have taken two steps. First, we used longer recog-
nition units which we found significantly improved
MAP while using our baseline RUR technique. As
a second method for handling recognition errors,
we also considered variants of our ranking func-
tion. In particular, we incorporated query degrada-
tions hypothesized using factored phrase-based ma-
chine translation. Comparing the MAP for PBQD-
Fac with MAP using the generative baseline for the
most improved indexing system (the word system),
we find that this degradation approach again statisti-
cally significantly improved MAP. That is, these two
strategies for handling recognition errors in RUR ap-
pear to work well in combination.
Although we focused on vocabulary-independent
RUR, downstream tasks such as ad hoc speech
retrieval will also want to incorporate evidence
from in-vocabulary query words. This makes
1We note however that the preferred operating point in the
tradeoff between precision and recall will be task specific. For
example, it is known that precision errors become increasingly
important as collection size grows (Shao et al, 2008).
188
Query Degradations
Method Phone Source PER QD Model Baseline 1 5 50 500
Degraded Model Phonemes 64.4 PBQD-Fac 0.0387 0.0479 0.0581 0.0614 0.0612
Multigrams 32.1 CMQD 0.1258 0.1258 0.1272 0.1158 0.0991
Multigrams 32.1 PBQD 0.1258 0.1160 0.1283 0.1347 0.1317
Multigrams 32.1 PBQD-Fac 0.1258 0.1238 0.1399 0.1510 0.1527
Words 20.5 PBQD-Fac 0.1255 0.1162 0.1509 0.1787 0.1753
Table 1: PER and MAP results for baseline and degradation models. The best result for each indexing approach is
shown in bold.
our query degradation approach which indexed
phonemes from word-based LVCSR particularly at-
tractive. Not only did it achieve the best MAP in
our evaluation, but this approach also allows us to
construct recognition lattices for both in and out-of-
vocabulary query words without running a second,
costly, recognition step.
6 Conclusion
Our goal in this work was to rank utterances by our
confidence that they contained a previously unseen
query word. We proposed a new approach to this
task using hypothesized degradations of the query
word?s phoneme sequence, which we produced us-
ing a factored phrase-based machine translation
model. This approach was principally motivated by
the mismatch between the query?s phonemes and
the recognition phoneme sequences due to errorful
speech indexing. Our approach was constructed and
evaluated using phoneme-, multigram-, and word-
based indexing, and significant improvements in
MAP using each indexing system were achieved.
Critically, these significant improvements were in
addition to the significant gains we achieved by con-
structing our index with longer recognition units.
While PBQD-Fac outperformed CMQD averag-
ing over all queries in our evaluation, as expected,
there may be particular query words for which this
is not the case. Table 2 shows example degrada-
tions using both the CMQD and PBQD-Fac degra-
dation models for multigrams. The query word is
Mengele. We see that CMQD degradations are near
(in an edit distance sense) to the reference pronun-
ciation (M-EH-NX-EY-L-EH), while the phrase-
based degradations tend to sound like commonly oc-
CMQD Phrase-based
M-EH-NX-EY-L-EH M-EH-N-T-AX-L
M-EH-NX-EY-L M-EH-N-T-AX-L-AA-T
M-NX-EY-L-EH AH-AH-AH-AH-M-EH-N-T-AX-L
M-EH-NX-EY-EH M-EH-N-DH-EY-L-EH
M-EH-NX-L-EH M-EH-N-T-AX-L-IY
Table 2: The top five degradations and associated proba-
bilities using the CMQD and PBQD-Fac models, for the
term Mengele using multigram indexing.
curring words (mental, meant a lot, men they. . . ,
mentally). In this case, the lexical phoneme se-
quence does not occur in the PBQD-Fac degrada-
tions until degradation nineteen. Because delet-
ing EH has the same cost irrespective of context
for CMQD, both CMQD degradations 2 and 3 are
given the same pronunciation weight. Here, CMQD
performs considerably better, achieving an average
precision of 0.1707, while PBQD-Fac obtains only
0.0300. This suggests that occasionally the phrase-
based language model may exert too much influence
on the degradations, which is likely to increase the
incidence of false alarms. One solution, for future
work, might be to incorporate a false alarm model
(e.g., down-weighting putative occurrences which
look suspiciously like non-query words). Second,
we might consider training the degradation model
in a discriminative framework (e.g., training to op-
timize a measure that will penalize degradations
which cause false alarms, even if they are good can-
didates from the perspective of MLE). We hope that
the ideas presented in this paper will provide a solid
foundation for this future work.
189
References
W. Byrne et al 2004. Automatic Recognition of Spon-
taneous Speech for Access to Multilingual Oral His-
tory Archives. IEEE Transactions on Speech and Au-
dio Processing, Special Issue on Spontaneous Speech
Processing, 12(4):420?435, July.
U.V. Chaudhari and M. Picheny. 2007. Improvements in
phone based audio search via constrained match with
high order confusion estimates. Automatic Speech
Recognition & Understanding, 2007. ASRU. IEEE
Workshop on, pages 665?670, Dec.
Kareem Darwish and Walid Magdy. 2007. Error cor-
rection vs. query garbling for Arabic OCR document
retrieval. ACM Trans. Inf. Syst., 26(1):5.
Kareem M. Darwish. 2003. Probabilistic Methods for
Searching OCR-Degraded Arabic Text. Ph.D. thesis,
University of Maryland, College Park, MD, USA. Di-
rected by Bruce Jacob and Douglas W. Oard.
S. Deligne and F. Bimbot. 1997. Inference of Variable-
length Acoustic Units for Continuous Speech Recog-
nition. In ICASSP ?97: Proceedings of the IEEE Inter-
national Conference on Acoustics, Speech, and Signal
Processing, pages 1731?1734, Munich, Germany.
Jonathan Fiscus et al 2006. English Spoken Term De-
tection 2006 Results. In Presentation at NIST?s 2006
STD Eval Workshop.
J.T. Foote et al 1997. Unconstrained keyword spot-
ting using phone lattices with application to spoken
document retrieval. Computer Speech and Language,
11:207?224.
Philipp Koehn and Hieu Hoang. 2007. Factored Transla-
tion Models. In EMNLP ?07: Conference on Empiri-
cal Methods in Natural Language Processing, June.
Philipp Koehn et al 2003. Statistical phrase-based
translation. In NAACL ?03: Proceedings of the 2003
Conference of the North American Chapter of the As-
sociation for Computational Linguistics on Human
Language Technology, pages 48?54, Morristown, NJ,
USA. Association for Computational Linguistics.
Philipp Koehn et al 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In ACL ?07: Pro-
ceedings of the 2007 Conference of the Association
for Computational Linguistics, demonstration session,
June.
Okan Kolak. 2005. Rapid Resource Transfer for Mul-
tilingual Natural Language Processing. Ph.D. thesis,
University of Maryland, College Park, MD, USA. Di-
rected by Philip Resnik.
Jonathan Mamou and Bhuvana Ramabhadran. 2008.
Phonetic Query Expansion for Spoken Document Re-
trieval. In Interspeech ?08: Conference of the Interna-
tional Speech Communication Association.
Spyros Matsoukas et al 2005. The 2004 BBN 1xRT
Recognition Systems for English Broadcast News and
Conversational Telephone Speech. In Interspeech ?05:
Conference of the International Speech Communica-
tion Association, pages 1641?1644.
K. Ng and V.W. Zue. 2000. Subword-based approaches
for spoken document retrieval. Speech Commun.,
32(3):157?186.
J. Scott Olsson. 2008a. Combining Speech Retrieval Re-
sults with Generalized Additive Models. In ACL ?08:
Proceedings of the 2008 Conference of the Association
for Computational Linguistics.
J. Scott Olsson. 2008b. Vocabulary Independent Dis-
criminative Term Frequency Estimation. In Inter-
speech ?08: Conference of the International Speech
Communication Association.
Pavel Pecina, Petra Hoffmannova, Gareth J.F. Jones, Jian-
qiang Wang, and Douglas W. Oard. 2007. Overview
of the CLEF-2007 Cross-Language Speech Retrieval
Track. In Proceedings of the CLEF 2007 Workshop
on Cross-Language Information Retrieval and Evalu-
ation, September.
R. Prasad et al 2005. The 2004 BBN/LIMSI 20xRT En-
glish Conversational Telephone Speech Recognition
System. In Interspeech ?05: Conference of the Inter-
national Speech Communication Association.
S.E. Robertson. 1977. The Probability Ranking Princi-
ple in IR. Journal of Documentation, pages 281?286.
M. Saraclar and R. Sproat. 2004. Lattice-Based Search
for Spoken Utterance Retrieval. In NAACL ?04: Pro-
ceedings of the 2004 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology.
P. Schone et al 2005. Searching Conversational Tele-
phone Speech in Any of the World?s Languages.
Jian Shao et al 2008. Towards Vocabulary-Independent
Speech Indexing for Large-Scale Repositories. In In-
terspeech ?08: Conference of the International Speech
Communication Association.
A. Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In ICSLP ?02: Proceedings of 2002 In-
ternational Conference on Spoken Language Process-
ing.
I. H. Witten and T. C. Bell. 1991. The Zero-Frequency
Problem: Estimating the Probabilities of Novel Events
in Adaptive Text Compression. IEEE Trans. Informa-
tion Theory, 37(4):1085?1094.
Peng Yu and Frank Seide. 2005. Fast Two-
Stage Vocabulary-Independent Search In Spontaneous
Speech. In ICASSP ?05: Proceedings of the 2005
IEEE International Conference on Acoustics, Speech,
and Signal Processing.
P. Yu et al Sept. 2005. Vocabulary-Independent Index-
ing of Spontaneous Speech. IEEE Transactions on
Speech and Audio Processing, 13(5):635?643.
190
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 200?208,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Context-based Message Expansion for Disentanglement
of Interleaved Text Conversations
Lidan Wang
Computer Science Dept./UMIACS
University of Maryland, College Park
College Park, MD 20742
lidan@cs.umd.edu
Douglas W. Oard
College of Information Studies/UMIACS
and HLT Center of Excellence
University of Maryland, College Park
College Park, MD 20742
oard@umd.edu
Abstract
Computational processing of text exchanged
in interactive venues in which participants en-
gage in simultaneous conversations can bene-
fit from techniques for automatically grouping
overlapping sequences of messages into sepa-
rate conversations, a problem known as ?dis-
entanglement.? While previous methods ex-
ploit both lexical and non-lexical information
that exists in conversations for this task, the
inter-dependency between the meaning of a
message and its temporal and social contexts
is largely ignored. Our approach exploits con-
textual properties (both explicit and hidden)
to probabilistically expand each message to
provide a more accurate message representa-
tion. Extensive experimental evaluations show
our approach outperforms the best previously
known technique.
1 Introduction
Conversational media such as the text messages
found in Internet Relay Chat presents both new op-
portunities and new challenges. Among the chal-
lenges are that individual messages are often quite
short, for the reason that conversational participants
are able to assemble the required context over the
course of a conversation. A natural consequence of
this is that many tasks that we would like to perform
on conversational media (e.g., search, summariza-
tion, or automated response) would benefit from re-
assembly of individual messages into complete con-
versations. This task has been studied extensively in
the context of email (where it is often referred to as
?threading?) (Yeh et al, 2006). The extensive meta-
data associated with email and the relatively rich
content of some email messages makes email some-
what of a special case in the broad set of conversa-
tion recovery tasks, however. At the opposite ex-
treme, conversation ?threading? in multi-party spo-
ken interactions (e.g., meetings) would be a com-
pelling application, but the word error rate of current
automated transcription techniques somewhat limits
access to the lexical evidence that we know is use-
ful for this task. The recent interest in identifying
individual conversations from online-discussions, a
task that some refer to as ?disentanglement,? there-
fore seems to be something of a middle ground in
the research space: computationally tractable, repre-
sentative to some degree of a broader class of prob-
lems, and directly useful as a pre-processing step for
a range of important applications.
One way to think of this task is as a clustering
problem?we seek to partition the messages into a
set of disjoint clusters, where each cluster represents
a conversation among a set of participants on a topic.
This formulation raises the natural question of how
we should design a similarity measure. Since the
messages are often too short to be meaningful by
themselves, techniques based solely on lexical over-
lap (e.g., inner products of term vectors weighted
by some function of term frequency, document fre-
quency and message length) are unlikely to be suc-
cessful. For instance, consider the multi-party ex-
change in Figure 1, in which a single message may
not convey much about the topic without consider-
ing what has been said before, and who said it.
Fortunately for us, additional sources of evidence
200
(18323 Ricardo) is there a way to emulate input for a 
program listening on a COM port? 
(18911 Azzie) Ricardo: Hello there, how is it going?
(18939 Ricardo) pretty good, just at the office, about to 
leave. How are you?
(18970 Azzie) well, end of semester work, what could 
be better?
(18980 Josephina) if it's just reading from /dev/ttyS0 or 
something you could somehow get it to just read from a 
named pipe instead
(19034 Ricardo) Josephina: I might just have to end up 
modifying the entire program... 
(19045 Ricardo) so it can read from a different input 
stream
Figure 1: An example of the text message stream. The
number before each author?s name denotes the time-
stamp of the message.
are available. As we describe below, messages
are strongly correlated both temporally (i.e., across
time) and socially (i.e,, across participants). For
example, in our running example in Figure 1, Ri-
cardo?s message (19045 Ricardo) ?so it can read
from a different input stream? elaborates on his
previous message (19034 Ricardo) to Josephina.
Messages that are close in time and from the
same speaker can share related meanings. Simi-
larly, we see that Ricardo?s messages to Josephina
(19034 Ricardo and 19045 Ricardo) are responses
to earlier comments made by Josephina (18980
Josephina), and that fact is signaled by Ricardo in-
voking Josephena?s name. This is an example of
social correlation: lexicalized references to identity
can also provide useful evidence. If we take so-
cial and temporal context into account, we should be
able to do better at recognizing conversations than
we could using lexical overlap alone.
In recent years, several approaches have been de-
veloped for detecting conversational threads in dy-
namic text streams (Elsner et al, 2008; Shen et
al., 2006; Wang et al, 2008). Although they use
both lexical and non-lexical information (e.g., time,
name mentions in message) for this task, they have
ignored the temporal and social contexts a message
appears in, which provide valuable cues for inter-
preting the message. Correlation clustering used in
a two-step approach (Elsner et al, 2008) exploits
message contexts to some degree, but its perfor-
mance is largely limited by the classifier used in the
first-step which computes message similarity with-
out considering the temporal and social contexts of
each message.
Our approach exploits contextual properties (both
explicit and hidden) to probabilistically expand each
message to provide a more accurate message rep-
resentation. The new representation leads to a much
improved performance for conversation disentangle-
ment. We note that this is a general approach and can
be applied to the representation of non-chat data that
exhibits temporal and social correlations as well.
The results that we obtain with this technique are
close to the limit of what we can measure using
present test collections and evaluation measures. To
the best of our knowledge, our work is the first to
apply document expansion to the conversation dis-
entanglement problem.
2 Related Work
Previous work in conversation disentanglement
(i.e. thread detection) has shown the conven-
tional lexical-based clustering is not suitable for text
streams because messages are often too short and
incomplete. They focus on using discourse/chat-
specific features to bias the lexical-based message
similarity (Elsner et al, 2008; Shen et al, 2006;
Wang et al, 2008). These features provide the
means to link messages that may not have sufficient
lexical overlap but are nevertheless likely to be top-
ically related. However, our work is different from
them in several aspects:
(1) They treat individual messages as the basic ele-
ments for clustering, and ignore the social and tem-
poral contexts of the messages. In our work, each
message is probabilistically expanded using reliable
information from its contexts and the expanded mes-
sages are the basic elements for clustering.
(2) Messages have different amount of explicit infor-
mation. For example, messages that initiate conver-
sations may have more name mentions than subse-
quent messages (i.e. for establishing conversations).
Previous work only uses what are explicitly present
in each message, and clusters may be erroneously
assigned for messages that lack enough explicit in-
201
formation. Our work exploits both explicit and im-
plicit context for each message due to how we define
contexts (Section 3.2.1).
(3) Most work imposes a fixed window size for clus-
tering and it may break up long conversations or may
not be fine-grained enough for short conversations.
Given each message, we use an exponential decay
model to naturally encode time effect and assign dif-
ferential weights to messages in its contexts.
Another thread of related work is document ex-
pansion. It was previously studied in (Singhal et al,
1999) in the context of the speech retrieval, helping
to overcome limitations in the transcription accuracy
by selecting additional terms from lexically simi-
lar (text) documents. Document expansion has also
been applied to cross-language retrieval in (Levow
et al, 2005), in that case to overcome limitations
in translation resources. The technique has recently
been re-visited (Tao et al, 2006; Kurland et al,
2004; Liu et al, 2004) in the language modeling
framework, where lexically related documents are
used to enlarge the sample space for a document
to improve the accuracy of the estimated document
language model. However, these lexical-based ap-
proaches are less well suited to conversational in-
teraction, because conversational messages are often
short, they therefore may not overlap sufficiently in
words with other messages to provide a useful basis
for expansion. Our technique can be viewed as an
extension of these previous methods to text streams.
Our work is also related to text segmentation (Ji
et al, 2003) and meeting segmentation (Malioutov
et al, 2006; Malioutov et al, 2007; Galley et al,
2003; Eisenstein et al, 2008). Text segmentation
identifies boundaries of topic changes in long text
documents, but we form threads of messages from
streams consisting of short messages. Meeting con-
versations are not as highly interleaving as chat con-
versations, where participants can create a new con-
versation at any time.
3 Method
This section describes our technique for clustering
messages into threads based on the lexical similar-
ity of documents that have been expanded based on
social and temporal evidence.
3.1 Context-Free Message Model
To represent the semantic information of messages
and threads (clusters of messages), most of the prior
approaches build a document representation on each
message alone (using word features and time-stamp
and/or discourse features found in the message). We
call such a model a context-free message model.
Most commonly, a message is represented as a vec-
tor (Salton, 1989). Each dimension corresponds to
a separate term. If a term occurs in the message,
its value in the vector is non-zero. Several dif-
ferent ways of computing these values, known as
term weights, have been developed. One of the best
known schemes is tf-idf weighting.
However, in conversational text, a context-free
model cannot fully capture the semantics of mes-
sages. The meaning of a message is highly depen-
dent on other messages in its context. For example,
in our running example in Figure 1, to fully interpret
the message 19045 Ricardo, we need to first read
his previous message (19034 Ricardo) to Josephina.
Further, messages on the same topic may have lit-
tle or no overlap in words (Figure 1), and the mes-
sages between participants are highly interactive and
are often too short and incomplete to fully capture a
topic on their own.
3.2 Context-Sensitive Message Model
Our main idea is to exploit the temporal and so-
cial aspects of the conversations to build a context-
sensitive document model for each message. We
do this by first identifying the temporal and social
contexts for each message, then probabilistically ex-
panding the content of each message with selected
messages in each context. As we have seen, a mes-
sage?s contexts provide valuable cues for interpret-
ing the message. Finally, we cluster the messages
into distinct conversations based on their new repre-
sentation models.
We present the formal definitions of each context
and discuss how to model them in Section 3.2.1. In
Section 3.2.2, we show how to efficiently identify
the related messages in each context, and how to use
them to expand our representation of the message.
3.2.1 Social and Temporal Contexts
Social contexts: we define two kinds of social con-
texts: author context and conversational context. We
202
?400 ?200 0 200 4000
0.20.4
0.60.8
1
Time diff. between messages from same authorP
robabili
ty in sa
me thre
ad
?400 ?200 0 200 4000
0.20.4
0.60.8
1
Time difference between name mentionP
robabili
ty in sa
me thre
ad
?400 ?200 0 200 4000
0.20.4
0.60.8
1
Time difference between message pairsPr
obabilit
y in sam
e thread
(i) (ii) (iii)
Figure 2: (i) Relationship between messages from the same author (ii) Relationship between messages that mention
each other?s authors, and (iii) All pairs of messages as a function of time. Estimation is based on training data used in
experiments.
explain them in detail below.
Author context: the author context of a message
m, denoted by CA(m), is the set of other messages
written by m?s author am:
CA(m) = {mi|ami = am,m 6= mi}
Further, because of the nature of human conversa-
tions, we would be less surprised to find messages
from the same person belonging to the same conver-
sation if they are close in time rather than far apart.
This is illustrated in Figure 2(i) 1, which shows the
probability that a pair of messages written by the
same person belong to the same conversation as a
function of the time difference between them. Not
surprisingly, messages in m?s author context have
probabilities which are influenced by their temporal
proximity to m.
We use a normal distribution (Figure 2(i)) to en-
code the notion of author context. Given two mes-
sages mi and mj written by the same author, each
with time-stamp ti and tj , respectively, the proba-
bility that mj is topically related to mi given their
time difference d = tj ? ti is:
Pa(d) = N(?a, ?2a) = 1?a
?2pie
? (d??a)
2
2?2a
The exponential decay helps to limit the influence
from temporally remote messages. For message mi,
this distribution models the uncertainty that mes-
sages in its author context (i.e. other messages mj
from the same author) belong to the same conver-
sation by assigning assigning a high value to mj if
1Gaussian kernels shown for illustration purpose in Figure 2
are un-normalized.
tj ? ti is small. The mean ?a is chosen to be zero so
that the curve is centered at each message. The vari-
ance can be readily estimated from training data.
Conversational context: the second kind of so-
cial context is the conversational context, which is
constructed from name mentions. As pointed out by
previous linguistic studies of discourse, especially
analysis of multi-party conversation (ONeill et al,
2003), one key difference between multi-party con-
versation and typical two-party conversation is the
frequency with which participants mention each oth-
ers? names. Name mentioning is hypothesized as a
strategy for participants to compensate for the lack
of cues normally present in face-to-face dialogue
(ONeill et al, 2003; Elsner et al, 2008). Although
infrequent, name mentions (such as Azzie?s com-
ments to Ricardo in Figure 1) provide a means for
linking two speakers and their messages.
The conversational context of m, CC(m), is de-
fined to be the set of all messages written by peo-
ple whose names are mentioned in any of am?s mes-
sages (where am is the author of m), or who mention
am in their messages. Let Ma denote all messages
written by author a. The conversational context of
m is:
CC(m) = {?a Ma|mention(am, a)}
? {?a Ma|mention(a, am)}
where mention(am, a) = true if author am men-
tions a in any of am?s messages. Mention(a, am)
is similarly defined.
Discussion: From the definition, mj is included in
mi?s conversational context if the author of mi men-
203
tions the author of mj in any of mi?s messages, or
vice versa. For instance, the conversational con-
text for Ricardo?s message (19034 Ricardo) in Fig-
ure 1 includes the messages from Josephina (18980
Josephina) due to the mentioning of Josephina in
his message. However, it may well be the case that
mi does not contain any name mentions, e.g. Ri-
cardo?s message to Azzie (18939 Ricardo). In this
case, if Ricardo is being mentioned by another au-
thor (here Azzie asks Ricardo a question by start-
ing with his name in 18939 Azzie), message (18939
Ricardo)?s conversational context will contain all of
Azzie?s messages (18911 and 18970 Azzie) accord-
ing to the above definition. This intuitively captures
the implicit question-answer patterns in conversa-
tional speech: Ricardo?s subsequent answer is a re-
sponse to Azzie?s comments, hence they are in each
other?s conversational context.
Our definition also accounts for another source of
implicit context. In interactive conversations name
mention is a tool for getting people?s attention and
starting a conversation. Once a participant ai estab-
lishes a conversation with aj (such that ai may men-
tion aj?s name in an initial message mp to aj), ai
may stop mentioning aj?s name in subsequent mes-
sages (mq) to aj . This is illustrated in Ricardo?s last
message to Josephina in Figure 1. Our definition
accounts for the conversation continuity between aj
and ai by including messages from aj in the conver-
sational context of subsequent messages mq from ai
(note mq may or may not mention aj). For instance,
message 19045 Ricardo continues the conversation
with Josephina from 19034 Ricardo, message 19045
Ricardo thus has Josephina?s messages as part of its
conversational context.
In general, a person can participate in multiple
conversations over time, but as time goes on the
topic of interest may shift and the person may start
talking to other people. So the messages in the con-
versational context of mi due to earlier discussions
with other people should be assigned a lower con-
fidence value for mi. For example, five hours later
Ricardo may still be active, but it is unlikely he still
chats with Josephina on the same topic, so the ear-
lier messages by Josephina should receive a small
confidence value in the conversational context of Ri-
cardo?s later messages. We illustrate this idea in Fig-
ure 2(ii). It shows the probability that message mj ,
where mj ? CC(mi), belongs to the same thread
as mi, given their time difference tj ? ti. This
is encoded with a normal probability distribution,
N(?c, ?c) where ?c = 0 and variance is estimated
from training data. Let d = tj ? ti, the probability
they are topically related given mj ? CC(mi) is:
Pc(d) = 1?c
?2pie
? d2
2?2c
Temporal context: temporal context for message
m, CT (m), refers to all other messages:
CT (m) = M \m
where M denotes the entire set of messages. The
intuition is that nearby messages to m can provide
further evidence to the semantics of m. This is illus-
trated in Figure 2(iii). From the viewpoint of doc-
ument smoothing, this can also be regarded as us-
ing temporally nearby messages to smooth the rep-
resentation of m. So given mi, we again model its
temporal context by fitting a normal probability dis-
tribution N(?t, ?t), so that if mj ? CT (mi) and
d = tj ? ti, the probability that mj is topically re-
lated to mi is:
Pt(d) = 1?t
?2pie
? d2
2?2t
3.2.2 Constructing Expanded Messages
We have shown how to use the social and tem-
poral aspects of conversational text to identify and
model the contexts of each message, and how to
assign confidence values to messages in its con-
texts. We now show how to use a message?s con-
texts and their associated messages to probabilisti-
cally expand the given message. We hypothesize
that the expanded message provides a more accurate
message representation and that this improved repre-
sentation can lead to improved accuracy for conver-
sation disentanglement. We will test this hypothesis
in the experiment section.
Each message m is represented as a vector of es-
timated term counts. We expand m using the nor-
malized messages in its contexts. For the expanded
message m? of m we estimate the term counts as a
linear mixture of term counts from each message in
204
each context:
c(w,m?) = ?c(w,m) + (1? ?){
?C
?
mj?CC(m)
Pc(dji)? c(w,mj)
+ ?A
?
mj?CA(m)
Pa(dji)? c(w,mj)
+ ?T
?
mj?CT (m)
Pt(dji)? c(w,mj)}
These parameter values are tuned on training data:
? controls how much relative weight we give to lex-
ical content of m (0.45 in our experiments), and
?C , ?A and ?T are the relative weights assigned
to the conversational, author and temporal contexts
(0.6, 0.3, and 0.1 in our experiments, respectively).
A context with large variance in its normal density
graph should receive a small ? value. This is be-
cause a large variance in context k implies more un-
certainty on a message mj being topically related to
m while mj is in the context k of m. In Figure 2,
the conversational context (Figure 2(ii)) has the min-
imum variance among all contexts, hence, it is more
accurate for linking messages related in topic and it
is assigned a higher ? value (0.6), while the tempo-
ral context has the lowest ? value (0.1). Finally, for
a message mj in context k of mi, Pk(dji) indicates
how strongly we believe mj is topically related to
mi, given their time difference dji.
Because of the exponential decays of the normal
densities that model contexts k, messages in a con-
text will contribute differentially to mi. Temporally
distant messages will have a very low density.
3.3 Single-Pass Clustering
The expanded messages are the basic elements for
clustering. The cosine is used to measure similarity:
sim(mi,mj) =
?
w
c(w,mi)c(w,mj)
?mi??mj?
Single-pass clustering is then performed: treat the
first message as a single-message cluster T ; for each
remaining message m compute ?T :
sim(m,T ) = maxmi?T sim(mi,m)
For the thread T that maximizes sim(m,T ), if
sim(m,T ) > tsim, where tsim is a threshold (0.7 in
Min Mean Max
Number of Conversations 50.00 81.33 128.00
Avg. Conv. Length 6.20 10.60 16.00
Avg. Conv. Density 2.53 2.75 2.92
Table 1: Statistics on the IRC chat transcript data (Elsner
et al, 2008). The reported values are based on annota-
tions from six different annotations for the 800 lines of
chat transcript.
our experiments) empirically estimated from train-
ing data, add m to T ; else, start a new cluster con-
taining only m. The time complexity of this algo-
rithm is O(n2), which is tractable for problems of
moderate size.
4 Experiments
The collection used in the experiments consists of
real text streams produced in Internet Relay Chat,
created by (Elsner et al, 2008) and annotated inde-
pendently by six annotators. As an upper (human)
baseline for each of the three measures reported be-
low, we report the average agreement between all
pairs of annotators (i.e., treating one annotator as
truth and another as a ?system?). For our experi-
ment results, we report the average across all anno-
tators of the agreement between our system and each
annotator.
The test collection also contains both a develop-
ment set and an evaluation set. We used the devel-
opment set to approximate the normal densities used
in our context models and the evaluation set to ob-
tain the results reported below. Some statistics for
the 800 annotated messages in the chat transcript of
the evaluation collection are shown in Table 1. As
that table shows, the average number of active con-
versation at a given time is 2.75, which makes thread
detection a non-trivial task.
4.1 Evaluation Measures
We conduct comparisons using three commonly
used evaluation measures for the thread detection
task. As a measure of the systems ability to group
related messages we report the F -measure (Shen et
al., 2006):
F =?
i
ni
n maxj(F (i, j))
205
where i is a ground-truth conversation with length
ni, and n is the length of entire transcript. F (i, j)
is the harmonic mean of recall (fraction of the mes-
sages in the i also present in j) and precision (frac-
tion of messages in j also present in i), and F is
a weighted sum over all ground-truth conversations
(i.e., F is microaveraged).
Two other evaluation measures are ?one-to-one
accuracy? and ?local agreement? (Elsner et al,
2008). ?One-to-one accuracy? measures how well
we extract whole conversations intact (e.g., as might
be required for summarization). It is computed by
finding the max-weight bipartite matching between
the set of detected threads and the set of real threads,
where weight is defined in terms of percentage over-
laps for each ground truth and detected thread pair.
Some applications (e.g., real-time monitoring)
may not require that we look at entire conversations
ar once; in this case a ?local agreement? measure
might make more sense. ?loc3? between system and
human annotations as the average (over all possible
sets of three consecutive messages) of whether those
3 consecutive messages are assigned consistently by
the ground truth and the system. For example, if
both the ground truth and the system cluster the first
and third messages together and place the second
message in a different cluster, then agreement would
be recorded.
4.2 Methods Used in Comparison
We compare with the following methods:
Elsner et al 2008 (best previously known tech-
nique): Message similarity is computed with lexical
and discourse features, but without document
expansion.
Blocks of k: Every consecutive group of k messages
is a conversation.
Pause of k: Every pause of k seconds or more
separate two conversations.
Speaker: Each speaker?s messages are treated as a
single conversation.
All different: Each utterance is a separate thread.
All same: The entire transcript is one conversation.
4.3 Results
Figure 3 compares the effectiveness of different
schemes in terms of the F measure. We show results
Figure 3: F measure. The dotted line represents inter-
annotator agreement.
from the best baseline, Elsner and our technique
(which we call the Context model). The average F
between human annotators is shown with the dotted
line at 0.55; we would expect this to be an upper
bound for any model. Our method substantially out-
performs the other methods, with a 24% improve-
ment over Elsner and 48% improvement over the
best baseline (speaker). Viewed another way, our
system achieves 98% of human performance, while
Elsner and the best baseline achieve 79% and 66% of
that bound, respectively. From this, we can conclude
that our Context model is quite effective at cluster-
ing messages from same conversation together.
To illustrate the impact of conversation length,
we binned the lengths of ground-truth conversations
from a single assessor into bins of size 5 (i.e., 3?7
messages, 8?12 messages, . . .; there were no ground
truth bins of size 1 or 2). Figure 4 plots the approx-
imated microaveraged F at the center value of each
bin (i.e., the F for each ground truth cluster, scaled
by the number of messages in the cluster). These
fine-grained values provide insight into the contri-
bution of conversations of different sizes to the over-
all microaveraged F . The Context model performs
well for every conversation length, but particularly
so for conversations containing 35 or more messages
as shown by the widened gap in that region. Long
conversations usually have richer social and tempo-
ral contexts for each message. The context model
can benefit more from drawing evidences from these
sources and using them to expand the message, thus
makes it possible to group messages of the same
206
Figure 4: Dependence of F on ground-truth conversation
size, in number of messages.
Figure 5: One-to-one measure. The dotted line represents
inter-annotator agreement.
conversation together. The other two methods that
ignore contextual properties do not do well in com-
parison.
To measure how well we extract whole conversa-
tions intact, Figure 5 shows the results in terms of
the one-to-one measure, where each real conversa-
tion is matched up with a distinct detected conversa-
tion thread. It is computed by max-weight bipartite
matching such that the total message overlap is max-
imized between the sets of detected threads and real
threads. The average by this measure between hu-
man annotators is 0.53. In this case, the proposed
context model achieves an 14% increase over El-
sner and 32% increase over the best baseline, and
it is within 88% of human performance. This fairly
clearly indicates that our Context model can disen-
tangle interleaved conversations relatively well.
Finally, Figure 6 presents the results for ?local-3?
to evaluate the system?s ability to do local annota-
Figure 6: Local-3 measure. The dotted line represents
inter-annotator agreement.
tions. The difference between the best baseline and
maximum upper bound is small, implying limited
room for potential improvement by any non-baseline
techniques. Our result again compares favorably
with the previously reported result and the best base-
line, although with a smaller margin of 20% over the
best baseline and 3% over Elsner as a result of the
relatively high baseline for this measure.
5 Conclusion and Future Work
We have presented an approach that exploits contex-
tual properties to probabilistically expand each mes-
sage to provide a more accurate message represen-
tation for dynamic conversations. It is a general ap-
proach and can be applied to the representation of
non-chat data that exhibits temporal and social cor-
relations as well. For conversation disentanglement,
it outperforms the best previously known technique.
Our work raises three important questions: (1) to
what extent is the single test collection that we have
used representative of the broad range of ?text chat?
applications?, (2) to what extent do the measures we
have reported correlate to effective performance of
downstream tasks such as summarization or auto-
mated response?, and (3) can we re-conceptualize
the formalized problem in a way that would result
in greater inter-annotator agreement, and hence pro-
vide scope for further refinements in our technique.
These problems will be the focus of our future work.
207
References
Micha Elsner and Eugene Charniak. 2008. You talk-
ing to me? A Corpus and Algorithm for Conversa-
tion Disentanglement. In ACL 2008: Proceedings of
the 46th Annual Meeting on Association for Compu-
tational Linguistics, pages 834-842, Columbus, OH,
USA. Association for Computational Linguistics.
Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.
2006. Thread Detection in Dynamic Text Mes-
sage Streams. In SIGIR 2006: Proceedings of the
29th annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 35-42, Seattle, WA, USA. Association for Com-
puting Machinery.
Yi-Chia Wang, Mahesh Joshi, William Cohen, and Car-
olyn Rose. 2008. Recovering Implicit Thread Struc-
ture in Newsgroup Style Conversations. In ICWSM
2008: Proceedings of the 2nd International Confer-
ence on Weblogs and Social Media, pages 152-160,
Seattle, WA, USA. Association for the Advancement
of Artificial Intelligence.
Tao Tao, Xuanhui Wang, Qiaozhu Mei, and ChengXi-
ang Zhai. 2006. Language Model Information Re-
trieval with Document Expansion. In HLT-NAACL
2006: Proceedings of the Human Language Technol-
ogy Conference of the North American Chapter of the
ACL, pages 407-414, New York, NY, USA. Associa-
tion for Computational Linguistics.
Oren Kurland and Lillian Lee. 2004. Corpus Structure,
Language Models, and AdHoc Information Retrieval.
In SIGIR 2004: Proceedings of the 27th annual in-
ternational ACM SIGIR conference on Research and
development in information retrieval, pages 194-201,
Sheffield, UK. Association for Computing Machinery.
Xiaoyong Liu and W Croft. 2004. Cluster-based Re-
trieval Using Language Models. In SIGIR 2004: Pro-
ceedings of the 27th annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 186-193, Sheffield, UK. Associa-
tion for Computing Machinery.
Amit Singhal and Fernando Pereira. 1999. Document
Expansion for Speech Retrieval. In SIGIR 1999: Pro-
ceedings of the 22nd annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 34-41, Berkeley, CA, USA. Asso-
ciation for Computing Machinery.
Xiang Ji and Hongyuan Zha 2003. Domain-Independent
Text Segmentation using Anisotropic Diffusion and
Dynamic Programming. In SIGIR 2003: Proceedings
of the 26th annual international ACM SIGIR confer-
ence on Research and development in information re-
trieval, pages 322-329, Toronto, Canada. Association
for Computing Machinery.
Michel Galley, Kathleen McKeown, Eric Lussier, and
Hongyan Jing. 2003. Discourse Segmentation of
Multi-Party Conversation. In ACL 2003: Proceed-
ings of the 41st Annual Meeting of the Association for
Computational Linguistics, pages 562-569, Sapporo,
Japan. Association for Computational Linguistics.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
Unsupervised Topic Segmentation. In EMNLP 2008:
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 334-
343, Honolulu, Hawaii, USA. Association for Compu-
tational Linguistics.
Igor Malioutov and Regina Barzilay 2006. Minimum-
Cut Model for Spoken Lecture Segmentation. In ACL
2006: Proceedings of the 44rd Annual Meeting of the
Association for Computational Linguistics, pages 25-
32, Sydney, Australia. Association for Computational
Linguistics.
Igor Malioutov, Alex Park, Regina Barzilay, and James
Glass. 2007. Making Sense of Sound: Unsuper-
vised Topic Segmentation over Acoustic Input. In ACL
2007: Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 504-
511, Prague, Czech Republic. Association for Compu-
tational Linguistics.
Jen-Yuan Yeh and Aaron Harnly. 2006. Email Thread
Reassembly Using Similarity Matching. In CEAS
2006: The 3rd Conference on Email and Anti-Spam,
pages 64-71, Mountain View, CA, USA.
Jacki ONeill and David Martin. 2003. Text Chat in Ac-
tion. In ACM SIGGROUP 2003: Proceedings of the
2003 International ACM SIGGROUP Conference on
Supporting Group Work, pages 40-49, New York, NY,
USA. ACM Press.
Gerard Salton. 1989. Automatic Text Processing: the
Transformation, Analysis and Retrieval of Information
by Computer. Addison-Wesley Longman Publishing
Co., Inc., Boston, MA, USA, 1989.
Gina-Anne Levow, Douglas Oard, and Philip Resnik.
2005. Dictionary-based techniques for cross-language
information retrieval. In Information Processing and
Management Special Issue: Cross-Language Informa-
tion Retrieval, 41(3): 523-547.
208
Proceedings of ACL-08: HLT, pages 941?949,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Resolving Personal Names in Email Using Context Expansion
Tamer Elsayed,?Douglas W. Oard,? and Galileo Namata?
Human Language Technology Center of Excellence and
UMIACS Laboratory for Computational Linguistics and Information Processing (CLIP)
University of Maryland, College Park, MD 20742
{telsayed, oard, gnamata}@umd.edu
Abstract
This paper describes a computational ap-
proach to resolving the true referent of a
named mention of a person in the body of an
email. A generative model of mention gener-
ation is used to guide mention resolution. Re-
sults on three relatively small collections indi-
cate that the accuracy of this approach com-
pares favorably to the best known techniques,
and results on the full CMU Enron collection
indicate that it scales well to larger collections.
1 Introduction
The increasing prevalence of informal text from
which a dialog structure can be reconstructed (e.g.,
email or instant messaging), raises new challenges if
we are to help users make sense of this cacophony.
Large collections offer greater scope for assembling
evidence to help with that task, but they pose addi-
tional challenges as well. With well over 100,000
unique email addresses in the CMU version of the
Enron collection (Klimt and Yang, 2004), common
names (e.g., John) might easily refer to any one of
several hundred people. In this paper, we associate
named mentions in unstructured text (i.e., the body
of an email and/or the subject line) to modeled iden-
tities. We see at least two direct applications for this
work: (1) helping searchers who are unfamiliar with
the contents of an email collection (e.g., historians or
lawyers) better understand the context of emails that
they find, and (2) augmenting more typical social
networks (based on senders and recipients) with ad-
ditional links based on references found in unstruc-
tured text.
Most approaches to resolving identity can be de-
composed into four sub-problems: (1) finding a ref-
erence that requires resolution, (2) identifying can-
didates, (3) assembling evidence, and (4) choosing
?Department of Computer Science
?College of Information Studies
among the candidates based on the evidence. For
the work reported in this paper, we rely on the user
to designate references requiring resolution (which
we model as a predetermined set of mention-queries
for which the correct referent is known). Candidate
identification is a computational expedient that per-
mits the evidence assembly effort to be efficiently
focused; we use only simple techniques for that task.
Our principal contributions are the approaches we
take to evidence generation (leveraging three ways
of linking to other emails where evidence might be
found: reply chains, social interaction, and topical
similarity) and our approach to choosing among can-
didates (based on a generative model of reference
production). We evaluate the effectiveness of our
approach on four collections, three of which have
previously reported results for comparison, and one
that is considerably larger than the others.
The remainder of this paper is as follows. Sec-
tion 2 surveys prior work. Section 3 then describes
our approach to modeling identity and ranking can-
didates. Section 4 presents results, and Section 5
concludes.
2 Related Work
The problem of identity resolution in email is a spe-
cial case of the more general problem referred to as
?Entity Resolution.? Entity resolution is generically
defined as a process of determining the mapping
from references (e.g., names, phrases) observed in
data to real-world entities (e.g., persons, locations).
In our case, the problem is to map mentions in emails
to the identities of the individuals being referred to.
Various approaches have been proposed for en-
tity resolution. In structured data (e.g., databases),
approaches have included minimizing the number
of ?matching? and ?merging? operations (Benjel-
loun et al, 2006), using global relational informa-
tion(Malin, 2005; Bhattacharya and Getoor, 2007;
Reuther, 2006) and using a probabilistic generative
941
model (Bhattacharya and Getoor, 2006). None of
these approaches, however, both make use of con-
versational, topical, and time aspects, shown impor-
tant in resolving personal names (Reuther, 2006),
and take into account global relational informa-
tion. Similarly, approaches in unstructured data
(e.g., text) have involved using clustering techniques
over biographical facts (Mann and Yarowsky, 2003),
within-document resolution (Blume, 2005), and dis-
criminative unsupervised generative models (Li et
al., 2005). These too are insufficient for our prob-
lem since they suffer from inability scale or to han-
dle early negotiation.
Specific to the problem of resolving mentions in
email collections, Abadi (Abadi, 2003) used email
orders from an online retailer to resolve product
mentions in orders and Holzer et al (Holzer et al,
2005) used the Web to acquire information about
individuals mentioned in headers of an email col-
lection. Our work is focused on resolving personal
name references in the full email including the mes-
sage body; a problem first explored by Diehl et al
(Diehl et al, 2006) using header-based traffic anal-
ysis techniques. Minkov et al(Minkov et al, 2006)
studied the same problem using a lazy graph walk
based on both headers and content. Those two re-
cent studies reported results on different test collec-
tions, however, making direct comparisons difficult.
We have therefore adopted their test collections in
order to establish a common point of reference.
3 Mention Resolution Approach
The problem we are interested in is the resolution
of a personal-name mention (i.e., a named reference
to a person) m, in a specific email em in the given
collection of emails E, to its true referent. We as-
sume that the user will designate such mention. This
can be formulated as a known-item retrieval problem
(Allen, 1989) since there is always only one right an-
swer. Our goal is to develop a system that provides a
list of potential candidates, ranked according to how
strongly the system believes that a candidate is the
true referent meant by the email author. In this pa-
per, we propose a probabilistic approach that ranks
the candidates based on the estimated probability of
having been mentioned. Formally, we seek to esti-
mate the probability p(c|m) that a potential candi-
date c is the one referred to by the given mention m,
over all candidates C.
We define a mention m as a tuple < lm, em >,
where lm is the ?literal? string of characters that rep-
resentsm and em is the email wherem is observed.1
We assume that m can be resolved to a distinguish-
able participant for whom at least one email address
is present in the collection.2
The probabilistic approach we propose is moti-
vated by a generative scenario of mentioning people
in email. The scenario begins with the author of the
email em, intending to refer to a person in that email.
To do that s/he will:
1. Select a person c to whom s/he will refer
2. Select an appropriate context xk to mention c
3. Select a specific lexical reference lm to refer to
c given the context xk.
For example, suppose ?John? is sending an email
to ?Steve? and wants to mention a common friend
?Edward.? ?John? knows that he and Steve know
2 people named Edward, one is a friend of both
known by ?Ed? and the other is his soccer trainer.
If ?John? would like to talk about the former, he
would use ?Ed? but he would likely use ?Edward?
plus some terms (e.g., ?soccer?, ?team?, etc) for the
latter. ?John? relies on the social context, or the topi-
cal context, for ?Steve? to disambiguate the mention.
The steps of this scenario impose a certain struc-
ture to our solution. First, we need to have a
representational model for each candidate identity.
Second, we need to reconstruct the context of the
queried mention. Third, it requires a computational
model of identity that supports reasoning about iden-
tities. Finally, it requires a resolution technique that
leverages both the identity models and the context
to rank the potential candidates. In this section,
we will present our resolution approach within that
structure. We first discuss how to build both repre-
sentational and computational models of identity in
section 3.1. Next, we introduce a definition of the
contextual space and how we can reconstruct it in
1The exact position in em where lm is observed should also
be included in the definition, but we ignore it assuming that all
matched literal mentions in one email refer to the same identity.
2Resolving mentions that refer to non-participants is outside
the scope of this paper.
942
section 3.2. Finally, we link those pieces together
by the resolution algorithm in section 3.3.
3.1 Computational Model of Identity
Representation: In a collection of emails, indi-
viduals often use different email addresses, multi-
ple forms of their proper names, and different nick-
names. In order to track references to a person over
a large collection, we need to capture as many as
possible of these referential attributes in one rep-
resentation. We extend our simple representation
of identity proposed in (Elsayed and Oard, 2006)
where an identity is represented by a set of pair-
wise co-occurrence of referential attributes (i.e., co-
occurrence ?associations?), and each extracted as-
sociation has a frequency of occurrence. The at-
tributes are extracted from the headers and saluta-
tion and signature lines. For example, an ?address-
nickname? association < a, n > is inferred when-
ever a nickname n is usually observed in signature
lines of emails sent from email address a. Three
types of referential attributes were identified in the
original representation: email addresses, names, and
nicknames. We add usernames as well to account
for the absence of any other type of names. Names,
nicknames, and usernames are distinguishable based
on where each is extracted: email addresses and
names from headers, nicknames from salutation
and signature lines, and usernames from email ad-
dresses. Since (except in rare cases) an email ad-
dress is bound to one personal identity, the model
leverages email addresses as the basis by mandat-
ing that at least one email address must appear in
any observed association. As an off-line preprocess-
ing step, we extract the referential attributes from the
whole collection and build the identity models. The
first step in the resolution process is to determine the
list of identity models that are viable candidates as
the true referent. For the experiments reported in this
paper, any identity model with a first name or nick-
name that exactly matches the mention is considered
a candidate.
Labeling Observed Names: For the purpose of re-
solving name mentions, it is necessary to compute
the probability p(l|c) that a person c is referred to by
a given ?literal? mention l. Intuitively, that probabil-
ity can be estimated based on the observed ?name-
type? of l and how often that association occurs in
the represented model. We define T as the set of
3 different types of single-token name-types: first,
last, and nickname. We did not handle middle names
and initials, just for simplicity. Names that are ex-
tracted from salutation and signature lines are la-
beled as nicknames whereas full names extracted
from headers are first normalized to ?First Last?
form and then each single token is labeled based on
its relative position as being the first or last name.
Usernames are treated similarly to full names if they
have more than one token, otherwise they are ig-
nored. Note that the same single-token name may
appear as a first name and a nickname.
Figure 1: A computational model of identity.
Reasoning: Having tokenized and labeled all
names, we propose to model the association of a
single-token name l of type t to an identity c by a
simple 3-node Bayesian network illustrated in Fig-
ure 1. In the network, the observed mention l is
distributed conditionally on both the identity c and
the name-type t. p(c) is the prior probability of ob-
serving the identity c in the collection. p(t|c) is the
probability that a name-type t is used to refer to c.
p(l|t, c) is the probability of referring to c by l of
type t. These probabilities can be inferred from the
representational model as follows:
p(c) =
|assoc(c)|
?
c??C |assoc(c
?)|
p(t|c) =
freq(t, c)
?
t??T freq(t
? , c)
p(l|t, c) =
freq(l, t, c)
?
l??assoc(c) freq(l
? , t, c)
where assoc(c) is the set of observed associations of
referential attributes in the represented model c.
The probability of observing a mention l given
that it belongs to an identity c, without assuming a
specific token type, can then be inferred as follows:
p(l|c) =
?
t?T
p(t|c) p(l|t, c)
In the case of a multi-token names (e.g., John
Smith), we assume that the first is either a first name
943
or nickname and the last is a last name, and compute
it accordingly as follows:
p(l1l2|c) = {
?
t?{f,n}
p(t|c) p(l1|t, c)} ? p(l2|last, c)
where f and n above denotes first name and nick-
name respectively.
Email addresses are also handled, but in a differ-
ent way. Since we assume each of them uniquely
identifies the identity, all email addresses for one
identity are mapped to just one of them, which then
has half of the probability mass (because it appears
in every extracted co-occurrence association).
Our computational model of identity can be
thought of as a language model over a set of per-
sonal references and thus it is important to account
for unobserved references. If we know that a spe-
cific first name often has a common nickname (by a
dictionary of commonly used first to nickname map-
pings (e.g., Robert to Bob)), but this nickname was
not observed in the corpus, we will need to apply
smoothing. We achieve that by assuming the nick-
name would have been observed n times where n is
some fraction (0.75 in our experiments) of the fre-
quency of the observed name. We repeat that for
each unobserved nickname and then treat them as if
they were actually observed.
3.2 Contextual Space
Figure 2: Contextual Space
It is obvious that understanding the context of an
ambiguous mention will help with resolving it.
Fortunately, the nature of email as a conversa-
tional medium and the link-relationships between
emails and people over time can reveal clues that can
be exploited to partially reconstruct that context.
We define the contextual space X(m) of a men-
tion m as a mixture of 4 types of contexts with ?k as
the mixing coefficient of context xk. The four con-
texts (illustrated in Figure 2) are:
(1) Local Context: the email em where the named
person is mentioned.
(2) Conversational Context: emails in the broader
discussion that includes em, typically the thread that
contains it.
(3) Social Context: discussions that some or all of
the participants (sender and receivers) of em joined
or initiated at around the time of the mention-email.
These might bear some otherwise-undetected rela-
tionship to the mention-email.
(4) Topical Context: discussions that are topically
similar to the mention-discussion that took place at
around the time of em, regardless of whether the dis-
cussions share any common participants.
These generally represent a growing (although not
strictly nested) contextual space around the queried
mention. We assume that all mentions in an email
share the same contextual space. Therefore, we can
treat the context of a mention as the context of its
email. However, each email in the collection has
its own contextual space that could overlap with an-
other email?s space.
3.2.1 Formal Definition
We define K as the set of the 4 types of contexts.
A context xk is represented by a probability distri-
bution over all emails in the collection. An email ej
belongs to the kth context of another email ei with
probability p(ej |xk(ei)). How we actually represent
each context and estimate the distribution depends
upon the type of the context. We explain that in de-
tail in section 3.2.2.
3.2.2 Context Reconstruction
In this section, we describe how each context is
constructed.
Local Context: Since this is simply em, all of the
probability mass is assigned to it.
Conversational Context: Threads (i.e., reply
chains) are imperfect approximations of focused
discussions, since people sometimes switch topics
within a thread (and indeed sometimes within the
same email). We nonetheless expect threads to ex-
hibit a useful degree of focus and we have there-
fore adopted them as a computational representation
of a discussion in our experiments. To reconstruct
threads in the collection, we adopted the technique
introduced in (Lewis and Knowles, 1997). Thread
944
reconstruction results in a unique tree containing the
mention-email. Although we can distinguish be-
tween different paths or subtrees of that tree, we
elected to have a uniform distribution over all emails
in the same thread. This also applies to threads re-
trieved in the social and topical contexts as well.
Social Context: Discussions that share common
participants may also be useful, though we expect
their utility to decay somewhat with time. To recon-
struct that context, we temporally rank emails that
share at least one participant with em in a time pe-
riod around em and then expand each by its thread
(with duplicate removal). Emails in each thread are
then each assigned a weight that equals the recip-
rocal of its thread rank. We do that separately for
emails that temporally precede or follow em. Fi-
nally, weights are normalized to produce one distri-
bution for the whole social context.
Topical Context: Identifying topically-similar con-
tent is a traditional query-by-example problem that
has been well researched in, for example, the TREC
routing task (Lewis, 1996) and the Topic Detection
and Tracking evaluations (Allan, 2002). Individual
emails may be quite terse, but we can exploit the
conversational structure to obtain topically related
text. In our experiments, we tracked back to the
root of the thread in which em was found and used
the subject line and the body text of that root email
as a query to Lucene3 to identify topically-similar
emails. Terms found in the subject line are dou-
bled in the query to emphasize what is sometimes
a concise description of the original topic. Subse-
quent processing is then similar to that used for the
social context, except that the emails are first ranked
by their topical, rather than temporal, similarity.
The approaches we adopted to reconstruct the so-
cial and topical contexts were chosen for their rel-
ative simplicity, but there are clearly more sophis-
ticated alternatives. For example, topic modeling
techniques (McCallum et al, 2005) could be lever-
aged in the reconstruction of the topical context.
3.3 Mention Resolution
Given a specific mention m and the set of identity
models C, our goal now is to compute p(c|m) for
each candidate c and rank them accordingly.
3http://lucene.apache.org
3.3.1 Context-Free Mention Resolution
If we resolve m out of its context, then we can
compute p(c|m) by applying Bayes? rule as follows:
p(c|m) ? p(c|lm) =
p(lm|c) p(c)
?
c??C p(l
m|c?) p(c?)
All the terms above are estimated as discussed ear-
lier in section 3.1. We call this approach ?backoff?
since it can be used as a fall-back strategy. It is con-
sidered the baseline approach in our experiments.
3.3.2 Contextual Mention Resolution
We now discuss the more realistic situation in
which we use the context to resolve m. By expand-
ing the mention with its context, we get
p(c|m) = p(c|lm, X(em))
We then apply Bayes? rule to get
p(c|lm, X(em)) =
p(c, lm, X(em))
p(lm, X(em))
where p(lm, X(em)) is the probability of observ-
ing lm in the context. We can ignore this probabil-
ity since it is constant across all candidates in our
ranking. We now restrict our focus to the numera-
tor p(c, lm, X(em)), that is the probability that the
sender chose to refer to c by lm in the contextual
space. As we discussed in section 3.2, X is defined
as a mixture of contexts therefore we can further ex-
pand it as follows:
p(c, lm, X(em)) =
?
k
?k p(c, l
m, xk(e
m))
Following the intuitive generative scenario we intro-
duced earlier, the context-specific probability can be
decomposed as follows:
p(c, lm, xk(e
m)) = p(c)
? p(xk(e
m)|c)
? p(lm|xk(e
m), c)
where p(c) is the probability of selecting a can-
didate c, p(xk(em)|c) is the probability of select-
ing xk as an appropriate context to mention c, and
p(lm|xk(em), c) is the probability of choosing to
mention c by lm given that xk is the appropriate con-
text.
Choosing person to mention: p(c) can be estimated
as discussed in section 3.1.
Choosing appropriate context: By applying Bayes?
rule to compute p(xk(em)|c) we get
p(xk(e
m)|c) =
p(c|xk(em)) p(xk(em))
p(c)
945
p(xk(em)) is the probability of choosing xk to gen-
erally mention people. In our experiments, we
assumed a uniform distribution over all contexts.
p(c|xk(em)) is the probability of mentioning c in
xk(em). Given that the context is defined as a distri-
bution over emails, this can be expanded to
p(c|xk(e
m)) =
?
ei?E
p(ei|xk(e
m) p(c|ei))
where p(c|ei) is the probability that c is mentioned
in the email ei. This, in turn, can be estimated us-
ing the probability of referring to c by at least one
unique reference observed in that email. By assum-
ing that all lexical matches in the same email refer to
the same person, and that all lexically-unique refer-
ences are statistically independent, we can compute
that probability as follows:
p(c|ei) = 1? p(c is not mentioned in ei)
= 1?
?
m??M(ei)
(1? p(c|m?))
where p(c|m
?
) is the probability that c is the true
referent of m
?
. This is the same general problem
of resolving mentions, but now concerning a related
mention m
?
found in the context of m. To handle
this, there are two alternative solutions: (1) break the
cycle and compute context-free resolution probabil-
ities for those related mentions, or (2) jointly resolve
all mentions. In this paper, we will only consider the
first, leaving joint resolution for future work.
Choosing a name-mention: To estimate
p(lm|xk(em), c), we suggest that the email au-
thor would choose either to select a reference (or a
modified version of a reference) that was previously
mentioned in the context or just ignore the context.
Hence, we estimate that probability as follows:
p(lm|xk(e
m), c) = ? p(lm ? xk(e
m)|c)
+(1? ?) p(lm|c)
where ? ? [0, 1] is a mixing parameter (set at 0.9
in our experiments), and p(lm|c) is estimated as in
section 3.1. p(lm ? xk(em)|c) can be estimated as
follows:
p(lm ? xk(e
m)|c) =
?
m??xk
p(lm|lm
?
)p(lm
?
|xk) p(c|l
m
?
)
where p(lm|lm
?
) is the probability of modifying lm
?
into lm. We assume all possible mentions of c
are equally similar to m and estimate p(lm|lm
?
) by
1
|possible mentions of c| . p(l
m
?
|xk) is the probability of
observing lm
?
in xk, which we estimate by its rel-
ative frequency in that context. Finally, p(c|lm
?
) is
again a mention resolution problem concerning the
reference ri which can be resolved as shown earlier.
The Aho-Corasick linear-time algorithm (Aho
and Corasick, 1975) is used to find mentions of
names, using a corpus-based dictionary that includes
all names, nicknames, and email addresses extracted
in the preprocessing step.
4 Experimental Evaluation
We evaluate our mention resolution approach using
four test collections, all are based on the CMU ver-
sion of the Enron collection; each was created by se-
lecting a subset of that collection, selecting a set of
query-mentions within emails from that subset, and
creating an answer key in which each query-mention
is associated with a single email address.
The first two test collections were created by
Minkov et al(Minkov et al, 2006). These test col-
lections correspond to two email accounts, ?sager-
e? (the ?Sager? collection) and ?shapiro-r? (the
?Shapiro? collection). Their mention-queries and
answer keys were generated automatically by iden-
tifying name mentions that correspond uniquely to
individuals referenced in the cc header, and elimi-
nating that cc entry from the header.
The third test collection, which we call the
?Enron-subset? is an extended version of the test
collection created by Diehl at al (Diehl et al, 2006).
Emails from all top-level folders were included
in the collection, but only those that were both
sent by and received by at least one email address
of the form <name1>.<name2>@enron.com were
retained. A set of 78 mention-queries were manu-
ally selected and manually associated with the email
address of the true referent by the third author using
an interactive search system developed specifically
to support that task. The set of queries was lim-
ited to those that resolve to an address of the form
<name1>.<name2>@enron.com. Names found in
salutation or signature lines or that exactly match
<name1> or <name2> of any of the email partic-
ipants were not selected as query-mentions. Those
78 queries include the 54 used by Diehl et al
946
Table 1: Test collections used in the experiments.
Test Coll. Emails IDs Queries Candidates
Sager 1,628 627 51 4 (1-11)
Shapiro 974 855 49 8 (1-21)
Enron-sub 54,018 27,340 78 152 (1-489)
Enron-all 248,451 123,783 78 518 (3-1785)
For our fourth test collection (?Enron-all?), we
used the same 78 mention-queries and the answer
key from the Enron-subset collection, but we used
the full CMU version of the Enron collection (with
duplicates removed). We use this collection to as-
sess the scalability of our techniques.
Some descriptive statistics for each test collection
are shown in Table 1. The Sager and Shapiro col-
lections are typical of personal collections, while
the other two represent organizational collections.
These two types of collections differ markedly in
the number of known identities and the candidate
list sizes as shown in the table (the candidate list
size is presented as an average over that collection?s
mention-queries and as the full range of values).
4.1 Evaluation Measures
There are two commonly used single-valued eval-
uation measures for ?known item?-retrieval tasks.
The ?Success @ 1? measure characterizes the ac-
curacy of one-best selection, computed as the mean
across queries of the precision at the top rank for
each query. For a single-valued figure of merit that
considers every list position, we use ?Mean Recip-
rocal Rank? (MRR), computed as the mean across
queries of the inverse of the rank at which the cor-
rect referent is found.
4.2 Results
There are four basic questions which we address in
our experimental evaluation: (1) How does our ap-
proach perform compared to other approaches?, (2)
How is it affected by the size of the collection and
by increasing the time period?, (3) Which context
makes the most important contribution to the resolu-
tion task? and (4) Does the mixture help?
In our experiments, we set the mixing coefficients
?k and the context priors p(xk) to a uniform distri-
bution over all reconstructed contexts.
To compare our system performance with results
Table 2: Accuracy results with different time periods.
Period MRR Success @ 1
(days) Prob. Minkov Prob. Minkov
10 0.899 0.889 0.843 0.804
Sager 100 0.911 0.889 0.863 0.804
200 0.911 0.889 0.863 0.804
10 0.913 0.879 0.857 0.779
Shapiro 100 0.910 0.879 0.837 0.779
200 0.911 0.837 0.878 0.779
10 0.878 - 0.821 -
Enron-sub 100 0.911 - 0.846 -
200 0.911 - 0.846 -
10 0.890 - 0.821 -
Enron-all 100 0.888 - 0.821 -
200 0.888 - 0.821 -
previously reported, we experimented with differ-
ent (symmetric) time periods for selecting threads
in the social and topical contexts. Three represen-
tative time periods, in days, were arbitrarily chosen:
10 (i.e., +/- 5) days, 100 (i.e., +/- 50) days, and 200
(i.e., +/- 100) days. In each case, the mention-email
defines the center of this period.
A summary of the our results (denoted by ?Prob.?)
are shown in Table 2 with the best results for each
test collection highlighted in bold. The table also in-
cludes the results reported in Minkov et al(Minkov
et al, 2006) for the small collections for comparison
purposes.4 Each score for our system was the best
over all combinations of contexts for these collec-
tions and time periods. Given these scores, our re-
sults compare favorably with the previously reported
results for both Sager and Shapiro collections.
Another notable thing about our results is that
they seem to be good enough for practical appli-
cations. Specifically, our one-best selection (over
all tried conditions) is correct at least 82% of the
time over all collections, including the largest one.
Of course, the Enron-focused selection of mention-
queries in every case is an important caveat on these
results; we do not yet know how well our techniques
will hold up with less evidence, as might be the case
for mentions of people from outside Enron.
It is encouraging that testing on the largest col-
4For the ?Enron-subset? collection, we do not know which
54 mention-queries Diehl et alused in (Diehl et al, 2006)
947
lection (with all unrelated and thus noisy data) did
not hurt the effectiveness much. For the three differ-
ent time periods we tried, there was no systematic
effect.
Figure 3: Individual contexts, period set to 100 days.
Individual Contexts: Our choice of contexts was
motivated by intuition rather than experiments, so
we also took this opportunity to characterize the
contribution of each context to the results. We
did that by setting some of the context mixing-
coefficients to zero and leaving the others equally-
weighted. Figure 3 shows the MRR achieved with
each context. In that figure, the ?backoff? curve in-
dicates how well the simple context-free resolution
would do. The difference between the two small-
est and the two largest collections is immediately
apparent?this backoff is remarkably effective for the
smaller collections, and almost useless for the larger
ones, suggesting that the two smaller collections are
essentially much easier. The social context is clearly
quite useful, more so than any other single context,
for every collection. This tends to support our ex-
pectation that social networks can be as informative
as content networks in email collections. The topical
context also seems to be useful on its own. The con-
versational context is moderately useful on its own
in the larger collections. The local context alone is
not very informative for the larger collections.
Mixture of Contexts: The principal motivation for
combining different types of contexts is that differ-
ent sources may provide complementary evidence.
To characterize that effect, we look at combinations
of contexts. Figure 4 shows three such context com-
binations, anchored by the social context alone, with
a 100-day window (the results for 10 and 200 day
periods are similar). Reassuringly, adding more con-
texts (hence more evidence) turns out to be a rea-
Figure 4: Mixture of contexts, period set to 100 days.
sonable choice in most cases. For the full combi-
nation, we notice a drop in the effectiveness from
the addition of the topical context.5 This suggests
that the construction of the topical context may need
more careful design, and/or that learned ?k?s could
yield better evidence combination (since these re-
sults were obtained with equal ?k?s).
5 Conclusion
We have presented an approach to mention resolu-
tion in email that flexibly makes use of expanding
contexts to accurately resolve the identity of a given
mention. Our approach focuses on four naturally
occurring contexts in email, including a message,
a thread, other emails with senders and/or recipi-
ents in common, and other emails with significant
topical content in common. Our approach outper-
forms previously reported techniques and it scales
well to larger collections. Moreover, our results
serve to highlight the importance of social context
when resolving mentions in social media, which is
an idea that deserves more attention generally. In fu-
ture work, we plan to extend our test collection with
mention queries that must be resolved in the ?long
tail? of the identity distribution where less evidence
is available. We are also interested in exploring iter-
ative approaches to jointly resolving mentions.
Acknowledgments
The authors would like to thank Lise Getoor for her
helpful advice.
5This also occurs even when topical context is combined
with only social context.
948
References
Daniel J. Abadi. 2003. Comparing domain-specific and
non-domain-specific anaphora resolution techniques.
Cambridge University MPhil Dissertation.
Alfred V. Aho and Margaret J. Corasick. 1975. Effi-
cient string matching: an aid to bibliographic search.
In Communications of the ACM.
James Allan, editor. 2002. Topic detection and tracking:
event-based information organization. Kluwer Aca-
demic Publishers, Norwell, MA, USA.
Bryce Allen. 1989. Recall cues in known-item retrieval.
JASIS, 40(4):246?252.
Omar Benjelloun, Hector Garcia-Molina, Hideki Kawai,
Tait Eliott Larson, David Menestrina, Qi Su, Sut-
thipong Thavisomboon, and Jennifer Widom. 2006.
Generic entity resolution in the serf project. IEEE
Data Engineering Bulletin, June.
Indrajit Bhattacharya and Lise Getoor. 2006. A latent
dirichlet model for unsupervised entity resolution. In
The SIAM International Conference on Data Mining
(SIAM-SDM), Bethesda, MD, USA.
Indrajit Bhattacharya and Lise Getoor. 2007. Collective
entity resolution in relational data. ACM Transactions
on Knowledge Discovery from Data, 1(1), March.
Matthias Blume. 2005. Automatic entity disambigua-
tion: Benefits to NER, relation extraction, link anal-
ysis, and inference. In International Conference on
Intelligence Analysis, May.
Chris Diehl, Lise Getoor, and Galileo Namata. 2006.
Name reference resolution in organizational email
archives. In Proceddings of SIAM International Con-
ference on Data Mining, Bethesda, MD , USA, April
20-22.
Tamer Elsayed and Douglas W. Oard. 2006. Modeling
identity in archival collections of email: A prelimi-
nary study. In Proceedings of the 2006 Conference
on Email and Anti-Spam (CEAS 06), pages 95?103,
Mountain View, California, July.
Ralf Holzer, Bradley Malin, and Latanya Sweeney. 2005.
Email alias detection using social network analysis. In
LinkKDD ?05: Proceedings of the 3rd international
workshop on Link discovery, pages 52?57, New York,
NY, USA. ACM Press.
Bryan Klimt and Yiming Yang. 2004. Introducing the
Enron corpus. In Conference on Email and Anti-Spam,
Mountain view, CA, USA, July 30-31.
David D. Lewis and Kimberly A. Knowles. 1997.
Threading electronic mail: a preliminary study. Inf.
Process. Manage., 33(2):209?217.
David D. Lewis. 1996. The trec-4 filtering track. In The
Fourth Text REtrieval Conference (TREC-4), pages
165?180, Gaithersburg, Maryland.
Xin Li, Paul Morie, and Dan Roth. 2005. Semantic inte-
gration in text: from ambiguous names to identifiable
entities. AI Magazine. Special Issue on Semantic Inte-
gration, 26(1):45?58.
Bradley Malin. 2005. Unsupervised name disambigua-
tion via social network similarity. In Workshop on
Link Analysis, Counter-terrorism, and Security, in
conjunction with the SIAM International Conference
on Data Mining, Newport Beach, CA, USA, April 21-
23.
Gideon S. Mann and David Yarowsky. 2003. Unsuper-
vised personal name disambiguation. In Proceedings
of the seventh conference on Natural language learn-
ing at HLT-NAACL 2003, pages 33?40, Morristown,
NJ, USA. Association for Computational Linguistics.
Andrew McCallum, Andres Corrada-Emmanuel, and
XueruiWang Wang. 2005. Topic and role discovery
in social networks. In IJCAI.
Einat Minkov, William W. Cohen, and Andrew Y. Ng.
2006. Contextual search and name disambiguation in
email using graphs. In SIGIR ?06: Proceedings of
the 29th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 27?34, New York, NY, USA. ACM Press.
Patric Reuther. 2006. Personal name matching: New test
collections and a social network based approach.
949
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 265?268,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Pairwise Document Similarity in Large Collections with MapReduce
Tamer Elsayed,?Jimmy Lin,? and Douglas W. Oard?
Human Language Technology Center of Excellence and
UMIACS Laboratory for Computational Linguistics and Information Processing
University of Maryland, College Park, MD 20742
{telsayed,jimmylin,oard}@umd.edu
Abstract
This paper presents a MapReduce algorithm
for computing pairwise document similarity
in large document collections. MapReduce is
an attractive framework because it allows us
to decompose the inner products involved in
computing document similarity into separate
multiplication and summation stages in a way
that is well matched to efficient disk access
patterns across several machines. On a col-
lection consisting of approximately 900,000
newswire articles, our algorithm exhibits lin-
ear growth in running time and space in terms
of the number of documents.
1 Introduction
Computing pairwise similarity on large document
collections is a task common to a variety of prob-
lems such as clustering and cross-document coref-
erence resolution. For example, in the PubMed
search engine,1 which provides access to the life sci-
ences literature, a ?more like this? browsing feature
is implemented as a simple lookup of document-
document similarity scores, computed offline. This
paper considers a large class of similarity functions
that can be expressed as an inner product of term
weight vectors.
For document collections that fit into random-
access memory, the solution is straightforward. As
collection size grows, however, it ultimately be-
comes necessary to resort to disk storage, at which
point aligning computation order with disk access
patterns becomes a challenge. Further growth in the
?Department of Computer Science
?The iSchool, College of Information Studies
1http://www.ncbi.nlm.nih.gov/PubMed
document collection will ultimately make it desir-
able to spread the computation over several proces-
sors, at which point interprocess communication be-
comes a second potential bottleneck for which the
computation order must be optimized. Although
tailored implementations can be designed for spe-
cific parallel processing architectures, the MapRe-
duce framework (Dean and Ghemawat, 2004) offers
an attractive solution to these challenges. In this pa-
per, we describe how pairwise similarity computa-
tion for large collections can be efficiently imple-
mented with MapReduce. We empirically demon-
strate that removing high frequency (and therefore
low entropy) terms results in approximately linear
growth in required disk space and running time with
increasing collection size for collections containing
several hundred thousand documents.
2 MapReduce Framework
MapReduce builds on the observation that many
tasks have the same structure: a computation is ap-
plied over a large number of records (e.g., docu-
ments) to generate partial results, which are then ag-
gregated in some fashion. Naturally, the per-record
computation and aggregation vary by task, but the
basic structure remains fixed. Taking inspiration
from higher-order functions in functional program-
ming, MapReduce provides an abstraction that in-
volves the programmer defining a ?mapper? and a
?reducer?, with the following signatures:
map: (k1, v1)? [(k2, v2)]
reduce: (k2, [v2])? [(k3, v3)]
Key/value pairs form the basic data structure in
MapReduce. The ?mapper? is applied to every input
265
Shu
fflin
g: g
rou
p va
lues
 by 
key
s
ma
p
ma
p
ma
p
ma
p
red
uce
red
uce
red
uce
inp
ut
inp
ut
inp
ut
inp
ut
out
put
out
put
out
put
Figure 1: Illustration of the MapReduce framework: the
?mapper? is applied to all input records, which generates
results that are aggregated by the ?reducer?.
key/value pair to generate an arbitrary number of in-
termediate key/value pairs. The ?reducer? is applied
to all values associated with the same intermediate
key to generate output key/value pairs (see Figure 1).
On top of a distributed file system (Ghemawat
et al, 2003), the runtime transparently handles all
other aspects of execution (e.g., scheduling and fault
tolerance), on clusters ranging from a few to a few
thousand nodes. MapReduce is an attractive frame-
work because it shields the programmer from dis-
tributed processing issues such as synchronization,
data exchange, and load balancing.
3 Pairwise Document Similarity
Our work focuses on a large class of document simi-
larity metrics that can be expressed as an inner prod-
uct of term weights. A document d is represented as
a vector Wd of term weights wt,d, which indicate
the importance of each term t in the document, ig-
noring the relative ordering of terms (?bag of words?
model). We consider symmetric similarity measures
defined as follows:
sim(di, dj) =
?
t?V
wt,di ? wt,dj (1)
where sim(di, dj) is the similarity between docu-
ments di and dj and V is the vocabulary set. In this
type of similarity measure, a term will contribute to
the similarity between two documents only if it has
non-zero weights in both. Therefore, t ? V can be
replaced with t ? di ? dj in equation 1.
Generalizing this to the problem of computing
similarity between all pairs of documents, we note
Algorithm 1 Compute Pairwise Similarity Matrix
1: ?i, j : sim[i, j]? 0
2: for all t ? V do
3: pt ? postings(t)
4: for all di, dj ? pt do
5: sim[i, j]? sim[i, j] + wt,di ? wt,dj
that a term contributes to each pair that contains it.2
For example, if a term appears in documents x, y,
and z, it contributes only to the similarity scores be-
tween (x, y), (x, z), and (y, z). The list of docu-
ments that contain a particular term is exactly what
is contained in the postings of an inverted index.
Thus, by processing all postings, we can compute
the entire pairwise similarity matrix by summing
term contributions.
Algorithm 1 formalizes this idea: postings(t) de-
notes the list of documents that contain term t. For
simplicity, we assume that term weights are also
stored in the postings. For small collections, this al-
gorithm can be run efficiently to compute the entire
similarity matrix in memory. For larger collections,
disk access optimization is needed?which is pro-
vided by the MapReduce runtime, without requiring
explicit coordination.
We propose an efficient solution to the pairwise
document similarity problem, expressed as two sep-
arate MapReduce jobs (illustrated in Figure 2):
1) Indexing: We build a standard inverted in-
dex (Frakes and Baeza-Yates, 1992), where each
term is associated with a list of docid?s for docu-
ments that contain it and the associated term weight.
Mapping over all documents, the mapper, for each
term in the document, emits the term as the key, and
a tuple consisting of the docid and term weight as the
value. The MapReduce runtime automatically han-
dles the grouping of these tuples, which the reducer
then writes out to disk, thus generating the postings.
2) Pairwise Similarity: Mapping over each post-
ing, the mapper generates key tuples corresponding
to pairs of docids in the postings: in total, 12m(m?1)
pairs where m is the posting length. These key tu-
ples are associated with the product of the corre-
sponding term weights?they represent the individ-
2Actually, since we focus on symmetric similarity functions,
we only need to compute half the pairs.
266
d 1
(A,(
d 1,
2))
(B,(
d 1,
1))
(C,(
d 1,
1))
(B,(
d 2,
1))
(D,(
d 2,
2))
(A,(
d 3,
1))
(B,(
d 3,
2))
(E,(
d 3,
1))
(A,[
(d 1,
2),
(d 3,
1)])
(B,[
(d 1,
1), (d 2,
1), 
(d 3,
2)])
(C,[
(d 1,
1)])
(D,[
(d 2,
2)])
(E,[
(d 3,
1)])
d 2 d 3
((d 1
,d 3
),2)
((d 1
,d 2
),1)
((d 1
,d 3
),2)
((d 2
,d 3
),2)
((d 1
,d 2
),[1]
)
((d 1
,d 3
),[2, 2
])
((d 2
,d 3
),[2]
)
((d 1
,d 2
),1)
((d 1
,d 3
),4)
((d 2
,d 3
),2)
?A 
A B
 
C?
?B 
D D
?
?A 
B B
 
E?
ma
p
ma
p
ma
p
re
duc
e
re
duc
e
re
duc
e
ma
p
ma
p
ma
p
shu
ffle
ma
p
ma
p
shu
ffle
Ind
ex
ing
Pa
irw
ise
Sim
ilar
ity
re
duc
e
re
duc
e
re
duc
e
re
duc
e
re
duc
e
(A,[
(d 1,
2),
(d 3,
1)])
(B,[
(d 1,
1), (d 2,
1), 
(d 3,
2)])
(C,[
(d 1,
1)])
(D,[
(d 2,
2)])
(E,[
(d 3,
1)])
Figure 2: Computing pairwise similarity of a toy collection of 3 documents. A simple term weighting scheme (wt,d =
tft,d) is chosen for illustration.
ual term contributions to the final inner product. The
MapReduce runtime sorts the tuples and then the re-
ducer sums all the individual score contributions for
a pair to generate the final similarity score.
4 Experimental Evaluation
In our experiments, we used Hadoop ver-
sion 0.16.0,3 an open-source Java implementation
of MapReduce, running on a cluster with 20 ma-
chines (1 master, 19 slave). Each machine has two
single-core processors (running at either 2.4GHz or
2.8GHz), 4GB memory, and 100GB disk.
We implemented the symmetric variant of Okapi-
BM25 (Olsson and Oard, 2007) as the similarity
function. We used the AQUAINT-2 collection of
newswire text, containing 906k documents, totaling
approximately 2.5 gigabytes. Terms were stemmed.
To test the scalability of our technique, we sampled
the collection into subsets of 10, 20, 25, 50, 67, 75,
80, 90, and 100 percent of the documents.
After stopword removal (using Lucene?s stop-
word list), we implemented a df-cut, where a frac-
tion of the terms with the highest document frequen-
cies is eliminated.4 This has the effect of remov-
ing non-discriminative terms. In our experiments,
we adopt a 99% cut, which means that the most fre-
quent 1% of terms were discarded (9,093 terms out
of a total vocabulary size of 909,326). This tech-
nique greatly increases the efficiency of our algo-
rithm, since the number of tuples emitted by the
3http://hadoop.apache.org/
4In text classification, removal of rare terms is more com-
mon. Here we use df-cut to remove common terms.
R2  = 0.
997
020406080100120140 0
10
20
30
40
50
60
70
80
90
100
Corpu
s Size
 
(%)
Computation Time (minutes)
Figure 3: Running time of pairwise similarity compar-
isons, for subsets of AQUAINT-2.
mappers in the pairwise similarity phase is domi-
nated by the length of the longest posting (in the
worst case, if a term appears in all documents, it
would generate approximately 1012 tuples).
Figure 3 shows the running time of the pairwise
similarity phase for different collection sizes.5 The
computation for the entire collection finishes in ap-
proximately two hours. Empirically, we find that
running time increases linearly with collection size,
which is an extremely desirable property. To get a
sense of the space complexity, we compute the num-
ber of intermediate document pairs that are emit-
ted by the mappers. The space savings are large
(3.7 billion rather than 8.1 trillion intermediate pairs
for the entire collection), and space requirements
grow linearly with collection size over this region
(R2 = 0.9975).
5The entire collection was indexed in about 3.5 minutes.
267
01,0002,0003,0004,0005,0006,0007,0008,0009,000
0
10
20
30
40
50
60
70
80
90
100
Corpu
s Size
 
(%)
Intermediate Pairs (billions)
df-cut
 
at 99%
df-cut
 
at 99.9
%
df-cut
 
at 99.9
9%
df-cut
 
at 99.9
99%
no df-
cut
Figure 4: Effect of changing df -cut thresholds on the
number of intermediate document-pairs emitted, for sub-
sets of AQUAINT-2.
5 Discussion and Future Work
In addition to empirical results, it would be desir-
able to derive an analytical model of our algorithm?s
complexity. Here we present a preliminary sketch of
such an analysis and discuss its implications. The
complexity of our pairwise similarity algorithm is
tied to the number of document pairs that are emit-
ted by the mapper, which equals the total number of
products required in O(N2) inner products, where
N is the collection size. This is equal to:
1
2
?
t?V
dft(dft ? 1) (2)
where dft is the document frequency, or equivalently
the length of the postings for term t. Given that to-
kens in natural language generally obey Zipf?s Law,
and vocabulary size and collection size can be re-
lated via Heap?s Law, it may be possible to develop
a closed form approximation to the above series.
Given the necessity of computing O(N2) inner
products, it may come as a surprise that empirically
our algorithm scales linearly (at least for the collec-
tion sizes we explored). We believe that the key to
this behavior is our df-cut technique, which elimi-
nates the head of the df distribution. In our case,
eliminating the top 1% of terms reduces the number
of document pairs by several orders of magnitude.
However, the impact of this technique on effective-
ness (e.g., in a query-by-example experiment) has
not yet been characterized. Indeed, a df-cut thresh-
old of 99% might seem rather aggressive, removing
meaning-bearing terms such as ?arthritis? and ?Cor-
nell? in addition to perhaps less problematic terms
such as ?sleek? and ?frail.? But redundant use of
related terms is common in news stories, which we
would expect to reduce the adverse effect on many
applications of removing these low entropy terms.
Moreover, as Figure 4 illustrates, relaxing the df-
cut to a 99.9% threshold still results in approxi-
mately linear growth in the requirement for interme-
diate storage (at least over this region).6 In essence,
optimizing the df-cut is an efficiency vs. effective-
ness tradeoff that is best made in the context of a
specific application. Finally, we note that alternative
approaches to similar problems based on locality-
sensitive hashing (Andoni and Indyk, 2008) face
similar tradeoffs in tuning for a particular false pos-
itive rate; cf. (Bayardo et al, 2007).
6 Conclusion
We present a MapReduce algorithm for efficiently
computing pairwise document similarity in large
document collections. In addition to offering spe-
cific benefits for a number of real-world tasks, we
also believe that our work provides an example of
a programming paradigm that could be useful for a
broad range of text analysis problems.
Acknowledgments
This work was supported in part by the Intramural
Research Program of the NIH/NLM/NCBI.
References
A. Andoni and P. Indyk. 2008. Near-optimal hashing
algorithms for approximate nearest neighbor in high
dimensions. CACM, 51(1):117?122.
R. Bayardo, Y. Ma, and R. Srikant. 2007. Scaling up all
pairs similarity search. In WWW ?07.
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied data processing on large clusters. In OSDI ?04.
W. Frakes and R. Baeza-Yates. 1992. Information Re-
trieval: Data Structures and Algorithms.
S. Ghemawat, H. Gobioff, and S. Leung. 2003. The
Google File System. In SOSP ?03.
J. Olsson and D. Oard. 2007. Improving text classifi-
cation for oral history archives with temporal domain
knowledge. In SIGIR ?07.
6More recent experiments suggest that a df-cut of 99.9% re-
sults in almost no loss of effectiveness on a query-by-example
task, compared to no df-cut.
268
Mandarin-English Information (MEI): 
Investigating Translingual Speech Retrieval 
Helen Meng, 1 Sanjeev Khudanpur, ~ Gina Levow, 3 Douglas W. Oard, 3 Hsin-Min Wang' 
1The Chinese University of Hong Kong, 2Johns Hopkins University, 
3University of Maryland and 4Academia Sinica (Taiwan) 
{hmmeng@se.cuhk.edu.hk, sanjeev@clsp.jhu.edu, gina@umiacs.umd.edu, 
oard@, glue.umd.edu, whm@ iis.sinica.edu.tw } 
Abstract 
We describe a system which supports 
English text queries searching for 
Mandarin Chinese spoken documents. 
This is one of the first attempts to tightly 
couple speech recognition with machine 
translation technologies for cross-media 
and cross-language retrieval. The 
Mandarin Chinese news audio are indexed 
with word and subword units by speech 
recognition. Translation of these multi- 
scale units can effect cross-language 
information retrieval. The integrated 
technologies will be evaluated based on 
the performance of translingnal speech 
retrieval. 
1. Introduction 
Massive quantities of audio and multimedia 
programs are becoming available. For example, 
in mid-February 2000, www.real.com listed 
1432 radio stations, 381 Internet-only 
broadcasters, and 86 television stations with 
Internet-accessible content, with 529 
broadcasting in languages other than English. 
Monolingual speech retrieval is now practical, as 
evidenced by services such as SpeechBot 
(speechbot.research.compaq.com), and it is clear 
that there is a potential demand for translingual 
speech retrieval if effective techniques can be 
developed. The Mandarin-English Information 
(MEI) project represents one of the first efforts 
in that direction. 
MEI is one of the four projects elected for 
the Johns Hopkins University (JHU) Summer 
Workshop 2000.1 Our research focus is on the 
integration of speech recognition and embedded 
translation technologies in the context of 
translingual speech retrieval. Possible 
applications of this work include audio and 
video browsing, spoken document retrieval, 
automated routing of information, and 
automatically alerting the user when special 
events occur. 
At the time of this writing, most of the MEI 
team members have been identified. This paper 
provides an update beyond our first proposal 
\[Meng et al, 2000\]. We present some ongoing 
work of our current eam members, as well as 
our ideas on an evolving plan for the upcoming 
JHU Summer Workshop 2000. We believe the 
input from the research community will benefit 
us greatly in formulating ourfinal plan. 
2. Background 
2.1 Translingual Information Retrieval 
The earliest work on large-vocabulary cross- 
language information retrieval from free-text 
(i,e., without manual topic indexing) was 
reported in 1990 \[Landauer and Littman, 1990\], 
and the topic has received increasing attention 
over the last five years \[Oard and Diekema, 
1998\]. Work on large-vocabulary retrieval from 
recorded speech is more recent, with some initial 
work reported in 1995 using subword indexing 
\[Wechsler and Schauble, 1995\], followed by the 
first TREC 2 Spoken Document Retrieval (SDR) 
I http://www.clsp,jhu.edu/ws2000/ 
2 Text REtrieval Conference, http://trec.nist.gov 
23 
evaluation \[Garofolo et al, 2000\]. The Topic 
Detection and Tracking (TDT) evaluations, 
which started in 1998, fall within our definition 
of speech retrieval for this purpose, differing 
from other evaluations principally in the nature 
of the criteria that human assessors use when 
assessing the relevance of a news stow to an 
information eed. In TDT, stories are assessed 
for relevance to an event, while in TREC stories 
are assessed for relevance to an explicitly stated 
information eed that is often subject- rather 
than event-oriented. 
The TDT-33 evaluation marked the first 
case of translingual speech retrieval - the task of 
finding information in a collection of recorded 
speech based on evidence of the information 
need that might be expressed (at least partially) 
in a different language. Translingual speech 
retrieval thus merges two lines of research that 
have developed separately until now. In the 
TDT-3 topic tracking evaluation, recognizer 
transcripts which have recognition errors were 
available, and it appears that every team made 
use of them. This provides a valuable point of 
reference for investigation of techniques that 
more tightly couple speech recognition with 
translingual retrieval. We plan to explore one 
way of doing this in the Mandarin-English 
Information (MEI) project. 
2.2 The Chinese Language 
In order to retrieve Mandarin audio documents, 
we should consider a number of linguistic 
characteristics of the Chinese language: 
The Chinese language has many dialects. 
Different dialects are characterized by their 
differences in the phonetics, vocabularies and 
syntax. Mandarin, also known as Putonglma 
("the common language"), is the most widely 
used dialect. Another major dialect is Cantonese, 
predominant in Hong Kong, Macau, South 
China and many overseas Chinese communities. 
Chinese is a syllable-based language, 
where each syllable carries a lexical tone. 
Mandarin has about 400 base syllables and four 
lexical tones, plus a "light" tone for reduced 
syllables. There are about 1,200 distinct, tonal 
syllables for Mandarin. Certain syllable-tone 
3 http://morph.ldc.upenn.edu/Projects/TDT3/ 
combinations are non-existent in the language. 
The acoustic correlates of the lexical tone 
include the syllable's fundamental frequency 
(pitch contour) and duration. However, these 
acoustic features are also highly dependent on 
prosodic variations of spoken utterances. 
The structure of Mandarin (base) syllables 
is (CG)V(X), where (CG) the syllable onset - C 
the initial consonant, G is the optional medial 
glide, V is the nuclear vowel, and X is the coda 
(which may be a glide, alveolar nasal or velar 
nasal). Syllable onsets and codas are optional. 
Generally C is known as the syllable initial, and 
the rest (GVX) syllable final. 4 Mandarin has 
approximately 21 initials and 39 finals. 5 
In its written form, Chinese is a sequence 
of characters. A word may contain one or more 
characters. Each character is pronounced as a 
tonal syllable. The character-syllable mapping is 
degenerate. On one hand, a given character may 
have multiple syllable pronunciations - for 
example, the character/d" may be pronounced as 
/hang2/, 6/hang4/, or/xing2/. On the other hand, 
a given tonal syllable may correspond to 
multiple characters. Consider the two-syllable 
pronunciation/fu4 shu4/, which corresponds toa 
two-character word. Possible homophones 
include ~, ,  (meaning "rich"), ~ ~tR, ("negative 
number"), ~1~1~, ("complex number" or 
"plural"), ~1~ ("repeat"). 7 
Aside from homographs and homophones, 
another source of ambiguity in the Chinese 
language is the definition of a Chinese word. 
The word has no delimiters, and the distinction 
between a word and a phrase is often vague. The 
lexical structure of the Chinese word is very 
different compared to English. Inflectional 
forms are minimal, while morphology and word 
derivations abide by a different set of rules. A 
word may inherit the syntax and semantics of 
(some of) its compositional characters, for 
4 http://m?rph'ldc'upenn'edu/Pr?jects/Chinese/intr?'html 
5 The corresponding linguistic haracteristics of Cantonese 
are very similar. 
6 These are Mandarin pinyin, the number encodes the tone 
of the syllable. 
7 Example drawn from \[Leung, 1999\]. 
24 
example, 8 ~ means red (a noun or an 
adjective), ~., means color (a noun), and ~. ,  
together means "the color red"(a noun) or 
simply "red" (an adjective). Alternatively, a 
word may take on totally different 
characteristics of its own, e.g. ~. means east (a 
noun or an adjective), ~ means west (a noun or 
an adjective), and .~.~ together means thing (a 
noun). Yet another case is where the 
compositional characters of a word do not form 
independent lexical entries in isolation, e.g. D~ 
means fancy (a verb), but its characters do not 
occur individually. Possible ways of deriving 
new words from characters are legion. The 
problem of identifying the words string in a 
character sequence is known as the segmentation 
/ tokenization problem. Consider the syllable 
string: 
/zhe4 yil wan3 hui4 ru2 chang2 ju3 xing2/ 
The corresponding character string has three 
possible segmentations - all are correct, but each 
involves a distinct set of words: 
(Meaning: It will be take place tonight as usual.) 
(Meaning: The evening banquet will take place 
as usual.) 
(Meaning: If this evening banquet akes place 
frequently...) 
The above considerations lead to a number 
of techniques we plan to use for our task. We 
concentrate on three equally critical problems 
related to our theme of translingual speech 
retrieval: (i) indexing Mandarin Chinese audio 
with word and subword units, (ii) translating 
variable-size units for cross-language 
information retrieval, and (iii) devising effective 
retrieval strategies for English text queries and 
Mandarin Chinese news audio. 
3. Multiscale Audio Indexing 
A popular approach to spoken document 
retrieval is to apply Large-Vocabulary 
s Examples drawn from \[Meng and Ip, 1999\]. 
Continuous Speech Recognition (LVCSR) 9 for 
audio indexing, followed by text retrieval 
techniques. Mandarin Chinese presents a 
challenge for word-level indexing by LVCSR, 
because of the ambiguity in tokenizing a 
sentence into words (as mentioned earlier). 
Furthermore, LVCSR with a static vocabulary is
hampered by the out-of-vocabulary (OOV) 
problem, especially when searching sources with 
topical coverage as diverse as that found in 
broadcast news. 
By virtue of the monosyllabic nature of the 
Chinese language and its dialects, the syllable 
inventory can provide a complete phonological 
coverage for spoken documents, and circumvent 
the OOV problem in news audio indexing, 
offering the potential for greater recall in 
subsequent retrieval. The approach thus supports 
searches for previously unknown query terms in 
the indexed audio. 
The pros and cons of subword indexing for 
an English spoken document retrieval task was 
studied in \[Ng, 2000\]. Ng pointed out that the  
exclusion of lexical knowledge when subword 
indexing is performed in isolation may adversely 
impact discrimination power for retrieval, but 
that some of that impact can be mitigated by 
modeling sequential constraints among subword 
units. We plan to investigate the efficacy of 
using both word and subword units for 
Mandarin audio indexing \[Meng et al, 2000\]. 
Although Ng found that such an approach 
produced little gain over words alone for 
English, the structure of Mandarin Chinese may 
produce more useful subword features. 
3.1 Modeling Syllable Sequence Constraints 
We have thus far used overlapping syllable N- 
grams for spoken document retrieval for two 
Chinese dialects - Mandarin and Cantonese. 
Results on a known-item retrieval task with over 
1,800 error-free news transcripts \[Meng et al, 
1999\] indicate that constraints from overlapping 
bigrams can yield significant improvements in 
retrieval performance over syllable unigrams, 
producing retrieval performance competitive 
9 The lexicon size of a typical large-vocabulary 
continuous speech recognizer can range from 10,000 
to 100,000 word forms. 
25 
with that obtained using automatically tokenized 
Chinese words. 
The study in \[Chen, Wang and Lee, 2000\] 
also used syllable pairs with skipped syllables in 
between. This is because many Chinese 
abbreviations are derived from skipping 
characters, e.g. J .~:~.~t:~  ~ National 
Science Council" can be abbreviated as l~r~ 
(including only the first, third and the last 
characters). Moreover, synonyms often differ by 
one or two characters, e.g. both ~ ' /~4~ and 
~.~,,Ag mean "Chinese culture". Inclusion o f  
these "skipped syllable pairs" also contributed to
retrieval performance. 
When modeling sequential syllable 
constraints, lexical constraints on recognized 
words may be helpful. We thus plan to exp\]Iore 
the potential for integrated sequential model\]ling 
of both words and syllables \[Meng et al, 20013\]. 
4. Multiseale Embedded Translation 
Figures 1 and 2 illustrate two translingual 
retrieval strategies. In query translation, English 
text queries are transformed into Mandarin and 
then used to retrieve Mandarin documents. For 
document translation, Mandarin documents are 
translated into English before they are indexed 
and then matched with English queries. 
McCarley has reported improved effectiveness 
from techniques that couple the two techniques 
\[McCarley, 1999\], but time constraints may 
limit us to explonng only the query translation 
strategy dunng the six-week Workshop. 
4,1 Word  Translat ion 
While we make use of sub-word 
transcription tosmooth out-of-vocabulary(OOV) 
problems in speech recognition as described 
above, and to alleviate the OOV problem :for 
translation as we discuss in the next section, 
accurate translation generally relies on the 
additional information available at the word and 
phrase levels. Since the "bag of words" 
information retrieval techniques do not 
incorporate any meaningful degree of language 
understanding to assess similarity between 
queries and documents, a word-for-word (or, 
more generally, term-for-term) embedded 
translation approach can achieve a useful level 
of effectiveness for many translingual retrieval 
applications \[Oard and Diekema, 1998\]. 
We have developed such a technique for the 
TDT-3 topic tracking evaluation \[Levow and 
Oard, 2000\]. For that work we extracted an 
enriched bilingual Mandarin-English term list by 
combining two term lists: (i) A list assembled 
by the Linguistic Data Consortium from freely 
available on-line resources; and (ii) entries from 
the CETA file (sometimes referred to as 
"Optilex"). This is a Chinese to English 
translation resource that was manually compiled 
by a team of linguists from more than 250 text 
sources, including special and general-purpose 
print dictionaries, and other text sources uch as 
newspapers. The CETA file contains over 
250,000 entries, but for our lexical work we 
extracted a subset of those entries drawn from 
contemporary general-purpose sources. We also 
excluded efinitions uch as "particle indicating 
a yes/no question." Our resulting Chinese to 
English merged bilingual term list contains 
translations for almost 200,000 Chinese terms, 
with average of almost two translation 
alternatives per term. We have also used the 
same resources to construct an initial English to 
Chinese bilingual term list that we plan to refine 
before the Workshop. 
Three significant challenges faced by term- 
to-term translation systems are term selection in 
the source language, the source language 
coverage of the bilingual term list, and 
translation selection in the target language when 
more than one alternative translation is known. 
Word segmentation is a natural by-product of 
large vocabulary Mandarin speech recognition, 
and white space provides word boundaries for 
the English queries. We thus plan to choose 
words as our basic term set, perhaps augmenting 
this with the multiword expressions found in the 
bilingual term list. 
Achieving adequate source language 
coverage is challenging in news retrieval 
applications of the type modelled by TDT, 
because proper names and technical terms that 
may not be present in general-purpose lexical 
resources often provide important retrieval cues. 
Parallel (translation equivalent) corpora have 
proven to be a useful source of translation 
26 
equivalent terms, but obtaining appropriate 
domain-specific parallel corpora in electronic 
form may not be practical in some applications. 
We therefore plan to investigate the use of 
comparable corpora to learn translation 
equivalents, based on techniques in \[Fung, 
1998\]. Subword translation, described below, 
provides a complementary way of handling 
terms for which translation equivalents cannot 
be reliably extracted from the available 
comparable corpora. 
One way of dealing with multiple 
translations is to weight the alternative 
translations using either a statistical translation 
model trained on parallel or comparable corpora 
to estimate translation probability conditioned 
on the source language term. When such 
resources are not sufficiently informative, it is 
generally possible to back off to an 
unconditioned preference statistic based on 
usage frequency of each possible translation i  a 
representative monolingual corpus in the target 
language. In retrospective r trieval applications 
the collection being searched can be used for 
this purpose. We have applied simple versions 
of this approach with good results \[Levow and 
Oard, 2000\]. 
We have recently observed that a simpler 
technique introduced by \[Pirkola, 1998\] can 
produce xcellent results. The key idea is to use 
the structure of the lexicon, in which several 
target language terms can represent a single 
source language term, to induce structure in the 
translated query that the retrieval system can 
automatically exploit. In essence, the translated 
query becomes a bag of bags of terms, where 
each smaller bag corresponds to the set of 
possible translations for one source-language 
term. We plan to implement his structured 
query translation approach using the Inquery 
\[Callan, 1992\] "synonym" operator in the same 
manner as \[Pirkola, 1998\], and to the potential to 
extend the technique to accommodate alternative 
recognition hypothesis and subword units as 
well: 
4.2 Subword  Translat ion 
Since Mandarin spoken documents can be 
indexed with both words and subwords, the 
translation (or "phonetic transliteration") of 
subword units is of particular interest. We plan 
to make use of cross-language phonetic 
mappings derived from English and Mandarin 
pronunciation rules for this purpose. This should 
be especially useful for handling named entities 
in the queries, e.g. names of people, places and 
organizations, etc. which are generally important 
for retrieval, but may not be easily translated. 
Chinese translations of English proper nouns 
may involve semantic as well as phonetic 
mappings. For example, "Northern Ireland" is 
translated as :~b~ttlM - -  where the first 
character ~ means 'north', and the remaining 
characters ~tllllll are pronounced as /ai4-er3- 
lan2L Hence the translation is both semantic 
and phonetic. When Chinese translations strive 
to attain phonetic similarity, the mapping may 
be inconsistent. For example, consider the 
translation of "Kosovo" - sampling Chinese 
newspapers in China, Taiwan and Hong Kong 
produces the following translations: 
~-~r~ /kel-suo3-wo4?, ~-~ /kel-suo3-fo2/, 
~'~&/kel-suo3-ful/f l4"~dt/kel-suo3-fu2/, or 
~/ke  1-suo3-fo2/. 
As can be seen, there is no systematic 
mapping to the Chinese character sequences, but 
the translated Chinese pronunciations bear some 
resemblance to the English pronunciation (/k ow 
s ax vow/). In order to support retrieval under 
these circumstances, the approach should 
involve approximate matches between the 
English pronunciation and the Chinese 
pronunciation. The matching algorithm should 
also accommodate phonological variations. 
Pronunciation dictionaries, or pronunciation 
generation tools for both English words and 
Chinese words / characters will be useful for the 
matching algorithm. We can probably leverage 
off of ideas in the development of universal 
speech recognizers \[Cohen et al, 1997\]. 
5. Mulfiscale Retrieval 
5.1 Coupling Words and Subwords 
We intend to use both words and subwords for 
retrieval. Loose coupling would involve separate 
retrieval runs using words and subwords, 
producing two ranked lists, followed by list 
merging using techniques such as those explored 
by \[Voorhees, 1995\]. Tight coupling, by 
27 
contrast, would require creation of a unified 
index containing both word and subword units, 
resulting in a single ranked list. We hope to 
explore both techniques during the Workshop. 
5.2 Imperfect Indexing and Translat ion 
It should be noted that speech recognition 
exacerbates uncertainty when indexing audio, 
and that translation or transliteration exacerbates 
uncertainty when translating queries and/or 
documents. To achieve robustness for retrieval, 
we have tried three techniques that we have 
found useful: (i) Syllable lattices were used in 
\[Wang, 1999\] and \[Chien et al, 2000\] for 
monolingual Chinese retrieval experiments. The 
lattices were pruned to constrain the search 
space, but were able to achieve robust retrieval 
based on imperfect recognized transcripts. (ii) 
Query expansion, in which syllable transcription 
were expanded to include possibly confusable 
syllable sequences based on a syllable confusion 
matrix derived from recognition errors, was used 
in \[Meng et al, 1999\]. (iii) We have expanded 
the document representation using terms 
extracted from similar documents in a 
comparable collection \[Levow and Oard, 2000\], 
and similar techniques are known to work well 
in the case of query translation (Ballesteros and 
Croft, 1997). We hope to add to this set: of 
techniques by exploring the potential for query 
expansion based on cross-language phonetic 
mapping. 
6. Using the TDT-3 Collection 
We plan to use the TDT-2 collection for 
development testing and the TDT-3 collection 
for evaluation. Both collections provide 
documents from two English newswire sources, 
six English broadcast news audio sources, two 
Mandarin Chinese newswire sources, and one 
Mandarin broadcast news source (Voice of 
America). Manually established story 
boundaries are available for all audio 
collections, and we plan to exploit that 
information to simplify our experiment design. 
The TDT-2 collection includes complete 
relevance assessments for 20 topics, and the 
TDT-3 collection provides the same for 60 
additional topics, 56 of which have at least one 
relevant audio story. For each topic, at least four 
English stories and four Chinese stories are 
known. 
We plan to automatically derive text queries 
based on one or more English stories that are 
presented as exemplars, and to use those queries 
to search the Mandarin audio collection. 
Manually constructed queries will provide a 
contrastive condition. Unlike the TDT "topic 
tracking" task in which stories must be declared 
relevant or not relevant in the order of their 
arrival, we plan to perform retrospective 
retrieval experiments in which all documents are 
known when the query is issued. By relaxing 
the temporal ordering of the TDT topic tracking 
task, we can meaningfully search for Mandarin 
Chinese stories that may have arrived before the 
exemplar story or stories. We thus plan to report 
ranked retrieval measures of effectiveness uch 
as average precision in addition to the detection 
statistics (miss and false alarm) typically 
reported in TDT. 
7.  Summary 
This paper presents our current ideas and 
evolving plan for the MEI project, to take place 
at the Johns Hopkins University Summer 
Workshop 2000. Translingual speech retrieval is 
a long-term research direction, and our team 
looks forward to jointly taking an initial step to 
tackle the problem. The authors welcome all 
comments and suggestions, aswe strive to better 
define the problem in preparation for the six- 
week Workshop. 
Acknowledgments 
The authors wish to thank Patrick Schone, Erika 
Grams, Fred Jelinek, Charles Wayne, Kenney 
? Ng, John Garofolo, and the participants in the 
December 1999 WS2000 planning meeting and 
the TDT-3 workshop for their many helpful 
suggestions. The Hopkins Summer Workshop 
series is supported by grants from the National 
Science Foundation. Our results reported in this 
paper eference thesis work in progress of Wai- 
Kit Lo (Ph.D. candidate, The Chinese Unversity 
of Hong Kong) and Berlin Chen (Ph.D. 
candidate, National Taiwan University). 
28 
References 
Ballesteros and W. B. Croft, "Phrasal 
Translation and Query Expansion Techniques 
for Cross-Language Information Retrieval," 
Proceedings ofACM SIGIR, 1997. 
Callan, J. P., W. B. Croft, and S. M. Harding, 
"The INQUERY Retrieval System," 
Proceedings of the 3rd International Conference 
on Database and Expert Systems Applications, 
1992. 
Carbonnell, J., Y. Yang, R. Frederking and R.D. 
Brown, "Translingual Information Retrieval: A 
Comparative Evaluation," Proceedings ofIJCAI, 
1997. 
Chen, B., H.M. Wang, and L.S. Lee, "Retrieval 
of Broadcast News Speech in Mandarin Chinese 
Collected in Taiwan using Syllable-Level 
Statistical Characteristics," Proceedings of 
ICASSP, 2000. 
Chien, L. F., H. M. Wang, B. R. Bai, and S. C. 
Lin, "A Spoken-Access Approach for Chinese 
Text and Speech Information Retrieval," Journal 
of the American Society for Information 
Science, 51 (4), pp. 313-323, 2000. 
Choy, C. Y., "Acoustic Units for Mandarin 
Chinese Speech Recognition," M.Phil. Thesis, 
The Chinese University of Hong Kong, Hong 
Kong SAR, China, 1999. 
Cohen, P., S. Dharanipragada, J. Gros, M. 
Mondowski, C. Neti, S. Roukos and T. Ward, 
"Towards a Universal Speech Recognizer for 
Multiple Languages," Proceedings of ASRU, 
1997. 
Fung, P., "A Statistical View on Bilingual 
Lexicon Extraction: From parallel corpora to 
non-parallel corpora," Proceedings of AMTA, 
1998. 
Garofolo, J.S., Auzanne, G.P., Voorhees, E.M., 
"The TREC Spoken Document Retrieval Track: 
A Success Story," Proceedings of the Recherche 
d'informations A sistre par Ordinateur: Content- 
Based Multimedia Information Access 
Conference, April 12-14, 2000,to be published. 
Knight, K. and J. Graehl, "Machine 
Transliteration," Proceedings ofACL, 1997. 
Landauer, T. K. and M.L. Littman, "Fully 
Automatic Cross-Language Document Retrieval 
Using Latent Semantic Indexing," Proceedings 
of the 6 th Annual Conference of the UW Centre 
for the New Oxford English Dictionary, 1990. 
Leung, R., "Lexical Access for Large 
Vocabulary Chinese Speech Recognition," M. 
Phil. Thesis, The Chinese University of Hong 
Kong, Hong Kong SAR, China 1999. 
Levow, G. and D.W. Oard, "Translingual Topic 
Tracking with PRISE," Working notes of the 
DARPA TDT-3 Workshop, 2000. 
Lin, C. H., L. S. Lee, and P. Y. Ting, "A New 
Framework for Recognition of Mandarin 
Syllables with Tones using Sub-Syllabic Units," 
Proceedings ofICASSP, 1993. 
Liu~ F. H., M. Picheny, P. Srinivasa, M. 
Monkowski and J. Chen, "Speech Recognition 
on Mandarin Call Home: A Large-Vocabulary, 
Conversational, nd Telephone Speech Corpus," 
Proceedings ofICASSP, 1996. 
McCarley, S., "Should we Translate the 
Documents or the Queries in Cross-Language 
Information Retrieval," Proceedings of ACL, 
1999. 
Meng, H. and C. W. Ip, "An Analytical Study o f  
Transformational Tagging of Chinese Text," 
Proceedings of the Research On Computational 
Lingustics (ROCLING) Conference, 1999. 
Meng, H., W. K. Lo, Y. C. Li and P. C. Ching, 
"A Study on the Use of Syllables for Chinese 
Spoken Document Retrieval," Technical Report 
SEEM1999-11, The Chinese University of Hong 
Kong, 1999. 
Meng, H., Khudanpur, S., Oard, D. W. and 
Wang, H. M., "Mandarin-English Information 
(MEI)," Working notes of the DARPA TDT-3 
Workshop, 2000. 
Ng, K., "Subword-based Approaches for Spoken 
Document Retrieval," Ph.D. Thesis, MIT, 
February 2000. 
Oard, D. W. and A.R. Diekema, "Cross- 
Language Information Retrieval," Annual 
Review of Information Science and Technology, 
vol.33, 1998. 
Pirkola, A., "The effects of query structure and 
dictionary setups in dictionary-based cross- 
language information retrieval," Proceedings of 
ACM SIGIR, 1998. 
Sheridan P. and J. P. Ballerini, "Experiments in
Multilingual Information Retrieval using the 
29 
SPIDER System," Proceedings of ACM SIGIR, 
1996. 
Voorhees, E., "Learning Collection Fusion 
Strategies," Proceedings of SIGIR, 1995. 
Wang, H. M., "Retrieval of Mandarin Spoken 
Documents Based on Syllable Lattice 
Matching," Proceedings of the Fourth 
International Workshop on Information 
Retrieval in Asian Languages, 1999. 
Wechsler, M. and P. Schaiible, "Speech 
Retrieval Based on Automatic Indexing," 
Proceedings of MIRO- 1995. 
English Text Queries 
(words) 
Words that are present entities and unknown words 
translation dictionary 
\[ Trans'a~on I I Transliteration 
Mand~Sn Queries (with words and syllables) 
Mandarin Spoken Documents \[ 
(indexed with word and subword units) 
"7 
Information Retrieval 
Engine 
I Evaluate Retrieval 
Performance 
Figure 1. Query translation strategy. 
Mandarin Spoken Documents 
(indexed with word and subword units) 
l 
Translation 
Documents in 
English 
English Text Queries 
(words) 
I Information Retrieval 
Engine 
Evaluate 
Retrieval 
Performance 
Figure 2. Document translation strategy. 
3O 
Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 1?2,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
 Cross-Language Information Access: Looking Backward, Looking For-
ward 
Douglas W. Oard 
 
College of Information Studies and Institute for Advanced Computer Studies 
University of Maryland, College Park, MD, USA 
 
 
 
The problem of providing people with the informa-
tion that they seek when that information happens 
to be in an unfamiliar language is not new.  Rather, 
what is new is what we can do to help address that 
challenge.  To illustrate this point, I?ll start my talk 
with a brief recap of two earlier generations of 
automated support for cross-language information 
access, the first from roughly 1964 to 1985, and 
the second from roughly 1989 to the present.  With 
that as background, I?ll then take stock of where 
we are, and where I see unmet needs that call for 
capabilities beyond what can currently be accom-
plished.  I?ll conclude with a few observations 
about how we might expect the role of the research 
community to evolve as progressively more capa-
ble cross-language information access technologies 
become commercially viable. 
 
About the Speaker 
Douglas Oard holds joint appointments as an As-
sociate Professor in the College of Information 
Studies and in the Institute for Advanced Com-
puter Studies at the University of Maryland, Col-
lege Park.  He earned his Ph.D. in Electrical 
Engineering from the University of Maryland.  Dr. 
Oard?s research interests center around the use of 
emerging technologies to support information 
seeking by end users.  One of the leading research-
ers on cross-language information retrieval, he has 
helped lead nine evaluation campaigns focused on 
that problem for the Text Retrieval Conference 
(TREC) and the Cross-Language Evaluation Fo-
rum (CLEF).  In addition to his work on ranking 
algorithms and interaction design for cross-
language information retrieval, his recent research 
has focused on support for search and sense mak-
ing in large collections of conversational media.  
Additional information is available at 
http://www.glue.umd.edu/~oard/. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
1
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
2
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 357?360,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Arabic Cross-Document Coreference Detection
Asad Sayeed,
1,2
Tamer Elsayed,
1,2
Nikesh Garera,
1,6
David Alexander,
1,3
Tan Xu,
1,4
Douglas W. Oard,
1,4,5
David Yarowsky,
1,6
Christine Piatko
1
1
Human Language Technology Center of Excellence, Johns Hopkins University, Baltimore,
MD, USA?
2
Dept. of Computer Science, University of Maryland, College Park, MD,
USA?
3
BBN Technologies, Cambridge, MA, USA?
4
College of Information Studies,
University of Maryland, College Park, MD, USA?
5
UMIACS, University of Maryland, College
Park, MD, USA?
6
Dept. of Computer Science, Johns Hopkins University, Baltimore, MD, USA
{asayeed,telsayed}@cs.umd.edu, ngarera@cs.jhu.edu, dalexand@bbn.com,
{tanx,oard}@umd.edu, yarowsky@cs.jhu.edu, Christine.Piatko@jhuapl.edu
Abstract
We describe a set of techniques for Ara-
bic cross-document coreference resolu-
tion. We compare a baseline system of
exact mention string-matching to ones that
include local mention context information
as well as information from an existing
machine translation system. It turns out
that the machine translation-based tech-
nique outperforms the baseline, but local
entity context similarity does not. This
helps to point the way for future cross-
document coreference work in languages
with few existing resources for the task.
1 Introduction
Our world contains at least two noteworthy
George Bushes: President George H. W. Bush and
President George W. Bush. They are both fre-
quently referred to as ?George Bush.? If we wish
to use a search engine to find documents about
one of them, we are likely also to find documents
about the other. Improving our ability to find all
documents referring to one and none referring to
the other in a targeted search is a goal of cross-
document entity coreference detection. Here we
describe some results from a system we built to
perform this task on Arabic documents. We base
our work partly on previous work done by Bagga
and Baldwin (Bagga and Baldwin, 1998), which
has also been used in later work (Chen and Mar-
tin, 2007). Other work such as Lloyd et al (Lloyd,
2006) focus on techniques specific to English.
The main contribution of this work to cross-
document coreference lies in the conditions under
which it was done. Even now, there is no large-
scale resource?in terms of annotated data?for
cross-document coreference in Arabic as there is
in English (e.g. WebPeople (Artiles, 2008)). Thus,
we employed techniques for high-performance
processing in a resource-poor environment. We
provide early steps in cross-document coreference
detection for resource-poor languages.
2 Approach
We treat cross-document entities as a set of graphs
consisting of links between within-document enti-
ties. The graphs are disjoint. Each of our systems
produces a list of such links as within-document
entity pairs (A,B). We obtain within-document
entities by running the corpus through a within-
document coreference resolver?in this case, Serif
from BBN Technologies.
To create the entity clusters, we use a union-
find algorithm over the pairs. If links (A,B)
and (C,B) appear in the system output, then
{A,B,C} are one entity. Similarly, if (X,Y )
and (Z, Y ) appear in the output, then it will find
that {X,Y, Z} are one entity. If the algorithm
later discovers link (B,Z) in the system output, it
will decide that {A,B,C,X, Y, Z} are an entity.
This is efficiently implemented via a hash table
whose keys and values are both within-document
entity IDs, allowing the implementation of easily-
searched linked lists.
2.1 The baseline system
The baseline system uses a string matching cri-
terion to determine whether two within-document
entities are similar enough to be considered as part
of the same cross-document entity. Given within-
document entities A and B, the criterion is imple-
mented as follows:
1. Find the mention strings {a
1
, a
2
, . . .} and
357
{b
1
, b
2
, . . .} of A and B, respectively that are
the longest for that within-document entity
in the given document. (There may be more
than one longest mention of equal length for
a given entity.)
2. If any longest mention strings a
n
and b
m
exist
such that a
n
= b
m
(exact string match), then
A and B are considered to be part of the same
cross-document entity. Otherwise, they are
considered to be different entities.
When the system decides that two within-
document entities are connected as a single cross-
document entity, it emits a link between within-
document entities A and B represented as the pair
(A, B). We maintain a list of such links, but we
omit all links between within-document entities in
the same document.
The output of the system is a list of pairwise
links. The following two experimental systems
also produce lists of pairwise links. Union is per-
formed between the baseline system?s list and the
lists produced by the other systems to create lists
of pairs that include the information in the base-
line. However, each of the following systems?
outputs are merged separately with the baseline.
By including the baseline results in each system,
we are able to clarify the potential of each addi-
tional technique to improve performance over a
technique that is cheap to run under any circum-
stances, especially given that our experiments are
focused on increasing the number of links in an
Arabic context where links are likely to be dis-
rupted by spelling variations.
2.2 Translingual projection
We implement a novel cross-language approach
for Arabic coreference resolution by expanding
the space of exact match comparisons to approxi-
mate matches of English translations of the Arabic
strings. The intuition for this approach is that of-
ten the Arabic strings of the same named entity
may differ due to misspellings, titles, or aliases
that can be corrected in the English space. The
English translations were obtained using a stan-
dard statistical machine translation system (Chi-
ang, 2007; Li, 2008) and then compared using an
alias match.
The algorithm below describes the approach,
applied to any Arabic named entities that fail the
baseline string-match test:
1. For a given candidate Arabic named entity
pair (A,B), we project them into English by
translating the mentions using a standard sta-
tistical machine translation toolkit. Using the
projected English pair, say, (A
?
, B
?
) we per-
form the following tests to determine whether
A and B are co-referent:
(a) We do an exact string-match test in the
English space using the projected enti-
ties (A
?
, B
?
). The exact string match test
is done exactly as in the baseline system,
using the set of longest named entities in
their respective co-reference chains.
(b) If (A
?
, B
?
) fail in the exact string-match
test as in the baseline, then we test
whether they belong to a list of high con-
fidence co-referent named-entity pairs
1
precomputed for English using alias-
lists derived from Wikipedia.
(c) If (A
?
, B
?
) fails (a) and (b) then (A,B)
is deemed as non-coreferent.
While we hypothesize that translingual projection
via English should help in increasing recall since
it can work with non-exact string matches, it may
also help in increasing precision based on the as-
sumption that a name of American or English ori-
gin might have different variants in Arabic and that
translating to English can help in merging those
variants, as shown in figure 1.
 ????? ??????
 ????? 
?????? 
 ??????? 
 ???????
(Ms. Aisha)
(Aisha)
(Clenton)
(Clinton)
(Cilinton)
Aisha
Aisha
Clinton
Clinton
Clinton
Translate
via SMT
Figure 1: Illustration of translingual projection
method for resolving Arabic named entity strings
via English space. The English strings in paren-
theses indicate the literal glosses of the Arabic
strings prior to translation.
2.3 Entity context similarity
The context of mentions can play an important role
in merging or splitting potential coreferent men-
1
For example: (Sean Michael Waltman, Sean Waltman)
are high confidence-matches even though they are not an
exact-string match.
358
tions. We hypothesize that two mentions in two
different documents have a good chance of refer-
ring to the same entity if they are mentioned in
contexts that are topically very similar. A way of
representing a mention context is to consider the
words in the mention?s neighborhood. The con-
text of a mention can be defined as the words that
surround the mention in a window of n (50 in our
experiments) tokens centered by the mention. In
our experiments, we used highly similar contexts
to link mentions that might be coreferent.
Computing context similarity between every
pair of large number of mentions requires a highly
scalable and efficient mechanism. This can be
achieved using MapReduce, a distributed comput-
ing framework (Dean, 2004)
Elsayed et al (Elsayed, 2008) proposed an ef-
ficient MapReduce solution for the problem of
computing the pairwise similarity matrix in large
collections. They considered a ?bag-of-words?
model where similarity of two documents d
i
and d
j
is measured as follows: sim(d
i
, d
j
) =
?
t?d
i
?d
j
w
t,d
i
? w
t,d
j
, where w(t, d) is the weight
of term t in document d. A term contributes to
each pair that contains it. The list of documents
that contain a term is what is contained in the post-
ings of an inverted index. Thus, by processing
all postings, the pairwise similarity matrix can be
computed by summing term contributions. We use
the MapReduce framework for two jobs, inverted
indexing and pairwise similarity.
Elsayed et al suggested an efficient df-cut strat-
egy that eliminates terms that appear in many doc-
uments (having high df ) and thus contribute less
in similarity but cost in computation (e.g., a 99%
df-cut means that the most frequent 1% of the
terms were discarded). We adopted that approach
for computing similarities between the contexts
of two mentions. The processing unit was rep-
resented as a bag of n words in a window sur-
rounding each mention of a within-document en-
tity. Given a relatively small mention context, we
used a high df-cut value of 99.9%.
3 Experiments
We performed our experiments in the context of
the Automatic Content Extraction (ACE) eval-
uation of 2008, run by the National Institute
of Standards and Technology (NIST). The eval-
uation corpus contained approximately 10,000
documents from the following domains: broad-
cast conversation transcripts, broadcast news tran-
scripts, conversational telephone speech tran-
scripts, newswire, Usenet Newsgroup/Discussion
Groups, and weblogs. Systems were required to
process the large source sets completely. For per-
formance measurement after the evaluation, NIST
selected 412 of the Arabic source documents out
of the larger set (NIST, 2008).
For development purposes we used the NIST
ACE 2005 Arabic data with within-document
ground truth. This consisted of 1,245 documents.
We also used exactly 12,000 randomly selected
documents from the LDC Arabic Gigaword Third
Edition corpus, processed through Serif. The Ara-
bic Gigaword corpus was used to select a thresh-
old of 0.4956 for the context similarity technique
via inspection of (A,B) link scores by a native
speaker of Arabic.
It must be emphasized that there was no ground
truth available for this task in Arabic. Performing
this task in the absence of significant training or
evaluation data is one emphasis of this work.
3.1 Evaluation measures
We used NIST?s scoring techniques to evaluate the
performance of our systems. Scoring for the ACE
evaluation is done using an scoring script provided
by NIST which produces many kinds of statistics.
NIST mainly uses a measure called the ACE value,
but it also computes B-cubed.
B-Cubed represents the task of finding cross-
document entities in the following way: if a user
of the system is searching for a particular Bush
and finds document D, he or she should be able to
find all of the other documents with the same Bush
in them as links from D?that is, cross-document
entities represent graphs connecting documents.
Bagga and Baldwin are able to define precision,
recall, and F-measure over a collection of docu-
ments in this way.
The ACE Value represents a score similar to
B-Cubed, except that every mention and within-
document entity is weighted in NIST?s specifica-
tion by a number of factors. Every entity is worth 1
point, a missing entity worth 0, and attribute errors
are discounted by multiplying by a factor (0.75 for
CLASS, 0.5 for TYPE, and 0.9 for SUBTYPE).
Before scoring can be accomplished, the enti-
ties found by the system must be mapped onto
those found in the reference provided by NIST.
The ACE scorer does this document-by-document,
359
selecting the mapping that produces the highest
score. A description of the evaluation method and
entity categorization is available at (NIST, 2008).
3.2 Results and discussion
The results of running the ACE evaluation script
on the system output are shown in table 1. The
translingual projection system achieves higher
scores than all other systems on all measures. Al-
though it achieves only a 2 point improvement
over the baseline ACE value, it should be noted
that this represents a substantial number of at-
tributes per cross-document entity that it is getting
right.
Thresh B-Cubed ACE
System hold Prec Rec F Val.
Baseline 37.5 44.1 40.6 19.2
TrnsProj 38.4 44.8 41.3 21.2
CtxtSim 0.2 37.6 35.2 36.4 15.9
CtxtSim 0.3 37.4 43.8 40.3 18.9
CtxtSim 0.4 37.5 44.1 40.6 19.3
CtxtSim 0.4956 37.5 44.1 40.6 19.3
CtxtSim 0.6 37.5 44.1 40.6 19.2
Table 1: Scores from ACE evaluation script.
On the other hand, as the context similarity
threshold increases, we notice that the B-Cubed
measures reach identical values with the baseline
but never exceed it. But as it decreases, it loses
B-Cubed recall and ACE value.
While two within-document entities whose
longest mention strings match exactly and are le-
gitimately coreferent are likely to be mentioned in
the same contexts, it seems that a lower (more lib-
eral) threshold introduces spurious links and cre-
ates a different entity clustering.
Translingual projection appears to include links
that exact string matching in Arabic does not?
part of its purpose is to add close matches to those
found by exact string matching. It is able to in-
clude these links partly because it allows access to
resources in English that are not available for Ara-
bic such as Wikipedia alias lists.
4 Conclusions and Future Work
We have evaluated and discussed a set of tech-
niques for cross-document coreference in Arabic
that can be applied in the absence of significant
training and evaluation data. As it turns out, an
approach based on machine translation is slightly
better than a string-matching baseline, across all
measures. It worked by using translations from
Arabic to English in order to liberalize the string-
matching criterion, suggesting that using further
techniques via English to discover links may be
a fruitful future research path. This also seems
to suggest that a Bagga and Baldwin-style vector-
space model may not be the first approach to pur-
sue in future work on Arabic.
However, varying other parameters in the con-
text similarity approach should be tried in order
to gain a fuller picture of performance. One of
them is the df-cut of the MapReduce-based sim-
ilarity computation. Another is the width of the
word token window we used?we may have used
one that is too tight to be better than exact Arabic
string-matching.
References
Javier Artiles and Satoshi Sekine and Julio Gonzalo
2008. Web People Search?Results of the first eval-
uation and the plan for the second. WWW 2008.
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space
model. COLING-ACL 1998.
Y. Chen and J. Martin. 2007. Towards robust unsuper-
vised personal name disambiguation. EMNLP.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied Data Processing on Large Clusters. OSDI.
T. Elsayed and J. Lin and D. W. Oard. 2008. Pair-
wise Document Similarity in Large Collections with
MapReduce. ACL/HLT.
Z. Li and S. Khudanpur. 2008. A Scalable Decoder for
Parsing-based Machine Translation with Equivalent
Language Model State Maintenance. ACL SSST.
L. Lloyd and Andrew Mehler and Steven Skiena. 2006.
Identifying Co-referential Names Across Large Cor-
pora. Combinatorial Pattern Matching.
NIST. 2008. Automatic Content Extraction 2008 Eval-
uation Plan (ACE08).
360
Proceedings of ACL-08: HLT, pages 461?469,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Combining Speech Retrieval Results with Generalized Additive Models
J. Scott Olsson? and Douglas W. Oard?
UMIACS Laboratory for Computational Linguistics and Information Processing
University of Maryland, College Park, MD 20742
Human Language Technology Center of Excellence
John Hopkins University, Baltimore, MD 21211
olsson@math.umd.edu, oard@umd.edu
Abstract
Rapid and inexpensive techniques for auto-
matic transcription of speech have the po-
tential to dramatically expand the types of
content to which information retrieval tech-
niques can be productively applied, but lim-
itations in accuracy and robustness must be
overcome before that promise can be fully
realized. Combining retrieval results from
systems built on various errorful representa-
tions of the same collection offers some po-
tential to address these challenges. This pa-
per explores that potential by applying Gener-
alized Additive Models to optimize the combi-
nation of ranked retrieval results obtained us-
ing transcripts produced automatically for the
same spoken content by substantially differ-
ent recognition systems. Topic-averaged re-
trieval effectiveness better than any previously
reported for the same collection was obtained,
and even larger gains are apparent when using
an alternative measure emphasizing results on
the most difficult topics.
1 Introduction
Speech retrieval, like other tasks that require trans-
forming the representation of language, suffers from
both random and systematic errors that are intro-
duced by the speech-to-text transducer. Limita-
tions in signal processing, acoustic modeling, pro-
nunciation, vocabulary, and language modeling can
be accommodated in several ways, each of which
make different trade-offs and thus induce different
? Dept. of Mathematics/AMSC, UMD
? College of Information Studies, UMD
error characteristics. Moreover, different applica-
tions produce different types of challenges and dif-
ferent opportunities. As a result, optimizing a sin-
gle recognition system for all transcription tasks is
well beyond the reach of present technology, and
even systems that are apparently similar on average
can make different mistakes on different sources. A
natural response to this challenge is to combine re-
trieval results from multiple systems, each imper-
fect, to achieve reasonably robust behavior over a
broader range of tasks. In this paper, we compare
alternative ways of combining these ranked lists.
Note, we do not assume access to the internal work-
ings of the recognition systems, or even to the tran-
scripts produced by those systems.
System combination has a long history in infor-
mation retrieval. Most often, the goal is to combine
results from systems that search different content
(?collection fusion?) or to combine results from dif-
ferent systems on the same content (?data fusion?).
When working with multiple transcriptions of the
same content, we are again presented with new op-
portunities. In this paper we compare some well
known techniques for combination of retrieval re-
sults with a new evidence combination technique
based on a general framework known as Gener-
alized Additive Models (GAMs). We show that
this new technique significantly outperforms sev-
eral well known information retrieval fusion tech-
niques, and we present evidence that it is the ability
of GAMs to combine inputs non-linearly that at least
partly explains our improvements.
The remainder of this paper is organized as fol-
lows. We first review prior work on evidence com-
461
bination in information retrieval in Section 2, and
then introduce Generalized Additive Models in Sec-
tion 3. Section 4 describes the design of our ex-
periments with a 589 hour collection of conversa-
tional speech for which information retrieval queries
and relevance judgments are available. Section 5
presents the results of our experiments, and we con-
clude in Section 6 with a brief discussion of implica-
tions of our results and the potential for future work
on this important problem.
2 Previous Work
One approach for combining ranked retrieval results
is to simply linearly combine the multiple system
scores for each topic and document. This approach
has been extensively applied in the literature (Bartell
et al, 1994; Callan et al, 1995; Powell et al, 2000;
Vogt and Cottrell, 1999), with varying degrees of
success, owing in part to the potential difficulty of
normalizing scores across retrieval systems. In this
study, we partially abstract away from this poten-
tial difficulty by using the same retrieval system on
both representations of the collection documents (so
that we don?t expect score distributions to be signif-
icantly different for the combination inputs).
Of course, many fusion techniques using more ad-
vanced score normalization methods have been pro-
posed. Shaw and Fox (1994) proposed a number
of such techniques, perhaps the most successful of
which is known as CombMNZ. CombMNZ has been
shown to achieve strong performance and has been
used in many subsequent studies (Lee, 1997; Mon-
tague and Aslam, 2002; Beitzel et al, 2004; Lillis et
al., 2006). In this study, we also use CombMNZ
as a baseline for comparison, and following Lil-
lis et al (2006) and Lee (1997), compute it in the
following way. First, we normalize each score si
as norm(si) =
si?min(s)
max(s)?min(s) , where max(s) and
min(s) are the maximum and minimum scores seen
in the input result list. After normalization, the
CombMNZ score for a document d is computed as
CombMNZd =
L?
`
Ns,d ? |Nd > 0|.
Here, L is the number of ranked lists to be com-
bined, N`,d is the normalized score of document d
in ranked list `, and |Nd > 0| is the number of non-
zero normalized scores given to d by any result set.
Manmatha et al (2001) showed that retrieval
scores from IR systems could be modeled using a
Normal distribution for relevant documents and ex-
ponential distribution for non-relevant documents.
However, in their study, fusion results using these
comparatively complex normalization approaches
achieved performance no better than the much sim-
pler CombMNZ.
A simple rank-based fusion technique is inter-
leaving (Voorhees et al, 1994). In this approach,
the highest ranked document from each list is taken
in turn (ignoring duplicates) and placed at the top of
the new, combined list.
Many probabilistic combination approaches have
also been developed, a recent example being Lillis
et al (2006). Perhaps the most closely related pro-
posal, using logistic regression, was made first by
Savoy et al (1988). Logistic regression is one exam-
ple from the broad class of models which GAMs en-
compass. Unlike GAMs in their full generality how-
ever, logistic regression imposes a comparatively
high degree of linearity in the model structure.
2.1 Combining speech retrieval results
Previous work on single-collection result fusion has
naturally focused on combining results from multi-
ple retrieval systems. In this case, the potential for
performance improvements depends critically on the
uniqueness of the different input systems being com-
bined. Accordingly, small variations in the same
system often do not combine to produce results bet-
ter than the best of their inputs (Beitzel et al, 2004).
Errorful document collections such as conversa-
tional speech introduce new difficulties and oppor-
tunities for data fusion. This is so, in particular,
because even the same system can produce drasti-
cally different retrieval results when multiple repre-
sentations of the documents (e.g., multiple transcript
hypotheses) are available. Consider, for example,
Figure 1 which shows, for each term in each of our
title queries, the proportion of relevant documents
containing that term in only one of our two tran-
script hypotheses. Critically, by plotting this propor-
tion against the term?s inverse document frequency,
we observe that the most discriminative query terms
are often not available in both document represen-
462
1
2
3
4
5
0.00.20.40.60.81.0
Inve
rse D
ocum
ent F
requ
ency
Proportion of relevant docs with term in only one transcript source
Figure 1: For each term in each query, the proportion of
relevant documents containing the term vs. inverse doc-
ument frequency. For increasingly discriminative terms
(higher idf ), we observe that the probability of only one
transcript containing the term increases dramatically.
tations. As these high-idf terms make large contri-
butions to retrieval scores, this suggests that even an
identical retrieval system may return a large score
using one transcript hypothesis, and yet a very low
score using another. Accordingly, a linear combina-
tion of scores is unlikely to be optimal.
A second example illustrates the difficulty. Sup-
pose recognition system A can recognize a particu-
lar high-idf query term, but system B never can. In
the extreme case, the term may simply be out of vo-
cabulary, although this may occur for various other
reasons (e.g., poor language modeling or pronuncia-
tion dictionaries). Here again, a linear combination
of scores will fail, as will rank-based interleaving.
In the latter case, we will alternate between taking a
plausible document from systemA and an inevitably
worse result from the crippled system B.
As a potential solution for these difficulties, we
consider the use of generalized additive models for
retrieval fusion.
3 Generalized Additive Models
Generalized Additive Models (GAMs) are a gen-
eralization of Generalized Linear Models (GLMs),
while GLMs are a generalization of the well known
linear model. In a GLM, the distribution of an ob-
served random variable Yi is related to the linear pre-
dictor ?i through a smooth monotonic link function
g,
g(?i) = ?i = Xi?.
Here, Xi is the ith row of the model matrix X (one
set of observations corresponding to one observed
yi) and ? is a vector of unknown parameters to be
learned from the data. If we constrain our link func-
tion g to be the identity transformation, and assume
Yi is Normal, then our GLM reduces to a simple lin-
ear model.
But GLMs are considerably more versatile than
linear models. First, rather than only the Normal dis-
tribution, the response Yi is free to have any distribu-
tion belonging to the exponential family of distribu-
tions. This family includes many useful distributions
such as the Binomial, Normal, Gamma, and Poisson.
Secondly, by allowing non-identity link functions g,
some degree of non-linearity may be incorporated in
the model structure.
A well known GLM in the NLP community is lo-
gistic regression (which may alternatively be derived
as a maximum entropy classifier). In logistic regres-
sion, the response is assumed to be Binomial and the
chosen link function is the logit transformation,
g(?i) = logit(?i) = log
(
?i
1? ?i
)
.
Generalized additive models allow for additional
model flexibility by allowing the linear predictor to
now also contain learned smooth functions fj of the
covariates xk. For example,
g(?i) = X?i ? + f1(x1i) + f2(x2i) + f3(x3i, x4i).
As in a GLM, ?i ? E(Yi) and Yi belongs to the
exponential family. Strictly parametric model com-
ponents are still permitted, which we represent as a
row of the model matrix X?i (with associated param-
eters ?).
GAMs may be thought of as GLMs where one
or more covariate has been transformed by a basis
expansion, f(x) =
?q
j bj(x)?j . Given a set of q
basis functions bj spanning a q-dimensional space
463
of smooth transformations, we are back to the lin-
ear problem of learning coefficients ?j which ?opti-
mally? fit the data. If we knew the appropriate trans-
formation of our covariates (say the logarithm), we
could simply apply it ourselves. GAMs allow us to
learn these transformations from the data, when we
expect some transformation to be useful but don?t
know it?s form a priori. In practice, these smooth
functions may be represented and the model pa-
rameters may be learned in various ways. In this
work, we use the excellent open source package
mgcv (Wood, 2006), which uses penalized likeli-
hood maximization to prevent arbitrarily ?wiggly?
smooth functions (i.e., overfitting). Smooths (in-
cluding multidimensional smooths) are represented
by thin plate regression splines (Wood, 2003).
3.1 Combining speech retrieval results with
GAMs
The chief difficulty introduced in combining ranked
speech retrieval results is the severe disagreement in-
troduced by differing document hypotheses. As we
saw in Figure 1, it is often the case that the most dis-
criminative query terms occur in only one transcript
source.
3.1.1 GLM with factors
Our first new approach for handling differences in
transcripts is an extension of the logistic regression
model previously used in data fusion work, (Savoy
et al, 1988). Specifically, we augment the model
with the first-order interaction of scores x1x2 and
the factor ?i, so that
logit{E(Ri)} = ?0+?i+x1?1+x2?2+x1x2?3,
where the relevance Ri ? Binomial. A factor is
essentially a learned intercept for different subsets
of the response. In this case,
?i =
?
?
?
?BOTH if both representations matched qi
?IBM only di,IBM matched qi
?BBN only di,BBN matched qi
where ?i corresponds to data row i, with associ-
ated document representations di,source and query
qi. The intuition is simply that we?d like our model
to have different biases for or against relevance
based on which transcript source retrieved the doc-
ument. This is a small-dimensional way of damp-
ening the effects of significant disagreements in the
document representations.
3.1.2 GAM with multidimensional smooth
If a document?s score is large in both systems, we
expect it to have high probability of relevance. How-
ever, as a document?s score increases linearly in one
source, we have no reason to expect its probability
of relevance to also increase linearly. Moreover, be-
cause the most discriminative terms are likely to be
found in only one transcript source, even an absent
score for a document does not ensure a document
is not relevant. It is clear then that the mapping
from document scores to probability of relevance is
in general a complex nonlinear surface. The limited
degree of nonlinear structure afforded to GLMs by
non-identity link functions is unlikely to sufficiently
capture this intuition.
Instead, we can model this non-linearity using a
generalized additive model with multidimensional
smooth f(xIBM , xBBN ), so that
logit{E(Ri)} = ?0 + f(xIBM , xBBN ).
Again, Ri ? Binomial and ?0 is a learned inter-
cept (which, alternatively, may be absorbed by the
smooth f ).
Figure 2 shows the smoothing transformation f
learned during our evaluation. Note the small de-
crease in predicted probability of relevance as the
retrieval score from one system decreases, while the
probability curves upward again as the disagreement
increases. This captures our intuition that systems
often disagree strongly because discriminative terms
are often not recognized in all transcript sources.
We can think of the probability of relevance map-
ping learned by the factor model of Section 3.1.1 as
also being a surface defined over the space of input
document scores. That model, however, was con-
strained to be linear. It may be visualized as a col-
lection of affine planes (with common normal vec-
tors, but each shifted upwards by their factor level?s
weight and the common intercept).
464
4 Experiments
4.1 Dataset
Our dataset is a collection of 272 oral history inter-
views from the MALACH collection. The task is
to retrieve short speech segments which were man-
ually designated as being topically coherent by pro-
fessional indexers. There are 8,104 such segments
(corresponding to roughly 589 hours of conversa-
tional speech) and 96 assessed topics. We follow the
topic partition used for the 2007 evaluation by the
Cross Language Evaluation Forum?s cross-language
speech retrieval track (Pecina et al, 2007). This
gives us 63 topics on which to train our combination
systems and 33 topics for evaluation.
4.2 Evaluation
4.2.1 Geometric Mean Average Precision
Average precision (AP) is the average of the pre-
cision values obtained after each document relevant
to a particular query is retrieved. To assess the
effectiveness of a system across multiple queries,
a commonly used measure is mean average preci-
sion (MAP). Mean average precision is defined as
the arithmetic mean of per-topic average precision,
MAP = 1n
?
n APn. A consequence of the arith-
metic mean is that, if a system improvement dou-
bles AP for one topic from 0.02 to 0.04, while si-
multaneously decreasing AP on another from 0.4 to
0.38, the MAP will be unchanged. If we prefer to
highlight performance differences on the lowest per-
forming topics, a widely used alternative is the geo-
metric mean of average precision (GMAP), first in-
troduced in the TREC 2004 robust track (Voorhees,
2006).
GMAP = n
?
?
n
APn
Robertson (2006) presents a justification and analy-
sis of GMAP and notes that it may alternatively be
computed as an arithmetic mean of logs,
GMAP = exp
1
n
?
n
log APn.
4.2.2 Significance Testing for GMAP
A standard way of measuring the significance of
system improvements in MAP is to compare aver-
age precision (AP) on each of the evaluation queries
using the Wilcoxon signed-rank test. This test, while
not requiring a particular distribution on the mea-
surements, does assume that they belong to an in-
terval scale. Similarly, the arithmetic mean of MAP
assumes AP has interval scale. As Robertson (2006)
has pointed out, it is in no sense clear that AP
(prior to any transformation) satisfies this assump-
tion. This becomes an argument for GMAP, since it
may also be defined using an arithmetic mean of log-
transformed average precisions. That is to say, the
logarithm is simply one possible monotonic trans-
formation which is arguably as good as any other,
including the identify transform, in terms of whether
the transformed value satisfies the interval assump-
tion. This log transform (and hence GMAP) is use-
ful simply because it highlights improvements on
the most difficult queries.
We apply the same reasoning to test for statistical
significance in GMAP improvements. That is, we
test for significant improvements in GMAP by ap-
plying the Wilcoxon signed rank test to the paired,
transformed average precisions, log AP. We handle
tied pairs and compute exact p-values using the Stre-
itberg & Ro?hmel Shift-Algorithm (1990). For topics
with AP = 0, we follow the Robust Track conven-
tion and add  = 0.00001. The authors are not aware
of significance tests having been previously reported
on GMAP.
4.3 Retrieval System
We use Okapi BM25 (Robertson et al, 1996) as
our basic retrieval system, which defines a document
D?s retrieval score for query Q as
s(D,Q) =
n?
i=1
idf(qi)
(k3+1)qfik3+qfi )f(qi, D)(k1 + 1)
f(qi, D) + k1(1? b+ b
|D|
avgdl )
,
where the inverse document frequency (idf ) is de-
fined as
idf(qi) = log
N ? n(qi) + 0.5
n(qi) + 0.5
,
N is the size of the collection, n(qi) is the docu-
ment frequency for term qi, qfi is the frequency of
term qi in query Q, f(qi, D) is the term frequency
of query term qi in document D, |D| is the length
of the matching document, and avgdl is the average
length of a document in the collection. We set the
465
BBN
 Sco
re
IBM Score
linear predictor
Figure 2: The two dimensional smooth f(sIBM, sBBN)
learned to predict relevance given input scores from IBM
and BBN transcripts.
parameters to k1 = 1, k3 = 1, b = .5, which gave
good results on a single transcript.
4.4 Speech Recognition Transcripts
Our first set of speech recognition transcripts was
produced by IBM for the MALACH project, and
used for several years in the CLEF cross-language
speech retrieval (CL-SR) track (Pecina et al, 2007).
The IBM recognizer was built using a manually
produced pronunciation dictionary and 200 hours
of transcribed audio. The resulting interview tran-
scripts have a reported mean word error rate (WER)
of approximately 25% on held out data, which was
obtained by priming the language model with meta-
data available from pre-interview questionnaires.
This represents significant improvements over IBM
transcripts used in earlier CL-SR evaluations, which
had a best reported WER of 39.6% (Byrne et al,
2004). This system is reported to have run at ap-
proximately 10 times real time.
4.4.1 New Transcripts for MALACH
We were graciously permitted to use BBN Tech-
nology?s speech recognition system to produce a
second set of ASR transcripts for our experiments
(Prasad et al, 2005; Matsoukas et al, 2005). We se-
lected the one side of the audio having largest RMS
amplitude for training and decoding. This channel
was down-sampled to 8kHz and segmented using an
available broadcast news segmenter. Because we did
not have a pronunciation dictionary which covered
the transcribed audio, we automatically generated
pronunciations for roughly 14k words using a rule-
based transliterator and the CMU lexicon. Using
the same 200 hours of transcribed audio, we trained
acoustic models as described in (Prasad et al, 2005).
We use a mixture of the training transcripts and var-
ious newswire sources for our language model train-
ing. We did not attempt to prime the language model
for particular interviewees or otherwise utilize any
interview metadata. For decoding, we ran a fast (ap-
proximately 1 times real time) system, as described
in (Matsoukas et al, 2005). Unfortunately, as we do
not have the same development set used by IBM, a
direct comparison of WER is not possible. Testing
on a small held out set of 4.3 hours, we observed our
system had a WER of 32.4%.
4.5 Combination Methods
For baseline comparisons, we ran our evaluation on
each of the two transcript sources (IBM and our new
transcripts), the linear combination chosen to opti-
mize MAP (LC-MAP), the linear combination cho-
sen to optimize GMAP (LC-GMAP), interleaving
(IL), and CombMNZ. We denote our additive fac-
tor model as Factor GLM, and our multidimensional
smooth GAM model as MD-GAM.
Linear combination parameters were chosen to
optimize performance on the training set, sweeping
the weight for each source at intervals of 0.01. For
the generalized additive models, we maximized the
penalized likelihood of the training examples under
our model, as described in Section 3.
5 Results
Table 1 shows our complete set of results. This
includes baseline scores from our new set of
transcripts, each of our baseline combination ap-
proaches, and results from our proposed combina-
tion models. Although we are chiefly interested in
improvements on difficult topics (i.e., GMAP), we
present MAP for comparison. Results in bold in-
dicate the largest mean value of the measure (ei-
ther AP or log AP), while daggers (?) indicate the
466
Type Model MAP GMAP
T IBM 0.0531 (-.2) 0.0134 (-11.8)
- BBN 0.0532 0.0152
- LC-MAP 0.0564 (+6.0) 0.0158 (+3.9)
- LC-GMAP 0.0587 (+10.3) 0.0154 (+1.3)
- IL 0.0592 (+11.3) 0.0165 (+8.6)
- CombMNZ 0.0550 (+3.4) 0.0150 (-1.3)
- Factor GLM 0.0611 (+14.9)? 0.0161 (+5.9)
- MD-GAM 0.0561 (+5.5)? 0.0180 (+18.4)?
TD IBM 0.0415 (-15.1) 0.0173 (-9.9)
- BBN 0.0489 0.0192
- LC-MAP 0.0519 (+6.1)? 0.0201 (+4.7)?
- LC-GMAP 0.0531 (+8.6)? 0.0200 (+4.2)
- IL 0.0507 (+3.7) 0.0210 (+9.4)
- CombMNZ 0.0495 (+1.2)? 0.0196 (+2.1)
- Factor GLM 0.0526 (+7.6)? 0.0198 (+3.1)
- MD-GAM 0.0529 (+8.2)? 0.0223 (+16.2)?
Table 1: MAP and GMAP for each combination ap-
proach, using the evaluation query set from the CLEF-
2007 CL-SR (MALACH) collection. Shown in paren-
theses is the relative improvement in score over the best
single transcripts results (i.e., using our new set of tran-
scripts). The best (mean) score for each condition is in
bold.
combination is a statistically significant improve-
ment (? = 0.05) over our new transcript set (that
is, over the best single transcript result). Tests for
statistically significant improvements in GMAP are
computed using our paired log AP test, as discussed
in Section 4.2.2.
First, we note that the GAM model with multi-
dimensional smooth gives the largest GMAP im-
provement for both title and title-description runs.
Secondly, it is the only combination approach able
to produce statistically significant relative improve-
ments on both measures for both conditions. For
GMAP, our measure of interest, these improve-
ments are 18.4% and 16.2% respectively.
One surprising observation from Table 1 is that
the mean improvement in log AP for interleaving is
fairly large and yet not statistically significant (it is
in fact a larger mean improvement than several other
baseline combination approaches which are signifi-
cant improvements. This may suggest that interleav-
ing suffers from a large disparity between its best
and worst performance on the query set.
0.001 0.002 0.005 0.010 0.020 0.050 0.100 0.200
0.00
1
0.00
2
0.00
5
0.01
0
0.02
0
0.05
0
Term recall in IBM transcripts
Term
 reca
ll in B
BN t
rans
cripts
impact guilt
attitudzionism
previou
assembl
Figure 3: The proportion of relevant documents returned
in IBM and BBN transcripts for discriminative title words
(title words occurring in less than .01 of the collection).
Point size is proportional to the improvement in average
precision using (1) the best linear combination chosen to
optimize GMAP (4) and (2) the combination using MD-
GAM (?).
Figure 3 examines whether our improvements
come systematically from only one of the transcript
sources. It shows the proportion of relevant docu-
ments in each transcript source containing the most
discriminative title words (words occurring in less
than .01 of the collection). Each point represents
one term for one topic. The size of the point is pro-
portional to the difference in AP observed on that
topic by using MD-GAM and by using LC-GMAP.
If the difference is positive (MD-GAM wins), we
plot ?, otherwise 4. First, we observe that, when
it wins, MD-GAM tends to increase AP much more
than when LC-GMAP wins. While there are many
wins also for LC-GMAP, the effects of the larger
MD-GAM improvements will dominate for many of
the most difficult queries. Secondly, there does not
appear to be any evidence that one transcript source
has much higher term-recall than the other.
5.1 Oracle linear combination
A chief advantage of our MD-GAM combination
model is that it is able to map input scores non-
linearly onto a probability of document relevance.
467
Type Model GMAP
T Oracle-LC-GMAP 0.0168
- MD-GAM 0.0180 (+7.1)
TD Oracle-LC-GMAP 0.0222
- MD-GAM 0.0223 (+0.5)
Table 2: GMAP results for an oracle experiment in
which MD-GAM was fairly trained and LC-GMAP was
unfairly optimized on the test queries.
To make an assessment of how much this capabil-
ity helps the system, we performed an oracle exper-
iment where we again constrained MD-GAM to be
fairly trained but allowed LC-GMAP to cheat and
choose the combination optimizing GMAP on the
test data. Table 2 lists the results. While the im-
provement with MD-GAM is now not statistically
significant (primarily because of our small query
set), we found it still out-performed the oracle linear
combination. For title-only queries, this improve-
ment was surprisingly large at 7.1% relative.
6 Conclusion
While speech retrieval is one example of retrieval
under errorful document representations, other sim-
ilar tasks may also benefit from these combination
models. This includes the task of cross-language re-
trieval, as well as the retrieval of documents obtained
by optical character recognition.
Within speech retrieval, further work also remains
to be done. For example, various other features are
likely to be useful in predicting optimal system com-
bination. These might include, for example, confi-
dence scores, acoustic confusability, or other strong
cues that one recognition system is unlikely to have
properly recognized a query term. We look forward
to investigating these possibilities in future work.
The question of how much a system should ex-
pose its internal workings (e.g., its document rep-
resentations) to external systems is a long standing
problem in meta-search. We?ve taken the rather nar-
row view that systems might only expose the list of
scores they assigned to retrieved documents, a plau-
sible scenario considering the many systems now
emerging which are effectively doing this already.
Some examples include EveryZing,1 the MIT Lec-
1http://www.everyzing.com/
ture Browser,2 and Comcast?s video search.3 This
trend is likely to continue as the underlying repre-
sentations of the content are themselves becoming
increasingly complex (e.g., word and subword level
lattices or confusion networks). The cost of expos-
ing such a vast quantity of such complex data rapidly
becomes difficult to justify.
But if the various representations of the con-
tent are available, there are almost certainly other
combination approaches worth investigating. Some
possible approaches include simple linear combi-
nations of the putative term frequencies, combina-
tions of one best transcript hypotheses (e.g., us-
ing ROVER (Fiscus, 1997)), or methods exploiting
word-lattice information (Evermann and Woodland,
2000).
Our planet?s 6.6 billion people speak many more
words every day than even the largest Web search
engines presently index. While much of this is
surely not worth hearing again (or even once!), some
of it is surely precious beyond measure. Separating
the wheat from the chaff in this cacophony is the rai-
son d?etre for information retrieval, and it is hard to
conceive of an information retrieval challenge with
greater scope or greater potential to impact our soci-
ety than improving our access to the spoken word.
Acknowledgements
The authors are grateful to BBN Technologies, who
generously provided access to their speech recogni-
tion system for this research.
References
Brian T. Bartell, Garrison W. Cottrell, and Richard K.
Belew. 1994. Automatic combination of multi-
ple ranked retrieval systems. In Proceedings of the
17th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 173?181.
Steven M. Beitzel, Eric C. Jensen, Abdur Chowdhury,
David Grossman, Ophir Frieder, and Nazli Goharian.
2004. Fusion of effective retrieval strategies in the
same information retrieval system. J. Am. Soc. Inf. Sci.
Technol., 55(10):859?868.
W. Byrne, D. Doermann, M. Franz, S. Gustman, J. Hajic,
D.W. Oard, M. Picheny, J. Psutka, B. Ramabhadran,
2http://web.sls.csail.mit.edu/lectures/
3http://videosearch.comcast.net
468
D. Soergel, T. Ward, and Wei-Jing Zhu. 2004. Au-
tomatic recognition of spontaneous speech for access
to multilingual oral history archives. IEEE Transac-
tions on Speech and Audio Processing, Special Issue
on Spontaneous Speech Processing, 12(4):420?435,
July.
J. P. Callan, Z. Lu, and W. Bruce Croft. 1995. Search-
ing Distributed Collections with Inference Networks .
In E. A. Fox, P. Ingwersen, and R. Fidel, editors, Pro-
ceedings of the 18th Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 21?28, Seattle, Washington.
ACM Press.
G. Evermann and P.C. Woodland. 2000. Posterior prob-
ability decoding, confidence estimation and system
combination. In Proceedings of the Speech Transcrip-
tion Workshop, May.
Jonathan G. Fiscus. 1997. A Post-Processing System to
Yield Reduced Word Error Rates: Recogniser Output
Voting Error Reduction (ROVER). In Proceedings of
the IEEE ASRU Workshop, pages 347?352.
Jong-Hak Lee. 1997. Analyses of multiple evidence
combination. In SIGIR Forum, pages 267?276.
David Lillis, Fergus Toolan, Rem Collier, and John Dun-
nion. 2006. Probfuse: a probabilistic approach to data
fusion. In SIGIR ?06: Proceedings of the 29th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 139?146,
New York, NY, USA. ACM.
R. Manmatha, T. Rath, and F. Feng. 2001. Modeling
score distributions for combining the outputs of search
engines. In SIGIR ?01: Proceedings of the 24th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 267?275,
New York, NY, USA. ACM.
Spyros Matsoukas, Rohit Prasad, Srinivas Laxminarayan,
Bing Xiang, Long Nguyen, and Richard Schwartz.
2005. The 2004 BBN 1xRT Recognition Systems
for English Broadcast News and Conversational Tele-
phone Speech. In Interspeech 2005, pages 1641?1644.
Mark Montague and Javed A. Aslam. 2002. Condorcet
fusion for improved retrieval. In CIKM ?02: Proceed-
ings of the eleventh international conference on Infor-
mation and knowledge management, pages 538?548,
New York, NY, USA. ACM.
Pavel Pecina, Petra Hoffmannova, Gareth J.F. Jones, Jian-
qiang Wang, and Douglas W. Oard. 2007. Overview
of the CLEF-2007 Cross-Language Speech Retrieval
Track. In Proceedings of the CLEF 2007 Workshop
on Cross-Language Information Retrieval and Evalu-
ation, September.
Allison L. Powell, James C. French, James P. Callan,
Margaret E. Connell, and Charles L. Viles. 2000.
The impact of database selection on distributed search-
ing. In Research and Development in Information Re-
trieval, pages 232?239.
R. Prasad, S. Matsoukas, C.L. Kao, J. Ma, D.X. Xu,
T. Colthurst, O. Kimball, R. Schwartz, J.L. Gauvain,
L. Lamel, H. Schwenk, G. Adda, and F. Lefevre.
2005. The 2004 BBN/LIMSI 20xRT English Conver-
sational Telephone Speech Recognition System. In In-
terspeech 2005.
S. Robertson, S. Walker, S. Jones, and M. Hancock-
Beaulieu M. Gatford. 1996. Okapi at TREC-3. In
Text REtrieval Conference, pages 21?30.
Stephen Robertson. 2006. On GMAP: and other trans-
formations. In CIKM ?06: Proceedings of the 15th
ACM international conference on Information and
knowledge management, pages 78?83, New York, NY,
USA. ACM.
J. Savoy, A. Le Calve?, and D. Vrajitoru. 1988. Report on
the TREC-5 experiment: Data fusion and collection
fusion.
Joseph A. Shaw and Edward A. Fox. 1994. Combination
of multiple searches. In Proceedings of the 2nd Text
REtrieval Conference (TREC-2).
Bernd Streitberg and Joachim Ro?hmel. 1990. On tests
that are uniformly more powerful than the Wilcoxon-
Mann-Whitney test. Biometrics, 46(2):481?484.
Christopher C. Vogt and Garrison W. Cottrell. 1999. Fu-
sion via a linear combination of scores. Information
Retrieval, 1(3):151?173.
Ellen M. Voorhees, Narendra Kumar Gupta, and Ben
Johnson-Laird. 1994. The collection fusion problem.
In D. K. Harman, editor, The Third Text REtrieval Con-
ference (TREC-3), pages 500?225. National Institute
of Standards and Technology.
Ellen M. Voorhees. 2006. Overview of the TREC 2005
robust retrieval track. In Ellem M. Voorhees and L.P.
Buckland, editors, The Fourteenth Text REtrieval Con-
ference, (TREC 2005), Gaithersburg, MD: NIST.
Simon N. Wood. 2003. Thin plate regression splines.
Journal Of The Royal Statistical Society Series B,
65(1):95?114.
Simon Wood. 2006. Generalized Additive Models: An
Introduction with R. Chapman and Hall/CRC.
469
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1270?1280,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Simulating Early-Termination Search for Verbose Spoken Queries
Jerome White
IBM Research
Bangalore, KA India
jerome.white@in.ibm.com
Douglas W. Oard
University of Maryland
College Park, MD USA
oard@umd.edu
Nitendra Rajput
IBM Research
New Delhi, India
rnitendra@in.ibm.com
Marion Zalk
University of Melbourne
Melbourne, VIC Australia
m.zalk@student.unimelb.edu.au
Abstract
Building search engines that can respond to
spoken queries with spoken content requires
that the system not just be able to find useful
responses, but also that it know when it has
heard enough about what the user wants to be
able to do so. This paper describes a simula-
tion study with queries spoken by non-native
speakers that suggests that indicates that find-
ing relevant content is often possible within
a half minute, and that combining features
based on automatically recognized words with
features designed for automated prediction of
query difficulty can serve as a useful basis for
predicting when that useful content has been
found.
1 Introduction
Much of the early work on what has come to be
called ?speech retrieval? has focused on the use of
text queries to rank segments that are automatically
extracted from spoken content. While such an ap-
proach can be useful in a desktop environment, half
of the world?s Internet users can access the global
information network only using a voice-only mobile
phone. This raises two challenges: 1) in such set-
tings, both the query and the content must be spo-
ken, and 2) the language being spoken will often be
one for which we lack accurate speech recognition.
The Web has taught us that the ?ten blue links?
paradigm can be a useful response to short queries.
That works because typed queries are often fairly
precise, and tabular responses are easily skimmed.
However, spoken queries, and in particular open-
domain spoken queries for unrestricted spoken con-
tent, pose new challenges that call for new thinking
about interaction design. This paper explores the po-
tential of a recently proposed alternative, in which
the spoken queries are long, and only one response
can be played at a time by the system. This ap-
proach, which has been called Query by Babbling,
requires that the user ramble on about what they
are looking for, that the system be able to estimate
when it has found a good response, and that the user
be able to continue the search interaction by bab-
bling on if the first response does not fully meet their
needs (Oard, 2012).
One might question whether users actually will
?babble? for extended periods about their informa-
tion need. There are two reasons to believe that
some users might. First, we are particularly inter-
ested in ultimately serving users who search for in-
formation in languages for which we do not have us-
able speech recognition systems. Speech-to-speech
matching in such cases will be challenging, and we
would not expect short queries to work well. Sec-
ond, we seek to principally serve users who will be
new to search, and thus not yet conditioned to issue
short queries. As with Web searchers, we can ex-
pect them to explore initially, then to ultimately set-
tle on query strategies that work well enough to meet
their needs. If longer queries work better for them,
it seems reasonable to expect that they would use
longer queries. Likewise, if systems cannot effec-
tively use longer queries to produce useful results,
then people will not use them.
To get a sense for whether such an interaction
modality is feasible, we performed a simulation
1270
study for this paper in which we asked people to
babble on some topic for which we already have rel-
evance judgments results. We transcribe those bab-
bles using automatic speech recognition (ASR), then
note how many words must be babbled in each case
before an information retrieval system is first able to
place a relevant document in rank one. From this
perspective, our results show that people are indeed
often able to babble usefully; and, moreover, that
current information retrieval technology could of-
ten place relevant results at rank one within half a
minute or so of babbling even with contemporary
speech recognition technology.
The question then arises as to whether a system
can be built that would recognize when an answer
is available at rank one. Barging in with an answer
before that point wastes time and disrupts the user;
barging in long after that point also wastes time, but
also risks user abandonment. We therefore want a
?Goldilocks? system that can get it just about right.
To this end, we introduce an evaluation measure that
differentially penalizes early and late responses. Our
experiments using such a measure show that systems
can be built that, on average, do better than could be
achieved by any fixed response delay.
The remainder of this paper is organized as fol-
lows: We begin in Section 2 with a brief review of
related work. Section 3 then describes the design
of the ranking component of our experiment; Sec-
tion 4 follows with some exploratory analysis of the
ranking results using our test collection. Section 6
completes the description of our methods with an
explanation of how the stopping classifier is built;
Section 7 then presents end-to-end evaluation results
using a new measure designed for this task. Sec-
tion 8 concludes the paper with some remarks on
future work.
2 Background
The rapid adoption of remarkably inexpensive mo-
bile telephone services among low-literacy users
in developing and emerging markets has generated
considerable interest in so-called ?spoken forum?
projects (Sherwani et al, 2009; Agarwal et al, 2010;
Medhi et al, 2011; Mudliar et al, 2012). It is rel-
atively straightforward to collect and store spoken
content regardless of the language in which it is spo-
ken; organizing and searching that content is, how-
ever, anything but straightforward. Indeed, the cur-
rent lack of effective search services is one of the
key inhibitors that has, to date, limited spoken fo-
rums to experimental settings with at most a few
hundred users. If a ?spoken web? is to achieve the
same degree of impact on the lives of low-literacy
users in the developing world that the World Wide
Web has achieved over the past decade in the devel-
oped world, we will need to develop the same key
enabler: an effective search engine.
At present, spoken dialog systems of conventional
design, such as Siri, rely on complex and expen-
sive language-specific engineering, which can eas-
ily be justified for the ?languages of wealth? such
as English, German, and Chinese; but perhaps not
for many of the almost 400 languages that are each
spoken by a million or more people.1 An alterna-
tive would be to adopt more of an ?information re-
trieval? perspective by directly matching words spo-
ken in the query with words that had been spoken in
the content to be searched. Some progress has been
made on this task in the MediaEval benchmark eval-
uation, which has included a spoken content match-
ing task each year since 2011 (Metze et al, 2012).
Results for six low-resource Indian and African lan-
guages indicate that miss rates of about 0.5 can be
achieved on individual terms, with false alarm rates
below 0.01, by tuning acoustic components that had
originally been developed for languages with rea-
sonably similar phonetic inventories. Our goal in
this paper is to begin to explore how such capabil-
ities might be employed in a complete search en-
gine for spoken forum content, as will be evaluated
for the first time at MediaEval 2013.2 The princi-
pal impediment to development in this first year of
that evaluation is the need for relevance judgments,
which are not currently available for spoken content
of the type we wish to search. That consideration
has motivated our design of the simulation study re-
ported in this paper.
1http://www.ethnologue.com/statistics/
size
2http://www.multimediaeval.org/
mediaeval2013/qa4sw2013/
1271
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  20  40  60  80  100
Re
cip
roc
al 
Ra
nk
Babble position (words)
Topic 274
Babble 1Babble 2Babble 3
Figure 1: Reciprocal ranks at for each query making up a given babble. When retrieving results, a babbler either
?latches? on to a relevant document (Babble 1), moves back-and-forth between relevant documents (Babble 3), or fails
to elicit a relevant document at all (Babble 2).
3 Setup and Method
The approach taken in this paper is to simulate, as
closely as possible, babbling about topics for which
we a) already have relevance judgments available,
and b) have the ability to match partial babbles with
potential answers in ways that reflect the errors in-
troduced by speech processing. To this end, we
chose to ask non-native English speakers to babble,
in English, about an information need that is stimu-
lated by an existing English Text Retrieval Confer-
ence (TREC) topic for which we already have rel-
evance judgments. An English Automatic Speech
Recognition (ASR) system was then used to gener-
ate recognized words for those babbles. Those rec-
ognized words, in turn, have been used to rank order
the (character-coded written text) news documents
that were originally used in TREC, the documents
for which we have relevance judgments. Our goal
then becomes twofold: to first rank the documents
in such a way as to get a relevant document into rank
one; and then to recognize when we have done so.
Figure 1 is a visual representation of retrieval re-
sults as a person babbles. For three different bab-
bles prompted by TREC Topic 274, it shows the re-
ciprocal rank for the query that is posed after each
additional word is recognized. We are primarily in-
terested in cases where the reciprocal rank is one.3
3A reciprocal rank of one indicates that a known relevant
document is in position one; a reciprocal rank of 0.5 indicates
In these three babbles we see all cases that the re-
trieval system must take into account: babbles that
never yield a relevant first-ranked document (Bab-
ble 2); babbles that eventually yield a relevant first-
rank document, and that continue to do so as the
person speaks (Babble 1); and babbles that alternate
between good and bad results as the speaker contin-
ues (Babble 3).
3.1 Acquiring Babbles
Ten TREC-5 Ad Hoc topics were selected for this
study: 255, 257, 258, 260, 266, 271, 274, 276, 287,
and 297 based on our expectation of which of the 50
TREC 5 topics would be most suitable for prompted
babbles. In making this choice, we avoided TREC
topics that we felt would require specialized do-
main knowledge, experience with a particular cul-
ture, or detailed knowledge of an earlier time period,
such as when the topics had been crafted. For each
topic, three babbles were created by people speak-
ing at length about the same information need that
the TREC topic reflected. For convenience, the peo-
ple who created the babbles were second-language
speakers of English selected from information tech-
nology companies. There were a total of ten bab-
blers; each recorded, in English, babbles for three
topics, yielding a total of thirty babbles. We main-
tained a balance across topics when assigning topic
that the most highly ranked known relevant document is in po-
sition two; 0.33 indicates position three; and so on.
1272
Transcribed babble Text from ASR
So long time back one of my friend had a Toyota
Pryus it uses electric and petrol to increase the to
reduce the consumption and increase the mileage
I would now want to get information about why
car operators manufacturers or what do they think
about electric vehicles in the US well this is what
the stories say that the car lobby made sure that the
electric vehicles do not get enough support and the
taxes are high by the government but has it changed
now are there new technologies that enable to lower
cost and also can increase speed for electric vehi-
cles I am sure something is being done because of
the rising prices of fuel these days
So long time at one of my friends headed towards
the previous accuses electric in petrol to increase
the to reduce the consumption and increase the
minutes and would now want to get information
about why car operator manufacturers on what to
think about electric vehicles in the us versus what
the story said that the car lobby make sure that the
electric vehicles to not get enough support to an
attack and I try to comment but has changed now
arctic new technologies that enabled to cover costs
and also can increase speak for electric vehicles I?m
sure some clinton gore carls junior chef
Table 1: Text from an example babble (274-1). The left is transcribed through human comprehension; the right is the
output from an automatic speech recognition engine.
numbers to babblers. All babblers had more than
sixteen years of formal education, had a strong com-
mand on the English language, and had some in-
formation about the topics that they selected. They
were all briefed about our motivation for collecting
this data, and about the concept of query by bab-
bling.
The babbles were created using a phone interface.
Each subject was asked to call an interactive voice
response (IVR) system. The system prompted the
user for a three digit topic ID. After obtaining the
topic ID, the system then prompted the user to start
speaking about what they were looking for. TREC
topics contain a short title, a description, and a nar-
rative. The title is generally something a user might
post as an initial Web query; the description is some-
thing one person might say to another person who
might then help them search; the narrative is a few
sentences meant to reflect what the user might jot
down as notes to themselves on what they were actu-
ally looking for. For easy reference, the system pro-
vided a short description?derived from the descrip-
tion and narrative of the TREC topics?that gave
the user the context around which to speak. The
user was expected to begin speaking after hearing
a system-generated cue, at which time their speech
was recorded. Two text files were produced from the
audio babbles: one produced via manual transcrip-
TREC Topic WER
ID Title Mean SD
255 Environmental protect. 0.434 0.203
257 Cigarette consumption 0.623 0.281
258 Computer security 0.549 0.289
260 Evidence of human life 0.391 0.051
266 Prof. scuba diving 0.576 0.117
271 Solar power 0.566 0.094
274 Electric automobiles 0.438 0.280
276 School unif./dress code 0.671 0.094
287 Electronic surveillance 0.519 0.246
297 Right to die pros/cons 0.498 0.181
Average 0.527 0.188
Table 2: Average ASR Word Error Rate over 3 babbles
per topic (SD=Standard Deviation).
tion,4 and one produced by an ASR system; Table 1
presents an example. The ASR transcripts of the
babbles were used by our system as a basis for rank-
ing, and as a basis for making the decision on when
to barge-in, what we call the ?stopping point.? The
manual transcriptions were used only for scoring the
Word Error Rate (WER) of the ASR transcript for
each babble.
4The transcriber is the third author of this paper.
1273
Judgment at First Rank
Babble Words Relevant Not Relevant Unknown Scorable First Rel Last Rel WER
257-3 74 5 64 5 93% @13 @66 0.414
276-3 61 7 46 8 87% @36 @42 0.720
258-1 146 2 118 26 82% @28 @29 0.528
297-1 117 58 19 40 66% @56 @117 0.594
274-3 94 57 0 47 61% @22 @94 0.250
274-1 105 49 13 43 59% @57 @105 0.437
257-1 191 104 0 87 54% @52 @188 0.764
271-1 145 42 26 76 48% @38 @109 0.556
287-2 61 26 0 35 43% @33 @61 0.889
260-2 93 22 8 63 32% @69 @93 0.500
276-2 69 11 2 56 19% @47 @69 0.795
260-3 82 6 8 68 17% @17 @62 0.370
258-2 94 14 1 79 16% @24 @60 0.389
297-3 90 4 2 84 7% @52 @56 0.312
266-2 115 6 0 109 5% @47 @52 0.745
Table 3: Rank-1 relevance (?Rel?) judgments and position of first and last scorable guesses.
3.2 System Setup
The TREC-5 Associated Press (AP) and Wall Street
Journal (WSJ) news stories were indexed by In-
dri (Strohman et al, 2004) using the Krovetz stem-
mer (Krovetz, 1993), standard English stopword set-
tings, and language model matching. Each babble
was turned into a set of nested queries by sequen-
tially concatenating words. Specifically, the first
query contained only the first word from the bab-
ble, the second query only the first two words, and
so on. Thus, the number of queries presented to In-
dri for a given babble was equivalent to the num-
ber of words in the babble, with each query differ-
ing only by the number of words it contained. The
results were scored using trec eval version 9.0.
For evaluation, we were interested in the reciprocal
rank; in particular, where the reciprocal rank was
one. This measure tells us when Indri was able to
place a known relevant document at rank one.
4 Working with Babbles
Our experiment design presents three key chal-
lenges. The first is ranking well despite errors in
speech processing. Table 2 shows the average Word
Error Rate (WER) for each topic, over three babbles.
Averaging further over all thirty babbles, we see that
about half the words are correctly recognized. While
this may seem low, it is in line with observations
from other spoken content retrieval research: over
classroom lectures (Chelba et al, 2007), call center
recordings (Mamou et al, 2006), and conversational
telephone speech (Chia et al, 2010). Moreover, it is
broadly consistent with the reported term-matching
results for low density languages in MediaEval.
The second challenge lies in the scorability of the
system guesses. Table 3 provides an overview of
where relevance was found within our collection of
babbles. It includes only the subset of babbles for
which, during the babble, at least one known rele-
vant document was found at the top of the ranked
list. The table presents the number of recognized
words?a proxy for the number of potential stop-
ping points?and at how many of those potential
stopping points the document ranked in position 1
is known to be relevant, known not to be relevant, or
of unknown relevance. Because of the way in which
TREC relevance judgments were created, unknown
relevance indicates that no TREC system returned
the document near the top of their ranked list. At
TREC, documents with unknown relevance are typ-
1274
ically scored as if they are not relevant;5 we make
the same assumption.
Table 3 also shows how much we would need to
rely on that assumption: the ?scorable? fraction for
which the relevance of the top-ranked document is
known, rather than assumed, ranges from 93 per cent
down to 5 per cent. In the averages that we report be-
low, we omit the five babbles with scorable fractions
of 30 per cent or less. On average, over the 10 top-
ics for which more than 30 per cent of the potential
stopping points are scorable, there are 37 stopping
points at which our system could have been scored
as successful based on a known relevant document
in position 1. In three of these cases, the challenge
for our stopping classifier is extreme, with only a
handful?between two and seven?of such opportu-
nities.
A third challenge is knowing when to interrupt
to present results. The ultimate goal of our work
is to predict when the system should interrupt the
babbler and barge-in to present an answer in which
they might be interested. Table 3 next presents
the word positions at which known relevant docu-
ments first and last appear in rank one (?First Rel?).
This are the earliest and latest scorable successful
stopping points. As can be seen, the first possi-
ble stopping point exhibits considerable variation,
as does the last. For some babbles?babble 274-3,
for example?almost any choice of stopping points
would be fine. In other cases?babble 258-1, for
example?a stopping point prediction would need to
be spot on to get any useful results at all. Moreover,
we can see both cases in different babbles for the
same topic despite the fact that both babblers were
prompted by the same topic; for example, babbles
257-1 and 257-3, which are, respectively, fairly easy
and fairly hard.
Finally, we can look for interaction effects be-
tween speech processing errors and scorability. The
rightmost column of Table 3 shows the measured
WER for each scorable babble. Of the 10 scorable
babbles for which more than 30 per cent of the po-
tential stopping points are scorable, three turned out
to be extremely challenging for ASR, with word er-
ror rates above 0.7. Overall, however, the WER for
5On the assumption that the TREC systems together span
the range of responses that are likely to be relevant.
the 10 babbles on which we focus is 0.56, which is
about the same as the average WER over all 30 bab-
bles.
In addition to the 15 babbles shown in Table 3,
there are another 15 babbles for which no relevant
document was retrievable. Of those, only a single
babble?babble 255-2, at 54 per cent scorable and
a WER of 0.402?had more than 30 per cent of the
potential stopping points scorable.
5 Learning to Stop
There are several ways in which we could pre-
dict when to stop the search and barge-in with an
answer?in this paper, we consider a machine learn-
ing approach. The idea is that by building a clas-
sifier with enough information about known good
and bad babbles, a learner can make such predic-
tions better than other methods. Our stopping pre-
diction models uses four types of features for each
potential stopping point: the number of words spo-
ken so far, the average word length so far, some
?surface characteristics? of those words, and some
query performance prediction metrics. The surface
characteristics that we used were originally devel-
oped to quantify writing style?they are particularly
useful for generating readability grades of a given
document. Although many metrics for readability
have been proposed, we choose a subset: Flesch
Reading Ease (Flesch, 1948), Flesch-Kincaid Grade
Level (Kincaid et al, 1975), Automated Readabil-
ity Index (Senter and Smith, 1967), Coleman-Liau
index (Coleman and Liau, 1975), Gunning fog in-
dex (Gunning, 1968), LIX (Brown and Eskenazi,
2005), and SMOG Grading (McLaughlin, 1969).
Our expectation was that a better readability value
should correspond to use of words that are more suc-
cinct and expressive, and that a larger number of
more expressive words should help the search en-
gine to get good responses highly ranked.
As post-retrieval query difficulty prediction mea-
sures, we choose three that have been prominent
in information retrieval research: clarity (Cronen-
Townsend et al, 2002), weighted information
gain (Zhou and Croft, 2007), and normalized query
commitment (Shtok et al, 2012). Although each
takes a distinct approach, the methods all compare
some aspect of the documents retrieved by a query
1275
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  20  40  60  80  100
Re
cip
roc
al 
Ra
nk
Babble position (words)
Topic 274, Babble 1
True positive True negative False negative False positive
Figure 2: Predictions for babble 274-1 made by a decision tree classifier trained on 27 babbles for the nine other topics.
For each point, the mean reciprocal rank is annotated to indicate the correctness of the guess made by the classifier.
Note that in this case, the classifier never made a false positive. See Figure 1 for an unannotated version of this same
babble.
Confusion Matrix
Class. Tn Fp Fn Tp F1 Acy.
Bayes 1288 1259 61 291 0.31 55%
Reg. 2522 25 253 99 0.42 90%
Trees 2499 48 70 282 0.83 96%
Table 4: Cross validation accuracy (?Acy.?) measures for
stop-prediction classifiers: naive Bayes, logistic regres-
sion, and Decision trees.
with the complete collection of documents in the
collection from which that retrieval was performed.
They seek to provide some measure of information
about how likely a query is to have ranked the docu-
ments well when relevance judgments are not avail-
able. Clarity measures the difference in the language
models induced by the retrieved results and the cor-
pus as a whole. Weighted information gain and nor-
malized query commitment look at the scores of
the retrieved documents, the former comparing the
mean score of the retrieved set with that of the entire
corpus; the latter measuring the standard deviation
of the scores for the retrieved set.
Features of all four types were were created for
each query that was run for each babble; that is after
receiving each new word. A separate classifier was
then trained for each topic by creating a binary ob-
jective function for all 27 babbles for the nine other
topics, then using every query for every one of those
babbles as training instances. The objective func-
tion produces 1 if the query actually retrieved a rel-
evant document at first rank, and 0 otherwise. Fig-
ure 2 shows an example of how this training data
was created for one babble, and Table 4 shows the
resulting hold-one-topic-out cross-validation results
for intrinsic measures of classifier accuracy for three
Weka classifiers6. As can be seen, the decision tree
classifier seems to be a good choice, so in Section 7
we compare the stopping prediction model based
on a decision tree classifier trained using hold-one-
topic-out cross-validation with three baseline mod-
els.
6 Evaluation Design
This section describes our evaluation measure and
the baselines to which we compared.
6.1 Evaluation Measure
To evaluate a stopping prediction model, the funda-
mental goal is to stop with a relevant document in
rank one, and to do so as close in time as possible
to the first such opportunity. If the first guess is bad,
it would be reasonable to score a second guess, with
some penalty.
Specifically, there are several things that we
6Naive Bayes, logistic regression, and decision trees (J48)
1276
would like our evaluation framework to describe.
Keeping in mind that ultimately the system will in-
terrupt the speaker to notify them of results, we first
want to avoid the interruption before we have found
a good answer. Our evaluation measure gives no
credit for such a guess. Second, we want to avoid
interrupting long after finding the first relevant an-
swer. Credit is reduced with increasing delays after
the first point where we could have barged in. Third,
when we do barge-in, there must indeed be a good
answer in rank one. This will be true if we barge-
in at the first opportunity, but if we barge-in later
the good answer we had found might have dropped
back out of the first position. No credit is given if
we barge-in such a case. Finally, if a bad position
for first barge-in is chosen, we would like at least to
get it right the second time. Thus, we limit ourselves
to two tries, awarding half the credit on the second
try that we could have received had we barged in at
the same point on the first try.
The delay penalty is modeled using an exponen-
tial distribution that declines with each new word
that arrives after the first opportunity. Let q0 be the
first point within a query where the reciprocal rank
is one. Let pi be the first ?yes? guess of the predic-
tor after point q0. The score is thus e?(q0?pi), where
? is the half-life, or the number of words by which
the exponential decay has dropped to one-half. The
equation is scaled by 0.5 if i is the second element
(guess) of p, and by 0.25 if it is the third. From Fig-
ure 1, some cases the potential stopping points are
consecutive, while in others they are intermittent?
we penalize delays from the first good opportunity
even when there is no relevant document in position
one because we feel that best models the user ex-
perience. Unjudged documents in position one are
treated as non-relevant.
6.2 Stopping Prediction Baselines
We chose one deterministic and one random base-
line for comparison. The deterministic baseline
made its first guess at a calculated point in the bab-
ble, and continued to guess at each word thereafter.
The initial guess was determined by taking the aver-
age of the first scorable point of the other 27 out-of-
topic babbles.
The random baseline drew the first and second
words at which to guess ?yes? as samples from a
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0  10  20  30  40  50  60  70  80
Cla
ssi
fie
r g
ue
ss
First Opportunity
treesrandomdeterministic
Figure 3: First guesses for various classifiers plotted
against the first instance of rank one documents within
a babble. Points below the diagonal are places where the
classifier guessed too early; points above are guesses too
late. All 11 babbles for which the decision tree classifier
made a guess are shown.
uniform distribution. Specifically, drawing samples
uniformly, without replacement, across the average
number of words in all other out-of-topic babbles.
7 Results
Figure 3 shows the extent to which each classifiers
first guess is early, on time, or late. These points
falls, respectively, below the main diagonal, on the
main diagonal, or above the main diagonal. Early
guesses result in large penalties from our scoring
function, dropping the maximum score from 1.0 to
0.5; for late guesses the penalty depends on how
late the guess is. As can be seen, our decision tree
classifier (?trees?) guesses early more often than it
guesses late. For an additional four cases (not plot-
ted), the decision tree classifier never makes a guess.
Figure 4 shows the results for scoring at most
three guesses. These results are averaged over all
eleven babbles for which the decision tree classi-
fier made at least one guess; no guess was made on
babbles 257-3, 266-2, 260-3, or 274-3. These re-
1277
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0  5  10  15  20  25  30
Sc
ore
Window
trees random deterministic
Figure 4: Evaluation using all available babbles in which the tree classifier made a guess.
sults are shown for a half-life of five words, which
is a relatively steep penalty function, essentially re-
moving all credit after about ten seconds at normal
speaking rates. The leftmost point in each figure,
plotted at a ?window size? of one, shows the results
for the stopping prediction models as we have de-
scribed them. It is possible, and indeed not unusual,
for our decision tree classifier to make two or three
guesses in a row, however, in part because it has no
feature telling it how long is has been since its most
recent guess. To see whether adding a bit of patience
would help, we added a deterministic period follow-
ing each guess in which no additional guess would
be allowed. We call the point at which this delay ex-
pires, and a guess is again allowed, the delay ?win-
dow.?
As can be seen, a window size of ten or eleven?
allowing the next guess no sooner than the tenth or
eleventh subsequent word?is optimal for the deci-
sion tree classifier when averaged over these eleven
babbles. The random classifier has an optimal point
between window sizes of 21 and 26, but is gener-
ally not as good as the other classifiers. The deter-
ministic classifier displays the most variability, but
for window sizes greater than 14, it is the best solu-
tion. Although it has fewer features available to it?
knowing only the mean number of words to the first
opportunity for other topics?it is able to outperform
the decision tree classifier for relatively large win-
dow sizes.
From this analysis we conclude that our decision
tree classifier shows promise; and that going for-
ward, it would likely be beneficial to integrate fea-
tures of the deterministic classifier. We can also
conclude that these results are, at best, suggestive?
a richer test collection will ultimately be required.
Moreover, we need some approach to accommodate
the four cases in which the decision tree classifier
never guesses. Setting a maximum point at which
the first guess will be tried could be a useful initial
heuristic, and one that would be reasonable to apply
in practice.
8 Conclusions and Future Work
We have used a simulation study to show that build-
ing a system for query by babbling is feasible. More-
over, we have suggested a reasonable evaluation
measure for this task, and we have shown that sev-
eral simple baselines for predicting stopping points
can be beaten by a decision tree classifier. Our next
step is to try these same techniques with spoken
questions and spoken answers in a low-resource lan-
guage using the test collection that is being devel-
oped for the MediaEval 2013 Question Answering
for the Spoken Web task.
Another potentially productive direction for fu-
ture work would be to somehow filter the queries
in ways that improve the rankings. Many potential
users of this technology in the actual developing re-
gion settings that we wish to ultimately serve will
likely have no experience with Internet search en-
gines, and thus they may be even less likely to fo-
1278
cus their babbles on useful terms to the same extent
that our babblers did in these experiments. There
has been some work on techniques for recognizing
useful query terms in long queries, but of course we
will need to do that with spoken queries, and more-
over with queries spoken in a language for which
we have at lest limited speech processing capabili-
ties available. How best to model such a situation
in a simulation study is not yet clear, so we have
deferred this question until the MediaEval speech-
to-speech test collection becomes available.
In the long term, many of the questions we are ex-
ploring will also has implications for open-domain
Web search in other hands- or eyes-free applications
such as driving a car or operating an aircraft.
Acknowledgments
We thank Anna Shtok for her assistance with the un-
derstanding and implementation of the various query
prediction metrics. We also thank the anonymous
babblers who provided data that was imperative to
this study. Finally, we would like to thank the re-
viewers, whose comments helped to improve the
work overall.
References
[Agarwal et al2010] Sheetal K. Agarwal, Anupam Jain,
Arun Kumar, Amit A. Nanavati, and Nitendra Rajput.
2010. The spoken web: A web for the underprivi-
leged. SIGWEB Newsletter, pages 1:1?1:9, June.
[Brown and Eskenazi2005] Jonathan Brown and Maxine
Eskenazi. 2005. Student, text and curriculum mod-
eling for reader-specific document retrieval. In Pro-
ceedings of the IASTED International Conference on
Human-Computer Interaction. Phoenix, AZ.
[Chelba et al2007] Ciprian Chelba, Jorge Silva, and Alex
Acero. 2007. Soft indexing of speech content for
search in spoken documents. Computer Speech and
Language, 21(3):458?478.
[Chia et al2010] Tee Kiah Chia, Khe Chai Sim, Haizhou
Li, and Hwee Tou Ng. 2010. Statistical lattice-based
spoken document retrieval. ACM Transactions on In-
formation Systems, 28(1):2:1?2:30, January.
[Coleman and Liau1975] Meri Coleman and TL Liau.
1975. A computer readability formula designed for
machine scoring. Journal of Applied Psychology,
60(2):283.
[Cronen-Townsend et al2002] Steve Cronen-Townsend,
Yun Zhou, and W. Bruce Croft. 2002. Predict-
ing query performance. In Proceedings of the 25th
annual international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?02, pages 299?306, New York, NY, USA. ACM.
[Flesch1948] Rudolf Flesch. 1948. A new readabil-
ity yardstick. The Journal of applied psychology,
32(3):221.
[Gunning1968] Robert Gunning. 1968. The technique of
clear writing. McGraw-Hill New York.
[Kincaid et al1975] J Peter Kincaid, Robert P Fish-
burne Jr, Richard L Rogers, and Brad S Chissom.
1975. Derivation of new readability formulas (auto-
mated readability index, fog count and flesch reading
ease formula) for navy enlisted personnel. Technical
report, DTIC Document.
[Krovetz1993] Robert Krovetz. 1993. Viewing morphol-
ogy as an inference process. In Proceedings of the
16th annual international ACM SIGIR conference on
Research and development in information retrieval,
SIGIR ?93, pages 191?202, New York, NY, USA.
ACM.
[Mamou et al2006] Jonathan Mamou, David Carmel, and
Ron Hoory. 2006. Spoken document retrieval from
call-center conversations. In Proceedings of the 29th
annual international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?06, pages 51?58, New York, NY, USA. ACM.
[McLaughlin1969] G Harry McLaughlin. 1969. Smog
grading: A new readability formula. Journal of read-
ing, 12(8):639?646.
[Medhi et al2011] Indrani Medhi, Somani Patnaik,
Emma Brunskill, S.N. Nagasena Gautama, William
Thies, and Kentaro Toyama. 2011. Designing
mobile interfaces for novice and low-literacy users.
ACM Transactions on Computer-Human Interaction,
18(1):2:1?2:28.
[Metze et al2012] Florian Metze, Etienne Barnard, Mare-
lie Davel, Charl Van Heerden, Xavier Anguera, Guil-
laume Gravier, Nitendra Rajput, et al 2012. The spo-
ken web search task. In Working Notes Proceedings of
the MediaEval 2012 Workshop.
[Mudliar et al2012] Preeti Mudliar, Jonathan Donner,
and William Thies. 2012. Emergent practices around
cgnet swara, voice forum for citizen journalism in ru-
ral india. In Proceedings of the Fifth International
Conference on Information and Communication Tech-
nologies and Development, ICTD ?12, pages 159?168,
New York, NY, USA. ACM.
[Oard2012] Douglas W. Oard. 2012. Query by babbling.
In CIKM Workshop on Information and Knowledge
Management for Developing Regions, October.
[Senter and Smith1967] RJ Senter and EA Smith. 1967.
Automated readability index. Technical report, DTIC
Document.
1279
[Sherwani et al2009] Jahanzeb Sherwani, Sooraj Palijo,
Sarwat Mirza, Tanveer Ahmed, Nosheen Ali, and Roni
Rosenfeld. 2009. Speech vs. touch-tone: Tele-
phony interfaces for information access by low liter-
ate users. In International Conference on Information
and Communication Technologies and Development,
pages 447?457.
[Shtok et al2012] Anna Shtok, Oren Kurland, David
Carmel, Fiana Raiber, and Gad Markovits. 2012.
Predicting query performance by query-drift estima-
tion. ACM Transactions on Information Systems,
30(2):11:1?11:35, May.
[Strohman et al2004] T. Strohman, D. Metzler, H. Turtle,
and W. B. Croft. 2004. Indri: A language model-
based search engine for complex queries. In Interna-
tional Conference on Intelligence Analysis.
[Zhou and Croft2007] Yun Zhou and W. Bruce Croft.
2007. Query performance prediction in web search en-
vironments. In Proceedings of the 30th annual inter-
national ACM SIGIR conference on Research and de-
velopment in information retrieval, SIGIR ?07, pages
543?550, New York, NY, USA. ACM.
1280
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 417?426,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Encouraging Consistent Translation Choices
Ferhan Ture,
1
Douglas W. Oard,
2,4
Philip Resnik
3,4
1
Department of Computer Science
2
College of Information Studies
3
Department of Linguistics
4
Institute for Advanced Computer Studies
University of Maryland, College Park, MD 20740 USA
fture@cs.umd.edu, oard@umd.edu, resnik@umd.edu
Abstract
It has long been observed that monolingual text
exhibits a tendency toward ?one sense per dis-
course,? and it has been argued that a related
?one translation per discourse? constraint is op-
erative in bilingual contexts as well. In this pa-
per, we introduce a novel method using forced
decoding to confirm the validity of this con-
straint, and we demonstrate that it can be ex-
ploited in order to improve machine translation
quality. Three ways of incorporating such a
preference into a hierarchical phrase-based MT
model are proposed, and the approach where all
three are combined yields the greatest improve-
ments  for  both  Arabic-English  and  Chinese-
English translation experiments.
1 Introduction
In statistical Machine Translation (MT), the state-of-
the-art approach is to translate phrases in the context
of a sentence and to re-order those phrases appro-
priately. Intuitively, it seems as if it should also be
possible to draw on information outside of a single
sentence to further improve translation quality. In
this paper, we challenge the conventional approach
of translating each sentence independently, and ar-
gue that it can indeed also be beneficial to consider
document-scale context when translating text. Mo-
tivated by the success of a ?one sense per discourse?
heuristic in Word Sense Disambiguation (WSD), we
explore the potential  benefit of leveraging a ?one
translation per discourse? heuristic in MT.
The paper is organized as follows. We begin with
related work in Section 2. Next, we provide new
confirmation that the hypothesized one-translation-
per-discourse  condition  does  indeed  often  hold,
based  on  a  novel  analysis  using  forced  decoding
(Section 3). We incorporate this idea into a hierarchi-
cal MT framework by adding three new document-
scale features to the translation model (Section 4).
We then present  experimental  results  demonstrat-
ing  solid  improvements  in  translation  quality  ob-
tained by leveraging these features, both for Arabic-
English (Ar-En) and Chinese-English (Zh-En) trans-
lation (Section 5). Conclusions and future work are
presented in Section 6.
2 Related work
Exploiting  discourse-level  context  has  to  date
received  only  limited  attention  in  MT re-
search (e.g., (Gime?nez  and  Ma`rquez, 2007; Liu
et al, 2010; Carpuat, 2009; Brown, 2008; Xiao et
al., 2011)). Exploratory analysis of reference trans-
lations by Carpuat  (2009)  motivates  a  hypothesis
that MT systems might benefit from the ?one sense
per discourse? heuristic, first introduced by Gale et
al. (1992), which has proven to be effective in the
context of WSD (Yarowsky, 1995). Carpuat?s ap-
proach was to do post-processing on the translation
output to impose a ?one translation per discourse?
constraint where the system would otherwise have
made a different choice. A manual evaluation on
a sample of sentences suggested promise from the
technique, which  Carpuat  suggested  in  favor  of
exploring more integrated approaches.
Xiao et al (2011) took this one step further and
implement an approach where they identified am-
biguous translations within each document, and at-
417
tempt to fix them by replacing each ambiguity with
the most frequent translation choice. Based on their
error analysis, the authors indicate two shortcomings
when trying to find the correct translation of a given
phrase. First, frequency may not provide sufficient
information to distinguish between translation can-
didates, which is why we take rareness into account
when scoring translation candidates. Another prob-
lem is, like any other heuristic, that there may be
cases where the heuristic fails and there are multi-
ple senses per discourse. Guaranteeing consistency
hurts performance in such situations, which is why
we implement the heuristic as a model feature, and
let the model score decide for each case.
We are aware of a few other analyses that have
shown promising results based on a similar motiva-
tion. For instance, Wasser and Dorr (2008)?s ap-
proach biases the MT system based on term statistics
from relevant documents in comparable corpora. Ma
et al (2011) show that a translation memory can be
used to find similar source sentences, and consecu-
tively adapt translation choices towards consistency.
Domain adaptation for MT has has also been shown
to be useful in some cases (Bertoldi and Federico,
2009; Hildebrand et al, 2005; Sanchis-Trilles and
Casacuberta, 2010; Tiedemann, 2010; Zhao et al,
2004), so to the extent we consider documents to be
micro-domains we might expect similar approaches
to be useful at document scale. Indeed, hints that
such ideas may work have been available for some
time. For example, there is clear evidence that the
behavior of human translators can provide evidence
that is often useful for automating WSD (Diab and
Resnik, 2002; Ng et al, 2003). When coupled with
the one-sense-per-discourse heuristic, this suggests
that the reverse may also be true.
3 Exploratory analysis
It is well known that writing styles vary by genre,
and in particular that the amount of vocabulary vari-
ation within a document depends to some extent on
the genre (e.g., higher in poetry than in engineering
writing). The degree to which authors tend to make
consistent word choices in any particular genre is,
therefore, an empirical question. In order to gain in-
sight into the extent to which human translators make
consistent vocabulary choices in the types of materi-
als that we wish to translate (in this work, news sto-
ries), we first explore the degree of support for our
one-translation-per-discourse hypothesis in the ref-
erence translations of a standard MT test collection.
We used the Ar-En MT08 data set, which con-
tains 74 newswire documents with a total  of 813
sentences, each of which has four reference trans-
lations. Throughout this paper we consistently use
the  document  (i.e., one  news  story)  as  a  conve-
nient discourse unit, although of course finer-scale or
broader-scale discourse units might also be explored
in future work. Moreover, throughout this paper we
use the hierarchical phrase-based translation system
(Hiero), which is based on a synchronous context-
free grammar (SCFG) model (Chiang, 2005). In a
SCFG, the rule [X] ||| ? ||| ? indicates that con-
text free expansion X ? ? in the source language
can occur synchronously with X ? ? in the target
language. In this case, we call ? the left hand side
(LHS) of the rule, and ? the right hand side (RHS)
of the rule.
To determine the extent and nature of translation
consistency choices made by human translators, we
randomly selected one of the four sets of reference
translations (first set, with id 0) and we used forced
decoding to find all possible sequences of rules that
could transform the source sentence into the target
sentence. In forced decoding, given a pair of source
and target sentences, and a grammar consisting of
learned translation rules with associated probabili-
ties, the decoder searches all possible derivations for
the one sequence of rules that is most likely (under
the learned translation model) to synchronously pro-
duce the source sentence on the LHS and the target
sentence on the RHS. For instance, consider the fol-
lowing Arabic sentence as input:
???? ??? ?????????? ??????? .
and its uncased reference translation:
there is a link between the three attacks .
The following four rules, which are part of the SCFG
learned from the the same translation pairs, allows
the decoder to find a sequence of derivations that
?translates? the source-side Arabic sentence into the
418
X16 
X7  ??? X12 
X3  ??????????  ????  ??????? .
R1 
R2 R3 
R4 
X16 
X7 between X12 
X3  the  there  attacks .  three  is  a  link 
R1 
R2 R3 
R4 
Figure 1: Illustration of forced decoding.
target-side reference translation.
1
R1. [X12] ||| ???? ||| there is a link
R2. [X16] ||| [2] ??? [1] ||| [X12, 1] between [X7, 2]
R3. [X7] ||| [1] ?????????? . ||| [X3, 1] attacks .
R4. [X3] ||| ??????? ||| the three
Figure 1 illustrates how the decoder uses these
rules  to  produce the source and target  sides  syn-
chronously.
As we repeated this  procedure  for  all  sentence
pairs, we kept track of all rules that were actually
used by the decoder to generate a reference English
translation from the corresponding Arabic sentences.
Our next step was to identify cases in which the
SCFG could reasonably have produced a substan-
tially  different  translation. Whenever  an  Arabic
phrase f occurs multiple times in a document, and f
appears on the LHS of two or more different gram-
mar rules in the SCFG, we count this as a single
?case?.
2
These cases correspond to unique (source
phrase f , document d) pairs in which a translation
process using that SCFG could have chosen to pro-
duce two or more different translations of f in d.
Since the multiple appearances of f are distributed
among sentences of d, each counted case may cor-
respond to a number of sentences ranging from 1 to
the number of sentences in that document.
Table 1 shows a small sample of the cases (i.e.,
(source phrase f , document d) pairs) identified as a
result of forced decoding. There were 321 such cases
in our dataset and there were 672 sentences in which
at least one case occurred. This is not an uncommon
phenomenon; these 672 sentences comprise 83% of
1
Since our goal was an exploratory analysis, the MT08 test
set was combined with the training set in order to ensure reach-
ability of the reference translations using the learned grammar.
Proper train/dev/test splits were, of course, used for the evalua-
tion results reported in Section 5.
2
We define a phrase as any text that constitutes the entire
LHS of a grammar rule.
the test set. However, many of these cases repre-
sent either unlikely choices or inconsequential dif-
ferences, so some post-processing is called for.
Since  grammar  rules  are  typically  more  fine-
grained than is necessary for our purposes (e.g., to
capture various punctuation and determiner differ-
ences that do not affect the ?sense? of the transla-
tion), we applied a few simple heuristics to edit the
source and target  sides and group all  such minor
variations into a single ?mega-rule? (e.g., ?how???,
how?, ?third???a third?, ?want???we want?). For
this, we removed nonterminal symbols and punc-
tuation, and  considered  two target  phrases e and
e? to  be different only  if edit distance(e, e?) >
max(length(e), length(e?))/2, where the edit dis-
tance is based on character removal and insertion.
For instance, the third example in Table 1 would
have been considered to be translated consistently
as a result of this heuristic, as opposed to the first
example. We also eliminated cases in which no rea-
sonable alternatives were available in the translation
grammar (i.e., cases where the second most probable
rule with the same LHS was assigned a probability
below 0.1 in the grammar). Cases 4 and 5 would
have been removed by this heuristic.
After this filtering and aggregation we were left
with 176 (f , d) pairs in which the translation model
could reasonably have selected between rules that
would have produced substantially different English
translations of f in d (such as cases 1?3 and 6?9).
It was these 176 cases, affecting a total of 512 sen-
tences (63% of test set) for which we then examined
what forced decoding could tell us about translation
consistency.
So now that we know what the human who pro-
duced the reference translations actually did (accord-
ing to forced decoding), and in which cases they
might reasonably have chosen to do something sub-
stantially different (according to the SCFG), we can
ask in which cases the human (effectively) made a
consistent choice of translation rules when encoun-
tering the same Arabic phrase in the same document.
In 128 of the 176 cases, that is what they did (i.e.,
when the same phrase occurred multiple times in a
single document and more than one translation was
reasonably possible, forced decoding indicated that
the human translator translated that phrase in essen-
tially the same way). These cases affected the trans-
419
Case
Translation counts
Source phrase Doc #
???? 566 that killed = 1
killing of = 1
??????? 782 hostages = 2
??????? 138 hostage = 1
hostages = 2
????? 466 korea = 2
????? 763 korea = 2
?? 30 from = 2
?? 7 of = 1
from = 1
?? ?????? 717 of the current = 2
???? 30 the = 1
which =1
Table 1: A sample of cases (i.e., (source phrase f , docu-
ment d) pairs) identified as a result of forced decoding.
lation of 455 sentences (56% of the test set), suggest-
ing that if we can replicate this human behavior in a
system, it might affect a nontrivial number of trans-
lation choices.
These statistics also suggest, however, that there
may be some risk incurred in such a process, since
in 48 of the 176 cases, the human translator opted
for a substantially different translation. When we
closely examined these 48 instances, we found that
19 (40%) involved changing a content-bearing word
(sometimes to a word with similar meaning). The re-
maining 29 (60%) involved function words or simi-
lar constructions. See Figures 2 and 3 for examples.
1a. [X] ||| ???? ||| had allowed
1b. [X] ||| ???? ||| has permitted
2a. [X] ||| [X,1] ???? ||| examining [X,1]
2b. [X] ||| [X,1] ???? ||| is considering [X,1]
3a. [X] ||| [X,1] ?????? ||| neighbors
3b. [X] ||| [X,1] ?????? ||| neighboring countries
Figure 2: Examples of differences in lexical choice for
content-bearing words within the same document.
We can make several observations based on this
analysis. First, there does indeed seem to be ev-
idence to support the one-translation-per-discourse
heuristic, and to suggest that respecting that heuris-
4a. [X] ||| ?? ||| on
4b. [X] ||| ?? ||| in
4c. [X] ||| ?? ||| ?s
5a. [X] ||| ?? ||| had
5b. [X] ||| ?? ||| was
Figure 3: Examples of differences in lexical choice for
other types of lexical units within the same document.
tic could improve translation outcomes for a substan-
tial number of sentences. Second, even when a ref-
erence translation contains different translations of
the same phrase, this may sometimes be the result of
stylistic choices rather than an intent by the transla-
tor to affect the expressed meaning. If a system were
try to ?fix? such cases by enforcing consistent trans-
lation, the resulting translation might be somewhat
more stilted, but perhaps not less accurate or less in-
telligible. Finally, sentence structure conventions or
other language-specific phenomena may sometimes
require the same phrase to be translated differently,
so some way of encouraging consistency while still
allowing the model to consider other contextual fac-
tors might be better than always imposing a hard con-
sistency constraint.
4 Approach
To incorporate document-level features into an MT
system  that  would  otherwise  operate  with  only
sentence-level  evidence, we  added  three  super-
sentential ?consistency features? to the translation
model. The decoder computes scores for these fea-
tures in two passes over each document; in each pass,
each sentence in the document is decoded. In the
first pass, the decoder keeps track of the number of
occurrences of some aspects of each grammar rule
and stores that information. The consistency fea-
tures are disabled during this pass, and do not affect
decoder scoring. In the second pass, each grammar
rule is assigned as many as three consistency feature
scores, each of which is based on some frozen counts
from the first pass. These features are designed to
introduce a bias towards translation consistency, but
to leave the final decision to the decoder, which of
course also has access to other  features from the
translation and language model. At this point we are
more interested in effectiveness than efficiency, so
420
we simply note that this approach doubles the run-
ning time of the decoder and that future work on a
more elegant implementation might be productive.
We explore three ways to compute features in this
section. The essential idea behind all of them is to
define some feature function that increases monoton-
ically with an increase in some count that we believe
to be informative, and in which the rate of increase is
damped more strongly as that count increases. Sev-
eral feature functions could satisfy those broad re-
quirements; in this section, we describe three vari-
ants, C1, C2 and C3, and discuss the potential bene-
fits and drawbacks of each.
C1: Counting rules In this variant, we count in-
stances of the same entire grammar rule, where a rule
r contains both the source phrase f and the target
phrase e. During the first pass, whenever a grammar
rule is chosen by the decoder for the one-best output,
the count for that rule is incremented. Given a gram-
mar rule r and the number of times r was counted in
the first pass (given by N{r}), the consistency fea-
ture score is computed as follows:
C1(r) =
2.2N{r}
1.2 + N{r}
(1)
Equation 1 is the term frequency component of the
well known Okapi BM25 term weighting function,
when parameters are set to the conventional values
k = 1.2, b = 0. This is an increasing and con-
cave function in which the count has a diminishing
marginal effect on the feature score. It has proven
to be useful in information retrieval applications, in
which the goal is to model ?aboutness? based on term
counts (Robertson et al, 1994). Because our goal is
to demonstrate the potential of consistency features,
it seemed reasonable to work with some simple func-
tion that has a shape like the one we desired. We
leave exploration of optimal damping functions for
future work.
A drawback of this C1 approach is that as we saw
in Section 3, grammar rules in phrase-based MT sys-
tems tend to be somewhat more fine-grained than
seems optimal for constructing a consistency fea-
ture. For instance, consider the following rules that
all translate the same Arabic term:
R1. [X] ||| [X,1] ????? ||| [X,1] the bodies
R2. [X] ||| [X,1] ????? ||| [X,1] the organs
R3. [X] ||| [X,1] ????? ||| [X,1] organs
R4. [X] ||| ????? [X,1] ||| the organs of [X,1]
R5. [X] ||| ????? [X,1] ||| [X,1] bodies
Based on these grammar rules, we as human read-
ers infer that this Arabic phrase can be translated in
two different ways: as organs or as bodies. An opti-
mal application of the one-translation-per-discourse
heuristic would thus group the rules based on the
presence of one of those words. However, in the C1
variant, each of these rules would be counted sepa-
rately because of differences that in some cases do
not directly affect the choice of content words. For
instance, on the source side, the Arabic token ap-
pears to the right of the nonterminal symbol in R1,
R2 andR3, while it is to the left of the nonterminal in
R4 andR5. On the target side, differences are due to
both nonterminal symbol position and the existence
of determiners. Motivated by many examples like
this, we came up with an alternative way of count-
ing rules.
C2: Counting target tokens To partially address
this sparseness issue, variant C2 focuses only on the
target side. We extract all target tokens whenever a
grammar rule is used by the decoder in a one-best
derivation and increment a counter for each. Since
we are mainly interested in content words (e.g. bod-
ies, organs), we use simple pattern matching to dis-
card nonterminal symbols and punctuation, and we
ignore terms that appear in more than 50% of all doc-
uments (a convenient way of discarding common to-
kens such as the, or, and). This approach separates
the rules in the example above into two groups: rules
with bodies on the target side and rules with organs
on the target side. Upon completion of the first pass,
the consistency feature score for rule r is then de-
termined by first computing a score for each unique
target-side token w using:
bm25(w) = 2.2N{w}
1.2 + N{w}
log
D + 1
DF (w) + 0.5
(2)
where in this caseN{w}maps tokens to their respec-
tive counts in the document, D is the total number
of documents in the collection, and DF (document
frequency) is the number of documents in which the
token occurs. This is a fuller version of the BM25
function in which (in the information retrieval ap-
plication) both high term frequencies and rare terms
421
are rewarded. We then set the feature score for each
rule r to the maximum score of any of its target-side
terminal tokens:
C2(r) = max
e?RHS(r)
bm25(e) (3)
Our motivation for choosing the maximum is that
when there is more than one content word that sur-
vives the pruning of common terms, we want the
score to be influenced most strongly by the most im-
portant of those terms. Since BM25 term weights
can be thought of as a measure of term importance,
taking the maximum is a simple expedient.
Although counting only target-side tokens yields
coarser granularity than counting rules, ignoring the
source side of the rule risks combining target side
statistics from translations of unrelated source lan-
guage terms. Consider the following grammar rule:
R6. [X] ||| <s> [X,1] ????? ||| <s> [X,1] life support
Since the counter for life and support will both be
incremented whenever rule R6 fires in the one-best
decoding during the first pass, problems could arise
if a rule with a different LHS that also contains sup-
port on the RHS were to fire in the same document,
for example:
R7. [X] ||| ?????? ||| support
If we don?t take the source side into account, both oc-
currences of support will be grouped together when
counting and R7 will receive extra score from the
consistency feature whenever R6 is used by the de-
coder. Of course, this problem will only arise when
the LHS of R6 and R7 are present in the same doc-
ument, and how often that happens (and thus how
large the risk from this factor is) is an empirical ques-
tion. We therefore developed a third alternative as a
middle ground between the fine-grained C1 and the
coarse-grained C2.
C3: Counting  token  translation  pairs In  this
variant, we count each terminal (source token, tar-
get token) pair that survives pruning. Specifically,
if grammar rule [X]|||f1f2...fm|||e1e2...en fires, we
increment the count of every pair ?fi, ej?, where fi
is aligned to ej . After the first pass, we compute the
feature value of each observed pair, based on this
count and the DF of the target-side of the pair. We
chose to use only the target token in the DF com-
putation (i.e., aggregating over all source tokens) to
reduce sparsity effects. Similar to C2, the feature of
a rule r is defined by the maximum of scores of all
pairs extracted from r.
C3(r) = max
f?LHS(r)
e?RHS(r)
?f,e? aligned
bm25(?f, e?) (4)
Since each variant has its benefits and drawbacks, we
can include all three in the system and let the tuning
process decide on how each should be weighted.
5 Evaluation and Discussion
We have evaluated the one-translation-per-discourse
feature using the cdecMT system (Dyer et al, 2010).
We started by building a baseline system using stan-
dard features in cdec: lexical and phrase transla-
tion probabilities in both directions, word and arity
penalty features, and a 5-gram language model. We
then added each of the three consistency feature vari-
ants, along with all two-way and the one three-way
combinations of them, thus yielding a total of eight
systems for comparison, including the baseline.
For training the Ar-En system, we used the dataset
from the DARPA GALE evaluation (Olive et  al.,
2011), which consists of NIST and LDC releases.
The corpus was filtered to remove sentence pairs
with  anomalous  length  ratios  and  subsampled  to
yield a training set containing 3.4 million parallel
sentence pairs. The Arabic text was preprocessed to
produce two different segmentations (simple punctu-
ation tokenization with orthographic normalization,
and LDC?s ATBv3 representation (Maamouri et al,
2008)), represented together using cdec?s lattice in-
put format (Dyer et al, 2008).
The Zh-En system was trained on parallel train-
ing text consisting of the non-UN portions and non-
HK Hansards portions of the NIST training corpora.
Chinese was automatically segmented by the Stan-
ford segmenter (Tseng et al, 2005), and traditional
characters were simplified. After subsampling and
filtering, we obtain a training corpus of 1.6 million
parallel sentences.
Both  training  sets  were  word-aligned  with
GIZA++ (Och and Ney, 2003), using 5 Model  1
and  5  HMM iterations. A SCFG was  then  ex-
tracted from these alignments using a suffix array
extractor (Chiang, 2007). Evaluation was done with
multi-reference BLEU (Papineni et al, 2002) on test
422
sets with four references for each language pair, and
MIRA was used for tuning (Crammer et al, 2006).
In our experiments, we run the first decoding phase
using feature weights that are guessed heuristically
based on weights from previously tuned systems.
All feature weights, including the discourse feature,
were then tuned together, based on the output  of
the  second  decoding  phase. For  Ar-En  parame-
ter  tuning, we  used  the  MT06 newswire  dataset,
which contains 104 documents and a total of 1,797
sentences. For testing, we used the MT08 dataset
described  above  (74  documents, 813  sentences).
For Zh-En experiments, the MT02 newswire dataset
(100 documents, 878 sentences) was used for tuning,
and evaluation was done on the MT06 test set (79
documents, 1,664 sentences). For  both language
pairs, DF values were computed from the tuning
set for both tuning and evaluation experiments.
When we used NIST?s official metric (BLEU-4)
to compare our results to the official NIST evalu-
ation (NIST, 2006; NIST, 2008), our baseline sys-
tem achieved 54.70 for  Ar-En and 31.69 for  Zh-
En. Based on reported NIST results, our baseline
would have ranked 4
th
in the Zh-En MT06 evalua-
tion, and would have outperformed all Ar-En MT08
systems. We used a slightly different IBM-BLEU
metric for the rest of our evaluation. In this case,
the baseline system achieved 53.07 BLEU points
for  Ar-En  and  30.43  points  for  Zh-En. Among
more recent papers, the best reported results were
56.87  for  Ar-En  MT08 (Zhao  et  al., 2011a)  and
35.87 for Zh-En MT06 (Zhao et al, 2011b), although
many papers report BLEU scores below 53 points
for Arabic (Carpuat et al, 2011) and 32 points for
Chinese (Monz, 2011). The systems that outper-
formed our baseline applied novel techniques, and
used larger language models, as well as many non-
standard features. We argue that these novelties are
complementary to our approach, and therefore do not
damage the credibility of our baseline.
Among the single-feature runs, C3 had the best
performance  in  Ar-En  experiments, with  53.84
BLEU points, whereas C2 yielded the best results
for Zh-En with a BLEU score of 30.96. In any case,
all three variants outperformed the baseline (see Ta-
ble 2). When multiple features were combined, we
generally observed an increase in BLEU, suggesting
that our features have usefully different error char-
Method BLEU
Ar-En Zh-En
Baseline 53.07 30.43
C1 53.82 30.59
C2 53.70 30.96
C3 53.84 30.54
C12 53.82 30.79
C13 53.82 30.76
C23 53.88 30.63
C123 53.98 31.42
Table 2: Evaluation results: BLEU scores with four ref-
erences for Ar-En and Zh-En experiments.
Method # documents
Ar-En Zh-En
Docs 74 79
C1 37 30
C2 37 35
C3 42 36
C123 43 41
Table 3: Doc-level analysis: Number of documents where
each variant outperforms baseline.
acteristics. The combination of all three variants,
C123, yielded the best results, nearly 1.0 BLEU point
higher  than  the  baseline  for  both  language  pairs.
Evaluation results are summarized in Table 2.
Given our focus on documents, it  is  natural  to
ask  what  fraction  of  the  documents  were  helped
or  harmed  by  consistency  features. Document-
level  BLEU scores  for  Arabic-to-English  transla-
tions show that C3 outperformed the baseline on a
larger number of documents than any other single
feature (42/74=57%), compared with 37/74 (50%)
for both C1 and C2. C123 did better by this measure
as well, with BLEU increasing for 43 of the docu-
ments. There were no documents where the BLEU
score  was  exactly  the  same, therefore  the  BLEU
score declined for the remaining documents. As Ta-
ble 3 indicates, document-level BLEU for the Zh-En
experiments shows similar results.
We can also look at our results in a more fine-
grained way, focusing on differences in how each
system translated the same source-language phrase.
For  this  analysis, we  defined  English  phrases e
and e? to  be different if edit distance(e, e?) >
423
Method Ar-En Zh-En
Cases Test set Cases Test set
C1 77 24% 401 48%
C2 127 35% 686 60%
C3 101 33% 491 53%
Any 197 68% 968 94%
C123 141 41% 651 59%
Table 4: Effect of applying variants of the consistency
feature (Any=C1 or C2 or C3).
max(length(e), length(e?))/2. By  this  way  of
counting, there are 197 unique (Arabic phrase, docu-
ment) pairs for which at least one single-feature sys-
tem produced translations differently from the base-
line system. Together, these cases affect 553 sen-
tences (68%) in 67 of the 74 documents, with as
many as 12 differences observed in a single doc-
ument. The  number  of  such  differences  is  even
higher for Chinese-to-English translation, probably
due to lower confidence from the translation model
and longer documents. Table 4 shows the number of
changes by each system, and the percentage of the
test set affected by these changes.
In order to gain greater insight into the effect of
the consistency features, we randomly sampled 60
of the 197 cases and analyzed the influence of the
change to the document BLEU score. In 25 of the
sampled cases, at least one of the three systems made
a change that improved the BLEU score, whereas the
score was adversely affected for at least one system
in 13 cases. BLEU remained unchanged in the re-
maining 22 cases, mostly due to the use of multi-
ple reference translations. When we analyze the ef-
fect of each system separately, we see that C2 was
the most aggressive, making 25 changes that influ-
enced BLEU (16 positive, 9 negative). C1 was the
most conservative, with only 13 such changes (8 pos-
itive, 5 negative). Consistent with the overall BLEU
scores, C3 evidenced the best ratio between benefit
and harm, making 20 changes that affected the score
(16 positive, 4 negative).
Looking at specific cases can yield some insight
into how the consistency features achieve improve-
ments. For example, results improved when trans-
lating the phrase ???????, (Eng. organizational,
regulatory), which appears in the context of organi-
zational groups that support terrorist ideology. The
baseline system translated this as organizational in
one case, and regulatory in another. Variants C1
and C2 changed this behavior, so that the translation
was organizational in both cases. One of the refer-
ence translations used organizational in one case and
dropped the phrase in the other, and the other three
translators  provided  consistent  translations  (using
organized and organizational). As a result, applying
the one-translation-per-discourse heuristic improved
the multi-reference BLEU score.
On the other hand, here is one of the cases where
our  feature  hurt  performance. The  phrase ??
?? (Eng. border/frontier troops/guards) appears
in two sentences of a Chinese news story about vio-
lence along the India - Nepal border. All reference
translations consistently used the word border in the
translation, as it is a better choice in this context.
The baseline system translated the phrase as fron-
tier guards and border troops in the two sentences.
All system variants replaced border with frontier to
maintain consistency, and therefore produced worse
translations, causing a decrease in BLEU score.
Examples can, however, also point up limitations
in our ability to measure improvements. In one of
the test documents, the Arabic phrase ?????? ???
(Eng. sneak, infiltrate, enter without approval) ap-
pears in the context of Turkey trying to enter the Eu-
ropean Union. This was translated by the baseline
system as sneak into in one occurrence and infiltrate
into in another. C1 didn?t change the output, but
C2 and C3 translated the phrase as infiltrate into in
both cases. Although all of the four reference trans-
lators were consistent within their choices, each of
them chose different translations, namely worm its
way, enter, sneak and sneak into. This resulted in
a decrease in BLEU score for the two systems that
chose infiltrate into. This case illustrates a limita-
tion to fine-grained use of BLEU alone as a basis
for analysis, since we might argue that infiltrate into
is no less appropriate than sneak into in this con-
text. In other words, some of the reductions we see
in BLEU may not be actual errors but rather sim-
ply changes that take us outside of the coverage of
the test set. We did not find any cases in our sample
in which improvements in BLEU seemed to reward
changes that adversely affected meaning. From this,
424
we conclude that BLEU is a somewhat conservative
measure when used in this way, and that the actual
overall improvement in translation quality over our
baseline may be somewhat more than our roughly
1.0 measured BLEU improvement would suggest.
6 Conclusions and Future Work
In this paper, we started with a new way of look-
ing at, and largely supporting, the ?one translation
per discourse? hypothesis using forced decoding of
human reference translations. We then leveraged
insights  from that  analysis  to  design  the  transla-
tion model consistency features, obtaining solid im-
provements for both Ar-En and Zh-En translation.
In future work, we plan to explore additional vari-
ants. For example, we can further address sparsity by
incorporating monolingual paraphrase detection on
the source side, the target side or both. We can and
should explore other monotonically increasing con-
cave feature functions in addition to the Okapi BM25
function that we have found to be useful in this work,
we should explore alternatives to our use of the max-
imum function in C2 and C3, and we should con-
sider optimizing to measures other than BLEU (e.g.,
METEOR) that extend the range of rewarded lexical
choices by leveraging monolingual paraphrase evi-
dence.
In designing our features we were guided by our
intuition about which kinds of consistency should be
rewarded. Data can be superior to intuition, how-
ever, and our forced decoding technique might also
be helpful in generating new insights that could help
to guide the design of even more useful features. For
example, our forced decoding clearly points to cases
in which translators have chosen different structural
variants when translating the same phrase, and closer
examination of these cases might help us to automat-
ically detect which kinds of structural variation can
most profitably be moderated using a consistency
feature. We should also note that we have only done
forced decoding to date in one language pair (Ar-
En), and there might be more to be learned about
language-specific issues from doing the same anal-
ysis for additional language pairs.
Finally, the time seems propitious to reconsider
our choice of document-scale as our discourse con-
text. Documents have much to recommend them, but
much of the content that we might wish to translate
(conversational speech, text chat, email threads, . . . )
doesn?t present the kinds of obvious and unambigu-
ous document boundaries that we find in MT test
collections that are built from news stories. More-
over, some documents (e.g., textbooks) may be too
diverse for an entire document to be the right scale
for consistency. We might also be able to produc-
tively group similar documents into clusters in which
the vocabulary choices are (or should be) mutually
reinforcing.
We therefore  end where  we began, with  many
questions to be answered. Now, however, we have
somewhat different questions ? not whether to en-
courage consistency at a super-sentential scale, but
rather when and how best to do that.
Acknowledgements
This research was supported in part by the BOLT
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-12-C-0015. Any
opinions, findings, conclusions or recommendations
expressed in this paper are those of the authors and
do not necessarily reflect the view of DARPA.
References
Nicola  Bertoldi  and  Marcello  Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proceedings of the Fourth
Workshop on Statistical Machine Translation (StatMT
?09), pages 182?189.
Ralf D. Brown. 2008. Exploiting document-level context
for data-driven machine translation. In Proceedings of
the the Eighth Conference of the Association for Ma-
chine Translation in the Americas (AMTA ?08).
Marine Carpuat, Yuval Marton, and Nizar Habash. 2011.
Improved Arabic-to-English statistical machine trans-
lation  by  reordering  post-verbal  subjects  for  word
alignment. Machine Translation, pages 1?16.
Marine Carpuat. 2009. One translation per discourse. In
Proceedings of the Workshop on Semantic Evaluations:
Recent Achievements and Future Directions, DEW ?09,
pages 19?27.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL ?05.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33:201?228.
425
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proceedings of ACL ?02.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing Word Lattice Translation. In Pro-
ceedings of ACL-HLT?08, pages 1012?1020, June.
Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam
Lopez, Ferhan Ture, Vladimir Eidelman, Juri Ganitke-
vitch, Phil Blunsom, and Philip Resnik. 2010. cdec: a
decoder, alignment, and learning framework for finite-
state and context-free translation models. In ACLDe-
mos ?10, pages 7?12.
William A.  Gale, Kenneth W.  Church, and  David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the workshop on Speech and Natural Lan-
guage, HLT ?91, pages 233?237.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2007. Context-aware
discriminative phrase selection for statistical machine
translation. In Proceedings  of  StatMT ?07, pages
159?166.
AS Hildebrand, M Eck, S Vogel, and Alex Waibel. 2005.
Adaptation of the translation model for statistical ma-
chine translation based on information retrieval. In
Proceedings of The European Association for Machine
Translation (EAMT ?05).
Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li. 2010.
Improving statistical machine translation with mono-
lingual collocation. In ACL ?10.
Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-
abith. 2011. Consistent translation using discrimina-
tive learning: a translation memory-inspired approach.
In Proceedings of ACL-HLT?11, pages 1239?1248.
Mohamed Maamouri, Ann Bies, and Seth Kulick. 2008.
Enhancing the Arabic Treebank: A Collaborative Ef-
fort toward New Annotation Guidelines. In LREC ?08.
Christof Monz. 2011. Statistical Machine Translation
with Local Language Models. In EMNLP ?11.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Ex-
ploiting parallel texts for word sense disambiguation:
an empirical study. In ACL ?03.
NIST. 2006. http://www.itl.nist.gov/iad/mig/tests/mt/2006/.
NIST. 2008. http://www.itl.nist.gov/iad/mig/tests/mt/2008/.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19?51.
Joseph Olive, Caitlin  Christianson, and John McCary.
2011. Handbook  of  Natural  Language  Processing
and Machine Translation: DARPAGlobal Autonomous
Language  Exploitation. Springer  Publishing  Com-
pany, Inc., 1st edition.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL ?02.
Stephen E. Robertson, Steve Walker, Susan Jones, Miche-
line  Hancock-Beaulieu, and  Mike  Gatford. 1994.
Okapi at TREC-3. In TREC.
Germa?n  Sanchis-Trilles  and  Francisco  Casacuberta.
2010. Bayesian  adaptation  for  statistical  machine
translation. In Proceedings of the workshop on Struc-
tural and Syntactic Pattern Recognition (SSPR ?10),
pages 620?629.
Jo?rg Tiedemann. 2010. Context adaptation in statistical
machine translation using models with exponentially
decaying cache. In Proceedings of the workshop on
Domain Adaptation for Natural Language Processing
(DANLP ?10), pages 8?15.
Huihsin Tseng, Pi-Chuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional  random  field  word  segmenter. In Fourth
SIGHAN Workshop on Chinese Language Processing.
Michael M.  Wasser  and  Bonnie  Dorr. 2008. Ma-
chine  translation  with  cross-lingual  information  re-
trieval based document relevance scores. Unpublished.
Tong Xiao, Jingbo Zhu, Shujie  Yao, and Hao Zhang.
2011. Document-level consistency verification in ma-
chine translation. InMachine Translation Summit XIII
(MTS?11).
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In ACL ?95.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004. Lan-
guage model adaptation for statistical machine transla-
tion with structured query models. In COLING ?04.
Bing Zhao, Young-Suk Lee, Xiaoqiang Luo, and Liu Li.
2011a. Learning to transform and select elementary
trees for improved syntax-based machine translations.
In ACL-HLT ?11, pages 846?855.
Yinggong Zhao, Yangsheng Ji, Ning Xi, Shujian Huang,
and Jiajun Chen. 2011b. Language model weight
adaptation based on cross-entropy for statistical ma-
chine translation. In Pacific Asia Conference on Lan-
guage, Information and Computation (PACLIC ?11).
426
Proceedings of the NAACL HLT 2013 Demonstration Session, pages 32?35,
Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational Linguistics
KELVIN: a tool for automated knowledge base construction
Paul McNamee, James Mayfield
Johns Hopkins University
Human Language Technology Center of Excellence
Tim Finin, Tim Oates
University of Maryland
Baltimore County
Dawn Lawrie
Loyola University Maryland
Tan Xu, Douglas W. Oard
University of Maryland
College Park
Abstract
We present KELVIN, an automated system for
processing a large text corpus and distilling a
knowledge base about persons, organizations,
and locations. We have tested the KELVIN
system on several corpora, including: (a) the
TAC KBP 2012 Cold Start corpus which con-
sists of public Web pages from the University
of Pennsylvania, and (b) a subset of 26k news
articles taken from English Gigaword 5th edi-
tion.
Our NAACL HLT 2013 demonstration per-
mits a user to interact with a set of search-
able HTML pages, which are automatically
generated from the knowledge base. Each
page contains information analogous to the
semi-structured details about an entity that are
present in Wikipedia Infoboxes, along with
hyperlink citations to supporting text.
1 Introduction
The Text Analysis Conference (TAC) Knowledge
Base Population (KBP) Cold Start task1 requires
systems to take set of documents and produce a
comprehensive set of <Subject, Predicate, Object>
triples that encode relationships between and at-
tributes of the named-entities that are mentioned in
the corpus. Systems are evaluated based on the fi-
delity of the constructed knowledge base. For the
2012 evaluation, a fixed schema of 42 relations (or
slots), and their logical inverses was provided, for
example:
? X:Organization employs Y:Person
1See details at http://www.nist.gov/tac/2012/
KBP/task_guidelines/index.html
? X:Person has-job-title title
? X:Organization headquartered-in Y:Location
Multiple layers of NLP software are required for
this undertaking, including at the least: detection of
named-entities, intra-document co-reference resolu-
tion, relation extraction, and entity disambiguation.
To help prevent a bias towards learning about
prominent entities at the expense of generality,
KELVIN refrains from mining facts from sources
such as documents obtained through Web search,
Wikipedia2, or DBpedia.3 Only facts that are as-
serted in and gleaned from the source documents are
posited.
Other systems that create large-scale knowledge
bases from general text include the Never-Ending
Language Learning (NELL) system at Carnegie
Mellon University (Carlson et al, 2010), and the
TextRunner system developed at the University of
Washington (Etzioni et al, 2008).
2 Washington Post KB
No gold-standard KBs were available to us to assist
during the development of KELVIN, so we relied on
qualitative assessment to gauge the effectiveness of
our extracted relations ? by manually examining ten
random samples for each relations, we ascertained
that most relations were between 30-80% accurate.
Although the TAC KBP 2012 Cold Start task was a
pilot evaluation of a new task using a novel evalua-
tion methodology, the KELVIN system did attain the
highest reported F1 scores.4
2http://en.wikipedia.org/
3http://www.dbpedia.org/
40.497 0-hop & 0.363 all-hops, as reported in the prelimi-
nary TAC 2012 Evaluation Results.
32
During our initial development we worked with
a 26,143 document collection of 2010 Washington
Post articles and the system discovered 194,059 re-
lations about 57,847 named entities. KELVIN learns
some interesting, but rather dubious relations from
the Washington Post articles5
? Sen. Harry Reid is an employee of the ?Repub-
lican Party.? Sen. Reid is also an employee of
the ?Democratic Party.?
? Big Foot is an employee of Starbucks.
? MacBook Air is a subsidiary of Apple Inc.
? Jill Biden is married to Jill Biden.
However, KELVIN also learns quite a number of
correct facts, including:
? Warren Buffett owns shares of Berkshire Hath-
away, Burlington Northern Santa Fe, the Wash-
ington Post Co., and four other stocks.
? Jared Fogle is an employee of Subway.
? Freeman Hrabowski works for UMBC,
founded the Meyerhoff Scholars Program, and
graduated from Hampton University and the
University of Illinois.
? Supreme Court Justice Elena Kagan attended
Oxford, Harvard, and Princeton.
? Southwest Airlines is headquartered in Texas.
? Ian Soboroff is a computer scientist6 employed
by NIST.7
3 Pipeline Components
3.1 SERIF
BBN?s SERIF tool8 (Boschee et al, 2005) provides
a considerable suite of document annotations that
are an excellent basis for building a knowledge base.
The functions SERIF can provide are based largely
5All 2010 Washington Post articles from English Gigaword
5th ed. (LDC2011T07).
6Ian is the sole computer scientist discovered in processing
a year of news. In contrast, KELVIN found 52 lobbyists.
7From Washington Post article (WPB ENG 20100506.0012
in LDC2011T07).
8Statistical Entity & Relation Information Finding.
Slotname Count
per:employee of 60,690
org:employees 44,663
gpe:employees 16,027
per:member of 14,613
org:membership 14,613
org:city of headquarters 12,598
gpe:headquarters in city 12,598
org:parents 6,526
org:country of headquarters 4,503
gpe:headquarters in country 4,503
Table 1: Most prevalent slots extracted by SERIF from
the Washington Post texts.
Slotname Count
per:title 44,896
per:employee of 39,101
per:member of 20,735
per:countries of residence 8,192
per:origin 4,187
per:statesorprovinces of residence 3,376
per:cities of residence 3,376
per:country of birth 1,577
per:age 1,233
per:spouse 1,057
Table 2: Most prevalent slots extracted by FACETS from
the Washington Post texts.
on the NIST ACE specification,9 and include: (a)
identifying named-entities and classifying them by
type and subtype; (b) performing intra-document
co-reference analysis, including named mentions,
as well as co-referential nominal and pronominal
mentions; (c) parsing sentences and extracting intra-
sentential relations between entities; and, (d) detect-
ing certain types of events.
In Table 1 we list the most common slots SERIF
extracts from the Washington Post articles.
3.2 FACETS
FACETS, another BBN tool, is an add-on pack-
age that takes SERIF output and produces role and
argument annotations about person noun phrases.
FACETS is implemented using a conditional-
9The principal types of ACE named-entities are per-
sons, organizations, and geo-political entities (GPEs).
GPEs are inhabited locations with a government. See
http://www.itl.nist.gov/iad/mig/tests/ace/
2008/doc/ace08-evalplan.v1.2d.pdf.
33
Figure 1: Simple rendering of KB page about former Florida congressman Joe Scarborough. Many facts are correct
? he lived in and was employed by the State of Florida; he has a brother George; he was a member of the Republican
House of Representatives; and, he is employed by MSNBC.
exponential learner trained on broadcast news. The
attributes FACETS can recognize include general at-
tributes like religion and age (which anyone might
have), as well as role-specific attributes, such as
medical specialty for physicians, or academic insti-
tution for someone associated with an university.
In Table 2 we report the most prevalent slots
FACETS extracts from the Washington Post.10
3.3 CUNY toolkit
To increase our coverage of relations we also in-
tegrated the KBP Slot Filling Toolkit (Chen et al,
2011) developed at the CUNY BLENDER Lab.
Given that the KBP toolkit was designed for the tra-
ditional slot filling task at TAC, this primarily in-
volved creating the queries that the tool expected as
input and parallelizing the toolkit to handle the vast
number of queries issued in the cold start scenarios.
To informally gauge the accuracy of slots
extracted from the CUNY tool, some coarse as-
sessment was done over a small collection of 807
New York Times articles that include the string
?University of Kansas.? From this collection, 4264
slots were identified. Nine different types of slots
were filled in order of frequency: per:title (37%),
per:employee of (23%), per:cities of residence
(17%), per:stateorprovinces of residence (6%),
10Note FACETS can independently extract some slots that
SERIF is capable of discovering (e.g., employment relations).
org:top members/employees (6%), org:member of
(6%), per:countries of residence (2%), per:spouse
(2%), and per:member of (1%). We randomly sam-
pled 10 slot-fills of each type, and found accuracy
to vary from 20-70%.
3.4 Coreference
We used two methods for entity coreference. Un-
der the theory that name ambiguity may not be a
huge problem, we adopted a baseline approach of
merging entities across different documents if their
canonical mentions were an exact string match af-
ter some basic normalizations, such as removing
punctuation and conversion to lower-case charac-
ters. However we also used the JHU HLTCOE
CALE system (Stoyanov et al, 2012), which maps
named-entity mentions to the TAC-KBP reference
KB, which was derived from a 2008 snapshot of En-
glish Wikipedia. For entities that are not found in the
KB, we reverted to exact string match. CALE entity
linking proved to be the more effective approach for
the Cold Start task.
3.5 Timex2 Normalization
SERIF recognizes, but does not normalize, temporal
expressions, so we used the Stanford SUTime pack-
age, to normalize date values.
34
Figure 2: Supporting text for some assertions about Mr. Scarborough. Source documents are also viewable by
following hyperlinks.
3.6 Lightweight Inference
We performed a small amount of light inference to
fill some slots. For example, if we identified that
a person P worked for organization O, and we also
extracted a job title T for P, and if T matched a set
of titles such as president or minister we asserted
that the tuple <O, org:top members employees, P>
relation also held.
4 Ongoing Work
There are a number of improvements that we are un-
dertaking, including: scaling to much larger corpora,
detecting contradictions, expanding the use of infer-
ence, exploiting the confidence of extracted infor-
mation, and applying KELVIN to various genres of
text.
5 Script Outline
The KB generated by KELVIN is best explored us-
ing a Wikipedia metaphor. Thus our demonstration
consists of a web browser that starts with a list of
moderately prominent named-entities that the user
can choose to examine (e.g., investor Warren Buf-
fett, Supreme Court Justice Elena Kagan, Southwest
Airlines Co., the state of Florida). Selecting any
entity takes one to a page displaying its known at-
tributes and relations, with links to documents that
serve as provenance for each assertion. On every
page, each entity is hyperlinked to its own canon-
ical page; therefore the user is able to browse the
KB much as one browses Wikipedia by simply fol-
lowing links. A sample generated page is shown in
Figure 1 and text that supports some of the learned
assertions in the figure is shown in Figure 2. We
also provide a search interface to support jump-
ing to a desired entity and can demonstrate access-
ing the data encoded in the semantic web language
RDF (World Wide Web Consortium, 2013), which
supports ontology browsing and executing complex
SPARQL queries (Prud?Hommeaux and Seaborne,
2008) such as ?List the employers of people living
in Nebraska or Kansas who are older than 40.?
References
E. Boschee, R. Weischedel, and A. Zamanian. 2005. Au-
tomatic information extraction. In Proceedings of the
2005 International Conference on Intelligence Analy-
sis, McLean, VA, pages 2?4.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth Conference on Artificial Intelligence
(AAAI 2010).
Z. Chen, S. Tamang, A. Lee, X. Li, and H. Ji. 2011.
Knowledge Base Population (KBP) Toolkit @ CUNY
BLENDER LAB Manual.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open information extraction
from the web. Commun. ACM, 51(12):68?74, Decem-
ber.
E Prud?Hommeaux and A. Seaborne. 2008. SPARQL
query language for RDF. Technical report, World
Wide Web Consortium, January.
Veselin Stoyanov, James Mayfield, Tan Xu, Douglas W.
Oard, Dawn Lawrie, Tim Oates, and Tim Finin. 2012.
A context-aware approach to entity linking. In Pro-
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge Ex-
traction, AKBC-WEKEX ?12, pages 62?67, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
World Wide Web Consortium. 2013. Resource Descrip-
tion Framework Specification. ?http://http://
www.w3.org/RDF/. ?[Online; accessed 8 April,
2013]?.
35
