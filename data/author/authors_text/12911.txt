Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 228?237, Prague, June 2007. c?2007 Association for Computational Linguistics
Semi-Supervised Classification for Extracting Protein Interaction Sentences
using Dependency Parsing
Gu?nes? Erkan
University of Michigan
gerkan@umich.edu
Arzucan ?Ozgu?r
University of Michigan
ozgur@umich.edu
Dragomir R. Radev
University of Michigan
radev@umich.edu
Abstract
We introduce a relation extraction method to
identify the sentences in biomedical text that
indicate an interaction among the protein
names mentioned. Our approach is based on
the analysis of the paths between two protein
names in the dependency parse trees of the
sentences. Given two dependency trees, we
define two separate similarity functions (ker-
nels) based on cosine similarity and edit dis-
tance among the paths between the protein
names. Using these similarity functions, we
investigate the performances of two classes
of learning algorithms, Support Vector Ma-
chines and k-nearest-neighbor, and the semi-
supervised counterparts of these algorithms,
transductive SVMs and harmonic functions,
respectively. Significant improvement over
the previous results in the literature is re-
ported as well as a new benchmark dataset
is introduced. Semi-supervised algorithms
perform better than their supervised ver-
sion by a wide margin especially when the
amount of labeled data is limited.
1 Introduction
Protein-protein interactions play an important role
in vital biological processes such as metabolic and
signaling pathways, cell cycle control, and DNA
replication and transcription (Phizicky and Fields,
1995). A number of (mostly manually curated)
databases such as MINT (Zanzoni et al, 2002),
BIND (Bader et al, 2003), and SwissProt (Bairoch
and Apweiler, 2000) have been created to store pro-
tein interaction information in structured and stan-
dard formats. However, the amount of biomedical
literature regarding protein interactions is increas-
ing rapidly and it is difficult for interaction database
curators to detect and curate protein interaction in-
formation manually. Thus, most of the protein in-
teraction information remains hidden in the text of
the papers in the biomedical literature. Therefore,
the development of information extraction and text
mining techniques for automatic extraction of pro-
tein interaction information from free texts has be-
come an important research area.
In this paper, we introduce an information extrac-
tion approach to identify sentences in text that in-
dicate an interaction relation between two proteins.
Our method is different than most of the previous
studies (see Section 2) on this problem in two as-
pects: First, we generate the dependency parses of
the sentences that we analyze, making use of the
dependency relationships among the words. This
enables us to make more syntax-aware inferences
about the roles of the proteins in a sentence com-
pared to the classical pattern-matching information
extraction methods. Second, we investigate semi-
supervised machine learning methods on top of the
dependency features we generate. Although there
have been a number of learning-based studies in this
domain, our methods are the first semi-supervised
efforts to our knowledge. The high cost of label-
ing free text for this problem makes semi-supervised
methods particularly valuable.
We focus on two semi-supervised learning meth-
ods: transductive SVMs (TSVM) (Joachims, 1999),
228
and harmonic functions (Zhu et al, 2003). We also
compare these two methods with their supervised
counterparts, namely SVMs and k-nearest neigh-
bor algorithm. Because of the nature of these al-
gorithms, we propose two similarity functions (ker-
nels in SVM terminology) among the instances of
the learning problem. The instances in this problem
are natural language sentences with protein names in
them, and the similarity functions are defined on the
positions of the protein names in the corresponding
parse trees. Our motivating assumption is that the
path between two protein names in a dependency
tree is a good description of the semantic relation
between them in the corresponding sentence. We
consider two similarity functions; one based on the
cosine similarity and the other based on the edit dis-
tance among such paths.
2 Related Work
There have been many approaches to extract pro-
tein interactions from free text. One of them is
based on matching pre-specified patterns and rules
(Blaschke et al, 1999; Ono et al, 2001). How-
ever, complex cases that are not covered by the
pre-defined patterns and rules cannot be extracted
by these methods. Huang et al (2004) proposed a
method where patterns are discovered automatically
from a set of sentences by dynamic programming.
Bunescu et al (2005) have studied the performance
of rule learning algorithms. They propose two meth-
ods for protein interaction extraction. One is based
on the rule learning method Rapier and the other
on longest common subsequences. They show that
these methods outperform hand-written rules.
Another class of approaches is using more syntax-
aware natural language processing (NLP) tech-
niques. Both full and partial (shallow) parsing
strategies have been applied in the literature. In
partial parsing the sentence structure is decomposed
partially and local dependencies between certain
phrasal components are extracted. An example of
the application of this method is relational parsing
for the inhibition relation (Pustejovsky et al, 2002).
In full parsing, however, the full sentence structure
is taken into account. Temkin and Gilder (2003)
used a full parser with a lexical analyzer and a con-
text free grammar (CFG) to extract protein-protein
interaction from text. Another study that uses full-
sentence parsing to extract human protein interac-
tions is (Daraselia et al, 2004). Alternatively,
Yakushiji et al (2005) propose a system based on
head-driven phrase structure grammar (HPSG). In
their system protein interaction expressions are pre-
sented as predicate argument structure patterns from
the HPSG parser. These parsing approaches con-
sider only syntactic properties of the sentences and
do not take into account semantic properties. Thus,
although they are complicated and require many re-
sources, their performance is not satisfactory.
Machine learning techniques for extracting pro-
tein interaction information have gained interest in
the recent years. The PreBIND system uses SVM to
identify the existence of protein interactions in ab-
stracts and uses this type of information to enhance
manual expert reviewing for the BIND database
(Donaldson et al, 2003). Words and word bigrams
are used as binary features. This system is also
tested with the Naive Bayes classifier, but SVM is
reported to perform better. Mitsumori et al (2006)
also use SVM to extract protein-protein interac-
tions. They use bag-of-words features, specifically
the words around the protein names. These sys-
tems do not use any syntactic or semantic informa-
tion. Sugiyama et al (2003) extract features from
the sentences based on the verbs and nouns in the
sentences such as the verbal forms, and the part of
speech tags of the 20 words surrounding the verb
(10 before and 10 after it). Further features are used
to indicate whether a noun is found, as well as the
part of speech tags for the 20 words surrounding
the noun, and whether the noun contains numeri-
cal characters, non-alpha characters, or uppercase
letters. They construct k-nearest neighbor, decision
tree, neural network, and SVM classifiers by using
these features. They report that the SVM classifier
performs the best. They use part-of-speech informa-
tion, but do not consider any dependency or seman-
tic information.
The paper is organized as follows. In Section 3 we
describe our method of extracting features from the
dependency parse trees of the sentences and defin-
ing the similarity between two sentences. In Section
4 we discuss our supervised and semi-supervised
methods. In Section 5 we describe the data sets and
evaluation metrics that we used, and present our re-
229
sults. We conclude in Section 6.
3 Sentence Similarity Based on
Dependency Parsing
In order to apply the semi-supervised harmonic
functions and its supervised counterpart kNN, and
the kernel based TSVM and SVM methods, we need
to define a similarity measure between two sen-
tences. For this purpose, we use the dependency
parse trees of the sentences. Unlike a syntactic parse
(which describes the syntactic constituent structure
of a sentence), the dependency parse of a sentence
captures the semantic predicate-argument relation-
ships among its words. The idea of using depen-
dency parse trees for relation extraction in general
was studied by Bunescu and Mooney (2005a). To
extract the relationship between two entities, they
design a kernel function that uses the shortest path in
the dependency tree between them. The motivation
is based on the observation that the shortest path be-
tween the entities usually captures the necessary in-
formation to identify their relationship. They show
that their approach outperforms the dependency tree
kernel of Culotta and Sorensen (2004), which is
based on the subtree that contains the two entities.
We adapt the idea of Bunescu and Mooney (2005a)
to the task of identifying protein-protein interaction
sentences. We define the similarity between two
sentences based on the paths between two proteins
in the dependency parse trees of the sentences.
In this study we assume that the protein names
have already been annotated and focus instead on
the task of extracting protein-protein interaction sen-
tences for a given protein pair. We parse the sen-
tences with the Stanford Parser1 (de Marneffe et al,
2006). From the dependency parse trees of each sen-
tence we extract the shortest path between a protein
pair.
For example, Figure 1 shows the dependency tree
we got for the sentence ?The results demonstrated
that KaiC interacts rhythmically with KaiA, KaiB,
and SasA.? This example sentence illustrates that
the dependency path between a protein pair captures
the relevant information regarding the relationship
between the proteins better compared to using the
words in the unparsed sentence. Consider the pro-
1http://nlp.stanford.edu/software/lex-parser.shtml
tein pair KaiC and SasA. The words in the sentence
between these proteins are interacts, rhythmically,
with, KaiA, KaiB, and and. Among these words
rhythmically, KaiA, and and KaiB are not directly
related to the interaction relationship between KaiC
and SasA. On the other hand, the words in the depen-
dency path between this protein pair give sufficient
information to identify their relationship.
In this sentence we have four proteins (KaiC,
KaiA, KaiB, and SasA). So there are six pairs of
proteins for which a sentence may or may not be de-
scribing an interaction. The following are the paths
between the six protein pairs. In this example there
is a single path between each protein pair. However,
there may be more than one paths between a pro-
tein pair, if one or both appear multiple times in the
sentence. In such cases, we select the shortest paths
between the protein pairs.
ccomp
prep_with
results interacts
The
KaiA KaiB
rhytmically SasAthat KaiC
demonstrated
nsubj
complm nsubj
advmod
conj_and conj_and
det
Figure 1: The dependency tree of the sentence ?The
results demonstrated that KaiC interacts rhythmi-
cally with KaiA, KaiB, and SasA.?
1. KaiC - nsubj - interacts - prep with - SasA
2. KaiC - nsubj - interacts - prep with - SasA - conj and -
KaiA
3. KaiC - nsubj - interacts - prep with ? SasA - conj and -
KaiB
4. SasA - conj and - KaiA
5. SasA - conj and - KaiB
6. KaiA ? conj and ? SasA - conj and - KaiB
If a sentence contains n different proteins, there
are
(n
2
)
different pairs of proteins. We use machine
learning approaches to classify each sentence as an
interaction sentence or not for a protein pair. A sen-
tence may be an interaction sentence for one protein
230
pair, while not for another protein pair. For instance,
our example sentence is a positive interaction sen-
tence for the KaiC and SasA protein pair. However,
it is a negative interaction sentence for the KaiA and
SasA protein pair, i.e., it does not describe an inter-
action between this pair of proteins. Thus, before
parsing a sentence, we make multiple copies of it,
one for each protein pair. To reduce data sparseness,
we rename the proteins in the pair as PROTX1 and
PROTX2, and all the other proteins in the sentence
as PROTX0. So, for our example sentence we have
the following instances in the training set:
1. PROTX1 - nsubj - interacts - prep with - PROTX2
2. PROTX1 - nsubj - interacts - prep with - PROTX0 -
conj and - PROTX2
3. PROTX1 - nsubj - interacts - prep with ? PROTX0 -
conj and - PROTX2
4. PROTX1 - conj and - PROTX2
5. PROTX1 - conj and - PROTX2
6. PROTX1 ? conj and ? PROTX0 - conj and - PROTX2
The first three instances are positive as they describe
an interaction between PROTX1 and PROTX2. The
last three are negative, as they do not describe an
interaction between PROTX1 and PROTX2.
We define the similarity between two instances
based on cosine similarity and edit distance based
similarity between the paths in the instances.
3.1 Cosine Similarity
Suppose pi and pj are the paths between PROTX1
and PROTX2 in instance xi and instance xj , respec-
tively. We represent pi and pj as vectors of term
frequencies in the vector-space model. The cosine
similarity measure is the cosine of the angle between
these two vectors and is calculated as follows:
cos sim(pi, pj) = cos(pi,pj) =
pi ? pj
?pi??pj?
(1)
that is, it is the dot product of pi and pj divided by
the lengths of pi and pj. The cosine similarity mea-
sure takes values in the range [0, 1]. If all the terms
in pi and pj are common, then it takes the maximum
value of 1. If none of the terms are common, then it
takes the minimum value of 0.
3.2 Similarity Based on Edit Distance
A shortcoming of cosine similarity is that it only
takes into account the common terms, but does not
consider their order in the path. For this reason, we
also use a similarity measure based on edit distance
(also called Levenshtein distance). Edit distance be-
tween two strings is the minimum number of op-
erations that have to be performed to transform the
first string to the second. In the original character-
based edit distance there are three types of opera-
tions. These are insertion, deletion, or substitution
of a single character. We modify the character-based
edit distance into a word-based one, where the oper-
ations are defined as insertion, deletion, or substitu-
tion of a single word.
The edit distance between path 1 and path 2 of
our example sentence is 2. We insert PROTX0 and
conj and to path 1 to convert it to path 2.
1. PROTX1 - nsubj - interacts - prep with - insert (PROTX0)
- insert (conj and) ? PROTX2
2. PROTX1 - nsubj - interacts - prep with - PROTX0 -
conj and - PROTX2
We normalize edit distance by dividing it by the
length (number of words) of the longer path, so that
it takes values in the range [0, 1]. We convert the dis-
tance measure into a similarity measure as follows.
edit sim(pi, pj) = e??(edit distance(pi,pj)) (2)
Bunescu and Mooney (2005a) propose a similar
method for relation extraction in general. However,
their similarity measure is based on the number of
the overlapping words between two paths. When
two paths have different lengths, they assume the
similarity between them is zero. On the other hand,
our edit distance based measure can also account for
deletions and insertions of words.
4 Semi-Supervised Machine Learning
Approaches
4.1 kNN and Harmonic Functions
When a similarity measure is defined among the in-
stances of a learning problem, a simple and natural
choice is to use a nearest neighbor based approach
that classifies each instance by looking at the labels
of the instances that are most similar to it. Per-
haps the simplest and most popular similarity-based
231
learning algorithm is the k-nearest neighbor classifi-
cation method (kNN). Let U be the set of unlabeled
instances, and L be the set of labeled instances in
a learning problem. Given an instance x ? U , let
NLk (x) be the set of top k instances in L that are
most similar to x with respect to some similarity
measure. The kNN equation for a binary classifi-
cation problem can be written as:
y(x) =
?
z?NLk (x)
sim(x, z)y(z)
?
z??NLk (x)
sim(x, z?) (3)
where y(z) ? {0, 1} is the label of the instance z.2
Note that y(x) can take any real value in the [0, 1]
interval. The final classification decision is made by
setting a threshold in this interval (e.g. 0.5) and clas-
sifying the instances above the threshold as positive
and others as negative. For our problem, each in-
stance is a dependency path between the proteins in
the pair and the similarity function can be one of the
functions we have defined in Section 3.
Equation 3 can be seen as averaging the labels (0
or 1) of the nearest neighbors of each unlabeled in-
stance. This suggests a generalized semi-supervised
version of the same algorithm by incorporating un-
labeled instances as neighbors as well:
y(x) =
?
z?NL?Uk (x)
sim(x, z)y(z)
?
z??NL?Uk (x)
sim(x, z?) (4)
Unlike Equation 3, the unlabeled instances are also
considered in Equation 4 when finding the nearest
neighbors. We can visualize this as an undirected
graph, where each data instance (labeled or unla-
beled) is a node that is connected to its k nearest
neighbor nodes. The value of y(?) is set to 0 or 1
for labeled nodes depending on their class. For each
unlabeled node x, y(x) is equal to the average of the
y(?) values of its neighbors. Such a function that
satisfies the average property on all unlabeled nodes
is called a harmonic function and is known to exist
and have a unique solution (Doyle and Snell, 1984).
Harmonic functions were first introduced as a semi-
supervised learning method by Zhu et al (2003).
There are interesting alternative interpretations of
2Equation 3 is the weighted (or soft) version of the kNN
algorithm. In the classical voting scheme, x is classified in the
category that the majority of its neighbors belong to.
a harmonic function on a graph. One of them can
be explained in terms of random walks on a graph.
Consider a random walk on a graph where at each
time point we move from the current node to one of
its neighbors. The next node is chosen among the
neighbors of the current node with probability pro-
portional to the weight (similarity) of the edge that
connects the two nodes. Assuming we start the ran-
dom walk from the node x, y(x) in Equation 4 is
then equal to the probability that this random walk
will hit a node labeled 1 before it hits a node labeled
0.
4.2 Transductive SVM
Support vector machines (SVM) is a supervised ma-
chine learning approach designed for solving two-
class pattern recognition problems. The aim is to
find the decision surface that separates the positive
and negative labeled training examples of a class
with maximum margin (Burges, 1998).
Transductive support vector machines (TSVM)
are an extension of SVM, where unlabeled data is
used in addition to labeled data. The aim now is
to assign labels to the unlabeled data and find a de-
cision surface that separates the positive and nega-
tive instances of the original labeled data and the
(now labeled) unlabeled data with maximum mar-
gin. Intuitively, the unlabeled data pushes the deci-
sion boundary away from the dense regions. How-
ever, unlike SVM, the optimization problem now
is NP-hard (Zhu, 2005). Pointers to studies for
approximation algorithms can be found in (Zhu,
2005).
In Section 3 we defined the similarity between
two instances based on the cosine similarity and
the edit distance based similarity between the paths
in the instances. Here, we use these path similar-
ity measures as kernels for SVM and TSVM and
modify the SV M light package (Joachims, 1999) by
plugging in our two kernel functions.
A well-defined kernel function should be sym-
metric positive definite. While cosine kernel is well-
defined, Cortes et al (2004) proved that edit kernel
is not always positive definite. However, it is pos-
sible to make the kernel matrix positive definite by
adjusting the ? parameter, which is a positive real
number. Li and Jiang (2005) applied the edit kernel
to predict initiation sites in eucaryotic mRNAs and
232
obtained improved results compared to polynomial
kernel.
5 Experimental Results
5.1 Data Sets
One of the problems in the field of protein-protein
interaction extraction is that different studies gen-
erally use different data sets and evaluation met-
rics. Thus, it is difficult to compare their re-
sults. Bunescu et al (2005) manually developed the
AIMED corpus3 for protein-protein interaction and
protein name recognition. They tagged 199 Medline
abstracts, obtained from the Database of Interacting
Proteins (DIP) (Xenarios et al, 2001) and known to
contain protein interactions. This corpus is becom-
ing a standard, as it has been used in the recent stud-
ies by (Bunescu et al, 2005; Bunescu and Mooney,
2005b; Bunescu and Mooney, 2006; Mitsumori et
al., 2006; Yakushiji et al, 2005).
In our study we used the AIMED corpus and the
CB (Christine Brun) corpus that is provided as a re-
source by BioCreAtIvE II (Critical Assessment for
Information Extraction in Biology) challenge eval-
uation4. We pre-processed the CB corpus by first
annotating the protein names in the corpus automat-
ically and then, refining the annotation manually. As
discussed in Section 3, we pre-processed both of the
data sets as follows. We replicated each sentence
for each different protein pair. For n different pro-
teins in a sentence,
(n
2
)
new sentences are created,
as there are that many different pairs of proteins.
In each newly created sentence we marked the pro-
tein pair considered for interaction as PROTX1 and
PROTX2, and all the remaining proteins in the sen-
tence as PROTX0. If a sentence describes an inter-
action between PROTX1 and PROTX2, it is labeled
as positive, otherwise it is labeled as negative. The
summary of the data sets after pre-processing is dis-
played in Table 15.
Since previous studies that use AIMED corpus
perform 10-fold cross-validation. We also per-
formed 10-fold cross-validation in both data sets and
report the average results over the runs.
3ftp://ftp.cs.utexas.edu/pub/mooney/bio-data/
4http://biocreative.sourceforge.net/biocreative 2.html
5The pre-processed data sets are available at
http://belobog.si.umich.edu/clair/biocreative
Data Set Sentences + Sentences - Sentences
AIMED 4026 951 3075
CB 4056 2202 1854
Table 1: Data Sets
5.2 Evaluation Metrics
We use precision, recall, and F-score as our metrics
to evaluate the performances of the methods. Preci-
sion (pi) and recall (?) are defined as follows:
pi = TPTP + FP ; ? =
TP
TP + FN (5)
Here, TP (True Positives) is the number of sen-
tences classified correctly as positive; FP (False
Positives) is the number of negative sentences that
are classified as positive incorrectly by the classifier;
and FN (False Negatives) is the number of positive
sentences that are classified as negative incorrectly
by the classifier.
F-score is the harmonic mean of recall and precision.
F -score = 2pi?pi + ? (6)
5.3 Results and Discussion
We evaluate and compare the performances of
the semi-supervised machine learning approaches
(TSVM and harmonic functions) with their super-
vised counterparts (SVM and kNN) for the task of
protein-protein interaction extraction. As discussed
in Section 3, we use cosine similarity and edit dis-
tance based similarity as similarity functions in har-
monic functions and kNN, and as kernel functions
in TSVM and SVM. Our instances consist of the
shortest paths between the protein pairs in the de-
pendency parse trees of the sentences. In our ex-
periments, we tuned the ? parameter of the edit
distance based path similarity function to 4.5 with
cross-validation. The results in Table 2 and Table 3
are obtained with 10-fold cross-validation. We re-
port the average results over the runs.
Table 2 shows the results obtained for the AIMED
data set. Edit distance based path similarity function
performs considerably better than the cosine sim-
ilarity function with harmonic functions and kNN
and usually slightly better with SVM and TSVM.
We achieve our best F-score performance of 59.96%
with TSVM with edit kernel. While SVM with edit
233
kernel achieves the highest precision of 77.52%, it
performs slightly worse than SVM with cosine ker-
nel in terms of F-score measure. TSVM performs
slightly better than SVM, both of which perform bet-
ter than harmonic functions. kNN is the worst per-
forming algorithm for this data set.
In Table 2, we also show the results obtained pre-
viously in the literature by using the same data set.
Yakushiji et al (2005) use an HPSG parser to pro-
duce predicate argument structures. They utilize
these structures to automatically construct protein
interaction extraction rules. Mitsumori et al (2006)
use SVM with the unparsed text around the pro-
tein names as features to extract protein interac-
tion sentences. Here, we show their best result ob-
tained by using the three words to the left and to the
right of the proteins. The most closely related study
to ours is that by Bunescu and Mooney (2005a).
They define a kernel function based on the short-
est path between two entities of a relationship in
the dependency parse tree of a sentence (the SPK
method). They apply this method to the domain
of protein-protein interaction extraction in (Bunescu
and Mooney, 2006). Here, they also test the meth-
ods ELCS (Extraction Using Longest Common Sub-
sequences) (Bunescu et al, 2005) and SSK (Sub-
sequence Kernel) (Bunescu and Mooney, 2005b).
We cannot compare our results to theirs directly,
because they report their results as a precision-
recall graph. However, the best F-score in their
graph seems to be around 0.50 and definitely lower
than the best F-scores we have achieved (? 0.59).
Bunescu and Mooney (2006) also use SVM as their
learning method in their SPK approach. They define
their similarity based on the number of overlapping
words between two paths and assign a similarity of
zero if the two paths have different lengths. Our
improved performance with SVM and the shortest
path dependency features may be due to the edit-
distance based kernel, which takes into account not
only the overlapping words, but also word order and
accounts for deletions and insertions of words. Our
results show that, SVM, TSVM, and harmonic func-
tions achieve better F-score and recall performances
than the previous studies by Yakushiji et al (2005),
Mitsumori et al (2006), and the SSK and ELCS ap-
proaches of Bunescu and Mooney (2006). SVM and
TSVM also achieve higher precision scores. Since,
Mitsumori et al (2006) also use SVM in their study,
our improved results with SVM confirms our moti-
vation of using dependency paths as features.
Table 3 shows the results we got with the CB
data set. The F-score performance with the edit
distance based similarity function is always better
than that of cosine similarity function for this data
set. The difference in performances is considerable
for harmonic functions and kNN. Our best F-score
is achieved with TSVM with edit kernel (85.22%).
TSVM performs slightly better than SVM. When
cosine similarity function is used, kNN performs
better than harmonic functions. However, when edit
distance based similarity is used, harmonic functions
achieve better performance. SVM and TSVM per-
form better than harmonic functions. But, the gap in
performance is low when edit distance based simi-
larity is used with harmonic functions.
Method Precision Recall F-Score
SVM-edit 77.52 43.51 55.61
SVM-cos 61.99 54.99 58.09
TSVM-edit 59.59 60.68 59.96
TSVM-cos 58.37 61.19 59.62
Harmonic-edit 44.17 74.20 55.29
Harmonic-cos 36.02 67.65 46.97
kNN-edit 68.77 42.17 52.20
kNN-cos 40.37 49.49 44.36
(Yakushiji et al, 2005) 33.70 33.10 33.40
(Mitsumori et al, 2006) 54.20 42.60 47.70
Table 2: Experimental Results ? AIMED Data Set
Method Precision Recall F-Score
SVM-edit 85.15 84.79 84.96
SVM-cos 87.83 81.45 84.49
TSVM-edit 85.62 84.89 85.22
TSVM-cos 85.67 84.31 84.96
Harmonic-edit 86.69 80.15 83.26
Harmonic-cos 72.28 70.91 71.56
kNN-edit 72.89 86.95 79.28
kNN-cos 65.42 89.49 75.54
Table 3: Experimental Results ? CB Data Set
Semi-supervised approaches are usually more ef-
fective when there is less labeled data than unlabeled
data, which is usually the case in real applications.
To see the effect of semi-supervised approaches we
perform experiments by varying the amount of la-
234
00.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
300020001000500200100502010
F-
Sc
o
re
Number of Labeled Sentences
kNN
Harmonic
SVM
TSVM
Figure 2: The F-score on the AIMED dataset with
varying sizes of training data
beled training sentences in the range [10, 3000]. For
each labeled training set size, sentences are selected
randomly among all the sentences, and the remain-
ing sentences are used as the unlabeled test set. The
results that we report are the averages over 10 such
random runs for each labeled training set size. We
report the results for the algorithms when edit dis-
tance based similarity is used, as it mostly performs
better than cosine similarity. Figure 2 shows the
results obtained over the AIMED data set. Semi-
supervised approaches TSVM and harmonic func-
tions perform considerably better than their super-
vised counterparts SVM and kNN when we have
small number of labeled training data. It is inter-
esting to note that, although SVM is one of the best
performing algorithms with more training data, it is
the worst performing algorithm with small amount
of labeled training sentences. Its performance starts
to increase when number of training data is larger
than 200. Eventually, its performance gets close to
that of the other algorithms. Harmonic functions is
the best performing algorithm when we have less
than 200 labeled training data. TSVM achieves bet-
ter performance when there are more than 500 la-
beled training sentences.
Figure 3 shows the results obtained over the CB
data set. When we have less than 500 labeled sen-
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
300020001000500200100502010
F-
Sc
o
re
Number of Labeled Sentences
kNN
Harmonic
SVM
TSVM
Figure 3: The F-score on the CB dataset with vary-
ing sizes of training data
tences, harmonic functions and TSVM perform sig-
nificantly better than kNN, while SVM is the worst
performing algorithm. When we have more than
500 labeled training sentences, kNN is the worst per-
forming algorithm, while the performance of SVM
increases and gets similar to that of TSVM and
slightly better than that of harmonic functions.
6 Conclusion
We introduced a relation extraction approach based
on dependency parsing and machine learning to
identify protein interaction sentences in biomedical
text. Unlike syntactic parsing, dependency parsing
captures the semantic predicate argument relation-
ships between the entities in addition to the syntac-
tic relationships. We extracted the shortest paths be-
tween protein pairs in the dependency parse trees of
the sentences and defined similarity functions (ker-
nels in SVM terminology) for these paths based on
cosine similarity and edit distance. Supervised ma-
chine learning approaches have been applied to this
domain. However, they rely only on labeled training
data, which is difficult to gather. To our knowledge,
this is the first effort in this domain to apply semi-
supervised algorithms, which make use of both la-
beled and unlabeled data. We evaluated and com-
pared the performances of two semi-supervised ma-
235
chine learning approaches (harmonic functions and
TSVM), with their supervised counterparts (kNN
and SVM). We showed that, edit distance based sim-
ilarity function performs better than cosine simi-
larity function since it takes into account not only
common words, but also word order. Our 10-fold
cross validation results showed that, TSVM per-
forms slightly better than SVM, both of which per-
form better than harmonic functions. The worst per-
forming algorithm is kNN. We compared our results
with previous results published with the AIMED
data set. We achieved the best F-score performance
with TSVM with the edit distance kernel (59.96%)
which is significantly higher than the previously re-
ported results for the same data set.
In most real-world applications there are much
more unlabeled data than labeled data. Semi-
supervised approaches are usually more effective in
these cases, because they make use of both the la-
beled and unlabeled instances when making deci-
sions. To test this hypothesis for the application
of extracting protein interaction sentences from text,
we performed experiments by varying the number
of labeled training sentences. Our results show
that, semi-supervised algorithms perform consider-
ably better than their supervised counterparts, when
there are small number of labeled training sentences.
An interesting result is that, in such cases SVM per-
forms significantly worse than the other algorithms.
Harmonic functions achieve the best performance
when there are only a few labeled training sentences.
As number of labeled training sentences increases
the performance gap between supervised and semi-
supervised algorithms decreases.
Acknowledgments
This work was supported in part by grants R01-
LM008106 and U54-DA021519 from the US Na-
tional Institutes of Health.
References
G. Bader, D. Betel, and C. Hogue. 2003. Bind - the
biomolecular interaction network database. Nucleic
Acids Research, 31(1):248?250.
A. Bairoch and R. Apweiler. 2000. The swiss-prot pro-
tein sequence database and its supplement trembl in
2000. Nucleic Acids Research, 28(1):45?48.
C. Blaschke, M. A. Andrade, C. A. Ouzounis, and A. Va-
lencia. 1999. Automatic extraction of biological in-
formation from scientific text: Protein-protein interac-
tions. In Proceedings of the AAAI Conference on In-
telligent Systems for Molecular Biology (ISMB 1999),
pages 60?67.
R. C. Bunescu and R. J. Mooney. 2005a. A shortest
path dependency kernel for relation extraction. In Pro-
ceedings of the Human Language Technology Confer-
ence and Conference on Empirical Methods in Natu-
ral Language Processing, pages 724?731, Vancouver,
B.C, October.
R. C. Bunescu and R. J. Mooney. 2005b. Subsequence
kernels for relation extraction. In Proceedings of the
19th Conference on Neural Information Processing
Systems (NIPS), Vancouver, B.C, December.
R. C. Bunescu and R. J. Mooney, 2006. Text Mining and
Natural Language Processing, chapter Extracting Re-
lations from Text: From Word Sequences to Depen-
dency Paths. forthcoming book.
R. Bunescu, R. Ge, J. R. Kate, M. E. Marcotte, R. J.
Mooney, K. A. Ramani, and W. Y. Wong. 2005. Com-
parative experiments on learning information extrac-
tors for proteins and their interactions. Artificial Intel-
ligence in Medicine, 33(2):139?155, February.
C. J. C. Burges. 1998. A tutorial on support vector
machines for pattern recognition. Data Mining and
Knowledge Discovery, 2(2):121?167.
C. Cortes, P. Haffner, and M. Mohri. 2004. Rational
kernels: Theory and algorithms. Journal of Machine
Learning Research, (5):1035?1062, August.
A. Culotta and J. Sorensen. 2004. Dependency tree ker-
nels for relation extraction. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics (ACL-04), Barcelona, Spain, July.
N. Daraselia, A. Yuryev, S. Egorov, S. Novichkova,
A. Nikitin, and I. Mazo. 2004. Extracting human
protein interactions from medline using a full-sentence
parser. Bioinformatics, 20(5):604?611.
M-C. de Marneffe, B. MacCartney, and C. D. Manning.
2006. Generating Typed Dependency Parses from
Phrase Structure Parses. In Proceedings of the IEEE /
ACL 2006 Workshop on Spoken Language Technology.
The Stanford Natural Language Processing Group.
I. Donaldson, J. Martin, B. de Bruijn, C. Wolting,
V. Lay, B. Tuekam, S. Zhang, B. Baskin, G. D. Bader,
K. Michalockova, T. Pawson, and C. W. V. Hogue.
2003. Prebind and textomy - mining the biomedical
literature for protein-protein interactions using a sup-
port vector machine. BMC Bioinformatics, 4:11.
236
P. G. Doyle and J. L. Snell. 1984. Random Walks
and Electric Networks. Mathematical Association of
America.
M. Huang, X. Zhu, Y. Hao, D. G. Payan, K. Qu, and
M. Li. 2004. Discovering patterns to extract protein-
protein interactions from full texts. Bioinformatics,
20(18):3604?3612.
T. Joachims. 1999. Transductive inference for text
classification using support vector machines. In Ivan
Bratko and Saso Dzeroski, editors, Proceedings of
ICML-99, 16th International Conference on Machine
Learning, pages 200?209. Morgan Kaufmann Publish-
ers, San Francisco, US.
H. Li and T. Jiang. 2005. A class of edit kernels for
svms to predict translation initiation sites in eukaryotic
mrnas. Journal of Computational Biology, 12(6):702?
718.
T. Mitsumori, M. Murata, Y. Fukuda, K. Doi, and H. Doi.
2006. Extracting protein-protein interaction informa-
tion from biomedical text with svm. IEICE Trans-
actions on Information and Systems, E89-D(8):2464?
2466.
T. Ono, H. Hishigaki, A. Tanigami, and T. Takagi.
2001. Automated extraction of information on
protein-protein interactions from the biological liter-
ature. Bioinformatics, 17(2):155?161.
E. M. Phizicky and S. Fields. 1995. Protein-protein in-
teractions: methods for detection and analysis. Micro-
biol. Rev., 59(1):94?123, March.
J. Pustejovsky, J. Castano, J. Zhang, M. Kotecki, and
B. Cochran. 2002. Robust relational parsing over
biomedical literature: Extracting inhibit relations. In
Proceedings of the seventh Pacific Symposium on Bio-
computing (PSB 2002), pages 362?373.
K. Sugiyama, K. Hatano, M. Yoshikawa, and S. Uemura.
2003. Extracting information on protein-protein in-
teractions from biological literature based on machine
learning approaches. Genome Informatics, 14:699?
700.
J. M. Temkin and M. R. Gilder. 2003. Extraction of pro-
tein interaction information from unstructured text us-
ing a context-free grammar. Bioinformatics, 19:2046?
2053.
I. Xenarios, E. Fernandez, L. Salwinski, X. J. Duan, M. J.
Thompson, E. M. Marcotte, and D. Eisenberg. 2001.
Dip: The database of interacting proteins: 2001 up-
date. Nucleic Acids Res., 29:239 ? 241, January.
A. Yakushiji, Y. Miyao, Y. Tateisi, and J. Tsujii. 2005.
Biomedical information extraction with predicate-
argument structure patterns. In Proceedings of The
Eleventh Annual Meeting of The Association for Natu-
ral Language Processing, pages 93?96.
A. Zanzoni, L. Montecchi-Palazzi, M. Quondam,
G. Ausiello, M. Helmer-Citterich, and G. Cesareni.
2002. Mint: A molecular interaction database. FEBS
Letters, 513:135?140.
X. Zhu, Z. Ghahramani, and J. D. Lafferty. 2003. Semi-
supervised learning using gaussian fields and har-
monic functions. In T. Fawcett and N. Mishra, editors,
ICML, pages 912?919. AAAI Press.
X. Zhu. 2005. Semi-supervised learning lit-
erature survey. Technical Report 1530, Com-
puter Sciences, University of Wisconsin-Madison.
http://www.cs.wisc.edu/?jerryzhu/pub/ssl survey.pdf.
237
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1398?1407,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Detecting Speculations and their Scopes in Scientific Text
Arzucan
?
Ozg?ur
Department of EECS
University of Michigan
Ann Arbor, MI 48109, USA
ozgur@umich.edu
Dragomir R. Radev
Department of EECS and
School of Information
University of Michigan
Ann Arbor, MI 48109, USA
radev@umich.edu
Abstract
Distinguishing speculative statements
from factual ones is important for most
biomedical text mining applications. We
introduce an approach which is based
on solving two sub-problems to identify
speculative sentence fragments. The first
sub-problem is identifying the speculation
keywords in the sentences and the second
one is resolving their linguistic scopes.
We formulate the first sub-problem as a
supervised classification task, where we
classify the potential keywords as real
speculation keywords or not by using
a diverse set of linguistic features that
represent the contexts of the keywords.
After detecting the actual speculation
keywords, we use the syntactic structures
of the sentences to determine their scopes.
1 Introduction
Speculation, also known as hedging, is a fre-
quently used language phenomenon in scientific
articles, especially in experimental studies, which
are common in the biomedical domain. When re-
searchers are not completely certain about the in-
ferred conclusions, they use speculative language
to convey this uncertainty. Consider the follow-
ing example sentences from abstracts of articles in
the biomedical domain. The abstracts are available
at the U.S. National Library of Medicine PubMed
web page
1
. The PubMed Identifier (PMID) of the
corresponding article is given in parenthesis.
1. We showed that the Roaz protein bound specifically to
O/E-1 by using the yeast two-hybrid system. (PMID:
9151733)
2. These data suggest that p56lck is physically associated
with Fc gamma RIIIA (CD16) and functions to mediate
1
http://www.ncbi.nlm.nih.gov/pubmed/
signaling events related to the control of NK cellular
cytotoxicity. (PMID: 8405050)
The first sentence is definite, whereas the sec-
ond one contains speculative information, which is
conveyed by the use of the word ?suggest?. While
speculative information might still be useful for
biomedical scientists, it is important that it is dis-
tinguished from the factual information.
Recognizing speculations in scientific text has
gained interest in the recent years. Previous
studies focus on identifying speculative sentences
(Light et al, 2004; Medlock and Briscoe, 2007;
Szarvas, 2008; Kilicoglu and Bergler, 2008).
However, in many cases, not the entire sentence,
but fragments of a sentence are speculative. Con-
sider the following example sentences.
1. The mature mitochondrial forms of the erythroid and
housekeeping ALAS isozymes are predicted to have
molecular weights of 59.5 kd and 64.6 kd, respectively.
(PMID: 2050125)
2. Like RAD9, RAD9B associates with HUS1, RAD1, and
RAD17, suggesting that it is a RAD9 paralog that
engages in similar biochemical reactions. (PMID:
14611806)
Both sentences are speculative, since they con-
tain speculative information, which is signaled by
the use of the word ?predicted? in the first sen-
tence and the word ?suggesting? in the second
sentence. The scope of the speculation keyword
?predicted? in the first sentence spans the entire
sentence. Therefore, classifying the sentence as
speculative does not cause information loss. How-
ever, the scope of the speculation keyword ?sug-
gesting? in the second sentence applies only to
the second clause of the sentence. In other words,
only the statement ?RAD9B is a RAD9 paralog
that engages in similar biochemical reactions? is
speculative. The statement ?Like RAD9, RAD9B
associates with HUS1, RAD1, and RAD17? con-
veys factual information. Therefore, classifying
1398
the entire sentence as speculative will result in in-
formation loss.
In this paper, we aim to go beyond recogniz-
ing speculative sentences and tackle the problem
of identifying speculative fragments of sentences.
We propose an approach which is based on solv-
ing two sub-problems: (1) detecting the real spec-
ulation keywords, (2) resolving their linguistic
scopes in the sentences. As the previous exam-
ples demonstrated speculations are signaled with
speculation keywords (e.g. might, suggest, likely,
hypothesize, could, predict, and etc.). However,
these keywords are not always used in a specula-
tive context. In other words, they are not always
real speculation keywords. Unlike previous ap-
proaches which classify sentences as speculative
or not, we formulate the problem as classifying the
keywords as real speculation keywords or not. We
extract a diverse set of features such as linguistic
features that represent the context of the keyword
and positional features of the sentence in which
the keyword occurs. We use these features with
Support Vector Machines (SVM) to learn models
to classify whether the occurrence of a keyword
is in a speculative context or not. After detecting
the real speculation keywords, we use the syntactic
structures of the sentences to identify their linguis-
tic scopes.
2 Related Work
Although hedging in scientific articles has been
studied from a linguistics perspective since the
1990s (e.g. (Hyland, 1998)), it has only gained in-
terest from a natural language processing perspec-
tive in the recent years.
The problem of identifying speculative sen-
tences in biomedical articles has been introduced
by Light et al (2004). The authors discussed the
possible application areas of recognizing specu-
lative language and investigated whether the no-
tion of speculative sentences can be characterized
to enable manual annotation. The authors devel-
oped two automated systems to classify sentences
as speculative or not. The first method is based
on substring matching. A sentence is classified as
speculative if it contains one of the 14 predefined
strings (suggest, potential, likely, may, at least, in
part, possibl, further investigation, unlikely, pu-
tative, insights, point toward, promise, propose).
The second method is based on using SVM with
bag-of-words features. The substring matching
method performed slightly better than the SVM
with bag-of-words features approach.
Medlock and Briscoe (2007) extended the work
of Light et al (2004) by refining their annota-
tion guidelines and creating a publicly available
data set (FlyBase data set) for speculative sen-
tence classification. They proposed a weakly su-
pervised machine learning approach to classify
sentences as speculative or not with the aim of
minimizing the need for manually labeled train-
ing data. Their approach achieved 76% preci-
sion/recall break-even point (BEP) performance
on the FlyBase data set, compared to the BEP
of 60% obtained by Light et al?s (2004) sub-
string matching approach on the same data set.
Szarvas (2008) extended the weakly supervised
machine learning methodology of Medlock and
Briscoe (2007) by applying feature selection to re-
duce the number of candidate keywords, by us-
ing limited manual supervision to filter the fea-
tures, and by extending the feature representation
with bigrams and trigrams. In addition, by fol-
lowing the annotation guidelines of Medlock and
Briscoe (2007), Szarvas (2008) made available the
BMC Bioinformatics data set, by annotating four
full text papers from the open access BMC Bioin-
formatics website. They achieved a BEP perfor-
mance of 85.29% and an F-measure of 85.08% on
the FlyBase data set. The F-measure performance
achieved on the BMC Bioinformatics data set was
74.93% when the FlyBase data set was used for
training. Kilicoglu and Bergler (2008) compiled
a list of speculation keywords from the examples
in (Hyland, 1998) and extended this list by us-
ing WordNet (Fellbaum, 1998) and UMLS SPE-
CIALIST Lexicon (McCray et al, 1994). They
used manually crafted syntactic patterns to iden-
tify speculative sentences and achieved a BEP and
an F-measure of 85% on the FlyBase data set and a
BEP and an F-measure of 82% on the BMC Bioin-
formatics data set.
Unlike pervious studies, which treat the prob-
lem of identifying speculative language as a sen-
tence classification task, we tackle the more chal-
lenging problem of identifying the portions of sen-
tences which are speculative. In other words, we
allow a sentence to include both speculative and
non-speculative parts. We introduce and eval-
uate a diverse set of features that represent the
context of a keyword and use these features in
a supervised machine learning setting to classify
1399
the keywords as real speculation keywords or not.
Then, we develop a rule-based method to deter-
mine their linguistic scopes by considering the
keyword-specific features and the syntactic struc-
tures of the sentences. To the best of our knowl-
edge, the BioScope corpus (Vincze et al, 2008) is
the only available data set that has been annotated
for speculative sentence fragments and we report
the first results on this corpus.
3 Corpus
The BioScope corpus
2
has been annotated at the
token level for speculation keywords and at the
sentence level for their linguistic scopes (Vincze
et al, 2008). The corpus consists of three sub-
corpora: medical free texts (radiology reports),
biomedical article abstracts, and biomedical full
text articles. In this paper we focus on identifying
speculations in scientific text. Therefore, we use
the biomedical article abstracts and the biomedi-
cal full text articles in our experiments. The statis-
tics (number of documents, number of sentences,
and number of occurrences of speculation key-
words) for these two sub-corpora are given in Ta-
ble 1. The scientific abstracts in the BioScope cor-
Data Set Documents Sentences Hedge Keywords
Abstracts 1273 11871 2694
Full Papers 9 2670 682
Table 1: Summary of the biomedical scientific articles sub-
corpora of the BioScope corpus
pus were included from the Genia corpus (Col-
lier et al, 1999). The full text papers consist of
five articles from the FlyBase data set and four
articles from the open access BMC Bioinformat-
ics website. The sentences in the FlyBase and
BMC Bioinformatics data sets were annotated as
speculative or not and made available by Med-
lock and Briscoe (2007) and Szarvas (2008), re-
spectively and have been used by previous stud-
ies in identifying speculative sentences (Medlock
and Briscoe, 2007; Kilicoglu and Bergler, 2008;
Szarvas, 2008). Vincze et al (2008) annotated
these full text papers and the Genia abstracts for
speculation keywords and their scopes and in-
cluded them to the BioScope corpus. The key-
words were annotated with a minimalist strategy.
In other words, the minimal unit that expresses
speculation was annotated as a keyword. A key-
word can be a single word (e.g. suggest, predict,
2
Available at: http://www.inf.u-szeged.hu/rgai/bioscope
might) or a phrase (complex keyword), if none of
the words constituting the phrase expresses a spec-
ulation by itself. For example the phrase ?no ev-
idence of?? in the sentence ?Direct sequencing of
the viral genomes and reinfection kinetics showed
no evidence of wild-type reversion even after pro-
longed infection with the Tat- virus.? is an example
of a complex keyword, since the words forming
the phrase can only express speculation together.
In contrast to the minimalist strategy followed
when annotating the keywords, the annotation of
scopes of the keywords was performed by assign-
ing the scope to the largest syntactic unit possible
by including all the elements between the keyword
and the target word to the scope (in order to avoid
scopes without a keyword) and by including the
modifiers of the target word to the scope (Vincze
et al, 2008). The reader can refer to (Vincze et al,
2008) for the details of the corpus and the annota-
tion guidelines.
The inter-annotator agreement rate was mea-
sured as the F-measure of the annotations of the
first annotator by considering the annotations of
the second one as the gold standard. The agree-
ment rate for speculation keyword annotation is
reported as 92.05% for the abstracts and 90.81%
for the full text articles and the agreement rate for
speculation scope resolution is reported as 94.04%
for the abstracts and 89.67% for the full text ar-
ticles (Vincze et al, 2008). These rates can be
considered as the upper bounds for the automated
methods proposed in this paper.
4 Identifying Speculation Keywords
Words and phrases such as ?might?, ?suggest?,
?likely?, ?no evidence of?, and ?remains to be
elucidated? that can render statements speculative
are called speculation keywords. Speculation key-
words are not always used in speculative context.
For instance, consider the following sentences:
1. Thus, it appears that the T-cell-specific activation of
the proenkephalin promoter is mediated by NF-kappa
B. (PMID: 91117203)
2. Differentiation assays using water soluble phorbol es-
ters reveal that differentiation becomes irreversible
soon after AP-1 appears. (PMID: 92088960)
The keyword ?appears? in the first sentence ren-
ders it speculative. However, in the second sen-
tence, ?appears? is not used in a speculative con-
text.
1400
The first sub-problem that we need to solve in
order to identify speculative sentence fragments is
identifying the real speculation keywords in a sen-
tence (i.e. the keywords which convey speculative
meaning in the sentence). We formulate the prob-
lem as a supervised classification task. We extract
the list of keywords from the training data which
has been labeled for speculation keywords. We
match this list of keywords in the unlabeled (test
data) and train a model to classify each occurrence
of a keyword in the unlabeled test set as a real
speculation keyword or not. The challenge of the
task can be demonstrated by the following statis-
tics from the Genia Abstracts of the BioScope cor-
pus. There are 1273 abstracts in the corpus. There
are 138 unique speculation keywords and the to-
tal number of their occurrence in the abstracts is
6125. In only 2694 (less than 50%) of their occur-
rences they are used in speculative context (i.e.,
are real speculation keywords).
In this study we focus on identifying the fea-
tures that represent the context of a speculation
keyword and use SVM with linear kernel (we
used the SVM
light
package (Joachims, 1999)) as
our classification algorithm. The following sub-
section describes the set of features that we pro-
pose.
4.1 Feature Extraction
We introduce a set of diverse types of features
including keyword specific features such as the
stem and the part-of-speech (POS) of the keyword,
and keyword context features such as the words
surrounding the keyword, the dependency rela-
tion types originating at the keyword, the other
keywords that occur in the same sentence as the
keyword, and positional features such as the sec-
tion of the paper in which the keyword occurs.
While designing the features, we were inspired by
studies on other natural language processing prob-
lems such as Word Sense Disambiguation (WSD)
and summarization. For example, machine learn-
ing methods with features based on part-of-speech
tags, word stems, surrounding and co-occurring
words, and dependency relationships have been
successfully used in WSD (Montoyo et al, 2005;
Ng and Lee, 1996; Dligach and Palmer, 2008) and
positional features such as the position of a sen-
tence in the document have been used in text sum-
marization (e.g. (Radev et al, 2004)).
4.1.1 Keyword Features
Statistics from the BioScope corpus suggest that
different keywords have different likelihoods of
being used in a speculative context (Vincze et al,
2008). For example, the keyword ?suggest? has
been used in a speculative context in all its oc-
currences in the abstracts and in the full papers.
On the other hand, ?appear? is a real specula-
tion keyword in 86% of its occurrences in the ab-
stracts and in 83% of its occurrences in the full
papers, whereas ?can? is a real speculation key-
word in 12% of its occurrences in the abstracts and
in 16% of its occurrences in the full papers. POS
of a keyword might also play a role in determining
whether it is a real speculation keyword or not. For
example, consider the keyword ?can?. It is more
likely to have been used in a speculative context
when it is a modal verb, than when it is a noun.
Based on these observations, we hypothesize that
features specific to a keyword such as the keyword
itself, the stem of the keyword, and the POS of
the keyword might be useful in discriminating the
speculative versus non-speculative use of it. We
use Porter?s Stemming Algorithm (Porter, 1980)
to obtain the stems of the keywords and Stanford
Parser (de Marneffe et al, 2006) to get the POS of
the keywords. If a keywords consists of multiple
words, we use the concatenation of the POS of the
words constituting the keyword as a feature. For
example, the extracted POS feature for the key-
words ?no evidence? and ?no proof? is ?DT.NN?
.
4.1.2 Dependency Relation Features
Besides the occurrence of a speculation keyword,
the syntactic structure of the sentence also plays
an important role in characterizing speculations.
Kilicoglu and Bergler (2008) showed that man-
ually identified syntactic patterns are effective in
classifying sentences as speculative or not. They
identified that, while some keywords do not in-
dicate hedging when used alone, they might act
as good indicators of hedging when used with a
clausal complement or with an infinitival clause.
For example, the ?appears? keyword in the ex-
ample sentences, which are given in the beginning
of Section 4, is not a real speculation keyword in
the second example ?...soon after AP-1 appears.?
, whereas it is a real speculation keyword in the
first example, where it is used with a that clausal
complement ?...it appears that...?. Similarly, ?ap-
pears? is used in a speculative context in the fol-
1401
lowing sentence, where it is used with an infini-
tival clause: ?Synergistic transactivation of the
BMRF1 promoter by the Z/c-myb combination ap-
pears to involve direct binding by the Z protein.?.
Another observation is that, some keywords
act as real speculation keywords only when used
with a negation. For example, words such as
?know?, ?evidence?, and ?proof? express cer-
tainty when used alone, but express a speculation
when used with a negation (e.g., ?not known?,
?no evidence?, ?no proof? ).
Auxiliaries in verbal elements might also give
clues for the speculative meaning of the main
verbs. Consider the example sentence: ?Our find-
ings may indicate the presence of a reactivated
virus hosted in these cells.?. The modal auxiliary
?may? acts as a clue for the speculative context of
the main verb ?indicate?.
We defined boolean features to represent the
syntactic structures of the contexts of the key-
words. We used the Stanford Dependency Parser
(de Marneffe et al, 2006) to parse the sentences
that contain a candidate speculation keyword and
extracted the following features from the depen-
dency parse trees.
Clausal Complement: A Boolean feature which is set to 1,
if the keyword has a child which is connected to it with
a clausal complement or infinitival clause dependency
type.
Negation: A Boolean feature which is set to 1, if the key-
word (1) has a child which is connected to it with a
negation dependency type (e.g. ?not known?: ?not? is
a child of ?known?, and the Stanford Dependency Type
connecting them is ?neg?) or (2) the determiner ?no? is
a child of the keyword (e.g., ?no evidence?: ?no? is a
child of ?evidence? and the Stanford Dependency Type
connecting them is ?det?).
Auxiliary: A Boolean feature which is set to 1, if the key-
word has a child which is connected to it with an auxil-
iary dependency type (e.g. ?may indicate?: ?may? is a
child of ?indicate?, and the Stanford Dependency Type
connecting them is ?aux?).
If a keyword consists of multiple-words, we ex-
amine the children of the word which is the an-
cestor of the other words constituting the key-
word. For example, ?no evidence? is a multi-word
keyword, where ?evidence? is the parent of ?no?.
Therefore, we extract the dependency parse tree
features for the word ?evidence?.
4.1.3 Surrounding Words
Recent studies showed that using machine learn-
ing with variants of the ?bag-of-words? feature
representation is effective in classifying sentences
as speculative vs. non-speculative (Light et al,
2004; Medlock and Briscoe, 2007; Szarvas, 2008).
Therefore, we also decided to include bag-of-
words features that represent the context of the
speculation keyword. We extracted the words sur-
rounding the keyword and performed experiments
both with and without stemming, and with win-
dow sizes of one, two, and three. Consider the
sentence: ?Our findings may indicate the presence
of a reactivated virus hosted in these cells.?. The
bag-of-words features for the keyword ?indicate?,
when a window size of three and no stemming is
used are: ?our?, ?findings?, ?may?, ?indicate?,
?the?, ?presence?, ?of?. In other words, the fea-
ture set consists of the keyword, the three words to
the left of the keyword, and the three words to the
right of the keyword.
4.1.4 Positional Features
Different parts of a scientific article might have
different characteristics in terms of the usage of
speculative language. For example, Hyland (1998)
analyzed a data set of molecular biology articles
and reported that the distribution of speculations
is similar between abstracts and full text articles,
whereas the Results and Discussion sections tend
to contain more speculative statements compared
to the other sections (e.g. Materials and Methods
or Introduction and Background sections). The
analysis of Light et al (2004) showed that the last
sentence of an abstract is more likely to be specu-
lative than non-speculative.
For the scientific abstracts data set, we defined
the following boolean features to represent the po-
sition of the sentence the keyword occurs in. Our
intuition is that titles and the first sentences in the
abstract tend to be non-speculative, whereas the
last sentence of the abstract tends to be specula-
tive.
Title: A Boolean feature which is set to 1, if the keyword
occurs in the title.
First Sentence: A Boolean feature which is set to 1, if the
keyword occurs in the first sentence of the abstract.
Last Sentence: A Boolean feature which is set to 1, if the
keyword occurs in the last sentence of the abstract.
For the scientific full text articles data set, we
defined the following features that represent the
position of the sentence in which the keyword oc-
curs. Our assumption is that the ?Results and Dis-
cussion? and the ?Conclusion? sections tend to
1402
contain more speculative statements than the ?Ma-
terials and Methods? and ?Introduction and Back-
ground? sections. We also assume that figure and
table legends are not likely to contain speculative
statements.
Title: A Boolean feature which is set to 1, if the keyword
occurs in the title of the article, or in the title of a sec-
tion or sub-section.
First Sentence: A Boolean feature which is set to 1, if the
keyword occurs in the first sentence of the abstract.
Last Sentence: A Boolean feature which is set to 1, if the
keyword occurs in the last sentence of the abstract.
Background: A Boolean feature which is set to 1, if the
keyword occurs in the Background or Introduction sec-
tion.
Results: A Boolean feature which is set to 1, if the keyword
occurs in the Results or in the Discussion section.
Methods: A Boolean feature which is set to 1, if the key-
word occurs in the Materials and Methods section.
Conclusion: A Boolean feature which is set to 1, if the key-
word occurs in the Conclusion section.
Legend: A Boolean feature which is set to 1, if the keyword
occurs in a table or figure legend.
4.1.5 Co-occurring Keywords
Speculation keywords usually co-occur in the sen-
tences. Consider the sentence: ?We, therefore,
wished to determine whether T3SO4 could mimic
the action of thyroid hormone in vitro.?. Here,
?whether? and ?could? are speculation keywords
and their co-occurence might be a clue for their
speculative context. Therefore, we decided to in-
clude the co-occurring keywords to the feature set
of a keyword.
5 Resolving the Scope of a Speculation
After identifying the real speculation keywords,
the next step is determining their scopes in the sen-
tences, so that the speculative sentence fragments
can be detected. Manual analysis of sample sen-
tences from the BioScope corpus and their parse
trees suggests that the scope of a keyword can be
characterized by its part-of-speech and the syntac-
tic structure of the sentence in which it occurs.
Consider the example sentence whose parse tree
is shown in Figure 1. The sentence contains three
speculation keywords, ?or? and two occurrences
of ?might?. The scope of the conjunction ?or?, ex-
tends to the ?VP? whose children it coordinates.
In other words, the scope of ?or? is ?[might be
one of the earliest crucial steps in the lysis of nor-
mal and dex-resistant CEM cells, or might serve
as a marker for the process]?. Here, ?or? con-
veys a speculative meaning, since we are not cer-
tain which of the two sub-clauses (sub-clause 1:
[might be one of the earliest crucial steps in the
lysis of normal and dex-resistant CEM cells] or
sub-clause 2: [might serve as a marker for the pro-
cess]) is correct. The scope of both occurrences
of the modal verb ?might? is the parent ?VP?. In
other words, the scope of the first occurrence of
?might? is ?[might be one of the earliest crucial
steps in the lysis of normal and dex-resistant CEM
cells]? and the scope of the second occurrence of
?might? is ?[might serve as a marker for the pro-
cess]?. By examining the keywords, sample sen-
tences and their syntactic parse trees we devel-
oped the following rule-based approach to resolve
the scopes of speculation keywords. The exam-
ples given in this section are based on the syntactic
structure of the Penn Tree Bank. But, the rules are
generic (e.g. ?the scope of a verb followed by an
infinitival clause, extends to the whole sentence?).
The scope of a conjunction or a determiner (e.g.
or, and/or, vs) is the syntactic phrase to which it
is attached. For example, the scope of ?or? in
Figure 1 is the ?VP? immediately dominating the
?CC?.
The scope of a modal verb (e.g. may, might,
could) is the ?VP? to which it is attached. For
example, the scope of ?might? in Figure 1 is the
?VP? immediately dominating the ?MD?.
The scope of an adjective or an adverb starts
with the keyword and ends with the last token of
the highest level ?NP? which dominates the ad-
jective or the adverb. Consider the sentence ?The
endocrine events that are rapidly expressed (sec-
onds) are due to a [possible interaction with cellu-
lar membrane].? The scope of the speculation key-
word ?possible? is enclosed in rectangular brack-
ets. The sub-tree that this scope maps to is: ?(NP
(NP (DT a) (JJ possible) (NN interaction)) (PP
(IN with) (NP (JJ cellular) (NN membrane))))?.
If there does not exist a ?NP? dominating the ad-
verb or adjective keyword, the scope extends to
the whole sentence. For example the scope of
the speculation adverb ?probably? in the sentence
?[The remaining portion of the ZFB motif was
probably lost in TPases of insect Transib trans-
posons]? is the whole sentence.
The scope of a verb followed by an infinitival
1403
Figure 1: The syntactic parse tree of the sentence ?Positive induction of GR mRNA might be one of the earliest crucial steps
in the lysis of normal and dex-resistant CEM cells, or might serve as a marker for the process.?
clause extends to the whole sentence. For exam-
ple, the scope of the verb ?appears? followed by
the ?to? infinitival clause is the whole sentence in
?[The block of pupariation appears to involve sig-
naling through the adenosine receptor (AdoR)]?.
The scope of a verb in passive voice extends
to the whole sentence such as the scope of ?sug-
gested? in ?[The existence of such an indepen-
dent mechanism has also been suggested in mam-
mals]?.
If none of the above rules apply, the scope of a
keyword starts with the keyword and ends at the
end of the sentence (or clause). An example is
the scope of ?suggested? in ?This [suggested that
there is insufficient data currently available to de-
termine a reliable ratio for human]?.
6 Evaluation
We evaluated our approach on two different types
of scientific text from the biomedical domain,
namely the scientific abstracts sub-corpus and the
full text articles sub-corpus of the BioScope cor-
pus (see Section 3). We used stratified 10-fold
cross-validation to evaluate the performance on
the abstracts. In each fold, 90% of the abstracts are
used for training and 10% are used to test. To facil-
itate comparison with future studies the PubMed
Identifiers of the abstracts that we used as a test
set in each fold are provided
3
. The full text pa-
pers sub-corpus consists of nine articles. We used
leave-one-out cross-validation to evaluate the per-
3
http://belobog.si.umich.edu/clair/bioscope/
formance on the full text papers. In each iteration
eight articles are used for training and one article
is used to test. We report the average results over
the runs for each data set.
6.1 Evaluation of Identifying Speculation
Keywords
To classify whether the occurrence of a keyword is
in speculative context or not, we built linear SVM
models by using various combinations of the fea-
tures introduced in Section 4.1. Tables 2 and 3
summarize the results obtained for the abstracts
and the full text papers, respectively. BOW N is
the bag-of-words features obtained from the words
surrounding the keyword (see Section 4.1.3). N is
the window size. We experimented both with the
stemmed and non-stemmed versions of this fea-
ture type. The non-stemmed versions performed
slightly better than the stemmed versions. The rea-
son might be due to the different likelihoods of
being used in a speculative context of different in-
flected forms of words. For example, consider the
words ?appears? and ?appearance?. They have the
same stems, but ?appearance? is less likely to be a
real speculation keyword than ?appears?. Another
observation is that, decreasing the window size
led to improvement in performance. This suggests
that the words right before and right after the can-
didate speculation keyword are more effective in
distinguishing its speculative vs. non-speculative
context compared to a wider local context. Wider
local context might create sparse data and degrade
1404
performance. Consider the example, ?it appears
that TP53 interacts with AR?. The keyword ?ap-
pears?, and BOW1 (?it? and ?that?) are more rel-
evant for the speculative context of the keyword
than ?TP53?, ?interacts?, and ?with?. Therefore,
for the rest of the experiments we used the BOW
1 version, i.e., the non-stemmed surrounding bag-
of-words with window size of 1. KW stands for
the keyword specific features, i.e., the keyword, its
stem, and its part-of-speech (discussed in Section
4.1.1). DEP stands for the dependency relation
features (discussed in Section 4.1.2). POS stands
for the positional features (discussed in Section
4.1.4) and CO-KW stands for the co-occurring
keywords feature (discussed in Section 4.1.5).
Our results are not directly comparable with
the prior studies about identifying speculative sen-
tences (see Section 2), since we attempted to solve
a different problem, which is identifying specula-
tive parts of sentences. Only the substring match-
ing approach that was introduced in (Light et al,
2004) could be adapted as a keyword classification
task, since the substrings are keywords themselves
and we used this approach as a baseline in the
keyword classification sub-problem. We compare
the performances of our models with two baseline
methods, which are based on the substring match-
ing approach. Light et al (2004) have shown that
the substring matching method with a predefined
set of 14 strings performs slightly better than an
SVM model with bag-of-words features in classi-
fying sentences as speculative vs. non-speculative
(see Section 2). In baseline 1, we use the 14 strings
identified in (Light et al, 2004) and classify all the
keywords in the test set that match any of them as
real speculation keywords. Baseline 2 is similar
to baseline 1, with the difference that rather than
using the set of strings in (Light et al, 2004), we
extract the set of keywords from the training set
and classify all the words (or phrases) in the test
set that match any of the keywords in the list as
real speculation keywords.
Baseline 1 achieves high precision, but low re-
call. Whereas, baseline 2 achieves high recall in
the expense of low precision. All the SVM mod-
els in Tables 2 and 3 achieve more balanced preci-
sion and recall values, with F-measure values sig-
nificantly higher than the baseline methods. We
start with a model that uses only the keyword-
specific features (KW). This type of feature alone
achieved a significantly better performance than
the baseline methods (90.61% F-measure for the
abstracts and 80.57% F-measure for the full text
papers), suggesting that the keyword-specific fea-
tures are important in determining its specula-
tive context. We extended the feature set by in-
cluding the dependency relation (DEP), surround-
ing words (BOW 1), positional (POS), and co-
occurring keywords (CO-KW) features. Each new
type of included feature improved the performance
of the model for the abstracts. The best F-measure
(91.69%) is achieved by using all the proposed
types of features. This performance is close to the
upper bound, which is the human inter-annotator
agreement F-measure of 92.05%.
Including the co-occurring keywords to the fea-
ture set for full text articles slightly improved pre-
cision, but deceased recall, which led to lower F-
measure. The best F-measure (82.82%) for the
full text articles is achieved by using all the fea-
ture types except the co-occurring keywords. The
achieved performance is significantly higher than
the baseline methods, but lower than the human
inter-annotator agreement F-measure of 90.81%.
The lower performance for the full text papers
might be due to the small size of the data set (9
full text papers compared to 1273 abstracts).
Method Recall Precision F-Measure
Baseline 1 52.84 92.71 67.25
Baseline 2 97.54 43.66 60.30
BOW 3 - stemmed 81.47 92.36 86.51
BOW 2 - stemmed 81.56 93.29 86.97
BOW 1 - stemmed 83.08 93.83 88.05
BOW 3 82.58 92.04 86.98
BOW 2 82.77 92.74 87.41
BOW 1 83.27 93.67 88.10
KW: kw, kw-stem, kw-pos 88.62 92.77 90.61
KW, DEP 88.77 92.67 90.64
KW, DEP, BOW 1 88.46 94.71 91.43
KW, DEP, BOW 1, POS 88.16 95.21 91.50
KW, DEP, BOW 1, POS, CO-KW 88.22 95.56 91.69
Table 2: Results for the Scientific Abstracts
Method Recall Precision F-Measure
Baseline 1 33.77 86.75 47.13
Baseline 2 88.22 52.57 64.70
BOW 3 - stemmed 70.79 83.88 76.58
BOW 2 - stemmed 72.31 85.49 78.11
BOW 1 - stemmed 73.49 84.35 78.41
BOW 3 70.54 82.56 75.88
BOW 2 71.52 85.93 77.94
BOW 1 73.72 86.27 79.43
KW: kw, kw-stem, kw-pos 75.21 87.08 80.57
KW, DEP 75.02 89.49 81.53
KW, DEP, BOW 1 76.15 89.54 82.27
KW, DEP, BOW 1, POS 76.17 90.81 82.82
KW, DEP, BOW 1, POS, CO-KW 75.76 90.82 82.58
Table 3: Results for the Scientific Full Text Papers
1405
6.2 Evaluation of Resolving the Scope of a
Speculation
We compared the proposed rule-based approach
for scope resolution with two baseline methods.
Previous studies classify sentences as speculative
or not, therefore implicitly assigning the scope of
a speculation to the whole sentence (Light et al,
2004; Medlock and Briscoe, 2007; Szarvas, 2008;
Kilicoglu and Bergler, 2008). Baseline 1 follows
this approach and assigns the scope of a specu-
lation keyword to the whole sentence. Szarvas
(2008) suggest assigning the scope of a keyword
from its occurrence to the end of the sentence.
They state that this approach works accurately for
clinical free texts, but no any results are reported
(Szarvas, 2008). Baseline 2 follows the approach
proposed in (Szarvas, 2008) and assigns the scope
of a keyword to the fragment of the sentence that
starts with the keyword and ends at the end of the
sentence. Table 4 summarizes the accuracy results
obtained for the abstracts and the full text papers.
The poor performance of baseline 1, empha-
sizes the importance of detecting the portions of
sentences that are speculative, since less than 5%
of the sentences that contain speculation keywords
are entirely speculative. Classifying the entire sen-
tences as speculative or not leads to loss in infor-
mation for more than 95% of the sentences. The
rule-based method significantly outperformed the
two baseline methods, indicating that the part-of-
speech of the keywords and the syntactic parses
of the sentences are effective in characterizing the
speculation scopes.
Method Accuracy-Abstracts Accuracy-Full text
Baseline 1 4.82 4.29
Baseline 2 67.60 42.82
Rule-based method 79.89 61.13
Table 4: Scope resolution results
7 Conclusion
We presented an approach to identify speculative
sentence fragments in scientific articles. Our ap-
proach is based on solving two sub-problems. The
first one is identifying the keywords which are
used in speculative context and the second one is
determining the scopes of these keywords in the
sentences. We evaluated our approach for two
types of scientific texts, namely abstracts and full
text papers from the BioScope corpus.
We formulated the first sub-problem as a super-
vised classification task, where the aim is to learn
models to classify the candidate speculation key-
words as real speculation keywords or not. We fo-
cused on identifying different types of linguistic
features that capture the contexts of the keywords.
We achieved a performance which is significantly
better than the baseline methods and comparable
to the upper-bound, which is the human inter-
annotator agreement F-measure.
We hypothesized that the scope of a specula-
tion keyword can be characterized by its part-of-
speech and the syntactic structure of the sentence
and developed rules to map the scope of a key-
word to the nodes in the syntactic parse tree. We
achieved a significantly better performance com-
pared to the baseline methods. The considerably
lower performance of the baseline of assigning the
scope of a speculation keyword to the whole sen-
tence indicates the importance of detecting specu-
lative sentence portions rather than classifying the
entire sentences as speculative or not.
Acknowledgements
This work was supported in part by the NIH Grant
U54 DA021519 to the National Center for Integra-
tive Biomedical Informatics.
References
Nigel Collier, Hyun S. Park, Norihiro Ogata, Yuka
Tateishi, Chikashi Nobata, Tomoko Ohta, Tateshi
Sekimizu, Hisao Imai, Katsutoshi Ibushi, and Jun I.
Tsujii. 1999. The GENIA project: corpus-based
knowledge acquisition and information extraction
from genome research papers. In Proceedings of the
ninth conference on European chapter of the Asso-
ciation for Computational Linguistics, pages 271?
272. Association for Computational Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of LREC-06.
Dmitriy Dligach and Martha Palmer. 2008. Novel Se-
mantic Features for Verb Sense Disambiguation. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
Ken Hyland. 1998. Hedging in Scientific Research
Articles. John Benjamins Publishing Co.
1406
T. Joachims, 1999. Advances in Kernel Methods-
Support Vector Learning, chapter Making Large-
Scale SVM Learning Practical. MIT-Press.
Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing speculative language in biomedical research ar-
ticles: a linguistically motivated perspective. BMC
Bioinformatics, 9(Suppl 11).
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The language of bioscience: Facts, spec-
ulations, and statements in between. In Lynette
Hirschman and James Pustejovsky, editors, HLT-
NAACL 2004 Workshop: BioLINK 2004, Linking
Biological Literature, Ontologies and Databases,
pages 17?24, Boston, Massachusetts, USA, May 6.
Association for Computational Linguistics.
A. T. McCray, S. Srinivasan, and A. C. Browne. 1994.
Lexical methods for managing variation in biomed-
ical terminologies. Proc Annu Symp Comput Appl
Med Care, pages 235?239.
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 992?999, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Andres Montoyo, Armando Suarez, German Rigau,
and Manuel Palomar. 2005. Combining knowledge-
and corpus-based word-sense-disambiguation meth-
ods. Journal of Artificial Intelligence Research,
23:299?330.
H. T. Ng and H. B Lee. 1996. Integrating multi-
ple knowledge sources to disambiguate word senses:
An exemplar-based approach. In Proceedings of the
34th Annual Meeting of the Association for Compu-
tational Linguistics.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 3(14):130?137.
Dragomir R. Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda Celebi, Stanko
Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam,
Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio
Saggion, Simone Teufel, Adam Winkel, and Zhang
Zhu. 2004. Mead - a platform for multidocument
multilingual text summarization. In Proceedings of
LREC 2004.
Gyorgy Szarvas. 2008. Hedge classification in
biomedical texts with a weakly supervised selection
of keywords. In ACL 2008.
Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-
orgy Mora, and Janos Csirik. 2008. The BioScope
corpus: biomedical texts annotated for uncertainty,
negation and their scopes. BMC Bioinformatics,
9(Suppl 11).
1407
Proceedings of the Workshop on BioNLP: Shared Task, pages 111?114,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Supervised Classification for Extracting Biomedical Events
Arzucan O?zgu?r
Department of EECS
University of Michigan
Ann Arbor, MI 48109, USA
ozgur@umich.edu
Dragomir R. Radev
Department of EECS and
School of Information
University of Michigan
Ann Arbor, MI 48109, USA
radev@umich.edu
Abstract
We introduce a supervised approach for ex-
tracting bio-molecular events by using linguis-
tic features that represent the contexts of the
candidate event triggers and participants. We
use Support Vector Machines as our learning
algorithm and train separate models for event
types that are described with a single theme
participant, multiple theme participants, or a
theme and a cause participant. We perform ex-
periments with linear kernel and edit-distance
based kernel and report our results on the
BioNLP?09 Shared Task test data set.
1 Introduction
Most previous work on biomedical information ex-
traction focuses on identifying relationships among
biomedical entities (e.g. protein-protein interac-
tions). Unlike relationships, which are in general
characterized with a pair of entities, events can be
characterized with event types and multiple entities
in varying roles. The BioNLP?09 Shared Task ad-
dresses the extraction of bio-molecular events from
the biomedical literature (Kim et al, 2009). We par-
ticipated in the ?Event Detection and Characteriza-
tion? task (Task 1). The goal was to recognize the
events concerning the given proteins by detecting
the event triggers, determining the event types, and
identifying the event participants.
In this study, we approach the problem as a su-
pervised classification task. We group the event
types into three general classes based on the num-
ber and types of participants that they involve. The
first class includes the event types that are described
with a single theme participant. The second class in-
cludes the event types that are described with one or
more theme participants. The third class includes
the events that are described with a theme and/or
a cause participant. We learn support vector ma-
chine (SVM) models for each class of events to clas-
sify each candidate event trigger/participant pair as
a real trigger/participant pair or not. We use vari-
ous types of linguistic features such as lexical, posi-
tional, and dependency relation features that repre-
sent the contexts of the candidate trigger/participant
pairs. The results that we submitted to the shared
task were based on using a linear kernel function. In
this paper, we also report our results based on using
an edit-distance based kernel defined on the shortest
dependency relation type paths between a candidate
trigger/participant pair.
2 System Description
2.1 Event Type Classes
We grouped the nine event types targeted at the
BioNLP?09 Shared Task into three general event
classes based on the number and types of partici-
pants that they involve.
Class 1 Events: Events that involve a single theme participant
(Gene expression, Transcription, Protein catabolism, Lo-
calization, and Phosphorylation event types).
Class 2 Events: Events that can involve one or more theme
participants (Binding event type).
Class 3 Events: Events that can be described with a theme
and/or a cause participant (Regulation, Positive regula-
tion, and Negative regulation event types). Unlike Class 1
111
and Class 2 events, where the participants are proteins, the
participants of Class 3 events can be proteins or events.
Since the event types in each class are similar to
each other based on the number and roles of par-
ticipants that they involve and different from the
event types in the other classes, we learned sepa-
rate classification models for each class. We for-
mulated the classification task as the classification
of trigger/participant pairs. We extracted positive
and negative training instances (trigger/participant
pairs) from the training data for each class of events.
We considered only the pairs that appear in the
same sentence. We used the tokenized and sentence
split abstracts provided by the shared task organiz-
ers1. Consider the sentence ?The phosphorylation of
TRAF2 inhibits binding to the CD40 cytoplasmic do-
main?. This sentence describes the following three
events:
1. Event1: Type: Phosphorylation Trigger: phosphorylation
Theme: TRAF2
2. Event2: Type: Binding Trigger: binding Theme1:
TRAF2 Theme2: CD40
3. Event3: Type: Negative regulation Trigger: inhibits
Theme: Event2 Cause: Event1
Event1 belongs to Class 1. The trigger/participant
pair (phosphorylation, TRAF2) is a positive instance
for Class 1. Event2 belongs to Class 2. It has
two theme participants. The instances for Class 2
events are created by decomposing the events into
trigger/theme pairs. The two positive instances ex-
tracted from the decomposition of Event2 are (bind-
ing, TRAF2) and (binding, CD40). Event3 belongs
to Class 3. It consists of two semantically differ-
ent participants, namely a theme and a cause. We
trained two separate models for Class 3 events, i.e.,
one model to classify the themes and another model
to classify the causes. Another distinguishing char-
acteristic of Class 3 events is that a participant of
an event can be a protein or an event. We repre-
sent the participants that are events with their cor-
responding event triggers. We decompose Event3
into its theme and cause and represent its cause
Event1 with its trigger word ?phosphorylation? and
1http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA
/SharedTask/tools.html
its theme Event2 with its trigger word ?binding?. As
a result, (inhibits, binding) and (inhibits, phosphory-
lation) are included as positive instances to the Class
3 theme and Class 3 cause training sets, respectively.
Negative instances for Class 1 and Class 2 are cre-
ated by including all the trigger/protein pairs which
are not among the positive instances of that class.
Negative instances for Class 3 theme and Class 3
cause are created by including all the trigger/protein
and trigger1/trigger2 pairs which are not among the
positive instances of that class. For example, (phos-
phorylation, CD40) is a negative instance for Class
1 and (inhibits, TRAF2) is a negative instance for
Class 3 theme and Class 3 cause.
2.2 Feature Extraction
2.2.1 Lexical and Part-of-Speech Features
We used the candidate trigger and its part-of-
speech, which was obtained by using the Stanford
Parser, as features, based on our observation that dif-
ferent candidate triggers might have different likeli-
hoods of being a real trigger for a certain event. For
example, ?transcription? is a trigger for the Tran-
scription event 277 times in the training set and has
not been used as a trigger for other types of events.
On the other hand, ?concentration? is used only
once as a trigger for a Transcription event and three
times as a trigger for Regulation events.
2.2.2 Positional Features
We used two features to represent the relative po-
sition of the participant with regard to the trigger
in the sentence. The first feature has two values,
namely ?before? (the participant appears before the
trigger) or ?after? (the participant appears after the
trigger). The second feature encodes the distance
between the trigger and the participant. Distance is
measured as the number of tokens between the trig-
ger and the participant. Our intuition is that, if a
candidate trigger and participant are far away from
each other, it is less likely that they characterize an
event.
2.2.3 Dependency Relation Features
A dependency parse tree captures the semantic
predicate-argument dependencies among the words
of a sentence. Dependency tree paths between pro-
tein pairs have successfully been used to identify
112
protein interactions (Bunescu and Mooney, 2007;
Erkan et al, 2007). In this paper, we use the
dependency paths to extract events. For a given
trigger/participant pair, we extract the shortest path
from the trigger to the participant, from the depen-
dency parse of the sentence. We use the McClosky-
Charniak parses which are converted to the Stan-
ford Typed Dependencies format and provided to the
participants by the shared task organizers. Previous
approaches use both the words and the dependency
relation types to represent the paths (Bunescu and
Mooney, 2007; Erkan et al, 2007). Consider the de-
pendency tree in Figure 1. The path from ?phospho-
rylation? to ?CD40? is ?nsubj inhibits acomp bind-
ing prep to domain num?. Due to the large num-
ber of possible words, using the words on the paths
might lead to data sparsity problems and to poor
generalization. Suppose we have a sentence with
similar semantics, where the synonym word ?pre-
vents? is used instead of ?inhibits?. If we use the
words on the path to represent the path feature, we
end up with two different paths for the two sen-
tences that have similar semantics. Therefore, in
this study we use only the dependency relation types
among the words to represent the paths. For ex-
ample, the path feature extracted for the (phospho-
rylation, CD40) negative trigger/participant pair is
?nsubj acomp prep to num? and the path feature ex-
tracted for the (phosphorylation, TRAF2) positive
trigger/participant pair is ?prep of?.
inhibits
phosphorylation binding
TRAF2 domain
cytoplasmic CD40 the
acomp
prep_of prep_to
amod detnum
nsubj
Figure 1: The dependency tree of the sentence ?The phos-
phorylation of TRAF2 inhibits binding to the CD40 cyto-
plasmic domain.?
2.3 Classification
We used the SVM light library (Joachims, 1999)
with two different kernel functions and feature sets
for learning the classification models. Our first ap-
proach is based on using linear SVM with the fea-
tures described in Section 2.2. In this approach
the path feature is used as a nominal feature. Our
second approach is based on integrating to SVM a
kernel function based on the word-based edit dis-
tance between the dependency relation paths, where
each dependency relation type on the path is treated
as a word. For example, the word-based edit dis-
tance between the paths ?prep of? and ?prep of
prep with? is 1, since 1 insertion operation (i.e., in-
serting ?prep with? to the first path) is sufficient to
transform the first path to the second one. The edit-
distance based similarity between two paths pi and
pj and the corresponding kernel function are defined
as follows (Erkan et al, 2007).
edit sim(pi, pj) = e??(edit distance(pi,pj)) (1)
3 Experimental Results
The data provided for the shared task is prepared
from the GENIA corpus (Kim et al, 2008). We used
the training and the development sets for training.
The candidate triggers are detected by using a dic-
tionary based approach, where the dictionary is ex-
tracted from the training set. We filtered out the
noisy trigger candidates such as ?with?, ?+?, ?:?, and
?-?, which are rarely used as real triggers and com-
monly used in other contexts. The candidate trig-
ger/participant pairs are classified by using the clas-
sifiers learned for Class 1, Class 2, and/or Class 3
depending on whether the candidate trigger matched
one of the triggers in these classes. The SVM score
is used to disambiguate the event types, if a candi-
date trigger matches a trigger in more than one of the
event classes. A trigger which is ambiguous among
the event types in the same class is assigned to the
event type for which it is most frequently used as a
trigger.
The results that we submitted to the shared task
were obtained by using the linear SVM approach
with the set of features described in Section 2.2.
After submitting the results, we noticed that we
made an error in pre-processing the data set. While
aligning the provided dependency parses with the
113
sentence, we incorrectly assumed that all the sen-
tences had dependency parses and ended up using
the wrong dependency parses for most of the sen-
tences. The overall performance scores for our of-
ficial submission are 30.42% recall, 14.11% preci-
sion, and 19.28% F-measure. The results obtained
after correcting the error are reported in Table 1.
Correcting the error significantly improved the per-
formance of the system. Table 2 shows the re-
sults obtained by using SVM with dependency path
edit kernel. The two SVM models achieve similar
performances. The performance for the regulation
events is considerably lower, since errors in identi-
fying the events are carried to identifying the event
participants of a regulation event. The performances
for the events which have multiple participants, i.e.,
binding and regulation events, are lower compared
to the events with a single participant. The perfor-
mance is higher when computed by decomposing
the events (49.00 and 31.82 F-measure for binding
and regulation events, respectively). This suggests
that even when participants of events are identified
correctly, there is significant amount of error in com-
posing the events.
Event Type Recall Precision F-measure
Localization 41.95 60.83 49.66
Binding 31.41 34.94 33.08
Gene expression 61.36 69.00 64.96
Transcription 37.23 30.72 33.66
Protein catabolism 64.29 64.29 64.29
Phosphorylation 68.15 80.70 73.90
Event Total 50.82 56.80 53.64
Regulation 15.12 19.82 17.15
Positive regulation 24.21 33.33 28.05
Negative regulation 21.64 32.93 26.11
Regulation Total 22.02 30.72 25.65
All Total 35.86 44.69 39.79
Table 1: Approximate span & recursive matching results
using linear SVM with the set of features described in
Section 2.2 (after correcting the error in pre-processing
the data set).
4 Conclusion
We described a supervised approach to extract bio-
molecular events. We grouped the event types into
three general classes based on the number and types
of participants that they can involve and learned sep-
arate SVM models for each class. We used various
Event Type Recall Precision F-measure
Localization 49.43 64.18 55.84
Binding 31.70 35.03 33.28
Gene expression 66.34 69.72 67.99
Transcription 39.42 25.59 31.03
Protein catabolism 78.57 73.33 75.86
Phosphorylation 76.30 80.47 78.33
Event Total 55.13 56.62 55.86
Regulation 17.87 16.46 17.13
Positive regulation 26.45 26.03 26.24
Negative regulation 25.33 32.54 28.49
Regulation Total 24.68 25.34 25.01
All Total 39.31 40.37 39.83
Table 2: Approximate span & recursive matching results
using SVM with dependency relation path edit kernel.
types of linguistic features that represent the context
of the candidate event trigger/participant pairs. We
achieved an F-measure of 39.83% on the shared task
test data. Error analysis suggests that improving the
approach of event composition for types of events
with multiple participants and improving the strat-
egy for detecting and disambiguating triggers can
enhance the performance of the system.
Acknowledgments
This work was supported in part by the NIH Grant
U54 DA021519.
References
R. C. Bunescu and R. J. Mooney, 2007. Text Mining and
Natural Language Processing, Chapter Extracting Re-
lations from Text: From Word Sequences to Depen-
dency Paths, pages 29?44, Springer.
Gu?nes? Erkan, Arzucan O?zgu?r, and Dragomir R. Radev.
2007. Semi-supervised classification for extracting
protein interaction sentences using dependency pars-
ing. In Proceedings of EMNLP, pages 228?237.
T. Joachims, 1999. Advances in Kernel Methods-Support
Vector Learning, Chapter Making Large-Scale SVM
Learning Practical. MIT-Press.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(1).
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop. To
appear.
114
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 895?903,
Beijing, August 2010
Citation Summarization Through Keyphrase Extraction
Vahed Qazvinian
Department of EECS
University of Michigan
vahed@umich.edu
Dragomir R. Radev
School of Information and
Department of EECS
University of Michigan
radev@umich.edu
Arzucan ?Ozgu?r
Department of EECS
University of Michigan
ozgur@umich.edu
Abstract
This paper presents an approach to sum-
marize single scientific papers, by extract-
ing its contributions from the set of cita-
tion sentences written in other papers. Our
methodology is based on extracting sig-
nificant keyphrases from the set of cita-
tion sentences and using these keyphrases
to build the summary. Comparisons show
how this methodology excels at the task
of single paper summarization, and how it
out-performs other multi-document sum-
marization methods.
1 Introduction
In recent years statistical physicists and computer
scientists have shown great interest in analyzing
complex adaptive systems. The study of such sys-
tems can provide valuable insight on the behav-
ioral aspects of the involved agents with potential
applications in economics and science. One such
aspect is to understand what motivates people to
provide the n+1st review of an artifact given that
they are unlikely to add something significant that
has not already been said or emphasized. Cita-
tions are part of such complex systems where ar-
ticles use citations as a way to mention different
contributions of other papers, resulting in a col-
lective system.
The focus of this work is on the corpora cre-
ated based on citation sentences. A citation sen-
tence is a sentence in an article containing a ci-
tation and can contain zero or more nuggets (i.e.,
non-overlapping contributions) about the cited ar-
ticle. For example the following sentences are a
few citation sentences that appeared in the NLP
literature in past that talk about Resnik?s work.
The STRAND system (Resnik, 1999), for example, uses
structural markup information from the pages, without
looking at their content, to attempt to align them.
Resnik (1999) addressed the issue of
language identification for finding Web pages in
the languages of interest.
Mining the Web for bilingual text (Resnik, 1999) is not
likely to provide sufficient quantities of high quality
data..
The set of citations is important to analyze be-
cause human summarizers have put their effort
collectively but independently to read the target
article and cite its important contributions. This
has been shown in other work too (Elkiss et al,
2008; Nanba et al, 2004; Qazvinian and Radev,
2008; Mei and Zhai, 2008; Mohammad et al,
2009). In this work, we introduce a technique
to summarize the set of citation sentences and
cover the major contributions of the target paper.
Our methodology first finds the set of keyphrases
that represent important information units (i.e.,
nuggets), and then finds the best set of k sentences
to cover more, and more important nuggets.
Our results confirm the effectiveness of the
method and show that it outperforms other state
of the art summarization techniques. Moreover,
as shown in the paper, this method does not need
to calculate the full cosine similarity matrix for a
document cluster, which is the most time consum-
ing part of the mentioned baseline methods.
1.1 Related Work
Previous work has used citations to produce sum-
maries of scientific work (Qazvinian and Radev,
895
2008; Mei and Zhai, 2008; Elkiss et al, 2008).
Other work (Bradshaw, 2003; Bradshaw, 2002)
benefits from citations to determine the content of
articles and introduce ?Reference Directed Index-
ing? to improve the results of a search engine.
In other work, (Nanba and Okumura, 1999) an-
alyze citation sentences and automatically cate-
gorize citations into three groups using 160 pre-
defined phrase-based rules to support a system for
writing a survey. Previous research has shown
the importance of the citation summaries in un-
derstanding what a paper contributes. In partic-
ular, (Elkiss et al, 2008) performed a large-scale
study on citation summaries and their importance.
Results from this experiment confirmed that the
?Self Cohesion? (Elkiss et al, 2008) of a citation
summary of an article is consistently higher than
the that of its abstract and that citations contain
additional information that does not appear in ab-
stracts.
Kan et al (2002) use annotated bibliographies
to cover certain aspects of summarization and sug-
gest using metadata and critical document features
as well as the prominent content-based features to
summarize documents. Kupiec et al (1995) use
a statistical method and show how extracts can
be used to create summaries but use no annotated
metadata in summarization.
Siddharthan and Teufel describe a new task to
decide the scientific attribution of an article (Sid-
dharthan and Teufel, 2007) and show high hu-
man agreement as well as an improvement in the
performance of Argumentative Zoning (Teufel,
2005). Argumentative Zoning is a rhetorical clas-
sification task, in which sentences are labeled as
one of Own, Other, Background, Textual, Aim,
Basis, Contrast according to their role in the au-
thor?s argument. These all show the importance
of citation summaries and the vast area for new
work to analyze them to produce a summary for a
given topic.
The Maximal Marginal Relevance (MMR)
summarization method, which is based on a
greedy algorithm, is described in (Carbonell and
Goldstein, 1998). MMR uses the full similarity
matrix to choose the sentences that are the least
similar to the sentences already selected for the
summary. We selected this method as one of our
Fact Occurrences
f1: ? Supervised Learning? 5
f2: ? instance/concept relations? 3
f3: ?Part-of-Speech tagging? 3
f4: ?filtering QA results? 2
f5: ?lexico-semantic information? 2
f6: ?hyponym relations? 2
Table 2: Nuggets of P03-1001 extracted by anno-
tators.
baseline methods, which we have explained in
more details in Section 4.
2 Data
In order to evaluate our method, we use the ACL
Anthology Network (AAN), which is a collec-
tion of papers from the Computational Linguistics
journal and proceedings from ACL conferences
and workshops and includes more than 13, 000 pa-
pers (Radev et al, 2009). We use 25 manually an-
notated papers from (Qazvinian and Radev, 2008),
which are highly cited articles in AAN. Table 1
shows the ACL ID, title, and the number of cita-
tion sentences for these papers.
The annotation guidelines asked a number of
annotators to read the citation summary of each
paper and extract a list of the main contribu-
tions of that paper. Each item on the list is a
non-overlapping contribution (nugget) perceived
by reading the citation summary. The annota-
tion strictly instructed the annotators to focus on
the citing sentences to do the task and not their
own background on the topic. Then, extracted
nuggets are reviewed and those nuggets that have
only been mentioned by 1 annotator are removed.
Finally, the union of the rest is used as a set of
nuggets representing each paper.
Table 2 lists the nuggets extracted by annotators
for P03-1001.
3 Methodology
Our methodology assumes that each citation sen-
tence covers 0 or more nuggets about the cited
papers, and tries to pick sentences that maximize
nugget coverage with respect to summary length.
These nuggets are essentially represented using
keyphrases. Therefore, we try to extract signifi-
cant keyphrases in order to represent nuggets each
sentence contains. Here, the keyphrases are ex-
896
ACL-ID Title # citations
N03-1017 Statistical Phrase-Based Translation 180
P02-1006 Learning Surface Text Patterns For A Question Answering System 74
P05-1012 On-line Large-Margin Training Of Dependency Parsers 71
C96-1058 Three New Probabilistic Models For Dependency Parsing: An Exploration 66
P05-1033 A Hierarchical Phrase-Based Model For Statistical Machine Translation 65
P97-1003 Three Generative, Lexicalized Models For Statistical Parsing 55
P99-1065 A Statistical Parser For Czech 54
J04-4002 The Alignment Template Approach To Statistical Machine Translation 50
D03-1017 Towards Answering Opinion Questions: Separating Facts From Opinions ... 42
P05-1013 Pseudo-Projective Dependency Parsing 40
W00-0403 Centroid-Based Summarization Of Multiple Documents: Sentence Extraction, ... 31
P03-1001 Offline Strategies For Online Question Answering: Answering Questions Before They Are Asked 27
N04-1033 Improvements In Phrase-Based Statistical Machine Translation 24
A00-2024 Cut And Paste Based Text Summarization 20
W00-0603 A Rule-Based Question Answering System For Reading Comprehension Tests 19
A00-1043 Sentence Reduction For Automatic Text Summarization 19
C00-1072 The Automated Acquisition Of Topic Signatures For Text Summarization 19
W05-1203 Measuring The Semantic Similarity Of Texts 17
W03-0510 The Potential And Limitations Of Automatic Sentence Extraction For Summarization 15
W03-0301 An Evaluation Exercise For Word Alignment 14
A00-1023 A Question Answering System Supported By Information Extraction 13
D04-9907 Scaling Web-Based Acquisition Of Entailment Relations 12
P05-1014 The Distributional Inclusion Hypotheses And Lexical Entailment 10
H05-1047 A Semantic Approach To Recognizing Textual Entailment 8
H05-1079 Recognising Textual Entailment With Logical Inference 9
Table 1: List of papers chosen from AAN for evaluation together with the number of sentences citing
each.
unique all max freq
unigrams 229,631 7,746,792 437,308
bigrams 2,256,385 7,746,791 73,957
3-grams 5,125,249 7,746,790 3,600
4-grams 6,713,568 7,746,789 2,408
Table 3: Statistics on the abstract corpus in AAN
used as the background data
pressed using N -grams, and thus these building
units are the key elements to our summarization.
For each citation sentence di, our method first ex-
tracts a set of important keyphrases, Di, and then
tries to find sentences that have a larger number of
important and non-redundant keyphrases. In order
to take the first step, we extract statistically sig-
nificantly frequent N -grams (up to N = 4) from
each citing sentence and use them as the set of
representative keyphrases for that citing sentence.
3.1 Automatic Keyphrase Extraction
A list of keyphrases for each citation sentence can
be generated by extracting N -grams that occur
significantly frequently in that sentence compared
to a large corpus of such N -grams. Our method
for such an extraction is inspired by the previ-
ous work by Tomokiyo and Hurst (Tomokiyo and
Hurst, 2003).
A language model, M, is a statistical model
that assigns probabilities to a sequence of N -
grams. Every language model is a probability dis-
tribution over all N -grams and thus the probabili-
ties of all N -grams of the same length sum up to
1. In order to extract keyphrases from a text us-
ing statistical significance we need two language
models. The first model is referred to as the Back-
ground Model (BM) and is built using a large
text corpus. Here we build the BM using the text
of all the paper abstracts provided in AAN1. The
second language model is called the Foreground
Model (FM) and is the model built on the text
from which keyphrases are being extracted. In
this work, the set of all citation sentences that cite
a particular target paper are used to build a fore-
ground language model.
Let gi be an N -gram of size i and CM(gi) de-
note the count of gi in the modelM. First, we ex-
tract the counts of each N -grams in both the back-
ground (BM) and the foreground corpora (FM).
1http://chernobog.si.umich.edu/clair/anthology/index.cgi
897
MBM =
X
gi?{BM?FM}
1
NBM =
X
gi?{BM?FM}
CBM(gi)
NFM =
X
gi?FM
CFM(gi)
p?FM(gi) = CFM(gi)/NFM
p?BM(gi) = (CBM(gi) + 1)/(MBM +NBM)
The last equation is also known as Laplace
smoothing (Manning and Schutze, 2002) and han-
dles the N -grams in the foreground corpus that
have a 0 occurrence frequency in the background
corpus. Next, we extract N -grams from the fore-
ground corpus that have significant frequencies
compared to the frequency of the same N -grams
in the background model and its individual terms
in the foreground model.
To measure how randomly a set of consecu-
tive terms are forming an N -gram, Tomokiyo and
Hurst (Tomokiyo and Hurst, 2003) use pointwise
divergence. In particular, for an N -gram of size i,
gi = (w1w2 ? ? ?wi),
?gi(FMi?FM1) = p?FM(gi) log(
p?FM(gi)
Qi
j=1 p?FM(wj)
)
This equation shows the extent to which the
terms forming gi have occurred together ran-
domly. In other words, it indicates the extent of in-
formation that we lose by assuming independence
of each word by applying the unigram model, in-
stead of the N -gram model.
In addition, to measure how randomly a se-
quence of words appear in the foreground model
with respect to the background model, we use
pointwise divergence as well. Here, pointwise di-
vergence defines how much information we lose
by assuming that gi is drawn from the background
model instead of the foreground model:
?gi(FMi?BMi) = p?FM(gi) log(
p?FM(gi)
p?BM(gi)
)
(Corley and Mihalcea, 2005) applied or uti-
lized lexical based word overlap measures.
{overlap measures, word overlap, lexical
based, utilized lexical}
Table 4: Example: citation sentence for W05-
1203 written by D06-1621, and its extracted bi-
grams.
We set the criteria of choosing a sequence of
words as significant to be whether it has posi-
tive pointwise divergence with respect to both the
background model, and individual terms of the
foreground model. In other words we extract all gi
from FM for which the both properties are posi-
tive:
?gi(FMi?BMi) > 0
?gi(FMi?FM1) ? 0
The equality condition in the second equation
is specifically set to handle unigrams, in which
p?FM(gi) =
?i
j=1 p?FM(wj).
In order to handle the text corpora and build-
ing the language models, we have used the CMU-
Cambridge Language Model toolkit (Clarkson
and Rosenfeld, 1997). We use the set of cita-
tion sentences for each paper to build foreground
language models. Furthermore, we employ this
tool and make the background model using nearly
11,000 abstracts from AAN. Table 3 summarizes
some of the statistics about the background data.
Once keyphrases (significant N -grams) of each
sentence are extracted, we remove all N -grams in
which more than half of the terms are stopwords.
For instance, we remove all stopword unigrams,
if any, and all bigrams with at least one stop-
word in them. For 3-grams and 4-grams we use
a threshold of 2 and 3 stopwords respectively. Af-
ter that, the set of remaining N -grams is used to
represent each sentence and to build summaries.
Table 4 shows an example of a citation sentence
from D06-1621 citing W05-1203 (Corley and Mi-
halcea, 2005), and its extracted bigrams.
3.2 Sentence Selection
After extracting the set of keyphrases for each sen-
tence, di, the sentence is represented using its set
898
of N -grams, denoted by Di. Then, the goal is
to pick sentences (sets) for each paper that cover
more important and non-redundant keyphrases.
Essentially, keyphrases that have been repeated in
more sentences are more important and could rep-
resent more important nuggets. Therefore, sen-
tences that contain more frequent keyphrases are
more important. Based on this intuition we define
the reward of building a summary comprising a
set of keyphrases S as
f(S) = |S ?A|
where A is the set of all keyphrases from sen-
tences not in the summary.
The set function f has three main properties.
First, it is non-negative. Second, it is mono-
tone (i.e., For every set v we have f(S + v) ?
f(S)). Third, f is sub-modular. The submodular-
ity means that for a set v and two sets S ? T we
have
f(S + v)? f(S) ? f(T + v)? f(T )
Intuitively, this property implies that adding a set
v to S will increase the reward at least as much
as it would to a larger set T . In the summariza-
tion setting, this means that adding a sentence to
a smaller summary will increase the reward of the
summary at least as much as adding it to a larger
summary that subsumes it. The following theorem
formalizes this and is followed by a proof.
Theorem 1 The reward function f is submodular.
Proof
We start by defining a gain function G of adding
sentence (set) Di to Sk?1 where Sk?1 is the set
of keyphrases in a summary built using k? 1 sen-
tences, and Di is a candidate sentence to be added:
G(Di,Sk?1) = f(Sk?1 ?Di)? f(Sk?1)
Simple investigation through a Venn diagram
proof shows that G can be re-written as
G(Di,Sk?1) = |Di ? (?j 6=iDj)? Sk?1|
Let?s denote Di? (?j 6=iDj) by ?i. The follow-
ing equations prove the theorem.
Sk?1 ? Sk
S ?k?1 ? S ?k
?i ? S ?k?1 ? ?i ? S ?k
?i ? Sk?1 ? ?i ? Sk
| ?i ?Sk?1| ? | ?i ?Sk|
G(Di,Sk?1) ? G(Di,Sk)
f(Sk?1 ?Di)? f(Sk?1) ? f(Sk ?Di)? f(Sk)
Here, S ?k is the set of all N -grams in the vo-
cabulary that are not present in Sk. The gain of
adding a sentence, Di, to an empty summary is a
non-negative value.
G(Di,S0) = C ? 0
By induction, we will get
G(Di,S0) ? G(Di,S1) ? ? ? ? ? G(Di,Sk) ? 0
2
Theorem 1 implies the general case of submodu-
larity:
?m,n, 0 ? m ? n ? |D| ? G(Di,Sm) ? G(Di,Sn)
Maximizing this submodular function is an NP-
hard problem (Khuller et al, 1999). A common
way to solve this maximization problem is to start
with an empty set, and in each iteration pick a set
that maximizes the gain. It has been shown be-
fore in (Kulik et al, 2009) that if f is a submod-
ular, nondecreasing set function and f(?) = 0,
then such a greedy algorithm finds a set S , whose
gain is at least as high as (1 ? 1/e) of the best
possible solution. Therefore, we can optimize the
keyphrase coverage as described in Algorithm 1.
4 Experimental Setup
We use the annotated data described in Section 2.
In summary, the annotation consisted of two parts:
nugget extraction and nugget distribution analy-
sis. Five annotators were employed to annotate
the sentences in each of the 25 citation summaries
and write down the nuggets (non-overlapping con-
tributions) of the target paper. Then using these
899
Summary generated using bigram-based keyphrases
ID Sentence
P06-1048:1 Ziff-Davis Corpus Most previous work (Jing 2000; Knight and Marcu 2002; Riezler et al2003; Nguyen et al2004a; Turner and Charniak 2005;
McDonald 2006) has relied on automatically constructed parallel corpora for training and evaluation purposes.
J05-4004:18 Between these two extremes, there has been a relatively modest amount of work in sentence simplification (Chandrasekar, Doran, and Bangalore
1996; Mahesh 1997; Carroll et al1998; Grefenstette 1998; Jing 2000; Knight and Marcu 2002) and document compression (Daume III and Marcu
2002; Daume III and Marcu 2004; Zajic, Dorr, and Schwartz 2004) in which words, phrases, and sentences are selected in an extraction process.
A00-2024:9 The evaluation of sentence reduction (see (Jing, 2000) for details) used a corpus of 500 sentences and their reduced forms in human-written abstracts.
N03-1026:17 To overcome this problem, linguistic parsing and generation systems are used in the sentence condensation approaches of Knight and Marcu (2000)
and Jing (2000).
P06-2019:5 Jing (2000) was perhaps the first to tackle the sentence compression problem.
Table 5: Bigram-based summary generated for A00-1043.
Algorithm 1 The greedy algorithm for summary
generation
k ? the number of sentences in the summary
Di ? keyphrases in di
S ? ?
for l = 1 to k do
sl ? argmaxDi?D |Di ? (?j 6=iDj)|
S ? S ? sl
for j = 1 to |D| do
Dj ? Dj ? sl
end for
end for
return S
nugget sets, each sentence was annotated with the
nuggets it contains. This results in a sentence-
fact matrix that helps with the evaluation of the
summary. The summarization goal and the intu-
ition behind the summarizing system is to select a
few (5 in our experiments) sentences and cover as
many nuggets as possible. Each sentence in a cita-
tion summary may contain 0 or more nuggets and
not all nuggets are mentioned an equal number of
times. Covering some nuggets (contributions) is
therefore more important than others and should
be weighted highly.
To capture this property, the pyramid score
seems the best evaluation metric to use. We use
the pyramid evaluation method (Nenkova and Pas-
sonneau, 2004) at the sentence level to evaluate
the summary created for each set. We benefit
from the list of annotated nuggets provided by the
annotators as the ground truth of the summariza-
tion evaluation. These annotations give the list of
nuggets covered by each sentence in each citation
summary, which are equivalent to the summariza-
tion content unit (SCU) as described in (Nenkova
and Passonneau, 2004).
The pyramid score for a summary is calculated
as follows. Assume a pyramid that has n tiers, Ti,
where tier Ti > Tj if i > j (i.e., Ti is not below
Tj , and that if a nugget appears in more sentences,
it falls in a higher tier.). Tier Ti contains nuggets
that appeared in i sentences, and thus has weight
i. Suppose |Ti| shows the number of nuggets in
tier Ti, and Qi is the size of a subset of Ti whose
members appear in the summary. Further suppose
Q shows the sum of the weights of the facts that
are covered by the summary. Q =
?n
i=1 i?Qi.
In addition, the optimal pyramid score for a sum-
mary with X facts, is
Max =
n
X
i=j+1
i? |Ti|+ j ? (X ?
n
X
i=j+1
|Ti|)
where j = maxi(
?n
t=i |Tt| ? X). The pyra-
mid score for a summary is then calculated as fol-
lows.
P = QMax
This score ranges from 0 to 1, and a high
score shows the summary contains more heavily
weighted facts.
4.1 Baselines and Gold Standards
To evaluate the quality of the summaries gen-
erated by the greedy algorithm, we compare its
pyramid score in each of the 25 citation sum-
maries with those of a gold standard, a random
summary, and four other methods. The gold stan-
dards are summaries created manually using 5
sentences. The 5 sentences are manually selected
in a way to cover as many nuggets as possible with
higher priority for the nuggets with higher fre-
quencies. We also created random summaries us-
ing Mead (Radev et al, 2004). These summaries
900
are basically a random selection of 5 sentences
from the pool of sentences in the citation sum-
mary. Generally we expect the summaries cre-
ated by the greedy method to be significantly bet-
ter than random ones.
In addition to the gold and random summaries,
we also used 4 baseline state of the art sum-
marizers: LexRank, the clustering C-RR and
C-LexRank, and Maximal Marginal Relevance
(MMR). LexRank (Erkan and Radev, 2004) works
based on a random walk on the cosine similar-
ity of sentences and prints out the most frequently
visited sentences. Said differently, LexRank first
builds a network in which nodes are sentences and
edges are cosine similarity values. It then uses the
eigenvalue centralities to find the most central sen-
tences. For each set, the top 5 sentences on the list
are chosen for the summary.
The clustering methods, C-RR and C-LexRank,
work by clustering the cosine similarity network
of sentences. In such a network, nodes are sen-
tences and edges are cosine similarity of node
pairs. Clustering would intuitively put nodes with
similar nuggets in the same clusters as they are
more similar to each other. The C-RR method as
described in (Qazvinian and Radev, 2008) uses a
round-robin fashion to pick sentences from each
cluster, assuming that the clustering will put the
sentences with similar facts into the same clus-
ters. Unlike C-RR, C-LexRank uses LexRank to
find the most salient sentences in each cluster, and
prints out the most central nodes of each cluster as
summary sentences.
Finally, MMR uses the full cosine similarity
matrix and greedily chooses sentences that are the
least similar to those already selected for the sum-
mary (Carbonell and Goldstein, 1998). In partic-
ular,
MMR = arg min
di?D?A
[
max
dj?A
Sim(di, dj)
]
where A is the set of sentences in the summary,
initially set to A = ?. This method is different
from ours in that it chooses the least similar sen-
tence to the summary in each iteration.
4.2 Results and Discussion
As mentioned before, we use the text of the ab-
stracts of all the papers in AAN as the back-
ground, and each citation set as a separate fore-
ground corpus. For each citation set, we use the
method described in Section 3.1 to extract signif-
icant N -grams of each sentence. We then use the
keyphrase set representation of each sentence to
build the summaries using Algorithm 1. For each
of the 25 citation summaries, we build 4 differ-
ent summaries using unigrams, bigrams, 3-grams,
and 4-grams respectively. Table 5 shows a 5-
sentence summary created using algorithm 1 for
the paper A00-1043 (Jing, 2000).
The pyramid scores for different methods are
reported in Figure 1 together with the scores
of gold standards, manually created to cover as
many nuggets as possible in 5 sentences, as
well as summary evaluations of the 4 baseline
methods described above. This Figure shows
how the keyphrase based summarization method
when employing N -grams of size 3 or smaller,
outperforms other baseline systems significantly.
More importantly, Figure 1 also indicates that this
method shows more stable results and low varia-
tion in summary quality when keyphrases of size 3
or smaller are employed. In contrast, MMR shows
high variation in summary qualities making sum-
maries that obtain pyramid scores as low as 0.15.
Another important advantage of this method is
that we do not need to calculate the cosine simi-
larity of the pairs of sentences, which would add a
running time of O(|D|2|V |) in the number of doc-
uments, |D|, and the size of the vocabulary |V | to
the algorithm.
5 Conclusion and Future Work
This paper presents a summarization methodol-
ogy that employs keyphrase extraction to find im-
portant contributions of scientific articles. The
summarization is based on citation sentences and
picks sentences to cover nuggets (represented by
keyphrases) or contributions of the target papers.
In this setting the best summary would have as few
sentences and at the same time as many nuggets
as possible. In this work, we use pointwise KL-
divergence to extract statistically significant N -
grams and use them to represent nuggets. We
then apply a new set function for the task of sum-
marizing scientific articles. We have proved that
this function is submodular and concluded that a
901
00.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Gold Mead LexRank C?RR C?LexRank MMR 1?gram 2?gram 3?gram 4?gram
Py
ra
m
id
 S
co
re
Figure 1: Evaluation Results (summaries with 5 sentences): The median pyramid score over 25 datasets
using different methods.
greedy algorithm will result in a near-optimum set
of covered nuggets using only 5 sentences. Our
experiments in this paper confirm that the sum-
maries created based on the presented algorithm
are better than randomly generated summary, and
also outperform other state of the art summariza-
tion methods in most cases. Moreover, we show
how this method generates more stable summaries
with lower variation in summary quality when N -
grams of size 3 or smaller are employed.
A future direction for this work is to perform
post-processing on the summaries and re-generate
sentences that cover the extracted nuggets. How-
ever, the ultimate goal is to eventually develop
systems that can produce summaries of entire
research areas, summaries that will enable re-
searchers to easily and quickly switch between
fields of research.
One future study that will help us generate
better summaries is to understand how nuggets
are generated by authors. In fact, modeling the
nugget coverage behavior of paper authors will
help us identify more important nuggets and dis-
cover some aspects of the paper that would oth-
erwise be too difficult by just reading the paper
itself.
6 Acknowledgements
This work is in part supported by the National
Science Foundation grant ?iOPENER: A Flexi-
ble Framework to Support Rapid Learning in Un-
familiar Research Domains?, jointly awarded to
University of Michigan and University of Mary-
land as IIS 0705832, and in part by the NIH Grant
U54 DA021519 to the National Center for Inte-
grative Biomedical Informatics.
Any opinions, findings, and conclusions or rec-
ommendations expressed in this paper are those
of the authors and do not necessarily reflect the
views of the supporters.
References
Bradshaw, Shannon. 2002. Reference Directed Index-
ing: Indexing Scientific Literature in the Context of
Its Use. Ph.D. thesis, Northwestern University.
Bradshaw, Shannon. 2003. Reference directed index-
ing: Redeeming relevance for subject search in ci-
tation indexes. In Proceedings of the 7th European
902
Conference on Research and Advanced Technology
for Digital Libraries.
Carbonell, Jaime G. and Jade Goldstein. 1998. The
use of MMR, diversity-based reranking for reorder-
ing documents and producing summaries. In SI-
GIR?98, pages 335?336.
Clarkson, PR and R Rosenfeld. 1997. Statistical lan-
guage modeling using the cmu-cambridge toolkit.
Proceedings ESCA Eurospeech, 47:45?148.
Elkiss, Aaron, Siwei Shen, Anthony Fader, Gu?nes?
Erkan, David States, and Dragomir R. Radev. 2008.
Blind men and elephants: What do citation sum-
maries tell us about a research article? Journal of
the American Society for Information Science and
Technology, 59(1):51?62.
Erkan, Gu?nes? and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research.
Jing, Hongyan. 2000. Sentence reduction for auto-
matic text summarization. In Proceedings of the
sixth conference on Applied natural language pro-
cessing, pages 310?315, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Kan, Min-Yen, Judith L. Klavans, and Kathleen R.
McKeown. 2002. Using the Annotated Bibliogra-
phy as a Resource for Indicative Summarization. In
Proceedings of LREC 2002, Las Palmas, Spain.
Khuller, Samir, Anna Moss, and Joseph (Seffi) Naor.
1999. The budgeted maximum coverage problem.
Inf. Process. Lett., 70(1):39?45.
Kulik, Ariel, Hadas Shachnai, and Tami Tamir. 2009.
Maximizing submodular set functions subject to
multiple linear constraints. In SODA ?09, pages
545?554.
Kupiec, Julian, Jan Pedersen, and Francine Chen.
1995. A trainable document summarizer. In SIGIR
?95, pages 68?73, New York, NY, USA. ACM.
Manning, Christopher D. and Hirich Schutze. 2002.
Foundations of Statistical Natural Language Pro-
cessing. The MIT Press, Cambridge, Mas-
sachusetts, London, England.
Mei, Qiaozhu and ChengXiang Zhai. 2008. Generat-
ing impact-based summaries for scientific literature.
In Proceedings of ACL ?08, pages 816?824.
Mohammad, Saif, Bonnie Dorr, Melissa Egan, Ahmed
Hassan, Pradeep Muthukrishan, Vahed Qazvinian,
Dragomir Radev, and David Zajic. 2009. Using
citations to generate surveys of scientific paradigms.
In NAACL 2009, pages 584?592, June.
Nanba, Hidetsugu and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference
information. In IJCAI1999, pages 926?931.
Nanba, Hidetsugu, Noriko Kando, and Manabu Oku-
mura. 2004. Classification of research papers us-
ing citation links and citation types: Towards au-
tomatic review article generation. In Proceedings
of the 11th SIG Classification Research Workshop,
pages 117?134, Chicago, USA.
Nenkova, Ani and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. Proceedings of the HLT-NAACL con-
ference.
Qazvinian, Vahed and Dragomir R. Radev. 2008. Sci-
entific paper summarization using citation summary
networks. In COLING 2008, Manchester, UK.
Radev, Dragomir, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda C?elebi, Stanko
Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam,
Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio
Saggion, Simone Teufel, Michael Topper, Adam
Winkel, and Zhu Zhang. 2004. MEAD - a platform
for multidocument multilingual text summarization.
In LREC 2004, Lisbon, Portugal, May.
Radev, Dragomir R., Pradeep Muthukrishnan, and Va-
hed Qazvinian. 2009. The ACL anthology network
corpus. In ACL workshop on Natural Language
Processing and Information Retrieval for Digital Li-
braries.
Siddharthan, Advaith and Simone Teufel. 2007.
Whose idea was this, and why does it matter? at-
tributing scientific work to citations. In Proceedings
of NAACL/HLT-07.
Teufel, Simone. 2005. Argumentative Zoning for Im-
proved Citation Indexing. Computing Attitude and
Affect in Text: Theory and Applications, pages 159?
170.
Tomokiyo, Takashi and Matthew Hurst. 2003. A lan-
guage model approach to keyphrase extraction. In
Proceedings of the ACL 2003 workshop on Multi-
word expressions, pages 33?40.
903
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 554?561, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
BOUNCE: Sentiment Classification in Twitter using Rich Feature Sets
Nadin Ko?kciyan?, Arda C?elebi?, Arzucan O?zgu?r, Suzan U?sku?darl?
Department of Computer Engineering
Bogazici University
Istanbul, Turkey
{nadin.kokciyan,arda.celebi,arzucan.ozgur,suzan.uskudarli}@boun.edu.tr
Abstract
The widespread use of Twitter makes it very
interesting to determine the opinions and the
sentiments expressed by its users. The short-
ness of the length and the highly informal na-
ture of tweets render it very difficult to auto-
matically detect such information. This paper
reports the results to a challenge, set forth by
SemEval-2013 Task 2, to determine the posi-
tive, neutral, or negative sentiments of tweets.
Two systems are explained: System A for de-
termining the sentiment of a phrase within a
tweet and System B for determining the senti-
ment of a tweet. Both approaches rely on rich
feature sets, which are explained in detail.
1 Introduction
Twitter consists of a massive number of posts on a
wide range of subjects, making it very interesting to
extract information and sentiments from them. For
example, answering questions like ?What do Twitter
users feel about the brand X?? are quite interesting.
The constrained length and highly informal nature
of tweets presents a serious challenge for the auto-
mated extraction of such sentiments.
Twitter supports special tokens (i.e. mentions and
hashtags), which have been utilized to determine the
sentiment of tweets. In (Go et al, 2009), emoticons
are used to label tweets. In (Davidov et al, 2010),
Twitter emoticons as well as hashtags are used to la-
bel tweets. O?Connor et al (2010) demonstrated
a correlation between sentiments identified in pub-
lic opinion polls and those in tweets. A subjectivity
? These authors contributed equally to this work
lexicon was used to identify the positive and nega-
tive words in a tweet. In (Barbosa and Feng, 2010),
subjective tweets are used for sentiment classifica-
tion. They propose the use of word specific (e.g.
POS tags) and tweet specific (e.g. presence of a link)
features. Most of these studies use their own anno-
tated data sets for evaluation, which makes it diffi-
cult to compare the performances of their proposed
approaches.
Sentiment Analysis in Twitter 2013 (SemEval
2013 Task 2) (Wilson et al, 2013) presented a chal-
lenge for exploring different approaches examin-
ing sentiments conveyed in tweets: interval-level
(phrase-level) sentiment classification (TaskA) and
message-level sentiment classification (TaskB). Sen-
timent are considered as positive, negative, or neu-
tral. For TaskA, the goal is to determine the sen-
timent of an interval (consecutive word sequence)
within a tweet. For TaskB, the goal is to determine
sentiment of an entire tweet. For example, let?s con-
sider a tweet like ?Can?t wait until the DLC for ME3
comes out tomorrow. :-)?. For TaskA, the interval
0-1 (Can?t wait) is ?positive? and the interval 10-10
(:-)) is ?positive?. For TaskB, this tweet is ?positive?.
In this paper, we present two systems, one for
TaskA and one for TaskB. In both cases machine
learning methods were utilized with rich feature sets
based on the characteristics of tweets. Our results
suggest that our approach is promising for sentiment
classification in Twitter.
2 Approach
The task of detecting the sentiments of a tweet or
an interval therein, is treated as a classification of
554
  TaskA        
 Tweets
   with
Intervals
Positive
Classifier
Tweet
Classifier
TaskA
Interval Classifier
TaskB
Multiple Binary Classifier
  TaskB
 Tweets
+/-/0
Classified 
Tweets
Classified 
Tweet Intervals
+/-/0
Preprocessor
+
Feature
Generator
Negative
Classifier
Lexicons
Figure 1: The Overview of BOUNCE System
tweets into positive, negative, or neutral sets. Fig-
ure 1 gives the overview of our approach. The Pre-
processor module tokenizes the tweets that are used
by the Feature Generator. At this stage, the tweets
are represented as feature vectors. For TaskA, the
feature vectors are used by the Interval Classifier
that predicts the labels of the tweet intervals. For
TaskB, the feature vectors are used by the Positive
Classifier and the Negative Classifier which report
on the positivity and negativity of the tweets. The
Tweet Classifier determines the tweet labels using a
rule-based method. Each step is described in detail
in the following subsections.
2.1 Lexicons
The core of our approach to sentiment analysis relies
on word lists that are used to determine the positive
and negative words or phrases. Several acquired lists
are used in addition to one that we curated. AFINN
(Nielsen, 2011) is the main sentiment word list in-
cluding 2477 words rated between -5 to 5 for va-
lence. SentiWordNet (Baccianella et al, 2010), de-
rived from the Princeton English WordNet (Miller,
1995), assigns positive, negative, or objective scores
to each synset in WordNet. We considered the av-
erage of a word?s synsets as its SentiWordNet score.
Thus, synsets are disregarded and no disambiguation
of the sense of a word in a given context is done.
The SentiWordNet score of a word is not used if it
has objective synsets, since it indicates that the word
might have been used in an objective sense. We use
a list of emotion words and categories that is created
by DeRose1. Furthermore, a slang dictionary down-
1http://derose.net/steve/resources/emotionwords/ewords.html
loaded from the Urban Dictionary2 containing over
16,000 phrases (with no sentiment) is used. Finally,
we curated a sentiment word list initiated with a list
of positive and negative words obtained from Gen-
eral Inquirer (Stone et al, 1966), and refined by sen-
timent emitting words from a frequency-based or-
dered word list generated from the training data set
of SemEval-2013 Task A. Naturally, this list is more
specialized to the Twitter domain.
2.2 Preprocessing
Prior to feature generation, tweets were prepro-
cessed to yield text with more common wording.
For this, CMU?s Ark Tokenizer and Part-of-Speech
(POS) Tagger (Gimpel et al, 2011), which has been
specifically trained for tweets, was used. Tweets are
tokenized and POS tagged.
2.3 Feature Sets
In addition to the lexical or syntactic characteristics,
the manner in which tweets are written may reveal
sentiment. Orthogonal shapes of words (esp. fully
or partially capitalized words), expressions of a sin-
gle word or a phrase in the form of a hashtag, posi-
tions of certain tokens in a tweet are prominent char-
acteristics of tweets. In addition to these, tweets may
convey multiple sentiments. This leads to sequence-
based features, where we append features for each
sentiment emitted by a word or a phrase in a tweet.
Moreover, since TaskA asks for sentiment of inter-
vals in a tweet, we also engineer features to catch
clues from the surrounding context of the interval,
2http://www.urbandictionary.com
555
such as the sentiments and lengths of the neighbor-
ing intervals. For TaskB, the usage of hashtags and
last words in tweets were occasionally sentimental,
thus we considered them as features as well. We ex-
plain all features in detail in Section 3.
2.4 Classification
Maximum entropy models (Berger et al, 1996) have
been used in sentiment analysis (Fei et al, 2010).
They model all given data and treat the remainder as
uniform as possible making no assumptions about
what is not provided. For this, TaskA system uses
the MaxEnt tool (Zhang, 2011).
Naive Bayes is a simple probabilistic model based
on Bayes? Theorem that assumes independence be-
tween features. It has performed well in sentiment
classification of Twitter data (Go et al, 2009; Bifet
and Frank, 2010). TaskB data was not evenly dis-
tributed. There were very few negative tweets com-
pared to positive tweets. Using a single classifier
to distinguish the classes from each other resulted
in poor performance in identifying negative tweets.
Therefore, TaskB system utilizes multiple binary
classifiers that use the one-vs-all strategy. Maximum
Entropy and Naive Bayes models were considered
and the model that performed best on the develop-
ment set was chosen for each classifier. As a result,
the positive classifier (Bpos) is based on the Max-
imum Entropy model, whereas the negative classi-
fier (Bneg) is based on Naive Bayes. TaskB system
uses the Natural Language Toolkit (Loper and Bird,
2002).
3 Systems
In this section, TaskA and TaskB systems are ex-
plained in detail. All features used in the final ex-
periments for both tasks are shown in Table 1.
3.1 TaskA System
TaskA is a classification task where we classify a
given interval as having positive, negative or neutral
sentiment. TaskA feature sets are shown in Table 1.
lexical features: These features use directly
words (or tokens) from tweets as features. single-
word feature uses the word of the single-word inter-
vals, whereas slang features are created for match-
ing uni-grams and bi-grams from our slang dictio-
nary. We also use emoticons as features, as well as
the words or phrases that emit emotion according to
the lexicons described in Section 2.1.
score-based features: These features use the
scores obtained from the AFINN and SentiWordNet
(SWN) lexicons. We use separate scores for the pos-
itive and negative sentiments, since one interval may
contain multiple words with opposite sentiment. In
case of multiple positive or negative occurances, we
take the arithmetic mean of those.
shape-based features: These features capture the
length of an interval, whether it contains a capital-
ized word or all words are capitalized, whether it
contains a URL, or ends with an exclamation mark.
tag-based features: In addition to numeric val-
ues of sentiments, we use the tokens ?positive? and
?negative? to express the type of sentiment. When
multiple words emit a sentiment in a given interval,
their corresponding tokens are appended to create a
single feature out of it, sequences. Moreover, we
have another set of features which also contains the
POS tags of these sentiment words.
indicator features: These features are used in or-
der to expose how many sentiment emitting words
from our currated large lexicon exist in a given inter-
val. hasNegation indicates the presence of a nega-
tion word like not or can?t in the interval, whereas
numOfPosIndicators and numOfNegIndicators gives
the number of tokens that convey positive and nega-
tive sentiment, respectively.
context features: In addition to the features gen-
erated from the given interval, these features capture
the context information from the neighboring inter-
vals. Feature surroundings combines the length of
the interval along with the lengths of the intervals on
both sides, whereas surrounding-shape and extra-
surrounding-shape features use number of positive
and negative sentiment indicators for the intervals.
We also use their normalized forms (those starting
with norm-) where we divide the number of indi-
cators by the length of the interval. Features with
-extra- use two adjacent intervals from both sides.
Intervals that are not available are represented with
NA.
3.2 TaskB System
TaskB is a classification task where we determine
the sentiment (positive, negative, or neutral) of a
tweet. TaskB system uses a rule-based method to
556
Feature Set Feature Example Feature Instance used by
lexical-based
single-word-* single-word-worst A, B
slang-* slang-shit A, Bpos
emoticons-* emoticons-:) A
emitted-emotions-* emitted-emotions-angry A, B
score-based
afinn-positive:#, afinn-negative:# afinn-positive:4, afinn-negative:-2 A, B
swn-positive:#, swn-negative:# swn-positive:2, swn-negative:-3 A
shape-based
length-# length-10 A
hasAllCap-T/F hasAllCap-T A
fullCap-T/F fullCap-T A
hasURL-T/F hasURL-F A, B
endsWExlamation-T/F endsWExlamation-T A, Bneg
tag-based
our-seq-* our-seq-positive-positive A, B
our-tag-seq-*, swn-seq-*, swn-tag-seq-* afinn-seq-positive-a-positive-n A
afinn-seq-*, afinn-tag-seq-* afinn-seq-positive-a-negative-n A
indicators
hasNegation-T/F hasNegation-F A
numOfPosIndicators-# numOfPosIndicators-2 A
numOfNegIndicators-# numOfNegIndicators-0 A
context
surroundings-#-#-# surroundings-1-2-NA A
surr-shape-#-#-# surrounding-shape-NA-2-1 A
extra-surr-shape-#-#-#-#-# extra-surr-shape-NA-2-1-0-1 A
norm-surr-shape-#-#-# norm-surr-shape-0.5-0.2-0.0 A
norm-extra-surr-shape-#-#-#-#-# norm-extra-surr-shape-NA-0.5-0.2-0.0-0.2 A
left-sentiment-*, right-sentiment-* left-sentiment-positive A
twitter-tags
hasEmoticon-T/F hasEmoticon-T B
hasMention-T/F hasMention-T B
hasHashtag-T/F hasHashtag-F B
[emoticon|mention|hash]-count-# mention-count-3 B
repetition
unigram-*n unigram-[no+] B
$character-count-# o-count-7 B
lastword
lastword-*n lastword-[OMG+] B
lastwordshape-* lastwordshape-XXXX B
chat chatword-* for word ?gz?: chatword-congratulations B
interjection interjection-*n interjection-[lo+l] B
negation
negword-*n negword-never Bneg
negword-count-# negword-count-3 Bneg
negcapword-count-# negcapword-count-1 Bneg
hash
hashword-* hashword-good B
hashtag-#* hashtag-#good B
hash-sentiment-[positive|negative] hash-sentiment-positive B
lingemotion [noun|verb|adverb|adjective]-$emotion noun-fear B
oursent
for tweet: a nice morning.. I hate work.. damn!
oursent-* oursent-nice, oursent-hate, oursent-damn B
oursent-longseq-* oursent-longseq-pnn B
oursent-shortseq-* oursent-shortseq-pn B
oursent-first-last-* oursent-first-last-pn B
afinn-phrases
phrase-firstsense-[positive|negative] phrase-firstsense-positive B
phrase-lastsense-[positive|negative] phrase-lastsense-negative B
afinnword-* afinnword-nice, afinnword-hate, afinnword-damn B
afinn-firstsense-[positive|negative] afinn-firstsense-positive B
afinn-lastsense-[positive|negative] afinn-lastsense-positive B
emo emo-pattern-* for =) : emo-pattern-HAPPY B
Table 1: Feature sets used in TaskA and TaskB
557
Dataset Type Positive Negative Neutral+Objective Tot. No. of Instances
TaskA
Training 5290 (5865) 2771(3120) 16118 (17943) 24179 (26928)
Development 589 (648) 392 (430) 1993 (2202) 2974 (3280)
Test 2734 1541 160 4435
TaskB
Training 3274 (3640) 1291 (1458) 4155 (4586) 8720 (9684)
Development 523 (575) 309 (340) 674 (739) 1506 (1654)
Test 1572 601 1640 3813
Table 2: Number of instances used in TaskA and TaskB
decide on the sentiment label of a tweet. For each
tweet, the probabilities of belonging to the posi-
tive class (Probpos) and negative class (Probneg)
are computed by the Bpos and Bneg classifiers, re-
spectively. If Probpos is greater than Probneg, and
greater than a predefined threshold, then the tweet
is classified as ?positive?, otherwise it is classified
as ?neutral?. On the other hand, if Probneg is
greater than Probpos, and greater than the prede-
fined threshold, then the tweet is classified as ?neg-
ative?, otherwise it is classified as ?neutral?. The
threshold is set to 0.45, since it gives the optimal F-
score on the development set. TaskB features along
with examples are shown in Table 1.
twitter-tags: hasEmoticon, hasMention, ha-
sURL, and hasHashtag indicate whether the corre-
sponding term (e.g. mention) exists in the tweet.
repetition: Words with repeating letters are
added as a feature ?n. ?n represents the normalized
version (i.e., no repeating letters) of a word. For ex-
ample, ?nooooooo? is shortened to [no+]. We also
keep the count of the repeated character.
wordshape: Shape of each word in a tweet is con-
sidered. For example, the shape of ?NOoOo!!? is
?XXxXx!!?.
lastword: The normalized form and the shape of
the last word are used as features. For example, if
the lastword is ?OMGG?, then lastword ?[OMG+]?
and lastwordshape ?XXXX? are used as features.
chat: A list of chat abbreviations that express sen-
timent is manually created. Each abbreviation is re-
placed by its corresponding word.
interjection: An interjection is a word that ex-
presses an emotion or sentiment (e.g. hurraah,
loool). Interjection wordn is used as a feature.
negation: We manually created a negation list ex-
tended by word clusters from (Owoputi et al, 2013).
A negation word is represented by spellings such
as not, n0t, and naht. Each negation wordn (e.g
neve[r+]) is considered. We keep the count of nega-
tion words and all capitalized negation words.
hash: If the hashtag is ?#good? then #good and
good become hash features. If the hashtag is a sen-
timent expressing word according to our sentiment
word list, then we keep the sentiment information.
lingemotion: Nodebox Linguistics3 package
gives emotional values of words for expressions of
emotions such as fear and sadness. POS augmented
expression information is used as a feature.
oursent: Each word in a tweet that exists in our
sentiment word list is considered. When multiple
sentiment expressing words are found, a sentiment
sequence feature is used. oursent-longseq keeps
the long sequence, whereas oursent-shortseq keeps
same sequence without repetitive sentiments. We
also consider the first and last sentiments emitted by
a tweet.
afinn: We consider each word that exists in
AFINN. If a negation exists before this word, the
opposite sentiment is considered. For example, if a
tweet contains the bigram ?not good?, then the senti-
ment of the bigram is set to ?negative?. The AFINN
scores of the positive and negative words, as well as
the first and last sentiments emitted by the tweet are
considered.
phrases: Each n-gram (n > 1) of a tweet that
exists in our sentiment phrase list is considered.
afinn-phrases: Phrases are retrieved using the
phrases feature. Each sentiment that appears in
a phrase is kept, hence we obtain a sentiment se-
quence. The first and last sentiments of this se-
quence are also considered. Then, the phrases are
removed from the tweet text and the afinn feature is
applied.
emo: We manually created an emoticon list where
3http://nodebox.net/code/index.php/Linguistics
558
each term is associated with an emotion pattern such
as HAPPY. These emotion patterns are used as a fea-
ture.
others: Bpos uses the slang feature from the lexi-
cal feature set, and Bneg uses endsWExlamation fea-
ture from the indicators feature set.
4 Experiments and Results
4.1 Data
The data set provided by the task organizers was an-
notated by using Amazon Mechanical Turk4. The
annotations of the tweets in the training and devel-
opment sets were provided to the task participants.
However, the tweets had to be downloaded from
Twitter by using the script made available by the or-
ganizers. We were unable to download all the tweets
in the training and development sets, since some
tweets were deleted and others were not publicly
accessible due to their updated authorization status.
The number of actual tweets (numbers in parenthe-
ses) and the number of collected tweets are shown in
Table 2. Almost 10% of the data for both tasks are
missing. For the test data, however, the tweets were
directly provided to the participants.
4.2 Results on TaskA
We start our experiments with features generated
from lexicons and emoticons. Called our baseline,
it achieved an f-score of 47.8 on the devset in Ta-
ble 3. As we add other features at each step, we
reach an average f-score of 81.6 on the devset at
the end. Among those features, the most contribut-
ing ones are lexical feature single-word, indicator
feature hasNegation, and especially shape feature
length. The success of the length feature is mostly
due to the nature of intervals, where the long ones
tend to be neutral, and the rest are mostly positive
or negative. Another noteworthy result is that our
curated word list contributed more compared to the
others. When the final model is used on the test set,
we get the results in Table 5. Having low neutral f-
score might be due to the fact that there were only a
few neutral intervals in the test set, which might in-
dicate that their characteristics may not be the same
as the ones in the devset.
4https://www.mturk.com/mturk/
Added Features Avg. F-Score
afinn-positive, afinn-negetive
47.8swn-positive, swn-negative,
emoticons, emitted-emotions
+ hasAllCap, fullCap, hasURL,
50.1
endsWExclamation
+ slang 51.5
+ single-word 56.8
+ afinn-seq, swn-seq, afinn-tag-seq,
57.7
swn-tag-seq
+ our-seq, our-tag-seq 60.2
+ hasNegation 64.8
+ numOfPosIndicators,
65.3
numOfNegIndicators
+ length 75.2
+ left-sentiment, right-sentiment 76.5
+ surroundings, surrounding-shape 78.9
+ extra-surrounding-shape 80.6
+ norm-surrounding-shape,
81.6
norm-extra-surrounding-shape
Table 3: Macro-averaged F-Score on the TaskA dev. set
Added Features
Average
F-Score
oursent (baseline) 58.59
+ afinn-phrases 64.64
+ tags + hash 65.43
+ interjection + chat 65.53
+ emo + lingemotion 65.92
+ repetition + lastword 66.01
+ negation + others 66.32
Table 4: Macro-averaged F-Score on the TaskB dev. set
4.3 Results on TaskB
The baseline model is considered to include oursent
feature that gives an average f-score of 58.59. Next,
we added the afinn-phrases feature which increased
the average f-score to 64.64. This increase can be
explained by the sentiment scores and sequence pat-
terns that afinn-phrases is based on. Following that
model, the other added features slightly increased
the average f-score to 66.32 as shown in Table 4.
The final model is used over the test set of TaskB,
where we obtained an f-score of 63.53 as shown in
Table 5.
559
Class Precision Recall F-Score
TestA
positive 89.7 88.3 89.0
negative 86.6 82.7 84.6
neutral 10.7 18.1 13.4
average(pos+neg) 88.15 85.5 86.8
TestB
positive 82.3 55.6 66.4
negative 48.7 80.2 60.6
neutral 68.2 73.3 70.7
average(pos+neg) 65.56 67.93 63.53
Table 5: Results on the test sets for both tasks
5 Conclusion
We presented two systems one for TaskA (a Maxi-
mum Entropy model) and one for TaskB (Maximum
Entropy + Naive Bayes models) based on using rich
feature sets. For Task A, we started with a baseline
system that just uses ordinary features like sentiment
scores of words. As we added new features, we ob-
served that lexical features and shape-based features
are the ones that contribute most to the performance
of the system. Including the context features and the
indicator feature for negations led to considerable
improvement in performance as well. For TaskB,
we first created a baseline model that uses sentiment
words and phrases from the AFINN lexicon as fea-
tures. Each feature that we added to the system re-
sulted in improvement in performance. The nega-
tion and endsWExclamation features only improved
the performance of the negative classifier, whereas
the slang feature only improved the performance of
the positive classifier.
Our results show that using rich feature sets with
machine learning algorithms is a promising ap-
proach for sentiment classification in Twitter. Our
TaskA system ranked 3rd among 23 systems and
TaskB system ranked 4th among 35 systems partici-
pating in SemEval 2013 Task 2.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Proceedings of the Seventh Conference
on International Language Resources and Evaluation
(LREC?10), Valletta, Malta, May. European Language
Resources Association (ELRA).
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, COLING ?10,
pages 36?44, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22:39?71.
Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in twitter streaming data. In Proceed-
ings of the 13th international conference on Discov-
ery science, DS?10, pages 1?15, Berlin, Heidelberg.
Springer-Verlag.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10, pages 241?249, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Xiaoxu Fei, Huizhen Wang, and Jingbo Zhu. 2010. Sen-
timent word identification using the maximum entropy
model. In International Conference on Natural Lan-
guage Processing and Knowledge Engineering (NLP-
KE), pages 1?4.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
twitter: annotation, features, and experiments. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
?11, pages 42?47. Association for Computational Lin-
guistics.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Technical report, Stanford University.
Edward Loper and Steven Bird. 2002. Nltk: the natural
language toolkit. In Proceedings of the ACL-02 Work-
shop on Effective tools and methodologies for teach-
ing natural language processing and computational
linguistics - Volume 1, ETMTNLP ?02, pages 63?70,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM, 38:39?41.
Finn A?. Nielsen. 2011. A new ANEW: Evaluation of
a word list for sentiment analysis in microblogs. In
Proceedings of the ESWC2011 Workshop on ?Making
Sense of Microposts?: Big things come in small pack-
ages.
560
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From Tweets to Polls: Linking Text Sentiment to
Public Opinion Time Series. In Proceedings of the
International AAAI Conference on Weblogs and Social
Media.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conver-
sational text with word clusters. In Proceedings of
NAACL.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General Inquirer:
A Computer Approach to Content Analysis. MIT
Press.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ?13, June.
Le Zhang. 2011. Maximum entropy modeling toolkit for
python and c++. http://homepages.inf.ed.
ac.uk/lzhang10/maxent_toolkit.html.
Accessed: 2013-04-13.
561
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 170?177,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Bacteria Biotope Detection, Ontology-based Normalization, and Relation
Extraction using Syntactic Rules
I?lknur Karadeniz
Department of Computer Engineering
Bog?azic?i University
34342, Bebek, I?stanbul, Turkey
ilknur.karadeniz@boun.edu.tr
Arzucan O?zgu?r
Department of Computer Engineering
Bog?azic?i University
34342, Bebek, I?stanbul, Turkey
arzucan.ozgur@boun.edu.tr
Abstract
The absence of a comprehensive database
of locations where bacteria live is an im-
portant obstacle for biologists to under-
stand and study the interactions between
bacteria and their habitats. This paper re-
ports the results to a challenge, set forth by
the Bacteria Biotopes Task of the BioNLP
Shared Task 2013. Two systems are ex-
plained: Sub-task 1 system for identifying
habitat mentions in unstructured biomedi-
cal text and normalizing them through the
OntoBiotope ontology and Sub-task 2 sys-
tem for extracting localization and part-
of relations between bacteria and habitats.
Both approaches rely on syntactic rules
designed by considering the shallow lin-
guistic analysis of the text. Sub-task 2
system also makes use of discourse-based
rules. The two systems achieve promising
results on the shared task test data set.
1 Introduction
As the number of publications in the biomedical
domain continues to increase rapidly, information
retrieval systems which extract valuable informa-
tion from these publications have become more
important for scientists to access and utilize the
knowledge contained in them.
Most previous tasks on biomedical informa-
tion extraction focus on identifying interactions
and events among bio-molecules (Krallinger et al,
2008; Kim et al, 2009). The Bacteria Biotope
Task (Bossy et al, 2011; Bossy et al, 2012) is one
of the new challenges in this domain, which was
firstly presented in the BioNLP 2011 Shared Task.
The main goals of the Bacteria Biotope Task were
to extract bacteria locations, categorize them into
one of the eight types (Environment, Host, Host-
Part, Geographical, Water, Food, Medical, Soil),
and detect Localization and PartOf events between
bacteria and habitats. Automatically extracting
this information from textual sources is crucial for
creating a comprehensive database of bacteria and
habitat relations. Such a resource would be of
great value for research studies and applications
in several fields such as microbiology, health sci-
ences, and food processing.
Three teams participated in the Bacteria
Biotope Task using different methodologies
(Bossy et al, 2011; Bossy et al, 2012). Bibliome
INRA (Ratkovic et al, 2012), which achieved the
best F-score (45%) among these teams, imple-
mented a system which used both linguistic fea-
tures and reasoning over an ontology to predict lo-
cation boundaries and types. Bibliome also uti-
lized some resources such as NCBI Taxonomy1,
list of Agrovoc geographical names2, and an in-
house developed ontology for specific location
types. UTurku (Bjo?rne et al, 2012), presented a
machine-learning based system which can be used
to find solutions for all main tasks with a few al-
teration in the system. UTurku used this generic
system with additional named entity recognition
patterns and external resources, whereas JAIST
(Nguyen and Tsuruoka, 2011) used CRFs in order
to recognize entities and their types.
UTurku and JAIST treated event extraction as a
classification problem by using machine learning
approaches, while Bibliome created and utilized
a trigger-word list. Bibliome tried to find events
by checking if a trigger-word and entities co-occur
in the scope of the same sentence. Bibliome was
the only team that considered coreference resolu-
tion. Not considering coreference resolution de-
teriorated the performance of JAIST?s system less
than that of UTurku?s system, since JAIST?s sys-
tem operated in the scope of a paragraph, while
UTurku?s system operated in the scope of a sen-
1http://www.ncbi.nlm.nih.gov/Taxonomy/
2http://aims.fao.org/standards/agrovoc/about
170
tence.
The Bacteria Biotope Task (BB) in the BioNLP
2013 Shared Task (Bossy et al, 2013) gives an-
other opportunity to scientists to develop and com-
pare their systems on a reliable platform. This task
contains three subtasks. For Sub-task 1, partici-
pants are expected to detect the names and posi-
tions of habitat entities, as well as to normalize
these habitats through the OntoBiotope (MBTO)
Ontology concepts. For Sub-task 2, when the
names, types, and positions of the entities (bacte-
ria, habitat, geographical) are given, participants
are expected to extract relations which can be ei-
ther between bacteria and habitat pairs (Localiza-
tion event) or between host and host part pairs
(PartOf event). Sub-task 3 is the same as Sub-
task 2, except that the gold standard entities are
not provided to the participants.
In this paper, we present two systems, one
for Sub-task 1 (Entity Detection and Categoriza-
tion) and one for Sub-task 2 (Localization Rela-
tion Extraction) of the Bacteria Biotope Task in
the BioNLP 2013 Shared Task. Both systems are
rule-based and utilize the shallow syntactic analy-
sis of the documents. The Sub-task 2 system also
makes use of the discourse of the documents. The
technical details of our systems are explained in
the following sections.
2 Data Set
The corpus provided by the organizers was cre-
ated by collecting documents from many differ-
ent web sites, which contain general information
about bacteria and habitats. The data set, consist-
ing of 52 training, 26 development, and 26 test
documents, was annotated by the bioinformati-
cians of the Bibliome team of MIG Laboratory at
the Institut National de Recherche Agronomique
(INRA).
For the training and development phases of Sub-
task 1, document texts with manually annotated
habitat entities and the concepts assigned to them
through the OntoBiotope ontology were provided,
while in the test phase, only the unannotated docu-
ment texts were given by the task organizers. The
OntoBiotope ontology which contains 1,700 con-
cepts organized in a hierarchy of is-a relations was
also provided by the organizers for this task.
For the training and development phases of Sub-
task 2, document texts with manually annotated
bacteria, habitat and geographical entities, as well
as the localization and part-of relations were pro-
vided, while in the test phase, document texts an-
notated only for bacteria, habitat and geographical
entities were given.
3 Bacteria Biotope Detection and
Ontology-based Normalizaton
For Sub-task 1 (Entity Detection and Categoriza-
tion), we implemented a system which applies
syntactic rules to biomedical text after a pre-
processing phase, where a given text is split into
sentences and parsed using a shallow parser. The
workflow of our Sub-task 1 system is shown in
Figure 1. Firstly, each input file is split into sen-
tences using the Genia Sentence Splitter (Geni-
aSS) (Saetre et al, 2007). The outputs of the
splitter are given to the Genia Tagger (Tsuruoka
et al, 2005; Tsuruoka and Tsujii, 2005) as input
files with the aim of obtaining the lemmas, the
part-of-speech (POS) tags, and the constituent cat-
egories of the words in the given biomedical text
(e.g., surface form: ticks; lemma: tick; POS tag:
NNS; phrase structure: I-NP). We utilized these
syntactic information at the following steps of our
system.
In the following subsections, a detailed expla-
nation for the detection of habitat boundaries and
their normalization through the OntoBiotope On-
tology concepts is provided.
3.1 Entity Boundary Detection
Entity boundary detection, which is the first step of
Sub-task 1, includes automatic extraction of habi-
tat entities from a given natural language text, and
detection of the entity boundaries precisely. In
other words, the habitat boundaries that are re-
trieved from the texts should not include any un-
necessary and non-informative words. In order to
achieve this goal, we assume that bacteria habitats
are embedded in text as noun phrases, and all noun
phrases are possible candidates for habitat entities.
Based on this assumption, our system follows the
steps that are explained below by using the mod-
ules that are shown in Figure 1.
As explained before, the Sentence Splitter,
POS Tagger, and Shallow Parser are the mod-
ules that are utilized in the pre-processing phase.
The Noun Phrase Extractor & Simplifier
module firstly detects the noun phrases in the
text by using the Genia Tagger and then post-
processes these noun phrases by using some syn-
171
POS Tagger
Sentence Splitter
Noun Phrase  Extractor & Simplifier  
Habitat Name Recognizer & NormalizerHabitat Na e Recognizer & Nor alizer
Syntactic Rules
Shallow Parser
Lemmatizer
Documents
Ontology
Normalized Habitat Entities
Figure 1: Workflow of the Sub-task 1 System
tactic rules. The functions of this module include
the removal of some unnecessary words from the
noun phrases, which are not informative for envi-
ronmental locations of bacteria. To distinguish in-
formative words from non-informative ones, our
system utilizes the POS Tags of each word that
compose the noun phrases in question. For ex-
ample, words that have determiners or possessive
pronouns as their POS Tags should not be included
to the boundaries of the candidate habitat enti-
ties. For example, the in the noun phrase ?the
soybean plant Glycine max? and its in the noun
phrase ?its infectious saliva? are eliminated from
the candidate noun phrases, restricting the habi-
tat boundary, and creating new candidate noun
phrases.
The Noun Phrase Extractor & Simplifier mod-
ule also includes a mechanism to handle noun
phrases that contain the conjunction ?and?. First,
such noun phrases are separated from the conjunc-
tion ?and? into two sub-phrases. Next, each sub-
phrase is searched in the OntoBiotope ontology.
If the ontology entries matched for the two sub-
phrases have the same direct ancestor (i.e., the
two ontology entries have a common is-a relation),
then the noun phrase consisting of the two sub-
phrases connected with the conjunction ?and? is
identified as a single habitat entity. On the other
hand, if the ontology entries matched for the two
sub-phrases don?t have a common direct ances-
tor, then each sub-phrase is identified as a sepa-
rate habitat entity. For example, each of the entity
boundaries of the phrases ?nasal and oral cav-
ity? , ?fresh and salt water?, and ?human and
sheep? are handled differently from each other as
described below.
? For the first phrase, ?nasal? is the first sub-
phrase and ?oral cavity? is the second sub-
phrase. The direct ancestor (i.e., the first level
is-a concept) of the first sub-phrase ?nasal?
is ?respiratory tract part? and that of the
second sub-phrase ?oral cavity? is ?buccal?.
Since ?respiratory tract part? and ?buccal?
is-a concepts are not the same, ?nasal cav-
ity? and ?oral cavity? are generated as two
separate habitats. In other words, if there is
not a direct common is-a concept between
the matching terms for the sub-phrases in
the OntoBiotope ontology, then one habitat
entity ?nasal cavity? is generated from the
noun phrase by adding the second part of the
second sub-phrase ?cavity? to the first sub-
phrase ?nasal? and another entity is gener-
ated by taking the second sub-phrase as a
whole ?oral cavity?.
? For the second sample phrase, ?fresh? is the
first sub-phrase and ?salt water? is the sec-
ond sub-phrase. The first sub-phrase ?fresh?
matches with an ontology entry whose direct
ancestor is ?environmental water with chem-
ical property? and the second sub-phrase
?salt water? matches with an ontology entry
that has two different direct ancestors ?en-
vironmental water with chemical property?
and ?saline water?. Since ?environmental
water with chemical property? is a common
ancestor for both sub-phrases in the ontology,
a single habitat entity ?fresh and salt water?
is generated. In other words, if there is a di-
rect common ancestor between the matching
terms for the sub-phrases in the OntoBiotope
ontology, then only one habitat entity that is
composed of the whole noun phrase is gener-
ated.
172
? For the third phrase, ?human? is the first
sub-phrase and ?sheep? is the second sub-
phrase. In this case, two separate habitat en-
tities ?human? and ?sheep? are generated di-
rectly from the two sub-phrases since they
don?t have a common ancestor in the ontol-
ogy.
At the end of these phases, purified sub-noun
phrases, which are habitat entity candidates whose
boundaries are roughly determined by the deletion
of non-informative modifiers from noun phrases,
are obtained.
To determine whether a candidate noun phrase
is a habitat entity or not, the Habitat Name Rec-
ognizer & Normalizer module searches all on-
tology entries, which compose the OntoBiotope
Ontology, to find an exact match with the candi-
date noun phrase or with parts of it. In this step,
the names, exact synonyms, and related synonyms
of ontology entries (ontology entry features) are
compared with the candidate noun phrase.
[Term]
id: MBTO:00001828
name: digestive tract
related synonym: ?gastrointestinal tract? [TyDI:23802]
exact synonym: ?GI tract? [TyDI:23803]
related synonym: ?intestinal region? [TyDI:23805]
related synonym: ?gastrointestinal? [TyDI:23806]
exact synonym: ?GIT? [TyDI:23807]
related synonym: ?alimentary canal? [TyDI:24621]
is a: MBTO:00000797 ! organ
Table 1: First ontology entity match for human
gastrointestinal tract.
For example, if our candidate noun phrase is
?the human gastrointestinal tract?, after the post-
processing phase, the purified candidate phrase
will be ?human gastrointestinal tract?. When
the search step for this simplified candidate entity
is handled, two different ontology entries are re-
turned by our system as matches (see Table 1 for
the first ontology entry match and Table 2 for the
second one). These two ontology entries are re-
turned as results by our system because the first
one contains the related synonym: ?gastrointesti-
nal tract? and the second one contains the name:
human. Since the system returns matches for
the candidate noun phrase ?human gastrointesti-
nal tract?, it is verified that one or more habitat
entities can be extracted from this phrase.
To detect the exact habitat boundaries, manually
developed syntactic rules are utilized in addition to
[Term]
id: MBTO:00001402
name: human
related synonym: ?person? [TyDI:25453]
related synonym: ?individual? [TyDI:25454]
exact synonym: ?subject? [TyDI:25374]
exact synonym: ?homo sapiens? [TyDI:26681]
related synonym: ?people? [TyDI:25455]
is a: MBTO:00001514 ! mammalian
Table 2: Second ontology entity match for human
gastrointestinal tract.
the ontology entry matching algorithm, which is
used for entity verification of a candidate phrase.
Our system determines the boundaries according
to the following syntactic rules:
? If an ontology entry matches exactly with the
noun phrase, take the boundaries of the noun
phrase as the boundaries of the habitat, and
use the whole phrase to create a new habitat
entity.
? If an ontology entry matches beginning from
the first word of the noun phrase, but does
not match totally, take the boundaries of the
matched parts of the phrase, and create a new
habitat entity using the partial phrase.
? If an ontology entry matches beginning from
an internal word of the noun phrase, take the
boundaries of the noun phrase as the bound-
aries of the habitat, and use the whole phrase
to create a new habitat entity. For exam-
ple, in Table 1, the match of the noun phrase
?human gastrointestinal tract? with the re-
lated synonym: ?gastrointestinal tract? gen-
erates ?human gastrointestinal tract? as a
habitat entity.
In many cases habitat entity names occur in dif-
ferent inflected forms in text. For example, the
habitat name ?human?, can occur in text in its plu-
ral form as ?humans?. We used the Lemmatizer
module in order to be able to match the differ-
ent inflected forms of habitat names occurring in
text against the corresponding entires in the Onto-
Biotope ontology. This module applies the rules
described above to the lemmatized forms of the
candidate noun phrases, which are obtained using
the Genia Tagger.
After running the same algorithm also for lem-
matized forms of the noun phrase, a merging algo-
rithm is used for the matching results of the sur-
173
face and lemmatized forms of the noun phrases in
order to create an output file, which contains the
predicted habitat entities and their positions in the
input text.
3.2 Ontology Categorization
For Sub-task 1, detection of the entities and their
boundaries is not sufficient. In order to obtain
normalized entity names, participants are also ex-
pected to assign at least one ontology concept
from the OntoBiotope Ontology to all habitat en-
tities, which are automatically extracted by their
systems from the input text.
While our system detects entities and their
boundaries (as explained in detail in Section 3.1),
it also assigns ontology concepts to the re-
trieved entities. All assigned concepts are ref-
erenced by the MBTO-IDs of the matched on-
tology entries (e.g, MBTO:00001402 for human
and MBTO:00001828 for human gastrointestinal
tract) (see Table 3).
4 Event Extraction
For Sub-task 2 (Localization Event Extraction
Task), we used different methods according to the
relation type that we are trying to extract. The
workflow of our system is shown in Figure 2. The
details of our approach are explained in the fol-
lowing sub-sections.
4.1 Localization Event Extraction
In order to extract localization relations, we as-
sume that discourse changes with the beginning of
a new paragraph. Our system firstly splits the in-
put text into paragraphs. Next, the entities (bacte-
ria and habitats) that occur in the given paragraph
are identified. We assume that the paragraph is
about the bacterium whose name occurs first in the
paragraph. Therefore, we assign all the habitat en-
tities to that bacterium. If the name of this bac-
terium occurs in previous paragraphs as well, then
the boundary of the bacterium entity is set to its
first occurrence in the document.
We also have a special case for boundary de-
termination of bacteria in the localization rela-
tion. If a bacterium name contains the word
?strain? , we assign the first occurrence of its
name without the word ?strain? (e.g, Bifidobac-
terium longum NCC2705 instead of Bifidobac-
terium longum strain NCC2705).
Figure 2: Workflow of the Sub-task 2 System
4.2 PartOf Event Extraction
In order to detect partOf relations between hosts
and host parts in a given biomedical text, we as-
sumed that such relations can only exist if the
host and the host part entities occur in the same
paragraph. Based on this assumption, we pro-
pose that if a habitat name is a subunit of the term
which identifies another habitat that passes in the
same discourse, then they are likely to be related
through a partOf relation. In other words, if one
habitat contains the other one, and obeys some
syntactic rules, then there is a relation. For exam-
ple, ?respiratory track of animals? is a habitat and
?animals? is another habitat, both of which are in
the same paragraph. Since the ?respiratory track
of animals? phrase contains the ?animals? phrase
and the word ?of?, and the ?animals? phrase is on
the right hand side of the ?respiratory track of ani-
mals? phrase, our system detects a partOf relation
between them.
5 Evaluation
The official evaluation results on the test set are
provided using different criteria for the two sub-
tasks by the task organizers3.
3http://2013.bionlp-st.org/tasks/bacteria-biotopes/test-
results
174
EntityID Boundary Entity
T1 Habitat 113 118 human
T2 Habitat 113 141 human gastrointestinal tract
ID EntityID Reference
N1 OntoBiotope Annotation:T1 Referent:MBTO:00001402
N2 OntoBiotope Annotation:T2 Referent:MBTO:00001828
Table 3: Detected entities and boundaries from the human gastrointestinal tract noun phrase
For Sub-task 1, submissions are evaluated con-
sidering the Slot Error Rate (SER), which depends
on the number of substitutions S, deletions D, in-
sertions I, and N. N is the number of habitats in
the reference, while D and I are the number of
reference and predicted entities that could not be
paired, respectively.
SER =
S + D + I
N
(1)
The number of substitutions S is calculated by
using Equation 2. Here J is the Jaccard index be-
tween the reference and the predicted entity, which
measures the accuracy of the boundaries of the
predicted entity (Bossy et al, 2012). W is a param-
eter that defines the semantic similarity between
the ontology concepts related to the reference en-
tity and to the predicted entity (Wang et al, 2007).
This similarity is based on the is-a relationships
between concepts, and used for penalizing ances-
tor/descendent predictions more compared to sib-
ling predictions as it approaches to 1.
S = J ?W (2)
For Sub-task 2, precision, recall, and f-score
metrics are used for evaluation. In the following
subsections, our official evaluation results for Sub-
task 1 and Sub-task 2 are given.
5.1 Results of Sub-task 1
Our official evaluation results on test set are shown
in Table 4. Our system ranked second according to
the SER value among four participating systems in
the shared task.
The official results of our system on the test set
for entity boundary detection are shown in Table 5.
Our system obtained the smallest SER value for
detecting the entity boundaries (i.e., the best per-
formance) among the other participating systems.
Our ontology categorization evaluation results
on the test set, which do not take into account the
Main Results
S 112.70
I 43
D 89
M 305.30
P 520
SER 0.48
Recall 0.60
Precision 0.59
F1 0.59
Table 4: Main results on test set for Sub-task 1(En-
tity Boundary Detection & Ontology Categoriza-
tion)
Entity Boundary Evaluation
S 82.71
M 335.29
SER 0.42
Recall 0.66
Precision 0.64
F1 0.65
Table 5: Entity boundary detection results on the
test set for Sub-task 1
entities? boundaries are shown in Table 6. Our sys-
tem ranked second on the main evaluation where
the parameter w (described in Section 5) was set
to 0.65. As shown in the table, as the w value in-
creases, our results get better. According to the of-
ficial results, our system ranked first for w = 1 with
the highest f-score, and our SER result is same as
the best system for w = 0.8.
The parameter w can can be seen as a penal-
ization value for the false concept references. As
w increases, the false references to distant ances-
tors and descendants of the true reference concepts
are penalized more, whereas as w decreases the
false references to the siblings are penalized more
severely.
The results also show that our system is able to
achieve balanced precision and recall values. In
other words, the recall and precision values are
close to each other.
175
w S M SER Recall Precision F
1 38.64 379.36 0.34 0.75 0.73 0.74
0.8 44.90 373.10 0.35 0.74 0.72 0.73
0.65 50.95 367.05 0.36 0.72 0.71 0.71
0.1 70.78 347.22 0.40 0.68 0.67 0.68
Table 6: Ontology Categorization results for Sub-
task 1 on the test set
5.2 Results of Sub-task 2
The precision, recall, and f-measure metrics are
used to evaluate the Sub-task 2 results on the test
set. Our main evaluation results, which consider
detection of both Localization and PartOf event re-
lations for Sub-task 2 are shown in the first row of
Table 7, whereas our results that are calculated for
the two event types separately are shown in the Lo-
calization and PartOf rows of the table. Accord-
ing to the official results, our system ranked third
for detecting all event types. On the other hand, it
achieved the best results for detecting the PartOf
events.
Recall Precision F
All 0.21 0.38 0.27
Localization 0.23 0.38 0.29
PartOf 0.15 0.40 0.22
Table 7: Main results on test set for Sub-task 2
6 Conclusion
In this study, we presented two systems that are
implemented in the scope of the BioNLP Shared
Task 2013 - Bacteria Biotope Task. The aim of
the Sub-task 1 system is the identification of habi-
tat mentions in unstructured biomedical text and
their normalization through the OntoBiotope on-
tology, whereas the goal of the Sub-task 2 system
is the extraction of localization and part-of rela-
tions between bacteria and habitats when the enti-
ties are given. Both systems are based on syntactic
rules designed by considering the shallow syntac-
tic analysis of the text, while the Sub-task 2 system
also makes use of discourse-based rules.
According to the official evaluation, both of our
systems achieved promising results on the shared
task test data set. Based on the main evaluation
where the parameter w is set to 0.65, our Sub-task
1 system ranked second among four participating
systems and it ranked first for predicting the entity
boundaries when ontology categorization outputs
are not considered. The results show that our sys-
tem performs better as w increases and achieves
the best performance when w = 1 and w = 0.8. Our
Sub-task 2 system achieved encouraging results by
ranking first in predicting the PartOf events, and
ranking third when all event types are considered.
The proposed systems can be enhanced by in-
corporating a stemming module and including
more syntax and discourse based rules.
Acknowledgments
This work has been supported by Marie Curie
FP7-Reintegration-Grants within the 7th Euro-
pean Community Framework Programme.
References
Jari Bjo?rne, Filip Ginter, and Tapio Salakoski. 2012.
University of Turku in the BioNLP?11 Shared Task.
BMC Bioinformatics, 13 Suppl 11:S4.
Robert Bossy, Julien Jourde, Philippe Bessie`res,
Maarten van de Guchte, and Claire Ne?dellec. 2011.
Bionlp shared task 2011: bacteria biotope. In Pro-
ceedings of the BioNLP Shared Task 2011 Work-
shop, BioNLP Shared Task ?11, pages 56?64,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Robert Bossy, Julien Jourde, Alain P. Manine, Philippe
Veber, Erick Alphonse, Maarten van de Guchte,
Philippe Bessieres, and Claire Nedellec. 2012.
BioNLP Shared Task - The Bacteria Track. BMC
Bioinformatics, 13(Suppl 11):S3+.
Robert Bossy, Wiktoria Golik, Zorana Ratkovic,
Philippe Bessir`es, and Claire Ne?dellec. 2013.
BioNLP shared task 2013 - an overview of the bac-
teria biotope task. In Proceedings of BioNLP Shared
Task 2013 Workshop, Sofia, Bulgaria, AUG. Associ-
ation for Computational Linguistics.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of bionlp?09 shared task on event extraction. In
Proceedings of the Workshop on Current Trends in
Biomedical Natural Language Processing: Shared
Task, BioNLP ?09, pages 1?9, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Martin Krallinger, Florian Leitner, Carlos Rodriguez-
penagos, and Alfonso Valencia. 2008. Overview of
the protein-protein interaction annotation extraction
task of biocreative ii. Genome Biology, pages 2?4.
Nhung T. H. Nguyen and Yoshimasa Tsuruoka. 2011.
Extracting bacteria biotopes with semi-supervised
named entity recognition and coreference resolution.
In Proceedings of the BioNLP Shared Task 2011
Workshop, BioNLP Shared Task ?11, pages 94?101,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
176
Zorana Ratkovic, Wiktoria Golik, and Pierre Warnier.
2012. Event extraction of bacteria biotopes: a
knowledge-intensive NLP-based approach. BMC
Bioinformatics, 13:S8+.
Rune Saetre, Kazuhiro Yoshida, Akane Yakushiji,
Yusuke Miyao, Yuichiro Matsubayashi, and Tomoko
Ohta. 2007. AKANE System: Protein-Protein
Interaction Pairs in the BioCreAtIvE2 Challenge,
PPI-IPS subtask. In Lynette Hirschman, Martin
Krallinger, and Alfonso Valencia, editors, Proceed-
ings of the Second BioCreative Challenge Workshop.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy for
tagging sequence data. In HLT ?05: Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, pages 467?474. Association for Computa-
tional Linguistics.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Developing a Ro-
bust Part-of-Speech Tagger for Biomedical Text. In
Panayiotis Bozanis and Elias N. Houstis, editors,
Advances in Informatics, volume 3746, chapter 36,
pages 382?392. Springer Berlin Heidelberg.
J. Z. Wang, Z. Du, R. Payattakool, P. S. Yu, and
C. F. Chen. 2007. A new method to measure the
semantic similarity of GO terms. Bioinformatics,
23(10):1274?1281, May.
177
