Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 36?44,
Athens, Greece, 31 March, 2009. c?2009 Association for Computational Linguistics
Unsupervised Concept Discovery In Hebrew Using Simple Unsupervised
Word Prefix Segmentation for Hebrew and Arabic
Elad Dinur1 Dmitry Davidov2
1Institute of Computer Science
2ICNC
The Hebrew University of Jerusalem
Ari Rappoport1
Abstract
Fully unsupervised pattern-based methods
for discovery of word categories have been
proven to be useful in several languages.
The majority of these methods rely on the
existence of function words as separate
text units. However, in morphology-rich
languages, in particular Semitic languages
such as Hebrew and Arabic, the equiva-
lents of such function words are usually
written as morphemes attached as prefixes
to other words. As a result, they are missed
by word-based pattern discovery methods,
causing many useful patterns to be unde-
tected and a drastic deterioration in per-
formance. To enable high quality lexical
category acquisition, we propose a sim-
ple unsupervised word segmentation algo-
rithm that separates these morphemes. We
study the performance of the algorithm for
Hebrew and Arabic, and show that it in-
deed improves a state-of-art unsupervised
concept acquisition algorithm in Hebrew.
1 Introduction
In many NLP tasks, we wish to extract informa-
tion or perform processing on text using minimal
knowledge on the input natural language. Towards
this goal, we sometimes find it useful to divide the
set of words in natural language to function words
and content words, a division that applies in the
vast majority of languages. Function words (or
grammatical words, e.g., a, an, the, in, of, etc) are
words that have little or highly ambiguous lexi-
cal meaning, and serve to express grammatical or
semantic relationships with the other words in a
sentence.
In some morphologically-rich languages, im-
portant function words are not written as space-
separated units but as morphemes attached as pre-
fixes to other words. This fact can cause prob-
lems when statistically analyzing text in these lan-
guages, for two main reasons: (1) the vocabulary
of the language grows, as our lexical knowledge
comes solely from a corpus (words appear with
and without the function morphemes); (2) infor-
mation derived from the presence of these mor-
phemes in the sentence is usually lost.
In this paper we address the important task of
a fully unsupervised acquisition of Hebrew lexical
categories (or concepts ? words sharing a signifi-
cant aspect of their meaning). We are not aware of
any previous work on this task for Hebrew. Due
to the problem above, the performance of many
acquisition algorithms deteriorates unacceptably.
This happens, for example, in the (Davidov and
Rappoport, 2006) algorithm that utilizes automati-
cally detected function words as the main building
block for pattern construction.
In order to overcome this problem, one should
separate such prefixes from the compound words
(words consisting of function morphemes attached
to content words) in the input corpus. When
we consider some particular word, there are fre-
quently many options to split it to smaller strings.
Fortunately, the set of function words is small and
closed, and the set of grammatical sequences of
function prefixes is also small. Hence we assume
it does not cost us much to know in advance what
are the possible sequences for a specific language.
Even when considering the small number of
possible function words, the task of separating
them is not simple, as some words may be ambigu-
ous. When reading a word that starts with a prefix
known to be a function morpheme, the word may
36
be a compound word, or it may be a meaningful
word by itself. For example, the word ?hsws? in
Hebrew1 can be interpreted as ?hsws? (hesitation),
or ?h sws? (the horse). The segmentation of the
word is context dependent ? the same string may
be segmented differently in different contexts.
One way of doing such word prefix segmenta-
tion is to perform a complete morphological dis-
ambiguation of the sentence. The disambigua-
tion algorithm finds for each word its morpho-
logical attributes (POS tag, gender, etc.), and de-
cides whether a word is a compound word or a
word without prefixes. A disambiguation algo-
rithm generally relies on a language-specific mor-
phological analyzer. It may also require a large
manually tagged corpus, construction of which for
some particular language or domain requires sub-
stantial human labor. We avoid the utilization of
such costly and language-specific disambiguation
algorithms and manually annotated data.
In this paper we present a novel method to sep-
arate function word prefixes, and evaluate it us-
ing manually labeled gold standards in Hebrew
and Arabic. We incorporate the method into a
pattern-based Hebrew concept acquisition frame-
work and show that it greatly improves state-of-art
results for unsupervised lexical category acquisi-
tion. This improvement allows the pattern-based
unsupervised framework to use one-tenth of the
Hebrew data in order to reach a similar level of
results.
Section 2 discusses related work, and Section 3
reviews the word categories discovery algorithm.
Section 4 presents the word prefix segmentation
algorithm. Results are given in Section 5.
2 Related Work
In this paper we develop an unsupervised frame-
work for segmentation of the function words for
languages where context is important for correct
segmentation. Our main target language is He-
brew, and we experimented with Arabic as well.
As far as we know, there is no work on unsu-
pervised segmentation of words in Hebrew which
does not utilize language-specific tools such as
morphological analyzers.
Lee et al (2003) addressed supervised word
segmentation in Arabic and have some aspects
similar to our approach. As in their study, we
1Transcription is according to (Ornan, 2005), except for
Shin which is denoted by ?$?.
also have a pre-supplied list of possible prefix
sequences and assume a trigram model in order
to find the most probable morpheme sequence.
Both studies evaluate performance on a segmented
text, and not just on words in the lexicon. How-
ever, their algorithm, while achieving good per-
formance (97% accuracy), relies on a training set
? a manually segmented corpus of about 110,000
words, while our unsupervised framework does
not require any annotation and is thus easier to im-
plement and to apply to different domains and lan-
guages.
Snyder and Barzilay (2008) study the task of un-
supervised morphological segmentation of multi-
ple languages. Their algorithm automatically in-
duces a segmentation and morpheme alignment of
short parallel phrases from a multilingual corpus.
Their corpus (The Hebrew Bible and translations)
contains parallel phrases in English, Arabic, He-
brew and Aramaic. They obtain 63.87 F-Score
for Hebrew words segmentation (prefix and suf-
fix), where recall and precision is calculated based
on all possible segmentation points.
Another type of segmentation algorithms in-
volves utilization of language-specific morpholog-
ical analyzers for complete morphological disam-
biguation. In Hebrew each word usually has more
than one possible POS (along with other attributes,
such as gender, number, etc.). Assuming we have
a morphological analyzer (producing the set of
possible analyses for a given word), we can try to
discover the correct segmentation of each word.
Levinger et al (1995) developed a method for
disambiguation of the results provided by a mor-
phological analyzer for Hebrew. Adler and El-
hadad (2006) proposed an unsupervised algorithm
for word segmentation. They estimate an initial
language model (using (Levinger et al, 1995))
and improve this model with EM. Direct compar-
ison to their work is problematic, however, since
we avoid utilization of a language-specific mor-
phology/POS analyzer. There are also studies of
this type that utilize labeled data (Bar-Haim et al,
2005), where the language model is learned from
the training data.
Extensive research has been done on word seg-
mentation, where, unlike in our study, the segmen-
tation is evaluated for every word, regardless of its
context. Creutz (2003) presents an algorithm for
unsupervised segmentation under these assump-
tions. He proposes a probabilistic model which
37
utilizes the distributions of morpheme length and
frequency to estimate the quality of the induced
morphemes. Dasgupta and Ng (2007) improves
over (Creutz, 2003) by suggesting a simpler ap-
proach. They segment a prefix using the word
frequency with and without a prefix. Other re-
cent studies that follow the context-independent
setup include (Creutz and Lagus, 2005; Keshava
and Pitler, 2005; Demberg, 2007). They test
their methods on English, Finnish and Turkish.
All of these studies, however, assume context-
independency of segmentation, disregarding the
ambiguity that may come from context. This
makes it problematic to apply the proposed meth-
ods to context-dependent morphology types as in
Hebrew and Arabic.
The guiding goal in the present paper is the con-
cept acquisition problem. Concept acquisition of
different kinds has been studied extensively. The
two main classification axes for this task are the
type of human input and annotation, and the basic
algorithmic approach used. The two main algo-
rithmic approaches are clustering of context fea-
ture vectors and pattern-based discovery.
The first approach is to map each word to a fea-
ture vector and cluster these vectors. Example of
such algorithms are (Pereira et al, 1993) and (Lin,
1998) that use syntactic features in the vector def-
inition. Pantel and Lin (2002) improves on the lat-
ter by clustering by committee.
Recently, there is a growing interest in the sec-
ond main algorithmic approach, usage of lexico-
syntactic patterns. Patterns have been shown to
produce more accurate results than feature vectors,
at a lower computational cost on large corpora
(Pantel et al, 2004). Thus (Dorow et al, 2005)
discover categories using two basic pre-specified
patterns (?x and y?, ?x or y?).
Some recent studies have proposed frameworks
that attempt to avoid any implicit or explicit pre-
specification of patterns. Davidov and Rappoport
(2006) proposed a method that detects function
words by their high frequency, and utilizes these
words for the discovery of symmetric patterns.
Their method is based on two assumptions: (1)
some function words in the language symmetri-
cally connect words belonging to the same cat-
egory; (2) such function words can be detected
as the most frequent words in language. While
these assumptions are reasonable for many lan-
guages, for some morphologically rich languages
the second assumption may fail. This is due to
the fact that some languages like Hebrew and Ara-
bic may express relationships not by isolated func-
tion words but by morphemes attached in writing
to other words.
As an example, consider the English word
?and?, which was shown to be very useful in con-
cept acquisition (Dorow et al, 2005). In Hebrew
this word is usually expressed as the morpheme
?w? attached to the second word in a conjunc-
tion (?... wsws? ? ?... and horse?). Patterns dis-
covered by such automatic pattern discovery al-
gorithms are based on isolated words, and hence
fail to capture ?and?-based relationships that are
very useful for detection of words belonging to the
same concept. Davidov and Rappoport (2006) re-
ports very good results for English and Russian.
However, no previous work applies a fully unsu-
pervised concept acquisition for Hebrew.
In our study we combine their concept ac-
quisition framework with a simple unsupervised
word segmentation technique. Our evaluation con-
firms the weakness of word-based frameworks for
morphology-rich languages such as Hebrew, and
shows that utilizing the proposed word segmen-
tation can overcome this weakness while keeping
the concept acquisition approach fully unsuper-
vised.
3 Unsupervised Discovery of Word
Categories
In this study we use word segmentation to improve
the (Davidov and Rappoport, 2006) method for
discovery of word categories, sets of words shar-
ing a significant aspect of their meaning. An ex-
ample for such a discovered category is the set of
verbs {dive, snorkel, swim, float, surf, sail, drift,
...}. Below we briefly describe this category ac-
quisition algorithm.
The algorithm consists of three stages as fol-
lows. First, it discovers a set of pattern candidates,
which are defined by a combination of high fre-
quency words (denoted by H) and slots for low
frequency (content) words (denoted by C). An ex-
ample for such a pattern candidate is ?x belongs to
y?, where ?x? and ?y? stand for content word slots.
The patterns are found according to a predefined
set of possible meta-patterns. The meta-patterns
are language-independent2 and consist of up to 4
2They do not include any specific words, only a relative
order of high/low frequency words, and hence can be used on
38
words in total, from which two are (non-adjacent)
content words. Four meta-patterns are used: CHC,
CHCH, CHHC, HCHC.
Second, those patterns which give rise to sym-
metric lexical relationships are identified. The
meaning of phrases constructed from those pat-
terns is (almost) invariant to the order of the con-
tent words contained in them. An example for
such a pattern is ?x and y?. In order to iden-
tify such useful patterns, for each pattern we build
a graph following (Widdows and Dorow, 2002).
The graph is constructed from a node for each con-
tent word, and a directed arc from the node ?x? to
?y? if the corresponding content words appear in
the pattern such that ?x? precedes ?y?. Then we
calculate several symmetry measures on the graph
structure and select the patterns with best values
for these measures.
The third stage is the generation of categories.
We extract tightly connected sets of words from
the unified graph which combines all graphs of se-
lected patterns. Such sets of words define the de-
sired categories.
The patterns which include the ?x and y? sub-
string are among the most useful patterns for gen-
eration of categories (they were used in (Dorow et
al., 2005) and discovered in all 5 languages tested
in (Davidov and Rappoport, 2006)). However, in
Hebrew such patterns can not be found in the same
way, since the function word ?and? is the prefix ?w?
and not a standalone high frequency word.
Another popular set of patterns are ones includ-
ing ?x or y?. Such patterns can be identified in
Hebrew, as ?or? in Hebrew is a separate word.
However, even in this case, the content word rep-
resented by ?x? or ?y? may appear with a pre-
fix. This damages the construction of the pattern
graph, since two different nodes may be created
instead of one ? one for a regular content word,
the other for the same word with a prefix. Conse-
quently, it is reasonable to assume that segmenting
the corpus in advance should improve the results
of discovery of word categories.
4 Word Segmentation Algorithm
We assume we know the small and closed set of
grammatical function word prefix sequences in the
language3. Our input is a sentence, and our ob-
any languages with explicit word segmentation.
3Unlike development of labeled training data, handcraft-
ing such a closed set is straightforward for many languages
and does not requires any significant time/human labor
jective is to return the correct segmentation of the
sentence. A sentence L is a sequence of words
{w1, w2, ..., wn}. A segmentation Si of L is a se-
quence of morphemes {m1, m2, ..., mk} and l(Si)
is the number of morphemes in the sequence. Note
that l(Si) may be different for each segmentation.
The best segmentation S will be calculated by:
P (Si) = p(m1)p(m2|m1)
l(Si)
?
i=3
p(mi|mi?1mi?2)
S = arg max
Si
P (Si)
Calculation of joint probabilities requires a tri-
gram model of the language. Below we describe
the construction of the trigram model and then we
detail the algorithm for efficient calculation of S.
4.1 Construction of trigram model
Creating the trigram language model is done in
two stages: (1) we segment a corpus automati-
cally, and (2) we learn a trigram language model
from the segmented corpus.
4.1.1 Initial corpus segmentation
For initial corpus segmentation, we define a sta-
tistical measure for the segmentation of individual
words. Let wx be a word, such that w is the pre-
fix of the word composed of a sequence of func-
tion word prefixes and x is a string of letters. Let
f(x) be the frequency of the word x in the cor-
pus. Denote by al the average length of the strings
(with prefixes) in the language. This can be eas-
ily estimated from the corpus ? every string that
appears in the corpus is counted once. l(x) is the
number of characters in the word x. We utilize
two parameters G, H , where G < H (we used
G = 2.5, H = 3.5) and define the following func-
tions :
factor(x) =
{
al?G?l(x)
al?H l(x) < al ? G
0 otherwise
Rank(wx) =
f(wx)
f(wx) + f(x)
+ factor(x)
Note that the expression f(wx)f(wx)+f(x) is a number
in (0, 1], inversely correlated with the frequency of
the prefixed word. Thus higher Rank(wx) values
indicate that the word is less likely to be composed
of the prefix w followed by the word x.
39
The expression al?G?l(x)al?H is a number in (0, 1],
therefore factor(x) ? [0, 1]. H is G ? 1 in order
to keep the expression smaller than 1. The term
factor(x) is greater as x is shorter. The factor
is meant to express the fact that short words are
less likely to have a prefix. We have examined
this in Hebrew ? as there are no words of length
1, two letter words have no prefix. We have ana-
lyzed 102 randomly chosen three letter words, and
found that only 19 of them were prefixed words.
We have analyzed 100 randomly chosen four let-
ter words, and found that 40 of them were pre-
fixed words. The result was about the same for
five letter words. In order to decide whether a
word needs to be separated, we define a thresh-
old T ? [0, 1]. We allow word separation only
when Rank(wx) is lower than T . When there
are more than two possible sequences of function
word prefixes (?mhsws?,?m hsws?, ?mh sws?),
we choose the segmentation with the lower rank.
4.1.2 Learning the trigram model
The learning of the language model is based on
counts of the corpus, assigning a special symbol,
?u/k? (unknown) for all words that do not appear
in the corpus. As estimated by (Lee et al, 2003),
we set the probability of ?u/k? to be 1E ? 9. The
value of the symbol ?u/k? was observed to be sig-
nificant. We found that the value proposed by (Lee
et al, 2003) for Arabic gives good results also for
Hebrew.
4.2 Dynamic programming approach for
word segmentation
The naive method to find S is to iterate over
all possible segmentations of the sentence. This
method may fail to handle long sentences, as
the number of segmentations grows exponentially
with the length of the sentence. To overcome this
problem, we use dynamic programming.
Each morpheme has an index i to its place in a
segmentation sequence. Iteratively, for index i, for
every morpheme which appears in some segmen-
tation in index i, we calculate the best segmen-
tation of the sequence m1 . . .mi. Two problems
arise here: (1) we need to calculate which mor-
phemes may appear in a given index; (2) we need
to constrain the calculation, such that only valid
segmentations would be considered.
To calculate which morphemes can appear in a
given index we define the object Morpheme. It
contains the morpheme (string), the index of a
word in the sentence the morpheme belongs to,
reference to the preceding Morpheme in the same
word, and indication whether it is the last mor-
pheme in the word. For each index of the sen-
tence segmentation, we create a list of Morphemes
(index-list).
For each word wi, and for segmentation
m1i , .., m
k
i , we create Morphemes M1i , .., Mki . We
traverse sequentially the words in the sentence,
and for each segmentation we add the sequence of
Morphemes to all possible index-lists. The index-
list for the first Morpheme M1i is the combination
of successors of all the index-lists that contain a
Morpheme Mki?1. The constraints are enforced
easily ? if a Morpheme M ji is the first in a word,
the preceding Morpheme in the sequence must be
the last Morpheme of the previous word. Oth-
erwise, the preceding Morpheme must be M j?1i ,
which is referenced by M ji .
4.3 Limitations
While our model handles the majority of cases, it
does not fully comply with a linguistic analysis of
Hebrew, as there are a few minor exceptions. We
assumed that there is no ambiguity in the function
word prefixes. This is not entirely correct, as in
Hebrew we have two different kinds of exceptions
for this rule. For example, the prefix ?k$? (when),
can also be interpreted as the prefix ?k? (as) fol-
lowed by the prefix ?$? (that). As the second in-
terpretation is rare, we always assumed it is the
prefix ?k$?. This rule was applied wherever an
ambiguity exists. However, we did not treat this
problem as it is very rare, and in the development
set and test set it did not appear even once.
A harder problem is encountered when process-
ing the word ?bbyt?. Two interpretations could
be considered here: ?b byt? (?in a house?), and
?b h byt? (?in the house?). Whether this actu-
ally poses a problem or not depends on the ap-
plication. We assume that the correct segmenta-
tion here is ?b byt?. Without any additional lin-
guistic knowledge (for example, diacritical vowel
symbols should suffice in Hebrew), solving these
problems requires some prior discriminative data.
5 Evaluation and Results
We evaluate our algorithm in two stages. First we
test the quality of our unsupervised word segmen-
tation framework on Hebrew and Arabic, compar-
ing our segmentation results to a manually anno-
40
With factor(x) Without factor(x)
T Prec. Recall F-Measure Accuracy Prec. Recall F-Measure Accuracy
0.70 0.844 0.798 0.820 0.875 0.811 0.851 0.830 0.881
0.73 0.841 0.828 0.834 0.883 0.808 0.866 0.836 0.884
0.76 0.837 0.846 0.841 0.886 0.806 0.882 0.842 0.887
0.79 0.834 0.870 0.851 0.893 0.803 0.897 0.847 0.890
0.82 0.826 0.881 0.852 0.892 0.795 0.904 0.846 0.888
0.85 0.820 0.893 0.854 0.892 0.787 0.911 0.844 0.886
0.88 0.811 0.904 0.855 0.891 0.778 0.917 0.841 0.882
Table 1: Ranks vs. Threshold T for Hebrew.
With factor(x) Without factor(x)
T Prec. Recall F-Measure Accuracy Prec. Recall F-Measure Accuracy
0.91 0.940 0.771 0.846 0.892 0.903 0.803 0.850 0.891
0.93 0.930 0.797 0.858 0.898 0.903 0.840 0.870 0.904
0.95 0.931 0.810 0.866 0.904 0.902 0.856 0.878 0.909
0.97 0.927 0.823 0.872 0.906 0.896 0.869 0.882 0.911
0.99 0.925 0.848 0.872 0.915 0.878 0.896 0.886 0.913
1.00 0.923 0.852 0.886 0.915 0.841 0.896 0.867 0.895
Table 2: Ranks vs. Threshold T for Arabic.
Algorithm P R F A
Rank seg. 0.834 0.870 0.851 0.893
Baseline 0.561 0.491 0.523 0.69
Morfessor 0.630 0.689 0.658 0.814
Table 3: Segmentation results comparison.
tated gold standard. Then we incorporate word
segmentation into a concept acquisition frame-
work and compare the performance of this frame-
work with and without word segmentation.
5.1 Corpora and annotation
For our experiments in Hebrew we used a 19MB
Hebrew corpus obtained from the ?Mila? Knowl-
edge Center for Processing Hebrew4. The cor-
pus consists of 143,689 different words, and a
total of 1,512,737 word tokens. A sample text
of size about 24,000 words was taken from the
corpus, manually segmented by human annotators
and used as a gold standard in our segmentation
evaluation. In order to estimate the quality of our
algorithm for Arabic, we used a 7MB Arabic news
items corpus, and a similarly manually annotated
test text of 4715 words. The Arabic corpus is too
small for meaningful category discovery, so we
used it only in the segmentation evaluation.
5.2 Evaluation of segmentation framework
In order to estimate the performance of word seg-
mentation as a standalone algorithm we applied
our algorithm on the Hebrew and Arabic corpora,
4http://mila.cs.technion.ac.il.
using different parameter settings. We first cal-
culated the word frequencies, then applied initial
segmentation as described in Section 4. Then we
used SRILM (Stolcke, 2002) to learn the trigram
model from the segmented corpus. We utilized
Good-Turing discounting with Katz backoff, and
we gave words that were not in the training set the
constant probability 1E ? 9. Finally we utilized
the obtained trigram model to select sentence seg-
mentations. To test the influence of the factor(x)
component of the Rank value, we repeated our
experiment with and without usage of this com-
ponent. We also ran our algorithm with a set of
different threshold T values in order to study the
influence of this parameter.
Tables 1 and 2 show the obtained results for He-
brew and Arabic respectively. Precision is the ra-
tio of correct prefixes to the total number of de-
tected prefixes in the text. Recall is the ratio of pre-
fixes that were split correctly to the total number
of prefixes. Accuracy is the number of correctly
segmented words divided by the total number of
words.
As can be seen from the results, the best F-score
with and without usage of the factor(x) compo-
nent are about the same, but usage of this compo-
nent gives higher precision for the same F-score.
From comparison of Arabic and Hebrew perfor-
mance we can also see that segmentation decisions
for the task in Arabic are likely to be easier, since
the accuracy for T=1 is very high. It means that,
unlike in Hebrew (where the best results were ob-
tained for T=0.79), a word which starts with a pre-
41
Method us k-means random
avg ?shared meaning?(%) 85 24.61 10
avg triplet score(1-4) 1.57 2.32 3.71
avg category score(1-10) 9.35 6.62 3.5
Table 4: Human evaluation results.
abuse, robbery, murder, assault, extortion
good, cheap, beautiful, comfortable
son, daughter, brother, parent
when, how, where
essential, important, central, urgent
Table 5: A sample from the lexical categories dis-
covered in Hebrew (translated to English).
fix should generally be segmented.
We also compared our best results to the base-
line and to previous work. The baseline draws a
segmentation uniformly for each word, from the
possible segmentations of the word. In an at-
tempt to partially reproduce (Creutz and Lagus,
2005) on our data, we also compared our results
to the results obtained from Morfessor Categories-
MAP, version 0.9.1 (Described in (Creutz and La-
gus, 2005)). The Morfessor Categories-MAP al-
gorithm gets a list of words and their frequen-
cies, and returns the segmentation for every word.
Since Morfessor may segment words with prefixes
which do not exist in our predefined list of valid
prefixes, we did not segment the words that had
illegal prefixes as segmented by Morfessor.
Results for this comparison are shown in Table
3. Our method significantly outperforms both the
baseline and Morfessor-based segmentation. We
have also tried to improve the language model by
a self training scheme on the same corpus but we
observed only a slight improvement, giving 0.848
Precision and 0.872 Recall.
5.3 Discovery of word categories
We divide the evaluation of the word categories
discovery into two parts. The first is evaluating
the improvement in the quantity of found lexical
categories. The second is evaluating the quality
of these categories. We have applied the algo-
rithm to a Hebrew corpus of size 130MB5, which
is sufficient for a proof of concept. We compared
the output of the categories discovery on two dif-
ferent settings, with function word separation and
without such separation. In both settings we omit-
5Again obtained from the ?Mila? Knowledge Center for
Processing Hebrew.
N A J
With Separation 148 4.1 1
No Separation 36 2.9 0
Table 6: Lexical categories discovery results com-
parison. N: number of categories. A: average cat-
egory size. J: ?junk? words.
ted all punctuation symbols. In both runs of the
algorithm we used the same parameters. Eight
symmetric patterns were automatically chosen for
each run. Two of the patterns that were chosen
by the algorithm in the unseparated case were also
chosen in the separated case.
5.3.1 Manual estimation of category quality
Evaluating category quality is challenging since
no exhaustive lists or gold standards are widely
accepted even in English, certainly so in resource-
poor languages such as Hebrew. Hence we follow
the human judgment evaluation scheme presented
in (Davidov and Rappoport, 2006), for the cate-
gories obtained from the segmented corpus.
We compared three methods of word categories
discovery. The first is random sampling of words
into categories. The second is k-means, where
each word is mapped to a vector, and similarity is
calculated as described in (Pantel and Lin, 2002).
We applied k-means to the set of vectors, with sim-
ilarity as a distance function. If a vector had low
similarity with all means, we leave it unattached.
Therefore some clusters contained only one vec-
tor. Running the algorithm 10 times, with different
initial means each time, produced 60 clusters with
three or more words. An interesting phenomenon
we observed is that this method produces very nice
clusters of named entities. The last method is the
one in (Davidov and Rappoport, 2006).
The experiment contained two parts. In Part
I, subjects were given 40 triplets of words and
were asked to rank them using the following scale:
(1) the words definitely share a significant part
of their meaning; (2) the words have a shared
meaning but only in some context; (3) the words
have a shared meaning only under a very un-
usual context/situation; (4) the words do not share
any meaning; (5) I am not familiar enough with
some/all of the words.
The 40 triplets were obtained as follows. 20 of
our categories were selected at random from the
non-overlapping categories we have discovered,
and three words were selected from each of these
42
at random. 10 triplets were selected in the same
manner from the categories produced by k-means,
and 10 triplets were selected at random from con-
tent words in the same document.
In Part II, subjects were given the full categories
represented by the triplets that were graded as 1 or
2 in Part I (the full ?good? categories in terms of
sharing of meaning). Subjects were asked to grade
the categories from 1 (worst) to 10 (best) accord-
ing to how much the full category had met the ex-
pectations they had when seeing only the triplet.
Nine people participated in the evaluation. A
summary of the results is given in Table 4.
The categories obtained from the unsegmented
corpus are too few and too small for a significant
evaluation. Therefore we applied the evaluation
scheme only for the segmented corpus.
The results from the segmented corpus contain
some interesting categories, with a 100% preci-
sion, like colors, Arab leaders, family members
and cities. An interesting category is {Arabic, En-
glish, Russian, French, German, Yiddish, Polish,
Math}. A sample of some other interesting cate-
gories can be seen in Table 5.
5.3.2 Segmentation effect on category
discovery
In Table 6, we find that there is a major improve-
ment in the number of acquired categories, and an
interesting improvement in the average category
size. One might expect that as a consequence of
an incorrect segmentation of a word, ?junk? words
may appear in the discovered categories. As can
be seen, only one ?junk? word was categorized.
Throughout this paper we have assumed that
function word properties of languages such as He-
brew and Arabic decrease performance of whole-
word pattern-based concept acquisition methods.
To check this assumption, we have applied the
concept acquisition algorithm on several web-
based corpora of several languages, while choos-
ing corpora size to be exactly equal to the size of
the Hebrew corpus (130Mb) and utilizing exactly
the same parameters. We did not perform quality
evaluation6, but measured the number of concepts
and concept size. Indeed the number of categories
was (190, 170, 159, 162, 150, 29) for Russian, En-
glish, Spanish, French, Turkish and Arabic respec-
tively, clearly inferior for Arabic in comparison to
these European and Slavic languages. A similar
6Brief manual examination suggests no significant drops
in concept quality.
tendency was observed for average concept size.
At the same time prefix separation does help to ex-
tract 148 concepts for Hebrew, making it nearly in-
line with other languages. In contrast, our prelim-
inary experiments on English and Russian suggest
that the effect of applying similar morphological
segmentation on these languages in insignificant.
In order to test whether more data can substi-
tute segmentation even for Hebrew, we have ob-
tained by means of crawling and web queries a
larger (while potentially much more noisy) web-
based 2GB Hebrew corpus which is based on fo-
rum and news contents. Our goal was to estimate
which unsegmented corpus size (if any) can bring
similar performance (in terms of concept number,
size and quality). We gradually increased corpus
size and applied the concept acquisition algorithm
on this corpus. Finally, we have obtained similar,
nearly matching, results to our 130MB corpus for
a 1.2GB Hebrew subcorpus of the 2GB Hebrew
corpus. The results remain stable for 4 different
1.2GB subsets taken from the same 2GB corpus.
This suggests that while segmentation can be sub-
stituted with more data, it may take roughly x10
more data for Hebrew to obtain the same results
without segmentation as with it.
6 Summary
We presented a simple method for separating func-
tion word prefixes from words. The method re-
quires very little language-specific knowledge (the
prefixes), and it can be applied to any morpholog-
ically rich language. We showed that this segmen-
tation dramatically improves lexical acquisition in
Hebrew, where nearly ?10 data is required to ob-
tain the same number of concepts without segmen-
tation.
While in this paper we evaluated our framework
on the discovery of concepts, we have recently
proposed fully unsupervised frameworks for the
discovery of different relationship types (Davidov
et al, 2007; Davidov and Rappoport, 2008a; Davi-
dov and Rappoport, 2008b). Many of these meth-
ods are mostly based on function words, and may
greatly benefit from the proposed segmentation
framework.
References
Meni Adler, Michael Elhadad, 2006. An Unsupervised
Morpheme-Based HMM for Hebrew Morphological
Disambiguation. ACL ?06.
43
Roy Bar-Haim, Khalil Simaan, Yoad Winter, 2005.
Choosing an Optimal Architecture for Segmentation
and POS-Tagging of Modern Hebrew. ACL Work-
shop on Computational Approaches to Semitic Lan-
guages ?05.
Mathias Creutz, 2003. Unsupervised Segmentation of
Words Using Prior Distributions of Morph Length
and Frequency. ACL ?03.
Mathias Creutz and Krista Lagus, 2005. Unsuper-
vised Morpheme Segmentation and Morphology In-
duction from Text Corpora Using Morfessor 1.0.
In Computer and Information Science, Report A81,
Helsinki University of Technology.
Sajib Dasgupta and Vincent Ng, 2007. High Perfor-
mance, Language-Independent Morphological Seg-
mentation. NAACL/HLT ?07.
Dmitry Davidov, Ari Rappoport, 2006. Efficient
Unsupervised Discovery of Word Categories Us-
ing Symmetric Patterns and High Frequency Words.
ACL ?06.
Dmitry Davidov, Ari Rappoport, Moshe Koppel, 2007.
Fully Unsupervised Discovery of Concept-Specific
Relationships by Web Mining. ACL ?07.
Dmitry Davidov, Ari Rappoport, 2008a. Classification
of Semantic Relationships between Nominals Using
Pattern Clusters. ACL ?08.
Dmitry Davidov, Ari Rappoport, 2008b. Unsupervised
Discovery of Generic Relationships Using Pattern
Clusters and its Evaluation by Automatically Gen-
erated SAT Analogy Questions. ACL ?08.
Vera Demberg, 2007. A Language-Independent Un-
supervised Model for Morphological Segmentation.
ACL ?07.
Beate Dorow, Dominic Widdows, Katarina Ling, Jean-
Pierre Eckmann, Danilo Sergi, Elisha Moses, 2005.
Using Curvature and Markov Clustering in Graphs
for Lexical Acquisition and Word Sense Discrimi-
nation. MEANING ?05.
Samarth Keshava, Emily Pitler, 2006. A Simpler, Intu-
itive Approach to Morpheme Induction. In Proceed-
ings of 2nd Pascal Challenges Workshop, Venice,
Italy.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-
sama Emam, Hany Hassan, 2003. Language Model
Based Arabic Word Segmentation. ACL ?03.
Moshe Levinger, Uzzi Ornan, Alon Itai, 1995. Learn-
ing Morpho-Lexical Probabilities from an Untagged
Corpus with an Application to Hebrew. Comput.
Linguistics, 21:383:404.
Dekang Lin, 1998. Automatic Retrieval and Cluster-
ing of Similar Words. COLING ?98.
Uzzi Ornan, 2005. Hebrew in Latin Script. Lesonenu,
LXIV:137:151 (in Hebrew).
Patrick Pantel, Dekang Lin, 2002. Discovering Word
Senses from Text. SIGKDD ?02.
Patrick Pantel, Deepak Ravichandran, Eduard Hovy,
2004. Towards Terascale Knowledge Acquisition.
COLING ?04.
Fernando Pereira, Naftali Tishby, Lillian Lee, 1993.
Distributional Clustering of English Words. ACL
?93.
Benjamin Snyder, Regina Bazilay, 2008. Unsuper-
vised Multilingual Learning for Morphological Seg-
mentation. ACL/HLT ?08.
Andreas Stolcke, 2002. SRILM ? an Extensible Lan-
guage Modeling Toolkit. ICSLP, pages 901-904,
Denver, Colorado.
Dominic Widdows, Beate Dorow, 2002. A Graph
Model for Unsupervised Lexical Acquisition. COL-
ING ?02.
44
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1310?1319,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Identifying and Following Expert Investors in Stock Microblogs
1Roy Bar-Haim, 1Elad Dinur, 1,2Ronen Feldman, 1Moshe Fresko and 1Guy Goldstein
1Digital Trowel, Airport City, Israel
2School of Business Administration, The Hebrew University of Jerusalem, Jerusalem, Israel
{roy, moshe}@digitaltrowel.com, ronen.feldman@huji.ac.il
Abstract
Information published in online stock invest-
ment message boards, and more recently in
stock microblogs, is considered highly valu-
able by many investors. Previous work fo-
cused on aggregation of sentiment from all
users. However, in this work we show that it
is beneficial to distinguish expert users from
non-experts. We propose a general framework
for identifying expert investors, and use it as a
basis for several models that predict stock rise
from stock microblogging messages (stock
tweets). In particular, we present two methods
that combine expert identification and per-user
unsupervised learning. These methods were
shown to achieve relatively high precision in
predicting stock rise, and significantly outper-
form our baseline. In addition, our work pro-
vides an in-depth analysis of the content and
potential usefulness of stock tweets.
1 Introduction
Online investment message boards such as Yahoo!
Finance and Raging Bull allow investors to share
trading ideas, advice and opinions on public com-
panies. Recently, stock microblogging services such
as StockTwits (which started as a filtering service
over the Twitter platform) have become very popu-
lar. These forums are considered by many investors
as highly valuable sources for making their trading
decisions.
This work aims to mine useful investment in-
formation from messages published in stock mi-
croblogs. We shall henceforth refer to these mes-
sages as stock tweets. Ultimately, we would like to
transform those tweets into buy and sell decisions.
Given a set of stock-related messages, this process
typically comprises two steps:
1. Classify each message as ?bullish? (having a
positive outlook on the stock), ?bearish? (hav-
ing a negative outlook on the stock), or neutral.
2. Make trading decisions based on these message
classifications.
Previous work on stock investment forums and
microblogs usually regarded the first step (message
classification) as a sentiment analysis problem, and
aligned bullish with positive sentiment and bearish
with negative sentiment. Messages were classified
by matching positive and negative terms from sen-
timent lexicons, learning from a hand-labeled set of
messages, or some combination of the two (Das and
Chen, 2007; Antweiler and Frank, 2004; Chua et al,
2009; Zhang and Skiena, 2010; Sprenger andWelpe,
2010). Trading decisions were made by aggregating
the sentiment for a given stock over all the tweets,
and picking stocks with strongest sentiment signal
(buying the most bullish stocks and short-selling the
most bearish ones).
Sentiment aggregation reflects the opinion of the
investors community as a whole, but overlooks the
variability in user expertise. Clearly, not all investors
are born equal, and if we could tell experts from non-
experts, we would reduce the noise in these forums
and obtain high-quality signals to follow. This pa-
per presents a framework for identifying experts in
stock microblogs by monitoring their performance
in a training period. We show that following the ex-
perts results in more precise predictions.
1310
Based on the expert identification framework, we
experiment with different methods for deriving pre-
dictions from stock tweets. While previous work
largely aligned bullishness with message sentiment,
our in-depth content analysis of stock tweets (to be
presented in section 2.2) suggests that this view is
too simplistic. To start with, one important dif-
ference between bullishness/bearishness and posi-
tive/negative sentiment is that while the former rep-
resents belief about the future, the latter may also
refer to the past or present. For example, a user re-
porting on making profit from a buying stock yester-
day and selling it today is clearly positive about the
stock, but does not express any prediction about its
future performance. Furthermore, messages that do
refer to the future differ considerably in their signif-
icance. A tweet reporting on buying a stock by the
user conveys a much stronger bullishness signal than
a tweet that merely expresses an opinion. Overall, it
would seem that judging bullishness is far more elu-
sive than judging sentiment.
We therefore propose and compare two alterna-
tive approaches that sidestep the complexities of as-
sessing tweets bullishness. These two approaches
can be viewed as representing two extremes. The
first approach restricts our attention to the most ex-
plicit signals of bullishness and bearishness, namely,
tweets that report actual buy and sell transactions
performed by the user. In the second approach we
learn directly the relation between tweets content
and stock prices, following previous work on pre-
dicting stock price movement from factual sources
such as news articles (Lavrenko et al, 2000; Koppel
and Shtrimberg, 2004; Schumaker and Chen, 2010).
This approach poses no restrictions on the tweets
content and avoids any stipulated tweet classifica-
tion. However, user-generated messages are largely
subjective, and their correlation with the stock prices
depends on user?s expertise. This introduces much
noise into the learning process. We show that by
making the learning user-sensitive we can improve
the results substantially. Overall, our work illus-
trates the feasibility of finding expert investors, and
the utility of following them.
2 Stock Tweets
2.1 Stock Tweets Language
Stock tweets, as Twitter messages in general, are
short textual messages of up to 140 characters. They
are distinguished by having one or more references
to stock symbols (tickers), prefixed by a dollar sign.
For instance, the stock of Apple, Inc. is referenced
as $AAPL. Two other noteworthy Twitter conven-
tions that are also found in stock tweets are hashtags,
user-defined labels starting with ?#?, and references
to other users, starting with ?@?. Table 1 lists some
examples of stock tweets.
As common with Twitter messages, stock tweets
are typically abbreviated and ungrammatical utter-
ances. The language is informal and includes many
slang expressions, many of which are unique to the
stock tweets community. Thus, many positive and
negative expressions common to stock tweets are not
found in standard sentiment lexicons. Their unique
language and terminology often make stock tweets
hard to understand for an outsider. Many words
are abbreviated and appear in several non-standard
forms. For example, the word bought may also ap-
pear as bot or bght, and today may appear as 2day.
Stock tweets also contain many sentiment expres-
sions which may appear in many variations, e.g.
wow, woooow, woooooooow and so on. These char-
acteristics make the analysis of stock tweets a par-
ticularly challenging task.
2.2 Content Analysis
A preliminary step of this research was an exten-
sive data analysis, aimed to gain better understand-
ing of the major types of content conveyed in stock
tweets. First, we developed a taxonomy of tweet
categories while reading a few thousands of tweets.
Based on this taxonomy we then tagged a sample
of 350 tweets to obtain statistics on the frequency
of each category. The sample contained only tweets
that mention exactly one ticker. The following types
of tweets were considered irrelevant:
? Tweets that express question. These tweets
were labeled as Question.
? Obscure tweets, e.g. ?$AAPL fat?, tweets
that contain insufficient information (e.g.
?http://url.com $AAPL?) and tweets that seem
1311
Example %
Fact
News $KFRC: Deutsche Bank starts at Buy 14.3%
Chart Pattern $C (Citigroup Inc) $3.81 crossed its 2nd Pivot Point Support
http://empirasign.com/s/x4c
10.9%
Trade bot back some $AXP this morning 12.9%
Trade Outcome Sold $CELG at 55.80 for day-trade, +0.90 (+1.6%)X 2.9%
Opinion
Speculation thinking of hedging my shorts by buying some oil. thinking of
buying as much $goog as i can in my IRA. but i need more doing,
less thinking.
4.0%
Chart Prediction http://chart.ly/wsy5ny $GS - not looking good for this one -
breaks this support line on volume will nibble a few short
12.9%
Recommendation $WFC if you have to own financials, WFC would be my choice.
http://fsc.bz/448 #WORDEN
1.7%
Sentiment $ivn is rocking 8.6%
Question $aapl breaking out but in this mkt should wait till close? 7.1%
Irrelevant $CLNE follow Mr. Clean $$ 24.9%
Table 1: Tweets categories and their relative frequencies
to contain no useful information (e.g ?Even
Steve Jobs is wrong sometimes... $AAPL
http://ow.ly/1Tw0Z?). These tweets were la-
beled Irrelevant.
The rest of the tweets were classified into two major
categories: Facts and Opinions.
Facts can be divided into four main subcategories:
1. News: such tweets are generally in the form of
a tweeted headline describing news or a current
event generally drawn from mass media. As
such they are reliable but, since the information
is available in far greater detail elsewhere, their
added value is limited.
2. Chart Pattern: technical analysis aims to pro-
vide insight into trends and emerging patterns
in a stock?s price. These tweets describe pat-
terns in the stock?s chart without the inclusion
of any predicted or projected movement, an im-
portant contrast to Chart Prediction, which is
an opinion tweet described below. Chart pat-
tern tweets, like news, are a condensed form of
information already available through more in-
depth sources and as such their added value is
limited.
3. Trade: reports an actual purchase or sale of a
stock by the user. We consider this as the most
valuable form of tweet.
4. Trade Outcome: provides details of an ?inverse
trade?, the secondary trade to exit the initial
position along with the outcome of the over-
all trade (profit/loss). The value of these tweets
is debatable since although they provide details
of a trade, they generally describe the ?exit?
transaction. This creates a dilemma for ana-
lysts since traders will often exit not because
of a perceived change in the stock?s potential
but as a result of many short-term trading ac-
tivities. For this reason trade outcome provides
a moderate insight into a user?s position which
should be viewed with some degree of caution.
Opinions can also be divided into four main subcat-
egories:
1. Speculation: provides individual predictions of
future events relating to a company or actions
of the company. These are amongst the least
reliable categories, as the individual user is typ-
ically unable to justify his or her insight into the
predicted action.
2. Chart Prediction: describes a user?s prediction
of a future chart movement based on technical
analysis of the stock?s chart.
3. Recommendation: As with analyst recommen-
dations, this category represents users who
summarize their understanding and insight into
1312
a stock with a simple and effective recommen-
dation to take a certain course of action with
regard to a particular share. Recommendation
is the less determinate counterpart to Trade.
4. Sentiment: These tweets express pure senti-
ment toward the stock, rather than any factual
content.
Table 1 shows examples for each of the tweet cate-
gories, as well as their relative frequency in the ana-
lyzed sample.
3 An Expert Finding Framework
In this section we present a general procedure for
finding experts in stock microblogs. Based on this
procedure, we will develop in the next sections sev-
eral models for extracting reliable trading signals
from tweets.
We assume that a stock tweet refers to exactly one
stock, and therefore there is a one-to-one mapping
between tweets and stocks. Other tweets are dis-
carded. We define expertise as the ability to pre-
dict stock rise with high precision. Thus, a user is
an expert if a high percentage of his or her bullish
tweets is followed by a stock rise. In principle, we
could analogously follow bearish tweets, and see if
they are followed by a stock fall. However, bearish
tweets are somewhat more difficult to interpret: for
example, selling a share may indicate a negative out-
look on the stock, but it may also result from other
considerations, e.g. following a trading strategy that
holds the stock for a fixed period (cf. the discussion
on Trade Outcome tweets in the previous section).
We now describe a procedure that determines
whether a user u is an expert. The procedure re-
ceives a training set T of tweets posted by u, where
each tweet is annotated with its posting time. It is
also given a classifier C, which classifies each tweet
as bullish or not bullish (either bearish or neutral).
The procedure first applies the classifier C to iden-
tify the bullish tweets in T . It then determines the
correctness of each bullish tweet. Given a tweet t,
we observe the price change of the stock referenced
by t over a one day period starting at the next trading
day. The exact definition of mapping tweets to stock
prices is given in section 5.1. A one-day holding
period was chosen as it was found to perform well
in previous works on tweet-based trading (Zhang
and Skiena, 2010; Sprenger and Welpe, 2010), in
particular for long positions (buy transactions). A
bullish tweet is considered correct if it is followed
by a stock rise, and as incorrect otherwise1. Given a
set of tweets, we define its precision as the percent-
age of correct tweets in the set. Let Cu, Iu denote
the number of correct and incorrect bullish tweets
of user u, respectively. The precision of u?s bullish
tweets is therefore:
Pu =
Cu
Cu + Iu
Let Pbl be the baseline precision. In this work we
chose the baseline precision to be the proportion of
tweets that are followed by a stock rise in the whole
training set (including all the users). This represents
the expected precision when picking tweets at ran-
dom. Clearly, if Pu ? Pbl then u is not an expert.
If Pu > Pbl, we apply the following statistical test
to assess whether the difference is statistically sig-
nificant. First, we compute the expected number of
correct and incorrect transactions Cbl, Ibl according
to the baseline:
Cbl = Pbl ? (Cu + Iu)
Ibl = (1? Pbl)? (Cu + Iu)
We then compare the observed counts (Cu, Iu) to
the expected counts (Cbl, Ibl), using Pearson?s Chi-
square test. Since it is required for this test that
Cbl and Ibl are at least 5, cases that do not meet
this requirement are discarded. If the resulting p-
value satisfies the required significance level ?, then
u is considered an expert. In this work we take
? = 0.05. Note that since the statistical test takes
into account the number of observations, it will re-
ject cases where the number of the observations is
very small, even if the precision is very high. The
output of the procedure is a classification of u as
expert/non-expert, as well as the p-value (for ex-
perts). The expert finding procedure is summarized
in Algorithm 1.
In the next two sections we propose several alter-
natives for the classifier C.
1For about 1% of the tweets the stock price did not change
in the next trading day. These tweets are also considered correct
throughout this work.
1313
Algorithm 1 Determine if a user u is an expert
Input: set of tweets T posted by u, bullishness
classifier C, baseline probability Pbl, significance
level ?
Output: NON-EXPERT/(EXPERT, p-value)
Tbullish ? tweets in T classified by C as bullish
Cu ? 0 ; Iu ? 0
for each t ? Tbullish do
if t is followed by a stock rise then
Cu++
else
Iu++
end if
end for
Pu = CuCu+Iuif Pu ? Pbl then
return NON-EXPERT
else
Cbl ? Pbl ? (Cu + Iu)
Ibl ? (1? Pbl)? (Cu + Iu)
p? ChiSquareTest(Cu, Iu, Cbl, Ibl)
if p > ? then
return NON-EXPERT
else
return (EXPERT, p)
end if
end if
4 Following Explicit Transactions
The first approach we attempt for classifying bullish
(and bearish) tweets aims to identify only tweets that
report buy and sell transactions (that is, tweets in
the Trade category). According to our data analysis
(reported in section 2.2), about 13% of the tweets
belong to this category. There are two reasons to
focus on these tweets. First, as we already noted,
actual transactions are clearly the strongest signal
of bulishness/bearishness. Second, the buy and sell
actions are usually reported using a closed set of
expressions, making these tweets relatively easy to
identify. A few examples for buy and sell tweets are
shown in Table 2.
While buy and sell transactions can be captured
reasonably well by a relatively small set of patterns,
the examples in Table 2 show that stock tweets have
sell sold sum $OMNI 2.14 +12%
buy bot $MSPD for earnings testing
new indicator as well.
sell Out 1/2 $RIMM calls @ 1.84
(+0.81)
buy added to $joez 2.56
buy I picked up some $X JUL 50 Puts@
3.20 for gap fill play about an hour
ago.
buy long $BIDU 74.01
buy $$ Anxiously sitting at the bid on
$CWCO @ 11.85 It seems the ask
and I are at an impasse. 20 min of
this so far. Who will budge? (not
me)
buy In 300 $GOOG @ 471.15.
sell sold $THOR 41.84 for $400 the
FreeFactory is rocking
sell That was quick stopped out $ICE
sell Initiated a short position in $NEM.
Table 2: Buy and sell tweets
their unique language for reporting these transac-
tions, which must be investigated in order to come
by these patterns. Thus, in order to develop a clas-
sifier for these tweets, we created a training and test
corpora as follows. Based on our preliminary anal-
ysis of several thousand tweets, we composed a vo-
cabulary of keywords which trade tweets must in-
clude2. This vocabulary contained words such as in,
out, bot, bght, sld and so on. Filtering out tweets that
match none of the keywords removed two thirds of
the tweets. Out of the remaining tweets, about 5700
tweets were tagged. The training set contains about
3700 tweets, 700 of which are transactions. The test
set contains about 2000 tweets, 350 of which are
transactions.
Since the transaction tweets can be characterized
by a closed set of recurring patterns, we developed
a classifier that is based on a few dozens of man-
ually composed pattern matching rules, formulated
as regular expressions. The classifier works in three
stages:
1. Normalization: The tweet is transformed into
a canonical form. For example, user name
2That is, we did not come across any trade tweet that does
not include at least one of the keywords in the large sample we
analyzed, so we assume that such tweets are negligible.
1314
Dataset Transaction P R F1
Train Buy 94.0% 84.0% 0.89Sell 96.0% 83.0% 0.89
Test Buy 85.0% 70.0% 0.77Sell 88.5% 79.0% 0.84
Table 3: Results for buy/sell transactition classifier. Pre-
cision (P), Recall (R), and F-measure (F1) are reported.
is transformed into USERNAME; ticker name
is transformed into TICKER; buy, buying,
bought, bot, bght are transformed into BUY,
and so on.
2. Matching: Trying to match one of the buy/sell
patterns in the normalized tweet.
3. Filtering: Filtering out tweets that match ?dis-
qualifying? patterns. The simplest examples
are a tweet starting with an ?if? or a tweet con-
taining a question mark.
The results of the classifier on the train and test set
are summarized in Table 3. The results show that
our classifier identifies buy/sell transactions with a
good precision and a reasonable recall.
5 Unsupervised Learning from Stock
Prices
The drawback of the method presented in the pre-
vious section is that it only considers a small part
of the available tweets. In this section we propose
an alternative method, which considers all the avail-
able tweets, and does not require any tagged corpus
of tweets. Instead, we use actual stock price move-
ments as our labels.
5.1 Associating Tweets with Stock Prices
We used stock prices to label tweets as follows. Each
tweet message has a time stamp (eastern time), indi-
cating when it was published. Our policy is to buy
in the opening price of the next trading day (PB),
and sell on the opening price of the following trad-
ing day (PS). Tweets that are posted until 9:25 in the
morning (market hours begin at 9:30) are associated
with the same day, while those are posted after that
time are associated with the next trading date.
5.2 Training
Given the buy and sell prices associated with each
tweet, we construct positive and negative training
examples as follows: positive examples are tweets
where PS?PBPB ? 3%, and negative examples are
tweets where PS?PBPB ? ?3%.We used the SVM-light package (Joachims,
1999), with the following features:
? The existence of the following elements in the
message text:
? Reference to a ticker
? Reference to a user
? URL
? Number
? Hashtag
? Question mark
? The case-insensitive words in the message after
dropping the above elements.
? The 3, 4, 5 letter prefixes of each word.
? The name of the user who authored the tweet,
if it is a frequent user (at least 50 messages in
the training data). Otherwise, the user name is
taken to be ?anonymous?.
? Whether the stock price was up or down 1% or
more in the previous trading day.
? 2, 3, 4-word expressions which are typical to
tweets (that is, their relative frequency in tweets
is much higher than in general news text).
6 Empirical Evaluation
In this section we focus on the empirical task of
tweet ranking: ordering the tweets in the test set ac-
cording to their likelihood to be followed by a stock
rise. This is similar to the common IR task of rank-
ing documents according to their relevance. A per-
fect ranking would place all the correct tweets before
all the incorrect ones.
We present several ranking models that use the
expert finding framework and the bullishness classi-
fication methods discussed in the previous sections
as building blocks. The performance of these mod-
els is evaluated on the test set. By considering the
1315
precision at various points along the list of ranked
tweets, we can compare the precision-recall trade-
offs achieved by each model.
Before we discuss the ranking models and the em-
pirical results, we describe the datasets used to train
and test these models.
6.1 Datasets
Stock tweets were downloaded from the StockTwits
website3, during two periods: from April 25, 2010
to November 1, 2011, and from December 14, 2010
to February 3, 2011. A total of 700K tweets mes-
sages were downloaded. Tweets that do not contain
exactly one stock ticker (traded in NYSE or NAS-
DAQ) were filtered out. The remaining 340K tweets
were divided as follows:
? Development set: April 25, 2010 to August 31,
2010: 124K messages
? Held out set: September 1, 2010 to November
1, 2010: 110K messages
? Test set: December 14, 2010 to February 3,
2011: 106K messages
We consider the union of the development and held
out sets as our training set.
6.2 Ranking Models
6.2.1 Joint-All Model
This is our baseline model, as it does not attempt
to identify experts. It learns a single SVM model
as described in Section 5 from all the tweets in the
training set. It then applies the SVM model to each
tweet in the test set, and ranks them according to the
SVM classification score.
6.2.2 Transaction Model
This model finds expert users in the training set
(Algorithm 1), using the buy/sell classifier described
in Section 4. Tweets classified as buy are considered
bullish, and the rest are considered non-bullish. Ex-
pert users are ranked according to their p value (in
ascending order). The same classifier is then applied
to the tweets of the expert users in the test set. The
tweets classified as bullish are ordered according to
the ranking of their author (first all the bullish tweets
3stocktwits.com
of the highest-ranked expert user, then all the bullish
tweets of the expert ranked second, and so on).
6.2.3 Per-User Model
The joint all model suffers from the tweets of
non-experts twice: at training time, these tweets in-
troduce much noise into the training of the SVM
model. At test time, we follow these unreliable
tweets along with the more reliable tweets of the ex-
perts. The per-user model addresses both problems.
This model learns from the development set a sep-
arate SVM model Cu for each user u, based solely
on the user?s tweets. We then optimize the clas-
sification threshold of the learnt SVM model Cu
as follows. Setting the threshold to ? results in a
new classifier Cu,?. Algorithm 1 is applied to u?s
tweets in the held-out set (denoted Hu), using the
classifier Cu,?. For the ease of presentation, we de-
fine ExpertPValue(Hu, Cu,?,Pbl,?) as a function that
calls Algorithm 1 with the given parameters, and re-
turns the obtained p-value if u is an expert and 1
otherwise. We search exhaustively for the thresh-
old ?? for which this function is minimized (in other
words, the threshold that results in the best p-value).
The threshold of Cu is then set to ??, and the user?s
p-value is set to the best p-value found. If u is a
non-expert for all of the attempted ? values then u is
discarded. Otherwise, u is identified as an expert.
The rest of the process is similar to the transac-
tion model: the tweets of each expert u in the test
set are classified using the optimized per-user clas-
sifier Cu. The final ranking is obtained by sorting
the tweets that were classified as bullish according
to the p-value of their author. The per-user ranking
procedure is summarized in Algorithm 2.
6.2.4 Joint-Experts Model
The joint experts model makes use of the experts
identified by the per-user model, and builds a sin-
gle joint SVM model from the tweets of these users.
This results in a model that is trained on more exam-
ples than in the previous per-user method, but unlike
the joint all method, it learns only from high-quality
users. As with the joint all model, test tweets are
ranked according to the SVM?s score. However, the
model considers only the tweets of expert users in
the test set.
1316








 	

	

	

       




	

	
Figure 1: Empirical model comparison
Algorithm 2 Per-user ranking model
Input: dev. set D, held-out set H, test set S , base-
line probability Pbl, significance level ?
Output: A ranked listR of tweets in S
// Learning from the training set
E ? ? // set of expert users
for each user u do
Du ? u?s tweets in D
Cu ? SVM classifier learnt from Du
Hu ? u?s tweets inH
?? = argmin? ExpertPValue(Hu, Cu,?,Pbl,?)
Cu ? Cu,??
pu ?ExpertPValue(Hu, Cu,??,Pbl,?)if pu ? ? then
add u to E
end if
end for
// Classifying and ranking the test set
for each user u ? E do
Sbullish,u ? u?s tweets in S that were classified
as bullish by Cu
end for
R ? tweets in ?u Sbullish,u sorted by pureturn R
6.3 Results
Figure 1 summarizes the results obtained for the
various models. Each model was used to rank the
tweets according to the confidence that they predict
a positive stock price movement. Each data point
corresponds to the precision obtained for the first k
tweets ranked by the model, and the results for vary-
ing k values illustrate the precision/recall tradeoff of
the model. These data points were obtained as fol-
lows:
? For methods that learn a single SVM model
(joint all and joint experts), the graph was ob-
tained by decreasing the threshold of the SVM
classifier, at fixed intervals of 0.05. For each
threshold value, k is the number of tweets clas-
sified as bullish by the model.
? For methods that rank the users by their p value
and order the tweets accordingly (transaction
and per user), the i-th data point corresponds
to the cumulative precision for the tweets clas-
sified as bullish by the first i users. For the per
user method we show the cumulative results for
the first 20 users. For the transaction method
we show all the users that were identified as ex-
perts.
The random line is our baseline. It shows the ex-
pected results for randomly ordering the tweets in
the test set. The expected precision at any point is
equal to the percentage of tweets in the test set that
were followed by a stock rise, which was found to
be 51.4%.
We first consider the joint all method, which
learns a single model from all the tweets. The only
1317
Correct Incorrect P p
87 46 65.4 0.001
142 86 62.3 0.001
162 103 61.1 0.002
220 158 58.2 0.008
232 168 58.0 0.008
244 176 58.1 0.006
299 229 56.6 0.016
335 255 56.8 0.009
338 268 55.8 0.031
344 269 56.1 0.019
419 346 54.8 0.062
452 387 53.9 0.152
455 389 53.9 0.145
479 428 52.8 0.395
481 430 52.8 0.398
487 435 52.8 0.388
675 564 54.5 0.030
683 569 54.6 0.026
690 573 54.6 0.022
720 591 54.9 0.011
Table 4: Per user model: cumulative results for first 20
users. The table lists the number of correct and incorrect
tweets, the precision P and the significance level p.
per-user information available to this model is a fea-
ture fed to the SVM classifier, which, as we found,
does not contribute to the results. Except for the
first 58 tweets, which achieved precision of 55%,
the precision quickly dropped to a level of around
52%, which is just a little better than the random
baseline. Next, we consider the transaction configu-
ration, which is based on detecting buy transactions.
Only 10 users were found to be experts according to
this method, and in the test period these users had a
total of 173 tweets. These 173 tweets achieve good
precision (57.1% for the first 161 tweets, and 54.9%
for the first 173 tweets). However this method re-
sulted in a low number of transactions. This happens
because it is able to utilize only a small fraction of
the tweets (explicit buy transactions).
Remarkably, per user and joint experts, the two
methods which rely on identifying the experts via
unsupervised learning are by far the best methods.
Both models seem to have comparable performance,
where the results of the join experts model are some-
what smoother, as expected. Table 4 shows cumu-
lative results for the first 20 users in the per-user
model. The results show that this model achieves
good precision for a relatively large number of
tweets, and for most of the data points reported in the
table the results significantly outperform the base-
line (as indicated by the p value). Overall, these re-
sults show the effectiveness of our methods for find-
ing experts through unsupervised learning.
7 Related Work
A growing body of work aims at extracting senti-
ment and opinions from tweets, and exploit this in-
formation in a variety of application domains. Davi-
dov et al (2010) propose utilizing twitter hash-
tag and smileys to learn enhanced sentiment types.
O?Connor et al (2010) propose a sentiment detec-
tor based on Twitter data that may be used as a re-
placement for public opinion polls. Bollen et al
(2011) measure six different dimensions of public
mood from a very large tweet collection, and show
that some of these dimensions improve the predica-
tion of changes in the Dow Jones Industrial Average
(DJIA).
Sentiment analysis of news articles and financial
blogs and their application for stock prediction were
the subject of several studies in recent years. Some
of these works focus on document-level sentiment
classification (Devitt and Ahmad, 2007; O?Hare et
al., 2009). Other works also aimed at predicting
stock movement (Lavrenko et al, 2000; Koppel
and Shtrimberg, 2004; Schumaker and Chen, 2010).
All these methods rely on predefined sentiment lex-
icons, manually classified training texts, or their
combination. Lavrenko et al (2000), Koppel and
Shtrimberg (2004), and Schumaker and Chen (2010)
exploit stock prices for training, and thus save the
need in supervised learning.
Previous work on stock message boards include
(Das and Chen, 2007; Antweiler and Frank, 2004;
Chua et al, 2009). (Sprenger andWelpe, 2010) is, to
the best of our knowledge, the first work to address
specifically stock microblogs. All these works take
a similar approach for classifying message bullish-
ness: they train a classifier (Na??ve Bayes, which Das
and Chen combined with additional classifiers and
a sentiment lexicon, and Chua et al presented im-
provement for) on a collection of manually labeled
messages (classified into Buy, Sell, Hold). Interest-
ingly, Chua et al made use of an Australian mes-
1318
sage board (HotCopper), where, unlike most of the
stock message boards, these labels are added by the
message author. Another related work is (Zhang and
Skiena, 2010), who apply lexicon-based sentiment
analysis to several sources of news and blogs, in-
cluding tweets. However, their data set does not in-
clude stock microblogs, but tweets mentioning the
official company name.
Our work differs from previous work on stock
messages in two vital aspects. Firstly, these works
did not attempt to distinguish between experts and
non-expert users, but aggregated the sentiment over
all the users when studying the relation between sen-
timent and the stock market. Secondly, unlike these
works, our best-performing methods are completely
unsupervised, and require no manually tagged train-
ing data or sentiment lexicons.
8 Conclusion
This paper investigated the novel task of finding ex-
pert investors in online stock forums. In particular,
we focused on stock microblogs. We proposed a
framework for finding expert investors, and exper-
imented with several methods for tweet classifica-
tion using this framework. We found that combin-
ing our framework with user-specific unsupervised
learning allows us to predict stock price movement
with high precision, and the results were shown to be
statistically significant. Our results illustrate the im-
portance of distinguishing experts from non-experts.
An additional contribution of this work is an in-
depth analysis of stock tweets, which sheds light on
their content and its potential utility.
In future work we plan to improve the features of
the SVM classifier, and further investigate the use-
fulness of our approach for trading.
References
Werner Antweiler and Murray Z. Frank. 2004. Is all
that talk just noise? the information content of in-
ternet stock message boards. Journal of Finance,
59(3):1259?1294.
Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011.
Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1 ? 8.
Christopher Chua, Maria Milosavljevic, and James R.
Curran. 2009. A sentiment detection engine for in-
ternet stock message boards. In Proceedings of the
Australasian Language Technology Association Work-
shop 2009.
Sanjiv R. Das and Mike Y. Chen. 2007. Yahoo! for
Amazon: Sentiment extraction from small talk on the
Web. Management Science, 53(9):1375?1388.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10. Association for Computational Linguis-
tics.
Ann Devitt and Khurshid Ahmad. 2007. Sentiment
polarity identification in financial news: A cohesion-
based approach. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 984?991.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scholkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning. MIT-Press.
Moshe Koppel and Itai Shtrimberg. 2004. Good news
or bad news? Let the market decide. In Proceedings
of the AAAI Spring Symposium on Exploring Attitude
and Affect in Text: Theories and Applications.
Victor Lavrenko, Matt Schmill, Dawn Lawrie, Paul
Ogilvie, David Jensen, and James Allan. 2000. Min-
ing of concurrent text and time series. In Proceedings
of the 6th ACM SIGKDD Int?l Conference on Knowl-
edge Discovery and Data Mining Workshop on Text
Mining.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010. From
tweets to polls: Linking text sentiment to public opin-
ion time series. In Proceedings of the International
AAAI Conference on Weblogs and Social Media.
Neil O?Hare, Michael Davy, Adam Bermingham, Paul
Ferguson, Pvraic Sheridan, Cathal Gurrin, and Alan F
Smeaton. 2009. Topic-dependent sentiment analysis
of financial blogs. In TSA?09 - 1st International CIKM
Workshop on Topic-Sentiment Analysis for Mass Opin-
ion Measurement.
Robert P. Schumaker and Hsinchun Chen. 2010. A dis-
crete stock price prediction engine based on financial
news. Computer, 43:51?56.
Timm O. Sprenger and Isabell M. Welpe. 2010. Tweets
and trades: The information content of stock mi-
croblogs. Technical report, TUM School of Manage-
ment, December. working paper.
Wenbin Zhang and Steven Skiena. 2010. Trading
strategies to exploit blog and news sentiment. In
ICWSM?10.
1319
