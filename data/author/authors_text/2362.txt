Abstraction Summarization for Managing the Biomedical  
Research Literature 
 
 
Marcelo Fiszman Thomas C. Rindflesch Halil Kilicoglu 
Lister Hill National Center for Biomedical Communications 
National Library of Medicine 
Bethesda, MD 20894 
{fiszman|tcr|halil}@nlm.nih.gov 
 
 
Abstract 
We explore a semantic abstraction approach 
to automatic summarization in the biomedical 
domain. The approach relies on a semantic 
processor that functions as the source inter-
preter and produces a list of predications. A 
transformation stage then generalizes and 
condenses this list, ultimately generating a 
conceptual condensate for a disorder input 
topic. The final condensate is displayed in 
graphical form. We provide a set of principles 
for the transformation stage and describe the 
application of this approach to multidocument 
input. Finally, we examine the characteristics 
and quality of the condensates produced. 
1 Introduction 
Several approaches to text-based information manage-
ment applications are being pursued, including word-
based statistical processing and those depending on 
string matching, syntax, or semantics. Statistical sys-
tems have enjoyed considerable success for information 
retrieval, especially using the vector space model (Sal-
ton et al, 1975). Since the SIR system (Raphael, 1968), 
some have felt that automatic information management 
could best be addressed using semantic information. 
Subsequent research (Schank, 1975; Wilks, 1976) ex-
panded this paradigm. More recently, a number of ex-
amples of knowledge-based applications show 
considerable promise. These include systems for ma-
chine translation (Viegas et al, 1998), question answer-
ing, (Harabagiu et al, 2001; Clark et al, 2003), and 
information retrieval (Mihalcea and Moldovan, 2000).  
In the biomedical domain, the MEDLINE? biblio-
graphic database provides opportunities for keeping 
abreast of the research literature. However, the large 
size of this online resource presents potential challenges 
to the user. Query results often include hundreds or 
thousands of citations (including title and abstract). 
Automatic summarization offers potential help in man-
aging such results; however, the most popular approach, 
extraction, faces challenges when applied to multi-
document summarization (McKeown et al, 2001). 
Abstraction summarization offers an attractive alter-
native for managing citations resulting from MEDLINE 
searches. We present a knowledge-rich abstraction ap-
proach that depends on underspecified semantic inter-
pretation of biomedical text. As an example, a graphical 
representation (Batagelj, 2003) of the semantic predica-
tions serving as a summary (or conceptual condensate) 
from our system is shown in Figure 1. The input text 
was a MEDLINE citation with title ?Gastrointestinal 
tolerability and effectiveness of rofecoxib versus 
naproxen in the treatment of osteoarthritis: a random-
ized, controlled trial.? 
 
Figure 1. Semantic abstraction summarization 
Our semantic interpreter and the abstraction sum-
marizer based on it both draw on semantic information 
from the Unified Medical Language System? (UMLS),? 
a resource for structured knowledge in the biomedical 
domain. After introducing the semantic interpreter, we 
describe the transformation phase of our paradigm, dis-
cussing principles that depend on semantic notions in 
order to condense the semantic predications represent-
ing the content of text. Initially, this process was applied 
to summarizing single documents. We discuss its adap-
tation to multidocument input, specifically to the set of 
citations resulting from a query to the MEDLINE data-
base. Although we have not yet formally evaluated the 
effectiveness of the resulting condensate, we discuss its 
characteristics and possibilities as both an indicative and 
informative summary. 
2 
2.1 
2.2 
Background 
Lexical Semantics  
Research in lexical semantics (Cruse, 1986) provides 
insight into the interaction of reference and linguistic 
structure. In addition to paradigmatic lexical phenomena 
such as synonymy, hypernymy, and meronymy, diathe-
sis alternation (Levin and Rappaport Hovav, 1996), 
deep case (Fillmore, 1968), and the interaction of predi-
cational structure and events (Tenny and Pustejovsky, 
2000) are being investigated. Some of the consequences 
of research in lexical semantics, with particular attention 
to natural language processing, are discussed by Puste-
jovsky et al (1993) and Nirenburg and Raskin (1996). 
Implemented systems often draw on the information 
contained in WordNet (Fellbaum, 1998). 
In the biomedical domain, UMLS knowledge pro-
vides considerable support for text-based systems. 
(Burgun and Bodenreider (2001) compare the UMLS to 
WordNet.) The UMLS (Humphreys et al, 1998) con-
sists of three components: the Metathesaurus,? Seman-
tic Network (McCray, 1993), and SPECIALIST 
Lexicon (McCray et al, 1994). The Metathesaurus is at 
the core and contains more than 900,000 concepts com-
piled from more than sixty controlled vocabularies. 
Many of these have hierarchical structure, and some 
contain meronymic information in addition to hy-
pernymy. Editors combine terms in the constituent vo-
cabularies into a set of synonyms (cf. WordNet?s 
synsets), which constitutes a concept. One term in this 
set is called the ?preferred name? and is used as the 
concept name, as shown in (1). 
(1) Concept: Dyspnea  Synonyms: Breath-
lessness, Shortness of breath, Breathless, Diffi-
culty breathing, Respiration difficulty, etc. 
In addition, each concept in the Metathesaurus is as-
signed at least one semantic type (such as ?Sign or 
Symptom? for (1)), which categorizes the concept in the 
biomedical domain. The semantic types available are 
drawn from the Semantic Network, in which they are 
organized hierarchically in two single-inheritance trees, 
one under the root ?Entity? and another under ?Event?.  
The Semantic Network also contains semantic 
predications with semantic types as arguments. The 
predicates are semantic relations relevant to the bio-
medical domain and are organized as subtypes of five 
classes, such as TEMPORALLY_RELATED_TO and  
FUNCTIONALLY_RELATED_TO.  Examples are shown in 
(2). 
(2)  ?Pharmacologic Substance? TREATS ?Disease 
or Syndrome?, ?Virus? CAUSES ?Disease or 
Syndrome? 
Lexical semantic information in the UMLS is dis-
tributed between the Metathesaurus and the Semantic 
Network. The Semantic Network stipulates permissible 
argument categories for classes of semantic predica-
tions, although it does not refer to deep case relations. 
The Metathesaurus encodes synonymy, hypernymy, and 
meronymy (especially for human anatomy). Synonymy 
is represented by including synonymous terms under a 
single concept. Word sense ambiguity is represented to 
some extent in the Metathesaurus. For example dis-
charge is represented by the two concepts in (3), with 
different semantic types. 
(3) Discharge, Body Substance: ?Body Substance? 
Patient Discharge: ?Health Care Activity? 
The SPECIALIST Lexicon contains orthographic in-
formation (such as spelling variants) and syntactic in-
formation, including inflections for nouns and verbs and 
sub-categorization for verbs. A suite of lexical access 
tools accommodate other phenomena, including deriva-
tional variation.  
 SemRep 
Our summarization system relies on semantic predica-
tions provided by SemRep (Rindflesch and Fiszman, 
2003), a program that draws on UMLS information to 
provide underspecified semantic interpretation in the 
biomedical domain (Srinivasan and Rindflesch, 2002; 
Rindflesch et al, 2000). Semantic interpretation is based 
on a categorical analysis that is underspecified in that it 
is a partial parse (cf. McDonald, 1992). This analysis 
depends on the SPECIALIST Lexicon and the Xerox 
part-of-speech tagger  (Cutting et al, 1992) and pro-
vides simple noun phrases that are mapped to concepts 
in the UMLS Metathesaurus using MetaMap (Aronson, 
2001).  
The categorial analysis enhanced with Metathesau-
rus concepts and associated semantic types provides the 
basis for semantic interpretation, which relies on two 
components: a set of ?indicator? rules and an (under-
specified) dependency grammar. Indicator rules map 
between syntactic phenomena (such as verbs, nominali-
zations, and prepositions) and predicates in the Seman-
tic Network. For example, such rules stipulate that the 
preposition for indicates the semantic predicate TREATS 
in sumatriptan for migraine. The application of an indi-
cator rule satisfies the first of several necessary condi-
tions for the interpretation of a semantic predication.  
Argument identification is controlled by a partial 
dependency grammar. As is common in such grammars, 
a general principle disallows intercalated dependencies 
(crossing lines). Further, a noun phrase may not be used 
as an argument in the interpretation of more than one 
semantic predication, without license. (Coordination 
and relativization license noun phrase reuse.) A final 
principle states that if a rule can apply it must apply. 
Semantic interpretation in SemRep is not based on 
the ?real? syntactic structure of the sentence; however  
linear order of the components of the partial parse is 
crucial. Argument identification rules are articulated for 
each indicator in terms of surface subject and object.  
For example, subjects of verbs are to the left and objects 
are to the right. (Passivization is accommodated before 
final interpretation.) There are also rules for preposi-
tions and several rules for arguments of nominaliza-
tions.  
The final condition on the interpretation of an asso-
ciative semantic predication is that it must conform to 
the appropriate relationship in the Semantic Network. 
For example, if a predication is being constructed on the 
basis of an indicator rule for TREATS, the syntactic ar-
guments identified by the dependency grammar must 
have been mapped to Metathesaurus concepts with se-
mantic types that conform to the semantic arguments of 
TREATS in the Semantic Network, such as ?Pharma-
cologic Substance? and ?Disease or Syndrome?. Hy-
pernymic propositions are further controlled by 
hierarchical information in the Metathesaurus (Rind-
flesch and Fiszman, 2003). 
In processing the sentence in (4), SemRep first con-
structs the partial categorical representation given 
schematically in (5).  This is enhanced with semantic 
information from the Metathesaurus as shown in (6), 
where the corresponding concept for each relevant noun 
phrase is shown, along with its semantic type. The final 
semantic interpretation for  (4) is given in (7). 
 
(4) Mycoplasma pneumonia is an infection of the 
lung caused by Mycoplasma pneumoniae 
(5) [[Mycoplasma pneumonia] [is] [an infection] 
[of the lung] [caused] [by Mycoplasma pneumo-
niae]] 
(6) ?Mycoplasma pneumonia???Disease or Syn-
drome? 
?Infection???Disease or Syndrome? 
?Lung???Body Part, Organ, or Organ Compo-
nent? 
?Mycoplasma pneumoniae???Bacterium? 
(7) Mycoplasma Pneumonia ISA Infection  
Lung LOCATION_OF Infection 
Lung LOCATION_OF Mycoplasma Pneumonia 
Mycoplasma pneumoniae CAUSES Infection  
Mycoplasma pneumoniae CAUSES My-
coplasma Pneumonia 
3 
3.1 
Automatic Summarization 
Automatic summarization is ?a reductive transformation 
of source text to summary text through content reduc-
tion, selection, and/or generalization on what is impor-
tant in the source? (Sparck Jones, 1999). Two 
paradigms are being pursued: extraction and abstraction 
(Hahn and Mani, 2000). Extraction concentrates on cre-
ating a summary from the actual text occurring in the 
source document, relying on notions such as frequency 
of occurrence and cue phrases to identify important in-
formation. 
Abstraction, on the other hand, relies either on lin-
guistic processing followed by structural compaction 
(Mani et al, 1999) or on interpretation of the source text 
into a semantic representation, which is then condensed 
to retain only the most important information asserted in 
the source. The semantic abstraction paradigm is attrac-
tive due to its ability to manipulate information that may 
not have been explicitly articulated in the source docu-
ment. However, due to the challenges in providing se-
mantic representation, semantic abstraction has not been 
widely pursued, although the TOPIC system (Hahn and 
Reimer, 1999) is a notable exception.  
Semantic Abstraction Summarization 
We are devising an approach to automatic summariza-
tion in the semantic abstraction paradigm, relying on 
SemRep for semantic interpretation of source text. The 
transformation stage that condenses these predications is 
guided by principles articulated in terms of frequency of 
occurrence as well as lexical semantic phenomena.  
We do not produce a textual summary; instead, we 
present the disorder condensates in graphical format. 
We first discuss the application of this approach to 
summarizing single documents (full text research arti-
cles on treatment of disease) and then consider its ex-
tension to multidocument input in the form of biomedi-
cal scientific abstracts directed at clinical researchers.  
The transformation stage takes as input a list of 
Sem
3.2 Transformation 
In the semantic abstraction paradigm the transformation 
b. Connectivity: Also include ?useful? additional 
c. Novelty: Do not include predications that the 
d. Saliency: Only include the most frequently oc-
Alth urrence (saliency) plays a 
rol
(relevance), a condensation process, identi-
fies
ders} 
 {Disorders} 
isorders} 
ders} 
s of 
sem
a generalization process 
and
ovelty) provides further condensation by 
elim
tion phase 
and
n these principles are applied to the semantic 
pre
 
Rep predications and a seed disorder concept. The 
output is a conceptual condensate for the input concept. 
Before transformation begins, predications are subjected 
to a focused word sense disambiguation filter. Branded 
drug names such as Advantage (Advantage brand of 
Imidacloprid) and Direct (Direct type of resin cement), 
which are ambiguous with the more common meaning 
of their names, are resolved to their non-pharmaceutical 
sense.  
stage condenses and generalizes, and in our approach 
these processes are based on four general principles: 
a. Relevance: Include predications on the topic of 
the summary 
predications 
user already knows 
curring predications 
ough frequency of occ
e in determining predications to be included in the 
summary, the other three principles depend crucially on 
lexical semantic information from the UMLS. These 
four principles guide the phases involved in creating a 
summary. 
Phase 1 
 predications on a given topic (in this study, disor-
ders) and is controlled by a semantic schema 
(Jacquelinet et al, 2003) for that topic. The schema is 
represented as a set of predications in which the predi-
cate is drawn from a relation in the UMLS Semantic 
Network and the arguments are represented as a ?do-
main? covering a class of concepts in the Metathesaurus 
(Disorders, for example).  
{Disorders} ISA {Disor
{Etiological process} CAUSES
{Treatment} TREATS  {Disorders} 
{Body location} LOCATION_OF {D
{Disorders} OCCURS_IN {Disorders} 
{Disorders} CO-OCCURS_WITH {Disor
Each domain for the schema is defined in term
antic categorization in the Semantic Network. For 
example {Disorders} is a subset of the semantic group 
Disorders (McCray et al, 2001) and contains the fol-
lowing semantic types: ?Disease or Syndrome?, ?Neo-
plastic Process?, ?Mental or Behavioral Dysfunction?, 
and ?Sign or Symptom?. Although the schema is not 
complete, it represents a substantial amount of what can 
be said about disorders. Predications produced by Sem-
Rep must conform to this schema in order to be in-
cluded in the conceptual condensate; such predications 
are called ?core predications.? 
Phase 2 (connectivity) is 
 identifies predications occurring in neighboring 
semantic space of the core. This is accomplished by 
retrieving all the predications that share an argument 
with one of the core predications. For example, from 
Naproxen TREATS Osteoarthritis, non-core predica-
tions such as Naproxen ISA NSAID are included in the 
condensate. 
Phase 3 (n
inating predications that have a generic argument, 
as determined by hierarchical depth in the Metathesau-
rus. Arguments occurring less than an empirically de-
termined distance from the root are considered too 
general to be useful, and predications containing them 
are eliminated. For example Pharmaceutical Prepara-
tions TREATS Migraine is not included in the conden-
sate for migraine because ?Pharmaceutical 
Preparations? was determined to be generic.  
Phase 4 (saliency) is the final transforma
 its operations are adapted from TOPIC?s (Hahn and 
Reimer, 1999) saliency operators. Frequency of occur-
rence for arguments, predicates, and predications are 
calculated, and those occurring more frequently than the 
average are kept in the condensate; others are elimi-
nated.  
Whe
dications produced by SemRep for a full-text article 
with 214 sentences (Lisse et al, 2003) concerned with 
comparing naproxen and rofecoxib for treating os-
teoarthritis, with respect to effectiveness and gastroin-
testinal tolerability, the resulting condensate is given in 
Figure 2. (The abstract for this article was summarized 
in Figure 1.) 
 
Figure 2. Semantic abstraction summarization 
of a journal article on osteoarthritis 
4 Multidocument Summarization  
The MEDLINE database, developed and maintained by 
the N  than 12 
million citations (dating from the 1960?s to the present) 
d at the same time retaining differences 
that
ramework for de-
term
Th
results 
for  
Eval ation, especially for 
mult ev et al, 2003). 
It is usually classified as intrinsic (measures the quality 
nd marked the predica-
tion
h 
set
ational Library of Medicine, contains more
drawn from nearly 4,600 journals in the biomedical do-
main. Access is provided by a statistical information 
retrieval system. Due to the size of the database, 
searches often retrieve large numbers of items. For ex-
ample, the query ?diabetes? returns 207,997 citations. 
Although users can restrict searches by language, date 
and publication type (as well as specific journals), re-
sults can still be large. For example, a query for treat-
ment (only) for diabetes, limited to articles published in 
2003 and having an abstract in English finds 3,621 
items; limiting this further to articles describing clinical 
trials still returns 390 citations. We describe the adapta-
tion of our abstraction summarization process to multi-
document input for managing the results of searches in 
MEDLINE.  
Extending summarization to multidocument input 
presents challenges in removing redundancies across 
documents an
 might be important. One issue is devising a frame-
work on which to compute similarities and differences 
across documents. Radev (2000) defines twenty-four 
relationships (such as equivalence, subsumption, and 
contradiction) that might apply at various structural lev-
els across documents. Sub-events (Daniel et al, 2003) 
and sub-topics (Saggion and Lapalme, 2002) also con-
tribute to the framework used for comparing documents 
in multidocument summarization.  
A particular challenge to multidocument summariza-
tion in the extraction paradigm is determining what 
parts of documents conform to the f
ining similarities and differences. A recent study 
(Kan et al, 2001) uses topic composition from text 
headers, but other studies in the extraction paradigm 
(Goldstein et al, 1999), extraction coupled with rhetori-
cal structural identification (Teufel and Moens, 2002), 
and syntactic abstraction paradigms use  different meth-
odologies (Barzilay et al, 1999; McKeown et al, 1999). 
Our semantic abstraction summarization system 
naturally extends to multidocument input with no modi-
fication from the system designed for single documents. 
e disorder schema serves as the framework for identi-
fying sub-topics, and predications retrieved across sev-
eral documents must conform to its structure. 
Informational equivalence (and redundancy) is com-
puted on this basis. For example, all predications that 
conform to the schema line {Treatment} TREATS  
{Disorders} constitute a representation of a subtopic in 
the disorder domain. Exact matches in this set constitute 
redundant information, and other types of relationships 
can be computed on the basis of partial matches. Al-
though we concentrate on similarities across documents, 
differences could be computed by examining predica-
tions that are not shared among citations.  
We have begun testing our system applied to the re-
sults of MEDLINE searches on disorders, concentrating 
on the most recent 300 citations retrieved. The 
 migraine are represented graphically in Figure 3. 
Traversing the predicates (arcs) in this condensate pro-
vides an informative summary of these citations. 
5 Evaluation and Results 
uation in automatic summariz
idocument input, is  daunting (Rad
of the summary as related to the source documents) or 
extrinsic (how the summary affects some other task). 
Since we do not have a gold standard to compare the 
final condensates against, we performed a linguistic 
evaluation on the quality of the condensates generated 
for four diseases: migraine, angina pectoris, Crohn?s 
disease, and pneumonia. The input for each summary 
was 300 MEDLINE citations. 
Table 1 presents evaluation results. The first author 
(MF) examined the source sentence that SemRep used 
to generate each predication a
s as either correct or incorrect. Precision was calcu-
lated as the total number of correct predications divided 
by the total number of predications in the condensate. 
We also measured the reduction (compression) for 
each of the four disorder concepts. In Table 1, ?Base? is 
the number of predications SemRep produced from eac
 of 300 citations. ?Final? is the number of predica-
tions left after the final transformation. Therefore, this is 
a compression ratio on the semantic space of predica-
tions, and is different from text compression in the tradi-
tional sense. 
 
Concept Base Final C I Precision 
Migraine 2485 102 72 30 71% 
A 3 8
ia 
   
ngina 2989 41 3  80% 
Crohn?s 3077 135 71 64 53% 
Pneumon 2694 28 27 1 96% 
Total 11245 306 203 103 66% 
Table 1. ts the f di se 
r  = r
In Crohn?s disease (with lowest precision) a single 
Sem
for 52% of th ocessing the 
sen
 Resul  for our sea concepts  
C = Co rect, I Incor ect 
Rep error type in  argument identification accounts 
e mistakes. For example in pr
tence 36 patients with inflammatory bowel disease 
(11 with ulcerative colitis and 25 with Crohn?s disease), 
the parenthesized material caused SemRep to incor-
rectly  returned ?Inflammatory Bowel Diseases CO- 
OCCURS_WITH Ulcerative Colitis? and ?Ulcerative 
Colitis predicate CO-OCCURS_WITH Crohn?s Dis-
ease.? Word sense ambiguity also contributed to a large 
number of errors.  
6 Content Characterization 
We sformation stage 
has and predications 
during the summarization process. SemRep produced 
S_IN; 
and
pes in the final 
con
he 
fin
nitially parsed, only 63 are represented in 
the
o far do not accommodate. Some of 
the
lusion and Future Directions 
We raction 
summarization that produces conceptual condensates for 
condensate to the text that produced them. We 
als
ional Library of Medicine 
Re
01. Effective mapping of biomedical 
LS Metathesaurus: The MetaMap pro-
Ba
edi-
Ba
e context of multi-document summa-
Bu
and semantic classes in WordNet and the 
Cl
 to text meaning processing. Pro-
Cr
ge.  
examined the effect that  the tran
on the distribution of predicates 
2,485 predications from 300 citations retrieved for mi-
graine. Of these, 1,638 are distributed over four predi-
cates in the disorder schema (327?TREATS; 148?ISA; 
180?LOCATION_OF; 54?CAUSES; 720?
OCCURS_IN; and 209?CO-OCCURS_WITH).  
After phases 1, 2, and 3 of the transformation proc-
ess, 311 predications remain (134?TREATS; 41?ISA; 
12?LOCATION_OF; 5?CAUSES; 68?OCCUR
 51?CO-OCCURS_WITH). This reduction is largely 
due to hierarchical pruning in phase 3. 
Phase 4 operations, based on frequency of occur-
rence pruning (saliency), further condensed the list, and 
the top three TREATS predication ty
densate are (13?Sumatriptan TREATS Migraine; 6?
Botulinum Toxins TREATS Migraine; and 6?feverfew 
extract TREATS Migraine). This list represents the fact 
that Sumatriptan is a popular treatment for migraine.  
Besides frequency, another way of looking at the 
predications is typicality (Kan et al, 2001), or distribu-
tion of predications across citations.  Looking at t
al condensate for migraine and focusing on TREATS, 
the most widely distributed predications are  ?Sumatrip-
tan TREATS Migraine,? which occurs in  ten citations; 
?Botulinum Toxins TREATS Migraine? (three cita-
tions); and  ?feverfew extract TREATS Migraine? (two 
citations). 
One can also view the final condensate from the per-
spective of citations, rather than predications. Of the 
300 citations i
 final condensate, one with six predications, one with 
five predications, three with four predications, and so 
on. It is tempting to hypothesize that more highly rele-
vant citations will have produced more predications, but 
this must be formally tested in the context of the user?s 
retrieval objective.  
An informal examination of the citations that con-
tributed to the final condensate for migraine revealed 
differences that we s
se, such as publication and study type, could be ad-
dressed outside of natural language processing with 
MEDLINE metadata. Others, including medication de-
livery system and target population of the disorder 
topic, are amenable to current processing either through 
extension of the disease schema or enhancements to 
SemRep.  
7 Conc
propose a framework based on semantic abst
disorder topics that are both indicative and informative. 
The approach uses a biomedical semantic processor as 
the source interpreter. After semantic interpretation, a 
series of transformations condense the predications pro-
duced, and a final condensate is displayed in graphical 
form. 
In the future, we would like to link the predications 
in the 
o plan to evaluate the effectiveness of this approach 
in retrieving useful articles for clinical researchers. Fi-
nally, we would like to investigate additional ways of 
visualizing the condensates.  
Acknowledgements The first author was supported 
by an appointment to the Nat
search Participation Program administered by the 
Oak Ridge Institute for Science and Education through 
an inter-agency agreement between the U.S. Department 
of Energy and the National Library of Medicine. 
References 
Aronson AR. 20
text to the UM
gram. Proceedings of the AMIA Symp, pp 17-21. 
tagelj AM. 2003. Pajek - Analysis and Visualization 
of Large Networks. In M. J?nger and P. Mutzel, 
tors, Graph Drawing Software. Springer Verlag, Ber-
lin, pp 77-103. 
rzilay R, McKeown KR, Elhadad M. 1999. Informa-
tion fusion in th
rization. Proceedings of the 37th Annual Meeting of 
the Association of Computational Linguistics, pp 
550-557. 
rgun A, Bodenreider O. 2001. Comparing terms, 
concepts, 
Unified Medical Language System. Proceedings of 
the NAACL Workshop on WordNet and Other Lexical 
Resources: Applications, Extensions and Customiza-
tions, pp 77-82. 
ark P, Harrison P, Thompson J. 2003. A knowledge-
driven approach
ceedings of the HLT-NAACL Workshop on Text 
Meaning, pp 1-6.  
use DA. 1986. Lexical semantics. Cambridge Univer-
sity Press, Cambrid
Cutting D, Kupiec J, Pedersen J, Sibun P. 1992. A prac-
tical part-of-speech tagger. Proceedings of the Third 
Da
marization. Proceedings of HLT-
Fi
. 
Fe
trac-
Ha
6.  
erators for 
Ha
 
Hu
 System: 
Ja
eux P. 2003. Developing the 
Ka
on for 
Le
ment realization. unpublished ms.  
B, 
M
M
d Intelligent Systems. Law-
M
ts. 
M
hniruk A, Pate V, 
M
tion Man-
M
. Proceedings of the Annual Symp Comput 
M
t 1):216-20. 
w Mexico State Univer-
Pu
 Linguistics, 19:331-58. 
e, pp 33-145. 
st ACL 
Ra
Celebi A, Liu D, Drabek E. 2003. Evaluation chal-
Conference on Applied Natural Language Process-
ing, pp 133-40. 
niel N, Radev D, Allison T. 2003. Sub-event based 
multi-document sum
NAACL Workshop on Text Summarization, pp 9-16. 
llmore CJ. 1968. The case for case. In E. Bach and 
RT. Harms, editors, Universals in Linguistic Theory
Holt Rinehart and Winston, New York, pp 1-88. 
llbaum C. 1998. WordNet: An Electronic Lexical Da-
tabase. The MIT Press, Cambridge, MA 
Goldstein J, Mittal V, Carbonell J, Kantrowitz M. 2000. 
Multi-document summarization by sentence ex
tion. Proceedings of the ANLP/NAACL Workshop on 
Automatic Summarization, pp 40-48.  
hn U, Mani I. 2000. The challenges of automatic 
summarization. Computer, 33(11):29-3
Hahn U, Reimer U. 1999. Knowledge-based text sum-
marization: salience and generalization op
knowledge base abstraction. In I. Mani and MT. 
Maybury, editors, Advances in Automatic Text Sum-
marization. MIT Press, Cambridge, pp 215-32.  
rabagiu S, Moldovan D, Pasca M, Mihalcea R, 
Surdeanu M, Bunescu R; Girju R, Rus V, Morarescu
P. 2001. The role of lexico-semantic feedback in 
open-domain textual question-answering. Proceed-
ings of the 39th Annual Meeting of the Association 
for Computational Linguistics, pp 274-81.  
mphreys BL, Lindberg DA, Schoolman HM, Barnett 
GO. 1998. The Unified Medical Language
An informatics research collaboration. J Am Med In-
form Assoc, 5(1):1-11. 
cquelinet C, Burgun A, Delamarre D, Strang N, Djab-
bour S, Boutin B, Le B
ontological foundations of a terminological system 
for end-stage diseases, organ failure, dialysis and 
transplantation. Int J Med Inf, 70(2-3):317-28. 
n M, McKeown KR, Klavans JL. 2001. Domain-
specific informative and indicative summarizati
information retrieval. Workshop on Text Summariza-
tion (DUC3).  
vin B, Rappaport Hovav M. 1996. From lexical se-
mantics to argu
Lisse JR, Perlman M, Johansson G, Shoemaker JR, 
Schechtman J, Skalky CS, Dixon ME, Polis A
Mollen AJ, Geba GP. 2003. Gastrointestinal toler-
ability and effectiveness of rofecoxib versus 
naproxen in the treatment of osteoarthritis: a random-
ized, controlled trial. Ann Intern Med, 139(7):539-46. 
ani I, Gates B, Bloedorn E. 1999. Improving summa-
ries by revising them. Proceedings of the 37th An-
nual Meeting of the Association of Computational 
Linguistics, pp 558-65. 
cDonald DD. 1992. Robust partial parsing through 
incremental, multi-algorithm processing. In PS Ja-
cobs, editor, Text-base
rence Erlbaum Associates, New Jersey, pp 83-99. 
cKeown KR, Klavans JL, Hazivassiloglou V, Barzi-
lay R., Eskin E. 1999. Towards multidocument sum-
marization by reformulation: progress and prospec
Proceedings of the Sixteenth National Conference on 
Artificial Intelligence, pp 453-60. 
cKeown HR, Chang, SF, Cimino J, Feiner SK, 
Friedman C, Gravano L, Hatzivassiloglou V, Johnson 
S,  Jordan DA, Klavans JL, Kus
Teufel S. 2001. PERSIVAL, a system for personal-
ized search and summarization over multimedia 
healthcare information. JCDL, pp 331-40. 
cCray AT. 1993. Representing biomedical knowledge 
in the UMLS Semantic Network. High-Performance 
Medical Libraries: Advances in Informa
agement for the Virtual Era. Meckler Publishing, pp 
45-55. 
cCray AT, Srinivasan S, Browne AC. 1994. Lexical 
methods for managing variation in biomedical termi-
nologies
Appl Med Care, pp:235-9. 
cCray AT, Burgun A, Bodenreider O. 2001. Aggre-
gating UMLS semantic types for reducing conceptual 
complexity.  Medinfo, 10(P
Mihalcea R, Moldovan D. 2000. Semantic indexing 
using WordNet senses. Proceedings of the ACL 
Workshop on IR and NLP. 
Nirenburg S, Raskin V. 1996. Ten choices for lexical 
semantics. Memoranda in Computer and Cognitive 
Science. MCCS-96-304. Ne
sity. 
stejovsky J., Bergler S, Anick P. 1993. Lexical se-
mantic techniques for corpus analysis. Computa-
tional
Raphael B. 1968. SIR: Semantic information retrieval. 
In Minsky, M. (ed.) Semantic Information Process-
ing. The MIT Press, Cambridg
Radev D. 2000. A Common theory of information fu-
sion from multiple text sources, step one: cross-
document structure. Proceedings of 1
SIGDIAL Workshop on Discourse and Dialogue. 
dev D, Teufel S, Saggion H, Lam W, Blitzer J, Qi H, 
lenges in large-scale multi-document summarization: 
the MEAD project. Proceedings of ACL. 
Rindflesch TC, Bean CA, Sneiderman CA. 2000. Ar-
gument identification for arterial branching predica-
tions asserted in cardiac catheterization reports. 
Proceedings of the AMIA Symp, pp 704-8. 
:462-77.  
Sal
Sri ring text mining 
Sp tomatic summarizing: factors 
Sc
Publishing Co, Amster-
Te
 Experiments with relevance and rhetorical 
Te
di-
Vi
icative Forms in 
W
cs: An in-
Advances in Automatic Text Summarization. MIT 
Press, Cambridge, pp 1-13. 
hank RC. 1975. Conceptual information processing. 
Amsterdam. North-Holland 
dam, 
ufel S, Moens M. 2002. Summarizing scientific arti-
cles -Rindflesch TC, Fiszman M. 2003. The interaction of 
domain knowledge and linguistic structure in natural 
language processing: interpreting hypernymic propo-
sitions in biomedical text. J Biomed Infor, 36
status. Computational Linguistics, 28(4):409-445. 
nny C, Pustejovsk J. 2000. A history of events in lin-
guistic theory. In C. Tenny and J. Pustejovsky, e
tors, Events as Grammatical Objects, CSLI 
Publications, Stanford, pp 3-37. 
egas E, Mahesh K, Nirenburg  S. 1998. Semantics in 
action. In P. St. Dizier, editor, Pred
Saggion H, Lapalme G. Generating indicative-
informative summaries with SumUM. 2002. Compu-
tational Linguistics, 28(4):497-526. 
ton G, Wong A, Yang CS. 1975. A vector space 
Natural Language and in Lexical Knowledge Bases. 
Kluwer Academic Publishers, Dordrecht.  
ilks YA. 1976. Parsing English II. In E. Charniak and 
Y. Wilks, editors, Computational semanti
model for automatic indexing. Communications of 
the ACM, (18):613-20. 
nivasan P, Rindflesch T. 2002. Explo
from MEDLINE. Proceedings of the AMIA Symp, pp 
722-6. 
arck Jones K. 1999. Au
troduction to artificial intelligence and natural lan-
guage comprehension. North Holland Publishing 
Company, Amsterdam, pp 155-84. .and directions In I. Mani and MT. Maybury, editors, 
 
Figure 3. Semantic abstraction summarization on citations retrieved for migraine. Arrow thickness re-
flects redundant information (i.e. informational equivalence of sentences across multiple documents)  
MPLUS:  A Probabilistic Medical Language Understanding  System 
Lee M. Christensen, Peter J. Haug, and Marcelo Fiszman 
Department of Medical Informatics, LDS Hospital/University of Utah, Salt Lake City, UT 
E-mail: ldlchris@ihc.com, ldphaug@ihc.com, ldmfiszm@ihc.com 
 
Abstract 
This paper describes the basic philosophy 
and implementation of MPLUS (M+), a 
robust medical text analysis tool that uses a 
semantic model based on Bayesian 
Networks (BNs).  BNs provide a concise 
and useful formalism for representing 
semantic patterns in medical text, and for 
recognizing and reasoning over those 
patterns. BNs are noise-tolerant, and 
facilitate the training of M+. 
1 
2 
Introduction 
In the field of medical informatics, 
computerized tools are being developed that 
depend on databases of clinical information.  
These include alerting systems for improved 
patient care, data mining systems for quality 
assurance and research, and diagnostic systems 
for more complex medical decision support.  
These systems require data that is appropriately 
structured and coded.  Since a large portion of 
the information stored in patient databases is in 
the form of free text, manually coding this 
information in a format accessible to these tools 
can be time consuming and expensive.  In recent 
years, natural language processing (NLP) 
methodologies have been studied as a means of 
automating this task. There have been many 
projects involving automated medical language 
analysis, including deciphering pathology 
reports (Smart and Roux, 1995), physical exam 
findings (Lin et al, 1991), and radiology reports 
(Friedman et al, 1994; Ranum, 1989; Koehler, 
1998).   
M+ is the latest in a line of NLP tools 
developed at LDS Hospital in Salt Lake City, 
Utah.  Its predecessors include SPRUS (Ranum, 
1989) and SymText (Koehler, 1998).  These 
tools have been used in the realm of radiology 
reports, admitting diagnoses (Haug et al, 1997), 
radiology utilization review (Fiszman, 2002) 
and syndromic detection (Chapman et al, 
2002).  Some of the character of these tools 
derives from common characteristics of 
radiology reports, their initial target domain.  
 Because of the off-the-cuff nature of 
radiology dictation, a report will frequently 
contain text that is telegraphic or otherwise not 
well formed grammatically.  Our desire was not 
only to take advantage of phrasal structure to 
discover semantic patterns in text, but also to be 
able to infer those patterns from lexical and 
contextual cues when necessary. 
Most NLP systems capable of semantic 
analysis employ representational formalisms 
with ties to classical logic, including semantic 
grammars (Friedman et al, 1994), unification-
based semantics (Moore, 1989), and description 
logics (Romacker and Hahn, 2000). M+ and its 
predecessors employ Bayesian Networks (Pearl, 
1988), a methodology outside this tradition.  
This study discusses the philosophy and 
implementation of M+, and attempts to show 
how Bayesian Networks can be useful in 
medical text analysis.  
The M+ Semantic Model 
2.1 Semantic Bayesian Networks 
M+ uses Bayesian Networks (BNs) to represent 
the basic semantic types and relations within a 
medical domain such as chest radiology reports.   
M+  BNs are structurally similar to semantic 
networks, in that they are implemented as 
directed acyclic graphs, with  nodes 
representing word and concept types, and links 
representing relations between those types.  BNs 
also have a character as frames or slot-filler 
representations (Minsky, 1975).  Each node is 
treated as a variable, with an associated  list of 
possible values.  For instance a node 
representing "disease severity" might include 
the possible values {"severe", "moderate", 
"mild"}. Each value  has a probability, either 
assigned or inferred, of being the true value of 
that node.   
In addition to providing a framework 
for representation, a BN is also a probabilistic 
inference engine.  The probability of each 
possible value of a node is conditioned on the 
probabilities of the values of neighboring nodes, 
                                            Association for Computational Linguistics.
                            the Biomedical Domain, Philadelphia, July 2002, pp. 29-36.
                         Proceedings of the Workshop on Natural Language Processing in
through a training process that learns a Bayesian 
joint probability function from a set of training 
cases.  After a BN is trained, a node can be 
assigned a value by setting the probability of 
that value to 1, and the probabilities of the 
alternate values to 0.  This results in a cascading 
update of the value probabilities in all 
unassigned nodes, in effect predicting what the 
values of the unassigned nodes should be, given 
the initial assignments.  The sum of the 
probabilities for the values of a given node is 
constrained to equal 1, making the values 
mutually exclusive, and reflecting uncertainty if 
more than one value has a nonzero probability.  
Please note that in this paper, "BN instance" 
refers to the state of a BN after assignments 
have been made.   
A training case for a BN is a list of node 
/ value assignments.  For instance, consider a 
simple BN for chest anatomy phrases, as shown 
in Figure 1. 
Figure 1.  BN for simple chest anatomy phrases. 
A training case for this BN applied to 
the phrase "right upper lobe" could be: 
side=right 
verticality=upper 
location=lobe 
interpretation= *right-upper-lobe 
 
In the context of the Bayesian learning, 
this case has an effect similar to a production 
rule which states "If  you find the words 'right', 
'upper' and 'lobe' together in a phrase, infer the 
meaning *right-upper-lobe".  After training on 
this case, assigning one or more values from this 
case would increase the probabilities of the 
other values; for instance assigning side= 
"right" would increase the probability of the 
value interpretation= *right-upper-lobe. 
Interpretive concepts such as *right-
upper-lobe are atomic symbols which are either 
invented by the human trainer, or else obtained 
from a medical knowledge database such as the 
UMLS metathesaurus.  By convention, concept 
names in M+ are preceded with an asterisk. 
A medical domain is represented in M+ 
as a network of BNs, with word-level and lower 
concept-level BNs providing input to higher 
concept-level BNs.  Figure 2 shows a partial 
view of the network of BNs used to model the 
M+ Head CT (Computerized Tomography) 
domain, instantiated with the phrase "temporal 
subdural hemorrhage".   Each BN instance is 
shown with a list of nodes and most probable 
values. Note that input nodes of higher BNs in 
this model have the same name as, and take 
input from, the summary nodes of lower BNs.  
Word level BNs have input nodes named 
"head", "mod1" and "mod2", corresponding to 
the syntactic head and modifiers of a phrase.  
Each node in a BN has a distinguished "null" 
value, whose meaning is that no information 
relevant to that node, explicit or inferable, is 
present in the represented phrase. 
Figure 2.  Network of M+ BNs, applied to 
"temporal subdural hemorrhage".  
One way in which M+ differs from its 
predecessor SymText (Koehler, 1998) is in the 
size and modularity of its semantic BNs.  The 
SymText BNs group observation and disease 
concepts together with state ("present", 
"absent"), change-of-state ("old", "chronic"), 
anatomic location and other concept types.  M+ 
trades the inferential advantages of such 
monolithic BNs for the modularity and 
composability of smaller BNs such as those 
shown in figure 2.  Figure 3 shows a single 
instance of the SymText Chest Radiology 
Findings BN, instantiated with the sentence 
"There is dense infiltrative opacity in the right 
upper lobe". 
*observations :  *localized upper lobe infiltrate (0.888) 
     *state :  *present (0.989) 
         state term :  null (0.966) 
     *topic concept :  *poorly-marginated opacity (0.877) 
         topic term :  opacity  (1.0) 
         topic modifier :  infiltrative (1.0) 
      *measurement concept :  *null (0.999) 
         measurement term :  null (0.990) 
         first value :  null (0.998) 
         second value :  null (0.999) 
         values link :  null (0.999) 
         size descriptor :  null (0.999) 
     *tissue concept :  *lung parenchyma (0.906) 
         tissue term : alveolar (1.0) 
     *severity concept :  *high severity (0.893) 
         severity term :  dense (1.0) 
     *anatomic concept :  *right upper lobe (0.999) 
         *anatomic link concept :  *involving (1.0) 
             anatomic link term :  in (1.0) 
         anatomic location term :  lobe (1.0) 
         anatomic location modifier :  null (0.999) 
         anatomic modifier side :  right (1.0) 
         anatomic modifier superior/inferior : upper (1.0) 
         anatomic modifier lateral/medial : null (0.999) 
         anatomic modifier anterior/posterior : null (0.999) 
         anatomic modifier central/peripheral : null (0.955) 
     *change concept :  *null (0.569) 
         change with time :  null (0.567) 
         change degree :  null (0.904) 
         change quality :  null (0.923) 
Figure 3.  SymText BN instantiation. 
2.2 Parse-Driven BN Instantiation 
M+ BNs are instantiated as part of the 
syntactic parse process.  M+ syntactic and 
semantic analyses are interleaved, in contrast 
with NLP systems that perform semantic 
analysis after the parse has finished. 
M+ uses a bottom-up chart parser, with 
a context free grammar (CFG).  As a word such 
as "right" is recognized by the parser, a word-
level phrase object is created and a BN instance 
containing the assignment side= "right" is 
attached to that phrase.  As larger grammatical 
patterns are recognized, the BN instances 
attached to subphrases within those patterns are 
unified and attached to the new phrases, as 
described in section 3.  The result of this 
process is a set of completed BN instances, as 
illustrated in figure 2.  Each BN instance is a 
template containing word and concept-level 
value assignments, and the interpretive concepts 
inferred from those assignments.  The templates 
themselves are nested in a symbolic expression, 
as described in section 2.3, to facilitate 
composing multiple BN instances in 
representations of arbitrary complexity. 
Each phrase recognized by the parser is 
assigned a probability, based on a weighted sum 
of the joint probabilities of its associated BN 
instances, and adjusted for various syntactic and 
semantic constraint violations.  Phrases are 
processed in order of probability; thus the parse 
involves a semantically-guided best-first search. 
Syntactic and semantic analysis in M+ 
are mutually constraining.  If a grammatically 
possible phrase is uninterpretable, i.e. if its 
subphrase interpretations cannot be unified, it is 
rejected.  If the interpretation has a low 
probability, the phrase is less likely to appear in 
the final parse tree.  On the other hand, 
interpretations are constructed as phrases are 
recognized. The exception to this rule is when 
an ungrammatical fragment of text is 
encountered.  M+ then uses a semantically-
guided phrase repair procedure not described in 
this paper. 
2.3 The M+ Abstract Semantic Language  
The probabilistic reasoning afforded by BNs is 
superior to classical logic in important ways 
(Pearl, 1988).  However, BNs are limited in 
expressive power relative to first-order logics 
(Koller and Pfeffer, 1997), and commercially 
available implementations lack the flexibility of 
symbolic languages.  Friedman et alhave made 
considerable headway in giving BNs many 
useful characteristics of first order languages, in 
what they call probabilistic relational models, or 
PRMs (e.g. Friedman at al.  1999).   
While we are waiting for industry-
standard PRMs, we have tried to make our 
semantic BNs more useful by combining them 
with a first-order language, called the M+ 
Abstract Semantic Language (ASL), 
implemented within M+.  Specifically, BNs are 
treated as object types within the ASL.  There is 
a "chest anatomy" type, for instance, and a 
"chest radiology findings" type, corresponding 
to BNs of those same names.  The interpretation 
of a phrase is an expression in the ASL, 
containing predicates that state the relation of 
BN instances to one another, and to the phrase 
they describe. For instance, the interpretation of 
"hazy right lower lobe opacity" could be the 
expression  
(and (head-of #phrase1 #find1) 
                       (located-at #find1 #loc1)) 
 
where #phrase1 identifies a syntactic phrase 
object, and #find1 and #loc1 are tokens 
representing instances of the findings BN 
(instanced with the words "hazy" and "opacity") 
and the anatomic BN (instanced with "right", 
"lower" and "lobe"), respectively.  The relation 
'head-of' denotes that the findings BN is the 
main or "head" BN for that phrase.  Conversely, 
"hazy right lower lobe opacity" can be thought 
of as a findings-type phrase, with an anatomic-
type modifier. 
This expression captures the abstract or 
"skeletal" structure of the interpretation, while 
the BN instances contain the details and specific 
inferences.   One can think of the meaning of an 
expression like (located-at #find1 #loc1) in 
abstract terms, e.g. "some-finding located-at 
some-location".  Alternatively, the meaning of a 
BN token might be thought of as the most 
probable interpretive concept within that BN 
instance.  In this case, (located-at #find1 #loc1) 
could mean "*localized-infiltrate located-at 
*left-lower-lobe". 
Because the object types in the ASL are 
the abstract concept types represented by the 
BNs, semantic rules formulated in this language 
constitute an "abstract semantic grammar" 
(ASG). The ASG recognizes patterns of 
semantic relations among the BNs, and supports 
analysis and inference based on those patterns.  
It also permits rule-based control over the 
creation, instantiation, and use of the BNs, 
including defining pathways for information 
sharing among BNs using virtual evidence 
(Pearl, 1988).    
One use of the ASG is in post-parse 
processing of interpretations.  After the M+ 
parser has constructed an interpretation, post-
parse ASG productions may augment or alter 
this interpretation.  One rule instructs "If two 
pathological conditions exist in a 'consistent-
with' relation, and the first condition has a state 
modifier (i.e. *present or *absent), and the 
second condition does not, apply the first 
condition's state to the second condition".   
For instance, in the ambiguous sentence 
"There is no opacity consistent with 
pneumonia", if the parser doesn't correctly 
determine the scope of "no", it may produce the 
an interpretation in which *pneumonia lacks a 
state modifier, and is therefore inferred (by 
default) to be present.  This rule correctly 
attaches (state-of *pneumonia *absent) to this 
interpretation. 
One important consequence of the 
modularity of the M+ BNs, and of the ability to 
nest them within the ASL, is that M+ can 
compose BN instances in expressions of 
arbitrary complexity.  For instance, it is 
straightforward to represent the multiple 
anatomic concepts in the phrase "opacity in the 
inferior segment of the left upper lobe, adjacent 
to the heart": 
(and (head-of #phrase1 #find1)  
                          (located-at #find1 #anat1) 
                          (qualified-by #anat1 #anat2)  
                          (adjacent-to #anat1 #anat3)) 
 
where the interpretive concepts of #anat1, 
#anat2 and #anat3 are *left-upper-lobe, 
*inferior-segment, and *heart, respectively. 
The set of binary predicates that 
constitutes a phrase interpretation in M+ forms a 
directed acyclic graph; thus we can refer to the 
interpretation as an interpretation graph.  The 
interpretation graph of a new phrase is formed 
by unifying the graphs of its subphrases, as 
described in section 3.  
2.4 Advantages of Bayesian Networks 
As mentioned, a BN training case bears a 
similarity to a production rule.  It would be 
straightforward to implement the training cases 
as a set of rules, and apply them to text analysis 
using a deductive reasoning engine.  However, 
Bayesian reasoning has important advantages 
over first order logic, including: 
1- BNs are able to respond gracefully to 
input "noise".  A semantic BN may produce 
reasonable inferences from phrasal patterns that 
only partially match any given training case, or 
that overlap different cases, or that contain 
words in an unexpected order.  For instance, 
having trained on multi-word phrases containing 
"opacity", the single word "opacity" could raise 
the probabilities of several interpretations such 
as *localized-infiltrate and *parenchymal-
abnormality, both of which are reasonable 
hypotheses for the underlying cause of opacity 
on a chest x-ray film. 
2- Bayesian inference works bi-
directionally; i.e. it is abductive as well as 
deductive.  If instead of assigning word-level 
nodes, one assigns the value of the summary 
node, the probability of word values having a 
high correlation with that summary will 
increase.  For instance, assigning the value 
*localized-infiltrate will raise the probability 
that the topic word is "opacity".   
Bi-directional inference provides a 
means for modeling the effects of lexical 
context.  A value assignment made to one word 
node can alter value probabilities at unassigned 
word nodes, in a path of inference that passes 
through the connecting concept nodes.  For 
instance, if a BN were trained on "right upper 
lobe" and "left upper lobe", but had never seen 
the term "bilateral", applying the BN to the 
phrase "bilateral upper lobes" would increase 
the  probabilities of both "left" and "right", 
suggesting that "bilateral" is semantically 
similar to "left" and "right".  This is one 
approach to guessing the node assignments of 
unknown words, a step in the direction of 
automated learning of new training cases.    
Similarly, if the system encounters a 
phrase with a misspelling such as "rght upper 
lobe", by noting the orthographic similarity of 
"rght" to "right" and the fact that "right" is 
highly predicted from surrounding words, it can 
determine that "rght" is a misspelling of "right".  
The spell checker currently used by M+ 
employs this technique. 
3 Generating  Interpretation 
Graphs 
As mentioned, in M+ the interpretation graph of 
a phrase is created by unifying the graphs of its 
child phrases.  High joint probabilities in the 
resulting BN instances are one source of 
evidence that the words thus brought together 
exist in the expected semantic pattern.  
However, corroborating evidence must be 
sought in the syntax of the text.  Words which 
appear together in a training phrase may not be 
in that same relation in a given text.  For 
instance, "no" and "pneumonia" support 
different conclusions in "no evidence of 
pneumonia" and "patient has pneumonia with no 
apparent complicating factors".  M+ therefore 
only attempts to unify sub-interpretations that 
appear, on syntactic grounds, to be talking about 
the same things.  This is less constraining than 
production rules that look for words in a 
specific order, but more constraining than 
simply pulling key words out of a string of text.  
The following are examples of rules 
used to guide the unification of ASL 
interpretation graphs.  For convenience, several 
shorthand functional notations are used:  If P 
represents a phrase on the parse chart, root-
bn(P) represents the root or head BN instance in 
P's interpretation graph, and type-of(root-bn(P)) 
is the BN type of root-bn(P).  If A and B are 
sibling child phrases of parent phrase C, then C 
= parent-phrase(A,B).  Note that for 
convenience, BN instances in the interpretation 
graphs in Figures 4 - 6 are represented 
alternately as the words slotted in those 
instances, and as the most probable interpretive 
concepts inferred by those instances. 
3.1 Same-type Unification 
If phrase A syntactically modifies phrase B, 
then M+ assumes that some semantic relation 
exists between A and B.  The nature of that 
relation is partly determinable from type-
of(root-bn(A)) and type-of(root-bn(B)).  If type-
of(root-bn(A)) = type-of(root-bn(B)), that 
relation is simply one where root-bn(A) and 
root-bn(B) are partial descriptions of a single 
concept.  If root-bn(A) and root-bn(B) are 
unifiable, M+ composes their input to form 
root-bn(parent-phrase(A,B)).   
If in addition there are two unifiable 
same-type BN instances X and Y linked to root-
bn(A) and root-bn(B) respectively, via arcs of 
the same name, then X and Y also describe a 
single concept, and the arcs describe a single 
relationship.  For instance, if X and Y describe 
the anatomic locations of  root-bn(A) and root-
bn(B), and if root-bn(A) and root-bn(B) are 
partial descriptions of a single "finding", then X 
and Y are partial descriptions of a single 
anatomic location, and ought to be unified. 
Figure 4:  Same-type unification 
In figure 4, in the Chest X-ray domain, 
the phrase "bilateral hazy lower lobe opacity" is 
interpreted by unifying the interpretations of its 
subphrases "bilateral hazy" and "lower lobe 
opacity".  Note that without any corresponding 
syntactic transformation, this rule brings about a 
"virtual transformation", whereby words are 
grouped together within BN instances in a 
manner that reflects the conceptual structure of 
the text.  In this example "bilateral hazy lower 
lobe opacity" is treated as ("bilateral lower 
lobe") ("hazy opacity"). 
Figure 6: Grammar rule - based unification. 
3.2 Different-type Unification 
If phrase A syntactically modifies phrase B, and 
type-of(root-bn(A)) <> type-of(root-bn(B)), 
then root-bn(A) and root-bn(B) represent 
different concepts within some semantic 
relation.  M+ uses the ASG to identify that 
relation and to add it to the interpretation graph 
in the form of a path of named arcs connecting 
root-bn(A) and root-bn(B).  This path may 
include implicit connecting BN instances.  
M+ Implementation 4 
5 
M+ is written in Common Lisp, with some C 
routines for BN access.  The M+ architecture 
consists of six basic components:  The parser, 
concept space, rule base, lexicon, ASL inference 
engine, and Bayesian network component.   
For instance, to interpret "subdural 
hemorrhage" in the Head CT domain, M+ 
attempts to unify the graphs for the subphrases 
"subdural" and "hemorrhage", where type-
of(root-bn("subdural")) = location, and type-
of(root-bn("hemorrhage")) = topic.  M+ 
identifies the connecting path for these two 
types as shown in figure 2, and adds that path to 
the interpretation as shown in figure 5.  Note 
that this path contains instances of the 
"observation" and "anatomy" BN types.  
As mentioned, the parser is an 
implementation of a bottom up chart parser with 
context free grammar.   
The concept space is a table of symbols 
representing types, objects and relations within 
the ASL.  These include BN names, BN node 
value names, inter-BN relation names, and a 
small ontology of useful concepts such as those 
related to time. 
Figure 5.  Different-type unification. 
The rule base contains rules, which 
comprise the syntactic grammar and ASG.  
The lexicon is a table of Lisp-readable 
word information entries, obtained in part from 
the UMLS Specialist Lexicon. 
The ASL inference engine combines 
symbolic unification with backward-chaining 
inference. It can be used to match an ASG 
pattern against an interpretation graph, and to 
perform tests associated with grammar rules.  
3.3 Grammar Rule Based Unification The Bayesian network component utilizes 
the Norsys Netica(TM) API, and includes a set of 
Lisp and C language routines for instantiating 
and retrieving probabilities from BNs. 
Individual grammar rules in M+ can recognize 
semantic relations, and add connecting arcs to 
the interpretation graph.  For instance, M+ has a 
rule which recognizes findings-type phrases 
connected with strings of the "suggesting" 
variety, and connects their graphs with a 
'consistent-with' arc.  This is used to interpret 
"opacity suggesting possible infarct" in the 
Head CT domain, as shown in figure 6. 
Training M+ 
Porting M+ to a new medical domain involves 
gathering a corpus of training sentences for the 
domain, using the Netica(TM) graphical interface 
to create domain-specific BNs, and generating 
training cases for the new BNs.   
The most time-consuming task is the 
creation of training cases.  We have developed a 
prototype version of a Web-based tool which 
largely automates this task.  The basic idea is to 
enable M+ to guess the BN value assignments 
of unknown words, then use it to parse phrases 
similar to phrases already seen.  For instance, 
having been trained on the phrase "right upper 
lobe", the parser is able to produce reasonable 
parses, with some "guessed" value assignments, 
for "left upper lobe", "right middle lobe", 
"bilateral lungs", etc.  The BN assignments 
produced by the parse are output as tentative 
new cases to be reviewed and corrected by the 
human trainer. 
The training process begins with an 
initial set of interpreted "seed" phrases.  From 
this set, the tool can apply the parser to phrases 
similar to this set, and so semi-automatically 
traverse ever widening semantically contiguous 
areas within the space of corpus phrases.  As the 
training proceeds, the role of the human trainer 
increasingly becomes one of providing 
correction and interpretations for semantic 
patterns the system is increasingly able to 
discover on its own. 
To parse phrases containing unknown 
words, M+ uses a technique based on a variation 
of the vector space model of lexical semantic 
similarity (Manning and Schutze, 1999).  As 
M+ encounters an unknown word, it gathers a 
list of training corpus words judged similar to 
that word, as predicted by the vector space 
measure.  It then identifies BN nodes whose 
known values significantly overlap with this list, 
and provisionally assigns the unknown word as 
a new value for those nodes.  The assignment 
resulting in the best parsetree is selected for the 
new provisional training case. 
6 Evaluation 
M+ was evaluated for the extraction of 
American College of Radiology (ACR) 
utilization review codes from Head CT reports 
(Fiszman, 2002). The ACR codes compare the 
outcome in a report with the suspected diagnosis 
provided by emergency department physicians. 
If the outcome relates to the suspected diagnosis 
then the report should be encoded as positive 
(P). If the outcome is negative and does not 
relate to the suspected diagnosis then the report 
should be encoded as negative  (N). In order to 
extract those ACR codes we trained M+ to 
extract eleven broad disease concepts, then 
inferred the ACR codes based on the application 
of a rule to the M+ output:  If any of the 
concepts was present, the report was considered 
positive, else the report was considered 
negative. 
Twenty six hundred head CT scan 
reports were used for this evaluation.  Six 
hundred reports were randomly selected for 
testing, and the rest were used to train M+ in 
this domain.  The performance of M+ on this 
task was measured against that of four board 
certified physicians, using a gold standard based 
on majority vote, as described in (Fiszman, 
2002).   For each subject we calculated recall, 
precision and specificity with their respective 95 
% confidence intervals for the capture of ACR 
utilization codes.  
From 600 head CT reports, 67 were 
judged to be positive (P) by the gold standard 
physicians and 534 were judged to be negative 
(N). Therefore the positive rate for head CT in 
this sample was 11%.  Recall, precision and 
specificity for every subject are presented with 
their respective 95% confidence intervals in 
Table 1. The physicians had an average recall of 
88% (CI, 84% to 92.%), an average precision of 
86% (CI, 81% to 90%), and average specificity 
of 98% (CI, 97% to 99%). M+  had recall of 
87% (CI, 78% to 95%), precision of 85% (CI, 
77% to 94%) and specificity of 98% (CI, 97% 
to 99). 
Table 1.  Results of  ACR utilization code study. 
Subject Recall Specificity Precision 
Physician1 0.83 
(0.74-0.92) 
0.99 
(0.98-1.00) 
0.91 
(0.84-0.99) 
Physician2 0.88 
(0.81-0.97) 
0.98 
(0.97-0.99) 
0.84 
(0.75-0.93) 
Physician3 0.93 
(0.87-1.00) 
0.98 
(0.97-0.99) 
0.86 
(0.78-0.95) 
Physician4 0.88 
(0.96-0.99) 
0.97 
(0.96-0.99) 
0.81 
(0.71-0.90) 
M+ 0.87 
(0.78-0.95) 
0.98 
(0.97-0.99) 
0.85 
(0.77-0.94) 
 
The results on Head CT reports are 
encouraging, but there are limitations. We only 
evaluated 600 reports, because it's very hard to 
get physicians to produce gold standard data for 
medical reports. The prevalence of positive 
reports is only 11% and reflects the fact that the 
individual brain conditions  have very low 
prevalence. 
7 
8 
Conclusions 
M+ and its predecessors have demonstrated that 
BNs provide a useful semantic model for 
medical text processing.  In practice, a medical 
NLP system will frequently encounter missing 
and unknown words,  unknown and 
ungrammatical phrase structures, and 
telegraphic usages.  Knowledge databases will 
be imperfect and incomplete.  Using BNs for 
semantic representation brings a noise-tolerant, 
partial match-tolerant, context-sensitive 
character to the recognition of semantic 
patterns, and to relevant inferences based on 
those patterns.  In addition, BNs can be used to 
guess the semantic types of unknown words, 
providing a basis for bootstrapping the system's 
semantic knowledge. 
Acknowledgements 
Many thanks to Wendy W. Chapman for her 
advice and input in this paper, and her efforts to 
make M+ a useful addition to the RODS project 
at the University of Pittsburgh. 
References 
 
Chapman W., Christensen L. M., Wagner M., Haug 
P. J., Ivanov O., Dowling J. N., Olszewski R. T. 
2002.  Syndromic Detection from Free-text 
Triage Diagnoses: Evaluation of a Medical 
Language Processing System before Deployment 
in the Winter Olympics. Proc AMIA Symp. 
(submitted). 
Chomsky, Noam. 1965. Aspects of the theory of 
syntax.  Special technical report (Massachusetts 
Institute of Technology, Research Laboratory of 
Electronics); no. 11. Cambridge, MA: MIT Press. 
Fiszman M., Blatter D.D., Christensen L.M., Oderich 
G., Macedo T., Eidelwein A.P., Haug P.J.  2002.  
Utilization review of head CT scans: value of a 
medical language processing system. American 
Journal of Roentgenology (AJR). (submitted) 
Friedman C, Alderson PO, Austin JH, Cimino JJ, 
Johnson SB.  1994,  A general natural-language 
text processor for clinical radiology.  J Am Med 
Inform Assoc. Mar-Apr;1(2) pp. 161-74. 
Friedman N., Getoor L., Koller D. and Pfeffer A. 
1999.  Learning Probabilistic Relational Models.  
Proceedings of the 16th International Joint 
Conference on Artificial Intelligence (IJCAI):  
pp. 1300-1307. 
Haug P. J., Christensen L., Gundersen M., Clemons 
B., Koehler S., Bauer K. 1997. A natural 
language parsing system for encoding admitting 
diagnoses. Proc AMIA Symp. 81: pp. 4-8. 
Koehler, S. B. 1998.  SymText: A natural language 
understanding system for encoding free text 
medical data. Ph.D. Dissertation, University of 
Utah. 
Koller D., and Pfeffer A.  1997. Object-Oriented 
Bayesian Networks.  Proceedings of the 13th 
Annual Conference on Uncertainty in AI:  pp. 
302-313.  
Lin R, Lenert L, Middleton B, Shiffman S. A free-
text processing system to capture physical 
findings: Canonical Phrase Identification System 
(CAPIS). Proc Annu Symp Comput Appl Med 
Care. pp. 843-7. 
Manning C. D. and Schutze H. 1999.  Foundations of 
Statistical Natural Language Processing.  MIT 
Press.  
Minsky, M. 1975.  A framework for representing 
knowledge.  In The Psychology of Human Vision, 
ed. P. H. Winston, pp. 211-277.  McGraw Hill. 
Moore, R. C. 1989. Unification-based Semantic 
Interpretation.  Proceedings of the 27th Annual 
Meeting of the Association for Computational 
Linguistics, pp33-41. 
Pearl, Judea.  1988.  Probabilistic inference in 
intelligent systems.  Networks of plausible 
inference:  Morgan Kaufmann. 
Ranum D.L. 1989. Knowledge-based understanding 
of radiology text.  Comput Methods Programs 
Biomed.  Oct-Nov;30(2-3) pp.209-215. 
Romacker, Martin and Hahn, Udo. 2000.  An 
empirical assessment of semantic interpretation. 
ANLP/NAACL 2000 -- Proceedings of the 6th 
Applied Natural Language Processing 
Conference & the 1st Conference of the North 
American Chapter of the Association for 
Computational Linguistics. pp. 327-334.  
 Schank, R.C. and R. Abelson. 1997.  Scripts, Plans, 
Goals, and Understanding.  Hillsdale, NJ: 
Lawrence Erlbaum. 
Smart, J. F. and M. Roux. 1995. A  model for 
medical knowledge representation application to 
the analysis of descriptive pathology reports.  
Methods Inf Med.  Sep;34(4) pp. 352-60. 
BioNLP 2007: Biological, translational, and clinical language processing, pages 137?144,
Prague, June 2007. c?2007 Association for Computational Linguistics
Interpreting Comparative Constructions in Biomedical Text 
Marcelo Fiszman,1   Dina Demner-Fushman,2    
Francois M. Lang,2  Philip Goetz,2 
Thomas C. Rindflesch2 
1University of Tennessee ? GSM, Knoxville, TN 37920 
mfiszman@utmck.edu 
2Lister Hill National Center for Biomedical Communications 
National Library of Medicine, Bethesda, MD 20894 
{ddemner|goetzp|flang|trindflesch}@mail.nih.gov 
 
Abstract 
We propose a methodology using 
underspecified semantic interpretation to 
process comparative constructions in 
MEDLINE citations, concentrating on two 
structures that are prevalent in the research 
literature reporting on clinical trials for 
drug therapies. The method exploits an 
existing semantic processor, SemRep, 
which constructs predications based on the 
Unified Medical Language System. Results 
of a preliminary evaluation were recall of 
70%, precision of 96%, and F-score of 
81%. We discuss the generalization of the 
methodology to other entities such as 
therapeutic and diagnostic procedures. The 
available structures in computable format 
are potentially useful for interpreting 
outcome statements in MEDLINE 
citations. 
1 Introduction 
As natural language processing (NLP) is 
increasingly able to support advanced information 
management techniques for research in medicine 
and biology, it is being incrementally improved to 
provide extended coverage and more accurate 
results. In this paper, we discuss the extension of 
an existing semantic interpretation system to 
address comparative structures. These structures 
provide a way of explicating the characteristics of 
one entity in terms of a second, thereby enhancing 
the description of the first. This phenomenon is 
important in clinical research literature reporting 
the results of clinical trials.  
In the abstracts of these reports, a treatment for 
some disease is typically discussed using two types 
of comparative structures. The first announces that 
the (primary) therapy focused on in the study will 
be compared to some other (secondary) therapy. A 
typical example is (1). 
(1) Lansoprazole compared with 
ranitidine for the treatment of 
nonerosive gastroesophageal reflux 
disease. 
An outcome statement (2) often appears near the 
end of the abstract, asserting results in terms of the 
relative merits of the primary therapy compared to 
the secondary. 
(2) Lansoprazole is more 
effective than ranitidine in 
patients with endoscopically 
confirmed non-erosive reflux 
esophagitis. 
The processing of comparative expressions such 
as (1) and (2) was incorporated into an existing 
system, SemRep [Rindflesch and Fiszman, 2003; 
Rindflesch et al, 2005], which constructs semantic 
predications by mapping assertions in biomedical 
text to the Unified Medical Language System? 
(UMLS)? [Humphreys et al, 1998].  
2 Background 
2.1 Comparative structures in English 
The range of comparative expressions in English is 
extensive and complex. Several linguistic studies 
have investigated their characteristics, with 
differing assumptions about syntax and semantics 
(for example [Ryan, 1981; Rayner and Banks, 
1990; Staab and Hahn, 1997; Huddleston and 
Pullum, 2002]). Our study concentrates on  
137
structures in which two drugs are compared with 
respect to a shared attribute (e.g. how well they 
treat some disease). An assessment of their relative 
merit in this regard is indicated by their positions 
on a scale. The compared terms are expressed as 
noun phrases, which can be considered to be 
conjoined. The shared characteristic focused on is 
expressed as a predicate outside the comparative 
structure. An adjective or noun is used to denote 
the scale, and words such as than, as, with, and to 
serve as cues to identify the compared terms, the 
scale, and the relative position of the terms on the 
scale.  
The first type of structure we address (called  
comp1 and illustrated in (3)) merely asserts that the 
primary and secondary terms (in bold) are being 
compared. A possible cue for identifying these 
structures is a form of compare. A further 
characteristic is that the compared terms are 
separated by a conjunction, or a preposition, as in 
(3). 
(3) To compare misoprostol with 
dinoprostone for cervical ripening 
and labor induction. 
As shown in (4), a scale may be  mentioned 
(efficacy); however, in this study, we only identify 
the compared terms in structures of this type.  
(4) To compare the efficacy of 
misoprostol with dinoprostone for 
cervical ripening and labor 
induction. 
In the more complex comparative expression we 
accommodate (called comp2), the relative ranking 
of two compared terms is indicated on a scale 
denoted by an adjective (e.g. effective in (5)). The 
relative position of the compared terms in scalar 
comparative structures of this type expresses either 
equality or inequality. Inequality is further divided 
into superiority, where the primary compared term 
is higher on the scale than the secondary, and 
inferiority, where the opposite is true. Cues 
associated with the adjective designating the scale 
signal these phenomena (e.g. as ADJ as in (5) for 
equality, ADJer than in (6) for superiority, and less 
ADJ than in (7) for inferiority).  
(5) Azithromycin is as effective 
as erythromycin estolate for the 
treatment of pertussis in children. 
(6) Naproxen is safer than 
aspirin in the treatment of the 
arthritis of rheumatic fever. 
(7) Sodium valproate was 
significantly less effective than 
prochlorperazine in reducing pain 
or nausea. 
In examples (3) through (7), the characteristic the 
compared drugs have in common is treatment of 
some disorder, for example treatment of pertussis 
in children in (5).  
Few studies describe an implemented automatic 
analysis of comparatives; however, Friedman 
[Friedman, 1989] is a notable exception. Jindal and 
Liu [Jindal and Liu, 2006] use machine learning to 
identify some comparative structures, but do not 
provide a semantic interpretation. We exploit 
SemRep machinery to interpret the aspects of 
comparative structures just described. 
2.2 SemRep 
SemRep [Rindflesch and Fiszman, 2003; 
Rindflesch et al, 2005] recovers underspecified 
semantic propositions in biomedical text based on 
a partial syntactic analysis and structured domain 
knowledge from the UMLS. Several systems that 
extract entities and relations are under 
development in both the clinical and molecular 
biology domains. Examples of systems for clinical 
text are described in [Friedman et al, 1994], 
[Johnson et al, 1993], [Hahn et al, 2002], and 
[Christensen et al, 2002]. In molecular biology, 
examples include [Yen et al, 2006], [Chun et al, 
2006], [Blaschke et al, 1999], [Leroy et al, 2003], 
[Rindflesch et al, 2005], [Friedman et al, 2001], 
and [Lussier et al, 2006].  
During SemRep processing, a partial syntactic 
parse is produced that depends on lexical look-up 
in the SPECIALIST lexicon [McCray et al, 1994] 
and a part-of-speech tagger [Smith et al, 2004]. 
MetaMap [Aronson, 2001] then matches noun 
phrases to concepts in the Metathesaurus? and 
determines the semantic type for each concept. For 
example, the structure in (9), produced for (8), 
allows both syntactic and semantic information to 
be used in further SemRep processing that 
interprets semantic predications.  
(8) Lansoprazole for the 
treatment of gastroesophageal 
reflux disease 
138
(9) [[head(noun(Lansoprazole),me
taconc(?lansoprazole?:[phsu]))],[p
rep(for),det(the),head(noun(treatm
ent))],[prep(of),mod(adj(gastroeso
phageal)),mod(noun(reflux)),head(n
oun(disease),metaconc(?Gastroesoph
ageal reflux disease?:[dsyn]))]] 
Predicates are derived from indicator rules that 
map syntactic phenomena (such as verbs and 
nominalizations) to relationships in the UMLS 
Semantic Network. Argument identification is 
guided by dependency grammar rules as well as 
constraints imposed by the Semantic Network. In 
processing (8), for example, an indicator rule links 
the nominalization treatment with the Semantic 
Network relation ?Pharmacologic Substance 
TREATS Disease or Syndrome.? Since the 
semantic types of the syntactic arguments 
identified for treatment in this sentence 
(?Pharmacologic Substance? for ?lansoprazole? and 
?Disease or Syndrome? for ?Gastroesophageal 
reflux disease?) match the corresponding semantic 
types in the relation from the Semantic Network, 
the predication in (10) is constructed, where 
subject and object are Metathesaurus concepts.  
(10) lansoprazole TREATS 
Gastroesophageal reflux disease 
3 Methods 
3.1 Linguistic patterns 
We extracted sentences for developing 
comparative processing from a set of  some 10,000 
MEDLINE citations reporting on the results of 
clinical trials, a rich source of comparative 
structures. In this sample, the most frequent 
patterns for comp1 (only announces that two terms 
are compared) and comp2 (includes a scale and 
positions on that scale) are given in (11) and (12). 
In the patterns, Term1 and Term2 refer to the 
primary and secondary compared terms, 
respectively. ?{BE}? means that some form of be 
is optional, and slash indicates disjunction. These 
patterns served as guides for enhancing SemRep 
argument identification machinery but were not 
implemented as such. That is, they indicate 
necessary components but do not preclude 
intervening modifiers and qualifiers.   
(11) comp1: Compared terms 
C1:   Term1 {BE} compare with/to Term2 
C2:   compare Term1 with/to Term2 
C3:   compare Term1 and/versus Term2 
C4a: Term1 comparison with/to Term2 
C4b: comparison of Term1 with/to Term2 
C4c: comparison of Term1 and/versus Term2 
C5   Term1 versus Term2 
(12) comp2: Scalar patterns 
S1:   Term1 BE as ADJ as {BE} Term2 
S2a: Term1 BE more ADJ than {BE} Term2 
S2b: Term1 BE ADJer than {BE}Term2  
S2c: Term1 BE less ADJ than {BE} Term2 
S4:   Term1 BE superior to Term2 
S5:   Term1 BE inferior to Term2 
As with SemRep in general, the interpretation of 
comparative structures exploits underspecified 
syntactic structure enhanced with Metathesaurus 
concepts and semantic types. Semantic groups 
[McCray et al, 2001] from the Semantic Network 
are also available. For this project, we exploit the 
group Chemicals & Drugs, which contains such 
semantic types as ?Pharmacologic Substance?, 
?Antibiotic?, and ?Immunologic Factor?. (The 
principles used here also apply to compared terms 
with semantic types from other semantic groups, 
such as ?Procedures?.) In the comp1 patterns, a 
form of compare acts as an indicator of a 
comparative predication. In comp2, the adjective 
serves that function. Other words appearing in the 
patterns cue the indicator word (in comp2) and 
help identify the compared terms (in both comp1 
and comp2). The conjunction versus  is special in 
that it cues the secondary compared term (Term2) 
in comp1, but may also indicate a comp1 structure 
in the absence of a form of compare (C5).  
3.2 Interpreting comp1 patterns  
When SemRep encounters a form of compare, it 
assumes a comp1 structure and looks to the right 
for the first noun phrase immediately preceded by 
with, to, and, or versus. If the head of this phrase is 
mapped to a concept having a semantic type in the 
group Chemicals & Drugs, it is marked as the 
secondary compared term. The algorithm then 
looks to the left of that term for a noun phrase 
having a semantic type also in the group Chemicals 
& Drugs, which becomes the primary compared 
term. When this processing is applied to (13), the 
semantic predication (14) is produced, in which the 
predicate is COMPARED_WITH; the first 
argument is the primary compared term and the 
139
other is the secondary. As noted earlier, although a 
scale is sometimes asserted in these structures (as 
in (13)), SemRep does not retrieve it. An assertion 
regarding position on the scale never appears in 
comp1 structures.  
(13) To compare the efficacy and 
tolerability of Hypericum 
perforatum with imipramine in 
patients with mild to moderate 
depression. 
(14) Hypericum perforatum 
COMPARED_WITH Imipramine 
SemRep considers noun phrases occurring 
immediately to the right and left of versus as being 
compared terms if their heads have been mapped to 
Metathesaurus concepts having semantic types 
belonging to the group Chemicals & Drugs. Such 
noun phrases are interpreted as part of a comp1 
structure, even if a form of compare has not 
occurred. The predication (16) is derived from 
(15).  
(15) Intravenous lorazepam versus 
dimenhydrinate for treatment of 
vertigo in the emergency 
department: a randomized clinical 
trial. 
(16) Lorazepam COMPARED_WITH 
Dimenhydrinate 
SemRep treats compared terms as being 
coordinated. For example, this identification 
allows both ?Lorazepam? and ?Dimenhydrinate? 
to function as arguments of TREATS in (15). 
Consequently, in addition to (16), the predications 
in (17) are returned as the semantic interpretation 
of (15). Such processing is done for all comp1 and 
comp2 structures (although these results are not 
given for (13) and are not further discussed in this 
paper). 
(17) Lorazepam TREATS Vertigo  
 Dimenhydrinate TREATS 
Vertigo 
3.3 Interpreting comp2 patterns  
In addition to identifying two compared terms 
when processing comp2 patterns, a scale must be 
named and the relative position of the terms on that 
scale indicated. The algorithm for finding 
compared terms in comp2 structures begins by 
locating one of the cues as, than, or to and then 
examines the next noun phrase to the right. If its 
head has been mapped to a concept with a 
semantic type in the group Chemicals & Drugs, it 
is marked as the secondary compared term. As in 
comp1, the algorithm then looks to the left for the 
first noun phrase having a head in the same 
semantic group, and that phrase is marked as the 
primary compared term.  
To find the scale name, SemRep examines the 
secondary compared term and then locates the first 
adjective to its left. The nominalization of that 
adjective (as found in the SPECIALIST Lexicon) 
is designated as the scale and serves as an 
argument of the predicate SCALE in the 
interpretation. For adjectives superior and inferior 
(patterns S4 and S5 in (12)) the scale name is 
?goodness.? 
In determining relative position on the scale, 
equality is contrasted with inequality. If the 
adjective of the construction is immediately 
preceded by as (pattern S1 in (12) above), the two 
compared terms have the same position on the 
scale (equality), and are construed as arguments of 
a predication with predicate SAME_AS. In all 
other comp2 constructions, the compared terms are 
in a relationship of inequality. The primary 
compared term is considered higher on the scale 
unless the adjective is inferior or is preceded by 
less, in which case the secondary term is higher. 
The predicates HIGHER_THAN and 
LOWER_THAN are used to construct predications 
with the compared terms to interpret position on 
the scale. The equality construction in (18) is 
expressed as the predications in (19).  
(18) Candesartan is as effective 
as lisinopril once daily in 
reducing blood pressure. 
(19) Candesartan COMPARED_WITH 
lisinopril 
 SCALE:Effectiveness  
 Candesartan SAME_AS 
lisinopril 
The superiority construction in (20) is expressed as 
the predications in (21).  
(20) Losartan was more effective 
than atenolol in reducing 
cardiovascular morbidity and 
mortality in patients with 
hypertension, diabetes, and LVH. 
(21) Losartan COMPARED_WITH 
Atenolol 
140
 SCALE:Effectiveness 
 Losartan HIGHER_THAN 
Atenolol 
The inferiority construction in (22) is expressed as 
the predications in (23).  
(22) Morphine-6-glucoronide was 
significantly less potent than 
morphine in producing pupil 
constriction. 
(23) morphine-6-glucoronide 
COMPARED_WITH Morphine 
 SCALE:Potency 
 morphine-6-glucoronide 
LOWER_THAN Morphine 
3.4 Accommodating negation  
Negation in comparative structures affects the 
position of the compared terms on the scale, and is 
accommodated differently for equality and for 
inequality. When a scalar comparison of equality 
(pattern S1, as ADJ as) is negated, the primary 
term is lower on the scale than the secondary 
(rather than being at least equal). For example, in 
interpreting the negated equality construction in 
(24), SemRep produces (25). 
(24) Amoxicillin-clavulanate was 
not as effective as ciprofloxacin 
for treating uncomplicated bladder 
infection in women. 
(25) Amoxicillin-clavulanate 
COMPARED_WITH Ciprofloxaci 
 SCALE:Effectiveness 
 Amoxicillin-clavulanate 
LOWER_THAN Ciprofloxacin 
For patterns of inequality, SemRep negates the 
predication indicating position on the scale. For 
example, the predications in (27) represent the 
negated superiority comparison in (26). Negation 
of inferiority comparatives (e.g. ?X is not less 
effective than Y?) is extremely rare in our sample.  
(26) These data show that 
celecoxib is not better than 
diclofenac (P = 0.414) in terms of 
ulcer complications. 
(27) celecoxib COMPARED_WITH 
diclofenac 
 SCALE:Goodness  
 celecoxib NEG_HIGHER_THAN 
diclofenac 
3.5 Evaluation 
To evaluate the effectiveness of the developed 
methods we created a test set of 300 sentences 
containing comparative structures. These were 
extracted by the second author (who did not 
participate in the development of the methodology) 
from 3000 MEDLINE citations published later in 
date than the  citations used to develop the 
methodology. The citations were retrieved with a 
PubMed query specifying randomized controlled 
studies and comparative studies on drug therapy.  
Sentences containing direct comparisons of the 
pharmacological actions of two drugs expressed in 
the target structures (comp1 and comp2) were 
extracted starting from the latest retrieved citation 
and continuing until 300 sentences with 
comparative structures had been examined. These 
were annotated with the PubMed ID of the citation, 
names of two drugs (COMPARED_WITH 
predication), the scale on which they are compared 
(SCALE), and the relative position of the primary 
drug with respect to the secondary (SAME_AS, 
HIGHER_THAN, or LOWER_THAN).  
The test sentences were processed using 
SemRep and evaluated against the annotated test 
set. We then computed recall and precision in 
several ways: overall for all comparative 
structures, for comp1 structures only, and for 
comp2 structures only. To understand how the 
overall identification of comparatives is influenced 
by the components of the construction, we also 
computed recall and precision separately for drug 
names, scale, and position on scale (SAME_AS, 
HIGHER_THAN and LOWER_THAN taken 
together). Recall measures the proportion of 
manually annotated categories that have been 
correctly identified automatically. Precision 
measures what proportion of the automatically 
annotated categories is correct.  
In addition, the overall identification of 
comparative structures was evaluated using the F-
measure [Rijsbergen, 1979], which combines recall 
and precision. The F-measure was computed using 
macro-averaging and micro-averaging. Macro-
averaging was computed over each category first 
and then averaged over the three categories (drug 
names, scale, and position on scale). This approach 
gives equal weight to each category. In micro-
averaging (which gives an equal weight to the 
performance on each sentence) recall and precision 
141
were obtained by summing over all individual 
sentences. Because it is impossible to enumerate 
all entities and relations which are not drugs, scale, 
or position we did not use the classification error 
rate and other metrics that require computing of 
true negative values. 
4 Results 
Upon inspection of the SemRep processing results 
we noticed that the test set contained nine 
duplicates.  In addition, four sentences were not 
processed for various technical reasons. We report 
the results for the remaining 287 sentences, which 
contain 288 comparative structures occurring in 
168 MEDLINE citations. Seventy four citations 
contain 85 comp2 structures. The remaining 203 
structures are comp1.  
Correct identification of comparative structures 
of both types depends on two factors: 1) 
recognition of both drugs being compared, and 2) 
recognition of the presence of a comparative 
structure itself. In addition, correct identification of 
the comp2 structures depends on recognition of the 
scale on which the drugs are compared and the 
relative position of the drugs on the scale. Table 1 
presents recall, precision, and F-score reflecting 
these factors. 
 
Table 1. SemRep performance 
Task Recall Precision F-score
Overall 0.70 0.96 0.81 
Drug extraction 0.69 0.96 0.81 
Comp1 0.74 0.98 0.84 
Comp2  0.62 0.92 0.74 
Scale  0.62 1.00 0.77 
Position on scale 0.62 0.98 0.76 
 
We considered drug identification to be correct 
only if both drugs participating in the relationship 
were identified correctly. The recall results 
indicate that approximately 30% of the drugs and 
comparative structures of comp1, as well as 40% 
of comp2 structures, remain unrecognized; 
however, all components are identified with high 
precision. Macro-averaging over compared drug 
names, scale, and position on scale categories we 
achieve an F-score = 0.78. The micro-average 
score for 287 comparative sentences is 0.5. 
5 Discussion 
In examining SemRep errors, we determined that 
more than 60% of the false negatives (for both 
comp1 and comp2) were due to ?empty heads? 
[Chodorow et al, 1985; Guthrie et al, 1990], in 
which the syntactic head of a noun phrase does not 
reflect semantic thrust. Such heads prevent 
SemRep from accurately determining the semantic 
type and group of the noun phrase. In our sample, 
expressions interpreted as empty heads include 
those referring to drug dosage and formulations, 
such as extended release (the latter often 
abbreviated as XR). Examples of missed 
interpretations are in sentences (28) and (29), 
where the empty heads are in bold. Ahlers et al 
[Ahlers et al, 2007] discuss enhancements to 
SemRep for accommodating empty heads. These 
mechanisms are being incorporated into the 
processing for comparative structures.  
(28) Oxybutynin 15 mg was more 
effective than propiverine 20 mg 
in reducing symptomatic and 
asymptomatic IDCs in ambulatory 
patients. 
(29) Intravesical atropine was as 
effective as oxybutynin immediate 
release for increasing bladder 
capacity and it was probably 
better with less antimuscarinic 
side effects 
False positives were due exclusively to word 
sense ambiguity. For example, in (30) bid (twice a 
day) was mapped to the concept ?BID protein?, 
which belongs to the semantic group Chemicals & 
Drugs. The most recent version of MetaMap, 
which will soon be called by comparative 
processing, exploits word sense disambiguation 
[Humphrey et al, 2006] and will likely resolve 
some of these errors.  
(30) Retapamulin ointment 1% (bid) 
for 5 days was as effective as 
oral cephalexin (bid) for 10 days 
in treatment of patients with SID, 
and was well tolerated. 
Although, in this paper, we tested the method on 
structures in which the compared terms belong to 
the semantic group Chemicals & Drugs, we can 
straightforwardly generalize the method by adding 
other semantic groups to the algorithm. For 
142
example, if SemRep recognized the noun phrases 
in bold in (31) and (32) as belonging to the group 
Procedures, comparative processing could proceed 
as for Chemicals & Drugs.  
(31) Comparison of multi-slice 
spiral CT and magnetic resonance 
imaging in evaluation of the un-
resectability of blood vessels in 
pancreatic tumor. 
(32) Dynamic multi-slice spiral 
CT is better than dynamic magnetic 
resonance to some extent in 
evaluating the un-resectability of 
peripancreatic blood vessels in 
pancreatic tumor. 
The semantic predications returned by SemRep 
to represent comparative expressions can be 
considered a type of executable knowledge that 
supports reasoning. Since the arguments in these 
predications have been mapped to the UMLS, a 
structured knowledge source, they can be 
manipulated using that knowledge. It is also 
possible to compute the transitive closure of all 
SemRep output for a collection of texts to 
determine which drug was asserted in that 
collection to be the best with respect to some 
characteristic. This ability could be very useful in 
supporting question-answering applications. 
As noted earlier, it is common in reporting on 
the results of randomized clinical trials and 
systematic reviews that a comp1 structure appears 
early in the discourse to announce the objectives of 
the study and that a comp2 structure often appears 
near the end to give the results. Another example 
of this phenomenon appears in (33) and (34) (from 
PMID 15943841).  
(33) To compare the efficacy of 
famotidine and omeprazole in 
Japanese patients with non-erosive 
gastro-oesophageal reflux disease 
by a prospective randomized 
multicentre trial. 
(34) Omeprazole is more effective 
than famotidine for the control of 
gastro-oesophageal reflux disease 
symptoms in H. pylori-negative 
patients. 
We suggest one example of an application that 
can benefit from the information provided by the 
knowledge inherent in the semantic interpretation 
of comparative structures, and that is the 
interpretation of outcome statements in MEDLINE 
citations, as a method for supporting automatic 
access to the latest results from clinical trials 
research. 
6 Conclusion 
We expanded a symbolic semantic interpreter to 
identify comparative constructions in biomedical 
text. The method relies on underspecified syntactic 
analysis and domain knowledge from the UMLS.  
We identify two compared terms and scalar 
comparative structures in MEDLINE citations. 
Although we restricted the method to comparisons 
of drug therapies, the method can be easily 
generalized to other entities such as diagnostic and 
therapeutic procedures. The availability of this 
information in computable format can support the 
identification of outcome sentences in MEDLINE, 
which in turn supports translation of biomedical 
research into improvements in quality of patient 
care. 
Acknowledgement This study was supported in 
part by the Intramural Research Programs of the 
National Institutes of Health, National Library of 
Medicine. 
References  
Ahlers C, Fiszman M, Demner-Fushman D, Lang F, 
Rindflesch TC. 2007. Extracting semantic 
predications from MEDLINE citations for 
pharmacogenomics. Pacific Symposium on 
Biocomputing  12:209-220. 
Aronson AR. 2001. Effective mapping of biomedical 
text to the UMLS Metathesaurus: The MetaMap 
program. Proc AMIA Symp, 17-21. 
Blaschke C, Andrade MA, Ouzounis C, and Valencia A. 
1999. Automatic extraction of biological information 
from scientific text: protein-protein interactions. 
Proceedings of the 7th International Conference on 
Intelligent Systems for Molecular Biology. Morgan 
Kaufman Publishers, San Francisco, CA. 
Christensen L, Haug PJ, and Fiszman M. 2002. 
MPLUS: A probabilistic medical language 
understanding system. Proceedings of the Workshop 
on Natural Language Processing in the Biomedical 
Domain, Association for Computational Linguistics, 
29-36. 
Chodorow MS, Byrd RI, and Heidom GE. 1985. 
Extracting Semantic Hierarchies from a Large On-
143
Line Dictionary. Proceedings of the 23rd Annual 
Meeting of the Association for Computational 
Linguistics, 299-304. 
Chun HW, Tsuruoka Y, Kim J-D, Shiba R, Nagata N, 
Hishiki T, and Tsujii J. 2006, Extraction of gene-
disease relations from Medline using domain 
dictionaries and machine learning. Pac Symp 
Biocomput, 4-15. 
Friedman C. 1989. A general computational treatment 
of the comparative. Proc 27th Annual Meeting Assoc 
Comp Linguistics, 161-168. 
Friedman C, Alderson PO, Austin JH, Cimino JJ, and 
Johnson SB. 1994.  A general natural-language text 
processor for clinical radiology. J Am Med Inform 
Assoc, 1(2):161-74. 
Friedman C, Kra P, Yu H, Krauthammer M, and 
Rzhetsky A. 2001.  GENIES: a natural-language 
processing system for the extraction of molecular 
pathways from journal articles. Bioinformatics, 17 
Suppl 1:S74-S82. 
Guthrie L, Slater BM, Wilks Y, Bruce R. 1990. Is there 
content in empty heads? Proceedings of the 13th 
Conference on Computational Linguistics, v3:138 ? 
143.   
Hahn U, Romacker M, and Schulz S. 2002. 
MEDSYNDIKATE--a natural language system for 
the extraction of medical information from findings 
reports. Int J Med Inf, 67(1-3):63-74. 
Huddleston R, and Pullum GK. 2002. The Cambridge 
Grammar of the English Language. Cambridge 
University Press, Cambridge, UK. 
Humphrey SM, Rogers WJ, Kilicoglu H, Demner-
Fushman D, Rindflesch TC. 2006. Word sense 
disambiguation by selecting the best semantic type 
based on Journal Descriptor Indexing: Preliminary 
experiment. J Am Soc Inf SciTech 57(1):96-113. 
Humphreys BL, Lindberg DA, Schoolman HM, and 
Barnett OG. 1998. The Unified Medical Language 
System: An informatics research collaboration. J Am 
Med Inform Assoc, 5(1):1-11. 
Jindal, Nitin and Bing Liu. 2006. Identifying 
comparative sentences in text documents. 
Proceedings of the 29th Annual International ACM 
SIGIR Conference on Research & Development on 
Information Retrieval. 
Johnson SB, Aguirre A, Peng P, and Cimino J. 1993. 
Interpreting natural language queries using the 
UMLS. Proc Annu Symp Comput Appl Med Care, 
294-8. 
Leroy G, Chen H, and Martinez JD. 2003 A shallow 
parser based on closed-class words to capture 
relations in biomedical text. J Biomed Inform, 
36(3):145-158. 
Lussier YA, Borlawsky T, Rappaport D, Liu Y, and 
Friedman C. 2006 PhenoGO: assigning phenotypic 
context to Gene Ontology annotations with natural 
language processing. Pac Symp Biocomput, 64-75. 
McCray AT, Srinivasan S, and Browne AC. 1994. 
Lexical methods for managing variation in 
biomedical terminologies. Proc Annu Symp Comput 
Appl Med Care, 235-9. 
McCray AT, Burgun A, and Bodenreider O. 2001 
Aggregating UMLS semantic types for reducing 
conceptual complexity. Medinfo, 10(Pt 1): 216-20. 
Rayner M and Banks A. 1990. An implementable 
semantics for comparative constructions. 
Computational Linguistics, 16(2):86-112. 
Rindflesch TC. 1995. Integrating natural language 
processing and biomedical domain knowledge for 
increased information retrieval effectiveness. Proc 
5th  Annual Dual-use Technologies and Applications 
Conference, 260-5. 
Rindflesch TC and Fiszman M. 2003. The interaction of 
domain knowledge and linguistic structure in natural 
language processing: Interpreting hypernymic 
propositions in biomedical text. J Biomed Inform, 
36(6):462-77. 
Rindflesch TC, Marcelo Fiszman , and Bisharah Libbus.  
2005. Semantic interpretation for the biomedical 
research literature. Medical informatics: Knowledge 
management and data mining in biomedicine. 
Springer, New York, NY. 
Rijsbergen V. 1979.  Information Retrieval, 
Butterworth-Heinemann, Newton, MA. 
Ryan K. 1981. Corepresentational grammar and parsing 
English comparatives. Proc 19th Annual Meeting  
Assoc Comp Linguistics, 13-18. 
Smith L, Rindflesch T, and Wilbur WJ. 2004. MedPost: 
a part-of-speech tagger for biomedical text. 
Bioinformatics, 20(14):2320-1. 
Staab S and Hahn U. Comparatives in context. 1997. 
Proc 14th National Conference on Artificial 
Intelligence and 9th Innovative Applications of 
Artificial Intelligence Conference, 616-621. 
Yen YT, Chen B, Chiu HW, Lee YC, Li YC, and Hsu 
CY. 2006. Developing an NLP and IR-based 
algorithm for analyzing gene-disease relationships.
 
144
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 46?54,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Arguments of Nominals in Semantic Interpretation of Biomedical Text 
 
Halil Kilicoglu,1,2 Marcelo Fiszman,2 Graciela Rosemblat,2  
Sean Marimpietri,3 Thomas C. Rindflesch2 
1Concordia University, Montreal, QC, Canada 
2National Library of Medicine, Bethesda, MD, USA 
3University of California, Berkeley, CA, USA 
h_kilico@cse.concordia.ca, sean.marimpietri@gmail.com  
{fiszmanm,grosemblat,trindflesch}@mail.nih.gov 
 
Abstract 
Based on linguistic generalizations, we 
enhanced an existing semantic processor, 
SemRep, for effective interpretation of a 
wide range of patterns used to express 
arguments of nominalization in clinically 
oriented biomedical text. Nominaliza-
tions are pervasive in the scientific litera-
ture, yet few text mining systems ade-
quately address them, thus missing a 
wealth of information. We evaluated the 
system by assessing the algorithm inde-
pendently and by determining its contri-
bution to SemRep generally. The first 
evaluation demonstrated the strength of 
the method through an F-score of 0.646 
(P=0.743, R=0.569), which is more than 
20 points higher than the baseline. The 
second evaluation showed that overall 
SemRep results were increased to F-score 
0.689 (P=0.745, R=0.640), approximate-
ly 25 points better than processing with-
out nominalizations. 
1 Introduction 
Extracting semantic relations from text and 
representing them as predicate-argument struc-
tures is increasingly seen as foundational for 
mining the biomedical literature (Kim et al, 
2008). Most research has focused on relations 
indicated by verbs (Wattarujeekrit et al, 2004; 
Kogan et al, 2005). However nominalizations, 
gerunds, and relational nouns also take argu-
ments. For example, the following sentence has 
three nominalizations, treatment, suppression, 
and lactation (nominalized forms of the verbs 
treat, suppress, and lactate, respectively). Agon-
ist is derived from agonize, but indicates an 
agent rather than an event. 
Bromocriptine, an ergot alkaloid dopamine 
agonist, is a recent common treatment for 
suppression of lactation in postpartum wom-
en. 
In promoting economy of expression, nomina-
lizations are pervasive in scientific discourse, 
particularly the molecular biology sublanguage, 
due to the highly nested and complex biomolecu-
lar interactions described (Friedman et al, 2002). 
However, Cohen et al (2008) point out that no-
minalizations are more difficult to process than 
verbs. Although a few systems deal with them, 
the focus is often limited in both the nominaliza-
tions recognized and the patterns used to express 
their arguments.  Inability to interpret nominal 
constructions in a general way limits the effec-
tiveness of such systems, since a wealth of 
knowledge is missed.  
In this paper, we discuss our recent work on 
interpreting nominal forms and their arguments. 
We concentrate on nominalizations; however, the 
analysis also applies to other argument-taking 
nouns. Based on training data, we developed a 
set of linguistic generalizations and enhanced an 
existing semantic processor, SemRep, for effec-
tive interpretation of a wide range of patterns 
used to express arguments of nominalization in 
clinically oriented biomedical text. We evaluated 
the enhancements in two ways: by examining the 
ability to identify arguments of nominals inde-
pendently and the effect these enhancements had 
on the overall quality of SemRep output. 
2 Background 
The theoretical linguistics literature has ad-
dressed the syntax of nominalizations (e.g. 
Chomsky, 1970; Grimshaw, 1990; Grimshaw 
and Williams, 1993), however, largely as support 
for theoretical argumentation, rather than de-
tailed description of the facts. Quirk et al (1985) 
concentrate on the morphological derivation of 
46
nominalizations from verbs. Within the context 
of NomBank, a project dedicated to annotation of 
argument structure, Meyers et al (2004a) de-
scribe the linguistics of nominalizations, empha-
sizing semantic roles.  However, major syntactic 
patterns of argument realization are also noted. 
Cohen et al (2008) provide a comprehensive 
overview of nominalizations in biomedical text. 
They include a review of the relevant literature, 
and discuss a range of linguistic considerations, 
including morphological derivation, passiviza-
tion, transitivity, and semantic topics (e.g. 
agent/instrument (activator) vs. ac-
tion/process/state (activation)). Based on an 
analysis of the PennBioIE corpus (Kulick et al, 
2004), detailed distributional results are provided 
on alternation patterns for several nominaliza-
tions with high frequency of occurrence in bio-
medical text, such as activation and treatment.  
In computational linguistics, PUNDIT (Dahl 
et al, 1987) exploited similarities between nomi-
nalizations and related verbs.  Hull and Gomez 
(1996) describe semantic interpretation for a li-
mited set of nominalizations, relying on Word-
Net (Fellbaum, 1998) senses for restricting fillers 
of semantic roles. Meyers et al (1998) present a 
procedure which maps syntactic and semantic 
information for verbs into a set of patterns for 
nominalizations. They use NOMLEX (MacLeod 
et al, 1998), a nominalization lexicon, as the ba-
sis for this transformation. More recently, the 
availability of the NomBank corpus (Meyers et 
al., 2004b) has supported supervised machine 
learning for nominal semantic role labeling (e.g. 
Pradhan et al, 2004; Jiang and Ng, 2006; Liu 
and Ng, 2007). In contrast, Pad? et al (2008) use 
unsupervised machine learning for semantic role 
labeling of eventive nominalizations by exploit-
ing similarities between the argument structure 
of event nominalizations and corresponding 
verbs. Gurevich and Waterman (2009) use a 
large parsed corpus of Wikipedia to derive lexi-
cal models for determining the underlying argu-
ment structure of nominalizations.   
Nominalizations have only recently garnered 
attention in biomedical language processing. Ge-
neScene (Leroy and Chen, 2005) considers only 
arguments of nominalizations marked by prepo-
sitional cues. Similarly, Schuman and Bergler 
(2006) focus on the problem of prepositional 
phrase attachment. In the BioNLP?09 Shared 
Task on Event Extraction (Kim et al, 2009), the 
most frequent predicates were nominals. Several 
participating systems discuss techniques that ac-
commodate nominalizations (e.g. K. B. Cohen et 
al., 2009; Kilicoglu and Bergler, 2009). Nomina-
lizations have not previously been addressed in 
clinically oriented text.  
2.1 SemRep 
SemRep (Rindflesch and Fiszman, 2003) auto-
matically extracts semantic predications (logical 
subject-predicate-logical object triples) from un-
structured text (titles and abstracts) of MED-
LINE citations.  It uses domain knowledge from 
the Unified Medical Language System? (UMLS 
?) (Bodenreider, 2004), and the interaction of 
this knowledge and (underspecified) syntactic 
structure supports a robust system. SemRep ex-
tracts a range of semantic predications relating to 
clinical medicine (e.g. TREATS, DIAGNOSES, AD-
MINISTERED_TO, PROCESS_OF, LOCATION_OF), 
substance interactions (INTERACTS_WITH, INHI-
BITS, STIMULATES), and genetic etiology of dis-
ease (ASSOCIATED_WITH, PREDISPOSES, CAUS-
ES). For example, the program identifies the fol-
lowing predications from input text MRI re-
vealed a lacunar infarction in the left internal 
capsule. Arguments are concepts from the 
UMLS Metathesaurus and predicates are rela-
tions from the Semantic Network.  
Magnetic Resonance Imaging DIAGNOSES Infarc-
tion, Lacunar  
Internal Capsule LOCATION_OF Infarction, Lacu-
nar 
Processing relies on an underspecified syntac-
tic analysis based on the UMLS SPECIALIST 
Lexicon (McCray et al, 1994) and the MedPost 
part-of-speech tagger (Smith et al, 2004). Output 
includes phrase identification, and for simple 
noun phrases, labeling of heads and modifiers.  
[HEAD(MRI)] [revealed] [a MOD(lacunar), 
HEAD(infarction)] [in the MOD(left) MOD(internal), 
HEAD(capsule).] 
MetaMap (Aronson and Lang, 2010) maps sim-
ple noun phrases to UMLS Metathesaurus con-
cepts, as shown below. Associated semantic 
types are particularly important for subsequent 
processing. 
[HEAD(MRI){Magnetic Resonance Imaging (Di-
agnostic Procedure)}] [revealed] [a 
MOD(lacunar), HEAD(infarction) {Infarction, Lacu-
nar(Disease or Syndrome)}] [in the MOD(left) 
MOD(internal), HEAD(capsule) {Internal Cap-
sule(Body Part, Organ, or Organ Component)}.] 
47
This structure is the basis for extracting semantic 
predications, which relies on several mechan-
isms. Indicator rules map syntactic phenomena, 
such as verbs, nominalizations, prepositions, and 
modifier-head structure in the simple noun 
phrase to ontological predications. Examples in-
clude: 
reveal (verb) ? DIAGNOSES 
in (prep) ? LOCATION_OF 
SemRep currently has 630 indicator rules. Onto-
logical predications are based on a modified ver-
sion of the UMLS Semantic Network and have 
semantic types as arguments. For example:  
Diagnostic Procedure DIAGNOSES Disease or 
Syndrome 
Body Part, Organ, or Organ Component LOCA-
TION_OF Disease or Syndrome 
Construction of a semantic predication begins 
with the application of an indicator rule, and is 
then constrained by two things. Arguments must 
satisfy syntactic restrictions for the indicator and 
must have been mapped to Metathesaurus con-
cepts that match the arguments of the ontological 
predication indicated. As part of this processing, 
several syntactic phenomena are addressed, in-
cluding passivization, argument coordination, 
and some types of relativization. For both verb 
and preposition indicators, underspecified syn-
tactic rules simply ensure that subjects are on the 
left and objects on the right. Enhancing SemRep 
for nominalizations involved extending the syn-
tactic constraints for arguments of nominaliza-
tion indicators.  
3 Methods 
In order to gain insight into the principles under-
lying expression of nominal arguments, we first 
determined the 50 most common nominalizations 
in MEDLINE citations that also occur in the 
UMLS SPECIALIST Lexicon, and then analyzed 
a corpus of 1012 sentences extracted from 476 
citations containing those nominalizations. We 
further limited these sentences to those with no-
minalizations containing two overt arguments 
(since SemRep only extracts predications with 
two arguments), resulting in a final set of 383 
sentences. We determined 14 alternation patterns 
for nominalizations based on this analysis and 
devised an algorithm to accommodate them. We 
then conducted two evaluations, one to assess the 
effectiveness of the algorithm independently of 
other considerations and another to assess the 
contribution of enhanced nominalization 
processing to SemRep generally.  
3.1 Nominal Alternations  
Much work in identifying arguments of nomina-
lizations assigns semantic role, such as agent, 
patient, etc., but SemRep does not. In this analy-
sis, arguments are logical subject and object. Re-
lational nouns often allow only one argument 
(e.g. the weight of the evidence), and either one 
or both of the arguments of a nominalization or 
gerund may be left unexpressed. SemRep doesn?t 
interpret nominalizations with unexpressed ar-
guments. If both arguments appear, they fall into 
one of several patterns, and the challenge in no-
minalization processing is to accommodate these 
patterns. Cohen et al (2008) note several such 
patterns, including those in which both argu-
ments are to the right of the nominalization, cued 
by prepositions (treatment of fracture with sur-
gery), the nominalization separates the argu-
ments (fracture treatment with surgery, surgical 
treatment for fracture), and both arguments pre-
cede the nominalizations, as modifiers of it (sur-
gical fracture treatment and fracture surgical 
treatment).  
Cohen et al (2008) do not list several patterns 
we observed in the clinical domain, including 
those in which the subject appears to the right 
marked by a verb (the treatment of fracture is 
surgery) or as an appositive (the treatment of 
fracture, surgery), and those in which the subject 
appears to the left and the nominalization is ei-
ther in a prepositional phrase (surgery in the 
treatment of fracture, surgery in fracture treat-
ment) or is preceded by a verb or is parenthetical 
(surgery is (the best) treatment for fracture; sur-
gery is (the best) fracture treatment; surgery, the 
best fracture treatment). One pattern, in which 
both arguments are on the right and the subject 
precedes the object, is seen most commonly in 
the clinical domain when the nominalization has 
a lexically specified cue (e.g. the contribution of 
stem cells to kidney repair). The nominal alterna-
tion patterns are listed in Table 1. 
Generalizations about arguments of nominali-
zations are based on the position of the argu-
ments, both with respect to each other and to the 
nominalization, and whether they modify the 
nominalization or not. A modifying argument is 
internal to the simple noun phrase of which the 
nominalization is the head; other arguments 
(both to the left and to the right) are external. 
(Relativization is considered external to the sim-
ple noun phrase.) 
48
  [NOM] [PREP OBJ] [PREP SUBJ]  
Treatment of fracture with surgery 
[NOM] [PREP OBJ], [SUBJ] 
The treatment of fracture, surgery 
[NOM] [PREP OBJ] ([SUBJ]) 
The treatment of fracture (surgery) 
[NOM] [PREP OBJ] [BE] [SUBJ] 
The treatment of fracture is surgery 
[NOM] [PREP SUBJ] [PREP OBJ] 
Treatment with surgery of fracture 
[SUBJ NOM] [PREP OBJ] 
Surgical treatment of fracture 
[SUBJ] [PREP NOM] [PREP OBJ] 
Surgery in the treatment of fracture 
[SUBJ] [BE] [NOM] [PREP OBJ] 
Surgery is the treatment of fracture 
[OBJ NOM] [BE] [SUBJ] 
Fracture treatment is surgery 
[OBJ NOM] [PREP SUBJ] 
Fracture treatment with surgery 
[SUBJ] [PREP OBJ NOM] 
Surgery for fracture treatment 
[SUBJ] [BE] [OBJ NOM] 
Surgery is the fracture treatment 
[SUBJ OBJ NOM] 
Surgical fracture treatment 
[OBJ SUBJ NOM] 
Fracture surgical treatment 
Table 1. Patterns 
Argument cuing plays a prominent role in de-
fining these patterns. A cue is an overt syntactic 
element associated with an argument, and can be 
a preposition, a verb (most commonly a form of 
be), a comma, or parenthesis. A cued argument is 
in a dependency with the cue, which is itself in a 
dependency with the nominalization. The cue 
must occur between the nominalization and the 
argument, whether the argument is to the right 
(e.g. treatment of fracture) or to the left (e.g. 
surgery in the treatment). Prepositional cues for 
the objects of some nominalizations are stipu-
lated in the lexicon; some of these are obligatory 
(e.g. contribution ? to), while others are optional 
(treatment ? for).  
External arguments of nominalizations must 
be cued, and cues unambiguously signal the role 
of the argument, according to the following 
cuing rules (Cohen et al, 2008). Verbs, comma, 
parenthesis, and the prepositions by, with, and 
via cue subjects only. (By is used for semantic 
role agent and with for instrument, but SemRep 
does not exploit this distinction.) Of cues sub-
jects only if the nominalization has an obligatory 
(object) cue; it must cue objects otherwise. There 
is a class of nominalizations (e.g. cause) that do 
not allow a prepositionally cued subject. Consi-
derable variation is seen in the order of subject 
and object; however, if the subject intervenes 
between the nominalization and the object, both 
must have equal cuing status (the only possibili-
ties are that both be either uncued or cued with a 
preposition).  
3.2 Algorithm 
In extending SemRep for identifying arguments 
of nominalizations, existing machinery was ex-
ploited, namely shallow parsing, mapping simple 
noun phrases to Metathesaurus concepts, and the 
application of indicator rules to map nominaliza-
tions to enhanced Semantic Network ontological 
predications (which imposes restrictions on the 
semantic type of arguments). Finally, syntactic 
argument identification was enhanced specifical-
ly for nominalizations and exploits the linguistic 
generalizations noted. For example in the sen-
tence below, phrases have been identified and 
cervical cancer has been mapped to the Metathe-
saurus concept ?Cervix carcinoma? with seman-
tic type ?Neoplastic Process?, and vaccination to 
?Vaccination? (?Therapeutic or Preventive Pro-
cedure?). An indicator rule for prevention maps 
to the ontological predication ?Therapeutic or 
Preventive Procedure PREVENTS Neoplastic 
Process? (among others) in generating the predi-
cation: ?Vaccination PREVENTS Cervix carcino-
ma.? 
Therefore, prevention of cervical cancer with 
HPV vaccination may have a significant fi-
nancial impact.  
Processing to identify arguments for preven-
tion begins by determining whether the nomina-
lization has a lexically specified object cue. This 
information is needed to determine the cuing 
function of of. Since it is common for there to be 
at least one argument on the right, identification 
of arguments begins there. Arguments on the 
right are external and must be cued. If a cued 
argument is found, its role is determined by the 
argument cuing rules. Since prevention does not 
have a lexically specified cue, of marks its ob-
ject. Further, the semantic type of the concept for 
the object of of matches the object of the onto-
logical predication (?Neoplastic Process?).  
The algorithm next looks to the right of the 
first argument for the second argument. Since 
processing addresses only two arguments for 
nominalizations, subject and object, once the role 
49
of the first has been determined, the second can 
be inferred. For cued arguments, the process 
checks that the cue is compatible with the cuing 
rules. In all cases, the relevant semantic type 
must match the subject of the ontological predi-
cation. In this instance, with cues subjects and 
?Therapeutic or Preventive Process? matches the 
subject of the ontological predication indicated.  
If only one noun phrase to the right satisfies 
the argument cuing rules, the second argument 
must be on the left. A modifier immediately to 
the left of the nominalization (and thus an inter-
nal argument) is sought first, and its role inferred 
from the first argument. Since internal arguments 
are not cued, there is no need to ensure cuing 
compatibility. The predication ?Operative Sur-
gical Procedures TREATS Pregnancy, Ectopic? 
is found for resolution in  
Surgical resolution of an ectopic pregnancy 
in a captive gerenuk (Litocranius walleri wal-
leri).  
Resolution is an indicator for the ontological 
predication ?Therapeutic or Preventive Proce-
dure TREATS Disease or Syndrome.? Surgical 
maps to ?Operative Surgical Procedures? (?The-
rapeutic or Preventive Procedure?), which 
matches the subject of this predication, and ec-
topic pregnancy maps to ?Pregnancy, Ectopic? 
(?Disease or Syndrome?), which matches its ob-
ject.  Of marks the object of resolution. 
An argument to the left of a nominalization 
may be external, in which case a cue is neces-
sary. For preceding treatment satisfies this re-
quirement in the following sentence. 
Preclinical data have supported the use of 
fludarabine and cyclophosphamide (FC) in 
combination for the treatment of indolent 
lymphoid malignancies. 
The two drugs in this sentence map to concepts 
with semantic type ?Pharmacologic Substance? 
and the malignancy has ?Neoplastic Process?, as 
above. There is an ontological predication for 
TREATS with subject ?Pharmacologic Substance?. 
After coordination processing in SemRep, two 
predications are generated for treatment:  
Cyclophosphamide TREATS Malignant lymphoid 
neoplasm 
Fludarabine TREATS Malignant lymphoid neop-
lasm 
If there is no argument to the right, both ar-
guments must be on the left. A modifier imme-
diately to the left of the nominalization is sought 
first. Given the properties of cuing (the cue in-
tervenes between the argument and the nominali-
zation), if both arguments occur to the left, at 
least one of them must be internal, since it is not 
possible to have more than one external argu-
ment on the left (e.g. *Surgery is fracture for 
treatment). The role of the first argument is 
found based on semantic type. The first modifier 
to the left of treatment in the following sentence 
is epilepsy, which has semantic type ?Disease or 
Syndrome?, matching the object of the ontologi-
cal predication for TREATS.  
Patients with most chances of benefiting from 
surgical epilepsy treatment 
The second modifier to the left, surgical maps to 
the concept ?Operative Surgical Procedures,? 
whose semantic type matches the subject of the 
ontological predication. These conditions allow 
construction of the predication ?Operative Sur-
gical Procedures TREATS Epilepsy.?   
In the next sentence, the indicator rule for pre-
diction maps to the ontological predication 
?Amino Acid, Peptide, or Protein PREDISPOSES 
Disease or Syndrome.?  
The potential clinical role of measuring these 
apolipoproteins for ischemic stroke predic-
tion warrants further study. 
Ischemic stroke satisfies the object of this predi-
cation and apolipoproteins the subject. Since the 
external subject is cued by for, all constraints are 
satisfied and the predication ?Apolipoproteins 
PREDISPOSES Ischemic stroke? is generated.   
3.3 Evaluation 
Three-hundred sentences from 239 MEDLINE 
citations (titles and abstracts) were selected for 
annotating a test set. Some had previously been 
selected for various aspects of SemRep evalua-
tion; others were chosen randomly. A small 
number (30) were sentences in the GENIA event 
corpus (Kim et al, 2008) with bio-event-
triggering nominalizations. Annotation was con-
ducted by three of the authors. One, a linguist 
(A), judged all sentences, while the other two, a 
computer scientist (B) and a medical informatics 
researcher (C), annotated a subset. Annotation 
was not limited to nominalizations. The statistics 
regarding the individual annotations are given 
below. The numbers in parentheses show the 
number of annotated predications indicated by 
nominalizations. 
 
50
Annotator # of Sentences # of Predications 
A  300 533 (286) 
B 200 387 (190) 
C 132 244 (134) 
Table 2. Annotation statistics 
As guidance, annotators were provided UMLS 
Metathesaurus concepts for the sentences. How-
ever, they consulted the Metathesaurus directly 
to check questionable mappings. Annotation fo-
cused on the 25 predicate types SemRep ad-
dresses.  
We measured inter-annotator agreement, de-
fined as the F-score of one set of annotations, 
when the second is taken as the gold standard. 
After individual annotations were complete, two 
annotators (A and C) assessed all three sets of 
annotations and created the final reference stan-
dard. The reference standard has 569 predica-
tions, 300 of which (52.7%) are indicated by 
nominalizations. We further measured the 
agreement between individual sets of annotations 
and the reference standard. Results are given be-
low: 
 
Annotator pair # of Sentences IAA 
A-B  200 0.794 
A-C 132 0.974 
B-C 103 0.722 
A-Gold 300 0.925 
B-Gold 200 0.889 
C-Gold 132 0.906 
Table 3. Inter-annotator agreement 
We performed two evaluations. The first (ev-
al1) evaluated nominalizations in isolation, while 
the second (eval2) assessed the effect of the en-
hancements on overall semantic interpretation in 
SemRep. For eval1, we restricted SemRep to 
extract predications indicated by nominalizations 
only. The baseline was a nominalization argu-
ment identification rule which simply stipulates 
that the subject of a predicate is a concept to the 
left (starting from the modifier of the nominali-
zation, if any), and the object is a concept to the 
right. This baseline implements the underspecifi-
cation principle of SemRep, without any addi-
tional logic. We compared the results from this 
baseline to those from the algorithm described 
above to identify arguments of nominalizations. 
The gold standard for eval1 was limited to predi-
cations indicated by nominalizations.  
We investigated the effect of nominalization 
processing on SemRep generally in eval2, for 
which the baseline implementation was SemRep 
with no nominalization processing. The results 
for this baseline were evaluated against those 
obtained using SemRep with no restrictions. 
Typical evaluation metrics, precision, recall, and 
F-score, were calculated. 
4 Results and Discussion 
The results for the two evaluations are presented 
below.  
 Precision Recall F-Score 
eval1 
Baseline 0.484 0.359 0.412 
With NOM 0.743 0.569 0.645 
 
eval2 
Baseline 0.640 0.333 0.438 
With NOM 0.745 0.640 0.689 
Table 4. Evaluation results 
Results illustrate the importance of nominali-
zation processing for effectiveness of semantic 
interpretation and show that the SemRep metho-
dology naturally extends to this phenomenon. 
With a single, simple, rule (eval1 baseline), Se-
mRep achieves an F-score of 0.412. With addi-
tional processing based on linguistic generaliza-
tions, F-score improves more than 20 points. 
Further, the addition of nominalization 
processing not only enhances the coverage of 
SemRep (more than 30 points), but also increases 
precision (more than 10 points). While nominali-
zations are generally considered more difficult to 
process than verbs (Cohen et al, 2008), we were 
able to accommodate them with greater precision 
than other types of indicators, including verbs 
(0.743 vs. 0.64 in eval1 with NOM vs. eval2 
baseline) with our patterns.  
 Precision Recall F-Score 
eval1 
Baseline 0.233 0.140 0.175 
With NOM 0.690 0.400 0.506 
 
eval2 
Baseline (No 
NOM) 
0.667 0.278 0.392 
With NOM 0.698 0.514 0.592 
Table 5. Results for molecular biology sentences 
 
Limiting the evaluation to sentences focusing on 
biomolecular interactions (from GENIA), while 
not conclusive due to the small number of sen-
tences (30), also shows similar patterns, as 
shown in Table 5. As expected, while overall 
51
quality of predications is lower, since molecular 
biology text is significantly more complex than 
that in the clinical domain, improvements with 
nominalization processing are clearly seen. 
Errors were mostly due to aspects of SemRep 
orthogonal to but interacting with nominalization 
processing. Complex coordination structure was 
the main source of recall errors, as in the follow-
ing example.  
RESULTS: The best predictors of incident 
metabolic syndrome were waist circumfe-
rence (odds ratio [OR] 1.7 [1.3-2.0] per 11 
cm), HDL cholesterol (0.6 [0.4-0.7] per 15 
mg/dl), and proinsulin (1.7 [1.4-2.0] per 3.3 
pmol/l). [PMID 14988303] 
While the system was able to identify the predi-
cation ?Waist circumference PREDISPOSES 
Metabolic syndrome,? it was unable to find the 
predications below, due to its inability to identify 
the coordination of waist circumference, HDL 
cholesterol, and proinsulin.  
(FN) Proinsulin PREDISPOSES Metabolic syn-
drome 
(FN) High Density Lipoprotein Cholesterol PRE-
DISPOSES Metabolic syndrome 
 
Mapping of noun phrases to the correct UMLS 
concepts (MetaMap) is a source of both false 
positives and false negatives, particularly in the 
context of the molecular biology sentences, 
where acronyms and abbreviations are common 
and their disambiguation is nontrivial (Okazaki 
et al, 2010). For example, in the following sen-
tence  
PTK inhibition with Gen attenuated both 
LPS-induced NF-kappaB DNA binding and 
TNF-alpha production in human monocytes. 
[PMID 10210645] 
PTK was mapped to ?Ephrin receptor EphA8? 
rather than to ?Protein Tyrosine Kinase?, causing 
both a false positive and a false negative.  
(FP) Genistein INHIBITS Ephrin receptor EphA8 
(FN) Genistein INHIBITS Protein Tyrosine Kinase 
Some errors were due to failure to recognize a 
relative clause by SemRep. Only the head of 
such a structure is allowed to be an argument 
outside the structure. In the sentence below, the 
subject of treatment is hyperthermic intraperito-
neal intraoperative chemotherapy, which is the 
head of the reduced relative clause, after cytore-
ductive surgery.  
Hyperthermic intraperitoneal intraoperative 
chemotherapy after cytoreductive surgery for 
the treatment of abdominal sarcomatosis: 
clinical outcome and prognostic factors in 60 
consecutive patients. [PMID 15112276] 
SemRep failed to recognize the relative clause, 
and therefore the nominalization algorithm took 
the noun phrase inside it as the subject of treat-
ment, since it satisfies both semantic type and 
argument constraints.  
(FP) Cytoreductive surgery  TREATS Sarcamato-
sis NOS 
(FN) intraperitoneal therapy TREATS Sarcamato-
sis NOS 
A small number of errors were due solely to 
nominalization processing. In the following sen-
tence, the object of contribution is cued with in, 
rather than lexically specified to, which causes a 
recall error.  
Using SOCS-1 knockout mice, we investi-
gated the contribution of SOCS-1 in the 
development of insulin resistance induced 
by a high-fat diet (HFD). [PMID 
18929539] 
(FN) Cytokine Inducible SH-2 Containing Pro-
tein PREDISPOSES Insulin Resistance 
Accurate identification of the arguments of 
nominalizations in the molecular biology sub-
domain is more challenging than in clinically-
oriented text. Some of the syntactic structure re-
sponsible for this complexity is discussed by K. 
B. Cohen et al (2009). In particular, they note 
the problem of an argument being separated from 
the nominalization, and point out the problem of 
specifying the intervening structure. Although 
we have not focused on molecular biology, the 
analysis developed for clinical medicine shows 
promise in that domain as well. One relevant ex-
tension could address the syntactic configuration 
in which intervening structure involves an argu-
ment of a nominalization shared with a verb oc-
curring to the left of the nominalization, as in-
duced and activation interact in the following 
sentence: 
IL-2 induced less STAT1 alpha activation 
and IFN-alpha induced greater STAT5 acti-
vation in NK3.3 cells compared with preacti-
vated primary NK cells. [PMID 8683106] 
This could be addressed with an extension of our 
rule that subjects of nominalizations can be cued 
with verbs. With respect to argument identifica-
tion, induce can function like a form of be.  
52
5 Conclusion 
We discuss a linguistically principled implemen-
tation for identifying arguments of nominaliza-
tions in clinically focused biomedical text. The 
full range of such structures is rarely addressed 
by existing text mining systems, thus missing 
valuable information. The algorithm is imple-
mented inside SemRep, a general semantic inter-
preter for biomedical text. We evaluated the sys-
tem both by assessing the algorithm independent-
ly and by determining the contribution it makes 
to SemRep generally. The first evaluation re-
sulted in an F-score of 0.646 (P=0.743, 
R=0.569), which is 20 points higher than the 
baseline, while the second showed that overall 
SemRep results were increased to F-score 0.689 
(P=0.745, R=0.640), approximately 25 points 
better than processing without nominalizations. 
Since our nominalization processing is by ex-
tending SemRep, rather than by creating a dedi-
cated system, we provide the interpretation of 
these structures in a broader context. An array of 
semantic predications generated by mapping to 
an ontology (UMLS) normalizes the interpreta-
tion of verbs and nominalizations.  Processing is 
linguistically based, and several syntactic phe-
nomena are addressed, including passivization, 
argument coordination, and relativization. The 
benefits of such processing include effective ap-
plications for extracting information on genetic 
diseases from text (Masseroli et al, 2006), as 
well as research in medical knowledge summari-
zation (Fiszman et al, 2004; Fiszman et al, 
2009), literature-based discovery (Ahlers et al, 
2007; Hristovski et al, 2010), and enhanced in-
formation retrieval (Kilicoglu et al, 2008; T. 
Cohen et al, 2009). 
 
Acknowledgments 
This study was supported in part by the Intra-
mural Research Program of the National Insti-
tutes of Health, National Library of Medicine. 
References 
C. B. Ahlers, D. Hristovski, H. Kilicoglu, T. C. 
Rindflesch. 2007. Using the literature-based dis-
covery paradigm to investigate drug mechanisms. 
In Proceedings of AMIA Annual Symposium, pages 
6-10. 
A. R. Aronson and F.-M. Lang. 2010. An overview of 
MetaMap: historical perspective and recent ad-
vances. Journal of the American Medical Informat-
ics Association, 17:229-236. 
O. Bodenreider. 2004. The Unified Medical Language 
System (UMLS): integrating biomedical terminol-
ogy. Nucleic Acids Research, 32(Database is-
sue):D267-70.  
N. Chomsky. 1970. Remarks on nominalization. In 
Jacobs, Roderick, and Peter S. Rosenbaum (eds.) 
Readings in English transformational grammar. 
Boston: Ginn and Company, pages 184-221.  
K. B. Cohen, M. Palmer, L. Hunter. 2008. Nominali-
zation and alternations in biomedical language. 
PLoS ONE, 3(9): e3158. 
K. B. Cohen, K. H. Verspoor, H. L. Johnson, C. 
Roeder, P. V. Ogren, W. A. Baumgartner, E. 
White, H. Tipney, L. Hunter. 2009. High-precision 
biological event extraction with a concept recog-
nizer. In Proceedings of the BioNLP 2009 Work-
shop Companion Volume for Shared Task, pages 
50-58. 
T. Cohen, R. Schvaneveldt, T. C. Rindflesch. 2009. 
Predication-based semantic indexing: Permutations 
as a means to encode predications in semantic 
space. In Proceedings of AMIA Annual Symposium, 
pages 114-118. 
D. A. Dahl, M. S. Palmer, R. J. Passonneau. 1987. 
Nominalizations in PUNDIT. In Proceedings of 
ACL, pages 131-139. 
C. Fellbaum. 1998. WordNet: An Electronic Lexical 
Database. The MIT Press, Cambridge, MA. 
M. Fiszman, D. Demner-Fushman, H. Kilicoglu, T. C. 
Rindflesch. 2009. Automatic summarization of 
MEDLINE citations for evidence-based medical 
treatment: A topic-oriented evaluation. Journal of 
Biomedical Informatics, 42(5):801-813.  
M. Fiszman, T. C. Rindflesch, H. Kilicoglu. 2004. 
Abstraction summarization for managing the bio-
medical research literature. In Proceedings of 
HLT/NAACL Workshop on Computational Lexical 
Semantics, pages 76-83. 
C. Friedman, P. Kra, A. Rzhetsky. 2002. Two bio-
medical sublanguages: a description based on the 
theories of Zellig Harris. Journal of Biomedical In-
formatics, 35:222?235. 
J. Grimshaw. 1990.  Argument Structure. MIT Press, 
Cambridge, MA. 
J. Grimshaw and E. Williams. 1993. Nominalizations 
and predicative prepositional phrases. In J. Puste-
jovsky (ed.) Semantics and the Lexicon. Dordrecht: 
Kluwer Academic Publishers, pages  97-106.  
O. Gurevich and S. A. Waterman. 2009. Mining of 
parsed data to derive deverbal argument structure. 
In Proceedings of the 2009 Workshop on Grammar 
Engineering Across Frameworks. pages 19-27. 
D. Hristovski, A. Kastrin, B. Peterlin, T. C. 
Rindflesch. 2010. Combining semantic relations 
53
and DNA microarray data for novel hypothesis 
generation. In C. Blaschke, H. Shatkay (Eds.) 
ISMB/ECCB2009, Lecture Notes in Bioinformatics, 
Heidelberg: Springer-Verlag, pages 53-61.  
R. D. Hull and F. Gomez. 1996. Semantic interpreta-
tion of nominalizations. In Proceedings of AAAI, 
pages 1062-1068. 
Z. P. Jiang and H. T. Ng. 2006. Semantic role labeling 
of NomBank: A maximum entropy approach. In 
Proceedings of EMNLP?06, pages 138?145. 
H. Kilicoglu and S. Bergler. 2009. Syntactic depen-
dency based heuristics for biological event extrac-
tion. In Proceedings of the BioNLP 2009 Workshop 
Companion Volume for Shared Task, pages 119-
127. 
H. Kilicoglu, M. Fiszman, A. Rodriguez, D. Shin, A. 
M. Ripple, T. C. Rindflesch. 2008. Semantic 
MEDLINE: A Web application to manage the re-
sults of PubMed searches. In Proceedings of 
SMBM?08, pages 69-76. 
J-D. Kim, T. Ohta, S. Pyysalo, Y. Kano, J. Tsujii. 
2009. Overview of BioNLP?09 Shared Task on 
Event Extraction. In Proceedings of the BioNLP 
2009 Workshop Companion Volume for Shared 
Task, pages 1-9. 
J-D. Kim, T. Ohta, J. Tsujii. 2008. Corpus annotation 
for mining biomedical events from literature. BMC 
Bioinformatics, 9(1):10. 
Y. Kogan, N. Collier, S. Pakhomov, M. Krautham-
mer. 2005. Towards semantic role labeling & IE in 
the medical literature. In Proceedings of AMIA An-
nual Symposium, pages 410?414. 
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. 
McDonald, M. Palmer. A. Schein, L. Ungar. 2004. 
Integrated annotation for biomedical information 
extraction. In Proceedings of BioLINK: Linking 
Biological Literature, Ontologies and Databases, 
pages 61?68. 
G. Leroy and H. Chen. 2005.  Genescene: An ontolo-
gy-enhanced integration of linguistic and co-
occurrence based relations in biomedical texts. 
Journal of the American Society for Information 
Science and Technology, 56(5): 457?468. 
C. Liu and H. Ng. 2007. Learning predictive struc-
tures for semantic role labeling of NomBank. In 
Proceedings of ACL, pages 208?215. 
C. Macleod, R. Grishman, A. Meyers, L. Barrett, R. 
Reeves. 1998. NOMLEX: A lexicon of nominali-
zations. In Proceedings of EURALEX?98. 
M. Masseroli, H. Kilicoglu, F-M. Lang, T. C. 
Rindflesch. 2006. Argument-predicate distance as a 
filter for enhancing precision in extracting predica-
tions on the genetic etiology of disease. BMC Bio-
informatics, 7:291. 
A. T. McCray, S. Srinivasan, A. C. Browne. 1994. 
Lexical methods for managing variation in biomed-
ical terminologies. In Proceedings of 18th Annual 
Symposium on Computer Applications in Medical 
Care, pages 235?239. 
A. Meyers, C. Macleod, R. Yanbarger, R. Grishman, 
L. Barrett, R. Reeves. 1998. Using NOMLEX to 
produce nominalization patterns for information 
extraction. In Proceedings of the Workshop on 
Computational Treatment of Nominals (COL-
ING/ACL), pages 25-32. 
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. 
Zielinska, B. Young, R. Grishman. 2004a. Anno-
tating noun argument structure for NomBank. In 
Proceedings of LREC. 
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. 
Zielinska, B. Young, R. Grishman. 2004b. The 
NomBank project: An interim report. In Proceed-
ings of HLT-NAACL 2004 Workshop: Frontiers in 
Corpus Annotation, pages 24?31. 
N. Okazaki, S. Ananiadou, J. Tsujii. 2010. Building a 
high quality sense inventory for improved abbrevi-
ation disambiguation. Bioinformatics: btq129+. 
S. Pad?, M. Pennacchiotti, C. Sporleder. 2008. Se-
mantic role assignment for event nominalisations 
by leveraging verbal data. In Proceedings of CoL-
ing?08, pages 665-672. 
S. Pradhan, H. Sun, W. Ward, J. Martin, D. Jurafsky. 
2004. Parsing arguments of nominalizations in 
English and Chinese. In Proceedings of 
HLT/NAACL, pages 141?144. 
R. Quirk, S. Greenbaum, G. Leech, J. Svartvik. 1985. 
A Comprehensive Grammar of the English Lan-
guage. Longman, London. 
T. C. Rindflesch and M. Fiszman. 2003. The interac-
tion of domain knowledge and linguistic structure 
in natural language processing: Interpreting hyper-
nymic propositions in biomedical text. Journal of 
Biomedical Informatics, 36(6):462-77. 
J. Schuman and S. Bergler. 2006. Postnominal prepo-
sitional phrase attachment in proteomics. In Pro-
ceedings of BioNLP Workshop on Linking Natural 
Language Processing and Biology, pages 82?89. 
L. Smith, T. C. Rindflesch, W. J. Wilbur. 2004. Med-
Post: a part-of-speech tagger for biomedical text. 
Bioinformatics, 20(14):2320-2321. 
T. Wattarujeekrit, P. K. Shah, N. Collier. 2004. PAS-
Bio: Predicate-argument structures for event ex-
traction in molecular biology. BMC Bioinformat-
ics, 5:155.  
 
54
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 54?62,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Interpreting Consumer Health Questions: The Role of Anaphora and 
Ellipsis 
 
 
Halil Kilicoglu, Marcelo Fiszman, Dina Demner-Fushman 
Lister Hill National Center for Biomedical Communications  
National Library of Medicine  
Bethesda, MD, USA 
{kilicogluh, fiszmanm, ddemner}@mail.nih.gov 
 
  
 
Abstract 
While interest in biomedical question answer-
ing has been growing, research in consumer 
health question answering remains relatively 
sparse. In this paper, we focus on the task of 
consumer health question understanding. We 
present a rule-based methodology that relies 
on lexical and syntactic information as well as 
anaphora/ellipsis resolution to construct struc-
tured representations of questions (frames). 
Our results indicate the viability of our ap-
proach and demonstrate the important role 
played by anaphora and ellipsis in interpreting 
consumer health questions. 
1 Introduction 
Question understanding is a major challenge in 
automatic question answering. An array of ap-
proaches has been developed for this task in the 
course of TREC Question Answering evaluations 
(see Prager (2006) for an overview). These col-
lectively developed approaches to question un-
derstanding were successfully applied and ex-
panded upon in IBM?s Watson system (Lally et 
al., 2012). Currently, Watson is being retargeted 
towards biomedical question answering, joining 
the ongoing research in domain-specific question 
answering (for a review, see Simpson and 
Demner-Fushman, 2012). 
Much research in automatic question answer-
ing has focused on answering well-formed fac-
toid questions. However, real-life questions that 
need to be handled by such systems are often 
posed by lay people and are not necessarily well-
formed or explicit. This is particularly evident in 
questions involving health issues. Zhang (2010), 
focusing on health-related questions submitted to 
Yahoo Answers, found that these questions pri-
marily described diseases and symptoms (ac-
companied by some demographic information), 
were fairly long, dense (incorporating more than 
one question), and contained many abbreviations 
and misspellings. For example, consider the fol-
lowing question posed by a consumer: 
(1) my question is this: I was born w/a esopha-
gus atresia w/dextrocardia. While the heart 
hasn't caused problems,the other has. I get 
food caught all the time. My question is...is 
there anything that can fix it cause I can't eat 
anything lately without getting it caught. I 
need help or will starve! 
It is clear that the person asking this question 
is mainly interested in learning about treatment 
options for his/her disease, in particular with re-
spect to his/her esophagus. Most of the textual 
content is not particularly relevant in understand-
ing the question (I need help or will starve! or I 
get food caught all the time). In addition, note 
the presence of anaphora (it referring to esopha-
gus atresia) and ellipsis (the other has [caused prob-
lems]), which should be resolved in order to auto-
matically interpret the question. Finally, note the 
informal fix instead of the more formal treat, and 
cause instead of because.  
The National Library of Medicine? (NLM?) 
receives questions from consumers on a variety 
of health-related topics. These questions are cur-
rently manually answered by customer support 
services. The overall goal of our work is to assist 
the customer support services by automatically 
interpreting these questions, using information 
retrieval techniques to find relevant documents 
and passages, and presenting the information in 
concise form for their assessment. 
In this paper, we specifically focus on ques-
tion understanding, rather than information re-
54
trieval aspects of our ongoing work. Our goal in 
question understanding is to capture the core as-
pects of the question in a structured representa-
tion (question frame), which can then be used to 
form a query for the search engine. In the current 
work, we primarily investigate and evaluate the 
role of anaphora and ellipsis resolution in under-
standing the questions. Our results confirm the 
viability of rule-based question understanding 
based on exploiting lexico-syntactic patterns and 
clearly demonstrate that anaphora and ellipsis 
resolution are beneficial for this task.  
2 Background 
Despite the growing interest to biomedical ques-
tion answering (Cairns et al, 2012; Ni et al, 
2012; Bauer and Berleant, 2012), consumer 
health question answering remains a fairly un-
derstudied area of research. The initial research 
has focused on the analysis of consumer lan-
guage (McCray et al, 1999) and the types of 
questions they asked. Spink et al (2004) found 
that health-related queries submitted to three web 
search engines in 2001 were often advice seeking 
and personalized, and fell into five major catego-
ries: general health, weight issues, reproductive 
health and puberty, pregnancy/obstetrics, and 
human relationships. Observing that health que-
ries constituted no more than 9.5% of all queries 
and declined over time, they concluded that the 
users turn more to the specialized resources for 
the answers to health-related questions. Similar 
to the findings of Zhang (2010), Beloborodov et 
al. (2013) found that diseases and symptoms 
were the most popular topics in a resource simi-
lar to Yahoo Answers, Otvety@Mail.Ru. They 
analyzed Otvety@Mail.Ru questions by mapping 
questions to body parts and organs, applying La-
tent Dirichlet Allocation method with Gibbs 
sampling to discover topics, and using a 
knowledge-based method to classify questions as 
evidence-directed or hypothesis-directed. 
First efforts in automated consumer health 
question processing were to classify the ques-
tions using machine learning techniques. In one 
study, frequently asked questions about diabetes 
were classified according to two somewhat or-
thogonal taxonomies: according to the ?medical 
type of the question? (Causes, Diagnostic, Pre-
vention, Symptoms, Treatment, etc.) and accord-
ing to the ?expected answer type? (Boolean, 
Causal, Definition, Factoid, Person, Place, etc.) 
(Cruchet et al, 2008). Support Vector Machine 
(SVM) classification achieved an F-score in low 
80s in classifying English questions to the ex-
pected answer type. The results for French and 
medical type classification in both languages 
were much lower. Liu et al (2011) found that 
SVM trained to distinguish questions asked by 
consumers from those posed by healthcare pro-
fessionals achieve F-scores in the high 80s - low 
90s. One of distinguishing characteristics of the 
consumer questions in Liu et al?s study was the 
significantly higher use of personal pronouns 
(compared to professional questions). This fea-
ture was found to be useful for machine learning; 
however, the abundance of pronouns in the long 
dense questions is also a potential source of fail-
ure in understanding the question.  
Vicedo and Ferr?ndez (2000) have shown that 
pronominal anaphora resolution improves several 
aspects of the QA systems? performance. This 
observation was supported by Harabagiu et al 
(2005) who have manually resolved coreference 
and ellipsis for 14 of the 25 scenarios in the 
TREC 2005 evaluation. Hickl et al (2006) have 
incorporated into their question answering sys-
tem a heuristic based question coreference mod-
ule that resolved referring expressions in the 
question series to antecedents mentioned in pre-
vious questions or in the target description. To 
our knowledge, coreference and ellipsis resolu-
tion has not been previously attempted in con-
sumer health question understanding. 
Another essential aspect in processing con-
sumer questions is defining a formal representa-
tion capable of capturing all important points 
needed for further processing in automatic query 
generation (in the systems that use document 
passage retrieval to find a set of potential an-
swers) and answer extraction and unification. 
Ontologies provide effective representation 
mechanisms for concepts, whereas relations are 
better captured in frame-like or event-related 
structures (Hunter and Cohen, 2006). Frame-
based representation of extracted knowledge has 
a long-standing tradition in the biomedical do-
main, for example, in MedLEE (Friedman et al, 
1994). Demner-Fushman et al (2011) showed 
that frame-based representation of clinical ques-
tions improve identification of patients eligible 
for cohort inclusion. Demner-Fushman and Ab-
hyankar (2012) extracted frames in four steps: 1) 
identification of domain concepts, 2) extraction 
of patient demographics (e.g., age, gender) and 
social history, 3) establishing dependencies be-
tween the concepts using the Stanford dependen-
cy parser (de Marneffe et al, 2006), and 4) add-
ing concepts not involved in the relations to the 
55
frame as a list of keywords.  Event-based repre-
sentations have also seen increasing use in recent 
years in biomedical text mining, with the availa-
bility of biological event corpora, including 
GENIA event (Kim et al, 2008) and GREC 
(Thompson et al, 2009), and shared task chal-
lenges (Kim et al, 2012). Most state-of-the-art 
systems address the event extraction task by 
adopting machine learning techniques, such as 
dual composition-based models (Riedel and 
McCallum, 2011), stacking-based model integra-
tion (McClosky et al, 2012), and domain adapta-
tion (Miwa et al, 2012). Good performance has 
also been reported with some rule-based systems 
(Kilicoglu and Bergler, 2012). Syntactic depend-
ency parsing has been a key component in all 
state-of-the-art event extraction systems, as well. 
The role of coreference resolution in event ex-
traction has recently been acknowledged (Kim et 
al., 2012), even though efforts in integrating co-
reference resolution into event extraction pipe-
lines have generally resulted in only modest im-
provements (Yoshikawa et al, 2011; Miwa et 
al., 2012; Kilicoglu and Bergler, 2012). 
Coreference resolution has also been tackled 
in open domain natural language processing. 
State-of-the-art systems often employ a combina-
tion of lexical, syntactic, shallow semantic and 
discourse information (e.g., speaker identifica-
tion) with deterministic rules (Lee et al, 2011). 
Interestingly, coreference resolution is one re-
search area, in which deterministic frameworks 
generally outperform machine learning models 
(Haghighi and Klein, 2009; Lee et al, 2011).  
In contrast to coreference resolution, ellipsis 
resolution remains an understudied NLP prob-
lem. One type of ellipsis that received some at-
tention is null instantiation (Fillmore and Baker, 
2001), whereby the goal is to recover the refer-
ents for an uninstantiated semantic role of a tar-
get predicate from the wider discourse context. A 
semantic evaluation challenge that focused on 
null instantiation was proposed, although partici-
pation was limited (Ruppenhofer et al, 2010). 
Gerber and Chai (2012) focused on implicit ar-
gumentation (i.e., null instantiation) for nominal 
predicates. They annotated a corpus of implicit 
arguments for a small number of nominal predi-
cates and trained a discriminative model based 
on syntactic, semantic and discourse features 
collected from various linguistic resources. Fo-
cusing on a different type of ellipsis, Bos and 
Spenader (2011) annotated a corpus of verb 
phrase ellipsis; however, so far there have been 
little work in verb phrase ellipsis resolution. We 
are also not aware of any work in ellipsis resolu-
tion in biomedical NLP.  
3 Methods   
We use a pipeline model for question analysis, 
which results in frame annotations that capture 
the content of the question. Our rule-based meth-
od begins with identifying terms (named enti-
ties/triggers) in question text. Next, we recognize 
anaphoric mentions and, if any, perform anapho-
ra resolution. The next step is to link frame trig-
gers with their theme and question cue by ex-
ploiting syntactic dependency relations. Finally, 
if frames with implicit arguments exist (that is, 
frames in which theme or question cue was not 
instantiated), we attempt to recover these argu-
ments by ellipsis resolution. In this section, we 
first describe our data selection. Then, we ex-
plain the steps in our pipeline, with particular 
emphasis on anaphora and ellipsis. The pipeline 
diagram is illustrated in Figure 1.  
 
 
Figure 1. The system pipeline diagram 
3.1 Data Selection and Annotation 
In this study, we focused on questions about ge-
netic diseases, due to their increasing prevalence. 
Since the majority of the consumers? questions 
submitted to NLM are about treatment and prog-
nosis, we selected mainly these types of ques-
tions for our training set. Note that while these 
questions mostly focused on treatment and prog-
nosis, some of them also include other types of 
questions, asking for general information or 
about diagnosis, etiology, and susceptibility 
(thus, confirming the finding of Zhang (2010)). 
The majority of selected questions were asked by 
real consumers in 2012. Due to our interest in 
genetics questions, we augmented this set with 
56
some frequently asked questions from the Genet-
ic and Rare Disease Information Center 
(GARD) 1 . Our selection yielded 32 treatment 
and 22 prognosis questions. An example treat-
ment question was provided earlier (1). The fol-
lowing is a training question on prognosis: 
(2) They have diagnosed my niece with Salla 
disease. I understand that this is a very rare 
disease and that its main origin is Finland. 
Can you please let me know what to expect? 
My niece is 7 years old. It has taken them 6 
years to finally come up with this diagnosis. 
We used training questions to gain linguistic 
insights into the problem, to develop and refine 
our methodology, and as the basis of a trig-
ger/question cue dictionary. 
After the system was developed, we selected 
29 previously unseen treatment-focused ques-
tions posed to GARD for testing. We annotated 
them with target frames (41 instances) using brat 
annotation tool (Stenetorp et al, 2012) and eval-
uated our system results against these frames. 29 
of the target frames were treatment frames. Addi-
tionally, there were 1 etiology, 6 general infor-
mation, 2 diagnosis, and 3 prognosis frames. 
3.2 Syntactic Dependency Parsing 
Our question analysis module uses typed de-
pendency relations as the basis of syntactic in-
formation. We extract syntactic dependencies 
using Stanford Parser (de Marneffe et al, 2006) 
and use its collapsed dependency format. We 
rely on Stanford Parser for tokenization, lemma-
tization, and part-of-speech tagging, as well. 
3.3 Named Entity/Trigger Detection 
We use simple dictionary lookup to map entity 
mentions in text to UMLS Metathesaurus con-
cepts (Lindberg, 1993). So far, we have focused 
on recognizing three mention categories: prob-
lems, interventions, and patients. Based on 
UMLS 2007AC release, we constructed a dic-
tionary of string/concept pairs. We limited the 
dictionary to concepts with predefined semantic 
types. For example, all problems in the diction-
ary have a semantic type that belongs to the Dis-
orders semantic group (McCray et al, 2001), 
such as Neoplastic Process and Congenital Ab-
normality. Currently our dictionary contains ap-
proximately 260K string/concept pairs. 
Dictionary lookup is also used to detect trig-
gers and question cues. We constructed a trigger 
                                                 
1 https://rarediseases.info.nih.gov/GARD/ 
and question cue dictionary based on training 
data and limited expansion. The dictionary cur-
rently contains 117 triggers and 14 question cues.  
3.4 Recognizing Anaphoric Mentions 
We focus on identifying two types of anaphoric 
phenomena: pronominal anaphora (including 
anaphora of personal and demonstrative pro-
nouns) and sortal anaphora. The following ex-
amples from the training questions illustrate 
these types. Anaphoric mentions are underlined 
and their antecedents are in bold. 
? Personal pronominal anaphora: My daughter 
has just been diagnosed with Meier-Gorlin 
syndrome. I would like to learn more about 
it ? 
? Demonstrative pronominal anaphora: We just 
found out that our grandson has 48,XXYY 
syndrome. ?  I was wondering if you could 
give us some information on what to expect 
and the prognosis for this and ..  
? Sortal anaphora: I have a 24-month-old niece 
who has the following symptoms of Cohen 
syndrome: ? I would like seek your help in 
learning more about this condition. 
To recognize mentions of personal pronominal 
and sortal anaphora, we mainly adapted the rule-
based techniques outlined in Kilicoglu and Ber-
gler (2012), itself based on the deterministic co-
reference resolution approach described in 
Haghighi and Klein (2009). While Kilicoglu and 
Bergler (2012) focused on anaphora involving 
gene/protein terms, our adaptation focuses on 
those involving problems and patients. In addi-
tion, we expanded their work by developing rules 
to recognize demonstrative pronominal anapho-
ra.  
3.4.1 Personal Pronouns 
Kilicoglu and Bergler (2012) focused on only 
resolving it and they, since, in scientific article 
genre, resolving other third person pronouns (he, 
she) was less relevant. We currently recognize 
these two pronouns, as well. For personal pro-
nouns, we merely tag the word as a pronominal 
anaphor if it is tagged as a pronoun and is in 
third person (i.e., she, he, it, they).  
3.4.2 Demonstrative Pronouns 
We rely on typed syntactic dependencies as well 
as part-of-speech tags to recognize demonstrative 
pronominal anaphora. A word is tagged as 
demonstrative pronominal anaphor if it is one of 
this, that, those, or these and if it is not the de-
57
pendent in a det (determiner) dependency (in 
other words, it is not a pronominal modifier). 
Furthermore, we ensure that the pronoun that 
does not act as a complementizer, requiring that 
it not be the dependent in a complm (complemen-
tizer) dependency. 
3.4.3 Sortal Anaphora 
In the current work, we limited sortal anaphora 
to problem terms. As in Kilicoglu and Bergler 
(2012), we require that the anaphoric noun 
phrases not include any named entity terms. 
Thus, we allow the syndrome as an anaphoric 
mention, while blocking the Stickler syndrome.  
To recognize sortal anaphora, we look for the 
presence of det dependency, where the depend-
ent is one of this, that, these, those, or the.  
Once the named entities, question cues, trig-
gers, and anaphoric mentions are identified in a 
sentence, we collapse the syntactic dependencies 
from the sentence to simplify further processing. 
This is illustrated in Table 1 for the sentence in 
(3). 
(3) My partner is a carrier for Simpson-Golabi-
Behmel syndrome and her son was diag-
nosed with this rare condition.  
 
Dependencies before Dependencies after 
amod (syndrome, simpson-golabi-behmel) 
prep_for(carrier, simpson-golabi-behmel syndrome) 
prep_for(carrier,syndrome) 
det(condition,this) 
prep_with (diagnosed, this rare condition) amod(condition, rare) 
prep_with(diagnosed, condition) 
Table 1: Syntactic dependency transformations 
3.5 Anaphora  Resolution 
Anaphora resolution is the task of finding the 
antecedent for an anaphoric mention in prior dis-
course. Our anaphora resolution method is again 
based on the work of Kilicoglu and Bergler 
(2012). However, we made simplifying assump-
tions based on our examination of the training 
questions. First observation is that each question 
is mainly about one salient topic (problem) and 
anaphoric mentions are highly likely to refer to 
this topic. Secondly, the salient topic often ap-
pears as the first named entity in the question.  
Based on these observations, we did not attempt 
to use the relatively complex, semantic graph-
based resolution strategies (e.g., graph distance) 
outlined in that work. Furthermore, we have not 
attempted to address set-instance anaphora or 
event anaphora in this work, since we did not see 
examples of these in the training data. 
Anaphora resolution begins with identifying 
the candidate antecedents (problems, patients) in 
prior discourse, which are then evaluated for syn-
tactic and semantic compatibility. For pronomi-
nal anaphora, compatibility involves person and 
number agreement between the anaphoric men-
tion and the antecedent. For sortal anaphora, 
number agreement as well as satisfying one of 
the following constraints is required: 
? Head word constraint: The head of the ana-
phoric NP and the antecedent NP match. 
This constraint allows Wolf-Hirschhorn Syn-
drome as an antecedent for this syndrome, 
matching on the word syndrome. 
? Hypernymy constraint: The head of the ana-
phoric NP is a problem hypernym and the 
antecedent is a problem term. Similar to 
gene/protein hypernym list in Kilicoglu and 
Bergler (2012), we used a small list of prob-
lem hypernym words, including disease, dis-
order, illness, syndrome, condition, and 
problem. This constraint allows Simpson-
Golabi-Behmel syndrome as an antecedent 
for this rare condition in example (3). 
We expanded number agreement test to in-
clude singular mass nouns, so that plural anapho-
ra (e.g., they) can refer to mass nouns such as 
family, group, population. In addition, we de-
fined lists of gendered nouns (e.g., son, father, 
nephew, etc. for male and wife, daughter, niece, 
etc. for female) and required gender agreement 
for pronominal anaphora. 
After the candidate antecedents are identified, 
we assign them salience scores based on the or-
der in which they appear in the question and their 
frequency in the question. The terms that appear 
earlier in the question and occur more frequently 
receive higher scores. The most salient anteced-
ent is then taken to be the coreferent. 
3.6 Frame Construction 
We adapted the frame extraction process based 
on lexico-syntactic information outlined in 
Demner-Fushman et al (2012) and somewhat 
58
modified the frames to accommodate consumer 
health questions. For each question posed, we 
aim to construct a frame which consists of the 
following elements: type, theme, and question 
cue: theme refers to the topic of the question 
(problem name, etc.), while type refers to the 
aspect of the theme that the question is about 
(treatment, prognosis, etc.) and question cue to 
the question words (what, how, are there, etc.). 
Theme element is semantically typed and is re-
stricted to the UMLS semantic group Disorders. 
From the question in (1), the following frame 
should be extracted: 
 
Treatment fix 
      Theme Esophageal atresia  
(Disease or Syndrome) 
      QCue Is there 
Table 2: Frame example 
 
We rely on syntactic dependencies to link frame 
indicators to their themes and question cues. We 
currently search for the following types of syn-
tactic dependencies between the indicator men-
tion and the argument mentions: dobj (direct ob-
ject), nsubjpass (passive nominal subject), nn 
(noun compound modifier), rcmod (relative 
clause modifier), xcomp (open clausal comple-
ment), acomp (adjectival complement), prep_of, 
prep_to, prep_for, prep_on, prep_from, 
prep_with, prep_regarding, prep_about (prepo-
sitional modifier cued by of, to, for, on, from, 
with, regarding, about, respectively). Two spe-
cial rules address the following cases: 
? If the dependency exists between a trigger of 
type T and another of type General Infor-
mation, the General Information trigger be-
comes a question cue for the frame type T. 
This handles cases such as ?Is there infor-
mation regarding prognosis..? where there is 
a prep_regarding dependency between the 
General Information trigger ?information? 
and the Prognosis trigger ?prognosis?. This 
results in ?information? becoming the ques-
tion cue for the Prognosis frame. 
? If a dependency exists between a trigger T 
and a patient term P and another between the 
patient term P and a potential theme argu-
ment A, the potential theme argument A is 
assigned as the theme of the frame indicated 
by T. This handles cases such as ?What is the 
life expectancy for a child with Dravet syn-
drome?? whereby Dravet syndrome is as-
signed the Theme role for the Prognosis 
frame indicated by life expectancy. 
3.6.1 Ellipsis Resolution 
The frame construction step may result in frames 
with uninstantiated themes or question cues. If a 
constructed frame includes a question cue but no 
theme, we attempt to recover the theme argument 
from prior discourse by ellipsis processing. Con-
sider the question in (4) and the frame in Table 3 
extracted from it in previous steps: 
(4) They have diagnosed my niece with Salla 
disease. ?Can you please let me know what 
to expect? ? 
Prognosis expect 
      Theme - 
      QCue what 
Table 3: Frame with uninstantiated Theme role 
 
In the context of consumer health questions, 
the main difficulty with resolving such cases is 
recognizing whether it is indeed a legitimate case 
of ellipsis. We use the following dependency-
based heuristics to determine the presence of el-
lipsis: 
? Check for the presence of a syntactic de-
pendency of one of the types listed in Sec-
tion 3.5, in which the frame trigger appears 
as an element. If such a dependency does not 
exist, consider it a case of ellipsis.  
? Otherwise, consider the other element of the 
dependency: 
o If the other element does not corre-
spond to a term, we cannot make a 
decision regarding ellipsis, since we 
do not know the semantics of this 
other element. 
o If it corresponds to an element that 
has already been used in creating the 
frame, the dependency is accounted 
for.  
? If all the dependencies involving the frame 
trigger are accounted for, consider it a case 
of ellipsis. 
In example (4), the trigger expect is found to 
be in an xcomp dependency with the question cue 
know, which has already been used in the frame. 
Therefore this dependency is accounted for, and 
we consider this a case of ellipsis.  On the other 
hand, consider the example:  
(5) My child has been diagnosed with pachgyria. 
What can I expect for my child?s future? 
As in the previous example, the Theme role of 
the Prognosis frame indicated by expect is unin-
stantiated. However, it is not considered an ellip-
59
tical case, since there is a prep_for dependency 
between expect and future, a word that is seman-
tically unresolved. 
Once the presence of ellipsis is ensured, we 
fill the Theme role of the frame with the most 
salient term in the question text, as in anaphora 
resolution. 
In rare cases, the frame may include a theme 
but not a question cue.  This may be due to a lack 
of explicit question expression (such as in the 
question ?treatment for Von Hippel-Lindau syn-
drome.?) or due to shortcomings in dependency-
based linking of frame triggers to question cues. 
If no fully instantiated frame was extracted from 
the question, as a last resort, we construct a 
frame without the question cue in an effort to 
increase recall.  
4 Results and Discussion 
We extracted frames from the test questions and 
compared the results with the annotated target 
frames. As evaluation metrics, we calculated 
precision, recall, and F-score. To assess the ef-
fect of various components of the system, we 
evaluated several scenarios: 
? Frame extraction without anaphora/ellipsis 
resolution (indicated as A in Table 4 below) 
? Frame extraction with anaphora/ellipsis reso-
lution (B) 
? Frame extraction without anaphora/ellipsis 
resolution but with gold triggers/named enti-
ties (C) 
? Frame extraction with anaphora/ellipsis reso-
lution and gold triggers/named entities (D) 
The evaluation results are provided in Table 4. In 
the second column, the numbers in parentheses 
correspond to the numbers of correctly identified 
frames. 
 
 # of frames Recall Precision F-score 
A 14 (13) 0.32 0.93 0.48 
B 26 (22) 0.54 0.85 0.66 
C 17 (16) 0.39 0.84 0.55 
D 35 (33) 0.80 0.94 0.86 
Table 4: Evaluation results 
 
The evaluation results show that the depend-
ency-based frame extraction method with dic-
tionary lookup is generally effective; it is precise 
in identifying frames, even though it misses 
many relevant frames, typical of most rule-based 
systems. On the other hand, anaphora/ellipsis 
resolution helps a great deal in recovering the 
relevant frames and only has a minor negative 
effect on precision of the frames, the overall ef-
fect being significantly positive. Note also that 
the increase in recall without gold triggers/named 
entities is about 40%, while that with gold trig-
gers/named entities is more than double, indicat-
ing that accurate term recognition contributes to 
better anaphora/ellipsis resolution and, in turn, to 
better question understanding. 
The dictionary-based named entity/trigger/ 
question cue detection is relatively simple, and 
while it yields good precision, the lack of terms 
in the corresponding dictionary causes recall er-
rors. An example is given in (6). The named enti-
ty Reed syndrome was not recognized due to its 
absence in the dictionary, causing two false 
negative errors. 
(6) A friend of mine was just told she has Reed 
syndrome? I was wondering if you could let 
me know where I can find more information 
on this topic. I am wondering what treat-
ments there are for this, ? 
Similarly, dependency-based frame construc-
tion is straightforward in that it mostly requires 
direct dependency relations between the trigger 
and the arguments.  While the two additional 
rules we implemented redress the shortcomings 
of this straightforward approach, there are cases 
in which dependency-based mechanism is still 
lacking. An example is given in (7). The lack of 
a direct dependency between treatments and this 
condition causes a recall error. A more sophisti-
cated mechanism based on dependency chains 
could recover such frames; however, such chains 
would also increase the likelihood of precision 
errors.  
(7) Are people with Lebers hereditary optic neu-
ropathy partially blind for a long period of 
time ?. ?Are there any surgical treatments 
available to alter this condition or is it per-
manent for life? 
Anaphora/ellipsis processing clearly benefited 
our question understanding system. However, we 
noted several errors due to shortcomings in this 
processing. For example, from the sentence in 
(8), the system constructed a General Infor-
mation frame with the trigger wonder and the 
Theme argument central core disease, which 
caused a false positive error.  
(8) After 34 years of living with central core dis-
ease, ?. My lower back doesn't seem to 
work, and I wonder if I will ever be able to 
walk up stairs or run.  
60
The system recognized that the trigger wonder 
had an uninstantiated theme argument, which it 
attempted to recover by ellipsis processing. 
However, this processing misidentified the case 
as legitimate ellipsis due to the dependency rela-
tions wonder is involved in. A more sophisticat-
ed approach would take into account specific 
selectional restrictions of predicates like wonder; 
however, the overall utility of such linguistic 
knowledge in the context of consumer health 
questions, which are often ungrammatical and 
not particularly well-written, remains uncertain. 
Our anaphora resolution method was unable to 
resolve some cases of anaphora. For example, 
consider the question in (6). The anaphoric men-
tion this topic corefers with Reed syndrome. 
However, we miss this anaphora since we did not 
consider topic as a problem hypernym in scenar-
io D, in which gold named entities are used. 
5 Conclusions and Future Work 
We presented a rule-based approach to consumer 
health question understanding which relies on 
lexico-syntactic information and anapho-
ra/ellipsis resolution. We showed that lexico-
syntactic information provides a good baseline in 
understanding such questions and that resolving 
anaphora and ellipsis has a significant impact on 
this task. 
With regard to question understanding, future 
work includes generalization of the system to 
questions on topics other than genetic disorders 
(e.g., drugs) and aspects (such as complications, 
prevention, ingredients, location information, 
etc.) and broader evaluation. We also plan to au-
tomate dictionary development to some extent 
and address misspellings and acronyms in ques-
tions. We have been extending our frames to in-
clude ancillary keywords (named entities ex-
tracted from the question) that are expected to 
assist the search engine in pinpointing the rele-
vant answer passages, similar to Demner-
Fushman and Abhyankar (2012). We will also 
continue to develop our anaphora/ellipsis pro-
cessing module, addressing the issues revealed 
by our evaluation as well as other anaphoric phe-
nomena, such as recognition of pleonastic it. 
Acknowledgments 
This research was supported by the Intramural 
Research Program of the NIH, National Library 
of Medicine. 
References  
Michael A. Bauer and Daniel Berleant. 2012. Usabil-
ity survey of biomedical question answering sys-
tems. Human Genomics, 6(1):17. 
Alexander Beloborodov, Artem Kuznetsov, Pavel 
Braslavski. 2013. Characterizing Health-Related 
Community Question Answering. In Advances in 
Information Retrieval, 680-683. 
Brian L. Cairns, Rodney D. Nielsen, James J. Masanz, 
James H. Martin, Martha S. Palmer, Wayne H. 
Ward, Guergana K. Savova. 2011. The MiPACQ 
clinical question answering system. In AMIA An-
nual Symposium Proceedings, pages 171-180. 
Sarah Cruchet, Arnaud Gaudinat, C?lia Boyer. 2008. 
Supervised approach to recognize question type in 
a QA system for health. Studies in Health Technol-
ogy and Informatics, 136:407-412. 
Marie-Catherine de Marneffe, Bill MacCartney, 
Christopher D. Manning. 2006. Generating typed 
dependency parses from phrase structure parses. In 
Proceedings of the 5th International Conference on 
Language Resources and Evaluation, pages 449-
454. 
Dina Demner-Fushman, Swapna Abhyankar, Antonio 
Jimeno-Yepes, Russell F. Loane, Bastien Rance, 
Fran?ois-Michel Lang, Nicholas C. Ide, Emilia 
Apostolova, Alan R. Aronson. 2011. A 
Knowledge-Based Approach to Medical Records 
Retrieval. In Proceedings of Text Retrieval Confer-
ence 2011.  
Dina Demner-Fushman and Swapna Abhyankar. 
2012. Syntactic-Semantic Frames for Clinical Co-
hort Identification Queries. Lecture Notes in Com-
puter Science, 7348:100-112. 
Charles J. Fillmore and Collin F. Baker. 2001. Frame 
semantics for text understanding. In Proceedings of 
the NAACL?01 Workshop on WordNet and Other 
Lexical Resources. 
Carol Friedman, Philip O. Alderson, John HM Austin, 
James J. Cimino, and Stephen B. Johnson. 1994. A 
general natural-language text processor for clinical 
radiology. Journal of the American Medical Infor-
matics Association, 1(2): 161-174. 
Matthew S. Gerber and Joyce Y. Chai. 2012. Seman-
tic Role Labeling of Implicit Arguments for Nomi-
nal Predicates. Computational Linguistics, 38(4): 
755-798. 
Aria Haghighi and Dan Klein. 2009. Simple corefer-
ence resolution with rich syntactic and semantic 
features. In Proceedings of EMNLP 2009, pages 
1152-1161. 
Sanda Harabagiu, Dan Moldovan, Christine Clark, 
Mitchell Bowden, Andrew Hickl, Patrick Wang. 
2005. Employing two question answering systems 
in TREC-2005. In Proceedings of Text Retrieval 
Conference  2005. 
61
Andrew Hickl, John Williams, Jeremy Bensley, Kirk 
Roberts, Ying Shi, Bryan Rink. 2006. Question 
Answering with LCC?s CHAUCER at TREC 2006. 
In Proceedings of Text Retrieval Conference  2006. 
Lawrence Hunter and Kevin B. Cohen. 2006. Bio-
medical language processing: what's beyond Pub-
Med? Molecular Cell. 21(5):589-94. 
Halil Kilicoglu and Sabine Bergler 2012. Biological 
Event Composition. BMC Bioinformatics, 13 
(Supplement 11):S7.  
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 
2008. Corpus annotation for mining biomedical 
events from literature. BMC Bioinformatics, 9:10. 
Jin-Dong Kim, Ngan Nguyen, Yue Wang, Jun?ichi 
Tsujii, Toshihisa Takagi, Akinori Yonezawa. 2012. 
The Genia Event and Protein Coreference tasks of 
the BioNLP Shared Task 2011.  BMC Bioinformat-
ics, 13(Supplement 11):S1.  
Adam Lally, John M. Prager, Michael C. McCord, 
Branimir Boguraev, Siddharth Patwardhan, James 
Fan, Paul Fodor, Jennifer Chu-Carroll. 2012. Ques-
tion analysis: How Watson reads a clue. IBM Jour-
nal of Research and Development, 56(3):2. 
Heeyoung Lee, Yves Peirsman, Angel Chang, Na-
thanael Chambers, Mihai Surdeanu, Dan Jurafsky. 
2011. Stanford's Multi-Pass Sieve Coreference 
Resolution System at the CoNLL-2011 Shared 
Task. In Proceedings of the CoNLL-2011 Shared 
Task, pages 28-34. 
Donald A.B. Lindberg, Betsy L. Humphreys, Alexa T. 
McCray. 1993. The Unified Medical Language 
System. Methods of information in medicine, 
32(4): 281-291. 
Feifan Liu, Lamont D. Antieau, Hong Yu. 2011. To-
ward automated consumer question answering: au-
tomatically separating consumer questions from 
professional questions in the healthcare domain. 
Journal of Biomedical Informatics, 44(6): 1032-
1038. 
David McClosky, Sebastian Riedel, Mihai Surdeanu, 
Andrew McCallum, Christopher Manning. 2012. 
Combining joint models for biomedical event ex-
traction. BMC Bioinformatics, 13 (Supplement 11): 
S9. 
Alexa McCray, Russell Loane, Allen Browne, Anan-
tha Bangalore. 1999. Terminology issues in user 
access to Web-based medical information. In 
AMIA Annual Symposium Proceedings, pages 107?
111. 
Alexa McCray, Anita Burgun, Olivier Bodenreider. 
2001. Aggregating UMLS semantic types for re-
ducing conceptual complexity. In Proceedings of 
Medinfo, 10(Pt1): 216-220. 
Makoto Miwa, Paul Thompson, Sophia Ananiadou. 
2012. Boosting automatic event extraction from the 
literature using domain adaptation and coreference 
resolution. Bioinformatics, 28(13):1759-1765. 
Yuan Ni, Huijia Zhu, Peng Cai, Lei Zhang, Zhaoming 
Qui, Feng Cao. 2012. CliniQA: highly reliable 
clinical question answering system. Studies in 
Health Technology and Information, 180:215-219. 
John M. Prager. 2006. Open-domain question answer-
ing. Foundations and Trends in Information Re-
trieval, 1(2):91-231. 
Sebastian Riedel and Andrew McCallum. 2011. Fast 
and robust joint models for biomedical event ex-
traction. In Proceedings of EMNLP 2011, pages 1-
12. 
Josef Ruppenhofer, Caroline Sporleder, Roser Mo-
rante, Collin Baker, Martha Palmer. 2010. 
SemEval-2010 Task 10: Linking Events and Their 
Participants in Discourse. In Proceedings of the 5th 
International Workshop on Semantic Evaluation, 
pages 45-50. 
Matthew S. Simpson and Dina Demner-Fushman. 
2012. Biomedical Text Mining: a Survey of Recent 
Progress. Mining Text Data 2012:465-517. 
Amanda Spink, Yin Yang, Jim Jansen, Pirrko 
Nykanen, Daniel P. Lorence, Seda Ozmutlu, H. 
Cenk Ozmutlu. 2004. A study of medical and 
health queries to web search engines. Health In-
formation & Libraries Journal. 21(1):44-51. 
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?, 
Tomoko Ohta, Sophia Ananiadou, Jun?ichi Tsujii. 
2012. Brat: a Web-based Tool for NLP-Assisted 
Text Annotation. In Proceedings of the Demon-
strations Sessions at EACL 2012, pages 102-107. 
Paul Thompson, Syed A. Iqbal, John McNaught, So-
phia Ananiadou. 2009. Construction of an annotat-
ed corpus to support biomedical information ex-
traction. BMC Bioinformatics, 10:349. 
Jos? L. Vicedo and Antonio Ferr?ndez. 2000. Im-
portance of pronominal anaphora resolution in 
question answering systems. In Proceedings of the 
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 555-562. 
Katsumasa Yoshikawa, Sebastian Riedel, Tsutomu 
Hirao, Masayuki Asahara, Yuji Matsumoto. 2011. 
Coreference Based Event-Argument Relation Ex-
traction on Biomedical Text. Journal of Biomedi-
cal Semantics, 2 (Supplement 5):S6. 
Yan Zhang. 2010. Contextualizing Consumer Health 
Information Searching: an Analysis of Questions in 
a Social Q&A Community. In Proceedings of the 
1st ACM International Health Informatics Sympo-
sium (IHI?10), pages 210-219. 
62
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 29?37,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Decomposing Consumer Health Questions
Kirk Roberts, Halil Kilicoglu, Marcelo Fiszman, and Dina Demner-Fushman
National Library of Medicine
National Institutes of Health
Bethesda, MD 20894
robertske@nih.gov, {kilicogluh,fiszmanm,ddemner}@mail.nih.gov
Abstract
This paper presents a method for decom-
posing long, complex consumer health
questions. Our approach largely decom-
poses questions using their syntactic struc-
ture, recognizing independent questions
embedded in clauses, as well as coordi-
nations and exemplifying phrases. Addi-
tionally, we identify elements specific to
disease-related consumer health questions,
such as the focus disease and background
information. To achieve this, our approach
combines rank-and-filter machine learning
methods with rule-based methods. Our
results demonstrate significant improve-
ments over the heuristic methods typically
employed for question decomposition that
rely only on the syntactic parse tree.
1 Introduction
Natural language questions provide an intuitive
method for consumers (non-experts) to query for
health-related content. The most intuitive way
for consumers to formulate written questions is
the same way they write to other humans: multi-
sentence, complex questions that contain back-
ground information and often more than one spe-
cific question. Consider the following:
? Will Fabry disease affect a transplanted kidney?
Previous to the transplant the disease was be-
ing managed with an enzyme supplement. Will
this need to be continued? What cautions or ad-
ditional treatments are required to manage the
disease with a transplanted kidney?
This complex question contains three question
sentences and one background sentence. The fo-
cus (Fabry disease) is stated in the first question
but is necessary for a full understanding of the
other questions as well. The background sentence
is necessary to understand the second question:
the anaphor this must be resolved to an enzyme
treatment, and the predicate continue?s implicit ar-
gument that must be re-constructed from the dis-
course (i.e., continue after a kidney transplant).
The final question sentence uses a coordination
to ask two separate questions (cautions and addi-
tional treatments). A decomposition of this com-
plex question would then result in four questions:
1. Will Fabry disease affect a transplanted kidney?
2. Will enzyme treatment for Fabry disease need to
be continued after a kidney transplant?
3. What cautions are required to manage Fabry
disease with a transplanted kidney?
4. What additional treatments are required to man-
age Fabry disease with a transplanted kidney?
Each question above could be independently an-
swered by a question answering (QA) system.
While previous work has discussed methods for
resolving co-reference and implicit arguments in
consumer health questions (Kilicoglu et al., 2013),
it does not address question decomposition.
In this work, we propose methods for auto-
matically recognizing six annotation types use-
ful for decomposing consumer health questions.
These annotations distinguish between sentences
that contain questions and background informa-
tion. They also identify when a question sentence
can be split in multiple independent questions, and
29
when they contain optional or coordinated infor-
mation embedded within a question.
For each of these decomposition annotations,
we propose a combination of machine learning
(ML) and rule based methods. The ML methods
largely take the form of a 3-step rank-and-filter
approach, where candidates are generated, ranked
by an ML classifier, then the top-ranked candidate
is passed through a separate ML filtering classi-
fier. We evaluate each of these methods on a set of
1,467 consumer health questions related to genetic
and rare diseases.
2 Background
QA in the biomedical domain has been well-
studied (Demner-Fushman and Lin, 2007; Cairns
et al., 2011; Cao et al., 2011) as a means for re-
trieving medical information. This work has typ-
ically focused, however, on questions posed by
medical professionals, and the methods proposed
for question analysis generally assume a single,
concise question. For example, Demner-Fushman
and Abhyankar (2012) propose a method for ex-
tracting frames from queries for the purpose of
cohort retrieval. Their method assumes syntactic
dependencies exist between the necessary frame
elements, and is thus not well-suited to handle
long, multi-sentence questions. Similarly, Ander-
sen et al. (2012) proposes a method for converting
a concise question into a structured query. How-
ever, many medical questions require background
information that is difficult to encode in a single
question sentence. Instead, it is often more natural
to ask multiple questions over several sentences,
providing background information to give context
to the questions. Yu and Cao (2008) use a ML
method to recognize question types in professional
health questions. Their method can identify more
than one type per complex question. Without de-
composing the full question into its sub-questions,
however, the type cannot be associated with its
specific span, or with other information specific to
the sub-question. This other information can in-
clude answer types, question focus, and other an-
swer constraints. By decomposing multi-sentence
questions, these question-specific attributes can be
extracted, and the discourse structure of the larger
question can be better understood.
Question decomposition has been utilized be-
fore in open-domain QA approaches, but rarely
evaluated on its own. Lacatusu et al. (2006)
demonstrates how question decomposition can im-
prove the performance of a multi-sentence sum-
marization system. They perform what we refer
to as syntactic question decomposition, where the
syntactic structure of the question is used to iden-
tify sub-questions that can be answered in isola-
tion. A second form of question decomposition is
semantic decomposition, which can semantically
break individual questions apart to answer them
in stages. For instance, the question ?When did
the third U.S. President die?? can be semantically
decomposed ?Who was the third U.S. President??
and ?When did X die??, where the answer to the
first question is substituted into the second. Katz
and Grau (2005) discusses this kind of decompo-
sition using the syntactic structure, though it is not
empirically validated. Hartrumpf (2008) proposes
a decomposition method using only the deep se-
mantic structure. Finally, Harabagiu et al. (2006)
proposes a different type of question decomposi-
tion based on a random walk over similar ques-
tions extracted from a corpus. In our work, we
focus on syntactic question decomposition. We
demonstrate the importance of empirical evalua-
tion of question decomposition, notably the pit-
falls of heuristic approaches that rely entirely on
the syntactic parse tree. Syntactic parsers trained
on Treebank are particularly poor at both analyz-
ing questions (Judge et al., 2006) and coordination
boundaries (Hogan, 2007). Robust question de-
composition methods, therefore, must be able to
overcome many of these difficulties.
3 Consumer Health Question
Decomposition
Our goal is to decompose multi-sentence, multi-
faceted consumer health questions into concise
questions coupled with important contextual in-
formation. To this end, we utilize a set of an-
notations that identify the decomposable elements
and important contextual elements. A more de-
tailed description of these annotations is provided
in Roberts et al. (2014). The annotations are pub-
licly available at our institution website
1
. Here, we
briefly describe each annotation:
(1) BACKGROUND - a sentence indicating useful
contextual information, but lacks a question.
(2) QUESTION - a sentence or clause that indi-
cates an independent question.
1
http://lhncbc.nlm.nih.gov/project/consumer-health-
question-answering
30
Sentence Splitting 
Request 
Question 
Sentence 
Ignore 
Sentence 
Background 
Sentence 
Candidate Generation 
UMLS 
SVM Candidate Ranking 
Boundary Fixing 
Focus 
Focus Recognition 
Sentence Classification 
Background Classification 
SVM Comorbidity Classification 
SVM Diagnosis Classification 
SVM Family History Classification 
SVM ISF Classification 
SVM Lifestyle Classification 
SVM Symptom Classification 
SVM Test Classification 
Candidate Generation 
SVM Candidate Filter 
Question Recognition 
SVM Sentence Classification 
Question 
Candidate Generation 
SVM Candidate Ranking 
Exemplification Recognition 
Candidate Filter 
Candidate Generation 
SVM Candidate Ranking 
Coordination Recognition 
SVM Candidate Filter Coordination 
Exemplification 
Stanford 
Parser 
WordNet 
SVM Treatment Classification 
Figure 1: Question Decomposition Architecture. Modules with solid green lines indicate machine learn-
ing classifiers. Modules with dotted green lines indicate rule-based classifiers.
(3) COORDINATION - a phrase that spans a set of
decomposable items.
(4) EXEMPLIFICATION - a phrase that spans an
optional item.
(5) IGNORE - a sentence indicating nothing of
value is present.
(6) FOCUS - an NP indicating the theme of the
consumer health question.
Further explanations of each annotation are pro-
vided in Sections 4-9. To convert these annota-
tions into separate, decomposed questions, a sim-
ple set of recursive rules is used. The rules enu-
merate all ways of including one conjunct from
each COORDINATION as well as whether or not
to include the phrase within an EXEMPLIFICA-
TION. These rules must be applied recursively to
handle overlapping annotations (e.g., a COORDI-
NATION within an EXEMPLIFICATION). Our im-
plementation is straight-forward and not discussed
further in this paper. The BACKGROUND and FO-
CUS annotations do not play a direct role in this
process, though they provide important contextual
elements and are useful for co-reference, and are
thus still considered part of the overall decompo-
sition process.
It should also be noted that some questions are
syntactically decomposable, but doing so alters
their original meaning. Consider the following
two question sentences:
? Can this disease be cured or can we only treat
the symptoms?
? Are males or females worse affected?
While the first example contains two ?Can...?
questions and the second example contains the co-
ordination ?males or females?, both questions are
providing a choice between two alternatives and
decomposing them would alter the semantic na-
ture of the original question. In these cases, we do
not consider the questions to be decomposable.
Data We use a set of consumer health ques-
tions collected from the Genetic and Rare Dis-
eases Information Center (GARD), which main-
tains a website
2
with publicly available consumer-
submitted questions and professionally-authored
answers about genetic and rare diseases. We col-
lected 1,467 consumer health questions, consist-
ing of 4,115 sentences, 1,713 BACKGROUND sen-
tences, 37 IGNORE sentences, 2,465 QUESTIONs,
367 COORDINATIONs, 53 EXEMPLIFICATIONs,
and 1,513 FOCUS annotations. Questions with
more than one FOCUS are generally concerned
with the relation between diseases. Further infor-
mation about the corpus and the annotation pro-
cess can be found in Roberts et al. (2014).
System Architecture The architecture of our
question decomposition method is illustrated in
2
http://rarediseases.info.nih.gov/gard
31
Figure 1. To avoid confusion, in the rest of this
paper we refer to a complex consumer health ques-
tion simply as a request. Requests are sent to
the independent FOCUS recognition module (Sec-
tion 4), and then proceed through a pipeline that
includes the classification of sentences (Section 5),
the identification of separate QUESTIONs within
a question sentence (Section 6), the recognition
of COORDINATIONs (Section 7) and EXEMPLIFI-
CATIONs (Section 8), and the sub-classification of
BACKGROUND sentences (Section 9).
Experimental Setup The remainder of this pa-
per describes the individual modules in Figure 1.
For simplicity, we show results on the GARD data
for each task in its corresponding section. In all
cases, experiments are conducted using a 5-fold
cross-validation on the GARD data. The cross-
validation folds are organized at the request level
so that no two items from the same request will be
split between the training and testing data.
4 Identifying the Focal Disease
The FOCUS is the condition that disease-centered
questions are centered around. Many other dis-
eases may be mentioned, but the FOCUS is the dis-
ease of central concern. This is similar to the as-
sumption made about a central disease in Medline
abstracts (Demner-Fushman and Lin, 2007). Of-
ten the FOCUS is stated in the first sentence (typ-
ically a BACKGROUND) of the request while the
questions are near the end. The questions can-
not generally be answered outside the context of
the FOCUS, however, so its identification is a crit-
ical part of decomposition. As shown in Figure 1,
we use a 3-step process: (1) a high-recall method
identifies potential FOCUS diseases in the data, (2)
a support vector machine (SVM) ranks the FO-
CUS candidates, and (3) the highest-ranking can-
didate?s boundary is modified with a set of rules to
better match our annotation standard.
To identify candidates for the FOCUS, we use a
lexicon constructed from UMLS (Lindberg et al.,
1993). UMLS includes very generic terms, such as
disease and cancer, that are too general to exactly
match a FOCUS in our data. We allow these terms
to be candidates so as to not miss any FOCUS that
doesn?t exactly match an entry in UMLS. When
such a general term is selected as the top-ranked
FOCUS, the rules described below are capable of
expanding the term to the full disease name.
To rank candidates, we utilize an SVM (Fan et
E/R P R F
1
1st UMLS Disorder
E 19.6 19.0 19.3
R 28.2 27.4 27.8
SVM
E 56.4 54.7 55.6
R 89.2 86.5 87.9
SVM + Rules
E 74.8 72.5 73.6
R 89.5 86.8 88.1
Table 1: FOCUS recognition results. E = exact
match; R = relaxed match.
al., 2008) with a small number of feature types:
? Unigrams. Identifies generic words such as dis-
ease and syndrome that indicate good FOCUS
candidates, while also recognizing noisy UMLS
terms that are often false positives.
? UMLS semantic group (McCray et al., 2001).
? UMLS semantic type.
? Sentence Offset. The FOCUS is typically in the
first sentence, and is far more likely to be at the
beginning of the request than the end.
? Lexicon Offset. The FOCUS is typically the first
disease mentioned.
During training, the SVM considers any candidate
that overlaps the gold FOCUS to be correct. This
enables our approach to train on FOCUS examples
that do not perfectly align with a UMLS concept.
At test time, all candidates are classified, ranked
by the classifier?s confidence, and the top-ranked
candidate is considered the FOCUS.
As mentioned above, there are differences be-
tween how a FOCUS is annotated in our data and
how it is represented in the UMLS. We therefore
use a series of heuristics to alter the boundary to a
more usable FOCUS after it is chosen by the SVM.
The rules are applied iteratively to widen the FO-
CUS boundary until it cannot be expanded any fur-
ther. If a generic disease word is the only token
in the FOCUS, we add the token to the left. Con-
versely, if the token on the right is a generic dis-
ease word, it is added as well. If the word to the
left is capitalized, it is safe to assume it is part of
the disease?s name and so it is added as well. Fi-
nally, several rules recognize the various ways in
which a disease sub-type might be specified (e.g.,
Behcet?s syndrome vascular type, type 2 diabetes,
Charcot-Marie-Tooth disease type 2C).
We evaluate FOCUS recognition with both an
exact match, where the gold and automatic FOCUS
boundaries must line up perfectly, and a relaxed
match, which only requires a partial overlap. As a
baseline, we compare our results against a fully
rule-based system where the first UMLS Disor-
der term in the request is considered the FOCUS.
32
We also evaluate the effectiveness of our bound-
ary altering rules by measuring performance with-
out these rules. The results are shown in Table 1.
The baseline method shows significant problems
in precision and recall. It is not able to ignore
noisy UMLS terms (e.g., aim is both a gene and
a treatment). The SVM improves upon the rule-
based method by over 50 points in F
1
for relaxed
matching. Adding the boundary fixing rules has
little effect on relaxed matching, but greatly im-
proves exact matching: precision and recall are
improved by 18.4 and 17.8 points, respectively.
5 Classifying Sentences
Before precise question boundaries can be rec-
ognized, we first identify sentences that con-
tain QUESTIONs, as distinguished from BACK-
GROUND and IGNORE sentences. It should be
noted that many of the question sentences in our
data are not typical wh-word questions. About
20% of the questions in our data end in a period.
For instance:
? Please tell me more about this condition.
? I was wondering if you could let me know where
I can find more information on this topic.
? I would like to get in contact with other families
that have this illness.
We consider a sentence to be a question if it con-
tains any information request, explicit or implicit.
After sentence splitting, we identify sentences
using a multi-class SVM with three feature types:
? Unigrams with parts-of-speech (POS). Reduces
unigram ambiguities, such as what-WP (a pro-
noun, indicative of a question) versus what-
WDT (a determiner, not indicative).
? Bigrams.
? Parse tree tags. All Treebank tags from the syn-
tactic parse tree. Captures syntactic question
clues such as the phrase tags SQ (question sen-
tence) and WHNP (wh-word noun phrase).
The SVM classifier performs at 97.8%. For com-
parison, an SVM with only unigram features per-
forms at 97.2%. While the unigram model does a
good job classifying sentences, suggesting this is
a very easy task, the improved feature set reduces
the number of errors by 20%.
6 Identifying Questions
QUESTION recognition is the task of identifying
when a conjunction like and joins two independent
questions into a single sentence:
? [What causes the condition]
QUESTION
[and what
treatment is available?]
QUESTION
? [What is this disease]
QUESTION
[and what steps
can I take to protect my daughter?]
QUESTION
We consider the identification of separate QUES-
TIONs within a single sentence to be a differ-
ent task from COORDINATION recognition, which
finds phrases whose conjuncts can be treated in-
dependently. Linguistically, these tasks are quite
similar, but the distinction lies in whether the
right-conjunct syntactically depends on anything
to its left. For instance:
? I would like to learn [more about this condition
and what the prognosis is for a baby born with
it]
COORDINATION
.
Here, the right-conjunct starts with a question
stem (what), but is not a complete, grammatical
question on its own. Alternatively, this could be
re-formed into two separate QUESTIONs:
? [I would like to learn more about this
condition,]
QUESTION
[and what is the prognosis
is for a baby born with it.]
QUESTION
We make this distinction because the QUESTION
recognition task requires one fewer step since the
boundaries extend to the entire sentence, prevent-
ing error propagation from an input module. Fur-
ther, the features that differentiate our QUESTION
and COORDINATION annotations are different.
The two-step process for recognizing QUES-
TIONs includes: (1) a high-recall candidate gener-
ator, and (2) an SVM to eliminate candidates that
are not separate QUESTIONs. The candidates for
QUESTION recognition are simply all the ways a
sentence can be split by the conjunctions and, or,
as well as, and the forward slash (?/?). In our data,
this candidate generation process has a recall of
98.6, as a few examples were missed where candi-
dates were not separated by one of the above con-
junctions.
To filter candidates, we use an SVM with three
features types:
? The conjunction separating the QUESTIONs.
? Unigrams in the left-conjunct. Identifies when
the left-conjunct is not a QUESTION, or when a
question is part of a COORDINATION.
? The right-conjunct?s parse tree tag. Recog-
nizes when the right-conjunct is an independent
clause that may safely be split.
33
P R F
1
QUESTION split recognition
Baseline 24.7 82.4 38.0
SVM 67.7 64.7 66.2
Overall QUESTION recognition
Baseline 87.3 92.8 90.0
SVM 97.7 97.4 97.5
Table 2: QUESTION recognition results.
For evaluation, we measure both the F
1
score
for correct candidates, and the overall F
1
for all
QUESTION annotations (i.e., all QUESTION sen-
tences). We also evaluate a baseline method that
utilizes the parse tree to recognize separate QUES-
TIONs by splitting sentences where a conjunction
separates independent clauses. The results are
shown in Table 2. The baseline method has good
recall for recognizing where a sentence should be
split into multiple QUESTIONs, but it lacks preci-
sion. This is largely because it is unable to differ-
entiate clausal COORDINATIONs such as the above
example, as well as when the left-conjunct is not
actually a separate question. For instance:
? Our grandson was diagnosed recently with this
disease and I am wondering if you could send
me information on it.
The SVM-based method can overcome this prob-
lem by looking at the words in the left-conjunct.
Both methods, however, fail to recognize when
two independent question clauses are asking the
same question but providing alternative answers:
? Will this condition be with him throughout his
life, or is it possible that it will clear up?
While there are methods for handling this issue
for COORDINATION recognition, addressed be-
low, recognizing non-splittable QUESTIONs re-
quires far deeper semantic understanding which
we leave to future work.
7 Identifying Coordinations
COORDINATION recognition is the task of identi-
fying when a conjunction joins phrases within a
QUESTION that can in be separate questions:
? How can I learn more about [treatments and
clinical trials]
COORDINATION
?
? Are [muscle twitching, muscle cramps, and
muscle pain]
COORDINATION
effects of having sil-
icosis?
Unlike QUESTION recognition, the boundaries of
a COORDINATION need to be determined as well
as whether the conjuncts can semantically be split
into separate questions. We thus use a three-step
process for recognizing COORDINATIONs: (1) a
high-recall candidate generator, (2) an SVM to
rank all the candidates for a given conjunction, and
(3) an SVM to filter out top-ranked candidates.
Candidate generation begins with the identifica-
tion of valid conjunctions within a QUESTION an-
notation. We use the same four conjunctions as in
QUESTION recognition: and, or, as well as, and
the forward slash. For each of these, all possi-
ble left and right boundaries are generated, so in
a QUESTION with 4 tokens on either side of the
conjunction, there would be 16 candidates. Addi-
tionally, two adjectives separated by a comma and
immediately followed by a noun are considered a
candidate (e.g., ?a [safe, permanent]
COORDINATION
treatment?). In our data, this candidate generation
process has a recall of 98.9, as a few instances ex-
ist in which a conjunction is not used, such as:
? I am looking for any information you have
about heavy metal toxicity, [treatment,
outcomes]
EXEMPLIFICATION+COORDINATION
.
To rank candidates, we use an SVM with the
following feature types:
? If the left-conjunct is congruent with the high-
est node in the syntactic parse tree whose right-
most leaf is also the right-most token in the left-
conjunct. Essentially, this is equivalent to say-
ing whether or not the syntactic parser agrees
with the left-conjunct?s boundary.
? The equivalent heuristic for the right-conjunct.
? If a noun is in both, just the left conjunct, just
the right conjunct, or neither conjunct.
? The Levenshtein distance between the POS tag
sequences for the left- and right-conjuncts.
The first two features encode the information a
rule-based method would use if it relied entirely
on the syntactic parse tree. The remaining features
help the classifier overcome cases where the parser
may be wrong.
At training time, all candidates for a given con-
junction are generated and only the candidate that
matches the gold COORDINATION is considered
a positive example. Additionally, we annotated
the boundaries for negative COORDINATIONs (i.e.,
syntactic coordinations that do not fit our annota-
tion standard). There were 203 such instances in
the GARD data. These are considered gold CO-
ORDINATIONs for boundary ranking only.
To filter the top-ranked candidates, we use an
SVM with several feature types:
34
E/R P R F
1
Baseline
E 28.1 36.5 31.8
R 62.9 75.8 68.7
Rank + Filter
E 38.2 34.8 36.4
R 78.5 69.0 73.5
Table 3: COORDINATION recognition results.
E = exact match; R = relaxed match.
? The conjunction.
? Unigrams in the left-conjunct.
? POS of the first word in both conjuncts. CO-
ORDINATIONs often have the same first POS in
both conjuncts.
? The word immediately before the candidate.
E.g., between is a good negative indicator.
? Unigrams in the question but not the candidate.
? If the candidate takes up almost the entire ques-
tion (all but 3 tokens). Typically, COORDINA-
TIONs are much smaller than the full question.
? If more than one conjunction is in the candidate.
? If a word in the left-conjunct has an antonym
in the right conjunct. Antonyms are recognized
via WordNet (Fellbaum, 1998).
At training time, the positive examples are drawn
from the annotated COORDINATIONs, while the
negative examples are drawn from the 203 non-
gold annotations mentioned above.
In addition to evaluating this method, we
evaluate a baseline method that relies entirely
on the syntactic parse to identify COORDINA-
TION boundaries without filtering. The results
are shown in Table 3. The rank-and-filter ap-
proach shows significant gains over the rule-based
method in precision and F
1
. As can be seen in
the difference between exact and relaxed match-
ing, most of the loss for both the baseline and ML
methods come in boundary detection. Most meth-
ods overly rely upon the syntactic parser, which
performs poorly both on questions and coordina-
tions. The ML method, though, is sometimes able
to overcome this problem.
8 Identifying Exemplifications
EXEMPLIFICATION recognition is the task of iden-
tifying when a phrase provides an optional, exem-
plifying example with a more specific type of in-
formation than that asked by the rest of the ques-
tion. For instance, the following contains both an
EXEMPLIFICATION and a COORDINATION:
? Is there anything out there that can help
him [such as [medications or alternative
therapies]
COORDINATION
]
EXEMPLIFICATION
?
We could consider this to denote 3 questions:
? Is there anything out there that can help him?
? Is there anything out there that can help him
such as medications?
? Is there anything out there that can help him
such as alternative therapies?
In the latter two questions, we consider the phrase
such as to now denote a mandatory constraint on
the answer to each question, whereas in the origi-
nal question it would be considered optional.
EXEMPLIFICATION recognition is similar to
COORDINATION recognition, and its three-step
process is thus similar as well: (1) a high-recall
candidate generator, (2) an SVM to rank all the
candidates for a given trigger phrase, and (3) a set
of rules to filter out top-ranked candidates.
Candidate generation begins with the identifica-
tion of valid trigger words and phrases. These in-
clude: especially, including, particularly, specifi-
cally, and such as. For each of these, all possible
right boundaries are generated, thus EXEMPLIFI-
CATIONs have far fewer candidates than COORDI-
NATIONs. Additionally, all phrases within paren-
theses are added as EXEMPLIFICATIONs. In our
data, this candidate generation process has a recall
of 98.1, missing instances without a trigger (see
the example also missed by COORDINATION can-
didate generation in Section 7).
To rank candidates, we use an SVM with the
following feature types:
? If the right-conjunct is the highest parse node
as defined in the COORDINATION boundary fea-
ture.
? If a dependency relation crosses from the right-
conjunct to any word outside the candidate.
? POS of the word after the candidate.
As with COORDINATIONs, we annotated bound-
aries for negative EXEMPLIFICATIONs matching
the trigger words and used them as positive exam-
ples for boundary ranking.
To filter the top-ranked candidates, we use two
simple rules. First, EXEMPLIFICATIONs within
parentheses are filtered if they are acronyms or
acronym expansions. Second, cases such as the
below example are removed by looking at the
words before the candidate:
? I am particularly interested in learning more
about genetic testing for the syndrome.
In addition to evaluating this method, we eval-
uate a baseline method that relies entirely on the
35
E/R P R F
1
Baseline
E 28.9 62.3 39.5
R 39.5 84.9 53.9
Rank + Filter
E 60.8 58.5 59.6
R 80.4 77.4 78.8
Table 4: EXEMPLIFICATION recognition results.
E = exact match; R = relaxed match.
syntactic parser to identify EXEMPLIFICATION
boundaries and performs no filtering. The re-
sults are shown in Table 4. The rank-and-filter
approach shows significant gains over the rule-
based method in precision and F
1
, more than dou-
bling precision for both exact and relaxed match-
ing. There is still a drop in performance when go-
ing from relaxed to exact matching, again largely
due to the reliance on the syntactic parser.
9 Classifying Background Information
BACKGROUND sentences contain contextual in-
formation, such as whether or not a patient has
been diagnosed with the focal disease or what
symptoms they are experiencing. This informa-
tion was annotated at the sentence level, partly be-
cause of annotation convenience, but also because
phrase boundaries are not always clear for medical
concepts (Hahn et al., 2012; Forbush et al., 2013).
A difficult factor in this task, and especially on
the GARD dataset, is that consumers are not al-
ways asking about a disease for themselves. In-
stead, often they ask on behalf of another individ-
ual, often a family member. The BACKGROUND
types are thus annotated based on the person of
interest, who we refer to as the patient (in the lin-
guistic sense). For instance, if a mother has a dis-
ease but is asking about her son (e.g., asking about
the probability of her son inheriting the disease),
that sentence would be a FAMILY HISTORY, as
opposed to a DIAGNOSIS sentence.
The GARD corpus is annotated with eight
BACKGROUND types:
? COMORBIDITY
? DIAGNOSIS
? FAMILY HISTORY
? ISF (information
search failure)
? LIFESTYLE
? SYMPTOM
? TEST
? TREATMENT
ISF sentences indicate previous attempts to find
the requested information have failed, and are a
good signal to the QA system to enable more in-
depth search strategies. LIFESTYLE sentences de-
scribe the patient?s life habits (e.g., smoking, ex-
ercise). Currently, the automatic identification of
Type P R F
1
# Anns
COMORBIDITY 0.0 0.0 0.0 23
DIAGNOSIS 80.8 80.3 80.5 690
FAMILY HISTORY 67.4 38.4 48.9 151
ISF 75.0 65.9 70.1 41
LIFESTYLE 0.0 0.0 0.0 13
SYMPTOM 76.6 48.1 59.1 320
TEST 37.5 4.9 8.7 61
TREATMENT 87.3 35.0 50.0 137
Overall: Micro-F
1
: 67.3 Macro-F
1
: 39.7
Table 5: BACKGROUND results.
BACKGROUND types has not been a major focus
of our effort as no handling exists for it within our
QA system. We report a baseline method and re-
sults here to provide some insight into the diffi-
culty of the task.
BACKGROUND types are a multi-labeling prob-
lem, so we use eight binary classifiers, one for
each type. Each classifier utilizes only unigram
and bigram features. The results for the mod-
els are shown in Table 5. COMORBIDITY and
LIFESTYLE are too rare in the data (23 and 13
instances, respectively) for the classifier to iden-
tify. DIAGNOSIS questions are identified fairly
well because this is the most common type (690
instances) and because of the constrained vocabu-
lary for expressing a diagnosis. The performance
of the rest of the types is largely proportional to
the number of instances in the data, though ISF
performs quite well given only 41 instances.
10 Conclusion
We have presented a method for decomposing
consumer health questions by recognizing six an-
notation types. Some of these types are general
enough to use in open-domain question decom-
position (BACKGROUND, IGNORE, QUESTION,
COORDINATION, EXEMPLIFICATION), while oth-
ers are targeted specifically at consumer health
questions (FOCUS and the BACKGROUND sub-
types). We demonstrate that ML methods can
improve upon heuristic methods relying on the
syntactic parse tree, though parse errors are of-
ten difficult to overcome. Since significant im-
provements in performance would likely require
major advances in open-domain syntactic parsing,
we instead envision further integration of the key
tasks in consumer health question analysis: (1) in-
tegration of co-reference and implicit argument in-
formation, (2) improved identification of BACK-
GROUND types, and (3) identification of discourse
relations within questions to further leverage ques-
tion decomposition.
36
Acknowledgements
This work was supported by the intramural re-
search program at the U.S. National Library of
Medicine, National Institutes of Health. We would
additionally like to thank Stephanie M. Morri-
son and Janine Lewis for their help accessing the
GARD data.
References
Ulrich Andersen, Anna Braasch, Lina Henriksen,
Csaba Huszka, Anders Johannsen, Lars Kayser,
Bente Maegaard, Ole Norgaard, Stefan Schulz, and
J?urgen Wedekind. 2012. Creation and use of Lan-
guage Resources in a Question-Answering eHealth
System. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation,
pages 2536?2542.
Brian L. Cairns, Rodney D. Nielsen, James J. Masanz,
James H. Martin, Martha S. Palmer, Wayne H. Ward,
and Guergana K. Savova. 2011. The MiPACQ Clin-
ical Question Answering System. In Proceedings of
the AMIA Annual Symposium, pages 171?180.
YongGang Cao, Feifan Liu, Pippa Simpson, Lamont
Antieau, Andrew Bennett, James J. Cimino, John
Ely, and Hong Yu. 2011. AskHERMES: An on-
line question answering system for complex clini-
cal questions. Journal of Biomedical Informatics,
44:277?288.
Dina Demner-Fushman and Swapna Abhyankar. 2012.
Syntactic-Semantic Frames for Clinical Cohort
Identification Queries. In Data Integration in the
Life Sciences, volume 7348 of Lecture Notes in
Computer Science, pages 100?112.
Dina Demner-Fushman and Jimmy Lin. 2007. An-
swering Clinical Questions with Knowledge-Based
and Statistical Techniques. Computational Linguis-
tics, 33(1).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. Journal
of Machine Learning Research, 9:1871?1874.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Tyler B. Forbush, Adi V. Gundlapalli, Miland N.
Palmer, Shuying Shen, Brett R. South, Guy Divita,
Marjorie Carter, Andrew Redd, Jorie M. Butler, and
Matthew Samore. 2013. ?Sitting on Pins and Nee-
dles?: Characterization of Symptom Descriptions in
Clinical Notes. In AMIA Summit on Clinical Re-
search Informatics, pages 67?71.
Udo Hahn, Elena Beisswanger, Ekaterina Buyko, Erik
Faessler, Jenny Traum?uller, Susann Schr?oder, and
Kerstin Hornbostel. 2012. Iterative Refinement
and Quality Checking of Annotation Guidelines ?
How to Deal Effectively with Semantically Sloppy
Named Entity Types, such as Pathological Phenom-
ena. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation,
pages 3881?3885.
Sanda Harabagiu, Finley Lacatusu, and Andrew Hickl.
2006. Answer Complex Questions with Random
Walk Models. In Proceedings of the 29th Annual
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, pages 220?227.
Sven Hartrumpf. 2008. Semantic Decomposition
for Question Answering. In Proceedings on the
18th European Conference on Artificial Intelligence,
pages 313?317.
Dierdre Hogan. 2007. Coordinate Noun Phrase Dis-
ambiguation in a Generative Parsing Model. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 680?687.
John Judge, Aoife Cahill, and Josef van Genabith.
2006. QuestionBank: Creating a Corpus of Parse-
Annotated Questions. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 497?504.
Yarden Katz and Bernardo C. Grau. 2005. Repre-
senting Qualitative Spatial Information in OWL-DL.
Proceedings of OWL: Experiences and Directions.
Halil Kilicoglu, Marcelo Fiszman, and Dina Demner-
Fushman. 2013. Interpreting Consumer Health
Questions: The Role of Anaphora and Ellipsis. In
Proceedings of the 2013 BioNLP Workshop, pages
54?62.
Finley Lacatusu, Andrew Hickl, and Sanda Harabagiu.
2006. Impact of Question Decomposition on the
Quality of Answer Summaries. In Proceedings of
LREC, pages 1147?1152.
Donald A.B. Lindberg, Betsy L. Humphreys, and
Alexa T. McCray. 1993. The Unified Medical Lan-
guage System. Methods of Information in Medicine,
32(4):281?291.
Alexa T McCray, Anita Burgun, and Olivier Boden-
reider. 2001. Aggregating UMLS Semantic Types
for Reducing Conceptual Complexity. In Studies
in Health Technology and Informatics (MEDINFO),
volume 84(1), pages 216?220.
Kirk Roberts, Kate Masterton, Marcelo Fiszman, Halil
Kilicoglu, and Dina Demner-Fushman. 2014. An-
notating Question Decomposition on Complex Med-
ical Questions. In Proceedings of LREC.
Hong Yu and YongGang Cao. 2008. Automatically
Extracting Information Needs from Ad Hoc Clini-
cal Questions. In Proceedings of the AMIA Annual
Symposium.
37
